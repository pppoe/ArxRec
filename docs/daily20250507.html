<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250506.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SMORE: Simultaneous Map and Object REconstruction", "author": "Nathaniel Chodosh and Anish Madan and Simon Lucey and Deva Ramanan", "abstract": "  We present a method for dynamic surface reconstruction of large-scale urban\nscenes from LiDAR. Depth-based reconstructions tend to focus on small-scale\nobjects or large-scale SLAM reconstructions that treat moving objects as\noutliers. We take a holistic perspective and optimize a compositional model of\na dynamic scene that decomposes the world into rigidly-moving objects and the\nbackground. To achieve this, we take inspiration from recent novel view\nsynthesis methods and frame the reconstruction problem as a global optimization\nover neural surfaces, ego poses, and object poses, which minimizes the error\nbetween composed spacetime surfaces and input LiDAR scans. In contrast to view\nsynthesis methods, which typically minimize 2D errors with gradient descent, we\nminimize a 3D point-to-surface error by coordinate descent, which we decompose\ninto registration and surface reconstruction steps. Each step can be handled\nwell by off-the-shelf methods without any re-training. We analyze the surface\nreconstruction step for rolling-shutter LiDARs, and show that deskewing\noperations common in continuous time SLAM can be applied to dynamic objects as\nwell, improving results over prior art by an order of magnitude. Beyond\npursuing dynamic reconstruction as a goal in and of itself, we propose that\nsuch a system can be used to auto-label partially annotated sequences and\nproduce ground truth annotation for hard-to-label problems such as depth\ncompletion and scene flow. Please see https://anishmadan23.github.io/smore/ for\nmore visual results.\n", "link": "http://arxiv.org/abs/2406.13896v4", "date": "2025-05-06", "relevancy": 3.1048, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6593}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6321}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMORE%3A%20Simultaneous%20Map%20and%20Object%20REconstruction&body=Title%3A%20SMORE%3A%20Simultaneous%20Map%20and%20Object%20REconstruction%0AAuthor%3A%20Nathaniel%20Chodosh%20and%20Anish%20Madan%20and%20Simon%20Lucey%20and%20Deva%20Ramanan%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20dynamic%20surface%20reconstruction%20of%20large-scale%20urban%0Ascenes%20from%20LiDAR.%20Depth-based%20reconstructions%20tend%20to%20focus%20on%20small-scale%0Aobjects%20or%20large-scale%20SLAM%20reconstructions%20that%20treat%20moving%20objects%20as%0Aoutliers.%20We%20take%20a%20holistic%20perspective%20and%20optimize%20a%20compositional%20model%20of%0Aa%20dynamic%20scene%20that%20decomposes%20the%20world%20into%20rigidly-moving%20objects%20and%20the%0Abackground.%20To%20achieve%20this%2C%20we%20take%20inspiration%20from%20recent%20novel%20view%0Asynthesis%20methods%20and%20frame%20the%20reconstruction%20problem%20as%20a%20global%20optimization%0Aover%20neural%20surfaces%2C%20ego%20poses%2C%20and%20object%20poses%2C%20which%20minimizes%20the%20error%0Abetween%20composed%20spacetime%20surfaces%20and%20input%20LiDAR%20scans.%20In%20contrast%20to%20view%0Asynthesis%20methods%2C%20which%20typically%20minimize%202D%20errors%20with%20gradient%20descent%2C%20we%0Aminimize%20a%203D%20point-to-surface%20error%20by%20coordinate%20descent%2C%20which%20we%20decompose%0Ainto%20registration%20and%20surface%20reconstruction%20steps.%20Each%20step%20can%20be%20handled%0Awell%20by%20off-the-shelf%20methods%20without%20any%20re-training.%20We%20analyze%20the%20surface%0Areconstruction%20step%20for%20rolling-shutter%20LiDARs%2C%20and%20show%20that%20deskewing%0Aoperations%20common%20in%20continuous%20time%20SLAM%20can%20be%20applied%20to%20dynamic%20objects%20as%0Awell%2C%20improving%20results%20over%20prior%20art%20by%20an%20order%20of%20magnitude.%20Beyond%0Apursuing%20dynamic%20reconstruction%20as%20a%20goal%20in%20and%20of%20itself%2C%20we%20propose%20that%0Asuch%20a%20system%20can%20be%20used%20to%20auto-label%20partially%20annotated%20sequences%20and%0Aproduce%20ground%20truth%20annotation%20for%20hard-to-label%20problems%20such%20as%20depth%0Acompletion%20and%20scene%20flow.%20Please%20see%20https%3A//anishmadan23.github.io/smore/%20for%0Amore%20visual%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13896v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMORE%253A%2520Simultaneous%2520Map%2520and%2520Object%2520REconstruction%26entry.906535625%3DNathaniel%2520Chodosh%2520and%2520Anish%2520Madan%2520and%2520Simon%2520Lucey%2520and%2520Deva%2520Ramanan%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520dynamic%2520surface%2520reconstruction%2520of%2520large-scale%2520urban%250Ascenes%2520from%2520LiDAR.%2520Depth-based%2520reconstructions%2520tend%2520to%2520focus%2520on%2520small-scale%250Aobjects%2520or%2520large-scale%2520SLAM%2520reconstructions%2520that%2520treat%2520moving%2520objects%2520as%250Aoutliers.%2520We%2520take%2520a%2520holistic%2520perspective%2520and%2520optimize%2520a%2520compositional%2520model%2520of%250Aa%2520dynamic%2520scene%2520that%2520decomposes%2520the%2520world%2520into%2520rigidly-moving%2520objects%2520and%2520the%250Abackground.%2520To%2520achieve%2520this%252C%2520we%2520take%2520inspiration%2520from%2520recent%2520novel%2520view%250Asynthesis%2520methods%2520and%2520frame%2520the%2520reconstruction%2520problem%2520as%2520a%2520global%2520optimization%250Aover%2520neural%2520surfaces%252C%2520ego%2520poses%252C%2520and%2520object%2520poses%252C%2520which%2520minimizes%2520the%2520error%250Abetween%2520composed%2520spacetime%2520surfaces%2520and%2520input%2520LiDAR%2520scans.%2520In%2520contrast%2520to%2520view%250Asynthesis%2520methods%252C%2520which%2520typically%2520minimize%25202D%2520errors%2520with%2520gradient%2520descent%252C%2520we%250Aminimize%2520a%25203D%2520point-to-surface%2520error%2520by%2520coordinate%2520descent%252C%2520which%2520we%2520decompose%250Ainto%2520registration%2520and%2520surface%2520reconstruction%2520steps.%2520Each%2520step%2520can%2520be%2520handled%250Awell%2520by%2520off-the-shelf%2520methods%2520without%2520any%2520re-training.%2520We%2520analyze%2520the%2520surface%250Areconstruction%2520step%2520for%2520rolling-shutter%2520LiDARs%252C%2520and%2520show%2520that%2520deskewing%250Aoperations%2520common%2520in%2520continuous%2520time%2520SLAM%2520can%2520be%2520applied%2520to%2520dynamic%2520objects%2520as%250Awell%252C%2520improving%2520results%2520over%2520prior%2520art%2520by%2520an%2520order%2520of%2520magnitude.%2520Beyond%250Apursuing%2520dynamic%2520reconstruction%2520as%2520a%2520goal%2520in%2520and%2520of%2520itself%252C%2520we%2520propose%2520that%250Asuch%2520a%2520system%2520can%2520be%2520used%2520to%2520auto-label%2520partially%2520annotated%2520sequences%2520and%250Aproduce%2520ground%2520truth%2520annotation%2520for%2520hard-to-label%2520problems%2520such%2520as%2520depth%250Acompletion%2520and%2520scene%2520flow.%2520Please%2520see%2520https%253A//anishmadan23.github.io/smore/%2520for%250Amore%2520visual%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13896v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMORE%3A%20Simultaneous%20Map%20and%20Object%20REconstruction&entry.906535625=Nathaniel%20Chodosh%20and%20Anish%20Madan%20and%20Simon%20Lucey%20and%20Deva%20Ramanan&entry.1292438233=%20%20We%20present%20a%20method%20for%20dynamic%20surface%20reconstruction%20of%20large-scale%20urban%0Ascenes%20from%20LiDAR.%20Depth-based%20reconstructions%20tend%20to%20focus%20on%20small-scale%0Aobjects%20or%20large-scale%20SLAM%20reconstructions%20that%20treat%20moving%20objects%20as%0Aoutliers.%20We%20take%20a%20holistic%20perspective%20and%20optimize%20a%20compositional%20model%20of%0Aa%20dynamic%20scene%20that%20decomposes%20the%20world%20into%20rigidly-moving%20objects%20and%20the%0Abackground.%20To%20achieve%20this%2C%20we%20take%20inspiration%20from%20recent%20novel%20view%0Asynthesis%20methods%20and%20frame%20the%20reconstruction%20problem%20as%20a%20global%20optimization%0Aover%20neural%20surfaces%2C%20ego%20poses%2C%20and%20object%20poses%2C%20which%20minimizes%20the%20error%0Abetween%20composed%20spacetime%20surfaces%20and%20input%20LiDAR%20scans.%20In%20contrast%20to%20view%0Asynthesis%20methods%2C%20which%20typically%20minimize%202D%20errors%20with%20gradient%20descent%2C%20we%0Aminimize%20a%203D%20point-to-surface%20error%20by%20coordinate%20descent%2C%20which%20we%20decompose%0Ainto%20registration%20and%20surface%20reconstruction%20steps.%20Each%20step%20can%20be%20handled%0Awell%20by%20off-the-shelf%20methods%20without%20any%20re-training.%20We%20analyze%20the%20surface%0Areconstruction%20step%20for%20rolling-shutter%20LiDARs%2C%20and%20show%20that%20deskewing%0Aoperations%20common%20in%20continuous%20time%20SLAM%20can%20be%20applied%20to%20dynamic%20objects%20as%0Awell%2C%20improving%20results%20over%20prior%20art%20by%20an%20order%20of%20magnitude.%20Beyond%0Apursuing%20dynamic%20reconstruction%20as%20a%20goal%20in%20and%20of%20itself%2C%20we%20propose%20that%0Asuch%20a%20system%20can%20be%20used%20to%20auto-label%20partially%20annotated%20sequences%20and%0Aproduce%20ground%20truth%20annotation%20for%20hard-to-label%20problems%20such%20as%20depth%0Acompletion%20and%20scene%20flow.%20Please%20see%20https%3A//anishmadan23.github.io/smore/%20for%0Amore%20visual%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13896v4&entry.124074799=Read"},
{"title": "LiftFeat: 3D Geometry-Aware Local Feature Matching", "author": "Yepeng Liu and Wenpeng Lai and Zhou Zhao and Yuxuan Xiong and Jinchi Zhu and Jun Cheng and Yongchao Xu", "abstract": "  Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled \\textit{LiftFeat}, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.\n", "link": "http://arxiv.org/abs/2505.03422v1", "date": "2025-05-06", "relevancy": 3.0118, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6855}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5622}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiftFeat%3A%203D%20Geometry-Aware%20Local%20Feature%20Matching&body=Title%3A%20LiftFeat%3A%203D%20Geometry-Aware%20Local%20Feature%20Matching%0AAuthor%3A%20Yepeng%20Liu%20and%20Wenpeng%20Lai%20and%20Zhou%20Zhao%20and%20Yuxuan%20Xiong%20and%20Jinchi%20Zhu%20and%20Jun%20Cheng%20and%20Yongchao%20Xu%0AAbstract%3A%20%20%20Robust%20and%20efficient%20local%20feature%20matching%20plays%20a%20crucial%20role%20in%0Aapplications%20such%20as%20SLAM%20and%20visual%20localization%20for%20robotics.%20Despite%20great%0Aprogress%2C%20it%20is%20still%20very%20challenging%20to%20extract%20robust%20and%20discriminative%0Avisual%20features%20in%20scenarios%20with%20drastic%20lighting%20changes%2C%20low%20texture%20areas%2C%0Aor%20repetitive%20patterns.%20In%20this%20paper%2C%20we%20propose%20a%20new%20lightweight%20network%0Acalled%20%5Ctextit%7BLiftFeat%7D%2C%20which%20lifts%20the%20robustness%20of%20raw%20descriptor%20by%0Aaggregating%203D%20geometric%20feature.%20Specifically%2C%20we%20first%20adopt%20a%20pre-trained%0Amonocular%20depth%20estimation%20model%20to%20generate%20pseudo%20surface%20normal%20label%2C%0Asupervising%20the%20extraction%20of%203D%20geometric%20feature%20in%20terms%20of%20predicted%0Asurface%20normal.%20We%20then%20design%20a%203D%20geometry-aware%20feature%20lifting%20module%20to%0Afuse%20surface%20normal%20feature%20with%20raw%202D%20descriptor%20feature.%20Integrating%20such%203D%0Ageometric%20feature%20enhances%20the%20discriminative%20ability%20of%202D%20feature%20description%0Ain%20extreme%20conditions.%20Extensive%20experimental%20results%20on%20relative%20pose%0Aestimation%2C%20homography%20estimation%2C%20and%20visual%20localization%20tasks%2C%20demonstrate%0Athat%20our%20LiftFeat%20outperforms%20some%20lightweight%20state-of-the-art%20methods.%20Code%0Awill%20be%20released%20at%20%3A%20https%3A//github.com/lyp-deeplearning/LiftFeat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiftFeat%253A%25203D%2520Geometry-Aware%2520Local%2520Feature%2520Matching%26entry.906535625%3DYepeng%2520Liu%2520and%2520Wenpeng%2520Lai%2520and%2520Zhou%2520Zhao%2520and%2520Yuxuan%2520Xiong%2520and%2520Jinchi%2520Zhu%2520and%2520Jun%2520Cheng%2520and%2520Yongchao%2520Xu%26entry.1292438233%3D%2520%2520Robust%2520and%2520efficient%2520local%2520feature%2520matching%2520plays%2520a%2520crucial%2520role%2520in%250Aapplications%2520such%2520as%2520SLAM%2520and%2520visual%2520localization%2520for%2520robotics.%2520Despite%2520great%250Aprogress%252C%2520it%2520is%2520still%2520very%2520challenging%2520to%2520extract%2520robust%2520and%2520discriminative%250Avisual%2520features%2520in%2520scenarios%2520with%2520drastic%2520lighting%2520changes%252C%2520low%2520texture%2520areas%252C%250Aor%2520repetitive%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520lightweight%2520network%250Acalled%2520%255Ctextit%257BLiftFeat%257D%252C%2520which%2520lifts%2520the%2520robustness%2520of%2520raw%2520descriptor%2520by%250Aaggregating%25203D%2520geometric%2520feature.%2520Specifically%252C%2520we%2520first%2520adopt%2520a%2520pre-trained%250Amonocular%2520depth%2520estimation%2520model%2520to%2520generate%2520pseudo%2520surface%2520normal%2520label%252C%250Asupervising%2520the%2520extraction%2520of%25203D%2520geometric%2520feature%2520in%2520terms%2520of%2520predicted%250Asurface%2520normal.%2520We%2520then%2520design%2520a%25203D%2520geometry-aware%2520feature%2520lifting%2520module%2520to%250Afuse%2520surface%2520normal%2520feature%2520with%2520raw%25202D%2520descriptor%2520feature.%2520Integrating%2520such%25203D%250Ageometric%2520feature%2520enhances%2520the%2520discriminative%2520ability%2520of%25202D%2520feature%2520description%250Ain%2520extreme%2520conditions.%2520Extensive%2520experimental%2520results%2520on%2520relative%2520pose%250Aestimation%252C%2520homography%2520estimation%252C%2520and%2520visual%2520localization%2520tasks%252C%2520demonstrate%250Athat%2520our%2520LiftFeat%2520outperforms%2520some%2520lightweight%2520state-of-the-art%2520methods.%2520Code%250Awill%2520be%2520released%2520at%2520%253A%2520https%253A//github.com/lyp-deeplearning/LiftFeat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiftFeat%3A%203D%20Geometry-Aware%20Local%20Feature%20Matching&entry.906535625=Yepeng%20Liu%20and%20Wenpeng%20Lai%20and%20Zhou%20Zhao%20and%20Yuxuan%20Xiong%20and%20Jinchi%20Zhu%20and%20Jun%20Cheng%20and%20Yongchao%20Xu&entry.1292438233=%20%20Robust%20and%20efficient%20local%20feature%20matching%20plays%20a%20crucial%20role%20in%0Aapplications%20such%20as%20SLAM%20and%20visual%20localization%20for%20robotics.%20Despite%20great%0Aprogress%2C%20it%20is%20still%20very%20challenging%20to%20extract%20robust%20and%20discriminative%0Avisual%20features%20in%20scenarios%20with%20drastic%20lighting%20changes%2C%20low%20texture%20areas%2C%0Aor%20repetitive%20patterns.%20In%20this%20paper%2C%20we%20propose%20a%20new%20lightweight%20network%0Acalled%20%5Ctextit%7BLiftFeat%7D%2C%20which%20lifts%20the%20robustness%20of%20raw%20descriptor%20by%0Aaggregating%203D%20geometric%20feature.%20Specifically%2C%20we%20first%20adopt%20a%20pre-trained%0Amonocular%20depth%20estimation%20model%20to%20generate%20pseudo%20surface%20normal%20label%2C%0Asupervising%20the%20extraction%20of%203D%20geometric%20feature%20in%20terms%20of%20predicted%0Asurface%20normal.%20We%20then%20design%20a%203D%20geometry-aware%20feature%20lifting%20module%20to%0Afuse%20surface%20normal%20feature%20with%20raw%202D%20descriptor%20feature.%20Integrating%20such%203D%0Ageometric%20feature%20enhances%20the%20discriminative%20ability%20of%202D%20feature%20description%0Ain%20extreme%20conditions.%20Extensive%20experimental%20results%20on%20relative%20pose%0Aestimation%2C%20homography%20estimation%2C%20and%20visual%20localization%20tasks%2C%20demonstrate%0Athat%20our%20LiftFeat%20outperforms%20some%20lightweight%20state-of-the-art%20methods.%20Code%0Awill%20be%20released%20at%20%3A%20https%3A//github.com/lyp-deeplearning/LiftFeat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03422v1&entry.124074799=Read"},
{"title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis", "author": "Vibhas Vats and Md. Alimoor Reza and David Crandall and Soon-heung Jung", "abstract": "  Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.\n", "link": "http://arxiv.org/abs/2505.03470v1", "date": "2025-05-06", "relevancy": 3.0037, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6152}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5964}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blending%203D%20Geometry%20and%20Machine%20Learning%20for%20Multi-View%20Stereopsis&body=Title%3A%20Blending%203D%20Geometry%20and%20Machine%20Learning%20for%20Multi-View%20Stereopsis%0AAuthor%3A%20Vibhas%20Vats%20and%20Md.%20Alimoor%20Reza%20and%20David%20Crandall%20and%20Soon-heung%20Jung%0AAbstract%3A%20%20%20Traditional%20multi-view%20stereo%20%28MVS%29%20methods%20primarily%20depend%20on%20photometric%0Aand%20geometric%20consistency%20constraints.%20In%20contrast%2C%20modern%20learning-based%0Aalgorithms%20often%20rely%20on%20the%20plane%20sweep%20algorithm%20to%20infer%203D%20geometry%2C%0Aapplying%20explicit%20geometric%20consistency%20%28GC%29%20checks%20only%20as%20a%20post-processing%0Astep%2C%20with%20no%20impact%20on%20the%20learning%20process%20itself.%20In%20this%20work%2C%20we%20introduce%0AGC%20MVSNet%20plus%20plus%2C%20a%20novel%20approach%20that%20actively%20enforces%20geometric%0Aconsistency%20of%20reference%20view%20depth%20maps%20across%20multiple%20source%20views%20%28multi%0Aview%29%20and%20at%20various%20scales%20%28multi%20scale%29%20during%20the%20learning%20phase%20%28see%20Fig.%0A1%29.%20This%20integrated%20GC%20check%20significantly%20accelerates%20the%20learning%20process%20by%0Adirectly%20penalizing%20geometrically%20inconsistent%20pixels%2C%20effectively%20halving%20the%0Anumber%20of%20training%20iterations%20compared%20to%20other%20MVS%20methods.%20Furthermore%2C%20we%0Aintroduce%20a%20densely%20connected%20cost%20regularization%20network%20with%20two%20distinct%0Ablock%20designs%20simple%20and%20feature%20dense%20optimized%20to%20harness%20dense%20feature%0Aconnections%20for%20enhanced%20regularization.%20Extensive%20experiments%20demonstrate%20that%0Aour%20approach%20achieves%20a%20new%20state%20of%20the%20art%20on%20the%20DTU%20and%20BlendedMVS%20datasets%0Aand%20secures%20second%20place%20on%20the%20Tanks%20and%20Temples%20benchmark.%20To%20our%20knowledge%2C%0AGC%20MVSNet%20plus%20plus%20is%20the%20first%20method%20to%20enforce%20multi-view%2C%20multi-scale%0Asupervised%20geometric%20consistency%20during%20learning.%20Our%20code%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlending%25203D%2520Geometry%2520and%2520Machine%2520Learning%2520for%2520Multi-View%2520Stereopsis%26entry.906535625%3DVibhas%2520Vats%2520and%2520Md.%2520Alimoor%2520Reza%2520and%2520David%2520Crandall%2520and%2520Soon-heung%2520Jung%26entry.1292438233%3D%2520%2520Traditional%2520multi-view%2520stereo%2520%2528MVS%2529%2520methods%2520primarily%2520depend%2520on%2520photometric%250Aand%2520geometric%2520consistency%2520constraints.%2520In%2520contrast%252C%2520modern%2520learning-based%250Aalgorithms%2520often%2520rely%2520on%2520the%2520plane%2520sweep%2520algorithm%2520to%2520infer%25203D%2520geometry%252C%250Aapplying%2520explicit%2520geometric%2520consistency%2520%2528GC%2529%2520checks%2520only%2520as%2520a%2520post-processing%250Astep%252C%2520with%2520no%2520impact%2520on%2520the%2520learning%2520process%2520itself.%2520In%2520this%2520work%252C%2520we%2520introduce%250AGC%2520MVSNet%2520plus%2520plus%252C%2520a%2520novel%2520approach%2520that%2520actively%2520enforces%2520geometric%250Aconsistency%2520of%2520reference%2520view%2520depth%2520maps%2520across%2520multiple%2520source%2520views%2520%2528multi%250Aview%2529%2520and%2520at%2520various%2520scales%2520%2528multi%2520scale%2529%2520during%2520the%2520learning%2520phase%2520%2528see%2520Fig.%250A1%2529.%2520This%2520integrated%2520GC%2520check%2520significantly%2520accelerates%2520the%2520learning%2520process%2520by%250Adirectly%2520penalizing%2520geometrically%2520inconsistent%2520pixels%252C%2520effectively%2520halving%2520the%250Anumber%2520of%2520training%2520iterations%2520compared%2520to%2520other%2520MVS%2520methods.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520densely%2520connected%2520cost%2520regularization%2520network%2520with%2520two%2520distinct%250Ablock%2520designs%2520simple%2520and%2520feature%2520dense%2520optimized%2520to%2520harness%2520dense%2520feature%250Aconnections%2520for%2520enhanced%2520regularization.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520approach%2520achieves%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520the%2520DTU%2520and%2520BlendedMVS%2520datasets%250Aand%2520secures%2520second%2520place%2520on%2520the%2520Tanks%2520and%2520Temples%2520benchmark.%2520To%2520our%2520knowledge%252C%250AGC%2520MVSNet%2520plus%2520plus%2520is%2520the%2520first%2520method%2520to%2520enforce%2520multi-view%252C%2520multi-scale%250Asupervised%2520geometric%2520consistency%2520during%2520learning.%2520Our%2520code%2520is%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blending%203D%20Geometry%20and%20Machine%20Learning%20for%20Multi-View%20Stereopsis&entry.906535625=Vibhas%20Vats%20and%20Md.%20Alimoor%20Reza%20and%20David%20Crandall%20and%20Soon-heung%20Jung&entry.1292438233=%20%20Traditional%20multi-view%20stereo%20%28MVS%29%20methods%20primarily%20depend%20on%20photometric%0Aand%20geometric%20consistency%20constraints.%20In%20contrast%2C%20modern%20learning-based%0Aalgorithms%20often%20rely%20on%20the%20plane%20sweep%20algorithm%20to%20infer%203D%20geometry%2C%0Aapplying%20explicit%20geometric%20consistency%20%28GC%29%20checks%20only%20as%20a%20post-processing%0Astep%2C%20with%20no%20impact%20on%20the%20learning%20process%20itself.%20In%20this%20work%2C%20we%20introduce%0AGC%20MVSNet%20plus%20plus%2C%20a%20novel%20approach%20that%20actively%20enforces%20geometric%0Aconsistency%20of%20reference%20view%20depth%20maps%20across%20multiple%20source%20views%20%28multi%0Aview%29%20and%20at%20various%20scales%20%28multi%20scale%29%20during%20the%20learning%20phase%20%28see%20Fig.%0A1%29.%20This%20integrated%20GC%20check%20significantly%20accelerates%20the%20learning%20process%20by%0Adirectly%20penalizing%20geometrically%20inconsistent%20pixels%2C%20effectively%20halving%20the%0Anumber%20of%20training%20iterations%20compared%20to%20other%20MVS%20methods.%20Furthermore%2C%20we%0Aintroduce%20a%20densely%20connected%20cost%20regularization%20network%20with%20two%20distinct%0Ablock%20designs%20simple%20and%20feature%20dense%20optimized%20to%20harness%20dense%20feature%0Aconnections%20for%20enhanced%20regularization.%20Extensive%20experiments%20demonstrate%20that%0Aour%20approach%20achieves%20a%20new%20state%20of%20the%20art%20on%20the%20DTU%20and%20BlendedMVS%20datasets%0Aand%20secures%20second%20place%20on%20the%20Tanks%20and%20Temples%20benchmark.%20To%20our%20knowledge%2C%0AGC%20MVSNet%20plus%20plus%20is%20the%20first%20method%20to%20enforce%20multi-view%2C%20multi-scale%0Asupervised%20geometric%20consistency%20during%20learning.%20Our%20code%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03470v1&entry.124074799=Read"},
{"title": "Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for\n  Self-Supervised RGB-T Tracking", "author": "Shenglan Li and Rui Yao and Yong Zhou and Hancheng Zhu and Kunyang Sun and Bing Liu and Zhiwen Shao and Jiaqi Zhao", "abstract": "  To reduce the reliance on large-scale annotations, self-supervised RGB-T\ntracking approaches have garnered significant attention. However, the omission\nof the object region by erroneous pseudo-label or the introduction of\nbackground noise affects the efficiency of modality fusion, while pseudo-label\nnoise triggered by similar object noise can further affect the tracking\nperformance. In this paper, we propose GDSTrack, a novel approach that\nintroduces dynamic graph fusion and temporal diffusion to address the above\nchallenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the\nmodalities of neighboring frames, treats them as distractor noise, and\nleverages the denoising capability of a generative model. Specifically, by\nconstructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the\nproposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic\nadjacency matrix to guide graph attention, focusing on and fusing the object's\ncoherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features\nfrom neighboring frames as interference, and thus improving robustness against\nsimilar-object noise. Extensive experiments conducted on four public RGB-T\ntracking datasets demonstrate that GDSTrack outperforms the existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/LiShenglana/GDSTrack.\n", "link": "http://arxiv.org/abs/2505.03507v1", "date": "2025-05-06", "relevancy": 2.9136, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5906}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5854}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-Guided%20Dynamic%20Graph%20Fusion%20and%20Temporal%20Diffusion%20for%0A%20%20Self-Supervised%20RGB-T%20Tracking&body=Title%3A%20Modality-Guided%20Dynamic%20Graph%20Fusion%20and%20Temporal%20Diffusion%20for%0A%20%20Self-Supervised%20RGB-T%20Tracking%0AAuthor%3A%20Shenglan%20Li%20and%20Rui%20Yao%20and%20Yong%20Zhou%20and%20Hancheng%20Zhu%20and%20Kunyang%20Sun%20and%20Bing%20Liu%20and%20Zhiwen%20Shao%20and%20Jiaqi%20Zhao%0AAbstract%3A%20%20%20To%20reduce%20the%20reliance%20on%20large-scale%20annotations%2C%20self-supervised%20RGB-T%0Atracking%20approaches%20have%20garnered%20significant%20attention.%20However%2C%20the%20omission%0Aof%20the%20object%20region%20by%20erroneous%20pseudo-label%20or%20the%20introduction%20of%0Abackground%20noise%20affects%20the%20efficiency%20of%20modality%20fusion%2C%20while%20pseudo-label%0Anoise%20triggered%20by%20similar%20object%20noise%20can%20further%20affect%20the%20tracking%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20GDSTrack%2C%20a%20novel%20approach%20that%0Aintroduces%20dynamic%20graph%20fusion%20and%20temporal%20diffusion%20to%20address%20the%20above%0Achallenges%20in%20self-supervised%20RGB-T%20tracking.%20GDSTrack%20dynamically%20fuses%20the%0Amodalities%20of%20neighboring%20frames%2C%20treats%20them%20as%20distractor%20noise%2C%20and%0Aleverages%20the%20denoising%20capability%20of%20a%20generative%20model.%20Specifically%2C%20by%0Aconstructing%20an%20adjacency%20matrix%20via%20an%20Adjacency%20Matrix%20Generator%20%28AMG%29%2C%20the%0Aproposed%20Modality-guided%20Dynamic%20Graph%20Fusion%20%28MDGF%29%20module%20uses%20a%20dynamic%0Aadjacency%20matrix%20to%20guide%20graph%20attention%2C%20focusing%20on%20and%20fusing%20the%20object%27s%0Acoherent%20regions.%20Temporal%20Graph-Informed%20Diffusion%20%28TGID%29%20models%20MDGF%20features%0Afrom%20neighboring%20frames%20as%20interference%2C%20and%20thus%20improving%20robustness%20against%0Asimilar-object%20noise.%20Extensive%20experiments%20conducted%20on%20four%20public%20RGB-T%0Atracking%20datasets%20demonstrate%20that%20GDSTrack%20outperforms%20the%20existing%0Astate-of-the-art%20methods.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/LiShenglana/GDSTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-Guided%2520Dynamic%2520Graph%2520Fusion%2520and%2520Temporal%2520Diffusion%2520for%250A%2520%2520Self-Supervised%2520RGB-T%2520Tracking%26entry.906535625%3DShenglan%2520Li%2520and%2520Rui%2520Yao%2520and%2520Yong%2520Zhou%2520and%2520Hancheng%2520Zhu%2520and%2520Kunyang%2520Sun%2520and%2520Bing%2520Liu%2520and%2520Zhiwen%2520Shao%2520and%2520Jiaqi%2520Zhao%26entry.1292438233%3D%2520%2520To%2520reduce%2520the%2520reliance%2520on%2520large-scale%2520annotations%252C%2520self-supervised%2520RGB-T%250Atracking%2520approaches%2520have%2520garnered%2520significant%2520attention.%2520However%252C%2520the%2520omission%250Aof%2520the%2520object%2520region%2520by%2520erroneous%2520pseudo-label%2520or%2520the%2520introduction%2520of%250Abackground%2520noise%2520affects%2520the%2520efficiency%2520of%2520modality%2520fusion%252C%2520while%2520pseudo-label%250Anoise%2520triggered%2520by%2520similar%2520object%2520noise%2520can%2520further%2520affect%2520the%2520tracking%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GDSTrack%252C%2520a%2520novel%2520approach%2520that%250Aintroduces%2520dynamic%2520graph%2520fusion%2520and%2520temporal%2520diffusion%2520to%2520address%2520the%2520above%250Achallenges%2520in%2520self-supervised%2520RGB-T%2520tracking.%2520GDSTrack%2520dynamically%2520fuses%2520the%250Amodalities%2520of%2520neighboring%2520frames%252C%2520treats%2520them%2520as%2520distractor%2520noise%252C%2520and%250Aleverages%2520the%2520denoising%2520capability%2520of%2520a%2520generative%2520model.%2520Specifically%252C%2520by%250Aconstructing%2520an%2520adjacency%2520matrix%2520via%2520an%2520Adjacency%2520Matrix%2520Generator%2520%2528AMG%2529%252C%2520the%250Aproposed%2520Modality-guided%2520Dynamic%2520Graph%2520Fusion%2520%2528MDGF%2529%2520module%2520uses%2520a%2520dynamic%250Aadjacency%2520matrix%2520to%2520guide%2520graph%2520attention%252C%2520focusing%2520on%2520and%2520fusing%2520the%2520object%2527s%250Acoherent%2520regions.%2520Temporal%2520Graph-Informed%2520Diffusion%2520%2528TGID%2529%2520models%2520MDGF%2520features%250Afrom%2520neighboring%2520frames%2520as%2520interference%252C%2520and%2520thus%2520improving%2520robustness%2520against%250Asimilar-object%2520noise.%2520Extensive%2520experiments%2520conducted%2520on%2520four%2520public%2520RGB-T%250Atracking%2520datasets%2520demonstrate%2520that%2520GDSTrack%2520outperforms%2520the%2520existing%250Astate-of-the-art%2520methods.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiShenglana/GDSTrack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-Guided%20Dynamic%20Graph%20Fusion%20and%20Temporal%20Diffusion%20for%0A%20%20Self-Supervised%20RGB-T%20Tracking&entry.906535625=Shenglan%20Li%20and%20Rui%20Yao%20and%20Yong%20Zhou%20and%20Hancheng%20Zhu%20and%20Kunyang%20Sun%20and%20Bing%20Liu%20and%20Zhiwen%20Shao%20and%20Jiaqi%20Zhao&entry.1292438233=%20%20To%20reduce%20the%20reliance%20on%20large-scale%20annotations%2C%20self-supervised%20RGB-T%0Atracking%20approaches%20have%20garnered%20significant%20attention.%20However%2C%20the%20omission%0Aof%20the%20object%20region%20by%20erroneous%20pseudo-label%20or%20the%20introduction%20of%0Abackground%20noise%20affects%20the%20efficiency%20of%20modality%20fusion%2C%20while%20pseudo-label%0Anoise%20triggered%20by%20similar%20object%20noise%20can%20further%20affect%20the%20tracking%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20GDSTrack%2C%20a%20novel%20approach%20that%0Aintroduces%20dynamic%20graph%20fusion%20and%20temporal%20diffusion%20to%20address%20the%20above%0Achallenges%20in%20self-supervised%20RGB-T%20tracking.%20GDSTrack%20dynamically%20fuses%20the%0Amodalities%20of%20neighboring%20frames%2C%20treats%20them%20as%20distractor%20noise%2C%20and%0Aleverages%20the%20denoising%20capability%20of%20a%20generative%20model.%20Specifically%2C%20by%0Aconstructing%20an%20adjacency%20matrix%20via%20an%20Adjacency%20Matrix%20Generator%20%28AMG%29%2C%20the%0Aproposed%20Modality-guided%20Dynamic%20Graph%20Fusion%20%28MDGF%29%20module%20uses%20a%20dynamic%0Aadjacency%20matrix%20to%20guide%20graph%20attention%2C%20focusing%20on%20and%20fusing%20the%20object%27s%0Acoherent%20regions.%20Temporal%20Graph-Informed%20Diffusion%20%28TGID%29%20models%20MDGF%20features%0Afrom%20neighboring%20frames%20as%20interference%2C%20and%20thus%20improving%20robustness%20against%0Asimilar-object%20noise.%20Extensive%20experiments%20conducted%20on%20four%20public%20RGB-T%0Atracking%20datasets%20demonstrate%20that%20GDSTrack%20outperforms%20the%20existing%0Astate-of-the-art%20methods.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/LiShenglana/GDSTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03507v1&entry.124074799=Read"},
{"title": "AquaticVision: Benchmarking Visual SLAM in Underwater Environment with\n  Events and Frames", "author": "Yifan Peng and Yuze Hong and Ziyang Hong and Apple Pui-Yi Chui and Junfeng Wu", "abstract": "  Many underwater applications, such as offshore asset inspections, rely on\nvisual inspection and detailed 3D reconstruction. Recent advancements in\nunderwater visual SLAM systems for aquatic environments have garnered\nsignificant attention in marine robotics research. However, existing underwater\nvisual SLAM datasets often lack groundtruth trajectory data, making it\ndifficult to objectively compare the performance of different SLAM algorithms\nbased solely on qualitative results or COLMAP reconstruction. In this paper, we\npresent a novel underwater dataset that includes ground truth trajectory data\nobtained using a motion capture system. Additionally, for the first time, we\nrelease visual data that includes both events and frames for benchmarking\nunderwater visual positioning. By providing event camera data, we aim to\nfacilitate the development of more robust and advanced underwater visual SLAM\nalgorithms. The use of event cameras can help mitigate challenges posed by\nextremely low light or hazy underwater conditions. The webpage of our dataset\nis https://sites.google.com/view/aquaticvision-lias.\n", "link": "http://arxiv.org/abs/2505.03448v1", "date": "2025-05-06", "relevancy": 2.8764, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5873}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AquaticVision%3A%20Benchmarking%20Visual%20SLAM%20in%20Underwater%20Environment%20with%0A%20%20Events%20and%20Frames&body=Title%3A%20AquaticVision%3A%20Benchmarking%20Visual%20SLAM%20in%20Underwater%20Environment%20with%0A%20%20Events%20and%20Frames%0AAuthor%3A%20Yifan%20Peng%20and%20Yuze%20Hong%20and%20Ziyang%20Hong%20and%20Apple%20Pui-Yi%20Chui%20and%20Junfeng%20Wu%0AAbstract%3A%20%20%20Many%20underwater%20applications%2C%20such%20as%20offshore%20asset%20inspections%2C%20rely%20on%0Avisual%20inspection%20and%20detailed%203D%20reconstruction.%20Recent%20advancements%20in%0Aunderwater%20visual%20SLAM%20systems%20for%20aquatic%20environments%20have%20garnered%0Asignificant%20attention%20in%20marine%20robotics%20research.%20However%2C%20existing%20underwater%0Avisual%20SLAM%20datasets%20often%20lack%20groundtruth%20trajectory%20data%2C%20making%20it%0Adifficult%20to%20objectively%20compare%20the%20performance%20of%20different%20SLAM%20algorithms%0Abased%20solely%20on%20qualitative%20results%20or%20COLMAP%20reconstruction.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20underwater%20dataset%20that%20includes%20ground%20truth%20trajectory%20data%0Aobtained%20using%20a%20motion%20capture%20system.%20Additionally%2C%20for%20the%20first%20time%2C%20we%0Arelease%20visual%20data%20that%20includes%20both%20events%20and%20frames%20for%20benchmarking%0Aunderwater%20visual%20positioning.%20By%20providing%20event%20camera%20data%2C%20we%20aim%20to%0Afacilitate%20the%20development%20of%20more%20robust%20and%20advanced%20underwater%20visual%20SLAM%0Aalgorithms.%20The%20use%20of%20event%20cameras%20can%20help%20mitigate%20challenges%20posed%20by%0Aextremely%20low%20light%20or%20hazy%20underwater%20conditions.%20The%20webpage%20of%20our%20dataset%0Ais%20https%3A//sites.google.com/view/aquaticvision-lias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAquaticVision%253A%2520Benchmarking%2520Visual%2520SLAM%2520in%2520Underwater%2520Environment%2520with%250A%2520%2520Events%2520and%2520Frames%26entry.906535625%3DYifan%2520Peng%2520and%2520Yuze%2520Hong%2520and%2520Ziyang%2520Hong%2520and%2520Apple%2520Pui-Yi%2520Chui%2520and%2520Junfeng%2520Wu%26entry.1292438233%3D%2520%2520Many%2520underwater%2520applications%252C%2520such%2520as%2520offshore%2520asset%2520inspections%252C%2520rely%2520on%250Avisual%2520inspection%2520and%2520detailed%25203D%2520reconstruction.%2520Recent%2520advancements%2520in%250Aunderwater%2520visual%2520SLAM%2520systems%2520for%2520aquatic%2520environments%2520have%2520garnered%250Asignificant%2520attention%2520in%2520marine%2520robotics%2520research.%2520However%252C%2520existing%2520underwater%250Avisual%2520SLAM%2520datasets%2520often%2520lack%2520groundtruth%2520trajectory%2520data%252C%2520making%2520it%250Adifficult%2520to%2520objectively%2520compare%2520the%2520performance%2520of%2520different%2520SLAM%2520algorithms%250Abased%2520solely%2520on%2520qualitative%2520results%2520or%2520COLMAP%2520reconstruction.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520novel%2520underwater%2520dataset%2520that%2520includes%2520ground%2520truth%2520trajectory%2520data%250Aobtained%2520using%2520a%2520motion%2520capture%2520system.%2520Additionally%252C%2520for%2520the%2520first%2520time%252C%2520we%250Arelease%2520visual%2520data%2520that%2520includes%2520both%2520events%2520and%2520frames%2520for%2520benchmarking%250Aunderwater%2520visual%2520positioning.%2520By%2520providing%2520event%2520camera%2520data%252C%2520we%2520aim%2520to%250Afacilitate%2520the%2520development%2520of%2520more%2520robust%2520and%2520advanced%2520underwater%2520visual%2520SLAM%250Aalgorithms.%2520The%2520use%2520of%2520event%2520cameras%2520can%2520help%2520mitigate%2520challenges%2520posed%2520by%250Aextremely%2520low%2520light%2520or%2520hazy%2520underwater%2520conditions.%2520The%2520webpage%2520of%2520our%2520dataset%250Ais%2520https%253A//sites.google.com/view/aquaticvision-lias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AquaticVision%3A%20Benchmarking%20Visual%20SLAM%20in%20Underwater%20Environment%20with%0A%20%20Events%20and%20Frames&entry.906535625=Yifan%20Peng%20and%20Yuze%20Hong%20and%20Ziyang%20Hong%20and%20Apple%20Pui-Yi%20Chui%20and%20Junfeng%20Wu&entry.1292438233=%20%20Many%20underwater%20applications%2C%20such%20as%20offshore%20asset%20inspections%2C%20rely%20on%0Avisual%20inspection%20and%20detailed%203D%20reconstruction.%20Recent%20advancements%20in%0Aunderwater%20visual%20SLAM%20systems%20for%20aquatic%20environments%20have%20garnered%0Asignificant%20attention%20in%20marine%20robotics%20research.%20However%2C%20existing%20underwater%0Avisual%20SLAM%20datasets%20often%20lack%20groundtruth%20trajectory%20data%2C%20making%20it%0Adifficult%20to%20objectively%20compare%20the%20performance%20of%20different%20SLAM%20algorithms%0Abased%20solely%20on%20qualitative%20results%20or%20COLMAP%20reconstruction.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20underwater%20dataset%20that%20includes%20ground%20truth%20trajectory%20data%0Aobtained%20using%20a%20motion%20capture%20system.%20Additionally%2C%20for%20the%20first%20time%2C%20we%0Arelease%20visual%20data%20that%20includes%20both%20events%20and%20frames%20for%20benchmarking%0Aunderwater%20visual%20positioning.%20By%20providing%20event%20camera%20data%2C%20we%20aim%20to%0Afacilitate%20the%20development%20of%20more%20robust%20and%20advanced%20underwater%20visual%20SLAM%0Aalgorithms.%20The%20use%20of%20event%20cameras%20can%20help%20mitigate%20challenges%20posed%20by%0Aextremely%20low%20light%20or%20hazy%20underwater%20conditions.%20The%20webpage%20of%20our%20dataset%0Ais%20https%3A//sites.google.com/view/aquaticvision-lias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03448v1&entry.124074799=Read"},
{"title": "Polar Coordinate-Based 2D Pose Prior with Neural Distance Field", "author": "Qi Gan and Sao Mai Nguyen and Eric Fenaux and Stephan Cl\u00e9men\u00e7on and Moun\u00eem El Yacoubi", "abstract": "  Human pose capture is essential for sports analysis, enabling precise\nevaluation of athletes' movements. While deep learning-based human pose\nestimation (HPE) models from RGB videos have achieved impressive performance on\npublic datasets, their effectiveness in real-world sports scenarios is often\nhindered by motion blur, occlusions, and domain shifts across different pose\nrepresentations. Fine-tuning these models can partially alleviate such\nchallenges but typically requires large-scale annotated data and still\nstruggles to generalize across diverse sports environments. To address these\nlimitations, we propose a 2D pose prior-guided refinement approach based on\nNeural Distance Fields (NDF). Unlike existing approaches that rely solely on\nangular representations of human poses, we introduce a polar coordinate-based\nrepresentation that explicitly incorporates joint connection lengths, enabling\na more accurate correction of erroneous pose estimations. Additionally, we\ndefine a novel non-geodesic distance metric that separates angular and radial\ndiscrepancies, which we demonstrate is better suited for polar representations\nthan traditional geodesic distances. To mitigate data scarcity, we develop a\ngradient-based batch-projection augmentation strategy, which synthesizes\nrealistic pose samples through iterative refinement. Our method is evaluated on\na long jump dataset, demonstrating its ability to improve 2D pose estimation\nacross multiple pose representations, making it robust across different\ndomains. Experimental results show that our approach enhances pose plausibility\nwhile requiring only limited training data. Code is available at:\nhttps://github.com/QGAN2019/polar-NDF.\n", "link": "http://arxiv.org/abs/2505.03445v1", "date": "2025-05-06", "relevancy": 2.8731, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6024}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.563}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polar%20Coordinate-Based%202D%20Pose%20Prior%20with%20Neural%20Distance%20Field&body=Title%3A%20Polar%20Coordinate-Based%202D%20Pose%20Prior%20with%20Neural%20Distance%20Field%0AAuthor%3A%20Qi%20Gan%20and%20Sao%20Mai%20Nguyen%20and%20Eric%20Fenaux%20and%20Stephan%20Cl%C3%A9men%C3%A7on%20and%20Moun%C3%AEm%20El%20Yacoubi%0AAbstract%3A%20%20%20Human%20pose%20capture%20is%20essential%20for%20sports%20analysis%2C%20enabling%20precise%0Aevaluation%20of%20athletes%27%20movements.%20While%20deep%20learning-based%20human%20pose%0Aestimation%20%28HPE%29%20models%20from%20RGB%20videos%20have%20achieved%20impressive%20performance%20on%0Apublic%20datasets%2C%20their%20effectiveness%20in%20real-world%20sports%20scenarios%20is%20often%0Ahindered%20by%20motion%20blur%2C%20occlusions%2C%20and%20domain%20shifts%20across%20different%20pose%0Arepresentations.%20Fine-tuning%20these%20models%20can%20partially%20alleviate%20such%0Achallenges%20but%20typically%20requires%20large-scale%20annotated%20data%20and%20still%0Astruggles%20to%20generalize%20across%20diverse%20sports%20environments.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%202D%20pose%20prior-guided%20refinement%20approach%20based%20on%0ANeural%20Distance%20Fields%20%28NDF%29.%20Unlike%20existing%20approaches%20that%20rely%20solely%20on%0Aangular%20representations%20of%20human%20poses%2C%20we%20introduce%20a%20polar%20coordinate-based%0Arepresentation%20that%20explicitly%20incorporates%20joint%20connection%20lengths%2C%20enabling%0Aa%20more%20accurate%20correction%20of%20erroneous%20pose%20estimations.%20Additionally%2C%20we%0Adefine%20a%20novel%20non-geodesic%20distance%20metric%20that%20separates%20angular%20and%20radial%0Adiscrepancies%2C%20which%20we%20demonstrate%20is%20better%20suited%20for%20polar%20representations%0Athan%20traditional%20geodesic%20distances.%20To%20mitigate%20data%20scarcity%2C%20we%20develop%20a%0Agradient-based%20batch-projection%20augmentation%20strategy%2C%20which%20synthesizes%0Arealistic%20pose%20samples%20through%20iterative%20refinement.%20Our%20method%20is%20evaluated%20on%0Aa%20long%20jump%20dataset%2C%20demonstrating%20its%20ability%20to%20improve%202D%20pose%20estimation%0Aacross%20multiple%20pose%20representations%2C%20making%20it%20robust%20across%20different%0Adomains.%20Experimental%20results%20show%20that%20our%20approach%20enhances%20pose%20plausibility%0Awhile%20requiring%20only%20limited%20training%20data.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/QGAN2019/polar-NDF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolar%2520Coordinate-Based%25202D%2520Pose%2520Prior%2520with%2520Neural%2520Distance%2520Field%26entry.906535625%3DQi%2520Gan%2520and%2520Sao%2520Mai%2520Nguyen%2520and%2520Eric%2520Fenaux%2520and%2520Stephan%2520Cl%25C3%25A9men%25C3%25A7on%2520and%2520Moun%25C3%25AEm%2520El%2520Yacoubi%26entry.1292438233%3D%2520%2520Human%2520pose%2520capture%2520is%2520essential%2520for%2520sports%2520analysis%252C%2520enabling%2520precise%250Aevaluation%2520of%2520athletes%2527%2520movements.%2520While%2520deep%2520learning-based%2520human%2520pose%250Aestimation%2520%2528HPE%2529%2520models%2520from%2520RGB%2520videos%2520have%2520achieved%2520impressive%2520performance%2520on%250Apublic%2520datasets%252C%2520their%2520effectiveness%2520in%2520real-world%2520sports%2520scenarios%2520is%2520often%250Ahindered%2520by%2520motion%2520blur%252C%2520occlusions%252C%2520and%2520domain%2520shifts%2520across%2520different%2520pose%250Arepresentations.%2520Fine-tuning%2520these%2520models%2520can%2520partially%2520alleviate%2520such%250Achallenges%2520but%2520typically%2520requires%2520large-scale%2520annotated%2520data%2520and%2520still%250Astruggles%2520to%2520generalize%2520across%2520diverse%2520sports%2520environments.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%25202D%2520pose%2520prior-guided%2520refinement%2520approach%2520based%2520on%250ANeural%2520Distance%2520Fields%2520%2528NDF%2529.%2520Unlike%2520existing%2520approaches%2520that%2520rely%2520solely%2520on%250Aangular%2520representations%2520of%2520human%2520poses%252C%2520we%2520introduce%2520a%2520polar%2520coordinate-based%250Arepresentation%2520that%2520explicitly%2520incorporates%2520joint%2520connection%2520lengths%252C%2520enabling%250Aa%2520more%2520accurate%2520correction%2520of%2520erroneous%2520pose%2520estimations.%2520Additionally%252C%2520we%250Adefine%2520a%2520novel%2520non-geodesic%2520distance%2520metric%2520that%2520separates%2520angular%2520and%2520radial%250Adiscrepancies%252C%2520which%2520we%2520demonstrate%2520is%2520better%2520suited%2520for%2520polar%2520representations%250Athan%2520traditional%2520geodesic%2520distances.%2520To%2520mitigate%2520data%2520scarcity%252C%2520we%2520develop%2520a%250Agradient-based%2520batch-projection%2520augmentation%2520strategy%252C%2520which%2520synthesizes%250Arealistic%2520pose%2520samples%2520through%2520iterative%2520refinement.%2520Our%2520method%2520is%2520evaluated%2520on%250Aa%2520long%2520jump%2520dataset%252C%2520demonstrating%2520its%2520ability%2520to%2520improve%25202D%2520pose%2520estimation%250Aacross%2520multiple%2520pose%2520representations%252C%2520making%2520it%2520robust%2520across%2520different%250Adomains.%2520Experimental%2520results%2520show%2520that%2520our%2520approach%2520enhances%2520pose%2520plausibility%250Awhile%2520requiring%2520only%2520limited%2520training%2520data.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/QGAN2019/polar-NDF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polar%20Coordinate-Based%202D%20Pose%20Prior%20with%20Neural%20Distance%20Field&entry.906535625=Qi%20Gan%20and%20Sao%20Mai%20Nguyen%20and%20Eric%20Fenaux%20and%20Stephan%20Cl%C3%A9men%C3%A7on%20and%20Moun%C3%AEm%20El%20Yacoubi&entry.1292438233=%20%20Human%20pose%20capture%20is%20essential%20for%20sports%20analysis%2C%20enabling%20precise%0Aevaluation%20of%20athletes%27%20movements.%20While%20deep%20learning-based%20human%20pose%0Aestimation%20%28HPE%29%20models%20from%20RGB%20videos%20have%20achieved%20impressive%20performance%20on%0Apublic%20datasets%2C%20their%20effectiveness%20in%20real-world%20sports%20scenarios%20is%20often%0Ahindered%20by%20motion%20blur%2C%20occlusions%2C%20and%20domain%20shifts%20across%20different%20pose%0Arepresentations.%20Fine-tuning%20these%20models%20can%20partially%20alleviate%20such%0Achallenges%20but%20typically%20requires%20large-scale%20annotated%20data%20and%20still%0Astruggles%20to%20generalize%20across%20diverse%20sports%20environments.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%202D%20pose%20prior-guided%20refinement%20approach%20based%20on%0ANeural%20Distance%20Fields%20%28NDF%29.%20Unlike%20existing%20approaches%20that%20rely%20solely%20on%0Aangular%20representations%20of%20human%20poses%2C%20we%20introduce%20a%20polar%20coordinate-based%0Arepresentation%20that%20explicitly%20incorporates%20joint%20connection%20lengths%2C%20enabling%0Aa%20more%20accurate%20correction%20of%20erroneous%20pose%20estimations.%20Additionally%2C%20we%0Adefine%20a%20novel%20non-geodesic%20distance%20metric%20that%20separates%20angular%20and%20radial%0Adiscrepancies%2C%20which%20we%20demonstrate%20is%20better%20suited%20for%20polar%20representations%0Athan%20traditional%20geodesic%20distances.%20To%20mitigate%20data%20scarcity%2C%20we%20develop%20a%0Agradient-based%20batch-projection%20augmentation%20strategy%2C%20which%20synthesizes%0Arealistic%20pose%20samples%20through%20iterative%20refinement.%20Our%20method%20is%20evaluated%20on%0Aa%20long%20jump%20dataset%2C%20demonstrating%20its%20ability%20to%20improve%202D%20pose%20estimation%0Aacross%20multiple%20pose%20representations%2C%20making%20it%20robust%20across%20different%0Adomains.%20Experimental%20results%20show%20that%20our%20approach%20enhances%20pose%20plausibility%0Awhile%20requiring%20only%20limited%20training%20data.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/QGAN2019/polar-NDF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03445v1&entry.124074799=Read"},
{"title": "Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack\n  Detection", "author": "Fangling Jiang and Qi Li and Bing Liu and Weining Wang and Caifeng Shan and Zhenan Sun and Ming-Hsuan Yang", "abstract": "  3D mask presentation attack detection is crucial for protecting face\nrecognition systems against the rising threat of 3D mask attacks. While most\nexisting methods utilize multimodal features or remote photoplethysmography\n(rPPG) signals to distinguish between real faces and 3D masks, they face\nsignificant challenges, such as the high costs associated with multimodal\nsensors and limited generalization ability. Detection-related text descriptions\noffer concise, universal information and are cost-effective to obtain. However,\nthe potential of vision-language multimodal features for 3D mask presentation\nattack detection remains unexplored. In this paper, we propose a novel\nknowledge-based prompt learning framework to explore the strong generalization\ncapability of vision-language models for 3D mask presentation attack detection.\nSpecifically, our approach incorporates entities and triples from knowledge\ngraphs into the prompt learning process, generating fine-grained, task-specific\nexplicit prompts that effectively harness the knowledge embedded in pre-trained\nvision-language models. Furthermore, considering different input images may\nemphasize distinct knowledge graph elements, we introduce a visual-specific\nknowledge filter based on an attention mechanism to refine relevant elements\naccording to the visual context. Additionally, we leverage causal graph theory\ninsights into the prompt learning process to further enhance the generalization\nability of our method. During training, a spurious correlation elimination\nparadigm is employed, which removes category-irrelevant local image patches\nusing guidance from knowledge-based text features, fostering the learning of\ngeneralized causal prompts that align with category-relevant local patches.\nExperimental results demonstrate that the proposed method achieves\nstate-of-the-art intra- and cross-scenario detection performance on benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2505.03610v1", "date": "2025-05-06", "relevancy": 2.8572, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Knowledge-based%20Prompts%20for%20Robust%203D%20Mask%20Presentation%20Attack%0A%20%20Detection&body=Title%3A%20Learning%20Knowledge-based%20Prompts%20for%20Robust%203D%20Mask%20Presentation%20Attack%0A%20%20Detection%0AAuthor%3A%20Fangling%20Jiang%20and%20Qi%20Li%20and%20Bing%20Liu%20and%20Weining%20Wang%20and%20Caifeng%20Shan%20and%20Zhenan%20Sun%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%203D%20mask%20presentation%20attack%20detection%20is%20crucial%20for%20protecting%20face%0Arecognition%20systems%20against%20the%20rising%20threat%20of%203D%20mask%20attacks.%20While%20most%0Aexisting%20methods%20utilize%20multimodal%20features%20or%20remote%20photoplethysmography%0A%28rPPG%29%20signals%20to%20distinguish%20between%20real%20faces%20and%203D%20masks%2C%20they%20face%0Asignificant%20challenges%2C%20such%20as%20the%20high%20costs%20associated%20with%20multimodal%0Asensors%20and%20limited%20generalization%20ability.%20Detection-related%20text%20descriptions%0Aoffer%20concise%2C%20universal%20information%20and%20are%20cost-effective%20to%20obtain.%20However%2C%0Athe%20potential%20of%20vision-language%20multimodal%20features%20for%203D%20mask%20presentation%0Aattack%20detection%20remains%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aknowledge-based%20prompt%20learning%20framework%20to%20explore%20the%20strong%20generalization%0Acapability%20of%20vision-language%20models%20for%203D%20mask%20presentation%20attack%20detection.%0ASpecifically%2C%20our%20approach%20incorporates%20entities%20and%20triples%20from%20knowledge%0Agraphs%20into%20the%20prompt%20learning%20process%2C%20generating%20fine-grained%2C%20task-specific%0Aexplicit%20prompts%20that%20effectively%20harness%20the%20knowledge%20embedded%20in%20pre-trained%0Avision-language%20models.%20Furthermore%2C%20considering%20different%20input%20images%20may%0Aemphasize%20distinct%20knowledge%20graph%20elements%2C%20we%20introduce%20a%20visual-specific%0Aknowledge%20filter%20based%20on%20an%20attention%20mechanism%20to%20refine%20relevant%20elements%0Aaccording%20to%20the%20visual%20context.%20Additionally%2C%20we%20leverage%20causal%20graph%20theory%0Ainsights%20into%20the%20prompt%20learning%20process%20to%20further%20enhance%20the%20generalization%0Aability%20of%20our%20method.%20During%20training%2C%20a%20spurious%20correlation%20elimination%0Aparadigm%20is%20employed%2C%20which%20removes%20category-irrelevant%20local%20image%20patches%0Ausing%20guidance%20from%20knowledge-based%20text%20features%2C%20fostering%20the%20learning%20of%0Ageneralized%20causal%20prompts%20that%20align%20with%20category-relevant%20local%20patches.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20method%20achieves%0Astate-of-the-art%20intra-%20and%20cross-scenario%20detection%20performance%20on%20benchmark%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Knowledge-based%2520Prompts%2520for%2520Robust%25203D%2520Mask%2520Presentation%2520Attack%250A%2520%2520Detection%26entry.906535625%3DFangling%2520Jiang%2520and%2520Qi%2520Li%2520and%2520Bing%2520Liu%2520and%2520Weining%2520Wang%2520and%2520Caifeng%2520Shan%2520and%2520Zhenan%2520Sun%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%25203D%2520mask%2520presentation%2520attack%2520detection%2520is%2520crucial%2520for%2520protecting%2520face%250Arecognition%2520systems%2520against%2520the%2520rising%2520threat%2520of%25203D%2520mask%2520attacks.%2520While%2520most%250Aexisting%2520methods%2520utilize%2520multimodal%2520features%2520or%2520remote%2520photoplethysmography%250A%2528rPPG%2529%2520signals%2520to%2520distinguish%2520between%2520real%2520faces%2520and%25203D%2520masks%252C%2520they%2520face%250Asignificant%2520challenges%252C%2520such%2520as%2520the%2520high%2520costs%2520associated%2520with%2520multimodal%250Asensors%2520and%2520limited%2520generalization%2520ability.%2520Detection-related%2520text%2520descriptions%250Aoffer%2520concise%252C%2520universal%2520information%2520and%2520are%2520cost-effective%2520to%2520obtain.%2520However%252C%250Athe%2520potential%2520of%2520vision-language%2520multimodal%2520features%2520for%25203D%2520mask%2520presentation%250Aattack%2520detection%2520remains%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aknowledge-based%2520prompt%2520learning%2520framework%2520to%2520explore%2520the%2520strong%2520generalization%250Acapability%2520of%2520vision-language%2520models%2520for%25203D%2520mask%2520presentation%2520attack%2520detection.%250ASpecifically%252C%2520our%2520approach%2520incorporates%2520entities%2520and%2520triples%2520from%2520knowledge%250Agraphs%2520into%2520the%2520prompt%2520learning%2520process%252C%2520generating%2520fine-grained%252C%2520task-specific%250Aexplicit%2520prompts%2520that%2520effectively%2520harness%2520the%2520knowledge%2520embedded%2520in%2520pre-trained%250Avision-language%2520models.%2520Furthermore%252C%2520considering%2520different%2520input%2520images%2520may%250Aemphasize%2520distinct%2520knowledge%2520graph%2520elements%252C%2520we%2520introduce%2520a%2520visual-specific%250Aknowledge%2520filter%2520based%2520on%2520an%2520attention%2520mechanism%2520to%2520refine%2520relevant%2520elements%250Aaccording%2520to%2520the%2520visual%2520context.%2520Additionally%252C%2520we%2520leverage%2520causal%2520graph%2520theory%250Ainsights%2520into%2520the%2520prompt%2520learning%2520process%2520to%2520further%2520enhance%2520the%2520generalization%250Aability%2520of%2520our%2520method.%2520During%2520training%252C%2520a%2520spurious%2520correlation%2520elimination%250Aparadigm%2520is%2520employed%252C%2520which%2520removes%2520category-irrelevant%2520local%2520image%2520patches%250Ausing%2520guidance%2520from%2520knowledge-based%2520text%2520features%252C%2520fostering%2520the%2520learning%2520of%250Ageneralized%2520causal%2520prompts%2520that%2520align%2520with%2520category-relevant%2520local%2520patches.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%250Astate-of-the-art%2520intra-%2520and%2520cross-scenario%2520detection%2520performance%2520on%2520benchmark%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Knowledge-based%20Prompts%20for%20Robust%203D%20Mask%20Presentation%20Attack%0A%20%20Detection&entry.906535625=Fangling%20Jiang%20and%20Qi%20Li%20and%20Bing%20Liu%20and%20Weining%20Wang%20and%20Caifeng%20Shan%20and%20Zhenan%20Sun%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%203D%20mask%20presentation%20attack%20detection%20is%20crucial%20for%20protecting%20face%0Arecognition%20systems%20against%20the%20rising%20threat%20of%203D%20mask%20attacks.%20While%20most%0Aexisting%20methods%20utilize%20multimodal%20features%20or%20remote%20photoplethysmography%0A%28rPPG%29%20signals%20to%20distinguish%20between%20real%20faces%20and%203D%20masks%2C%20they%20face%0Asignificant%20challenges%2C%20such%20as%20the%20high%20costs%20associated%20with%20multimodal%0Asensors%20and%20limited%20generalization%20ability.%20Detection-related%20text%20descriptions%0Aoffer%20concise%2C%20universal%20information%20and%20are%20cost-effective%20to%20obtain.%20However%2C%0Athe%20potential%20of%20vision-language%20multimodal%20features%20for%203D%20mask%20presentation%0Aattack%20detection%20remains%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aknowledge-based%20prompt%20learning%20framework%20to%20explore%20the%20strong%20generalization%0Acapability%20of%20vision-language%20models%20for%203D%20mask%20presentation%20attack%20detection.%0ASpecifically%2C%20our%20approach%20incorporates%20entities%20and%20triples%20from%20knowledge%0Agraphs%20into%20the%20prompt%20learning%20process%2C%20generating%20fine-grained%2C%20task-specific%0Aexplicit%20prompts%20that%20effectively%20harness%20the%20knowledge%20embedded%20in%20pre-trained%0Avision-language%20models.%20Furthermore%2C%20considering%20different%20input%20images%20may%0Aemphasize%20distinct%20knowledge%20graph%20elements%2C%20we%20introduce%20a%20visual-specific%0Aknowledge%20filter%20based%20on%20an%20attention%20mechanism%20to%20refine%20relevant%20elements%0Aaccording%20to%20the%20visual%20context.%20Additionally%2C%20we%20leverage%20causal%20graph%20theory%0Ainsights%20into%20the%20prompt%20learning%20process%20to%20further%20enhance%20the%20generalization%0Aability%20of%20our%20method.%20During%20training%2C%20a%20spurious%20correlation%20elimination%0Aparadigm%20is%20employed%2C%20which%20removes%20category-irrelevant%20local%20image%20patches%0Ausing%20guidance%20from%20knowledge-based%20text%20features%2C%20fostering%20the%20learning%20of%0Ageneralized%20causal%20prompts%20that%20align%20with%20category-relevant%20local%20patches.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20method%20achieves%0Astate-of-the-art%20intra-%20and%20cross-scenario%20detection%20performance%20on%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03610v1&entry.124074799=Read"},
{"title": "Nonperiodic dynamic CT reconstruction using backward-warping INR with\n  regularization of diffeomorphism (BIRD)", "author": "Muge Du and Zhuozhao Zheng and Wenying Wang and Guotao Quan and Wuliang Shi and Le Shen and Li Zhang and Liang Li and Yinong Liu and Yuxiang Xing", "abstract": "  Dynamic computed tomography (CT) reconstruction faces significant challenges\nin addressing motion artifacts, particularly for nonperiodic rapid movements\nsuch as cardiac imaging with fast heart rates. Traditional methods struggle\nwith the extreme limited-angle problems inherent in nonperiodic cases. Deep\nlearning methods have improved performance but face generalization challenges.\nRecent implicit neural representation (INR) techniques show promise through\nself-supervised deep learning, but have critical limitations: computational\ninefficiency due to forward-warping modeling, difficulty balancing DVF\ncomplexity with anatomical plausibility, and challenges in preserving fine\ndetails without additional patient-specific pre-scans. This paper presents a\nnovel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It\naddresses these challenges through four key contributions: (1) backward-warping\ndeformation that enables direct computation of each dynamic voxel with\nsignificantly reduced computational cost, (2) diffeomorphism-based DVF\nregularization that ensures anatomically plausible deformations while\nmaintaining representational capacity, (3) motion-compensated analytical\nreconstruction that enhances fine details without requiring additional\npre-scans, and (4) dimensional-reduction design for efficient 4D coordinate\nencoding. Through various simulations and practical studies, including digital\nand physical phantoms and retrospective patient data, we demonstrate the\neffectiveness of our approach for nonperiodic dynamic CT reconstruction with\nenhanced details and reduced motion artifacts. The proposed framework enables\nmore accurate dynamic CT reconstruction with potential clinical applications,\nsuch as one-beat cardiac reconstruction, cinematic image sequences for\nfunctional imaging, and motion artifact reduction in conventional CT scans.\n", "link": "http://arxiv.org/abs/2505.03463v1", "date": "2025-05-06", "relevancy": 2.8274, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5715}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5625}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonperiodic%20dynamic%20CT%20reconstruction%20using%20backward-warping%20INR%20with%0A%20%20regularization%20of%20diffeomorphism%20%28BIRD%29&body=Title%3A%20Nonperiodic%20dynamic%20CT%20reconstruction%20using%20backward-warping%20INR%20with%0A%20%20regularization%20of%20diffeomorphism%20%28BIRD%29%0AAuthor%3A%20Muge%20Du%20and%20Zhuozhao%20Zheng%20and%20Wenying%20Wang%20and%20Guotao%20Quan%20and%20Wuliang%20Shi%20and%20Le%20Shen%20and%20Li%20Zhang%20and%20Liang%20Li%20and%20Yinong%20Liu%20and%20Yuxiang%20Xing%0AAbstract%3A%20%20%20Dynamic%20computed%20tomography%20%28CT%29%20reconstruction%20faces%20significant%20challenges%0Ain%20addressing%20motion%20artifacts%2C%20particularly%20for%20nonperiodic%20rapid%20movements%0Asuch%20as%20cardiac%20imaging%20with%20fast%20heart%20rates.%20Traditional%20methods%20struggle%0Awith%20the%20extreme%20limited-angle%20problems%20inherent%20in%20nonperiodic%20cases.%20Deep%0Alearning%20methods%20have%20improved%20performance%20but%20face%20generalization%20challenges.%0ARecent%20implicit%20neural%20representation%20%28INR%29%20techniques%20show%20promise%20through%0Aself-supervised%20deep%20learning%2C%20but%20have%20critical%20limitations%3A%20computational%0Ainefficiency%20due%20to%20forward-warping%20modeling%2C%20difficulty%20balancing%20DVF%0Acomplexity%20with%20anatomical%20plausibility%2C%20and%20challenges%20in%20preserving%20fine%0Adetails%20without%20additional%20patient-specific%20pre-scans.%20This%20paper%20presents%20a%0Anovel%20INR-based%20framework%2C%20BIRD%2C%20for%20nonperiodic%20dynamic%20CT%20reconstruction.%20It%0Aaddresses%20these%20challenges%20through%20four%20key%20contributions%3A%20%281%29%20backward-warping%0Adeformation%20that%20enables%20direct%20computation%20of%20each%20dynamic%20voxel%20with%0Asignificantly%20reduced%20computational%20cost%2C%20%282%29%20diffeomorphism-based%20DVF%0Aregularization%20that%20ensures%20anatomically%20plausible%20deformations%20while%0Amaintaining%20representational%20capacity%2C%20%283%29%20motion-compensated%20analytical%0Areconstruction%20that%20enhances%20fine%20details%20without%20requiring%20additional%0Apre-scans%2C%20and%20%284%29%20dimensional-reduction%20design%20for%20efficient%204D%20coordinate%0Aencoding.%20Through%20various%20simulations%20and%20practical%20studies%2C%20including%20digital%0Aand%20physical%20phantoms%20and%20retrospective%20patient%20data%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20for%20nonperiodic%20dynamic%20CT%20reconstruction%20with%0Aenhanced%20details%20and%20reduced%20motion%20artifacts.%20The%20proposed%20framework%20enables%0Amore%20accurate%20dynamic%20CT%20reconstruction%20with%20potential%20clinical%20applications%2C%0Asuch%20as%20one-beat%20cardiac%20reconstruction%2C%20cinematic%20image%20sequences%20for%0Afunctional%20imaging%2C%20and%20motion%20artifact%20reduction%20in%20conventional%20CT%20scans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonperiodic%2520dynamic%2520CT%2520reconstruction%2520using%2520backward-warping%2520INR%2520with%250A%2520%2520regularization%2520of%2520diffeomorphism%2520%2528BIRD%2529%26entry.906535625%3DMuge%2520Du%2520and%2520Zhuozhao%2520Zheng%2520and%2520Wenying%2520Wang%2520and%2520Guotao%2520Quan%2520and%2520Wuliang%2520Shi%2520and%2520Le%2520Shen%2520and%2520Li%2520Zhang%2520and%2520Liang%2520Li%2520and%2520Yinong%2520Liu%2520and%2520Yuxiang%2520Xing%26entry.1292438233%3D%2520%2520Dynamic%2520computed%2520tomography%2520%2528CT%2529%2520reconstruction%2520faces%2520significant%2520challenges%250Ain%2520addressing%2520motion%2520artifacts%252C%2520particularly%2520for%2520nonperiodic%2520rapid%2520movements%250Asuch%2520as%2520cardiac%2520imaging%2520with%2520fast%2520heart%2520rates.%2520Traditional%2520methods%2520struggle%250Awith%2520the%2520extreme%2520limited-angle%2520problems%2520inherent%2520in%2520nonperiodic%2520cases.%2520Deep%250Alearning%2520methods%2520have%2520improved%2520performance%2520but%2520face%2520generalization%2520challenges.%250ARecent%2520implicit%2520neural%2520representation%2520%2528INR%2529%2520techniques%2520show%2520promise%2520through%250Aself-supervised%2520deep%2520learning%252C%2520but%2520have%2520critical%2520limitations%253A%2520computational%250Ainefficiency%2520due%2520to%2520forward-warping%2520modeling%252C%2520difficulty%2520balancing%2520DVF%250Acomplexity%2520with%2520anatomical%2520plausibility%252C%2520and%2520challenges%2520in%2520preserving%2520fine%250Adetails%2520without%2520additional%2520patient-specific%2520pre-scans.%2520This%2520paper%2520presents%2520a%250Anovel%2520INR-based%2520framework%252C%2520BIRD%252C%2520for%2520nonperiodic%2520dynamic%2520CT%2520reconstruction.%2520It%250Aaddresses%2520these%2520challenges%2520through%2520four%2520key%2520contributions%253A%2520%25281%2529%2520backward-warping%250Adeformation%2520that%2520enables%2520direct%2520computation%2520of%2520each%2520dynamic%2520voxel%2520with%250Asignificantly%2520reduced%2520computational%2520cost%252C%2520%25282%2529%2520diffeomorphism-based%2520DVF%250Aregularization%2520that%2520ensures%2520anatomically%2520plausible%2520deformations%2520while%250Amaintaining%2520representational%2520capacity%252C%2520%25283%2529%2520motion-compensated%2520analytical%250Areconstruction%2520that%2520enhances%2520fine%2520details%2520without%2520requiring%2520additional%250Apre-scans%252C%2520and%2520%25284%2529%2520dimensional-reduction%2520design%2520for%2520efficient%25204D%2520coordinate%250Aencoding.%2520Through%2520various%2520simulations%2520and%2520practical%2520studies%252C%2520including%2520digital%250Aand%2520physical%2520phantoms%2520and%2520retrospective%2520patient%2520data%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520for%2520nonperiodic%2520dynamic%2520CT%2520reconstruction%2520with%250Aenhanced%2520details%2520and%2520reduced%2520motion%2520artifacts.%2520The%2520proposed%2520framework%2520enables%250Amore%2520accurate%2520dynamic%2520CT%2520reconstruction%2520with%2520potential%2520clinical%2520applications%252C%250Asuch%2520as%2520one-beat%2520cardiac%2520reconstruction%252C%2520cinematic%2520image%2520sequences%2520for%250Afunctional%2520imaging%252C%2520and%2520motion%2520artifact%2520reduction%2520in%2520conventional%2520CT%2520scans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonperiodic%20dynamic%20CT%20reconstruction%20using%20backward-warping%20INR%20with%0A%20%20regularization%20of%20diffeomorphism%20%28BIRD%29&entry.906535625=Muge%20Du%20and%20Zhuozhao%20Zheng%20and%20Wenying%20Wang%20and%20Guotao%20Quan%20and%20Wuliang%20Shi%20and%20Le%20Shen%20and%20Li%20Zhang%20and%20Liang%20Li%20and%20Yinong%20Liu%20and%20Yuxiang%20Xing&entry.1292438233=%20%20Dynamic%20computed%20tomography%20%28CT%29%20reconstruction%20faces%20significant%20challenges%0Ain%20addressing%20motion%20artifacts%2C%20particularly%20for%20nonperiodic%20rapid%20movements%0Asuch%20as%20cardiac%20imaging%20with%20fast%20heart%20rates.%20Traditional%20methods%20struggle%0Awith%20the%20extreme%20limited-angle%20problems%20inherent%20in%20nonperiodic%20cases.%20Deep%0Alearning%20methods%20have%20improved%20performance%20but%20face%20generalization%20challenges.%0ARecent%20implicit%20neural%20representation%20%28INR%29%20techniques%20show%20promise%20through%0Aself-supervised%20deep%20learning%2C%20but%20have%20critical%20limitations%3A%20computational%0Ainefficiency%20due%20to%20forward-warping%20modeling%2C%20difficulty%20balancing%20DVF%0Acomplexity%20with%20anatomical%20plausibility%2C%20and%20challenges%20in%20preserving%20fine%0Adetails%20without%20additional%20patient-specific%20pre-scans.%20This%20paper%20presents%20a%0Anovel%20INR-based%20framework%2C%20BIRD%2C%20for%20nonperiodic%20dynamic%20CT%20reconstruction.%20It%0Aaddresses%20these%20challenges%20through%20four%20key%20contributions%3A%20%281%29%20backward-warping%0Adeformation%20that%20enables%20direct%20computation%20of%20each%20dynamic%20voxel%20with%0Asignificantly%20reduced%20computational%20cost%2C%20%282%29%20diffeomorphism-based%20DVF%0Aregularization%20that%20ensures%20anatomically%20plausible%20deformations%20while%0Amaintaining%20representational%20capacity%2C%20%283%29%20motion-compensated%20analytical%0Areconstruction%20that%20enhances%20fine%20details%20without%20requiring%20additional%0Apre-scans%2C%20and%20%284%29%20dimensional-reduction%20design%20for%20efficient%204D%20coordinate%0Aencoding.%20Through%20various%20simulations%20and%20practical%20studies%2C%20including%20digital%0Aand%20physical%20phantoms%20and%20retrospective%20patient%20data%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20for%20nonperiodic%20dynamic%20CT%20reconstruction%20with%0Aenhanced%20details%20and%20reduced%20motion%20artifacts.%20The%20proposed%20framework%20enables%0Amore%20accurate%20dynamic%20CT%20reconstruction%20with%20potential%20clinical%20applications%2C%0Asuch%20as%20one-beat%20cardiac%20reconstruction%2C%20cinematic%20image%20sequences%20for%0Afunctional%20imaging%2C%20and%20motion%20artifact%20reduction%20in%20conventional%20CT%20scans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03463v1&entry.124074799=Read"},
{"title": "From Pixels to Polygons: A Survey of Deep Learning Approaches for\n  Medical Image-to-Mesh Reconstruction", "author": "Fengming Lin and Arezoo Zakeri and Yidan Xue and Michael MacRaild and Haoran Dou and Zherui Zhou and Ziwei Zou and Ali Sarrami-Foroushani and Jinming Duan and Alejandro F. Frangi", "abstract": "  Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,\nenabling the transformation of medical imaging data into three-dimensional mesh\nmodels that are critical in computational medicine and in silico trials for\nadvancing our understanding of disease mechanisms, and diagnostic and\ntherapeutic techniques in modern medicine. This survey systematically\ncategorizes existing approaches into four main categories: template models,\nstatistical models, generative models, and implicit models. Each category is\nanalysed in detail, examining their methodological foundations, strengths,\nlimitations, and applicability to different anatomical structures and imaging\nmodalities. We provide an extensive evaluation of these methods across various\nanatomical applications, from cardiac imaging to neurological studies,\nsupported by quantitative comparisons using standard metrics. Additionally, we\ncompile and analyze major public datasets available for medical mesh\nreconstruction tasks and discuss commonly used evaluation metrics and loss\nfunctions. The survey identifies current challenges in the field, including\nrequirements for topological correctness, geometric accuracy, and\nmulti-modality integration. Finally, we present promising future research\ndirections in this domain. This systematic review aims to serve as a\ncomprehensive reference for researchers and practitioners in medical image\nanalysis and computational medicine.\n", "link": "http://arxiv.org/abs/2505.03599v1", "date": "2025-05-06", "relevancy": 2.8191, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6132}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5391}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Pixels%20to%20Polygons%3A%20A%20Survey%20of%20Deep%20Learning%20Approaches%20for%0A%20%20Medical%20Image-to-Mesh%20Reconstruction&body=Title%3A%20From%20Pixels%20to%20Polygons%3A%20A%20Survey%20of%20Deep%20Learning%20Approaches%20for%0A%20%20Medical%20Image-to-Mesh%20Reconstruction%0AAuthor%3A%20Fengming%20Lin%20and%20Arezoo%20Zakeri%20and%20Yidan%20Xue%20and%20Michael%20MacRaild%20and%20Haoran%20Dou%20and%20Zherui%20Zhou%20and%20Ziwei%20Zou%20and%20Ali%20Sarrami-Foroushani%20and%20Jinming%20Duan%20and%20Alejandro%20F.%20Frangi%0AAbstract%3A%20%20%20Deep%20learning-based%20medical%20image-to-mesh%20reconstruction%20has%20rapidly%20evolved%2C%0Aenabling%20the%20transformation%20of%20medical%20imaging%20data%20into%20three-dimensional%20mesh%0Amodels%20that%20are%20critical%20in%20computational%20medicine%20and%20in%20silico%20trials%20for%0Aadvancing%20our%20understanding%20of%20disease%20mechanisms%2C%20and%20diagnostic%20and%0Atherapeutic%20techniques%20in%20modern%20medicine.%20This%20survey%20systematically%0Acategorizes%20existing%20approaches%20into%20four%20main%20categories%3A%20template%20models%2C%0Astatistical%20models%2C%20generative%20models%2C%20and%20implicit%20models.%20Each%20category%20is%0Aanalysed%20in%20detail%2C%20examining%20their%20methodological%20foundations%2C%20strengths%2C%0Alimitations%2C%20and%20applicability%20to%20different%20anatomical%20structures%20and%20imaging%0Amodalities.%20We%20provide%20an%20extensive%20evaluation%20of%20these%20methods%20across%20various%0Aanatomical%20applications%2C%20from%20cardiac%20imaging%20to%20neurological%20studies%2C%0Asupported%20by%20quantitative%20comparisons%20using%20standard%20metrics.%20Additionally%2C%20we%0Acompile%20and%20analyze%20major%20public%20datasets%20available%20for%20medical%20mesh%0Areconstruction%20tasks%20and%20discuss%20commonly%20used%20evaluation%20metrics%20and%20loss%0Afunctions.%20The%20survey%20identifies%20current%20challenges%20in%20the%20field%2C%20including%0Arequirements%20for%20topological%20correctness%2C%20geometric%20accuracy%2C%20and%0Amulti-modality%20integration.%20Finally%2C%20we%20present%20promising%20future%20research%0Adirections%20in%20this%20domain.%20This%20systematic%20review%20aims%20to%20serve%20as%20a%0Acomprehensive%20reference%20for%20researchers%20and%20practitioners%20in%20medical%20image%0Aanalysis%20and%20computational%20medicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Pixels%2520to%2520Polygons%253A%2520A%2520Survey%2520of%2520Deep%2520Learning%2520Approaches%2520for%250A%2520%2520Medical%2520Image-to-Mesh%2520Reconstruction%26entry.906535625%3DFengming%2520Lin%2520and%2520Arezoo%2520Zakeri%2520and%2520Yidan%2520Xue%2520and%2520Michael%2520MacRaild%2520and%2520Haoran%2520Dou%2520and%2520Zherui%2520Zhou%2520and%2520Ziwei%2520Zou%2520and%2520Ali%2520Sarrami-Foroushani%2520and%2520Jinming%2520Duan%2520and%2520Alejandro%2520F.%2520Frangi%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520medical%2520image-to-mesh%2520reconstruction%2520has%2520rapidly%2520evolved%252C%250Aenabling%2520the%2520transformation%2520of%2520medical%2520imaging%2520data%2520into%2520three-dimensional%2520mesh%250Amodels%2520that%2520are%2520critical%2520in%2520computational%2520medicine%2520and%2520in%2520silico%2520trials%2520for%250Aadvancing%2520our%2520understanding%2520of%2520disease%2520mechanisms%252C%2520and%2520diagnostic%2520and%250Atherapeutic%2520techniques%2520in%2520modern%2520medicine.%2520This%2520survey%2520systematically%250Acategorizes%2520existing%2520approaches%2520into%2520four%2520main%2520categories%253A%2520template%2520models%252C%250Astatistical%2520models%252C%2520generative%2520models%252C%2520and%2520implicit%2520models.%2520Each%2520category%2520is%250Aanalysed%2520in%2520detail%252C%2520examining%2520their%2520methodological%2520foundations%252C%2520strengths%252C%250Alimitations%252C%2520and%2520applicability%2520to%2520different%2520anatomical%2520structures%2520and%2520imaging%250Amodalities.%2520We%2520provide%2520an%2520extensive%2520evaluation%2520of%2520these%2520methods%2520across%2520various%250Aanatomical%2520applications%252C%2520from%2520cardiac%2520imaging%2520to%2520neurological%2520studies%252C%250Asupported%2520by%2520quantitative%2520comparisons%2520using%2520standard%2520metrics.%2520Additionally%252C%2520we%250Acompile%2520and%2520analyze%2520major%2520public%2520datasets%2520available%2520for%2520medical%2520mesh%250Areconstruction%2520tasks%2520and%2520discuss%2520commonly%2520used%2520evaluation%2520metrics%2520and%2520loss%250Afunctions.%2520The%2520survey%2520identifies%2520current%2520challenges%2520in%2520the%2520field%252C%2520including%250Arequirements%2520for%2520topological%2520correctness%252C%2520geometric%2520accuracy%252C%2520and%250Amulti-modality%2520integration.%2520Finally%252C%2520we%2520present%2520promising%2520future%2520research%250Adirections%2520in%2520this%2520domain.%2520This%2520systematic%2520review%2520aims%2520to%2520serve%2520as%2520a%250Acomprehensive%2520reference%2520for%2520researchers%2520and%2520practitioners%2520in%2520medical%2520image%250Aanalysis%2520and%2520computational%2520medicine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Pixels%20to%20Polygons%3A%20A%20Survey%20of%20Deep%20Learning%20Approaches%20for%0A%20%20Medical%20Image-to-Mesh%20Reconstruction&entry.906535625=Fengming%20Lin%20and%20Arezoo%20Zakeri%20and%20Yidan%20Xue%20and%20Michael%20MacRaild%20and%20Haoran%20Dou%20and%20Zherui%20Zhou%20and%20Ziwei%20Zou%20and%20Ali%20Sarrami-Foroushani%20and%20Jinming%20Duan%20and%20Alejandro%20F.%20Frangi&entry.1292438233=%20%20Deep%20learning-based%20medical%20image-to-mesh%20reconstruction%20has%20rapidly%20evolved%2C%0Aenabling%20the%20transformation%20of%20medical%20imaging%20data%20into%20three-dimensional%20mesh%0Amodels%20that%20are%20critical%20in%20computational%20medicine%20and%20in%20silico%20trials%20for%0Aadvancing%20our%20understanding%20of%20disease%20mechanisms%2C%20and%20diagnostic%20and%0Atherapeutic%20techniques%20in%20modern%20medicine.%20This%20survey%20systematically%0Acategorizes%20existing%20approaches%20into%20four%20main%20categories%3A%20template%20models%2C%0Astatistical%20models%2C%20generative%20models%2C%20and%20implicit%20models.%20Each%20category%20is%0Aanalysed%20in%20detail%2C%20examining%20their%20methodological%20foundations%2C%20strengths%2C%0Alimitations%2C%20and%20applicability%20to%20different%20anatomical%20structures%20and%20imaging%0Amodalities.%20We%20provide%20an%20extensive%20evaluation%20of%20these%20methods%20across%20various%0Aanatomical%20applications%2C%20from%20cardiac%20imaging%20to%20neurological%20studies%2C%0Asupported%20by%20quantitative%20comparisons%20using%20standard%20metrics.%20Additionally%2C%20we%0Acompile%20and%20analyze%20major%20public%20datasets%20available%20for%20medical%20mesh%0Areconstruction%20tasks%20and%20discuss%20commonly%20used%20evaluation%20metrics%20and%20loss%0Afunctions.%20The%20survey%20identifies%20current%20challenges%20in%20the%20field%2C%20including%0Arequirements%20for%20topological%20correctness%2C%20geometric%20accuracy%2C%20and%0Amulti-modality%20integration.%20Finally%2C%20we%20present%20promising%20future%20research%0Adirections%20in%20this%20domain.%20This%20systematic%20review%20aims%20to%20serve%20as%20a%0Acomprehensive%20reference%20for%20researchers%20and%20practitioners%20in%20medical%20image%0Aanalysis%20and%20computational%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03599v1&entry.124074799=Read"},
{"title": "Reinforced Correlation Between Vision and Language for Precise Medical\n  AI Assistant", "author": "Haonan Wang and Jiaji Mao and Lehan Wang and Qixiang Zhang and Marawan Elbatel and Yi Qin and Huijun Hu and Baoxun Li and Wenhui Deng and Weifeng Qin and Hongrui Li and Jialin Liang and Jun Shen and Xiaomeng Li", "abstract": "  Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.\n", "link": "http://arxiv.org/abs/2505.03380v1", "date": "2025-05-06", "relevancy": 2.8073, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforced%20Correlation%20Between%20Vision%20and%20Language%20for%20Precise%20Medical%0A%20%20AI%20Assistant&body=Title%3A%20Reinforced%20Correlation%20Between%20Vision%20and%20Language%20for%20Precise%20Medical%0A%20%20AI%20Assistant%0AAuthor%3A%20Haonan%20Wang%20and%20Jiaji%20Mao%20and%20Lehan%20Wang%20and%20Qixiang%20Zhang%20and%20Marawan%20Elbatel%20and%20Yi%20Qin%20and%20Huijun%20Hu%20and%20Baoxun%20Li%20and%20Wenhui%20Deng%20and%20Weifeng%20Qin%20and%20Hongrui%20Li%20and%20Jialin%20Liang%20and%20Jun%20Shen%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20Medical%20AI%20assistants%20support%20doctors%20in%20disease%20diagnosis%2C%20medical%20image%0Aanalysis%2C%20and%20report%20generation.%20However%2C%20they%20still%20face%20significant%0Achallenges%20in%20clinical%20use%2C%20including%20limited%20accuracy%20with%20multimodal%20content%0Aand%20insufficient%20validation%20in%20real-world%20settings.%20We%20propose%20RCMed%2C%20a%0Afull-stack%20AI%20assistant%20that%20improves%20multimodal%20alignment%20in%20both%20input%20and%0Aoutput%2C%20enabling%20precise%20anatomical%20delineation%2C%20accurate%20localization%2C%20and%0Areliable%20diagnosis%20through%20hierarchical%20vision-language%20grounding.%20A%0Aself-reinforcing%20correlation%20mechanism%20allows%20visual%20features%20to%20inform%0Alanguage%20context%2C%20while%20language%20semantics%20guide%20pixel-wise%20attention%2C%20forming%0Aa%20closed%20loop%20that%20refines%20both%20modalities.%20This%20correlation%20is%20enhanced%20by%20a%0Acolor%20region%20description%20strategy%2C%20translating%20anatomical%20structures%20into%0Asemantically%20rich%20text%20to%20learn%20shape-location-text%20relationships%20across%0Ascales.%20Trained%20on%2020%20million%20image-mask-description%20triplets%2C%20RCMed%20achieves%0Astate-of-the-art%20precision%20in%20contextualizing%20irregular%20lesions%20and%20subtle%0Aanatomical%20boundaries%2C%20excelling%20in%20165%20clinical%20tasks%20across%209%20modalities.%20It%0Aachieved%20a%2023.5%25%20relative%20improvement%20in%20cell%20segmentation%20from%20microscopy%0Aimages%20over%20prior%20methods.%20RCMed%27s%20strong%20vision-language%20alignment%20enables%0Aexceptional%20generalization%2C%20with%20state-of-the-art%20performance%20in%20external%0Avalidation%20across%2020%20clinically%20significant%20cancer%20types%2C%20including%20novel%0Atasks.%20This%20work%20demonstrates%20how%20integrated%20multimodal%20models%20capture%0Afine-grained%20patterns%2C%20enabling%20human-level%20interpretation%20in%20complex%20scenarios%0Aand%20advancing%20human-centric%20AI%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforced%2520Correlation%2520Between%2520Vision%2520and%2520Language%2520for%2520Precise%2520Medical%250A%2520%2520AI%2520Assistant%26entry.906535625%3DHaonan%2520Wang%2520and%2520Jiaji%2520Mao%2520and%2520Lehan%2520Wang%2520and%2520Qixiang%2520Zhang%2520and%2520Marawan%2520Elbatel%2520and%2520Yi%2520Qin%2520and%2520Huijun%2520Hu%2520and%2520Baoxun%2520Li%2520and%2520Wenhui%2520Deng%2520and%2520Weifeng%2520Qin%2520and%2520Hongrui%2520Li%2520and%2520Jialin%2520Liang%2520and%2520Jun%2520Shen%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520Medical%2520AI%2520assistants%2520support%2520doctors%2520in%2520disease%2520diagnosis%252C%2520medical%2520image%250Aanalysis%252C%2520and%2520report%2520generation.%2520However%252C%2520they%2520still%2520face%2520significant%250Achallenges%2520in%2520clinical%2520use%252C%2520including%2520limited%2520accuracy%2520with%2520multimodal%2520content%250Aand%2520insufficient%2520validation%2520in%2520real-world%2520settings.%2520We%2520propose%2520RCMed%252C%2520a%250Afull-stack%2520AI%2520assistant%2520that%2520improves%2520multimodal%2520alignment%2520in%2520both%2520input%2520and%250Aoutput%252C%2520enabling%2520precise%2520anatomical%2520delineation%252C%2520accurate%2520localization%252C%2520and%250Areliable%2520diagnosis%2520through%2520hierarchical%2520vision-language%2520grounding.%2520A%250Aself-reinforcing%2520correlation%2520mechanism%2520allows%2520visual%2520features%2520to%2520inform%250Alanguage%2520context%252C%2520while%2520language%2520semantics%2520guide%2520pixel-wise%2520attention%252C%2520forming%250Aa%2520closed%2520loop%2520that%2520refines%2520both%2520modalities.%2520This%2520correlation%2520is%2520enhanced%2520by%2520a%250Acolor%2520region%2520description%2520strategy%252C%2520translating%2520anatomical%2520structures%2520into%250Asemantically%2520rich%2520text%2520to%2520learn%2520shape-location-text%2520relationships%2520across%250Ascales.%2520Trained%2520on%252020%2520million%2520image-mask-description%2520triplets%252C%2520RCMed%2520achieves%250Astate-of-the-art%2520precision%2520in%2520contextualizing%2520irregular%2520lesions%2520and%2520subtle%250Aanatomical%2520boundaries%252C%2520excelling%2520in%2520165%2520clinical%2520tasks%2520across%25209%2520modalities.%2520It%250Aachieved%2520a%252023.5%2525%2520relative%2520improvement%2520in%2520cell%2520segmentation%2520from%2520microscopy%250Aimages%2520over%2520prior%2520methods.%2520RCMed%2527s%2520strong%2520vision-language%2520alignment%2520enables%250Aexceptional%2520generalization%252C%2520with%2520state-of-the-art%2520performance%2520in%2520external%250Avalidation%2520across%252020%2520clinically%2520significant%2520cancer%2520types%252C%2520including%2520novel%250Atasks.%2520This%2520work%2520demonstrates%2520how%2520integrated%2520multimodal%2520models%2520capture%250Afine-grained%2520patterns%252C%2520enabling%2520human-level%2520interpretation%2520in%2520complex%2520scenarios%250Aand%2520advancing%2520human-centric%2520AI%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforced%20Correlation%20Between%20Vision%20and%20Language%20for%20Precise%20Medical%0A%20%20AI%20Assistant&entry.906535625=Haonan%20Wang%20and%20Jiaji%20Mao%20and%20Lehan%20Wang%20and%20Qixiang%20Zhang%20and%20Marawan%20Elbatel%20and%20Yi%20Qin%20and%20Huijun%20Hu%20and%20Baoxun%20Li%20and%20Wenhui%20Deng%20and%20Weifeng%20Qin%20and%20Hongrui%20Li%20and%20Jialin%20Liang%20and%20Jun%20Shen%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Medical%20AI%20assistants%20support%20doctors%20in%20disease%20diagnosis%2C%20medical%20image%0Aanalysis%2C%20and%20report%20generation.%20However%2C%20they%20still%20face%20significant%0Achallenges%20in%20clinical%20use%2C%20including%20limited%20accuracy%20with%20multimodal%20content%0Aand%20insufficient%20validation%20in%20real-world%20settings.%20We%20propose%20RCMed%2C%20a%0Afull-stack%20AI%20assistant%20that%20improves%20multimodal%20alignment%20in%20both%20input%20and%0Aoutput%2C%20enabling%20precise%20anatomical%20delineation%2C%20accurate%20localization%2C%20and%0Areliable%20diagnosis%20through%20hierarchical%20vision-language%20grounding.%20A%0Aself-reinforcing%20correlation%20mechanism%20allows%20visual%20features%20to%20inform%0Alanguage%20context%2C%20while%20language%20semantics%20guide%20pixel-wise%20attention%2C%20forming%0Aa%20closed%20loop%20that%20refines%20both%20modalities.%20This%20correlation%20is%20enhanced%20by%20a%0Acolor%20region%20description%20strategy%2C%20translating%20anatomical%20structures%20into%0Asemantically%20rich%20text%20to%20learn%20shape-location-text%20relationships%20across%0Ascales.%20Trained%20on%2020%20million%20image-mask-description%20triplets%2C%20RCMed%20achieves%0Astate-of-the-art%20precision%20in%20contextualizing%20irregular%20lesions%20and%20subtle%0Aanatomical%20boundaries%2C%20excelling%20in%20165%20clinical%20tasks%20across%209%20modalities.%20It%0Aachieved%20a%2023.5%25%20relative%20improvement%20in%20cell%20segmentation%20from%20microscopy%0Aimages%20over%20prior%20methods.%20RCMed%27s%20strong%20vision-language%20alignment%20enables%0Aexceptional%20generalization%2C%20with%20state-of-the-art%20performance%20in%20external%0Avalidation%20across%2020%20clinically%20significant%20cancer%20types%2C%20including%20novel%0Atasks.%20This%20work%20demonstrates%20how%20integrated%20multimodal%20models%20capture%0Afine-grained%20patterns%2C%20enabling%20human-level%20interpretation%20in%20complex%20scenarios%0Aand%20advancing%20human-centric%20AI%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03380v1&entry.124074799=Read"},
{"title": "Panoramic Out-of-Distribution Segmentation", "author": "Mengfei Duan and Kailun Yang and Yuheng Zhang and Yihong Cao and Fei Teng and Kai Luo and Jiaming Zhang and Zhiyong Li and Shutao Li", "abstract": "  Panoramic imaging enables capturing 360{\\deg} images with an ultra-wide\nField-of-View (FoV) for dense omnidirectional perception. However, current\npanoramic semantic segmentation methods fail to identify outliers, and pinhole\nOut-of-distribution Segmentation (OoS) models perform unsatisfactorily in the\npanoramic domain due to background clutter and pixel distortions. To address\nthese issues, we introduce a new task, Panoramic Out-of-distribution\nSegmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the\nfirst solution, POS, which adapts to the characteristics of panoramic images\nthrough text-guided prompt distribution learning. Specifically, POS integrates\na disentanglement strategy designed to materialize the cross-domain\ngeneralization capability of CLIP. The proposed Prompt-based Restoration\nAttention (PRA) optimizes semantic decoding by prompt guidance and\nself-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)\nrefines the manifold of per-pixel mask embeddings via semantic prototype\nsupervision. Besides, to compensate for the scarcity of PanOoS datasets, we\nestablish two benchmarks: DenseOoS, which features diverse outliers in complex\nenvironments, and QuadOoS, captured by a quadruped robot with a panoramic\nannular lens system. Extensive experiments demonstrate superior performance of\nPOS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,\noutperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves\nleading closed-set segmentation capabilities. Code and datasets will be\navailable at https://github.com/MengfeiD/PanOoS.\n", "link": "http://arxiv.org/abs/2505.03539v1", "date": "2025-05-06", "relevancy": 2.8006, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panoramic%20Out-of-Distribution%20Segmentation&body=Title%3A%20Panoramic%20Out-of-Distribution%20Segmentation%0AAuthor%3A%20Mengfei%20Duan%20and%20Kailun%20Yang%20and%20Yuheng%20Zhang%20and%20Yihong%20Cao%20and%20Fei%20Teng%20and%20Kai%20Luo%20and%20Jiaming%20Zhang%20and%20Zhiyong%20Li%20and%20Shutao%20Li%0AAbstract%3A%20%20%20Panoramic%20imaging%20enables%20capturing%20360%7B%5Cdeg%7D%20images%20with%20an%20ultra-wide%0AField-of-View%20%28FoV%29%20for%20dense%20omnidirectional%20perception.%20However%2C%20current%0Apanoramic%20semantic%20segmentation%20methods%20fail%20to%20identify%20outliers%2C%20and%20pinhole%0AOut-of-distribution%20Segmentation%20%28OoS%29%20models%20perform%20unsatisfactorily%20in%20the%0Apanoramic%20domain%20due%20to%20background%20clutter%20and%20pixel%20distortions.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20new%20task%2C%20Panoramic%20Out-of-distribution%0ASegmentation%20%28PanOoS%29%2C%20achieving%20OoS%20for%20panoramas.%20Furthermore%2C%20we%20propose%20the%0Afirst%20solution%2C%20POS%2C%20which%20adapts%20to%20the%20characteristics%20of%20panoramic%20images%0Athrough%20text-guided%20prompt%20distribution%20learning.%20Specifically%2C%20POS%20integrates%0Aa%20disentanglement%20strategy%20designed%20to%20materialize%20the%20cross-domain%0Ageneralization%20capability%20of%20CLIP.%20The%20proposed%20Prompt-based%20Restoration%0AAttention%20%28PRA%29%20optimizes%20semantic%20decoding%20by%20prompt%20guidance%20and%0Aself-adaptive%20correction%2C%20while%20Bilevel%20Prompt%20Distribution%20Learning%20%28BPDL%29%0Arefines%20the%20manifold%20of%20per-pixel%20mask%20embeddings%20via%20semantic%20prototype%0Asupervision.%20Besides%2C%20to%20compensate%20for%20the%20scarcity%20of%20PanOoS%20datasets%2C%20we%0Aestablish%20two%20benchmarks%3A%20DenseOoS%2C%20which%20features%20diverse%20outliers%20in%20complex%0Aenvironments%2C%20and%20QuadOoS%2C%20captured%20by%20a%20quadruped%20robot%20with%20a%20panoramic%0Aannular%20lens%20system.%20Extensive%20experiments%20demonstrate%20superior%20performance%20of%0APOS%2C%20with%20AuPRC%20improving%20by%2034.25%25%20and%20FPR95%20decreasing%20by%2021.42%25%20on%20DenseOoS%2C%0Aoutperforming%20state-of-the-art%20pinhole-OoS%20methods.%20Moreover%2C%20POS%20achieves%0Aleading%20closed-set%20segmentation%20capabilities.%20Code%20and%20datasets%20will%20be%0Aavailable%20at%20https%3A//github.com/MengfeiD/PanOoS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoramic%2520Out-of-Distribution%2520Segmentation%26entry.906535625%3DMengfei%2520Duan%2520and%2520Kailun%2520Yang%2520and%2520Yuheng%2520Zhang%2520and%2520Yihong%2520Cao%2520and%2520Fei%2520Teng%2520and%2520Kai%2520Luo%2520and%2520Jiaming%2520Zhang%2520and%2520Zhiyong%2520Li%2520and%2520Shutao%2520Li%26entry.1292438233%3D%2520%2520Panoramic%2520imaging%2520enables%2520capturing%2520360%257B%255Cdeg%257D%2520images%2520with%2520an%2520ultra-wide%250AField-of-View%2520%2528FoV%2529%2520for%2520dense%2520omnidirectional%2520perception.%2520However%252C%2520current%250Apanoramic%2520semantic%2520segmentation%2520methods%2520fail%2520to%2520identify%2520outliers%252C%2520and%2520pinhole%250AOut-of-distribution%2520Segmentation%2520%2528OoS%2529%2520models%2520perform%2520unsatisfactorily%2520in%2520the%250Apanoramic%2520domain%2520due%2520to%2520background%2520clutter%2520and%2520pixel%2520distortions.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520a%2520new%2520task%252C%2520Panoramic%2520Out-of-distribution%250ASegmentation%2520%2528PanOoS%2529%252C%2520achieving%2520OoS%2520for%2520panoramas.%2520Furthermore%252C%2520we%2520propose%2520the%250Afirst%2520solution%252C%2520POS%252C%2520which%2520adapts%2520to%2520the%2520characteristics%2520of%2520panoramic%2520images%250Athrough%2520text-guided%2520prompt%2520distribution%2520learning.%2520Specifically%252C%2520POS%2520integrates%250Aa%2520disentanglement%2520strategy%2520designed%2520to%2520materialize%2520the%2520cross-domain%250Ageneralization%2520capability%2520of%2520CLIP.%2520The%2520proposed%2520Prompt-based%2520Restoration%250AAttention%2520%2528PRA%2529%2520optimizes%2520semantic%2520decoding%2520by%2520prompt%2520guidance%2520and%250Aself-adaptive%2520correction%252C%2520while%2520Bilevel%2520Prompt%2520Distribution%2520Learning%2520%2528BPDL%2529%250Arefines%2520the%2520manifold%2520of%2520per-pixel%2520mask%2520embeddings%2520via%2520semantic%2520prototype%250Asupervision.%2520Besides%252C%2520to%2520compensate%2520for%2520the%2520scarcity%2520of%2520PanOoS%2520datasets%252C%2520we%250Aestablish%2520two%2520benchmarks%253A%2520DenseOoS%252C%2520which%2520features%2520diverse%2520outliers%2520in%2520complex%250Aenvironments%252C%2520and%2520QuadOoS%252C%2520captured%2520by%2520a%2520quadruped%2520robot%2520with%2520a%2520panoramic%250Aannular%2520lens%2520system.%2520Extensive%2520experiments%2520demonstrate%2520superior%2520performance%2520of%250APOS%252C%2520with%2520AuPRC%2520improving%2520by%252034.25%2525%2520and%2520FPR95%2520decreasing%2520by%252021.42%2525%2520on%2520DenseOoS%252C%250Aoutperforming%2520state-of-the-art%2520pinhole-OoS%2520methods.%2520Moreover%252C%2520POS%2520achieves%250Aleading%2520closed-set%2520segmentation%2520capabilities.%2520Code%2520and%2520datasets%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/MengfeiD/PanOoS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panoramic%20Out-of-Distribution%20Segmentation&entry.906535625=Mengfei%20Duan%20and%20Kailun%20Yang%20and%20Yuheng%20Zhang%20and%20Yihong%20Cao%20and%20Fei%20Teng%20and%20Kai%20Luo%20and%20Jiaming%20Zhang%20and%20Zhiyong%20Li%20and%20Shutao%20Li&entry.1292438233=%20%20Panoramic%20imaging%20enables%20capturing%20360%7B%5Cdeg%7D%20images%20with%20an%20ultra-wide%0AField-of-View%20%28FoV%29%20for%20dense%20omnidirectional%20perception.%20However%2C%20current%0Apanoramic%20semantic%20segmentation%20methods%20fail%20to%20identify%20outliers%2C%20and%20pinhole%0AOut-of-distribution%20Segmentation%20%28OoS%29%20models%20perform%20unsatisfactorily%20in%20the%0Apanoramic%20domain%20due%20to%20background%20clutter%20and%20pixel%20distortions.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20new%20task%2C%20Panoramic%20Out-of-distribution%0ASegmentation%20%28PanOoS%29%2C%20achieving%20OoS%20for%20panoramas.%20Furthermore%2C%20we%20propose%20the%0Afirst%20solution%2C%20POS%2C%20which%20adapts%20to%20the%20characteristics%20of%20panoramic%20images%0Athrough%20text-guided%20prompt%20distribution%20learning.%20Specifically%2C%20POS%20integrates%0Aa%20disentanglement%20strategy%20designed%20to%20materialize%20the%20cross-domain%0Ageneralization%20capability%20of%20CLIP.%20The%20proposed%20Prompt-based%20Restoration%0AAttention%20%28PRA%29%20optimizes%20semantic%20decoding%20by%20prompt%20guidance%20and%0Aself-adaptive%20correction%2C%20while%20Bilevel%20Prompt%20Distribution%20Learning%20%28BPDL%29%0Arefines%20the%20manifold%20of%20per-pixel%20mask%20embeddings%20via%20semantic%20prototype%0Asupervision.%20Besides%2C%20to%20compensate%20for%20the%20scarcity%20of%20PanOoS%20datasets%2C%20we%0Aestablish%20two%20benchmarks%3A%20DenseOoS%2C%20which%20features%20diverse%20outliers%20in%20complex%0Aenvironments%2C%20and%20QuadOoS%2C%20captured%20by%20a%20quadruped%20robot%20with%20a%20panoramic%0Aannular%20lens%20system.%20Extensive%20experiments%20demonstrate%20superior%20performance%20of%0APOS%2C%20with%20AuPRC%20improving%20by%2034.25%25%20and%20FPR95%20decreasing%20by%2021.42%25%20on%20DenseOoS%2C%0Aoutperforming%20state-of-the-art%20pinhole-OoS%20methods.%20Moreover%2C%20POS%20achieves%0Aleading%20closed-set%20segmentation%20capabilities.%20Code%20and%20datasets%20will%20be%0Aavailable%20at%20https%3A//github.com/MengfeiD/PanOoS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03539v1&entry.124074799=Read"},
{"title": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal\n  Delivery Based on Agentic UAVs", "author": "Xinyuan Zhang and Yonglin Tian and Fei Lin and Yue Liu and Jing Ma and Korn\u00e9lia S\u00e1ra Szatm\u00e1ry and Fei-Yue Wang", "abstract": "  The growing demand for intelligent logistics, particularly fine-grained\nterminal delivery, underscores the need for autonomous UAV (Unmanned Aerial\nVehicle)-based delivery systems. However, most existing last-mile delivery\nstudies rely on ground robots, while current UAV-based Vision-Language\nNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,\nmaking them unsuitable for precise terminal delivery. To bridge this gap, we\npropose LogisticsVLN, a scalable aerial delivery system built on multimodal\nlarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLN\nintegrates lightweight Large Language Models (LLMs) and Visual-Language Models\n(VLMs) in a modular pipeline for request understanding, floor localization,\nobject detection, and action-decision making. To support research and\nevaluation in this new setting, we construct the Vision-Language Delivery (VLD)\ndataset within the CARLA simulator. Experimental results on the VLD dataset\nshowcase the feasibility of the LogisticsVLN system. In addition, we conduct\nsubtask-level evaluations of each module of our system, offering valuable\ninsights for improving the robustness and real-world deployment of foundation\nmodel-based vision-language delivery systems.\n", "link": "http://arxiv.org/abs/2505.03460v1", "date": "2025-05-06", "relevancy": 2.7378, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogisticsVLN%3A%20Vision-Language%20Navigation%20For%20Low-Altitude%20Terminal%0A%20%20Delivery%20Based%20on%20Agentic%20UAVs&body=Title%3A%20LogisticsVLN%3A%20Vision-Language%20Navigation%20For%20Low-Altitude%20Terminal%0A%20%20Delivery%20Based%20on%20Agentic%20UAVs%0AAuthor%3A%20Xinyuan%20Zhang%20and%20Yonglin%20Tian%20and%20Fei%20Lin%20and%20Yue%20Liu%20and%20Jing%20Ma%20and%20Korn%C3%A9lia%20S%C3%A1ra%20Szatm%C3%A1ry%20and%20Fei-Yue%20Wang%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20intelligent%20logistics%2C%20particularly%20fine-grained%0Aterminal%20delivery%2C%20underscores%20the%20need%20for%20autonomous%20UAV%20%28Unmanned%20Aerial%0AVehicle%29-based%20delivery%20systems.%20However%2C%20most%20existing%20last-mile%20delivery%0Astudies%20rely%20on%20ground%20robots%2C%20while%20current%20UAV-based%20Vision-Language%0ANavigation%20%28VLN%29%20tasks%20primarily%20focus%20on%20coarse-grained%2C%20long-range%20goals%2C%0Amaking%20them%20unsuitable%20for%20precise%20terminal%20delivery.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20LogisticsVLN%2C%20a%20scalable%20aerial%20delivery%20system%20built%20on%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20for%20autonomous%20terminal%20delivery.%20LogisticsVLN%0Aintegrates%20lightweight%20Large%20Language%20Models%20%28LLMs%29%20and%20Visual-Language%20Models%0A%28VLMs%29%20in%20a%20modular%20pipeline%20for%20request%20understanding%2C%20floor%20localization%2C%0Aobject%20detection%2C%20and%20action-decision%20making.%20To%20support%20research%20and%0Aevaluation%20in%20this%20new%20setting%2C%20we%20construct%20the%20Vision-Language%20Delivery%20%28VLD%29%0Adataset%20within%20the%20CARLA%20simulator.%20Experimental%20results%20on%20the%20VLD%20dataset%0Ashowcase%20the%20feasibility%20of%20the%20LogisticsVLN%20system.%20In%20addition%2C%20we%20conduct%0Asubtask-level%20evaluations%20of%20each%20module%20of%20our%20system%2C%20offering%20valuable%0Ainsights%20for%20improving%20the%20robustness%20and%20real-world%20deployment%20of%20foundation%0Amodel-based%20vision-language%20delivery%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogisticsVLN%253A%2520Vision-Language%2520Navigation%2520For%2520Low-Altitude%2520Terminal%250A%2520%2520Delivery%2520Based%2520on%2520Agentic%2520UAVs%26entry.906535625%3DXinyuan%2520Zhang%2520and%2520Yonglin%2520Tian%2520and%2520Fei%2520Lin%2520and%2520Yue%2520Liu%2520and%2520Jing%2520Ma%2520and%2520Korn%25C3%25A9lia%2520S%25C3%25A1ra%2520Szatm%25C3%25A1ry%2520and%2520Fei-Yue%2520Wang%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520intelligent%2520logistics%252C%2520particularly%2520fine-grained%250Aterminal%2520delivery%252C%2520underscores%2520the%2520need%2520for%2520autonomous%2520UAV%2520%2528Unmanned%2520Aerial%250AVehicle%2529-based%2520delivery%2520systems.%2520However%252C%2520most%2520existing%2520last-mile%2520delivery%250Astudies%2520rely%2520on%2520ground%2520robots%252C%2520while%2520current%2520UAV-based%2520Vision-Language%250ANavigation%2520%2528VLN%2529%2520tasks%2520primarily%2520focus%2520on%2520coarse-grained%252C%2520long-range%2520goals%252C%250Amaking%2520them%2520unsuitable%2520for%2520precise%2520terminal%2520delivery.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Apropose%2520LogisticsVLN%252C%2520a%2520scalable%2520aerial%2520delivery%2520system%2520built%2520on%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520for%2520autonomous%2520terminal%2520delivery.%2520LogisticsVLN%250Aintegrates%2520lightweight%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Visual-Language%2520Models%250A%2528VLMs%2529%2520in%2520a%2520modular%2520pipeline%2520for%2520request%2520understanding%252C%2520floor%2520localization%252C%250Aobject%2520detection%252C%2520and%2520action-decision%2520making.%2520To%2520support%2520research%2520and%250Aevaluation%2520in%2520this%2520new%2520setting%252C%2520we%2520construct%2520the%2520Vision-Language%2520Delivery%2520%2528VLD%2529%250Adataset%2520within%2520the%2520CARLA%2520simulator.%2520Experimental%2520results%2520on%2520the%2520VLD%2520dataset%250Ashowcase%2520the%2520feasibility%2520of%2520the%2520LogisticsVLN%2520system.%2520In%2520addition%252C%2520we%2520conduct%250Asubtask-level%2520evaluations%2520of%2520each%2520module%2520of%2520our%2520system%252C%2520offering%2520valuable%250Ainsights%2520for%2520improving%2520the%2520robustness%2520and%2520real-world%2520deployment%2520of%2520foundation%250Amodel-based%2520vision-language%2520delivery%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogisticsVLN%3A%20Vision-Language%20Navigation%20For%20Low-Altitude%20Terminal%0A%20%20Delivery%20Based%20on%20Agentic%20UAVs&entry.906535625=Xinyuan%20Zhang%20and%20Yonglin%20Tian%20and%20Fei%20Lin%20and%20Yue%20Liu%20and%20Jing%20Ma%20and%20Korn%C3%A9lia%20S%C3%A1ra%20Szatm%C3%A1ry%20and%20Fei-Yue%20Wang&entry.1292438233=%20%20The%20growing%20demand%20for%20intelligent%20logistics%2C%20particularly%20fine-grained%0Aterminal%20delivery%2C%20underscores%20the%20need%20for%20autonomous%20UAV%20%28Unmanned%20Aerial%0AVehicle%29-based%20delivery%20systems.%20However%2C%20most%20existing%20last-mile%20delivery%0Astudies%20rely%20on%20ground%20robots%2C%20while%20current%20UAV-based%20Vision-Language%0ANavigation%20%28VLN%29%20tasks%20primarily%20focus%20on%20coarse-grained%2C%20long-range%20goals%2C%0Amaking%20them%20unsuitable%20for%20precise%20terminal%20delivery.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20LogisticsVLN%2C%20a%20scalable%20aerial%20delivery%20system%20built%20on%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20for%20autonomous%20terminal%20delivery.%20LogisticsVLN%0Aintegrates%20lightweight%20Large%20Language%20Models%20%28LLMs%29%20and%20Visual-Language%20Models%0A%28VLMs%29%20in%20a%20modular%20pipeline%20for%20request%20understanding%2C%20floor%20localization%2C%0Aobject%20detection%2C%20and%20action-decision%20making.%20To%20support%20research%20and%0Aevaluation%20in%20this%20new%20setting%2C%20we%20construct%20the%20Vision-Language%20Delivery%20%28VLD%29%0Adataset%20within%20the%20CARLA%20simulator.%20Experimental%20results%20on%20the%20VLD%20dataset%0Ashowcase%20the%20feasibility%20of%20the%20LogisticsVLN%20system.%20In%20addition%2C%20we%20conduct%0Asubtask-level%20evaluations%20of%20each%20module%20of%20our%20system%2C%20offering%20valuable%0Ainsights%20for%20improving%20the%20robustness%20and%20real-world%20deployment%20of%20foundation%0Amodel-based%20vision-language%20delivery%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03460v1&entry.124074799=Read"},
{"title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion\n  Transformers", "author": "Sherwin Bahmani and Ivan Skorokhodov and Guocheng Qian and Aliaksandr Siarohin and Willi Menapace and Andrea Tagliasacchi and David B. Lindell and Sergey Tulyakov", "abstract": "  Numerous works have recently integrated 3D camera control into foundational\ntext-to-video models, but the resulting camera control is often imprecise, and\nvideo generation quality suffers. In this work, we analyze camera motion from a\nfirst principles perspective, uncovering insights that enable precise 3D camera\nmanipulation without compromising synthesis quality. First, we determine that\nmotion induced by camera movements in videos is low-frequency in nature. This\nmotivates us to adjust train and test pose conditioning schedules, accelerating\ntraining convergence while improving visual and motion quality. Then, by\nprobing the representations of an unconditional video diffusion transformer, we\nobserve that they implicitly perform camera pose estimation under the hood, and\nonly a sub-portion of their layers contain the camera information. This\nsuggested us to limit the injection of camera conditioning to a subset of the\narchitecture to prevent interference with other video features, leading to a 4x\nreduction of training parameters, improved training speed, and 10% higher\nvisual quality. Finally, we complement the typical dataset for camera control\nlearning with a curated dataset of 20K diverse, dynamic videos with stationary\ncameras. This helps the model distinguish between camera and scene motion and\nimproves the dynamics of generated pose-conditioned videos. We compound these\nfindings to design the Advanced 3D Camera Control (AC3D) architecture, the new\nstate-of-the-art model for generative video modeling with camera control.\n", "link": "http://arxiv.org/abs/2411.18673v4", "date": "2025-05-06", "relevancy": 2.7241, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7625}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6796}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AC3D%3A%20Analyzing%20and%20Improving%203D%20Camera%20Control%20in%20Video%20Diffusion%0A%20%20Transformers&body=Title%3A%20AC3D%3A%20Analyzing%20and%20Improving%203D%20Camera%20Control%20in%20Video%20Diffusion%0A%20%20Transformers%0AAuthor%3A%20Sherwin%20Bahmani%20and%20Ivan%20Skorokhodov%20and%20Guocheng%20Qian%20and%20Aliaksandr%20Siarohin%20and%20Willi%20Menapace%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%20and%20Sergey%20Tulyakov%0AAbstract%3A%20%20%20Numerous%20works%20have%20recently%20integrated%203D%20camera%20control%20into%20foundational%0Atext-to-video%20models%2C%20but%20the%20resulting%20camera%20control%20is%20often%20imprecise%2C%20and%0Avideo%20generation%20quality%20suffers.%20In%20this%20work%2C%20we%20analyze%20camera%20motion%20from%20a%0Afirst%20principles%20perspective%2C%20uncovering%20insights%20that%20enable%20precise%203D%20camera%0Amanipulation%20without%20compromising%20synthesis%20quality.%20First%2C%20we%20determine%20that%0Amotion%20induced%20by%20camera%20movements%20in%20videos%20is%20low-frequency%20in%20nature.%20This%0Amotivates%20us%20to%20adjust%20train%20and%20test%20pose%20conditioning%20schedules%2C%20accelerating%0Atraining%20convergence%20while%20improving%20visual%20and%20motion%20quality.%20Then%2C%20by%0Aprobing%20the%20representations%20of%20an%20unconditional%20video%20diffusion%20transformer%2C%20we%0Aobserve%20that%20they%20implicitly%20perform%20camera%20pose%20estimation%20under%20the%20hood%2C%20and%0Aonly%20a%20sub-portion%20of%20their%20layers%20contain%20the%20camera%20information.%20This%0Asuggested%20us%20to%20limit%20the%20injection%20of%20camera%20conditioning%20to%20a%20subset%20of%20the%0Aarchitecture%20to%20prevent%20interference%20with%20other%20video%20features%2C%20leading%20to%20a%204x%0Areduction%20of%20training%20parameters%2C%20improved%20training%20speed%2C%20and%2010%25%20higher%0Avisual%20quality.%20Finally%2C%20we%20complement%20the%20typical%20dataset%20for%20camera%20control%0Alearning%20with%20a%20curated%20dataset%20of%2020K%20diverse%2C%20dynamic%20videos%20with%20stationary%0Acameras.%20This%20helps%20the%20model%20distinguish%20between%20camera%20and%20scene%20motion%20and%0Aimproves%20the%20dynamics%20of%20generated%20pose-conditioned%20videos.%20We%20compound%20these%0Afindings%20to%20design%20the%20Advanced%203D%20Camera%20Control%20%28AC3D%29%20architecture%2C%20the%20new%0Astate-of-the-art%20model%20for%20generative%20video%20modeling%20with%20camera%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18673v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAC3D%253A%2520Analyzing%2520and%2520Improving%25203D%2520Camera%2520Control%2520in%2520Video%2520Diffusion%250A%2520%2520Transformers%26entry.906535625%3DSherwin%2520Bahmani%2520and%2520Ivan%2520Skorokhodov%2520and%2520Guocheng%2520Qian%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Willi%2520Menapace%2520and%2520Andrea%2520Tagliasacchi%2520and%2520David%2520B.%2520Lindell%2520and%2520Sergey%2520Tulyakov%26entry.1292438233%3D%2520%2520Numerous%2520works%2520have%2520recently%2520integrated%25203D%2520camera%2520control%2520into%2520foundational%250Atext-to-video%2520models%252C%2520but%2520the%2520resulting%2520camera%2520control%2520is%2520often%2520imprecise%252C%2520and%250Avideo%2520generation%2520quality%2520suffers.%2520In%2520this%2520work%252C%2520we%2520analyze%2520camera%2520motion%2520from%2520a%250Afirst%2520principles%2520perspective%252C%2520uncovering%2520insights%2520that%2520enable%2520precise%25203D%2520camera%250Amanipulation%2520without%2520compromising%2520synthesis%2520quality.%2520First%252C%2520we%2520determine%2520that%250Amotion%2520induced%2520by%2520camera%2520movements%2520in%2520videos%2520is%2520low-frequency%2520in%2520nature.%2520This%250Amotivates%2520us%2520to%2520adjust%2520train%2520and%2520test%2520pose%2520conditioning%2520schedules%252C%2520accelerating%250Atraining%2520convergence%2520while%2520improving%2520visual%2520and%2520motion%2520quality.%2520Then%252C%2520by%250Aprobing%2520the%2520representations%2520of%2520an%2520unconditional%2520video%2520diffusion%2520transformer%252C%2520we%250Aobserve%2520that%2520they%2520implicitly%2520perform%2520camera%2520pose%2520estimation%2520under%2520the%2520hood%252C%2520and%250Aonly%2520a%2520sub-portion%2520of%2520their%2520layers%2520contain%2520the%2520camera%2520information.%2520This%250Asuggested%2520us%2520to%2520limit%2520the%2520injection%2520of%2520camera%2520conditioning%2520to%2520a%2520subset%2520of%2520the%250Aarchitecture%2520to%2520prevent%2520interference%2520with%2520other%2520video%2520features%252C%2520leading%2520to%2520a%25204x%250Areduction%2520of%2520training%2520parameters%252C%2520improved%2520training%2520speed%252C%2520and%252010%2525%2520higher%250Avisual%2520quality.%2520Finally%252C%2520we%2520complement%2520the%2520typical%2520dataset%2520for%2520camera%2520control%250Alearning%2520with%2520a%2520curated%2520dataset%2520of%252020K%2520diverse%252C%2520dynamic%2520videos%2520with%2520stationary%250Acameras.%2520This%2520helps%2520the%2520model%2520distinguish%2520between%2520camera%2520and%2520scene%2520motion%2520and%250Aimproves%2520the%2520dynamics%2520of%2520generated%2520pose-conditioned%2520videos.%2520We%2520compound%2520these%250Afindings%2520to%2520design%2520the%2520Advanced%25203D%2520Camera%2520Control%2520%2528AC3D%2529%2520architecture%252C%2520the%2520new%250Astate-of-the-art%2520model%2520for%2520generative%2520video%2520modeling%2520with%2520camera%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18673v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AC3D%3A%20Analyzing%20and%20Improving%203D%20Camera%20Control%20in%20Video%20Diffusion%0A%20%20Transformers&entry.906535625=Sherwin%20Bahmani%20and%20Ivan%20Skorokhodov%20and%20Guocheng%20Qian%20and%20Aliaksandr%20Siarohin%20and%20Willi%20Menapace%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%20and%20Sergey%20Tulyakov&entry.1292438233=%20%20Numerous%20works%20have%20recently%20integrated%203D%20camera%20control%20into%20foundational%0Atext-to-video%20models%2C%20but%20the%20resulting%20camera%20control%20is%20often%20imprecise%2C%20and%0Avideo%20generation%20quality%20suffers.%20In%20this%20work%2C%20we%20analyze%20camera%20motion%20from%20a%0Afirst%20principles%20perspective%2C%20uncovering%20insights%20that%20enable%20precise%203D%20camera%0Amanipulation%20without%20compromising%20synthesis%20quality.%20First%2C%20we%20determine%20that%0Amotion%20induced%20by%20camera%20movements%20in%20videos%20is%20low-frequency%20in%20nature.%20This%0Amotivates%20us%20to%20adjust%20train%20and%20test%20pose%20conditioning%20schedules%2C%20accelerating%0Atraining%20convergence%20while%20improving%20visual%20and%20motion%20quality.%20Then%2C%20by%0Aprobing%20the%20representations%20of%20an%20unconditional%20video%20diffusion%20transformer%2C%20we%0Aobserve%20that%20they%20implicitly%20perform%20camera%20pose%20estimation%20under%20the%20hood%2C%20and%0Aonly%20a%20sub-portion%20of%20their%20layers%20contain%20the%20camera%20information.%20This%0Asuggested%20us%20to%20limit%20the%20injection%20of%20camera%20conditioning%20to%20a%20subset%20of%20the%0Aarchitecture%20to%20prevent%20interference%20with%20other%20video%20features%2C%20leading%20to%20a%204x%0Areduction%20of%20training%20parameters%2C%20improved%20training%20speed%2C%20and%2010%25%20higher%0Avisual%20quality.%20Finally%2C%20we%20complement%20the%20typical%20dataset%20for%20camera%20control%0Alearning%20with%20a%20curated%20dataset%20of%2020K%20diverse%2C%20dynamic%20videos%20with%20stationary%0Acameras.%20This%20helps%20the%20model%20distinguish%20between%20camera%20and%20scene%20motion%20and%0Aimproves%20the%20dynamics%20of%20generated%20pose-conditioned%20videos.%20We%20compound%20these%0Afindings%20to%20design%20the%20Advanced%203D%20Camera%20Control%20%28AC3D%29%20architecture%2C%20the%20new%0Astate-of-the-art%20model%20for%20generative%20video%20modeling%20with%20camera%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18673v4&entry.124074799=Read"},
{"title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI\n  Feedback", "author": "Liqiang Jing and Xinya Du", "abstract": "  Large Vision-Language Models (LVLMs) have demonstrated proficiency in\ntackling a variety of visual-language tasks. However, current LVLMs suffer from\nmisalignment between text and image modalities which causes three kinds of\nhallucination problems, i.e., object existence, object attribute, and object\nrelationship. To tackle this issue, existing methods mainly utilize\nReinforcement Learning (RL) to align modalities in LVLMs. However, they still\nsuffer from three main limitations: (1) General feedback can not indicate the\nhallucination type contained in the response; (2) Sparse rewards only give the\nsequence-level reward for the whole response; and (3)Annotation cost is\ntime-consuming and labor-intensive. To handle these limitations, we propose an\ninnovative method to align modalities in LVLMs through Fine-Grained Artificial\nIntelligence Feedback (FGAIF), which mainly consists of three steps: AI-based\nFeedback Collection, Fine-grained Reward Model Training, and Reinforcement\nLearning with Fine-grained Reward. Specifically, We first utilize AI tools to\npredict the types of hallucination for each segment in the response and obtain\na collection of fine-grained feedback. Then, based on the collected reward\ndata, three specialized reward models are trained to produce dense rewards.\nFinally, a novel fine-grained feedback module is integrated into the Proximal\nPolicy Optimization (PPO) algorithm. Extensive experiments are conducted on\nhallucination and general benchmarks, demonstrating the superior performance of\nour proposed method. Notably, compared with previous models trained with the\nRL-based aligning method, our proposed method is effective even with fewer\nparameters.\n", "link": "http://arxiv.org/abs/2404.05046v2", "date": "2025-05-06", "relevancy": 2.6953, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FGAIF%3A%20Aligning%20Large%20Vision-Language%20Models%20with%20Fine-grained%20AI%0A%20%20Feedback&body=Title%3A%20FGAIF%3A%20Aligning%20Large%20Vision-Language%20Models%20with%20Fine-grained%20AI%0A%20%20Feedback%0AAuthor%3A%20Liqiang%20Jing%20and%20Xinya%20Du%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20proficiency%20in%0Atackling%20a%20variety%20of%20visual-language%20tasks.%20However%2C%20current%20LVLMs%20suffer%20from%0Amisalignment%20between%20text%20and%20image%20modalities%20which%20causes%20three%20kinds%20of%0Ahallucination%20problems%2C%20i.e.%2C%20object%20existence%2C%20object%20attribute%2C%20and%20object%0Arelationship.%20To%20tackle%20this%20issue%2C%20existing%20methods%20mainly%20utilize%0AReinforcement%20Learning%20%28RL%29%20to%20align%20modalities%20in%20LVLMs.%20However%2C%20they%20still%0Asuffer%20from%20three%20main%20limitations%3A%20%281%29%20General%20feedback%20can%20not%20indicate%20the%0Ahallucination%20type%20contained%20in%20the%20response%3B%20%282%29%20Sparse%20rewards%20only%20give%20the%0Asequence-level%20reward%20for%20the%20whole%20response%3B%20and%20%283%29Annotation%20cost%20is%0Atime-consuming%20and%20labor-intensive.%20To%20handle%20these%20limitations%2C%20we%20propose%20an%0Ainnovative%20method%20to%20align%20modalities%20in%20LVLMs%20through%20Fine-Grained%20Artificial%0AIntelligence%20Feedback%20%28FGAIF%29%2C%20which%20mainly%20consists%20of%20three%20steps%3A%20AI-based%0AFeedback%20Collection%2C%20Fine-grained%20Reward%20Model%20Training%2C%20and%20Reinforcement%0ALearning%20with%20Fine-grained%20Reward.%20Specifically%2C%20We%20first%20utilize%20AI%20tools%20to%0Apredict%20the%20types%20of%20hallucination%20for%20each%20segment%20in%20the%20response%20and%20obtain%0Aa%20collection%20of%20fine-grained%20feedback.%20Then%2C%20based%20on%20the%20collected%20reward%0Adata%2C%20three%20specialized%20reward%20models%20are%20trained%20to%20produce%20dense%20rewards.%0AFinally%2C%20a%20novel%20fine-grained%20feedback%20module%20is%20integrated%20into%20the%20Proximal%0APolicy%20Optimization%20%28PPO%29%20algorithm.%20Extensive%20experiments%20are%20conducted%20on%0Ahallucination%20and%20general%20benchmarks%2C%20demonstrating%20the%20superior%20performance%20of%0Aour%20proposed%20method.%20Notably%2C%20compared%20with%20previous%20models%20trained%20with%20the%0ARL-based%20aligning%20method%2C%20our%20proposed%20method%20is%20effective%20even%20with%20fewer%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFGAIF%253A%2520Aligning%2520Large%2520Vision-Language%2520Models%2520with%2520Fine-grained%2520AI%250A%2520%2520Feedback%26entry.906535625%3DLiqiang%2520Jing%2520and%2520Xinya%2520Du%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520proficiency%2520in%250Atackling%2520a%2520variety%2520of%2520visual-language%2520tasks.%2520However%252C%2520current%2520LVLMs%2520suffer%2520from%250Amisalignment%2520between%2520text%2520and%2520image%2520modalities%2520which%2520causes%2520three%2520kinds%2520of%250Ahallucination%2520problems%252C%2520i.e.%252C%2520object%2520existence%252C%2520object%2520attribute%252C%2520and%2520object%250Arelationship.%2520To%2520tackle%2520this%2520issue%252C%2520existing%2520methods%2520mainly%2520utilize%250AReinforcement%2520Learning%2520%2528RL%2529%2520to%2520align%2520modalities%2520in%2520LVLMs.%2520However%252C%2520they%2520still%250Asuffer%2520from%2520three%2520main%2520limitations%253A%2520%25281%2529%2520General%2520feedback%2520can%2520not%2520indicate%2520the%250Ahallucination%2520type%2520contained%2520in%2520the%2520response%253B%2520%25282%2529%2520Sparse%2520rewards%2520only%2520give%2520the%250Asequence-level%2520reward%2520for%2520the%2520whole%2520response%253B%2520and%2520%25283%2529Annotation%2520cost%2520is%250Atime-consuming%2520and%2520labor-intensive.%2520To%2520handle%2520these%2520limitations%252C%2520we%2520propose%2520an%250Ainnovative%2520method%2520to%2520align%2520modalities%2520in%2520LVLMs%2520through%2520Fine-Grained%2520Artificial%250AIntelligence%2520Feedback%2520%2528FGAIF%2529%252C%2520which%2520mainly%2520consists%2520of%2520three%2520steps%253A%2520AI-based%250AFeedback%2520Collection%252C%2520Fine-grained%2520Reward%2520Model%2520Training%252C%2520and%2520Reinforcement%250ALearning%2520with%2520Fine-grained%2520Reward.%2520Specifically%252C%2520We%2520first%2520utilize%2520AI%2520tools%2520to%250Apredict%2520the%2520types%2520of%2520hallucination%2520for%2520each%2520segment%2520in%2520the%2520response%2520and%2520obtain%250Aa%2520collection%2520of%2520fine-grained%2520feedback.%2520Then%252C%2520based%2520on%2520the%2520collected%2520reward%250Adata%252C%2520three%2520specialized%2520reward%2520models%2520are%2520trained%2520to%2520produce%2520dense%2520rewards.%250AFinally%252C%2520a%2520novel%2520fine-grained%2520feedback%2520module%2520is%2520integrated%2520into%2520the%2520Proximal%250APolicy%2520Optimization%2520%2528PPO%2529%2520algorithm.%2520Extensive%2520experiments%2520are%2520conducted%2520on%250Ahallucination%2520and%2520general%2520benchmarks%252C%2520demonstrating%2520the%2520superior%2520performance%2520of%250Aour%2520proposed%2520method.%2520Notably%252C%2520compared%2520with%2520previous%2520models%2520trained%2520with%2520the%250ARL-based%2520aligning%2520method%252C%2520our%2520proposed%2520method%2520is%2520effective%2520even%2520with%2520fewer%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FGAIF%3A%20Aligning%20Large%20Vision-Language%20Models%20with%20Fine-grained%20AI%0A%20%20Feedback&entry.906535625=Liqiang%20Jing%20and%20Xinya%20Du&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20proficiency%20in%0Atackling%20a%20variety%20of%20visual-language%20tasks.%20However%2C%20current%20LVLMs%20suffer%20from%0Amisalignment%20between%20text%20and%20image%20modalities%20which%20causes%20three%20kinds%20of%0Ahallucination%20problems%2C%20i.e.%2C%20object%20existence%2C%20object%20attribute%2C%20and%20object%0Arelationship.%20To%20tackle%20this%20issue%2C%20existing%20methods%20mainly%20utilize%0AReinforcement%20Learning%20%28RL%29%20to%20align%20modalities%20in%20LVLMs.%20However%2C%20they%20still%0Asuffer%20from%20three%20main%20limitations%3A%20%281%29%20General%20feedback%20can%20not%20indicate%20the%0Ahallucination%20type%20contained%20in%20the%20response%3B%20%282%29%20Sparse%20rewards%20only%20give%20the%0Asequence-level%20reward%20for%20the%20whole%20response%3B%20and%20%283%29Annotation%20cost%20is%0Atime-consuming%20and%20labor-intensive.%20To%20handle%20these%20limitations%2C%20we%20propose%20an%0Ainnovative%20method%20to%20align%20modalities%20in%20LVLMs%20through%20Fine-Grained%20Artificial%0AIntelligence%20Feedback%20%28FGAIF%29%2C%20which%20mainly%20consists%20of%20three%20steps%3A%20AI-based%0AFeedback%20Collection%2C%20Fine-grained%20Reward%20Model%20Training%2C%20and%20Reinforcement%0ALearning%20with%20Fine-grained%20Reward.%20Specifically%2C%20We%20first%20utilize%20AI%20tools%20to%0Apredict%20the%20types%20of%20hallucination%20for%20each%20segment%20in%20the%20response%20and%20obtain%0Aa%20collection%20of%20fine-grained%20feedback.%20Then%2C%20based%20on%20the%20collected%20reward%0Adata%2C%20three%20specialized%20reward%20models%20are%20trained%20to%20produce%20dense%20rewards.%0AFinally%2C%20a%20novel%20fine-grained%20feedback%20module%20is%20integrated%20into%20the%20Proximal%0APolicy%20Optimization%20%28PPO%29%20algorithm.%20Extensive%20experiments%20are%20conducted%20on%0Ahallucination%20and%20general%20benchmarks%2C%20demonstrating%20the%20superior%20performance%20of%0Aour%20proposed%20method.%20Notably%2C%20compared%20with%20previous%20models%20trained%20with%20the%0ARL-based%20aligning%20method%2C%20our%20proposed%20method%20is%20effective%20even%20with%20fewer%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05046v2&entry.124074799=Read"},
{"title": "ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language\n  and Vision Assistant", "author": "Yifan Xiang and Zhenxi Zhang and Bin Li and Yixuan Weng and Shoujun Zhou and Yangfan He and Keqin Li", "abstract": "  Recent advances in personalized MLLMs enable effective capture of\nuser-specific concepts, supporting both recognition of personalized concepts\nand contextual captioning. However, humans typically explore and reason over\nrelations among objects and individuals, transcending surface-level information\nto achieve more personalized and contextual understanding. To this end,\nexisting methods may face three main limitations: Their training data lacks\nmulti-object sets in which relations among objects are learnable. Building on\nthe limited training data, their models overlook the relations between\ndifferent personalized concepts and fail to reason over them. Their experiments\nmainly focus on a single personalized concept, where evaluations are limited to\nrecognition and captioning tasks. To address the limitations, we present a new\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\ngraph prompting methods are designed to align KGs within the model's semantic\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\nboth open- and closed-ended settings. The proposed benchmark is designed to\nevaluate the relational reasoning and knowledge-connection capability of\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\nother competitive MLLMs. Results show that the proposed model not only learns\npersonalized knowledge but also performs relational reasoning in responses,\nachieving the SoTA performance compared with the competitive methods. All the\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.\n", "link": "http://arxiv.org/abs/2505.03654v1", "date": "2025-05-06", "relevancy": 2.6922, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReGraP-LLaVA%3A%20Reasoning%20enabled%20Graph-based%20Personalized%20Large%20Language%0A%20%20and%20Vision%20Assistant&body=Title%3A%20ReGraP-LLaVA%3A%20Reasoning%20enabled%20Graph-based%20Personalized%20Large%20Language%0A%20%20and%20Vision%20Assistant%0AAuthor%3A%20Yifan%20Xiang%20and%20Zhenxi%20Zhang%20and%20Bin%20Li%20and%20Yixuan%20Weng%20and%20Shoujun%20Zhou%20and%20Yangfan%20He%20and%20Keqin%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20personalized%20MLLMs%20enable%20effective%20capture%20of%0Auser-specific%20concepts%2C%20supporting%20both%20recognition%20of%20personalized%20concepts%0Aand%20contextual%20captioning.%20However%2C%20humans%20typically%20explore%20and%20reason%20over%0Arelations%20among%20objects%20and%20individuals%2C%20transcending%20surface-level%20information%0Ato%20achieve%20more%20personalized%20and%20contextual%20understanding.%20To%20this%20end%2C%0Aexisting%20methods%20may%20face%20three%20main%20limitations%3A%20Their%20training%20data%20lacks%0Amulti-object%20sets%20in%20which%20relations%20among%20objects%20are%20learnable.%20Building%20on%0Athe%20limited%20training%20data%2C%20their%20models%20overlook%20the%20relations%20between%0Adifferent%20personalized%20concepts%20and%20fail%20to%20reason%20over%20them.%20Their%20experiments%0Amainly%20focus%20on%20a%20single%20personalized%20concept%2C%20where%20evaluations%20are%20limited%20to%0Arecognition%20and%20captioning%20tasks.%20To%20address%20the%20limitations%2C%20we%20present%20a%20new%0Adataset%20named%20ReGraP%2C%20consisting%20of%20120%20sets%20of%20personalized%20knowledge.%20Each%0Aset%20includes%20images%2C%20KGs%2C%20and%20CoT%20QA%20pairs%20derived%20from%20the%20KGs%2C%20enabling%20more%0Astructured%20and%20sophisticated%20reasoning%20pathways.%20We%20propose%20ReGraP-LLaVA%2C%20an%0AMLLM%20trained%20with%20the%20corresponding%20KGs%20and%20CoT%20QA%20pairs%2C%20where%20soft%20and%20hard%0Agraph%20prompting%20methods%20are%20designed%20to%20align%20KGs%20within%20the%20model%27s%20semantic%0Aspace.%20We%20establish%20the%20ReGraP%20Benchmark%2C%20which%20contains%20diverse%20task%20types%3A%0Amultiple-choice%2C%20fill-in-the-blank%2C%20True/False%2C%20and%20descriptive%20questions%20in%0Aboth%20open-%20and%20closed-ended%20settings.%20The%20proposed%20benchmark%20is%20designed%20to%0Aevaluate%20the%20relational%20reasoning%20and%20knowledge-connection%20capability%20of%0Apersonalized%20MLLMs.%20We%20conduct%20experiments%20on%20the%20proposed%20ReGraP-LLaVA%20and%0Aother%20competitive%20MLLMs.%20Results%20show%20that%20the%20proposed%20model%20not%20only%20learns%0Apersonalized%20knowledge%20but%20also%20performs%20relational%20reasoning%20in%20responses%2C%0Aachieving%20the%20SoTA%20performance%20compared%20with%20the%20competitive%20methods.%20All%20the%0Acodes%20and%20datasets%20are%20released%20at%3A%20https%3A//github.com/xyfyyds/ReGraP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReGraP-LLaVA%253A%2520Reasoning%2520enabled%2520Graph-based%2520Personalized%2520Large%2520Language%250A%2520%2520and%2520Vision%2520Assistant%26entry.906535625%3DYifan%2520Xiang%2520and%2520Zhenxi%2520Zhang%2520and%2520Bin%2520Li%2520and%2520Yixuan%2520Weng%2520and%2520Shoujun%2520Zhou%2520and%2520Yangfan%2520He%2520and%2520Keqin%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520personalized%2520MLLMs%2520enable%2520effective%2520capture%2520of%250Auser-specific%2520concepts%252C%2520supporting%2520both%2520recognition%2520of%2520personalized%2520concepts%250Aand%2520contextual%2520captioning.%2520However%252C%2520humans%2520typically%2520explore%2520and%2520reason%2520over%250Arelations%2520among%2520objects%2520and%2520individuals%252C%2520transcending%2520surface-level%2520information%250Ato%2520achieve%2520more%2520personalized%2520and%2520contextual%2520understanding.%2520To%2520this%2520end%252C%250Aexisting%2520methods%2520may%2520face%2520three%2520main%2520limitations%253A%2520Their%2520training%2520data%2520lacks%250Amulti-object%2520sets%2520in%2520which%2520relations%2520among%2520objects%2520are%2520learnable.%2520Building%2520on%250Athe%2520limited%2520training%2520data%252C%2520their%2520models%2520overlook%2520the%2520relations%2520between%250Adifferent%2520personalized%2520concepts%2520and%2520fail%2520to%2520reason%2520over%2520them.%2520Their%2520experiments%250Amainly%2520focus%2520on%2520a%2520single%2520personalized%2520concept%252C%2520where%2520evaluations%2520are%2520limited%2520to%250Arecognition%2520and%2520captioning%2520tasks.%2520To%2520address%2520the%2520limitations%252C%2520we%2520present%2520a%2520new%250Adataset%2520named%2520ReGraP%252C%2520consisting%2520of%2520120%2520sets%2520of%2520personalized%2520knowledge.%2520Each%250Aset%2520includes%2520images%252C%2520KGs%252C%2520and%2520CoT%2520QA%2520pairs%2520derived%2520from%2520the%2520KGs%252C%2520enabling%2520more%250Astructured%2520and%2520sophisticated%2520reasoning%2520pathways.%2520We%2520propose%2520ReGraP-LLaVA%252C%2520an%250AMLLM%2520trained%2520with%2520the%2520corresponding%2520KGs%2520and%2520CoT%2520QA%2520pairs%252C%2520where%2520soft%2520and%2520hard%250Agraph%2520prompting%2520methods%2520are%2520designed%2520to%2520align%2520KGs%2520within%2520the%2520model%2527s%2520semantic%250Aspace.%2520We%2520establish%2520the%2520ReGraP%2520Benchmark%252C%2520which%2520contains%2520diverse%2520task%2520types%253A%250Amultiple-choice%252C%2520fill-in-the-blank%252C%2520True/False%252C%2520and%2520descriptive%2520questions%2520in%250Aboth%2520open-%2520and%2520closed-ended%2520settings.%2520The%2520proposed%2520benchmark%2520is%2520designed%2520to%250Aevaluate%2520the%2520relational%2520reasoning%2520and%2520knowledge-connection%2520capability%2520of%250Apersonalized%2520MLLMs.%2520We%2520conduct%2520experiments%2520on%2520the%2520proposed%2520ReGraP-LLaVA%2520and%250Aother%2520competitive%2520MLLMs.%2520Results%2520show%2520that%2520the%2520proposed%2520model%2520not%2520only%2520learns%250Apersonalized%2520knowledge%2520but%2520also%2520performs%2520relational%2520reasoning%2520in%2520responses%252C%250Aachieving%2520the%2520SoTA%2520performance%2520compared%2520with%2520the%2520competitive%2520methods.%2520All%2520the%250Acodes%2520and%2520datasets%2520are%2520released%2520at%253A%2520https%253A//github.com/xyfyyds/ReGraP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReGraP-LLaVA%3A%20Reasoning%20enabled%20Graph-based%20Personalized%20Large%20Language%0A%20%20and%20Vision%20Assistant&entry.906535625=Yifan%20Xiang%20and%20Zhenxi%20Zhang%20and%20Bin%20Li%20and%20Yixuan%20Weng%20and%20Shoujun%20Zhou%20and%20Yangfan%20He%20and%20Keqin%20Li&entry.1292438233=%20%20Recent%20advances%20in%20personalized%20MLLMs%20enable%20effective%20capture%20of%0Auser-specific%20concepts%2C%20supporting%20both%20recognition%20of%20personalized%20concepts%0Aand%20contextual%20captioning.%20However%2C%20humans%20typically%20explore%20and%20reason%20over%0Arelations%20among%20objects%20and%20individuals%2C%20transcending%20surface-level%20information%0Ato%20achieve%20more%20personalized%20and%20contextual%20understanding.%20To%20this%20end%2C%0Aexisting%20methods%20may%20face%20three%20main%20limitations%3A%20Their%20training%20data%20lacks%0Amulti-object%20sets%20in%20which%20relations%20among%20objects%20are%20learnable.%20Building%20on%0Athe%20limited%20training%20data%2C%20their%20models%20overlook%20the%20relations%20between%0Adifferent%20personalized%20concepts%20and%20fail%20to%20reason%20over%20them.%20Their%20experiments%0Amainly%20focus%20on%20a%20single%20personalized%20concept%2C%20where%20evaluations%20are%20limited%20to%0Arecognition%20and%20captioning%20tasks.%20To%20address%20the%20limitations%2C%20we%20present%20a%20new%0Adataset%20named%20ReGraP%2C%20consisting%20of%20120%20sets%20of%20personalized%20knowledge.%20Each%0Aset%20includes%20images%2C%20KGs%2C%20and%20CoT%20QA%20pairs%20derived%20from%20the%20KGs%2C%20enabling%20more%0Astructured%20and%20sophisticated%20reasoning%20pathways.%20We%20propose%20ReGraP-LLaVA%2C%20an%0AMLLM%20trained%20with%20the%20corresponding%20KGs%20and%20CoT%20QA%20pairs%2C%20where%20soft%20and%20hard%0Agraph%20prompting%20methods%20are%20designed%20to%20align%20KGs%20within%20the%20model%27s%20semantic%0Aspace.%20We%20establish%20the%20ReGraP%20Benchmark%2C%20which%20contains%20diverse%20task%20types%3A%0Amultiple-choice%2C%20fill-in-the-blank%2C%20True/False%2C%20and%20descriptive%20questions%20in%0Aboth%20open-%20and%20closed-ended%20settings.%20The%20proposed%20benchmark%20is%20designed%20to%0Aevaluate%20the%20relational%20reasoning%20and%20knowledge-connection%20capability%20of%0Apersonalized%20MLLMs.%20We%20conduct%20experiments%20on%20the%20proposed%20ReGraP-LLaVA%20and%0Aother%20competitive%20MLLMs.%20Results%20show%20that%20the%20proposed%20model%20not%20only%20learns%0Apersonalized%20knowledge%20but%20also%20performs%20relational%20reasoning%20in%20responses%2C%0Aachieving%20the%20SoTA%20performance%20compared%20with%20the%20competitive%20methods.%20All%20the%0Acodes%20and%20datasets%20are%20released%20at%3A%20https%3A//github.com/xyfyyds/ReGraP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03654v1&entry.124074799=Read"},
{"title": "Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene\n  Graph", "author": "Sergey Linok and Tatiana Zemskova and Svetlana Ladanova and Roman Titkov and Dmitry Yudin and Maxim Monastyrny and Aleksei Valenkov", "abstract": "  Locating objects described in natural language presents a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object grounding with simple (bare) queries, but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene graph representation with\nmetric and semantic spatial edges and utilizes a large language model as a\nhuman-to-agent interface through our deductive scene reasoning algorithm. BBQ\nemploys robust DINO-powered associations to construct 3D object-centric map and\nan advanced raycasting algorithm with a 2D vision-language model to describe\nthem as graph nodes. On the Replica and ScanNet datasets, we have demonstrated\nthat BBQ takes a leading place in open-vocabulary 3D semantic segmentation\ncompared to other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,\nour deductive approach demonstrates a significant improvement, enabling objects\ngrounding by complex queries compared to other state-of-the-art methods. The\ncombination of our design choices and software implementation has resulted in\nsignificant data processing speed in experiments on the robot on-board\ncomputer. This promising performance enables the application of our approach in\nintelligent robotics projects. We made the code publicly available at\nhttps://linukc.github.io/BeyondBareQueries/.\n", "link": "http://arxiv.org/abs/2406.07113v4", "date": "2025-05-06", "relevancy": 2.6835, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6726}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Grounding%20with%203D%20Scene%0A%20%20Graph&body=Title%3A%20Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Grounding%20with%203D%20Scene%0A%20%20Graph%0AAuthor%3A%20Sergey%20Linok%20and%20Tatiana%20Zemskova%20and%20Svetlana%20Ladanova%20and%20Roman%20Titkov%20and%20Dmitry%20Yudin%20and%20Maxim%20Monastyrny%20and%20Aleksei%20Valenkov%0AAbstract%3A%20%20%20Locating%20objects%20described%20in%20natural%20language%20presents%20a%20significant%0Achallenge%20for%20autonomous%20agents.%20Existing%20CLIP-based%20open-vocabulary%20methods%0Asuccessfully%20perform%203D%20object%20grounding%20with%20simple%20%28bare%29%20queries%2C%20but%20cannot%0Acope%20with%20ambiguous%20descriptions%20that%20demand%20an%20understanding%20of%20object%0Arelations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20modular%20approach%20called%20BBQ%0A%28Beyond%20Bare%20Queries%29%2C%20which%20constructs%203D%20scene%20graph%20representation%20with%0Ametric%20and%20semantic%20spatial%20edges%20and%20utilizes%20a%20large%20language%20model%20as%20a%0Ahuman-to-agent%20interface%20through%20our%20deductive%20scene%20reasoning%20algorithm.%20BBQ%0Aemploys%20robust%20DINO-powered%20associations%20to%20construct%203D%20object-centric%20map%20and%0Aan%20advanced%20raycasting%20algorithm%20with%20a%202D%20vision-language%20model%20to%20describe%0Athem%20as%20graph%20nodes.%20On%20the%20Replica%20and%20ScanNet%20datasets%2C%20we%20have%20demonstrated%0Athat%20BBQ%20takes%20a%20leading%20place%20in%20open-vocabulary%203D%20semantic%20segmentation%0Acompared%20to%20other%20zero-shot%20methods.%20Also%2C%20we%20show%20that%20leveraging%20spatial%0Arelations%20is%20especially%20effective%20for%20scenes%20containing%20multiple%20entities%20of%0Athe%20same%20semantic%20class.%20On%20challenging%20Sr3D%2B%2C%20Nr3D%20and%20ScanRefer%20benchmarks%2C%0Aour%20deductive%20approach%20demonstrates%20a%20significant%20improvement%2C%20enabling%20objects%0Agrounding%20by%20complex%20queries%20compared%20to%20other%20state-of-the-art%20methods.%20The%0Acombination%20of%20our%20design%20choices%20and%20software%20implementation%20has%20resulted%20in%0Asignificant%20data%20processing%20speed%20in%20experiments%20on%20the%20robot%20on-board%0Acomputer.%20This%20promising%20performance%20enables%20the%20application%20of%20our%20approach%20in%0Aintelligent%20robotics%20projects.%20We%20made%20the%20code%20publicly%20available%20at%0Ahttps%3A//linukc.github.io/BeyondBareQueries/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07113v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Bare%2520Queries%253A%2520Open-Vocabulary%2520Object%2520Grounding%2520with%25203D%2520Scene%250A%2520%2520Graph%26entry.906535625%3DSergey%2520Linok%2520and%2520Tatiana%2520Zemskova%2520and%2520Svetlana%2520Ladanova%2520and%2520Roman%2520Titkov%2520and%2520Dmitry%2520Yudin%2520and%2520Maxim%2520Monastyrny%2520and%2520Aleksei%2520Valenkov%26entry.1292438233%3D%2520%2520Locating%2520objects%2520described%2520in%2520natural%2520language%2520presents%2520a%2520significant%250Achallenge%2520for%2520autonomous%2520agents.%2520Existing%2520CLIP-based%2520open-vocabulary%2520methods%250Asuccessfully%2520perform%25203D%2520object%2520grounding%2520with%2520simple%2520%2528bare%2529%2520queries%252C%2520but%2520cannot%250Acope%2520with%2520ambiguous%2520descriptions%2520that%2520demand%2520an%2520understanding%2520of%2520object%250Arelations.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520modular%2520approach%2520called%2520BBQ%250A%2528Beyond%2520Bare%2520Queries%2529%252C%2520which%2520constructs%25203D%2520scene%2520graph%2520representation%2520with%250Ametric%2520and%2520semantic%2520spatial%2520edges%2520and%2520utilizes%2520a%2520large%2520language%2520model%2520as%2520a%250Ahuman-to-agent%2520interface%2520through%2520our%2520deductive%2520scene%2520reasoning%2520algorithm.%2520BBQ%250Aemploys%2520robust%2520DINO-powered%2520associations%2520to%2520construct%25203D%2520object-centric%2520map%2520and%250Aan%2520advanced%2520raycasting%2520algorithm%2520with%2520a%25202D%2520vision-language%2520model%2520to%2520describe%250Athem%2520as%2520graph%2520nodes.%2520On%2520the%2520Replica%2520and%2520ScanNet%2520datasets%252C%2520we%2520have%2520demonstrated%250Athat%2520BBQ%2520takes%2520a%2520leading%2520place%2520in%2520open-vocabulary%25203D%2520semantic%2520segmentation%250Acompared%2520to%2520other%2520zero-shot%2520methods.%2520Also%252C%2520we%2520show%2520that%2520leveraging%2520spatial%250Arelations%2520is%2520especially%2520effective%2520for%2520scenes%2520containing%2520multiple%2520entities%2520of%250Athe%2520same%2520semantic%2520class.%2520On%2520challenging%2520Sr3D%252B%252C%2520Nr3D%2520and%2520ScanRefer%2520benchmarks%252C%250Aour%2520deductive%2520approach%2520demonstrates%2520a%2520significant%2520improvement%252C%2520enabling%2520objects%250Agrounding%2520by%2520complex%2520queries%2520compared%2520to%2520other%2520state-of-the-art%2520methods.%2520The%250Acombination%2520of%2520our%2520design%2520choices%2520and%2520software%2520implementation%2520has%2520resulted%2520in%250Asignificant%2520data%2520processing%2520speed%2520in%2520experiments%2520on%2520the%2520robot%2520on-board%250Acomputer.%2520This%2520promising%2520performance%2520enables%2520the%2520application%2520of%2520our%2520approach%2520in%250Aintelligent%2520robotics%2520projects.%2520We%2520made%2520the%2520code%2520publicly%2520available%2520at%250Ahttps%253A//linukc.github.io/BeyondBareQueries/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07113v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Grounding%20with%203D%20Scene%0A%20%20Graph&entry.906535625=Sergey%20Linok%20and%20Tatiana%20Zemskova%20and%20Svetlana%20Ladanova%20and%20Roman%20Titkov%20and%20Dmitry%20Yudin%20and%20Maxim%20Monastyrny%20and%20Aleksei%20Valenkov&entry.1292438233=%20%20Locating%20objects%20described%20in%20natural%20language%20presents%20a%20significant%0Achallenge%20for%20autonomous%20agents.%20Existing%20CLIP-based%20open-vocabulary%20methods%0Asuccessfully%20perform%203D%20object%20grounding%20with%20simple%20%28bare%29%20queries%2C%20but%20cannot%0Acope%20with%20ambiguous%20descriptions%20that%20demand%20an%20understanding%20of%20object%0Arelations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20modular%20approach%20called%20BBQ%0A%28Beyond%20Bare%20Queries%29%2C%20which%20constructs%203D%20scene%20graph%20representation%20with%0Ametric%20and%20semantic%20spatial%20edges%20and%20utilizes%20a%20large%20language%20model%20as%20a%0Ahuman-to-agent%20interface%20through%20our%20deductive%20scene%20reasoning%20algorithm.%20BBQ%0Aemploys%20robust%20DINO-powered%20associations%20to%20construct%203D%20object-centric%20map%20and%0Aan%20advanced%20raycasting%20algorithm%20with%20a%202D%20vision-language%20model%20to%20describe%0Athem%20as%20graph%20nodes.%20On%20the%20Replica%20and%20ScanNet%20datasets%2C%20we%20have%20demonstrated%0Athat%20BBQ%20takes%20a%20leading%20place%20in%20open-vocabulary%203D%20semantic%20segmentation%0Acompared%20to%20other%20zero-shot%20methods.%20Also%2C%20we%20show%20that%20leveraging%20spatial%0Arelations%20is%20especially%20effective%20for%20scenes%20containing%20multiple%20entities%20of%0Athe%20same%20semantic%20class.%20On%20challenging%20Sr3D%2B%2C%20Nr3D%20and%20ScanRefer%20benchmarks%2C%0Aour%20deductive%20approach%20demonstrates%20a%20significant%20improvement%2C%20enabling%20objects%0Agrounding%20by%20complex%20queries%20compared%20to%20other%20state-of-the-art%20methods.%20The%0Acombination%20of%20our%20design%20choices%20and%20software%20implementation%20has%20resulted%20in%0Asignificant%20data%20processing%20speed%20in%20experiments%20on%20the%20robot%20on-board%0Acomputer.%20This%20promising%20performance%20enables%20the%20application%20of%20our%20approach%20in%0Aintelligent%20robotics%20projects.%20We%20made%20the%20code%20publicly%20available%20at%0Ahttps%3A//linukc.github.io/BeyondBareQueries/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07113v4&entry.124074799=Read"},
{"title": "Graph Drawing for LLMs: An Empirical Evaluation", "author": "Walter Didimo and Fabrizio Montecchiani and Tommaso Piselli", "abstract": "  Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.\n", "link": "http://arxiv.org/abs/2505.03678v1", "date": "2025-05-06", "relevancy": 2.672, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Drawing%20for%20LLMs%3A%20An%20Empirical%20Evaluation&body=Title%3A%20Graph%20Drawing%20for%20LLMs%3A%20An%20Empirical%20Evaluation%0AAuthor%3A%20Walter%20Didimo%20and%20Fabrizio%20Montecchiani%20and%20Tommaso%20Piselli%0AAbstract%3A%20%20%20Our%20work%20contributes%20to%20the%20fast-growing%20literature%20on%20the%20use%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20perform%20graph-related%20tasks.%20In%20particular%2C%20we%20focus%0Aon%20usage%20scenarios%20that%20rely%20on%20the%20visual%20modality%2C%20feeding%20the%20model%20with%20a%0Adrawing%20of%20the%20graph%20under%20analysis.%20We%20investigate%20how%20the%20model%27s%20performance%0Ais%20affected%20by%20the%20chosen%20layout%20paradigm%2C%20the%20aesthetics%20of%20the%20drawing%2C%20and%0Athe%20prompting%20technique%20used%20for%20the%20queries.%20We%20formulate%20three%20corresponding%0Aresearch%20questions%20and%20present%20the%20results%20of%20a%20thorough%20experimental%20analysis.%0AOur%20findings%20reveal%20that%20choosing%20the%20right%20layout%20paradigm%20and%20optimizing%20the%0Areadability%20of%20the%20input%20drawing%20from%20a%20human%20perspective%20can%20significantly%0Aimprove%20the%20performance%20of%20the%20model%20on%20the%20given%20task.%20Moreover%2C%20selecting%20the%0Amost%20effective%20prompting%20technique%20is%20a%20challenging%20yet%20crucial%20task%20for%0Aachieving%20optimal%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Drawing%2520for%2520LLMs%253A%2520An%2520Empirical%2520Evaluation%26entry.906535625%3DWalter%2520Didimo%2520and%2520Fabrizio%2520Montecchiani%2520and%2520Tommaso%2520Piselli%26entry.1292438233%3D%2520%2520Our%2520work%2520contributes%2520to%2520the%2520fast-growing%2520literature%2520on%2520the%2520use%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520perform%2520graph-related%2520tasks.%2520In%2520particular%252C%2520we%2520focus%250Aon%2520usage%2520scenarios%2520that%2520rely%2520on%2520the%2520visual%2520modality%252C%2520feeding%2520the%2520model%2520with%2520a%250Adrawing%2520of%2520the%2520graph%2520under%2520analysis.%2520We%2520investigate%2520how%2520the%2520model%2527s%2520performance%250Ais%2520affected%2520by%2520the%2520chosen%2520layout%2520paradigm%252C%2520the%2520aesthetics%2520of%2520the%2520drawing%252C%2520and%250Athe%2520prompting%2520technique%2520used%2520for%2520the%2520queries.%2520We%2520formulate%2520three%2520corresponding%250Aresearch%2520questions%2520and%2520present%2520the%2520results%2520of%2520a%2520thorough%2520experimental%2520analysis.%250AOur%2520findings%2520reveal%2520that%2520choosing%2520the%2520right%2520layout%2520paradigm%2520and%2520optimizing%2520the%250Areadability%2520of%2520the%2520input%2520drawing%2520from%2520a%2520human%2520perspective%2520can%2520significantly%250Aimprove%2520the%2520performance%2520of%2520the%2520model%2520on%2520the%2520given%2520task.%2520Moreover%252C%2520selecting%2520the%250Amost%2520effective%2520prompting%2520technique%2520is%2520a%2520challenging%2520yet%2520crucial%2520task%2520for%250Aachieving%2520optimal%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Drawing%20for%20LLMs%3A%20An%20Empirical%20Evaluation&entry.906535625=Walter%20Didimo%20and%20Fabrizio%20Montecchiani%20and%20Tommaso%20Piselli&entry.1292438233=%20%20Our%20work%20contributes%20to%20the%20fast-growing%20literature%20on%20the%20use%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20perform%20graph-related%20tasks.%20In%20particular%2C%20we%20focus%0Aon%20usage%20scenarios%20that%20rely%20on%20the%20visual%20modality%2C%20feeding%20the%20model%20with%20a%0Adrawing%20of%20the%20graph%20under%20analysis.%20We%20investigate%20how%20the%20model%27s%20performance%0Ais%20affected%20by%20the%20chosen%20layout%20paradigm%2C%20the%20aesthetics%20of%20the%20drawing%2C%20and%0Athe%20prompting%20technique%20used%20for%20the%20queries.%20We%20formulate%20three%20corresponding%0Aresearch%20questions%20and%20present%20the%20results%20of%20a%20thorough%20experimental%20analysis.%0AOur%20findings%20reveal%20that%20choosing%20the%20right%20layout%20paradigm%20and%20optimizing%20the%0Areadability%20of%20the%20input%20drawing%20from%20a%20human%20perspective%20can%20significantly%0Aimprove%20the%20performance%20of%20the%20model%20on%20the%20given%20task.%20Moreover%2C%20selecting%20the%0Amost%20effective%20prompting%20technique%20is%20a%20challenging%20yet%20crucial%20task%20for%0Aachieving%20optimal%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03678v1&entry.124074799=Read"},
{"title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for\n  Longitudinal Radiology Report Generation", "author": "Shanshan Song and Hui Tang and Honglong Yang and Xiaomeng Li", "abstract": "  Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.\n", "link": "http://arxiv.org/abs/2505.03401v1", "date": "2025-05-06", "relevancy": 2.6412, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5299}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5299}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DDaTR%3A%20Dynamic%20Difference-aware%20Temporal%20Residual%20Network%20for%0A%20%20Longitudinal%20Radiology%20Report%20Generation&body=Title%3A%20DDaTR%3A%20Dynamic%20Difference-aware%20Temporal%20Residual%20Network%20for%0A%20%20Longitudinal%20Radiology%20Report%20Generation%0AAuthor%3A%20Shanshan%20Song%20and%20Hui%20Tang%20and%20Honglong%20Yang%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20Radiology%20Report%20Generation%20%28RRG%29%20automates%20the%20creation%20of%20radiology%20reports%0Afrom%20medical%20imaging%2C%20enhancing%20the%20efficiency%20of%20the%20reporting%20process.%0ALongitudinal%20Radiology%20Report%20Generation%20%28LRRG%29%20extends%20RRG%20by%20incorporating%0Athe%20ability%20to%20compare%20current%20and%20prior%20exams%2C%20facilitating%20the%20tracking%20of%0Atemporal%20changes%20in%20clinical%20findings.%20Existing%20LRRG%20approaches%20only%20extract%0Afeatures%20from%20prior%20and%20current%20images%20using%20a%20visual%20pre-trained%20encoder%2C%0Awhich%20are%20then%20concatenated%20to%20generate%20the%20final%20report.%20However%2C%20these%0Amethods%20struggle%20to%20effectively%20capture%20both%20spatial%20and%20temporal%20correlations%0Aduring%20the%20feature%20extraction%20process.%20Consequently%2C%20the%20extracted%20features%0Ainadequately%20capture%20the%20information%20of%20difference%20across%20exams%20and%20thus%0Aunderrepresent%20the%20expected%20progressions%2C%20leading%20to%20sub-optimal%20performance%20in%0ALRRG.%20To%20address%20this%2C%20we%20develop%20a%20novel%20dynamic%20difference-aware%20temporal%0Aresidual%20network%20%28DDaTR%29.%20In%20DDaTR%2C%20we%20introduce%20two%20modules%20at%20each%20stage%20of%0Athe%20visual%20encoder%20to%20capture%20multi-level%20spatial%20correlations.%20The%20Dynamic%0AFeature%20Alignment%20Module%20%28DFAM%29%20is%20designed%20to%20align%20prior%20features%20across%0Amodalities%20for%20the%20integrity%20of%20prior%20clinical%20information.%20Prompted%20by%20the%0Aenriched%20prior%20features%2C%20the%20dynamic%20difference-aware%20module%20%28DDAM%29%20captures%0Afavorable%20difference%20information%20by%20identifying%20relationships%20across%20exams.%0AFurthermore%2C%20our%20DDaTR%20employs%20the%20dynamic%20residual%20network%20to%20unidirectionally%0Atransmit%20longitudinal%20information%2C%20effectively%20modelling%20temporal%20correlations.%0AExtensive%20experiments%20demonstrated%20superior%20performance%20over%20existing%20methods%0Aon%20three%20benchmarks%2C%20proving%20its%20efficacy%20in%20both%20RRG%20and%20LRRG%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDDaTR%253A%2520Dynamic%2520Difference-aware%2520Temporal%2520Residual%2520Network%2520for%250A%2520%2520Longitudinal%2520Radiology%2520Report%2520Generation%26entry.906535625%3DShanshan%2520Song%2520and%2520Hui%2520Tang%2520and%2520Honglong%2520Yang%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520Radiology%2520Report%2520Generation%2520%2528RRG%2529%2520automates%2520the%2520creation%2520of%2520radiology%2520reports%250Afrom%2520medical%2520imaging%252C%2520enhancing%2520the%2520efficiency%2520of%2520the%2520reporting%2520process.%250ALongitudinal%2520Radiology%2520Report%2520Generation%2520%2528LRRG%2529%2520extends%2520RRG%2520by%2520incorporating%250Athe%2520ability%2520to%2520compare%2520current%2520and%2520prior%2520exams%252C%2520facilitating%2520the%2520tracking%2520of%250Atemporal%2520changes%2520in%2520clinical%2520findings.%2520Existing%2520LRRG%2520approaches%2520only%2520extract%250Afeatures%2520from%2520prior%2520and%2520current%2520images%2520using%2520a%2520visual%2520pre-trained%2520encoder%252C%250Awhich%2520are%2520then%2520concatenated%2520to%2520generate%2520the%2520final%2520report.%2520However%252C%2520these%250Amethods%2520struggle%2520to%2520effectively%2520capture%2520both%2520spatial%2520and%2520temporal%2520correlations%250Aduring%2520the%2520feature%2520extraction%2520process.%2520Consequently%252C%2520the%2520extracted%2520features%250Ainadequately%2520capture%2520the%2520information%2520of%2520difference%2520across%2520exams%2520and%2520thus%250Aunderrepresent%2520the%2520expected%2520progressions%252C%2520leading%2520to%2520sub-optimal%2520performance%2520in%250ALRRG.%2520To%2520address%2520this%252C%2520we%2520develop%2520a%2520novel%2520dynamic%2520difference-aware%2520temporal%250Aresidual%2520network%2520%2528DDaTR%2529.%2520In%2520DDaTR%252C%2520we%2520introduce%2520two%2520modules%2520at%2520each%2520stage%2520of%250Athe%2520visual%2520encoder%2520to%2520capture%2520multi-level%2520spatial%2520correlations.%2520The%2520Dynamic%250AFeature%2520Alignment%2520Module%2520%2528DFAM%2529%2520is%2520designed%2520to%2520align%2520prior%2520features%2520across%250Amodalities%2520for%2520the%2520integrity%2520of%2520prior%2520clinical%2520information.%2520Prompted%2520by%2520the%250Aenriched%2520prior%2520features%252C%2520the%2520dynamic%2520difference-aware%2520module%2520%2528DDAM%2529%2520captures%250Afavorable%2520difference%2520information%2520by%2520identifying%2520relationships%2520across%2520exams.%250AFurthermore%252C%2520our%2520DDaTR%2520employs%2520the%2520dynamic%2520residual%2520network%2520to%2520unidirectionally%250Atransmit%2520longitudinal%2520information%252C%2520effectively%2520modelling%2520temporal%2520correlations.%250AExtensive%2520experiments%2520demonstrated%2520superior%2520performance%2520over%2520existing%2520methods%250Aon%2520three%2520benchmarks%252C%2520proving%2520its%2520efficacy%2520in%2520both%2520RRG%2520and%2520LRRG%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDaTR%3A%20Dynamic%20Difference-aware%20Temporal%20Residual%20Network%20for%0A%20%20Longitudinal%20Radiology%20Report%20Generation&entry.906535625=Shanshan%20Song%20and%20Hui%20Tang%20and%20Honglong%20Yang%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Radiology%20Report%20Generation%20%28RRG%29%20automates%20the%20creation%20of%20radiology%20reports%0Afrom%20medical%20imaging%2C%20enhancing%20the%20efficiency%20of%20the%20reporting%20process.%0ALongitudinal%20Radiology%20Report%20Generation%20%28LRRG%29%20extends%20RRG%20by%20incorporating%0Athe%20ability%20to%20compare%20current%20and%20prior%20exams%2C%20facilitating%20the%20tracking%20of%0Atemporal%20changes%20in%20clinical%20findings.%20Existing%20LRRG%20approaches%20only%20extract%0Afeatures%20from%20prior%20and%20current%20images%20using%20a%20visual%20pre-trained%20encoder%2C%0Awhich%20are%20then%20concatenated%20to%20generate%20the%20final%20report.%20However%2C%20these%0Amethods%20struggle%20to%20effectively%20capture%20both%20spatial%20and%20temporal%20correlations%0Aduring%20the%20feature%20extraction%20process.%20Consequently%2C%20the%20extracted%20features%0Ainadequately%20capture%20the%20information%20of%20difference%20across%20exams%20and%20thus%0Aunderrepresent%20the%20expected%20progressions%2C%20leading%20to%20sub-optimal%20performance%20in%0ALRRG.%20To%20address%20this%2C%20we%20develop%20a%20novel%20dynamic%20difference-aware%20temporal%0Aresidual%20network%20%28DDaTR%29.%20In%20DDaTR%2C%20we%20introduce%20two%20modules%20at%20each%20stage%20of%0Athe%20visual%20encoder%20to%20capture%20multi-level%20spatial%20correlations.%20The%20Dynamic%0AFeature%20Alignment%20Module%20%28DFAM%29%20is%20designed%20to%20align%20prior%20features%20across%0Amodalities%20for%20the%20integrity%20of%20prior%20clinical%20information.%20Prompted%20by%20the%0Aenriched%20prior%20features%2C%20the%20dynamic%20difference-aware%20module%20%28DDAM%29%20captures%0Afavorable%20difference%20information%20by%20identifying%20relationships%20across%20exams.%0AFurthermore%2C%20our%20DDaTR%20employs%20the%20dynamic%20residual%20network%20to%20unidirectionally%0Atransmit%20longitudinal%20information%2C%20effectively%20modelling%20temporal%20correlations.%0AExtensive%20experiments%20demonstrated%20superior%20performance%20over%20existing%20methods%0Aon%20three%20benchmarks%2C%20proving%20its%20efficacy%20in%20both%20RRG%20and%20LRRG%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03401v1&entry.124074799=Read"},
{"title": "A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based\n  Perspective", "author": "Ziwen Zhao and Yixin Su and Yuhua Li and Yixiong Zou and Ruixuan Li and Rui Zhang", "abstract": "  Graph self-supervised learning (SSL) is now a go-to method for pre-training\ngraph foundation models (GFMs). There is a wide variety of knowledge patterns\nembedded in the graph data, such as node properties and clusters, which are\ncrucial to learning generalized representations for GFMs. However, existing\nsurveys of GFMs have several shortcomings: they lack comprehensiveness\nregarding the most recent progress, have unclear categorization of\nself-supervised methods, and take a limited architecture-based perspective that\nis restricted to only certain types of graph models. As the ultimate goal of\nGFMs is to learn generalized graph knowledge, we provide a comprehensive survey\nof self-supervised GFMs from a novel knowledge-based perspective. We propose a\nknowledge-based taxonomy, which categorizes self-supervised graph models by the\nspecific graph knowledge utilized. Our taxonomy consists of microscopic (nodes,\nlinks, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge\n(global structure, manifolds, etc.). It covers a total of 9 knowledge\ncategories and more than 25 pretext tasks for pre-training GFMs, as well as\nvarious downstream task generalization strategies. Such a knowledge-based\ntaxonomy allows us to re-examine graph models based on new architectures more\nclearly, such as graph language models, as well as provide more in-depth\ninsights for constructing GFMs.\n", "link": "http://arxiv.org/abs/2403.16137v3", "date": "2025-05-06", "relevancy": 2.6182, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Self-Supervised%20Graph%20Foundation%20Models%3A%20Knowledge-Based%0A%20%20Perspective&body=Title%3A%20A%20Survey%20on%20Self-Supervised%20Graph%20Foundation%20Models%3A%20Knowledge-Based%0A%20%20Perspective%0AAuthor%3A%20Ziwen%20Zhao%20and%20Yixin%20Su%20and%20Yuhua%20Li%20and%20Yixiong%20Zou%20and%20Ruixuan%20Li%20and%20Rui%20Zhang%0AAbstract%3A%20%20%20Graph%20self-supervised%20learning%20%28SSL%29%20is%20now%20a%20go-to%20method%20for%20pre-training%0Agraph%20foundation%20models%20%28GFMs%29.%20There%20is%20a%20wide%20variety%20of%20knowledge%20patterns%0Aembedded%20in%20the%20graph%20data%2C%20such%20as%20node%20properties%20and%20clusters%2C%20which%20are%0Acrucial%20to%20learning%20generalized%20representations%20for%20GFMs.%20However%2C%20existing%0Asurveys%20of%20GFMs%20have%20several%20shortcomings%3A%20they%20lack%20comprehensiveness%0Aregarding%20the%20most%20recent%20progress%2C%20have%20unclear%20categorization%20of%0Aself-supervised%20methods%2C%20and%20take%20a%20limited%20architecture-based%20perspective%20that%0Ais%20restricted%20to%20only%20certain%20types%20of%20graph%20models.%20As%20the%20ultimate%20goal%20of%0AGFMs%20is%20to%20learn%20generalized%20graph%20knowledge%2C%20we%20provide%20a%20comprehensive%20survey%0Aof%20self-supervised%20GFMs%20from%20a%20novel%20knowledge-based%20perspective.%20We%20propose%20a%0Aknowledge-based%20taxonomy%2C%20which%20categorizes%20self-supervised%20graph%20models%20by%20the%0Aspecific%20graph%20knowledge%20utilized.%20Our%20taxonomy%20consists%20of%20microscopic%20%28nodes%2C%0Alinks%2C%20etc.%29%2C%20mesoscopic%20%28context%2C%20clusters%2C%20etc.%29%2C%20and%20macroscopic%20knowledge%0A%28global%20structure%2C%20manifolds%2C%20etc.%29.%20It%20covers%20a%20total%20of%209%20knowledge%0Acategories%20and%20more%20than%2025%20pretext%20tasks%20for%20pre-training%20GFMs%2C%20as%20well%20as%0Avarious%20downstream%20task%20generalization%20strategies.%20Such%20a%20knowledge-based%0Ataxonomy%20allows%20us%20to%20re-examine%20graph%20models%20based%20on%20new%20architectures%20more%0Aclearly%2C%20such%20as%20graph%20language%20models%2C%20as%20well%20as%20provide%20more%20in-depth%0Ainsights%20for%20constructing%20GFMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16137v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Self-Supervised%2520Graph%2520Foundation%2520Models%253A%2520Knowledge-Based%250A%2520%2520Perspective%26entry.906535625%3DZiwen%2520Zhao%2520and%2520Yixin%2520Su%2520and%2520Yuhua%2520Li%2520and%2520Yixiong%2520Zou%2520and%2520Ruixuan%2520Li%2520and%2520Rui%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520self-supervised%2520learning%2520%2528SSL%2529%2520is%2520now%2520a%2520go-to%2520method%2520for%2520pre-training%250Agraph%2520foundation%2520models%2520%2528GFMs%2529.%2520There%2520is%2520a%2520wide%2520variety%2520of%2520knowledge%2520patterns%250Aembedded%2520in%2520the%2520graph%2520data%252C%2520such%2520as%2520node%2520properties%2520and%2520clusters%252C%2520which%2520are%250Acrucial%2520to%2520learning%2520generalized%2520representations%2520for%2520GFMs.%2520However%252C%2520existing%250Asurveys%2520of%2520GFMs%2520have%2520several%2520shortcomings%253A%2520they%2520lack%2520comprehensiveness%250Aregarding%2520the%2520most%2520recent%2520progress%252C%2520have%2520unclear%2520categorization%2520of%250Aself-supervised%2520methods%252C%2520and%2520take%2520a%2520limited%2520architecture-based%2520perspective%2520that%250Ais%2520restricted%2520to%2520only%2520certain%2520types%2520of%2520graph%2520models.%2520As%2520the%2520ultimate%2520goal%2520of%250AGFMs%2520is%2520to%2520learn%2520generalized%2520graph%2520knowledge%252C%2520we%2520provide%2520a%2520comprehensive%2520survey%250Aof%2520self-supervised%2520GFMs%2520from%2520a%2520novel%2520knowledge-based%2520perspective.%2520We%2520propose%2520a%250Aknowledge-based%2520taxonomy%252C%2520which%2520categorizes%2520self-supervised%2520graph%2520models%2520by%2520the%250Aspecific%2520graph%2520knowledge%2520utilized.%2520Our%2520taxonomy%2520consists%2520of%2520microscopic%2520%2528nodes%252C%250Alinks%252C%2520etc.%2529%252C%2520mesoscopic%2520%2528context%252C%2520clusters%252C%2520etc.%2529%252C%2520and%2520macroscopic%2520knowledge%250A%2528global%2520structure%252C%2520manifolds%252C%2520etc.%2529.%2520It%2520covers%2520a%2520total%2520of%25209%2520knowledge%250Acategories%2520and%2520more%2520than%252025%2520pretext%2520tasks%2520for%2520pre-training%2520GFMs%252C%2520as%2520well%2520as%250Avarious%2520downstream%2520task%2520generalization%2520strategies.%2520Such%2520a%2520knowledge-based%250Ataxonomy%2520allows%2520us%2520to%2520re-examine%2520graph%2520models%2520based%2520on%2520new%2520architectures%2520more%250Aclearly%252C%2520such%2520as%2520graph%2520language%2520models%252C%2520as%2520well%2520as%2520provide%2520more%2520in-depth%250Ainsights%2520for%2520constructing%2520GFMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16137v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Self-Supervised%20Graph%20Foundation%20Models%3A%20Knowledge-Based%0A%20%20Perspective&entry.906535625=Ziwen%20Zhao%20and%20Yixin%20Su%20and%20Yuhua%20Li%20and%20Yixiong%20Zou%20and%20Ruixuan%20Li%20and%20Rui%20Zhang&entry.1292438233=%20%20Graph%20self-supervised%20learning%20%28SSL%29%20is%20now%20a%20go-to%20method%20for%20pre-training%0Agraph%20foundation%20models%20%28GFMs%29.%20There%20is%20a%20wide%20variety%20of%20knowledge%20patterns%0Aembedded%20in%20the%20graph%20data%2C%20such%20as%20node%20properties%20and%20clusters%2C%20which%20are%0Acrucial%20to%20learning%20generalized%20representations%20for%20GFMs.%20However%2C%20existing%0Asurveys%20of%20GFMs%20have%20several%20shortcomings%3A%20they%20lack%20comprehensiveness%0Aregarding%20the%20most%20recent%20progress%2C%20have%20unclear%20categorization%20of%0Aself-supervised%20methods%2C%20and%20take%20a%20limited%20architecture-based%20perspective%20that%0Ais%20restricted%20to%20only%20certain%20types%20of%20graph%20models.%20As%20the%20ultimate%20goal%20of%0AGFMs%20is%20to%20learn%20generalized%20graph%20knowledge%2C%20we%20provide%20a%20comprehensive%20survey%0Aof%20self-supervised%20GFMs%20from%20a%20novel%20knowledge-based%20perspective.%20We%20propose%20a%0Aknowledge-based%20taxonomy%2C%20which%20categorizes%20self-supervised%20graph%20models%20by%20the%0Aspecific%20graph%20knowledge%20utilized.%20Our%20taxonomy%20consists%20of%20microscopic%20%28nodes%2C%0Alinks%2C%20etc.%29%2C%20mesoscopic%20%28context%2C%20clusters%2C%20etc.%29%2C%20and%20macroscopic%20knowledge%0A%28global%20structure%2C%20manifolds%2C%20etc.%29.%20It%20covers%20a%20total%20of%209%20knowledge%0Acategories%20and%20more%20than%2025%20pretext%20tasks%20for%20pre-training%20GFMs%2C%20as%20well%20as%0Avarious%20downstream%20task%20generalization%20strategies.%20Such%20a%20knowledge-based%0Ataxonomy%20allows%20us%20to%20re-examine%20graph%20models%20based%20on%20new%20architectures%20more%0Aclearly%2C%20such%20as%20graph%20language%20models%2C%20as%20well%20as%20provide%20more%20in-depth%0Ainsights%20for%20constructing%20GFMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16137v3&entry.124074799=Read"},
{"title": "Task Reconstruction and Extrapolation for $\u03c0_0$ using Text Latent", "author": "Quanyi Li", "abstract": "  Vision-language-action models (VLAs) often achieve high performance on\ndemonstrated tasks but struggle significantly when required to extrapolate,\ncombining skills learned from different tasks in novel ways. For instance, VLAs\nmight successfully put the cream cheese in the bowl and put the bowl on top of\nthe cabinet, yet still fail to put the cream cheese on top of the cabinet. In\nthis work, we demonstrate that behaviors from distinct tasks can be effectively\nrecombined by manipulating the VLA's internal representations at inference\ntime. Concretely, we identify the text latent by averaging the text tokens'\nhidden states across all demonstrated trajectories for a specific base task.\nFor executing an extrapolated task, we can temporally interpolate the text\nlatent of the two base tasks and add it back to the text hidden states, so\nsub-behaviors from the two tasks will be activated sequentially. We evaluate\nthis approach using the newly created libero-ood benchmark, featuring 20 tasks\nextrapolated from standard LIBERO suites. The results on libero-ood show that\nall SOTA VLAs achieve < 15% success rate, while $\\pi0$ with text latent\ninterpolation reaches an 83% success rate. Further qualitative analysis reveals\na tendency for VLAs to exhibit spatial overfitting, mapping object names to\ndemonstrated locations rather than achieving genuine object and goal\nunderstanding. Additionally, we find that decoding the text latent yields\nhuman-unreadable prompts that can nevertheless instruct the VLA to achieve a\n70% success rate on standard LIBERO suites, enabling private instruction or\nbackdoor attacks.\n", "link": "http://arxiv.org/abs/2505.03500v1", "date": "2025-05-06", "relevancy": 2.6135, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5377}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5377}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Reconstruction%20and%20Extrapolation%20for%20%24%CF%80_0%24%20using%20Text%20Latent&body=Title%3A%20Task%20Reconstruction%20and%20Extrapolation%20for%20%24%CF%80_0%24%20using%20Text%20Latent%0AAuthor%3A%20Quanyi%20Li%0AAbstract%3A%20%20%20Vision-language-action%20models%20%28VLAs%29%20often%20achieve%20high%20performance%20on%0Ademonstrated%20tasks%20but%20struggle%20significantly%20when%20required%20to%20extrapolate%2C%0Acombining%20skills%20learned%20from%20different%20tasks%20in%20novel%20ways.%20For%20instance%2C%20VLAs%0Amight%20successfully%20put%20the%20cream%20cheese%20in%20the%20bowl%20and%20put%20the%20bowl%20on%20top%20of%0Athe%20cabinet%2C%20yet%20still%20fail%20to%20put%20the%20cream%20cheese%20on%20top%20of%20the%20cabinet.%20In%0Athis%20work%2C%20we%20demonstrate%20that%20behaviors%20from%20distinct%20tasks%20can%20be%20effectively%0Arecombined%20by%20manipulating%20the%20VLA%27s%20internal%20representations%20at%20inference%0Atime.%20Concretely%2C%20we%20identify%20the%20text%20latent%20by%20averaging%20the%20text%20tokens%27%0Ahidden%20states%20across%20all%20demonstrated%20trajectories%20for%20a%20specific%20base%20task.%0AFor%20executing%20an%20extrapolated%20task%2C%20we%20can%20temporally%20interpolate%20the%20text%0Alatent%20of%20the%20two%20base%20tasks%20and%20add%20it%20back%20to%20the%20text%20hidden%20states%2C%20so%0Asub-behaviors%20from%20the%20two%20tasks%20will%20be%20activated%20sequentially.%20We%20evaluate%0Athis%20approach%20using%20the%20newly%20created%20libero-ood%20benchmark%2C%20featuring%2020%20tasks%0Aextrapolated%20from%20standard%20LIBERO%20suites.%20The%20results%20on%20libero-ood%20show%20that%0Aall%20SOTA%20VLAs%20achieve%20%3C%2015%25%20success%20rate%2C%20while%20%24%5Cpi0%24%20with%20text%20latent%0Ainterpolation%20reaches%20an%2083%25%20success%20rate.%20Further%20qualitative%20analysis%20reveals%0Aa%20tendency%20for%20VLAs%20to%20exhibit%20spatial%20overfitting%2C%20mapping%20object%20names%20to%0Ademonstrated%20locations%20rather%20than%20achieving%20genuine%20object%20and%20goal%0Aunderstanding.%20Additionally%2C%20we%20find%20that%20decoding%20the%20text%20latent%20yields%0Ahuman-unreadable%20prompts%20that%20can%20nevertheless%20instruct%20the%20VLA%20to%20achieve%20a%0A70%25%20success%20rate%20on%20standard%20LIBERO%20suites%2C%20enabling%20private%20instruction%20or%0Abackdoor%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Reconstruction%2520and%2520Extrapolation%2520for%2520%2524%25CF%2580_0%2524%2520using%2520Text%2520Latent%26entry.906535625%3DQuanyi%2520Li%26entry.1292438233%3D%2520%2520Vision-language-action%2520models%2520%2528VLAs%2529%2520often%2520achieve%2520high%2520performance%2520on%250Ademonstrated%2520tasks%2520but%2520struggle%2520significantly%2520when%2520required%2520to%2520extrapolate%252C%250Acombining%2520skills%2520learned%2520from%2520different%2520tasks%2520in%2520novel%2520ways.%2520For%2520instance%252C%2520VLAs%250Amight%2520successfully%2520put%2520the%2520cream%2520cheese%2520in%2520the%2520bowl%2520and%2520put%2520the%2520bowl%2520on%2520top%2520of%250Athe%2520cabinet%252C%2520yet%2520still%2520fail%2520to%2520put%2520the%2520cream%2520cheese%2520on%2520top%2520of%2520the%2520cabinet.%2520In%250Athis%2520work%252C%2520we%2520demonstrate%2520that%2520behaviors%2520from%2520distinct%2520tasks%2520can%2520be%2520effectively%250Arecombined%2520by%2520manipulating%2520the%2520VLA%2527s%2520internal%2520representations%2520at%2520inference%250Atime.%2520Concretely%252C%2520we%2520identify%2520the%2520text%2520latent%2520by%2520averaging%2520the%2520text%2520tokens%2527%250Ahidden%2520states%2520across%2520all%2520demonstrated%2520trajectories%2520for%2520a%2520specific%2520base%2520task.%250AFor%2520executing%2520an%2520extrapolated%2520task%252C%2520we%2520can%2520temporally%2520interpolate%2520the%2520text%250Alatent%2520of%2520the%2520two%2520base%2520tasks%2520and%2520add%2520it%2520back%2520to%2520the%2520text%2520hidden%2520states%252C%2520so%250Asub-behaviors%2520from%2520the%2520two%2520tasks%2520will%2520be%2520activated%2520sequentially.%2520We%2520evaluate%250Athis%2520approach%2520using%2520the%2520newly%2520created%2520libero-ood%2520benchmark%252C%2520featuring%252020%2520tasks%250Aextrapolated%2520from%2520standard%2520LIBERO%2520suites.%2520The%2520results%2520on%2520libero-ood%2520show%2520that%250Aall%2520SOTA%2520VLAs%2520achieve%2520%253C%252015%2525%2520success%2520rate%252C%2520while%2520%2524%255Cpi0%2524%2520with%2520text%2520latent%250Ainterpolation%2520reaches%2520an%252083%2525%2520success%2520rate.%2520Further%2520qualitative%2520analysis%2520reveals%250Aa%2520tendency%2520for%2520VLAs%2520to%2520exhibit%2520spatial%2520overfitting%252C%2520mapping%2520object%2520names%2520to%250Ademonstrated%2520locations%2520rather%2520than%2520achieving%2520genuine%2520object%2520and%2520goal%250Aunderstanding.%2520Additionally%252C%2520we%2520find%2520that%2520decoding%2520the%2520text%2520latent%2520yields%250Ahuman-unreadable%2520prompts%2520that%2520can%2520nevertheless%2520instruct%2520the%2520VLA%2520to%2520achieve%2520a%250A70%2525%2520success%2520rate%2520on%2520standard%2520LIBERO%2520suites%252C%2520enabling%2520private%2520instruction%2520or%250Abackdoor%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Reconstruction%20and%20Extrapolation%20for%20%24%CF%80_0%24%20using%20Text%20Latent&entry.906535625=Quanyi%20Li&entry.1292438233=%20%20Vision-language-action%20models%20%28VLAs%29%20often%20achieve%20high%20performance%20on%0Ademonstrated%20tasks%20but%20struggle%20significantly%20when%20required%20to%20extrapolate%2C%0Acombining%20skills%20learned%20from%20different%20tasks%20in%20novel%20ways.%20For%20instance%2C%20VLAs%0Amight%20successfully%20put%20the%20cream%20cheese%20in%20the%20bowl%20and%20put%20the%20bowl%20on%20top%20of%0Athe%20cabinet%2C%20yet%20still%20fail%20to%20put%20the%20cream%20cheese%20on%20top%20of%20the%20cabinet.%20In%0Athis%20work%2C%20we%20demonstrate%20that%20behaviors%20from%20distinct%20tasks%20can%20be%20effectively%0Arecombined%20by%20manipulating%20the%20VLA%27s%20internal%20representations%20at%20inference%0Atime.%20Concretely%2C%20we%20identify%20the%20text%20latent%20by%20averaging%20the%20text%20tokens%27%0Ahidden%20states%20across%20all%20demonstrated%20trajectories%20for%20a%20specific%20base%20task.%0AFor%20executing%20an%20extrapolated%20task%2C%20we%20can%20temporally%20interpolate%20the%20text%0Alatent%20of%20the%20two%20base%20tasks%20and%20add%20it%20back%20to%20the%20text%20hidden%20states%2C%20so%0Asub-behaviors%20from%20the%20two%20tasks%20will%20be%20activated%20sequentially.%20We%20evaluate%0Athis%20approach%20using%20the%20newly%20created%20libero-ood%20benchmark%2C%20featuring%2020%20tasks%0Aextrapolated%20from%20standard%20LIBERO%20suites.%20The%20results%20on%20libero-ood%20show%20that%0Aall%20SOTA%20VLAs%20achieve%20%3C%2015%25%20success%20rate%2C%20while%20%24%5Cpi0%24%20with%20text%20latent%0Ainterpolation%20reaches%20an%2083%25%20success%20rate.%20Further%20qualitative%20analysis%20reveals%0Aa%20tendency%20for%20VLAs%20to%20exhibit%20spatial%20overfitting%2C%20mapping%20object%20names%20to%0Ademonstrated%20locations%20rather%20than%20achieving%20genuine%20object%20and%20goal%0Aunderstanding.%20Additionally%2C%20we%20find%20that%20decoding%20the%20text%20latent%20yields%0Ahuman-unreadable%20prompts%20that%20can%20nevertheless%20instruct%20the%20VLA%20to%20achieve%20a%0A70%25%20success%20rate%20on%20standard%20LIBERO%20suites%2C%20enabling%20private%20instruction%20or%0Abackdoor%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03500v1&entry.124074799=Read"},
{"title": "Enhancing Target-unspecific Tasks through a Features Matrix", "author": "Fangming Cui and Yonggang Zhang and Xuan Wang and Xinmei Tian and Jun Yu", "abstract": "  Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2505.03414v1", "date": "2025-05-06", "relevancy": 2.6083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Target-unspecific%20Tasks%20through%20a%20Features%20Matrix&body=Title%3A%20Enhancing%20Target-unspecific%20Tasks%20through%20a%20Features%20Matrix%0AAuthor%3A%20Fangming%20Cui%20and%20Yonggang%20Zhang%20and%20Xuan%20Wang%20and%20Xinmei%20Tian%20and%20Jun%20Yu%0AAbstract%3A%20%20%20Recent%20developments%20in%20prompt%20learning%20of%20large%20vision-language%20models%20have%0Asignificantly%20improved%20performance%20in%20target-specific%20tasks.%20However%2C%20these%0Aprompt%20optimizing%20methods%20often%20struggle%20to%20tackle%20the%20target-unspecific%20or%0Ageneralizable%20tasks%20effectively.%20It%20may%20be%20attributed%20to%20the%20fact%20that%0Aoverfitting%20training%20causes%20the%20model%20to%20forget%20its%20general%20knowledge%20having%0Astrong%20promotion%20on%20target-unspecific%20tasks.%20To%20alleviate%20this%20issue%2C%20we%0Apropose%20a%20novel%20Features%20Matrix%20%28FM%29%20regularization%20approach%20designed%20to%0Aenhance%20these%20models%20on%20target-unspecific%20tasks.%20Our%20method%20extracts%20and%0Aleverages%20general%20knowledge%2C%20shaping%20a%20Features%20Matrix%20%28FM%29.%20Specifically%2C%20the%0AFM%20captures%20the%20semantics%20of%20diverse%20inputs%20from%20a%20deep%20and%20fine%20perspective%2C%0Apreserving%20essential%20general%20knowledge%2C%20which%20mitigates%20the%20risk%20of%0Aoverfitting.%20Representative%20evaluations%20demonstrate%20that%3A%201%29%20the%20FM%20is%0Acompatible%20with%20existing%20frameworks%20as%20a%20generic%20and%20flexible%20module%2C%20and%202%29%0Athe%20FM%20significantly%20showcases%20its%20effectiveness%20in%20enhancing%20target-unspecific%0Atasks%2C%20achieving%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Target-unspecific%2520Tasks%2520through%2520a%2520Features%2520Matrix%26entry.906535625%3DFangming%2520Cui%2520and%2520Yonggang%2520Zhang%2520and%2520Xuan%2520Wang%2520and%2520Xinmei%2520Tian%2520and%2520Jun%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520prompt%2520learning%2520of%2520large%2520vision-language%2520models%2520have%250Asignificantly%2520improved%2520performance%2520in%2520target-specific%2520tasks.%2520However%252C%2520these%250Aprompt%2520optimizing%2520methods%2520often%2520struggle%2520to%2520tackle%2520the%2520target-unspecific%2520or%250Ageneralizable%2520tasks%2520effectively.%2520It%2520may%2520be%2520attributed%2520to%2520the%2520fact%2520that%250Aoverfitting%2520training%2520causes%2520the%2520model%2520to%2520forget%2520its%2520general%2520knowledge%2520having%250Astrong%2520promotion%2520on%2520target-unspecific%2520tasks.%2520To%2520alleviate%2520this%2520issue%252C%2520we%250Apropose%2520a%2520novel%2520Features%2520Matrix%2520%2528FM%2529%2520regularization%2520approach%2520designed%2520to%250Aenhance%2520these%2520models%2520on%2520target-unspecific%2520tasks.%2520Our%2520method%2520extracts%2520and%250Aleverages%2520general%2520knowledge%252C%2520shaping%2520a%2520Features%2520Matrix%2520%2528FM%2529.%2520Specifically%252C%2520the%250AFM%2520captures%2520the%2520semantics%2520of%2520diverse%2520inputs%2520from%2520a%2520deep%2520and%2520fine%2520perspective%252C%250Apreserving%2520essential%2520general%2520knowledge%252C%2520which%2520mitigates%2520the%2520risk%2520of%250Aoverfitting.%2520Representative%2520evaluations%2520demonstrate%2520that%253A%25201%2529%2520the%2520FM%2520is%250Acompatible%2520with%2520existing%2520frameworks%2520as%2520a%2520generic%2520and%2520flexible%2520module%252C%2520and%25202%2529%250Athe%2520FM%2520significantly%2520showcases%2520its%2520effectiveness%2520in%2520enhancing%2520target-unspecific%250Atasks%252C%2520achieving%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Target-unspecific%20Tasks%20through%20a%20Features%20Matrix&entry.906535625=Fangming%20Cui%20and%20Yonggang%20Zhang%20and%20Xuan%20Wang%20and%20Xinmei%20Tian%20and%20Jun%20Yu&entry.1292438233=%20%20Recent%20developments%20in%20prompt%20learning%20of%20large%20vision-language%20models%20have%0Asignificantly%20improved%20performance%20in%20target-specific%20tasks.%20However%2C%20these%0Aprompt%20optimizing%20methods%20often%20struggle%20to%20tackle%20the%20target-unspecific%20or%0Ageneralizable%20tasks%20effectively.%20It%20may%20be%20attributed%20to%20the%20fact%20that%0Aoverfitting%20training%20causes%20the%20model%20to%20forget%20its%20general%20knowledge%20having%0Astrong%20promotion%20on%20target-unspecific%20tasks.%20To%20alleviate%20this%20issue%2C%20we%0Apropose%20a%20novel%20Features%20Matrix%20%28FM%29%20regularization%20approach%20designed%20to%0Aenhance%20these%20models%20on%20target-unspecific%20tasks.%20Our%20method%20extracts%20and%0Aleverages%20general%20knowledge%2C%20shaping%20a%20Features%20Matrix%20%28FM%29.%20Specifically%2C%20the%0AFM%20captures%20the%20semantics%20of%20diverse%20inputs%20from%20a%20deep%20and%20fine%20perspective%2C%0Apreserving%20essential%20general%20knowledge%2C%20which%20mitigates%20the%20risk%20of%0Aoverfitting.%20Representative%20evaluations%20demonstrate%20that%3A%201%29%20the%20FM%20is%0Acompatible%20with%20existing%20frameworks%20as%20a%20generic%20and%20flexible%20module%2C%20and%202%29%0Athe%20FM%20significantly%20showcases%20its%20effectiveness%20in%20enhancing%20target-unspecific%0Atasks%2C%20achieving%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03414v1&entry.124074799=Read"},
{"title": "DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer\n  Questions in Dynamic Scenes", "author": "Sergey Linok and Vadim Semenov and Anastasia Trunova and Oleg Bulichev and Dmitry Yudin", "abstract": "  The analysis of events in dynamic environments poses a fundamental challenge\nin the development of intelligent agents and robots capable of interacting with\nhumans. Current approaches predominantly utilize visual models. However, these\nmethods often capture information implicitly from images, lacking interpretable\nspatial-temporal object representations. To address this issue we introduce\nDyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates\ncompressed spatial-temporal structural observation representation with the\ncognitive capabilities of large language models. The purpose of this\nintegration is to enable advanced question answering based on a sequence of\ntextual scene graphs. Extended evaluations on the STAR and AGQA datasets\nindicate that DyGEnc outperforms existing visual methods by a large margin of\n15-25% in addressing queries regarding the history of human-to-object\ninteractions. Furthermore, the proposed method can be seamlessly extended to\nprocess raw input images utilizing foundational models for extracting explicit\ntextual scene graphs, as substantiated by the results of a robotic experiment\nconducted with a wheeled manipulator platform. We hope that these findings will\ncontribute to the implementation of robust and compressed graph-based robotic\nmemory for long-horizon reasoning. Code is available at\ngithub.com/linukc/DyGEnc.\n", "link": "http://arxiv.org/abs/2505.03581v1", "date": "2025-05-06", "relevancy": 2.5713, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyGEnc%3A%20Encoding%20a%20Sequence%20of%20Textual%20Scene%20Graphs%20to%20Reason%20and%20Answer%0A%20%20Questions%20in%20Dynamic%20Scenes&body=Title%3A%20DyGEnc%3A%20Encoding%20a%20Sequence%20of%20Textual%20Scene%20Graphs%20to%20Reason%20and%20Answer%0A%20%20Questions%20in%20Dynamic%20Scenes%0AAuthor%3A%20Sergey%20Linok%20and%20Vadim%20Semenov%20and%20Anastasia%20Trunova%20and%20Oleg%20Bulichev%20and%20Dmitry%20Yudin%0AAbstract%3A%20%20%20The%20analysis%20of%20events%20in%20dynamic%20environments%20poses%20a%20fundamental%20challenge%0Ain%20the%20development%20of%20intelligent%20agents%20and%20robots%20capable%20of%20interacting%20with%0Ahumans.%20Current%20approaches%20predominantly%20utilize%20visual%20models.%20However%2C%20these%0Amethods%20often%20capture%20information%20implicitly%20from%20images%2C%20lacking%20interpretable%0Aspatial-temporal%20object%20representations.%20To%20address%20this%20issue%20we%20introduce%0ADyGEnc%20-%20a%20novel%20method%20for%20Encoding%20a%20Dynamic%20Graph.%20This%20method%20integrates%0Acompressed%20spatial-temporal%20structural%20observation%20representation%20with%20the%0Acognitive%20capabilities%20of%20large%20language%20models.%20The%20purpose%20of%20this%0Aintegration%20is%20to%20enable%20advanced%20question%20answering%20based%20on%20a%20sequence%20of%0Atextual%20scene%20graphs.%20Extended%20evaluations%20on%20the%20STAR%20and%20AGQA%20datasets%0Aindicate%20that%20DyGEnc%20outperforms%20existing%20visual%20methods%20by%20a%20large%20margin%20of%0A15-25%25%20in%20addressing%20queries%20regarding%20the%20history%20of%20human-to-object%0Ainteractions.%20Furthermore%2C%20the%20proposed%20method%20can%20be%20seamlessly%20extended%20to%0Aprocess%20raw%20input%20images%20utilizing%20foundational%20models%20for%20extracting%20explicit%0Atextual%20scene%20graphs%2C%20as%20substantiated%20by%20the%20results%20of%20a%20robotic%20experiment%0Aconducted%20with%20a%20wheeled%20manipulator%20platform.%20We%20hope%20that%20these%20findings%20will%0Acontribute%20to%20the%20implementation%20of%20robust%20and%20compressed%20graph-based%20robotic%0Amemory%20for%20long-horizon%20reasoning.%20Code%20is%20available%20at%0Agithub.com/linukc/DyGEnc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyGEnc%253A%2520Encoding%2520a%2520Sequence%2520of%2520Textual%2520Scene%2520Graphs%2520to%2520Reason%2520and%2520Answer%250A%2520%2520Questions%2520in%2520Dynamic%2520Scenes%26entry.906535625%3DSergey%2520Linok%2520and%2520Vadim%2520Semenov%2520and%2520Anastasia%2520Trunova%2520and%2520Oleg%2520Bulichev%2520and%2520Dmitry%2520Yudin%26entry.1292438233%3D%2520%2520The%2520analysis%2520of%2520events%2520in%2520dynamic%2520environments%2520poses%2520a%2520fundamental%2520challenge%250Ain%2520the%2520development%2520of%2520intelligent%2520agents%2520and%2520robots%2520capable%2520of%2520interacting%2520with%250Ahumans.%2520Current%2520approaches%2520predominantly%2520utilize%2520visual%2520models.%2520However%252C%2520these%250Amethods%2520often%2520capture%2520information%2520implicitly%2520from%2520images%252C%2520lacking%2520interpretable%250Aspatial-temporal%2520object%2520representations.%2520To%2520address%2520this%2520issue%2520we%2520introduce%250ADyGEnc%2520-%2520a%2520novel%2520method%2520for%2520Encoding%2520a%2520Dynamic%2520Graph.%2520This%2520method%2520integrates%250Acompressed%2520spatial-temporal%2520structural%2520observation%2520representation%2520with%2520the%250Acognitive%2520capabilities%2520of%2520large%2520language%2520models.%2520The%2520purpose%2520of%2520this%250Aintegration%2520is%2520to%2520enable%2520advanced%2520question%2520answering%2520based%2520on%2520a%2520sequence%2520of%250Atextual%2520scene%2520graphs.%2520Extended%2520evaluations%2520on%2520the%2520STAR%2520and%2520AGQA%2520datasets%250Aindicate%2520that%2520DyGEnc%2520outperforms%2520existing%2520visual%2520methods%2520by%2520a%2520large%2520margin%2520of%250A15-25%2525%2520in%2520addressing%2520queries%2520regarding%2520the%2520history%2520of%2520human-to-object%250Ainteractions.%2520Furthermore%252C%2520the%2520proposed%2520method%2520can%2520be%2520seamlessly%2520extended%2520to%250Aprocess%2520raw%2520input%2520images%2520utilizing%2520foundational%2520models%2520for%2520extracting%2520explicit%250Atextual%2520scene%2520graphs%252C%2520as%2520substantiated%2520by%2520the%2520results%2520of%2520a%2520robotic%2520experiment%250Aconducted%2520with%2520a%2520wheeled%2520manipulator%2520platform.%2520We%2520hope%2520that%2520these%2520findings%2520will%250Acontribute%2520to%2520the%2520implementation%2520of%2520robust%2520and%2520compressed%2520graph-based%2520robotic%250Amemory%2520for%2520long-horizon%2520reasoning.%2520Code%2520is%2520available%2520at%250Agithub.com/linukc/DyGEnc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyGEnc%3A%20Encoding%20a%20Sequence%20of%20Textual%20Scene%20Graphs%20to%20Reason%20and%20Answer%0A%20%20Questions%20in%20Dynamic%20Scenes&entry.906535625=Sergey%20Linok%20and%20Vadim%20Semenov%20and%20Anastasia%20Trunova%20and%20Oleg%20Bulichev%20and%20Dmitry%20Yudin&entry.1292438233=%20%20The%20analysis%20of%20events%20in%20dynamic%20environments%20poses%20a%20fundamental%20challenge%0Ain%20the%20development%20of%20intelligent%20agents%20and%20robots%20capable%20of%20interacting%20with%0Ahumans.%20Current%20approaches%20predominantly%20utilize%20visual%20models.%20However%2C%20these%0Amethods%20often%20capture%20information%20implicitly%20from%20images%2C%20lacking%20interpretable%0Aspatial-temporal%20object%20representations.%20To%20address%20this%20issue%20we%20introduce%0ADyGEnc%20-%20a%20novel%20method%20for%20Encoding%20a%20Dynamic%20Graph.%20This%20method%20integrates%0Acompressed%20spatial-temporal%20structural%20observation%20representation%20with%20the%0Acognitive%20capabilities%20of%20large%20language%20models.%20The%20purpose%20of%20this%0Aintegration%20is%20to%20enable%20advanced%20question%20answering%20based%20on%20a%20sequence%20of%0Atextual%20scene%20graphs.%20Extended%20evaluations%20on%20the%20STAR%20and%20AGQA%20datasets%0Aindicate%20that%20DyGEnc%20outperforms%20existing%20visual%20methods%20by%20a%20large%20margin%20of%0A15-25%25%20in%20addressing%20queries%20regarding%20the%20history%20of%20human-to-object%0Ainteractions.%20Furthermore%2C%20the%20proposed%20method%20can%20be%20seamlessly%20extended%20to%0Aprocess%20raw%20input%20images%20utilizing%20foundational%20models%20for%20extracting%20explicit%0Atextual%20scene%20graphs%2C%20as%20substantiated%20by%20the%20results%20of%20a%20robotic%20experiment%0Aconducted%20with%20a%20wheeled%20manipulator%20platform.%20We%20hope%20that%20these%20findings%20will%0Acontribute%20to%20the%20implementation%20of%20robust%20and%20compressed%20graph-based%20robotic%0Amemory%20for%20long-horizon%20reasoning.%20Code%20is%20available%20at%0Agithub.com/linukc/DyGEnc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03581v1&entry.124074799=Read"},
{"title": "Attention-aggregated Attack for Boosting the Transferability of Facial\n  Adversarial Examples", "author": "Jian-Wei Li and Wen-Ze Shao", "abstract": "  Adversarial examples have revealed the vulnerability of deep learning models\nand raised serious concerns about information security. The transfer-based\nattack is a hot topic in black-box attacks that are practical to real-world\nscenarios where the training datasets, parameters, and structure of the target\nmodel are unknown to the attacker. However, few methods consider the\nparticularity of class-specific deep models for fine-grained vision tasks, such\nas face recognition (FR), giving rise to unsatisfactory attacking performance.\nIn this work, we first investigate what in a face exactly contributes to the\nembedding learning of FR models and find that both decisive and auxiliary\nfacial features are specific to each FR model, which is quite different from\nthe biological mechanism of human visual system. Accordingly we then propose a\nnovel attack method named Attention-aggregated Attack (AAA) to enhance the\ntransferability of adversarial examples against FR, which is inspired by the\nattention divergence and aims to destroy the facial features that are critical\nfor the decision-making of other FR models by imitating their attentions on the\nclean face images. Extensive experiments conducted on various FR models\nvalidate the superiority and robust effectiveness of the proposed method over\nexisting methods.\n", "link": "http://arxiv.org/abs/2505.03383v1", "date": "2025-05-06", "relevancy": 2.569, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5243}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5229}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-aggregated%20Attack%20for%20Boosting%20the%20Transferability%20of%20Facial%0A%20%20Adversarial%20Examples&body=Title%3A%20Attention-aggregated%20Attack%20for%20Boosting%20the%20Transferability%20of%20Facial%0A%20%20Adversarial%20Examples%0AAuthor%3A%20Jian-Wei%20Li%20and%20Wen-Ze%20Shao%0AAbstract%3A%20%20%20Adversarial%20examples%20have%20revealed%20the%20vulnerability%20of%20deep%20learning%20models%0Aand%20raised%20serious%20concerns%20about%20information%20security.%20The%20transfer-based%0Aattack%20is%20a%20hot%20topic%20in%20black-box%20attacks%20that%20are%20practical%20to%20real-world%0Ascenarios%20where%20the%20training%20datasets%2C%20parameters%2C%20and%20structure%20of%20the%20target%0Amodel%20are%20unknown%20to%20the%20attacker.%20However%2C%20few%20methods%20consider%20the%0Aparticularity%20of%20class-specific%20deep%20models%20for%20fine-grained%20vision%20tasks%2C%20such%0Aas%20face%20recognition%20%28FR%29%2C%20giving%20rise%20to%20unsatisfactory%20attacking%20performance.%0AIn%20this%20work%2C%20we%20first%20investigate%20what%20in%20a%20face%20exactly%20contributes%20to%20the%0Aembedding%20learning%20of%20FR%20models%20and%20find%20that%20both%20decisive%20and%20auxiliary%0Afacial%20features%20are%20specific%20to%20each%20FR%20model%2C%20which%20is%20quite%20different%20from%0Athe%20biological%20mechanism%20of%20human%20visual%20system.%20Accordingly%20we%20then%20propose%20a%0Anovel%20attack%20method%20named%20Attention-aggregated%20Attack%20%28AAA%29%20to%20enhance%20the%0Atransferability%20of%20adversarial%20examples%20against%20FR%2C%20which%20is%20inspired%20by%20the%0Aattention%20divergence%20and%20aims%20to%20destroy%20the%20facial%20features%20that%20are%20critical%0Afor%20the%20decision-making%20of%20other%20FR%20models%20by%20imitating%20their%20attentions%20on%20the%0Aclean%20face%20images.%20Extensive%20experiments%20conducted%20on%20various%20FR%20models%0Avalidate%20the%20superiority%20and%20robust%20effectiveness%20of%20the%20proposed%20method%20over%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-aggregated%2520Attack%2520for%2520Boosting%2520the%2520Transferability%2520of%2520Facial%250A%2520%2520Adversarial%2520Examples%26entry.906535625%3DJian-Wei%2520Li%2520and%2520Wen-Ze%2520Shao%26entry.1292438233%3D%2520%2520Adversarial%2520examples%2520have%2520revealed%2520the%2520vulnerability%2520of%2520deep%2520learning%2520models%250Aand%2520raised%2520serious%2520concerns%2520about%2520information%2520security.%2520The%2520transfer-based%250Aattack%2520is%2520a%2520hot%2520topic%2520in%2520black-box%2520attacks%2520that%2520are%2520practical%2520to%2520real-world%250Ascenarios%2520where%2520the%2520training%2520datasets%252C%2520parameters%252C%2520and%2520structure%2520of%2520the%2520target%250Amodel%2520are%2520unknown%2520to%2520the%2520attacker.%2520However%252C%2520few%2520methods%2520consider%2520the%250Aparticularity%2520of%2520class-specific%2520deep%2520models%2520for%2520fine-grained%2520vision%2520tasks%252C%2520such%250Aas%2520face%2520recognition%2520%2528FR%2529%252C%2520giving%2520rise%2520to%2520unsatisfactory%2520attacking%2520performance.%250AIn%2520this%2520work%252C%2520we%2520first%2520investigate%2520what%2520in%2520a%2520face%2520exactly%2520contributes%2520to%2520the%250Aembedding%2520learning%2520of%2520FR%2520models%2520and%2520find%2520that%2520both%2520decisive%2520and%2520auxiliary%250Afacial%2520features%2520are%2520specific%2520to%2520each%2520FR%2520model%252C%2520which%2520is%2520quite%2520different%2520from%250Athe%2520biological%2520mechanism%2520of%2520human%2520visual%2520system.%2520Accordingly%2520we%2520then%2520propose%2520a%250Anovel%2520attack%2520method%2520named%2520Attention-aggregated%2520Attack%2520%2528AAA%2529%2520to%2520enhance%2520the%250Atransferability%2520of%2520adversarial%2520examples%2520against%2520FR%252C%2520which%2520is%2520inspired%2520by%2520the%250Aattention%2520divergence%2520and%2520aims%2520to%2520destroy%2520the%2520facial%2520features%2520that%2520are%2520critical%250Afor%2520the%2520decision-making%2520of%2520other%2520FR%2520models%2520by%2520imitating%2520their%2520attentions%2520on%2520the%250Aclean%2520face%2520images.%2520Extensive%2520experiments%2520conducted%2520on%2520various%2520FR%2520models%250Avalidate%2520the%2520superiority%2520and%2520robust%2520effectiveness%2520of%2520the%2520proposed%2520method%2520over%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-aggregated%20Attack%20for%20Boosting%20the%20Transferability%20of%20Facial%0A%20%20Adversarial%20Examples&entry.906535625=Jian-Wei%20Li%20and%20Wen-Ze%20Shao&entry.1292438233=%20%20Adversarial%20examples%20have%20revealed%20the%20vulnerability%20of%20deep%20learning%20models%0Aand%20raised%20serious%20concerns%20about%20information%20security.%20The%20transfer-based%0Aattack%20is%20a%20hot%20topic%20in%20black-box%20attacks%20that%20are%20practical%20to%20real-world%0Ascenarios%20where%20the%20training%20datasets%2C%20parameters%2C%20and%20structure%20of%20the%20target%0Amodel%20are%20unknown%20to%20the%20attacker.%20However%2C%20few%20methods%20consider%20the%0Aparticularity%20of%20class-specific%20deep%20models%20for%20fine-grained%20vision%20tasks%2C%20such%0Aas%20face%20recognition%20%28FR%29%2C%20giving%20rise%20to%20unsatisfactory%20attacking%20performance.%0AIn%20this%20work%2C%20we%20first%20investigate%20what%20in%20a%20face%20exactly%20contributes%20to%20the%0Aembedding%20learning%20of%20FR%20models%20and%20find%20that%20both%20decisive%20and%20auxiliary%0Afacial%20features%20are%20specific%20to%20each%20FR%20model%2C%20which%20is%20quite%20different%20from%0Athe%20biological%20mechanism%20of%20human%20visual%20system.%20Accordingly%20we%20then%20propose%20a%0Anovel%20attack%20method%20named%20Attention-aggregated%20Attack%20%28AAA%29%20to%20enhance%20the%0Atransferability%20of%20adversarial%20examples%20against%20FR%2C%20which%20is%20inspired%20by%20the%0Aattention%20divergence%20and%20aims%20to%20destroy%20the%20facial%20features%20that%20are%20critical%0Afor%20the%20decision-making%20of%20other%20FR%20models%20by%20imitating%20their%20attentions%20on%20the%0Aclean%20face%20images.%20Extensive%20experiments%20conducted%20on%20various%20FR%20models%0Avalidate%20the%20superiority%20and%20robust%20effectiveness%20of%20the%20proposed%20method%20over%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03383v1&entry.124074799=Read"},
{"title": "A Cognitive Paradigm Approach to Probe the Perception-Reasoning\n  Interface in VLMs", "author": "Mohit Vaishnav and Tanel Tammet", "abstract": "  A fundamental challenge in artificial intelligence involves understanding the\ncognitive mechanisms underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images or\nrequiring fine-grained compositional understanding? Drawing inspiration from\ncognitive science, this paper introduces a structured evaluation framework\nusing diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to\ndissect the perception-reasoning interface in VLMs. We propose three distinct\nevaluation paradigms, mirroring human problem-solving strategies: Direct Visual\nRule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule\nextraction and application), and Componential Analysis (CA; analytical\ndecomposition via task-agnostic textual descriptions). These paradigms\nsystematically vary cognitive load and probe processing stages. Notably, CA\nenables multi-image reasoning evaluation even for single-image architectures\nand isolates reasoning from perception by operating on textual descriptions.\nApplying this framework, we demonstrate that CA, leveraging powerful language\nmodels for reasoning over rich, independently generated descriptions, achieves\nnew state-of-the-art (SOTA) performance on challenging benchmarks including\nBongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm\nreasoning improves significantly when perceptual challenges are mitigated,\nrevealing a critical perception bottleneck. Our framework provides a valuable\ndiagnostic tool and suggests that decoupling perception (via rich,\ntask-agnostic description) from reasoning is a promising direction for robust\nand general visual intelligence.\n", "link": "http://arxiv.org/abs/2501.13620v5", "date": "2025-05-06", "relevancy": 2.5455, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Cognitive%20Paradigm%20Approach%20to%20Probe%20the%20Perception-Reasoning%0A%20%20Interface%20in%20VLMs&body=Title%3A%20A%20Cognitive%20Paradigm%20Approach%20to%20Probe%20the%20Perception-Reasoning%0A%20%20Interface%20in%20VLMs%0AAuthor%3A%20Mohit%20Vaishnav%20and%20Tanel%20Tammet%0AAbstract%3A%20%20%20A%20fundamental%20challenge%20in%20artificial%20intelligence%20involves%20understanding%20the%0Acognitive%20mechanisms%20underlying%20visual%20reasoning%20in%20sophisticated%20models%20like%0AVision-Language%20Models%20%28VLMs%29.%20How%20do%20these%20models%20integrate%20visual%20perception%0Awith%20abstract%20thought%2C%20especially%20when%20reasoning%20across%20multiple%20images%20or%0Arequiring%20fine-grained%20compositional%20understanding%3F%20Drawing%20inspiration%20from%0Acognitive%20science%2C%20this%20paper%20introduces%20a%20structured%20evaluation%20framework%0Ausing%20diverse%20visual%20reasoning%20tasks-Bongard%20Problems%20%28BPs%29%20and%20Winoground-to%0Adissect%20the%20perception-reasoning%20interface%20in%20VLMs.%20We%20propose%20three%20distinct%0Aevaluation%20paradigms%2C%20mirroring%20human%20problem-solving%20strategies%3A%20Direct%20Visual%0ARule%20Learning%20%28DVRL%3B%20holistic%20processing%29%2C%20Deductive%20Rule%20Learning%20%28DRL%3B%20rule%0Aextraction%20and%20application%29%2C%20and%20Componential%20Analysis%20%28CA%3B%20analytical%0Adecomposition%20via%20task-agnostic%20textual%20descriptions%29.%20These%20paradigms%0Asystematically%20vary%20cognitive%20load%20and%20probe%20processing%20stages.%20Notably%2C%20CA%0Aenables%20multi-image%20reasoning%20evaluation%20even%20for%20single-image%20architectures%0Aand%20isolates%20reasoning%20from%20perception%20by%20operating%20on%20textual%20descriptions.%0AApplying%20this%20framework%2C%20we%20demonstrate%20that%20CA%2C%20leveraging%20powerful%20language%0Amodels%20for%20reasoning%20over%20rich%2C%20independently%20generated%20descriptions%2C%20achieves%0Anew%20state-of-the-art%20%28SOTA%29%20performance%20on%20challenging%20benchmarks%20including%0ABongard-OpenWorld%2C%20Bongard-HOI%2C%20and%20Winoground.%20Ablation%20studies%20confirm%0Areasoning%20improves%20significantly%20when%20perceptual%20challenges%20are%20mitigated%2C%0Arevealing%20a%20critical%20perception%20bottleneck.%20Our%20framework%20provides%20a%20valuable%0Adiagnostic%20tool%20and%20suggests%20that%20decoupling%20perception%20%28via%20rich%2C%0Atask-agnostic%20description%29%20from%20reasoning%20is%20a%20promising%20direction%20for%20robust%0Aand%20general%20visual%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13620v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Cognitive%2520Paradigm%2520Approach%2520to%2520Probe%2520the%2520Perception-Reasoning%250A%2520%2520Interface%2520in%2520VLMs%26entry.906535625%3DMohit%2520Vaishnav%2520and%2520Tanel%2520Tammet%26entry.1292438233%3D%2520%2520A%2520fundamental%2520challenge%2520in%2520artificial%2520intelligence%2520involves%2520understanding%2520the%250Acognitive%2520mechanisms%2520underlying%2520visual%2520reasoning%2520in%2520sophisticated%2520models%2520like%250AVision-Language%2520Models%2520%2528VLMs%2529.%2520How%2520do%2520these%2520models%2520integrate%2520visual%2520perception%250Awith%2520abstract%2520thought%252C%2520especially%2520when%2520reasoning%2520across%2520multiple%2520images%2520or%250Arequiring%2520fine-grained%2520compositional%2520understanding%253F%2520Drawing%2520inspiration%2520from%250Acognitive%2520science%252C%2520this%2520paper%2520introduces%2520a%2520structured%2520evaluation%2520framework%250Ausing%2520diverse%2520visual%2520reasoning%2520tasks-Bongard%2520Problems%2520%2528BPs%2529%2520and%2520Winoground-to%250Adissect%2520the%2520perception-reasoning%2520interface%2520in%2520VLMs.%2520We%2520propose%2520three%2520distinct%250Aevaluation%2520paradigms%252C%2520mirroring%2520human%2520problem-solving%2520strategies%253A%2520Direct%2520Visual%250ARule%2520Learning%2520%2528DVRL%253B%2520holistic%2520processing%2529%252C%2520Deductive%2520Rule%2520Learning%2520%2528DRL%253B%2520rule%250Aextraction%2520and%2520application%2529%252C%2520and%2520Componential%2520Analysis%2520%2528CA%253B%2520analytical%250Adecomposition%2520via%2520task-agnostic%2520textual%2520descriptions%2529.%2520These%2520paradigms%250Asystematically%2520vary%2520cognitive%2520load%2520and%2520probe%2520processing%2520stages.%2520Notably%252C%2520CA%250Aenables%2520multi-image%2520reasoning%2520evaluation%2520even%2520for%2520single-image%2520architectures%250Aand%2520isolates%2520reasoning%2520from%2520perception%2520by%2520operating%2520on%2520textual%2520descriptions.%250AApplying%2520this%2520framework%252C%2520we%2520demonstrate%2520that%2520CA%252C%2520leveraging%2520powerful%2520language%250Amodels%2520for%2520reasoning%2520over%2520rich%252C%2520independently%2520generated%2520descriptions%252C%2520achieves%250Anew%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520challenging%2520benchmarks%2520including%250ABongard-OpenWorld%252C%2520Bongard-HOI%252C%2520and%2520Winoground.%2520Ablation%2520studies%2520confirm%250Areasoning%2520improves%2520significantly%2520when%2520perceptual%2520challenges%2520are%2520mitigated%252C%250Arevealing%2520a%2520critical%2520perception%2520bottleneck.%2520Our%2520framework%2520provides%2520a%2520valuable%250Adiagnostic%2520tool%2520and%2520suggests%2520that%2520decoupling%2520perception%2520%2528via%2520rich%252C%250Atask-agnostic%2520description%2529%2520from%2520reasoning%2520is%2520a%2520promising%2520direction%2520for%2520robust%250Aand%2520general%2520visual%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13620v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cognitive%20Paradigm%20Approach%20to%20Probe%20the%20Perception-Reasoning%0A%20%20Interface%20in%20VLMs&entry.906535625=Mohit%20Vaishnav%20and%20Tanel%20Tammet&entry.1292438233=%20%20A%20fundamental%20challenge%20in%20artificial%20intelligence%20involves%20understanding%20the%0Acognitive%20mechanisms%20underlying%20visual%20reasoning%20in%20sophisticated%20models%20like%0AVision-Language%20Models%20%28VLMs%29.%20How%20do%20these%20models%20integrate%20visual%20perception%0Awith%20abstract%20thought%2C%20especially%20when%20reasoning%20across%20multiple%20images%20or%0Arequiring%20fine-grained%20compositional%20understanding%3F%20Drawing%20inspiration%20from%0Acognitive%20science%2C%20this%20paper%20introduces%20a%20structured%20evaluation%20framework%0Ausing%20diverse%20visual%20reasoning%20tasks-Bongard%20Problems%20%28BPs%29%20and%20Winoground-to%0Adissect%20the%20perception-reasoning%20interface%20in%20VLMs.%20We%20propose%20three%20distinct%0Aevaluation%20paradigms%2C%20mirroring%20human%20problem-solving%20strategies%3A%20Direct%20Visual%0ARule%20Learning%20%28DVRL%3B%20holistic%20processing%29%2C%20Deductive%20Rule%20Learning%20%28DRL%3B%20rule%0Aextraction%20and%20application%29%2C%20and%20Componential%20Analysis%20%28CA%3B%20analytical%0Adecomposition%20via%20task-agnostic%20textual%20descriptions%29.%20These%20paradigms%0Asystematically%20vary%20cognitive%20load%20and%20probe%20processing%20stages.%20Notably%2C%20CA%0Aenables%20multi-image%20reasoning%20evaluation%20even%20for%20single-image%20architectures%0Aand%20isolates%20reasoning%20from%20perception%20by%20operating%20on%20textual%20descriptions.%0AApplying%20this%20framework%2C%20we%20demonstrate%20that%20CA%2C%20leveraging%20powerful%20language%0Amodels%20for%20reasoning%20over%20rich%2C%20independently%20generated%20descriptions%2C%20achieves%0Anew%20state-of-the-art%20%28SOTA%29%20performance%20on%20challenging%20benchmarks%20including%0ABongard-OpenWorld%2C%20Bongard-HOI%2C%20and%20Winoground.%20Ablation%20studies%20confirm%0Areasoning%20improves%20significantly%20when%20perceptual%20challenges%20are%20mitigated%2C%0Arevealing%20a%20critical%20perception%20bottleneck.%20Our%20framework%20provides%20a%20valuable%0Adiagnostic%20tool%20and%20suggests%20that%20decoupling%20perception%20%28via%20rich%2C%0Atask-agnostic%20description%29%20from%20reasoning%20is%20a%20promising%20direction%20for%20robust%0Aand%20general%20visual%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13620v5&entry.124074799=Read"},
{"title": "Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation Using\n  Vision Language Models", "author": "Bangguo Yu and Qihao Yuan and Kailai Li and Hamidreza Kasaei and Ming Cao", "abstract": "  Visual target navigation is a critical capability for autonomous robots\noperating in unknown environments, particularly in human-robot interaction\nscenarios. While classical and learning-based methods have shown promise, most\nexisting approaches lack common-sense reasoning and are typically designed for\nsingle-robot settings, leading to reduced efficiency and robustness in complex\nenvironments. To address these limitations, we introduce Co-NavGPT, a novel\nframework that integrates a Vision Language Model (VLM) as a global planner to\nenable common-sense multi-robot visual target navigation. Co-NavGPT aggregates\nsub-maps from multiple robots with diverse viewpoints into a unified global\nmap, encoding robot states and frontier regions. The VLM uses this information\nto assign frontiers across the robots, facilitating coordinated and efficient\nexploration. Experiments on the Habitat-Matterport 3D (HM3D) demonstrate that\nCo-NavGPT outperforms existing baselines in terms of success rate and\nnavigation efficiency, without requiring task-specific training. Ablation\nstudies further confirm the importance of semantic priors from the VLM. We also\nvalidate the framework in real-world scenarios using quadrupedal robots.\nSupplementary video and code are available at:\nhttps://sites.google.com/view/co-navgpt2.\n", "link": "http://arxiv.org/abs/2310.07937v3", "date": "2025-05-06", "relevancy": 2.5418, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6714}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6378}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-NavGPT%3A%20Multi-Robot%20Cooperative%20Visual%20Semantic%20Navigation%20Using%0A%20%20Vision%20Language%20Models&body=Title%3A%20Co-NavGPT%3A%20Multi-Robot%20Cooperative%20Visual%20Semantic%20Navigation%20Using%0A%20%20Vision%20Language%20Models%0AAuthor%3A%20Bangguo%20Yu%20and%20Qihao%20Yuan%20and%20Kailai%20Li%20and%20Hamidreza%20Kasaei%20and%20Ming%20Cao%0AAbstract%3A%20%20%20Visual%20target%20navigation%20is%20a%20critical%20capability%20for%20autonomous%20robots%0Aoperating%20in%20unknown%20environments%2C%20particularly%20in%20human-robot%20interaction%0Ascenarios.%20While%20classical%20and%20learning-based%20methods%20have%20shown%20promise%2C%20most%0Aexisting%20approaches%20lack%20common-sense%20reasoning%20and%20are%20typically%20designed%20for%0Asingle-robot%20settings%2C%20leading%20to%20reduced%20efficiency%20and%20robustness%20in%20complex%0Aenvironments.%20To%20address%20these%20limitations%2C%20we%20introduce%20Co-NavGPT%2C%20a%20novel%0Aframework%20that%20integrates%20a%20Vision%20Language%20Model%20%28VLM%29%20as%20a%20global%20planner%20to%0Aenable%20common-sense%20multi-robot%20visual%20target%20navigation.%20Co-NavGPT%20aggregates%0Asub-maps%20from%20multiple%20robots%20with%20diverse%20viewpoints%20into%20a%20unified%20global%0Amap%2C%20encoding%20robot%20states%20and%20frontier%20regions.%20The%20VLM%20uses%20this%20information%0Ato%20assign%20frontiers%20across%20the%20robots%2C%20facilitating%20coordinated%20and%20efficient%0Aexploration.%20Experiments%20on%20the%20Habitat-Matterport%203D%20%28HM3D%29%20demonstrate%20that%0ACo-NavGPT%20outperforms%20existing%20baselines%20in%20terms%20of%20success%20rate%20and%0Anavigation%20efficiency%2C%20without%20requiring%20task-specific%20training.%20Ablation%0Astudies%20further%20confirm%20the%20importance%20of%20semantic%20priors%20from%20the%20VLM.%20We%20also%0Avalidate%20the%20framework%20in%20real-world%20scenarios%20using%20quadrupedal%20robots.%0ASupplementary%20video%20and%20code%20are%20available%20at%3A%0Ahttps%3A//sites.google.com/view/co-navgpt2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07937v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-NavGPT%253A%2520Multi-Robot%2520Cooperative%2520Visual%2520Semantic%2520Navigation%2520Using%250A%2520%2520Vision%2520Language%2520Models%26entry.906535625%3DBangguo%2520Yu%2520and%2520Qihao%2520Yuan%2520and%2520Kailai%2520Li%2520and%2520Hamidreza%2520Kasaei%2520and%2520Ming%2520Cao%26entry.1292438233%3D%2520%2520Visual%2520target%2520navigation%2520is%2520a%2520critical%2520capability%2520for%2520autonomous%2520robots%250Aoperating%2520in%2520unknown%2520environments%252C%2520particularly%2520in%2520human-robot%2520interaction%250Ascenarios.%2520While%2520classical%2520and%2520learning-based%2520methods%2520have%2520shown%2520promise%252C%2520most%250Aexisting%2520approaches%2520lack%2520common-sense%2520reasoning%2520and%2520are%2520typically%2520designed%2520for%250Asingle-robot%2520settings%252C%2520leading%2520to%2520reduced%2520efficiency%2520and%2520robustness%2520in%2520complex%250Aenvironments.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Co-NavGPT%252C%2520a%2520novel%250Aframework%2520that%2520integrates%2520a%2520Vision%2520Language%2520Model%2520%2528VLM%2529%2520as%2520a%2520global%2520planner%2520to%250Aenable%2520common-sense%2520multi-robot%2520visual%2520target%2520navigation.%2520Co-NavGPT%2520aggregates%250Asub-maps%2520from%2520multiple%2520robots%2520with%2520diverse%2520viewpoints%2520into%2520a%2520unified%2520global%250Amap%252C%2520encoding%2520robot%2520states%2520and%2520frontier%2520regions.%2520The%2520VLM%2520uses%2520this%2520information%250Ato%2520assign%2520frontiers%2520across%2520the%2520robots%252C%2520facilitating%2520coordinated%2520and%2520efficient%250Aexploration.%2520Experiments%2520on%2520the%2520Habitat-Matterport%25203D%2520%2528HM3D%2529%2520demonstrate%2520that%250ACo-NavGPT%2520outperforms%2520existing%2520baselines%2520in%2520terms%2520of%2520success%2520rate%2520and%250Anavigation%2520efficiency%252C%2520without%2520requiring%2520task-specific%2520training.%2520Ablation%250Astudies%2520further%2520confirm%2520the%2520importance%2520of%2520semantic%2520priors%2520from%2520the%2520VLM.%2520We%2520also%250Avalidate%2520the%2520framework%2520in%2520real-world%2520scenarios%2520using%2520quadrupedal%2520robots.%250ASupplementary%2520video%2520and%2520code%2520are%2520available%2520at%253A%250Ahttps%253A//sites.google.com/view/co-navgpt2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07937v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-NavGPT%3A%20Multi-Robot%20Cooperative%20Visual%20Semantic%20Navigation%20Using%0A%20%20Vision%20Language%20Models&entry.906535625=Bangguo%20Yu%20and%20Qihao%20Yuan%20and%20Kailai%20Li%20and%20Hamidreza%20Kasaei%20and%20Ming%20Cao&entry.1292438233=%20%20Visual%20target%20navigation%20is%20a%20critical%20capability%20for%20autonomous%20robots%0Aoperating%20in%20unknown%20environments%2C%20particularly%20in%20human-robot%20interaction%0Ascenarios.%20While%20classical%20and%20learning-based%20methods%20have%20shown%20promise%2C%20most%0Aexisting%20approaches%20lack%20common-sense%20reasoning%20and%20are%20typically%20designed%20for%0Asingle-robot%20settings%2C%20leading%20to%20reduced%20efficiency%20and%20robustness%20in%20complex%0Aenvironments.%20To%20address%20these%20limitations%2C%20we%20introduce%20Co-NavGPT%2C%20a%20novel%0Aframework%20that%20integrates%20a%20Vision%20Language%20Model%20%28VLM%29%20as%20a%20global%20planner%20to%0Aenable%20common-sense%20multi-robot%20visual%20target%20navigation.%20Co-NavGPT%20aggregates%0Asub-maps%20from%20multiple%20robots%20with%20diverse%20viewpoints%20into%20a%20unified%20global%0Amap%2C%20encoding%20robot%20states%20and%20frontier%20regions.%20The%20VLM%20uses%20this%20information%0Ato%20assign%20frontiers%20across%20the%20robots%2C%20facilitating%20coordinated%20and%20efficient%0Aexploration.%20Experiments%20on%20the%20Habitat-Matterport%203D%20%28HM3D%29%20demonstrate%20that%0ACo-NavGPT%20outperforms%20existing%20baselines%20in%20terms%20of%20success%20rate%20and%0Anavigation%20efficiency%2C%20without%20requiring%20task-specific%20training.%20Ablation%0Astudies%20further%20confirm%20the%20importance%20of%20semantic%20priors%20from%20the%20VLM.%20We%20also%0Avalidate%20the%20framework%20in%20real-world%20scenarios%20using%20quadrupedal%20robots.%0ASupplementary%20video%20and%20code%20are%20available%20at%3A%0Ahttps%3A//sites.google.com/view/co-navgpt2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07937v3&entry.124074799=Read"},
{"title": "DISARM++: Beyond scanner-free harmonization", "author": "Luca Caldera and Lara Cavinato and Alessio Cirone and Isabella Cama and Sara Garbarino and Raffaele Lodi and Fabrizio Tagliavini and Anna Nigri and Silvia De Francesco and Andrea Cappozzo and Michele Piana and Francesca Ieva", "abstract": "  Harmonization of T1-weighted MR images across different scanners is crucial\nfor ensuring consistency in neuroimaging studies. This study introduces a novel\napproach to direct image harmonization, moving beyond feature standardization\nto ensure that extracted features remain inherently reliable for downstream\nanalysis. Our method enables image transfer in two ways: (1) mapping images to\na scanner-free space for uniform appearance across all scanners, and (2)\ntransforming images into the domain of a specific scanner used in model\ntraining, embedding its unique characteristics. Our approach presents strong\ngeneralization capability, even for unseen scanners not included in the\ntraining phase. We validated our method using MR images from diverse cohorts,\nincluding healthy controls, traveling subjects, and individuals with\nAlzheimer's disease (AD). The model's effectiveness is tested in multiple\napplications, such as brain age prediction (R2 = 0.60 \\pm 0.05), biomarker\nextraction, AD classification (Test Accuracy = 0.86 \\pm 0.03), and diagnosis\nprediction (AUC = 0.95). In all cases, our harmonization technique outperforms\nstate-of-the-art methods, showing improvements in both reliability and\npredictive accuracy. Moreover, our approach eliminates the need for extensive\npreprocessing steps, such as skull-stripping, which can introduce errors by\nmisclassifying brain and non-brain structures. This makes our method\nparticularly suitable for applications that require full-head analysis,\nincluding research on head trauma and cranial deformities. Additionally, our\nharmonization model does not require retraining for new datasets, allowing\nsmooth integration into various neuroimaging workflows. By ensuring\nscanner-invariant image quality, our approach provides a robust and efficient\nsolution for improving neuroimaging studies across diverse settings. The code\nis available at this link.\n", "link": "http://arxiv.org/abs/2505.03715v1", "date": "2025-05-06", "relevancy": 2.5192, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5089}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DISARM%2B%2B%3A%20Beyond%20scanner-free%20harmonization&body=Title%3A%20DISARM%2B%2B%3A%20Beyond%20scanner-free%20harmonization%0AAuthor%3A%20Luca%20Caldera%20and%20Lara%20Cavinato%20and%20Alessio%20Cirone%20and%20Isabella%20Cama%20and%20Sara%20Garbarino%20and%20Raffaele%20Lodi%20and%20Fabrizio%20Tagliavini%20and%20Anna%20Nigri%20and%20Silvia%20De%20Francesco%20and%20Andrea%20Cappozzo%20and%20Michele%20Piana%20and%20Francesca%20Ieva%0AAbstract%3A%20%20%20Harmonization%20of%20T1-weighted%20MR%20images%20across%20different%20scanners%20is%20crucial%0Afor%20ensuring%20consistency%20in%20neuroimaging%20studies.%20This%20study%20introduces%20a%20novel%0Aapproach%20to%20direct%20image%20harmonization%2C%20moving%20beyond%20feature%20standardization%0Ato%20ensure%20that%20extracted%20features%20remain%20inherently%20reliable%20for%20downstream%0Aanalysis.%20Our%20method%20enables%20image%20transfer%20in%20two%20ways%3A%20%281%29%20mapping%20images%20to%0Aa%20scanner-free%20space%20for%20uniform%20appearance%20across%20all%20scanners%2C%20and%20%282%29%0Atransforming%20images%20into%20the%20domain%20of%20a%20specific%20scanner%20used%20in%20model%0Atraining%2C%20embedding%20its%20unique%20characteristics.%20Our%20approach%20presents%20strong%0Ageneralization%20capability%2C%20even%20for%20unseen%20scanners%20not%20included%20in%20the%0Atraining%20phase.%20We%20validated%20our%20method%20using%20MR%20images%20from%20diverse%20cohorts%2C%0Aincluding%20healthy%20controls%2C%20traveling%20subjects%2C%20and%20individuals%20with%0AAlzheimer%27s%20disease%20%28AD%29.%20The%20model%27s%20effectiveness%20is%20tested%20in%20multiple%0Aapplications%2C%20such%20as%20brain%20age%20prediction%20%28R2%20%3D%200.60%20%5Cpm%200.05%29%2C%20biomarker%0Aextraction%2C%20AD%20classification%20%28Test%20Accuracy%20%3D%200.86%20%5Cpm%200.03%29%2C%20and%20diagnosis%0Aprediction%20%28AUC%20%3D%200.95%29.%20In%20all%20cases%2C%20our%20harmonization%20technique%20outperforms%0Astate-of-the-art%20methods%2C%20showing%20improvements%20in%20both%20reliability%20and%0Apredictive%20accuracy.%20Moreover%2C%20our%20approach%20eliminates%20the%20need%20for%20extensive%0Apreprocessing%20steps%2C%20such%20as%20skull-stripping%2C%20which%20can%20introduce%20errors%20by%0Amisclassifying%20brain%20and%20non-brain%20structures.%20This%20makes%20our%20method%0Aparticularly%20suitable%20for%20applications%20that%20require%20full-head%20analysis%2C%0Aincluding%20research%20on%20head%20trauma%20and%20cranial%20deformities.%20Additionally%2C%20our%0Aharmonization%20model%20does%20not%20require%20retraining%20for%20new%20datasets%2C%20allowing%0Asmooth%20integration%20into%20various%20neuroimaging%20workflows.%20By%20ensuring%0Ascanner-invariant%20image%20quality%2C%20our%20approach%20provides%20a%20robust%20and%20efficient%0Asolution%20for%20improving%20neuroimaging%20studies%20across%20diverse%20settings.%20The%20code%0Ais%20available%20at%20this%20link.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDISARM%252B%252B%253A%2520Beyond%2520scanner-free%2520harmonization%26entry.906535625%3DLuca%2520Caldera%2520and%2520Lara%2520Cavinato%2520and%2520Alessio%2520Cirone%2520and%2520Isabella%2520Cama%2520and%2520Sara%2520Garbarino%2520and%2520Raffaele%2520Lodi%2520and%2520Fabrizio%2520Tagliavini%2520and%2520Anna%2520Nigri%2520and%2520Silvia%2520De%2520Francesco%2520and%2520Andrea%2520Cappozzo%2520and%2520Michele%2520Piana%2520and%2520Francesca%2520Ieva%26entry.1292438233%3D%2520%2520Harmonization%2520of%2520T1-weighted%2520MR%2520images%2520across%2520different%2520scanners%2520is%2520crucial%250Afor%2520ensuring%2520consistency%2520in%2520neuroimaging%2520studies.%2520This%2520study%2520introduces%2520a%2520novel%250Aapproach%2520to%2520direct%2520image%2520harmonization%252C%2520moving%2520beyond%2520feature%2520standardization%250Ato%2520ensure%2520that%2520extracted%2520features%2520remain%2520inherently%2520reliable%2520for%2520downstream%250Aanalysis.%2520Our%2520method%2520enables%2520image%2520transfer%2520in%2520two%2520ways%253A%2520%25281%2529%2520mapping%2520images%2520to%250Aa%2520scanner-free%2520space%2520for%2520uniform%2520appearance%2520across%2520all%2520scanners%252C%2520and%2520%25282%2529%250Atransforming%2520images%2520into%2520the%2520domain%2520of%2520a%2520specific%2520scanner%2520used%2520in%2520model%250Atraining%252C%2520embedding%2520its%2520unique%2520characteristics.%2520Our%2520approach%2520presents%2520strong%250Ageneralization%2520capability%252C%2520even%2520for%2520unseen%2520scanners%2520not%2520included%2520in%2520the%250Atraining%2520phase.%2520We%2520validated%2520our%2520method%2520using%2520MR%2520images%2520from%2520diverse%2520cohorts%252C%250Aincluding%2520healthy%2520controls%252C%2520traveling%2520subjects%252C%2520and%2520individuals%2520with%250AAlzheimer%2527s%2520disease%2520%2528AD%2529.%2520The%2520model%2527s%2520effectiveness%2520is%2520tested%2520in%2520multiple%250Aapplications%252C%2520such%2520as%2520brain%2520age%2520prediction%2520%2528R2%2520%253D%25200.60%2520%255Cpm%25200.05%2529%252C%2520biomarker%250Aextraction%252C%2520AD%2520classification%2520%2528Test%2520Accuracy%2520%253D%25200.86%2520%255Cpm%25200.03%2529%252C%2520and%2520diagnosis%250Aprediction%2520%2528AUC%2520%253D%25200.95%2529.%2520In%2520all%2520cases%252C%2520our%2520harmonization%2520technique%2520outperforms%250Astate-of-the-art%2520methods%252C%2520showing%2520improvements%2520in%2520both%2520reliability%2520and%250Apredictive%2520accuracy.%2520Moreover%252C%2520our%2520approach%2520eliminates%2520the%2520need%2520for%2520extensive%250Apreprocessing%2520steps%252C%2520such%2520as%2520skull-stripping%252C%2520which%2520can%2520introduce%2520errors%2520by%250Amisclassifying%2520brain%2520and%2520non-brain%2520structures.%2520This%2520makes%2520our%2520method%250Aparticularly%2520suitable%2520for%2520applications%2520that%2520require%2520full-head%2520analysis%252C%250Aincluding%2520research%2520on%2520head%2520trauma%2520and%2520cranial%2520deformities.%2520Additionally%252C%2520our%250Aharmonization%2520model%2520does%2520not%2520require%2520retraining%2520for%2520new%2520datasets%252C%2520allowing%250Asmooth%2520integration%2520into%2520various%2520neuroimaging%2520workflows.%2520By%2520ensuring%250Ascanner-invariant%2520image%2520quality%252C%2520our%2520approach%2520provides%2520a%2520robust%2520and%2520efficient%250Asolution%2520for%2520improving%2520neuroimaging%2520studies%2520across%2520diverse%2520settings.%2520The%2520code%250Ais%2520available%2520at%2520this%2520link.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DISARM%2B%2B%3A%20Beyond%20scanner-free%20harmonization&entry.906535625=Luca%20Caldera%20and%20Lara%20Cavinato%20and%20Alessio%20Cirone%20and%20Isabella%20Cama%20and%20Sara%20Garbarino%20and%20Raffaele%20Lodi%20and%20Fabrizio%20Tagliavini%20and%20Anna%20Nigri%20and%20Silvia%20De%20Francesco%20and%20Andrea%20Cappozzo%20and%20Michele%20Piana%20and%20Francesca%20Ieva&entry.1292438233=%20%20Harmonization%20of%20T1-weighted%20MR%20images%20across%20different%20scanners%20is%20crucial%0Afor%20ensuring%20consistency%20in%20neuroimaging%20studies.%20This%20study%20introduces%20a%20novel%0Aapproach%20to%20direct%20image%20harmonization%2C%20moving%20beyond%20feature%20standardization%0Ato%20ensure%20that%20extracted%20features%20remain%20inherently%20reliable%20for%20downstream%0Aanalysis.%20Our%20method%20enables%20image%20transfer%20in%20two%20ways%3A%20%281%29%20mapping%20images%20to%0Aa%20scanner-free%20space%20for%20uniform%20appearance%20across%20all%20scanners%2C%20and%20%282%29%0Atransforming%20images%20into%20the%20domain%20of%20a%20specific%20scanner%20used%20in%20model%0Atraining%2C%20embedding%20its%20unique%20characteristics.%20Our%20approach%20presents%20strong%0Ageneralization%20capability%2C%20even%20for%20unseen%20scanners%20not%20included%20in%20the%0Atraining%20phase.%20We%20validated%20our%20method%20using%20MR%20images%20from%20diverse%20cohorts%2C%0Aincluding%20healthy%20controls%2C%20traveling%20subjects%2C%20and%20individuals%20with%0AAlzheimer%27s%20disease%20%28AD%29.%20The%20model%27s%20effectiveness%20is%20tested%20in%20multiple%0Aapplications%2C%20such%20as%20brain%20age%20prediction%20%28R2%20%3D%200.60%20%5Cpm%200.05%29%2C%20biomarker%0Aextraction%2C%20AD%20classification%20%28Test%20Accuracy%20%3D%200.86%20%5Cpm%200.03%29%2C%20and%20diagnosis%0Aprediction%20%28AUC%20%3D%200.95%29.%20In%20all%20cases%2C%20our%20harmonization%20technique%20outperforms%0Astate-of-the-art%20methods%2C%20showing%20improvements%20in%20both%20reliability%20and%0Apredictive%20accuracy.%20Moreover%2C%20our%20approach%20eliminates%20the%20need%20for%20extensive%0Apreprocessing%20steps%2C%20such%20as%20skull-stripping%2C%20which%20can%20introduce%20errors%20by%0Amisclassifying%20brain%20and%20non-brain%20structures.%20This%20makes%20our%20method%0Aparticularly%20suitable%20for%20applications%20that%20require%20full-head%20analysis%2C%0Aincluding%20research%20on%20head%20trauma%20and%20cranial%20deformities.%20Additionally%2C%20our%0Aharmonization%20model%20does%20not%20require%20retraining%20for%20new%20datasets%2C%20allowing%0Asmooth%20integration%20into%20various%20neuroimaging%20workflows.%20By%20ensuring%0Ascanner-invariant%20image%20quality%2C%20our%20approach%20provides%20a%20robust%20and%20efficient%0Asolution%20for%20improving%20neuroimaging%20studies%20across%20diverse%20settings.%20The%20code%0Ais%20available%20at%20this%20link.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03715v1&entry.124074799=Read"},
{"title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders", "author": "Chethan Krishnamurthy Ramanaik and Arjun Roy and Eirini Ntoutsi", "abstract": "  Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.\n", "link": "http://arxiv.org/abs/2505.03646v1", "date": "2025-05-06", "relevancy": 2.5146, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5193}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5009}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALMA%3A%20Aggregated%20Lipschitz%20Maximization%20Attack%20on%20Auto-encoders&body=Title%3A%20ALMA%3A%20Aggregated%20Lipschitz%20Maximization%20Attack%20on%20Auto-encoders%0AAuthor%3A%20Chethan%20Krishnamurthy%20Ramanaik%20and%20Arjun%20Roy%20and%20Eirini%20Ntoutsi%0AAbstract%3A%20%20%20Despite%20the%20extensive%20use%20of%20deep%20autoencoders%20%28AEs%29%20in%20critical%0Aapplications%2C%20their%20adversarial%20robustness%20remains%20relatively%20underexplored%0Acompared%20to%20classification%20models.%20AE%20robustness%20is%20characterized%20by%20the%0ALipschitz%20bounds%20of%20its%20components.%20Existing%20robustness%20evaluation%20frameworks%0Abased%20on%20white-box%20attacks%20do%20not%20fully%20exploit%20the%20vulnerabilities%20of%0Aintermediate%20ill-conditioned%20layers%20in%20AEs.%20In%20the%20context%20of%20optimizing%0Aimperceptible%20norm-bounded%20additive%20perturbations%20to%20maximize%20output%20damage%2C%0Aexisting%20methods%20struggle%20to%20effectively%20propagate%20adversarial%20loss%20gradients%0Athroughout%20the%20network%2C%20often%20converging%20to%20less%20effective%20perturbations.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20layer-conditioning-based%20adversarial%0Aoptimization%20objective%20that%20effectively%20guides%20the%20adversarial%20map%20toward%0Aregions%20of%20local%20Lipschitz%20bounds%20by%20enhancing%20loss%20gradient%20information%0Apropagation%20during%20attack%20optimization.%20We%20demonstrate%20through%20extensive%0Aexperiments%20on%20state-of-the-art%20AEs%20that%20our%20adversarial%20objective%20results%20in%0Astronger%20attacks%2C%20outperforming%20existing%20methods%20in%20both%20universal%20and%0Asample-specific%20scenarios.%20As%20a%20defense%20method%20against%20this%20attack%2C%20we%0Aintroduce%20an%20inference-time%20adversarially%20trained%20defense%20plugin%20that%20mitigates%0Athe%20effects%20of%20adversarial%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALMA%253A%2520Aggregated%2520Lipschitz%2520Maximization%2520Attack%2520on%2520Auto-encoders%26entry.906535625%3DChethan%2520Krishnamurthy%2520Ramanaik%2520and%2520Arjun%2520Roy%2520and%2520Eirini%2520Ntoutsi%26entry.1292438233%3D%2520%2520Despite%2520the%2520extensive%2520use%2520of%2520deep%2520autoencoders%2520%2528AEs%2529%2520in%2520critical%250Aapplications%252C%2520their%2520adversarial%2520robustness%2520remains%2520relatively%2520underexplored%250Acompared%2520to%2520classification%2520models.%2520AE%2520robustness%2520is%2520characterized%2520by%2520the%250ALipschitz%2520bounds%2520of%2520its%2520components.%2520Existing%2520robustness%2520evaluation%2520frameworks%250Abased%2520on%2520white-box%2520attacks%2520do%2520not%2520fully%2520exploit%2520the%2520vulnerabilities%2520of%250Aintermediate%2520ill-conditioned%2520layers%2520in%2520AEs.%2520In%2520the%2520context%2520of%2520optimizing%250Aimperceptible%2520norm-bounded%2520additive%2520perturbations%2520to%2520maximize%2520output%2520damage%252C%250Aexisting%2520methods%2520struggle%2520to%2520effectively%2520propagate%2520adversarial%2520loss%2520gradients%250Athroughout%2520the%2520network%252C%2520often%2520converging%2520to%2520less%2520effective%2520perturbations.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520novel%2520layer-conditioning-based%2520adversarial%250Aoptimization%2520objective%2520that%2520effectively%2520guides%2520the%2520adversarial%2520map%2520toward%250Aregions%2520of%2520local%2520Lipschitz%2520bounds%2520by%2520enhancing%2520loss%2520gradient%2520information%250Apropagation%2520during%2520attack%2520optimization.%2520We%2520demonstrate%2520through%2520extensive%250Aexperiments%2520on%2520state-of-the-art%2520AEs%2520that%2520our%2520adversarial%2520objective%2520results%2520in%250Astronger%2520attacks%252C%2520outperforming%2520existing%2520methods%2520in%2520both%2520universal%2520and%250Asample-specific%2520scenarios.%2520As%2520a%2520defense%2520method%2520against%2520this%2520attack%252C%2520we%250Aintroduce%2520an%2520inference-time%2520adversarially%2520trained%2520defense%2520plugin%2520that%2520mitigates%250Athe%2520effects%2520of%2520adversarial%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALMA%3A%20Aggregated%20Lipschitz%20Maximization%20Attack%20on%20Auto-encoders&entry.906535625=Chethan%20Krishnamurthy%20Ramanaik%20and%20Arjun%20Roy%20and%20Eirini%20Ntoutsi&entry.1292438233=%20%20Despite%20the%20extensive%20use%20of%20deep%20autoencoders%20%28AEs%29%20in%20critical%0Aapplications%2C%20their%20adversarial%20robustness%20remains%20relatively%20underexplored%0Acompared%20to%20classification%20models.%20AE%20robustness%20is%20characterized%20by%20the%0ALipschitz%20bounds%20of%20its%20components.%20Existing%20robustness%20evaluation%20frameworks%0Abased%20on%20white-box%20attacks%20do%20not%20fully%20exploit%20the%20vulnerabilities%20of%0Aintermediate%20ill-conditioned%20layers%20in%20AEs.%20In%20the%20context%20of%20optimizing%0Aimperceptible%20norm-bounded%20additive%20perturbations%20to%20maximize%20output%20damage%2C%0Aexisting%20methods%20struggle%20to%20effectively%20propagate%20adversarial%20loss%20gradients%0Athroughout%20the%20network%2C%20often%20converging%20to%20less%20effective%20perturbations.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20layer-conditioning-based%20adversarial%0Aoptimization%20objective%20that%20effectively%20guides%20the%20adversarial%20map%20toward%0Aregions%20of%20local%20Lipschitz%20bounds%20by%20enhancing%20loss%20gradient%20information%0Apropagation%20during%20attack%20optimization.%20We%20demonstrate%20through%20extensive%0Aexperiments%20on%20state-of-the-art%20AEs%20that%20our%20adversarial%20objective%20results%20in%0Astronger%20attacks%2C%20outperforming%20existing%20methods%20in%20both%20universal%20and%0Asample-specific%20scenarios.%20As%20a%20defense%20method%20against%20this%20attack%2C%20we%0Aintroduce%20an%20inference-time%20adversarially%20trained%20defense%20plugin%20that%20mitigates%0Athe%20effects%20of%20adversarial%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03646v1&entry.124074799=Read"},
{"title": "The Struggles of LLMs in Cross-lingual Code Clone Detection", "author": "Micheline B\u00e9n\u00e9dicte Moumoula and Abdoul Kader Kabore and Jacques Klein and Tegawend\u00e9 Bissyande", "abstract": "  With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection.\n", "link": "http://arxiv.org/abs/2408.04430v3", "date": "2025-05-06", "relevancy": 2.5121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Struggles%20of%20LLMs%20in%20Cross-lingual%20Code%20Clone%20Detection&body=Title%3A%20The%20Struggles%20of%20LLMs%20in%20Cross-lingual%20Code%20Clone%20Detection%0AAuthor%3A%20Micheline%20B%C3%A9n%C3%A9dicte%20Moumoula%20and%20Abdoul%20Kader%20Kabore%20and%20Jacques%20Klein%20and%20Tegawend%C3%A9%20Bissyande%0AAbstract%3A%20%20%20With%20the%20involvement%20of%20multiple%20programming%20languages%20in%20modern%20software%0Adevelopment%2C%20cross-lingual%20code%20clone%20detection%20has%20gained%20traction%20within%20the%0Asoftware%20engineering%20community.%20Numerous%20studies%20have%20explored%20this%20topic%2C%0Aproposing%20various%20promising%20approaches.%20Inspired%20by%20the%20significant%20advances%20in%0Amachine%20learning%20in%20recent%20years%2C%20particularly%20Large%20Language%20Models%20%28LLMs%29%2C%0Awhich%20have%20demonstrated%20their%20ability%20to%20tackle%20various%20tasks%2C%20this%20paper%0Arevisits%20cross-lingual%20code%20clone%20detection.%20We%20evaluate%20the%20performance%20of%0Afive%20%2805%29%20LLMs%20and%20eight%20prompts%20%2808%29%20for%20the%20identification%20of%20cross-lingual%0Acode%20clones.%20Additionally%2C%20we%20compare%20these%20results%20against%20two%20baseline%0Amethods.%20Finally%2C%20we%20evaluate%20a%20pre-trained%20embedding%20model%20to%20assess%20the%0Aeffectiveness%20of%20the%20generated%20representations%20for%20classifying%20clone%20and%0Anon-clone%20pairs.%20The%20studies%20involving%20LLMs%20and%20Embedding%20models%20are%20evaluated%0Ausing%20two%20widely%20used%20cross-lingual%20datasets%2C%20XLCoST%20and%20CodeNet.%20Our%20results%0Ashow%20that%20LLMs%20can%20achieve%20high%20F1%20scores%2C%20up%20to%200.99%2C%20for%20straightforward%0Aprogramming%20examples.%20However%2C%20they%20not%20only%20perform%20less%20well%20on%20programs%0Aassociated%20with%20complex%20programming%20challenges%20but%20also%20do%20not%20necessarily%0Aunderstand%20the%20meaning%20of%20%22code%20clones%22%20in%20a%20cross-lingual%20setting.%20We%20show%0Athat%20embedding%20models%20used%20to%20represent%20code%20fragments%20from%20different%0Aprogramming%20languages%20in%20the%20same%20representation%20space%20enable%20the%20training%20of%20a%0Abasic%20classifier%20that%20outperforms%20all%20LLMs%20by%20~1%20and%20~20%20percentage%20points%20on%0Athe%20XLCoST%20and%20CodeNet%20datasets%2C%20respectively.%20This%20finding%20suggests%20that%2C%0Adespite%20the%20apparent%20capabilities%20of%20LLMs%2C%20embeddings%20provided%20by%20embedding%0Amodels%20offer%20suitable%20representations%20to%20achieve%20state-of-the-art%20performance%0Ain%20cross-lingual%20code%20clone%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04430v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Struggles%2520of%2520LLMs%2520in%2520Cross-lingual%2520Code%2520Clone%2520Detection%26entry.906535625%3DMicheline%2520B%25C3%25A9n%25C3%25A9dicte%2520Moumoula%2520and%2520Abdoul%2520Kader%2520Kabore%2520and%2520Jacques%2520Klein%2520and%2520Tegawend%25C3%25A9%2520Bissyande%26entry.1292438233%3D%2520%2520With%2520the%2520involvement%2520of%2520multiple%2520programming%2520languages%2520in%2520modern%2520software%250Adevelopment%252C%2520cross-lingual%2520code%2520clone%2520detection%2520has%2520gained%2520traction%2520within%2520the%250Asoftware%2520engineering%2520community.%2520Numerous%2520studies%2520have%2520explored%2520this%2520topic%252C%250Aproposing%2520various%2520promising%2520approaches.%2520Inspired%2520by%2520the%2520significant%2520advances%2520in%250Amachine%2520learning%2520in%2520recent%2520years%252C%2520particularly%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Awhich%2520have%2520demonstrated%2520their%2520ability%2520to%2520tackle%2520various%2520tasks%252C%2520this%2520paper%250Arevisits%2520cross-lingual%2520code%2520clone%2520detection.%2520We%2520evaluate%2520the%2520performance%2520of%250Afive%2520%252805%2529%2520LLMs%2520and%2520eight%2520prompts%2520%252808%2529%2520for%2520the%2520identification%2520of%2520cross-lingual%250Acode%2520clones.%2520Additionally%252C%2520we%2520compare%2520these%2520results%2520against%2520two%2520baseline%250Amethods.%2520Finally%252C%2520we%2520evaluate%2520a%2520pre-trained%2520embedding%2520model%2520to%2520assess%2520the%250Aeffectiveness%2520of%2520the%2520generated%2520representations%2520for%2520classifying%2520clone%2520and%250Anon-clone%2520pairs.%2520The%2520studies%2520involving%2520LLMs%2520and%2520Embedding%2520models%2520are%2520evaluated%250Ausing%2520two%2520widely%2520used%2520cross-lingual%2520datasets%252C%2520XLCoST%2520and%2520CodeNet.%2520Our%2520results%250Ashow%2520that%2520LLMs%2520can%2520achieve%2520high%2520F1%2520scores%252C%2520up%2520to%25200.99%252C%2520for%2520straightforward%250Aprogramming%2520examples.%2520However%252C%2520they%2520not%2520only%2520perform%2520less%2520well%2520on%2520programs%250Aassociated%2520with%2520complex%2520programming%2520challenges%2520but%2520also%2520do%2520not%2520necessarily%250Aunderstand%2520the%2520meaning%2520of%2520%2522code%2520clones%2522%2520in%2520a%2520cross-lingual%2520setting.%2520We%2520show%250Athat%2520embedding%2520models%2520used%2520to%2520represent%2520code%2520fragments%2520from%2520different%250Aprogramming%2520languages%2520in%2520the%2520same%2520representation%2520space%2520enable%2520the%2520training%2520of%2520a%250Abasic%2520classifier%2520that%2520outperforms%2520all%2520LLMs%2520by%2520~1%2520and%2520~20%2520percentage%2520points%2520on%250Athe%2520XLCoST%2520and%2520CodeNet%2520datasets%252C%2520respectively.%2520This%2520finding%2520suggests%2520that%252C%250Adespite%2520the%2520apparent%2520capabilities%2520of%2520LLMs%252C%2520embeddings%2520provided%2520by%2520embedding%250Amodels%2520offer%2520suitable%2520representations%2520to%2520achieve%2520state-of-the-art%2520performance%250Ain%2520cross-lingual%2520code%2520clone%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04430v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Struggles%20of%20LLMs%20in%20Cross-lingual%20Code%20Clone%20Detection&entry.906535625=Micheline%20B%C3%A9n%C3%A9dicte%20Moumoula%20and%20Abdoul%20Kader%20Kabore%20and%20Jacques%20Klein%20and%20Tegawend%C3%A9%20Bissyande&entry.1292438233=%20%20With%20the%20involvement%20of%20multiple%20programming%20languages%20in%20modern%20software%0Adevelopment%2C%20cross-lingual%20code%20clone%20detection%20has%20gained%20traction%20within%20the%0Asoftware%20engineering%20community.%20Numerous%20studies%20have%20explored%20this%20topic%2C%0Aproposing%20various%20promising%20approaches.%20Inspired%20by%20the%20significant%20advances%20in%0Amachine%20learning%20in%20recent%20years%2C%20particularly%20Large%20Language%20Models%20%28LLMs%29%2C%0Awhich%20have%20demonstrated%20their%20ability%20to%20tackle%20various%20tasks%2C%20this%20paper%0Arevisits%20cross-lingual%20code%20clone%20detection.%20We%20evaluate%20the%20performance%20of%0Afive%20%2805%29%20LLMs%20and%20eight%20prompts%20%2808%29%20for%20the%20identification%20of%20cross-lingual%0Acode%20clones.%20Additionally%2C%20we%20compare%20these%20results%20against%20two%20baseline%0Amethods.%20Finally%2C%20we%20evaluate%20a%20pre-trained%20embedding%20model%20to%20assess%20the%0Aeffectiveness%20of%20the%20generated%20representations%20for%20classifying%20clone%20and%0Anon-clone%20pairs.%20The%20studies%20involving%20LLMs%20and%20Embedding%20models%20are%20evaluated%0Ausing%20two%20widely%20used%20cross-lingual%20datasets%2C%20XLCoST%20and%20CodeNet.%20Our%20results%0Ashow%20that%20LLMs%20can%20achieve%20high%20F1%20scores%2C%20up%20to%200.99%2C%20for%20straightforward%0Aprogramming%20examples.%20However%2C%20they%20not%20only%20perform%20less%20well%20on%20programs%0Aassociated%20with%20complex%20programming%20challenges%20but%20also%20do%20not%20necessarily%0Aunderstand%20the%20meaning%20of%20%22code%20clones%22%20in%20a%20cross-lingual%20setting.%20We%20show%0Athat%20embedding%20models%20used%20to%20represent%20code%20fragments%20from%20different%0Aprogramming%20languages%20in%20the%20same%20representation%20space%20enable%20the%20training%20of%20a%0Abasic%20classifier%20that%20outperforms%20all%20LLMs%20by%20~1%20and%20~20%20percentage%20points%20on%0Athe%20XLCoST%20and%20CodeNet%20datasets%2C%20respectively.%20This%20finding%20suggests%20that%2C%0Adespite%20the%20apparent%20capabilities%20of%20LLMs%2C%20embeddings%20provided%20by%20embedding%0Amodels%20offer%20suitable%20representations%20to%20achieve%20state-of-the-art%20performance%0Ain%20cross-lingual%20code%20clone%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04430v3&entry.124074799=Read"},
{"title": "Automated Action Generation based on Action Field for Robotic Garment\n  Manipulation", "author": "Hu Cheng and Fuyuki Tokuda and Kazuhiro Kosuge", "abstract": "  Garment manipulation using robotic systems is a challenging task due to the\ndiverse shapes and deformable nature of fabric. In this paper, we propose a\nnovel method for robotic garment manipulation that significantly improves the\naccuracy while reducing computational time compared to previous approaches. Our\nmethod features an action generator that directly interprets scene images and\ngenerates pixel-wise end-effector action vectors using a neural network. The\nnetwork also predicts a manipulation score map that ranks potential actions,\nallowing the system to select the most effective action. Extensive simulation\nexperiments demonstrate that our method achieves higher unfolding and alignment\nperformances and faster computation time than previous approaches. Real-world\nexperiments show that the proposed method generalizes well to different garment\ntypes and successfully flattens garments.\n", "link": "http://arxiv.org/abs/2505.03537v1", "date": "2025-05-06", "relevancy": 2.5038, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6638}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6367}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Action%20Generation%20based%20on%20Action%20Field%20for%20Robotic%20Garment%0A%20%20Manipulation&body=Title%3A%20Automated%20Action%20Generation%20based%20on%20Action%20Field%20for%20Robotic%20Garment%0A%20%20Manipulation%0AAuthor%3A%20Hu%20Cheng%20and%20Fuyuki%20Tokuda%20and%20Kazuhiro%20Kosuge%0AAbstract%3A%20%20%20Garment%20manipulation%20using%20robotic%20systems%20is%20a%20challenging%20task%20due%20to%20the%0Adiverse%20shapes%20and%20deformable%20nature%20of%20fabric.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20method%20for%20robotic%20garment%20manipulation%20that%20significantly%20improves%20the%0Aaccuracy%20while%20reducing%20computational%20time%20compared%20to%20previous%20approaches.%20Our%0Amethod%20features%20an%20action%20generator%20that%20directly%20interprets%20scene%20images%20and%0Agenerates%20pixel-wise%20end-effector%20action%20vectors%20using%20a%20neural%20network.%20The%0Anetwork%20also%20predicts%20a%20manipulation%20score%20map%20that%20ranks%20potential%20actions%2C%0Aallowing%20the%20system%20to%20select%20the%20most%20effective%20action.%20Extensive%20simulation%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20higher%20unfolding%20and%20alignment%0Aperformances%20and%20faster%20computation%20time%20than%20previous%20approaches.%20Real-world%0Aexperiments%20show%20that%20the%20proposed%20method%20generalizes%20well%20to%20different%20garment%0Atypes%20and%20successfully%20flattens%20garments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Action%2520Generation%2520based%2520on%2520Action%2520Field%2520for%2520Robotic%2520Garment%250A%2520%2520Manipulation%26entry.906535625%3DHu%2520Cheng%2520and%2520Fuyuki%2520Tokuda%2520and%2520Kazuhiro%2520Kosuge%26entry.1292438233%3D%2520%2520Garment%2520manipulation%2520using%2520robotic%2520systems%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%250Adiverse%2520shapes%2520and%2520deformable%2520nature%2520of%2520fabric.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520method%2520for%2520robotic%2520garment%2520manipulation%2520that%2520significantly%2520improves%2520the%250Aaccuracy%2520while%2520reducing%2520computational%2520time%2520compared%2520to%2520previous%2520approaches.%2520Our%250Amethod%2520features%2520an%2520action%2520generator%2520that%2520directly%2520interprets%2520scene%2520images%2520and%250Agenerates%2520pixel-wise%2520end-effector%2520action%2520vectors%2520using%2520a%2520neural%2520network.%2520The%250Anetwork%2520also%2520predicts%2520a%2520manipulation%2520score%2520map%2520that%2520ranks%2520potential%2520actions%252C%250Aallowing%2520the%2520system%2520to%2520select%2520the%2520most%2520effective%2520action.%2520Extensive%2520simulation%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520higher%2520unfolding%2520and%2520alignment%250Aperformances%2520and%2520faster%2520computation%2520time%2520than%2520previous%2520approaches.%2520Real-world%250Aexperiments%2520show%2520that%2520the%2520proposed%2520method%2520generalizes%2520well%2520to%2520different%2520garment%250Atypes%2520and%2520successfully%2520flattens%2520garments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Action%20Generation%20based%20on%20Action%20Field%20for%20Robotic%20Garment%0A%20%20Manipulation&entry.906535625=Hu%20Cheng%20and%20Fuyuki%20Tokuda%20and%20Kazuhiro%20Kosuge&entry.1292438233=%20%20Garment%20manipulation%20using%20robotic%20systems%20is%20a%20challenging%20task%20due%20to%20the%0Adiverse%20shapes%20and%20deformable%20nature%20of%20fabric.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20method%20for%20robotic%20garment%20manipulation%20that%20significantly%20improves%20the%0Aaccuracy%20while%20reducing%20computational%20time%20compared%20to%20previous%20approaches.%20Our%0Amethod%20features%20an%20action%20generator%20that%20directly%20interprets%20scene%20images%20and%0Agenerates%20pixel-wise%20end-effector%20action%20vectors%20using%20a%20neural%20network.%20The%0Anetwork%20also%20predicts%20a%20manipulation%20score%20map%20that%20ranks%20potential%20actions%2C%0Aallowing%20the%20system%20to%20select%20the%20most%20effective%20action.%20Extensive%20simulation%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20higher%20unfolding%20and%20alignment%0Aperformances%20and%20faster%20computation%20time%20than%20previous%20approaches.%20Real-world%0Aexperiments%20show%20that%20the%20proposed%20method%20generalizes%20well%20to%20different%20garment%0Atypes%20and%20successfully%20flattens%20garments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03537v1&entry.124074799=Read"},
{"title": "Causal Intervention Framework for Variational Auto Encoder Mechanistic\n  Interpretability", "author": "Dip Roy", "abstract": "  Mechanistic interpretability of deep learning models has emerged as a crucial\nresearch direction for understanding the functioning of neural networks. While\nsignificant progress has been made in interpreting discriminative models like\ntransformers, understanding generative models such as Variational Autoencoders\n(VAEs) remains challenging. This paper introduces a comprehensive causal\nintervention framework for mechanistic interpretability of VAEs. We develop\ntechniques to identify and analyze \"circuit motifs\" in VAEs, examining how\nsemantic factors are encoded, processed, and disentangled through the network\nlayers. Our approach uses targeted interventions at different levels: input\nmanipulations, latent space perturbations, activation patching, and causal\nmediation analysis. We apply our framework to both synthetic datasets with\nknown causal relationships and standard disentanglement benchmarks. Results\nshow that our interventions can successfully isolate functional circuits, map\ncomputational graphs to causal graphs of semantic factors, and distinguish\nbetween polysemantic and monosemantic units. Furthermore, we introduce metrics\nfor causal effect strength, intervention specificity, and circuit modularity\nthat quantify the interpretability of VAE components. Experimental results\ndemonstrate clear differences between VAE variants, with FactorVAE achieving\nhigher disentanglement scores (0.084) and effect strengths (mean 4.59) compared\nto standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework\nadvances the mechanistic understanding of generative models and provides tools\nfor more transparent and controllable VAE architectures.\n", "link": "http://arxiv.org/abs/2505.03530v1", "date": "2025-05-06", "relevancy": 2.4811, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Intervention%20Framework%20for%20Variational%20Auto%20Encoder%20Mechanistic%0A%20%20Interpretability&body=Title%3A%20Causal%20Intervention%20Framework%20for%20Variational%20Auto%20Encoder%20Mechanistic%0A%20%20Interpretability%0AAuthor%3A%20Dip%20Roy%0AAbstract%3A%20%20%20Mechanistic%20interpretability%20of%20deep%20learning%20models%20has%20emerged%20as%20a%20crucial%0Aresearch%20direction%20for%20understanding%20the%20functioning%20of%20neural%20networks.%20While%0Asignificant%20progress%20has%20been%20made%20in%20interpreting%20discriminative%20models%20like%0Atransformers%2C%20understanding%20generative%20models%20such%20as%20Variational%20Autoencoders%0A%28VAEs%29%20remains%20challenging.%20This%20paper%20introduces%20a%20comprehensive%20causal%0Aintervention%20framework%20for%20mechanistic%20interpretability%20of%20VAEs.%20We%20develop%0Atechniques%20to%20identify%20and%20analyze%20%22circuit%20motifs%22%20in%20VAEs%2C%20examining%20how%0Asemantic%20factors%20are%20encoded%2C%20processed%2C%20and%20disentangled%20through%20the%20network%0Alayers.%20Our%20approach%20uses%20targeted%20interventions%20at%20different%20levels%3A%20input%0Amanipulations%2C%20latent%20space%20perturbations%2C%20activation%20patching%2C%20and%20causal%0Amediation%20analysis.%20We%20apply%20our%20framework%20to%20both%20synthetic%20datasets%20with%0Aknown%20causal%20relationships%20and%20standard%20disentanglement%20benchmarks.%20Results%0Ashow%20that%20our%20interventions%20can%20successfully%20isolate%20functional%20circuits%2C%20map%0Acomputational%20graphs%20to%20causal%20graphs%20of%20semantic%20factors%2C%20and%20distinguish%0Abetween%20polysemantic%20and%20monosemantic%20units.%20Furthermore%2C%20we%20introduce%20metrics%0Afor%20causal%20effect%20strength%2C%20intervention%20specificity%2C%20and%20circuit%20modularity%0Athat%20quantify%20the%20interpretability%20of%20VAE%20components.%20Experimental%20results%0Ademonstrate%20clear%20differences%20between%20VAE%20variants%2C%20with%20FactorVAE%20achieving%0Ahigher%20disentanglement%20scores%20%280.084%29%20and%20effect%20strengths%20%28mean%204.59%29%20compared%0Ato%20standard%20VAE%20%280.064%2C%203.99%29%20and%20Beta-VAE%20%280.051%2C%203.43%29.%20Our%20framework%0Aadvances%20the%20mechanistic%20understanding%20of%20generative%20models%20and%20provides%20tools%0Afor%20more%20transparent%20and%20controllable%20VAE%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Intervention%2520Framework%2520for%2520Variational%2520Auto%2520Encoder%2520Mechanistic%250A%2520%2520Interpretability%26entry.906535625%3DDip%2520Roy%26entry.1292438233%3D%2520%2520Mechanistic%2520interpretability%2520of%2520deep%2520learning%2520models%2520has%2520emerged%2520as%2520a%2520crucial%250Aresearch%2520direction%2520for%2520understanding%2520the%2520functioning%2520of%2520neural%2520networks.%2520While%250Asignificant%2520progress%2520has%2520been%2520made%2520in%2520interpreting%2520discriminative%2520models%2520like%250Atransformers%252C%2520understanding%2520generative%2520models%2520such%2520as%2520Variational%2520Autoencoders%250A%2528VAEs%2529%2520remains%2520challenging.%2520This%2520paper%2520introduces%2520a%2520comprehensive%2520causal%250Aintervention%2520framework%2520for%2520mechanistic%2520interpretability%2520of%2520VAEs.%2520We%2520develop%250Atechniques%2520to%2520identify%2520and%2520analyze%2520%2522circuit%2520motifs%2522%2520in%2520VAEs%252C%2520examining%2520how%250Asemantic%2520factors%2520are%2520encoded%252C%2520processed%252C%2520and%2520disentangled%2520through%2520the%2520network%250Alayers.%2520Our%2520approach%2520uses%2520targeted%2520interventions%2520at%2520different%2520levels%253A%2520input%250Amanipulations%252C%2520latent%2520space%2520perturbations%252C%2520activation%2520patching%252C%2520and%2520causal%250Amediation%2520analysis.%2520We%2520apply%2520our%2520framework%2520to%2520both%2520synthetic%2520datasets%2520with%250Aknown%2520causal%2520relationships%2520and%2520standard%2520disentanglement%2520benchmarks.%2520Results%250Ashow%2520that%2520our%2520interventions%2520can%2520successfully%2520isolate%2520functional%2520circuits%252C%2520map%250Acomputational%2520graphs%2520to%2520causal%2520graphs%2520of%2520semantic%2520factors%252C%2520and%2520distinguish%250Abetween%2520polysemantic%2520and%2520monosemantic%2520units.%2520Furthermore%252C%2520we%2520introduce%2520metrics%250Afor%2520causal%2520effect%2520strength%252C%2520intervention%2520specificity%252C%2520and%2520circuit%2520modularity%250Athat%2520quantify%2520the%2520interpretability%2520of%2520VAE%2520components.%2520Experimental%2520results%250Ademonstrate%2520clear%2520differences%2520between%2520VAE%2520variants%252C%2520with%2520FactorVAE%2520achieving%250Ahigher%2520disentanglement%2520scores%2520%25280.084%2529%2520and%2520effect%2520strengths%2520%2528mean%25204.59%2529%2520compared%250Ato%2520standard%2520VAE%2520%25280.064%252C%25203.99%2529%2520and%2520Beta-VAE%2520%25280.051%252C%25203.43%2529.%2520Our%2520framework%250Aadvances%2520the%2520mechanistic%2520understanding%2520of%2520generative%2520models%2520and%2520provides%2520tools%250Afor%2520more%2520transparent%2520and%2520controllable%2520VAE%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Intervention%20Framework%20for%20Variational%20Auto%20Encoder%20Mechanistic%0A%20%20Interpretability&entry.906535625=Dip%20Roy&entry.1292438233=%20%20Mechanistic%20interpretability%20of%20deep%20learning%20models%20has%20emerged%20as%20a%20crucial%0Aresearch%20direction%20for%20understanding%20the%20functioning%20of%20neural%20networks.%20While%0Asignificant%20progress%20has%20been%20made%20in%20interpreting%20discriminative%20models%20like%0Atransformers%2C%20understanding%20generative%20models%20such%20as%20Variational%20Autoencoders%0A%28VAEs%29%20remains%20challenging.%20This%20paper%20introduces%20a%20comprehensive%20causal%0Aintervention%20framework%20for%20mechanistic%20interpretability%20of%20VAEs.%20We%20develop%0Atechniques%20to%20identify%20and%20analyze%20%22circuit%20motifs%22%20in%20VAEs%2C%20examining%20how%0Asemantic%20factors%20are%20encoded%2C%20processed%2C%20and%20disentangled%20through%20the%20network%0Alayers.%20Our%20approach%20uses%20targeted%20interventions%20at%20different%20levels%3A%20input%0Amanipulations%2C%20latent%20space%20perturbations%2C%20activation%20patching%2C%20and%20causal%0Amediation%20analysis.%20We%20apply%20our%20framework%20to%20both%20synthetic%20datasets%20with%0Aknown%20causal%20relationships%20and%20standard%20disentanglement%20benchmarks.%20Results%0Ashow%20that%20our%20interventions%20can%20successfully%20isolate%20functional%20circuits%2C%20map%0Acomputational%20graphs%20to%20causal%20graphs%20of%20semantic%20factors%2C%20and%20distinguish%0Abetween%20polysemantic%20and%20monosemantic%20units.%20Furthermore%2C%20we%20introduce%20metrics%0Afor%20causal%20effect%20strength%2C%20intervention%20specificity%2C%20and%20circuit%20modularity%0Athat%20quantify%20the%20interpretability%20of%20VAE%20components.%20Experimental%20results%0Ademonstrate%20clear%20differences%20between%20VAE%20variants%2C%20with%20FactorVAE%20achieving%0Ahigher%20disentanglement%20scores%20%280.084%29%20and%20effect%20strengths%20%28mean%204.59%29%20compared%0Ato%20standard%20VAE%20%280.064%2C%203.99%29%20and%20Beta-VAE%20%280.051%2C%203.43%29.%20Our%20framework%0Aadvances%20the%20mechanistic%20understanding%20of%20generative%20models%20and%20provides%20tools%0Afor%20more%20transparent%20and%20controllable%20VAE%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03530v1&entry.124074799=Read"},
{"title": "Real-Time Person Image Synthesis Using a Flow Matching Model", "author": "Jiwoo Jeong and Kirok Kim and Wooju Kim and Nam-Joon Kim", "abstract": "  Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.\n", "link": "http://arxiv.org/abs/2505.03562v1", "date": "2025-05-06", "relevancy": 2.4733, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6323}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6236}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Person%20Image%20Synthesis%20Using%20a%20Flow%20Matching%20Model&body=Title%3A%20Real-Time%20Person%20Image%20Synthesis%20Using%20a%20Flow%20Matching%20Model%0AAuthor%3A%20Jiwoo%20Jeong%20and%20Kirok%20Kim%20and%20Wooju%20Kim%20and%20Nam-Joon%20Kim%0AAbstract%3A%20%20%20Pose-Guided%20Person%20Image%20Synthesis%20%28PGPIS%29%20generates%20realistic%20person%20images%0Aconditioned%20on%20a%20target%20pose%20and%20a%20source%20image.%20This%20task%20plays%20a%20key%20role%20in%0Avarious%20real-world%20applications%2C%20such%20as%20sign%20language%20video%20generation%2C%20AR/VR%2C%0Agaming%2C%20and%20live%20streaming.%20In%20these%20scenarios%2C%20real-time%20PGPIS%20is%20critical%20for%0Aproviding%20immediate%20visual%20feedback%20and%20maintaining%20user%20immersion.However%2C%0Aachieving%20real-time%20performance%20remains%20a%20significant%20challenge%20due%20to%20the%0Acomplexity%20of%20synthesizing%20high-fidelity%20images%20from%20diverse%20and%20dynamic%20human%0Aposes.%20Recent%20diffusion-based%20methods%20have%20shown%20impressive%20image%20quality%20in%0APGPIS%2C%20but%20their%20slow%20sampling%20speeds%20hinder%20deployment%20in%20time-sensitive%0Aapplications.%20This%20latency%20is%20particularly%20problematic%20in%20tasks%20like%20generating%0Asign%20language%20videos%20during%20live%20broadcasts%2C%20where%20rapid%20image%20updates%20are%0Arequired.%20Therefore%2C%20developing%20a%20fast%20and%20reliable%20PGPIS%20model%20is%20a%20crucial%0Astep%20toward%20enabling%20real-time%20interactive%20systems.%20To%20address%20this%20challenge%2C%0Awe%20propose%20a%20generative%20model%20based%20on%20flow%20matching%20%28FM%29.%20Our%20approach%20enables%0Afaster%2C%20more%20stable%2C%20and%20more%20efficient%20training%20and%20sampling.%20Furthermore%2C%20the%0Aproposed%20model%20supports%20conditional%20generation%20and%20can%20operate%20in%20latent%20space%2C%0Amaking%20it%20especially%20suitable%20for%20real-time%20PGPIS%20applications%20where%20both%20speed%0Aand%20quality%20are%20critical.%20We%20evaluate%20our%20proposed%20method%2C%20Real-Time%20Person%0AImage%20Synthesis%20Using%20a%20Flow%20Matching%20Model%20%28RPFM%29%2C%20on%20the%20widely%20used%0ADeepFashion%20dataset%20for%20PGPIS%20tasks.%20Our%20results%20show%20that%20RPFM%20achieves%0Anear-real-time%20sampling%20speeds%20while%20maintaining%20performance%20comparable%20to%20the%0Astate-of-the-art%20models.%20Our%20methodology%20trades%20off%20a%20slight%2C%20acceptable%0Adecrease%20in%20generated-image%20accuracy%20for%20over%20a%20twofold%20increase%20in%20generation%0Aspeed%2C%20thereby%20ensuring%20real-time%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Person%2520Image%2520Synthesis%2520Using%2520a%2520Flow%2520Matching%2520Model%26entry.906535625%3DJiwoo%2520Jeong%2520and%2520Kirok%2520Kim%2520and%2520Wooju%2520Kim%2520and%2520Nam-Joon%2520Kim%26entry.1292438233%3D%2520%2520Pose-Guided%2520Person%2520Image%2520Synthesis%2520%2528PGPIS%2529%2520generates%2520realistic%2520person%2520images%250Aconditioned%2520on%2520a%2520target%2520pose%2520and%2520a%2520source%2520image.%2520This%2520task%2520plays%2520a%2520key%2520role%2520in%250Avarious%2520real-world%2520applications%252C%2520such%2520as%2520sign%2520language%2520video%2520generation%252C%2520AR/VR%252C%250Agaming%252C%2520and%2520live%2520streaming.%2520In%2520these%2520scenarios%252C%2520real-time%2520PGPIS%2520is%2520critical%2520for%250Aproviding%2520immediate%2520visual%2520feedback%2520and%2520maintaining%2520user%2520immersion.However%252C%250Aachieving%2520real-time%2520performance%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%250Acomplexity%2520of%2520synthesizing%2520high-fidelity%2520images%2520from%2520diverse%2520and%2520dynamic%2520human%250Aposes.%2520Recent%2520diffusion-based%2520methods%2520have%2520shown%2520impressive%2520image%2520quality%2520in%250APGPIS%252C%2520but%2520their%2520slow%2520sampling%2520speeds%2520hinder%2520deployment%2520in%2520time-sensitive%250Aapplications.%2520This%2520latency%2520is%2520particularly%2520problematic%2520in%2520tasks%2520like%2520generating%250Asign%2520language%2520videos%2520during%2520live%2520broadcasts%252C%2520where%2520rapid%2520image%2520updates%2520are%250Arequired.%2520Therefore%252C%2520developing%2520a%2520fast%2520and%2520reliable%2520PGPIS%2520model%2520is%2520a%2520crucial%250Astep%2520toward%2520enabling%2520real-time%2520interactive%2520systems.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520a%2520generative%2520model%2520based%2520on%2520flow%2520matching%2520%2528FM%2529.%2520Our%2520approach%2520enables%250Afaster%252C%2520more%2520stable%252C%2520and%2520more%2520efficient%2520training%2520and%2520sampling.%2520Furthermore%252C%2520the%250Aproposed%2520model%2520supports%2520conditional%2520generation%2520and%2520can%2520operate%2520in%2520latent%2520space%252C%250Amaking%2520it%2520especially%2520suitable%2520for%2520real-time%2520PGPIS%2520applications%2520where%2520both%2520speed%250Aand%2520quality%2520are%2520critical.%2520We%2520evaluate%2520our%2520proposed%2520method%252C%2520Real-Time%2520Person%250AImage%2520Synthesis%2520Using%2520a%2520Flow%2520Matching%2520Model%2520%2528RPFM%2529%252C%2520on%2520the%2520widely%2520used%250ADeepFashion%2520dataset%2520for%2520PGPIS%2520tasks.%2520Our%2520results%2520show%2520that%2520RPFM%2520achieves%250Anear-real-time%2520sampling%2520speeds%2520while%2520maintaining%2520performance%2520comparable%2520to%2520the%250Astate-of-the-art%2520models.%2520Our%2520methodology%2520trades%2520off%2520a%2520slight%252C%2520acceptable%250Adecrease%2520in%2520generated-image%2520accuracy%2520for%2520over%2520a%2520twofold%2520increase%2520in%2520generation%250Aspeed%252C%2520thereby%2520ensuring%2520real-time%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Person%20Image%20Synthesis%20Using%20a%20Flow%20Matching%20Model&entry.906535625=Jiwoo%20Jeong%20and%20Kirok%20Kim%20and%20Wooju%20Kim%20and%20Nam-Joon%20Kim&entry.1292438233=%20%20Pose-Guided%20Person%20Image%20Synthesis%20%28PGPIS%29%20generates%20realistic%20person%20images%0Aconditioned%20on%20a%20target%20pose%20and%20a%20source%20image.%20This%20task%20plays%20a%20key%20role%20in%0Avarious%20real-world%20applications%2C%20such%20as%20sign%20language%20video%20generation%2C%20AR/VR%2C%0Agaming%2C%20and%20live%20streaming.%20In%20these%20scenarios%2C%20real-time%20PGPIS%20is%20critical%20for%0Aproviding%20immediate%20visual%20feedback%20and%20maintaining%20user%20immersion.However%2C%0Aachieving%20real-time%20performance%20remains%20a%20significant%20challenge%20due%20to%20the%0Acomplexity%20of%20synthesizing%20high-fidelity%20images%20from%20diverse%20and%20dynamic%20human%0Aposes.%20Recent%20diffusion-based%20methods%20have%20shown%20impressive%20image%20quality%20in%0APGPIS%2C%20but%20their%20slow%20sampling%20speeds%20hinder%20deployment%20in%20time-sensitive%0Aapplications.%20This%20latency%20is%20particularly%20problematic%20in%20tasks%20like%20generating%0Asign%20language%20videos%20during%20live%20broadcasts%2C%20where%20rapid%20image%20updates%20are%0Arequired.%20Therefore%2C%20developing%20a%20fast%20and%20reliable%20PGPIS%20model%20is%20a%20crucial%0Astep%20toward%20enabling%20real-time%20interactive%20systems.%20To%20address%20this%20challenge%2C%0Awe%20propose%20a%20generative%20model%20based%20on%20flow%20matching%20%28FM%29.%20Our%20approach%20enables%0Afaster%2C%20more%20stable%2C%20and%20more%20efficient%20training%20and%20sampling.%20Furthermore%2C%20the%0Aproposed%20model%20supports%20conditional%20generation%20and%20can%20operate%20in%20latent%20space%2C%0Amaking%20it%20especially%20suitable%20for%20real-time%20PGPIS%20applications%20where%20both%20speed%0Aand%20quality%20are%20critical.%20We%20evaluate%20our%20proposed%20method%2C%20Real-Time%20Person%0AImage%20Synthesis%20Using%20a%20Flow%20Matching%20Model%20%28RPFM%29%2C%20on%20the%20widely%20used%0ADeepFashion%20dataset%20for%20PGPIS%20tasks.%20Our%20results%20show%20that%20RPFM%20achieves%0Anear-real-time%20sampling%20speeds%20while%20maintaining%20performance%20comparable%20to%20the%0Astate-of-the-art%20models.%20Our%20methodology%20trades%20off%20a%20slight%2C%20acceptable%0Adecrease%20in%20generated-image%20accuracy%20for%20over%20a%20twofold%20increase%20in%20generation%0Aspeed%2C%20thereby%20ensuring%20real-time%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03562v1&entry.124074799=Read"},
{"title": "Distribution-Conditional Generation: From Class Distribution to Creative\n  Generation", "author": "Fu Feng and Yucheng Xie and Xu Yang and Jing Wang and Xin Geng", "abstract": "  Text-to-image (T2I) diffusion models are effective at producing semantically\naligned images, but their reliance on training data distributions limits their\nability to synthesize truly novel, out-of-distribution concepts. Existing\nmethods typically enhance creativity by combining pairs of known concepts,\nyielding compositions that, while out-of-distribution, remain linguistically\ndescribable and bounded within the existing semantic space. Inspired by the\nsoft probabilistic outputs of classifiers on ambiguous inputs, we propose\nDistribution-Conditional Generation, a novel formulation that models creativity\nas image synthesis conditioned on class distributions, enabling semantically\nunconstrained creative generation. Building on this, we propose DisTok, an\nencoder-decoder framework that maps class distributions into a latent space and\ndecodes them into tokens of creative concept. DisTok maintains a dynamic\nconcept pool and iteratively sampling and fusing concept pairs, enabling the\ngeneration of tokens aligned with increasingly complex class distributions. To\nenforce distributional consistency, latent vectors sampled from a Gaussian\nprior are decoded into tokens and rendered into images, whose class\ndistributions-predicted by a vision-language model-supervise the alignment\nbetween input distributions and the visual semantics of generated tokens. The\nresulting tokens are added to the concept pool for subsequent composition.\nExtensive experiments demonstrate that DisTok, by unifying\ndistribution-conditioned fusion and sampling-based synthesis, enables efficient\nand flexible token-level generation, achieving state-of-the-art performance\nwith superior text-image alignment and human preference scores.\n", "link": "http://arxiv.org/abs/2505.03667v1", "date": "2025-05-06", "relevancy": 2.47, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6319}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6269}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution-Conditional%20Generation%3A%20From%20Class%20Distribution%20to%20Creative%0A%20%20Generation&body=Title%3A%20Distribution-Conditional%20Generation%3A%20From%20Class%20Distribution%20to%20Creative%0A%20%20Generation%0AAuthor%3A%20Fu%20Feng%20and%20Yucheng%20Xie%20and%20Xu%20Yang%20and%20Jing%20Wang%20and%20Xin%20Geng%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20are%20effective%20at%20producing%20semantically%0Aaligned%20images%2C%20but%20their%20reliance%20on%20training%20data%20distributions%20limits%20their%0Aability%20to%20synthesize%20truly%20novel%2C%20out-of-distribution%20concepts.%20Existing%0Amethods%20typically%20enhance%20creativity%20by%20combining%20pairs%20of%20known%20concepts%2C%0Ayielding%20compositions%20that%2C%20while%20out-of-distribution%2C%20remain%20linguistically%0Adescribable%20and%20bounded%20within%20the%20existing%20semantic%20space.%20Inspired%20by%20the%0Asoft%20probabilistic%20outputs%20of%20classifiers%20on%20ambiguous%20inputs%2C%20we%20propose%0ADistribution-Conditional%20Generation%2C%20a%20novel%20formulation%20that%20models%20creativity%0Aas%20image%20synthesis%20conditioned%20on%20class%20distributions%2C%20enabling%20semantically%0Aunconstrained%20creative%20generation.%20Building%20on%20this%2C%20we%20propose%20DisTok%2C%20an%0Aencoder-decoder%20framework%20that%20maps%20class%20distributions%20into%20a%20latent%20space%20and%0Adecodes%20them%20into%20tokens%20of%20creative%20concept.%20DisTok%20maintains%20a%20dynamic%0Aconcept%20pool%20and%20iteratively%20sampling%20and%20fusing%20concept%20pairs%2C%20enabling%20the%0Ageneration%20of%20tokens%20aligned%20with%20increasingly%20complex%20class%20distributions.%20To%0Aenforce%20distributional%20consistency%2C%20latent%20vectors%20sampled%20from%20a%20Gaussian%0Aprior%20are%20decoded%20into%20tokens%20and%20rendered%20into%20images%2C%20whose%20class%0Adistributions-predicted%20by%20a%20vision-language%20model-supervise%20the%20alignment%0Abetween%20input%20distributions%20and%20the%20visual%20semantics%20of%20generated%20tokens.%20The%0Aresulting%20tokens%20are%20added%20to%20the%20concept%20pool%20for%20subsequent%20composition.%0AExtensive%20experiments%20demonstrate%20that%20DisTok%2C%20by%20unifying%0Adistribution-conditioned%20fusion%20and%20sampling-based%20synthesis%2C%20enables%20efficient%0Aand%20flexible%20token-level%20generation%2C%20achieving%20state-of-the-art%20performance%0Awith%20superior%20text-image%20alignment%20and%20human%20preference%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution-Conditional%2520Generation%253A%2520From%2520Class%2520Distribution%2520to%2520Creative%250A%2520%2520Generation%26entry.906535625%3DFu%2520Feng%2520and%2520Yucheng%2520Xie%2520and%2520Xu%2520Yang%2520and%2520Jing%2520Wang%2520and%2520Xin%2520Geng%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520are%2520effective%2520at%2520producing%2520semantically%250Aaligned%2520images%252C%2520but%2520their%2520reliance%2520on%2520training%2520data%2520distributions%2520limits%2520their%250Aability%2520to%2520synthesize%2520truly%2520novel%252C%2520out-of-distribution%2520concepts.%2520Existing%250Amethods%2520typically%2520enhance%2520creativity%2520by%2520combining%2520pairs%2520of%2520known%2520concepts%252C%250Ayielding%2520compositions%2520that%252C%2520while%2520out-of-distribution%252C%2520remain%2520linguistically%250Adescribable%2520and%2520bounded%2520within%2520the%2520existing%2520semantic%2520space.%2520Inspired%2520by%2520the%250Asoft%2520probabilistic%2520outputs%2520of%2520classifiers%2520on%2520ambiguous%2520inputs%252C%2520we%2520propose%250ADistribution-Conditional%2520Generation%252C%2520a%2520novel%2520formulation%2520that%2520models%2520creativity%250Aas%2520image%2520synthesis%2520conditioned%2520on%2520class%2520distributions%252C%2520enabling%2520semantically%250Aunconstrained%2520creative%2520generation.%2520Building%2520on%2520this%252C%2520we%2520propose%2520DisTok%252C%2520an%250Aencoder-decoder%2520framework%2520that%2520maps%2520class%2520distributions%2520into%2520a%2520latent%2520space%2520and%250Adecodes%2520them%2520into%2520tokens%2520of%2520creative%2520concept.%2520DisTok%2520maintains%2520a%2520dynamic%250Aconcept%2520pool%2520and%2520iteratively%2520sampling%2520and%2520fusing%2520concept%2520pairs%252C%2520enabling%2520the%250Ageneration%2520of%2520tokens%2520aligned%2520with%2520increasingly%2520complex%2520class%2520distributions.%2520To%250Aenforce%2520distributional%2520consistency%252C%2520latent%2520vectors%2520sampled%2520from%2520a%2520Gaussian%250Aprior%2520are%2520decoded%2520into%2520tokens%2520and%2520rendered%2520into%2520images%252C%2520whose%2520class%250Adistributions-predicted%2520by%2520a%2520vision-language%2520model-supervise%2520the%2520alignment%250Abetween%2520input%2520distributions%2520and%2520the%2520visual%2520semantics%2520of%2520generated%2520tokens.%2520The%250Aresulting%2520tokens%2520are%2520added%2520to%2520the%2520concept%2520pool%2520for%2520subsequent%2520composition.%250AExtensive%2520experiments%2520demonstrate%2520that%2520DisTok%252C%2520by%2520unifying%250Adistribution-conditioned%2520fusion%2520and%2520sampling-based%2520synthesis%252C%2520enables%2520efficient%250Aand%2520flexible%2520token-level%2520generation%252C%2520achieving%2520state-of-the-art%2520performance%250Awith%2520superior%2520text-image%2520alignment%2520and%2520human%2520preference%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-Conditional%20Generation%3A%20From%20Class%20Distribution%20to%20Creative%0A%20%20Generation&entry.906535625=Fu%20Feng%20and%20Yucheng%20Xie%20and%20Xu%20Yang%20and%20Jing%20Wang%20and%20Xin%20Geng&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20are%20effective%20at%20producing%20semantically%0Aaligned%20images%2C%20but%20their%20reliance%20on%20training%20data%20distributions%20limits%20their%0Aability%20to%20synthesize%20truly%20novel%2C%20out-of-distribution%20concepts.%20Existing%0Amethods%20typically%20enhance%20creativity%20by%20combining%20pairs%20of%20known%20concepts%2C%0Ayielding%20compositions%20that%2C%20while%20out-of-distribution%2C%20remain%20linguistically%0Adescribable%20and%20bounded%20within%20the%20existing%20semantic%20space.%20Inspired%20by%20the%0Asoft%20probabilistic%20outputs%20of%20classifiers%20on%20ambiguous%20inputs%2C%20we%20propose%0ADistribution-Conditional%20Generation%2C%20a%20novel%20formulation%20that%20models%20creativity%0Aas%20image%20synthesis%20conditioned%20on%20class%20distributions%2C%20enabling%20semantically%0Aunconstrained%20creative%20generation.%20Building%20on%20this%2C%20we%20propose%20DisTok%2C%20an%0Aencoder-decoder%20framework%20that%20maps%20class%20distributions%20into%20a%20latent%20space%20and%0Adecodes%20them%20into%20tokens%20of%20creative%20concept.%20DisTok%20maintains%20a%20dynamic%0Aconcept%20pool%20and%20iteratively%20sampling%20and%20fusing%20concept%20pairs%2C%20enabling%20the%0Ageneration%20of%20tokens%20aligned%20with%20increasingly%20complex%20class%20distributions.%20To%0Aenforce%20distributional%20consistency%2C%20latent%20vectors%20sampled%20from%20a%20Gaussian%0Aprior%20are%20decoded%20into%20tokens%20and%20rendered%20into%20images%2C%20whose%20class%0Adistributions-predicted%20by%20a%20vision-language%20model-supervise%20the%20alignment%0Abetween%20input%20distributions%20and%20the%20visual%20semantics%20of%20generated%20tokens.%20The%0Aresulting%20tokens%20are%20added%20to%20the%20concept%20pool%20for%20subsequent%20composition.%0AExtensive%20experiments%20demonstrate%20that%20DisTok%2C%20by%20unifying%0Adistribution-conditioned%20fusion%20and%20sampling-based%20synthesis%2C%20enables%20efficient%0Aand%20flexible%20token-level%20generation%2C%20achieving%20state-of-the-art%20performance%0Awith%20superior%20text-image%20alignment%20and%20human%20preference%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03667v1&entry.124074799=Read"},
{"title": "Matching Distance and Geometric Distribution Aided Learning Multiview\n  Point Cloud Registration", "author": "Shiqi Li and Jihua Zhu and Yifan Xie and Naiwen Hu and Di Wang", "abstract": "  Multiview point cloud registration plays a crucial role in robotics,\nautomation, and computer vision fields. This paper concentrates on pose graph\nconstruction and motion synchronization within multiview registration. Previous\nmethods for pose graph construction often pruned fully connected graphs or\nconstructed sparse graph using global feature aggregated from local\ndescriptors, which may not consistently yield reliable results. To identify\ndependable pairs for pose graph construction, we design a network model that\nextracts information from the matching distance between point cloud pairs. For\nmotion synchronization, we propose another neural network model to calculate\nthe absolute pose in a data-driven manner, rather than optimizing inaccurate\nhandcrafted loss functions. Our model takes into account geometric distribution\ninformation and employs a modified attention mechanism to facilitate flexible\nand reliable feature interaction. Experimental results on diverse indoor and\noutdoor datasets confirm the effectiveness and generalizability of our\napproach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.\n", "link": "http://arxiv.org/abs/2505.03692v1", "date": "2025-05-06", "relevancy": 2.445, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6538}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5827}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matching%20Distance%20and%20Geometric%20Distribution%20Aided%20Learning%20Multiview%0A%20%20Point%20Cloud%20Registration&body=Title%3A%20Matching%20Distance%20and%20Geometric%20Distribution%20Aided%20Learning%20Multiview%0A%20%20Point%20Cloud%20Registration%0AAuthor%3A%20Shiqi%20Li%20and%20Jihua%20Zhu%20and%20Yifan%20Xie%20and%20Naiwen%20Hu%20and%20Di%20Wang%0AAbstract%3A%20%20%20Multiview%20point%20cloud%20registration%20plays%20a%20crucial%20role%20in%20robotics%2C%0Aautomation%2C%20and%20computer%20vision%20fields.%20This%20paper%20concentrates%20on%20pose%20graph%0Aconstruction%20and%20motion%20synchronization%20within%20multiview%20registration.%20Previous%0Amethods%20for%20pose%20graph%20construction%20often%20pruned%20fully%20connected%20graphs%20or%0Aconstructed%20sparse%20graph%20using%20global%20feature%20aggregated%20from%20local%0Adescriptors%2C%20which%20may%20not%20consistently%20yield%20reliable%20results.%20To%20identify%0Adependable%20pairs%20for%20pose%20graph%20construction%2C%20we%20design%20a%20network%20model%20that%0Aextracts%20information%20from%20the%20matching%20distance%20between%20point%20cloud%20pairs.%20For%0Amotion%20synchronization%2C%20we%20propose%20another%20neural%20network%20model%20to%20calculate%0Athe%20absolute%20pose%20in%20a%20data-driven%20manner%2C%20rather%20than%20optimizing%20inaccurate%0Ahandcrafted%20loss%20functions.%20Our%20model%20takes%20into%20account%20geometric%20distribution%0Ainformation%20and%20employs%20a%20modified%20attention%20mechanism%20to%20facilitate%20flexible%0Aand%20reliable%20feature%20interaction.%20Experimental%20results%20on%20diverse%20indoor%20and%0Aoutdoor%20datasets%20confirm%20the%20effectiveness%20and%20generalizability%20of%20our%0Aapproach.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/Shi-Qi-Li/MDGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatching%2520Distance%2520and%2520Geometric%2520Distribution%2520Aided%2520Learning%2520Multiview%250A%2520%2520Point%2520Cloud%2520Registration%26entry.906535625%3DShiqi%2520Li%2520and%2520Jihua%2520Zhu%2520and%2520Yifan%2520Xie%2520and%2520Naiwen%2520Hu%2520and%2520Di%2520Wang%26entry.1292438233%3D%2520%2520Multiview%2520point%2520cloud%2520registration%2520plays%2520a%2520crucial%2520role%2520in%2520robotics%252C%250Aautomation%252C%2520and%2520computer%2520vision%2520fields.%2520This%2520paper%2520concentrates%2520on%2520pose%2520graph%250Aconstruction%2520and%2520motion%2520synchronization%2520within%2520multiview%2520registration.%2520Previous%250Amethods%2520for%2520pose%2520graph%2520construction%2520often%2520pruned%2520fully%2520connected%2520graphs%2520or%250Aconstructed%2520sparse%2520graph%2520using%2520global%2520feature%2520aggregated%2520from%2520local%250Adescriptors%252C%2520which%2520may%2520not%2520consistently%2520yield%2520reliable%2520results.%2520To%2520identify%250Adependable%2520pairs%2520for%2520pose%2520graph%2520construction%252C%2520we%2520design%2520a%2520network%2520model%2520that%250Aextracts%2520information%2520from%2520the%2520matching%2520distance%2520between%2520point%2520cloud%2520pairs.%2520For%250Amotion%2520synchronization%252C%2520we%2520propose%2520another%2520neural%2520network%2520model%2520to%2520calculate%250Athe%2520absolute%2520pose%2520in%2520a%2520data-driven%2520manner%252C%2520rather%2520than%2520optimizing%2520inaccurate%250Ahandcrafted%2520loss%2520functions.%2520Our%2520model%2520takes%2520into%2520account%2520geometric%2520distribution%250Ainformation%2520and%2520employs%2520a%2520modified%2520attention%2520mechanism%2520to%2520facilitate%2520flexible%250Aand%2520reliable%2520feature%2520interaction.%2520Experimental%2520results%2520on%2520diverse%2520indoor%2520and%250Aoutdoor%2520datasets%2520confirm%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520our%250Aapproach.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/Shi-Qi-Li/MDGD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%20Distance%20and%20Geometric%20Distribution%20Aided%20Learning%20Multiview%0A%20%20Point%20Cloud%20Registration&entry.906535625=Shiqi%20Li%20and%20Jihua%20Zhu%20and%20Yifan%20Xie%20and%20Naiwen%20Hu%20and%20Di%20Wang&entry.1292438233=%20%20Multiview%20point%20cloud%20registration%20plays%20a%20crucial%20role%20in%20robotics%2C%0Aautomation%2C%20and%20computer%20vision%20fields.%20This%20paper%20concentrates%20on%20pose%20graph%0Aconstruction%20and%20motion%20synchronization%20within%20multiview%20registration.%20Previous%0Amethods%20for%20pose%20graph%20construction%20often%20pruned%20fully%20connected%20graphs%20or%0Aconstructed%20sparse%20graph%20using%20global%20feature%20aggregated%20from%20local%0Adescriptors%2C%20which%20may%20not%20consistently%20yield%20reliable%20results.%20To%20identify%0Adependable%20pairs%20for%20pose%20graph%20construction%2C%20we%20design%20a%20network%20model%20that%0Aextracts%20information%20from%20the%20matching%20distance%20between%20point%20cloud%20pairs.%20For%0Amotion%20synchronization%2C%20we%20propose%20another%20neural%20network%20model%20to%20calculate%0Athe%20absolute%20pose%20in%20a%20data-driven%20manner%2C%20rather%20than%20optimizing%20inaccurate%0Ahandcrafted%20loss%20functions.%20Our%20model%20takes%20into%20account%20geometric%20distribution%0Ainformation%20and%20employs%20a%20modified%20attention%20mechanism%20to%20facilitate%20flexible%0Aand%20reliable%20feature%20interaction.%20Experimental%20results%20on%20diverse%20indoor%20and%0Aoutdoor%20datasets%20confirm%20the%20effectiveness%20and%20generalizability%20of%20our%0Aapproach.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/Shi-Qi-Li/MDGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03692v1&entry.124074799=Read"},
{"title": "Fixed-Length Dense Fingerprint Representation", "author": "Zhiyu Pan and Xiongjun Guan and Yongjie Duan and Jianjiang Feng and Jie Zhou", "abstract": "  Fixed-length fingerprint representations, which map each fingerprint to a\ncompact and fixed-size feature vector, are computationally efficient and\nwell-suited for large-scale matching. However, designing a robust\nrepresentation that effectively handles diverse fingerprint modalities, pose\nvariations, and noise interference remains a significant challenge. In this\nwork, we propose a fixed-length dense descriptor of fingerprints, and introduce\nFLARE-a fingerprint matching framework that integrates the Fixed-Length dense\ndescriptor with pose-based Alignment and Robust Enhancement. This fixed-length\nrepresentation employs a three-dimensional dense descriptor to effectively\ncapture spatial relationships among fingerprint ridge structures, enabling\nrobust and locally discriminative representations. To ensure consistency within\nthis dense feature space, FLARE incorporates pose-based alignment using\ncomplementary estimation methods, along with dual enhancement strategies that\nrefine ridge clarity while preserving the original fingerprint modality. The\nproposed dense descriptor supports fixed-length representation while\nmaintaining spatial correspondence, enabling fast and accurate similarity\ncomputation. Extensive experiments demonstrate that FLARE achieves superior\nperformance across rolled, plain, latent, and contactless fingerprints,\nsignificantly outperforming existing methods in cross-modality and low-quality\nscenarios. Further analysis validates the effectiveness of the dense descriptor\ndesign, as well as the impact of alignment and enhancement modules on the\naccuracy of dense descriptor matching. Experimental results highlight the\neffectiveness and generalizability of FLARE as a unified and scalable solution\nfor robust fingerprint representation and matching. The implementation and code\nwill be publicly available at https://github.com/Yu-Yy/FLARE.\n", "link": "http://arxiv.org/abs/2505.03597v1", "date": "2025-05-06", "relevancy": 2.4434, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5166}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4809}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fixed-Length%20Dense%20Fingerprint%20Representation&body=Title%3A%20Fixed-Length%20Dense%20Fingerprint%20Representation%0AAuthor%3A%20Zhiyu%20Pan%20and%20Xiongjun%20Guan%20and%20Yongjie%20Duan%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Fixed-length%20fingerprint%20representations%2C%20which%20map%20each%20fingerprint%20to%20a%0Acompact%20and%20fixed-size%20feature%20vector%2C%20are%20computationally%20efficient%20and%0Awell-suited%20for%20large-scale%20matching.%20However%2C%20designing%20a%20robust%0Arepresentation%20that%20effectively%20handles%20diverse%20fingerprint%20modalities%2C%20pose%0Avariations%2C%20and%20noise%20interference%20remains%20a%20significant%20challenge.%20In%20this%0Awork%2C%20we%20propose%20a%20fixed-length%20dense%20descriptor%20of%20fingerprints%2C%20and%20introduce%0AFLARE-a%20fingerprint%20matching%20framework%20that%20integrates%20the%20Fixed-Length%20dense%0Adescriptor%20with%20pose-based%20Alignment%20and%20Robust%20Enhancement.%20This%20fixed-length%0Arepresentation%20employs%20a%20three-dimensional%20dense%20descriptor%20to%20effectively%0Acapture%20spatial%20relationships%20among%20fingerprint%20ridge%20structures%2C%20enabling%0Arobust%20and%20locally%20discriminative%20representations.%20To%20ensure%20consistency%20within%0Athis%20dense%20feature%20space%2C%20FLARE%20incorporates%20pose-based%20alignment%20using%0Acomplementary%20estimation%20methods%2C%20along%20with%20dual%20enhancement%20strategies%20that%0Arefine%20ridge%20clarity%20while%20preserving%20the%20original%20fingerprint%20modality.%20The%0Aproposed%20dense%20descriptor%20supports%20fixed-length%20representation%20while%0Amaintaining%20spatial%20correspondence%2C%20enabling%20fast%20and%20accurate%20similarity%0Acomputation.%20Extensive%20experiments%20demonstrate%20that%20FLARE%20achieves%20superior%0Aperformance%20across%20rolled%2C%20plain%2C%20latent%2C%20and%20contactless%20fingerprints%2C%0Asignificantly%20outperforming%20existing%20methods%20in%20cross-modality%20and%20low-quality%0Ascenarios.%20Further%20analysis%20validates%20the%20effectiveness%20of%20the%20dense%20descriptor%0Adesign%2C%20as%20well%20as%20the%20impact%20of%20alignment%20and%20enhancement%20modules%20on%20the%0Aaccuracy%20of%20dense%20descriptor%20matching.%20Experimental%20results%20highlight%20the%0Aeffectiveness%20and%20generalizability%20of%20FLARE%20as%20a%20unified%20and%20scalable%20solution%0Afor%20robust%20fingerprint%20representation%20and%20matching.%20The%20implementation%20and%20code%0Awill%20be%20publicly%20available%20at%20https%3A//github.com/Yu-Yy/FLARE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFixed-Length%2520Dense%2520Fingerprint%2520Representation%26entry.906535625%3DZhiyu%2520Pan%2520and%2520Xiongjun%2520Guan%2520and%2520Yongjie%2520Duan%2520and%2520Jianjiang%2520Feng%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Fixed-length%2520fingerprint%2520representations%252C%2520which%2520map%2520each%2520fingerprint%2520to%2520a%250Acompact%2520and%2520fixed-size%2520feature%2520vector%252C%2520are%2520computationally%2520efficient%2520and%250Awell-suited%2520for%2520large-scale%2520matching.%2520However%252C%2520designing%2520a%2520robust%250Arepresentation%2520that%2520effectively%2520handles%2520diverse%2520fingerprint%2520modalities%252C%2520pose%250Avariations%252C%2520and%2520noise%2520interference%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520fixed-length%2520dense%2520descriptor%2520of%2520fingerprints%252C%2520and%2520introduce%250AFLARE-a%2520fingerprint%2520matching%2520framework%2520that%2520integrates%2520the%2520Fixed-Length%2520dense%250Adescriptor%2520with%2520pose-based%2520Alignment%2520and%2520Robust%2520Enhancement.%2520This%2520fixed-length%250Arepresentation%2520employs%2520a%2520three-dimensional%2520dense%2520descriptor%2520to%2520effectively%250Acapture%2520spatial%2520relationships%2520among%2520fingerprint%2520ridge%2520structures%252C%2520enabling%250Arobust%2520and%2520locally%2520discriminative%2520representations.%2520To%2520ensure%2520consistency%2520within%250Athis%2520dense%2520feature%2520space%252C%2520FLARE%2520incorporates%2520pose-based%2520alignment%2520using%250Acomplementary%2520estimation%2520methods%252C%2520along%2520with%2520dual%2520enhancement%2520strategies%2520that%250Arefine%2520ridge%2520clarity%2520while%2520preserving%2520the%2520original%2520fingerprint%2520modality.%2520The%250Aproposed%2520dense%2520descriptor%2520supports%2520fixed-length%2520representation%2520while%250Amaintaining%2520spatial%2520correspondence%252C%2520enabling%2520fast%2520and%2520accurate%2520similarity%250Acomputation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FLARE%2520achieves%2520superior%250Aperformance%2520across%2520rolled%252C%2520plain%252C%2520latent%252C%2520and%2520contactless%2520fingerprints%252C%250Asignificantly%2520outperforming%2520existing%2520methods%2520in%2520cross-modality%2520and%2520low-quality%250Ascenarios.%2520Further%2520analysis%2520validates%2520the%2520effectiveness%2520of%2520the%2520dense%2520descriptor%250Adesign%252C%2520as%2520well%2520as%2520the%2520impact%2520of%2520alignment%2520and%2520enhancement%2520modules%2520on%2520the%250Aaccuracy%2520of%2520dense%2520descriptor%2520matching.%2520Experimental%2520results%2520highlight%2520the%250Aeffectiveness%2520and%2520generalizability%2520of%2520FLARE%2520as%2520a%2520unified%2520and%2520scalable%2520solution%250Afor%2520robust%2520fingerprint%2520representation%2520and%2520matching.%2520The%2520implementation%2520and%2520code%250Awill%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/Yu-Yy/FLARE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fixed-Length%20Dense%20Fingerprint%20Representation&entry.906535625=Zhiyu%20Pan%20and%20Xiongjun%20Guan%20and%20Yongjie%20Duan%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou&entry.1292438233=%20%20Fixed-length%20fingerprint%20representations%2C%20which%20map%20each%20fingerprint%20to%20a%0Acompact%20and%20fixed-size%20feature%20vector%2C%20are%20computationally%20efficient%20and%0Awell-suited%20for%20large-scale%20matching.%20However%2C%20designing%20a%20robust%0Arepresentation%20that%20effectively%20handles%20diverse%20fingerprint%20modalities%2C%20pose%0Avariations%2C%20and%20noise%20interference%20remains%20a%20significant%20challenge.%20In%20this%0Awork%2C%20we%20propose%20a%20fixed-length%20dense%20descriptor%20of%20fingerprints%2C%20and%20introduce%0AFLARE-a%20fingerprint%20matching%20framework%20that%20integrates%20the%20Fixed-Length%20dense%0Adescriptor%20with%20pose-based%20Alignment%20and%20Robust%20Enhancement.%20This%20fixed-length%0Arepresentation%20employs%20a%20three-dimensional%20dense%20descriptor%20to%20effectively%0Acapture%20spatial%20relationships%20among%20fingerprint%20ridge%20structures%2C%20enabling%0Arobust%20and%20locally%20discriminative%20representations.%20To%20ensure%20consistency%20within%0Athis%20dense%20feature%20space%2C%20FLARE%20incorporates%20pose-based%20alignment%20using%0Acomplementary%20estimation%20methods%2C%20along%20with%20dual%20enhancement%20strategies%20that%0Arefine%20ridge%20clarity%20while%20preserving%20the%20original%20fingerprint%20modality.%20The%0Aproposed%20dense%20descriptor%20supports%20fixed-length%20representation%20while%0Amaintaining%20spatial%20correspondence%2C%20enabling%20fast%20and%20accurate%20similarity%0Acomputation.%20Extensive%20experiments%20demonstrate%20that%20FLARE%20achieves%20superior%0Aperformance%20across%20rolled%2C%20plain%2C%20latent%2C%20and%20contactless%20fingerprints%2C%0Asignificantly%20outperforming%20existing%20methods%20in%20cross-modality%20and%20low-quality%0Ascenarios.%20Further%20analysis%20validates%20the%20effectiveness%20of%20the%20dense%20descriptor%0Adesign%2C%20as%20well%20as%20the%20impact%20of%20alignment%20and%20enhancement%20modules%20on%20the%0Aaccuracy%20of%20dense%20descriptor%20matching.%20Experimental%20results%20highlight%20the%0Aeffectiveness%20and%20generalizability%20of%20FLARE%20as%20a%20unified%20and%20scalable%20solution%0Afor%20robust%20fingerprint%20representation%20and%20matching.%20The%20implementation%20and%20code%0Awill%20be%20publicly%20available%20at%20https%3A//github.com/Yu-Yy/FLARE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03597v1&entry.124074799=Read"},
{"title": "HINT: Hypernetwork Approach to Training Weight Interval Regions in\n  Continual Learning", "author": "Patryk Krukowski and Anna Bielawska and Kamil Ksi\u0105\u017cek and Pawe\u0142 Wawrzy\u0144ski and Pawe\u0142 Batorski and Przemys\u0142aw Spurek", "abstract": "  Recently, a new Continual Learning (CL) paradigm was presented to control\ncatastrophic forgetting, called Interval Continual Learning (InterContiNet),\nwhich relies on enforcing interval constraints on the neural network parameter\nspace. Unfortunately, InterContiNet training is challenging due to the high\ndimensionality of the weight space, making intervals difficult to manage. To\naddress this issue, we introduce HINT, a technique that employs interval\narithmetic within the embedding space and utilizes a hypernetwork to map these\nintervals to the target network parameter space. We train interval embeddings\nfor consecutive tasks and train a hypernetwork to transform these embeddings\ninto weights of the target network. An embedding for a given task is trained\nalong with the hypernetwork, preserving the response of the target network for\nthe previous task embeddings. Interval arithmetic works with a more manageable,\nlower-dimensional embedding space rather than directly preparing intervals in a\nhigh-dimensional weight space. Our model allows faster and more efficient\ntraining. Furthermore, HINT maintains the guarantee of not forgetting. At the\nend of training, we can choose one universal embedding to produce a single\nnetwork dedicated to all tasks. In such a framework, hypernetwork is used only\nfor training and, finally, we can utilize one set of weights. HINT obtains\nsignificantly better results than InterContiNet and gives SOTA results on\nseveral benchmarks.\n", "link": "http://arxiv.org/abs/2405.15444v4", "date": "2025-05-06", "relevancy": 2.4397, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5011}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HINT%3A%20Hypernetwork%20Approach%20to%20Training%20Weight%20Interval%20Regions%20in%0A%20%20Continual%20Learning&body=Title%3A%20HINT%3A%20Hypernetwork%20Approach%20to%20Training%20Weight%20Interval%20Regions%20in%0A%20%20Continual%20Learning%0AAuthor%3A%20Patryk%20Krukowski%20and%20Anna%20Bielawska%20and%20Kamil%20Ksi%C4%85%C5%BCek%20and%20Pawe%C5%82%20Wawrzy%C5%84ski%20and%20Pawe%C5%82%20Batorski%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Recently%2C%20a%20new%20Continual%20Learning%20%28CL%29%20paradigm%20was%20presented%20to%20control%0Acatastrophic%20forgetting%2C%20called%20Interval%20Continual%20Learning%20%28InterContiNet%29%2C%0Awhich%20relies%20on%20enforcing%20interval%20constraints%20on%20the%20neural%20network%20parameter%0Aspace.%20Unfortunately%2C%20InterContiNet%20training%20is%20challenging%20due%20to%20the%20high%0Adimensionality%20of%20the%20weight%20space%2C%20making%20intervals%20difficult%20to%20manage.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20HINT%2C%20a%20technique%20that%20employs%20interval%0Aarithmetic%20within%20the%20embedding%20space%20and%20utilizes%20a%20hypernetwork%20to%20map%20these%0Aintervals%20to%20the%20target%20network%20parameter%20space.%20We%20train%20interval%20embeddings%0Afor%20consecutive%20tasks%20and%20train%20a%20hypernetwork%20to%20transform%20these%20embeddings%0Ainto%20weights%20of%20the%20target%20network.%20An%20embedding%20for%20a%20given%20task%20is%20trained%0Aalong%20with%20the%20hypernetwork%2C%20preserving%20the%20response%20of%20the%20target%20network%20for%0Athe%20previous%20task%20embeddings.%20Interval%20arithmetic%20works%20with%20a%20more%20manageable%2C%0Alower-dimensional%20embedding%20space%20rather%20than%20directly%20preparing%20intervals%20in%20a%0Ahigh-dimensional%20weight%20space.%20Our%20model%20allows%20faster%20and%20more%20efficient%0Atraining.%20Furthermore%2C%20HINT%20maintains%20the%20guarantee%20of%20not%20forgetting.%20At%20the%0Aend%20of%20training%2C%20we%20can%20choose%20one%20universal%20embedding%20to%20produce%20a%20single%0Anetwork%20dedicated%20to%20all%20tasks.%20In%20such%20a%20framework%2C%20hypernetwork%20is%20used%20only%0Afor%20training%20and%2C%20finally%2C%20we%20can%20utilize%20one%20set%20of%20weights.%20HINT%20obtains%0Asignificantly%20better%20results%20than%20InterContiNet%20and%20gives%20SOTA%20results%20on%0Aseveral%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15444v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHINT%253A%2520Hypernetwork%2520Approach%2520to%2520Training%2520Weight%2520Interval%2520Regions%2520in%250A%2520%2520Continual%2520Learning%26entry.906535625%3DPatryk%2520Krukowski%2520and%2520Anna%2520Bielawska%2520and%2520Kamil%2520Ksi%25C4%2585%25C5%25BCek%2520and%2520Pawe%25C5%2582%2520Wawrzy%25C5%2584ski%2520and%2520Pawe%25C5%2582%2520Batorski%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520new%2520Continual%2520Learning%2520%2528CL%2529%2520paradigm%2520was%2520presented%2520to%2520control%250Acatastrophic%2520forgetting%252C%2520called%2520Interval%2520Continual%2520Learning%2520%2528InterContiNet%2529%252C%250Awhich%2520relies%2520on%2520enforcing%2520interval%2520constraints%2520on%2520the%2520neural%2520network%2520parameter%250Aspace.%2520Unfortunately%252C%2520InterContiNet%2520training%2520is%2520challenging%2520due%2520to%2520the%2520high%250Adimensionality%2520of%2520the%2520weight%2520space%252C%2520making%2520intervals%2520difficult%2520to%2520manage.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520introduce%2520HINT%252C%2520a%2520technique%2520that%2520employs%2520interval%250Aarithmetic%2520within%2520the%2520embedding%2520space%2520and%2520utilizes%2520a%2520hypernetwork%2520to%2520map%2520these%250Aintervals%2520to%2520the%2520target%2520network%2520parameter%2520space.%2520We%2520train%2520interval%2520embeddings%250Afor%2520consecutive%2520tasks%2520and%2520train%2520a%2520hypernetwork%2520to%2520transform%2520these%2520embeddings%250Ainto%2520weights%2520of%2520the%2520target%2520network.%2520An%2520embedding%2520for%2520a%2520given%2520task%2520is%2520trained%250Aalong%2520with%2520the%2520hypernetwork%252C%2520preserving%2520the%2520response%2520of%2520the%2520target%2520network%2520for%250Athe%2520previous%2520task%2520embeddings.%2520Interval%2520arithmetic%2520works%2520with%2520a%2520more%2520manageable%252C%250Alower-dimensional%2520embedding%2520space%2520rather%2520than%2520directly%2520preparing%2520intervals%2520in%2520a%250Ahigh-dimensional%2520weight%2520space.%2520Our%2520model%2520allows%2520faster%2520and%2520more%2520efficient%250Atraining.%2520Furthermore%252C%2520HINT%2520maintains%2520the%2520guarantee%2520of%2520not%2520forgetting.%2520At%2520the%250Aend%2520of%2520training%252C%2520we%2520can%2520choose%2520one%2520universal%2520embedding%2520to%2520produce%2520a%2520single%250Anetwork%2520dedicated%2520to%2520all%2520tasks.%2520In%2520such%2520a%2520framework%252C%2520hypernetwork%2520is%2520used%2520only%250Afor%2520training%2520and%252C%2520finally%252C%2520we%2520can%2520utilize%2520one%2520set%2520of%2520weights.%2520HINT%2520obtains%250Asignificantly%2520better%2520results%2520than%2520InterContiNet%2520and%2520gives%2520SOTA%2520results%2520on%250Aseveral%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15444v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HINT%3A%20Hypernetwork%20Approach%20to%20Training%20Weight%20Interval%20Regions%20in%0A%20%20Continual%20Learning&entry.906535625=Patryk%20Krukowski%20and%20Anna%20Bielawska%20and%20Kamil%20Ksi%C4%85%C5%BCek%20and%20Pawe%C5%82%20Wawrzy%C5%84ski%20and%20Pawe%C5%82%20Batorski%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Recently%2C%20a%20new%20Continual%20Learning%20%28CL%29%20paradigm%20was%20presented%20to%20control%0Acatastrophic%20forgetting%2C%20called%20Interval%20Continual%20Learning%20%28InterContiNet%29%2C%0Awhich%20relies%20on%20enforcing%20interval%20constraints%20on%20the%20neural%20network%20parameter%0Aspace.%20Unfortunately%2C%20InterContiNet%20training%20is%20challenging%20due%20to%20the%20high%0Adimensionality%20of%20the%20weight%20space%2C%20making%20intervals%20difficult%20to%20manage.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20HINT%2C%20a%20technique%20that%20employs%20interval%0Aarithmetic%20within%20the%20embedding%20space%20and%20utilizes%20a%20hypernetwork%20to%20map%20these%0Aintervals%20to%20the%20target%20network%20parameter%20space.%20We%20train%20interval%20embeddings%0Afor%20consecutive%20tasks%20and%20train%20a%20hypernetwork%20to%20transform%20these%20embeddings%0Ainto%20weights%20of%20the%20target%20network.%20An%20embedding%20for%20a%20given%20task%20is%20trained%0Aalong%20with%20the%20hypernetwork%2C%20preserving%20the%20response%20of%20the%20target%20network%20for%0Athe%20previous%20task%20embeddings.%20Interval%20arithmetic%20works%20with%20a%20more%20manageable%2C%0Alower-dimensional%20embedding%20space%20rather%20than%20directly%20preparing%20intervals%20in%20a%0Ahigh-dimensional%20weight%20space.%20Our%20model%20allows%20faster%20and%20more%20efficient%0Atraining.%20Furthermore%2C%20HINT%20maintains%20the%20guarantee%20of%20not%20forgetting.%20At%20the%0Aend%20of%20training%2C%20we%20can%20choose%20one%20universal%20embedding%20to%20produce%20a%20single%0Anetwork%20dedicated%20to%20all%20tasks.%20In%20such%20a%20framework%2C%20hypernetwork%20is%20used%20only%0Afor%20training%20and%2C%20finally%2C%20we%20can%20utilize%20one%20set%20of%20weights.%20HINT%20obtains%0Asignificantly%20better%20results%20than%20InterContiNet%20and%20gives%20SOTA%20results%20on%0Aseveral%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15444v4&entry.124074799=Read"},
{"title": "Generating Synthetic Data via Augmentations for Improved Facial\n  Resemblance in DreamBooth and InstantID", "author": "Koray Ulusan and Benjamin Kiefer", "abstract": "  The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.\n", "link": "http://arxiv.org/abs/2505.03557v1", "date": "2025-05-06", "relevancy": 2.436, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6473}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6064}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Synthetic%20Data%20via%20Augmentations%20for%20Improved%20Facial%0A%20%20Resemblance%20in%20DreamBooth%20and%20InstantID&body=Title%3A%20Generating%20Synthetic%20Data%20via%20Augmentations%20for%20Improved%20Facial%0A%20%20Resemblance%20in%20DreamBooth%20and%20InstantID%0AAuthor%3A%20Koray%20Ulusan%20and%20Benjamin%20Kiefer%0AAbstract%3A%20%20%20The%20personalization%20of%20Stable%20Diffusion%20for%20generating%20professional%20portraits%0Afrom%20amateur%20photographs%20is%20a%20burgeoning%20area%2C%20with%20applications%20in%20various%0Adownstream%20contexts.%20This%20paper%20investigates%20the%20impact%20of%20augmentations%20on%0Aimproving%20facial%20resemblance%20when%20using%20two%20prominent%20personalization%0Atechniques%3A%20DreamBooth%20and%20InstantID.%20Through%20a%20series%20of%20experiments%20with%0Adiverse%20subject%20datasets%2C%20we%20assessed%20the%20effectiveness%20of%20various%20augmentation%0Astrategies%20on%20the%20generated%20headshots%27%20fidelity%20to%20the%20original%20subject.%20We%0Aintroduce%20FaceDistance%2C%20a%20wrapper%20around%20FaceNet%2C%20to%20rank%20the%20generations%20based%0Aon%20facial%20similarity%2C%20which%20aided%20in%20our%20assessment.%20Ultimately%2C%20this%20research%0Aprovides%20insights%20into%20the%20role%20of%20augmentations%20in%20enhancing%20facial%0Aresemblance%20in%20SDXL-generated%20portraits%2C%20informing%20strategies%20for%20their%0Aeffective%20deployment%20in%20downstream%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Synthetic%2520Data%2520via%2520Augmentations%2520for%2520Improved%2520Facial%250A%2520%2520Resemblance%2520in%2520DreamBooth%2520and%2520InstantID%26entry.906535625%3DKoray%2520Ulusan%2520and%2520Benjamin%2520Kiefer%26entry.1292438233%3D%2520%2520The%2520personalization%2520of%2520Stable%2520Diffusion%2520for%2520generating%2520professional%2520portraits%250Afrom%2520amateur%2520photographs%2520is%2520a%2520burgeoning%2520area%252C%2520with%2520applications%2520in%2520various%250Adownstream%2520contexts.%2520This%2520paper%2520investigates%2520the%2520impact%2520of%2520augmentations%2520on%250Aimproving%2520facial%2520resemblance%2520when%2520using%2520two%2520prominent%2520personalization%250Atechniques%253A%2520DreamBooth%2520and%2520InstantID.%2520Through%2520a%2520series%2520of%2520experiments%2520with%250Adiverse%2520subject%2520datasets%252C%2520we%2520assessed%2520the%2520effectiveness%2520of%2520various%2520augmentation%250Astrategies%2520on%2520the%2520generated%2520headshots%2527%2520fidelity%2520to%2520the%2520original%2520subject.%2520We%250Aintroduce%2520FaceDistance%252C%2520a%2520wrapper%2520around%2520FaceNet%252C%2520to%2520rank%2520the%2520generations%2520based%250Aon%2520facial%2520similarity%252C%2520which%2520aided%2520in%2520our%2520assessment.%2520Ultimately%252C%2520this%2520research%250Aprovides%2520insights%2520into%2520the%2520role%2520of%2520augmentations%2520in%2520enhancing%2520facial%250Aresemblance%2520in%2520SDXL-generated%2520portraits%252C%2520informing%2520strategies%2520for%2520their%250Aeffective%2520deployment%2520in%2520downstream%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Synthetic%20Data%20via%20Augmentations%20for%20Improved%20Facial%0A%20%20Resemblance%20in%20DreamBooth%20and%20InstantID&entry.906535625=Koray%20Ulusan%20and%20Benjamin%20Kiefer&entry.1292438233=%20%20The%20personalization%20of%20Stable%20Diffusion%20for%20generating%20professional%20portraits%0Afrom%20amateur%20photographs%20is%20a%20burgeoning%20area%2C%20with%20applications%20in%20various%0Adownstream%20contexts.%20This%20paper%20investigates%20the%20impact%20of%20augmentations%20on%0Aimproving%20facial%20resemblance%20when%20using%20two%20prominent%20personalization%0Atechniques%3A%20DreamBooth%20and%20InstantID.%20Through%20a%20series%20of%20experiments%20with%0Adiverse%20subject%20datasets%2C%20we%20assessed%20the%20effectiveness%20of%20various%20augmentation%0Astrategies%20on%20the%20generated%20headshots%27%20fidelity%20to%20the%20original%20subject.%20We%0Aintroduce%20FaceDistance%2C%20a%20wrapper%20around%20FaceNet%2C%20to%20rank%20the%20generations%20based%0Aon%20facial%20similarity%2C%20which%20aided%20in%20our%20assessment.%20Ultimately%2C%20this%20research%0Aprovides%20insights%20into%20the%20role%20of%20augmentations%20in%20enhancing%20facial%0Aresemblance%20in%20SDXL-generated%20portraits%2C%20informing%20strategies%20for%20their%0Aeffective%20deployment%20in%20downstream%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03557v1&entry.124074799=Read"},
{"title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios", "author": "Shiyi Zhang and Junhao Zhuang and Zhaoyang Zhang and Ying Shan and Yansong Tang", "abstract": "  Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/\n", "link": "http://arxiv.org/abs/2505.03730v1", "date": "2025-05-06", "relevancy": 2.432, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6343}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6177}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexiAct%3A%20Towards%20Flexible%20Action%20Control%20in%20Heterogeneous%20Scenarios&body=Title%3A%20FlexiAct%3A%20Towards%20Flexible%20Action%20Control%20in%20Heterogeneous%20Scenarios%0AAuthor%3A%20Shiyi%20Zhang%20and%20Junhao%20Zhuang%20and%20Zhaoyang%20Zhang%20and%20Ying%20Shan%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20Action%20customization%20involves%20generating%20videos%20where%20the%20subject%20performs%0Aactions%20dictated%20by%20input%20control%20signals.%20Current%20methods%20use%20pose-guided%20or%0Aglobal%20motion%20customization%20but%20are%20limited%20by%20strict%20constraints%20on%20spatial%0Astructure%2C%20such%20as%20layout%2C%20skeleton%2C%20and%20viewpoint%20consistency%2C%20reducing%0Aadaptability%20across%20diverse%20subjects%20and%20scenarios.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20FlexiAct%2C%20which%20transfers%20actions%20from%20a%20reference%0Avideo%20to%20an%20arbitrary%20target%20image.%20Unlike%20existing%20methods%2C%20FlexiAct%20allows%0Afor%20variations%20in%20layout%2C%20viewpoint%2C%20and%20skeletal%20structure%20between%20the%20subject%0Aof%20the%20reference%20video%20and%20the%20target%20image%2C%20while%20maintaining%20identity%0Aconsistency.%20Achieving%20this%20requires%20precise%20action%20control%2C%20spatial%20structure%0Aadaptation%2C%20and%20consistency%20preservation.%20To%20this%20end%2C%20we%20introduce%20RefAdapter%2C%0Aa%20lightweight%20image-conditioned%20adapter%20that%20excels%20in%20spatial%20adaptation%20and%0Aconsistency%20preservation%2C%20surpassing%20existing%20methods%20in%20balancing%20appearance%0Aconsistency%20and%20structural%20flexibility.%20Additionally%2C%20based%20on%20our%0Aobservations%2C%20the%20denoising%20process%20exhibits%20varying%20levels%20of%20attention%20to%0Amotion%20%28low%20frequency%29%20and%20appearance%20details%20%28high%20frequency%29%20at%20different%0Atimesteps.%20So%20we%20propose%20FAE%20%28Frequency-aware%20Action%20Extraction%29%2C%20which%2C%20unlike%0Aexisting%20methods%20that%20rely%20on%20separate%20spatial-temporal%20architectures%2C%20directly%0Aachieves%20action%20extraction%20during%20the%20denoising%20process.%20Experiments%0Ademonstrate%20that%20our%20method%20effectively%20transfers%20actions%20to%20subjects%20with%0Adiverse%20layouts%2C%20skeletons%2C%20and%20viewpoints.%20We%20release%20our%20code%20and%20model%0Aweights%20to%20support%20further%20research%20at%0Ahttps%3A//shiyi-zh0408.github.io/projectpages/FlexiAct/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexiAct%253A%2520Towards%2520Flexible%2520Action%2520Control%2520in%2520Heterogeneous%2520Scenarios%26entry.906535625%3DShiyi%2520Zhang%2520and%2520Junhao%2520Zhuang%2520and%2520Zhaoyang%2520Zhang%2520and%2520Ying%2520Shan%2520and%2520Yansong%2520Tang%26entry.1292438233%3D%2520%2520Action%2520customization%2520involves%2520generating%2520videos%2520where%2520the%2520subject%2520performs%250Aactions%2520dictated%2520by%2520input%2520control%2520signals.%2520Current%2520methods%2520use%2520pose-guided%2520or%250Aglobal%2520motion%2520customization%2520but%2520are%2520limited%2520by%2520strict%2520constraints%2520on%2520spatial%250Astructure%252C%2520such%2520as%2520layout%252C%2520skeleton%252C%2520and%2520viewpoint%2520consistency%252C%2520reducing%250Aadaptability%2520across%2520diverse%2520subjects%2520and%2520scenarios.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520FlexiAct%252C%2520which%2520transfers%2520actions%2520from%2520a%2520reference%250Avideo%2520to%2520an%2520arbitrary%2520target%2520image.%2520Unlike%2520existing%2520methods%252C%2520FlexiAct%2520allows%250Afor%2520variations%2520in%2520layout%252C%2520viewpoint%252C%2520and%2520skeletal%2520structure%2520between%2520the%2520subject%250Aof%2520the%2520reference%2520video%2520and%2520the%2520target%2520image%252C%2520while%2520maintaining%2520identity%250Aconsistency.%2520Achieving%2520this%2520requires%2520precise%2520action%2520control%252C%2520spatial%2520structure%250Aadaptation%252C%2520and%2520consistency%2520preservation.%2520To%2520this%2520end%252C%2520we%2520introduce%2520RefAdapter%252C%250Aa%2520lightweight%2520image-conditioned%2520adapter%2520that%2520excels%2520in%2520spatial%2520adaptation%2520and%250Aconsistency%2520preservation%252C%2520surpassing%2520existing%2520methods%2520in%2520balancing%2520appearance%250Aconsistency%2520and%2520structural%2520flexibility.%2520Additionally%252C%2520based%2520on%2520our%250Aobservations%252C%2520the%2520denoising%2520process%2520exhibits%2520varying%2520levels%2520of%2520attention%2520to%250Amotion%2520%2528low%2520frequency%2529%2520and%2520appearance%2520details%2520%2528high%2520frequency%2529%2520at%2520different%250Atimesteps.%2520So%2520we%2520propose%2520FAE%2520%2528Frequency-aware%2520Action%2520Extraction%2529%252C%2520which%252C%2520unlike%250Aexisting%2520methods%2520that%2520rely%2520on%2520separate%2520spatial-temporal%2520architectures%252C%2520directly%250Aachieves%2520action%2520extraction%2520during%2520the%2520denoising%2520process.%2520Experiments%250Ademonstrate%2520that%2520our%2520method%2520effectively%2520transfers%2520actions%2520to%2520subjects%2520with%250Adiverse%2520layouts%252C%2520skeletons%252C%2520and%2520viewpoints.%2520We%2520release%2520our%2520code%2520and%2520model%250Aweights%2520to%2520support%2520further%2520research%2520at%250Ahttps%253A//shiyi-zh0408.github.io/projectpages/FlexiAct/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiAct%3A%20Towards%20Flexible%20Action%20Control%20in%20Heterogeneous%20Scenarios&entry.906535625=Shiyi%20Zhang%20and%20Junhao%20Zhuang%20and%20Zhaoyang%20Zhang%20and%20Ying%20Shan%20and%20Yansong%20Tang&entry.1292438233=%20%20Action%20customization%20involves%20generating%20videos%20where%20the%20subject%20performs%0Aactions%20dictated%20by%20input%20control%20signals.%20Current%20methods%20use%20pose-guided%20or%0Aglobal%20motion%20customization%20but%20are%20limited%20by%20strict%20constraints%20on%20spatial%0Astructure%2C%20such%20as%20layout%2C%20skeleton%2C%20and%20viewpoint%20consistency%2C%20reducing%0Aadaptability%20across%20diverse%20subjects%20and%20scenarios.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20FlexiAct%2C%20which%20transfers%20actions%20from%20a%20reference%0Avideo%20to%20an%20arbitrary%20target%20image.%20Unlike%20existing%20methods%2C%20FlexiAct%20allows%0Afor%20variations%20in%20layout%2C%20viewpoint%2C%20and%20skeletal%20structure%20between%20the%20subject%0Aof%20the%20reference%20video%20and%20the%20target%20image%2C%20while%20maintaining%20identity%0Aconsistency.%20Achieving%20this%20requires%20precise%20action%20control%2C%20spatial%20structure%0Aadaptation%2C%20and%20consistency%20preservation.%20To%20this%20end%2C%20we%20introduce%20RefAdapter%2C%0Aa%20lightweight%20image-conditioned%20adapter%20that%20excels%20in%20spatial%20adaptation%20and%0Aconsistency%20preservation%2C%20surpassing%20existing%20methods%20in%20balancing%20appearance%0Aconsistency%20and%20structural%20flexibility.%20Additionally%2C%20based%20on%20our%0Aobservations%2C%20the%20denoising%20process%20exhibits%20varying%20levels%20of%20attention%20to%0Amotion%20%28low%20frequency%29%20and%20appearance%20details%20%28high%20frequency%29%20at%20different%0Atimesteps.%20So%20we%20propose%20FAE%20%28Frequency-aware%20Action%20Extraction%29%2C%20which%2C%20unlike%0Aexisting%20methods%20that%20rely%20on%20separate%20spatial-temporal%20architectures%2C%20directly%0Aachieves%20action%20extraction%20during%20the%20denoising%20process.%20Experiments%0Ademonstrate%20that%20our%20method%20effectively%20transfers%20actions%20to%20subjects%20with%0Adiverse%20layouts%2C%20skeletons%2C%20and%20viewpoints.%20We%20release%20our%20code%20and%20model%0Aweights%20to%20support%20further%20research%20at%0Ahttps%3A//shiyi-zh0408.github.io/projectpages/FlexiAct/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03730v1&entry.124074799=Read"},
{"title": "Visual Imitation Enables Contextual Humanoid Control", "author": "Arthur Allshire and Hongsuk Choi and Junyi Zhang and David McAllister and Anthony Zhang and Chung Min Kim and Trevor Darrell and Pieter Abbeel and Jitendra Malik and Angjoo Kanazawa", "abstract": "  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n", "link": "http://arxiv.org/abs/2505.03729v1", "date": "2025-05-06", "relevancy": 2.4311, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6172}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6154}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control&body=Title%3A%20Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control%0AAuthor%3A%20Arthur%20Allshire%20and%20Hongsuk%20Choi%20and%20Junyi%20Zhang%20and%20David%20McAllister%20and%20Anthony%20Zhang%20and%20Chung%20Min%20Kim%20and%20Trevor%20Darrell%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20How%20can%20we%20teach%20humanoids%20to%20climb%20staircases%20and%20sit%20on%20chairs%20using%20the%0Asurrounding%20environment%20context%3F%20Arguably%2C%20the%20simplest%20way%20is%20to%20just%20show%0Athem-casually%20capture%20a%20human%20motion%20video%20and%20feed%20it%20to%20humanoids.%20We%0Aintroduce%20VIDEOMIMIC%2C%20a%20real-to-sim-to-real%20pipeline%20that%20mines%20everyday%0Avideos%2C%20jointly%20reconstructs%20the%20humans%20and%20the%20environment%2C%20and%20produces%0Awhole-body%20control%20policies%20for%20humanoid%20robots%20that%20perform%20the%20corresponding%0Askills.%20We%20demonstrate%20the%20results%20of%20our%20pipeline%20on%20real%20humanoid%20robots%2C%0Ashowing%20robust%2C%20repeatable%20contextual%20control%20such%20as%20staircase%20ascents%20and%0Adescents%2C%20sitting%20and%20standing%20from%20chairs%20and%20benches%2C%20as%20well%20as%20other%0Adynamic%20whole-body%20skills-all%20from%20a%20single%20policy%2C%20conditioned%20on%20the%0Aenvironment%20and%20global%20root%20commands.%20VIDEOMIMIC%20offers%20a%20scalable%20path%20towards%0Ateaching%20humanoids%20to%20operate%20in%20diverse%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Imitation%2520Enables%2520Contextual%2520Humanoid%2520Control%26entry.906535625%3DArthur%2520Allshire%2520and%2520Hongsuk%2520Choi%2520and%2520Junyi%2520Zhang%2520and%2520David%2520McAllister%2520and%2520Anthony%2520Zhang%2520and%2520Chung%2520Min%2520Kim%2520and%2520Trevor%2520Darrell%2520and%2520Pieter%2520Abbeel%2520and%2520Jitendra%2520Malik%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520teach%2520humanoids%2520to%2520climb%2520staircases%2520and%2520sit%2520on%2520chairs%2520using%2520the%250Asurrounding%2520environment%2520context%253F%2520Arguably%252C%2520the%2520simplest%2520way%2520is%2520to%2520just%2520show%250Athem-casually%2520capture%2520a%2520human%2520motion%2520video%2520and%2520feed%2520it%2520to%2520humanoids.%2520We%250Aintroduce%2520VIDEOMIMIC%252C%2520a%2520real-to-sim-to-real%2520pipeline%2520that%2520mines%2520everyday%250Avideos%252C%2520jointly%2520reconstructs%2520the%2520humans%2520and%2520the%2520environment%252C%2520and%2520produces%250Awhole-body%2520control%2520policies%2520for%2520humanoid%2520robots%2520that%2520perform%2520the%2520corresponding%250Askills.%2520We%2520demonstrate%2520the%2520results%2520of%2520our%2520pipeline%2520on%2520real%2520humanoid%2520robots%252C%250Ashowing%2520robust%252C%2520repeatable%2520contextual%2520control%2520such%2520as%2520staircase%2520ascents%2520and%250Adescents%252C%2520sitting%2520and%2520standing%2520from%2520chairs%2520and%2520benches%252C%2520as%2520well%2520as%2520other%250Adynamic%2520whole-body%2520skills-all%2520from%2520a%2520single%2520policy%252C%2520conditioned%2520on%2520the%250Aenvironment%2520and%2520global%2520root%2520commands.%2520VIDEOMIMIC%2520offers%2520a%2520scalable%2520path%2520towards%250Ateaching%2520humanoids%2520to%2520operate%2520in%2520diverse%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Imitation%20Enables%20Contextual%20Humanoid%20Control&entry.906535625=Arthur%20Allshire%20and%20Hongsuk%20Choi%20and%20Junyi%20Zhang%20and%20David%20McAllister%20and%20Anthony%20Zhang%20and%20Chung%20Min%20Kim%20and%20Trevor%20Darrell%20and%20Pieter%20Abbeel%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20How%20can%20we%20teach%20humanoids%20to%20climb%20staircases%20and%20sit%20on%20chairs%20using%20the%0Asurrounding%20environment%20context%3F%20Arguably%2C%20the%20simplest%20way%20is%20to%20just%20show%0Athem-casually%20capture%20a%20human%20motion%20video%20and%20feed%20it%20to%20humanoids.%20We%0Aintroduce%20VIDEOMIMIC%2C%20a%20real-to-sim-to-real%20pipeline%20that%20mines%20everyday%0Avideos%2C%20jointly%20reconstructs%20the%20humans%20and%20the%20environment%2C%20and%20produces%0Awhole-body%20control%20policies%20for%20humanoid%20robots%20that%20perform%20the%20corresponding%0Askills.%20We%20demonstrate%20the%20results%20of%20our%20pipeline%20on%20real%20humanoid%20robots%2C%0Ashowing%20robust%2C%20repeatable%20contextual%20control%20such%20as%20staircase%20ascents%20and%0Adescents%2C%20sitting%20and%20standing%20from%20chairs%20and%20benches%2C%20as%20well%20as%20other%0Adynamic%20whole-body%20skills-all%20from%20a%20single%20policy%2C%20conditioned%20on%20the%0Aenvironment%20and%20global%20root%20commands.%20VIDEOMIMIC%20offers%20a%20scalable%20path%20towards%0Ateaching%20humanoids%20to%20operate%20in%20diverse%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03729v1&entry.124074799=Read"},
{"title": "PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model", "author": "Y. B. Wang and S. Z. Zhou and J. F. Wu and T. Hu and J. N. Zhang and Y. Liu", "abstract": "  Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2505.03603v1", "date": "2025-05-06", "relevancy": 2.4278, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6436}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5901}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAHA%3A%20Parts-Aware%20Audio-Driven%20Human%20Animation%20with%20Diffusion%20Model&body=Title%3A%20PAHA%3A%20Parts-Aware%20Audio-Driven%20Human%20Animation%20with%20Diffusion%20Model%0AAuthor%3A%20Y.%20B.%20Wang%20and%20S.%20Z.%20Zhou%20and%20J.%20F.%20Wu%20and%20T.%20Hu%20and%20J.%20N.%20Zhang%20and%20Y.%20Liu%0AAbstract%3A%20%20%20Audio-driven%20human%20animation%20technology%20is%20widely%20used%20in%20human-computer%0Ainteraction%2C%20and%20the%20emergence%20of%20diffusion%20models%20has%20further%20advanced%20its%0Adevelopment.%20Currently%2C%20most%20methods%20rely%20on%20multi-stage%20generation%20and%0Aintermediate%20representations%2C%20resulting%20in%20long%20inference%20time%20and%20issues%20with%0Ageneration%20quality%20in%20specific%20foreground%20regions%20and%20audio-motion%20consistency.%0AThese%20shortcomings%20are%20primarily%20due%20to%20the%20lack%20of%20localized%20fine-grained%0Asupervised%20guidance.%20To%20address%20above%20challenges%2C%20we%20propose%20PAHA%2C%20an%0Aend-to-end%20audio-driven%20upper-body%20human%20animation%20framework%20with%20diffusion%0Amodel.%20We%20introduce%20two%20key%20methods%3A%20Parts-Aware%20Re-weighting%20%28PAR%29%20and%20Parts%0AConsistency%20Enhancement%20%28PCE%29.%20PAR%20dynamically%20adjusts%20regional%20training%20loss%0Aweights%20based%20on%20pose%20confidence%20scores%2C%20effectively%20improving%20visual%20quality.%0APCE%20constructs%20and%20trains%20diffusion-based%20regional%20audio-visual%20classifiers%20to%0Aimprove%20the%20consistency%20of%20motion%20and%20co-speech%20audio.%20Afterwards%2C%20we%20design%0Atwo%20novel%20inference%20guidance%20methods%20for%20the%20foregoing%20classifiers%2C%20Sequential%0AGuidance%20%28SG%29%20and%20Differential%20Guidance%20%28DG%29%2C%20to%20balance%20efficiency%20and%20quality%0Arespectively.%20Additionally%2C%20we%20build%20CNAS%2C%20the%20first%20public%20Chinese%20News%20Anchor%0ASpeech%20dataset%2C%20to%20advance%20research%20and%20validation%20in%20this%20field.%20Extensive%0Aexperimental%20results%20and%20user%20studies%20demonstrate%20that%20PAHA%20significantly%0Aoutperforms%20existing%20methods%20in%20audio-motion%20alignment%20and%20video-related%0Aevaluations.%20The%20codes%20and%20CNAS%20dataset%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAHA%253A%2520Parts-Aware%2520Audio-Driven%2520Human%2520Animation%2520with%2520Diffusion%2520Model%26entry.906535625%3DY.%2520B.%2520Wang%2520and%2520S.%2520Z.%2520Zhou%2520and%2520J.%2520F.%2520Wu%2520and%2520T.%2520Hu%2520and%2520J.%2520N.%2520Zhang%2520and%2520Y.%2520Liu%26entry.1292438233%3D%2520%2520Audio-driven%2520human%2520animation%2520technology%2520is%2520widely%2520used%2520in%2520human-computer%250Ainteraction%252C%2520and%2520the%2520emergence%2520of%2520diffusion%2520models%2520has%2520further%2520advanced%2520its%250Adevelopment.%2520Currently%252C%2520most%2520methods%2520rely%2520on%2520multi-stage%2520generation%2520and%250Aintermediate%2520representations%252C%2520resulting%2520in%2520long%2520inference%2520time%2520and%2520issues%2520with%250Ageneration%2520quality%2520in%2520specific%2520foreground%2520regions%2520and%2520audio-motion%2520consistency.%250AThese%2520shortcomings%2520are%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520localized%2520fine-grained%250Asupervised%2520guidance.%2520To%2520address%2520above%2520challenges%252C%2520we%2520propose%2520PAHA%252C%2520an%250Aend-to-end%2520audio-driven%2520upper-body%2520human%2520animation%2520framework%2520with%2520diffusion%250Amodel.%2520We%2520introduce%2520two%2520key%2520methods%253A%2520Parts-Aware%2520Re-weighting%2520%2528PAR%2529%2520and%2520Parts%250AConsistency%2520Enhancement%2520%2528PCE%2529.%2520PAR%2520dynamically%2520adjusts%2520regional%2520training%2520loss%250Aweights%2520based%2520on%2520pose%2520confidence%2520scores%252C%2520effectively%2520improving%2520visual%2520quality.%250APCE%2520constructs%2520and%2520trains%2520diffusion-based%2520regional%2520audio-visual%2520classifiers%2520to%250Aimprove%2520the%2520consistency%2520of%2520motion%2520and%2520co-speech%2520audio.%2520Afterwards%252C%2520we%2520design%250Atwo%2520novel%2520inference%2520guidance%2520methods%2520for%2520the%2520foregoing%2520classifiers%252C%2520Sequential%250AGuidance%2520%2528SG%2529%2520and%2520Differential%2520Guidance%2520%2528DG%2529%252C%2520to%2520balance%2520efficiency%2520and%2520quality%250Arespectively.%2520Additionally%252C%2520we%2520build%2520CNAS%252C%2520the%2520first%2520public%2520Chinese%2520News%2520Anchor%250ASpeech%2520dataset%252C%2520to%2520advance%2520research%2520and%2520validation%2520in%2520this%2520field.%2520Extensive%250Aexperimental%2520results%2520and%2520user%2520studies%2520demonstrate%2520that%2520PAHA%2520significantly%250Aoutperforms%2520existing%2520methods%2520in%2520audio-motion%2520alignment%2520and%2520video-related%250Aevaluations.%2520The%2520codes%2520and%2520CNAS%2520dataset%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAHA%3A%20Parts-Aware%20Audio-Driven%20Human%20Animation%20with%20Diffusion%20Model&entry.906535625=Y.%20B.%20Wang%20and%20S.%20Z.%20Zhou%20and%20J.%20F.%20Wu%20and%20T.%20Hu%20and%20J.%20N.%20Zhang%20and%20Y.%20Liu&entry.1292438233=%20%20Audio-driven%20human%20animation%20technology%20is%20widely%20used%20in%20human-computer%0Ainteraction%2C%20and%20the%20emergence%20of%20diffusion%20models%20has%20further%20advanced%20its%0Adevelopment.%20Currently%2C%20most%20methods%20rely%20on%20multi-stage%20generation%20and%0Aintermediate%20representations%2C%20resulting%20in%20long%20inference%20time%20and%20issues%20with%0Ageneration%20quality%20in%20specific%20foreground%20regions%20and%20audio-motion%20consistency.%0AThese%20shortcomings%20are%20primarily%20due%20to%20the%20lack%20of%20localized%20fine-grained%0Asupervised%20guidance.%20To%20address%20above%20challenges%2C%20we%20propose%20PAHA%2C%20an%0Aend-to-end%20audio-driven%20upper-body%20human%20animation%20framework%20with%20diffusion%0Amodel.%20We%20introduce%20two%20key%20methods%3A%20Parts-Aware%20Re-weighting%20%28PAR%29%20and%20Parts%0AConsistency%20Enhancement%20%28PCE%29.%20PAR%20dynamically%20adjusts%20regional%20training%20loss%0Aweights%20based%20on%20pose%20confidence%20scores%2C%20effectively%20improving%20visual%20quality.%0APCE%20constructs%20and%20trains%20diffusion-based%20regional%20audio-visual%20classifiers%20to%0Aimprove%20the%20consistency%20of%20motion%20and%20co-speech%20audio.%20Afterwards%2C%20we%20design%0Atwo%20novel%20inference%20guidance%20methods%20for%20the%20foregoing%20classifiers%2C%20Sequential%0AGuidance%20%28SG%29%20and%20Differential%20Guidance%20%28DG%29%2C%20to%20balance%20efficiency%20and%20quality%0Arespectively.%20Additionally%2C%20we%20build%20CNAS%2C%20the%20first%20public%20Chinese%20News%20Anchor%0ASpeech%20dataset%2C%20to%20advance%20research%20and%20validation%20in%20this%20field.%20Extensive%0Aexperimental%20results%20and%20user%20studies%20demonstrate%20that%20PAHA%20significantly%0Aoutperforms%20existing%20methods%20in%20audio-motion%20alignment%20and%20video-related%0Aevaluations.%20The%20codes%20and%20CNAS%20dataset%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03603v1&entry.124074799=Read"},
{"title": "Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware\n  Learning", "author": "Qing Zhu and Qirong Mao and Jialin Zhang and Xiaohua Huang and Wenming Zheng", "abstract": "  Group-level emotion recognition (GER) is an inseparable part of human\nbehavior analysis, aiming to recognize an overall emotion in a multi-person\nscene. However, the existing methods are devoted to combing diverse emotion\ncues while ignoring the inherent uncertainties under unconstrained\nenvironments, such as congestion and occlusion occurring within a group.\nAdditionally, since only group-level labels are available, inconsistent emotion\npredictions among individuals in one group can confuse the network. In this\npaper, we propose an uncertainty-aware learning (UAL) method to extract more\nrobust representations for GER. By explicitly modeling the uncertainty of each\nindividual, we utilize stochastic embedding drawn from a Gaussian distribution\ninstead of deterministic point embedding. This representation captures the\nprobabilities of different emotions and generates diverse predictions through\nthis stochasticity during the inference stage. Furthermore,\nuncertainty-sensitive scores are adaptively assigned as the fusion weights of\nindividuals' face within each group. Moreover, we develop an image enhancement\nmodule to enhance the model's robustness against severe noise. The overall\nthree-branch model, encompassing face, object, and scene component, is guided\nby a proportional-weighted fusion strategy and integrates the proposed\nuncertainty-aware method to produce the final group-level output. Experimental\nresults demonstrate the effectiveness and generalization ability of our method\nacross three widely used databases.\n", "link": "http://arxiv.org/abs/2310.04306v2", "date": "2025-05-06", "relevancy": 2.3945, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6208}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5971}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20A%20Robust%20Group-level%20Emotion%20Recognition%20via%20Uncertainty-Aware%0A%20%20Learning&body=Title%3A%20Towards%20A%20Robust%20Group-level%20Emotion%20Recognition%20via%20Uncertainty-Aware%0A%20%20Learning%0AAuthor%3A%20Qing%20Zhu%20and%20Qirong%20Mao%20and%20Jialin%20Zhang%20and%20Xiaohua%20Huang%20and%20Wenming%20Zheng%0AAbstract%3A%20%20%20Group-level%20emotion%20recognition%20%28GER%29%20is%20an%20inseparable%20part%20of%20human%0Abehavior%20analysis%2C%20aiming%20to%20recognize%20an%20overall%20emotion%20in%20a%20multi-person%0Ascene.%20However%2C%20the%20existing%20methods%20are%20devoted%20to%20combing%20diverse%20emotion%0Acues%20while%20ignoring%20the%20inherent%20uncertainties%20under%20unconstrained%0Aenvironments%2C%20such%20as%20congestion%20and%20occlusion%20occurring%20within%20a%20group.%0AAdditionally%2C%20since%20only%20group-level%20labels%20are%20available%2C%20inconsistent%20emotion%0Apredictions%20among%20individuals%20in%20one%20group%20can%20confuse%20the%20network.%20In%20this%0Apaper%2C%20we%20propose%20an%20uncertainty-aware%20learning%20%28UAL%29%20method%20to%20extract%20more%0Arobust%20representations%20for%20GER.%20By%20explicitly%20modeling%20the%20uncertainty%20of%20each%0Aindividual%2C%20we%20utilize%20stochastic%20embedding%20drawn%20from%20a%20Gaussian%20distribution%0Ainstead%20of%20deterministic%20point%20embedding.%20This%20representation%20captures%20the%0Aprobabilities%20of%20different%20emotions%20and%20generates%20diverse%20predictions%20through%0Athis%20stochasticity%20during%20the%20inference%20stage.%20Furthermore%2C%0Auncertainty-sensitive%20scores%20are%20adaptively%20assigned%20as%20the%20fusion%20weights%20of%0Aindividuals%27%20face%20within%20each%20group.%20Moreover%2C%20we%20develop%20an%20image%20enhancement%0Amodule%20to%20enhance%20the%20model%27s%20robustness%20against%20severe%20noise.%20The%20overall%0Athree-branch%20model%2C%20encompassing%20face%2C%20object%2C%20and%20scene%20component%2C%20is%20guided%0Aby%20a%20proportional-weighted%20fusion%20strategy%20and%20integrates%20the%20proposed%0Auncertainty-aware%20method%20to%20produce%20the%20final%20group-level%20output.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20and%20generalization%20ability%20of%20our%20method%0Aacross%20three%20widely%20used%20databases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520A%2520Robust%2520Group-level%2520Emotion%2520Recognition%2520via%2520Uncertainty-Aware%250A%2520%2520Learning%26entry.906535625%3DQing%2520Zhu%2520and%2520Qirong%2520Mao%2520and%2520Jialin%2520Zhang%2520and%2520Xiaohua%2520Huang%2520and%2520Wenming%2520Zheng%26entry.1292438233%3D%2520%2520Group-level%2520emotion%2520recognition%2520%2528GER%2529%2520is%2520an%2520inseparable%2520part%2520of%2520human%250Abehavior%2520analysis%252C%2520aiming%2520to%2520recognize%2520an%2520overall%2520emotion%2520in%2520a%2520multi-person%250Ascene.%2520However%252C%2520the%2520existing%2520methods%2520are%2520devoted%2520to%2520combing%2520diverse%2520emotion%250Acues%2520while%2520ignoring%2520the%2520inherent%2520uncertainties%2520under%2520unconstrained%250Aenvironments%252C%2520such%2520as%2520congestion%2520and%2520occlusion%2520occurring%2520within%2520a%2520group.%250AAdditionally%252C%2520since%2520only%2520group-level%2520labels%2520are%2520available%252C%2520inconsistent%2520emotion%250Apredictions%2520among%2520individuals%2520in%2520one%2520group%2520can%2520confuse%2520the%2520network.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520an%2520uncertainty-aware%2520learning%2520%2528UAL%2529%2520method%2520to%2520extract%2520more%250Arobust%2520representations%2520for%2520GER.%2520By%2520explicitly%2520modeling%2520the%2520uncertainty%2520of%2520each%250Aindividual%252C%2520we%2520utilize%2520stochastic%2520embedding%2520drawn%2520from%2520a%2520Gaussian%2520distribution%250Ainstead%2520of%2520deterministic%2520point%2520embedding.%2520This%2520representation%2520captures%2520the%250Aprobabilities%2520of%2520different%2520emotions%2520and%2520generates%2520diverse%2520predictions%2520through%250Athis%2520stochasticity%2520during%2520the%2520inference%2520stage.%2520Furthermore%252C%250Auncertainty-sensitive%2520scores%2520are%2520adaptively%2520assigned%2520as%2520the%2520fusion%2520weights%2520of%250Aindividuals%2527%2520face%2520within%2520each%2520group.%2520Moreover%252C%2520we%2520develop%2520an%2520image%2520enhancement%250Amodule%2520to%2520enhance%2520the%2520model%2527s%2520robustness%2520against%2520severe%2520noise.%2520The%2520overall%250Athree-branch%2520model%252C%2520encompassing%2520face%252C%2520object%252C%2520and%2520scene%2520component%252C%2520is%2520guided%250Aby%2520a%2520proportional-weighted%2520fusion%2520strategy%2520and%2520integrates%2520the%2520proposed%250Auncertainty-aware%2520method%2520to%2520produce%2520the%2520final%2520group-level%2520output.%2520Experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520and%2520generalization%2520ability%2520of%2520our%2520method%250Aacross%2520three%2520widely%2520used%2520databases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20A%20Robust%20Group-level%20Emotion%20Recognition%20via%20Uncertainty-Aware%0A%20%20Learning&entry.906535625=Qing%20Zhu%20and%20Qirong%20Mao%20and%20Jialin%20Zhang%20and%20Xiaohua%20Huang%20and%20Wenming%20Zheng&entry.1292438233=%20%20Group-level%20emotion%20recognition%20%28GER%29%20is%20an%20inseparable%20part%20of%20human%0Abehavior%20analysis%2C%20aiming%20to%20recognize%20an%20overall%20emotion%20in%20a%20multi-person%0Ascene.%20However%2C%20the%20existing%20methods%20are%20devoted%20to%20combing%20diverse%20emotion%0Acues%20while%20ignoring%20the%20inherent%20uncertainties%20under%20unconstrained%0Aenvironments%2C%20such%20as%20congestion%20and%20occlusion%20occurring%20within%20a%20group.%0AAdditionally%2C%20since%20only%20group-level%20labels%20are%20available%2C%20inconsistent%20emotion%0Apredictions%20among%20individuals%20in%20one%20group%20can%20confuse%20the%20network.%20In%20this%0Apaper%2C%20we%20propose%20an%20uncertainty-aware%20learning%20%28UAL%29%20method%20to%20extract%20more%0Arobust%20representations%20for%20GER.%20By%20explicitly%20modeling%20the%20uncertainty%20of%20each%0Aindividual%2C%20we%20utilize%20stochastic%20embedding%20drawn%20from%20a%20Gaussian%20distribution%0Ainstead%20of%20deterministic%20point%20embedding.%20This%20representation%20captures%20the%0Aprobabilities%20of%20different%20emotions%20and%20generates%20diverse%20predictions%20through%0Athis%20stochasticity%20during%20the%20inference%20stage.%20Furthermore%2C%0Auncertainty-sensitive%20scores%20are%20adaptively%20assigned%20as%20the%20fusion%20weights%20of%0Aindividuals%27%20face%20within%20each%20group.%20Moreover%2C%20we%20develop%20an%20image%20enhancement%0Amodule%20to%20enhance%20the%20model%27s%20robustness%20against%20severe%20noise.%20The%20overall%0Athree-branch%20model%2C%20encompassing%20face%2C%20object%2C%20and%20scene%20component%2C%20is%20guided%0Aby%20a%20proportional-weighted%20fusion%20strategy%20and%20integrates%20the%20proposed%0Auncertainty-aware%20method%20to%20produce%20the%20final%20group-level%20output.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20and%20generalization%20ability%20of%20our%20method%0Aacross%20three%20widely%20used%20databases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04306v2&entry.124074799=Read"},
{"title": "EOPose : Exemplar-based object reposing using Generalized Pose\n  Correspondences", "author": "Sarthak Mehrotra and Rishabh Jain and Mayur Hemani and Balaji Krishnamurthy and Mausoom Sarkar", "abstract": "  Reposing objects in images has a myriad of applications, especially for\ne-commerce where several variants of product images need to be produced\nquickly. In this work, we leverage the recent advances in unsupervised keypoint\ncorrespondence detection between different object images of the same class to\npropose an end-to-end framework for generic object reposing. Our method,\nEOPose, takes a target pose-guidance image as input and uses its keypoint\ncorrespondence with the source object image to warp and re-render the latter\ninto the target pose using a novel three-step approach. Unlike generative\napproaches, our method also preserves the fine-grained details of the object\nsuch as its exact colors, textures, and brand marks. We also prepare a new\ndataset of paired objects based on the Objaverse dataset to train and test our\nnetwork. EOPose produces high-quality reposing output as evidenced by different\nimage quality metrics (PSNR, SSIM and FID). Besides a description of the method\nand the dataset, the paper also includes detailed ablation and user studies to\nindicate the efficacy of the proposed method\n", "link": "http://arxiv.org/abs/2505.03394v1", "date": "2025-05-06", "relevancy": 2.3908, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6204}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6008}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EOPose%20%3A%20Exemplar-based%20object%20reposing%20using%20Generalized%20Pose%0A%20%20Correspondences&body=Title%3A%20EOPose%20%3A%20Exemplar-based%20object%20reposing%20using%20Generalized%20Pose%0A%20%20Correspondences%0AAuthor%3A%20Sarthak%20Mehrotra%20and%20Rishabh%20Jain%20and%20Mayur%20Hemani%20and%20Balaji%20Krishnamurthy%20and%20Mausoom%20Sarkar%0AAbstract%3A%20%20%20Reposing%20objects%20in%20images%20has%20a%20myriad%20of%20applications%2C%20especially%20for%0Ae-commerce%20where%20several%20variants%20of%20product%20images%20need%20to%20be%20produced%0Aquickly.%20In%20this%20work%2C%20we%20leverage%20the%20recent%20advances%20in%20unsupervised%20keypoint%0Acorrespondence%20detection%20between%20different%20object%20images%20of%20the%20same%20class%20to%0Apropose%20an%20end-to-end%20framework%20for%20generic%20object%20reposing.%20Our%20method%2C%0AEOPose%2C%20takes%20a%20target%20pose-guidance%20image%20as%20input%20and%20uses%20its%20keypoint%0Acorrespondence%20with%20the%20source%20object%20image%20to%20warp%20and%20re-render%20the%20latter%0Ainto%20the%20target%20pose%20using%20a%20novel%20three-step%20approach.%20Unlike%20generative%0Aapproaches%2C%20our%20method%20also%20preserves%20the%20fine-grained%20details%20of%20the%20object%0Asuch%20as%20its%20exact%20colors%2C%20textures%2C%20and%20brand%20marks.%20We%20also%20prepare%20a%20new%0Adataset%20of%20paired%20objects%20based%20on%20the%20Objaverse%20dataset%20to%20train%20and%20test%20our%0Anetwork.%20EOPose%20produces%20high-quality%20reposing%20output%20as%20evidenced%20by%20different%0Aimage%20quality%20metrics%20%28PSNR%2C%20SSIM%20and%20FID%29.%20Besides%20a%20description%20of%20the%20method%0Aand%20the%20dataset%2C%20the%20paper%20also%20includes%20detailed%20ablation%20and%20user%20studies%20to%0Aindicate%20the%20efficacy%20of%20the%20proposed%20method%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEOPose%2520%253A%2520Exemplar-based%2520object%2520reposing%2520using%2520Generalized%2520Pose%250A%2520%2520Correspondences%26entry.906535625%3DSarthak%2520Mehrotra%2520and%2520Rishabh%2520Jain%2520and%2520Mayur%2520Hemani%2520and%2520Balaji%2520Krishnamurthy%2520and%2520Mausoom%2520Sarkar%26entry.1292438233%3D%2520%2520Reposing%2520objects%2520in%2520images%2520has%2520a%2520myriad%2520of%2520applications%252C%2520especially%2520for%250Ae-commerce%2520where%2520several%2520variants%2520of%2520product%2520images%2520need%2520to%2520be%2520produced%250Aquickly.%2520In%2520this%2520work%252C%2520we%2520leverage%2520the%2520recent%2520advances%2520in%2520unsupervised%2520keypoint%250Acorrespondence%2520detection%2520between%2520different%2520object%2520images%2520of%2520the%2520same%2520class%2520to%250Apropose%2520an%2520end-to-end%2520framework%2520for%2520generic%2520object%2520reposing.%2520Our%2520method%252C%250AEOPose%252C%2520takes%2520a%2520target%2520pose-guidance%2520image%2520as%2520input%2520and%2520uses%2520its%2520keypoint%250Acorrespondence%2520with%2520the%2520source%2520object%2520image%2520to%2520warp%2520and%2520re-render%2520the%2520latter%250Ainto%2520the%2520target%2520pose%2520using%2520a%2520novel%2520three-step%2520approach.%2520Unlike%2520generative%250Aapproaches%252C%2520our%2520method%2520also%2520preserves%2520the%2520fine-grained%2520details%2520of%2520the%2520object%250Asuch%2520as%2520its%2520exact%2520colors%252C%2520textures%252C%2520and%2520brand%2520marks.%2520We%2520also%2520prepare%2520a%2520new%250Adataset%2520of%2520paired%2520objects%2520based%2520on%2520the%2520Objaverse%2520dataset%2520to%2520train%2520and%2520test%2520our%250Anetwork.%2520EOPose%2520produces%2520high-quality%2520reposing%2520output%2520as%2520evidenced%2520by%2520different%250Aimage%2520quality%2520metrics%2520%2528PSNR%252C%2520SSIM%2520and%2520FID%2529.%2520Besides%2520a%2520description%2520of%2520the%2520method%250Aand%2520the%2520dataset%252C%2520the%2520paper%2520also%2520includes%2520detailed%2520ablation%2520and%2520user%2520studies%2520to%250Aindicate%2520the%2520efficacy%2520of%2520the%2520proposed%2520method%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EOPose%20%3A%20Exemplar-based%20object%20reposing%20using%20Generalized%20Pose%0A%20%20Correspondences&entry.906535625=Sarthak%20Mehrotra%20and%20Rishabh%20Jain%20and%20Mayur%20Hemani%20and%20Balaji%20Krishnamurthy%20and%20Mausoom%20Sarkar&entry.1292438233=%20%20Reposing%20objects%20in%20images%20has%20a%20myriad%20of%20applications%2C%20especially%20for%0Ae-commerce%20where%20several%20variants%20of%20product%20images%20need%20to%20be%20produced%0Aquickly.%20In%20this%20work%2C%20we%20leverage%20the%20recent%20advances%20in%20unsupervised%20keypoint%0Acorrespondence%20detection%20between%20different%20object%20images%20of%20the%20same%20class%20to%0Apropose%20an%20end-to-end%20framework%20for%20generic%20object%20reposing.%20Our%20method%2C%0AEOPose%2C%20takes%20a%20target%20pose-guidance%20image%20as%20input%20and%20uses%20its%20keypoint%0Acorrespondence%20with%20the%20source%20object%20image%20to%20warp%20and%20re-render%20the%20latter%0Ainto%20the%20target%20pose%20using%20a%20novel%20three-step%20approach.%20Unlike%20generative%0Aapproaches%2C%20our%20method%20also%20preserves%20the%20fine-grained%20details%20of%20the%20object%0Asuch%20as%20its%20exact%20colors%2C%20textures%2C%20and%20brand%20marks.%20We%20also%20prepare%20a%20new%0Adataset%20of%20paired%20objects%20based%20on%20the%20Objaverse%20dataset%20to%20train%20and%20test%20our%0Anetwork.%20EOPose%20produces%20high-quality%20reposing%20output%20as%20evidenced%20by%20different%0Aimage%20quality%20metrics%20%28PSNR%2C%20SSIM%20and%20FID%29.%20Besides%20a%20description%20of%20the%20method%0Aand%20the%20dataset%2C%20the%20paper%20also%20includes%20detailed%20ablation%20and%20user%20studies%20to%0Aindicate%20the%20efficacy%20of%20the%20proposed%20method%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03394v1&entry.124074799=Read"},
{"title": "Sharp Global Guarantees for Nonconvex Low-rank Recovery in the Noisy\n  Overparameterized Regime", "author": "Richard Y. Zhang", "abstract": "  Recent work established that rank overparameterization eliminates spurious\nlocal minima in nonconvex low-rank matrix recovery under the restricted\nisometry property (RIP). But this does not fully explain the practical success\nof overparameterization, because real algorithms can still become trapped at\nnonstrict saddle points (approximate second-order points with arbitrarily small\nnegative curvature) even when all local minima are global. Moreover, the result\ndoes not accommodate for noisy measurements, but it is unclear whether such an\nextension is even possible, in view of the many discontinuous and unintuitive\nbehaviors already known for the overparameterized regime. In this paper, we\nintroduce a novel proof technique that unifies, simplifies, and strengthens two\npreviously competing approaches -- one based on escape directions and the other\nbased on the inexistence of counterexample -- to provide sharp global\nguarantees in the noisy overparameterized regime. We show, once local minima\nhave been converted into global minima through slight overparameterization,\nthat near-second-order points achieve the same minimax-optimal recovery bounds\n(up to small constant factors) as significantly more expensive convex\napproaches. Our results are sharp with respect to the noise level and the\nsolution accuracy, and hold for both the symmetric parameterization $XX^{T}$,\nas well as the asymmetric parameterization $UV^{T}$ under a balancing\nregularizer; we demonstrate that the balancing regularizer is indeed necessary.\n", "link": "http://arxiv.org/abs/2104.10790v3", "date": "2025-05-06", "relevancy": 2.363, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4855}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4761}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharp%20Global%20Guarantees%20for%20Nonconvex%20Low-rank%20Recovery%20in%20the%20Noisy%0A%20%20Overparameterized%20Regime&body=Title%3A%20Sharp%20Global%20Guarantees%20for%20Nonconvex%20Low-rank%20Recovery%20in%20the%20Noisy%0A%20%20Overparameterized%20Regime%0AAuthor%3A%20Richard%20Y.%20Zhang%0AAbstract%3A%20%20%20Recent%20work%20established%20that%20rank%20overparameterization%20eliminates%20spurious%0Alocal%20minima%20in%20nonconvex%20low-rank%20matrix%20recovery%20under%20the%20restricted%0Aisometry%20property%20%28RIP%29.%20But%20this%20does%20not%20fully%20explain%20the%20practical%20success%0Aof%20overparameterization%2C%20because%20real%20algorithms%20can%20still%20become%20trapped%20at%0Anonstrict%20saddle%20points%20%28approximate%20second-order%20points%20with%20arbitrarily%20small%0Anegative%20curvature%29%20even%20when%20all%20local%20minima%20are%20global.%20Moreover%2C%20the%20result%0Adoes%20not%20accommodate%20for%20noisy%20measurements%2C%20but%20it%20is%20unclear%20whether%20such%20an%0Aextension%20is%20even%20possible%2C%20in%20view%20of%20the%20many%20discontinuous%20and%20unintuitive%0Abehaviors%20already%20known%20for%20the%20overparameterized%20regime.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20proof%20technique%20that%20unifies%2C%20simplifies%2C%20and%20strengthens%20two%0Apreviously%20competing%20approaches%20--%20one%20based%20on%20escape%20directions%20and%20the%20other%0Abased%20on%20the%20inexistence%20of%20counterexample%20--%20to%20provide%20sharp%20global%0Aguarantees%20in%20the%20noisy%20overparameterized%20regime.%20We%20show%2C%20once%20local%20minima%0Ahave%20been%20converted%20into%20global%20minima%20through%20slight%20overparameterization%2C%0Athat%20near-second-order%20points%20achieve%20the%20same%20minimax-optimal%20recovery%20bounds%0A%28up%20to%20small%20constant%20factors%29%20as%20significantly%20more%20expensive%20convex%0Aapproaches.%20Our%20results%20are%20sharp%20with%20respect%20to%20the%20noise%20level%20and%20the%0Asolution%20accuracy%2C%20and%20hold%20for%20both%20the%20symmetric%20parameterization%20%24XX%5E%7BT%7D%24%2C%0Aas%20well%20as%20the%20asymmetric%20parameterization%20%24UV%5E%7BT%7D%24%20under%20a%20balancing%0Aregularizer%3B%20we%20demonstrate%20that%20the%20balancing%20regularizer%20is%20indeed%20necessary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2104.10790v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharp%2520Global%2520Guarantees%2520for%2520Nonconvex%2520Low-rank%2520Recovery%2520in%2520the%2520Noisy%250A%2520%2520Overparameterized%2520Regime%26entry.906535625%3DRichard%2520Y.%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520work%2520established%2520that%2520rank%2520overparameterization%2520eliminates%2520spurious%250Alocal%2520minima%2520in%2520nonconvex%2520low-rank%2520matrix%2520recovery%2520under%2520the%2520restricted%250Aisometry%2520property%2520%2528RIP%2529.%2520But%2520this%2520does%2520not%2520fully%2520explain%2520the%2520practical%2520success%250Aof%2520overparameterization%252C%2520because%2520real%2520algorithms%2520can%2520still%2520become%2520trapped%2520at%250Anonstrict%2520saddle%2520points%2520%2528approximate%2520second-order%2520points%2520with%2520arbitrarily%2520small%250Anegative%2520curvature%2529%2520even%2520when%2520all%2520local%2520minima%2520are%2520global.%2520Moreover%252C%2520the%2520result%250Adoes%2520not%2520accommodate%2520for%2520noisy%2520measurements%252C%2520but%2520it%2520is%2520unclear%2520whether%2520such%2520an%250Aextension%2520is%2520even%2520possible%252C%2520in%2520view%2520of%2520the%2520many%2520discontinuous%2520and%2520unintuitive%250Abehaviors%2520already%2520known%2520for%2520the%2520overparameterized%2520regime.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520proof%2520technique%2520that%2520unifies%252C%2520simplifies%252C%2520and%2520strengthens%2520two%250Apreviously%2520competing%2520approaches%2520--%2520one%2520based%2520on%2520escape%2520directions%2520and%2520the%2520other%250Abased%2520on%2520the%2520inexistence%2520of%2520counterexample%2520--%2520to%2520provide%2520sharp%2520global%250Aguarantees%2520in%2520the%2520noisy%2520overparameterized%2520regime.%2520We%2520show%252C%2520once%2520local%2520minima%250Ahave%2520been%2520converted%2520into%2520global%2520minima%2520through%2520slight%2520overparameterization%252C%250Athat%2520near-second-order%2520points%2520achieve%2520the%2520same%2520minimax-optimal%2520recovery%2520bounds%250A%2528up%2520to%2520small%2520constant%2520factors%2529%2520as%2520significantly%2520more%2520expensive%2520convex%250Aapproaches.%2520Our%2520results%2520are%2520sharp%2520with%2520respect%2520to%2520the%2520noise%2520level%2520and%2520the%250Asolution%2520accuracy%252C%2520and%2520hold%2520for%2520both%2520the%2520symmetric%2520parameterization%2520%2524XX%255E%257BT%257D%2524%252C%250Aas%2520well%2520as%2520the%2520asymmetric%2520parameterization%2520%2524UV%255E%257BT%257D%2524%2520under%2520a%2520balancing%250Aregularizer%253B%2520we%2520demonstrate%2520that%2520the%2520balancing%2520regularizer%2520is%2520indeed%2520necessary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2104.10790v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20Global%20Guarantees%20for%20Nonconvex%20Low-rank%20Recovery%20in%20the%20Noisy%0A%20%20Overparameterized%20Regime&entry.906535625=Richard%20Y.%20Zhang&entry.1292438233=%20%20Recent%20work%20established%20that%20rank%20overparameterization%20eliminates%20spurious%0Alocal%20minima%20in%20nonconvex%20low-rank%20matrix%20recovery%20under%20the%20restricted%0Aisometry%20property%20%28RIP%29.%20But%20this%20does%20not%20fully%20explain%20the%20practical%20success%0Aof%20overparameterization%2C%20because%20real%20algorithms%20can%20still%20become%20trapped%20at%0Anonstrict%20saddle%20points%20%28approximate%20second-order%20points%20with%20arbitrarily%20small%0Anegative%20curvature%29%20even%20when%20all%20local%20minima%20are%20global.%20Moreover%2C%20the%20result%0Adoes%20not%20accommodate%20for%20noisy%20measurements%2C%20but%20it%20is%20unclear%20whether%20such%20an%0Aextension%20is%20even%20possible%2C%20in%20view%20of%20the%20many%20discontinuous%20and%20unintuitive%0Abehaviors%20already%20known%20for%20the%20overparameterized%20regime.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20proof%20technique%20that%20unifies%2C%20simplifies%2C%20and%20strengthens%20two%0Apreviously%20competing%20approaches%20--%20one%20based%20on%20escape%20directions%20and%20the%20other%0Abased%20on%20the%20inexistence%20of%20counterexample%20--%20to%20provide%20sharp%20global%0Aguarantees%20in%20the%20noisy%20overparameterized%20regime.%20We%20show%2C%20once%20local%20minima%0Ahave%20been%20converted%20into%20global%20minima%20through%20slight%20overparameterization%2C%0Athat%20near-second-order%20points%20achieve%20the%20same%20minimax-optimal%20recovery%20bounds%0A%28up%20to%20small%20constant%20factors%29%20as%20significantly%20more%20expensive%20convex%0Aapproaches.%20Our%20results%20are%20sharp%20with%20respect%20to%20the%20noise%20level%20and%20the%0Asolution%20accuracy%2C%20and%20hold%20for%20both%20the%20symmetric%20parameterization%20%24XX%5E%7BT%7D%24%2C%0Aas%20well%20as%20the%20asymmetric%20parameterization%20%24UV%5E%7BT%7D%24%20under%20a%20balancing%0Aregularizer%3B%20we%20demonstrate%20that%20the%20balancing%20regularizer%20is%20indeed%20necessary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2104.10790v3&entry.124074799=Read"},
{"title": "UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance\n  and Adaptive Multimodal Feature Fusion", "author": "Zhanyuan Jia and Ni Yao and Danyang Sun and Chuang Han and Yanting Li and Jiaofen Nan and Fubao Zhu and Chen Zhao and Weihua Zhou", "abstract": "  Background: Brain tumor segmentation has a significant impact on the\ndiagnosis and treatment of brain tumors. Accurate brain tumor segmentation\nremains challenging due to their irregular shapes, vague boundaries, and high\nvariability. Objective: We propose a brain tumor segmentation method that\ncombines deep learning with prior knowledge derived from a region-growing\nalgorithm. Methods: The proposed method utilizes a multi-scale feature fusion\n(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale\nfeatures and capture global contextual information. To enhance the model's\nrobustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)\nstrategy is employed for uncertainty estimation. Results: Extensive experiments\ndemonstrate that the proposed method achieves superior performance on Brain\nTumor Segmentation (BraTS) datasets, significantly outperforming various\nstate-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are\n89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)\nsegmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019\nvalidation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for\nET, WT, and TC segmentation, respectively. Ablation studies further confirmed\nthe contribution of each module to segmentation accuracy, indicating that each\ncomponent played a vital role in overall performance improvement. Conclusion:\nThis study proposed a novel 3D brain tumor segmentation network based on the\nU-Net architecture. By incorporating the prior knowledge and employing the\nuncertainty estimation method, the robustness and performance were improved.\nThe code for the proposed method is available at\nhttps://github.com/chenzhao2023/UPMAD_Net_BrainSeg.\n", "link": "http://arxiv.org/abs/2505.03494v1", "date": "2025-05-06", "relevancy": 2.3576, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6223}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5889}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UPMAD-Net%3A%20A%20Brain%20Tumor%20Segmentation%20Network%20with%20Uncertainty%20Guidance%0A%20%20and%20Adaptive%20Multimodal%20Feature%20Fusion&body=Title%3A%20UPMAD-Net%3A%20A%20Brain%20Tumor%20Segmentation%20Network%20with%20Uncertainty%20Guidance%0A%20%20and%20Adaptive%20Multimodal%20Feature%20Fusion%0AAuthor%3A%20Zhanyuan%20Jia%20and%20Ni%20Yao%20and%20Danyang%20Sun%20and%20Chuang%20Han%20and%20Yanting%20Li%20and%20Jiaofen%20Nan%20and%20Fubao%20Zhu%20and%20Chen%20Zhao%20and%20Weihua%20Zhou%0AAbstract%3A%20%20%20Background%3A%20Brain%20tumor%20segmentation%20has%20a%20significant%20impact%20on%20the%0Adiagnosis%20and%20treatment%20of%20brain%20tumors.%20Accurate%20brain%20tumor%20segmentation%0Aremains%20challenging%20due%20to%20their%20irregular%20shapes%2C%20vague%20boundaries%2C%20and%20high%0Avariability.%20Objective%3A%20We%20propose%20a%20brain%20tumor%20segmentation%20method%20that%0Acombines%20deep%20learning%20with%20prior%20knowledge%20derived%20from%20a%20region-growing%0Aalgorithm.%20Methods%3A%20The%20proposed%20method%20utilizes%20a%20multi-scale%20feature%20fusion%0A%28MSFF%29%20module%20and%20adaptive%20attention%20mechanisms%20%28AAM%29%20to%20extract%20multi-scale%0Afeatures%20and%20capture%20global%20contextual%20information.%20To%20enhance%20the%20model%27s%0Arobustness%20in%20low-confidence%20regions%2C%20the%20Monte%20Carlo%20Dropout%20%28MC%20Dropout%29%0Astrategy%20is%20employed%20for%20uncertainty%20estimation.%20Results%3A%20Extensive%20experiments%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20superior%20performance%20on%20Brain%0ATumor%20Segmentation%20%28BraTS%29%20datasets%2C%20significantly%20outperforming%20various%0Astate-of-the-art%20methods.%20On%20the%20BraTS2021%20dataset%2C%20the%20test%20Dice%20scores%20are%0A89.18%25%20for%20Enhancing%20Tumor%20%28ET%29%20segmentation%2C%2093.67%25%20for%20Whole%20Tumor%20%28WT%29%0Asegmentation%2C%20and%2091.23%25%20for%20Tumor%20Core%20%28TC%29%20segmentation.%20On%20the%20BraTS2019%0Avalidation%20set%2C%20the%20validation%20Dice%20scores%20are%2087.43%25%2C%2090.92%25%2C%20and%2090.40%25%20for%0AET%2C%20WT%2C%20and%20TC%20segmentation%2C%20respectively.%20Ablation%20studies%20further%20confirmed%0Athe%20contribution%20of%20each%20module%20to%20segmentation%20accuracy%2C%20indicating%20that%20each%0Acomponent%20played%20a%20vital%20role%20in%20overall%20performance%20improvement.%20Conclusion%3A%0AThis%20study%20proposed%20a%20novel%203D%20brain%20tumor%20segmentation%20network%20based%20on%20the%0AU-Net%20architecture.%20By%20incorporating%20the%20prior%20knowledge%20and%20employing%20the%0Auncertainty%20estimation%20method%2C%20the%20robustness%20and%20performance%20were%20improved.%0AThe%20code%20for%20the%20proposed%20method%20is%20available%20at%0Ahttps%3A//github.com/chenzhao2023/UPMAD_Net_BrainSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUPMAD-Net%253A%2520A%2520Brain%2520Tumor%2520Segmentation%2520Network%2520with%2520Uncertainty%2520Guidance%250A%2520%2520and%2520Adaptive%2520Multimodal%2520Feature%2520Fusion%26entry.906535625%3DZhanyuan%2520Jia%2520and%2520Ni%2520Yao%2520and%2520Danyang%2520Sun%2520and%2520Chuang%2520Han%2520and%2520Yanting%2520Li%2520and%2520Jiaofen%2520Nan%2520and%2520Fubao%2520Zhu%2520and%2520Chen%2520Zhao%2520and%2520Weihua%2520Zhou%26entry.1292438233%3D%2520%2520Background%253A%2520Brain%2520tumor%2520segmentation%2520has%2520a%2520significant%2520impact%2520on%2520the%250Adiagnosis%2520and%2520treatment%2520of%2520brain%2520tumors.%2520Accurate%2520brain%2520tumor%2520segmentation%250Aremains%2520challenging%2520due%2520to%2520their%2520irregular%2520shapes%252C%2520vague%2520boundaries%252C%2520and%2520high%250Avariability.%2520Objective%253A%2520We%2520propose%2520a%2520brain%2520tumor%2520segmentation%2520method%2520that%250Acombines%2520deep%2520learning%2520with%2520prior%2520knowledge%2520derived%2520from%2520a%2520region-growing%250Aalgorithm.%2520Methods%253A%2520The%2520proposed%2520method%2520utilizes%2520a%2520multi-scale%2520feature%2520fusion%250A%2528MSFF%2529%2520module%2520and%2520adaptive%2520attention%2520mechanisms%2520%2528AAM%2529%2520to%2520extract%2520multi-scale%250Afeatures%2520and%2520capture%2520global%2520contextual%2520information.%2520To%2520enhance%2520the%2520model%2527s%250Arobustness%2520in%2520low-confidence%2520regions%252C%2520the%2520Monte%2520Carlo%2520Dropout%2520%2528MC%2520Dropout%2529%250Astrategy%2520is%2520employed%2520for%2520uncertainty%2520estimation.%2520Results%253A%2520Extensive%2520experiments%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520superior%2520performance%2520on%2520Brain%250ATumor%2520Segmentation%2520%2528BraTS%2529%2520datasets%252C%2520significantly%2520outperforming%2520various%250Astate-of-the-art%2520methods.%2520On%2520the%2520BraTS2021%2520dataset%252C%2520the%2520test%2520Dice%2520scores%2520are%250A89.18%2525%2520for%2520Enhancing%2520Tumor%2520%2528ET%2529%2520segmentation%252C%252093.67%2525%2520for%2520Whole%2520Tumor%2520%2528WT%2529%250Asegmentation%252C%2520and%252091.23%2525%2520for%2520Tumor%2520Core%2520%2528TC%2529%2520segmentation.%2520On%2520the%2520BraTS2019%250Avalidation%2520set%252C%2520the%2520validation%2520Dice%2520scores%2520are%252087.43%2525%252C%252090.92%2525%252C%2520and%252090.40%2525%2520for%250AET%252C%2520WT%252C%2520and%2520TC%2520segmentation%252C%2520respectively.%2520Ablation%2520studies%2520further%2520confirmed%250Athe%2520contribution%2520of%2520each%2520module%2520to%2520segmentation%2520accuracy%252C%2520indicating%2520that%2520each%250Acomponent%2520played%2520a%2520vital%2520role%2520in%2520overall%2520performance%2520improvement.%2520Conclusion%253A%250AThis%2520study%2520proposed%2520a%2520novel%25203D%2520brain%2520tumor%2520segmentation%2520network%2520based%2520on%2520the%250AU-Net%2520architecture.%2520By%2520incorporating%2520the%2520prior%2520knowledge%2520and%2520employing%2520the%250Auncertainty%2520estimation%2520method%252C%2520the%2520robustness%2520and%2520performance%2520were%2520improved.%250AThe%2520code%2520for%2520the%2520proposed%2520method%2520is%2520available%2520at%250Ahttps%253A//github.com/chenzhao2023/UPMAD_Net_BrainSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UPMAD-Net%3A%20A%20Brain%20Tumor%20Segmentation%20Network%20with%20Uncertainty%20Guidance%0A%20%20and%20Adaptive%20Multimodal%20Feature%20Fusion&entry.906535625=Zhanyuan%20Jia%20and%20Ni%20Yao%20and%20Danyang%20Sun%20and%20Chuang%20Han%20and%20Yanting%20Li%20and%20Jiaofen%20Nan%20and%20Fubao%20Zhu%20and%20Chen%20Zhao%20and%20Weihua%20Zhou&entry.1292438233=%20%20Background%3A%20Brain%20tumor%20segmentation%20has%20a%20significant%20impact%20on%20the%0Adiagnosis%20and%20treatment%20of%20brain%20tumors.%20Accurate%20brain%20tumor%20segmentation%0Aremains%20challenging%20due%20to%20their%20irregular%20shapes%2C%20vague%20boundaries%2C%20and%20high%0Avariability.%20Objective%3A%20We%20propose%20a%20brain%20tumor%20segmentation%20method%20that%0Acombines%20deep%20learning%20with%20prior%20knowledge%20derived%20from%20a%20region-growing%0Aalgorithm.%20Methods%3A%20The%20proposed%20method%20utilizes%20a%20multi-scale%20feature%20fusion%0A%28MSFF%29%20module%20and%20adaptive%20attention%20mechanisms%20%28AAM%29%20to%20extract%20multi-scale%0Afeatures%20and%20capture%20global%20contextual%20information.%20To%20enhance%20the%20model%27s%0Arobustness%20in%20low-confidence%20regions%2C%20the%20Monte%20Carlo%20Dropout%20%28MC%20Dropout%29%0Astrategy%20is%20employed%20for%20uncertainty%20estimation.%20Results%3A%20Extensive%20experiments%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20superior%20performance%20on%20Brain%0ATumor%20Segmentation%20%28BraTS%29%20datasets%2C%20significantly%20outperforming%20various%0Astate-of-the-art%20methods.%20On%20the%20BraTS2021%20dataset%2C%20the%20test%20Dice%20scores%20are%0A89.18%25%20for%20Enhancing%20Tumor%20%28ET%29%20segmentation%2C%2093.67%25%20for%20Whole%20Tumor%20%28WT%29%0Asegmentation%2C%20and%2091.23%25%20for%20Tumor%20Core%20%28TC%29%20segmentation.%20On%20the%20BraTS2019%0Avalidation%20set%2C%20the%20validation%20Dice%20scores%20are%2087.43%25%2C%2090.92%25%2C%20and%2090.40%25%20for%0AET%2C%20WT%2C%20and%20TC%20segmentation%2C%20respectively.%20Ablation%20studies%20further%20confirmed%0Athe%20contribution%20of%20each%20module%20to%20segmentation%20accuracy%2C%20indicating%20that%20each%0Acomponent%20played%20a%20vital%20role%20in%20overall%20performance%20improvement.%20Conclusion%3A%0AThis%20study%20proposed%20a%20novel%203D%20brain%20tumor%20segmentation%20network%20based%20on%20the%0AU-Net%20architecture.%20By%20incorporating%20the%20prior%20knowledge%20and%20employing%20the%0Auncertainty%20estimation%20method%2C%20the%20robustness%20and%20performance%20were%20improved.%0AThe%20code%20for%20the%20proposed%20method%20is%20available%20at%0Ahttps%3A//github.com/chenzhao2023/UPMAD_Net_BrainSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03494v1&entry.124074799=Read"},
{"title": "Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and\n  Low-Visibility Conditions", "author": "Lukas Schichler and Karin Festl and Selim Solmaz and Daniel Watzenig", "abstract": "  Despite significant progress in autonomous navigation, a critical gap remains\nin ensuring reliable localization in hazardous environments such as tunnels,\nurban disaster zones, and underground structures. Tunnels present a uniquely\ndifficult scenario: they are not only prone to GNSS signal loss, but also\nprovide little features for visual localization due to their repetitive walls\nand poor lighting. These conditions degrade conventional vision-based and\nLiDAR-based systems, which rely on distinguishable environmental features. To\naddress this, we propose a novel sensor fusion framework that integrates a\nthermal camera with a LiDAR to enable robust localization in tunnels and other\nperceptually degraded environments. The thermal camera provides resilience in\nlow-light or smoke conditions, while the LiDAR delivers precise depth\nperception and structural awareness. By combining these sensors, our framework\nensures continuous and accurate localization across diverse and dynamic\nenvironments. We use an Extended Kalman Filter (EKF) to fuse multi-sensor\ninputs, and leverages visual odometry and SLAM (Simultaneous Localization and\nMapping) techniques to process the sensor data, enabling robust motion\nestimation and mapping even in GNSS-denied environments. This fusion of sensor\nmodalities not only enhances system resilience but also provides a scalable\nsolution for cyber-physical systems in connected and autonomous vehicles\n(CAVs). To validate the framework, we conduct tests in a tunnel environment,\nsimulating sensor degradation and visibility challenges. The results\ndemonstrate that our method sustains accurate localization where standard\napproaches deteriorate due to the tunnels featureless geometry. The frameworks\nversatility makes it a promising solution for autonomous vehicles, inspection\nrobots, and other cyber-physical systems operating in constrained, perceptually\npoor environments.\n", "link": "http://arxiv.org/abs/2505.03565v1", "date": "2025-05-06", "relevancy": 2.3545, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6321}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5582}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thermal-LiDAR%20Fusion%20for%20Robust%20Tunnel%20Localization%20in%20GNSS-Denied%20and%0A%20%20Low-Visibility%20Conditions&body=Title%3A%20Thermal-LiDAR%20Fusion%20for%20Robust%20Tunnel%20Localization%20in%20GNSS-Denied%20and%0A%20%20Low-Visibility%20Conditions%0AAuthor%3A%20Lukas%20Schichler%20and%20Karin%20Festl%20and%20Selim%20Solmaz%20and%20Daniel%20Watzenig%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%20autonomous%20navigation%2C%20a%20critical%20gap%20remains%0Ain%20ensuring%20reliable%20localization%20in%20hazardous%20environments%20such%20as%20tunnels%2C%0Aurban%20disaster%20zones%2C%20and%20underground%20structures.%20Tunnels%20present%20a%20uniquely%0Adifficult%20scenario%3A%20they%20are%20not%20only%20prone%20to%20GNSS%20signal%20loss%2C%20but%20also%0Aprovide%20little%20features%20for%20visual%20localization%20due%20to%20their%20repetitive%20walls%0Aand%20poor%20lighting.%20These%20conditions%20degrade%20conventional%20vision-based%20and%0ALiDAR-based%20systems%2C%20which%20rely%20on%20distinguishable%20environmental%20features.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20sensor%20fusion%20framework%20that%20integrates%20a%0Athermal%20camera%20with%20a%20LiDAR%20to%20enable%20robust%20localization%20in%20tunnels%20and%20other%0Aperceptually%20degraded%20environments.%20The%20thermal%20camera%20provides%20resilience%20in%0Alow-light%20or%20smoke%20conditions%2C%20while%20the%20LiDAR%20delivers%20precise%20depth%0Aperception%20and%20structural%20awareness.%20By%20combining%20these%20sensors%2C%20our%20framework%0Aensures%20continuous%20and%20accurate%20localization%20across%20diverse%20and%20dynamic%0Aenvironments.%20We%20use%20an%20Extended%20Kalman%20Filter%20%28EKF%29%20to%20fuse%20multi-sensor%0Ainputs%2C%20and%20leverages%20visual%20odometry%20and%20SLAM%20%28Simultaneous%20Localization%20and%0AMapping%29%20techniques%20to%20process%20the%20sensor%20data%2C%20enabling%20robust%20motion%0Aestimation%20and%20mapping%20even%20in%20GNSS-denied%20environments.%20This%20fusion%20of%20sensor%0Amodalities%20not%20only%20enhances%20system%20resilience%20but%20also%20provides%20a%20scalable%0Asolution%20for%20cyber-physical%20systems%20in%20connected%20and%20autonomous%20vehicles%0A%28CAVs%29.%20To%20validate%20the%20framework%2C%20we%20conduct%20tests%20in%20a%20tunnel%20environment%2C%0Asimulating%20sensor%20degradation%20and%20visibility%20challenges.%20The%20results%0Ademonstrate%20that%20our%20method%20sustains%20accurate%20localization%20where%20standard%0Aapproaches%20deteriorate%20due%20to%20the%20tunnels%20featureless%20geometry.%20The%20frameworks%0Aversatility%20makes%20it%20a%20promising%20solution%20for%20autonomous%20vehicles%2C%20inspection%0Arobots%2C%20and%20other%20cyber-physical%20systems%20operating%20in%20constrained%2C%20perceptually%0Apoor%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermal-LiDAR%2520Fusion%2520for%2520Robust%2520Tunnel%2520Localization%2520in%2520GNSS-Denied%2520and%250A%2520%2520Low-Visibility%2520Conditions%26entry.906535625%3DLukas%2520Schichler%2520and%2520Karin%2520Festl%2520and%2520Selim%2520Solmaz%2520and%2520Daniel%2520Watzenig%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%2520autonomous%2520navigation%252C%2520a%2520critical%2520gap%2520remains%250Ain%2520ensuring%2520reliable%2520localization%2520in%2520hazardous%2520environments%2520such%2520as%2520tunnels%252C%250Aurban%2520disaster%2520zones%252C%2520and%2520underground%2520structures.%2520Tunnels%2520present%2520a%2520uniquely%250Adifficult%2520scenario%253A%2520they%2520are%2520not%2520only%2520prone%2520to%2520GNSS%2520signal%2520loss%252C%2520but%2520also%250Aprovide%2520little%2520features%2520for%2520visual%2520localization%2520due%2520to%2520their%2520repetitive%2520walls%250Aand%2520poor%2520lighting.%2520These%2520conditions%2520degrade%2520conventional%2520vision-based%2520and%250ALiDAR-based%2520systems%252C%2520which%2520rely%2520on%2520distinguishable%2520environmental%2520features.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520novel%2520sensor%2520fusion%2520framework%2520that%2520integrates%2520a%250Athermal%2520camera%2520with%2520a%2520LiDAR%2520to%2520enable%2520robust%2520localization%2520in%2520tunnels%2520and%2520other%250Aperceptually%2520degraded%2520environments.%2520The%2520thermal%2520camera%2520provides%2520resilience%2520in%250Alow-light%2520or%2520smoke%2520conditions%252C%2520while%2520the%2520LiDAR%2520delivers%2520precise%2520depth%250Aperception%2520and%2520structural%2520awareness.%2520By%2520combining%2520these%2520sensors%252C%2520our%2520framework%250Aensures%2520continuous%2520and%2520accurate%2520localization%2520across%2520diverse%2520and%2520dynamic%250Aenvironments.%2520We%2520use%2520an%2520Extended%2520Kalman%2520Filter%2520%2528EKF%2529%2520to%2520fuse%2520multi-sensor%250Ainputs%252C%2520and%2520leverages%2520visual%2520odometry%2520and%2520SLAM%2520%2528Simultaneous%2520Localization%2520and%250AMapping%2529%2520techniques%2520to%2520process%2520the%2520sensor%2520data%252C%2520enabling%2520robust%2520motion%250Aestimation%2520and%2520mapping%2520even%2520in%2520GNSS-denied%2520environments.%2520This%2520fusion%2520of%2520sensor%250Amodalities%2520not%2520only%2520enhances%2520system%2520resilience%2520but%2520also%2520provides%2520a%2520scalable%250Asolution%2520for%2520cyber-physical%2520systems%2520in%2520connected%2520and%2520autonomous%2520vehicles%250A%2528CAVs%2529.%2520To%2520validate%2520the%2520framework%252C%2520we%2520conduct%2520tests%2520in%2520a%2520tunnel%2520environment%252C%250Asimulating%2520sensor%2520degradation%2520and%2520visibility%2520challenges.%2520The%2520results%250Ademonstrate%2520that%2520our%2520method%2520sustains%2520accurate%2520localization%2520where%2520standard%250Aapproaches%2520deteriorate%2520due%2520to%2520the%2520tunnels%2520featureless%2520geometry.%2520The%2520frameworks%250Aversatility%2520makes%2520it%2520a%2520promising%2520solution%2520for%2520autonomous%2520vehicles%252C%2520inspection%250Arobots%252C%2520and%2520other%2520cyber-physical%2520systems%2520operating%2520in%2520constrained%252C%2520perceptually%250Apoor%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thermal-LiDAR%20Fusion%20for%20Robust%20Tunnel%20Localization%20in%20GNSS-Denied%20and%0A%20%20Low-Visibility%20Conditions&entry.906535625=Lukas%20Schichler%20and%20Karin%20Festl%20and%20Selim%20Solmaz%20and%20Daniel%20Watzenig&entry.1292438233=%20%20Despite%20significant%20progress%20in%20autonomous%20navigation%2C%20a%20critical%20gap%20remains%0Ain%20ensuring%20reliable%20localization%20in%20hazardous%20environments%20such%20as%20tunnels%2C%0Aurban%20disaster%20zones%2C%20and%20underground%20structures.%20Tunnels%20present%20a%20uniquely%0Adifficult%20scenario%3A%20they%20are%20not%20only%20prone%20to%20GNSS%20signal%20loss%2C%20but%20also%0Aprovide%20little%20features%20for%20visual%20localization%20due%20to%20their%20repetitive%20walls%0Aand%20poor%20lighting.%20These%20conditions%20degrade%20conventional%20vision-based%20and%0ALiDAR-based%20systems%2C%20which%20rely%20on%20distinguishable%20environmental%20features.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20sensor%20fusion%20framework%20that%20integrates%20a%0Athermal%20camera%20with%20a%20LiDAR%20to%20enable%20robust%20localization%20in%20tunnels%20and%20other%0Aperceptually%20degraded%20environments.%20The%20thermal%20camera%20provides%20resilience%20in%0Alow-light%20or%20smoke%20conditions%2C%20while%20the%20LiDAR%20delivers%20precise%20depth%0Aperception%20and%20structural%20awareness.%20By%20combining%20these%20sensors%2C%20our%20framework%0Aensures%20continuous%20and%20accurate%20localization%20across%20diverse%20and%20dynamic%0Aenvironments.%20We%20use%20an%20Extended%20Kalman%20Filter%20%28EKF%29%20to%20fuse%20multi-sensor%0Ainputs%2C%20and%20leverages%20visual%20odometry%20and%20SLAM%20%28Simultaneous%20Localization%20and%0AMapping%29%20techniques%20to%20process%20the%20sensor%20data%2C%20enabling%20robust%20motion%0Aestimation%20and%20mapping%20even%20in%20GNSS-denied%20environments.%20This%20fusion%20of%20sensor%0Amodalities%20not%20only%20enhances%20system%20resilience%20but%20also%20provides%20a%20scalable%0Asolution%20for%20cyber-physical%20systems%20in%20connected%20and%20autonomous%20vehicles%0A%28CAVs%29.%20To%20validate%20the%20framework%2C%20we%20conduct%20tests%20in%20a%20tunnel%20environment%2C%0Asimulating%20sensor%20degradation%20and%20visibility%20challenges.%20The%20results%0Ademonstrate%20that%20our%20method%20sustains%20accurate%20localization%20where%20standard%0Aapproaches%20deteriorate%20due%20to%20the%20tunnels%20featureless%20geometry.%20The%20frameworks%0Aversatility%20makes%20it%20a%20promising%20solution%20for%20autonomous%20vehicles%2C%20inspection%0Arobots%2C%20and%20other%20cyber-physical%20systems%20operating%20in%20constrained%2C%20perceptually%0Apoor%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03565v1&entry.124074799=Read"},
{"title": "Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text\n  Representation Learning", "author": "Fran\u00e7ois Role and S\u00e9bastien Meyer and Victor Amblard", "abstract": "  Vision-language models (VLMs) allow to embed texts and images in a shared\nrepresentation space. However, it has been shown that these models are subject\nto a modality gap phenomenon meaning there exists a clear separation between\nthe embeddings from one modality and another in the embedding space. While this\nmisalignment is detrimental for downstream tasks such as multimodal retrieval,\nmultimodal clustering or zero-shot classification, etc. no generic and\npractical methods have so far been proposed to assess it precisely and even\nreduce it. We therefore propose novel measures and effective techniques\n(spectral- and optimal transport-based methods) to achieve this goal. Extensive\nexperiments conducted on several image-text datasets and models demonstrate\ntheir effectiveness and beneficial effects on downstream tasks. Our code is\navailable at the URL provided in the paper's abstract.\n", "link": "http://arxiv.org/abs/2505.03703v1", "date": "2025-05-06", "relevancy": 2.3124, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.599}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fill%20the%20Gap%3A%20Quantifying%20and%20Reducing%20the%20Modality%20Gap%20in%20Image-Text%0A%20%20Representation%20Learning&body=Title%3A%20Fill%20the%20Gap%3A%20Quantifying%20and%20Reducing%20the%20Modality%20Gap%20in%20Image-Text%0A%20%20Representation%20Learning%0AAuthor%3A%20Fran%C3%A7ois%20Role%20and%20S%C3%A9bastien%20Meyer%20and%20Victor%20Amblard%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20allow%20to%20embed%20texts%20and%20images%20in%20a%20shared%0Arepresentation%20space.%20However%2C%20it%20has%20been%20shown%20that%20these%20models%20are%20subject%0Ato%20a%20modality%20gap%20phenomenon%20meaning%20there%20exists%20a%20clear%20separation%20between%0Athe%20embeddings%20from%20one%20modality%20and%20another%20in%20the%20embedding%20space.%20While%20this%0Amisalignment%20is%20detrimental%20for%20downstream%20tasks%20such%20as%20multimodal%20retrieval%2C%0Amultimodal%20clustering%20or%20zero-shot%20classification%2C%20etc.%20no%20generic%20and%0Apractical%20methods%20have%20so%20far%20been%20proposed%20to%20assess%20it%20precisely%20and%20even%0Areduce%20it.%20We%20therefore%20propose%20novel%20measures%20and%20effective%20techniques%0A%28spectral-%20and%20optimal%20transport-based%20methods%29%20to%20achieve%20this%20goal.%20Extensive%0Aexperiments%20conducted%20on%20several%20image-text%20datasets%20and%20models%20demonstrate%0Atheir%20effectiveness%20and%20beneficial%20effects%20on%20downstream%20tasks.%20Our%20code%20is%0Aavailable%20at%20the%20URL%20provided%20in%20the%20paper%27s%20abstract.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFill%2520the%2520Gap%253A%2520Quantifying%2520and%2520Reducing%2520the%2520Modality%2520Gap%2520in%2520Image-Text%250A%2520%2520Representation%2520Learning%26entry.906535625%3DFran%25C3%25A7ois%2520Role%2520and%2520S%25C3%25A9bastien%2520Meyer%2520and%2520Victor%2520Amblard%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520allow%2520to%2520embed%2520texts%2520and%2520images%2520in%2520a%2520shared%250Arepresentation%2520space.%2520However%252C%2520it%2520has%2520been%2520shown%2520that%2520these%2520models%2520are%2520subject%250Ato%2520a%2520modality%2520gap%2520phenomenon%2520meaning%2520there%2520exists%2520a%2520clear%2520separation%2520between%250Athe%2520embeddings%2520from%2520one%2520modality%2520and%2520another%2520in%2520the%2520embedding%2520space.%2520While%2520this%250Amisalignment%2520is%2520detrimental%2520for%2520downstream%2520tasks%2520such%2520as%2520multimodal%2520retrieval%252C%250Amultimodal%2520clustering%2520or%2520zero-shot%2520classification%252C%2520etc.%2520no%2520generic%2520and%250Apractical%2520methods%2520have%2520so%2520far%2520been%2520proposed%2520to%2520assess%2520it%2520precisely%2520and%2520even%250Areduce%2520it.%2520We%2520therefore%2520propose%2520novel%2520measures%2520and%2520effective%2520techniques%250A%2528spectral-%2520and%2520optimal%2520transport-based%2520methods%2529%2520to%2520achieve%2520this%2520goal.%2520Extensive%250Aexperiments%2520conducted%2520on%2520several%2520image-text%2520datasets%2520and%2520models%2520demonstrate%250Atheir%2520effectiveness%2520and%2520beneficial%2520effects%2520on%2520downstream%2520tasks.%2520Our%2520code%2520is%250Aavailable%2520at%2520the%2520URL%2520provided%2520in%2520the%2520paper%2527s%2520abstract.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fill%20the%20Gap%3A%20Quantifying%20and%20Reducing%20the%20Modality%20Gap%20in%20Image-Text%0A%20%20Representation%20Learning&entry.906535625=Fran%C3%A7ois%20Role%20and%20S%C3%A9bastien%20Meyer%20and%20Victor%20Amblard&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20allow%20to%20embed%20texts%20and%20images%20in%20a%20shared%0Arepresentation%20space.%20However%2C%20it%20has%20been%20shown%20that%20these%20models%20are%20subject%0Ato%20a%20modality%20gap%20phenomenon%20meaning%20there%20exists%20a%20clear%20separation%20between%0Athe%20embeddings%20from%20one%20modality%20and%20another%20in%20the%20embedding%20space.%20While%20this%0Amisalignment%20is%20detrimental%20for%20downstream%20tasks%20such%20as%20multimodal%20retrieval%2C%0Amultimodal%20clustering%20or%20zero-shot%20classification%2C%20etc.%20no%20generic%20and%0Apractical%20methods%20have%20so%20far%20been%20proposed%20to%20assess%20it%20precisely%20and%20even%0Areduce%20it.%20We%20therefore%20propose%20novel%20measures%20and%20effective%20techniques%0A%28spectral-%20and%20optimal%20transport-based%20methods%29%20to%20achieve%20this%20goal.%20Extensive%0Aexperiments%20conducted%20on%20several%20image-text%20datasets%20and%20models%20demonstrate%0Atheir%20effectiveness%20and%20beneficial%20effects%20on%20downstream%20tasks.%20Our%20code%20is%0Aavailable%20at%20the%20URL%20provided%20in%20the%20paper%27s%20abstract.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03703v1&entry.124074799=Read"},
{"title": "Don't Mesh with Me: Generating Constructive Solid Geometry Instead of\n  Meshes by Fine-Tuning a Code-Generation LLM", "author": "Maximilian Mews and Ansar Aynetdinov and Vivian Schiller and Peter Eisert and Alan Akbik", "abstract": "  While recent advancements in machine learning, such as LLMs, are\nrevolutionizing software development and creative industries, they have had\nminimal impact on engineers designing mechanical parts, which remains largely a\nmanual process. Existing approaches to generating 3D geometry most commonly use\nmeshes as a 3D representation. While meshes are suitable for assets in video\ngames or animations, they lack sufficient precision and adaptability for\nmechanical engineering purposes. This paper introduces a novel approach for the\ngeneration of 3D geometry that generates surface-based Constructive Solid\nGeometry (CSG) by leveraging a code-generation LLM. First, we create a dataset\nof 3D mechanical parts represented as code scripts by converting Boundary\nRepresentation geometry (BREP) into CSG-based Python scripts. Second, we create\nannotations in natural language using GPT-4. The resulting dataset is used to\nfine-tune a code-generation LLM. The fine-tuned LLM can complete geometries\nbased on positional input and natural language in a plausible way,\ndemonstrating geometric understanding.\n", "link": "http://arxiv.org/abs/2411.15279v2", "date": "2025-05-06", "relevancy": 2.3034, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6115}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5598}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Mesh%20with%20Me%3A%20Generating%20Constructive%20Solid%20Geometry%20Instead%20of%0A%20%20Meshes%20by%20Fine-Tuning%20a%20Code-Generation%20LLM&body=Title%3A%20Don%27t%20Mesh%20with%20Me%3A%20Generating%20Constructive%20Solid%20Geometry%20Instead%20of%0A%20%20Meshes%20by%20Fine-Tuning%20a%20Code-Generation%20LLM%0AAuthor%3A%20Maximilian%20Mews%20and%20Ansar%20Aynetdinov%20and%20Vivian%20Schiller%20and%20Peter%20Eisert%20and%20Alan%20Akbik%0AAbstract%3A%20%20%20While%20recent%20advancements%20in%20machine%20learning%2C%20such%20as%20LLMs%2C%20are%0Arevolutionizing%20software%20development%20and%20creative%20industries%2C%20they%20have%20had%0Aminimal%20impact%20on%20engineers%20designing%20mechanical%20parts%2C%20which%20remains%20largely%20a%0Amanual%20process.%20Existing%20approaches%20to%20generating%203D%20geometry%20most%20commonly%20use%0Ameshes%20as%20a%203D%20representation.%20While%20meshes%20are%20suitable%20for%20assets%20in%20video%0Agames%20or%20animations%2C%20they%20lack%20sufficient%20precision%20and%20adaptability%20for%0Amechanical%20engineering%20purposes.%20This%20paper%20introduces%20a%20novel%20approach%20for%20the%0Ageneration%20of%203D%20geometry%20that%20generates%20surface-based%20Constructive%20Solid%0AGeometry%20%28CSG%29%20by%20leveraging%20a%20code-generation%20LLM.%20First%2C%20we%20create%20a%20dataset%0Aof%203D%20mechanical%20parts%20represented%20as%20code%20scripts%20by%20converting%20Boundary%0ARepresentation%20geometry%20%28BREP%29%20into%20CSG-based%20Python%20scripts.%20Second%2C%20we%20create%0Aannotations%20in%20natural%20language%20using%20GPT-4.%20The%20resulting%20dataset%20is%20used%20to%0Afine-tune%20a%20code-generation%20LLM.%20The%20fine-tuned%20LLM%20can%20complete%20geometries%0Abased%20on%20positional%20input%20and%20natural%20language%20in%20a%20plausible%20way%2C%0Ademonstrating%20geometric%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Mesh%2520with%2520Me%253A%2520Generating%2520Constructive%2520Solid%2520Geometry%2520Instead%2520of%250A%2520%2520Meshes%2520by%2520Fine-Tuning%2520a%2520Code-Generation%2520LLM%26entry.906535625%3DMaximilian%2520Mews%2520and%2520Ansar%2520Aynetdinov%2520and%2520Vivian%2520Schiller%2520and%2520Peter%2520Eisert%2520and%2520Alan%2520Akbik%26entry.1292438233%3D%2520%2520While%2520recent%2520advancements%2520in%2520machine%2520learning%252C%2520such%2520as%2520LLMs%252C%2520are%250Arevolutionizing%2520software%2520development%2520and%2520creative%2520industries%252C%2520they%2520have%2520had%250Aminimal%2520impact%2520on%2520engineers%2520designing%2520mechanical%2520parts%252C%2520which%2520remains%2520largely%2520a%250Amanual%2520process.%2520Existing%2520approaches%2520to%2520generating%25203D%2520geometry%2520most%2520commonly%2520use%250Ameshes%2520as%2520a%25203D%2520representation.%2520While%2520meshes%2520are%2520suitable%2520for%2520assets%2520in%2520video%250Agames%2520or%2520animations%252C%2520they%2520lack%2520sufficient%2520precision%2520and%2520adaptability%2520for%250Amechanical%2520engineering%2520purposes.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520for%2520the%250Ageneration%2520of%25203D%2520geometry%2520that%2520generates%2520surface-based%2520Constructive%2520Solid%250AGeometry%2520%2528CSG%2529%2520by%2520leveraging%2520a%2520code-generation%2520LLM.%2520First%252C%2520we%2520create%2520a%2520dataset%250Aof%25203D%2520mechanical%2520parts%2520represented%2520as%2520code%2520scripts%2520by%2520converting%2520Boundary%250ARepresentation%2520geometry%2520%2528BREP%2529%2520into%2520CSG-based%2520Python%2520scripts.%2520Second%252C%2520we%2520create%250Aannotations%2520in%2520natural%2520language%2520using%2520GPT-4.%2520The%2520resulting%2520dataset%2520is%2520used%2520to%250Afine-tune%2520a%2520code-generation%2520LLM.%2520The%2520fine-tuned%2520LLM%2520can%2520complete%2520geometries%250Abased%2520on%2520positional%2520input%2520and%2520natural%2520language%2520in%2520a%2520plausible%2520way%252C%250Ademonstrating%2520geometric%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Mesh%20with%20Me%3A%20Generating%20Constructive%20Solid%20Geometry%20Instead%20of%0A%20%20Meshes%20by%20Fine-Tuning%20a%20Code-Generation%20LLM&entry.906535625=Maximilian%20Mews%20and%20Ansar%20Aynetdinov%20and%20Vivian%20Schiller%20and%20Peter%20Eisert%20and%20Alan%20Akbik&entry.1292438233=%20%20While%20recent%20advancements%20in%20machine%20learning%2C%20such%20as%20LLMs%2C%20are%0Arevolutionizing%20software%20development%20and%20creative%20industries%2C%20they%20have%20had%0Aminimal%20impact%20on%20engineers%20designing%20mechanical%20parts%2C%20which%20remains%20largely%20a%0Amanual%20process.%20Existing%20approaches%20to%20generating%203D%20geometry%20most%20commonly%20use%0Ameshes%20as%20a%203D%20representation.%20While%20meshes%20are%20suitable%20for%20assets%20in%20video%0Agames%20or%20animations%2C%20they%20lack%20sufficient%20precision%20and%20adaptability%20for%0Amechanical%20engineering%20purposes.%20This%20paper%20introduces%20a%20novel%20approach%20for%20the%0Ageneration%20of%203D%20geometry%20that%20generates%20surface-based%20Constructive%20Solid%0AGeometry%20%28CSG%29%20by%20leveraging%20a%20code-generation%20LLM.%20First%2C%20we%20create%20a%20dataset%0Aof%203D%20mechanical%20parts%20represented%20as%20code%20scripts%20by%20converting%20Boundary%0ARepresentation%20geometry%20%28BREP%29%20into%20CSG-based%20Python%20scripts.%20Second%2C%20we%20create%0Aannotations%20in%20natural%20language%20using%20GPT-4.%20The%20resulting%20dataset%20is%20used%20to%0Afine-tune%20a%20code-generation%20LLM.%20The%20fine-tuned%20LLM%20can%20complete%20geometries%0Abased%20on%20positional%20input%20and%20natural%20language%20in%20a%20plausible%20way%2C%0Ademonstrating%20geometric%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15279v2&entry.124074799=Read"},
{"title": "Framework GNN-AID: Graph Neural Network Analysis Interpretation and\n  Defense", "author": "Kirill Lukyanov and Mikhail Drobyshevskiy and Georgii Sazonov and Mikhail Soloviov and Ilya Makarov", "abstract": "  The growing need for Trusted AI (TAI) highlights the importance of\ninterpretability and robustness in machine learning models. However, many\nexisting tools overlook graph data and rarely combine these two aspects into a\nsingle solution. Graph Neural Networks (GNNs) have become a popular approach,\nachieving top results across various tasks. We introduce GNN-AID (Graph Neural\nNetwork Analysis, Interpretation, and Defense), an open-source framework\ndesigned for graph data to address this gap. Built as a Python library, GNN-AID\nsupports advanced trust methods and architectural layers, allowing users to\nanalyze graph datasets and GNN behavior using attacks, defenses, and\ninterpretability methods.\n  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,\nand support for any GNNs through customizable interfaces. It also includes a\nweb interface with tools for graph visualization and no-code features like an\ninteractive model builder, simplifying the exploration and analysis of GNNs.\nThe framework also supports MLOps techniques, ensuring reproducibility and\nresult versioning to track and revisit analyses efficiently.\n  GNN-AID is a flexible tool for developers and researchers. It helps\ndevelopers create, analyze, and customize graph models, while also providing\naccess to prebuilt datasets and models for quick experimentation. Researchers\ncan use the framework to explore advanced topics on the relationship between\ninterpretability and robustness, test defense strategies, and combine methods\nto protect against different types of attacks.\n  We also show how defenses against evasion and poisoning attacks can conflict\nwhen applied to graph data, highlighting the complex connections between\ndefense strategies.\n  GNN-AID is available at\n\\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}\n", "link": "http://arxiv.org/abs/2505.03424v1", "date": "2025-05-06", "relevancy": 2.2726, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4763}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4532}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Framework%20GNN-AID%3A%20Graph%20Neural%20Network%20Analysis%20Interpretation%20and%0A%20%20Defense&body=Title%3A%20Framework%20GNN-AID%3A%20Graph%20Neural%20Network%20Analysis%20Interpretation%20and%0A%20%20Defense%0AAuthor%3A%20Kirill%20Lukyanov%20and%20Mikhail%20Drobyshevskiy%20and%20Georgii%20Sazonov%20and%20Mikhail%20Soloviov%20and%20Ilya%20Makarov%0AAbstract%3A%20%20%20The%20growing%20need%20for%20Trusted%20AI%20%28TAI%29%20highlights%20the%20importance%20of%0Ainterpretability%20and%20robustness%20in%20machine%20learning%20models.%20However%2C%20many%0Aexisting%20tools%20overlook%20graph%20data%20and%20rarely%20combine%20these%20two%20aspects%20into%20a%0Asingle%20solution.%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20a%20popular%20approach%2C%0Aachieving%20top%20results%20across%20various%20tasks.%20We%20introduce%20GNN-AID%20%28Graph%20Neural%0ANetwork%20Analysis%2C%20Interpretation%2C%20and%20Defense%29%2C%20an%20open-source%20framework%0Adesigned%20for%20graph%20data%20to%20address%20this%20gap.%20Built%20as%20a%20Python%20library%2C%20GNN-AID%0Asupports%20advanced%20trust%20methods%20and%20architectural%20layers%2C%20allowing%20users%20to%0Aanalyze%20graph%20datasets%20and%20GNN%20behavior%20using%20attacks%2C%20defenses%2C%20and%0Ainterpretability%20methods.%0A%20%20GNN-AID%20is%20built%20on%20PyTorch-Geometric%2C%20offering%20preloaded%20datasets%2C%20models%2C%0Aand%20support%20for%20any%20GNNs%20through%20customizable%20interfaces.%20It%20also%20includes%20a%0Aweb%20interface%20with%20tools%20for%20graph%20visualization%20and%20no-code%20features%20like%20an%0Ainteractive%20model%20builder%2C%20simplifying%20the%20exploration%20and%20analysis%20of%20GNNs.%0AThe%20framework%20also%20supports%20MLOps%20techniques%2C%20ensuring%20reproducibility%20and%0Aresult%20versioning%20to%20track%20and%20revisit%20analyses%20efficiently.%0A%20%20GNN-AID%20is%20a%20flexible%20tool%20for%20developers%20and%20researchers.%20It%20helps%0Adevelopers%20create%2C%20analyze%2C%20and%20customize%20graph%20models%2C%20while%20also%20providing%0Aaccess%20to%20prebuilt%20datasets%20and%20models%20for%20quick%20experimentation.%20Researchers%0Acan%20use%20the%20framework%20to%20explore%20advanced%20topics%20on%20the%20relationship%20between%0Ainterpretability%20and%20robustness%2C%20test%20defense%20strategies%2C%20and%20combine%20methods%0Ato%20protect%20against%20different%20types%20of%20attacks.%0A%20%20We%20also%20show%20how%20defenses%20against%20evasion%20and%20poisoning%20attacks%20can%20conflict%0Awhen%20applied%20to%20graph%20data%2C%20highlighting%20the%20complex%20connections%20between%0Adefense%20strategies.%0A%20%20GNN-AID%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/ispras/GNN-AID%7D%7Bgithub.com/ispras/GNN-AID%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFramework%2520GNN-AID%253A%2520Graph%2520Neural%2520Network%2520Analysis%2520Interpretation%2520and%250A%2520%2520Defense%26entry.906535625%3DKirill%2520Lukyanov%2520and%2520Mikhail%2520Drobyshevskiy%2520and%2520Georgii%2520Sazonov%2520and%2520Mikhail%2520Soloviov%2520and%2520Ilya%2520Makarov%26entry.1292438233%3D%2520%2520The%2520growing%2520need%2520for%2520Trusted%2520AI%2520%2528TAI%2529%2520highlights%2520the%2520importance%2520of%250Ainterpretability%2520and%2520robustness%2520in%2520machine%2520learning%2520models.%2520However%252C%2520many%250Aexisting%2520tools%2520overlook%2520graph%2520data%2520and%2520rarely%2520combine%2520these%2520two%2520aspects%2520into%2520a%250Asingle%2520solution.%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520a%2520popular%2520approach%252C%250Aachieving%2520top%2520results%2520across%2520various%2520tasks.%2520We%2520introduce%2520GNN-AID%2520%2528Graph%2520Neural%250ANetwork%2520Analysis%252C%2520Interpretation%252C%2520and%2520Defense%2529%252C%2520an%2520open-source%2520framework%250Adesigned%2520for%2520graph%2520data%2520to%2520address%2520this%2520gap.%2520Built%2520as%2520a%2520Python%2520library%252C%2520GNN-AID%250Asupports%2520advanced%2520trust%2520methods%2520and%2520architectural%2520layers%252C%2520allowing%2520users%2520to%250Aanalyze%2520graph%2520datasets%2520and%2520GNN%2520behavior%2520using%2520attacks%252C%2520defenses%252C%2520and%250Ainterpretability%2520methods.%250A%2520%2520GNN-AID%2520is%2520built%2520on%2520PyTorch-Geometric%252C%2520offering%2520preloaded%2520datasets%252C%2520models%252C%250Aand%2520support%2520for%2520any%2520GNNs%2520through%2520customizable%2520interfaces.%2520It%2520also%2520includes%2520a%250Aweb%2520interface%2520with%2520tools%2520for%2520graph%2520visualization%2520and%2520no-code%2520features%2520like%2520an%250Ainteractive%2520model%2520builder%252C%2520simplifying%2520the%2520exploration%2520and%2520analysis%2520of%2520GNNs.%250AThe%2520framework%2520also%2520supports%2520MLOps%2520techniques%252C%2520ensuring%2520reproducibility%2520and%250Aresult%2520versioning%2520to%2520track%2520and%2520revisit%2520analyses%2520efficiently.%250A%2520%2520GNN-AID%2520is%2520a%2520flexible%2520tool%2520for%2520developers%2520and%2520researchers.%2520It%2520helps%250Adevelopers%2520create%252C%2520analyze%252C%2520and%2520customize%2520graph%2520models%252C%2520while%2520also%2520providing%250Aaccess%2520to%2520prebuilt%2520datasets%2520and%2520models%2520for%2520quick%2520experimentation.%2520Researchers%250Acan%2520use%2520the%2520framework%2520to%2520explore%2520advanced%2520topics%2520on%2520the%2520relationship%2520between%250Ainterpretability%2520and%2520robustness%252C%2520test%2520defense%2520strategies%252C%2520and%2520combine%2520methods%250Ato%2520protect%2520against%2520different%2520types%2520of%2520attacks.%250A%2520%2520We%2520also%2520show%2520how%2520defenses%2520against%2520evasion%2520and%2520poisoning%2520attacks%2520can%2520conflict%250Awhen%2520applied%2520to%2520graph%2520data%252C%2520highlighting%2520the%2520complex%2520connections%2520between%250Adefense%2520strategies.%250A%2520%2520GNN-AID%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/ispras/GNN-AID%257D%257Bgithub.com/ispras/GNN-AID%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Framework%20GNN-AID%3A%20Graph%20Neural%20Network%20Analysis%20Interpretation%20and%0A%20%20Defense&entry.906535625=Kirill%20Lukyanov%20and%20Mikhail%20Drobyshevskiy%20and%20Georgii%20Sazonov%20and%20Mikhail%20Soloviov%20and%20Ilya%20Makarov&entry.1292438233=%20%20The%20growing%20need%20for%20Trusted%20AI%20%28TAI%29%20highlights%20the%20importance%20of%0Ainterpretability%20and%20robustness%20in%20machine%20learning%20models.%20However%2C%20many%0Aexisting%20tools%20overlook%20graph%20data%20and%20rarely%20combine%20these%20two%20aspects%20into%20a%0Asingle%20solution.%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20a%20popular%20approach%2C%0Aachieving%20top%20results%20across%20various%20tasks.%20We%20introduce%20GNN-AID%20%28Graph%20Neural%0ANetwork%20Analysis%2C%20Interpretation%2C%20and%20Defense%29%2C%20an%20open-source%20framework%0Adesigned%20for%20graph%20data%20to%20address%20this%20gap.%20Built%20as%20a%20Python%20library%2C%20GNN-AID%0Asupports%20advanced%20trust%20methods%20and%20architectural%20layers%2C%20allowing%20users%20to%0Aanalyze%20graph%20datasets%20and%20GNN%20behavior%20using%20attacks%2C%20defenses%2C%20and%0Ainterpretability%20methods.%0A%20%20GNN-AID%20is%20built%20on%20PyTorch-Geometric%2C%20offering%20preloaded%20datasets%2C%20models%2C%0Aand%20support%20for%20any%20GNNs%20through%20customizable%20interfaces.%20It%20also%20includes%20a%0Aweb%20interface%20with%20tools%20for%20graph%20visualization%20and%20no-code%20features%20like%20an%0Ainteractive%20model%20builder%2C%20simplifying%20the%20exploration%20and%20analysis%20of%20GNNs.%0AThe%20framework%20also%20supports%20MLOps%20techniques%2C%20ensuring%20reproducibility%20and%0Aresult%20versioning%20to%20track%20and%20revisit%20analyses%20efficiently.%0A%20%20GNN-AID%20is%20a%20flexible%20tool%20for%20developers%20and%20researchers.%20It%20helps%0Adevelopers%20create%2C%20analyze%2C%20and%20customize%20graph%20models%2C%20while%20also%20providing%0Aaccess%20to%20prebuilt%20datasets%20and%20models%20for%20quick%20experimentation.%20Researchers%0Acan%20use%20the%20framework%20to%20explore%20advanced%20topics%20on%20the%20relationship%20between%0Ainterpretability%20and%20robustness%2C%20test%20defense%20strategies%2C%20and%20combine%20methods%0Ato%20protect%20against%20different%20types%20of%20attacks.%0A%20%20We%20also%20show%20how%20defenses%20against%20evasion%20and%20poisoning%20attacks%20can%20conflict%0Awhen%20applied%20to%20graph%20data%2C%20highlighting%20the%20complex%20connections%20between%0Adefense%20strategies.%0A%20%20GNN-AID%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/ispras/GNN-AID%7D%7Bgithub.com/ispras/GNN-AID%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03424v1&entry.124074799=Read"},
{"title": "CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point\n  Cloud Fusion and Zero-Shot Image Inpainting", "author": "Huawei Sun and Bora Kunter Sahin and Georg Stettinger and Maximilian Bernhard and Matthias Schubert and Robert Wille", "abstract": "  Segmenting objects in an environment is a crucial task for autonomous driving\nand robotics, as it enables a better understanding of the surroundings of each\nagent. Although camera sensors provide rich visual details, they are vulnerable\nto adverse weather conditions. In contrast, radar sensors remain robust under\nsuch conditions, but often produce sparse and noisy data. Therefore, a\npromising approach is to fuse information from both sensors. In this work, we\npropose a novel framework to enhance camera-only baselines by integrating a\ndiffusion model into a camera-radar fusion architecture. We leverage radar\npoint features to create pseudo-masks using the Segment-Anything model,\ntreating the projected radar points as point prompts. Additionally, we propose\na noise reduction unit to denoise these pseudo-masks, which are further used to\ngenerate inpainted images that complete the missing information in the original\nimages. Our method improves the camera-only segmentation baseline by 2.63% in\nmIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the\nWaterscenes dataset. This demonstrates the effectiveness of our approach for\nsemantic segmentation using camera-radar fusion under adverse weather\nconditions.\n", "link": "http://arxiv.org/abs/2505.03679v1", "date": "2025-05-06", "relevancy": 2.2601, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5683}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.565}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaRaFFusion%3A%20Improving%202D%20Semantic%20Segmentation%20with%20Camera-Radar%20Point%0A%20%20Cloud%20Fusion%20and%20Zero-Shot%20Image%20Inpainting&body=Title%3A%20CaRaFFusion%3A%20Improving%202D%20Semantic%20Segmentation%20with%20Camera-Radar%20Point%0A%20%20Cloud%20Fusion%20and%20Zero-Shot%20Image%20Inpainting%0AAuthor%3A%20Huawei%20Sun%20and%20Bora%20Kunter%20Sahin%20and%20Georg%20Stettinger%20and%20Maximilian%20Bernhard%20and%20Matthias%20Schubert%20and%20Robert%20Wille%0AAbstract%3A%20%20%20Segmenting%20objects%20in%20an%20environment%20is%20a%20crucial%20task%20for%20autonomous%20driving%0Aand%20robotics%2C%20as%20it%20enables%20a%20better%20understanding%20of%20the%20surroundings%20of%20each%0Aagent.%20Although%20camera%20sensors%20provide%20rich%20visual%20details%2C%20they%20are%20vulnerable%0Ato%20adverse%20weather%20conditions.%20In%20contrast%2C%20radar%20sensors%20remain%20robust%20under%0Asuch%20conditions%2C%20but%20often%20produce%20sparse%20and%20noisy%20data.%20Therefore%2C%20a%0Apromising%20approach%20is%20to%20fuse%20information%20from%20both%20sensors.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20framework%20to%20enhance%20camera-only%20baselines%20by%20integrating%20a%0Adiffusion%20model%20into%20a%20camera-radar%20fusion%20architecture.%20We%20leverage%20radar%0Apoint%20features%20to%20create%20pseudo-masks%20using%20the%20Segment-Anything%20model%2C%0Atreating%20the%20projected%20radar%20points%20as%20point%20prompts.%20Additionally%2C%20we%20propose%0Aa%20noise%20reduction%20unit%20to%20denoise%20these%20pseudo-masks%2C%20which%20are%20further%20used%20to%0Agenerate%20inpainted%20images%20that%20complete%20the%20missing%20information%20in%20the%20original%0Aimages.%20Our%20method%20improves%20the%20camera-only%20segmentation%20baseline%20by%202.63%25%20in%0AmIoU%20and%20enhances%20our%20camera-radar%20fusion%20architecture%20by%201.48%25%20in%20mIoU%20on%20the%0AWaterscenes%20dataset.%20This%20demonstrates%20the%20effectiveness%20of%20our%20approach%20for%0Asemantic%20segmentation%20using%20camera-radar%20fusion%20under%20adverse%20weather%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaRaFFusion%253A%2520Improving%25202D%2520Semantic%2520Segmentation%2520with%2520Camera-Radar%2520Point%250A%2520%2520Cloud%2520Fusion%2520and%2520Zero-Shot%2520Image%2520Inpainting%26entry.906535625%3DHuawei%2520Sun%2520and%2520Bora%2520Kunter%2520Sahin%2520and%2520Georg%2520Stettinger%2520and%2520Maximilian%2520Bernhard%2520and%2520Matthias%2520Schubert%2520and%2520Robert%2520Wille%26entry.1292438233%3D%2520%2520Segmenting%2520objects%2520in%2520an%2520environment%2520is%2520a%2520crucial%2520task%2520for%2520autonomous%2520driving%250Aand%2520robotics%252C%2520as%2520it%2520enables%2520a%2520better%2520understanding%2520of%2520the%2520surroundings%2520of%2520each%250Aagent.%2520Although%2520camera%2520sensors%2520provide%2520rich%2520visual%2520details%252C%2520they%2520are%2520vulnerable%250Ato%2520adverse%2520weather%2520conditions.%2520In%2520contrast%252C%2520radar%2520sensors%2520remain%2520robust%2520under%250Asuch%2520conditions%252C%2520but%2520often%2520produce%2520sparse%2520and%2520noisy%2520data.%2520Therefore%252C%2520a%250Apromising%2520approach%2520is%2520to%2520fuse%2520information%2520from%2520both%2520sensors.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520to%2520enhance%2520camera-only%2520baselines%2520by%2520integrating%2520a%250Adiffusion%2520model%2520into%2520a%2520camera-radar%2520fusion%2520architecture.%2520We%2520leverage%2520radar%250Apoint%2520features%2520to%2520create%2520pseudo-masks%2520using%2520the%2520Segment-Anything%2520model%252C%250Atreating%2520the%2520projected%2520radar%2520points%2520as%2520point%2520prompts.%2520Additionally%252C%2520we%2520propose%250Aa%2520noise%2520reduction%2520unit%2520to%2520denoise%2520these%2520pseudo-masks%252C%2520which%2520are%2520further%2520used%2520to%250Agenerate%2520inpainted%2520images%2520that%2520complete%2520the%2520missing%2520information%2520in%2520the%2520original%250Aimages.%2520Our%2520method%2520improves%2520the%2520camera-only%2520segmentation%2520baseline%2520by%25202.63%2525%2520in%250AmIoU%2520and%2520enhances%2520our%2520camera-radar%2520fusion%2520architecture%2520by%25201.48%2525%2520in%2520mIoU%2520on%2520the%250AWaterscenes%2520dataset.%2520This%2520demonstrates%2520the%2520effectiveness%2520of%2520our%2520approach%2520for%250Asemantic%2520segmentation%2520using%2520camera-radar%2520fusion%2520under%2520adverse%2520weather%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaRaFFusion%3A%20Improving%202D%20Semantic%20Segmentation%20with%20Camera-Radar%20Point%0A%20%20Cloud%20Fusion%20and%20Zero-Shot%20Image%20Inpainting&entry.906535625=Huawei%20Sun%20and%20Bora%20Kunter%20Sahin%20and%20Georg%20Stettinger%20and%20Maximilian%20Bernhard%20and%20Matthias%20Schubert%20and%20Robert%20Wille&entry.1292438233=%20%20Segmenting%20objects%20in%20an%20environment%20is%20a%20crucial%20task%20for%20autonomous%20driving%0Aand%20robotics%2C%20as%20it%20enables%20a%20better%20understanding%20of%20the%20surroundings%20of%20each%0Aagent.%20Although%20camera%20sensors%20provide%20rich%20visual%20details%2C%20they%20are%20vulnerable%0Ato%20adverse%20weather%20conditions.%20In%20contrast%2C%20radar%20sensors%20remain%20robust%20under%0Asuch%20conditions%2C%20but%20often%20produce%20sparse%20and%20noisy%20data.%20Therefore%2C%20a%0Apromising%20approach%20is%20to%20fuse%20information%20from%20both%20sensors.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20framework%20to%20enhance%20camera-only%20baselines%20by%20integrating%20a%0Adiffusion%20model%20into%20a%20camera-radar%20fusion%20architecture.%20We%20leverage%20radar%0Apoint%20features%20to%20create%20pseudo-masks%20using%20the%20Segment-Anything%20model%2C%0Atreating%20the%20projected%20radar%20points%20as%20point%20prompts.%20Additionally%2C%20we%20propose%0Aa%20noise%20reduction%20unit%20to%20denoise%20these%20pseudo-masks%2C%20which%20are%20further%20used%20to%0Agenerate%20inpainted%20images%20that%20complete%20the%20missing%20information%20in%20the%20original%0Aimages.%20Our%20method%20improves%20the%20camera-only%20segmentation%20baseline%20by%202.63%25%20in%0AmIoU%20and%20enhances%20our%20camera-radar%20fusion%20architecture%20by%201.48%25%20in%20mIoU%20on%20the%0AWaterscenes%20dataset.%20This%20demonstrates%20the%20effectiveness%20of%20our%20approach%20for%0Asemantic%20segmentation%20using%20camera-radar%20fusion%20under%20adverse%20weather%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03679v1&entry.124074799=Read"},
{"title": "Towards Application-Specific Evaluation of Vision Models: Case Studies\n  in Ecology and Biology", "author": "Alex Hoi Hang Chan and Otto Brookes and Urs Waldmann and Hemal Naik and Iain D. Couzin and Majid Mirmehdi and No\u00ebl Adiko Houa and Emmanuelle Normand and Christophe Boesch and Lukas Boesch and Mimi Arandjelovic and Hjalmar K\u00fchl and Tilo Burghardt and Fumihiro Kano", "abstract": "  Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows.\n", "link": "http://arxiv.org/abs/2505.02825v2", "date": "2025-05-06", "relevancy": 2.2251, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Application-Specific%20Evaluation%20of%20Vision%20Models%3A%20Case%20Studies%0A%20%20in%20Ecology%20and%20Biology&body=Title%3A%20Towards%20Application-Specific%20Evaluation%20of%20Vision%20Models%3A%20Case%20Studies%0A%20%20in%20Ecology%20and%20Biology%0AAuthor%3A%20Alex%20Hoi%20Hang%20Chan%20and%20Otto%20Brookes%20and%20Urs%20Waldmann%20and%20Hemal%20Naik%20and%20Iain%20D.%20Couzin%20and%20Majid%20Mirmehdi%20and%20No%C3%ABl%20Adiko%20Houa%20and%20Emmanuelle%20Normand%20and%20Christophe%20Boesch%20and%20Lukas%20Boesch%20and%20Mimi%20Arandjelovic%20and%20Hjalmar%20K%C3%BChl%20and%20Tilo%20Burghardt%20and%20Fumihiro%20Kano%0AAbstract%3A%20%20%20Computer%20vision%20methods%20have%20demonstrated%20considerable%20potential%20to%0Astreamline%20ecological%20and%20biological%20workflows%2C%20with%20a%20growing%20number%20of%0Adatasets%20and%20models%20becoming%20available%20to%20the%20research%20community.%20However%2C%0Athese%20resources%20focus%20predominantly%20on%20evaluation%20using%20machine%20learning%0Ametrics%2C%20with%20relatively%20little%20emphasis%20on%20how%20their%20application%20impacts%0Adownstream%20analysis.%20We%20argue%20that%20models%20should%20be%20evaluated%20using%0Aapplication-specific%20metrics%20that%20directly%20represent%20model%20performance%20in%20the%0Acontext%20of%20its%20final%20use%20case.%20To%20support%20this%20argument%2C%20we%20present%20two%0Adisparate%20case%20studies%3A%20%281%29%20estimating%20chimpanzee%20abundance%20and%20density%20with%0Acamera%20trap%20distance%20sampling%20when%20using%20a%20video-based%20behaviour%20classifier%20and%0A%282%29%20estimating%20head%20rotation%20in%20pigeons%20using%20a%203D%20posture%20estimator.%20We%20show%0Athat%20even%20models%20with%20strong%20machine%20learning%20performance%20%28e.g.%2C%2087%25%20mAP%29%20can%0Ayield%20data%20that%20leads%20to%20discrepancies%20in%20abundance%20estimates%20compared%20to%0Aexpert-derived%20data.%20Similarly%2C%20the%20highest-performing%20models%20for%20posture%0Aestimation%20do%20not%20produce%20the%20most%20accurate%20inferences%20of%20gaze%20direction%20in%0Apigeons.%20Motivated%20by%20these%20findings%2C%20we%20call%20for%20researchers%20to%20integrate%0Aapplication-specific%20metrics%20in%20ecological/biological%20datasets%2C%20allowing%20for%0Amodels%20to%20be%20benchmarked%20in%20the%20context%20of%20their%20downstream%20application%20and%20to%0Afacilitate%20better%20integration%20of%20models%20into%20application%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Application-Specific%2520Evaluation%2520of%2520Vision%2520Models%253A%2520Case%2520Studies%250A%2520%2520in%2520Ecology%2520and%2520Biology%26entry.906535625%3DAlex%2520Hoi%2520Hang%2520Chan%2520and%2520Otto%2520Brookes%2520and%2520Urs%2520Waldmann%2520and%2520Hemal%2520Naik%2520and%2520Iain%2520D.%2520Couzin%2520and%2520Majid%2520Mirmehdi%2520and%2520No%25C3%25ABl%2520Adiko%2520Houa%2520and%2520Emmanuelle%2520Normand%2520and%2520Christophe%2520Boesch%2520and%2520Lukas%2520Boesch%2520and%2520Mimi%2520Arandjelovic%2520and%2520Hjalmar%2520K%25C3%25BChl%2520and%2520Tilo%2520Burghardt%2520and%2520Fumihiro%2520Kano%26entry.1292438233%3D%2520%2520Computer%2520vision%2520methods%2520have%2520demonstrated%2520considerable%2520potential%2520to%250Astreamline%2520ecological%2520and%2520biological%2520workflows%252C%2520with%2520a%2520growing%2520number%2520of%250Adatasets%2520and%2520models%2520becoming%2520available%2520to%2520the%2520research%2520community.%2520However%252C%250Athese%2520resources%2520focus%2520predominantly%2520on%2520evaluation%2520using%2520machine%2520learning%250Ametrics%252C%2520with%2520relatively%2520little%2520emphasis%2520on%2520how%2520their%2520application%2520impacts%250Adownstream%2520analysis.%2520We%2520argue%2520that%2520models%2520should%2520be%2520evaluated%2520using%250Aapplication-specific%2520metrics%2520that%2520directly%2520represent%2520model%2520performance%2520in%2520the%250Acontext%2520of%2520its%2520final%2520use%2520case.%2520To%2520support%2520this%2520argument%252C%2520we%2520present%2520two%250Adisparate%2520case%2520studies%253A%2520%25281%2529%2520estimating%2520chimpanzee%2520abundance%2520and%2520density%2520with%250Acamera%2520trap%2520distance%2520sampling%2520when%2520using%2520a%2520video-based%2520behaviour%2520classifier%2520and%250A%25282%2529%2520estimating%2520head%2520rotation%2520in%2520pigeons%2520using%2520a%25203D%2520posture%2520estimator.%2520We%2520show%250Athat%2520even%2520models%2520with%2520strong%2520machine%2520learning%2520performance%2520%2528e.g.%252C%252087%2525%2520mAP%2529%2520can%250Ayield%2520data%2520that%2520leads%2520to%2520discrepancies%2520in%2520abundance%2520estimates%2520compared%2520to%250Aexpert-derived%2520data.%2520Similarly%252C%2520the%2520highest-performing%2520models%2520for%2520posture%250Aestimation%2520do%2520not%2520produce%2520the%2520most%2520accurate%2520inferences%2520of%2520gaze%2520direction%2520in%250Apigeons.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520call%2520for%2520researchers%2520to%2520integrate%250Aapplication-specific%2520metrics%2520in%2520ecological/biological%2520datasets%252C%2520allowing%2520for%250Amodels%2520to%2520be%2520benchmarked%2520in%2520the%2520context%2520of%2520their%2520downstream%2520application%2520and%2520to%250Afacilitate%2520better%2520integration%2520of%2520models%2520into%2520application%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Application-Specific%20Evaluation%20of%20Vision%20Models%3A%20Case%20Studies%0A%20%20in%20Ecology%20and%20Biology&entry.906535625=Alex%20Hoi%20Hang%20Chan%20and%20Otto%20Brookes%20and%20Urs%20Waldmann%20and%20Hemal%20Naik%20and%20Iain%20D.%20Couzin%20and%20Majid%20Mirmehdi%20and%20No%C3%ABl%20Adiko%20Houa%20and%20Emmanuelle%20Normand%20and%20Christophe%20Boesch%20and%20Lukas%20Boesch%20and%20Mimi%20Arandjelovic%20and%20Hjalmar%20K%C3%BChl%20and%20Tilo%20Burghardt%20and%20Fumihiro%20Kano&entry.1292438233=%20%20Computer%20vision%20methods%20have%20demonstrated%20considerable%20potential%20to%0Astreamline%20ecological%20and%20biological%20workflows%2C%20with%20a%20growing%20number%20of%0Adatasets%20and%20models%20becoming%20available%20to%20the%20research%20community.%20However%2C%0Athese%20resources%20focus%20predominantly%20on%20evaluation%20using%20machine%20learning%0Ametrics%2C%20with%20relatively%20little%20emphasis%20on%20how%20their%20application%20impacts%0Adownstream%20analysis.%20We%20argue%20that%20models%20should%20be%20evaluated%20using%0Aapplication-specific%20metrics%20that%20directly%20represent%20model%20performance%20in%20the%0Acontext%20of%20its%20final%20use%20case.%20To%20support%20this%20argument%2C%20we%20present%20two%0Adisparate%20case%20studies%3A%20%281%29%20estimating%20chimpanzee%20abundance%20and%20density%20with%0Acamera%20trap%20distance%20sampling%20when%20using%20a%20video-based%20behaviour%20classifier%20and%0A%282%29%20estimating%20head%20rotation%20in%20pigeons%20using%20a%203D%20posture%20estimator.%20We%20show%0Athat%20even%20models%20with%20strong%20machine%20learning%20performance%20%28e.g.%2C%2087%25%20mAP%29%20can%0Ayield%20data%20that%20leads%20to%20discrepancies%20in%20abundance%20estimates%20compared%20to%0Aexpert-derived%20data.%20Similarly%2C%20the%20highest-performing%20models%20for%20posture%0Aestimation%20do%20not%20produce%20the%20most%20accurate%20inferences%20of%20gaze%20direction%20in%0Apigeons.%20Motivated%20by%20these%20findings%2C%20we%20call%20for%20researchers%20to%20integrate%0Aapplication-specific%20metrics%20in%20ecological/biological%20datasets%2C%20allowing%20for%0Amodels%20to%20be%20benchmarked%20in%20the%20context%20of%20their%20downstream%20application%20and%20to%0Afacilitate%20better%20integration%20of%20models%20into%20application%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02825v2&entry.124074799=Read"},
{"title": "Supervised and Unsupervised Textile Classification via Near-Infrared\n  Hyperspectral Imaging and Deep Learning", "author": "Maria Kainz and Johannes K. Krondorfer and Malte Jaschik and Maria Jernej and Harald Ganster", "abstract": "  Recycling textile fibers is critical to reducing the environmental impact of\nthe textile industry. Hyperspectral near-infrared (NIR) imaging combined with\nadvanced deep learning algorithms offers a promising solution for efficient\nfiber classification and sorting. In this study, we investigate supervised and\nunsupervised deep learning models and test their generalization capabilities on\ndifferent textile structures. We show that optimized convolutional neural\nnetworks (CNNs) and autoencoder networks achieve robust generalization under\nvarying conditions. These results highlight the potential of hyperspectral\nimaging and deep learning to advance sustainable textile recycling through\naccurate and robust classification.\n", "link": "http://arxiv.org/abs/2505.03575v1", "date": "2025-05-06", "relevancy": 2.2005, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5721}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5446}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20and%20Unsupervised%20Textile%20Classification%20via%20Near-Infrared%0A%20%20Hyperspectral%20Imaging%20and%20Deep%20Learning&body=Title%3A%20Supervised%20and%20Unsupervised%20Textile%20Classification%20via%20Near-Infrared%0A%20%20Hyperspectral%20Imaging%20and%20Deep%20Learning%0AAuthor%3A%20Maria%20Kainz%20and%20Johannes%20K.%20Krondorfer%20and%20Malte%20Jaschik%20and%20Maria%20Jernej%20and%20Harald%20Ganster%0AAbstract%3A%20%20%20Recycling%20textile%20fibers%20is%20critical%20to%20reducing%20the%20environmental%20impact%20of%0Athe%20textile%20industry.%20Hyperspectral%20near-infrared%20%28NIR%29%20imaging%20combined%20with%0Aadvanced%20deep%20learning%20algorithms%20offers%20a%20promising%20solution%20for%20efficient%0Afiber%20classification%20and%20sorting.%20In%20this%20study%2C%20we%20investigate%20supervised%20and%0Aunsupervised%20deep%20learning%20models%20and%20test%20their%20generalization%20capabilities%20on%0Adifferent%20textile%20structures.%20We%20show%20that%20optimized%20convolutional%20neural%0Anetworks%20%28CNNs%29%20and%20autoencoder%20networks%20achieve%20robust%20generalization%20under%0Avarying%20conditions.%20These%20results%20highlight%20the%20potential%20of%20hyperspectral%0Aimaging%20and%20deep%20learning%20to%20advance%20sustainable%20textile%20recycling%20through%0Aaccurate%20and%20robust%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520and%2520Unsupervised%2520Textile%2520Classification%2520via%2520Near-Infrared%250A%2520%2520Hyperspectral%2520Imaging%2520and%2520Deep%2520Learning%26entry.906535625%3DMaria%2520Kainz%2520and%2520Johannes%2520K.%2520Krondorfer%2520and%2520Malte%2520Jaschik%2520and%2520Maria%2520Jernej%2520and%2520Harald%2520Ganster%26entry.1292438233%3D%2520%2520Recycling%2520textile%2520fibers%2520is%2520critical%2520to%2520reducing%2520the%2520environmental%2520impact%2520of%250Athe%2520textile%2520industry.%2520Hyperspectral%2520near-infrared%2520%2528NIR%2529%2520imaging%2520combined%2520with%250Aadvanced%2520deep%2520learning%2520algorithms%2520offers%2520a%2520promising%2520solution%2520for%2520efficient%250Afiber%2520classification%2520and%2520sorting.%2520In%2520this%2520study%252C%2520we%2520investigate%2520supervised%2520and%250Aunsupervised%2520deep%2520learning%2520models%2520and%2520test%2520their%2520generalization%2520capabilities%2520on%250Adifferent%2520textile%2520structures.%2520We%2520show%2520that%2520optimized%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520and%2520autoencoder%2520networks%2520achieve%2520robust%2520generalization%2520under%250Avarying%2520conditions.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520hyperspectral%250Aimaging%2520and%2520deep%2520learning%2520to%2520advance%2520sustainable%2520textile%2520recycling%2520through%250Aaccurate%2520and%2520robust%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20and%20Unsupervised%20Textile%20Classification%20via%20Near-Infrared%0A%20%20Hyperspectral%20Imaging%20and%20Deep%20Learning&entry.906535625=Maria%20Kainz%20and%20Johannes%20K.%20Krondorfer%20and%20Malte%20Jaschik%20and%20Maria%20Jernej%20and%20Harald%20Ganster&entry.1292438233=%20%20Recycling%20textile%20fibers%20is%20critical%20to%20reducing%20the%20environmental%20impact%20of%0Athe%20textile%20industry.%20Hyperspectral%20near-infrared%20%28NIR%29%20imaging%20combined%20with%0Aadvanced%20deep%20learning%20algorithms%20offers%20a%20promising%20solution%20for%20efficient%0Afiber%20classification%20and%20sorting.%20In%20this%20study%2C%20we%20investigate%20supervised%20and%0Aunsupervised%20deep%20learning%20models%20and%20test%20their%20generalization%20capabilities%20on%0Adifferent%20textile%20structures.%20We%20show%20that%20optimized%20convolutional%20neural%0Anetworks%20%28CNNs%29%20and%20autoencoder%20networks%20achieve%20robust%20generalization%20under%0Avarying%20conditions.%20These%20results%20highlight%20the%20potential%20of%20hyperspectral%0Aimaging%20and%20deep%20learning%20to%20advance%20sustainable%20textile%20recycling%20through%0Aaccurate%20and%20robust%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03575v1&entry.124074799=Read"},
{"title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey", "author": "Da Zheng and Lun Du and Junwei Su and Yuchen Tian and Yuqi Zhu and Jintian Zhang and Lanning Wei and Ningyu Zhang and Huajun Chen", "abstract": "  Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.\n", "link": "http://arxiv.org/abs/2505.03418v1", "date": "2025-05-06", "relevancy": 2.1833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Augmented%20Complex%20Problem%20Solving%20with%20Large%20Language%20Models%3A%0A%20%20A%20Survey&body=Title%3A%20Knowledge%20Augmented%20Complex%20Problem%20Solving%20with%20Large%20Language%20Models%3A%0A%20%20A%20Survey%0AAuthor%3A%20Da%20Zheng%20and%20Lun%20Du%20and%20Junwei%20Su%20and%20Yuchen%20Tian%20and%20Yuqi%20Zhu%20and%20Jintian%20Zhang%20and%20Lanning%20Wei%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20Problem-solving%20has%20been%20a%20fundamental%20driver%20of%20human%20progress%20in%20numerous%0Adomains.%20With%20advancements%20in%20artificial%20intelligence%2C%20Large%20Language%20Models%0A%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20capable%20of%20tackling%20complex%20problems%0Aacross%20diverse%20domains.%20Unlike%20traditional%20computational%20systems%2C%20LLMs%20combine%0Araw%20computational%20power%20with%20an%20approximation%20of%20human%20reasoning%2C%20allowing%20them%0Ato%20generate%20solutions%2C%20make%20inferences%2C%20and%20even%20leverage%20external%0Acomputational%20tools.%20However%2C%20applying%20LLMs%20to%20real-world%20problem-solving%0Apresents%20significant%20challenges%2C%20including%20multi-step%20reasoning%2C%20domain%0Aknowledge%20integration%2C%20and%20result%20verification.%20This%20survey%20explores%20the%0Acapabilities%20and%20limitations%20of%20LLMs%20in%20complex%20problem-solving%2C%20examining%0Atechniques%20including%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20knowledge%20augmentation%2C%0Aand%20various%20LLM-based%20and%20tool-based%20verification%20techniques.%20Additionally%2C%20we%0Ahighlight%20domain-specific%20challenges%20in%20various%20domains%2C%20such%20as%20software%0Aengineering%2C%20mathematical%20reasoning%20and%20proving%2C%20data%20analysis%20and%20modeling%2C%0Aand%20scientific%20research.%20The%20paper%20further%20discusses%20the%20fundamental%0Alimitations%20of%20the%20current%20LLM%20solutions%20and%20the%20future%20directions%20of%20LLM-based%0Acomplex%20problems%20solving%20from%20the%20perspective%20of%20multi-step%20reasoning%2C%20domain%0Aknowledge%20integration%20and%20result%20verification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Augmented%2520Complex%2520Problem%2520Solving%2520with%2520Large%2520Language%2520Models%253A%250A%2520%2520A%2520Survey%26entry.906535625%3DDa%2520Zheng%2520and%2520Lun%2520Du%2520and%2520Junwei%2520Su%2520and%2520Yuchen%2520Tian%2520and%2520Yuqi%2520Zhu%2520and%2520Jintian%2520Zhang%2520and%2520Lanning%2520Wei%2520and%2520Ningyu%2520Zhang%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520Problem-solving%2520has%2520been%2520a%2520fundamental%2520driver%2520of%2520human%2520progress%2520in%2520numerous%250Adomains.%2520With%2520advancements%2520in%2520artificial%2520intelligence%252C%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520capable%2520of%2520tackling%2520complex%2520problems%250Aacross%2520diverse%2520domains.%2520Unlike%2520traditional%2520computational%2520systems%252C%2520LLMs%2520combine%250Araw%2520computational%2520power%2520with%2520an%2520approximation%2520of%2520human%2520reasoning%252C%2520allowing%2520them%250Ato%2520generate%2520solutions%252C%2520make%2520inferences%252C%2520and%2520even%2520leverage%2520external%250Acomputational%2520tools.%2520However%252C%2520applying%2520LLMs%2520to%2520real-world%2520problem-solving%250Apresents%2520significant%2520challenges%252C%2520including%2520multi-step%2520reasoning%252C%2520domain%250Aknowledge%2520integration%252C%2520and%2520result%2520verification.%2520This%2520survey%2520explores%2520the%250Acapabilities%2520and%2520limitations%2520of%2520LLMs%2520in%2520complex%2520problem-solving%252C%2520examining%250Atechniques%2520including%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%252C%2520knowledge%2520augmentation%252C%250Aand%2520various%2520LLM-based%2520and%2520tool-based%2520verification%2520techniques.%2520Additionally%252C%2520we%250Ahighlight%2520domain-specific%2520challenges%2520in%2520various%2520domains%252C%2520such%2520as%2520software%250Aengineering%252C%2520mathematical%2520reasoning%2520and%2520proving%252C%2520data%2520analysis%2520and%2520modeling%252C%250Aand%2520scientific%2520research.%2520The%2520paper%2520further%2520discusses%2520the%2520fundamental%250Alimitations%2520of%2520the%2520current%2520LLM%2520solutions%2520and%2520the%2520future%2520directions%2520of%2520LLM-based%250Acomplex%2520problems%2520solving%2520from%2520the%2520perspective%2520of%2520multi-step%2520reasoning%252C%2520domain%250Aknowledge%2520integration%2520and%2520result%2520verification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Augmented%20Complex%20Problem%20Solving%20with%20Large%20Language%20Models%3A%0A%20%20A%20Survey&entry.906535625=Da%20Zheng%20and%20Lun%20Du%20and%20Junwei%20Su%20and%20Yuchen%20Tian%20and%20Yuqi%20Zhu%20and%20Jintian%20Zhang%20and%20Lanning%20Wei%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen&entry.1292438233=%20%20Problem-solving%20has%20been%20a%20fundamental%20driver%20of%20human%20progress%20in%20numerous%0Adomains.%20With%20advancements%20in%20artificial%20intelligence%2C%20Large%20Language%20Models%0A%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20capable%20of%20tackling%20complex%20problems%0Aacross%20diverse%20domains.%20Unlike%20traditional%20computational%20systems%2C%20LLMs%20combine%0Araw%20computational%20power%20with%20an%20approximation%20of%20human%20reasoning%2C%20allowing%20them%0Ato%20generate%20solutions%2C%20make%20inferences%2C%20and%20even%20leverage%20external%0Acomputational%20tools.%20However%2C%20applying%20LLMs%20to%20real-world%20problem-solving%0Apresents%20significant%20challenges%2C%20including%20multi-step%20reasoning%2C%20domain%0Aknowledge%20integration%2C%20and%20result%20verification.%20This%20survey%20explores%20the%0Acapabilities%20and%20limitations%20of%20LLMs%20in%20complex%20problem-solving%2C%20examining%0Atechniques%20including%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20knowledge%20augmentation%2C%0Aand%20various%20LLM-based%20and%20tool-based%20verification%20techniques.%20Additionally%2C%20we%0Ahighlight%20domain-specific%20challenges%20in%20various%20domains%2C%20such%20as%20software%0Aengineering%2C%20mathematical%20reasoning%20and%20proving%2C%20data%20analysis%20and%20modeling%2C%0Aand%20scientific%20research.%20The%20paper%20further%20discusses%20the%20fundamental%0Alimitations%20of%20the%20current%20LLM%20solutions%20and%20the%20future%20directions%20of%20LLM-based%0Acomplex%20problems%20solving%20from%20the%20perspective%20of%20multi-step%20reasoning%2C%20domain%0Aknowledge%20integration%20and%20result%20verification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03418v1&entry.124074799=Read"},
{"title": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time", "author": "Yizhou Chen and Xiaoyue Wu and Yeheng Zong and Yuzhen Chen and Anran Li and Bohao Zhang and Ram Vasudevan", "abstract": "  Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.\n", "link": "http://arxiv.org/abs/2502.15037v5", "date": "2025-05-06", "relevancy": 2.1807, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5532}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5458}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEFT%3A%20Differentiable%20Branched%20Discrete%20Elastic%20Rods%20for%20Modeling%0A%20%20Furcated%20DLOs%20in%20Real-Time&body=Title%3A%20DEFT%3A%20Differentiable%20Branched%20Discrete%20Elastic%20Rods%20for%20Modeling%0A%20%20Furcated%20DLOs%20in%20Real-Time%0AAuthor%3A%20Yizhou%20Chen%20and%20Xiaoyue%20Wu%20and%20Yeheng%20Zong%20and%20Yuzhen%20Chen%20and%20Anran%20Li%20and%20Bohao%20Zhang%20and%20Ram%20Vasudevan%0AAbstract%3A%20%20%20Autonomous%20wire%20harness%20assembly%20requires%20robots%20to%20manipulate%20complex%0Abranched%20cables%20with%20high%20precision%20and%20reliability.%20A%20key%20challenge%20in%0Aautomating%20this%20process%20is%20predicting%20how%20these%20flexible%20and%20branched%0Astructures%20behave%20under%20manipulation.%20Without%20accurate%20predictions%2C%20it%20is%0Adifficult%20for%20robots%20to%20reliably%20plan%20or%20execute%20assembly%20operations.%20While%0Aexisting%20research%20has%20made%20progress%20in%20modeling%20single-threaded%20Deformable%0ALinear%20Objects%20%28DLOs%29%2C%20extending%20these%20approaches%20to%20Branched%20Deformable%20Linear%0AObjects%20%28BDLOs%29%20presents%20fundamental%20challenges.%20The%20junction%20points%20in%20BDLOs%0Acreate%20complex%20force%20interactions%20and%20strain%20propagation%20patterns%20that%20cannot%0Abe%20adequately%20captured%20by%20simply%20connecting%20multiple%20single-DLO%20models.%20To%0Aaddress%20these%20challenges%2C%20this%20paper%20presents%20Differentiable%20discrete%20branched%0AElastic%20rods%20for%20modeling%20Furcated%20DLOs%20in%20real-Time%20%28DEFT%29%2C%20a%20novel%20framework%0Athat%20combines%20a%20differentiable%20physics-based%20model%20with%20a%20learning%20framework%0Ato%3A%201%29%20accurately%20model%20BDLO%20dynamics%2C%20including%20dynamic%20propagation%20at%0Ajunction%20points%20and%20grasping%20in%20the%20middle%20of%20a%20BDLO%2C%202%29%20achieve%20efficient%0Acomputation%20for%20real-time%20inference%2C%20and%203%29%20enable%20planning%20to%20demonstrate%0Adexterous%20BDLO%20manipulation.%20A%20comprehensive%20series%20of%20real-world%20experiments%0Ademonstrates%20DEFT%27s%20efficacy%20in%20terms%20of%20accuracy%2C%20computational%20speed%2C%20and%0Ageneralizability%20compared%20to%20state-of-the-art%20alternatives.%20Project%0Apage%3Ahttps%3A//roahmlab.github.io/DEFT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15037v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEFT%253A%2520Differentiable%2520Branched%2520Discrete%2520Elastic%2520Rods%2520for%2520Modeling%250A%2520%2520Furcated%2520DLOs%2520in%2520Real-Time%26entry.906535625%3DYizhou%2520Chen%2520and%2520Xiaoyue%2520Wu%2520and%2520Yeheng%2520Zong%2520and%2520Yuzhen%2520Chen%2520and%2520Anran%2520Li%2520and%2520Bohao%2520Zhang%2520and%2520Ram%2520Vasudevan%26entry.1292438233%3D%2520%2520Autonomous%2520wire%2520harness%2520assembly%2520requires%2520robots%2520to%2520manipulate%2520complex%250Abranched%2520cables%2520with%2520high%2520precision%2520and%2520reliability.%2520A%2520key%2520challenge%2520in%250Aautomating%2520this%2520process%2520is%2520predicting%2520how%2520these%2520flexible%2520and%2520branched%250Astructures%2520behave%2520under%2520manipulation.%2520Without%2520accurate%2520predictions%252C%2520it%2520is%250Adifficult%2520for%2520robots%2520to%2520reliably%2520plan%2520or%2520execute%2520assembly%2520operations.%2520While%250Aexisting%2520research%2520has%2520made%2520progress%2520in%2520modeling%2520single-threaded%2520Deformable%250ALinear%2520Objects%2520%2528DLOs%2529%252C%2520extending%2520these%2520approaches%2520to%2520Branched%2520Deformable%2520Linear%250AObjects%2520%2528BDLOs%2529%2520presents%2520fundamental%2520challenges.%2520The%2520junction%2520points%2520in%2520BDLOs%250Acreate%2520complex%2520force%2520interactions%2520and%2520strain%2520propagation%2520patterns%2520that%2520cannot%250Abe%2520adequately%2520captured%2520by%2520simply%2520connecting%2520multiple%2520single-DLO%2520models.%2520To%250Aaddress%2520these%2520challenges%252C%2520this%2520paper%2520presents%2520Differentiable%2520discrete%2520branched%250AElastic%2520rods%2520for%2520modeling%2520Furcated%2520DLOs%2520in%2520real-Time%2520%2528DEFT%2529%252C%2520a%2520novel%2520framework%250Athat%2520combines%2520a%2520differentiable%2520physics-based%2520model%2520with%2520a%2520learning%2520framework%250Ato%253A%25201%2529%2520accurately%2520model%2520BDLO%2520dynamics%252C%2520including%2520dynamic%2520propagation%2520at%250Ajunction%2520points%2520and%2520grasping%2520in%2520the%2520middle%2520of%2520a%2520BDLO%252C%25202%2529%2520achieve%2520efficient%250Acomputation%2520for%2520real-time%2520inference%252C%2520and%25203%2529%2520enable%2520planning%2520to%2520demonstrate%250Adexterous%2520BDLO%2520manipulation.%2520A%2520comprehensive%2520series%2520of%2520real-world%2520experiments%250Ademonstrates%2520DEFT%2527s%2520efficacy%2520in%2520terms%2520of%2520accuracy%252C%2520computational%2520speed%252C%2520and%250Ageneralizability%2520compared%2520to%2520state-of-the-art%2520alternatives.%2520Project%250Apage%253Ahttps%253A//roahmlab.github.io/DEFT/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15037v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEFT%3A%20Differentiable%20Branched%20Discrete%20Elastic%20Rods%20for%20Modeling%0A%20%20Furcated%20DLOs%20in%20Real-Time&entry.906535625=Yizhou%20Chen%20and%20Xiaoyue%20Wu%20and%20Yeheng%20Zong%20and%20Yuzhen%20Chen%20and%20Anran%20Li%20and%20Bohao%20Zhang%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Autonomous%20wire%20harness%20assembly%20requires%20robots%20to%20manipulate%20complex%0Abranched%20cables%20with%20high%20precision%20and%20reliability.%20A%20key%20challenge%20in%0Aautomating%20this%20process%20is%20predicting%20how%20these%20flexible%20and%20branched%0Astructures%20behave%20under%20manipulation.%20Without%20accurate%20predictions%2C%20it%20is%0Adifficult%20for%20robots%20to%20reliably%20plan%20or%20execute%20assembly%20operations.%20While%0Aexisting%20research%20has%20made%20progress%20in%20modeling%20single-threaded%20Deformable%0ALinear%20Objects%20%28DLOs%29%2C%20extending%20these%20approaches%20to%20Branched%20Deformable%20Linear%0AObjects%20%28BDLOs%29%20presents%20fundamental%20challenges.%20The%20junction%20points%20in%20BDLOs%0Acreate%20complex%20force%20interactions%20and%20strain%20propagation%20patterns%20that%20cannot%0Abe%20adequately%20captured%20by%20simply%20connecting%20multiple%20single-DLO%20models.%20To%0Aaddress%20these%20challenges%2C%20this%20paper%20presents%20Differentiable%20discrete%20branched%0AElastic%20rods%20for%20modeling%20Furcated%20DLOs%20in%20real-Time%20%28DEFT%29%2C%20a%20novel%20framework%0Athat%20combines%20a%20differentiable%20physics-based%20model%20with%20a%20learning%20framework%0Ato%3A%201%29%20accurately%20model%20BDLO%20dynamics%2C%20including%20dynamic%20propagation%20at%0Ajunction%20points%20and%20grasping%20in%20the%20middle%20of%20a%20BDLO%2C%202%29%20achieve%20efficient%0Acomputation%20for%20real-time%20inference%2C%20and%203%29%20enable%20planning%20to%20demonstrate%0Adexterous%20BDLO%20manipulation.%20A%20comprehensive%20series%20of%20real-world%20experiments%0Ademonstrates%20DEFT%27s%20efficacy%20in%20terms%20of%20accuracy%2C%20computational%20speed%2C%20and%0Ageneralizability%20compared%20to%20state-of-the-art%20alternatives.%20Project%0Apage%3Ahttps%3A//roahmlab.github.io/DEFT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15037v5&entry.124074799=Read"},
{"title": "Ergodic Generative Flows", "author": "Leo Maxime Brunswic and Mateo Clemente and Rui Heng Yang and Adam Sigal and Amir Rasouli and Yinchuan Li", "abstract": "  Generative Flow Networks (GFNs) were initially introduced on directed acyclic\ngraphs to sample from an unnormalized distribution density. Recent works have\nextended the theoretical framework for generative methods allowing more\nflexibility and enhancing application range. However, many challenges remain in\ntraining GFNs in continuous settings and for imitation learning (IL), including\nintractability of flow-matching loss, limited tests of non-acyclic training,\nand the need for a separate reward model in imitation learning. The present\nwork proposes a family of generative flows called Ergodic Generative Flows\n(EGFs) which are used to address the aforementioned issues. First, we leverage\nergodicity to build simple generative flows with finitely many globally defined\ntransformations (diffeomorphisms) with universality guarantees and tractable\nflow-matching loss (FM loss). Second, we introduce a new loss involving\ncross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It\nis designed for IL training without a separate reward model. We evaluate\nIL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using\nthe KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning\nexperiments with a target reward, using the FM loss.\n", "link": "http://arxiv.org/abs/2505.03561v1", "date": "2025-05-06", "relevancy": 2.1744, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5851}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5626}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ergodic%20Generative%20Flows&body=Title%3A%20Ergodic%20Generative%20Flows%0AAuthor%3A%20Leo%20Maxime%20Brunswic%20and%20Mateo%20Clemente%20and%20Rui%20Heng%20Yang%20and%20Adam%20Sigal%20and%20Amir%20Rasouli%20and%20Yinchuan%20Li%0AAbstract%3A%20%20%20Generative%20Flow%20Networks%20%28GFNs%29%20were%20initially%20introduced%20on%20directed%20acyclic%0Agraphs%20to%20sample%20from%20an%20unnormalized%20distribution%20density.%20Recent%20works%20have%0Aextended%20the%20theoretical%20framework%20for%20generative%20methods%20allowing%20more%0Aflexibility%20and%20enhancing%20application%20range.%20However%2C%20many%20challenges%20remain%20in%0Atraining%20GFNs%20in%20continuous%20settings%20and%20for%20imitation%20learning%20%28IL%29%2C%20including%0Aintractability%20of%20flow-matching%20loss%2C%20limited%20tests%20of%20non-acyclic%20training%2C%0Aand%20the%20need%20for%20a%20separate%20reward%20model%20in%20imitation%20learning.%20The%20present%0Awork%20proposes%20a%20family%20of%20generative%20flows%20called%20Ergodic%20Generative%20Flows%0A%28EGFs%29%20which%20are%20used%20to%20address%20the%20aforementioned%20issues.%20First%2C%20we%20leverage%0Aergodicity%20to%20build%20simple%20generative%20flows%20with%20finitely%20many%20globally%20defined%0Atransformations%20%28diffeomorphisms%29%20with%20universality%20guarantees%20and%20tractable%0Aflow-matching%20loss%20%28FM%20loss%29.%20Second%2C%20we%20introduce%20a%20new%20loss%20involving%0Across-entropy%20coupled%20to%20weak%20flow-matching%20control%2C%20coined%20KL-weakFM%20loss.%20It%0Ais%20designed%20for%20IL%20training%20without%20a%20separate%20reward%20model.%20We%20evaluate%0AIL-EGFs%20on%20toy%202D%20tasks%20and%20real-world%20datasets%20from%20NASA%20on%20the%20sphere%2C%20using%0Athe%20KL-weakFM%20loss.%20Additionally%2C%20we%20conduct%20toy%202D%20reinforcement%20learning%0Aexperiments%20with%20a%20target%20reward%2C%20using%20the%20FM%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DErgodic%2520Generative%2520Flows%26entry.906535625%3DLeo%2520Maxime%2520Brunswic%2520and%2520Mateo%2520Clemente%2520and%2520Rui%2520Heng%2520Yang%2520and%2520Adam%2520Sigal%2520and%2520Amir%2520Rasouli%2520and%2520Yinchuan%2520Li%26entry.1292438233%3D%2520%2520Generative%2520Flow%2520Networks%2520%2528GFNs%2529%2520were%2520initially%2520introduced%2520on%2520directed%2520acyclic%250Agraphs%2520to%2520sample%2520from%2520an%2520unnormalized%2520distribution%2520density.%2520Recent%2520works%2520have%250Aextended%2520the%2520theoretical%2520framework%2520for%2520generative%2520methods%2520allowing%2520more%250Aflexibility%2520and%2520enhancing%2520application%2520range.%2520However%252C%2520many%2520challenges%2520remain%2520in%250Atraining%2520GFNs%2520in%2520continuous%2520settings%2520and%2520for%2520imitation%2520learning%2520%2528IL%2529%252C%2520including%250Aintractability%2520of%2520flow-matching%2520loss%252C%2520limited%2520tests%2520of%2520non-acyclic%2520training%252C%250Aand%2520the%2520need%2520for%2520a%2520separate%2520reward%2520model%2520in%2520imitation%2520learning.%2520The%2520present%250Awork%2520proposes%2520a%2520family%2520of%2520generative%2520flows%2520called%2520Ergodic%2520Generative%2520Flows%250A%2528EGFs%2529%2520which%2520are%2520used%2520to%2520address%2520the%2520aforementioned%2520issues.%2520First%252C%2520we%2520leverage%250Aergodicity%2520to%2520build%2520simple%2520generative%2520flows%2520with%2520finitely%2520many%2520globally%2520defined%250Atransformations%2520%2528diffeomorphisms%2529%2520with%2520universality%2520guarantees%2520and%2520tractable%250Aflow-matching%2520loss%2520%2528FM%2520loss%2529.%2520Second%252C%2520we%2520introduce%2520a%2520new%2520loss%2520involving%250Across-entropy%2520coupled%2520to%2520weak%2520flow-matching%2520control%252C%2520coined%2520KL-weakFM%2520loss.%2520It%250Ais%2520designed%2520for%2520IL%2520training%2520without%2520a%2520separate%2520reward%2520model.%2520We%2520evaluate%250AIL-EGFs%2520on%2520toy%25202D%2520tasks%2520and%2520real-world%2520datasets%2520from%2520NASA%2520on%2520the%2520sphere%252C%2520using%250Athe%2520KL-weakFM%2520loss.%2520Additionally%252C%2520we%2520conduct%2520toy%25202D%2520reinforcement%2520learning%250Aexperiments%2520with%2520a%2520target%2520reward%252C%2520using%2520the%2520FM%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ergodic%20Generative%20Flows&entry.906535625=Leo%20Maxime%20Brunswic%20and%20Mateo%20Clemente%20and%20Rui%20Heng%20Yang%20and%20Adam%20Sigal%20and%20Amir%20Rasouli%20and%20Yinchuan%20Li&entry.1292438233=%20%20Generative%20Flow%20Networks%20%28GFNs%29%20were%20initially%20introduced%20on%20directed%20acyclic%0Agraphs%20to%20sample%20from%20an%20unnormalized%20distribution%20density.%20Recent%20works%20have%0Aextended%20the%20theoretical%20framework%20for%20generative%20methods%20allowing%20more%0Aflexibility%20and%20enhancing%20application%20range.%20However%2C%20many%20challenges%20remain%20in%0Atraining%20GFNs%20in%20continuous%20settings%20and%20for%20imitation%20learning%20%28IL%29%2C%20including%0Aintractability%20of%20flow-matching%20loss%2C%20limited%20tests%20of%20non-acyclic%20training%2C%0Aand%20the%20need%20for%20a%20separate%20reward%20model%20in%20imitation%20learning.%20The%20present%0Awork%20proposes%20a%20family%20of%20generative%20flows%20called%20Ergodic%20Generative%20Flows%0A%28EGFs%29%20which%20are%20used%20to%20address%20the%20aforementioned%20issues.%20First%2C%20we%20leverage%0Aergodicity%20to%20build%20simple%20generative%20flows%20with%20finitely%20many%20globally%20defined%0Atransformations%20%28diffeomorphisms%29%20with%20universality%20guarantees%20and%20tractable%0Aflow-matching%20loss%20%28FM%20loss%29.%20Second%2C%20we%20introduce%20a%20new%20loss%20involving%0Across-entropy%20coupled%20to%20weak%20flow-matching%20control%2C%20coined%20KL-weakFM%20loss.%20It%0Ais%20designed%20for%20IL%20training%20without%20a%20separate%20reward%20model.%20We%20evaluate%0AIL-EGFs%20on%20toy%202D%20tasks%20and%20real-world%20datasets%20from%20NASA%20on%20the%20sphere%2C%20using%0Athe%20KL-weakFM%20loss.%20Additionally%2C%20we%20conduct%20toy%202D%20reinforcement%20learning%0Aexperiments%20with%20a%20target%20reward%2C%20using%20the%20FM%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03561v1&entry.124074799=Read"},
{"title": "PhysLLM: Harnessing Large Language Models for Cross-Modal Remote\n  Physiological Sensing", "author": "Yiping Xie and Bo Zhao and Mingtong Dai and Jian-Ping Zhou and Yue Sun and Tao Tan and Weicheng Xie and Linlin Shen and Zitong Yu", "abstract": "  Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios.\n", "link": "http://arxiv.org/abs/2505.03621v1", "date": "2025-05-06", "relevancy": 2.173, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5467}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysLLM%3A%20Harnessing%20Large%20Language%20Models%20for%20Cross-Modal%20Remote%0A%20%20Physiological%20Sensing&body=Title%3A%20PhysLLM%3A%20Harnessing%20Large%20Language%20Models%20for%20Cross-Modal%20Remote%0A%20%20Physiological%20Sensing%0AAuthor%3A%20Yiping%20Xie%20and%20Bo%20Zhao%20and%20Mingtong%20Dai%20and%20Jian-Ping%20Zhou%20and%20Yue%20Sun%20and%20Tao%20Tan%20and%20Weicheng%20Xie%20and%20Linlin%20Shen%20and%20Zitong%20Yu%0AAbstract%3A%20%20%20Remote%20photoplethysmography%20%28rPPG%29%20enables%20non-contact%20physiological%0Ameasurement%20but%20remains%20highly%20susceptible%20to%20illumination%20changes%2C%20motion%0Aartifacts%2C%20and%20limited%20temporal%20modeling.%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%0Acapturing%20long-range%20dependencies%2C%20offering%20a%20potential%20solution%20but%20struggle%0Awith%20the%20continuous%2C%20noise-sensitive%20nature%20of%20rPPG%20signals%20due%20to%20their%0Atext-centric%20design.%20To%20bridge%20this%20gap%2C%20we%20introduce%20PhysLLM%2C%20a%20collaborative%0Aoptimization%20framework%20that%20synergizes%20LLMs%20with%20domain-specific%20rPPG%0Acomponents.%20Specifically%2C%20the%20Text%20Prototype%20Guidance%20%28TPG%29%20strategy%20is%0Aproposed%20to%20establish%20cross-modal%20alignment%20by%20projecting%20hemodynamic%20features%0Ainto%20LLM-interpretable%20semantic%20space%2C%20effectively%20bridging%20the%0Arepresentational%20gap%20between%20physiological%20signals%20and%20linguistic%20tokens.%0ABesides%2C%20a%20novel%20Dual-Domain%20Stationary%20%28DDS%29%20Algorithm%20is%20proposed%20for%0Aresolving%20signal%20instability%20through%20adaptive%20time-frequency%20domain%20feature%0Are-weighting.%20Finally%2C%20rPPG%20task-specific%20cues%20systematically%20inject%0Aphysiological%20priors%20through%20physiological%20statistics%2C%20environmental%20contextual%0Aanswering%2C%20and%20task%20description%2C%20leveraging%20cross-modal%20learning%20to%20integrate%0Aboth%20visual%20and%20textual%20information%2C%20enabling%20dynamic%20adaptation%20to%20challenging%0Ascenarios%20like%20variable%20illumination%20and%20subject%20movements.%20Evaluation%20on%20four%0Abenchmark%20datasets%2C%20PhysLLM%20achieves%20state-of-the-art%20accuracy%20and%20robustness%2C%0Ademonstrating%20superior%20generalization%20across%20lighting%20variations%20and%20motion%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysLLM%253A%2520Harnessing%2520Large%2520Language%2520Models%2520for%2520Cross-Modal%2520Remote%250A%2520%2520Physiological%2520Sensing%26entry.906535625%3DYiping%2520Xie%2520and%2520Bo%2520Zhao%2520and%2520Mingtong%2520Dai%2520and%2520Jian-Ping%2520Zhou%2520and%2520Yue%2520Sun%2520and%2520Tao%2520Tan%2520and%2520Weicheng%2520Xie%2520and%2520Linlin%2520Shen%2520and%2520Zitong%2520Yu%26entry.1292438233%3D%2520%2520Remote%2520photoplethysmography%2520%2528rPPG%2529%2520enables%2520non-contact%2520physiological%250Ameasurement%2520but%2520remains%2520highly%2520susceptible%2520to%2520illumination%2520changes%252C%2520motion%250Aartifacts%252C%2520and%2520limited%2520temporal%2520modeling.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%250Acapturing%2520long-range%2520dependencies%252C%2520offering%2520a%2520potential%2520solution%2520but%2520struggle%250Awith%2520the%2520continuous%252C%2520noise-sensitive%2520nature%2520of%2520rPPG%2520signals%2520due%2520to%2520their%250Atext-centric%2520design.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520PhysLLM%252C%2520a%2520collaborative%250Aoptimization%2520framework%2520that%2520synergizes%2520LLMs%2520with%2520domain-specific%2520rPPG%250Acomponents.%2520Specifically%252C%2520the%2520Text%2520Prototype%2520Guidance%2520%2528TPG%2529%2520strategy%2520is%250Aproposed%2520to%2520establish%2520cross-modal%2520alignment%2520by%2520projecting%2520hemodynamic%2520features%250Ainto%2520LLM-interpretable%2520semantic%2520space%252C%2520effectively%2520bridging%2520the%250Arepresentational%2520gap%2520between%2520physiological%2520signals%2520and%2520linguistic%2520tokens.%250ABesides%252C%2520a%2520novel%2520Dual-Domain%2520Stationary%2520%2528DDS%2529%2520Algorithm%2520is%2520proposed%2520for%250Aresolving%2520signal%2520instability%2520through%2520adaptive%2520time-frequency%2520domain%2520feature%250Are-weighting.%2520Finally%252C%2520rPPG%2520task-specific%2520cues%2520systematically%2520inject%250Aphysiological%2520priors%2520through%2520physiological%2520statistics%252C%2520environmental%2520contextual%250Aanswering%252C%2520and%2520task%2520description%252C%2520leveraging%2520cross-modal%2520learning%2520to%2520integrate%250Aboth%2520visual%2520and%2520textual%2520information%252C%2520enabling%2520dynamic%2520adaptation%2520to%2520challenging%250Ascenarios%2520like%2520variable%2520illumination%2520and%2520subject%2520movements.%2520Evaluation%2520on%2520four%250Abenchmark%2520datasets%252C%2520PhysLLM%2520achieves%2520state-of-the-art%2520accuracy%2520and%2520robustness%252C%250Ademonstrating%2520superior%2520generalization%2520across%2520lighting%2520variations%2520and%2520motion%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysLLM%3A%20Harnessing%20Large%20Language%20Models%20for%20Cross-Modal%20Remote%0A%20%20Physiological%20Sensing&entry.906535625=Yiping%20Xie%20and%20Bo%20Zhao%20and%20Mingtong%20Dai%20and%20Jian-Ping%20Zhou%20and%20Yue%20Sun%20and%20Tao%20Tan%20and%20Weicheng%20Xie%20and%20Linlin%20Shen%20and%20Zitong%20Yu&entry.1292438233=%20%20Remote%20photoplethysmography%20%28rPPG%29%20enables%20non-contact%20physiological%0Ameasurement%20but%20remains%20highly%20susceptible%20to%20illumination%20changes%2C%20motion%0Aartifacts%2C%20and%20limited%20temporal%20modeling.%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%0Acapturing%20long-range%20dependencies%2C%20offering%20a%20potential%20solution%20but%20struggle%0Awith%20the%20continuous%2C%20noise-sensitive%20nature%20of%20rPPG%20signals%20due%20to%20their%0Atext-centric%20design.%20To%20bridge%20this%20gap%2C%20we%20introduce%20PhysLLM%2C%20a%20collaborative%0Aoptimization%20framework%20that%20synergizes%20LLMs%20with%20domain-specific%20rPPG%0Acomponents.%20Specifically%2C%20the%20Text%20Prototype%20Guidance%20%28TPG%29%20strategy%20is%0Aproposed%20to%20establish%20cross-modal%20alignment%20by%20projecting%20hemodynamic%20features%0Ainto%20LLM-interpretable%20semantic%20space%2C%20effectively%20bridging%20the%0Arepresentational%20gap%20between%20physiological%20signals%20and%20linguistic%20tokens.%0ABesides%2C%20a%20novel%20Dual-Domain%20Stationary%20%28DDS%29%20Algorithm%20is%20proposed%20for%0Aresolving%20signal%20instability%20through%20adaptive%20time-frequency%20domain%20feature%0Are-weighting.%20Finally%2C%20rPPG%20task-specific%20cues%20systematically%20inject%0Aphysiological%20priors%20through%20physiological%20statistics%2C%20environmental%20contextual%0Aanswering%2C%20and%20task%20description%2C%20leveraging%20cross-modal%20learning%20to%20integrate%0Aboth%20visual%20and%20textual%20information%2C%20enabling%20dynamic%20adaptation%20to%20challenging%0Ascenarios%20like%20variable%20illumination%20and%20subject%20movements.%20Evaluation%20on%20four%0Abenchmark%20datasets%2C%20PhysLLM%20achieves%20state-of-the-art%20accuracy%20and%20robustness%2C%0Ademonstrating%20superior%20generalization%20across%20lighting%20variations%20and%20motion%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03621v1&entry.124074799=Read"},
{"title": "Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI\n  Synthesis: Advancing Pretraining and Clinical Applications", "author": "Ziyu Li and Yujian Hu and Zhengyao Ding and Yiheng Mao and Haitao Li and Fan Yi and Hongkun Zhang and Zhengxing Huang", "abstract": "  Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.\n", "link": "http://arxiv.org/abs/2505.03426v1", "date": "2025-05-06", "relevancy": 2.1717, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5497}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5391}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phenotype-Guided%20Generative%20Model%20for%20High-Fidelity%20Cardiac%20MRI%0A%20%20Synthesis%3A%20Advancing%20Pretraining%20and%20Clinical%20Applications&body=Title%3A%20Phenotype-Guided%20Generative%20Model%20for%20High-Fidelity%20Cardiac%20MRI%0A%20%20Synthesis%3A%20Advancing%20Pretraining%20and%20Clinical%20Applications%0AAuthor%3A%20Ziyu%20Li%20and%20Yujian%20Hu%20and%20Zhengyao%20Ding%20and%20Yiheng%20Mao%20and%20Haitao%20Li%20and%20Fan%20Yi%20and%20Hongkun%20Zhang%20and%20Zhengxing%20Huang%0AAbstract%3A%20%20%20Cardiac%20Magnetic%20Resonance%20%28CMR%29%20imaging%20is%20a%20vital%20non-invasive%20tool%20for%0Adiagnosing%20heart%20diseases%20and%20evaluating%20cardiac%20health.%20However%2C%20the%20limited%0Aavailability%20of%20large-scale%2C%20high-quality%20CMR%20datasets%20poses%20a%20major%20challenge%0Ato%20the%20effective%20application%20of%20artificial%20intelligence%20%28AI%29%20in%20this%20domain.%0AEven%20the%20amount%20of%20unlabeled%20data%20and%20the%20health%20status%20it%20covers%20are%20difficult%0Ato%20meet%20the%20needs%20of%20model%20pretraining%2C%20which%20hinders%20the%20performance%20of%20AI%0Amodels%20on%20downstream%20tasks.%20In%20this%20study%2C%20we%20present%20Cardiac%20Phenotype-Guided%0ACMR%20Generation%20%28CPGG%29%2C%20a%20novel%20approach%20for%20generating%20diverse%20CMR%20data%20that%0Acovers%20a%20wide%20spectrum%20of%20cardiac%20health%20status.%20The%20CPGG%20framework%20consists%20of%0Atwo%20stages%3A%20in%20the%20first%20stage%2C%20a%20generative%20model%20is%20trained%20using%20cardiac%0Aphenotypes%20derived%20from%20CMR%20data%3B%20in%20the%20second%20stage%2C%20a%20masked%20autoregressive%0Adiffusion%20model%2C%20conditioned%20on%20these%20phenotypes%2C%20generates%20high-fidelity%20CMR%0Acine%20sequences%20that%20capture%20both%20structural%20and%20functional%20features%20of%20the%0Aheart%20in%20a%20fine-grained%20manner.%20We%20synthesized%20a%20massive%20amount%20of%20CMR%20to%0Aexpand%20the%20pretraining%20data.%20Experimental%20results%20show%20that%20CPGG%20generates%0Ahigh-quality%20synthetic%20CMR%20data%2C%20significantly%20improving%20performance%20on%20various%0Adownstream%20tasks%2C%20including%20diagnosis%20and%20cardiac%20phenotypes%20prediction.%20These%0Agains%20are%20demonstrated%20across%20both%20public%20and%20private%20datasets%2C%20highlighting%0Athe%20effectiveness%20of%20our%20approach.%20Code%20is%20availabel%20at%0Ahttps%3A//anonymous.4open.science/r/CPGG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhenotype-Guided%2520Generative%2520Model%2520for%2520High-Fidelity%2520Cardiac%2520MRI%250A%2520%2520Synthesis%253A%2520Advancing%2520Pretraining%2520and%2520Clinical%2520Applications%26entry.906535625%3DZiyu%2520Li%2520and%2520Yujian%2520Hu%2520and%2520Zhengyao%2520Ding%2520and%2520Yiheng%2520Mao%2520and%2520Haitao%2520Li%2520and%2520Fan%2520Yi%2520and%2520Hongkun%2520Zhang%2520and%2520Zhengxing%2520Huang%26entry.1292438233%3D%2520%2520Cardiac%2520Magnetic%2520Resonance%2520%2528CMR%2529%2520imaging%2520is%2520a%2520vital%2520non-invasive%2520tool%2520for%250Adiagnosing%2520heart%2520diseases%2520and%2520evaluating%2520cardiac%2520health.%2520However%252C%2520the%2520limited%250Aavailability%2520of%2520large-scale%252C%2520high-quality%2520CMR%2520datasets%2520poses%2520a%2520major%2520challenge%250Ato%2520the%2520effective%2520application%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520in%2520this%2520domain.%250AEven%2520the%2520amount%2520of%2520unlabeled%2520data%2520and%2520the%2520health%2520status%2520it%2520covers%2520are%2520difficult%250Ato%2520meet%2520the%2520needs%2520of%2520model%2520pretraining%252C%2520which%2520hinders%2520the%2520performance%2520of%2520AI%250Amodels%2520on%2520downstream%2520tasks.%2520In%2520this%2520study%252C%2520we%2520present%2520Cardiac%2520Phenotype-Guided%250ACMR%2520Generation%2520%2528CPGG%2529%252C%2520a%2520novel%2520approach%2520for%2520generating%2520diverse%2520CMR%2520data%2520that%250Acovers%2520a%2520wide%2520spectrum%2520of%2520cardiac%2520health%2520status.%2520The%2520CPGG%2520framework%2520consists%2520of%250Atwo%2520stages%253A%2520in%2520the%2520first%2520stage%252C%2520a%2520generative%2520model%2520is%2520trained%2520using%2520cardiac%250Aphenotypes%2520derived%2520from%2520CMR%2520data%253B%2520in%2520the%2520second%2520stage%252C%2520a%2520masked%2520autoregressive%250Adiffusion%2520model%252C%2520conditioned%2520on%2520these%2520phenotypes%252C%2520generates%2520high-fidelity%2520CMR%250Acine%2520sequences%2520that%2520capture%2520both%2520structural%2520and%2520functional%2520features%2520of%2520the%250Aheart%2520in%2520a%2520fine-grained%2520manner.%2520We%2520synthesized%2520a%2520massive%2520amount%2520of%2520CMR%2520to%250Aexpand%2520the%2520pretraining%2520data.%2520Experimental%2520results%2520show%2520that%2520CPGG%2520generates%250Ahigh-quality%2520synthetic%2520CMR%2520data%252C%2520significantly%2520improving%2520performance%2520on%2520various%250Adownstream%2520tasks%252C%2520including%2520diagnosis%2520and%2520cardiac%2520phenotypes%2520prediction.%2520These%250Agains%2520are%2520demonstrated%2520across%2520both%2520public%2520and%2520private%2520datasets%252C%2520highlighting%250Athe%2520effectiveness%2520of%2520our%2520approach.%2520Code%2520is%2520availabel%2520at%250Ahttps%253A//anonymous.4open.science/r/CPGG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phenotype-Guided%20Generative%20Model%20for%20High-Fidelity%20Cardiac%20MRI%0A%20%20Synthesis%3A%20Advancing%20Pretraining%20and%20Clinical%20Applications&entry.906535625=Ziyu%20Li%20and%20Yujian%20Hu%20and%20Zhengyao%20Ding%20and%20Yiheng%20Mao%20and%20Haitao%20Li%20and%20Fan%20Yi%20and%20Hongkun%20Zhang%20and%20Zhengxing%20Huang&entry.1292438233=%20%20Cardiac%20Magnetic%20Resonance%20%28CMR%29%20imaging%20is%20a%20vital%20non-invasive%20tool%20for%0Adiagnosing%20heart%20diseases%20and%20evaluating%20cardiac%20health.%20However%2C%20the%20limited%0Aavailability%20of%20large-scale%2C%20high-quality%20CMR%20datasets%20poses%20a%20major%20challenge%0Ato%20the%20effective%20application%20of%20artificial%20intelligence%20%28AI%29%20in%20this%20domain.%0AEven%20the%20amount%20of%20unlabeled%20data%20and%20the%20health%20status%20it%20covers%20are%20difficult%0Ato%20meet%20the%20needs%20of%20model%20pretraining%2C%20which%20hinders%20the%20performance%20of%20AI%0Amodels%20on%20downstream%20tasks.%20In%20this%20study%2C%20we%20present%20Cardiac%20Phenotype-Guided%0ACMR%20Generation%20%28CPGG%29%2C%20a%20novel%20approach%20for%20generating%20diverse%20CMR%20data%20that%0Acovers%20a%20wide%20spectrum%20of%20cardiac%20health%20status.%20The%20CPGG%20framework%20consists%20of%0Atwo%20stages%3A%20in%20the%20first%20stage%2C%20a%20generative%20model%20is%20trained%20using%20cardiac%0Aphenotypes%20derived%20from%20CMR%20data%3B%20in%20the%20second%20stage%2C%20a%20masked%20autoregressive%0Adiffusion%20model%2C%20conditioned%20on%20these%20phenotypes%2C%20generates%20high-fidelity%20CMR%0Acine%20sequences%20that%20capture%20both%20structural%20and%20functional%20features%20of%20the%0Aheart%20in%20a%20fine-grained%20manner.%20We%20synthesized%20a%20massive%20amount%20of%20CMR%20to%0Aexpand%20the%20pretraining%20data.%20Experimental%20results%20show%20that%20CPGG%20generates%0Ahigh-quality%20synthetic%20CMR%20data%2C%20significantly%20improving%20performance%20on%20various%0Adownstream%20tasks%2C%20including%20diagnosis%20and%20cardiac%20phenotypes%20prediction.%20These%0Agains%20are%20demonstrated%20across%20both%20public%20and%20private%20datasets%2C%20highlighting%0Athe%20effectiveness%20of%20our%20approach.%20Code%20is%20availabel%20at%0Ahttps%3A//anonymous.4open.science/r/CPGG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03426v1&entry.124074799=Read"},
{"title": "Taking a Big Step: Large Learning Rates in Denoising Score Matching\n  Prevent Memorization", "author": "Yu-Han Wu and Pierre Marion and G\u00e9rard Biau and Claire Boyer", "abstract": "  Denoising score matching plays a pivotal role in the performance of\ndiffusion-based generative models. However, the empirical optimal score--the\nexact solution to the denoising score matching--leads to memorization, where\ngenerated samples replicate the training data. Yet, in practice, only a\nmoderate degree of memorization is observed, even without explicit\nregularization. In this paper, we investigate this phenomenon by uncovering an\nimplicit regularization mechanism driven by large learning rates. Specifically,\nwe show that in the small-noise regime, the empirical optimal score exhibits\nhigh irregularity. We then prove that, when trained by stochastic gradient\ndescent with a large enough learning rate, neural networks cannot stably\nconverge to a local minimum with arbitrarily small excess risk. Consequently,\nthe learned score cannot be arbitrarily close to the empirical optimal score,\nthereby mitigating memorization. To make the analysis tractable, we consider\none-dimensional data and two-layer neural networks. Experiments validate the\ncrucial role of the learning rate in preventing memorization, even beyond the\none-dimensional setting.\n", "link": "http://arxiv.org/abs/2502.03435v2", "date": "2025-05-06", "relevancy": 2.1644, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5569}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5343}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taking%20a%20Big%20Step%3A%20Large%20Learning%20Rates%20in%20Denoising%20Score%20Matching%0A%20%20Prevent%20Memorization&body=Title%3A%20Taking%20a%20Big%20Step%3A%20Large%20Learning%20Rates%20in%20Denoising%20Score%20Matching%0A%20%20Prevent%20Memorization%0AAuthor%3A%20Yu-Han%20Wu%20and%20Pierre%20Marion%20and%20G%C3%A9rard%20Biau%20and%20Claire%20Boyer%0AAbstract%3A%20%20%20Denoising%20score%20matching%20plays%20a%20pivotal%20role%20in%20the%20performance%20of%0Adiffusion-based%20generative%20models.%20However%2C%20the%20empirical%20optimal%20score--the%0Aexact%20solution%20to%20the%20denoising%20score%20matching--leads%20to%20memorization%2C%20where%0Agenerated%20samples%20replicate%20the%20training%20data.%20Yet%2C%20in%20practice%2C%20only%20a%0Amoderate%20degree%20of%20memorization%20is%20observed%2C%20even%20without%20explicit%0Aregularization.%20In%20this%20paper%2C%20we%20investigate%20this%20phenomenon%20by%20uncovering%20an%0Aimplicit%20regularization%20mechanism%20driven%20by%20large%20learning%20rates.%20Specifically%2C%0Awe%20show%20that%20in%20the%20small-noise%20regime%2C%20the%20empirical%20optimal%20score%20exhibits%0Ahigh%20irregularity.%20We%20then%20prove%20that%2C%20when%20trained%20by%20stochastic%20gradient%0Adescent%20with%20a%20large%20enough%20learning%20rate%2C%20neural%20networks%20cannot%20stably%0Aconverge%20to%20a%20local%20minimum%20with%20arbitrarily%20small%20excess%20risk.%20Consequently%2C%0Athe%20learned%20score%20cannot%20be%20arbitrarily%20close%20to%20the%20empirical%20optimal%20score%2C%0Athereby%20mitigating%20memorization.%20To%20make%20the%20analysis%20tractable%2C%20we%20consider%0Aone-dimensional%20data%20and%20two-layer%20neural%20networks.%20Experiments%20validate%20the%0Acrucial%20role%20of%20the%20learning%20rate%20in%20preventing%20memorization%2C%20even%20beyond%20the%0Aone-dimensional%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaking%2520a%2520Big%2520Step%253A%2520Large%2520Learning%2520Rates%2520in%2520Denoising%2520Score%2520Matching%250A%2520%2520Prevent%2520Memorization%26entry.906535625%3DYu-Han%2520Wu%2520and%2520Pierre%2520Marion%2520and%2520G%25C3%25A9rard%2520Biau%2520and%2520Claire%2520Boyer%26entry.1292438233%3D%2520%2520Denoising%2520score%2520matching%2520plays%2520a%2520pivotal%2520role%2520in%2520the%2520performance%2520of%250Adiffusion-based%2520generative%2520models.%2520However%252C%2520the%2520empirical%2520optimal%2520score--the%250Aexact%2520solution%2520to%2520the%2520denoising%2520score%2520matching--leads%2520to%2520memorization%252C%2520where%250Agenerated%2520samples%2520replicate%2520the%2520training%2520data.%2520Yet%252C%2520in%2520practice%252C%2520only%2520a%250Amoderate%2520degree%2520of%2520memorization%2520is%2520observed%252C%2520even%2520without%2520explicit%250Aregularization.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520this%2520phenomenon%2520by%2520uncovering%2520an%250Aimplicit%2520regularization%2520mechanism%2520driven%2520by%2520large%2520learning%2520rates.%2520Specifically%252C%250Awe%2520show%2520that%2520in%2520the%2520small-noise%2520regime%252C%2520the%2520empirical%2520optimal%2520score%2520exhibits%250Ahigh%2520irregularity.%2520We%2520then%2520prove%2520that%252C%2520when%2520trained%2520by%2520stochastic%2520gradient%250Adescent%2520with%2520a%2520large%2520enough%2520learning%2520rate%252C%2520neural%2520networks%2520cannot%2520stably%250Aconverge%2520to%2520a%2520local%2520minimum%2520with%2520arbitrarily%2520small%2520excess%2520risk.%2520Consequently%252C%250Athe%2520learned%2520score%2520cannot%2520be%2520arbitrarily%2520close%2520to%2520the%2520empirical%2520optimal%2520score%252C%250Athereby%2520mitigating%2520memorization.%2520To%2520make%2520the%2520analysis%2520tractable%252C%2520we%2520consider%250Aone-dimensional%2520data%2520and%2520two-layer%2520neural%2520networks.%2520Experiments%2520validate%2520the%250Acrucial%2520role%2520of%2520the%2520learning%2520rate%2520in%2520preventing%2520memorization%252C%2520even%2520beyond%2520the%250Aone-dimensional%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taking%20a%20Big%20Step%3A%20Large%20Learning%20Rates%20in%20Denoising%20Score%20Matching%0A%20%20Prevent%20Memorization&entry.906535625=Yu-Han%20Wu%20and%20Pierre%20Marion%20and%20G%C3%A9rard%20Biau%20and%20Claire%20Boyer&entry.1292438233=%20%20Denoising%20score%20matching%20plays%20a%20pivotal%20role%20in%20the%20performance%20of%0Adiffusion-based%20generative%20models.%20However%2C%20the%20empirical%20optimal%20score--the%0Aexact%20solution%20to%20the%20denoising%20score%20matching--leads%20to%20memorization%2C%20where%0Agenerated%20samples%20replicate%20the%20training%20data.%20Yet%2C%20in%20practice%2C%20only%20a%0Amoderate%20degree%20of%20memorization%20is%20observed%2C%20even%20without%20explicit%0Aregularization.%20In%20this%20paper%2C%20we%20investigate%20this%20phenomenon%20by%20uncovering%20an%0Aimplicit%20regularization%20mechanism%20driven%20by%20large%20learning%20rates.%20Specifically%2C%0Awe%20show%20that%20in%20the%20small-noise%20regime%2C%20the%20empirical%20optimal%20score%20exhibits%0Ahigh%20irregularity.%20We%20then%20prove%20that%2C%20when%20trained%20by%20stochastic%20gradient%0Adescent%20with%20a%20large%20enough%20learning%20rate%2C%20neural%20networks%20cannot%20stably%0Aconverge%20to%20a%20local%20minimum%20with%20arbitrarily%20small%20excess%20risk.%20Consequently%2C%0Athe%20learned%20score%20cannot%20be%20arbitrarily%20close%20to%20the%20empirical%20optimal%20score%2C%0Athereby%20mitigating%20memorization.%20To%20make%20the%20analysis%20tractable%2C%20we%20consider%0Aone-dimensional%20data%20and%20two-layer%20neural%20networks.%20Experiments%20validate%20the%0Acrucial%20role%20of%20the%20learning%20rate%20in%20preventing%20memorization%2C%20even%20beyond%20the%0Aone-dimensional%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03435v2&entry.124074799=Read"},
{"title": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in\n  LLM-Based Agents", "author": "Schaun Wheeler and Olivier Jeunen", "abstract": "  Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.\n", "link": "http://arxiv.org/abs/2505.03434v1", "date": "2025-05-06", "relevancy": 2.1492, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Procedural%20Memory%20Is%20Not%20All%20You%20Need%3A%20Bridging%20Cognitive%20Gaps%20in%0A%20%20LLM-Based%20Agents&body=Title%3A%20Procedural%20Memory%20Is%20Not%20All%20You%20Need%3A%20Bridging%20Cognitive%20Gaps%20in%0A%20%20LLM-Based%20Agents%0AAuthor%3A%20Schaun%20Wheeler%20and%20Olivier%20Jeunen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20represent%20a%20landmark%20achievement%20in%20Artificial%0AIntelligence%20%28AI%29%2C%20demonstrating%20unprecedented%20proficiency%20in%20procedural%20tasks%0Asuch%20as%20text%20generation%2C%20code%20completion%2C%20and%20conversational%20coherence.%20These%0Acapabilities%20stem%20from%20their%20architecture%2C%20which%20mirrors%20human%20procedural%0Amemory%20--%20the%20brain%27s%20ability%20to%20automate%20repetitive%2C%20pattern-driven%20tasks%0Athrough%20practice.%20However%2C%20as%20LLMs%20are%20increasingly%20deployed%20in%20real-world%0Aapplications%2C%20it%20becomes%20impossible%20to%20ignore%20their%20limitations%20operating%20in%0Acomplex%2C%20unpredictable%20environments.%20This%20paper%20argues%20that%20LLMs%2C%20while%0Atransformative%2C%20are%20fundamentally%20constrained%20by%20their%20reliance%20on%20procedural%0Amemory.%20To%20create%20agents%20capable%20of%20navigating%20%60%60wicked%27%27%20learning%20environments%0A--%20where%20rules%20shift%2C%20feedback%20is%20ambiguous%2C%20and%20novelty%20is%20the%20norm%20--%20we%20must%0Aaugment%20LLMs%20with%20semantic%20memory%20and%20associative%20learning%20systems.%20By%20adopting%0Aa%20modular%20architecture%20that%20decouples%20these%20cognitive%20functions%2C%20we%20can%20bridge%0Athe%20gap%20between%20narrow%20procedural%20expertise%20and%20the%20adaptive%20intelligence%0Arequired%20for%20real-world%20problem-solving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcedural%2520Memory%2520Is%2520Not%2520All%2520You%2520Need%253A%2520Bridging%2520Cognitive%2520Gaps%2520in%250A%2520%2520LLM-Based%2520Agents%26entry.906535625%3DSchaun%2520Wheeler%2520and%2520Olivier%2520Jeunen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520represent%2520a%2520landmark%2520achievement%2520in%2520Artificial%250AIntelligence%2520%2528AI%2529%252C%2520demonstrating%2520unprecedented%2520proficiency%2520in%2520procedural%2520tasks%250Asuch%2520as%2520text%2520generation%252C%2520code%2520completion%252C%2520and%2520conversational%2520coherence.%2520These%250Acapabilities%2520stem%2520from%2520their%2520architecture%252C%2520which%2520mirrors%2520human%2520procedural%250Amemory%2520--%2520the%2520brain%2527s%2520ability%2520to%2520automate%2520repetitive%252C%2520pattern-driven%2520tasks%250Athrough%2520practice.%2520However%252C%2520as%2520LLMs%2520are%2520increasingly%2520deployed%2520in%2520real-world%250Aapplications%252C%2520it%2520becomes%2520impossible%2520to%2520ignore%2520their%2520limitations%2520operating%2520in%250Acomplex%252C%2520unpredictable%2520environments.%2520This%2520paper%2520argues%2520that%2520LLMs%252C%2520while%250Atransformative%252C%2520are%2520fundamentally%2520constrained%2520by%2520their%2520reliance%2520on%2520procedural%250Amemory.%2520To%2520create%2520agents%2520capable%2520of%2520navigating%2520%2560%2560wicked%2527%2527%2520learning%2520environments%250A--%2520where%2520rules%2520shift%252C%2520feedback%2520is%2520ambiguous%252C%2520and%2520novelty%2520is%2520the%2520norm%2520--%2520we%2520must%250Aaugment%2520LLMs%2520with%2520semantic%2520memory%2520and%2520associative%2520learning%2520systems.%2520By%2520adopting%250Aa%2520modular%2520architecture%2520that%2520decouples%2520these%2520cognitive%2520functions%252C%2520we%2520can%2520bridge%250Athe%2520gap%2520between%2520narrow%2520procedural%2520expertise%2520and%2520the%2520adaptive%2520intelligence%250Arequired%2520for%2520real-world%2520problem-solving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Procedural%20Memory%20Is%20Not%20All%20You%20Need%3A%20Bridging%20Cognitive%20Gaps%20in%0A%20%20LLM-Based%20Agents&entry.906535625=Schaun%20Wheeler%20and%20Olivier%20Jeunen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20represent%20a%20landmark%20achievement%20in%20Artificial%0AIntelligence%20%28AI%29%2C%20demonstrating%20unprecedented%20proficiency%20in%20procedural%20tasks%0Asuch%20as%20text%20generation%2C%20code%20completion%2C%20and%20conversational%20coherence.%20These%0Acapabilities%20stem%20from%20their%20architecture%2C%20which%20mirrors%20human%20procedural%0Amemory%20--%20the%20brain%27s%20ability%20to%20automate%20repetitive%2C%20pattern-driven%20tasks%0Athrough%20practice.%20However%2C%20as%20LLMs%20are%20increasingly%20deployed%20in%20real-world%0Aapplications%2C%20it%20becomes%20impossible%20to%20ignore%20their%20limitations%20operating%20in%0Acomplex%2C%20unpredictable%20environments.%20This%20paper%20argues%20that%20LLMs%2C%20while%0Atransformative%2C%20are%20fundamentally%20constrained%20by%20their%20reliance%20on%20procedural%0Amemory.%20To%20create%20agents%20capable%20of%20navigating%20%60%60wicked%27%27%20learning%20environments%0A--%20where%20rules%20shift%2C%20feedback%20is%20ambiguous%2C%20and%20novelty%20is%20the%20norm%20--%20we%20must%0Aaugment%20LLMs%20with%20semantic%20memory%20and%20associative%20learning%20systems.%20By%20adopting%0Aa%20modular%20architecture%20that%20decouples%20these%20cognitive%20functions%2C%20we%20can%20bridge%0Athe%20gap%20between%20narrow%20procedural%20expertise%20and%20the%20adaptive%20intelligence%0Arequired%20for%20real-world%20problem-solving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03434v1&entry.124074799=Read"},
{"title": "Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps\n  from T1-Weighted MRI using CycleGAN Models", "author": "Xin Du and Francesca M. Cozzi and Rajesh Jena", "abstract": "  Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.\n", "link": "http://arxiv.org/abs/2505.03662v1", "date": "2025-05-06", "relevancy": 2.1484, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5413}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5413}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revolutionizing%20Brain%20Tumor%20Imaging%3A%20Generating%20Synthetic%203D%20FA%20Maps%0A%20%20from%20T1-Weighted%20MRI%20using%20CycleGAN%20Models&body=Title%3A%20Revolutionizing%20Brain%20Tumor%20Imaging%3A%20Generating%20Synthetic%203D%20FA%20Maps%0A%20%20from%20T1-Weighted%20MRI%20using%20CycleGAN%20Models%0AAuthor%3A%20Xin%20Du%20and%20Francesca%20M.%20Cozzi%20and%20Rajesh%20Jena%0AAbstract%3A%20%20%20Fractional%20anisotropy%20%28FA%29%20and%20directionally%20encoded%20colour%20%28DEC%29%20maps%20are%0Aessential%20for%20evaluating%20white%20matter%20integrity%20and%20structural%20connectivity%20in%0Aneuroimaging.%20However%2C%20the%20spatial%20misalignment%20between%20FA%20maps%20and%0Atractography%20atlases%20hinders%20their%20effective%20integration%20into%20predictive%0Amodels.%20To%20address%20this%20issue%2C%20we%20propose%20a%20CycleGAN%20based%20approach%20for%0Agenerating%20FA%20maps%20directly%20from%20T1-weighted%20MRI%20scans%2C%20representing%20the%20first%0Aapplication%20of%20this%20technique%20to%20both%20healthy%20and%20tumour-affected%20tissues.%20Our%0Amodel%2C%20trained%20on%20unpaired%20data%2C%20produces%20high%20fidelity%20maps%2C%20which%20have%20been%0Arigorously%20evaluated%20using%20Structural%20Similarity%20Index%20%28SSIM%29%20and%20Peak%0ASignal-to-Noise%20Ratio%20%28PSNR%29%2C%20demonstrating%20particularly%20robust%20performance%20in%0Atumour%20regions.%20Radiological%20assessments%20further%20underscore%20the%20model%27s%0Apotential%20to%20enhance%20clinical%20workflows%20by%20providing%20an%20AI-driven%20alternative%0Athat%20reduces%20the%20necessity%20for%20additional%20scans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevolutionizing%2520Brain%2520Tumor%2520Imaging%253A%2520Generating%2520Synthetic%25203D%2520FA%2520Maps%250A%2520%2520from%2520T1-Weighted%2520MRI%2520using%2520CycleGAN%2520Models%26entry.906535625%3DXin%2520Du%2520and%2520Francesca%2520M.%2520Cozzi%2520and%2520Rajesh%2520Jena%26entry.1292438233%3D%2520%2520Fractional%2520anisotropy%2520%2528FA%2529%2520and%2520directionally%2520encoded%2520colour%2520%2528DEC%2529%2520maps%2520are%250Aessential%2520for%2520evaluating%2520white%2520matter%2520integrity%2520and%2520structural%2520connectivity%2520in%250Aneuroimaging.%2520However%252C%2520the%2520spatial%2520misalignment%2520between%2520FA%2520maps%2520and%250Atractography%2520atlases%2520hinders%2520their%2520effective%2520integration%2520into%2520predictive%250Amodels.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520CycleGAN%2520based%2520approach%2520for%250Agenerating%2520FA%2520maps%2520directly%2520from%2520T1-weighted%2520MRI%2520scans%252C%2520representing%2520the%2520first%250Aapplication%2520of%2520this%2520technique%2520to%2520both%2520healthy%2520and%2520tumour-affected%2520tissues.%2520Our%250Amodel%252C%2520trained%2520on%2520unpaired%2520data%252C%2520produces%2520high%2520fidelity%2520maps%252C%2520which%2520have%2520been%250Arigorously%2520evaluated%2520using%2520Structural%2520Similarity%2520Index%2520%2528SSIM%2529%2520and%2520Peak%250ASignal-to-Noise%2520Ratio%2520%2528PSNR%2529%252C%2520demonstrating%2520particularly%2520robust%2520performance%2520in%250Atumour%2520regions.%2520Radiological%2520assessments%2520further%2520underscore%2520the%2520model%2527s%250Apotential%2520to%2520enhance%2520clinical%2520workflows%2520by%2520providing%2520an%2520AI-driven%2520alternative%250Athat%2520reduces%2520the%2520necessity%2520for%2520additional%2520scans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revolutionizing%20Brain%20Tumor%20Imaging%3A%20Generating%20Synthetic%203D%20FA%20Maps%0A%20%20from%20T1-Weighted%20MRI%20using%20CycleGAN%20Models&entry.906535625=Xin%20Du%20and%20Francesca%20M.%20Cozzi%20and%20Rajesh%20Jena&entry.1292438233=%20%20Fractional%20anisotropy%20%28FA%29%20and%20directionally%20encoded%20colour%20%28DEC%29%20maps%20are%0Aessential%20for%20evaluating%20white%20matter%20integrity%20and%20structural%20connectivity%20in%0Aneuroimaging.%20However%2C%20the%20spatial%20misalignment%20between%20FA%20maps%20and%0Atractography%20atlases%20hinders%20their%20effective%20integration%20into%20predictive%0Amodels.%20To%20address%20this%20issue%2C%20we%20propose%20a%20CycleGAN%20based%20approach%20for%0Agenerating%20FA%20maps%20directly%20from%20T1-weighted%20MRI%20scans%2C%20representing%20the%20first%0Aapplication%20of%20this%20technique%20to%20both%20healthy%20and%20tumour-affected%20tissues.%20Our%0Amodel%2C%20trained%20on%20unpaired%20data%2C%20produces%20high%20fidelity%20maps%2C%20which%20have%20been%0Arigorously%20evaluated%20using%20Structural%20Similarity%20Index%20%28SSIM%29%20and%20Peak%0ASignal-to-Noise%20Ratio%20%28PSNR%29%2C%20demonstrating%20particularly%20robust%20performance%20in%0Atumour%20regions.%20Radiological%20assessments%20further%20underscore%20the%20model%27s%0Apotential%20to%20enhance%20clinical%20workflows%20by%20providing%20an%20AI-driven%20alternative%0Athat%20reduces%20the%20necessity%20for%20additional%20scans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03662v1&entry.124074799=Read"},
{"title": "Robustness in AI-Generated Detection: Enhancing Resistance to\n  Adversarial Attacks", "author": "Sun Haoxuan and Hong Yan and Zhan Jiahui and Chen Haoxing and Lan Jun and Zhu Huijia and Wang Weiqiang and Zhang Liqing and Zhang Jianfu", "abstract": "  The rapid advancement of generative image technology has introduced\nsignificant security concerns, particularly in the domain of face generation\ndetection. This paper investigates the vulnerabilities of current AI-generated\nface detection systems. Our study reveals that while existing detection methods\noften achieve high accuracy under standard conditions, they exhibit limited\nrobustness against adversarial attacks. To address these challenges, we propose\nan approach that integrates adversarial training to mitigate the impact of\nadversarial examples. Furthermore, we utilize diffusion inversion and\nreconstruction to further enhance detection robustness. Experimental results\ndemonstrate that minor adversarial perturbations can easily bypass existing\ndetection systems, but our method significantly improves the robustness of\nthese systems. Additionally, we provide an in-depth analysis of adversarial and\nbenign examples, offering insights into the intrinsic characteristics of\nAI-generated content. All associated code will be made publicly available in a\ndedicated repository to facilitate further research and verification.\n", "link": "http://arxiv.org/abs/2505.03435v1", "date": "2025-05-06", "relevancy": 2.1439, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5436}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.538}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20in%20AI-Generated%20Detection%3A%20Enhancing%20Resistance%20to%0A%20%20Adversarial%20Attacks&body=Title%3A%20Robustness%20in%20AI-Generated%20Detection%3A%20Enhancing%20Resistance%20to%0A%20%20Adversarial%20Attacks%0AAuthor%3A%20Sun%20Haoxuan%20and%20Hong%20Yan%20and%20Zhan%20Jiahui%20and%20Chen%20Haoxing%20and%20Lan%20Jun%20and%20Zhu%20Huijia%20and%20Wang%20Weiqiang%20and%20Zhang%20Liqing%20and%20Zhang%20Jianfu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20generative%20image%20technology%20has%20introduced%0Asignificant%20security%20concerns%2C%20particularly%20in%20the%20domain%20of%20face%20generation%0Adetection.%20This%20paper%20investigates%20the%20vulnerabilities%20of%20current%20AI-generated%0Aface%20detection%20systems.%20Our%20study%20reveals%20that%20while%20existing%20detection%20methods%0Aoften%20achieve%20high%20accuracy%20under%20standard%20conditions%2C%20they%20exhibit%20limited%0Arobustness%20against%20adversarial%20attacks.%20To%20address%20these%20challenges%2C%20we%20propose%0Aan%20approach%20that%20integrates%20adversarial%20training%20to%20mitigate%20the%20impact%20of%0Aadversarial%20examples.%20Furthermore%2C%20we%20utilize%20diffusion%20inversion%20and%0Areconstruction%20to%20further%20enhance%20detection%20robustness.%20Experimental%20results%0Ademonstrate%20that%20minor%20adversarial%20perturbations%20can%20easily%20bypass%20existing%0Adetection%20systems%2C%20but%20our%20method%20significantly%20improves%20the%20robustness%20of%0Athese%20systems.%20Additionally%2C%20we%20provide%20an%20in-depth%20analysis%20of%20adversarial%20and%0Abenign%20examples%2C%20offering%20insights%20into%20the%20intrinsic%20characteristics%20of%0AAI-generated%20content.%20All%20associated%20code%20will%20be%20made%20publicly%20available%20in%20a%0Adedicated%20repository%20to%20facilitate%20further%20research%20and%20verification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520in%2520AI-Generated%2520Detection%253A%2520Enhancing%2520Resistance%2520to%250A%2520%2520Adversarial%2520Attacks%26entry.906535625%3DSun%2520Haoxuan%2520and%2520Hong%2520Yan%2520and%2520Zhan%2520Jiahui%2520and%2520Chen%2520Haoxing%2520and%2520Lan%2520Jun%2520and%2520Zhu%2520Huijia%2520and%2520Wang%2520Weiqiang%2520and%2520Zhang%2520Liqing%2520and%2520Zhang%2520Jianfu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520generative%2520image%2520technology%2520has%2520introduced%250Asignificant%2520security%2520concerns%252C%2520particularly%2520in%2520the%2520domain%2520of%2520face%2520generation%250Adetection.%2520This%2520paper%2520investigates%2520the%2520vulnerabilities%2520of%2520current%2520AI-generated%250Aface%2520detection%2520systems.%2520Our%2520study%2520reveals%2520that%2520while%2520existing%2520detection%2520methods%250Aoften%2520achieve%2520high%2520accuracy%2520under%2520standard%2520conditions%252C%2520they%2520exhibit%2520limited%250Arobustness%2520against%2520adversarial%2520attacks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250Aan%2520approach%2520that%2520integrates%2520adversarial%2520training%2520to%2520mitigate%2520the%2520impact%2520of%250Aadversarial%2520examples.%2520Furthermore%252C%2520we%2520utilize%2520diffusion%2520inversion%2520and%250Areconstruction%2520to%2520further%2520enhance%2520detection%2520robustness.%2520Experimental%2520results%250Ademonstrate%2520that%2520minor%2520adversarial%2520perturbations%2520can%2520easily%2520bypass%2520existing%250Adetection%2520systems%252C%2520but%2520our%2520method%2520significantly%2520improves%2520the%2520robustness%2520of%250Athese%2520systems.%2520Additionally%252C%2520we%2520provide%2520an%2520in-depth%2520analysis%2520of%2520adversarial%2520and%250Abenign%2520examples%252C%2520offering%2520insights%2520into%2520the%2520intrinsic%2520characteristics%2520of%250AAI-generated%2520content.%2520All%2520associated%2520code%2520will%2520be%2520made%2520publicly%2520available%2520in%2520a%250Adedicated%2520repository%2520to%2520facilitate%2520further%2520research%2520and%2520verification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20in%20AI-Generated%20Detection%3A%20Enhancing%20Resistance%20to%0A%20%20Adversarial%20Attacks&entry.906535625=Sun%20Haoxuan%20and%20Hong%20Yan%20and%20Zhan%20Jiahui%20and%20Chen%20Haoxing%20and%20Lan%20Jun%20and%20Zhu%20Huijia%20and%20Wang%20Weiqiang%20and%20Zhang%20Liqing%20and%20Zhang%20Jianfu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20generative%20image%20technology%20has%20introduced%0Asignificant%20security%20concerns%2C%20particularly%20in%20the%20domain%20of%20face%20generation%0Adetection.%20This%20paper%20investigates%20the%20vulnerabilities%20of%20current%20AI-generated%0Aface%20detection%20systems.%20Our%20study%20reveals%20that%20while%20existing%20detection%20methods%0Aoften%20achieve%20high%20accuracy%20under%20standard%20conditions%2C%20they%20exhibit%20limited%0Arobustness%20against%20adversarial%20attacks.%20To%20address%20these%20challenges%2C%20we%20propose%0Aan%20approach%20that%20integrates%20adversarial%20training%20to%20mitigate%20the%20impact%20of%0Aadversarial%20examples.%20Furthermore%2C%20we%20utilize%20diffusion%20inversion%20and%0Areconstruction%20to%20further%20enhance%20detection%20robustness.%20Experimental%20results%0Ademonstrate%20that%20minor%20adversarial%20perturbations%20can%20easily%20bypass%20existing%0Adetection%20systems%2C%20but%20our%20method%20significantly%20improves%20the%20robustness%20of%0Athese%20systems.%20Additionally%2C%20we%20provide%20an%20in-depth%20analysis%20of%20adversarial%20and%0Abenign%20examples%2C%20offering%20insights%20into%20the%20intrinsic%20characteristics%20of%0AAI-generated%20content.%20All%20associated%20code%20will%20be%20made%20publicly%20available%20in%20a%0Adedicated%20repository%20to%20facilitate%20further%20research%20and%20verification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03435v1&entry.124074799=Read"},
{"title": "RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT", "author": "Chuyu Zhao and Hao Huang and Jiashuo Guo and Ziyu Shen and Zhongwei Zhou and Jie Liu and Zekuan Yu", "abstract": "  Semi-supervised learning has become a compelling approach for 3D tooth\nsegmentation from CBCT scans, where labeled data is minimal. However, existing\nmethods still face two persistent challenges: limited corrective supervision in\nstructurally ambiguous or mislabeled regions during supervised training and\nperformance degradation caused by unreliable pseudo-labels on unlabeled data.\nTo address these problems, we propose Region-Aware Instructive Learning (RAIL),\na dual-group dual-student, semi-supervised framework. Each group contains two\nstudent models guided by a shared teacher network. By alternating training\nbetween the two groups, RAIL promotes intergroup knowledge transfer and\ncollaborative region-aware instruction while reducing overfitting to the\ncharacteristics of any single model. Specifically, RAIL introduces two\ninstructive mechanisms. Disagreement-Focused Supervision (DFS) Controller\nimproves supervised learning by instructing predictions only within areas where\nstudent outputs diverge from both ground truth and the best student, thereby\nconcentrating supervision on structurally ambiguous or mislabeled areas. In the\nunsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces\nagreement in regions with high model certainty while reducing the effect of\nlow-confidence predictions during training. This helps prevent our model from\nlearning unstable patterns and improves the overall reliability of\npseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets\nshow that RAIL surpasses state-of-the-art methods under limited annotation. Our\ncode will be available at https://github.com/Tournesol-Saturday/RAIL.\n", "link": "http://arxiv.org/abs/2505.03538v1", "date": "2025-05-06", "relevancy": 2.1423, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5659}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5256}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAIL%3A%20Region-Aware%20Instructive%20Learning%20for%20Semi-Supervised%20Tooth%0A%20%20Segmentation%20in%20CBCT&body=Title%3A%20RAIL%3A%20Region-Aware%20Instructive%20Learning%20for%20Semi-Supervised%20Tooth%0A%20%20Segmentation%20in%20CBCT%0AAuthor%3A%20Chuyu%20Zhao%20and%20Hao%20Huang%20and%20Jiashuo%20Guo%20and%20Ziyu%20Shen%20and%20Zhongwei%20Zhou%20and%20Jie%20Liu%20and%20Zekuan%20Yu%0AAbstract%3A%20%20%20Semi-supervised%20learning%20has%20become%20a%20compelling%20approach%20for%203D%20tooth%0Asegmentation%20from%20CBCT%20scans%2C%20where%20labeled%20data%20is%20minimal.%20However%2C%20existing%0Amethods%20still%20face%20two%20persistent%20challenges%3A%20limited%20corrective%20supervision%20in%0Astructurally%20ambiguous%20or%20mislabeled%20regions%20during%20supervised%20training%20and%0Aperformance%20degradation%20caused%20by%20unreliable%20pseudo-labels%20on%20unlabeled%20data.%0ATo%20address%20these%20problems%2C%20we%20propose%20Region-Aware%20Instructive%20Learning%20%28RAIL%29%2C%0Aa%20dual-group%20dual-student%2C%20semi-supervised%20framework.%20Each%20group%20contains%20two%0Astudent%20models%20guided%20by%20a%20shared%20teacher%20network.%20By%20alternating%20training%0Abetween%20the%20two%20groups%2C%20RAIL%20promotes%20intergroup%20knowledge%20transfer%20and%0Acollaborative%20region-aware%20instruction%20while%20reducing%20overfitting%20to%20the%0Acharacteristics%20of%20any%20single%20model.%20Specifically%2C%20RAIL%20introduces%20two%0Ainstructive%20mechanisms.%20Disagreement-Focused%20Supervision%20%28DFS%29%20Controller%0Aimproves%20supervised%20learning%20by%20instructing%20predictions%20only%20within%20areas%20where%0Astudent%20outputs%20diverge%20from%20both%20ground%20truth%20and%20the%20best%20student%2C%20thereby%0Aconcentrating%20supervision%20on%20structurally%20ambiguous%20or%20mislabeled%20areas.%20In%20the%0Aunsupervised%20phase%2C%20Confidence-Aware%20Learning%20%28CAL%29%20Modulator%20reinforces%0Aagreement%20in%20regions%20with%20high%20model%20certainty%20while%20reducing%20the%20effect%20of%0Alow-confidence%20predictions%20during%20training.%20This%20helps%20prevent%20our%20model%20from%0Alearning%20unstable%20patterns%20and%20improves%20the%20overall%20reliability%20of%0Apseudo-labels.%20Extensive%20experiments%20on%20four%20CBCT%20tooth%20segmentation%20datasets%0Ashow%20that%20RAIL%20surpasses%20state-of-the-art%20methods%20under%20limited%20annotation.%20Our%0Acode%20will%20be%20available%20at%20https%3A//github.com/Tournesol-Saturday/RAIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAIL%253A%2520Region-Aware%2520Instructive%2520Learning%2520for%2520Semi-Supervised%2520Tooth%250A%2520%2520Segmentation%2520in%2520CBCT%26entry.906535625%3DChuyu%2520Zhao%2520and%2520Hao%2520Huang%2520and%2520Jiashuo%2520Guo%2520and%2520Ziyu%2520Shen%2520and%2520Zhongwei%2520Zhou%2520and%2520Jie%2520Liu%2520and%2520Zekuan%2520Yu%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%2520has%2520become%2520a%2520compelling%2520approach%2520for%25203D%2520tooth%250Asegmentation%2520from%2520CBCT%2520scans%252C%2520where%2520labeled%2520data%2520is%2520minimal.%2520However%252C%2520existing%250Amethods%2520still%2520face%2520two%2520persistent%2520challenges%253A%2520limited%2520corrective%2520supervision%2520in%250Astructurally%2520ambiguous%2520or%2520mislabeled%2520regions%2520during%2520supervised%2520training%2520and%250Aperformance%2520degradation%2520caused%2520by%2520unreliable%2520pseudo-labels%2520on%2520unlabeled%2520data.%250ATo%2520address%2520these%2520problems%252C%2520we%2520propose%2520Region-Aware%2520Instructive%2520Learning%2520%2528RAIL%2529%252C%250Aa%2520dual-group%2520dual-student%252C%2520semi-supervised%2520framework.%2520Each%2520group%2520contains%2520two%250Astudent%2520models%2520guided%2520by%2520a%2520shared%2520teacher%2520network.%2520By%2520alternating%2520training%250Abetween%2520the%2520two%2520groups%252C%2520RAIL%2520promotes%2520intergroup%2520knowledge%2520transfer%2520and%250Acollaborative%2520region-aware%2520instruction%2520while%2520reducing%2520overfitting%2520to%2520the%250Acharacteristics%2520of%2520any%2520single%2520model.%2520Specifically%252C%2520RAIL%2520introduces%2520two%250Ainstructive%2520mechanisms.%2520Disagreement-Focused%2520Supervision%2520%2528DFS%2529%2520Controller%250Aimproves%2520supervised%2520learning%2520by%2520instructing%2520predictions%2520only%2520within%2520areas%2520where%250Astudent%2520outputs%2520diverge%2520from%2520both%2520ground%2520truth%2520and%2520the%2520best%2520student%252C%2520thereby%250Aconcentrating%2520supervision%2520on%2520structurally%2520ambiguous%2520or%2520mislabeled%2520areas.%2520In%2520the%250Aunsupervised%2520phase%252C%2520Confidence-Aware%2520Learning%2520%2528CAL%2529%2520Modulator%2520reinforces%250Aagreement%2520in%2520regions%2520with%2520high%2520model%2520certainty%2520while%2520reducing%2520the%2520effect%2520of%250Alow-confidence%2520predictions%2520during%2520training.%2520This%2520helps%2520prevent%2520our%2520model%2520from%250Alearning%2520unstable%2520patterns%2520and%2520improves%2520the%2520overall%2520reliability%2520of%250Apseudo-labels.%2520Extensive%2520experiments%2520on%2520four%2520CBCT%2520tooth%2520segmentation%2520datasets%250Ashow%2520that%2520RAIL%2520surpasses%2520state-of-the-art%2520methods%2520under%2520limited%2520annotation.%2520Our%250Acode%2520will%2520be%2520available%2520at%2520https%253A//github.com/Tournesol-Saturday/RAIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAIL%3A%20Region-Aware%20Instructive%20Learning%20for%20Semi-Supervised%20Tooth%0A%20%20Segmentation%20in%20CBCT&entry.906535625=Chuyu%20Zhao%20and%20Hao%20Huang%20and%20Jiashuo%20Guo%20and%20Ziyu%20Shen%20and%20Zhongwei%20Zhou%20and%20Jie%20Liu%20and%20Zekuan%20Yu&entry.1292438233=%20%20Semi-supervised%20learning%20has%20become%20a%20compelling%20approach%20for%203D%20tooth%0Asegmentation%20from%20CBCT%20scans%2C%20where%20labeled%20data%20is%20minimal.%20However%2C%20existing%0Amethods%20still%20face%20two%20persistent%20challenges%3A%20limited%20corrective%20supervision%20in%0Astructurally%20ambiguous%20or%20mislabeled%20regions%20during%20supervised%20training%20and%0Aperformance%20degradation%20caused%20by%20unreliable%20pseudo-labels%20on%20unlabeled%20data.%0ATo%20address%20these%20problems%2C%20we%20propose%20Region-Aware%20Instructive%20Learning%20%28RAIL%29%2C%0Aa%20dual-group%20dual-student%2C%20semi-supervised%20framework.%20Each%20group%20contains%20two%0Astudent%20models%20guided%20by%20a%20shared%20teacher%20network.%20By%20alternating%20training%0Abetween%20the%20two%20groups%2C%20RAIL%20promotes%20intergroup%20knowledge%20transfer%20and%0Acollaborative%20region-aware%20instruction%20while%20reducing%20overfitting%20to%20the%0Acharacteristics%20of%20any%20single%20model.%20Specifically%2C%20RAIL%20introduces%20two%0Ainstructive%20mechanisms.%20Disagreement-Focused%20Supervision%20%28DFS%29%20Controller%0Aimproves%20supervised%20learning%20by%20instructing%20predictions%20only%20within%20areas%20where%0Astudent%20outputs%20diverge%20from%20both%20ground%20truth%20and%20the%20best%20student%2C%20thereby%0Aconcentrating%20supervision%20on%20structurally%20ambiguous%20or%20mislabeled%20areas.%20In%20the%0Aunsupervised%20phase%2C%20Confidence-Aware%20Learning%20%28CAL%29%20Modulator%20reinforces%0Aagreement%20in%20regions%20with%20high%20model%20certainty%20while%20reducing%20the%20effect%20of%0Alow-confidence%20predictions%20during%20training.%20This%20helps%20prevent%20our%20model%20from%0Alearning%20unstable%20patterns%20and%20improves%20the%20overall%20reliability%20of%0Apseudo-labels.%20Extensive%20experiments%20on%20four%20CBCT%20tooth%20segmentation%20datasets%0Ashow%20that%20RAIL%20surpasses%20state-of-the-art%20methods%20under%20limited%20annotation.%20Our%0Acode%20will%20be%20available%20at%20https%3A//github.com/Tournesol-Saturday/RAIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03538v1&entry.124074799=Read"},
{"title": "A Fusion-Guided Inception Network for Hyperspectral Image\n  Super-Resolution", "author": "Usman Muhammad and Jorma Laaksonen", "abstract": "  The fusion of low-spatial-resolution hyperspectral images (HSIs) with\nhigh-spatial-resolution conventional images (e.g., panchromatic or RGB) has\nplayed a significant role in recent advancements in HSI super-resolution.\nHowever, this fusion process relies on the availability of precise alignment\nbetween image pairs, which is often challenging in real-world scenarios. To\nmitigate this limitation, we propose a single-image super-resolution model\ncalled the Fusion-Guided Inception Network (FGIN). Specifically, we first\nemploy a spectral-spatial fusion module to effectively integrate spectral and\nspatial information at an early stage. Next, an Inception-like hierarchical\nfeature extraction strategy is used to capture multiscale spatial dependencies,\nfollowed by a dedicated multi-scale fusion block. To further enhance\nreconstruction quality, we incorporate an optimized upsampling module that\ncombines bilinear interpolation with depthwise separable convolutions.\nExperimental evaluations on two publicly available hyperspectral datasets\ndemonstrate the competitive performance of our method.\n", "link": "http://arxiv.org/abs/2505.03431v1", "date": "2025-05-06", "relevancy": 2.1415, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5386}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5363}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fusion-Guided%20Inception%20Network%20for%20Hyperspectral%20Image%0A%20%20Super-Resolution&body=Title%3A%20A%20Fusion-Guided%20Inception%20Network%20for%20Hyperspectral%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Usman%20Muhammad%20and%20Jorma%20Laaksonen%0AAbstract%3A%20%20%20The%20fusion%20of%20low-spatial-resolution%20hyperspectral%20images%20%28HSIs%29%20with%0Ahigh-spatial-resolution%20conventional%20images%20%28e.g.%2C%20panchromatic%20or%20RGB%29%20has%0Aplayed%20a%20significant%20role%20in%20recent%20advancements%20in%20HSI%20super-resolution.%0AHowever%2C%20this%20fusion%20process%20relies%20on%20the%20availability%20of%20precise%20alignment%0Abetween%20image%20pairs%2C%20which%20is%20often%20challenging%20in%20real-world%20scenarios.%20To%0Amitigate%20this%20limitation%2C%20we%20propose%20a%20single-image%20super-resolution%20model%0Acalled%20the%20Fusion-Guided%20Inception%20Network%20%28FGIN%29.%20Specifically%2C%20we%20first%0Aemploy%20a%20spectral-spatial%20fusion%20module%20to%20effectively%20integrate%20spectral%20and%0Aspatial%20information%20at%20an%20early%20stage.%20Next%2C%20an%20Inception-like%20hierarchical%0Afeature%20extraction%20strategy%20is%20used%20to%20capture%20multiscale%20spatial%20dependencies%2C%0Afollowed%20by%20a%20dedicated%20multi-scale%20fusion%20block.%20To%20further%20enhance%0Areconstruction%20quality%2C%20we%20incorporate%20an%20optimized%20upsampling%20module%20that%0Acombines%20bilinear%20interpolation%20with%20depthwise%20separable%20convolutions.%0AExperimental%20evaluations%20on%20two%20publicly%20available%20hyperspectral%20datasets%0Ademonstrate%20the%20competitive%20performance%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fusion-Guided%2520Inception%2520Network%2520for%2520Hyperspectral%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DUsman%2520Muhammad%2520and%2520Jorma%2520Laaksonen%26entry.1292438233%3D%2520%2520The%2520fusion%2520of%2520low-spatial-resolution%2520hyperspectral%2520images%2520%2528HSIs%2529%2520with%250Ahigh-spatial-resolution%2520conventional%2520images%2520%2528e.g.%252C%2520panchromatic%2520or%2520RGB%2529%2520has%250Aplayed%2520a%2520significant%2520role%2520in%2520recent%2520advancements%2520in%2520HSI%2520super-resolution.%250AHowever%252C%2520this%2520fusion%2520process%2520relies%2520on%2520the%2520availability%2520of%2520precise%2520alignment%250Abetween%2520image%2520pairs%252C%2520which%2520is%2520often%2520challenging%2520in%2520real-world%2520scenarios.%2520To%250Amitigate%2520this%2520limitation%252C%2520we%2520propose%2520a%2520single-image%2520super-resolution%2520model%250Acalled%2520the%2520Fusion-Guided%2520Inception%2520Network%2520%2528FGIN%2529.%2520Specifically%252C%2520we%2520first%250Aemploy%2520a%2520spectral-spatial%2520fusion%2520module%2520to%2520effectively%2520integrate%2520spectral%2520and%250Aspatial%2520information%2520at%2520an%2520early%2520stage.%2520Next%252C%2520an%2520Inception-like%2520hierarchical%250Afeature%2520extraction%2520strategy%2520is%2520used%2520to%2520capture%2520multiscale%2520spatial%2520dependencies%252C%250Afollowed%2520by%2520a%2520dedicated%2520multi-scale%2520fusion%2520block.%2520To%2520further%2520enhance%250Areconstruction%2520quality%252C%2520we%2520incorporate%2520an%2520optimized%2520upsampling%2520module%2520that%250Acombines%2520bilinear%2520interpolation%2520with%2520depthwise%2520separable%2520convolutions.%250AExperimental%2520evaluations%2520on%2520two%2520publicly%2520available%2520hyperspectral%2520datasets%250Ademonstrate%2520the%2520competitive%2520performance%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fusion-Guided%20Inception%20Network%20for%20Hyperspectral%20Image%0A%20%20Super-Resolution&entry.906535625=Usman%20Muhammad%20and%20Jorma%20Laaksonen&entry.1292438233=%20%20The%20fusion%20of%20low-spatial-resolution%20hyperspectral%20images%20%28HSIs%29%20with%0Ahigh-spatial-resolution%20conventional%20images%20%28e.g.%2C%20panchromatic%20or%20RGB%29%20has%0Aplayed%20a%20significant%20role%20in%20recent%20advancements%20in%20HSI%20super-resolution.%0AHowever%2C%20this%20fusion%20process%20relies%20on%20the%20availability%20of%20precise%20alignment%0Abetween%20image%20pairs%2C%20which%20is%20often%20challenging%20in%20real-world%20scenarios.%20To%0Amitigate%20this%20limitation%2C%20we%20propose%20a%20single-image%20super-resolution%20model%0Acalled%20the%20Fusion-Guided%20Inception%20Network%20%28FGIN%29.%20Specifically%2C%20we%20first%0Aemploy%20a%20spectral-spatial%20fusion%20module%20to%20effectively%20integrate%20spectral%20and%0Aspatial%20information%20at%20an%20early%20stage.%20Next%2C%20an%20Inception-like%20hierarchical%0Afeature%20extraction%20strategy%20is%20used%20to%20capture%20multiscale%20spatial%20dependencies%2C%0Afollowed%20by%20a%20dedicated%20multi-scale%20fusion%20block.%20To%20further%20enhance%0Areconstruction%20quality%2C%20we%20incorporate%20an%20optimized%20upsampling%20module%20that%0Acombines%20bilinear%20interpolation%20with%20depthwise%20separable%20convolutions.%0AExperimental%20evaluations%20on%20two%20publicly%20available%20hyperspectral%20datasets%0Ademonstrate%20the%20competitive%20performance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03431v1&entry.124074799=Read"},
{"title": "Adversarial Robustness of Deep Learning Models for Inland Water Body\n  Segmentation from SAR Images", "author": "Siddharth Kothari and Srinivasan Murali and Sankalp Kothari and Ujjwal Verma and Jaya Sreevalsan-Nair", "abstract": "  Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (GitHub link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)\n", "link": "http://arxiv.org/abs/2505.01884v2", "date": "2025-05-06", "relevancy": 2.1373, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5645}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5225}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Robustness%20of%20Deep%20Learning%20Models%20for%20Inland%20Water%20Body%0A%20%20Segmentation%20from%20SAR%20Images&body=Title%3A%20Adversarial%20Robustness%20of%20Deep%20Learning%20Models%20for%20Inland%20Water%20Body%0A%20%20Segmentation%20from%20SAR%20Images%0AAuthor%3A%20Siddharth%20Kothari%20and%20Srinivasan%20Murali%20and%20Sankalp%20Kothari%20and%20Ujjwal%20Verma%20and%20Jaya%20Sreevalsan-Nair%0AAbstract%3A%20%20%20Inland%20water%20body%20segmentation%20from%20Synthetic%20Aperture%20Radar%20%28SAR%29%20images%20is%0Aan%20important%20task%20needed%20for%20several%20applications%2C%20such%20as%20flood%20mapping.%20While%0ASAR%20sensors%20capture%20data%20in%20all-weather%20conditions%20as%20high-resolution%20images%2C%0Adifferentiating%20water%20and%20water-like%20surfaces%20from%20SAR%20images%20is%20not%0Astraightforward.%20Inland%20water%20bodies%2C%20such%20as%20large%20river%20basins%2C%20have%20complex%0Ageometry%2C%20which%20adds%20to%20the%20challenge%20of%20segmentation.%20U-Net%20is%20a%20widely%20used%0Adeep%20learning%20model%20for%20land-water%20segmentation%20of%20SAR%20images.%20In%20practice%2C%0Amanual%20annotation%20is%20often%20used%20to%20generate%20the%20corresponding%20water%20masks%20as%0Aground%20truth.%20Manual%20annotation%20of%20the%20images%20is%20prone%20to%20label%20noise%20owing%20to%0Adata%20poisoning%20attacks%2C%20especially%20due%20to%20complex%20geometry.%20In%20this%20work%2C%20we%0Asimulate%20manual%20errors%20in%20the%20form%20of%20adversarial%20attacks%20on%20the%20U-Net%20model%0Aand%20study%20the%20robustness%20of%20the%20model%20to%20human%20errors%20in%20annotation.%20Our%0Aresults%20indicate%20that%20U-Net%20can%20tolerate%20a%20certain%20level%20of%20corruption%20before%0Aits%20performance%20drops%20significantly.%20This%20finding%20highlights%20the%20crucial%20role%0Athat%20the%20quality%20of%20manual%20annotations%20plays%20in%20determining%20the%20effectiveness%0Aof%20the%20segmentation%20model.%20The%20code%20and%20the%20new%20dataset%2C%20along%20with%20adversarial%0Aexamples%20for%20robust%20training%2C%20are%20publicly%20available.%20%28GitHub%20link%20-%0Ahttps%3A//github.com/GVCL/IWSeg-SAR-Poison.git%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Robustness%2520of%2520Deep%2520Learning%2520Models%2520for%2520Inland%2520Water%2520Body%250A%2520%2520Segmentation%2520from%2520SAR%2520Images%26entry.906535625%3DSiddharth%2520Kothari%2520and%2520Srinivasan%2520Murali%2520and%2520Sankalp%2520Kothari%2520and%2520Ujjwal%2520Verma%2520and%2520Jaya%2520Sreevalsan-Nair%26entry.1292438233%3D%2520%2520Inland%2520water%2520body%2520segmentation%2520from%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520images%2520is%250Aan%2520important%2520task%2520needed%2520for%2520several%2520applications%252C%2520such%2520as%2520flood%2520mapping.%2520While%250ASAR%2520sensors%2520capture%2520data%2520in%2520all-weather%2520conditions%2520as%2520high-resolution%2520images%252C%250Adifferentiating%2520water%2520and%2520water-like%2520surfaces%2520from%2520SAR%2520images%2520is%2520not%250Astraightforward.%2520Inland%2520water%2520bodies%252C%2520such%2520as%2520large%2520river%2520basins%252C%2520have%2520complex%250Ageometry%252C%2520which%2520adds%2520to%2520the%2520challenge%2520of%2520segmentation.%2520U-Net%2520is%2520a%2520widely%2520used%250Adeep%2520learning%2520model%2520for%2520land-water%2520segmentation%2520of%2520SAR%2520images.%2520In%2520practice%252C%250Amanual%2520annotation%2520is%2520often%2520used%2520to%2520generate%2520the%2520corresponding%2520water%2520masks%2520as%250Aground%2520truth.%2520Manual%2520annotation%2520of%2520the%2520images%2520is%2520prone%2520to%2520label%2520noise%2520owing%2520to%250Adata%2520poisoning%2520attacks%252C%2520especially%2520due%2520to%2520complex%2520geometry.%2520In%2520this%2520work%252C%2520we%250Asimulate%2520manual%2520errors%2520in%2520the%2520form%2520of%2520adversarial%2520attacks%2520on%2520the%2520U-Net%2520model%250Aand%2520study%2520the%2520robustness%2520of%2520the%2520model%2520to%2520human%2520errors%2520in%2520annotation.%2520Our%250Aresults%2520indicate%2520that%2520U-Net%2520can%2520tolerate%2520a%2520certain%2520level%2520of%2520corruption%2520before%250Aits%2520performance%2520drops%2520significantly.%2520This%2520finding%2520highlights%2520the%2520crucial%2520role%250Athat%2520the%2520quality%2520of%2520manual%2520annotations%2520plays%2520in%2520determining%2520the%2520effectiveness%250Aof%2520the%2520segmentation%2520model.%2520The%2520code%2520and%2520the%2520new%2520dataset%252C%2520along%2520with%2520adversarial%250Aexamples%2520for%2520robust%2520training%252C%2520are%2520publicly%2520available.%2520%2528GitHub%2520link%2520-%250Ahttps%253A//github.com/GVCL/IWSeg-SAR-Poison.git%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustness%20of%20Deep%20Learning%20Models%20for%20Inland%20Water%20Body%0A%20%20Segmentation%20from%20SAR%20Images&entry.906535625=Siddharth%20Kothari%20and%20Srinivasan%20Murali%20and%20Sankalp%20Kothari%20and%20Ujjwal%20Verma%20and%20Jaya%20Sreevalsan-Nair&entry.1292438233=%20%20Inland%20water%20body%20segmentation%20from%20Synthetic%20Aperture%20Radar%20%28SAR%29%20images%20is%0Aan%20important%20task%20needed%20for%20several%20applications%2C%20such%20as%20flood%20mapping.%20While%0ASAR%20sensors%20capture%20data%20in%20all-weather%20conditions%20as%20high-resolution%20images%2C%0Adifferentiating%20water%20and%20water-like%20surfaces%20from%20SAR%20images%20is%20not%0Astraightforward.%20Inland%20water%20bodies%2C%20such%20as%20large%20river%20basins%2C%20have%20complex%0Ageometry%2C%20which%20adds%20to%20the%20challenge%20of%20segmentation.%20U-Net%20is%20a%20widely%20used%0Adeep%20learning%20model%20for%20land-water%20segmentation%20of%20SAR%20images.%20In%20practice%2C%0Amanual%20annotation%20is%20often%20used%20to%20generate%20the%20corresponding%20water%20masks%20as%0Aground%20truth.%20Manual%20annotation%20of%20the%20images%20is%20prone%20to%20label%20noise%20owing%20to%0Adata%20poisoning%20attacks%2C%20especially%20due%20to%20complex%20geometry.%20In%20this%20work%2C%20we%0Asimulate%20manual%20errors%20in%20the%20form%20of%20adversarial%20attacks%20on%20the%20U-Net%20model%0Aand%20study%20the%20robustness%20of%20the%20model%20to%20human%20errors%20in%20annotation.%20Our%0Aresults%20indicate%20that%20U-Net%20can%20tolerate%20a%20certain%20level%20of%20corruption%20before%0Aits%20performance%20drops%20significantly.%20This%20finding%20highlights%20the%20crucial%20role%0Athat%20the%20quality%20of%20manual%20annotations%20plays%20in%20determining%20the%20effectiveness%0Aof%20the%20segmentation%20model.%20The%20code%20and%20the%20new%20dataset%2C%20along%20with%20adversarial%0Aexamples%20for%20robust%20training%2C%20are%20publicly%20available.%20%28GitHub%20link%20-%0Ahttps%3A//github.com/GVCL/IWSeg-SAR-Poison.git%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01884v2&entry.124074799=Read"},
{"title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis", "author": "Zhiyuan Li and Wenshuai Zhao and Joni Pajarinen", "abstract": "  Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS's strong performance against\nstate-of-the-art MARL baselines across both symmetric and asymmetric scenarios.\nNotably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate,\nrepresenting a 30 percentage point advantage over QMIX (27\\%). Project page can\nbe found at https://stellar-entremet-1720bb.netlify.app/.\n", "link": "http://arxiv.org/abs/2502.10148v2", "date": "2025-05-06", "relevancy": 2.1294, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5616}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5293}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Multi-Agent%20Planning%20with%20Adaptive%20Skill%20Synthesis&body=Title%3A%20Cooperative%20Multi-Agent%20Planning%20with%20Adaptive%20Skill%20Synthesis%0AAuthor%3A%20Zhiyuan%20Li%20and%20Wenshuai%20Zhao%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Despite%20much%20progress%20in%20training%20distributed%20artificial%20intelligence%20%28AI%29%2C%0Abuilding%20cooperative%20multi-agent%20systems%20with%20multi-agent%20reinforcement%0Alearning%20%28MARL%29%20faces%20challenges%20in%20sample%20efficiency%2C%20interpretability%2C%20and%0Atransferability.%20Unlike%20traditional%20learning-based%20methods%20that%20require%0Aextensive%20interaction%20with%20the%20environment%2C%20large%20language%20models%20%28LLMs%29%0Ademonstrate%20remarkable%20capabilities%20in%20zero-shot%20planning%20and%20complex%0Areasoning.%20However%2C%20existing%20LLM-based%20approaches%20heavily%20rely%20on%20text-based%0Aobservations%20and%20struggle%20with%20the%20non-Markovian%20nature%20of%20multi-agent%0Ainteractions%20under%20partial%20observability.%20We%20present%20COMPASS%2C%20a%20novel%0Amulti-agent%20architecture%20that%20integrates%20vision-language%20models%20%28VLMs%29%20with%20a%0Adynamic%20skill%20library%20and%20structured%20communication%20for%20decentralized%0Aclosed-loop%20decision-making.%20The%20skill%20library%2C%20bootstrapped%20from%0Ademonstrations%2C%20evolves%20via%20planner-guided%20tasks%20to%20enable%20adaptive%20strategies.%0ACOMPASS%20propagates%20entity%20information%20through%20multi-hop%20communication%20under%0Apartial%20observability.%20Evaluations%20on%20the%20improved%20StarCraft%20Multi-Agent%0AChallenge%20%28SMACv2%29%20demonstrate%20COMPASS%27s%20strong%20performance%20against%0Astate-of-the-art%20MARL%20baselines%20across%20both%20symmetric%20and%20asymmetric%20scenarios.%0ANotably%2C%20in%20the%20symmetric%20Protoss%205v5%20task%2C%20COMPASS%20achieved%20a%2057%5C%25%20win%20rate%2C%0Arepresenting%20a%2030%20percentage%20point%20advantage%20over%20QMIX%20%2827%5C%25%29.%20Project%20page%20can%0Abe%20found%20at%20https%3A//stellar-entremet-1720bb.netlify.app/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10148v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Multi-Agent%2520Planning%2520with%2520Adaptive%2520Skill%2520Synthesis%26entry.906535625%3DZhiyuan%2520Li%2520and%2520Wenshuai%2520Zhao%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Despite%2520much%2520progress%2520in%2520training%2520distributed%2520artificial%2520intelligence%2520%2528AI%2529%252C%250Abuilding%2520cooperative%2520multi-agent%2520systems%2520with%2520multi-agent%2520reinforcement%250Alearning%2520%2528MARL%2529%2520faces%2520challenges%2520in%2520sample%2520efficiency%252C%2520interpretability%252C%2520and%250Atransferability.%2520Unlike%2520traditional%2520learning-based%2520methods%2520that%2520require%250Aextensive%2520interaction%2520with%2520the%2520environment%252C%2520large%2520language%2520models%2520%2528LLMs%2529%250Ademonstrate%2520remarkable%2520capabilities%2520in%2520zero-shot%2520planning%2520and%2520complex%250Areasoning.%2520However%252C%2520existing%2520LLM-based%2520approaches%2520heavily%2520rely%2520on%2520text-based%250Aobservations%2520and%2520struggle%2520with%2520the%2520non-Markovian%2520nature%2520of%2520multi-agent%250Ainteractions%2520under%2520partial%2520observability.%2520We%2520present%2520COMPASS%252C%2520a%2520novel%250Amulti-agent%2520architecture%2520that%2520integrates%2520vision-language%2520models%2520%2528VLMs%2529%2520with%2520a%250Adynamic%2520skill%2520library%2520and%2520structured%2520communication%2520for%2520decentralized%250Aclosed-loop%2520decision-making.%2520The%2520skill%2520library%252C%2520bootstrapped%2520from%250Ademonstrations%252C%2520evolves%2520via%2520planner-guided%2520tasks%2520to%2520enable%2520adaptive%2520strategies.%250ACOMPASS%2520propagates%2520entity%2520information%2520through%2520multi-hop%2520communication%2520under%250Apartial%2520observability.%2520Evaluations%2520on%2520the%2520improved%2520StarCraft%2520Multi-Agent%250AChallenge%2520%2528SMACv2%2529%2520demonstrate%2520COMPASS%2527s%2520strong%2520performance%2520against%250Astate-of-the-art%2520MARL%2520baselines%2520across%2520both%2520symmetric%2520and%2520asymmetric%2520scenarios.%250ANotably%252C%2520in%2520the%2520symmetric%2520Protoss%25205v5%2520task%252C%2520COMPASS%2520achieved%2520a%252057%255C%2525%2520win%2520rate%252C%250Arepresenting%2520a%252030%2520percentage%2520point%2520advantage%2520over%2520QMIX%2520%252827%255C%2525%2529.%2520Project%2520page%2520can%250Abe%2520found%2520at%2520https%253A//stellar-entremet-1720bb.netlify.app/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10148v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Multi-Agent%20Planning%20with%20Adaptive%20Skill%20Synthesis&entry.906535625=Zhiyuan%20Li%20and%20Wenshuai%20Zhao%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Despite%20much%20progress%20in%20training%20distributed%20artificial%20intelligence%20%28AI%29%2C%0Abuilding%20cooperative%20multi-agent%20systems%20with%20multi-agent%20reinforcement%0Alearning%20%28MARL%29%20faces%20challenges%20in%20sample%20efficiency%2C%20interpretability%2C%20and%0Atransferability.%20Unlike%20traditional%20learning-based%20methods%20that%20require%0Aextensive%20interaction%20with%20the%20environment%2C%20large%20language%20models%20%28LLMs%29%0Ademonstrate%20remarkable%20capabilities%20in%20zero-shot%20planning%20and%20complex%0Areasoning.%20However%2C%20existing%20LLM-based%20approaches%20heavily%20rely%20on%20text-based%0Aobservations%20and%20struggle%20with%20the%20non-Markovian%20nature%20of%20multi-agent%0Ainteractions%20under%20partial%20observability.%20We%20present%20COMPASS%2C%20a%20novel%0Amulti-agent%20architecture%20that%20integrates%20vision-language%20models%20%28VLMs%29%20with%20a%0Adynamic%20skill%20library%20and%20structured%20communication%20for%20decentralized%0Aclosed-loop%20decision-making.%20The%20skill%20library%2C%20bootstrapped%20from%0Ademonstrations%2C%20evolves%20via%20planner-guided%20tasks%20to%20enable%20adaptive%20strategies.%0ACOMPASS%20propagates%20entity%20information%20through%20multi-hop%20communication%20under%0Apartial%20observability.%20Evaluations%20on%20the%20improved%20StarCraft%20Multi-Agent%0AChallenge%20%28SMACv2%29%20demonstrate%20COMPASS%27s%20strong%20performance%20against%0Astate-of-the-art%20MARL%20baselines%20across%20both%20symmetric%20and%20asymmetric%20scenarios.%0ANotably%2C%20in%20the%20symmetric%20Protoss%205v5%20task%2C%20COMPASS%20achieved%20a%2057%5C%25%20win%20rate%2C%0Arepresenting%20a%2030%20percentage%20point%20advantage%20over%20QMIX%20%2827%5C%25%29.%20Project%20page%20can%0Abe%20found%20at%20https%3A//stellar-entremet-1720bb.netlify.app/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10148v2&entry.124074799=Read"},
{"title": "Corner Cases: How Size and Position of Objects Challenge\n  ImageNet-Trained Models", "author": "Mishal Fatima and Steffen Jung and Margret Keuper", "abstract": "  Backgrounds in images play a major role in contributing to spurious\ncorrelations among different data points. Owing to aesthetic preferences of\nhumans capturing the images, datasets can exhibit positional (location of the\nobject within a given frame) and size (region-of-interest to image ratio)\nbiases for different classes. In this paper, we show that these biases can\nimpact how much a model relies on spurious features in the background to make\nits predictions. To better illustrate our findings, we propose a synthetic\ndataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images\nwith various backgrounds, object positions, and object sizes. By evaluating the\ndataset on different pretrained models, we find that most models rely heavily\non spurious features in the background when the region-of-interest (ROI) to\nimage ratio is small and the object is far from the center of the image.\nMoreover, we also show that current methods that aim to mitigate harmful\nspurious features, do not take into account these factors, hence fail to\nachieve considerable performance gains for worst-group accuracies when the size\nand location of core features in an image change.\n", "link": "http://arxiv.org/abs/2505.03569v1", "date": "2025-05-06", "relevancy": 2.1252, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5658}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Corner%20Cases%3A%20How%20Size%20and%20Position%20of%20Objects%20Challenge%0A%20%20ImageNet-Trained%20Models&body=Title%3A%20Corner%20Cases%3A%20How%20Size%20and%20Position%20of%20Objects%20Challenge%0A%20%20ImageNet-Trained%20Models%0AAuthor%3A%20Mishal%20Fatima%20and%20Steffen%20Jung%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Backgrounds%20in%20images%20play%20a%20major%20role%20in%20contributing%20to%20spurious%0Acorrelations%20among%20different%20data%20points.%20Owing%20to%20aesthetic%20preferences%20of%0Ahumans%20capturing%20the%20images%2C%20datasets%20can%20exhibit%20positional%20%28location%20of%20the%0Aobject%20within%20a%20given%20frame%29%20and%20size%20%28region-of-interest%20to%20image%20ratio%29%0Abiases%20for%20different%20classes.%20In%20this%20paper%2C%20we%20show%20that%20these%20biases%20can%0Aimpact%20how%20much%20a%20model%20relies%20on%20spurious%20features%20in%20the%20background%20to%20make%0Aits%20predictions.%20To%20better%20illustrate%20our%20findings%2C%20we%20propose%20a%20synthetic%0Adataset%20derived%20from%20ImageNet1k%2C%20Hard-Spurious-ImageNet%2C%20which%20contains%20images%0Awith%20various%20backgrounds%2C%20object%20positions%2C%20and%20object%20sizes.%20By%20evaluating%20the%0Adataset%20on%20different%20pretrained%20models%2C%20we%20find%20that%20most%20models%20rely%20heavily%0Aon%20spurious%20features%20in%20the%20background%20when%20the%20region-of-interest%20%28ROI%29%20to%0Aimage%20ratio%20is%20small%20and%20the%20object%20is%20far%20from%20the%20center%20of%20the%20image.%0AMoreover%2C%20we%20also%20show%20that%20current%20methods%20that%20aim%20to%20mitigate%20harmful%0Aspurious%20features%2C%20do%20not%20take%20into%20account%20these%20factors%2C%20hence%20fail%20to%0Aachieve%20considerable%20performance%20gains%20for%20worst-group%20accuracies%20when%20the%20size%0Aand%20location%20of%20core%20features%20in%20an%20image%20change.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorner%2520Cases%253A%2520How%2520Size%2520and%2520Position%2520of%2520Objects%2520Challenge%250A%2520%2520ImageNet-Trained%2520Models%26entry.906535625%3DMishal%2520Fatima%2520and%2520Steffen%2520Jung%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Backgrounds%2520in%2520images%2520play%2520a%2520major%2520role%2520in%2520contributing%2520to%2520spurious%250Acorrelations%2520among%2520different%2520data%2520points.%2520Owing%2520to%2520aesthetic%2520preferences%2520of%250Ahumans%2520capturing%2520the%2520images%252C%2520datasets%2520can%2520exhibit%2520positional%2520%2528location%2520of%2520the%250Aobject%2520within%2520a%2520given%2520frame%2529%2520and%2520size%2520%2528region-of-interest%2520to%2520image%2520ratio%2529%250Abiases%2520for%2520different%2520classes.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520these%2520biases%2520can%250Aimpact%2520how%2520much%2520a%2520model%2520relies%2520on%2520spurious%2520features%2520in%2520the%2520background%2520to%2520make%250Aits%2520predictions.%2520To%2520better%2520illustrate%2520our%2520findings%252C%2520we%2520propose%2520a%2520synthetic%250Adataset%2520derived%2520from%2520ImageNet1k%252C%2520Hard-Spurious-ImageNet%252C%2520which%2520contains%2520images%250Awith%2520various%2520backgrounds%252C%2520object%2520positions%252C%2520and%2520object%2520sizes.%2520By%2520evaluating%2520the%250Adataset%2520on%2520different%2520pretrained%2520models%252C%2520we%2520find%2520that%2520most%2520models%2520rely%2520heavily%250Aon%2520spurious%2520features%2520in%2520the%2520background%2520when%2520the%2520region-of-interest%2520%2528ROI%2529%2520to%250Aimage%2520ratio%2520is%2520small%2520and%2520the%2520object%2520is%2520far%2520from%2520the%2520center%2520of%2520the%2520image.%250AMoreover%252C%2520we%2520also%2520show%2520that%2520current%2520methods%2520that%2520aim%2520to%2520mitigate%2520harmful%250Aspurious%2520features%252C%2520do%2520not%2520take%2520into%2520account%2520these%2520factors%252C%2520hence%2520fail%2520to%250Aachieve%2520considerable%2520performance%2520gains%2520for%2520worst-group%2520accuracies%2520when%2520the%2520size%250Aand%2520location%2520of%2520core%2520features%2520in%2520an%2520image%2520change.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Corner%20Cases%3A%20How%20Size%20and%20Position%20of%20Objects%20Challenge%0A%20%20ImageNet-Trained%20Models&entry.906535625=Mishal%20Fatima%20and%20Steffen%20Jung%20and%20Margret%20Keuper&entry.1292438233=%20%20Backgrounds%20in%20images%20play%20a%20major%20role%20in%20contributing%20to%20spurious%0Acorrelations%20among%20different%20data%20points.%20Owing%20to%20aesthetic%20preferences%20of%0Ahumans%20capturing%20the%20images%2C%20datasets%20can%20exhibit%20positional%20%28location%20of%20the%0Aobject%20within%20a%20given%20frame%29%20and%20size%20%28region-of-interest%20to%20image%20ratio%29%0Abiases%20for%20different%20classes.%20In%20this%20paper%2C%20we%20show%20that%20these%20biases%20can%0Aimpact%20how%20much%20a%20model%20relies%20on%20spurious%20features%20in%20the%20background%20to%20make%0Aits%20predictions.%20To%20better%20illustrate%20our%20findings%2C%20we%20propose%20a%20synthetic%0Adataset%20derived%20from%20ImageNet1k%2C%20Hard-Spurious-ImageNet%2C%20which%20contains%20images%0Awith%20various%20backgrounds%2C%20object%20positions%2C%20and%20object%20sizes.%20By%20evaluating%20the%0Adataset%20on%20different%20pretrained%20models%2C%20we%20find%20that%20most%20models%20rely%20heavily%0Aon%20spurious%20features%20in%20the%20background%20when%20the%20region-of-interest%20%28ROI%29%20to%0Aimage%20ratio%20is%20small%20and%20the%20object%20is%20far%20from%20the%20center%20of%20the%20image.%0AMoreover%2C%20we%20also%20show%20that%20current%20methods%20that%20aim%20to%20mitigate%20harmful%0Aspurious%20features%2C%20do%20not%20take%20into%20account%20these%20factors%2C%20hence%20fail%20to%0Aachieve%20considerable%20performance%20gains%20for%20worst-group%20accuracies%20when%20the%20size%0Aand%20location%20of%20core%20features%20in%20an%20image%20change.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03569v1&entry.124074799=Read"},
{"title": "A Unified Framework for Exploratory Learning-Aided Community Detection\n  Under Topological Uncertainty", "author": "Yu Hou and Cong Tran and Ming Li and Won-Yong Shin", "abstract": "  In social networks, the discovery of community structures has received\nconsiderable attention as a fundamental problem in various network analysis\ntasks. However, due to privacy concerns or access restrictions, the network\nstructure is often uncertain, thereby rendering established community detection\napproaches ineffective without costly network topology acquisition. To tackle\nthis challenge, we present META-CODE, a unified framework for detecting\noverlapping communities via exploratory learning aided by easy-to-collect node\nmetadata when networks are topologically unknown (or only partially known).\nSpecifically, META-CODE consists of three iterative steps in addition to the\ninitial network inference step: 1) node-level community-affiliation embeddings\nbased on graph neural networks (GNNs) trained by our new reconstruction loss,\n2) network exploration via community-affiliation-based node queries, and 3)\nnetwork inference using an edge connectivity-based Siamese neural network model\nfrom the explored network. Through extensive experiments on three real-world\ndatasets including two large networks, we demonstrate: (a) the superiority of\nMETA-CODE over benchmark community detection methods, achieving remarkable\ngains up to 65.55% on the Facebook dataset over the best competitor among our\nselected competitive methods in terms of normalized mutual information (NMI),\n(b) the impact of each module in META-CODE, (c) the effectiveness of node\nqueries in META-CODE based on empirical evaluations and theoretical findings,\nand (d) the convergence of the inferred network.\n", "link": "http://arxiv.org/abs/2304.04497v4", "date": "2025-05-06", "relevancy": 2.12, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Exploratory%20Learning-Aided%20Community%20Detection%0A%20%20Under%20Topological%20Uncertainty&body=Title%3A%20A%20Unified%20Framework%20for%20Exploratory%20Learning-Aided%20Community%20Detection%0A%20%20Under%20Topological%20Uncertainty%0AAuthor%3A%20Yu%20Hou%20and%20Cong%20Tran%20and%20Ming%20Li%20and%20Won-Yong%20Shin%0AAbstract%3A%20%20%20In%20social%20networks%2C%20the%20discovery%20of%20community%20structures%20has%20received%0Aconsiderable%20attention%20as%20a%20fundamental%20problem%20in%20various%20network%20analysis%0Atasks.%20However%2C%20due%20to%20privacy%20concerns%20or%20access%20restrictions%2C%20the%20network%0Astructure%20is%20often%20uncertain%2C%20thereby%20rendering%20established%20community%20detection%0Aapproaches%20ineffective%20without%20costly%20network%20topology%20acquisition.%20To%20tackle%0Athis%20challenge%2C%20we%20present%20META-CODE%2C%20a%20unified%20framework%20for%20detecting%0Aoverlapping%20communities%20via%20exploratory%20learning%20aided%20by%20easy-to-collect%20node%0Ametadata%20when%20networks%20are%20topologically%20unknown%20%28or%20only%20partially%20known%29.%0ASpecifically%2C%20META-CODE%20consists%20of%20three%20iterative%20steps%20in%20addition%20to%20the%0Ainitial%20network%20inference%20step%3A%201%29%20node-level%20community-affiliation%20embeddings%0Abased%20on%20graph%20neural%20networks%20%28GNNs%29%20trained%20by%20our%20new%20reconstruction%20loss%2C%0A2%29%20network%20exploration%20via%20community-affiliation-based%20node%20queries%2C%20and%203%29%0Anetwork%20inference%20using%20an%20edge%20connectivity-based%20Siamese%20neural%20network%20model%0Afrom%20the%20explored%20network.%20Through%20extensive%20experiments%20on%20three%20real-world%0Adatasets%20including%20two%20large%20networks%2C%20we%20demonstrate%3A%20%28a%29%20the%20superiority%20of%0AMETA-CODE%20over%20benchmark%20community%20detection%20methods%2C%20achieving%20remarkable%0Agains%20up%20to%2065.55%25%20on%20the%20Facebook%20dataset%20over%20the%20best%20competitor%20among%20our%0Aselected%20competitive%20methods%20in%20terms%20of%20normalized%20mutual%20information%20%28NMI%29%2C%0A%28b%29%20the%20impact%20of%20each%20module%20in%20META-CODE%2C%20%28c%29%20the%20effectiveness%20of%20node%0Aqueries%20in%20META-CODE%20based%20on%20empirical%20evaluations%20and%20theoretical%20findings%2C%0Aand%20%28d%29%20the%20convergence%20of%20the%20inferred%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.04497v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%2520Exploratory%2520Learning-Aided%2520Community%2520Detection%250A%2520%2520Under%2520Topological%2520Uncertainty%26entry.906535625%3DYu%2520Hou%2520and%2520Cong%2520Tran%2520and%2520Ming%2520Li%2520and%2520Won-Yong%2520Shin%26entry.1292438233%3D%2520%2520In%2520social%2520networks%252C%2520the%2520discovery%2520of%2520community%2520structures%2520has%2520received%250Aconsiderable%2520attention%2520as%2520a%2520fundamental%2520problem%2520in%2520various%2520network%2520analysis%250Atasks.%2520However%252C%2520due%2520to%2520privacy%2520concerns%2520or%2520access%2520restrictions%252C%2520the%2520network%250Astructure%2520is%2520often%2520uncertain%252C%2520thereby%2520rendering%2520established%2520community%2520detection%250Aapproaches%2520ineffective%2520without%2520costly%2520network%2520topology%2520acquisition.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520present%2520META-CODE%252C%2520a%2520unified%2520framework%2520for%2520detecting%250Aoverlapping%2520communities%2520via%2520exploratory%2520learning%2520aided%2520by%2520easy-to-collect%2520node%250Ametadata%2520when%2520networks%2520are%2520topologically%2520unknown%2520%2528or%2520only%2520partially%2520known%2529.%250ASpecifically%252C%2520META-CODE%2520consists%2520of%2520three%2520iterative%2520steps%2520in%2520addition%2520to%2520the%250Ainitial%2520network%2520inference%2520step%253A%25201%2529%2520node-level%2520community-affiliation%2520embeddings%250Abased%2520on%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520trained%2520by%2520our%2520new%2520reconstruction%2520loss%252C%250A2%2529%2520network%2520exploration%2520via%2520community-affiliation-based%2520node%2520queries%252C%2520and%25203%2529%250Anetwork%2520inference%2520using%2520an%2520edge%2520connectivity-based%2520Siamese%2520neural%2520network%2520model%250Afrom%2520the%2520explored%2520network.%2520Through%2520extensive%2520experiments%2520on%2520three%2520real-world%250Adatasets%2520including%2520two%2520large%2520networks%252C%2520we%2520demonstrate%253A%2520%2528a%2529%2520the%2520superiority%2520of%250AMETA-CODE%2520over%2520benchmark%2520community%2520detection%2520methods%252C%2520achieving%2520remarkable%250Agains%2520up%2520to%252065.55%2525%2520on%2520the%2520Facebook%2520dataset%2520over%2520the%2520best%2520competitor%2520among%2520our%250Aselected%2520competitive%2520methods%2520in%2520terms%2520of%2520normalized%2520mutual%2520information%2520%2528NMI%2529%252C%250A%2528b%2529%2520the%2520impact%2520of%2520each%2520module%2520in%2520META-CODE%252C%2520%2528c%2529%2520the%2520effectiveness%2520of%2520node%250Aqueries%2520in%2520META-CODE%2520based%2520on%2520empirical%2520evaluations%2520and%2520theoretical%2520findings%252C%250Aand%2520%2528d%2529%2520the%2520convergence%2520of%2520the%2520inferred%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.04497v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Exploratory%20Learning-Aided%20Community%20Detection%0A%20%20Under%20Topological%20Uncertainty&entry.906535625=Yu%20Hou%20and%20Cong%20Tran%20and%20Ming%20Li%20and%20Won-Yong%20Shin&entry.1292438233=%20%20In%20social%20networks%2C%20the%20discovery%20of%20community%20structures%20has%20received%0Aconsiderable%20attention%20as%20a%20fundamental%20problem%20in%20various%20network%20analysis%0Atasks.%20However%2C%20due%20to%20privacy%20concerns%20or%20access%20restrictions%2C%20the%20network%0Astructure%20is%20often%20uncertain%2C%20thereby%20rendering%20established%20community%20detection%0Aapproaches%20ineffective%20without%20costly%20network%20topology%20acquisition.%20To%20tackle%0Athis%20challenge%2C%20we%20present%20META-CODE%2C%20a%20unified%20framework%20for%20detecting%0Aoverlapping%20communities%20via%20exploratory%20learning%20aided%20by%20easy-to-collect%20node%0Ametadata%20when%20networks%20are%20topologically%20unknown%20%28or%20only%20partially%20known%29.%0ASpecifically%2C%20META-CODE%20consists%20of%20three%20iterative%20steps%20in%20addition%20to%20the%0Ainitial%20network%20inference%20step%3A%201%29%20node-level%20community-affiliation%20embeddings%0Abased%20on%20graph%20neural%20networks%20%28GNNs%29%20trained%20by%20our%20new%20reconstruction%20loss%2C%0A2%29%20network%20exploration%20via%20community-affiliation-based%20node%20queries%2C%20and%203%29%0Anetwork%20inference%20using%20an%20edge%20connectivity-based%20Siamese%20neural%20network%20model%0Afrom%20the%20explored%20network.%20Through%20extensive%20experiments%20on%20three%20real-world%0Adatasets%20including%20two%20large%20networks%2C%20we%20demonstrate%3A%20%28a%29%20the%20superiority%20of%0AMETA-CODE%20over%20benchmark%20community%20detection%20methods%2C%20achieving%20remarkable%0Agains%20up%20to%2065.55%25%20on%20the%20Facebook%20dataset%20over%20the%20best%20competitor%20among%20our%0Aselected%20competitive%20methods%20in%20terms%20of%20normalized%20mutual%20information%20%28NMI%29%2C%0A%28b%29%20the%20impact%20of%20each%20module%20in%20META-CODE%2C%20%28c%29%20the%20effectiveness%20of%20node%0Aqueries%20in%20META-CODE%20based%20on%20empirical%20evaluations%20and%20theoretical%20findings%2C%0Aand%20%28d%29%20the%20convergence%20of%20the%20inferred%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.04497v4&entry.124074799=Read"},
{"title": "AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised\n  and Active Learning", "author": "Pablo G\u00f3mez and David O'Ryan", "abstract": "  Anomaly detection in large datasets is essential in fields such as astronomy\nand computer vision; however, supervised methods typically require extensive\nanomaly labelling, which is often impractical. We present AnomalyMatch, an\nanomaly detection framework combining the semi-supervised FixMatch algorithm\nusing EfficientNet classifiers with active learning. By treating anomaly\ndetection as a semi-supervised binary classification problem, we efficiently\nutilise limited labelled and abundant unlabelled images. We allow iterative\nmodel refinement in a user interface for expert verification of high-confidence\nanomalies and correction of false positives. Built for astronomical data,\nAnomalyMatch generalises readily to other domains facing similar data\nchallenges. Evaluations on the GalaxyMNIST astronomical dataset and the\nminiImageNet natural-image benchmark under severe class imbalance (1% anomalies\nfor miniImageNet) display strong performance: starting from five to ten\nlabelled anomalies and after three active learning cycles, we achieve an\naverage AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective\nAUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with\n71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.\nAnomalyMatch is tailored for large-scale applications, efficiently processing\npredictions for 100 million images within three days on a single GPU.\nIntegrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted\ndiscovery of scientifically valuable anomalies in vast astronomical datasets.\nOur results underscore the exceptional utility and scalability of this approach\nfor anomaly discovery, highlighting the value of specialised approaches for\ndomains characterised by severe label scarcity.\n", "link": "http://arxiv.org/abs/2505.03509v1", "date": "2025-05-06", "relevancy": 2.1151, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.544}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.527}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnomalyMatch%3A%20Discovering%20Rare%20Objects%20of%20Interest%20with%20Semi-supervised%0A%20%20and%20Active%20Learning&body=Title%3A%20AnomalyMatch%3A%20Discovering%20Rare%20Objects%20of%20Interest%20with%20Semi-supervised%0A%20%20and%20Active%20Learning%0AAuthor%3A%20Pablo%20G%C3%B3mez%20and%20David%20O%27Ryan%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20large%20datasets%20is%20essential%20in%20fields%20such%20as%20astronomy%0Aand%20computer%20vision%3B%20however%2C%20supervised%20methods%20typically%20require%20extensive%0Aanomaly%20labelling%2C%20which%20is%20often%20impractical.%20We%20present%20AnomalyMatch%2C%20an%0Aanomaly%20detection%20framework%20combining%20the%20semi-supervised%20FixMatch%20algorithm%0Ausing%20EfficientNet%20classifiers%20with%20active%20learning.%20By%20treating%20anomaly%0Adetection%20as%20a%20semi-supervised%20binary%20classification%20problem%2C%20we%20efficiently%0Autilise%20limited%20labelled%20and%20abundant%20unlabelled%20images.%20We%20allow%20iterative%0Amodel%20refinement%20in%20a%20user%20interface%20for%20expert%20verification%20of%20high-confidence%0Aanomalies%20and%20correction%20of%20false%20positives.%20Built%20for%20astronomical%20data%2C%0AAnomalyMatch%20generalises%20readily%20to%20other%20domains%20facing%20similar%20data%0Achallenges.%20Evaluations%20on%20the%20GalaxyMNIST%20astronomical%20dataset%20and%20the%0AminiImageNet%20natural-image%20benchmark%20under%20severe%20class%20imbalance%20%281%25%20anomalies%0Afor%20miniImageNet%29%20display%20strong%20performance%3A%20starting%20from%20five%20to%20ten%0Alabelled%20anomalies%20and%20after%20three%20active%20learning%20cycles%2C%20we%20achieve%20an%0Aaverage%20AUROC%20of%200.95%20%28miniImageNet%29%20and%200.86%20%28GalaxyMNIST%29%2C%20with%20respective%0AAUPRC%20of%200.77%20and%200.71.%20After%20active%20learning%20cycles%2C%20anomalies%20are%20ranked%20with%0A71%25%20%28miniImageNet%29%20to%2093%25%20precision%20in%20the%201%25%20of%20the%20highest-ranked%20images.%0AAnomalyMatch%20is%20tailored%20for%20large-scale%20applications%2C%20efficiently%20processing%0Apredictions%20for%20100%20million%20images%20within%20three%20days%20on%20a%20single%20GPU.%0AIntegrated%20into%20ESAs%20Datalabs%20platform%2C%20AnomalyMatch%20facilitates%20targeted%0Adiscovery%20of%20scientifically%20valuable%20anomalies%20in%20vast%20astronomical%20datasets.%0AOur%20results%20underscore%20the%20exceptional%20utility%20and%20scalability%20of%20this%20approach%0Afor%20anomaly%20discovery%2C%20highlighting%20the%20value%20of%20specialised%20approaches%20for%0Adomains%20characterised%20by%20severe%20label%20scarcity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalyMatch%253A%2520Discovering%2520Rare%2520Objects%2520of%2520Interest%2520with%2520Semi-supervised%250A%2520%2520and%2520Active%2520Learning%26entry.906535625%3DPablo%2520G%25C3%25B3mez%2520and%2520David%2520O%2527Ryan%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520in%2520large%2520datasets%2520is%2520essential%2520in%2520fields%2520such%2520as%2520astronomy%250Aand%2520computer%2520vision%253B%2520however%252C%2520supervised%2520methods%2520typically%2520require%2520extensive%250Aanomaly%2520labelling%252C%2520which%2520is%2520often%2520impractical.%2520We%2520present%2520AnomalyMatch%252C%2520an%250Aanomaly%2520detection%2520framework%2520combining%2520the%2520semi-supervised%2520FixMatch%2520algorithm%250Ausing%2520EfficientNet%2520classifiers%2520with%2520active%2520learning.%2520By%2520treating%2520anomaly%250Adetection%2520as%2520a%2520semi-supervised%2520binary%2520classification%2520problem%252C%2520we%2520efficiently%250Autilise%2520limited%2520labelled%2520and%2520abundant%2520unlabelled%2520images.%2520We%2520allow%2520iterative%250Amodel%2520refinement%2520in%2520a%2520user%2520interface%2520for%2520expert%2520verification%2520of%2520high-confidence%250Aanomalies%2520and%2520correction%2520of%2520false%2520positives.%2520Built%2520for%2520astronomical%2520data%252C%250AAnomalyMatch%2520generalises%2520readily%2520to%2520other%2520domains%2520facing%2520similar%2520data%250Achallenges.%2520Evaluations%2520on%2520the%2520GalaxyMNIST%2520astronomical%2520dataset%2520and%2520the%250AminiImageNet%2520natural-image%2520benchmark%2520under%2520severe%2520class%2520imbalance%2520%25281%2525%2520anomalies%250Afor%2520miniImageNet%2529%2520display%2520strong%2520performance%253A%2520starting%2520from%2520five%2520to%2520ten%250Alabelled%2520anomalies%2520and%2520after%2520three%2520active%2520learning%2520cycles%252C%2520we%2520achieve%2520an%250Aaverage%2520AUROC%2520of%25200.95%2520%2528miniImageNet%2529%2520and%25200.86%2520%2528GalaxyMNIST%2529%252C%2520with%2520respective%250AAUPRC%2520of%25200.77%2520and%25200.71.%2520After%2520active%2520learning%2520cycles%252C%2520anomalies%2520are%2520ranked%2520with%250A71%2525%2520%2528miniImageNet%2529%2520to%252093%2525%2520precision%2520in%2520the%25201%2525%2520of%2520the%2520highest-ranked%2520images.%250AAnomalyMatch%2520is%2520tailored%2520for%2520large-scale%2520applications%252C%2520efficiently%2520processing%250Apredictions%2520for%2520100%2520million%2520images%2520within%2520three%2520days%2520on%2520a%2520single%2520GPU.%250AIntegrated%2520into%2520ESAs%2520Datalabs%2520platform%252C%2520AnomalyMatch%2520facilitates%2520targeted%250Adiscovery%2520of%2520scientifically%2520valuable%2520anomalies%2520in%2520vast%2520astronomical%2520datasets.%250AOur%2520results%2520underscore%2520the%2520exceptional%2520utility%2520and%2520scalability%2520of%2520this%2520approach%250Afor%2520anomaly%2520discovery%252C%2520highlighting%2520the%2520value%2520of%2520specialised%2520approaches%2520for%250Adomains%2520characterised%2520by%2520severe%2520label%2520scarcity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyMatch%3A%20Discovering%20Rare%20Objects%20of%20Interest%20with%20Semi-supervised%0A%20%20and%20Active%20Learning&entry.906535625=Pablo%20G%C3%B3mez%20and%20David%20O%27Ryan&entry.1292438233=%20%20Anomaly%20detection%20in%20large%20datasets%20is%20essential%20in%20fields%20such%20as%20astronomy%0Aand%20computer%20vision%3B%20however%2C%20supervised%20methods%20typically%20require%20extensive%0Aanomaly%20labelling%2C%20which%20is%20often%20impractical.%20We%20present%20AnomalyMatch%2C%20an%0Aanomaly%20detection%20framework%20combining%20the%20semi-supervised%20FixMatch%20algorithm%0Ausing%20EfficientNet%20classifiers%20with%20active%20learning.%20By%20treating%20anomaly%0Adetection%20as%20a%20semi-supervised%20binary%20classification%20problem%2C%20we%20efficiently%0Autilise%20limited%20labelled%20and%20abundant%20unlabelled%20images.%20We%20allow%20iterative%0Amodel%20refinement%20in%20a%20user%20interface%20for%20expert%20verification%20of%20high-confidence%0Aanomalies%20and%20correction%20of%20false%20positives.%20Built%20for%20astronomical%20data%2C%0AAnomalyMatch%20generalises%20readily%20to%20other%20domains%20facing%20similar%20data%0Achallenges.%20Evaluations%20on%20the%20GalaxyMNIST%20astronomical%20dataset%20and%20the%0AminiImageNet%20natural-image%20benchmark%20under%20severe%20class%20imbalance%20%281%25%20anomalies%0Afor%20miniImageNet%29%20display%20strong%20performance%3A%20starting%20from%20five%20to%20ten%0Alabelled%20anomalies%20and%20after%20three%20active%20learning%20cycles%2C%20we%20achieve%20an%0Aaverage%20AUROC%20of%200.95%20%28miniImageNet%29%20and%200.86%20%28GalaxyMNIST%29%2C%20with%20respective%0AAUPRC%20of%200.77%20and%200.71.%20After%20active%20learning%20cycles%2C%20anomalies%20are%20ranked%20with%0A71%25%20%28miniImageNet%29%20to%2093%25%20precision%20in%20the%201%25%20of%20the%20highest-ranked%20images.%0AAnomalyMatch%20is%20tailored%20for%20large-scale%20applications%2C%20efficiently%20processing%0Apredictions%20for%20100%20million%20images%20within%20three%20days%20on%20a%20single%20GPU.%0AIntegrated%20into%20ESAs%20Datalabs%20platform%2C%20AnomalyMatch%20facilitates%20targeted%0Adiscovery%20of%20scientifically%20valuable%20anomalies%20in%20vast%20astronomical%20datasets.%0AOur%20results%20underscore%20the%20exceptional%20utility%20and%20scalability%20of%20this%20approach%0Afor%20anomaly%20discovery%2C%20highlighting%20the%20value%20of%20specialised%20approaches%20for%0Adomains%20characterised%20by%20severe%20label%20scarcity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03509v1&entry.124074799=Read"},
{"title": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction\n  Game", "author": "Eric Zhou and Shreyas Basavatia and Moontashir Siam and Zexin Chen and Mark O. Riedl", "abstract": "  We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.\n", "link": "http://arxiv.org/abs/2505.03547v1", "date": "2025-05-06", "relevancy": 2.1108, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5581}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.517}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STORY2GAME%3A%20Generating%20%28Almost%29%20Everything%20in%20an%20Interactive%20Fiction%0A%20%20Game&body=Title%3A%20STORY2GAME%3A%20Generating%20%28Almost%29%20Everything%20in%20an%20Interactive%20Fiction%0A%20%20Game%0AAuthor%3A%20Eric%20Zhou%20and%20Shreyas%20Basavatia%20and%20Moontashir%20Siam%20and%20Zexin%20Chen%20and%20Mark%20O.%20Riedl%0AAbstract%3A%20%20%20We%20introduce%20STORY2GAME%2C%20a%20novel%20approach%20to%20using%20Large%20Language%20Models%20to%0Agenerate%20text-based%20interactive%20fiction%20games%20that%20starts%20by%20generating%20a%0Astory%2C%20populates%20the%20world%2C%20and%20builds%20the%20code%20for%20actions%20in%20a%20game%20engine%0Athat%20enables%20the%20story%20to%20play%20out%20interactively.%20Whereas%20a%20given%20set%20of%0Ahard-coded%20actions%20can%20artificially%20constrain%20story%20generation%2C%20the%20ability%20to%0Agenerate%20actions%20means%20the%20story%20generation%20process%20can%20be%20more%20open-ended%20but%0Astill%20allow%20for%20experiences%20that%20are%20grounded%20in%20a%20game%20state.%20The%20key%20to%0Asuccessful%20action%20generation%20is%20to%20use%20LLM-generated%20preconditions%20and%20effects%0Aof%20actions%20in%20the%20stories%20as%20guides%20for%20what%20aspects%20of%20the%20game%20state%20must%20be%0Atracked%20and%20changed%20by%20the%20game%20engine%20when%20a%20player%20performs%20an%20action.%20We%0Aalso%20introduce%20a%20technique%20for%20dynamically%20generating%20new%20actions%20to%0Aaccommodate%20the%20player%27s%20desire%20to%20perform%20actions%20that%20they%20think%20of%20that%20are%0Anot%20part%20of%20the%20story.%20Dynamic%20action%20generation%20may%20require%20on-the-fly%20updates%0Ato%20the%20game%20engine%27s%20state%20representation%20and%20revision%20of%20previously%20generated%0Aactions.%20We%20evaluate%20the%20success%20rate%20of%20action%20code%20generation%20with%20respect%20to%0Awhether%20a%20player%20can%20interactively%20play%20through%20the%20entire%20generated%20story.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTORY2GAME%253A%2520Generating%2520%2528Almost%2529%2520Everything%2520in%2520an%2520Interactive%2520Fiction%250A%2520%2520Game%26entry.906535625%3DEric%2520Zhou%2520and%2520Shreyas%2520Basavatia%2520and%2520Moontashir%2520Siam%2520and%2520Zexin%2520Chen%2520and%2520Mark%2520O.%2520Riedl%26entry.1292438233%3D%2520%2520We%2520introduce%2520STORY2GAME%252C%2520a%2520novel%2520approach%2520to%2520using%2520Large%2520Language%2520Models%2520to%250Agenerate%2520text-based%2520interactive%2520fiction%2520games%2520that%2520starts%2520by%2520generating%2520a%250Astory%252C%2520populates%2520the%2520world%252C%2520and%2520builds%2520the%2520code%2520for%2520actions%2520in%2520a%2520game%2520engine%250Athat%2520enables%2520the%2520story%2520to%2520play%2520out%2520interactively.%2520Whereas%2520a%2520given%2520set%2520of%250Ahard-coded%2520actions%2520can%2520artificially%2520constrain%2520story%2520generation%252C%2520the%2520ability%2520to%250Agenerate%2520actions%2520means%2520the%2520story%2520generation%2520process%2520can%2520be%2520more%2520open-ended%2520but%250Astill%2520allow%2520for%2520experiences%2520that%2520are%2520grounded%2520in%2520a%2520game%2520state.%2520The%2520key%2520to%250Asuccessful%2520action%2520generation%2520is%2520to%2520use%2520LLM-generated%2520preconditions%2520and%2520effects%250Aof%2520actions%2520in%2520the%2520stories%2520as%2520guides%2520for%2520what%2520aspects%2520of%2520the%2520game%2520state%2520must%2520be%250Atracked%2520and%2520changed%2520by%2520the%2520game%2520engine%2520when%2520a%2520player%2520performs%2520an%2520action.%2520We%250Aalso%2520introduce%2520a%2520technique%2520for%2520dynamically%2520generating%2520new%2520actions%2520to%250Aaccommodate%2520the%2520player%2527s%2520desire%2520to%2520perform%2520actions%2520that%2520they%2520think%2520of%2520that%2520are%250Anot%2520part%2520of%2520the%2520story.%2520Dynamic%2520action%2520generation%2520may%2520require%2520on-the-fly%2520updates%250Ato%2520the%2520game%2520engine%2527s%2520state%2520representation%2520and%2520revision%2520of%2520previously%2520generated%250Aactions.%2520We%2520evaluate%2520the%2520success%2520rate%2520of%2520action%2520code%2520generation%2520with%2520respect%2520to%250Awhether%2520a%2520player%2520can%2520interactively%2520play%2520through%2520the%2520entire%2520generated%2520story.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STORY2GAME%3A%20Generating%20%28Almost%29%20Everything%20in%20an%20Interactive%20Fiction%0A%20%20Game&entry.906535625=Eric%20Zhou%20and%20Shreyas%20Basavatia%20and%20Moontashir%20Siam%20and%20Zexin%20Chen%20and%20Mark%20O.%20Riedl&entry.1292438233=%20%20We%20introduce%20STORY2GAME%2C%20a%20novel%20approach%20to%20using%20Large%20Language%20Models%20to%0Agenerate%20text-based%20interactive%20fiction%20games%20that%20starts%20by%20generating%20a%0Astory%2C%20populates%20the%20world%2C%20and%20builds%20the%20code%20for%20actions%20in%20a%20game%20engine%0Athat%20enables%20the%20story%20to%20play%20out%20interactively.%20Whereas%20a%20given%20set%20of%0Ahard-coded%20actions%20can%20artificially%20constrain%20story%20generation%2C%20the%20ability%20to%0Agenerate%20actions%20means%20the%20story%20generation%20process%20can%20be%20more%20open-ended%20but%0Astill%20allow%20for%20experiences%20that%20are%20grounded%20in%20a%20game%20state.%20The%20key%20to%0Asuccessful%20action%20generation%20is%20to%20use%20LLM-generated%20preconditions%20and%20effects%0Aof%20actions%20in%20the%20stories%20as%20guides%20for%20what%20aspects%20of%20the%20game%20state%20must%20be%0Atracked%20and%20changed%20by%20the%20game%20engine%20when%20a%20player%20performs%20an%20action.%20We%0Aalso%20introduce%20a%20technique%20for%20dynamically%20generating%20new%20actions%20to%0Aaccommodate%20the%20player%27s%20desire%20to%20perform%20actions%20that%20they%20think%20of%20that%20are%0Anot%20part%20of%20the%20story.%20Dynamic%20action%20generation%20may%20require%20on-the-fly%20updates%0Ato%20the%20game%20engine%27s%20state%20representation%20and%20revision%20of%20previously%20generated%0Aactions.%20We%20evaluate%20the%20success%20rate%20of%20action%20code%20generation%20with%20respect%20to%0Awhether%20a%20player%20can%20interactively%20play%20through%20the%20entire%20generated%20story.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03547v1&entry.124074799=Read"},
{"title": "CombAlign: Enhancing Model Expressiveness in Unsupervised Graph\n  Alignment", "author": "Songyang Chen and Yu Liu and Lei Zou and Zexuan Wang and Youfang Lin", "abstract": "  Unsupervised graph alignment finds the node correspondence between a pair of\nattributed graphs by only exploiting graph structure and node features. One\ncategory of recent studies first computes the node representation and then\nmatches nodes with the largest embedding-based similarity, while the other\ncategory reduces the problem to optimal transport (OT) via Gromov-Wasserstein\nlearning. However, it remains largely unexplored in the model expressiveness,\nas well as how theoretical expressivity impacts prediction accuracy. We\ninvestigate the model expressiveness from two aspects. First, we characterize\nthe model's discriminative power in distinguishing matched and unmatched node\npairs across two graphs. Second, we study the model's capability of\nguaranteeing node matching properties such as one-to-one matching and mutual\nalignment. Motivated by our theoretical analysis, we put forward a hybrid\napproach named CombAlign with stronger expressive power. Specifically, we\nenable cross-dimensional feature interaction for OT-based learning and propose\nan embedding-based method inspired by the Weisfeiler-Lehman test. We also apply\nnon-uniform marginals obtained from the embedding-based modules to OT as priors\nfor more expressiveness. Based on that, we propose a traditional\nalgorithm-based refinement, which combines our OT and embedding-based\npredictions using the ensemble learning strategy and reduces the problem to\nmaximum weight matching. With carefully designed edge weights, we ensure those\nmatching properties and further enhance prediction accuracy. By extensive\nexperiments, we demonstrate a significant improvement of 14.5% in alignment\naccuracy compared to state-of-the-art approaches and confirm the soundness of\nour theoretical analysis.\n", "link": "http://arxiv.org/abs/2406.13216v3", "date": "2025-05-06", "relevancy": 2.1105, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5444}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5232}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CombAlign%3A%20Enhancing%20Model%20Expressiveness%20in%20Unsupervised%20Graph%0A%20%20Alignment&body=Title%3A%20CombAlign%3A%20Enhancing%20Model%20Expressiveness%20in%20Unsupervised%20Graph%0A%20%20Alignment%0AAuthor%3A%20Songyang%20Chen%20and%20Yu%20Liu%20and%20Lei%20Zou%20and%20Zexuan%20Wang%20and%20Youfang%20Lin%0AAbstract%3A%20%20%20Unsupervised%20graph%20alignment%20finds%20the%20node%20correspondence%20between%20a%20pair%20of%0Aattributed%20graphs%20by%20only%20exploiting%20graph%20structure%20and%20node%20features.%20One%0Acategory%20of%20recent%20studies%20first%20computes%20the%20node%20representation%20and%20then%0Amatches%20nodes%20with%20the%20largest%20embedding-based%20similarity%2C%20while%20the%20other%0Acategory%20reduces%20the%20problem%20to%20optimal%20transport%20%28OT%29%20via%20Gromov-Wasserstein%0Alearning.%20However%2C%20it%20remains%20largely%20unexplored%20in%20the%20model%20expressiveness%2C%0Aas%20well%20as%20how%20theoretical%20expressivity%20impacts%20prediction%20accuracy.%20We%0Ainvestigate%20the%20model%20expressiveness%20from%20two%20aspects.%20First%2C%20we%20characterize%0Athe%20model%27s%20discriminative%20power%20in%20distinguishing%20matched%20and%20unmatched%20node%0Apairs%20across%20two%20graphs.%20Second%2C%20we%20study%20the%20model%27s%20capability%20of%0Aguaranteeing%20node%20matching%20properties%20such%20as%20one-to-one%20matching%20and%20mutual%0Aalignment.%20Motivated%20by%20our%20theoretical%20analysis%2C%20we%20put%20forward%20a%20hybrid%0Aapproach%20named%20CombAlign%20with%20stronger%20expressive%20power.%20Specifically%2C%20we%0Aenable%20cross-dimensional%20feature%20interaction%20for%20OT-based%20learning%20and%20propose%0Aan%20embedding-based%20method%20inspired%20by%20the%20Weisfeiler-Lehman%20test.%20We%20also%20apply%0Anon-uniform%20marginals%20obtained%20from%20the%20embedding-based%20modules%20to%20OT%20as%20priors%0Afor%20more%20expressiveness.%20Based%20on%20that%2C%20we%20propose%20a%20traditional%0Aalgorithm-based%20refinement%2C%20which%20combines%20our%20OT%20and%20embedding-based%0Apredictions%20using%20the%20ensemble%20learning%20strategy%20and%20reduces%20the%20problem%20to%0Amaximum%20weight%20matching.%20With%20carefully%20designed%20edge%20weights%2C%20we%20ensure%20those%0Amatching%20properties%20and%20further%20enhance%20prediction%20accuracy.%20By%20extensive%0Aexperiments%2C%20we%20demonstrate%20a%20significant%20improvement%20of%2014.5%25%20in%20alignment%0Aaccuracy%20compared%20to%20state-of-the-art%20approaches%20and%20confirm%20the%20soundness%20of%0Aour%20theoretical%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13216v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombAlign%253A%2520Enhancing%2520Model%2520Expressiveness%2520in%2520Unsupervised%2520Graph%250A%2520%2520Alignment%26entry.906535625%3DSongyang%2520Chen%2520and%2520Yu%2520Liu%2520and%2520Lei%2520Zou%2520and%2520Zexuan%2520Wang%2520and%2520Youfang%2520Lin%26entry.1292438233%3D%2520%2520Unsupervised%2520graph%2520alignment%2520finds%2520the%2520node%2520correspondence%2520between%2520a%2520pair%2520of%250Aattributed%2520graphs%2520by%2520only%2520exploiting%2520graph%2520structure%2520and%2520node%2520features.%2520One%250Acategory%2520of%2520recent%2520studies%2520first%2520computes%2520the%2520node%2520representation%2520and%2520then%250Amatches%2520nodes%2520with%2520the%2520largest%2520embedding-based%2520similarity%252C%2520while%2520the%2520other%250Acategory%2520reduces%2520the%2520problem%2520to%2520optimal%2520transport%2520%2528OT%2529%2520via%2520Gromov-Wasserstein%250Alearning.%2520However%252C%2520it%2520remains%2520largely%2520unexplored%2520in%2520the%2520model%2520expressiveness%252C%250Aas%2520well%2520as%2520how%2520theoretical%2520expressivity%2520impacts%2520prediction%2520accuracy.%2520We%250Ainvestigate%2520the%2520model%2520expressiveness%2520from%2520two%2520aspects.%2520First%252C%2520we%2520characterize%250Athe%2520model%2527s%2520discriminative%2520power%2520in%2520distinguishing%2520matched%2520and%2520unmatched%2520node%250Apairs%2520across%2520two%2520graphs.%2520Second%252C%2520we%2520study%2520the%2520model%2527s%2520capability%2520of%250Aguaranteeing%2520node%2520matching%2520properties%2520such%2520as%2520one-to-one%2520matching%2520and%2520mutual%250Aalignment.%2520Motivated%2520by%2520our%2520theoretical%2520analysis%252C%2520we%2520put%2520forward%2520a%2520hybrid%250Aapproach%2520named%2520CombAlign%2520with%2520stronger%2520expressive%2520power.%2520Specifically%252C%2520we%250Aenable%2520cross-dimensional%2520feature%2520interaction%2520for%2520OT-based%2520learning%2520and%2520propose%250Aan%2520embedding-based%2520method%2520inspired%2520by%2520the%2520Weisfeiler-Lehman%2520test.%2520We%2520also%2520apply%250Anon-uniform%2520marginals%2520obtained%2520from%2520the%2520embedding-based%2520modules%2520to%2520OT%2520as%2520priors%250Afor%2520more%2520expressiveness.%2520Based%2520on%2520that%252C%2520we%2520propose%2520a%2520traditional%250Aalgorithm-based%2520refinement%252C%2520which%2520combines%2520our%2520OT%2520and%2520embedding-based%250Apredictions%2520using%2520the%2520ensemble%2520learning%2520strategy%2520and%2520reduces%2520the%2520problem%2520to%250Amaximum%2520weight%2520matching.%2520With%2520carefully%2520designed%2520edge%2520weights%252C%2520we%2520ensure%2520those%250Amatching%2520properties%2520and%2520further%2520enhance%2520prediction%2520accuracy.%2520By%2520extensive%250Aexperiments%252C%2520we%2520demonstrate%2520a%2520significant%2520improvement%2520of%252014.5%2525%2520in%2520alignment%250Aaccuracy%2520compared%2520to%2520state-of-the-art%2520approaches%2520and%2520confirm%2520the%2520soundness%2520of%250Aour%2520theoretical%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13216v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CombAlign%3A%20Enhancing%20Model%20Expressiveness%20in%20Unsupervised%20Graph%0A%20%20Alignment&entry.906535625=Songyang%20Chen%20and%20Yu%20Liu%20and%20Lei%20Zou%20and%20Zexuan%20Wang%20and%20Youfang%20Lin&entry.1292438233=%20%20Unsupervised%20graph%20alignment%20finds%20the%20node%20correspondence%20between%20a%20pair%20of%0Aattributed%20graphs%20by%20only%20exploiting%20graph%20structure%20and%20node%20features.%20One%0Acategory%20of%20recent%20studies%20first%20computes%20the%20node%20representation%20and%20then%0Amatches%20nodes%20with%20the%20largest%20embedding-based%20similarity%2C%20while%20the%20other%0Acategory%20reduces%20the%20problem%20to%20optimal%20transport%20%28OT%29%20via%20Gromov-Wasserstein%0Alearning.%20However%2C%20it%20remains%20largely%20unexplored%20in%20the%20model%20expressiveness%2C%0Aas%20well%20as%20how%20theoretical%20expressivity%20impacts%20prediction%20accuracy.%20We%0Ainvestigate%20the%20model%20expressiveness%20from%20two%20aspects.%20First%2C%20we%20characterize%0Athe%20model%27s%20discriminative%20power%20in%20distinguishing%20matched%20and%20unmatched%20node%0Apairs%20across%20two%20graphs.%20Second%2C%20we%20study%20the%20model%27s%20capability%20of%0Aguaranteeing%20node%20matching%20properties%20such%20as%20one-to-one%20matching%20and%20mutual%0Aalignment.%20Motivated%20by%20our%20theoretical%20analysis%2C%20we%20put%20forward%20a%20hybrid%0Aapproach%20named%20CombAlign%20with%20stronger%20expressive%20power.%20Specifically%2C%20we%0Aenable%20cross-dimensional%20feature%20interaction%20for%20OT-based%20learning%20and%20propose%0Aan%20embedding-based%20method%20inspired%20by%20the%20Weisfeiler-Lehman%20test.%20We%20also%20apply%0Anon-uniform%20marginals%20obtained%20from%20the%20embedding-based%20modules%20to%20OT%20as%20priors%0Afor%20more%20expressiveness.%20Based%20on%20that%2C%20we%20propose%20a%20traditional%0Aalgorithm-based%20refinement%2C%20which%20combines%20our%20OT%20and%20embedding-based%0Apredictions%20using%20the%20ensemble%20learning%20strategy%20and%20reduces%20the%20problem%20to%0Amaximum%20weight%20matching.%20With%20carefully%20designed%20edge%20weights%2C%20we%20ensure%20those%0Amatching%20properties%20and%20further%20enhance%20prediction%20accuracy.%20By%20extensive%0Aexperiments%2C%20we%20demonstrate%20a%20significant%20improvement%20of%2014.5%25%20in%20alignment%0Aaccuracy%20compared%20to%20state-of-the-art%20approaches%20and%20confirm%20the%20soundness%20of%0Aour%20theoretical%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13216v3&entry.124074799=Read"},
{"title": "The Steganographic Potentials of Language Models", "author": "Artem Karpov and Tinuade Adeleke and Seong Hah Cho and Natalia Perez-Campanero", "abstract": "  The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.\n", "link": "http://arxiv.org/abs/2505.03439v1", "date": "2025-05-06", "relevancy": 2.1068, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Steganographic%20Potentials%20of%20Language%20Models&body=Title%3A%20The%20Steganographic%20Potentials%20of%20Language%20Models%0AAuthor%3A%20Artem%20Karpov%20and%20Tinuade%20Adeleke%20and%20Seong%20Hah%20Cho%20and%20Natalia%20Perez-Campanero%0AAbstract%3A%20%20%20The%20potential%20for%20large%20language%20models%20%28LLMs%29%20to%20hide%20messages%20within%20plain%0Atext%20%28steganography%29%20poses%20a%20challenge%20to%20detection%20and%20thwarting%20of%20unaligned%0AAI%20agents%2C%20and%20undermines%20faithfulness%20of%20LLMs%20reasoning.%20We%20explore%20the%0Asteganographic%20capabilities%20of%20LLMs%20fine-tuned%20via%20reinforcement%20learning%20%28RL%29%0Ato%3A%20%281%29%20develop%20covert%20encoding%20schemes%2C%20%282%29%20engage%20in%20steganography%20when%0Aprompted%2C%20and%20%283%29%20utilize%20steganography%20in%20realistic%20scenarios%20where%20hidden%0Areasoning%20is%20likely%2C%20but%20not%20prompted.%20In%20these%20scenarios%2C%20we%20detect%20the%0Aintention%20of%20LLMs%20to%20hide%20their%20reasoning%20as%20well%20as%20their%20steganography%0Aperformance.%20Our%20findings%20in%20the%20fine-tuning%20experiments%20as%20well%20as%20in%0Abehavioral%20non%20fine-tuning%20evaluations%20reveal%20that%20while%20current%20models%20exhibit%0Arudimentary%20steganographic%20abilities%20in%20terms%20of%20security%20and%20capacity%2C%0Aexplicit%20algorithmic%20guidance%20markedly%20enhances%20their%20capacity%20for%20information%0Aconcealment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Steganographic%2520Potentials%2520of%2520Language%2520Models%26entry.906535625%3DArtem%2520Karpov%2520and%2520Tinuade%2520Adeleke%2520and%2520Seong%2520Hah%2520Cho%2520and%2520Natalia%2520Perez-Campanero%26entry.1292438233%3D%2520%2520The%2520potential%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520hide%2520messages%2520within%2520plain%250Atext%2520%2528steganography%2529%2520poses%2520a%2520challenge%2520to%2520detection%2520and%2520thwarting%2520of%2520unaligned%250AAI%2520agents%252C%2520and%2520undermines%2520faithfulness%2520of%2520LLMs%2520reasoning.%2520We%2520explore%2520the%250Asteganographic%2520capabilities%2520of%2520LLMs%2520fine-tuned%2520via%2520reinforcement%2520learning%2520%2528RL%2529%250Ato%253A%2520%25281%2529%2520develop%2520covert%2520encoding%2520schemes%252C%2520%25282%2529%2520engage%2520in%2520steganography%2520when%250Aprompted%252C%2520and%2520%25283%2529%2520utilize%2520steganography%2520in%2520realistic%2520scenarios%2520where%2520hidden%250Areasoning%2520is%2520likely%252C%2520but%2520not%2520prompted.%2520In%2520these%2520scenarios%252C%2520we%2520detect%2520the%250Aintention%2520of%2520LLMs%2520to%2520hide%2520their%2520reasoning%2520as%2520well%2520as%2520their%2520steganography%250Aperformance.%2520Our%2520findings%2520in%2520the%2520fine-tuning%2520experiments%2520as%2520well%2520as%2520in%250Abehavioral%2520non%2520fine-tuning%2520evaluations%2520reveal%2520that%2520while%2520current%2520models%2520exhibit%250Arudimentary%2520steganographic%2520abilities%2520in%2520terms%2520of%2520security%2520and%2520capacity%252C%250Aexplicit%2520algorithmic%2520guidance%2520markedly%2520enhances%2520their%2520capacity%2520for%2520information%250Aconcealment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Steganographic%20Potentials%20of%20Language%20Models&entry.906535625=Artem%20Karpov%20and%20Tinuade%20Adeleke%20and%20Seong%20Hah%20Cho%20and%20Natalia%20Perez-Campanero&entry.1292438233=%20%20The%20potential%20for%20large%20language%20models%20%28LLMs%29%20to%20hide%20messages%20within%20plain%0Atext%20%28steganography%29%20poses%20a%20challenge%20to%20detection%20and%20thwarting%20of%20unaligned%0AAI%20agents%2C%20and%20undermines%20faithfulness%20of%20LLMs%20reasoning.%20We%20explore%20the%0Asteganographic%20capabilities%20of%20LLMs%20fine-tuned%20via%20reinforcement%20learning%20%28RL%29%0Ato%3A%20%281%29%20develop%20covert%20encoding%20schemes%2C%20%282%29%20engage%20in%20steganography%20when%0Aprompted%2C%20and%20%283%29%20utilize%20steganography%20in%20realistic%20scenarios%20where%20hidden%0Areasoning%20is%20likely%2C%20but%20not%20prompted.%20In%20these%20scenarios%2C%20we%20detect%20the%0Aintention%20of%20LLMs%20to%20hide%20their%20reasoning%20as%20well%20as%20their%20steganography%0Aperformance.%20Our%20findings%20in%20the%20fine-tuning%20experiments%20as%20well%20as%20in%0Abehavioral%20non%20fine-tuning%20evaluations%20reveal%20that%20while%20current%20models%20exhibit%0Arudimentary%20steganographic%20abilities%20in%20terms%20of%20security%20and%20capacity%2C%0Aexplicit%20algorithmic%20guidance%20markedly%20enhances%20their%20capacity%20for%20information%0Aconcealment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03439v1&entry.124074799=Read"},
{"title": "FastRM: An efficient and automatic explainability framework for\n  multimodal generative models", "author": "Gabriela Ben-Melech Stan and Estelle Aflalo and Man Luo and Shachar Rosenman and Tiep Le and Sayak Paul and Shao-Yen Tseng and Vasudev Lal", "abstract": "  Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning\ncapabilities over textual and visual inputs. However, these models remain prone\nto generating misinformation. Identifying and mitigating ungrounded responses\nis crucial for developing trustworthy AI. Traditional explainability methods\nsuch as gradient-based relevancy maps, offer insight into the decision process\nof models, but are often computationally expensive and unsuitable for real-time\noutput validation. In this work, we introduce FastRM, an efficient method for\npredicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides\nboth quantitative and qualitative assessment of model confidence. Experimental\nresults demonstrate that FastRM achieves a 99.8% reduction in computation time\nand a 44.4% reduction in memory footprint compared to traditional relevancy map\ngeneration. FastRM allows explainable AI to be more practical and scalable,\nthereby promoting its deployment in real-world applications and enabling users\nto more effectively evaluate the reliability of model outputs.\n", "link": "http://arxiv.org/abs/2412.01487v4", "date": "2025-05-06", "relevancy": 2.1025, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5318}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastRM%3A%20An%20efficient%20and%20automatic%20explainability%20framework%20for%0A%20%20multimodal%20generative%20models&body=Title%3A%20FastRM%3A%20An%20efficient%20and%20automatic%20explainability%20framework%20for%0A%20%20multimodal%20generative%20models%0AAuthor%3A%20Gabriela%20Ben-Melech%20Stan%20and%20Estelle%20Aflalo%20and%20Man%20Luo%20and%20Shachar%20Rosenman%20and%20Tiep%20Le%20and%20Sayak%20Paul%20and%20Shao-Yen%20Tseng%20and%20Vasudev%20Lal%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20reasoning%0Acapabilities%20over%20textual%20and%20visual%20inputs.%20However%2C%20these%20models%20remain%20prone%0Ato%20generating%20misinformation.%20Identifying%20and%20mitigating%20ungrounded%20responses%0Ais%20crucial%20for%20developing%20trustworthy%20AI.%20Traditional%20explainability%20methods%0Asuch%20as%20gradient-based%20relevancy%20maps%2C%20offer%20insight%20into%20the%20decision%20process%0Aof%20models%2C%20but%20are%20often%20computationally%20expensive%20and%20unsuitable%20for%20real-time%0Aoutput%20validation.%20In%20this%20work%2C%20we%20introduce%20FastRM%2C%20an%20efficient%20method%20for%0Apredicting%20explainable%20Relevancy%20Maps%20of%20LVLMs.%20Furthermore%2C%20FastRM%20provides%0Aboth%20quantitative%20and%20qualitative%20assessment%20of%20model%20confidence.%20Experimental%0Aresults%20demonstrate%20that%20FastRM%20achieves%20a%2099.8%25%20reduction%20in%20computation%20time%0Aand%20a%2044.4%25%20reduction%20in%20memory%20footprint%20compared%20to%20traditional%20relevancy%20map%0Ageneration.%20FastRM%20allows%20explainable%20AI%20to%20be%20more%20practical%20and%20scalable%2C%0Athereby%20promoting%20its%20deployment%20in%20real-world%20applications%20and%20enabling%20users%0Ato%20more%20effectively%20evaluate%20the%20reliability%20of%20model%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01487v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastRM%253A%2520An%2520efficient%2520and%2520automatic%2520explainability%2520framework%2520for%250A%2520%2520multimodal%2520generative%2520models%26entry.906535625%3DGabriela%2520Ben-Melech%2520Stan%2520and%2520Estelle%2520Aflalo%2520and%2520Man%2520Luo%2520and%2520Shachar%2520Rosenman%2520and%2520Tiep%2520Le%2520and%2520Sayak%2520Paul%2520and%2520Shao-Yen%2520Tseng%2520and%2520Vasudev%2520Lal%26entry.1292438233%3D%2520%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%2520reasoning%250Acapabilities%2520over%2520textual%2520and%2520visual%2520inputs.%2520However%252C%2520these%2520models%2520remain%2520prone%250Ato%2520generating%2520misinformation.%2520Identifying%2520and%2520mitigating%2520ungrounded%2520responses%250Ais%2520crucial%2520for%2520developing%2520trustworthy%2520AI.%2520Traditional%2520explainability%2520methods%250Asuch%2520as%2520gradient-based%2520relevancy%2520maps%252C%2520offer%2520insight%2520into%2520the%2520decision%2520process%250Aof%2520models%252C%2520but%2520are%2520often%2520computationally%2520expensive%2520and%2520unsuitable%2520for%2520real-time%250Aoutput%2520validation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520FastRM%252C%2520an%2520efficient%2520method%2520for%250Apredicting%2520explainable%2520Relevancy%2520Maps%2520of%2520LVLMs.%2520Furthermore%252C%2520FastRM%2520provides%250Aboth%2520quantitative%2520and%2520qualitative%2520assessment%2520of%2520model%2520confidence.%2520Experimental%250Aresults%2520demonstrate%2520that%2520FastRM%2520achieves%2520a%252099.8%2525%2520reduction%2520in%2520computation%2520time%250Aand%2520a%252044.4%2525%2520reduction%2520in%2520memory%2520footprint%2520compared%2520to%2520traditional%2520relevancy%2520map%250Ageneration.%2520FastRM%2520allows%2520explainable%2520AI%2520to%2520be%2520more%2520practical%2520and%2520scalable%252C%250Athereby%2520promoting%2520its%2520deployment%2520in%2520real-world%2520applications%2520and%2520enabling%2520users%250Ato%2520more%2520effectively%2520evaluate%2520the%2520reliability%2520of%2520model%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01487v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastRM%3A%20An%20efficient%20and%20automatic%20explainability%20framework%20for%0A%20%20multimodal%20generative%20models&entry.906535625=Gabriela%20Ben-Melech%20Stan%20and%20Estelle%20Aflalo%20and%20Man%20Luo%20and%20Shachar%20Rosenman%20and%20Tiep%20Le%20and%20Sayak%20Paul%20and%20Shao-Yen%20Tseng%20and%20Vasudev%20Lal&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20reasoning%0Acapabilities%20over%20textual%20and%20visual%20inputs.%20However%2C%20these%20models%20remain%20prone%0Ato%20generating%20misinformation.%20Identifying%20and%20mitigating%20ungrounded%20responses%0Ais%20crucial%20for%20developing%20trustworthy%20AI.%20Traditional%20explainability%20methods%0Asuch%20as%20gradient-based%20relevancy%20maps%2C%20offer%20insight%20into%20the%20decision%20process%0Aof%20models%2C%20but%20are%20often%20computationally%20expensive%20and%20unsuitable%20for%20real-time%0Aoutput%20validation.%20In%20this%20work%2C%20we%20introduce%20FastRM%2C%20an%20efficient%20method%20for%0Apredicting%20explainable%20Relevancy%20Maps%20of%20LVLMs.%20Furthermore%2C%20FastRM%20provides%0Aboth%20quantitative%20and%20qualitative%20assessment%20of%20model%20confidence.%20Experimental%0Aresults%20demonstrate%20that%20FastRM%20achieves%20a%2099.8%25%20reduction%20in%20computation%20time%0Aand%20a%2044.4%25%20reduction%20in%20memory%20footprint%20compared%20to%20traditional%20relevancy%20map%0Ageneration.%20FastRM%20allows%20explainable%20AI%20to%20be%20more%20practical%20and%20scalable%2C%0Athereby%20promoting%20its%20deployment%20in%20real-world%20applications%20and%20enabling%20users%0Ato%20more%20effectively%20evaluate%20the%20reliability%20of%20model%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01487v4&entry.124074799=Read"},
{"title": "CardioSyntax: end-to-end SYNTAX score prediction -- dataset, benchmark\n  and method", "author": "Alexander Ponomarchuk and Ivan Kruzhilov and Galina Zubkova and Artem Shadrin and Ruslan Utegenov and Ivan Bessonov and Pavel Blinov", "abstract": "  The SYNTAX score has become a widely used measure of coronary disease\nseverity, crucial in selecting the optimal mode of the revascularization\nprocedure. This paper introduces a new medical regression and classification\nproblem - automatically estimating SYNTAX score from coronary angiography. Our\nstudy presents a comprehensive CardioSYNTAX dataset of 3,018 patients for the\nSYNTAX score estimation and coronary dominance classification. The dataset\nfeatures a balanced distribution of individuals with zero and non-zero scores.\nThis dataset includes a first-of-its-kind, complete coronary angiography\nsamples captured through a multi-view X-ray video, allowing one to observe\ncoronary arteries from multiple perspectives. Furthermore, we present a novel,\nfully automatic end-to-end method for estimating the SYNTAX. For such a\ndifficult task, we have achieved a solid coefficient of determination R2 of\n0.51 in score value prediction and 77.3% accuracy for zero score\nclassification.\n", "link": "http://arxiv.org/abs/2407.19894v2", "date": "2025-05-06", "relevancy": 2.0925, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.421}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CardioSyntax%3A%20end-to-end%20SYNTAX%20score%20prediction%20--%20dataset%2C%20benchmark%0A%20%20and%20method&body=Title%3A%20CardioSyntax%3A%20end-to-end%20SYNTAX%20score%20prediction%20--%20dataset%2C%20benchmark%0A%20%20and%20method%0AAuthor%3A%20Alexander%20Ponomarchuk%20and%20Ivan%20Kruzhilov%20and%20Galina%20Zubkova%20and%20Artem%20Shadrin%20and%20Ruslan%20Utegenov%20and%20Ivan%20Bessonov%20and%20Pavel%20Blinov%0AAbstract%3A%20%20%20The%20SYNTAX%20score%20has%20become%20a%20widely%20used%20measure%20of%20coronary%20disease%0Aseverity%2C%20crucial%20in%20selecting%20the%20optimal%20mode%20of%20the%20revascularization%0Aprocedure.%20This%20paper%20introduces%20a%20new%20medical%20regression%20and%20classification%0Aproblem%20-%20automatically%20estimating%20SYNTAX%20score%20from%20coronary%20angiography.%20Our%0Astudy%20presents%20a%20comprehensive%20CardioSYNTAX%20dataset%20of%203%2C018%20patients%20for%20the%0ASYNTAX%20score%20estimation%20and%20coronary%20dominance%20classification.%20The%20dataset%0Afeatures%20a%20balanced%20distribution%20of%20individuals%20with%20zero%20and%20non-zero%20scores.%0AThis%20dataset%20includes%20a%20first-of-its-kind%2C%20complete%20coronary%20angiography%0Asamples%20captured%20through%20a%20multi-view%20X-ray%20video%2C%20allowing%20one%20to%20observe%0Acoronary%20arteries%20from%20multiple%20perspectives.%20Furthermore%2C%20we%20present%20a%20novel%2C%0Afully%20automatic%20end-to-end%20method%20for%20estimating%20the%20SYNTAX.%20For%20such%20a%0Adifficult%20task%2C%20we%20have%20achieved%20a%20solid%20coefficient%20of%20determination%20R2%20of%0A0.51%20in%20score%20value%20prediction%20and%2077.3%25%20accuracy%20for%20zero%20score%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCardioSyntax%253A%2520end-to-end%2520SYNTAX%2520score%2520prediction%2520--%2520dataset%252C%2520benchmark%250A%2520%2520and%2520method%26entry.906535625%3DAlexander%2520Ponomarchuk%2520and%2520Ivan%2520Kruzhilov%2520and%2520Galina%2520Zubkova%2520and%2520Artem%2520Shadrin%2520and%2520Ruslan%2520Utegenov%2520and%2520Ivan%2520Bessonov%2520and%2520Pavel%2520Blinov%26entry.1292438233%3D%2520%2520The%2520SYNTAX%2520score%2520has%2520become%2520a%2520widely%2520used%2520measure%2520of%2520coronary%2520disease%250Aseverity%252C%2520crucial%2520in%2520selecting%2520the%2520optimal%2520mode%2520of%2520the%2520revascularization%250Aprocedure.%2520This%2520paper%2520introduces%2520a%2520new%2520medical%2520regression%2520and%2520classification%250Aproblem%2520-%2520automatically%2520estimating%2520SYNTAX%2520score%2520from%2520coronary%2520angiography.%2520Our%250Astudy%2520presents%2520a%2520comprehensive%2520CardioSYNTAX%2520dataset%2520of%25203%252C018%2520patients%2520for%2520the%250ASYNTAX%2520score%2520estimation%2520and%2520coronary%2520dominance%2520classification.%2520The%2520dataset%250Afeatures%2520a%2520balanced%2520distribution%2520of%2520individuals%2520with%2520zero%2520and%2520non-zero%2520scores.%250AThis%2520dataset%2520includes%2520a%2520first-of-its-kind%252C%2520complete%2520coronary%2520angiography%250Asamples%2520captured%2520through%2520a%2520multi-view%2520X-ray%2520video%252C%2520allowing%2520one%2520to%2520observe%250Acoronary%2520arteries%2520from%2520multiple%2520perspectives.%2520Furthermore%252C%2520we%2520present%2520a%2520novel%252C%250Afully%2520automatic%2520end-to-end%2520method%2520for%2520estimating%2520the%2520SYNTAX.%2520For%2520such%2520a%250Adifficult%2520task%252C%2520we%2520have%2520achieved%2520a%2520solid%2520coefficient%2520of%2520determination%2520R2%2520of%250A0.51%2520in%2520score%2520value%2520prediction%2520and%252077.3%2525%2520accuracy%2520for%2520zero%2520score%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CardioSyntax%3A%20end-to-end%20SYNTAX%20score%20prediction%20--%20dataset%2C%20benchmark%0A%20%20and%20method&entry.906535625=Alexander%20Ponomarchuk%20and%20Ivan%20Kruzhilov%20and%20Galina%20Zubkova%20and%20Artem%20Shadrin%20and%20Ruslan%20Utegenov%20and%20Ivan%20Bessonov%20and%20Pavel%20Blinov&entry.1292438233=%20%20The%20SYNTAX%20score%20has%20become%20a%20widely%20used%20measure%20of%20coronary%20disease%0Aseverity%2C%20crucial%20in%20selecting%20the%20optimal%20mode%20of%20the%20revascularization%0Aprocedure.%20This%20paper%20introduces%20a%20new%20medical%20regression%20and%20classification%0Aproblem%20-%20automatically%20estimating%20SYNTAX%20score%20from%20coronary%20angiography.%20Our%0Astudy%20presents%20a%20comprehensive%20CardioSYNTAX%20dataset%20of%203%2C018%20patients%20for%20the%0ASYNTAX%20score%20estimation%20and%20coronary%20dominance%20classification.%20The%20dataset%0Afeatures%20a%20balanced%20distribution%20of%20individuals%20with%20zero%20and%20non-zero%20scores.%0AThis%20dataset%20includes%20a%20first-of-its-kind%2C%20complete%20coronary%20angiography%0Asamples%20captured%20through%20a%20multi-view%20X-ray%20video%2C%20allowing%20one%20to%20observe%0Acoronary%20arteries%20from%20multiple%20perspectives.%20Furthermore%2C%20we%20present%20a%20novel%2C%0Afully%20automatic%20end-to-end%20method%20for%20estimating%20the%20SYNTAX.%20For%20such%20a%0Adifficult%20task%2C%20we%20have%20achieved%20a%20solid%20coefficient%20of%20determination%20R2%20of%0A0.51%20in%20score%20value%20prediction%20and%2077.3%25%20accuracy%20for%20zero%20score%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19894v2&entry.124074799=Read"},
{"title": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective\n  Reasoning for LLM Alignment", "author": "Ruoxi Cheng and Haoxuan Ma and Weixin Wang", "abstract": "  The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness.\n", "link": "http://arxiv.org/abs/2503.18991v2", "date": "2025-05-06", "relevancy": 2.0654, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5261}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5102}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAIR%3A%20Hardness-Aware%20Inverse%20Reinforcement%20Learning%20with%20Introspective%0A%20%20Reasoning%20for%20LLM%20Alignment&body=Title%3A%20HAIR%3A%20Hardness-Aware%20Inverse%20Reinforcement%20Learning%20with%20Introspective%0A%20%20Reasoning%20for%20LLM%20Alignment%0AAuthor%3A%20Ruoxi%20Cheng%20and%20Haoxuan%20Ma%20and%20Weixin%20Wang%0AAbstract%3A%20%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20values%20remains%0Acritical%20yet%20hindered%20by%20four%20key%20challenges%3A%20%281%29%20scarcity%20of%20balanced%20safety%0Adatasets%2C%20%282%29%20alignment%20tax%2C%20%283%29%20vulnerability%20to%20jailbreak%20attacks%20due%20to%0Ashallow%20alignment%2C%20and%20%284%29%20inability%20to%20dynamically%20adapt%20rewards%20according%20to%0Atask%20difficulty.%20To%20address%20these%20limitations%2C%20we%20introduce%20HAIR%0A%28Hardness-Aware%20Inverse%20Reinforcement%20Learning%20with%20Introspective%20Reasoning%29%2C%20a%0Anovel%20alignment%20approach%20inspired%20by%20shadow%20models%20in%20membership%20inference%0Aattacks.%20Our%20approach%20consists%20of%20two%20main%20components%3A%20%281%29%20construction%20of%20a%0Abalanced%20safety%20Chain-of-Draft%20%28CoD%29%20dataset%20for%20seven%20harmful%20categories%20using%0Astructured%20prompts%20that%20leverage%20the%20introspective%20reasoning%20capabilities%20of%0ALLMs%3B%20and%20%282%29%20training%20of%20category-specific%20reward%20models%20with%20Group%20Relative%0APolicy%20Optimization%20%28GRPO%29%2C%20dynamically%20tuning%20optimization%20to%20task%20difficulty%0Aat%20both%20the%20data%20and%20model%20levels.%20Comprehensive%20experiments%20across%20four%0Aharmlessness%20and%20four%20usefulness%20benchmarks%20demonstrate%20that%20HAIR%20achieves%0Astate-of-the-art%20performance%2C%20outperforming%20all%20baseline%20methods%20in%20safety%0Awhile%20maintaining%20high%20levels%20of%20usefulness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAIR%253A%2520Hardness-Aware%2520Inverse%2520Reinforcement%2520Learning%2520with%2520Introspective%250A%2520%2520Reasoning%2520for%2520LLM%2520Alignment%26entry.906535625%3DRuoxi%2520Cheng%2520and%2520Haoxuan%2520Ma%2520and%2520Weixin%2520Wang%26entry.1292438233%3D%2520%2520The%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520values%2520remains%250Acritical%2520yet%2520hindered%2520by%2520four%2520key%2520challenges%253A%2520%25281%2529%2520scarcity%2520of%2520balanced%2520safety%250Adatasets%252C%2520%25282%2529%2520alignment%2520tax%252C%2520%25283%2529%2520vulnerability%2520to%2520jailbreak%2520attacks%2520due%2520to%250Ashallow%2520alignment%252C%2520and%2520%25284%2529%2520inability%2520to%2520dynamically%2520adapt%2520rewards%2520according%2520to%250Atask%2520difficulty.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520HAIR%250A%2528Hardness-Aware%2520Inverse%2520Reinforcement%2520Learning%2520with%2520Introspective%2520Reasoning%2529%252C%2520a%250Anovel%2520alignment%2520approach%2520inspired%2520by%2520shadow%2520models%2520in%2520membership%2520inference%250Aattacks.%2520Our%2520approach%2520consists%2520of%2520two%2520main%2520components%253A%2520%25281%2529%2520construction%2520of%2520a%250Abalanced%2520safety%2520Chain-of-Draft%2520%2528CoD%2529%2520dataset%2520for%2520seven%2520harmful%2520categories%2520using%250Astructured%2520prompts%2520that%2520leverage%2520the%2520introspective%2520reasoning%2520capabilities%2520of%250ALLMs%253B%2520and%2520%25282%2529%2520training%2520of%2520category-specific%2520reward%2520models%2520with%2520Group%2520Relative%250APolicy%2520Optimization%2520%2528GRPO%2529%252C%2520dynamically%2520tuning%2520optimization%2520to%2520task%2520difficulty%250Aat%2520both%2520the%2520data%2520and%2520model%2520levels.%2520Comprehensive%2520experiments%2520across%2520four%250Aharmlessness%2520and%2520four%2520usefulness%2520benchmarks%2520demonstrate%2520that%2520HAIR%2520achieves%250Astate-of-the-art%2520performance%252C%2520outperforming%2520all%2520baseline%2520methods%2520in%2520safety%250Awhile%2520maintaining%2520high%2520levels%2520of%2520usefulness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAIR%3A%20Hardness-Aware%20Inverse%20Reinforcement%20Learning%20with%20Introspective%0A%20%20Reasoning%20for%20LLM%20Alignment&entry.906535625=Ruoxi%20Cheng%20and%20Haoxuan%20Ma%20and%20Weixin%20Wang&entry.1292438233=%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20values%20remains%0Acritical%20yet%20hindered%20by%20four%20key%20challenges%3A%20%281%29%20scarcity%20of%20balanced%20safety%0Adatasets%2C%20%282%29%20alignment%20tax%2C%20%283%29%20vulnerability%20to%20jailbreak%20attacks%20due%20to%0Ashallow%20alignment%2C%20and%20%284%29%20inability%20to%20dynamically%20adapt%20rewards%20according%20to%0Atask%20difficulty.%20To%20address%20these%20limitations%2C%20we%20introduce%20HAIR%0A%28Hardness-Aware%20Inverse%20Reinforcement%20Learning%20with%20Introspective%20Reasoning%29%2C%20a%0Anovel%20alignment%20approach%20inspired%20by%20shadow%20models%20in%20membership%20inference%0Aattacks.%20Our%20approach%20consists%20of%20two%20main%20components%3A%20%281%29%20construction%20of%20a%0Abalanced%20safety%20Chain-of-Draft%20%28CoD%29%20dataset%20for%20seven%20harmful%20categories%20using%0Astructured%20prompts%20that%20leverage%20the%20introspective%20reasoning%20capabilities%20of%0ALLMs%3B%20and%20%282%29%20training%20of%20category-specific%20reward%20models%20with%20Group%20Relative%0APolicy%20Optimization%20%28GRPO%29%2C%20dynamically%20tuning%20optimization%20to%20task%20difficulty%0Aat%20both%20the%20data%20and%20model%20levels.%20Comprehensive%20experiments%20across%20four%0Aharmlessness%20and%20four%20usefulness%20benchmarks%20demonstrate%20that%20HAIR%20achieves%0Astate-of-the-art%20performance%2C%20outperforming%20all%20baseline%20methods%20in%20safety%0Awhile%20maintaining%20high%20levels%20of%20usefulness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18991v2&entry.124074799=Read"},
{"title": "High-order regularization dealing with ill-conditioned robot\n  localization problems", "author": "Xinghua Liu and Ming Cao", "abstract": "  In this work, we propose a high-order regularization method to solve the\nill-conditioned problems in robot localization. Numerical solutions to robot\nlocalization problems are often unstable when the problems are ill-conditioned.\nA typical way to solve ill-conditioned problems is regularization, and a\nclassical regularization method is the Tikhonov regularization. It is shown\nthat the Tikhonov regularization is a low-order case of our method. We find\nthat the proposed method is superior to the Tikhonov regularization in\napproximating some ill-conditioned inverse problems, such as some basic robot\nlocalization problems. The proposed method overcomes the over-smoothing problem\nin the Tikhonov regularization as it uses more than one term in the\napproximation of the matrix inverse, and an explanation for the over-smoothing\nof the Tikhonov regularization is given. Moreover, one a priori criterion,\nwhich improves the numerical stability of the ill-conditioned problem, is\nproposed to obtain an optimal regularization matrix. As most of the\nregularization solutions are biased, we also provide two bias-correction\ntechniques for the proposed high-order regularization. The simulation and\nexperimental results using an Ultra-Wideband sensor network in a 3D environment\nare discussed, demonstrating the performance of the proposed method.\n", "link": "http://arxiv.org/abs/2410.01919v2", "date": "2025-05-06", "relevancy": 2.0638, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5183}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-order%20regularization%20dealing%20with%20ill-conditioned%20robot%0A%20%20localization%20problems&body=Title%3A%20High-order%20regularization%20dealing%20with%20ill-conditioned%20robot%0A%20%20localization%20problems%0AAuthor%3A%20Xinghua%20Liu%20and%20Ming%20Cao%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20high-order%20regularization%20method%20to%20solve%20the%0Aill-conditioned%20problems%20in%20robot%20localization.%20Numerical%20solutions%20to%20robot%0Alocalization%20problems%20are%20often%20unstable%20when%20the%20problems%20are%20ill-conditioned.%0AA%20typical%20way%20to%20solve%20ill-conditioned%20problems%20is%20regularization%2C%20and%20a%0Aclassical%20regularization%20method%20is%20the%20Tikhonov%20regularization.%20It%20is%20shown%0Athat%20the%20Tikhonov%20regularization%20is%20a%20low-order%20case%20of%20our%20method.%20We%20find%0Athat%20the%20proposed%20method%20is%20superior%20to%20the%20Tikhonov%20regularization%20in%0Aapproximating%20some%20ill-conditioned%20inverse%20problems%2C%20such%20as%20some%20basic%20robot%0Alocalization%20problems.%20The%20proposed%20method%20overcomes%20the%20over-smoothing%20problem%0Ain%20the%20Tikhonov%20regularization%20as%20it%20uses%20more%20than%20one%20term%20in%20the%0Aapproximation%20of%20the%20matrix%20inverse%2C%20and%20an%20explanation%20for%20the%20over-smoothing%0Aof%20the%20Tikhonov%20regularization%20is%20given.%20Moreover%2C%20one%20a%20priori%20criterion%2C%0Awhich%20improves%20the%20numerical%20stability%20of%20the%20ill-conditioned%20problem%2C%20is%0Aproposed%20to%20obtain%20an%20optimal%20regularization%20matrix.%20As%20most%20of%20the%0Aregularization%20solutions%20are%20biased%2C%20we%20also%20provide%20two%20bias-correction%0Atechniques%20for%20the%20proposed%20high-order%20regularization.%20The%20simulation%20and%0Aexperimental%20results%20using%20an%20Ultra-Wideband%20sensor%20network%20in%20a%203D%20environment%0Aare%20discussed%2C%20demonstrating%20the%20performance%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-order%2520regularization%2520dealing%2520with%2520ill-conditioned%2520robot%250A%2520%2520localization%2520problems%26entry.906535625%3DXinghua%2520Liu%2520and%2520Ming%2520Cao%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520high-order%2520regularization%2520method%2520to%2520solve%2520the%250Aill-conditioned%2520problems%2520in%2520robot%2520localization.%2520Numerical%2520solutions%2520to%2520robot%250Alocalization%2520problems%2520are%2520often%2520unstable%2520when%2520the%2520problems%2520are%2520ill-conditioned.%250AA%2520typical%2520way%2520to%2520solve%2520ill-conditioned%2520problems%2520is%2520regularization%252C%2520and%2520a%250Aclassical%2520regularization%2520method%2520is%2520the%2520Tikhonov%2520regularization.%2520It%2520is%2520shown%250Athat%2520the%2520Tikhonov%2520regularization%2520is%2520a%2520low-order%2520case%2520of%2520our%2520method.%2520We%2520find%250Athat%2520the%2520proposed%2520method%2520is%2520superior%2520to%2520the%2520Tikhonov%2520regularization%2520in%250Aapproximating%2520some%2520ill-conditioned%2520inverse%2520problems%252C%2520such%2520as%2520some%2520basic%2520robot%250Alocalization%2520problems.%2520The%2520proposed%2520method%2520overcomes%2520the%2520over-smoothing%2520problem%250Ain%2520the%2520Tikhonov%2520regularization%2520as%2520it%2520uses%2520more%2520than%2520one%2520term%2520in%2520the%250Aapproximation%2520of%2520the%2520matrix%2520inverse%252C%2520and%2520an%2520explanation%2520for%2520the%2520over-smoothing%250Aof%2520the%2520Tikhonov%2520regularization%2520is%2520given.%2520Moreover%252C%2520one%2520a%2520priori%2520criterion%252C%250Awhich%2520improves%2520the%2520numerical%2520stability%2520of%2520the%2520ill-conditioned%2520problem%252C%2520is%250Aproposed%2520to%2520obtain%2520an%2520optimal%2520regularization%2520matrix.%2520As%2520most%2520of%2520the%250Aregularization%2520solutions%2520are%2520biased%252C%2520we%2520also%2520provide%2520two%2520bias-correction%250Atechniques%2520for%2520the%2520proposed%2520high-order%2520regularization.%2520The%2520simulation%2520and%250Aexperimental%2520results%2520using%2520an%2520Ultra-Wideband%2520sensor%2520network%2520in%2520a%25203D%2520environment%250Aare%2520discussed%252C%2520demonstrating%2520the%2520performance%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-order%20regularization%20dealing%20with%20ill-conditioned%20robot%0A%20%20localization%20problems&entry.906535625=Xinghua%20Liu%20and%20Ming%20Cao&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20high-order%20regularization%20method%20to%20solve%20the%0Aill-conditioned%20problems%20in%20robot%20localization.%20Numerical%20solutions%20to%20robot%0Alocalization%20problems%20are%20often%20unstable%20when%20the%20problems%20are%20ill-conditioned.%0AA%20typical%20way%20to%20solve%20ill-conditioned%20problems%20is%20regularization%2C%20and%20a%0Aclassical%20regularization%20method%20is%20the%20Tikhonov%20regularization.%20It%20is%20shown%0Athat%20the%20Tikhonov%20regularization%20is%20a%20low-order%20case%20of%20our%20method.%20We%20find%0Athat%20the%20proposed%20method%20is%20superior%20to%20the%20Tikhonov%20regularization%20in%0Aapproximating%20some%20ill-conditioned%20inverse%20problems%2C%20such%20as%20some%20basic%20robot%0Alocalization%20problems.%20The%20proposed%20method%20overcomes%20the%20over-smoothing%20problem%0Ain%20the%20Tikhonov%20regularization%20as%20it%20uses%20more%20than%20one%20term%20in%20the%0Aapproximation%20of%20the%20matrix%20inverse%2C%20and%20an%20explanation%20for%20the%20over-smoothing%0Aof%20the%20Tikhonov%20regularization%20is%20given.%20Moreover%2C%20one%20a%20priori%20criterion%2C%0Awhich%20improves%20the%20numerical%20stability%20of%20the%20ill-conditioned%20problem%2C%20is%0Aproposed%20to%20obtain%20an%20optimal%20regularization%20matrix.%20As%20most%20of%20the%0Aregularization%20solutions%20are%20biased%2C%20we%20also%20provide%20two%20bias-correction%0Atechniques%20for%20the%20proposed%20high-order%20regularization.%20The%20simulation%20and%0Aexperimental%20results%20using%20an%20Ultra-Wideband%20sensor%20network%20in%20a%203D%20environment%0Aare%20discussed%2C%20demonstrating%20the%20performance%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01919v2&entry.124074799=Read"},
{"title": "Faster MoE LLM Inference for Extremely Large Models", "author": "Haoqi Yang and Luohe Shi and Qiwei Li and Zuchao Li and Ping Wang and Bo Du and Mengjia Shen and Hai Zhao", "abstract": "  Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement.\n", "link": "http://arxiv.org/abs/2505.03531v1", "date": "2025-05-06", "relevancy": 2.0591, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5192}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20MoE%20LLM%20Inference%20for%20Extremely%20Large%20Models&body=Title%3A%20Faster%20MoE%20LLM%20Inference%20for%20Extremely%20Large%20Models%0AAuthor%3A%20Haoqi%20Yang%20and%20Luohe%20Shi%20and%20Qiwei%20Li%20and%20Zuchao%20Li%20and%20Ping%20Wang%20and%20Bo%20Du%20and%20Mengjia%20Shen%20and%20Hai%20Zhao%0AAbstract%3A%20%20%20Sparse%20Mixture%20of%20Experts%20%28MoE%29%20large%20language%20models%20%28LLMs%29%20are%20gradually%0Abecoming%20the%20mainstream%20approach%20for%20ultra-large-scale%20models.%20Existing%0Aoptimization%20efforts%20for%20MoE%20models%20have%20focused%20primarily%20on%20coarse-grained%0AMoE%20architectures.%20With%20the%20emergence%20of%20DeepSeek%20Models%2C%20fine-grained%20MoE%0Amodels%20are%20gaining%20popularity%2C%20yet%20research%20on%20them%20remains%20limited.%20Therefore%2C%0Awe%20want%20to%20discuss%20the%20efficiency%20dynamic%20under%20different%20service%20loads.%0AAdditionally%2C%20fine-grained%20models%20allow%20deployers%20to%20reduce%20the%20number%20of%0Arouted%20experts%2C%20both%20activated%20counts%20and%20total%20counts%2C%20raising%20the%20question%20of%0Ahow%20this%20reduction%20affects%20the%20trade-off%20between%20MoE%20efficiency%20and%0Aperformance.%20Our%20findings%20indicate%20that%20while%20deploying%20MoE%20models%20presents%0Agreater%20challenges%2C%20it%20also%20offers%20significant%20optimization%20opportunities.%0AReducing%20the%20number%20of%20activated%20experts%20can%20lead%20to%20substantial%20efficiency%0Aimprovements%20in%20certain%20scenarios%2C%20with%20only%20minor%20performance%20degradation.%0AReducing%20the%20total%20number%20of%20experts%20provides%20limited%20efficiency%20gains%20but%0Aresults%20in%20severe%20performance%20degradation.%20Our%20method%20can%20increase%20throughput%0Aby%20at%20least%2010%5C%25%20without%20any%20performance%20degradation.%20Overall%2C%20we%20conclude%20that%0AMoE%20inference%20optimization%20remains%20an%20area%20with%20substantial%20potential%20for%0Aexploration%20and%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520MoE%2520LLM%2520Inference%2520for%2520Extremely%2520Large%2520Models%26entry.906535625%3DHaoqi%2520Yang%2520and%2520Luohe%2520Shi%2520and%2520Qiwei%2520Li%2520and%2520Zuchao%2520Li%2520and%2520Ping%2520Wang%2520and%2520Bo%2520Du%2520and%2520Mengjia%2520Shen%2520and%2520Hai%2520Zhao%26entry.1292438233%3D%2520%2520Sparse%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520gradually%250Abecoming%2520the%2520mainstream%2520approach%2520for%2520ultra-large-scale%2520models.%2520Existing%250Aoptimization%2520efforts%2520for%2520MoE%2520models%2520have%2520focused%2520primarily%2520on%2520coarse-grained%250AMoE%2520architectures.%2520With%2520the%2520emergence%2520of%2520DeepSeek%2520Models%252C%2520fine-grained%2520MoE%250Amodels%2520are%2520gaining%2520popularity%252C%2520yet%2520research%2520on%2520them%2520remains%2520limited.%2520Therefore%252C%250Awe%2520want%2520to%2520discuss%2520the%2520efficiency%2520dynamic%2520under%2520different%2520service%2520loads.%250AAdditionally%252C%2520fine-grained%2520models%2520allow%2520deployers%2520to%2520reduce%2520the%2520number%2520of%250Arouted%2520experts%252C%2520both%2520activated%2520counts%2520and%2520total%2520counts%252C%2520raising%2520the%2520question%2520of%250Ahow%2520this%2520reduction%2520affects%2520the%2520trade-off%2520between%2520MoE%2520efficiency%2520and%250Aperformance.%2520Our%2520findings%2520indicate%2520that%2520while%2520deploying%2520MoE%2520models%2520presents%250Agreater%2520challenges%252C%2520it%2520also%2520offers%2520significant%2520optimization%2520opportunities.%250AReducing%2520the%2520number%2520of%2520activated%2520experts%2520can%2520lead%2520to%2520substantial%2520efficiency%250Aimprovements%2520in%2520certain%2520scenarios%252C%2520with%2520only%2520minor%2520performance%2520degradation.%250AReducing%2520the%2520total%2520number%2520of%2520experts%2520provides%2520limited%2520efficiency%2520gains%2520but%250Aresults%2520in%2520severe%2520performance%2520degradation.%2520Our%2520method%2520can%2520increase%2520throughput%250Aby%2520at%2520least%252010%255C%2525%2520without%2520any%2520performance%2520degradation.%2520Overall%252C%2520we%2520conclude%2520that%250AMoE%2520inference%2520optimization%2520remains%2520an%2520area%2520with%2520substantial%2520potential%2520for%250Aexploration%2520and%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20MoE%20LLM%20Inference%20for%20Extremely%20Large%20Models&entry.906535625=Haoqi%20Yang%20and%20Luohe%20Shi%20and%20Qiwei%20Li%20and%20Zuchao%20Li%20and%20Ping%20Wang%20and%20Bo%20Du%20and%20Mengjia%20Shen%20and%20Hai%20Zhao&entry.1292438233=%20%20Sparse%20Mixture%20of%20Experts%20%28MoE%29%20large%20language%20models%20%28LLMs%29%20are%20gradually%0Abecoming%20the%20mainstream%20approach%20for%20ultra-large-scale%20models.%20Existing%0Aoptimization%20efforts%20for%20MoE%20models%20have%20focused%20primarily%20on%20coarse-grained%0AMoE%20architectures.%20With%20the%20emergence%20of%20DeepSeek%20Models%2C%20fine-grained%20MoE%0Amodels%20are%20gaining%20popularity%2C%20yet%20research%20on%20them%20remains%20limited.%20Therefore%2C%0Awe%20want%20to%20discuss%20the%20efficiency%20dynamic%20under%20different%20service%20loads.%0AAdditionally%2C%20fine-grained%20models%20allow%20deployers%20to%20reduce%20the%20number%20of%0Arouted%20experts%2C%20both%20activated%20counts%20and%20total%20counts%2C%20raising%20the%20question%20of%0Ahow%20this%20reduction%20affects%20the%20trade-off%20between%20MoE%20efficiency%20and%0Aperformance.%20Our%20findings%20indicate%20that%20while%20deploying%20MoE%20models%20presents%0Agreater%20challenges%2C%20it%20also%20offers%20significant%20optimization%20opportunities.%0AReducing%20the%20number%20of%20activated%20experts%20can%20lead%20to%20substantial%20efficiency%0Aimprovements%20in%20certain%20scenarios%2C%20with%20only%20minor%20performance%20degradation.%0AReducing%20the%20total%20number%20of%20experts%20provides%20limited%20efficiency%20gains%20but%0Aresults%20in%20severe%20performance%20degradation.%20Our%20method%20can%20increase%20throughput%0Aby%20at%20least%2010%5C%25%20without%20any%20performance%20degradation.%20Overall%2C%20we%20conclude%20that%0AMoE%20inference%20optimization%20remains%20an%20area%20with%20substantial%20potential%20for%0Aexploration%20and%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03531v1&entry.124074799=Read"},
{"title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation", "author": "Hao Wen and Shizuo Tian and Borislav Pavlov and Wenjie Du and Yixuan Li and Ge Chang and Shanhui Zhao and Jiacheng Liu and Yunxin Liu and Ya-Qin Zhang and Yuanchun Li", "abstract": "  Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand powerful large language models that are difficult to be deployed\nlocally on end-users' devices, raising huge concerns about user privacy and\ncentralized serving cost. Inspired by the remarkable coding abilities of recent\nsmall language models (SLMs), we propose to convert the UI task automation\nproblem to a code generation problem, which can be effectively solved by an\non-device SLM and efficiently executed with an on-device code interpreter.\nUnlike normal coding tasks that can be extensively pre-trained with public\ndatasets, generating UI automation code is challenging due to the diversity,\ncomplexity, and variability of target apps. Therefore, we adopt a\ndocument-centered approach that automatically builds fine-grained API\ndocumentation for each app and generates diverse task samples based on this\ndocumentation. By guiding the agent with the synthetic documents and task\nsamples, it learns to generate precise and efficient scripts to complete unseen\ntasks. Based on detailed comparisons with state-of-the-art mobile UI agents,\nour approach effectively improves the mobile task automation with significantly\nhigher success rates and lower latency/token consumption. Code is open-sourced\nat https://github.com/MobileLLM/AutoDroid-V2.\n", "link": "http://arxiv.org/abs/2412.18116v3", "date": "2025-05-06", "relevancy": 2.0543, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5187}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5124}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoDroid-V2%3A%20Boosting%20SLM-based%20GUI%20Agents%20via%20Code%20Generation&body=Title%3A%20AutoDroid-V2%3A%20Boosting%20SLM-based%20GUI%20Agents%20via%20Code%20Generation%0AAuthor%3A%20Hao%20Wen%20and%20Shizuo%20Tian%20and%20Borislav%20Pavlov%20and%20Wenjie%20Du%20and%20Yixuan%20Li%20and%20Ge%20Chang%20and%20Shanhui%20Zhao%20and%20Jiacheng%20Liu%20and%20Yunxin%20Liu%20and%20Ya-Qin%20Zhang%20and%20Yuanchun%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20brought%20exciting%20new%20advances%20to%20mobile%20UI%0Aagents%2C%20a%20long-standing%20research%20field%20that%20aims%20to%20complete%20arbitrary%20natural%0Alanguage%20tasks%20through%20mobile%20UI%20interactions.%20However%2C%20existing%20UI%20agents%0Ausually%20demand%20powerful%20large%20language%20models%20that%20are%20difficult%20to%20be%20deployed%0Alocally%20on%20end-users%27%20devices%2C%20raising%20huge%20concerns%20about%20user%20privacy%20and%0Acentralized%20serving%20cost.%20Inspired%20by%20the%20remarkable%20coding%20abilities%20of%20recent%0Asmall%20language%20models%20%28SLMs%29%2C%20we%20propose%20to%20convert%20the%20UI%20task%20automation%0Aproblem%20to%20a%20code%20generation%20problem%2C%20which%20can%20be%20effectively%20solved%20by%20an%0Aon-device%20SLM%20and%20efficiently%20executed%20with%20an%20on-device%20code%20interpreter.%0AUnlike%20normal%20coding%20tasks%20that%20can%20be%20extensively%20pre-trained%20with%20public%0Adatasets%2C%20generating%20UI%20automation%20code%20is%20challenging%20due%20to%20the%20diversity%2C%0Acomplexity%2C%20and%20variability%20of%20target%20apps.%20Therefore%2C%20we%20adopt%20a%0Adocument-centered%20approach%20that%20automatically%20builds%20fine-grained%20API%0Adocumentation%20for%20each%20app%20and%20generates%20diverse%20task%20samples%20based%20on%20this%0Adocumentation.%20By%20guiding%20the%20agent%20with%20the%20synthetic%20documents%20and%20task%0Asamples%2C%20it%20learns%20to%20generate%20precise%20and%20efficient%20scripts%20to%20complete%20unseen%0Atasks.%20Based%20on%20detailed%20comparisons%20with%20state-of-the-art%20mobile%20UI%20agents%2C%0Aour%20approach%20effectively%20improves%20the%20mobile%20task%20automation%20with%20significantly%0Ahigher%20success%20rates%20and%20lower%20latency/token%20consumption.%20Code%20is%20open-sourced%0Aat%20https%3A//github.com/MobileLLM/AutoDroid-V2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18116v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoDroid-V2%253A%2520Boosting%2520SLM-based%2520GUI%2520Agents%2520via%2520Code%2520Generation%26entry.906535625%3DHao%2520Wen%2520and%2520Shizuo%2520Tian%2520and%2520Borislav%2520Pavlov%2520and%2520Wenjie%2520Du%2520and%2520Yixuan%2520Li%2520and%2520Ge%2520Chang%2520and%2520Shanhui%2520Zhao%2520and%2520Jiacheng%2520Liu%2520and%2520Yunxin%2520Liu%2520and%2520Ya-Qin%2520Zhang%2520and%2520Yuanchun%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520brought%2520exciting%2520new%2520advances%2520to%2520mobile%2520UI%250Aagents%252C%2520a%2520long-standing%2520research%2520field%2520that%2520aims%2520to%2520complete%2520arbitrary%2520natural%250Alanguage%2520tasks%2520through%2520mobile%2520UI%2520interactions.%2520However%252C%2520existing%2520UI%2520agents%250Ausually%2520demand%2520powerful%2520large%2520language%2520models%2520that%2520are%2520difficult%2520to%2520be%2520deployed%250Alocally%2520on%2520end-users%2527%2520devices%252C%2520raising%2520huge%2520concerns%2520about%2520user%2520privacy%2520and%250Acentralized%2520serving%2520cost.%2520Inspired%2520by%2520the%2520remarkable%2520coding%2520abilities%2520of%2520recent%250Asmall%2520language%2520models%2520%2528SLMs%2529%252C%2520we%2520propose%2520to%2520convert%2520the%2520UI%2520task%2520automation%250Aproblem%2520to%2520a%2520code%2520generation%2520problem%252C%2520which%2520can%2520be%2520effectively%2520solved%2520by%2520an%250Aon-device%2520SLM%2520and%2520efficiently%2520executed%2520with%2520an%2520on-device%2520code%2520interpreter.%250AUnlike%2520normal%2520coding%2520tasks%2520that%2520can%2520be%2520extensively%2520pre-trained%2520with%2520public%250Adatasets%252C%2520generating%2520UI%2520automation%2520code%2520is%2520challenging%2520due%2520to%2520the%2520diversity%252C%250Acomplexity%252C%2520and%2520variability%2520of%2520target%2520apps.%2520Therefore%252C%2520we%2520adopt%2520a%250Adocument-centered%2520approach%2520that%2520automatically%2520builds%2520fine-grained%2520API%250Adocumentation%2520for%2520each%2520app%2520and%2520generates%2520diverse%2520task%2520samples%2520based%2520on%2520this%250Adocumentation.%2520By%2520guiding%2520the%2520agent%2520with%2520the%2520synthetic%2520documents%2520and%2520task%250Asamples%252C%2520it%2520learns%2520to%2520generate%2520precise%2520and%2520efficient%2520scripts%2520to%2520complete%2520unseen%250Atasks.%2520Based%2520on%2520detailed%2520comparisons%2520with%2520state-of-the-art%2520mobile%2520UI%2520agents%252C%250Aour%2520approach%2520effectively%2520improves%2520the%2520mobile%2520task%2520automation%2520with%2520significantly%250Ahigher%2520success%2520rates%2520and%2520lower%2520latency/token%2520consumption.%2520Code%2520is%2520open-sourced%250Aat%2520https%253A//github.com/MobileLLM/AutoDroid-V2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18116v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoDroid-V2%3A%20Boosting%20SLM-based%20GUI%20Agents%20via%20Code%20Generation&entry.906535625=Hao%20Wen%20and%20Shizuo%20Tian%20and%20Borislav%20Pavlov%20and%20Wenjie%20Du%20and%20Yixuan%20Li%20and%20Ge%20Chang%20and%20Shanhui%20Zhao%20and%20Jiacheng%20Liu%20and%20Yunxin%20Liu%20and%20Ya-Qin%20Zhang%20and%20Yuanchun%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20brought%20exciting%20new%20advances%20to%20mobile%20UI%0Aagents%2C%20a%20long-standing%20research%20field%20that%20aims%20to%20complete%20arbitrary%20natural%0Alanguage%20tasks%20through%20mobile%20UI%20interactions.%20However%2C%20existing%20UI%20agents%0Ausually%20demand%20powerful%20large%20language%20models%20that%20are%20difficult%20to%20be%20deployed%0Alocally%20on%20end-users%27%20devices%2C%20raising%20huge%20concerns%20about%20user%20privacy%20and%0Acentralized%20serving%20cost.%20Inspired%20by%20the%20remarkable%20coding%20abilities%20of%20recent%0Asmall%20language%20models%20%28SLMs%29%2C%20we%20propose%20to%20convert%20the%20UI%20task%20automation%0Aproblem%20to%20a%20code%20generation%20problem%2C%20which%20can%20be%20effectively%20solved%20by%20an%0Aon-device%20SLM%20and%20efficiently%20executed%20with%20an%20on-device%20code%20interpreter.%0AUnlike%20normal%20coding%20tasks%20that%20can%20be%20extensively%20pre-trained%20with%20public%0Adatasets%2C%20generating%20UI%20automation%20code%20is%20challenging%20due%20to%20the%20diversity%2C%0Acomplexity%2C%20and%20variability%20of%20target%20apps.%20Therefore%2C%20we%20adopt%20a%0Adocument-centered%20approach%20that%20automatically%20builds%20fine-grained%20API%0Adocumentation%20for%20each%20app%20and%20generates%20diverse%20task%20samples%20based%20on%20this%0Adocumentation.%20By%20guiding%20the%20agent%20with%20the%20synthetic%20documents%20and%20task%0Asamples%2C%20it%20learns%20to%20generate%20precise%20and%20efficient%20scripts%20to%20complete%20unseen%0Atasks.%20Based%20on%20detailed%20comparisons%20with%20state-of-the-art%20mobile%20UI%20agents%2C%0Aour%20approach%20effectively%20improves%20the%20mobile%20task%20automation%20with%20significantly%0Ahigher%20success%20rates%20and%20lower%20latency/token%20consumption.%20Code%20is%20open-sourced%0Aat%20https%3A//github.com/MobileLLM/AutoDroid-V2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18116v3&entry.124074799=Read"},
{"title": "Frenet Corridor Planner: An Optimal Local Path Planning Framework for\n  Autonomous Driving", "author": "Faizan M. Tariq and Zheng-Hang Yeh and Avinash Singh and David Isele and Sangjae Bae", "abstract": "  Motivated by the requirements for effectiveness and efficiency, path-speed\ndecomposition-based trajectory planning methods have widely been adopted for\nautonomous driving applications. While a global route can be pre-computed\noffline, real-time generation of adaptive local paths remains crucial.\nTherefore, we present the Frenet Corridor Planner (FCP), an optimization-based\nlocal path planning strategy for autonomous driving that ensures smooth and\nsafe navigation around obstacles. Modeling the vehicles as safety-augmented\nbounding boxes and pedestrians as convex hulls in the Frenet space, our\napproach defines a drivable corridor by determining the appropriate deviation\nside for static obstacles. Thereafter, a modified space-domain bicycle\nkinematics model enables path optimization for smoothness, boundary clearance,\nand dynamic obstacle risk minimization. The optimized path is then passed to a\nspeed planner to generate the final trajectory. We validate FCP through\nextensive simulations and real-world hardware experiments, demonstrating its\nefficiency and effectiveness.\n", "link": "http://arxiv.org/abs/2505.03695v1", "date": "2025-05-06", "relevancy": 2.0434, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5214}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frenet%20Corridor%20Planner%3A%20An%20Optimal%20Local%20Path%20Planning%20Framework%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20Frenet%20Corridor%20Planner%3A%20An%20Optimal%20Local%20Path%20Planning%20Framework%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Faizan%20M.%20Tariq%20and%20Zheng-Hang%20Yeh%20and%20Avinash%20Singh%20and%20David%20Isele%20and%20Sangjae%20Bae%0AAbstract%3A%20%20%20Motivated%20by%20the%20requirements%20for%20effectiveness%20and%20efficiency%2C%20path-speed%0Adecomposition-based%20trajectory%20planning%20methods%20have%20widely%20been%20adopted%20for%0Aautonomous%20driving%20applications.%20While%20a%20global%20route%20can%20be%20pre-computed%0Aoffline%2C%20real-time%20generation%20of%20adaptive%20local%20paths%20remains%20crucial.%0ATherefore%2C%20we%20present%20the%20Frenet%20Corridor%20Planner%20%28FCP%29%2C%20an%20optimization-based%0Alocal%20path%20planning%20strategy%20for%20autonomous%20driving%20that%20ensures%20smooth%20and%0Asafe%20navigation%20around%20obstacles.%20Modeling%20the%20vehicles%20as%20safety-augmented%0Abounding%20boxes%20and%20pedestrians%20as%20convex%20hulls%20in%20the%20Frenet%20space%2C%20our%0Aapproach%20defines%20a%20drivable%20corridor%20by%20determining%20the%20appropriate%20deviation%0Aside%20for%20static%20obstacles.%20Thereafter%2C%20a%20modified%20space-domain%20bicycle%0Akinematics%20model%20enables%20path%20optimization%20for%20smoothness%2C%20boundary%20clearance%2C%0Aand%20dynamic%20obstacle%20risk%20minimization.%20The%20optimized%20path%20is%20then%20passed%20to%20a%0Aspeed%20planner%20to%20generate%20the%20final%20trajectory.%20We%20validate%20FCP%20through%0Aextensive%20simulations%20and%20real-world%20hardware%20experiments%2C%20demonstrating%20its%0Aefficiency%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrenet%2520Corridor%2520Planner%253A%2520An%2520Optimal%2520Local%2520Path%2520Planning%2520Framework%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DFaizan%2520M.%2520Tariq%2520and%2520Zheng-Hang%2520Yeh%2520and%2520Avinash%2520Singh%2520and%2520David%2520Isele%2520and%2520Sangjae%2520Bae%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520requirements%2520for%2520effectiveness%2520and%2520efficiency%252C%2520path-speed%250Adecomposition-based%2520trajectory%2520planning%2520methods%2520have%2520widely%2520been%2520adopted%2520for%250Aautonomous%2520driving%2520applications.%2520While%2520a%2520global%2520route%2520can%2520be%2520pre-computed%250Aoffline%252C%2520real-time%2520generation%2520of%2520adaptive%2520local%2520paths%2520remains%2520crucial.%250ATherefore%252C%2520we%2520present%2520the%2520Frenet%2520Corridor%2520Planner%2520%2528FCP%2529%252C%2520an%2520optimization-based%250Alocal%2520path%2520planning%2520strategy%2520for%2520autonomous%2520driving%2520that%2520ensures%2520smooth%2520and%250Asafe%2520navigation%2520around%2520obstacles.%2520Modeling%2520the%2520vehicles%2520as%2520safety-augmented%250Abounding%2520boxes%2520and%2520pedestrians%2520as%2520convex%2520hulls%2520in%2520the%2520Frenet%2520space%252C%2520our%250Aapproach%2520defines%2520a%2520drivable%2520corridor%2520by%2520determining%2520the%2520appropriate%2520deviation%250Aside%2520for%2520static%2520obstacles.%2520Thereafter%252C%2520a%2520modified%2520space-domain%2520bicycle%250Akinematics%2520model%2520enables%2520path%2520optimization%2520for%2520smoothness%252C%2520boundary%2520clearance%252C%250Aand%2520dynamic%2520obstacle%2520risk%2520minimization.%2520The%2520optimized%2520path%2520is%2520then%2520passed%2520to%2520a%250Aspeed%2520planner%2520to%2520generate%2520the%2520final%2520trajectory.%2520We%2520validate%2520FCP%2520through%250Aextensive%2520simulations%2520and%2520real-world%2520hardware%2520experiments%252C%2520demonstrating%2520its%250Aefficiency%2520and%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frenet%20Corridor%20Planner%3A%20An%20Optimal%20Local%20Path%20Planning%20Framework%20for%0A%20%20Autonomous%20Driving&entry.906535625=Faizan%20M.%20Tariq%20and%20Zheng-Hang%20Yeh%20and%20Avinash%20Singh%20and%20David%20Isele%20and%20Sangjae%20Bae&entry.1292438233=%20%20Motivated%20by%20the%20requirements%20for%20effectiveness%20and%20efficiency%2C%20path-speed%0Adecomposition-based%20trajectory%20planning%20methods%20have%20widely%20been%20adopted%20for%0Aautonomous%20driving%20applications.%20While%20a%20global%20route%20can%20be%20pre-computed%0Aoffline%2C%20real-time%20generation%20of%20adaptive%20local%20paths%20remains%20crucial.%0ATherefore%2C%20we%20present%20the%20Frenet%20Corridor%20Planner%20%28FCP%29%2C%20an%20optimization-based%0Alocal%20path%20planning%20strategy%20for%20autonomous%20driving%20that%20ensures%20smooth%20and%0Asafe%20navigation%20around%20obstacles.%20Modeling%20the%20vehicles%20as%20safety-augmented%0Abounding%20boxes%20and%20pedestrians%20as%20convex%20hulls%20in%20the%20Frenet%20space%2C%20our%0Aapproach%20defines%20a%20drivable%20corridor%20by%20determining%20the%20appropriate%20deviation%0Aside%20for%20static%20obstacles.%20Thereafter%2C%20a%20modified%20space-domain%20bicycle%0Akinematics%20model%20enables%20path%20optimization%20for%20smoothness%2C%20boundary%20clearance%2C%0Aand%20dynamic%20obstacle%20risk%20minimization.%20The%20optimized%20path%20is%20then%20passed%20to%20a%0Aspeed%20planner%20to%20generate%20the%20final%20trajectory.%20We%20validate%20FCP%20through%0Aextensive%20simulations%20and%20real-world%20hardware%20experiments%2C%20demonstrating%20its%0Aefficiency%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03695v1&entry.124074799=Read"},
{"title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based\n  Image Editing", "author": "Jinbin Bai and Wei Chow and Ling Yang and Xiangtai Li and Juncheng Li and Hanwang Zhang and Shuicheng Yan", "abstract": "  We present HumanEdit, a high-quality, human-rewarded dataset specifically\ndesigned for instruction-guided image editing, enabling precise and diverse\nimage manipulations through open-form language instructions. Previous\nlarge-scale editing datasets often incorporate minimal human feedback, leading\nto challenges in aligning datasets with human preferences. HumanEdit bridges\nthis gap by employing human annotators to construct data pairs and\nadministrators to provide feedback. With meticulously curation, HumanEdit\ncomprises 5,751 images and requires more than 2,500 hours of human effort\nacross four stages, ensuring both accuracy and reliability for a wide range of\nimage editing tasks. The dataset includes six distinct types of editing\ninstructions: Action, Add, Counting, Relation, Remove, and Replace,\nencompassing a broad spectrum of real-world scenarios. All images in the\ndataset are accompanied by masks, and for a subset of the data, we ensure that\nthe instructions are sufficiently detailed to support mask-free editing.\nFurthermore, HumanEdit offers comprehensive diversity and high-resolution $1024\n\\times 1024$ content sourced from various domains, setting a new versatile\nbenchmark for instructional image editing datasets. With the aim of advancing\nfuture research and establishing evaluation benchmarks in the field of image\nediting, we release HumanEdit at\nhttps://huggingface.co/datasets/BryanW/HumanEdit.\n", "link": "http://arxiv.org/abs/2412.04280v2", "date": "2025-05-06", "relevancy": 2.0385, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5527}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5066}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanEdit%3A%20A%20High-Quality%20Human-Rewarded%20Dataset%20for%20Instruction-based%0A%20%20Image%20Editing&body=Title%3A%20HumanEdit%3A%20A%20High-Quality%20Human-Rewarded%20Dataset%20for%20Instruction-based%0A%20%20Image%20Editing%0AAuthor%3A%20Jinbin%20Bai%20and%20Wei%20Chow%20and%20Ling%20Yang%20and%20Xiangtai%20Li%20and%20Juncheng%20Li%20and%20Hanwang%20Zhang%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20We%20present%20HumanEdit%2C%20a%20high-quality%2C%20human-rewarded%20dataset%20specifically%0Adesigned%20for%20instruction-guided%20image%20editing%2C%20enabling%20precise%20and%20diverse%0Aimage%20manipulations%20through%20open-form%20language%20instructions.%20Previous%0Alarge-scale%20editing%20datasets%20often%20incorporate%20minimal%20human%20feedback%2C%20leading%0Ato%20challenges%20in%20aligning%20datasets%20with%20human%20preferences.%20HumanEdit%20bridges%0Athis%20gap%20by%20employing%20human%20annotators%20to%20construct%20data%20pairs%20and%0Aadministrators%20to%20provide%20feedback.%20With%20meticulously%20curation%2C%20HumanEdit%0Acomprises%205%2C751%20images%20and%20requires%20more%20than%202%2C500%20hours%20of%20human%20effort%0Aacross%20four%20stages%2C%20ensuring%20both%20accuracy%20and%20reliability%20for%20a%20wide%20range%20of%0Aimage%20editing%20tasks.%20The%20dataset%20includes%20six%20distinct%20types%20of%20editing%0Ainstructions%3A%20Action%2C%20Add%2C%20Counting%2C%20Relation%2C%20Remove%2C%20and%20Replace%2C%0Aencompassing%20a%20broad%20spectrum%20of%20real-world%20scenarios.%20All%20images%20in%20the%0Adataset%20are%20accompanied%20by%20masks%2C%20and%20for%20a%20subset%20of%20the%20data%2C%20we%20ensure%20that%0Athe%20instructions%20are%20sufficiently%20detailed%20to%20support%20mask-free%20editing.%0AFurthermore%2C%20HumanEdit%20offers%20comprehensive%20diversity%20and%20high-resolution%20%241024%0A%5Ctimes%201024%24%20content%20sourced%20from%20various%20domains%2C%20setting%20a%20new%20versatile%0Abenchmark%20for%20instructional%20image%20editing%20datasets.%20With%20the%20aim%20of%20advancing%0Afuture%20research%20and%20establishing%20evaluation%20benchmarks%20in%20the%20field%20of%20image%0Aediting%2C%20we%20release%20HumanEdit%20at%0Ahttps%3A//huggingface.co/datasets/BryanW/HumanEdit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04280v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanEdit%253A%2520A%2520High-Quality%2520Human-Rewarded%2520Dataset%2520for%2520Instruction-based%250A%2520%2520Image%2520Editing%26entry.906535625%3DJinbin%2520Bai%2520and%2520Wei%2520Chow%2520and%2520Ling%2520Yang%2520and%2520Xiangtai%2520Li%2520and%2520Juncheng%2520Li%2520and%2520Hanwang%2520Zhang%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520We%2520present%2520HumanEdit%252C%2520a%2520high-quality%252C%2520human-rewarded%2520dataset%2520specifically%250Adesigned%2520for%2520instruction-guided%2520image%2520editing%252C%2520enabling%2520precise%2520and%2520diverse%250Aimage%2520manipulations%2520through%2520open-form%2520language%2520instructions.%2520Previous%250Alarge-scale%2520editing%2520datasets%2520often%2520incorporate%2520minimal%2520human%2520feedback%252C%2520leading%250Ato%2520challenges%2520in%2520aligning%2520datasets%2520with%2520human%2520preferences.%2520HumanEdit%2520bridges%250Athis%2520gap%2520by%2520employing%2520human%2520annotators%2520to%2520construct%2520data%2520pairs%2520and%250Aadministrators%2520to%2520provide%2520feedback.%2520With%2520meticulously%2520curation%252C%2520HumanEdit%250Acomprises%25205%252C751%2520images%2520and%2520requires%2520more%2520than%25202%252C500%2520hours%2520of%2520human%2520effort%250Aacross%2520four%2520stages%252C%2520ensuring%2520both%2520accuracy%2520and%2520reliability%2520for%2520a%2520wide%2520range%2520of%250Aimage%2520editing%2520tasks.%2520The%2520dataset%2520includes%2520six%2520distinct%2520types%2520of%2520editing%250Ainstructions%253A%2520Action%252C%2520Add%252C%2520Counting%252C%2520Relation%252C%2520Remove%252C%2520and%2520Replace%252C%250Aencompassing%2520a%2520broad%2520spectrum%2520of%2520real-world%2520scenarios.%2520All%2520images%2520in%2520the%250Adataset%2520are%2520accompanied%2520by%2520masks%252C%2520and%2520for%2520a%2520subset%2520of%2520the%2520data%252C%2520we%2520ensure%2520that%250Athe%2520instructions%2520are%2520sufficiently%2520detailed%2520to%2520support%2520mask-free%2520editing.%250AFurthermore%252C%2520HumanEdit%2520offers%2520comprehensive%2520diversity%2520and%2520high-resolution%2520%25241024%250A%255Ctimes%25201024%2524%2520content%2520sourced%2520from%2520various%2520domains%252C%2520setting%2520a%2520new%2520versatile%250Abenchmark%2520for%2520instructional%2520image%2520editing%2520datasets.%2520With%2520the%2520aim%2520of%2520advancing%250Afuture%2520research%2520and%2520establishing%2520evaluation%2520benchmarks%2520in%2520the%2520field%2520of%2520image%250Aediting%252C%2520we%2520release%2520HumanEdit%2520at%250Ahttps%253A//huggingface.co/datasets/BryanW/HumanEdit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04280v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanEdit%3A%20A%20High-Quality%20Human-Rewarded%20Dataset%20for%20Instruction-based%0A%20%20Image%20Editing&entry.906535625=Jinbin%20Bai%20and%20Wei%20Chow%20and%20Ling%20Yang%20and%20Xiangtai%20Li%20and%20Juncheng%20Li%20and%20Hanwang%20Zhang%20and%20Shuicheng%20Yan&entry.1292438233=%20%20We%20present%20HumanEdit%2C%20a%20high-quality%2C%20human-rewarded%20dataset%20specifically%0Adesigned%20for%20instruction-guided%20image%20editing%2C%20enabling%20precise%20and%20diverse%0Aimage%20manipulations%20through%20open-form%20language%20instructions.%20Previous%0Alarge-scale%20editing%20datasets%20often%20incorporate%20minimal%20human%20feedback%2C%20leading%0Ato%20challenges%20in%20aligning%20datasets%20with%20human%20preferences.%20HumanEdit%20bridges%0Athis%20gap%20by%20employing%20human%20annotators%20to%20construct%20data%20pairs%20and%0Aadministrators%20to%20provide%20feedback.%20With%20meticulously%20curation%2C%20HumanEdit%0Acomprises%205%2C751%20images%20and%20requires%20more%20than%202%2C500%20hours%20of%20human%20effort%0Aacross%20four%20stages%2C%20ensuring%20both%20accuracy%20and%20reliability%20for%20a%20wide%20range%20of%0Aimage%20editing%20tasks.%20The%20dataset%20includes%20six%20distinct%20types%20of%20editing%0Ainstructions%3A%20Action%2C%20Add%2C%20Counting%2C%20Relation%2C%20Remove%2C%20and%20Replace%2C%0Aencompassing%20a%20broad%20spectrum%20of%20real-world%20scenarios.%20All%20images%20in%20the%0Adataset%20are%20accompanied%20by%20masks%2C%20and%20for%20a%20subset%20of%20the%20data%2C%20we%20ensure%20that%0Athe%20instructions%20are%20sufficiently%20detailed%20to%20support%20mask-free%20editing.%0AFurthermore%2C%20HumanEdit%20offers%20comprehensive%20diversity%20and%20high-resolution%20%241024%0A%5Ctimes%201024%24%20content%20sourced%20from%20various%20domains%2C%20setting%20a%20new%20versatile%0Abenchmark%20for%20instructional%20image%20editing%20datasets.%20With%20the%20aim%20of%20advancing%0Afuture%20research%20and%20establishing%20evaluation%20benchmarks%20in%20the%20field%20of%20image%0Aediting%2C%20we%20release%20HumanEdit%20at%0Ahttps%3A//huggingface.co/datasets/BryanW/HumanEdit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04280v2&entry.124074799=Read"},
{"title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval\n  Augmented Generation", "author": "Matan Orbach and Ohad Eytan and Benjamin Sznajder and Ariel Gera and Odellia Boni and Yoav Kantor and Gal Bloch and Omri Levy and Hadas Abraham and Nitzan Barzilay and Eyal Shnarch and Michael E. Factor and Shila Ofek-Koifman and Paula Ta-Shma and Assaf Toledo", "abstract": "  Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order.\n", "link": "http://arxiv.org/abs/2505.03452v1", "date": "2025-05-06", "relevancy": 2.0297, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5179}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5121}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Analysis%20of%20Hyper-Parameter%20Optimization%20Methods%20for%20Retrieval%0A%20%20Augmented%20Generation&body=Title%3A%20An%20Analysis%20of%20Hyper-Parameter%20Optimization%20Methods%20for%20Retrieval%0A%20%20Augmented%20Generation%0AAuthor%3A%20Matan%20Orbach%20and%20Ohad%20Eytan%20and%20Benjamin%20Sznajder%20and%20Ariel%20Gera%20and%20Odellia%20Boni%20and%20Yoav%20Kantor%20and%20Gal%20Bloch%20and%20Omri%20Levy%20and%20Hadas%20Abraham%20and%20Nitzan%20Barzilay%20and%20Eyal%20Shnarch%20and%20Michael%20E.%20Factor%20and%20Shila%20Ofek-Koifman%20and%20Paula%20Ta-Shma%20and%20Assaf%20Toledo%0AAbstract%3A%20%20%20Finding%20the%20optimal%20Retrieval-Augmented%20Generation%20%28RAG%29%20configuration%20for%20a%0Agiven%20use%20case%20can%20be%20complex%20and%20expensive.%20Motivated%20by%20this%20challenge%2C%0Aframeworks%20for%20RAG%20hyper-parameter%20optimization%20%28HPO%29%20have%20recently%20emerged%2C%0Ayet%20their%20effectiveness%20has%20not%20been%20rigorously%20benchmarked.%20To%20address%20this%0Agap%2C%20we%20present%20a%20comprehensive%20study%20involving%205%20HPO%20algorithms%20over%205%0Adatasets%20from%20diverse%20domains%2C%20including%20a%20new%20one%20collected%20for%20this%20work%20on%0Areal-world%20product%20documentation.%20Our%20study%20explores%20the%20largest%20HPO%20search%0Aspace%20considered%20to%20date%2C%20with%20two%20optimized%20evaluation%20metrics.%20Analysis%20of%0Athe%20results%20shows%20that%20RAG%20HPO%20can%20be%20done%20efficiently%2C%20either%20greedily%20or%20with%0Aiterative%20random%20search%2C%20and%20that%20it%20significantly%20boosts%20RAG%20performance%20for%0Aall%20datasets.%20For%20greedy%20HPO%20approaches%2C%20we%20show%20that%20optimizing%20models%20first%0Ais%20preferable%20to%20the%20prevalent%20practice%20of%20optimizing%20sequentially%20according%20to%0Athe%20RAG%20pipeline%20order.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Analysis%2520of%2520Hyper-Parameter%2520Optimization%2520Methods%2520for%2520Retrieval%250A%2520%2520Augmented%2520Generation%26entry.906535625%3DMatan%2520Orbach%2520and%2520Ohad%2520Eytan%2520and%2520Benjamin%2520Sznajder%2520and%2520Ariel%2520Gera%2520and%2520Odellia%2520Boni%2520and%2520Yoav%2520Kantor%2520and%2520Gal%2520Bloch%2520and%2520Omri%2520Levy%2520and%2520Hadas%2520Abraham%2520and%2520Nitzan%2520Barzilay%2520and%2520Eyal%2520Shnarch%2520and%2520Michael%2520E.%2520Factor%2520and%2520Shila%2520Ofek-Koifman%2520and%2520Paula%2520Ta-Shma%2520and%2520Assaf%2520Toledo%26entry.1292438233%3D%2520%2520Finding%2520the%2520optimal%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520configuration%2520for%2520a%250Agiven%2520use%2520case%2520can%2520be%2520complex%2520and%2520expensive.%2520Motivated%2520by%2520this%2520challenge%252C%250Aframeworks%2520for%2520RAG%2520hyper-parameter%2520optimization%2520%2528HPO%2529%2520have%2520recently%2520emerged%252C%250Ayet%2520their%2520effectiveness%2520has%2520not%2520been%2520rigorously%2520benchmarked.%2520To%2520address%2520this%250Agap%252C%2520we%2520present%2520a%2520comprehensive%2520study%2520involving%25205%2520HPO%2520algorithms%2520over%25205%250Adatasets%2520from%2520diverse%2520domains%252C%2520including%2520a%2520new%2520one%2520collected%2520for%2520this%2520work%2520on%250Areal-world%2520product%2520documentation.%2520Our%2520study%2520explores%2520the%2520largest%2520HPO%2520search%250Aspace%2520considered%2520to%2520date%252C%2520with%2520two%2520optimized%2520evaluation%2520metrics.%2520Analysis%2520of%250Athe%2520results%2520shows%2520that%2520RAG%2520HPO%2520can%2520be%2520done%2520efficiently%252C%2520either%2520greedily%2520or%2520with%250Aiterative%2520random%2520search%252C%2520and%2520that%2520it%2520significantly%2520boosts%2520RAG%2520performance%2520for%250Aall%2520datasets.%2520For%2520greedy%2520HPO%2520approaches%252C%2520we%2520show%2520that%2520optimizing%2520models%2520first%250Ais%2520preferable%2520to%2520the%2520prevalent%2520practice%2520of%2520optimizing%2520sequentially%2520according%2520to%250Athe%2520RAG%2520pipeline%2520order.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Analysis%20of%20Hyper-Parameter%20Optimization%20Methods%20for%20Retrieval%0A%20%20Augmented%20Generation&entry.906535625=Matan%20Orbach%20and%20Ohad%20Eytan%20and%20Benjamin%20Sznajder%20and%20Ariel%20Gera%20and%20Odellia%20Boni%20and%20Yoav%20Kantor%20and%20Gal%20Bloch%20and%20Omri%20Levy%20and%20Hadas%20Abraham%20and%20Nitzan%20Barzilay%20and%20Eyal%20Shnarch%20and%20Michael%20E.%20Factor%20and%20Shila%20Ofek-Koifman%20and%20Paula%20Ta-Shma%20and%20Assaf%20Toledo&entry.1292438233=%20%20Finding%20the%20optimal%20Retrieval-Augmented%20Generation%20%28RAG%29%20configuration%20for%20a%0Agiven%20use%20case%20can%20be%20complex%20and%20expensive.%20Motivated%20by%20this%20challenge%2C%0Aframeworks%20for%20RAG%20hyper-parameter%20optimization%20%28HPO%29%20have%20recently%20emerged%2C%0Ayet%20their%20effectiveness%20has%20not%20been%20rigorously%20benchmarked.%20To%20address%20this%0Agap%2C%20we%20present%20a%20comprehensive%20study%20involving%205%20HPO%20algorithms%20over%205%0Adatasets%20from%20diverse%20domains%2C%20including%20a%20new%20one%20collected%20for%20this%20work%20on%0Areal-world%20product%20documentation.%20Our%20study%20explores%20the%20largest%20HPO%20search%0Aspace%20considered%20to%20date%2C%20with%20two%20optimized%20evaluation%20metrics.%20Analysis%20of%0Athe%20results%20shows%20that%20RAG%20HPO%20can%20be%20done%20efficiently%2C%20either%20greedily%20or%20with%0Aiterative%20random%20search%2C%20and%20that%20it%20significantly%20boosts%20RAG%20performance%20for%0Aall%20datasets.%20For%20greedy%20HPO%20approaches%2C%20we%20show%20that%20optimizing%20models%20first%0Ais%20preferable%20to%20the%20prevalent%20practice%20of%20optimizing%20sequentially%20according%20to%0Athe%20RAG%20pipeline%20order.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03452v1&entry.124074799=Read"},
{"title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model\n  Reasoning", "author": "Kolawole E. Ogunsina and Morayo A. Ogunsina", "abstract": "  Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.\n", "link": "http://arxiv.org/abs/2505.03553v1", "date": "2025-05-06", "relevancy": 2.0272, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hashgraph-Inspired%20Consensus%20Mechanism%20for%20Reliable%20Multi-Model%0A%20%20Reasoning&body=Title%3A%20A%20Hashgraph-Inspired%20Consensus%20Mechanism%20for%20Reliable%20Multi-Model%0A%20%20Reasoning%0AAuthor%3A%20Kolawole%20E.%20Ogunsina%20and%20Morayo%20A.%20Ogunsina%0AAbstract%3A%20%20%20Inconsistent%20outputs%20and%20hallucinations%20from%20large%20language%20models%20%28LLMs%29%20are%0Amajor%20obstacles%20to%20reliable%20AI%20systems.%20When%20different%20proprietary%20reasoning%0Amodels%20%28RMs%29%2C%20such%20as%20those%20by%20OpenAI%2C%20Google%2C%20Anthropic%2C%20DeepSeek%2C%20and%20xAI%2C%0Aare%20given%20the%20same%20complex%20request%2C%20they%20often%20produce%20divergent%20results%20due%20to%0Avariations%20in%20training%20and%20inference.%20This%20paper%20proposes%20a%20novel%20consensus%0Amechanism%2C%20inspired%20by%20distributed%20ledger%20technology%2C%20to%20validate%20and%20converge%0Athese%20outputs%2C%20treating%20each%20RM%20as%20a%20black-box%20peer.%20Building%20on%20the%20Hashgraph%0Aconsensus%20algorithm%2C%20our%20approach%20employs%20gossip-about-gossip%20communication%20and%0Avirtual%20voting%20to%20achieve%20agreement%20among%20an%20ensemble%20of%20RMs.%20We%20present%20an%0Aarchitectural%20design%20for%20a%20prototype%20system%20in%20which%20RMs%20iteratively%20exchange%0Aand%20update%20their%20answers%2C%20using%20information%20from%20each%20round%20to%20improve%20accuracy%0Aand%20confidence%20in%20subsequent%20rounds.%20This%20approach%20goes%20beyond%20simple%20majority%0Avoting%20by%20incorporating%20the%20knowledge%20and%20cross-verification%20content%20of%20every%0Amodel.%20We%20justify%20the%20feasibility%20of%20this%20Hashgraph-inspired%20consensus%20for%20AI%0Aensembles%20and%20outline%20its%20advantages%20over%20traditional%20ensembling%20techniques%20in%0Areducing%20nonfactual%20outputs.%20Preliminary%20considerations%20for%20implementation%2C%0Aevaluation%20criteria%20for%20convergence%20and%20accuracy%2C%20and%20potential%20challenges%20are%0Adiscussed.%20The%20proposed%20mechanism%20demonstrates%20a%20promising%20direction%20for%0Amulti-agent%20AI%20systems%20to%20self-validate%20and%20deliver%20high-fidelity%20responses%20in%0Acomplex%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hashgraph-Inspired%2520Consensus%2520Mechanism%2520for%2520Reliable%2520Multi-Model%250A%2520%2520Reasoning%26entry.906535625%3DKolawole%2520E.%2520Ogunsina%2520and%2520Morayo%2520A.%2520Ogunsina%26entry.1292438233%3D%2520%2520Inconsistent%2520outputs%2520and%2520hallucinations%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%250Amajor%2520obstacles%2520to%2520reliable%2520AI%2520systems.%2520When%2520different%2520proprietary%2520reasoning%250Amodels%2520%2528RMs%2529%252C%2520such%2520as%2520those%2520by%2520OpenAI%252C%2520Google%252C%2520Anthropic%252C%2520DeepSeek%252C%2520and%2520xAI%252C%250Aare%2520given%2520the%2520same%2520complex%2520request%252C%2520they%2520often%2520produce%2520divergent%2520results%2520due%2520to%250Avariations%2520in%2520training%2520and%2520inference.%2520This%2520paper%2520proposes%2520a%2520novel%2520consensus%250Amechanism%252C%2520inspired%2520by%2520distributed%2520ledger%2520technology%252C%2520to%2520validate%2520and%2520converge%250Athese%2520outputs%252C%2520treating%2520each%2520RM%2520as%2520a%2520black-box%2520peer.%2520Building%2520on%2520the%2520Hashgraph%250Aconsensus%2520algorithm%252C%2520our%2520approach%2520employs%2520gossip-about-gossip%2520communication%2520and%250Avirtual%2520voting%2520to%2520achieve%2520agreement%2520among%2520an%2520ensemble%2520of%2520RMs.%2520We%2520present%2520an%250Aarchitectural%2520design%2520for%2520a%2520prototype%2520system%2520in%2520which%2520RMs%2520iteratively%2520exchange%250Aand%2520update%2520their%2520answers%252C%2520using%2520information%2520from%2520each%2520round%2520to%2520improve%2520accuracy%250Aand%2520confidence%2520in%2520subsequent%2520rounds.%2520This%2520approach%2520goes%2520beyond%2520simple%2520majority%250Avoting%2520by%2520incorporating%2520the%2520knowledge%2520and%2520cross-verification%2520content%2520of%2520every%250Amodel.%2520We%2520justify%2520the%2520feasibility%2520of%2520this%2520Hashgraph-inspired%2520consensus%2520for%2520AI%250Aensembles%2520and%2520outline%2520its%2520advantages%2520over%2520traditional%2520ensembling%2520techniques%2520in%250Areducing%2520nonfactual%2520outputs.%2520Preliminary%2520considerations%2520for%2520implementation%252C%250Aevaluation%2520criteria%2520for%2520convergence%2520and%2520accuracy%252C%2520and%2520potential%2520challenges%2520are%250Adiscussed.%2520The%2520proposed%2520mechanism%2520demonstrates%2520a%2520promising%2520direction%2520for%250Amulti-agent%2520AI%2520systems%2520to%2520self-validate%2520and%2520deliver%2520high-fidelity%2520responses%2520in%250Acomplex%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hashgraph-Inspired%20Consensus%20Mechanism%20for%20Reliable%20Multi-Model%0A%20%20Reasoning&entry.906535625=Kolawole%20E.%20Ogunsina%20and%20Morayo%20A.%20Ogunsina&entry.1292438233=%20%20Inconsistent%20outputs%20and%20hallucinations%20from%20large%20language%20models%20%28LLMs%29%20are%0Amajor%20obstacles%20to%20reliable%20AI%20systems.%20When%20different%20proprietary%20reasoning%0Amodels%20%28RMs%29%2C%20such%20as%20those%20by%20OpenAI%2C%20Google%2C%20Anthropic%2C%20DeepSeek%2C%20and%20xAI%2C%0Aare%20given%20the%20same%20complex%20request%2C%20they%20often%20produce%20divergent%20results%20due%20to%0Avariations%20in%20training%20and%20inference.%20This%20paper%20proposes%20a%20novel%20consensus%0Amechanism%2C%20inspired%20by%20distributed%20ledger%20technology%2C%20to%20validate%20and%20converge%0Athese%20outputs%2C%20treating%20each%20RM%20as%20a%20black-box%20peer.%20Building%20on%20the%20Hashgraph%0Aconsensus%20algorithm%2C%20our%20approach%20employs%20gossip-about-gossip%20communication%20and%0Avirtual%20voting%20to%20achieve%20agreement%20among%20an%20ensemble%20of%20RMs.%20We%20present%20an%0Aarchitectural%20design%20for%20a%20prototype%20system%20in%20which%20RMs%20iteratively%20exchange%0Aand%20update%20their%20answers%2C%20using%20information%20from%20each%20round%20to%20improve%20accuracy%0Aand%20confidence%20in%20subsequent%20rounds.%20This%20approach%20goes%20beyond%20simple%20majority%0Avoting%20by%20incorporating%20the%20knowledge%20and%20cross-verification%20content%20of%20every%0Amodel.%20We%20justify%20the%20feasibility%20of%20this%20Hashgraph-inspired%20consensus%20for%20AI%0Aensembles%20and%20outline%20its%20advantages%20over%20traditional%20ensembling%20techniques%20in%0Areducing%20nonfactual%20outputs.%20Preliminary%20considerations%20for%20implementation%2C%0Aevaluation%20criteria%20for%20convergence%20and%20accuracy%2C%20and%20potential%20challenges%20are%0Adiscussed.%20The%20proposed%20mechanism%20demonstrates%20a%20promising%20direction%20for%0Amulti-agent%20AI%20systems%20to%20self-validate%20and%20deliver%20high-fidelity%20responses%20in%0Acomplex%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03553v1&entry.124074799=Read"},
{"title": "Rethinking Meta-Learning from a Learning Lens", "author": "Jingyao Wang and Wenwen Qiang and Changwen Zheng and Hui Xiong and Gang Hua", "abstract": "  Meta-learning seeks to learn a well-generalized model initialization from\ntraining tasks to solve unseen tasks. From the \"learning to learn\" perspective,\nthe quality of the initialization is modeled with one-step gradient decent in\nthe inner loop. However, contrary to theoretical expectations, our empirical\nanalysis reveals that this may expose meta-learning to underfitting. To bridge\nthe gap between theoretical understanding and practical implementation, we\nreconsider meta-learning from the \"Learning\" lens. We propose that the\nmeta-learning model comprises two interrelated components: parameters for model\ninitialization and a meta-layer for task-specific fine-tuning. These components\nwill lead to the risks of overfitting and underfitting depending on tasks, and\ntheir solutions, fewer parameters vs. more meta-layer, are often in conflict.\nTo address this, we aim to regulate the task information the model receives\nwithout modifying the data or model structure. Our theoretical analysis\nindicates that models adapted to different tasks can mutually reinforce each\nother, highlighting the effective information. Based on this insight, we\npropose TRLearner, a plug-and-play method that leverages task relation to\ncalibrate meta-learning. It first extracts task relation matrices and then\napplies relation-aware consistency regularization to guide optimization.\nExtensive theoretical and empirical evaluations demonstrate its effectiveness.\n", "link": "http://arxiv.org/abs/2409.08474v3", "date": "2025-05-06", "relevancy": 2.0224, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5017}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Meta-Learning%20from%20a%20Learning%20Lens&body=Title%3A%20Rethinking%20Meta-Learning%20from%20a%20Learning%20Lens%0AAuthor%3A%20Jingyao%20Wang%20and%20Wenwen%20Qiang%20and%20Changwen%20Zheng%20and%20Hui%20Xiong%20and%20Gang%20Hua%0AAbstract%3A%20%20%20Meta-learning%20seeks%20to%20learn%20a%20well-generalized%20model%20initialization%20from%0Atraining%20tasks%20to%20solve%20unseen%20tasks.%20From%20the%20%22learning%20to%20learn%22%20perspective%2C%0Athe%20quality%20of%20the%20initialization%20is%20modeled%20with%20one-step%20gradient%20decent%20in%0Athe%20inner%20loop.%20However%2C%20contrary%20to%20theoretical%20expectations%2C%20our%20empirical%0Aanalysis%20reveals%20that%20this%20may%20expose%20meta-learning%20to%20underfitting.%20To%20bridge%0Athe%20gap%20between%20theoretical%20understanding%20and%20practical%20implementation%2C%20we%0Areconsider%20meta-learning%20from%20the%20%22Learning%22%20lens.%20We%20propose%20that%20the%0Ameta-learning%20model%20comprises%20two%20interrelated%20components%3A%20parameters%20for%20model%0Ainitialization%20and%20a%20meta-layer%20for%20task-specific%20fine-tuning.%20These%20components%0Awill%20lead%20to%20the%20risks%20of%20overfitting%20and%20underfitting%20depending%20on%20tasks%2C%20and%0Atheir%20solutions%2C%20fewer%20parameters%20vs.%20more%20meta-layer%2C%20are%20often%20in%20conflict.%0ATo%20address%20this%2C%20we%20aim%20to%20regulate%20the%20task%20information%20the%20model%20receives%0Awithout%20modifying%20the%20data%20or%20model%20structure.%20Our%20theoretical%20analysis%0Aindicates%20that%20models%20adapted%20to%20different%20tasks%20can%20mutually%20reinforce%20each%0Aother%2C%20highlighting%20the%20effective%20information.%20Based%20on%20this%20insight%2C%20we%0Apropose%20TRLearner%2C%20a%20plug-and-play%20method%20that%20leverages%20task%20relation%20to%0Acalibrate%20meta-learning.%20It%20first%20extracts%20task%20relation%20matrices%20and%20then%0Aapplies%20relation-aware%20consistency%20regularization%20to%20guide%20optimization.%0AExtensive%20theoretical%20and%20empirical%20evaluations%20demonstrate%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08474v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Meta-Learning%2520from%2520a%2520Learning%2520Lens%26entry.906535625%3DJingyao%2520Wang%2520and%2520Wenwen%2520Qiang%2520and%2520Changwen%2520Zheng%2520and%2520Hui%2520Xiong%2520and%2520Gang%2520Hua%26entry.1292438233%3D%2520%2520Meta-learning%2520seeks%2520to%2520learn%2520a%2520well-generalized%2520model%2520initialization%2520from%250Atraining%2520tasks%2520to%2520solve%2520unseen%2520tasks.%2520From%2520the%2520%2522learning%2520to%2520learn%2522%2520perspective%252C%250Athe%2520quality%2520of%2520the%2520initialization%2520is%2520modeled%2520with%2520one-step%2520gradient%2520decent%2520in%250Athe%2520inner%2520loop.%2520However%252C%2520contrary%2520to%2520theoretical%2520expectations%252C%2520our%2520empirical%250Aanalysis%2520reveals%2520that%2520this%2520may%2520expose%2520meta-learning%2520to%2520underfitting.%2520To%2520bridge%250Athe%2520gap%2520between%2520theoretical%2520understanding%2520and%2520practical%2520implementation%252C%2520we%250Areconsider%2520meta-learning%2520from%2520the%2520%2522Learning%2522%2520lens.%2520We%2520propose%2520that%2520the%250Ameta-learning%2520model%2520comprises%2520two%2520interrelated%2520components%253A%2520parameters%2520for%2520model%250Ainitialization%2520and%2520a%2520meta-layer%2520for%2520task-specific%2520fine-tuning.%2520These%2520components%250Awill%2520lead%2520to%2520the%2520risks%2520of%2520overfitting%2520and%2520underfitting%2520depending%2520on%2520tasks%252C%2520and%250Atheir%2520solutions%252C%2520fewer%2520parameters%2520vs.%2520more%2520meta-layer%252C%2520are%2520often%2520in%2520conflict.%250ATo%2520address%2520this%252C%2520we%2520aim%2520to%2520regulate%2520the%2520task%2520information%2520the%2520model%2520receives%250Awithout%2520modifying%2520the%2520data%2520or%2520model%2520structure.%2520Our%2520theoretical%2520analysis%250Aindicates%2520that%2520models%2520adapted%2520to%2520different%2520tasks%2520can%2520mutually%2520reinforce%2520each%250Aother%252C%2520highlighting%2520the%2520effective%2520information.%2520Based%2520on%2520this%2520insight%252C%2520we%250Apropose%2520TRLearner%252C%2520a%2520plug-and-play%2520method%2520that%2520leverages%2520task%2520relation%2520to%250Acalibrate%2520meta-learning.%2520It%2520first%2520extracts%2520task%2520relation%2520matrices%2520and%2520then%250Aapplies%2520relation-aware%2520consistency%2520regularization%2520to%2520guide%2520optimization.%250AExtensive%2520theoretical%2520and%2520empirical%2520evaluations%2520demonstrate%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08474v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Meta-Learning%20from%20a%20Learning%20Lens&entry.906535625=Jingyao%20Wang%20and%20Wenwen%20Qiang%20and%20Changwen%20Zheng%20and%20Hui%20Xiong%20and%20Gang%20Hua&entry.1292438233=%20%20Meta-learning%20seeks%20to%20learn%20a%20well-generalized%20model%20initialization%20from%0Atraining%20tasks%20to%20solve%20unseen%20tasks.%20From%20the%20%22learning%20to%20learn%22%20perspective%2C%0Athe%20quality%20of%20the%20initialization%20is%20modeled%20with%20one-step%20gradient%20decent%20in%0Athe%20inner%20loop.%20However%2C%20contrary%20to%20theoretical%20expectations%2C%20our%20empirical%0Aanalysis%20reveals%20that%20this%20may%20expose%20meta-learning%20to%20underfitting.%20To%20bridge%0Athe%20gap%20between%20theoretical%20understanding%20and%20practical%20implementation%2C%20we%0Areconsider%20meta-learning%20from%20the%20%22Learning%22%20lens.%20We%20propose%20that%20the%0Ameta-learning%20model%20comprises%20two%20interrelated%20components%3A%20parameters%20for%20model%0Ainitialization%20and%20a%20meta-layer%20for%20task-specific%20fine-tuning.%20These%20components%0Awill%20lead%20to%20the%20risks%20of%20overfitting%20and%20underfitting%20depending%20on%20tasks%2C%20and%0Atheir%20solutions%2C%20fewer%20parameters%20vs.%20more%20meta-layer%2C%20are%20often%20in%20conflict.%0ATo%20address%20this%2C%20we%20aim%20to%20regulate%20the%20task%20information%20the%20model%20receives%0Awithout%20modifying%20the%20data%20or%20model%20structure.%20Our%20theoretical%20analysis%0Aindicates%20that%20models%20adapted%20to%20different%20tasks%20can%20mutually%20reinforce%20each%0Aother%2C%20highlighting%20the%20effective%20information.%20Based%20on%20this%20insight%2C%20we%0Apropose%20TRLearner%2C%20a%20plug-and-play%20method%20that%20leverages%20task%20relation%20to%0Acalibrate%20meta-learning.%20It%20first%20extracts%20task%20relation%20matrices%20and%20then%0Aapplies%20relation-aware%20consistency%20regularization%20to%20guide%20optimization.%0AExtensive%20theoretical%20and%20empirical%20evaluations%20demonstrate%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08474v3&entry.124074799=Read"},
{"title": "Concept Factorization via Self-Representation and Adaptive Graph\n  Structure Learning", "author": "Zhengqin Yang and Di Wu and Jia Chen and Xin Luo", "abstract": "  Concept Factorization (CF) models have attracted widespread attention due to\ntheir excellent performance in data clustering. In recent years, many variant\nmodels based on CF have achieved great success in clustering by taking into\naccount the internal geometric manifold structure of the dataset and using\ngraph regularization techniques. However, their clustering performance depends\ngreatly on the construction of the initial graph structure. In order to enable\nadaptive learning of the graph structure of the data, we propose a Concept\nFactorization Based on Self-Representation and Adaptive Graph Structure\nLearning (CFSRAG) Model. CFSRAG learns the affinity relationship between data\nthrough a self-representation method, and uses the learned affinity matrix to\nimplement dynamic graph regularization constraints, thereby ensuring dynamic\nlearning of the internal geometric structure of the data. Finally, we give the\nCFSRAG update rule and convergence analysis, and conduct comparative\nexperiments on four real datasets. The results show that our model outperforms\nother state-of-the-art models.\n", "link": "http://arxiv.org/abs/2505.03390v1", "date": "2025-05-06", "relevancy": 2.0143, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5339}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4931}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20Factorization%20via%20Self-Representation%20and%20Adaptive%20Graph%0A%20%20Structure%20Learning&body=Title%3A%20Concept%20Factorization%20via%20Self-Representation%20and%20Adaptive%20Graph%0A%20%20Structure%20Learning%0AAuthor%3A%20Zhengqin%20Yang%20and%20Di%20Wu%20and%20Jia%20Chen%20and%20Xin%20Luo%0AAbstract%3A%20%20%20Concept%20Factorization%20%28CF%29%20models%20have%20attracted%20widespread%20attention%20due%20to%0Atheir%20excellent%20performance%20in%20data%20clustering.%20In%20recent%20years%2C%20many%20variant%0Amodels%20based%20on%20CF%20have%20achieved%20great%20success%20in%20clustering%20by%20taking%20into%0Aaccount%20the%20internal%20geometric%20manifold%20structure%20of%20the%20dataset%20and%20using%0Agraph%20regularization%20techniques.%20However%2C%20their%20clustering%20performance%20depends%0Agreatly%20on%20the%20construction%20of%20the%20initial%20graph%20structure.%20In%20order%20to%20enable%0Aadaptive%20learning%20of%20the%20graph%20structure%20of%20the%20data%2C%20we%20propose%20a%20Concept%0AFactorization%20Based%20on%20Self-Representation%20and%20Adaptive%20Graph%20Structure%0ALearning%20%28CFSRAG%29%20Model.%20CFSRAG%20learns%20the%20affinity%20relationship%20between%20data%0Athrough%20a%20self-representation%20method%2C%20and%20uses%20the%20learned%20affinity%20matrix%20to%0Aimplement%20dynamic%20graph%20regularization%20constraints%2C%20thereby%20ensuring%20dynamic%0Alearning%20of%20the%20internal%20geometric%20structure%20of%20the%20data.%20Finally%2C%20we%20give%20the%0ACFSRAG%20update%20rule%20and%20convergence%20analysis%2C%20and%20conduct%20comparative%0Aexperiments%20on%20four%20real%20datasets.%20The%20results%20show%20that%20our%20model%20outperforms%0Aother%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520Factorization%2520via%2520Self-Representation%2520and%2520Adaptive%2520Graph%250A%2520%2520Structure%2520Learning%26entry.906535625%3DZhengqin%2520Yang%2520and%2520Di%2520Wu%2520and%2520Jia%2520Chen%2520and%2520Xin%2520Luo%26entry.1292438233%3D%2520%2520Concept%2520Factorization%2520%2528CF%2529%2520models%2520have%2520attracted%2520widespread%2520attention%2520due%2520to%250Atheir%2520excellent%2520performance%2520in%2520data%2520clustering.%2520In%2520recent%2520years%252C%2520many%2520variant%250Amodels%2520based%2520on%2520CF%2520have%2520achieved%2520great%2520success%2520in%2520clustering%2520by%2520taking%2520into%250Aaccount%2520the%2520internal%2520geometric%2520manifold%2520structure%2520of%2520the%2520dataset%2520and%2520using%250Agraph%2520regularization%2520techniques.%2520However%252C%2520their%2520clustering%2520performance%2520depends%250Agreatly%2520on%2520the%2520construction%2520of%2520the%2520initial%2520graph%2520structure.%2520In%2520order%2520to%2520enable%250Aadaptive%2520learning%2520of%2520the%2520graph%2520structure%2520of%2520the%2520data%252C%2520we%2520propose%2520a%2520Concept%250AFactorization%2520Based%2520on%2520Self-Representation%2520and%2520Adaptive%2520Graph%2520Structure%250ALearning%2520%2528CFSRAG%2529%2520Model.%2520CFSRAG%2520learns%2520the%2520affinity%2520relationship%2520between%2520data%250Athrough%2520a%2520self-representation%2520method%252C%2520and%2520uses%2520the%2520learned%2520affinity%2520matrix%2520to%250Aimplement%2520dynamic%2520graph%2520regularization%2520constraints%252C%2520thereby%2520ensuring%2520dynamic%250Alearning%2520of%2520the%2520internal%2520geometric%2520structure%2520of%2520the%2520data.%2520Finally%252C%2520we%2520give%2520the%250ACFSRAG%2520update%2520rule%2520and%2520convergence%2520analysis%252C%2520and%2520conduct%2520comparative%250Aexperiments%2520on%2520four%2520real%2520datasets.%2520The%2520results%2520show%2520that%2520our%2520model%2520outperforms%250Aother%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Factorization%20via%20Self-Representation%20and%20Adaptive%20Graph%0A%20%20Structure%20Learning&entry.906535625=Zhengqin%20Yang%20and%20Di%20Wu%20and%20Jia%20Chen%20and%20Xin%20Luo&entry.1292438233=%20%20Concept%20Factorization%20%28CF%29%20models%20have%20attracted%20widespread%20attention%20due%20to%0Atheir%20excellent%20performance%20in%20data%20clustering.%20In%20recent%20years%2C%20many%20variant%0Amodels%20based%20on%20CF%20have%20achieved%20great%20success%20in%20clustering%20by%20taking%20into%0Aaccount%20the%20internal%20geometric%20manifold%20structure%20of%20the%20dataset%20and%20using%0Agraph%20regularization%20techniques.%20However%2C%20their%20clustering%20performance%20depends%0Agreatly%20on%20the%20construction%20of%20the%20initial%20graph%20structure.%20In%20order%20to%20enable%0Aadaptive%20learning%20of%20the%20graph%20structure%20of%20the%20data%2C%20we%20propose%20a%20Concept%0AFactorization%20Based%20on%20Self-Representation%20and%20Adaptive%20Graph%20Structure%0ALearning%20%28CFSRAG%29%20Model.%20CFSRAG%20learns%20the%20affinity%20relationship%20between%20data%0Athrough%20a%20self-representation%20method%2C%20and%20uses%20the%20learned%20affinity%20matrix%20to%0Aimplement%20dynamic%20graph%20regularization%20constraints%2C%20thereby%20ensuring%20dynamic%0Alearning%20of%20the%20internal%20geometric%20structure%20of%20the%20data.%20Finally%2C%20we%20give%20the%0ACFSRAG%20update%20rule%20and%20convergence%20analysis%2C%20and%20conduct%20comparative%0Aexperiments%20on%20four%20real%20datasets.%20The%20results%20show%20that%20our%20model%20outperforms%0Aother%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03390v1&entry.124074799=Read"},
{"title": "J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities\n  Effectively in Inverse Kinematic Control of Serial Manipulators", "author": "Shivani Guptasarma and Matthew Strong and Honghao Zhen and Monroe Kennedy III", "abstract": "  J-PARSE is a method for smooth first-order inverse kinematic control of a\nserial manipulator near kinematic singularities. The commanded end-effector\nvelocity is interpreted component-wise, according to the available mobility in\neach dimension of the task space. First, a substitute \"Safety\" Jacobian matrix\nis created, keeping the aspect ratio of the manipulability ellipsoid above a\nthreshold value. The desired motion is then projected onto non-singular and\nsingular directions, and the latter projection scaled down by a factor informed\nby the threshold value. A right-inverse of the non-singular Safety Jacobian is\napplied to the modified command. In the absence of joint limits and collisions,\nthis ensures smooth transition into and out of low-rank poses, guaranteeing\nasymptotic stability for target poses within the workspace, and stability for\nthose outside. Velocity control with J-PARSE is benchmarked against the\nLeast-Squares and Damped Least-Squares inversions of the Jacobian, and shows\nhigh accuracy in reaching and leaving singular target poses. By expanding the\navailable workspace of manipulators, the method finds applications in servoing,\nteleoperation, and learning.\n", "link": "http://arxiv.org/abs/2505.00306v2", "date": "2025-05-06", "relevancy": 2.0041, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5138}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20J-PARSE%3A%20Jacobian-based%20Projection%20Algorithm%20for%20Resolving%20Singularities%0A%20%20Effectively%20in%20Inverse%20Kinematic%20Control%20of%20Serial%20Manipulators&body=Title%3A%20J-PARSE%3A%20Jacobian-based%20Projection%20Algorithm%20for%20Resolving%20Singularities%0A%20%20Effectively%20in%20Inverse%20Kinematic%20Control%20of%20Serial%20Manipulators%0AAuthor%3A%20Shivani%20Guptasarma%20and%20Matthew%20Strong%20and%20Honghao%20Zhen%20and%20Monroe%20Kennedy%20III%0AAbstract%3A%20%20%20J-PARSE%20is%20a%20method%20for%20smooth%20first-order%20inverse%20kinematic%20control%20of%20a%0Aserial%20manipulator%20near%20kinematic%20singularities.%20The%20commanded%20end-effector%0Avelocity%20is%20interpreted%20component-wise%2C%20according%20to%20the%20available%20mobility%20in%0Aeach%20dimension%20of%20the%20task%20space.%20First%2C%20a%20substitute%20%22Safety%22%20Jacobian%20matrix%0Ais%20created%2C%20keeping%20the%20aspect%20ratio%20of%20the%20manipulability%20ellipsoid%20above%20a%0Athreshold%20value.%20The%20desired%20motion%20is%20then%20projected%20onto%20non-singular%20and%0Asingular%20directions%2C%20and%20the%20latter%20projection%20scaled%20down%20by%20a%20factor%20informed%0Aby%20the%20threshold%20value.%20A%20right-inverse%20of%20the%20non-singular%20Safety%20Jacobian%20is%0Aapplied%20to%20the%20modified%20command.%20In%20the%20absence%20of%20joint%20limits%20and%20collisions%2C%0Athis%20ensures%20smooth%20transition%20into%20and%20out%20of%20low-rank%20poses%2C%20guaranteeing%0Aasymptotic%20stability%20for%20target%20poses%20within%20the%20workspace%2C%20and%20stability%20for%0Athose%20outside.%20Velocity%20control%20with%20J-PARSE%20is%20benchmarked%20against%20the%0ALeast-Squares%20and%20Damped%20Least-Squares%20inversions%20of%20the%20Jacobian%2C%20and%20shows%0Ahigh%20accuracy%20in%20reaching%20and%20leaving%20singular%20target%20poses.%20By%20expanding%20the%0Aavailable%20workspace%20of%20manipulators%2C%20the%20method%20finds%20applications%20in%20servoing%2C%0Ateleoperation%2C%20and%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJ-PARSE%253A%2520Jacobian-based%2520Projection%2520Algorithm%2520for%2520Resolving%2520Singularities%250A%2520%2520Effectively%2520in%2520Inverse%2520Kinematic%2520Control%2520of%2520Serial%2520Manipulators%26entry.906535625%3DShivani%2520Guptasarma%2520and%2520Matthew%2520Strong%2520and%2520Honghao%2520Zhen%2520and%2520Monroe%2520Kennedy%2520III%26entry.1292438233%3D%2520%2520J-PARSE%2520is%2520a%2520method%2520for%2520smooth%2520first-order%2520inverse%2520kinematic%2520control%2520of%2520a%250Aserial%2520manipulator%2520near%2520kinematic%2520singularities.%2520The%2520commanded%2520end-effector%250Avelocity%2520is%2520interpreted%2520component-wise%252C%2520according%2520to%2520the%2520available%2520mobility%2520in%250Aeach%2520dimension%2520of%2520the%2520task%2520space.%2520First%252C%2520a%2520substitute%2520%2522Safety%2522%2520Jacobian%2520matrix%250Ais%2520created%252C%2520keeping%2520the%2520aspect%2520ratio%2520of%2520the%2520manipulability%2520ellipsoid%2520above%2520a%250Athreshold%2520value.%2520The%2520desired%2520motion%2520is%2520then%2520projected%2520onto%2520non-singular%2520and%250Asingular%2520directions%252C%2520and%2520the%2520latter%2520projection%2520scaled%2520down%2520by%2520a%2520factor%2520informed%250Aby%2520the%2520threshold%2520value.%2520A%2520right-inverse%2520of%2520the%2520non-singular%2520Safety%2520Jacobian%2520is%250Aapplied%2520to%2520the%2520modified%2520command.%2520In%2520the%2520absence%2520of%2520joint%2520limits%2520and%2520collisions%252C%250Athis%2520ensures%2520smooth%2520transition%2520into%2520and%2520out%2520of%2520low-rank%2520poses%252C%2520guaranteeing%250Aasymptotic%2520stability%2520for%2520target%2520poses%2520within%2520the%2520workspace%252C%2520and%2520stability%2520for%250Athose%2520outside.%2520Velocity%2520control%2520with%2520J-PARSE%2520is%2520benchmarked%2520against%2520the%250ALeast-Squares%2520and%2520Damped%2520Least-Squares%2520inversions%2520of%2520the%2520Jacobian%252C%2520and%2520shows%250Ahigh%2520accuracy%2520in%2520reaching%2520and%2520leaving%2520singular%2520target%2520poses.%2520By%2520expanding%2520the%250Aavailable%2520workspace%2520of%2520manipulators%252C%2520the%2520method%2520finds%2520applications%2520in%2520servoing%252C%250Ateleoperation%252C%2520and%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=J-PARSE%3A%20Jacobian-based%20Projection%20Algorithm%20for%20Resolving%20Singularities%0A%20%20Effectively%20in%20Inverse%20Kinematic%20Control%20of%20Serial%20Manipulators&entry.906535625=Shivani%20Guptasarma%20and%20Matthew%20Strong%20and%20Honghao%20Zhen%20and%20Monroe%20Kennedy%20III&entry.1292438233=%20%20J-PARSE%20is%20a%20method%20for%20smooth%20first-order%20inverse%20kinematic%20control%20of%20a%0Aserial%20manipulator%20near%20kinematic%20singularities.%20The%20commanded%20end-effector%0Avelocity%20is%20interpreted%20component-wise%2C%20according%20to%20the%20available%20mobility%20in%0Aeach%20dimension%20of%20the%20task%20space.%20First%2C%20a%20substitute%20%22Safety%22%20Jacobian%20matrix%0Ais%20created%2C%20keeping%20the%20aspect%20ratio%20of%20the%20manipulability%20ellipsoid%20above%20a%0Athreshold%20value.%20The%20desired%20motion%20is%20then%20projected%20onto%20non-singular%20and%0Asingular%20directions%2C%20and%20the%20latter%20projection%20scaled%20down%20by%20a%20factor%20informed%0Aby%20the%20threshold%20value.%20A%20right-inverse%20of%20the%20non-singular%20Safety%20Jacobian%20is%0Aapplied%20to%20the%20modified%20command.%20In%20the%20absence%20of%20joint%20limits%20and%20collisions%2C%0Athis%20ensures%20smooth%20transition%20into%20and%20out%20of%20low-rank%20poses%2C%20guaranteeing%0Aasymptotic%20stability%20for%20target%20poses%20within%20the%20workspace%2C%20and%20stability%20for%0Athose%20outside.%20Velocity%20control%20with%20J-PARSE%20is%20benchmarked%20against%20the%0ALeast-Squares%20and%20Damped%20Least-Squares%20inversions%20of%20the%20Jacobian%2C%20and%20shows%0Ahigh%20accuracy%20in%20reaching%20and%20leaving%20singular%20target%20poses.%20By%20expanding%20the%0Aavailable%20workspace%20of%20manipulators%2C%20the%20method%20finds%20applications%20in%20servoing%2C%0Ateleoperation%2C%20and%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00306v2&entry.124074799=Read"},
{"title": "Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima", "author": "Richard Y. Zhang", "abstract": "  The classical low-rank matrix recovery problem is well-known to exhibit\n\\emph{benign nonconvexity} under the restricted isometry property (RIP): local\noptimization is guaranteed to converge to the global optimum, where the ground\ntruth is recovered. We investigate whether benign nonconvexity continues to\nhold when the factor matrices are constrained to be elementwise nonnegative --\na common practical requirement. In the simple setting of a rank-1 nonnegative\nground truth, we confirm that benign nonconvexity holds in the fully-observed\ncase with RIP constant $\\delta=0$. Surprisingly, however, this property fails\nto extend to the partially-observed case with any arbitrarily small RIP\nconstant $\\delta\\to0^{+}$, irrespective of rank overparameterization. This\nfinding exposes a critical theoretical gap: the continuity argument widely used\nto explain the empirical robustness of low-rank matrix recovery fundamentally\nbreaks down once nonnegative constraints are imposed.\n", "link": "http://arxiv.org/abs/2505.03717v1", "date": "2025-05-06", "relevancy": 1.9907, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.413}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4005}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonnegative%20Low-rank%20Matrix%20Recovery%20Can%20Have%20Spurious%20Local%20Minima&body=Title%3A%20Nonnegative%20Low-rank%20Matrix%20Recovery%20Can%20Have%20Spurious%20Local%20Minima%0AAuthor%3A%20Richard%20Y.%20Zhang%0AAbstract%3A%20%20%20The%20classical%20low-rank%20matrix%20recovery%20problem%20is%20well-known%20to%20exhibit%0A%5Cemph%7Bbenign%20nonconvexity%7D%20under%20the%20restricted%20isometry%20property%20%28RIP%29%3A%20local%0Aoptimization%20is%20guaranteed%20to%20converge%20to%20the%20global%20optimum%2C%20where%20the%20ground%0Atruth%20is%20recovered.%20We%20investigate%20whether%20benign%20nonconvexity%20continues%20to%0Ahold%20when%20the%20factor%20matrices%20are%20constrained%20to%20be%20elementwise%20nonnegative%20--%0Aa%20common%20practical%20requirement.%20In%20the%20simple%20setting%20of%20a%20rank-1%20nonnegative%0Aground%20truth%2C%20we%20confirm%20that%20benign%20nonconvexity%20holds%20in%20the%20fully-observed%0Acase%20with%20RIP%20constant%20%24%5Cdelta%3D0%24.%20Surprisingly%2C%20however%2C%20this%20property%20fails%0Ato%20extend%20to%20the%20partially-observed%20case%20with%20any%20arbitrarily%20small%20RIP%0Aconstant%20%24%5Cdelta%5Cto0%5E%7B%2B%7D%24%2C%20irrespective%20of%20rank%20overparameterization.%20This%0Afinding%20exposes%20a%20critical%20theoretical%20gap%3A%20the%20continuity%20argument%20widely%20used%0Ato%20explain%20the%20empirical%20robustness%20of%20low-rank%20matrix%20recovery%20fundamentally%0Abreaks%20down%20once%20nonnegative%20constraints%20are%20imposed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonnegative%2520Low-rank%2520Matrix%2520Recovery%2520Can%2520Have%2520Spurious%2520Local%2520Minima%26entry.906535625%3DRichard%2520Y.%2520Zhang%26entry.1292438233%3D%2520%2520The%2520classical%2520low-rank%2520matrix%2520recovery%2520problem%2520is%2520well-known%2520to%2520exhibit%250A%255Cemph%257Bbenign%2520nonconvexity%257D%2520under%2520the%2520restricted%2520isometry%2520property%2520%2528RIP%2529%253A%2520local%250Aoptimization%2520is%2520guaranteed%2520to%2520converge%2520to%2520the%2520global%2520optimum%252C%2520where%2520the%2520ground%250Atruth%2520is%2520recovered.%2520We%2520investigate%2520whether%2520benign%2520nonconvexity%2520continues%2520to%250Ahold%2520when%2520the%2520factor%2520matrices%2520are%2520constrained%2520to%2520be%2520elementwise%2520nonnegative%2520--%250Aa%2520common%2520practical%2520requirement.%2520In%2520the%2520simple%2520setting%2520of%2520a%2520rank-1%2520nonnegative%250Aground%2520truth%252C%2520we%2520confirm%2520that%2520benign%2520nonconvexity%2520holds%2520in%2520the%2520fully-observed%250Acase%2520with%2520RIP%2520constant%2520%2524%255Cdelta%253D0%2524.%2520Surprisingly%252C%2520however%252C%2520this%2520property%2520fails%250Ato%2520extend%2520to%2520the%2520partially-observed%2520case%2520with%2520any%2520arbitrarily%2520small%2520RIP%250Aconstant%2520%2524%255Cdelta%255Cto0%255E%257B%252B%257D%2524%252C%2520irrespective%2520of%2520rank%2520overparameterization.%2520This%250Afinding%2520exposes%2520a%2520critical%2520theoretical%2520gap%253A%2520the%2520continuity%2520argument%2520widely%2520used%250Ato%2520explain%2520the%2520empirical%2520robustness%2520of%2520low-rank%2520matrix%2520recovery%2520fundamentally%250Abreaks%2520down%2520once%2520nonnegative%2520constraints%2520are%2520imposed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonnegative%20Low-rank%20Matrix%20Recovery%20Can%20Have%20Spurious%20Local%20Minima&entry.906535625=Richard%20Y.%20Zhang&entry.1292438233=%20%20The%20classical%20low-rank%20matrix%20recovery%20problem%20is%20well-known%20to%20exhibit%0A%5Cemph%7Bbenign%20nonconvexity%7D%20under%20the%20restricted%20isometry%20property%20%28RIP%29%3A%20local%0Aoptimization%20is%20guaranteed%20to%20converge%20to%20the%20global%20optimum%2C%20where%20the%20ground%0Atruth%20is%20recovered.%20We%20investigate%20whether%20benign%20nonconvexity%20continues%20to%0Ahold%20when%20the%20factor%20matrices%20are%20constrained%20to%20be%20elementwise%20nonnegative%20--%0Aa%20common%20practical%20requirement.%20In%20the%20simple%20setting%20of%20a%20rank-1%20nonnegative%0Aground%20truth%2C%20we%20confirm%20that%20benign%20nonconvexity%20holds%20in%20the%20fully-observed%0Acase%20with%20RIP%20constant%20%24%5Cdelta%3D0%24.%20Surprisingly%2C%20however%2C%20this%20property%20fails%0Ato%20extend%20to%20the%20partially-observed%20case%20with%20any%20arbitrarily%20small%20RIP%0Aconstant%20%24%5Cdelta%5Cto0%5E%7B%2B%7D%24%2C%20irrespective%20of%20rank%20overparameterization.%20This%0Afinding%20exposes%20a%20critical%20theoretical%20gap%3A%20the%20continuity%20argument%20widely%20used%0Ato%20explain%20the%20empirical%20robustness%20of%20low-rank%20matrix%20recovery%20fundamentally%0Abreaks%20down%20once%20nonnegative%20constraints%20are%20imposed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03717v1&entry.124074799=Read"},
{"title": "Efficient Training of Physics-enhanced Neural ODEs via Direct\n  Collocation and Nonlinear Programming", "author": "Linus Langenkamp and Philip Hannebohm and Bernhard Bachmann", "abstract": "  We propose a novel approach for training Physics-enhanced Neural ODEs\n(PeNODEs) by expressing the training process as a dynamic optimization problem.\nThe full model, including neural components, is discretized using a high-order\nimplicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting\nin a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art\nNLP solvers such as Ipopt. This formulation enables simultaneous optimization\nof network parameters and state trajectories, addressing key limitations of ODE\nsolver-based training in terms of stability, runtime, and accuracy. Extending\non a recent direct collocation-based method for Neural ODEs, we generalize to\nPeNODEs, incorporate physical constraints, and present a custom, parallelized,\nopen-source implementation. Benchmarks on a Quarter Vehicle Model and a\nVan-der-Pol oscillator demonstrate superior accuracy, speed, and generalization\nwith smaller networks compared to other training techniques. We also outline a\nplanned integration into OpenModelica to enable accessible training of Neural\nDAEs.\n", "link": "http://arxiv.org/abs/2505.03552v1", "date": "2025-05-06", "relevancy": 1.9825, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5116}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Training%20of%20Physics-enhanced%20Neural%20ODEs%20via%20Direct%0A%20%20Collocation%20and%20Nonlinear%20Programming&body=Title%3A%20Efficient%20Training%20of%20Physics-enhanced%20Neural%20ODEs%20via%20Direct%0A%20%20Collocation%20and%20Nonlinear%20Programming%0AAuthor%3A%20Linus%20Langenkamp%20and%20Philip%20Hannebohm%20and%20Bernhard%20Bachmann%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20for%20training%20Physics-enhanced%20Neural%20ODEs%0A%28PeNODEs%29%20by%20expressing%20the%20training%20process%20as%20a%20dynamic%20optimization%20problem.%0AThe%20full%20model%2C%20including%20neural%20components%2C%20is%20discretized%20using%20a%20high-order%0Aimplicit%20Runge-Kutta%20method%20with%20flipped%20Legendre-Gauss-Radau%20points%2C%20resulting%0Ain%20a%20large-scale%20nonlinear%20program%20%28NLP%29%20efficiently%20solved%20by%20state-of-the-art%0ANLP%20solvers%20such%20as%20Ipopt.%20This%20formulation%20enables%20simultaneous%20optimization%0Aof%20network%20parameters%20and%20state%20trajectories%2C%20addressing%20key%20limitations%20of%20ODE%0Asolver-based%20training%20in%20terms%20of%20stability%2C%20runtime%2C%20and%20accuracy.%20Extending%0Aon%20a%20recent%20direct%20collocation-based%20method%20for%20Neural%20ODEs%2C%20we%20generalize%20to%0APeNODEs%2C%20incorporate%20physical%20constraints%2C%20and%20present%20a%20custom%2C%20parallelized%2C%0Aopen-source%20implementation.%20Benchmarks%20on%20a%20Quarter%20Vehicle%20Model%20and%20a%0AVan-der-Pol%20oscillator%20demonstrate%20superior%20accuracy%2C%20speed%2C%20and%20generalization%0Awith%20smaller%20networks%20compared%20to%20other%20training%20techniques.%20We%20also%20outline%20a%0Aplanned%20integration%20into%20OpenModelica%20to%20enable%20accessible%20training%20of%20Neural%0ADAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Training%2520of%2520Physics-enhanced%2520Neural%2520ODEs%2520via%2520Direct%250A%2520%2520Collocation%2520and%2520Nonlinear%2520Programming%26entry.906535625%3DLinus%2520Langenkamp%2520and%2520Philip%2520Hannebohm%2520and%2520Bernhard%2520Bachmann%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520for%2520training%2520Physics-enhanced%2520Neural%2520ODEs%250A%2528PeNODEs%2529%2520by%2520expressing%2520the%2520training%2520process%2520as%2520a%2520dynamic%2520optimization%2520problem.%250AThe%2520full%2520model%252C%2520including%2520neural%2520components%252C%2520is%2520discretized%2520using%2520a%2520high-order%250Aimplicit%2520Runge-Kutta%2520method%2520with%2520flipped%2520Legendre-Gauss-Radau%2520points%252C%2520resulting%250Ain%2520a%2520large-scale%2520nonlinear%2520program%2520%2528NLP%2529%2520efficiently%2520solved%2520by%2520state-of-the-art%250ANLP%2520solvers%2520such%2520as%2520Ipopt.%2520This%2520formulation%2520enables%2520simultaneous%2520optimization%250Aof%2520network%2520parameters%2520and%2520state%2520trajectories%252C%2520addressing%2520key%2520limitations%2520of%2520ODE%250Asolver-based%2520training%2520in%2520terms%2520of%2520stability%252C%2520runtime%252C%2520and%2520accuracy.%2520Extending%250Aon%2520a%2520recent%2520direct%2520collocation-based%2520method%2520for%2520Neural%2520ODEs%252C%2520we%2520generalize%2520to%250APeNODEs%252C%2520incorporate%2520physical%2520constraints%252C%2520and%2520present%2520a%2520custom%252C%2520parallelized%252C%250Aopen-source%2520implementation.%2520Benchmarks%2520on%2520a%2520Quarter%2520Vehicle%2520Model%2520and%2520a%250AVan-der-Pol%2520oscillator%2520demonstrate%2520superior%2520accuracy%252C%2520speed%252C%2520and%2520generalization%250Awith%2520smaller%2520networks%2520compared%2520to%2520other%2520training%2520techniques.%2520We%2520also%2520outline%2520a%250Aplanned%2520integration%2520into%2520OpenModelica%2520to%2520enable%2520accessible%2520training%2520of%2520Neural%250ADAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Training%20of%20Physics-enhanced%20Neural%20ODEs%20via%20Direct%0A%20%20Collocation%20and%20Nonlinear%20Programming&entry.906535625=Linus%20Langenkamp%20and%20Philip%20Hannebohm%20and%20Bernhard%20Bachmann&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20for%20training%20Physics-enhanced%20Neural%20ODEs%0A%28PeNODEs%29%20by%20expressing%20the%20training%20process%20as%20a%20dynamic%20optimization%20problem.%0AThe%20full%20model%2C%20including%20neural%20components%2C%20is%20discretized%20using%20a%20high-order%0Aimplicit%20Runge-Kutta%20method%20with%20flipped%20Legendre-Gauss-Radau%20points%2C%20resulting%0Ain%20a%20large-scale%20nonlinear%20program%20%28NLP%29%20efficiently%20solved%20by%20state-of-the-art%0ANLP%20solvers%20such%20as%20Ipopt.%20This%20formulation%20enables%20simultaneous%20optimization%0Aof%20network%20parameters%20and%20state%20trajectories%2C%20addressing%20key%20limitations%20of%20ODE%0Asolver-based%20training%20in%20terms%20of%20stability%2C%20runtime%2C%20and%20accuracy.%20Extending%0Aon%20a%20recent%20direct%20collocation-based%20method%20for%20Neural%20ODEs%2C%20we%20generalize%20to%0APeNODEs%2C%20incorporate%20physical%20constraints%2C%20and%20present%20a%20custom%2C%20parallelized%2C%0Aopen-source%20implementation.%20Benchmarks%20on%20a%20Quarter%20Vehicle%20Model%20and%20a%0AVan-der-Pol%20oscillator%20demonstrate%20superior%20accuracy%2C%20speed%2C%20and%20generalization%0Awith%20smaller%20networks%20compared%20to%20other%20training%20techniques.%20We%20also%20outline%20a%0Aplanned%20integration%20into%20OpenModelica%20to%20enable%20accessible%20training%20of%20Neural%0ADAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03552v1&entry.124074799=Read"},
{"title": "Physics-informed neural network estimation of active material properties\n  in time-dependent cardiac biomechanical models", "author": "Matthias H\u00f6fler and Francesco Regazzoni and Stefano Pagani and Elias Karabelas and Christoph Augustin and Gundolf Haase and Gernot Plank and Federica Caforio", "abstract": "  Active stress models in cardiac biomechanics account for the mechanical\ndeformation caused by muscle activity, thus providing a link between the\nelectrophysiological and mechanical properties of the tissue. The accurate\nassessment of active stress parameters is fundamental for a precise\nunderstanding of myocardial function but remains difficult to achieve in a\nclinical setting, especially when only displacement and strain data from\nmedical imaging modalities are available. This work investigates, through an\nin-silico study, the application of physics-informed neural networks (PINNs)\nfor inferring active contractility parameters in time-dependent cardiac\nbiomechanical models from these types of imaging data. In particular, by\nparametrising the sought state and parameter field with two neural networks,\nrespectively, and formulating an energy minimisation problem to search for the\noptimal network parameters, we are able to reconstruct in various settings\nactive stress fields in the presence of noise and with a high spatial\nresolution. To this end, we also advance the vanilla PINN learning algorithm\nwith the use of adaptive weighting schemes, ad-hoc regularisation strategies,\nFourier features, and suitable network architectures. In addition, we\nthoroughly analyse the influence of the loss weights in the reconstruction of\nactive stress parameters. Finally, we apply the method to the characterisation\nof tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.\nThis approach opens a new pathway to significantly improve the diagnosis,\ntreatment planning, and management of heart conditions associated with cardiac\nfibrosis.\n", "link": "http://arxiv.org/abs/2505.03382v1", "date": "2025-05-06", "relevancy": 1.9726, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5444}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4912}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-informed%20neural%20network%20estimation%20of%20active%20material%20properties%0A%20%20in%20time-dependent%20cardiac%20biomechanical%20models&body=Title%3A%20Physics-informed%20neural%20network%20estimation%20of%20active%20material%20properties%0A%20%20in%20time-dependent%20cardiac%20biomechanical%20models%0AAuthor%3A%20Matthias%20H%C3%B6fler%20and%20Francesco%20Regazzoni%20and%20Stefano%20Pagani%20and%20Elias%20Karabelas%20and%20Christoph%20Augustin%20and%20Gundolf%20Haase%20and%20Gernot%20Plank%20and%20Federica%20Caforio%0AAbstract%3A%20%20%20Active%20stress%20models%20in%20cardiac%20biomechanics%20account%20for%20the%20mechanical%0Adeformation%20caused%20by%20muscle%20activity%2C%20thus%20providing%20a%20link%20between%20the%0Aelectrophysiological%20and%20mechanical%20properties%20of%20the%20tissue.%20The%20accurate%0Aassessment%20of%20active%20stress%20parameters%20is%20fundamental%20for%20a%20precise%0Aunderstanding%20of%20myocardial%20function%20but%20remains%20difficult%20to%20achieve%20in%20a%0Aclinical%20setting%2C%20especially%20when%20only%20displacement%20and%20strain%20data%20from%0Amedical%20imaging%20modalities%20are%20available.%20This%20work%20investigates%2C%20through%20an%0Ain-silico%20study%2C%20the%20application%20of%20physics-informed%20neural%20networks%20%28PINNs%29%0Afor%20inferring%20active%20contractility%20parameters%20in%20time-dependent%20cardiac%0Abiomechanical%20models%20from%20these%20types%20of%20imaging%20data.%20In%20particular%2C%20by%0Aparametrising%20the%20sought%20state%20and%20parameter%20field%20with%20two%20neural%20networks%2C%0Arespectively%2C%20and%20formulating%20an%20energy%20minimisation%20problem%20to%20search%20for%20the%0Aoptimal%20network%20parameters%2C%20we%20are%20able%20to%20reconstruct%20in%20various%20settings%0Aactive%20stress%20fields%20in%20the%20presence%20of%20noise%20and%20with%20a%20high%20spatial%0Aresolution.%20To%20this%20end%2C%20we%20also%20advance%20the%20vanilla%20PINN%20learning%20algorithm%0Awith%20the%20use%20of%20adaptive%20weighting%20schemes%2C%20ad-hoc%20regularisation%20strategies%2C%0AFourier%20features%2C%20and%20suitable%20network%20architectures.%20In%20addition%2C%20we%0Athoroughly%20analyse%20the%20influence%20of%20the%20loss%20weights%20in%20the%20reconstruction%20of%0Aactive%20stress%20parameters.%20Finally%2C%20we%20apply%20the%20method%20to%20the%20characterisation%0Aof%20tissue%20inhomogeneities%20and%20detection%20of%20fibrotic%20scars%20in%20myocardial%20tissue.%0AThis%20approach%20opens%20a%20new%20pathway%20to%20significantly%20improve%20the%20diagnosis%2C%0Atreatment%20planning%2C%20and%20management%20of%20heart%20conditions%20associated%20with%20cardiac%0Afibrosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-informed%2520neural%2520network%2520estimation%2520of%2520active%2520material%2520properties%250A%2520%2520in%2520time-dependent%2520cardiac%2520biomechanical%2520models%26entry.906535625%3DMatthias%2520H%25C3%25B6fler%2520and%2520Francesco%2520Regazzoni%2520and%2520Stefano%2520Pagani%2520and%2520Elias%2520Karabelas%2520and%2520Christoph%2520Augustin%2520and%2520Gundolf%2520Haase%2520and%2520Gernot%2520Plank%2520and%2520Federica%2520Caforio%26entry.1292438233%3D%2520%2520Active%2520stress%2520models%2520in%2520cardiac%2520biomechanics%2520account%2520for%2520the%2520mechanical%250Adeformation%2520caused%2520by%2520muscle%2520activity%252C%2520thus%2520providing%2520a%2520link%2520between%2520the%250Aelectrophysiological%2520and%2520mechanical%2520properties%2520of%2520the%2520tissue.%2520The%2520accurate%250Aassessment%2520of%2520active%2520stress%2520parameters%2520is%2520fundamental%2520for%2520a%2520precise%250Aunderstanding%2520of%2520myocardial%2520function%2520but%2520remains%2520difficult%2520to%2520achieve%2520in%2520a%250Aclinical%2520setting%252C%2520especially%2520when%2520only%2520displacement%2520and%2520strain%2520data%2520from%250Amedical%2520imaging%2520modalities%2520are%2520available.%2520This%2520work%2520investigates%252C%2520through%2520an%250Ain-silico%2520study%252C%2520the%2520application%2520of%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529%250Afor%2520inferring%2520active%2520contractility%2520parameters%2520in%2520time-dependent%2520cardiac%250Abiomechanical%2520models%2520from%2520these%2520types%2520of%2520imaging%2520data.%2520In%2520particular%252C%2520by%250Aparametrising%2520the%2520sought%2520state%2520and%2520parameter%2520field%2520with%2520two%2520neural%2520networks%252C%250Arespectively%252C%2520and%2520formulating%2520an%2520energy%2520minimisation%2520problem%2520to%2520search%2520for%2520the%250Aoptimal%2520network%2520parameters%252C%2520we%2520are%2520able%2520to%2520reconstruct%2520in%2520various%2520settings%250Aactive%2520stress%2520fields%2520in%2520the%2520presence%2520of%2520noise%2520and%2520with%2520a%2520high%2520spatial%250Aresolution.%2520To%2520this%2520end%252C%2520we%2520also%2520advance%2520the%2520vanilla%2520PINN%2520learning%2520algorithm%250Awith%2520the%2520use%2520of%2520adaptive%2520weighting%2520schemes%252C%2520ad-hoc%2520regularisation%2520strategies%252C%250AFourier%2520features%252C%2520and%2520suitable%2520network%2520architectures.%2520In%2520addition%252C%2520we%250Athoroughly%2520analyse%2520the%2520influence%2520of%2520the%2520loss%2520weights%2520in%2520the%2520reconstruction%2520of%250Aactive%2520stress%2520parameters.%2520Finally%252C%2520we%2520apply%2520the%2520method%2520to%2520the%2520characterisation%250Aof%2520tissue%2520inhomogeneities%2520and%2520detection%2520of%2520fibrotic%2520scars%2520in%2520myocardial%2520tissue.%250AThis%2520approach%2520opens%2520a%2520new%2520pathway%2520to%2520significantly%2520improve%2520the%2520diagnosis%252C%250Atreatment%2520planning%252C%2520and%2520management%2520of%2520heart%2520conditions%2520associated%2520with%2520cardiac%250Afibrosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-informed%20neural%20network%20estimation%20of%20active%20material%20properties%0A%20%20in%20time-dependent%20cardiac%20biomechanical%20models&entry.906535625=Matthias%20H%C3%B6fler%20and%20Francesco%20Regazzoni%20and%20Stefano%20Pagani%20and%20Elias%20Karabelas%20and%20Christoph%20Augustin%20and%20Gundolf%20Haase%20and%20Gernot%20Plank%20and%20Federica%20Caforio&entry.1292438233=%20%20Active%20stress%20models%20in%20cardiac%20biomechanics%20account%20for%20the%20mechanical%0Adeformation%20caused%20by%20muscle%20activity%2C%20thus%20providing%20a%20link%20between%20the%0Aelectrophysiological%20and%20mechanical%20properties%20of%20the%20tissue.%20The%20accurate%0Aassessment%20of%20active%20stress%20parameters%20is%20fundamental%20for%20a%20precise%0Aunderstanding%20of%20myocardial%20function%20but%20remains%20difficult%20to%20achieve%20in%20a%0Aclinical%20setting%2C%20especially%20when%20only%20displacement%20and%20strain%20data%20from%0Amedical%20imaging%20modalities%20are%20available.%20This%20work%20investigates%2C%20through%20an%0Ain-silico%20study%2C%20the%20application%20of%20physics-informed%20neural%20networks%20%28PINNs%29%0Afor%20inferring%20active%20contractility%20parameters%20in%20time-dependent%20cardiac%0Abiomechanical%20models%20from%20these%20types%20of%20imaging%20data.%20In%20particular%2C%20by%0Aparametrising%20the%20sought%20state%20and%20parameter%20field%20with%20two%20neural%20networks%2C%0Arespectively%2C%20and%20formulating%20an%20energy%20minimisation%20problem%20to%20search%20for%20the%0Aoptimal%20network%20parameters%2C%20we%20are%20able%20to%20reconstruct%20in%20various%20settings%0Aactive%20stress%20fields%20in%20the%20presence%20of%20noise%20and%20with%20a%20high%20spatial%0Aresolution.%20To%20this%20end%2C%20we%20also%20advance%20the%20vanilla%20PINN%20learning%20algorithm%0Awith%20the%20use%20of%20adaptive%20weighting%20schemes%2C%20ad-hoc%20regularisation%20strategies%2C%0AFourier%20features%2C%20and%20suitable%20network%20architectures.%20In%20addition%2C%20we%0Athoroughly%20analyse%20the%20influence%20of%20the%20loss%20weights%20in%20the%20reconstruction%20of%0Aactive%20stress%20parameters.%20Finally%2C%20we%20apply%20the%20method%20to%20the%20characterisation%0Aof%20tissue%20inhomogeneities%20and%20detection%20of%20fibrotic%20scars%20in%20myocardial%20tissue.%0AThis%20approach%20opens%20a%20new%20pathway%20to%20significantly%20improve%20the%20diagnosis%2C%0Atreatment%20planning%2C%20and%20management%20of%20heart%20conditions%20associated%20with%20cardiac%0Afibrosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03382v1&entry.124074799=Read"},
{"title": "Towards Enterprise-Ready Computer Using Generalist Agent", "author": "Sami Marreed and Alon Oved and Avi Yaeli and Segev Shlomov and Ido Levy and Aviad Sela and Asaf Adi and Nir Mashkif", "abstract": "  This paper presents our ongoing work toward developing an enterprise-ready\nComputer Using Generalist Agent (CUGA) system. Our research highlights the\nevolutionary nature of building agentic systems suitable for enterprise\nenvironments. By integrating state-of-the-art agentic AI techniques with a\nsystematic approach to iterative evaluation, analysis, and refinement, we have\nachieved rapid and cost-effective performance gains, notably reaching a new\nstate-of-the-art performance on the WebArena benchmark. We detail our\ndevelopment roadmap, the methodology and tools that facilitated rapid learning\nfrom failures and continuous system refinement, and discuss key lessons learned\nand future challenges for enterprise adoption.\n", "link": "http://arxiv.org/abs/2503.01861v2", "date": "2025-05-06", "relevancy": 1.9721, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5107}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4906}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Enterprise-Ready%20Computer%20Using%20Generalist%20Agent&body=Title%3A%20Towards%20Enterprise-Ready%20Computer%20Using%20Generalist%20Agent%0AAuthor%3A%20Sami%20Marreed%20and%20Alon%20Oved%20and%20Avi%20Yaeli%20and%20Segev%20Shlomov%20and%20Ido%20Levy%20and%20Aviad%20Sela%20and%20Asaf%20Adi%20and%20Nir%20Mashkif%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20ongoing%20work%20toward%20developing%20an%20enterprise-ready%0AComputer%20Using%20Generalist%20Agent%20%28CUGA%29%20system.%20Our%20research%20highlights%20the%0Aevolutionary%20nature%20of%20building%20agentic%20systems%20suitable%20for%20enterprise%0Aenvironments.%20By%20integrating%20state-of-the-art%20agentic%20AI%20techniques%20with%20a%0Asystematic%20approach%20to%20iterative%20evaluation%2C%20analysis%2C%20and%20refinement%2C%20we%20have%0Aachieved%20rapid%20and%20cost-effective%20performance%20gains%2C%20notably%20reaching%20a%20new%0Astate-of-the-art%20performance%20on%20the%20WebArena%20benchmark.%20We%20detail%20our%0Adevelopment%20roadmap%2C%20the%20methodology%20and%20tools%20that%20facilitated%20rapid%20learning%0Afrom%20failures%20and%20continuous%20system%20refinement%2C%20and%20discuss%20key%20lessons%20learned%0Aand%20future%20challenges%20for%20enterprise%20adoption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Enterprise-Ready%2520Computer%2520Using%2520Generalist%2520Agent%26entry.906535625%3DSami%2520Marreed%2520and%2520Alon%2520Oved%2520and%2520Avi%2520Yaeli%2520and%2520Segev%2520Shlomov%2520and%2520Ido%2520Levy%2520and%2520Aviad%2520Sela%2520and%2520Asaf%2520Adi%2520and%2520Nir%2520Mashkif%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520our%2520ongoing%2520work%2520toward%2520developing%2520an%2520enterprise-ready%250AComputer%2520Using%2520Generalist%2520Agent%2520%2528CUGA%2529%2520system.%2520Our%2520research%2520highlights%2520the%250Aevolutionary%2520nature%2520of%2520building%2520agentic%2520systems%2520suitable%2520for%2520enterprise%250Aenvironments.%2520By%2520integrating%2520state-of-the-art%2520agentic%2520AI%2520techniques%2520with%2520a%250Asystematic%2520approach%2520to%2520iterative%2520evaluation%252C%2520analysis%252C%2520and%2520refinement%252C%2520we%2520have%250Aachieved%2520rapid%2520and%2520cost-effective%2520performance%2520gains%252C%2520notably%2520reaching%2520a%2520new%250Astate-of-the-art%2520performance%2520on%2520the%2520WebArena%2520benchmark.%2520We%2520detail%2520our%250Adevelopment%2520roadmap%252C%2520the%2520methodology%2520and%2520tools%2520that%2520facilitated%2520rapid%2520learning%250Afrom%2520failures%2520and%2520continuous%2520system%2520refinement%252C%2520and%2520discuss%2520key%2520lessons%2520learned%250Aand%2520future%2520challenges%2520for%2520enterprise%2520adoption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Enterprise-Ready%20Computer%20Using%20Generalist%20Agent&entry.906535625=Sami%20Marreed%20and%20Alon%20Oved%20and%20Avi%20Yaeli%20and%20Segev%20Shlomov%20and%20Ido%20Levy%20and%20Aviad%20Sela%20and%20Asaf%20Adi%20and%20Nir%20Mashkif&entry.1292438233=%20%20This%20paper%20presents%20our%20ongoing%20work%20toward%20developing%20an%20enterprise-ready%0AComputer%20Using%20Generalist%20Agent%20%28CUGA%29%20system.%20Our%20research%20highlights%20the%0Aevolutionary%20nature%20of%20building%20agentic%20systems%20suitable%20for%20enterprise%0Aenvironments.%20By%20integrating%20state-of-the-art%20agentic%20AI%20techniques%20with%20a%0Asystematic%20approach%20to%20iterative%20evaluation%2C%20analysis%2C%20and%20refinement%2C%20we%20have%0Aachieved%20rapid%20and%20cost-effective%20performance%20gains%2C%20notably%20reaching%20a%20new%0Astate-of-the-art%20performance%20on%20the%20WebArena%20benchmark.%20We%20detail%20our%0Adevelopment%20roadmap%2C%20the%20methodology%20and%20tools%20that%20facilitated%20rapid%20learning%0Afrom%20failures%20and%20continuous%20system%20refinement%2C%20and%20discuss%20key%20lessons%20learned%0Aand%20future%20challenges%20for%20enterprise%20adoption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01861v2&entry.124074799=Read"},
{"title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models", "author": "Teng Wang and Zhangyi Jiang and Zhenqi He and Shenyang Tong and Wenhan Yang and Yanan Zheng and Zeyu Li and Zifan He and Hailei Gong", "abstract": "  Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks.\n", "link": "http://arxiv.org/abs/2503.13551v3", "date": "2025-05-06", "relevancy": 1.9696, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Hierarchical%20Multi-Step%20Reward%20Models%20for%20Enhanced%20Reasoning%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Towards%20Hierarchical%20Multi-Step%20Reward%20Models%20for%20Enhanced%20Reasoning%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Teng%20Wang%20and%20Zhangyi%20Jiang%20and%20Zhenqi%20He%20and%20Shenyang%20Tong%20and%20Wenhan%20Yang%20and%20Yanan%20Zheng%20and%20Zeyu%20Li%20and%20Zifan%20He%20and%20Hailei%20Gong%0AAbstract%3A%20%20%20Recent%20studies%20show%20that%20Large%20Language%20Models%20%28LLMs%29%20achieve%20strong%0Areasoning%20capabilities%20through%20supervised%20fine-tuning%20or%20reinforcement%0Alearning.%20However%2C%20a%20key%20approach%2C%20the%20Process%20Reward%20Model%20%28PRM%29%2C%20suffers%20from%0Areward%20hacking%2C%20making%20it%20unreliable%20in%20identifying%20the%20best%20intermediate%20step.%0AIn%20addition%2C%20the%20cost%20of%20annotating%20reasoning%20processes%20for%20reward%20modeling%20is%0Ahigh%2C%20making%20large-scale%20collection%20of%20high-quality%20data%20challenging.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20reward%20model%20approach%20called%20the%20Hierarchical%0AReward%20Model%20%28HRM%29%2C%20which%20evaluates%20both%20individual%20and%20consecutive%20reasoning%0Asteps%20at%20both%20fine-grained%20and%20coarse-grained%20levels.%20HRM%20excels%20at%20assessing%0Amulti-step%20reasoning%20coherence%2C%20especially%20when%20flawed%20steps%20are%20later%0Acorrected%20through%20self-reflection.%20To%20further%20reduce%20the%20cost%20of%20generating%0Atraining%20data%2C%20we%20introduce%20a%20lightweight%20and%20effective%20data%20augmentation%0Astrategy%20called%20Hierarchical%20Node%20Compression%20%28HNC%29%2C%20which%20merges%20two%0Aconsecutive%20reasoning%20steps%20into%20one%20within%20the%20tree%20structure.%20By%20applying%20HNC%0Ato%20MCTS-generated%20reasoning%20trajectories%2C%20we%20enhance%20the%20diversity%20and%0Arobustness%20of%20HRM%20training%20data%20while%20introducing%20controlled%20noise%20with%20minimal%0Acomputational%20overhead.%20Empirical%20results%20on%20the%20PRM800K%20dataset%20show%20that%20HRM%2C%0Atogether%20with%20HNC%2C%20provides%20more%20stable%20and%20reliable%20evaluations%20than%20PRM.%0AFurthermore%2C%20cross-domain%20evaluations%20on%20the%20MATH500%20and%20GSM8K%20datasets%0Ademonstrate%20HRM%27s%20strong%20generalization%20and%20robustness%20across%20a%20variety%20of%0Areasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13551v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Hierarchical%2520Multi-Step%2520Reward%2520Models%2520for%2520Enhanced%2520Reasoning%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DTeng%2520Wang%2520and%2520Zhangyi%2520Jiang%2520and%2520Zhenqi%2520He%2520and%2520Shenyang%2520Tong%2520and%2520Wenhan%2520Yang%2520and%2520Yanan%2520Zheng%2520and%2520Zeyu%2520Li%2520and%2520Zifan%2520He%2520and%2520Hailei%2520Gong%26entry.1292438233%3D%2520%2520Recent%2520studies%2520show%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520achieve%2520strong%250Areasoning%2520capabilities%2520through%2520supervised%2520fine-tuning%2520or%2520reinforcement%250Alearning.%2520However%252C%2520a%2520key%2520approach%252C%2520the%2520Process%2520Reward%2520Model%2520%2528PRM%2529%252C%2520suffers%2520from%250Areward%2520hacking%252C%2520making%2520it%2520unreliable%2520in%2520identifying%2520the%2520best%2520intermediate%2520step.%250AIn%2520addition%252C%2520the%2520cost%2520of%2520annotating%2520reasoning%2520processes%2520for%2520reward%2520modeling%2520is%250Ahigh%252C%2520making%2520large-scale%2520collection%2520of%2520high-quality%2520data%2520challenging.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520novel%2520reward%2520model%2520approach%2520called%2520the%2520Hierarchical%250AReward%2520Model%2520%2528HRM%2529%252C%2520which%2520evaluates%2520both%2520individual%2520and%2520consecutive%2520reasoning%250Asteps%2520at%2520both%2520fine-grained%2520and%2520coarse-grained%2520levels.%2520HRM%2520excels%2520at%2520assessing%250Amulti-step%2520reasoning%2520coherence%252C%2520especially%2520when%2520flawed%2520steps%2520are%2520later%250Acorrected%2520through%2520self-reflection.%2520To%2520further%2520reduce%2520the%2520cost%2520of%2520generating%250Atraining%2520data%252C%2520we%2520introduce%2520a%2520lightweight%2520and%2520effective%2520data%2520augmentation%250Astrategy%2520called%2520Hierarchical%2520Node%2520Compression%2520%2528HNC%2529%252C%2520which%2520merges%2520two%250Aconsecutive%2520reasoning%2520steps%2520into%2520one%2520within%2520the%2520tree%2520structure.%2520By%2520applying%2520HNC%250Ato%2520MCTS-generated%2520reasoning%2520trajectories%252C%2520we%2520enhance%2520the%2520diversity%2520and%250Arobustness%2520of%2520HRM%2520training%2520data%2520while%2520introducing%2520controlled%2520noise%2520with%2520minimal%250Acomputational%2520overhead.%2520Empirical%2520results%2520on%2520the%2520PRM800K%2520dataset%2520show%2520that%2520HRM%252C%250Atogether%2520with%2520HNC%252C%2520provides%2520more%2520stable%2520and%2520reliable%2520evaluations%2520than%2520PRM.%250AFurthermore%252C%2520cross-domain%2520evaluations%2520on%2520the%2520MATH500%2520and%2520GSM8K%2520datasets%250Ademonstrate%2520HRM%2527s%2520strong%2520generalization%2520and%2520robustness%2520across%2520a%2520variety%2520of%250Areasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13551v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Hierarchical%20Multi-Step%20Reward%20Models%20for%20Enhanced%20Reasoning%20in%0A%20%20Large%20Language%20Models&entry.906535625=Teng%20Wang%20and%20Zhangyi%20Jiang%20and%20Zhenqi%20He%20and%20Shenyang%20Tong%20and%20Wenhan%20Yang%20and%20Yanan%20Zheng%20and%20Zeyu%20Li%20and%20Zifan%20He%20and%20Hailei%20Gong&entry.1292438233=%20%20Recent%20studies%20show%20that%20Large%20Language%20Models%20%28LLMs%29%20achieve%20strong%0Areasoning%20capabilities%20through%20supervised%20fine-tuning%20or%20reinforcement%0Alearning.%20However%2C%20a%20key%20approach%2C%20the%20Process%20Reward%20Model%20%28PRM%29%2C%20suffers%20from%0Areward%20hacking%2C%20making%20it%20unreliable%20in%20identifying%20the%20best%20intermediate%20step.%0AIn%20addition%2C%20the%20cost%20of%20annotating%20reasoning%20processes%20for%20reward%20modeling%20is%0Ahigh%2C%20making%20large-scale%20collection%20of%20high-quality%20data%20challenging.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20reward%20model%20approach%20called%20the%20Hierarchical%0AReward%20Model%20%28HRM%29%2C%20which%20evaluates%20both%20individual%20and%20consecutive%20reasoning%0Asteps%20at%20both%20fine-grained%20and%20coarse-grained%20levels.%20HRM%20excels%20at%20assessing%0Amulti-step%20reasoning%20coherence%2C%20especially%20when%20flawed%20steps%20are%20later%0Acorrected%20through%20self-reflection.%20To%20further%20reduce%20the%20cost%20of%20generating%0Atraining%20data%2C%20we%20introduce%20a%20lightweight%20and%20effective%20data%20augmentation%0Astrategy%20called%20Hierarchical%20Node%20Compression%20%28HNC%29%2C%20which%20merges%20two%0Aconsecutive%20reasoning%20steps%20into%20one%20within%20the%20tree%20structure.%20By%20applying%20HNC%0Ato%20MCTS-generated%20reasoning%20trajectories%2C%20we%20enhance%20the%20diversity%20and%0Arobustness%20of%20HRM%20training%20data%20while%20introducing%20controlled%20noise%20with%20minimal%0Acomputational%20overhead.%20Empirical%20results%20on%20the%20PRM800K%20dataset%20show%20that%20HRM%2C%0Atogether%20with%20HNC%2C%20provides%20more%20stable%20and%20reliable%20evaluations%20than%20PRM.%0AFurthermore%2C%20cross-domain%20evaluations%20on%20the%20MATH500%20and%20GSM8K%20datasets%0Ademonstrate%20HRM%27s%20strong%20generalization%20and%20robustness%20across%20a%20variety%20of%0Areasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13551v3&entry.124074799=Read"},
{"title": "Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency\n  with Decision Theory-Guided Deep Reinforcement Learning", "author": "Dian Chen and Zelin Wan and Dong Sam Ha and Jin-Hee Cho", "abstract": "  Solar sensor-based monitoring systems have become a crucial agricultural\ninnovation, advancing farm management and animal welfare through integrating\nsensor technology, Internet-of-Things, and edge and cloud computing. However,\nthe resilience of these systems to cyber-attacks and their adaptability to\ndynamic and constrained energy supplies remain largely unexplored. To address\nthese challenges, we propose a sustainable smart farm network designed to\nmaintain high-quality animal monitoring under various cyber and adversarial\nthreats, as well as fluctuating energy conditions. Our approach utilizes deep\nreinforcement learning (DRL) to devise optimal policies that maximize both\nmonitoring effectiveness and energy efficiency. To overcome DRL's inherent\nchallenge of slow convergence, we integrate transfer learning (TL) and decision\ntheory (DT) to accelerate the learning process. By incorporating DT-guided\nstrategies, we optimize monitoring quality and energy sustainability,\nsignificantly reducing training time while achieving comparable performance\nrewards. Our experimental results prove that DT-guided DRL outperforms\nTL-enhanced DRL models, improving system performance and reducing training\nruntime by 47.5%.\n", "link": "http://arxiv.org/abs/2505.03721v1", "date": "2025-05-06", "relevancy": 1.9589, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4989}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4839}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sustainable%20Smart%20Farm%20Networks%3A%20Enhancing%20Resilience%20and%20Efficiency%0A%20%20with%20Decision%20Theory-Guided%20Deep%20Reinforcement%20Learning&body=Title%3A%20Sustainable%20Smart%20Farm%20Networks%3A%20Enhancing%20Resilience%20and%20Efficiency%0A%20%20with%20Decision%20Theory-Guided%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Dian%20Chen%20and%20Zelin%20Wan%20and%20Dong%20Sam%20Ha%20and%20Jin-Hee%20Cho%0AAbstract%3A%20%20%20Solar%20sensor-based%20monitoring%20systems%20have%20become%20a%20crucial%20agricultural%0Ainnovation%2C%20advancing%20farm%20management%20and%20animal%20welfare%20through%20integrating%0Asensor%20technology%2C%20Internet-of-Things%2C%20and%20edge%20and%20cloud%20computing.%20However%2C%0Athe%20resilience%20of%20these%20systems%20to%20cyber-attacks%20and%20their%20adaptability%20to%0Adynamic%20and%20constrained%20energy%20supplies%20remain%20largely%20unexplored.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20sustainable%20smart%20farm%20network%20designed%20to%0Amaintain%20high-quality%20animal%20monitoring%20under%20various%20cyber%20and%20adversarial%0Athreats%2C%20as%20well%20as%20fluctuating%20energy%20conditions.%20Our%20approach%20utilizes%20deep%0Areinforcement%20learning%20%28DRL%29%20to%20devise%20optimal%20policies%20that%20maximize%20both%0Amonitoring%20effectiveness%20and%20energy%20efficiency.%20To%20overcome%20DRL%27s%20inherent%0Achallenge%20of%20slow%20convergence%2C%20we%20integrate%20transfer%20learning%20%28TL%29%20and%20decision%0Atheory%20%28DT%29%20to%20accelerate%20the%20learning%20process.%20By%20incorporating%20DT-guided%0Astrategies%2C%20we%20optimize%20monitoring%20quality%20and%20energy%20sustainability%2C%0Asignificantly%20reducing%20training%20time%20while%20achieving%20comparable%20performance%0Arewards.%20Our%20experimental%20results%20prove%20that%20DT-guided%20DRL%20outperforms%0ATL-enhanced%20DRL%20models%2C%20improving%20system%20performance%20and%20reducing%20training%0Aruntime%20by%2047.5%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSustainable%2520Smart%2520Farm%2520Networks%253A%2520Enhancing%2520Resilience%2520and%2520Efficiency%250A%2520%2520with%2520Decision%2520Theory-Guided%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DDian%2520Chen%2520and%2520Zelin%2520Wan%2520and%2520Dong%2520Sam%2520Ha%2520and%2520Jin-Hee%2520Cho%26entry.1292438233%3D%2520%2520Solar%2520sensor-based%2520monitoring%2520systems%2520have%2520become%2520a%2520crucial%2520agricultural%250Ainnovation%252C%2520advancing%2520farm%2520management%2520and%2520animal%2520welfare%2520through%2520integrating%250Asensor%2520technology%252C%2520Internet-of-Things%252C%2520and%2520edge%2520and%2520cloud%2520computing.%2520However%252C%250Athe%2520resilience%2520of%2520these%2520systems%2520to%2520cyber-attacks%2520and%2520their%2520adaptability%2520to%250Adynamic%2520and%2520constrained%2520energy%2520supplies%2520remain%2520largely%2520unexplored.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520sustainable%2520smart%2520farm%2520network%2520designed%2520to%250Amaintain%2520high-quality%2520animal%2520monitoring%2520under%2520various%2520cyber%2520and%2520adversarial%250Athreats%252C%2520as%2520well%2520as%2520fluctuating%2520energy%2520conditions.%2520Our%2520approach%2520utilizes%2520deep%250Areinforcement%2520learning%2520%2528DRL%2529%2520to%2520devise%2520optimal%2520policies%2520that%2520maximize%2520both%250Amonitoring%2520effectiveness%2520and%2520energy%2520efficiency.%2520To%2520overcome%2520DRL%2527s%2520inherent%250Achallenge%2520of%2520slow%2520convergence%252C%2520we%2520integrate%2520transfer%2520learning%2520%2528TL%2529%2520and%2520decision%250Atheory%2520%2528DT%2529%2520to%2520accelerate%2520the%2520learning%2520process.%2520By%2520incorporating%2520DT-guided%250Astrategies%252C%2520we%2520optimize%2520monitoring%2520quality%2520and%2520energy%2520sustainability%252C%250Asignificantly%2520reducing%2520training%2520time%2520while%2520achieving%2520comparable%2520performance%250Arewards.%2520Our%2520experimental%2520results%2520prove%2520that%2520DT-guided%2520DRL%2520outperforms%250ATL-enhanced%2520DRL%2520models%252C%2520improving%2520system%2520performance%2520and%2520reducing%2520training%250Aruntime%2520by%252047.5%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sustainable%20Smart%20Farm%20Networks%3A%20Enhancing%20Resilience%20and%20Efficiency%0A%20%20with%20Decision%20Theory-Guided%20Deep%20Reinforcement%20Learning&entry.906535625=Dian%20Chen%20and%20Zelin%20Wan%20and%20Dong%20Sam%20Ha%20and%20Jin-Hee%20Cho&entry.1292438233=%20%20Solar%20sensor-based%20monitoring%20systems%20have%20become%20a%20crucial%20agricultural%0Ainnovation%2C%20advancing%20farm%20management%20and%20animal%20welfare%20through%20integrating%0Asensor%20technology%2C%20Internet-of-Things%2C%20and%20edge%20and%20cloud%20computing.%20However%2C%0Athe%20resilience%20of%20these%20systems%20to%20cyber-attacks%20and%20their%20adaptability%20to%0Adynamic%20and%20constrained%20energy%20supplies%20remain%20largely%20unexplored.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20sustainable%20smart%20farm%20network%20designed%20to%0Amaintain%20high-quality%20animal%20monitoring%20under%20various%20cyber%20and%20adversarial%0Athreats%2C%20as%20well%20as%20fluctuating%20energy%20conditions.%20Our%20approach%20utilizes%20deep%0Areinforcement%20learning%20%28DRL%29%20to%20devise%20optimal%20policies%20that%20maximize%20both%0Amonitoring%20effectiveness%20and%20energy%20efficiency.%20To%20overcome%20DRL%27s%20inherent%0Achallenge%20of%20slow%20convergence%2C%20we%20integrate%20transfer%20learning%20%28TL%29%20and%20decision%0Atheory%20%28DT%29%20to%20accelerate%20the%20learning%20process.%20By%20incorporating%20DT-guided%0Astrategies%2C%20we%20optimize%20monitoring%20quality%20and%20energy%20sustainability%2C%0Asignificantly%20reducing%20training%20time%20while%20achieving%20comparable%20performance%0Arewards.%20Our%20experimental%20results%20prove%20that%20DT-guided%20DRL%20outperforms%0ATL-enhanced%20DRL%20models%2C%20improving%20system%20performance%20and%20reducing%20training%0Aruntime%20by%2047.5%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03721v1&entry.124074799=Read"},
{"title": "Pushing the boundary on Natural Language Inference", "author": "Pablo Miralles-Gonz\u00e1lez and Javier Huertas-Tato and Alejandro Mart\u00edn and David Camacho", "abstract": "  Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.\n", "link": "http://arxiv.org/abs/2504.18376v2", "date": "2025-05-06", "relevancy": 1.953, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20the%20boundary%20on%20Natural%20Language%20Inference&body=Title%3A%20Pushing%20the%20boundary%20on%20Natural%20Language%20Inference%0AAuthor%3A%20Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho%0AAbstract%3A%20%20%20Natural%20Language%20Inference%20%28NLI%29%20is%20a%20central%20task%20in%20natural%20language%0Aunderstanding%20with%20applications%20in%20fact-checking%2C%20question%20answering%2C%20and%0Ainformation%20retrieval.%20Despite%20its%20importance%2C%20current%20NLI%20systems%20heavily%20rely%0Aon%20supervised%20learning%20with%20datasets%20that%20often%20contain%20annotation%20artifacts%0Aand%20biases%2C%20limiting%20generalization%20and%20real-world%20applicability.%20In%20this%20work%2C%0Awe%20apply%20a%20reinforcement%20learning-based%20approach%20using%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%20for%20Chain-of-Thought%20%28CoT%29%20learning%20in%20NLI%2C%20eliminating%20the%0Aneed%20for%20labeled%20rationales%20and%20enabling%20this%20type%20of%20training%20on%20more%0Achallenging%20datasets%20such%20as%20ANLI.%20We%20fine-tune%207B%2C%2014B%2C%20and%2032B%20language%0Amodels%20using%20parameter-efficient%20techniques%20%28LoRA%20and%20QLoRA%29%2C%20demonstrating%0Astrong%20performance%20across%20standard%20and%20adversarial%20NLI%20benchmarks.%20Our%2032B%0AAWQ-quantized%20model%20surpasses%20state-of-the-art%20results%20on%207%20out%20of%2011%0Aadversarial%20sets%24%5Cunicode%7Bx2013%7D%24or%20on%20all%20of%20them%20considering%20our%0Areplication%24%5Cunicode%7Bx2013%7D%24within%20a%2022GB%20memory%20footprint%2C%20showing%20that%20robust%0Areasoning%20can%20be%20retained%20under%20aggressive%20quantization.%20This%20work%20provides%20a%0Ascalable%20and%20practical%20framework%20for%20building%20robust%20NLI%20systems%20without%0Asacrificing%20inference%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520the%2520boundary%2520on%2520Natural%2520Language%2520Inference%26entry.906535625%3DPablo%2520Miralles-Gonz%25C3%25A1lez%2520and%2520Javier%2520Huertas-Tato%2520and%2520Alejandro%2520Mart%25C3%25ADn%2520and%2520David%2520Camacho%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Inference%2520%2528NLI%2529%2520is%2520a%2520central%2520task%2520in%2520natural%2520language%250Aunderstanding%2520with%2520applications%2520in%2520fact-checking%252C%2520question%2520answering%252C%2520and%250Ainformation%2520retrieval.%2520Despite%2520its%2520importance%252C%2520current%2520NLI%2520systems%2520heavily%2520rely%250Aon%2520supervised%2520learning%2520with%2520datasets%2520that%2520often%2520contain%2520annotation%2520artifacts%250Aand%2520biases%252C%2520limiting%2520generalization%2520and%2520real-world%2520applicability.%2520In%2520this%2520work%252C%250Awe%2520apply%2520a%2520reinforcement%2520learning-based%2520approach%2520using%2520Group%2520Relative%2520Policy%250AOptimization%2520%2528GRPO%2529%2520for%2520Chain-of-Thought%2520%2528CoT%2529%2520learning%2520in%2520NLI%252C%2520eliminating%2520the%250Aneed%2520for%2520labeled%2520rationales%2520and%2520enabling%2520this%2520type%2520of%2520training%2520on%2520more%250Achallenging%2520datasets%2520such%2520as%2520ANLI.%2520We%2520fine-tune%25207B%252C%252014B%252C%2520and%252032B%2520language%250Amodels%2520using%2520parameter-efficient%2520techniques%2520%2528LoRA%2520and%2520QLoRA%2529%252C%2520demonstrating%250Astrong%2520performance%2520across%2520standard%2520and%2520adversarial%2520NLI%2520benchmarks.%2520Our%252032B%250AAWQ-quantized%2520model%2520surpasses%2520state-of-the-art%2520results%2520on%25207%2520out%2520of%252011%250Aadversarial%2520sets%2524%255Cunicode%257Bx2013%257D%2524or%2520on%2520all%2520of%2520them%2520considering%2520our%250Areplication%2524%255Cunicode%257Bx2013%257D%2524within%2520a%252022GB%2520memory%2520footprint%252C%2520showing%2520that%2520robust%250Areasoning%2520can%2520be%2520retained%2520under%2520aggressive%2520quantization.%2520This%2520work%2520provides%2520a%250Ascalable%2520and%2520practical%2520framework%2520for%2520building%2520robust%2520NLI%2520systems%2520without%250Asacrificing%2520inference%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20the%20boundary%20on%20Natural%20Language%20Inference&entry.906535625=Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho&entry.1292438233=%20%20Natural%20Language%20Inference%20%28NLI%29%20is%20a%20central%20task%20in%20natural%20language%0Aunderstanding%20with%20applications%20in%20fact-checking%2C%20question%20answering%2C%20and%0Ainformation%20retrieval.%20Despite%20its%20importance%2C%20current%20NLI%20systems%20heavily%20rely%0Aon%20supervised%20learning%20with%20datasets%20that%20often%20contain%20annotation%20artifacts%0Aand%20biases%2C%20limiting%20generalization%20and%20real-world%20applicability.%20In%20this%20work%2C%0Awe%20apply%20a%20reinforcement%20learning-based%20approach%20using%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%20for%20Chain-of-Thought%20%28CoT%29%20learning%20in%20NLI%2C%20eliminating%20the%0Aneed%20for%20labeled%20rationales%20and%20enabling%20this%20type%20of%20training%20on%20more%0Achallenging%20datasets%20such%20as%20ANLI.%20We%20fine-tune%207B%2C%2014B%2C%20and%2032B%20language%0Amodels%20using%20parameter-efficient%20techniques%20%28LoRA%20and%20QLoRA%29%2C%20demonstrating%0Astrong%20performance%20across%20standard%20and%20adversarial%20NLI%20benchmarks.%20Our%2032B%0AAWQ-quantized%20model%20surpasses%20state-of-the-art%20results%20on%207%20out%20of%2011%0Aadversarial%20sets%24%5Cunicode%7Bx2013%7D%24or%20on%20all%20of%20them%20considering%20our%0Areplication%24%5Cunicode%7Bx2013%7D%24within%20a%2022GB%20memory%20footprint%2C%20showing%20that%20robust%0Areasoning%20can%20be%20retained%20under%20aggressive%20quantization.%20This%20work%20provides%20a%0Ascalable%20and%20practical%20framework%20for%20building%20robust%20NLI%20systems%20without%0Asacrificing%20inference%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18376v2&entry.124074799=Read"},
{"title": "Improving Omics-Based Classification: The Role of Feature Selection and\n  Synthetic Data Generation", "author": "Diego Perazzolo and Pietro Fanton and Ilaria Barison and Marny Fedrigo and Annalisa Angelini and Chiara Castellani and Enrico Grisan", "abstract": "  Given the increasing complexity of omics datasets, a key challenge is not\nonly improving classification performance but also enhancing the transparency\nand reliability of model decisions. Effective model performance and feature\nselection are fundamental for explainability and reliability. In many cases,\nhigh dimensional omics datasets suffer from limited number of samples due to\nclinical constraints, patient conditions, phenotypes rarity and others\nconditions. Current omics based classification models often suffer from narrow\ninterpretability, making it difficult to discern meaningful insights where\ntrust and reproducibility are critical. This study presents a machine learning\nbased classification framework that integrates feature selection with data\naugmentation techniques to achieve high standard classification accuracy while\nensuring better interpretability. Using the publicly available dataset (E MTAB\n8026), we explore a bootstrap analysis in six binary classification scenarios\nto evaluate the proposed model's behaviour. We show that the proposed pipeline\nyields cross validated perfomance on small dataset that is conserved when the\ntrained classifier is applied to a larger test set. Our findings emphasize the\nfundamental balance between accuracy and feature selection, highlighting the\npositive effect of introducing synthetic data for better generalization, even\nin scenarios with very limited samples availability.\n", "link": "http://arxiv.org/abs/2505.03387v1", "date": "2025-05-06", "relevancy": 1.9502, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5038}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4794}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Omics-Based%20Classification%3A%20The%20Role%20of%20Feature%20Selection%20and%0A%20%20Synthetic%20Data%20Generation&body=Title%3A%20Improving%20Omics-Based%20Classification%3A%20The%20Role%20of%20Feature%20Selection%20and%0A%20%20Synthetic%20Data%20Generation%0AAuthor%3A%20Diego%20Perazzolo%20and%20Pietro%20Fanton%20and%20Ilaria%20Barison%20and%20Marny%20Fedrigo%20and%20Annalisa%20Angelini%20and%20Chiara%20Castellani%20and%20Enrico%20Grisan%0AAbstract%3A%20%20%20Given%20the%20increasing%20complexity%20of%20omics%20datasets%2C%20a%20key%20challenge%20is%20not%0Aonly%20improving%20classification%20performance%20but%20also%20enhancing%20the%20transparency%0Aand%20reliability%20of%20model%20decisions.%20Effective%20model%20performance%20and%20feature%0Aselection%20are%20fundamental%20for%20explainability%20and%20reliability.%20In%20many%20cases%2C%0Ahigh%20dimensional%20omics%20datasets%20suffer%20from%20limited%20number%20of%20samples%20due%20to%0Aclinical%20constraints%2C%20patient%20conditions%2C%20phenotypes%20rarity%20and%20others%0Aconditions.%20Current%20omics%20based%20classification%20models%20often%20suffer%20from%20narrow%0Ainterpretability%2C%20making%20it%20difficult%20to%20discern%20meaningful%20insights%20where%0Atrust%20and%20reproducibility%20are%20critical.%20This%20study%20presents%20a%20machine%20learning%0Abased%20classification%20framework%20that%20integrates%20feature%20selection%20with%20data%0Aaugmentation%20techniques%20to%20achieve%20high%20standard%20classification%20accuracy%20while%0Aensuring%20better%20interpretability.%20Using%20the%20publicly%20available%20dataset%20%28E%20MTAB%0A8026%29%2C%20we%20explore%20a%20bootstrap%20analysis%20in%20six%20binary%20classification%20scenarios%0Ato%20evaluate%20the%20proposed%20model%27s%20behaviour.%20We%20show%20that%20the%20proposed%20pipeline%0Ayields%20cross%20validated%20perfomance%20on%20small%20dataset%20that%20is%20conserved%20when%20the%0Atrained%20classifier%20is%20applied%20to%20a%20larger%20test%20set.%20Our%20findings%20emphasize%20the%0Afundamental%20balance%20between%20accuracy%20and%20feature%20selection%2C%20highlighting%20the%0Apositive%20effect%20of%20introducing%20synthetic%20data%20for%20better%20generalization%2C%20even%0Ain%20scenarios%20with%20very%20limited%20samples%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Omics-Based%2520Classification%253A%2520The%2520Role%2520of%2520Feature%2520Selection%2520and%250A%2520%2520Synthetic%2520Data%2520Generation%26entry.906535625%3DDiego%2520Perazzolo%2520and%2520Pietro%2520Fanton%2520and%2520Ilaria%2520Barison%2520and%2520Marny%2520Fedrigo%2520and%2520Annalisa%2520Angelini%2520and%2520Chiara%2520Castellani%2520and%2520Enrico%2520Grisan%26entry.1292438233%3D%2520%2520Given%2520the%2520increasing%2520complexity%2520of%2520omics%2520datasets%252C%2520a%2520key%2520challenge%2520is%2520not%250Aonly%2520improving%2520classification%2520performance%2520but%2520also%2520enhancing%2520the%2520transparency%250Aand%2520reliability%2520of%2520model%2520decisions.%2520Effective%2520model%2520performance%2520and%2520feature%250Aselection%2520are%2520fundamental%2520for%2520explainability%2520and%2520reliability.%2520In%2520many%2520cases%252C%250Ahigh%2520dimensional%2520omics%2520datasets%2520suffer%2520from%2520limited%2520number%2520of%2520samples%2520due%2520to%250Aclinical%2520constraints%252C%2520patient%2520conditions%252C%2520phenotypes%2520rarity%2520and%2520others%250Aconditions.%2520Current%2520omics%2520based%2520classification%2520models%2520often%2520suffer%2520from%2520narrow%250Ainterpretability%252C%2520making%2520it%2520difficult%2520to%2520discern%2520meaningful%2520insights%2520where%250Atrust%2520and%2520reproducibility%2520are%2520critical.%2520This%2520study%2520presents%2520a%2520machine%2520learning%250Abased%2520classification%2520framework%2520that%2520integrates%2520feature%2520selection%2520with%2520data%250Aaugmentation%2520techniques%2520to%2520achieve%2520high%2520standard%2520classification%2520accuracy%2520while%250Aensuring%2520better%2520interpretability.%2520Using%2520the%2520publicly%2520available%2520dataset%2520%2528E%2520MTAB%250A8026%2529%252C%2520we%2520explore%2520a%2520bootstrap%2520analysis%2520in%2520six%2520binary%2520classification%2520scenarios%250Ato%2520evaluate%2520the%2520proposed%2520model%2527s%2520behaviour.%2520We%2520show%2520that%2520the%2520proposed%2520pipeline%250Ayields%2520cross%2520validated%2520perfomance%2520on%2520small%2520dataset%2520that%2520is%2520conserved%2520when%2520the%250Atrained%2520classifier%2520is%2520applied%2520to%2520a%2520larger%2520test%2520set.%2520Our%2520findings%2520emphasize%2520the%250Afundamental%2520balance%2520between%2520accuracy%2520and%2520feature%2520selection%252C%2520highlighting%2520the%250Apositive%2520effect%2520of%2520introducing%2520synthetic%2520data%2520for%2520better%2520generalization%252C%2520even%250Ain%2520scenarios%2520with%2520very%2520limited%2520samples%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Omics-Based%20Classification%3A%20The%20Role%20of%20Feature%20Selection%20and%0A%20%20Synthetic%20Data%20Generation&entry.906535625=Diego%20Perazzolo%20and%20Pietro%20Fanton%20and%20Ilaria%20Barison%20and%20Marny%20Fedrigo%20and%20Annalisa%20Angelini%20and%20Chiara%20Castellani%20and%20Enrico%20Grisan&entry.1292438233=%20%20Given%20the%20increasing%20complexity%20of%20omics%20datasets%2C%20a%20key%20challenge%20is%20not%0Aonly%20improving%20classification%20performance%20but%20also%20enhancing%20the%20transparency%0Aand%20reliability%20of%20model%20decisions.%20Effective%20model%20performance%20and%20feature%0Aselection%20are%20fundamental%20for%20explainability%20and%20reliability.%20In%20many%20cases%2C%0Ahigh%20dimensional%20omics%20datasets%20suffer%20from%20limited%20number%20of%20samples%20due%20to%0Aclinical%20constraints%2C%20patient%20conditions%2C%20phenotypes%20rarity%20and%20others%0Aconditions.%20Current%20omics%20based%20classification%20models%20often%20suffer%20from%20narrow%0Ainterpretability%2C%20making%20it%20difficult%20to%20discern%20meaningful%20insights%20where%0Atrust%20and%20reproducibility%20are%20critical.%20This%20study%20presents%20a%20machine%20learning%0Abased%20classification%20framework%20that%20integrates%20feature%20selection%20with%20data%0Aaugmentation%20techniques%20to%20achieve%20high%20standard%20classification%20accuracy%20while%0Aensuring%20better%20interpretability.%20Using%20the%20publicly%20available%20dataset%20%28E%20MTAB%0A8026%29%2C%20we%20explore%20a%20bootstrap%20analysis%20in%20six%20binary%20classification%20scenarios%0Ato%20evaluate%20the%20proposed%20model%27s%20behaviour.%20We%20show%20that%20the%20proposed%20pipeline%0Ayields%20cross%20validated%20perfomance%20on%20small%20dataset%20that%20is%20conserved%20when%20the%0Atrained%20classifier%20is%20applied%20to%20a%20larger%20test%20set.%20Our%20findings%20emphasize%20the%0Afundamental%20balance%20between%20accuracy%20and%20feature%20selection%2C%20highlighting%20the%0Apositive%20effect%20of%20introducing%20synthetic%20data%20for%20better%20generalization%2C%20even%0Ain%20scenarios%20with%20very%20limited%20samples%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03387v1&entry.124074799=Read"},
{"title": "A Synergistic Framework of Nonlinear Acoustic Computing and\n  Reinforcement Learning for Real-World Human-Robot Interaction", "author": "Xiaoliang Chen and Xin Yu and Le Chang and Yunhe Huang and Jiashuai He and Shibo Zhang and Jin Li and Likai Lin and Ziyu Zeng and Xianling Tu and Shuyu Zhang", "abstract": "  This paper introduces a novel framework integrating nonlinear acoustic\ncomputing and reinforcement learning to enhance advanced human-robot\ninteraction under complex noise and reverberation. Leveraging physically\ninformed wave equations (e.g., Westervelt, KZK), the approach captures\nhigher-order phenomena such as harmonic generation and shock formation. By\nembedding these models in a reinforcement learning-driven control loop, the\nsystem adaptively optimizes key parameters (e.g., absorption, beamforming) to\nmitigate multipath interference and non-stationary noise. Experimental\nevaluations, covering far-field localization, weak signal detection, and\nmultilingual speech recognition, demonstrate that this hybrid strategy\nsurpasses traditional linear methods and purely data-driven baselines,\nachieving superior noise suppression, minimal latency, and robust accuracy in\ndemanding real-world scenarios. The proposed system demonstrates broad\napplication prospects in AI hardware, robot, machine audition, artificial\naudition, and brain-machine interfaces.\n", "link": "http://arxiv.org/abs/2505.01998v2", "date": "2025-05-06", "relevancy": 1.5116, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5457}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5014}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Synergistic%20Framework%20of%20Nonlinear%20Acoustic%20Computing%20and%0A%20%20Reinforcement%20Learning%20for%20Real-World%20Human-Robot%20Interaction&body=Title%3A%20A%20Synergistic%20Framework%20of%20Nonlinear%20Acoustic%20Computing%20and%0A%20%20Reinforcement%20Learning%20for%20Real-World%20Human-Robot%20Interaction%0AAuthor%3A%20Xiaoliang%20Chen%20and%20Xin%20Yu%20and%20Le%20Chang%20and%20Yunhe%20Huang%20and%20Jiashuai%20He%20and%20Shibo%20Zhang%20and%20Jin%20Li%20and%20Likai%20Lin%20and%20Ziyu%20Zeng%20and%20Xianling%20Tu%20and%20Shuyu%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20framework%20integrating%20nonlinear%20acoustic%0Acomputing%20and%20reinforcement%20learning%20to%20enhance%20advanced%20human-robot%0Ainteraction%20under%20complex%20noise%20and%20reverberation.%20Leveraging%20physically%0Ainformed%20wave%20equations%20%28e.g.%2C%20Westervelt%2C%20KZK%29%2C%20the%20approach%20captures%0Ahigher-order%20phenomena%20such%20as%20harmonic%20generation%20and%20shock%20formation.%20By%0Aembedding%20these%20models%20in%20a%20reinforcement%20learning-driven%20control%20loop%2C%20the%0Asystem%20adaptively%20optimizes%20key%20parameters%20%28e.g.%2C%20absorption%2C%20beamforming%29%20to%0Amitigate%20multipath%20interference%20and%20non-stationary%20noise.%20Experimental%0Aevaluations%2C%20covering%20far-field%20localization%2C%20weak%20signal%20detection%2C%20and%0Amultilingual%20speech%20recognition%2C%20demonstrate%20that%20this%20hybrid%20strategy%0Asurpasses%20traditional%20linear%20methods%20and%20purely%20data-driven%20baselines%2C%0Aachieving%20superior%20noise%20suppression%2C%20minimal%20latency%2C%20and%20robust%20accuracy%20in%0Ademanding%20real-world%20scenarios.%20The%20proposed%20system%20demonstrates%20broad%0Aapplication%20prospects%20in%20AI%20hardware%2C%20robot%2C%20machine%20audition%2C%20artificial%0Aaudition%2C%20and%20brain-machine%20interfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Synergistic%2520Framework%2520of%2520Nonlinear%2520Acoustic%2520Computing%2520and%250A%2520%2520Reinforcement%2520Learning%2520for%2520Real-World%2520Human-Robot%2520Interaction%26entry.906535625%3DXiaoliang%2520Chen%2520and%2520Xin%2520Yu%2520and%2520Le%2520Chang%2520and%2520Yunhe%2520Huang%2520and%2520Jiashuai%2520He%2520and%2520Shibo%2520Zhang%2520and%2520Jin%2520Li%2520and%2520Likai%2520Lin%2520and%2520Ziyu%2520Zeng%2520and%2520Xianling%2520Tu%2520and%2520Shuyu%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520framework%2520integrating%2520nonlinear%2520acoustic%250Acomputing%2520and%2520reinforcement%2520learning%2520to%2520enhance%2520advanced%2520human-robot%250Ainteraction%2520under%2520complex%2520noise%2520and%2520reverberation.%2520Leveraging%2520physically%250Ainformed%2520wave%2520equations%2520%2528e.g.%252C%2520Westervelt%252C%2520KZK%2529%252C%2520the%2520approach%2520captures%250Ahigher-order%2520phenomena%2520such%2520as%2520harmonic%2520generation%2520and%2520shock%2520formation.%2520By%250Aembedding%2520these%2520models%2520in%2520a%2520reinforcement%2520learning-driven%2520control%2520loop%252C%2520the%250Asystem%2520adaptively%2520optimizes%2520key%2520parameters%2520%2528e.g.%252C%2520absorption%252C%2520beamforming%2529%2520to%250Amitigate%2520multipath%2520interference%2520and%2520non-stationary%2520noise.%2520Experimental%250Aevaluations%252C%2520covering%2520far-field%2520localization%252C%2520weak%2520signal%2520detection%252C%2520and%250Amultilingual%2520speech%2520recognition%252C%2520demonstrate%2520that%2520this%2520hybrid%2520strategy%250Asurpasses%2520traditional%2520linear%2520methods%2520and%2520purely%2520data-driven%2520baselines%252C%250Aachieving%2520superior%2520noise%2520suppression%252C%2520minimal%2520latency%252C%2520and%2520robust%2520accuracy%2520in%250Ademanding%2520real-world%2520scenarios.%2520The%2520proposed%2520system%2520demonstrates%2520broad%250Aapplication%2520prospects%2520in%2520AI%2520hardware%252C%2520robot%252C%2520machine%2520audition%252C%2520artificial%250Aaudition%252C%2520and%2520brain-machine%2520interfaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Synergistic%20Framework%20of%20Nonlinear%20Acoustic%20Computing%20and%0A%20%20Reinforcement%20Learning%20for%20Real-World%20Human-Robot%20Interaction&entry.906535625=Xiaoliang%20Chen%20and%20Xin%20Yu%20and%20Le%20Chang%20and%20Yunhe%20Huang%20and%20Jiashuai%20He%20and%20Shibo%20Zhang%20and%20Jin%20Li%20and%20Likai%20Lin%20and%20Ziyu%20Zeng%20and%20Xianling%20Tu%20and%20Shuyu%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20framework%20integrating%20nonlinear%20acoustic%0Acomputing%20and%20reinforcement%20learning%20to%20enhance%20advanced%20human-robot%0Ainteraction%20under%20complex%20noise%20and%20reverberation.%20Leveraging%20physically%0Ainformed%20wave%20equations%20%28e.g.%2C%20Westervelt%2C%20KZK%29%2C%20the%20approach%20captures%0Ahigher-order%20phenomena%20such%20as%20harmonic%20generation%20and%20shock%20formation.%20By%0Aembedding%20these%20models%20in%20a%20reinforcement%20learning-driven%20control%20loop%2C%20the%0Asystem%20adaptively%20optimizes%20key%20parameters%20%28e.g.%2C%20absorption%2C%20beamforming%29%20to%0Amitigate%20multipath%20interference%20and%20non-stationary%20noise.%20Experimental%0Aevaluations%2C%20covering%20far-field%20localization%2C%20weak%20signal%20detection%2C%20and%0Amultilingual%20speech%20recognition%2C%20demonstrate%20that%20this%20hybrid%20strategy%0Asurpasses%20traditional%20linear%20methods%20and%20purely%20data-driven%20baselines%2C%0Aachieving%20superior%20noise%20suppression%2C%20minimal%20latency%2C%20and%20robust%20accuracy%20in%0Ademanding%20real-world%20scenarios.%20The%20proposed%20system%20demonstrates%20broad%0Aapplication%20prospects%20in%20AI%20hardware%2C%20robot%2C%20machine%20audition%2C%20artificial%0Aaudition%2C%20and%20brain-machine%20interfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01998v2&entry.124074799=Read"},
{"title": "Nonparametric IPSS: Fast, flexible feature selection with false\n  discovery control", "author": "Omar Melikechi and David B. Dunson and Jeffrey W. Miller", "abstract": "  Feature selection is a critical task in machine learning and statistics.\nHowever, existing feature selection methods either (i) rely on parametric\nmethods such as linear or generalized linear models, (ii) lack theoretical\nfalse discovery control, or (iii) identify few true positives. Here, we\nintroduce a general feature selection method with finite-sample false discovery\ncontrol based on applying integrated path stability selection (IPSS) to\narbitrary feature importance scores. The method is nonparametric whenever the\nimportance scores are nonparametric, and it estimates q-values, which are\nbetter suited to high-dimensional data than p-values. We focus on two special\ncases using importance scores from gradient boosting (IPSSGB) and random\nforests (IPSSRF). Extensive nonlinear simulations with RNA sequencing data show\nthat both methods accurately control the false discovery rate and detect more\ntrue positives than existing methods. Both methods are also efficient, running\nin under 20 seconds when there are 500 samples and 5000 features. We apply\nIPSSGB and IPSSRF to detect microRNAs and genes related to cancer, finding that\nthey yield better predictions with fewer features than existing approaches.\n", "link": "http://arxiv.org/abs/2410.02208v2", "date": "2025-05-06", "relevancy": 1.6766, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4445}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4075}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonparametric%20IPSS%3A%20Fast%2C%20flexible%20feature%20selection%20with%20false%0A%20%20discovery%20control&body=Title%3A%20Nonparametric%20IPSS%3A%20Fast%2C%20flexible%20feature%20selection%20with%20false%0A%20%20discovery%20control%0AAuthor%3A%20Omar%20Melikechi%20and%20David%20B.%20Dunson%20and%20Jeffrey%20W.%20Miller%0AAbstract%3A%20%20%20Feature%20selection%20is%20a%20critical%20task%20in%20machine%20learning%20and%20statistics.%0AHowever%2C%20existing%20feature%20selection%20methods%20either%20%28i%29%20rely%20on%20parametric%0Amethods%20such%20as%20linear%20or%20generalized%20linear%20models%2C%20%28ii%29%20lack%20theoretical%0Afalse%20discovery%20control%2C%20or%20%28iii%29%20identify%20few%20true%20positives.%20Here%2C%20we%0Aintroduce%20a%20general%20feature%20selection%20method%20with%20finite-sample%20false%20discovery%0Acontrol%20based%20on%20applying%20integrated%20path%20stability%20selection%20%28IPSS%29%20to%0Aarbitrary%20feature%20importance%20scores.%20The%20method%20is%20nonparametric%20whenever%20the%0Aimportance%20scores%20are%20nonparametric%2C%20and%20it%20estimates%20q-values%2C%20which%20are%0Abetter%20suited%20to%20high-dimensional%20data%20than%20p-values.%20We%20focus%20on%20two%20special%0Acases%20using%20importance%20scores%20from%20gradient%20boosting%20%28IPSSGB%29%20and%20random%0Aforests%20%28IPSSRF%29.%20Extensive%20nonlinear%20simulations%20with%20RNA%20sequencing%20data%20show%0Athat%20both%20methods%20accurately%20control%20the%20false%20discovery%20rate%20and%20detect%20more%0Atrue%20positives%20than%20existing%20methods.%20Both%20methods%20are%20also%20efficient%2C%20running%0Ain%20under%2020%20seconds%20when%20there%20are%20500%20samples%20and%205000%20features.%20We%20apply%0AIPSSGB%20and%20IPSSRF%20to%20detect%20microRNAs%20and%20genes%20related%20to%20cancer%2C%20finding%20that%0Athey%20yield%20better%20predictions%20with%20fewer%20features%20than%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonparametric%2520IPSS%253A%2520Fast%252C%2520flexible%2520feature%2520selection%2520with%2520false%250A%2520%2520discovery%2520control%26entry.906535625%3DOmar%2520Melikechi%2520and%2520David%2520B.%2520Dunson%2520and%2520Jeffrey%2520W.%2520Miller%26entry.1292438233%3D%2520%2520Feature%2520selection%2520is%2520a%2520critical%2520task%2520in%2520machine%2520learning%2520and%2520statistics.%250AHowever%252C%2520existing%2520feature%2520selection%2520methods%2520either%2520%2528i%2529%2520rely%2520on%2520parametric%250Amethods%2520such%2520as%2520linear%2520or%2520generalized%2520linear%2520models%252C%2520%2528ii%2529%2520lack%2520theoretical%250Afalse%2520discovery%2520control%252C%2520or%2520%2528iii%2529%2520identify%2520few%2520true%2520positives.%2520Here%252C%2520we%250Aintroduce%2520a%2520general%2520feature%2520selection%2520method%2520with%2520finite-sample%2520false%2520discovery%250Acontrol%2520based%2520on%2520applying%2520integrated%2520path%2520stability%2520selection%2520%2528IPSS%2529%2520to%250Aarbitrary%2520feature%2520importance%2520scores.%2520The%2520method%2520is%2520nonparametric%2520whenever%2520the%250Aimportance%2520scores%2520are%2520nonparametric%252C%2520and%2520it%2520estimates%2520q-values%252C%2520which%2520are%250Abetter%2520suited%2520to%2520high-dimensional%2520data%2520than%2520p-values.%2520We%2520focus%2520on%2520two%2520special%250Acases%2520using%2520importance%2520scores%2520from%2520gradient%2520boosting%2520%2528IPSSGB%2529%2520and%2520random%250Aforests%2520%2528IPSSRF%2529.%2520Extensive%2520nonlinear%2520simulations%2520with%2520RNA%2520sequencing%2520data%2520show%250Athat%2520both%2520methods%2520accurately%2520control%2520the%2520false%2520discovery%2520rate%2520and%2520detect%2520more%250Atrue%2520positives%2520than%2520existing%2520methods.%2520Both%2520methods%2520are%2520also%2520efficient%252C%2520running%250Ain%2520under%252020%2520seconds%2520when%2520there%2520are%2520500%2520samples%2520and%25205000%2520features.%2520We%2520apply%250AIPSSGB%2520and%2520IPSSRF%2520to%2520detect%2520microRNAs%2520and%2520genes%2520related%2520to%2520cancer%252C%2520finding%2520that%250Athey%2520yield%2520better%2520predictions%2520with%2520fewer%2520features%2520than%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonparametric%20IPSS%3A%20Fast%2C%20flexible%20feature%20selection%20with%20false%0A%20%20discovery%20control&entry.906535625=Omar%20Melikechi%20and%20David%20B.%20Dunson%20and%20Jeffrey%20W.%20Miller&entry.1292438233=%20%20Feature%20selection%20is%20a%20critical%20task%20in%20machine%20learning%20and%20statistics.%0AHowever%2C%20existing%20feature%20selection%20methods%20either%20%28i%29%20rely%20on%20parametric%0Amethods%20such%20as%20linear%20or%20generalized%20linear%20models%2C%20%28ii%29%20lack%20theoretical%0Afalse%20discovery%20control%2C%20or%20%28iii%29%20identify%20few%20true%20positives.%20Here%2C%20we%0Aintroduce%20a%20general%20feature%20selection%20method%20with%20finite-sample%20false%20discovery%0Acontrol%20based%20on%20applying%20integrated%20path%20stability%20selection%20%28IPSS%29%20to%0Aarbitrary%20feature%20importance%20scores.%20The%20method%20is%20nonparametric%20whenever%20the%0Aimportance%20scores%20are%20nonparametric%2C%20and%20it%20estimates%20q-values%2C%20which%20are%0Abetter%20suited%20to%20high-dimensional%20data%20than%20p-values.%20We%20focus%20on%20two%20special%0Acases%20using%20importance%20scores%20from%20gradient%20boosting%20%28IPSSGB%29%20and%20random%0Aforests%20%28IPSSRF%29.%20Extensive%20nonlinear%20simulations%20with%20RNA%20sequencing%20data%20show%0Athat%20both%20methods%20accurately%20control%20the%20false%20discovery%20rate%20and%20detect%20more%0Atrue%20positives%20than%20existing%20methods.%20Both%20methods%20are%20also%20efficient%2C%20running%0Ain%20under%2020%20seconds%20when%20there%20are%20500%20samples%20and%205000%20features.%20We%20apply%0AIPSSGB%20and%20IPSSRF%20to%20detect%20microRNAs%20and%20genes%20related%20to%20cancer%2C%20finding%20that%0Athey%20yield%20better%20predictions%20with%20fewer%20features%20than%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02208v2&entry.124074799=Read"},
{"title": "Towards Smart Point-and-Shoot Photography", "author": "Jiawan Li and Fei Zhou and Zhipeng Zhong and Jiongzhi Lin and Guoping Qiu", "abstract": "  Hundreds of millions of people routinely take photos using their smartphones\nas point and shoot (PAS) cameras, yet very few would have the photography\nskills to compose a good shot of a scene. While traditional PAS cameras have\nbuilt-in functions to ensure a photo is well focused and has the right\nbrightness, they cannot tell the users how to compose the best shot of a scene.\nIn this paper, we present a first of its kind smart point and shoot (SPAS)\nsystem to help users to take good photos. Our SPAS proposes to help users to\ncompose a good shot of a scene by automatically guiding the users to adjust the\ncamera pose live on the scene. We first constructed a large dataset containing\n320K images with camera pose information from 4000 scenes. We then developed an\ninnovative CLIP-based Composition Quality Assessment (CCQA) model to assign\npseudo labels to these images. The CCQA introduces a unique learnable text\nembedding technique to learn continuous word embeddings capable of discerning\nsubtle visual quality differences in the range covered by five levels of\nquality description words {bad, poor, fair, good, perfect}. And finally we have\ndeveloped a camera pose adjustment model (CPAM) which first determines if the\ncurrent view can be further improved and if so it outputs the adjust suggestion\nin the form of two camera pose adjustment angles. The two tasks of CPAM make\ndecisions in a sequential manner and each involves different sets of training\nsamples, we have developed a mixture-of-experts model with a gated loss\nfunction to train the CPAM in an end-to-end manner. We will present extensive\nresults to demonstrate the performances of our SPAS system using publicly\navailable image composition datasets.\n", "link": "http://arxiv.org/abs/2505.03638v1", "date": "2025-05-06", "relevancy": 1.6942, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5893}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5658}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Smart%20Point-and-Shoot%20Photography&body=Title%3A%20Towards%20Smart%20Point-and-Shoot%20Photography%0AAuthor%3A%20Jiawan%20Li%20and%20Fei%20Zhou%20and%20Zhipeng%20Zhong%20and%20Jiongzhi%20Lin%20and%20Guoping%20Qiu%0AAbstract%3A%20%20%20Hundreds%20of%20millions%20of%20people%20routinely%20take%20photos%20using%20their%20smartphones%0Aas%20point%20and%20shoot%20%28PAS%29%20cameras%2C%20yet%20very%20few%20would%20have%20the%20photography%0Askills%20to%20compose%20a%20good%20shot%20of%20a%20scene.%20While%20traditional%20PAS%20cameras%20have%0Abuilt-in%20functions%20to%20ensure%20a%20photo%20is%20well%20focused%20and%20has%20the%20right%0Abrightness%2C%20they%20cannot%20tell%20the%20users%20how%20to%20compose%20the%20best%20shot%20of%20a%20scene.%0AIn%20this%20paper%2C%20we%20present%20a%20first%20of%20its%20kind%20smart%20point%20and%20shoot%20%28SPAS%29%0Asystem%20to%20help%20users%20to%20take%20good%20photos.%20Our%20SPAS%20proposes%20to%20help%20users%20to%0Acompose%20a%20good%20shot%20of%20a%20scene%20by%20automatically%20guiding%20the%20users%20to%20adjust%20the%0Acamera%20pose%20live%20on%20the%20scene.%20We%20first%20constructed%20a%20large%20dataset%20containing%0A320K%20images%20with%20camera%20pose%20information%20from%204000%20scenes.%20We%20then%20developed%20an%0Ainnovative%20CLIP-based%20Composition%20Quality%20Assessment%20%28CCQA%29%20model%20to%20assign%0Apseudo%20labels%20to%20these%20images.%20The%20CCQA%20introduces%20a%20unique%20learnable%20text%0Aembedding%20technique%20to%20learn%20continuous%20word%20embeddings%20capable%20of%20discerning%0Asubtle%20visual%20quality%20differences%20in%20the%20range%20covered%20by%20five%20levels%20of%0Aquality%20description%20words%20%7Bbad%2C%20poor%2C%20fair%2C%20good%2C%20perfect%7D.%20And%20finally%20we%20have%0Adeveloped%20a%20camera%20pose%20adjustment%20model%20%28CPAM%29%20which%20first%20determines%20if%20the%0Acurrent%20view%20can%20be%20further%20improved%20and%20if%20so%20it%20outputs%20the%20adjust%20suggestion%0Ain%20the%20form%20of%20two%20camera%20pose%20adjustment%20angles.%20The%20two%20tasks%20of%20CPAM%20make%0Adecisions%20in%20a%20sequential%20manner%20and%20each%20involves%20different%20sets%20of%20training%0Asamples%2C%20we%20have%20developed%20a%20mixture-of-experts%20model%20with%20a%20gated%20loss%0Afunction%20to%20train%20the%20CPAM%20in%20an%20end-to-end%20manner.%20We%20will%20present%20extensive%0Aresults%20to%20demonstrate%20the%20performances%20of%20our%20SPAS%20system%20using%20publicly%0Aavailable%20image%20composition%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Smart%2520Point-and-Shoot%2520Photography%26entry.906535625%3DJiawan%2520Li%2520and%2520Fei%2520Zhou%2520and%2520Zhipeng%2520Zhong%2520and%2520Jiongzhi%2520Lin%2520and%2520Guoping%2520Qiu%26entry.1292438233%3D%2520%2520Hundreds%2520of%2520millions%2520of%2520people%2520routinely%2520take%2520photos%2520using%2520their%2520smartphones%250Aas%2520point%2520and%2520shoot%2520%2528PAS%2529%2520cameras%252C%2520yet%2520very%2520few%2520would%2520have%2520the%2520photography%250Askills%2520to%2520compose%2520a%2520good%2520shot%2520of%2520a%2520scene.%2520While%2520traditional%2520PAS%2520cameras%2520have%250Abuilt-in%2520functions%2520to%2520ensure%2520a%2520photo%2520is%2520well%2520focused%2520and%2520has%2520the%2520right%250Abrightness%252C%2520they%2520cannot%2520tell%2520the%2520users%2520how%2520to%2520compose%2520the%2520best%2520shot%2520of%2520a%2520scene.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520first%2520of%2520its%2520kind%2520smart%2520point%2520and%2520shoot%2520%2528SPAS%2529%250Asystem%2520to%2520help%2520users%2520to%2520take%2520good%2520photos.%2520Our%2520SPAS%2520proposes%2520to%2520help%2520users%2520to%250Acompose%2520a%2520good%2520shot%2520of%2520a%2520scene%2520by%2520automatically%2520guiding%2520the%2520users%2520to%2520adjust%2520the%250Acamera%2520pose%2520live%2520on%2520the%2520scene.%2520We%2520first%2520constructed%2520a%2520large%2520dataset%2520containing%250A320K%2520images%2520with%2520camera%2520pose%2520information%2520from%25204000%2520scenes.%2520We%2520then%2520developed%2520an%250Ainnovative%2520CLIP-based%2520Composition%2520Quality%2520Assessment%2520%2528CCQA%2529%2520model%2520to%2520assign%250Apseudo%2520labels%2520to%2520these%2520images.%2520The%2520CCQA%2520introduces%2520a%2520unique%2520learnable%2520text%250Aembedding%2520technique%2520to%2520learn%2520continuous%2520word%2520embeddings%2520capable%2520of%2520discerning%250Asubtle%2520visual%2520quality%2520differences%2520in%2520the%2520range%2520covered%2520by%2520five%2520levels%2520of%250Aquality%2520description%2520words%2520%257Bbad%252C%2520poor%252C%2520fair%252C%2520good%252C%2520perfect%257D.%2520And%2520finally%2520we%2520have%250Adeveloped%2520a%2520camera%2520pose%2520adjustment%2520model%2520%2528CPAM%2529%2520which%2520first%2520determines%2520if%2520the%250Acurrent%2520view%2520can%2520be%2520further%2520improved%2520and%2520if%2520so%2520it%2520outputs%2520the%2520adjust%2520suggestion%250Ain%2520the%2520form%2520of%2520two%2520camera%2520pose%2520adjustment%2520angles.%2520The%2520two%2520tasks%2520of%2520CPAM%2520make%250Adecisions%2520in%2520a%2520sequential%2520manner%2520and%2520each%2520involves%2520different%2520sets%2520of%2520training%250Asamples%252C%2520we%2520have%2520developed%2520a%2520mixture-of-experts%2520model%2520with%2520a%2520gated%2520loss%250Afunction%2520to%2520train%2520the%2520CPAM%2520in%2520an%2520end-to-end%2520manner.%2520We%2520will%2520present%2520extensive%250Aresults%2520to%2520demonstrate%2520the%2520performances%2520of%2520our%2520SPAS%2520system%2520using%2520publicly%250Aavailable%2520image%2520composition%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Smart%20Point-and-Shoot%20Photography&entry.906535625=Jiawan%20Li%20and%20Fei%20Zhou%20and%20Zhipeng%20Zhong%20and%20Jiongzhi%20Lin%20and%20Guoping%20Qiu&entry.1292438233=%20%20Hundreds%20of%20millions%20of%20people%20routinely%20take%20photos%20using%20their%20smartphones%0Aas%20point%20and%20shoot%20%28PAS%29%20cameras%2C%20yet%20very%20few%20would%20have%20the%20photography%0Askills%20to%20compose%20a%20good%20shot%20of%20a%20scene.%20While%20traditional%20PAS%20cameras%20have%0Abuilt-in%20functions%20to%20ensure%20a%20photo%20is%20well%20focused%20and%20has%20the%20right%0Abrightness%2C%20they%20cannot%20tell%20the%20users%20how%20to%20compose%20the%20best%20shot%20of%20a%20scene.%0AIn%20this%20paper%2C%20we%20present%20a%20first%20of%20its%20kind%20smart%20point%20and%20shoot%20%28SPAS%29%0Asystem%20to%20help%20users%20to%20take%20good%20photos.%20Our%20SPAS%20proposes%20to%20help%20users%20to%0Acompose%20a%20good%20shot%20of%20a%20scene%20by%20automatically%20guiding%20the%20users%20to%20adjust%20the%0Acamera%20pose%20live%20on%20the%20scene.%20We%20first%20constructed%20a%20large%20dataset%20containing%0A320K%20images%20with%20camera%20pose%20information%20from%204000%20scenes.%20We%20then%20developed%20an%0Ainnovative%20CLIP-based%20Composition%20Quality%20Assessment%20%28CCQA%29%20model%20to%20assign%0Apseudo%20labels%20to%20these%20images.%20The%20CCQA%20introduces%20a%20unique%20learnable%20text%0Aembedding%20technique%20to%20learn%20continuous%20word%20embeddings%20capable%20of%20discerning%0Asubtle%20visual%20quality%20differences%20in%20the%20range%20covered%20by%20five%20levels%20of%0Aquality%20description%20words%20%7Bbad%2C%20poor%2C%20fair%2C%20good%2C%20perfect%7D.%20And%20finally%20we%20have%0Adeveloped%20a%20camera%20pose%20adjustment%20model%20%28CPAM%29%20which%20first%20determines%20if%20the%0Acurrent%20view%20can%20be%20further%20improved%20and%20if%20so%20it%20outputs%20the%20adjust%20suggestion%0Ain%20the%20form%20of%20two%20camera%20pose%20adjustment%20angles.%20The%20two%20tasks%20of%20CPAM%20make%0Adecisions%20in%20a%20sequential%20manner%20and%20each%20involves%20different%20sets%20of%20training%0Asamples%2C%20we%20have%20developed%20a%20mixture-of-experts%20model%20with%20a%20gated%20loss%0Afunction%20to%20train%20the%20CPAM%20in%20an%20end-to-end%20manner.%20We%20will%20present%20extensive%0Aresults%20to%20demonstrate%20the%20performances%20of%20our%20SPAS%20system%20using%20publicly%0Aavailable%20image%20composition%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03638v1&entry.124074799=Read"},
{"title": "ParFam -- (Neural Guided) Symbolic Regression Based on Continuous Global\n  Optimization", "author": "Philipp Scholl and Katharina Bieker and Hillary Hauger and Gitta Kutyniok", "abstract": "  The problem of symbolic regression (SR) arises in many different\napplications, such as identifying physical laws or deriving mathematical\nequations describing the behavior of financial markets from given data. Various\nmethods exist to address the problem of SR, often based on genetic programming.\nHowever, these methods are usually complicated and involve various\nhyperparameters. In this paper, we present our new approach ParFam that\nutilizes parametric families of suitable symbolic functions to translate the\ndiscrete symbolic regression problem into a continuous one, resulting in a more\nstraightforward setup compared to current state-of-the-art methods. In\ncombination with a global optimizer, this approach results in a highly\neffective method to tackle the problem of SR. We theoretically analyze the\nexpressivity of ParFam and demonstrate its performance with extensive numerical\nexperiments based on the common SR benchmark suit SRBench, showing that we\nachieve state-of-the-art results. Moreover, we present an extension\nincorporating a pre-trained transformer network DL-ParFam to guide ParFam,\naccelerating the optimization process by up to two magnitudes. Our code and\nresults can be found at https://github.com/Philipp238/parfam.\n", "link": "http://arxiv.org/abs/2310.05537v4", "date": "2025-05-06", "relevancy": 0.9577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4731}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ParFam%20--%20%28Neural%20Guided%29%20Symbolic%20Regression%20Based%20on%20Continuous%20Global%0A%20%20Optimization&body=Title%3A%20ParFam%20--%20%28Neural%20Guided%29%20Symbolic%20Regression%20Based%20on%20Continuous%20Global%0A%20%20Optimization%0AAuthor%3A%20Philipp%20Scholl%20and%20Katharina%20Bieker%20and%20Hillary%20Hauger%20and%20Gitta%20Kutyniok%0AAbstract%3A%20%20%20The%20problem%20of%20symbolic%20regression%20%28SR%29%20arises%20in%20many%20different%0Aapplications%2C%20such%20as%20identifying%20physical%20laws%20or%20deriving%20mathematical%0Aequations%20describing%20the%20behavior%20of%20financial%20markets%20from%20given%20data.%20Various%0Amethods%20exist%20to%20address%20the%20problem%20of%20SR%2C%20often%20based%20on%20genetic%20programming.%0AHowever%2C%20these%20methods%20are%20usually%20complicated%20and%20involve%20various%0Ahyperparameters.%20In%20this%20paper%2C%20we%20present%20our%20new%20approach%20ParFam%20that%0Autilizes%20parametric%20families%20of%20suitable%20symbolic%20functions%20to%20translate%20the%0Adiscrete%20symbolic%20regression%20problem%20into%20a%20continuous%20one%2C%20resulting%20in%20a%20more%0Astraightforward%20setup%20compared%20to%20current%20state-of-the-art%20methods.%20In%0Acombination%20with%20a%20global%20optimizer%2C%20this%20approach%20results%20in%20a%20highly%0Aeffective%20method%20to%20tackle%20the%20problem%20of%20SR.%20We%20theoretically%20analyze%20the%0Aexpressivity%20of%20ParFam%20and%20demonstrate%20its%20performance%20with%20extensive%20numerical%0Aexperiments%20based%20on%20the%20common%20SR%20benchmark%20suit%20SRBench%2C%20showing%20that%20we%0Aachieve%20state-of-the-art%20results.%20Moreover%2C%20we%20present%20an%20extension%0Aincorporating%20a%20pre-trained%20transformer%20network%20DL-ParFam%20to%20guide%20ParFam%2C%0Aaccelerating%20the%20optimization%20process%20by%20up%20to%20two%20magnitudes.%20Our%20code%20and%0Aresults%20can%20be%20found%20at%20https%3A//github.com/Philipp238/parfam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05537v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParFam%2520--%2520%2528Neural%2520Guided%2529%2520Symbolic%2520Regression%2520Based%2520on%2520Continuous%2520Global%250A%2520%2520Optimization%26entry.906535625%3DPhilipp%2520Scholl%2520and%2520Katharina%2520Bieker%2520and%2520Hillary%2520Hauger%2520and%2520Gitta%2520Kutyniok%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520symbolic%2520regression%2520%2528SR%2529%2520arises%2520in%2520many%2520different%250Aapplications%252C%2520such%2520as%2520identifying%2520physical%2520laws%2520or%2520deriving%2520mathematical%250Aequations%2520describing%2520the%2520behavior%2520of%2520financial%2520markets%2520from%2520given%2520data.%2520Various%250Amethods%2520exist%2520to%2520address%2520the%2520problem%2520of%2520SR%252C%2520often%2520based%2520on%2520genetic%2520programming.%250AHowever%252C%2520these%2520methods%2520are%2520usually%2520complicated%2520and%2520involve%2520various%250Ahyperparameters.%2520In%2520this%2520paper%252C%2520we%2520present%2520our%2520new%2520approach%2520ParFam%2520that%250Autilizes%2520parametric%2520families%2520of%2520suitable%2520symbolic%2520functions%2520to%2520translate%2520the%250Adiscrete%2520symbolic%2520regression%2520problem%2520into%2520a%2520continuous%2520one%252C%2520resulting%2520in%2520a%2520more%250Astraightforward%2520setup%2520compared%2520to%2520current%2520state-of-the-art%2520methods.%2520In%250Acombination%2520with%2520a%2520global%2520optimizer%252C%2520this%2520approach%2520results%2520in%2520a%2520highly%250Aeffective%2520method%2520to%2520tackle%2520the%2520problem%2520of%2520SR.%2520We%2520theoretically%2520analyze%2520the%250Aexpressivity%2520of%2520ParFam%2520and%2520demonstrate%2520its%2520performance%2520with%2520extensive%2520numerical%250Aexperiments%2520based%2520on%2520the%2520common%2520SR%2520benchmark%2520suit%2520SRBench%252C%2520showing%2520that%2520we%250Aachieve%2520state-of-the-art%2520results.%2520Moreover%252C%2520we%2520present%2520an%2520extension%250Aincorporating%2520a%2520pre-trained%2520transformer%2520network%2520DL-ParFam%2520to%2520guide%2520ParFam%252C%250Aaccelerating%2520the%2520optimization%2520process%2520by%2520up%2520to%2520two%2520magnitudes.%2520Our%2520code%2520and%250Aresults%2520can%2520be%2520found%2520at%2520https%253A//github.com/Philipp238/parfam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05537v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ParFam%20--%20%28Neural%20Guided%29%20Symbolic%20Regression%20Based%20on%20Continuous%20Global%0A%20%20Optimization&entry.906535625=Philipp%20Scholl%20and%20Katharina%20Bieker%20and%20Hillary%20Hauger%20and%20Gitta%20Kutyniok&entry.1292438233=%20%20The%20problem%20of%20symbolic%20regression%20%28SR%29%20arises%20in%20many%20different%0Aapplications%2C%20such%20as%20identifying%20physical%20laws%20or%20deriving%20mathematical%0Aequations%20describing%20the%20behavior%20of%20financial%20markets%20from%20given%20data.%20Various%0Amethods%20exist%20to%20address%20the%20problem%20of%20SR%2C%20often%20based%20on%20genetic%20programming.%0AHowever%2C%20these%20methods%20are%20usually%20complicated%20and%20involve%20various%0Ahyperparameters.%20In%20this%20paper%2C%20we%20present%20our%20new%20approach%20ParFam%20that%0Autilizes%20parametric%20families%20of%20suitable%20symbolic%20functions%20to%20translate%20the%0Adiscrete%20symbolic%20regression%20problem%20into%20a%20continuous%20one%2C%20resulting%20in%20a%20more%0Astraightforward%20setup%20compared%20to%20current%20state-of-the-art%20methods.%20In%0Acombination%20with%20a%20global%20optimizer%2C%20this%20approach%20results%20in%20a%20highly%0Aeffective%20method%20to%20tackle%20the%20problem%20of%20SR.%20We%20theoretically%20analyze%20the%0Aexpressivity%20of%20ParFam%20and%20demonstrate%20its%20performance%20with%20extensive%20numerical%0Aexperiments%20based%20on%20the%20common%20SR%20benchmark%20suit%20SRBench%2C%20showing%20that%20we%0Aachieve%20state-of-the-art%20results.%20Moreover%2C%20we%20present%20an%20extension%0Aincorporating%20a%20pre-trained%20transformer%20network%20DL-ParFam%20to%20guide%20ParFam%2C%0Aaccelerating%20the%20optimization%20process%20by%20up%20to%20two%20magnitudes.%20Our%20code%20and%0Aresults%20can%20be%20found%20at%20https%3A//github.com/Philipp238/parfam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05537v4&entry.124074799=Read"},
{"title": "Detecting Quishing Attacks with Machine Learning Techniques Through QR\n  Code Analysis", "author": "Fouad Trad and Ali Chehab", "abstract": "  The rise of QR code based phishing (\"Quishing\") poses a growing cybersecurity\nthreat, as attackers increasingly exploit QR codes to bypass traditional\nphishing defenses. Existing detection methods predominantly focus on URL\nanalysis, which requires the extraction of the QR code payload, and may\ninadvertently expose users to malicious content. Moreover, QR codes can encode\nvarious types of data beyond URLs, such as Wi-Fi credentials and payment\ninformation, making URL-based detection insufficient for broader security\nconcerns. To address these gaps, we propose the first framework for quishing\ndetection that directly analyzes QR code structure and pixel patterns without\nextracting the embedded content. We generated a dataset of phishing and benign\nQR codes and we used it to train and evaluate multiple machine learning models,\nincluding Logistic Regression, Decision Trees, Random Forest, Naive Bayes,\nLightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of\n0.9106, demonstrating the feasibility of QR-centric detection. Through feature\nimportance analysis, we identify key visual indicators of malicious intent and\nrefine our feature set by removing non-informative pixels, improving\nperformance to an AUC of 0.9133 with a reduced feature space. Our findings\nreveal that the structural features of QR code correlate strongly with phishing\nrisk. This work establishes a foundation for quishing mitigation and highlights\nthe potential of direct QR analysis as a critical layer in modern phishing\ndefenses.\n", "link": "http://arxiv.org/abs/2505.03451v1", "date": "2025-05-06", "relevancy": 1.7637, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4427}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4413}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Quishing%20Attacks%20with%20Machine%20Learning%20Techniques%20Through%20QR%0A%20%20Code%20Analysis&body=Title%3A%20Detecting%20Quishing%20Attacks%20with%20Machine%20Learning%20Techniques%20Through%20QR%0A%20%20Code%20Analysis%0AAuthor%3A%20Fouad%20Trad%20and%20Ali%20Chehab%0AAbstract%3A%20%20%20The%20rise%20of%20QR%20code%20based%20phishing%20%28%22Quishing%22%29%20poses%20a%20growing%20cybersecurity%0Athreat%2C%20as%20attackers%20increasingly%20exploit%20QR%20codes%20to%20bypass%20traditional%0Aphishing%20defenses.%20Existing%20detection%20methods%20predominantly%20focus%20on%20URL%0Aanalysis%2C%20which%20requires%20the%20extraction%20of%20the%20QR%20code%20payload%2C%20and%20may%0Ainadvertently%20expose%20users%20to%20malicious%20content.%20Moreover%2C%20QR%20codes%20can%20encode%0Avarious%20types%20of%20data%20beyond%20URLs%2C%20such%20as%20Wi-Fi%20credentials%20and%20payment%0Ainformation%2C%20making%20URL-based%20detection%20insufficient%20for%20broader%20security%0Aconcerns.%20To%20address%20these%20gaps%2C%20we%20propose%20the%20first%20framework%20for%20quishing%0Adetection%20that%20directly%20analyzes%20QR%20code%20structure%20and%20pixel%20patterns%20without%0Aextracting%20the%20embedded%20content.%20We%20generated%20a%20dataset%20of%20phishing%20and%20benign%0AQR%20codes%20and%20we%20used%20it%20to%20train%20and%20evaluate%20multiple%20machine%20learning%20models%2C%0Aincluding%20Logistic%20Regression%2C%20Decision%20Trees%2C%20Random%20Forest%2C%20Naive%20Bayes%2C%0ALightGBM%2C%20and%20XGBoost.%20Our%20best-performing%20model%20%28XGBoost%29%20achieves%20an%20AUC%20of%0A0.9106%2C%20demonstrating%20the%20feasibility%20of%20QR-centric%20detection.%20Through%20feature%0Aimportance%20analysis%2C%20we%20identify%20key%20visual%20indicators%20of%20malicious%20intent%20and%0Arefine%20our%20feature%20set%20by%20removing%20non-informative%20pixels%2C%20improving%0Aperformance%20to%20an%20AUC%20of%200.9133%20with%20a%20reduced%20feature%20space.%20Our%20findings%0Areveal%20that%20the%20structural%20features%20of%20QR%20code%20correlate%20strongly%20with%20phishing%0Arisk.%20This%20work%20establishes%20a%20foundation%20for%20quishing%20mitigation%20and%20highlights%0Athe%20potential%20of%20direct%20QR%20analysis%20as%20a%20critical%20layer%20in%20modern%20phishing%0Adefenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Quishing%2520Attacks%2520with%2520Machine%2520Learning%2520Techniques%2520Through%2520QR%250A%2520%2520Code%2520Analysis%26entry.906535625%3DFouad%2520Trad%2520and%2520Ali%2520Chehab%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520QR%2520code%2520based%2520phishing%2520%2528%2522Quishing%2522%2529%2520poses%2520a%2520growing%2520cybersecurity%250Athreat%252C%2520as%2520attackers%2520increasingly%2520exploit%2520QR%2520codes%2520to%2520bypass%2520traditional%250Aphishing%2520defenses.%2520Existing%2520detection%2520methods%2520predominantly%2520focus%2520on%2520URL%250Aanalysis%252C%2520which%2520requires%2520the%2520extraction%2520of%2520the%2520QR%2520code%2520payload%252C%2520and%2520may%250Ainadvertently%2520expose%2520users%2520to%2520malicious%2520content.%2520Moreover%252C%2520QR%2520codes%2520can%2520encode%250Avarious%2520types%2520of%2520data%2520beyond%2520URLs%252C%2520such%2520as%2520Wi-Fi%2520credentials%2520and%2520payment%250Ainformation%252C%2520making%2520URL-based%2520detection%2520insufficient%2520for%2520broader%2520security%250Aconcerns.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520the%2520first%2520framework%2520for%2520quishing%250Adetection%2520that%2520directly%2520analyzes%2520QR%2520code%2520structure%2520and%2520pixel%2520patterns%2520without%250Aextracting%2520the%2520embedded%2520content.%2520We%2520generated%2520a%2520dataset%2520of%2520phishing%2520and%2520benign%250AQR%2520codes%2520and%2520we%2520used%2520it%2520to%2520train%2520and%2520evaluate%2520multiple%2520machine%2520learning%2520models%252C%250Aincluding%2520Logistic%2520Regression%252C%2520Decision%2520Trees%252C%2520Random%2520Forest%252C%2520Naive%2520Bayes%252C%250ALightGBM%252C%2520and%2520XGBoost.%2520Our%2520best-performing%2520model%2520%2528XGBoost%2529%2520achieves%2520an%2520AUC%2520of%250A0.9106%252C%2520demonstrating%2520the%2520feasibility%2520of%2520QR-centric%2520detection.%2520Through%2520feature%250Aimportance%2520analysis%252C%2520we%2520identify%2520key%2520visual%2520indicators%2520of%2520malicious%2520intent%2520and%250Arefine%2520our%2520feature%2520set%2520by%2520removing%2520non-informative%2520pixels%252C%2520improving%250Aperformance%2520to%2520an%2520AUC%2520of%25200.9133%2520with%2520a%2520reduced%2520feature%2520space.%2520Our%2520findings%250Areveal%2520that%2520the%2520structural%2520features%2520of%2520QR%2520code%2520correlate%2520strongly%2520with%2520phishing%250Arisk.%2520This%2520work%2520establishes%2520a%2520foundation%2520for%2520quishing%2520mitigation%2520and%2520highlights%250Athe%2520potential%2520of%2520direct%2520QR%2520analysis%2520as%2520a%2520critical%2520layer%2520in%2520modern%2520phishing%250Adefenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Quishing%20Attacks%20with%20Machine%20Learning%20Techniques%20Through%20QR%0A%20%20Code%20Analysis&entry.906535625=Fouad%20Trad%20and%20Ali%20Chehab&entry.1292438233=%20%20The%20rise%20of%20QR%20code%20based%20phishing%20%28%22Quishing%22%29%20poses%20a%20growing%20cybersecurity%0Athreat%2C%20as%20attackers%20increasingly%20exploit%20QR%20codes%20to%20bypass%20traditional%0Aphishing%20defenses.%20Existing%20detection%20methods%20predominantly%20focus%20on%20URL%0Aanalysis%2C%20which%20requires%20the%20extraction%20of%20the%20QR%20code%20payload%2C%20and%20may%0Ainadvertently%20expose%20users%20to%20malicious%20content.%20Moreover%2C%20QR%20codes%20can%20encode%0Avarious%20types%20of%20data%20beyond%20URLs%2C%20such%20as%20Wi-Fi%20credentials%20and%20payment%0Ainformation%2C%20making%20URL-based%20detection%20insufficient%20for%20broader%20security%0Aconcerns.%20To%20address%20these%20gaps%2C%20we%20propose%20the%20first%20framework%20for%20quishing%0Adetection%20that%20directly%20analyzes%20QR%20code%20structure%20and%20pixel%20patterns%20without%0Aextracting%20the%20embedded%20content.%20We%20generated%20a%20dataset%20of%20phishing%20and%20benign%0AQR%20codes%20and%20we%20used%20it%20to%20train%20and%20evaluate%20multiple%20machine%20learning%20models%2C%0Aincluding%20Logistic%20Regression%2C%20Decision%20Trees%2C%20Random%20Forest%2C%20Naive%20Bayes%2C%0ALightGBM%2C%20and%20XGBoost.%20Our%20best-performing%20model%20%28XGBoost%29%20achieves%20an%20AUC%20of%0A0.9106%2C%20demonstrating%20the%20feasibility%20of%20QR-centric%20detection.%20Through%20feature%0Aimportance%20analysis%2C%20we%20identify%20key%20visual%20indicators%20of%20malicious%20intent%20and%0Arefine%20our%20feature%20set%20by%20removing%20non-informative%20pixels%2C%20improving%0Aperformance%20to%20an%20AUC%20of%200.9133%20with%20a%20reduced%20feature%20space.%20Our%20findings%0Areveal%20that%20the%20structural%20features%20of%20QR%20code%20correlate%20strongly%20with%20phishing%0Arisk.%20This%20work%20establishes%20a%20foundation%20for%20quishing%20mitigation%20and%20highlights%0Athe%20potential%20of%20direct%20QR%20analysis%20as%20a%20critical%20layer%20in%20modern%20phishing%0Adefenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03451v1&entry.124074799=Read"},
{"title": "Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in\n  Magnetic Resonance Spectroscopy", "author": "Julian P. Merkofer and Dennis M. J. van de Sande and Alex A. Bhogal and Ruud J. G. van Sloun", "abstract": "  Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure\nthe metabolic composition of tissues, offering valuable insights into\nneurological disorders, tumor detection, and other metabolic dysfunctions.\nHowever, accurate metabolite quantification is hindered by challenges such as\nspectral overlap, low signal-to-noise ratio, and various artifacts. Traditional\nmethods like linear-combination modeling are susceptible to ambiguities and\ncommonly only provide a theoretical lower bound on estimation accuracy in the\nform of the Cram\\'er-Rao bound. This work introduces a Bayesian inference\nframework using Sylvester normalizing flows (SNFs) to approximate posterior\ndistributions over metabolite concentrations, enhancing quantification\nreliability. A physics-based decoder incorporates prior knowledge of MRS signal\nformation, ensuring realistic distribution representations. We validate the\nmethod on simulated 7T proton MRS data, demonstrating accurate metabolite\nquantification, well-calibrated uncertainties, and insights into parameter\ncorrelations and multi-modal distributions.\n", "link": "http://arxiv.org/abs/2505.03590v1", "date": "2025-05-06", "relevancy": 1.4677, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4876}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Sylvester%20Normalizing%20Flows%20for%20Bayesian%20Inference%20in%0A%20%20Magnetic%20Resonance%20Spectroscopy&body=Title%3A%20Physics-Informed%20Sylvester%20Normalizing%20Flows%20for%20Bayesian%20Inference%20in%0A%20%20Magnetic%20Resonance%20Spectroscopy%0AAuthor%3A%20Julian%20P.%20Merkofer%20and%20Dennis%20M.%20J.%20van%20de%20Sande%20and%20Alex%20A.%20Bhogal%20and%20Ruud%20J.%20G.%20van%20Sloun%0AAbstract%3A%20%20%20Magnetic%20resonance%20spectroscopy%20%28MRS%29%20is%20a%20non-invasive%20technique%20to%20measure%0Athe%20metabolic%20composition%20of%20tissues%2C%20offering%20valuable%20insights%20into%0Aneurological%20disorders%2C%20tumor%20detection%2C%20and%20other%20metabolic%20dysfunctions.%0AHowever%2C%20accurate%20metabolite%20quantification%20is%20hindered%20by%20challenges%20such%20as%0Aspectral%20overlap%2C%20low%20signal-to-noise%20ratio%2C%20and%20various%20artifacts.%20Traditional%0Amethods%20like%20linear-combination%20modeling%20are%20susceptible%20to%20ambiguities%20and%0Acommonly%20only%20provide%20a%20theoretical%20lower%20bound%20on%20estimation%20accuracy%20in%20the%0Aform%20of%20the%20Cram%5C%27er-Rao%20bound.%20This%20work%20introduces%20a%20Bayesian%20inference%0Aframework%20using%20Sylvester%20normalizing%20flows%20%28SNFs%29%20to%20approximate%20posterior%0Adistributions%20over%20metabolite%20concentrations%2C%20enhancing%20quantification%0Areliability.%20A%20physics-based%20decoder%20incorporates%20prior%20knowledge%20of%20MRS%20signal%0Aformation%2C%20ensuring%20realistic%20distribution%20representations.%20We%20validate%20the%0Amethod%20on%20simulated%207T%20proton%20MRS%20data%2C%20demonstrating%20accurate%20metabolite%0Aquantification%2C%20well-calibrated%20uncertainties%2C%20and%20insights%20into%20parameter%0Acorrelations%20and%20multi-modal%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Sylvester%2520Normalizing%2520Flows%2520for%2520Bayesian%2520Inference%2520in%250A%2520%2520Magnetic%2520Resonance%2520Spectroscopy%26entry.906535625%3DJulian%2520P.%2520Merkofer%2520and%2520Dennis%2520M.%2520J.%2520van%2520de%2520Sande%2520and%2520Alex%2520A.%2520Bhogal%2520and%2520Ruud%2520J.%2520G.%2520van%2520Sloun%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520spectroscopy%2520%2528MRS%2529%2520is%2520a%2520non-invasive%2520technique%2520to%2520measure%250Athe%2520metabolic%2520composition%2520of%2520tissues%252C%2520offering%2520valuable%2520insights%2520into%250Aneurological%2520disorders%252C%2520tumor%2520detection%252C%2520and%2520other%2520metabolic%2520dysfunctions.%250AHowever%252C%2520accurate%2520metabolite%2520quantification%2520is%2520hindered%2520by%2520challenges%2520such%2520as%250Aspectral%2520overlap%252C%2520low%2520signal-to-noise%2520ratio%252C%2520and%2520various%2520artifacts.%2520Traditional%250Amethods%2520like%2520linear-combination%2520modeling%2520are%2520susceptible%2520to%2520ambiguities%2520and%250Acommonly%2520only%2520provide%2520a%2520theoretical%2520lower%2520bound%2520on%2520estimation%2520accuracy%2520in%2520the%250Aform%2520of%2520the%2520Cram%255C%2527er-Rao%2520bound.%2520This%2520work%2520introduces%2520a%2520Bayesian%2520inference%250Aframework%2520using%2520Sylvester%2520normalizing%2520flows%2520%2528SNFs%2529%2520to%2520approximate%2520posterior%250Adistributions%2520over%2520metabolite%2520concentrations%252C%2520enhancing%2520quantification%250Areliability.%2520A%2520physics-based%2520decoder%2520incorporates%2520prior%2520knowledge%2520of%2520MRS%2520signal%250Aformation%252C%2520ensuring%2520realistic%2520distribution%2520representations.%2520We%2520validate%2520the%250Amethod%2520on%2520simulated%25207T%2520proton%2520MRS%2520data%252C%2520demonstrating%2520accurate%2520metabolite%250Aquantification%252C%2520well-calibrated%2520uncertainties%252C%2520and%2520insights%2520into%2520parameter%250Acorrelations%2520and%2520multi-modal%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Sylvester%20Normalizing%20Flows%20for%20Bayesian%20Inference%20in%0A%20%20Magnetic%20Resonance%20Spectroscopy&entry.906535625=Julian%20P.%20Merkofer%20and%20Dennis%20M.%20J.%20van%20de%20Sande%20and%20Alex%20A.%20Bhogal%20and%20Ruud%20J.%20G.%20van%20Sloun&entry.1292438233=%20%20Magnetic%20resonance%20spectroscopy%20%28MRS%29%20is%20a%20non-invasive%20technique%20to%20measure%0Athe%20metabolic%20composition%20of%20tissues%2C%20offering%20valuable%20insights%20into%0Aneurological%20disorders%2C%20tumor%20detection%2C%20and%20other%20metabolic%20dysfunctions.%0AHowever%2C%20accurate%20metabolite%20quantification%20is%20hindered%20by%20challenges%20such%20as%0Aspectral%20overlap%2C%20low%20signal-to-noise%20ratio%2C%20and%20various%20artifacts.%20Traditional%0Amethods%20like%20linear-combination%20modeling%20are%20susceptible%20to%20ambiguities%20and%0Acommonly%20only%20provide%20a%20theoretical%20lower%20bound%20on%20estimation%20accuracy%20in%20the%0Aform%20of%20the%20Cram%5C%27er-Rao%20bound.%20This%20work%20introduces%20a%20Bayesian%20inference%0Aframework%20using%20Sylvester%20normalizing%20flows%20%28SNFs%29%20to%20approximate%20posterior%0Adistributions%20over%20metabolite%20concentrations%2C%20enhancing%20quantification%0Areliability.%20A%20physics-based%20decoder%20incorporates%20prior%20knowledge%20of%20MRS%20signal%0Aformation%2C%20ensuring%20realistic%20distribution%20representations.%20We%20validate%20the%0Amethod%20on%20simulated%207T%20proton%20MRS%20data%2C%20demonstrating%20accurate%20metabolite%0Aquantification%2C%20well-calibrated%20uncertainties%2C%20and%20insights%20into%20parameter%0Acorrelations%20and%20multi-modal%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03590v1&entry.124074799=Read"},
{"title": "Modular Autonomous Virtualization System for Two-Dimensional\n  Semiconductor Quantum Dot Arrays", "author": "Anantha S. Rao and Donovan Buterakos and Barnaby van Straaten and Valentin John and C\u00e9cile X. Yu and Stefan D. Oosterhout and Lucas Stehouwer and Giordano Scappucci and Menno Veldhorst and Francesco Borsoi and Justyna P. Zwolak", "abstract": "  Arrays of gate-defined semiconductor quantum dots are among the leading\ncandidates for building scalable quantum processors. High-fidelity\ninitialization, control, and readout of spin qubit registers require exquisite\nand targeted control over key Hamiltonian parameters that define the\nelectrostatic environment. However, due to the tight gate pitch, capacitive\ncrosstalk between gates hinders independent tuning of chemical potentials and\ninterdot couplings. While virtual gates offer a practical solution, determining\nall the required cross-capacitance matrices accurately and efficiently in large\nquantum dot registers is an open challenge. Here, we establish a modular\nautomated virtualization system (MAViS) -- a general and modular framework for\nautonomously constructing a complete stack of multilayer virtual gates in real\ntime. Our method employs machine learning techniques to rapidly extract\nfeatures from two-dimensional charge stability diagrams. We then utilize\ncomputer vision and regression models to self-consistently determine all\nrelative capacitive couplings necessary for virtualizing plunger and barrier\ngates in both low- and high-tunnel-coupling regimes. Using MAViS, we\nsuccessfully demonstrate accurate virtualization of a dense two-dimensional\narray comprising ten quantum dots defined in a high-quality Ge/SiGe\nheterostructure. Our work offers an elegant and practical solution for the\nefficient control of large-scale semiconductor quantum dot systems.\n", "link": "http://arxiv.org/abs/2411.12516v2", "date": "2025-05-06", "relevancy": 1.4529, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4893}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4841}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Autonomous%20Virtualization%20System%20for%20Two-Dimensional%0A%20%20Semiconductor%20Quantum%20Dot%20Arrays&body=Title%3A%20Modular%20Autonomous%20Virtualization%20System%20for%20Two-Dimensional%0A%20%20Semiconductor%20Quantum%20Dot%20Arrays%0AAuthor%3A%20Anantha%20S.%20Rao%20and%20Donovan%20Buterakos%20and%20Barnaby%20van%20Straaten%20and%20Valentin%20John%20and%20C%C3%A9cile%20X.%20Yu%20and%20Stefan%20D.%20Oosterhout%20and%20Lucas%20Stehouwer%20and%20Giordano%20Scappucci%20and%20Menno%20Veldhorst%20and%20Francesco%20Borsoi%20and%20Justyna%20P.%20Zwolak%0AAbstract%3A%20%20%20Arrays%20of%20gate-defined%20semiconductor%20quantum%20dots%20are%20among%20the%20leading%0Acandidates%20for%20building%20scalable%20quantum%20processors.%20High-fidelity%0Ainitialization%2C%20control%2C%20and%20readout%20of%20spin%20qubit%20registers%20require%20exquisite%0Aand%20targeted%20control%20over%20key%20Hamiltonian%20parameters%20that%20define%20the%0Aelectrostatic%20environment.%20However%2C%20due%20to%20the%20tight%20gate%20pitch%2C%20capacitive%0Acrosstalk%20between%20gates%20hinders%20independent%20tuning%20of%20chemical%20potentials%20and%0Ainterdot%20couplings.%20While%20virtual%20gates%20offer%20a%20practical%20solution%2C%20determining%0Aall%20the%20required%20cross-capacitance%20matrices%20accurately%20and%20efficiently%20in%20large%0Aquantum%20dot%20registers%20is%20an%20open%20challenge.%20Here%2C%20we%20establish%20a%20modular%0Aautomated%20virtualization%20system%20%28MAViS%29%20--%20a%20general%20and%20modular%20framework%20for%0Aautonomously%20constructing%20a%20complete%20stack%20of%20multilayer%20virtual%20gates%20in%20real%0Atime.%20Our%20method%20employs%20machine%20learning%20techniques%20to%20rapidly%20extract%0Afeatures%20from%20two-dimensional%20charge%20stability%20diagrams.%20We%20then%20utilize%0Acomputer%20vision%20and%20regression%20models%20to%20self-consistently%20determine%20all%0Arelative%20capacitive%20couplings%20necessary%20for%20virtualizing%20plunger%20and%20barrier%0Agates%20in%20both%20low-%20and%20high-tunnel-coupling%20regimes.%20Using%20MAViS%2C%20we%0Asuccessfully%20demonstrate%20accurate%20virtualization%20of%20a%20dense%20two-dimensional%0Aarray%20comprising%20ten%20quantum%20dots%20defined%20in%20a%20high-quality%20Ge/SiGe%0Aheterostructure.%20Our%20work%20offers%20an%20elegant%20and%20practical%20solution%20for%20the%0Aefficient%20control%20of%20large-scale%20semiconductor%20quantum%20dot%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Autonomous%2520Virtualization%2520System%2520for%2520Two-Dimensional%250A%2520%2520Semiconductor%2520Quantum%2520Dot%2520Arrays%26entry.906535625%3DAnantha%2520S.%2520Rao%2520and%2520Donovan%2520Buterakos%2520and%2520Barnaby%2520van%2520Straaten%2520and%2520Valentin%2520John%2520and%2520C%25C3%25A9cile%2520X.%2520Yu%2520and%2520Stefan%2520D.%2520Oosterhout%2520and%2520Lucas%2520Stehouwer%2520and%2520Giordano%2520Scappucci%2520and%2520Menno%2520Veldhorst%2520and%2520Francesco%2520Borsoi%2520and%2520Justyna%2520P.%2520Zwolak%26entry.1292438233%3D%2520%2520Arrays%2520of%2520gate-defined%2520semiconductor%2520quantum%2520dots%2520are%2520among%2520the%2520leading%250Acandidates%2520for%2520building%2520scalable%2520quantum%2520processors.%2520High-fidelity%250Ainitialization%252C%2520control%252C%2520and%2520readout%2520of%2520spin%2520qubit%2520registers%2520require%2520exquisite%250Aand%2520targeted%2520control%2520over%2520key%2520Hamiltonian%2520parameters%2520that%2520define%2520the%250Aelectrostatic%2520environment.%2520However%252C%2520due%2520to%2520the%2520tight%2520gate%2520pitch%252C%2520capacitive%250Acrosstalk%2520between%2520gates%2520hinders%2520independent%2520tuning%2520of%2520chemical%2520potentials%2520and%250Ainterdot%2520couplings.%2520While%2520virtual%2520gates%2520offer%2520a%2520practical%2520solution%252C%2520determining%250Aall%2520the%2520required%2520cross-capacitance%2520matrices%2520accurately%2520and%2520efficiently%2520in%2520large%250Aquantum%2520dot%2520registers%2520is%2520an%2520open%2520challenge.%2520Here%252C%2520we%2520establish%2520a%2520modular%250Aautomated%2520virtualization%2520system%2520%2528MAViS%2529%2520--%2520a%2520general%2520and%2520modular%2520framework%2520for%250Aautonomously%2520constructing%2520a%2520complete%2520stack%2520of%2520multilayer%2520virtual%2520gates%2520in%2520real%250Atime.%2520Our%2520method%2520employs%2520machine%2520learning%2520techniques%2520to%2520rapidly%2520extract%250Afeatures%2520from%2520two-dimensional%2520charge%2520stability%2520diagrams.%2520We%2520then%2520utilize%250Acomputer%2520vision%2520and%2520regression%2520models%2520to%2520self-consistently%2520determine%2520all%250Arelative%2520capacitive%2520couplings%2520necessary%2520for%2520virtualizing%2520plunger%2520and%2520barrier%250Agates%2520in%2520both%2520low-%2520and%2520high-tunnel-coupling%2520regimes.%2520Using%2520MAViS%252C%2520we%250Asuccessfully%2520demonstrate%2520accurate%2520virtualization%2520of%2520a%2520dense%2520two-dimensional%250Aarray%2520comprising%2520ten%2520quantum%2520dots%2520defined%2520in%2520a%2520high-quality%2520Ge/SiGe%250Aheterostructure.%2520Our%2520work%2520offers%2520an%2520elegant%2520and%2520practical%2520solution%2520for%2520the%250Aefficient%2520control%2520of%2520large-scale%2520semiconductor%2520quantum%2520dot%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Autonomous%20Virtualization%20System%20for%20Two-Dimensional%0A%20%20Semiconductor%20Quantum%20Dot%20Arrays&entry.906535625=Anantha%20S.%20Rao%20and%20Donovan%20Buterakos%20and%20Barnaby%20van%20Straaten%20and%20Valentin%20John%20and%20C%C3%A9cile%20X.%20Yu%20and%20Stefan%20D.%20Oosterhout%20and%20Lucas%20Stehouwer%20and%20Giordano%20Scappucci%20and%20Menno%20Veldhorst%20and%20Francesco%20Borsoi%20and%20Justyna%20P.%20Zwolak&entry.1292438233=%20%20Arrays%20of%20gate-defined%20semiconductor%20quantum%20dots%20are%20among%20the%20leading%0Acandidates%20for%20building%20scalable%20quantum%20processors.%20High-fidelity%0Ainitialization%2C%20control%2C%20and%20readout%20of%20spin%20qubit%20registers%20require%20exquisite%0Aand%20targeted%20control%20over%20key%20Hamiltonian%20parameters%20that%20define%20the%0Aelectrostatic%20environment.%20However%2C%20due%20to%20the%20tight%20gate%20pitch%2C%20capacitive%0Acrosstalk%20between%20gates%20hinders%20independent%20tuning%20of%20chemical%20potentials%20and%0Ainterdot%20couplings.%20While%20virtual%20gates%20offer%20a%20practical%20solution%2C%20determining%0Aall%20the%20required%20cross-capacitance%20matrices%20accurately%20and%20efficiently%20in%20large%0Aquantum%20dot%20registers%20is%20an%20open%20challenge.%20Here%2C%20we%20establish%20a%20modular%0Aautomated%20virtualization%20system%20%28MAViS%29%20--%20a%20general%20and%20modular%20framework%20for%0Aautonomously%20constructing%20a%20complete%20stack%20of%20multilayer%20virtual%20gates%20in%20real%0Atime.%20Our%20method%20employs%20machine%20learning%20techniques%20to%20rapidly%20extract%0Afeatures%20from%20two-dimensional%20charge%20stability%20diagrams.%20We%20then%20utilize%0Acomputer%20vision%20and%20regression%20models%20to%20self-consistently%20determine%20all%0Arelative%20capacitive%20couplings%20necessary%20for%20virtualizing%20plunger%20and%20barrier%0Agates%20in%20both%20low-%20and%20high-tunnel-coupling%20regimes.%20Using%20MAViS%2C%20we%0Asuccessfully%20demonstrate%20accurate%20virtualization%20of%20a%20dense%20two-dimensional%0Aarray%20comprising%20ten%20quantum%20dots%20defined%20in%20a%20high-quality%20Ge/SiGe%0Aheterostructure.%20Our%20work%20offers%20an%20elegant%20and%20practical%20solution%20for%20the%0Aefficient%20control%20of%20large-scale%20semiconductor%20quantum%20dot%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12516v2&entry.124074799=Read"},
{"title": "Bellman Unbiasedness: Tractable and Provably Efficient Distributional\n  Reinforcement Learning with General Value Function Approximation", "author": "Taehyun Cho and Seungyub Han and Kyungjae Lee and Seokhun Ju and Dohyeong Kim and Jungwoo Lee", "abstract": "  Distributional reinforcement learning improves performance by capturing\nenvironmental stochasticity, but a comprehensive theoretical understanding of\nits effectiveness remains elusive. In addition, the intractable element of the\ninfinite dimensionality of distributions has been overlooked. In this paper, we\npresent a regret analysis of distributional reinforcement learning with general\nvalue function approximation in a finite episodic Markov decision process\nsetting. We first introduce a key notion of $\\textit{Bellman unbiasedness}$\nwhich is essential for exactly learnable and provably efficient distributional\nupdates in an online manner. Among all types of statistical functionals for\nrepresenting infinite-dimensional return distributions, our theoretical results\ndemonstrate that only moment functionals can exactly capture the statistical\ninformation. Secondly, we propose a provably efficient algorithm,\n$\\texttt{SF-LSVI}$, that achieves a tight regret bound of $\\tilde{O}(d_E\nH^{\\frac{3}{2}}\\sqrt{K})$ where $H$ is the horizon, $K$ is the number of\nepisodes, and $d_E$ is the eluder dimension of a function class.\n", "link": "http://arxiv.org/abs/2407.21260v2", "date": "2025-05-06", "relevancy": 1.3745, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4911}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4525}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bellman%20Unbiasedness%3A%20Tractable%20and%20Provably%20Efficient%20Distributional%0A%20%20Reinforcement%20Learning%20with%20General%20Value%20Function%20Approximation&body=Title%3A%20Bellman%20Unbiasedness%3A%20Tractable%20and%20Provably%20Efficient%20Distributional%0A%20%20Reinforcement%20Learning%20with%20General%20Value%20Function%20Approximation%0AAuthor%3A%20Taehyun%20Cho%20and%20Seungyub%20Han%20and%20Kyungjae%20Lee%20and%20Seokhun%20Ju%20and%20Dohyeong%20Kim%20and%20Jungwoo%20Lee%0AAbstract%3A%20%20%20Distributional%20reinforcement%20learning%20improves%20performance%20by%20capturing%0Aenvironmental%20stochasticity%2C%20but%20a%20comprehensive%20theoretical%20understanding%20of%0Aits%20effectiveness%20remains%20elusive.%20In%20addition%2C%20the%20intractable%20element%20of%20the%0Ainfinite%20dimensionality%20of%20distributions%20has%20been%20overlooked.%20In%20this%20paper%2C%20we%0Apresent%20a%20regret%20analysis%20of%20distributional%20reinforcement%20learning%20with%20general%0Avalue%20function%20approximation%20in%20a%20finite%20episodic%20Markov%20decision%20process%0Asetting.%20We%20first%20introduce%20a%20key%20notion%20of%20%24%5Ctextit%7BBellman%20unbiasedness%7D%24%0Awhich%20is%20essential%20for%20exactly%20learnable%20and%20provably%20efficient%20distributional%0Aupdates%20in%20an%20online%20manner.%20Among%20all%20types%20of%20statistical%20functionals%20for%0Arepresenting%20infinite-dimensional%20return%20distributions%2C%20our%20theoretical%20results%0Ademonstrate%20that%20only%20moment%20functionals%20can%20exactly%20capture%20the%20statistical%0Ainformation.%20Secondly%2C%20we%20propose%20a%20provably%20efficient%20algorithm%2C%0A%24%5Ctexttt%7BSF-LSVI%7D%24%2C%20that%20achieves%20a%20tight%20regret%20bound%20of%20%24%5Ctilde%7BO%7D%28d_E%0AH%5E%7B%5Cfrac%7B3%7D%7B2%7D%7D%5Csqrt%7BK%7D%29%24%20where%20%24H%24%20is%20the%20horizon%2C%20%24K%24%20is%20the%20number%20of%0Aepisodes%2C%20and%20%24d_E%24%20is%20the%20eluder%20dimension%20of%20a%20function%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21260v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBellman%2520Unbiasedness%253A%2520Tractable%2520and%2520Provably%2520Efficient%2520Distributional%250A%2520%2520Reinforcement%2520Learning%2520with%2520General%2520Value%2520Function%2520Approximation%26entry.906535625%3DTaehyun%2520Cho%2520and%2520Seungyub%2520Han%2520and%2520Kyungjae%2520Lee%2520and%2520Seokhun%2520Ju%2520and%2520Dohyeong%2520Kim%2520and%2520Jungwoo%2520Lee%26entry.1292438233%3D%2520%2520Distributional%2520reinforcement%2520learning%2520improves%2520performance%2520by%2520capturing%250Aenvironmental%2520stochasticity%252C%2520but%2520a%2520comprehensive%2520theoretical%2520understanding%2520of%250Aits%2520effectiveness%2520remains%2520elusive.%2520In%2520addition%252C%2520the%2520intractable%2520element%2520of%2520the%250Ainfinite%2520dimensionality%2520of%2520distributions%2520has%2520been%2520overlooked.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520regret%2520analysis%2520of%2520distributional%2520reinforcement%2520learning%2520with%2520general%250Avalue%2520function%2520approximation%2520in%2520a%2520finite%2520episodic%2520Markov%2520decision%2520process%250Asetting.%2520We%2520first%2520introduce%2520a%2520key%2520notion%2520of%2520%2524%255Ctextit%257BBellman%2520unbiasedness%257D%2524%250Awhich%2520is%2520essential%2520for%2520exactly%2520learnable%2520and%2520provably%2520efficient%2520distributional%250Aupdates%2520in%2520an%2520online%2520manner.%2520Among%2520all%2520types%2520of%2520statistical%2520functionals%2520for%250Arepresenting%2520infinite-dimensional%2520return%2520distributions%252C%2520our%2520theoretical%2520results%250Ademonstrate%2520that%2520only%2520moment%2520functionals%2520can%2520exactly%2520capture%2520the%2520statistical%250Ainformation.%2520Secondly%252C%2520we%2520propose%2520a%2520provably%2520efficient%2520algorithm%252C%250A%2524%255Ctexttt%257BSF-LSVI%257D%2524%252C%2520that%2520achieves%2520a%2520tight%2520regret%2520bound%2520of%2520%2524%255Ctilde%257BO%257D%2528d_E%250AH%255E%257B%255Cfrac%257B3%257D%257B2%257D%257D%255Csqrt%257BK%257D%2529%2524%2520where%2520%2524H%2524%2520is%2520the%2520horizon%252C%2520%2524K%2524%2520is%2520the%2520number%2520of%250Aepisodes%252C%2520and%2520%2524d_E%2524%2520is%2520the%2520eluder%2520dimension%2520of%2520a%2520function%2520class.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21260v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bellman%20Unbiasedness%3A%20Tractable%20and%20Provably%20Efficient%20Distributional%0A%20%20Reinforcement%20Learning%20with%20General%20Value%20Function%20Approximation&entry.906535625=Taehyun%20Cho%20and%20Seungyub%20Han%20and%20Kyungjae%20Lee%20and%20Seokhun%20Ju%20and%20Dohyeong%20Kim%20and%20Jungwoo%20Lee&entry.1292438233=%20%20Distributional%20reinforcement%20learning%20improves%20performance%20by%20capturing%0Aenvironmental%20stochasticity%2C%20but%20a%20comprehensive%20theoretical%20understanding%20of%0Aits%20effectiveness%20remains%20elusive.%20In%20addition%2C%20the%20intractable%20element%20of%20the%0Ainfinite%20dimensionality%20of%20distributions%20has%20been%20overlooked.%20In%20this%20paper%2C%20we%0Apresent%20a%20regret%20analysis%20of%20distributional%20reinforcement%20learning%20with%20general%0Avalue%20function%20approximation%20in%20a%20finite%20episodic%20Markov%20decision%20process%0Asetting.%20We%20first%20introduce%20a%20key%20notion%20of%20%24%5Ctextit%7BBellman%20unbiasedness%7D%24%0Awhich%20is%20essential%20for%20exactly%20learnable%20and%20provably%20efficient%20distributional%0Aupdates%20in%20an%20online%20manner.%20Among%20all%20types%20of%20statistical%20functionals%20for%0Arepresenting%20infinite-dimensional%20return%20distributions%2C%20our%20theoretical%20results%0Ademonstrate%20that%20only%20moment%20functionals%20can%20exactly%20capture%20the%20statistical%0Ainformation.%20Secondly%2C%20we%20propose%20a%20provably%20efficient%20algorithm%2C%0A%24%5Ctexttt%7BSF-LSVI%7D%24%2C%20that%20achieves%20a%20tight%20regret%20bound%20of%20%24%5Ctilde%7BO%7D%28d_E%0AH%5E%7B%5Cfrac%7B3%7D%7B2%7D%7D%5Csqrt%7BK%7D%29%24%20where%20%24H%24%20is%20the%20horizon%2C%20%24K%24%20is%20the%20number%20of%0Aepisodes%2C%20and%20%24d_E%24%20is%20the%20eluder%20dimension%20of%20a%20function%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21260v2&entry.124074799=Read"},
{"title": "Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time", "author": "Celeste Veronese and Daniele Meli and Alessandro Farinelli", "abstract": "  This paper proposes an integration of temporal logical reasoning and\nPartially Observable Markov Decision Processes (POMDPs) to achieve\ninterpretable decision-making under uncertainty with macro-actions. Our method\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\n(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guide\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\nsignificantly reducing inference time while ensuring robust performance. Such\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\ntraces of execution (belief-action pairs), thus eliminating the need for\nmanually designed heuristics and requiring only the specification of the POMDP\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\nmacro-actions demonstrate increased expressiveness and generality when compared\nto time-independent heuristics, indeed offering substantial computational\nefficiency improvements.\n", "link": "http://arxiv.org/abs/2505.03668v1", "date": "2025-05-06", "relevancy": 1.3956, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5129}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4851}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Symbolic%20Persistent%20Macro-Actions%20for%20POMDP%20Solving%20Over%20Time&body=Title%3A%20Learning%20Symbolic%20Persistent%20Macro-Actions%20for%20POMDP%20Solving%20Over%20Time%0AAuthor%3A%20Celeste%20Veronese%20and%20Daniele%20Meli%20and%20Alessandro%20Farinelli%0AAbstract%3A%20%20%20This%20paper%20proposes%20an%20integration%20of%20temporal%20logical%20reasoning%20and%0APartially%20Observable%20Markov%20Decision%20Processes%20%28POMDPs%29%20to%20achieve%0Ainterpretable%20decision-making%20under%20uncertainty%20with%20macro-actions.%20Our%20method%0Aleverages%20a%20fragment%20of%20Linear%20Temporal%20Logic%20%28LTL%29%20based%20on%20Event%20Calculus%0A%28EC%29%20to%20generate%20%5Cemph%7Bpersistent%7D%20%28i.e.%2C%20constant%29%20macro-actions%2C%20which%20guide%0AMonte%20Carlo%20Tree%20Search%20%28MCTS%29-based%20POMDP%20solvers%20over%20a%20time%20horizon%2C%0Asignificantly%20reducing%20inference%20time%20while%20ensuring%20robust%20performance.%20Such%0Amacro-actions%20are%20learnt%20via%20Inductive%20Logic%20Programming%20%28ILP%29%20from%20a%20few%0Atraces%20of%20execution%20%28belief-action%20pairs%29%2C%20thus%20eliminating%20the%20need%20for%0Amanually%20designed%20heuristics%20and%20requiring%20only%20the%20specification%20of%20the%20POMDP%0Atransition%20model.%20In%20the%20Pocman%20and%20Rocksample%20benchmark%20scenarios%2C%20our%20learned%0Amacro-actions%20demonstrate%20increased%20expressiveness%20and%20generality%20when%20compared%0Ato%20time-independent%20heuristics%2C%20indeed%20offering%20substantial%20computational%0Aefficiency%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Symbolic%2520Persistent%2520Macro-Actions%2520for%2520POMDP%2520Solving%2520Over%2520Time%26entry.906535625%3DCeleste%2520Veronese%2520and%2520Daniele%2520Meli%2520and%2520Alessandro%2520Farinelli%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520an%2520integration%2520of%2520temporal%2520logical%2520reasoning%2520and%250APartially%2520Observable%2520Markov%2520Decision%2520Processes%2520%2528POMDPs%2529%2520to%2520achieve%250Ainterpretable%2520decision-making%2520under%2520uncertainty%2520with%2520macro-actions.%2520Our%2520method%250Aleverages%2520a%2520fragment%2520of%2520Linear%2520Temporal%2520Logic%2520%2528LTL%2529%2520based%2520on%2520Event%2520Calculus%250A%2528EC%2529%2520to%2520generate%2520%255Cemph%257Bpersistent%257D%2520%2528i.e.%252C%2520constant%2529%2520macro-actions%252C%2520which%2520guide%250AMonte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529-based%2520POMDP%2520solvers%2520over%2520a%2520time%2520horizon%252C%250Asignificantly%2520reducing%2520inference%2520time%2520while%2520ensuring%2520robust%2520performance.%2520Such%250Amacro-actions%2520are%2520learnt%2520via%2520Inductive%2520Logic%2520Programming%2520%2528ILP%2529%2520from%2520a%2520few%250Atraces%2520of%2520execution%2520%2528belief-action%2520pairs%2529%252C%2520thus%2520eliminating%2520the%2520need%2520for%250Amanually%2520designed%2520heuristics%2520and%2520requiring%2520only%2520the%2520specification%2520of%2520the%2520POMDP%250Atransition%2520model.%2520In%2520the%2520Pocman%2520and%2520Rocksample%2520benchmark%2520scenarios%252C%2520our%2520learned%250Amacro-actions%2520demonstrate%2520increased%2520expressiveness%2520and%2520generality%2520when%2520compared%250Ato%2520time-independent%2520heuristics%252C%2520indeed%2520offering%2520substantial%2520computational%250Aefficiency%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Symbolic%20Persistent%20Macro-Actions%20for%20POMDP%20Solving%20Over%20Time&entry.906535625=Celeste%20Veronese%20and%20Daniele%20Meli%20and%20Alessandro%20Farinelli&entry.1292438233=%20%20This%20paper%20proposes%20an%20integration%20of%20temporal%20logical%20reasoning%20and%0APartially%20Observable%20Markov%20Decision%20Processes%20%28POMDPs%29%20to%20achieve%0Ainterpretable%20decision-making%20under%20uncertainty%20with%20macro-actions.%20Our%20method%0Aleverages%20a%20fragment%20of%20Linear%20Temporal%20Logic%20%28LTL%29%20based%20on%20Event%20Calculus%0A%28EC%29%20to%20generate%20%5Cemph%7Bpersistent%7D%20%28i.e.%2C%20constant%29%20macro-actions%2C%20which%20guide%0AMonte%20Carlo%20Tree%20Search%20%28MCTS%29-based%20POMDP%20solvers%20over%20a%20time%20horizon%2C%0Asignificantly%20reducing%20inference%20time%20while%20ensuring%20robust%20performance.%20Such%0Amacro-actions%20are%20learnt%20via%20Inductive%20Logic%20Programming%20%28ILP%29%20from%20a%20few%0Atraces%20of%20execution%20%28belief-action%20pairs%29%2C%20thus%20eliminating%20the%20need%20for%0Amanually%20designed%20heuristics%20and%20requiring%20only%20the%20specification%20of%20the%20POMDP%0Atransition%20model.%20In%20the%20Pocman%20and%20Rocksample%20benchmark%20scenarios%2C%20our%20learned%0Amacro-actions%20demonstrate%20increased%20expressiveness%20and%20generality%20when%20compared%0Ato%20time-independent%20heuristics%2C%20indeed%20offering%20substantial%20computational%0Aefficiency%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03668v1&entry.124074799=Read"},
{"title": "BCause: Human-AI collaboration to improve hybrid mapping and ideation in\n  argumentation-grounded deliberation", "author": "Lucas Anastasiou and Anna De Liddo", "abstract": "  Public deliberation, as in open discussion of issues of public concern, often\nsuffers from scattered and shallow discourse, poor sensemaking, and a\ndisconnect from actionable policy outcomes. This paper introduces BCause, a\ndiscussion system leveraging generative AI and human-machine collaboration to\ntransform unstructured dialogue around public issues (such as urban living,\npolicy changes, and current socio-economic transformations) into structured,\nactionable democratic processes. We present three innovations: (i) importing\nand transforming unstructured transcripts into argumentative discussions, (ii)\ngeo-deliberated problem-sensing via a Telegram bot for local issue reporting,\nand (iii) smart reporting with customizable widgets (e.g., summaries, topic\nmodelling, policy recommendations, clustered arguments). The system's human-AI\npartnership preserves critical human participation to ensure ethical oversight,\ncontextual relevance, and creative synthesis.\n", "link": "http://arxiv.org/abs/2505.03584v1", "date": "2025-05-06", "relevancy": 1.3704, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4796}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BCause%3A%20Human-AI%20collaboration%20to%20improve%20hybrid%20mapping%20and%20ideation%20in%0A%20%20argumentation-grounded%20deliberation&body=Title%3A%20BCause%3A%20Human-AI%20collaboration%20to%20improve%20hybrid%20mapping%20and%20ideation%20in%0A%20%20argumentation-grounded%20deliberation%0AAuthor%3A%20Lucas%20Anastasiou%20and%20Anna%20De%20Liddo%0AAbstract%3A%20%20%20Public%20deliberation%2C%20as%20in%20open%20discussion%20of%20issues%20of%20public%20concern%2C%20often%0Asuffers%20from%20scattered%20and%20shallow%20discourse%2C%20poor%20sensemaking%2C%20and%20a%0Adisconnect%20from%20actionable%20policy%20outcomes.%20This%20paper%20introduces%20BCause%2C%20a%0Adiscussion%20system%20leveraging%20generative%20AI%20and%20human-machine%20collaboration%20to%0Atransform%20unstructured%20dialogue%20around%20public%20issues%20%28such%20as%20urban%20living%2C%0Apolicy%20changes%2C%20and%20current%20socio-economic%20transformations%29%20into%20structured%2C%0Aactionable%20democratic%20processes.%20We%20present%20three%20innovations%3A%20%28i%29%20importing%0Aand%20transforming%20unstructured%20transcripts%20into%20argumentative%20discussions%2C%20%28ii%29%0Ageo-deliberated%20problem-sensing%20via%20a%20Telegram%20bot%20for%20local%20issue%20reporting%2C%0Aand%20%28iii%29%20smart%20reporting%20with%20customizable%20widgets%20%28e.g.%2C%20summaries%2C%20topic%0Amodelling%2C%20policy%20recommendations%2C%20clustered%20arguments%29.%20The%20system%27s%20human-AI%0Apartnership%20preserves%20critical%20human%20participation%20to%20ensure%20ethical%20oversight%2C%0Acontextual%20relevance%2C%20and%20creative%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBCause%253A%2520Human-AI%2520collaboration%2520to%2520improve%2520hybrid%2520mapping%2520and%2520ideation%2520in%250A%2520%2520argumentation-grounded%2520deliberation%26entry.906535625%3DLucas%2520Anastasiou%2520and%2520Anna%2520De%2520Liddo%26entry.1292438233%3D%2520%2520Public%2520deliberation%252C%2520as%2520in%2520open%2520discussion%2520of%2520issues%2520of%2520public%2520concern%252C%2520often%250Asuffers%2520from%2520scattered%2520and%2520shallow%2520discourse%252C%2520poor%2520sensemaking%252C%2520and%2520a%250Adisconnect%2520from%2520actionable%2520policy%2520outcomes.%2520This%2520paper%2520introduces%2520BCause%252C%2520a%250Adiscussion%2520system%2520leveraging%2520generative%2520AI%2520and%2520human-machine%2520collaboration%2520to%250Atransform%2520unstructured%2520dialogue%2520around%2520public%2520issues%2520%2528such%2520as%2520urban%2520living%252C%250Apolicy%2520changes%252C%2520and%2520current%2520socio-economic%2520transformations%2529%2520into%2520structured%252C%250Aactionable%2520democratic%2520processes.%2520We%2520present%2520three%2520innovations%253A%2520%2528i%2529%2520importing%250Aand%2520transforming%2520unstructured%2520transcripts%2520into%2520argumentative%2520discussions%252C%2520%2528ii%2529%250Ageo-deliberated%2520problem-sensing%2520via%2520a%2520Telegram%2520bot%2520for%2520local%2520issue%2520reporting%252C%250Aand%2520%2528iii%2529%2520smart%2520reporting%2520with%2520customizable%2520widgets%2520%2528e.g.%252C%2520summaries%252C%2520topic%250Amodelling%252C%2520policy%2520recommendations%252C%2520clustered%2520arguments%2529.%2520The%2520system%2527s%2520human-AI%250Apartnership%2520preserves%2520critical%2520human%2520participation%2520to%2520ensure%2520ethical%2520oversight%252C%250Acontextual%2520relevance%252C%2520and%2520creative%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BCause%3A%20Human-AI%20collaboration%20to%20improve%20hybrid%20mapping%20and%20ideation%20in%0A%20%20argumentation-grounded%20deliberation&entry.906535625=Lucas%20Anastasiou%20and%20Anna%20De%20Liddo&entry.1292438233=%20%20Public%20deliberation%2C%20as%20in%20open%20discussion%20of%20issues%20of%20public%20concern%2C%20often%0Asuffers%20from%20scattered%20and%20shallow%20discourse%2C%20poor%20sensemaking%2C%20and%20a%0Adisconnect%20from%20actionable%20policy%20outcomes.%20This%20paper%20introduces%20BCause%2C%20a%0Adiscussion%20system%20leveraging%20generative%20AI%20and%20human-machine%20collaboration%20to%0Atransform%20unstructured%20dialogue%20around%20public%20issues%20%28such%20as%20urban%20living%2C%0Apolicy%20changes%2C%20and%20current%20socio-economic%20transformations%29%20into%20structured%2C%0Aactionable%20democratic%20processes.%20We%20present%20three%20innovations%3A%20%28i%29%20importing%0Aand%20transforming%20unstructured%20transcripts%20into%20argumentative%20discussions%2C%20%28ii%29%0Ageo-deliberated%20problem-sensing%20via%20a%20Telegram%20bot%20for%20local%20issue%20reporting%2C%0Aand%20%28iii%29%20smart%20reporting%20with%20customizable%20widgets%20%28e.g.%2C%20summaries%2C%20topic%0Amodelling%2C%20policy%20recommendations%2C%20clustered%20arguments%29.%20The%20system%27s%20human-AI%0Apartnership%20preserves%20critical%20human%20participation%20to%20ensure%20ethical%20oversight%2C%0Acontextual%20relevance%2C%20and%20creative%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03584v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


