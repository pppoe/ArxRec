<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240915.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "NPGA: Neural Parametric Gaussian Avatars", "author": "Simon Giebenhain and Tobias Kirschstein and Martin R\u00fcnz and Lourdes Agapito and Matthias Nie\u00dfner", "abstract": "  The creation of high-fidelity, digital versions of human heads is an\nimportant stepping stone in the process of further integrating virtual\ncomponents into our everyday lives. Constructing such avatars is a challenging\nresearch problem, due to a high demand for photo-realism and real-time\nrendering performance. In this work, we propose Neural Parametric Gaussian\nAvatars (NPGA), a data-driven approach to create high-fidelity, controllable\navatars from multi-view video recordings. We build our method around 3D\nGaussian splatting for its highly efficient rendering and to inherit the\ntopological flexibility of point clouds. In contrast to previous work, we\ncondition our avatars' dynamics on the rich expression space of neural\nparametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we\ndistill the backward deformation field of our underlying NPHM into forward\ndeformations which are compatible with rasterization-based rendering. All\nremaining fine-scale, expression-dependent details are learned from the\nmulti-view videos. For increased representational capacity of our avatars, we\npropose per-Gaussian latent features that condition each primitives dynamic\nbehavior. To regularize this increased dynamic expressivity, we propose\nLaplacian terms on the latent features and predicted dynamics. We evaluate our\nmethod on the public NeRSemble dataset, demonstrating that NPGA significantly\noutperforms the previous state-of-the-art avatars on the self-reenactment task\nby 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from\nreal-world monocular videos.\n", "link": "http://arxiv.org/abs/2405.19331v2", "date": "2024-09-13", "relevancy": 3.6537, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7685}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7685}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NPGA%3A%20Neural%20Parametric%20Gaussian%20Avatars&body=Title%3A%20NPGA%3A%20Neural%20Parametric%20Gaussian%20Avatars%0AAuthor%3A%20Simon%20Giebenhain%20and%20Tobias%20Kirschstein%20and%20Martin%20R%C3%BCnz%20and%20Lourdes%20Agapito%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20The%20creation%20of%20high-fidelity%2C%20digital%20versions%20of%20human%20heads%20is%20an%0Aimportant%20stepping%20stone%20in%20the%20process%20of%20further%20integrating%20virtual%0Acomponents%20into%20our%20everyday%20lives.%20Constructing%20such%20avatars%20is%20a%20challenging%0Aresearch%20problem%2C%20due%20to%20a%20high%20demand%20for%20photo-realism%20and%20real-time%0Arendering%20performance.%20In%20this%20work%2C%20we%20propose%20Neural%20Parametric%20Gaussian%0AAvatars%20%28NPGA%29%2C%20a%20data-driven%20approach%20to%20create%20high-fidelity%2C%20controllable%0Aavatars%20from%20multi-view%20video%20recordings.%20We%20build%20our%20method%20around%203D%0AGaussian%20splatting%20for%20its%20highly%20efficient%20rendering%20and%20to%20inherit%20the%0Atopological%20flexibility%20of%20point%20clouds.%20In%20contrast%20to%20previous%20work%2C%20we%0Acondition%20our%20avatars%27%20dynamics%20on%20the%20rich%20expression%20space%20of%20neural%0Aparametric%20head%20models%20%28NPHM%29%2C%20instead%20of%20mesh-based%203DMMs.%20To%20this%20end%2C%20we%0Adistill%20the%20backward%20deformation%20field%20of%20our%20underlying%20NPHM%20into%20forward%0Adeformations%20which%20are%20compatible%20with%20rasterization-based%20rendering.%20All%0Aremaining%20fine-scale%2C%20expression-dependent%20details%20are%20learned%20from%20the%0Amulti-view%20videos.%20For%20increased%20representational%20capacity%20of%20our%20avatars%2C%20we%0Apropose%20per-Gaussian%20latent%20features%20that%20condition%20each%20primitives%20dynamic%0Abehavior.%20To%20regularize%20this%20increased%20dynamic%20expressivity%2C%20we%20propose%0ALaplacian%20terms%20on%20the%20latent%20features%20and%20predicted%20dynamics.%20We%20evaluate%20our%0Amethod%20on%20the%20public%20NeRSemble%20dataset%2C%20demonstrating%20that%20NPGA%20significantly%0Aoutperforms%20the%20previous%20state-of-the-art%20avatars%20on%20the%20self-reenactment%20task%0Aby%202.6%20PSNR.%20Furthermore%2C%20we%20demonstrate%20accurate%20animation%20capabilities%20from%0Areal-world%20monocular%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19331v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNPGA%253A%2520Neural%2520Parametric%2520Gaussian%2520Avatars%26entry.906535625%3DSimon%2520Giebenhain%2520and%2520Tobias%2520Kirschstein%2520and%2520Martin%2520R%25C3%25BCnz%2520and%2520Lourdes%2520Agapito%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520The%2520creation%2520of%2520high-fidelity%252C%2520digital%2520versions%2520of%2520human%2520heads%2520is%2520an%250Aimportant%2520stepping%2520stone%2520in%2520the%2520process%2520of%2520further%2520integrating%2520virtual%250Acomponents%2520into%2520our%2520everyday%2520lives.%2520Constructing%2520such%2520avatars%2520is%2520a%2520challenging%250Aresearch%2520problem%252C%2520due%2520to%2520a%2520high%2520demand%2520for%2520photo-realism%2520and%2520real-time%250Arendering%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520Neural%2520Parametric%2520Gaussian%250AAvatars%2520%2528NPGA%2529%252C%2520a%2520data-driven%2520approach%2520to%2520create%2520high-fidelity%252C%2520controllable%250Aavatars%2520from%2520multi-view%2520video%2520recordings.%2520We%2520build%2520our%2520method%2520around%25203D%250AGaussian%2520splatting%2520for%2520its%2520highly%2520efficient%2520rendering%2520and%2520to%2520inherit%2520the%250Atopological%2520flexibility%2520of%2520point%2520clouds.%2520In%2520contrast%2520to%2520previous%2520work%252C%2520we%250Acondition%2520our%2520avatars%2527%2520dynamics%2520on%2520the%2520rich%2520expression%2520space%2520of%2520neural%250Aparametric%2520head%2520models%2520%2528NPHM%2529%252C%2520instead%2520of%2520mesh-based%25203DMMs.%2520To%2520this%2520end%252C%2520we%250Adistill%2520the%2520backward%2520deformation%2520field%2520of%2520our%2520underlying%2520NPHM%2520into%2520forward%250Adeformations%2520which%2520are%2520compatible%2520with%2520rasterization-based%2520rendering.%2520All%250Aremaining%2520fine-scale%252C%2520expression-dependent%2520details%2520are%2520learned%2520from%2520the%250Amulti-view%2520videos.%2520For%2520increased%2520representational%2520capacity%2520of%2520our%2520avatars%252C%2520we%250Apropose%2520per-Gaussian%2520latent%2520features%2520that%2520condition%2520each%2520primitives%2520dynamic%250Abehavior.%2520To%2520regularize%2520this%2520increased%2520dynamic%2520expressivity%252C%2520we%2520propose%250ALaplacian%2520terms%2520on%2520the%2520latent%2520features%2520and%2520predicted%2520dynamics.%2520We%2520evaluate%2520our%250Amethod%2520on%2520the%2520public%2520NeRSemble%2520dataset%252C%2520demonstrating%2520that%2520NPGA%2520significantly%250Aoutperforms%2520the%2520previous%2520state-of-the-art%2520avatars%2520on%2520the%2520self-reenactment%2520task%250Aby%25202.6%2520PSNR.%2520Furthermore%252C%2520we%2520demonstrate%2520accurate%2520animation%2520capabilities%2520from%250Areal-world%2520monocular%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19331v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NPGA%3A%20Neural%20Parametric%20Gaussian%20Avatars&entry.906535625=Simon%20Giebenhain%20and%20Tobias%20Kirschstein%20and%20Martin%20R%C3%BCnz%20and%20Lourdes%20Agapito%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20The%20creation%20of%20high-fidelity%2C%20digital%20versions%20of%20human%20heads%20is%20an%0Aimportant%20stepping%20stone%20in%20the%20process%20of%20further%20integrating%20virtual%0Acomponents%20into%20our%20everyday%20lives.%20Constructing%20such%20avatars%20is%20a%20challenging%0Aresearch%20problem%2C%20due%20to%20a%20high%20demand%20for%20photo-realism%20and%20real-time%0Arendering%20performance.%20In%20this%20work%2C%20we%20propose%20Neural%20Parametric%20Gaussian%0AAvatars%20%28NPGA%29%2C%20a%20data-driven%20approach%20to%20create%20high-fidelity%2C%20controllable%0Aavatars%20from%20multi-view%20video%20recordings.%20We%20build%20our%20method%20around%203D%0AGaussian%20splatting%20for%20its%20highly%20efficient%20rendering%20and%20to%20inherit%20the%0Atopological%20flexibility%20of%20point%20clouds.%20In%20contrast%20to%20previous%20work%2C%20we%0Acondition%20our%20avatars%27%20dynamics%20on%20the%20rich%20expression%20space%20of%20neural%0Aparametric%20head%20models%20%28NPHM%29%2C%20instead%20of%20mesh-based%203DMMs.%20To%20this%20end%2C%20we%0Adistill%20the%20backward%20deformation%20field%20of%20our%20underlying%20NPHM%20into%20forward%0Adeformations%20which%20are%20compatible%20with%20rasterization-based%20rendering.%20All%0Aremaining%20fine-scale%2C%20expression-dependent%20details%20are%20learned%20from%20the%0Amulti-view%20videos.%20For%20increased%20representational%20capacity%20of%20our%20avatars%2C%20we%0Apropose%20per-Gaussian%20latent%20features%20that%20condition%20each%20primitives%20dynamic%0Abehavior.%20To%20regularize%20this%20increased%20dynamic%20expressivity%2C%20we%20propose%0ALaplacian%20terms%20on%20the%20latent%20features%20and%20predicted%20dynamics.%20We%20evaluate%20our%0Amethod%20on%20the%20public%20NeRSemble%20dataset%2C%20demonstrating%20that%20NPGA%20significantly%0Aoutperforms%20the%20previous%20state-of-the-art%20avatars%20on%20the%20self-reenactment%20task%0Aby%202.6%20PSNR.%20Furthermore%2C%20we%20demonstrate%20accurate%20animation%20capabilities%20from%0Areal-world%20monocular%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19331v2&entry.124074799=Read"},
{"title": "Autoregressive Sequence Modeling for 3D Medical Image Representation", "author": "Siwen Wang and Churan Wang and Fei Gao and Lixian Su and Fandong Zhang and Yizhou Wang and Yizhou Yu", "abstract": "  Three-dimensional (3D) medical images, such as Computed Tomography (CT) and\nMagnetic Resonance Imaging (MRI), are essential for clinical applications.\nHowever, the need for diverse and comprehensive representations is particularly\npronounced when considering the variability across different organs, diagnostic\ntasks, and imaging modalities. How to effectively interpret the intricate\ncontextual information and extract meaningful insights from these images\nremains an open challenge to the community. While current self-supervised\nlearning methods have shown potential, they often consider an image as a whole\nthereby overlooking the extensive, complex relationships among local regions\nfrom one or multiple images. In this work, we introduce a pioneering method for\nlearning 3D medical image representations through an autoregressive\npre-training framework. Our approach sequences various 3D medical images based\non spatial, contrast, and semantic correlations, treating them as\ninterconnected visual tokens within a token sequence. By employing an\nautoregressive sequence modeling task, we predict the next visual token in the\nsequence, which allows our model to deeply understand and integrate the\ncontextual information inherent in 3D medical images. Additionally, we\nimplement a random startup strategy to avoid overestimating token relationships\nand to enhance the robustness of learning. The effectiveness of our approach is\ndemonstrated by the superior performance over others on nine downstream tasks\nin public datasets.\n", "link": "http://arxiv.org/abs/2409.08691v1", "date": "2024-09-13", "relevancy": 2.9818, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6017}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5937}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Sequence%20Modeling%20for%203D%20Medical%20Image%20Representation&body=Title%3A%20Autoregressive%20Sequence%20Modeling%20for%203D%20Medical%20Image%20Representation%0AAuthor%3A%20Siwen%20Wang%20and%20Churan%20Wang%20and%20Fei%20Gao%20and%20Lixian%20Su%20and%20Fandong%20Zhang%20and%20Yizhou%20Wang%20and%20Yizhou%20Yu%0AAbstract%3A%20%20%20Three-dimensional%20%283D%29%20medical%20images%2C%20such%20as%20Computed%20Tomography%20%28CT%29%20and%0AMagnetic%20Resonance%20Imaging%20%28MRI%29%2C%20are%20essential%20for%20clinical%20applications.%0AHowever%2C%20the%20need%20for%20diverse%20and%20comprehensive%20representations%20is%20particularly%0Apronounced%20when%20considering%20the%20variability%20across%20different%20organs%2C%20diagnostic%0Atasks%2C%20and%20imaging%20modalities.%20How%20to%20effectively%20interpret%20the%20intricate%0Acontextual%20information%20and%20extract%20meaningful%20insights%20from%20these%20images%0Aremains%20an%20open%20challenge%20to%20the%20community.%20While%20current%20self-supervised%0Alearning%20methods%20have%20shown%20potential%2C%20they%20often%20consider%20an%20image%20as%20a%20whole%0Athereby%20overlooking%20the%20extensive%2C%20complex%20relationships%20among%20local%20regions%0Afrom%20one%20or%20multiple%20images.%20In%20this%20work%2C%20we%20introduce%20a%20pioneering%20method%20for%0Alearning%203D%20medical%20image%20representations%20through%20an%20autoregressive%0Apre-training%20framework.%20Our%20approach%20sequences%20various%203D%20medical%20images%20based%0Aon%20spatial%2C%20contrast%2C%20and%20semantic%20correlations%2C%20treating%20them%20as%0Ainterconnected%20visual%20tokens%20within%20a%20token%20sequence.%20By%20employing%20an%0Aautoregressive%20sequence%20modeling%20task%2C%20we%20predict%20the%20next%20visual%20token%20in%20the%0Asequence%2C%20which%20allows%20our%20model%20to%20deeply%20understand%20and%20integrate%20the%0Acontextual%20information%20inherent%20in%203D%20medical%20images.%20Additionally%2C%20we%0Aimplement%20a%20random%20startup%20strategy%20to%20avoid%20overestimating%20token%20relationships%0Aand%20to%20enhance%20the%20robustness%20of%20learning.%20The%20effectiveness%20of%20our%20approach%20is%0Ademonstrated%20by%20the%20superior%20performance%20over%20others%20on%20nine%20downstream%20tasks%0Ain%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Sequence%2520Modeling%2520for%25203D%2520Medical%2520Image%2520Representation%26entry.906535625%3DSiwen%2520Wang%2520and%2520Churan%2520Wang%2520and%2520Fei%2520Gao%2520and%2520Lixian%2520Su%2520and%2520Fandong%2520Zhang%2520and%2520Yizhou%2520Wang%2520and%2520Yizhou%2520Yu%26entry.1292438233%3D%2520%2520Three-dimensional%2520%25283D%2529%2520medical%2520images%252C%2520such%2520as%2520Computed%2520Tomography%2520%2528CT%2529%2520and%250AMagnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%252C%2520are%2520essential%2520for%2520clinical%2520applications.%250AHowever%252C%2520the%2520need%2520for%2520diverse%2520and%2520comprehensive%2520representations%2520is%2520particularly%250Apronounced%2520when%2520considering%2520the%2520variability%2520across%2520different%2520organs%252C%2520diagnostic%250Atasks%252C%2520and%2520imaging%2520modalities.%2520How%2520to%2520effectively%2520interpret%2520the%2520intricate%250Acontextual%2520information%2520and%2520extract%2520meaningful%2520insights%2520from%2520these%2520images%250Aremains%2520an%2520open%2520challenge%2520to%2520the%2520community.%2520While%2520current%2520self-supervised%250Alearning%2520methods%2520have%2520shown%2520potential%252C%2520they%2520often%2520consider%2520an%2520image%2520as%2520a%2520whole%250Athereby%2520overlooking%2520the%2520extensive%252C%2520complex%2520relationships%2520among%2520local%2520regions%250Afrom%2520one%2520or%2520multiple%2520images.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520pioneering%2520method%2520for%250Alearning%25203D%2520medical%2520image%2520representations%2520through%2520an%2520autoregressive%250Apre-training%2520framework.%2520Our%2520approach%2520sequences%2520various%25203D%2520medical%2520images%2520based%250Aon%2520spatial%252C%2520contrast%252C%2520and%2520semantic%2520correlations%252C%2520treating%2520them%2520as%250Ainterconnected%2520visual%2520tokens%2520within%2520a%2520token%2520sequence.%2520By%2520employing%2520an%250Aautoregressive%2520sequence%2520modeling%2520task%252C%2520we%2520predict%2520the%2520next%2520visual%2520token%2520in%2520the%250Asequence%252C%2520which%2520allows%2520our%2520model%2520to%2520deeply%2520understand%2520and%2520integrate%2520the%250Acontextual%2520information%2520inherent%2520in%25203D%2520medical%2520images.%2520Additionally%252C%2520we%250Aimplement%2520a%2520random%2520startup%2520strategy%2520to%2520avoid%2520overestimating%2520token%2520relationships%250Aand%2520to%2520enhance%2520the%2520robustness%2520of%2520learning.%2520The%2520effectiveness%2520of%2520our%2520approach%2520is%250Ademonstrated%2520by%2520the%2520superior%2520performance%2520over%2520others%2520on%2520nine%2520downstream%2520tasks%250Ain%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Sequence%20Modeling%20for%203D%20Medical%20Image%20Representation&entry.906535625=Siwen%20Wang%20and%20Churan%20Wang%20and%20Fei%20Gao%20and%20Lixian%20Su%20and%20Fandong%20Zhang%20and%20Yizhou%20Wang%20and%20Yizhou%20Yu&entry.1292438233=%20%20Three-dimensional%20%283D%29%20medical%20images%2C%20such%20as%20Computed%20Tomography%20%28CT%29%20and%0AMagnetic%20Resonance%20Imaging%20%28MRI%29%2C%20are%20essential%20for%20clinical%20applications.%0AHowever%2C%20the%20need%20for%20diverse%20and%20comprehensive%20representations%20is%20particularly%0Apronounced%20when%20considering%20the%20variability%20across%20different%20organs%2C%20diagnostic%0Atasks%2C%20and%20imaging%20modalities.%20How%20to%20effectively%20interpret%20the%20intricate%0Acontextual%20information%20and%20extract%20meaningful%20insights%20from%20these%20images%0Aremains%20an%20open%20challenge%20to%20the%20community.%20While%20current%20self-supervised%0Alearning%20methods%20have%20shown%20potential%2C%20they%20often%20consider%20an%20image%20as%20a%20whole%0Athereby%20overlooking%20the%20extensive%2C%20complex%20relationships%20among%20local%20regions%0Afrom%20one%20or%20multiple%20images.%20In%20this%20work%2C%20we%20introduce%20a%20pioneering%20method%20for%0Alearning%203D%20medical%20image%20representations%20through%20an%20autoregressive%0Apre-training%20framework.%20Our%20approach%20sequences%20various%203D%20medical%20images%20based%0Aon%20spatial%2C%20contrast%2C%20and%20semantic%20correlations%2C%20treating%20them%20as%0Ainterconnected%20visual%20tokens%20within%20a%20token%20sequence.%20By%20employing%20an%0Aautoregressive%20sequence%20modeling%20task%2C%20we%20predict%20the%20next%20visual%20token%20in%20the%0Asequence%2C%20which%20allows%20our%20model%20to%20deeply%20understand%20and%20integrate%20the%0Acontextual%20information%20inherent%20in%203D%20medical%20images.%20Additionally%2C%20we%0Aimplement%20a%20random%20startup%20strategy%20to%20avoid%20overestimating%20token%20relationships%0Aand%20to%20enhance%20the%20robustness%20of%20learning.%20The%20effectiveness%20of%20our%20approach%20is%0Ademonstrated%20by%20the%20superior%20performance%20over%20others%20on%20nine%20downstream%20tasks%0Ain%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08691v1&entry.124074799=Read"},
{"title": "ClearDepth: Enhanced Stereo Perception of Transparent Objects for\n  Robotic Manipulation", "author": "Kaixin Bai and Huajian Zeng and Lei Zhang and Yiwen Liu and Hongli Xu and Zhaopeng Chen and Jianwei Zhang", "abstract": "  Transparent object depth perception poses a challenge in everyday life and\nlogistics, primarily due to the inability of standard 3D sensors to accurately\ncapture depth on transparent or reflective surfaces. This limitation\nsignificantly affects depth map and point cloud-reliant applications,\nespecially in robotic manipulation. We developed a vision transformer-based\nalgorithm for stereo depth recovery of transparent objects. This approach is\ncomplemented by an innovative feature post-fusion module, which enhances the\naccuracy of depth recovery by structural features in images. To address the\nhigh costs associated with dataset collection for stereo camera-based\nperception of transparent objects, our method incorporates a parameter-aligned,\ndomain-adaptive, and physically realistic Sim2Real simulation for efficient\ndata generation, accelerated by AI algorithm. Our experimental results\ndemonstrate the model's exceptional Sim2Real generalizability in real-world\nscenarios, enabling precise depth mapping of transparent objects to assist in\nrobotic manipulation. Project details are available at\nhttps://sites.google.com/view/cleardepth/ .\n", "link": "http://arxiv.org/abs/2409.08926v1", "date": "2024-09-13", "relevancy": 2.9691, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5951}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClearDepth%3A%20Enhanced%20Stereo%20Perception%20of%20Transparent%20Objects%20for%0A%20%20Robotic%20Manipulation&body=Title%3A%20ClearDepth%3A%20Enhanced%20Stereo%20Perception%20of%20Transparent%20Objects%20for%0A%20%20Robotic%20Manipulation%0AAuthor%3A%20Kaixin%20Bai%20and%20Huajian%20Zeng%20and%20Lei%20Zhang%20and%20Yiwen%20Liu%20and%20Hongli%20Xu%20and%20Zhaopeng%20Chen%20and%20Jianwei%20Zhang%0AAbstract%3A%20%20%20Transparent%20object%20depth%20perception%20poses%20a%20challenge%20in%20everyday%20life%20and%0Alogistics%2C%20primarily%20due%20to%20the%20inability%20of%20standard%203D%20sensors%20to%20accurately%0Acapture%20depth%20on%20transparent%20or%20reflective%20surfaces.%20This%20limitation%0Asignificantly%20affects%20depth%20map%20and%20point%20cloud-reliant%20applications%2C%0Aespecially%20in%20robotic%20manipulation.%20We%20developed%20a%20vision%20transformer-based%0Aalgorithm%20for%20stereo%20depth%20recovery%20of%20transparent%20objects.%20This%20approach%20is%0Acomplemented%20by%20an%20innovative%20feature%20post-fusion%20module%2C%20which%20enhances%20the%0Aaccuracy%20of%20depth%20recovery%20by%20structural%20features%20in%20images.%20To%20address%20the%0Ahigh%20costs%20associated%20with%20dataset%20collection%20for%20stereo%20camera-based%0Aperception%20of%20transparent%20objects%2C%20our%20method%20incorporates%20a%20parameter-aligned%2C%0Adomain-adaptive%2C%20and%20physically%20realistic%20Sim2Real%20simulation%20for%20efficient%0Adata%20generation%2C%20accelerated%20by%20AI%20algorithm.%20Our%20experimental%20results%0Ademonstrate%20the%20model%27s%20exceptional%20Sim2Real%20generalizability%20in%20real-world%0Ascenarios%2C%20enabling%20precise%20depth%20mapping%20of%20transparent%20objects%20to%20assist%20in%0Arobotic%20manipulation.%20Project%20details%20are%20available%20at%0Ahttps%3A//sites.google.com/view/cleardepth/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClearDepth%253A%2520Enhanced%2520Stereo%2520Perception%2520of%2520Transparent%2520Objects%2520for%250A%2520%2520Robotic%2520Manipulation%26entry.906535625%3DKaixin%2520Bai%2520and%2520Huajian%2520Zeng%2520and%2520Lei%2520Zhang%2520and%2520Yiwen%2520Liu%2520and%2520Hongli%2520Xu%2520and%2520Zhaopeng%2520Chen%2520and%2520Jianwei%2520Zhang%26entry.1292438233%3D%2520%2520Transparent%2520object%2520depth%2520perception%2520poses%2520a%2520challenge%2520in%2520everyday%2520life%2520and%250Alogistics%252C%2520primarily%2520due%2520to%2520the%2520inability%2520of%2520standard%25203D%2520sensors%2520to%2520accurately%250Acapture%2520depth%2520on%2520transparent%2520or%2520reflective%2520surfaces.%2520This%2520limitation%250Asignificantly%2520affects%2520depth%2520map%2520and%2520point%2520cloud-reliant%2520applications%252C%250Aespecially%2520in%2520robotic%2520manipulation.%2520We%2520developed%2520a%2520vision%2520transformer-based%250Aalgorithm%2520for%2520stereo%2520depth%2520recovery%2520of%2520transparent%2520objects.%2520This%2520approach%2520is%250Acomplemented%2520by%2520an%2520innovative%2520feature%2520post-fusion%2520module%252C%2520which%2520enhances%2520the%250Aaccuracy%2520of%2520depth%2520recovery%2520by%2520structural%2520features%2520in%2520images.%2520To%2520address%2520the%250Ahigh%2520costs%2520associated%2520with%2520dataset%2520collection%2520for%2520stereo%2520camera-based%250Aperception%2520of%2520transparent%2520objects%252C%2520our%2520method%2520incorporates%2520a%2520parameter-aligned%252C%250Adomain-adaptive%252C%2520and%2520physically%2520realistic%2520Sim2Real%2520simulation%2520for%2520efficient%250Adata%2520generation%252C%2520accelerated%2520by%2520AI%2520algorithm.%2520Our%2520experimental%2520results%250Ademonstrate%2520the%2520model%2527s%2520exceptional%2520Sim2Real%2520generalizability%2520in%2520real-world%250Ascenarios%252C%2520enabling%2520precise%2520depth%2520mapping%2520of%2520transparent%2520objects%2520to%2520assist%2520in%250Arobotic%2520manipulation.%2520Project%2520details%2520are%2520available%2520at%250Ahttps%253A//sites.google.com/view/cleardepth/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClearDepth%3A%20Enhanced%20Stereo%20Perception%20of%20Transparent%20Objects%20for%0A%20%20Robotic%20Manipulation&entry.906535625=Kaixin%20Bai%20and%20Huajian%20Zeng%20and%20Lei%20Zhang%20and%20Yiwen%20Liu%20and%20Hongli%20Xu%20and%20Zhaopeng%20Chen%20and%20Jianwei%20Zhang&entry.1292438233=%20%20Transparent%20object%20depth%20perception%20poses%20a%20challenge%20in%20everyday%20life%20and%0Alogistics%2C%20primarily%20due%20to%20the%20inability%20of%20standard%203D%20sensors%20to%20accurately%0Acapture%20depth%20on%20transparent%20or%20reflective%20surfaces.%20This%20limitation%0Asignificantly%20affects%20depth%20map%20and%20point%20cloud-reliant%20applications%2C%0Aespecially%20in%20robotic%20manipulation.%20We%20developed%20a%20vision%20transformer-based%0Aalgorithm%20for%20stereo%20depth%20recovery%20of%20transparent%20objects.%20This%20approach%20is%0Acomplemented%20by%20an%20innovative%20feature%20post-fusion%20module%2C%20which%20enhances%20the%0Aaccuracy%20of%20depth%20recovery%20by%20structural%20features%20in%20images.%20To%20address%20the%0Ahigh%20costs%20associated%20with%20dataset%20collection%20for%20stereo%20camera-based%0Aperception%20of%20transparent%20objects%2C%20our%20method%20incorporates%20a%20parameter-aligned%2C%0Adomain-adaptive%2C%20and%20physically%20realistic%20Sim2Real%20simulation%20for%20efficient%0Adata%20generation%2C%20accelerated%20by%20AI%20algorithm.%20Our%20experimental%20results%0Ademonstrate%20the%20model%27s%20exceptional%20Sim2Real%20generalizability%20in%20real-world%0Ascenarios%2C%20enabling%20precise%20depth%20mapping%20of%20transparent%20objects%20to%20assist%20in%0Arobotic%20manipulation.%20Project%20details%20are%20available%20at%0Ahttps%3A//sites.google.com/view/cleardepth/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08926v1&entry.124074799=Read"},
{"title": "LTRL: Boosting Long-tail Recognition via Reflective Learning", "author": "Qihao Zhao and Yalun Dai and Shen Lin and Wei Hu and Fan Zhang and Jun Liu", "abstract": "  In real-world scenarios, where knowledge distributions exhibit long-tail.\nHumans manage to master knowledge uniformly across imbalanced distributions, a\nfeat attributed to their diligent practices of reviewing, summarizing, and\ncorrecting errors. Motivated by this learning process, we propose a novel\nlearning paradigm, called reflecting learning, in handling long-tail\nrecognition. Our method integrates three processes for reviewing past\npredictions during training, summarizing and leveraging the feature relation\nacross classes, and correcting gradient conflict for loss functions. These\ndesigns are lightweight enough to plug and play with existing long-tail\nlearning methods, achieving state-of-the-art performance in popular long-tail\nvisual benchmarks. The experimental results highlight the great potential of\nreflecting learning in dealing with long-tail recognition.\n", "link": "http://arxiv.org/abs/2407.12568v2", "date": "2024-09-13", "relevancy": 2.94, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6398}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5664}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LTRL%3A%20Boosting%20Long-tail%20Recognition%20via%20Reflective%20Learning&body=Title%3A%20LTRL%3A%20Boosting%20Long-tail%20Recognition%20via%20Reflective%20Learning%0AAuthor%3A%20Qihao%20Zhao%20and%20Yalun%20Dai%20and%20Shen%20Lin%20and%20Wei%20Hu%20and%20Fan%20Zhang%20and%20Jun%20Liu%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20where%20knowledge%20distributions%20exhibit%20long-tail.%0AHumans%20manage%20to%20master%20knowledge%20uniformly%20across%20imbalanced%20distributions%2C%20a%0Afeat%20attributed%20to%20their%20diligent%20practices%20of%20reviewing%2C%20summarizing%2C%20and%0Acorrecting%20errors.%20Motivated%20by%20this%20learning%20process%2C%20we%20propose%20a%20novel%0Alearning%20paradigm%2C%20called%20reflecting%20learning%2C%20in%20handling%20long-tail%0Arecognition.%20Our%20method%20integrates%20three%20processes%20for%20reviewing%20past%0Apredictions%20during%20training%2C%20summarizing%20and%20leveraging%20the%20feature%20relation%0Aacross%20classes%2C%20and%20correcting%20gradient%20conflict%20for%20loss%20functions.%20These%0Adesigns%20are%20lightweight%20enough%20to%20plug%20and%20play%20with%20existing%20long-tail%0Alearning%20methods%2C%20achieving%20state-of-the-art%20performance%20in%20popular%20long-tail%0Avisual%20benchmarks.%20The%20experimental%20results%20highlight%20the%20great%20potential%20of%0Areflecting%20learning%20in%20dealing%20with%20long-tail%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLTRL%253A%2520Boosting%2520Long-tail%2520Recognition%2520via%2520Reflective%2520Learning%26entry.906535625%3DQihao%2520Zhao%2520and%2520Yalun%2520Dai%2520and%2520Shen%2520Lin%2520and%2520Wei%2520Hu%2520and%2520Fan%2520Zhang%2520and%2520Jun%2520Liu%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520where%2520knowledge%2520distributions%2520exhibit%2520long-tail.%250AHumans%2520manage%2520to%2520master%2520knowledge%2520uniformly%2520across%2520imbalanced%2520distributions%252C%2520a%250Afeat%2520attributed%2520to%2520their%2520diligent%2520practices%2520of%2520reviewing%252C%2520summarizing%252C%2520and%250Acorrecting%2520errors.%2520Motivated%2520by%2520this%2520learning%2520process%252C%2520we%2520propose%2520a%2520novel%250Alearning%2520paradigm%252C%2520called%2520reflecting%2520learning%252C%2520in%2520handling%2520long-tail%250Arecognition.%2520Our%2520method%2520integrates%2520three%2520processes%2520for%2520reviewing%2520past%250Apredictions%2520during%2520training%252C%2520summarizing%2520and%2520leveraging%2520the%2520feature%2520relation%250Aacross%2520classes%252C%2520and%2520correcting%2520gradient%2520conflict%2520for%2520loss%2520functions.%2520These%250Adesigns%2520are%2520lightweight%2520enough%2520to%2520plug%2520and%2520play%2520with%2520existing%2520long-tail%250Alearning%2520methods%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520popular%2520long-tail%250Avisual%2520benchmarks.%2520The%2520experimental%2520results%2520highlight%2520the%2520great%2520potential%2520of%250Areflecting%2520learning%2520in%2520dealing%2520with%2520long-tail%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTRL%3A%20Boosting%20Long-tail%20Recognition%20via%20Reflective%20Learning&entry.906535625=Qihao%20Zhao%20and%20Yalun%20Dai%20and%20Shen%20Lin%20and%20Wei%20Hu%20and%20Fan%20Zhang%20and%20Jun%20Liu&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20where%20knowledge%20distributions%20exhibit%20long-tail.%0AHumans%20manage%20to%20master%20knowledge%20uniformly%20across%20imbalanced%20distributions%2C%20a%0Afeat%20attributed%20to%20their%20diligent%20practices%20of%20reviewing%2C%20summarizing%2C%20and%0Acorrecting%20errors.%20Motivated%20by%20this%20learning%20process%2C%20we%20propose%20a%20novel%0Alearning%20paradigm%2C%20called%20reflecting%20learning%2C%20in%20handling%20long-tail%0Arecognition.%20Our%20method%20integrates%20three%20processes%20for%20reviewing%20past%0Apredictions%20during%20training%2C%20summarizing%20and%20leveraging%20the%20feature%20relation%0Aacross%20classes%2C%20and%20correcting%20gradient%20conflict%20for%20loss%20functions.%20These%0Adesigns%20are%20lightweight%20enough%20to%20plug%20and%20play%20with%20existing%20long-tail%0Alearning%20methods%2C%20achieving%20state-of-the-art%20performance%20in%20popular%20long-tail%0Avisual%20benchmarks.%20The%20experimental%20results%20highlight%20the%20great%20potential%20of%0Areflecting%20learning%20in%20dealing%20with%20long-tail%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12568v2&entry.124074799=Read"},
{"title": "Exploring Graph Structure Comprehension Ability of Multimodal Large\n  Language Models: Case Studies", "author": "Zhiqiang Zhong and Davide Mottin", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in processing\nvarious data structures, including graphs. While previous research has focused\non developing textual encoding methods for graph representation, the emergence\nof multimodal LLMs presents a new frontier for graph comprehension. These\nadvanced models, capable of processing both text and images, offer potential\nimprovements in graph understanding by incorporating visual representations\nalongside traditional textual data. This study investigates the impact of graph\nvisualisations on LLM performance across a range of benchmark tasks at node,\nedge, and graph levels. Our experiments compare the effectiveness of multimodal\napproaches against purely textual graph representations. The results provide\nvaluable insights into both the potential and limitations of leveraging visual\ngraph modalities to enhance LLMs' graph structure comprehension abilities.\n", "link": "http://arxiv.org/abs/2409.08864v1", "date": "2024-09-13", "relevancy": 2.926, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6036}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Graph%20Structure%20Comprehension%20Ability%20of%20Multimodal%20Large%0A%20%20Language%20Models%3A%20Case%20Studies&body=Title%3A%20Exploring%20Graph%20Structure%20Comprehension%20Ability%20of%20Multimodal%20Large%0A%20%20Language%20Models%3A%20Case%20Studies%0AAuthor%3A%20Zhiqiang%20Zhong%20and%20Davide%20Mottin%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20processing%0Avarious%20data%20structures%2C%20including%20graphs.%20While%20previous%20research%20has%20focused%0Aon%20developing%20textual%20encoding%20methods%20for%20graph%20representation%2C%20the%20emergence%0Aof%20multimodal%20LLMs%20presents%20a%20new%20frontier%20for%20graph%20comprehension.%20These%0Aadvanced%20models%2C%20capable%20of%20processing%20both%20text%20and%20images%2C%20offer%20potential%0Aimprovements%20in%20graph%20understanding%20by%20incorporating%20visual%20representations%0Aalongside%20traditional%20textual%20data.%20This%20study%20investigates%20the%20impact%20of%20graph%0Avisualisations%20on%20LLM%20performance%20across%20a%20range%20of%20benchmark%20tasks%20at%20node%2C%0Aedge%2C%20and%20graph%20levels.%20Our%20experiments%20compare%20the%20effectiveness%20of%20multimodal%0Aapproaches%20against%20purely%20textual%20graph%20representations.%20The%20results%20provide%0Avaluable%20insights%20into%20both%20the%20potential%20and%20limitations%20of%20leveraging%20visual%0Agraph%20modalities%20to%20enhance%20LLMs%27%20graph%20structure%20comprehension%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Graph%2520Structure%2520Comprehension%2520Ability%2520of%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%253A%2520Case%2520Studies%26entry.906535625%3DZhiqiang%2520Zhong%2520and%2520Davide%2520Mottin%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520processing%250Avarious%2520data%2520structures%252C%2520including%2520graphs.%2520While%2520previous%2520research%2520has%2520focused%250Aon%2520developing%2520textual%2520encoding%2520methods%2520for%2520graph%2520representation%252C%2520the%2520emergence%250Aof%2520multimodal%2520LLMs%2520presents%2520a%2520new%2520frontier%2520for%2520graph%2520comprehension.%2520These%250Aadvanced%2520models%252C%2520capable%2520of%2520processing%2520both%2520text%2520and%2520images%252C%2520offer%2520potential%250Aimprovements%2520in%2520graph%2520understanding%2520by%2520incorporating%2520visual%2520representations%250Aalongside%2520traditional%2520textual%2520data.%2520This%2520study%2520investigates%2520the%2520impact%2520of%2520graph%250Avisualisations%2520on%2520LLM%2520performance%2520across%2520a%2520range%2520of%2520benchmark%2520tasks%2520at%2520node%252C%250Aedge%252C%2520and%2520graph%2520levels.%2520Our%2520experiments%2520compare%2520the%2520effectiveness%2520of%2520multimodal%250Aapproaches%2520against%2520purely%2520textual%2520graph%2520representations.%2520The%2520results%2520provide%250Avaluable%2520insights%2520into%2520both%2520the%2520potential%2520and%2520limitations%2520of%2520leveraging%2520visual%250Agraph%2520modalities%2520to%2520enhance%2520LLMs%2527%2520graph%2520structure%2520comprehension%2520abilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Graph%20Structure%20Comprehension%20Ability%20of%20Multimodal%20Large%0A%20%20Language%20Models%3A%20Case%20Studies&entry.906535625=Zhiqiang%20Zhong%20and%20Davide%20Mottin&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20processing%0Avarious%20data%20structures%2C%20including%20graphs.%20While%20previous%20research%20has%20focused%0Aon%20developing%20textual%20encoding%20methods%20for%20graph%20representation%2C%20the%20emergence%0Aof%20multimodal%20LLMs%20presents%20a%20new%20frontier%20for%20graph%20comprehension.%20These%0Aadvanced%20models%2C%20capable%20of%20processing%20both%20text%20and%20images%2C%20offer%20potential%0Aimprovements%20in%20graph%20understanding%20by%20incorporating%20visual%20representations%0Aalongside%20traditional%20textual%20data.%20This%20study%20investigates%20the%20impact%20of%20graph%0Avisualisations%20on%20LLM%20performance%20across%20a%20range%20of%20benchmark%20tasks%20at%20node%2C%0Aedge%2C%20and%20graph%20levels.%20Our%20experiments%20compare%20the%20effectiveness%20of%20multimodal%0Aapproaches%20against%20purely%20textual%20graph%20representations.%20The%20results%20provide%0Avaluable%20insights%20into%20both%20the%20potential%20and%20limitations%20of%20leveraging%20visual%0Agraph%20modalities%20to%20enhance%20LLMs%27%20graph%20structure%20comprehension%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08864v1&entry.124074799=Read"},
{"title": "Visual Language Tracking with Multi-modal Interaction: A Robust\n  Benchmark", "author": "Xuchen Li and Shiyu Hu and Xiaokun Feng and Dailing Zhang and Meiqi Wu and Jing Zhang and Kaiqi Huang", "abstract": "  Visual Language Tracking (VLT) enhances tracking by mitigating the\nlimitations of relying solely on the visual modality, utilizing high-level\nsemantic information through language. This integration of the language enables\nmore advanced human-machine interaction. The essence of interaction is\ncognitive alignment, which typically requires multiple information exchanges,\nespecially in the sequential decision-making process of VLT. However, current\nVLT benchmarks do not account for multi-round interactions during tracking.\nThey provide only an initial text and bounding box (bbox) in the first frame,\nwith no further interaction as tracking progresses, deviating from the original\nmotivation of the VLT task. To address these limitations, we propose a novel\nand robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal\nInteraction), which introduces multi-round interaction into the VLT task for\nthe first time. (1) We generate diverse, multi-granularity texts for\nmulti-round, multi-modal interaction based on existing mainstream VLT\nbenchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We\npropose a new VLT interaction paradigm that achieves multi-round interaction\nthrough text updates and object recovery. When multiple tracking failures\noccur, we provide the tracker with more aligned texts and corrected bboxes\nthrough interaction, thereby expanding the scope of VLT downstream tasks. (3)\nWe conduct comparative experiments on both traditional VLT benchmarks and\nVLT-MI, evaluating and analyzing the accuracy and robustness of trackers under\nthe interactive paradigm. This work offers new insights and paradigms for the\nVLT task, enabling a fine-grained evaluation of multi-modal trackers. We\nbelieve this approach can be extended to additional datasets in the future,\nsupporting broader evaluations and comparisons of video-language model\ncapabilities.\n", "link": "http://arxiv.org/abs/2409.08887v1", "date": "2024-09-13", "relevancy": 2.7916, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Language%20Tracking%20with%20Multi-modal%20Interaction%3A%20A%20Robust%0A%20%20Benchmark&body=Title%3A%20Visual%20Language%20Tracking%20with%20Multi-modal%20Interaction%3A%20A%20Robust%0A%20%20Benchmark%0AAuthor%3A%20Xuchen%20Li%20and%20Shiyu%20Hu%20and%20Xiaokun%20Feng%20and%20Dailing%20Zhang%20and%20Meiqi%20Wu%20and%20Jing%20Zhang%20and%20Kaiqi%20Huang%0AAbstract%3A%20%20%20Visual%20Language%20Tracking%20%28VLT%29%20enhances%20tracking%20by%20mitigating%20the%0Alimitations%20of%20relying%20solely%20on%20the%20visual%20modality%2C%20utilizing%20high-level%0Asemantic%20information%20through%20language.%20This%20integration%20of%20the%20language%20enables%0Amore%20advanced%20human-machine%20interaction.%20The%20essence%20of%20interaction%20is%0Acognitive%20alignment%2C%20which%20typically%20requires%20multiple%20information%20exchanges%2C%0Aespecially%20in%20the%20sequential%20decision-making%20process%20of%20VLT.%20However%2C%20current%0AVLT%20benchmarks%20do%20not%20account%20for%20multi-round%20interactions%20during%20tracking.%0AThey%20provide%20only%20an%20initial%20text%20and%20bounding%20box%20%28bbox%29%20in%20the%20first%20frame%2C%0Awith%20no%20further%20interaction%20as%20tracking%20progresses%2C%20deviating%20from%20the%20original%0Amotivation%20of%20the%20VLT%20task.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Aand%20robust%20benchmark%2C%20VLT-MI%20%28Visual%20Language%20Tracking%20with%20Multi-modal%0AInteraction%29%2C%20which%20introduces%20multi-round%20interaction%20into%20the%20VLT%20task%20for%0Athe%20first%20time.%20%281%29%20We%20generate%20diverse%2C%20multi-granularity%20texts%20for%0Amulti-round%2C%20multi-modal%20interaction%20based%20on%20existing%20mainstream%20VLT%0Abenchmarks%20using%20DTLLM-VLT%2C%20leveraging%20the%20world%20knowledge%20of%20LLMs.%20%282%29%20We%0Apropose%20a%20new%20VLT%20interaction%20paradigm%20that%20achieves%20multi-round%20interaction%0Athrough%20text%20updates%20and%20object%20recovery.%20When%20multiple%20tracking%20failures%0Aoccur%2C%20we%20provide%20the%20tracker%20with%20more%20aligned%20texts%20and%20corrected%20bboxes%0Athrough%20interaction%2C%20thereby%20expanding%20the%20scope%20of%20VLT%20downstream%20tasks.%20%283%29%0AWe%20conduct%20comparative%20experiments%20on%20both%20traditional%20VLT%20benchmarks%20and%0AVLT-MI%2C%20evaluating%20and%20analyzing%20the%20accuracy%20and%20robustness%20of%20trackers%20under%0Athe%20interactive%20paradigm.%20This%20work%20offers%20new%20insights%20and%20paradigms%20for%20the%0AVLT%20task%2C%20enabling%20a%20fine-grained%20evaluation%20of%20multi-modal%20trackers.%20We%0Abelieve%20this%20approach%20can%20be%20extended%20to%20additional%20datasets%20in%20the%20future%2C%0Asupporting%20broader%20evaluations%20and%20comparisons%20of%20video-language%20model%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Language%2520Tracking%2520with%2520Multi-modal%2520Interaction%253A%2520A%2520Robust%250A%2520%2520Benchmark%26entry.906535625%3DXuchen%2520Li%2520and%2520Shiyu%2520Hu%2520and%2520Xiaokun%2520Feng%2520and%2520Dailing%2520Zhang%2520and%2520Meiqi%2520Wu%2520and%2520Jing%2520Zhang%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3D%2520%2520Visual%2520Language%2520Tracking%2520%2528VLT%2529%2520enhances%2520tracking%2520by%2520mitigating%2520the%250Alimitations%2520of%2520relying%2520solely%2520on%2520the%2520visual%2520modality%252C%2520utilizing%2520high-level%250Asemantic%2520information%2520through%2520language.%2520This%2520integration%2520of%2520the%2520language%2520enables%250Amore%2520advanced%2520human-machine%2520interaction.%2520The%2520essence%2520of%2520interaction%2520is%250Acognitive%2520alignment%252C%2520which%2520typically%2520requires%2520multiple%2520information%2520exchanges%252C%250Aespecially%2520in%2520the%2520sequential%2520decision-making%2520process%2520of%2520VLT.%2520However%252C%2520current%250AVLT%2520benchmarks%2520do%2520not%2520account%2520for%2520multi-round%2520interactions%2520during%2520tracking.%250AThey%2520provide%2520only%2520an%2520initial%2520text%2520and%2520bounding%2520box%2520%2528bbox%2529%2520in%2520the%2520first%2520frame%252C%250Awith%2520no%2520further%2520interaction%2520as%2520tracking%2520progresses%252C%2520deviating%2520from%2520the%2520original%250Amotivation%2520of%2520the%2520VLT%2520task.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Aand%2520robust%2520benchmark%252C%2520VLT-MI%2520%2528Visual%2520Language%2520Tracking%2520with%2520Multi-modal%250AInteraction%2529%252C%2520which%2520introduces%2520multi-round%2520interaction%2520into%2520the%2520VLT%2520task%2520for%250Athe%2520first%2520time.%2520%25281%2529%2520We%2520generate%2520diverse%252C%2520multi-granularity%2520texts%2520for%250Amulti-round%252C%2520multi-modal%2520interaction%2520based%2520on%2520existing%2520mainstream%2520VLT%250Abenchmarks%2520using%2520DTLLM-VLT%252C%2520leveraging%2520the%2520world%2520knowledge%2520of%2520LLMs.%2520%25282%2529%2520We%250Apropose%2520a%2520new%2520VLT%2520interaction%2520paradigm%2520that%2520achieves%2520multi-round%2520interaction%250Athrough%2520text%2520updates%2520and%2520object%2520recovery.%2520When%2520multiple%2520tracking%2520failures%250Aoccur%252C%2520we%2520provide%2520the%2520tracker%2520with%2520more%2520aligned%2520texts%2520and%2520corrected%2520bboxes%250Athrough%2520interaction%252C%2520thereby%2520expanding%2520the%2520scope%2520of%2520VLT%2520downstream%2520tasks.%2520%25283%2529%250AWe%2520conduct%2520comparative%2520experiments%2520on%2520both%2520traditional%2520VLT%2520benchmarks%2520and%250AVLT-MI%252C%2520evaluating%2520and%2520analyzing%2520the%2520accuracy%2520and%2520robustness%2520of%2520trackers%2520under%250Athe%2520interactive%2520paradigm.%2520This%2520work%2520offers%2520new%2520insights%2520and%2520paradigms%2520for%2520the%250AVLT%2520task%252C%2520enabling%2520a%2520fine-grained%2520evaluation%2520of%2520multi-modal%2520trackers.%2520We%250Abelieve%2520this%2520approach%2520can%2520be%2520extended%2520to%2520additional%2520datasets%2520in%2520the%2520future%252C%250Asupporting%2520broader%2520evaluations%2520and%2520comparisons%2520of%2520video-language%2520model%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Language%20Tracking%20with%20Multi-modal%20Interaction%3A%20A%20Robust%0A%20%20Benchmark&entry.906535625=Xuchen%20Li%20and%20Shiyu%20Hu%20and%20Xiaokun%20Feng%20and%20Dailing%20Zhang%20and%20Meiqi%20Wu%20and%20Jing%20Zhang%20and%20Kaiqi%20Huang&entry.1292438233=%20%20Visual%20Language%20Tracking%20%28VLT%29%20enhances%20tracking%20by%20mitigating%20the%0Alimitations%20of%20relying%20solely%20on%20the%20visual%20modality%2C%20utilizing%20high-level%0Asemantic%20information%20through%20language.%20This%20integration%20of%20the%20language%20enables%0Amore%20advanced%20human-machine%20interaction.%20The%20essence%20of%20interaction%20is%0Acognitive%20alignment%2C%20which%20typically%20requires%20multiple%20information%20exchanges%2C%0Aespecially%20in%20the%20sequential%20decision-making%20process%20of%20VLT.%20However%2C%20current%0AVLT%20benchmarks%20do%20not%20account%20for%20multi-round%20interactions%20during%20tracking.%0AThey%20provide%20only%20an%20initial%20text%20and%20bounding%20box%20%28bbox%29%20in%20the%20first%20frame%2C%0Awith%20no%20further%20interaction%20as%20tracking%20progresses%2C%20deviating%20from%20the%20original%0Amotivation%20of%20the%20VLT%20task.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Aand%20robust%20benchmark%2C%20VLT-MI%20%28Visual%20Language%20Tracking%20with%20Multi-modal%0AInteraction%29%2C%20which%20introduces%20multi-round%20interaction%20into%20the%20VLT%20task%20for%0Athe%20first%20time.%20%281%29%20We%20generate%20diverse%2C%20multi-granularity%20texts%20for%0Amulti-round%2C%20multi-modal%20interaction%20based%20on%20existing%20mainstream%20VLT%0Abenchmarks%20using%20DTLLM-VLT%2C%20leveraging%20the%20world%20knowledge%20of%20LLMs.%20%282%29%20We%0Apropose%20a%20new%20VLT%20interaction%20paradigm%20that%20achieves%20multi-round%20interaction%0Athrough%20text%20updates%20and%20object%20recovery.%20When%20multiple%20tracking%20failures%0Aoccur%2C%20we%20provide%20the%20tracker%20with%20more%20aligned%20texts%20and%20corrected%20bboxes%0Athrough%20interaction%2C%20thereby%20expanding%20the%20scope%20of%20VLT%20downstream%20tasks.%20%283%29%0AWe%20conduct%20comparative%20experiments%20on%20both%20traditional%20VLT%20benchmarks%20and%0AVLT-MI%2C%20evaluating%20and%20analyzing%20the%20accuracy%20and%20robustness%20of%20trackers%20under%0Athe%20interactive%20paradigm.%20This%20work%20offers%20new%20insights%20and%20paradigms%20for%20the%0AVLT%20task%2C%20enabling%20a%20fine-grained%20evaluation%20of%20multi-modal%20trackers.%20We%0Abelieve%20this%20approach%20can%20be%20extended%20to%20additional%20datasets%20in%20the%20future%2C%0Asupporting%20broader%20evaluations%20and%20comparisons%20of%20video-language%20model%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08887v1&entry.124074799=Read"},
{"title": "Direct-CP: Directed Collaborative Perception for Connected and\n  Autonomous Vehicles via Proactive Attention", "author": "Yihang Tao and Senkang Hu and Zhengru Fang and Yuguang Fang", "abstract": "  Collaborative perception (CP) leverages visual data from connected and\nautonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV).\nDespite recent progress, current CP methods expand the ego vehicle's 360-degree\nperceptual range almost equally, which faces two key challenges. Firstly, in\nareas with uneven traffic distribution, focusing on directions with little\ntraffic offers limited benefits. Secondly, under limited communication budgets,\nallocating excessive bandwidth to less critical directions lowers the\nperception accuracy in more vital areas. To address these issues, we propose\nDirect-CP, a proactive and direction-aware CP system aiming at improving CP in\nspecific directions. Our key idea is to enable an ego vehicle to proactively\nsignal its interested directions and readjust its attention to enhance local\ndirectional CP performance. To achieve this, we first propose an RSU-aided\ndirection masking mechanism that assists an ego vehicle in identifying vital\ndirections. Additionally, we design a direction-aware selective attention\nmodule to wisely aggregate pertinent features based on ego vehicle's\ndirectional priorities, communication budget, and the positional data of CAVs.\nMoreover, we introduce a direction-weighted detection loss (DWLoss) to capture\nthe divergence between directional CP outcomes and the ground truth,\nfacilitating effective model training. Extensive experiments on the V2X-Sim 2.0\ndataset demonstrate that our approach achieves 19.8\\% higher local perception\naccuracy in interested directions and 2.5\\% higher overall perception accuracy\nthan the state-of-the-art methods in collaborative 3D object detection tasks.\n", "link": "http://arxiv.org/abs/2409.08840v1", "date": "2024-09-13", "relevancy": 2.7671, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5863}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct-CP%3A%20Directed%20Collaborative%20Perception%20for%20Connected%20and%0A%20%20Autonomous%20Vehicles%20via%20Proactive%20Attention&body=Title%3A%20Direct-CP%3A%20Directed%20Collaborative%20Perception%20for%20Connected%20and%0A%20%20Autonomous%20Vehicles%20via%20Proactive%20Attention%0AAuthor%3A%20Yihang%20Tao%20and%20Senkang%20Hu%20and%20Zhengru%20Fang%20and%20Yuguang%20Fang%0AAbstract%3A%20%20%20Collaborative%20perception%20%28CP%29%20leverages%20visual%20data%20from%20connected%20and%0Aautonomous%20vehicles%20%28CAV%29%20to%20enhance%20an%20ego%20vehicle%27s%20field%20of%20view%20%28FoV%29.%0ADespite%20recent%20progress%2C%20current%20CP%20methods%20expand%20the%20ego%20vehicle%27s%20360-degree%0Aperceptual%20range%20almost%20equally%2C%20which%20faces%20two%20key%20challenges.%20Firstly%2C%20in%0Aareas%20with%20uneven%20traffic%20distribution%2C%20focusing%20on%20directions%20with%20little%0Atraffic%20offers%20limited%20benefits.%20Secondly%2C%20under%20limited%20communication%20budgets%2C%0Aallocating%20excessive%20bandwidth%20to%20less%20critical%20directions%20lowers%20the%0Aperception%20accuracy%20in%20more%20vital%20areas.%20To%20address%20these%20issues%2C%20we%20propose%0ADirect-CP%2C%20a%20proactive%20and%20direction-aware%20CP%20system%20aiming%20at%20improving%20CP%20in%0Aspecific%20directions.%20Our%20key%20idea%20is%20to%20enable%20an%20ego%20vehicle%20to%20proactively%0Asignal%20its%20interested%20directions%20and%20readjust%20its%20attention%20to%20enhance%20local%0Adirectional%20CP%20performance.%20To%20achieve%20this%2C%20we%20first%20propose%20an%20RSU-aided%0Adirection%20masking%20mechanism%20that%20assists%20an%20ego%20vehicle%20in%20identifying%20vital%0Adirections.%20Additionally%2C%20we%20design%20a%20direction-aware%20selective%20attention%0Amodule%20to%20wisely%20aggregate%20pertinent%20features%20based%20on%20ego%20vehicle%27s%0Adirectional%20priorities%2C%20communication%20budget%2C%20and%20the%20positional%20data%20of%20CAVs.%0AMoreover%2C%20we%20introduce%20a%20direction-weighted%20detection%20loss%20%28DWLoss%29%20to%20capture%0Athe%20divergence%20between%20directional%20CP%20outcomes%20and%20the%20ground%20truth%2C%0Afacilitating%20effective%20model%20training.%20Extensive%20experiments%20on%20the%20V2X-Sim%202.0%0Adataset%20demonstrate%20that%20our%20approach%20achieves%2019.8%5C%25%20higher%20local%20perception%0Aaccuracy%20in%20interested%20directions%20and%202.5%5C%25%20higher%20overall%20perception%20accuracy%0Athan%20the%20state-of-the-art%20methods%20in%20collaborative%203D%20object%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect-CP%253A%2520Directed%2520Collaborative%2520Perception%2520for%2520Connected%2520and%250A%2520%2520Autonomous%2520Vehicles%2520via%2520Proactive%2520Attention%26entry.906535625%3DYihang%2520Tao%2520and%2520Senkang%2520Hu%2520and%2520Zhengru%2520Fang%2520and%2520Yuguang%2520Fang%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520%2528CP%2529%2520leverages%2520visual%2520data%2520from%2520connected%2520and%250Aautonomous%2520vehicles%2520%2528CAV%2529%2520to%2520enhance%2520an%2520ego%2520vehicle%2527s%2520field%2520of%2520view%2520%2528FoV%2529.%250ADespite%2520recent%2520progress%252C%2520current%2520CP%2520methods%2520expand%2520the%2520ego%2520vehicle%2527s%2520360-degree%250Aperceptual%2520range%2520almost%2520equally%252C%2520which%2520faces%2520two%2520key%2520challenges.%2520Firstly%252C%2520in%250Aareas%2520with%2520uneven%2520traffic%2520distribution%252C%2520focusing%2520on%2520directions%2520with%2520little%250Atraffic%2520offers%2520limited%2520benefits.%2520Secondly%252C%2520under%2520limited%2520communication%2520budgets%252C%250Aallocating%2520excessive%2520bandwidth%2520to%2520less%2520critical%2520directions%2520lowers%2520the%250Aperception%2520accuracy%2520in%2520more%2520vital%2520areas.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ADirect-CP%252C%2520a%2520proactive%2520and%2520direction-aware%2520CP%2520system%2520aiming%2520at%2520improving%2520CP%2520in%250Aspecific%2520directions.%2520Our%2520key%2520idea%2520is%2520to%2520enable%2520an%2520ego%2520vehicle%2520to%2520proactively%250Asignal%2520its%2520interested%2520directions%2520and%2520readjust%2520its%2520attention%2520to%2520enhance%2520local%250Adirectional%2520CP%2520performance.%2520To%2520achieve%2520this%252C%2520we%2520first%2520propose%2520an%2520RSU-aided%250Adirection%2520masking%2520mechanism%2520that%2520assists%2520an%2520ego%2520vehicle%2520in%2520identifying%2520vital%250Adirections.%2520Additionally%252C%2520we%2520design%2520a%2520direction-aware%2520selective%2520attention%250Amodule%2520to%2520wisely%2520aggregate%2520pertinent%2520features%2520based%2520on%2520ego%2520vehicle%2527s%250Adirectional%2520priorities%252C%2520communication%2520budget%252C%2520and%2520the%2520positional%2520data%2520of%2520CAVs.%250AMoreover%252C%2520we%2520introduce%2520a%2520direction-weighted%2520detection%2520loss%2520%2528DWLoss%2529%2520to%2520capture%250Athe%2520divergence%2520between%2520directional%2520CP%2520outcomes%2520and%2520the%2520ground%2520truth%252C%250Afacilitating%2520effective%2520model%2520training.%2520Extensive%2520experiments%2520on%2520the%2520V2X-Sim%25202.0%250Adataset%2520demonstrate%2520that%2520our%2520approach%2520achieves%252019.8%255C%2525%2520higher%2520local%2520perception%250Aaccuracy%2520in%2520interested%2520directions%2520and%25202.5%255C%2525%2520higher%2520overall%2520perception%2520accuracy%250Athan%2520the%2520state-of-the-art%2520methods%2520in%2520collaborative%25203D%2520object%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct-CP%3A%20Directed%20Collaborative%20Perception%20for%20Connected%20and%0A%20%20Autonomous%20Vehicles%20via%20Proactive%20Attention&entry.906535625=Yihang%20Tao%20and%20Senkang%20Hu%20and%20Zhengru%20Fang%20and%20Yuguang%20Fang&entry.1292438233=%20%20Collaborative%20perception%20%28CP%29%20leverages%20visual%20data%20from%20connected%20and%0Aautonomous%20vehicles%20%28CAV%29%20to%20enhance%20an%20ego%20vehicle%27s%20field%20of%20view%20%28FoV%29.%0ADespite%20recent%20progress%2C%20current%20CP%20methods%20expand%20the%20ego%20vehicle%27s%20360-degree%0Aperceptual%20range%20almost%20equally%2C%20which%20faces%20two%20key%20challenges.%20Firstly%2C%20in%0Aareas%20with%20uneven%20traffic%20distribution%2C%20focusing%20on%20directions%20with%20little%0Atraffic%20offers%20limited%20benefits.%20Secondly%2C%20under%20limited%20communication%20budgets%2C%0Aallocating%20excessive%20bandwidth%20to%20less%20critical%20directions%20lowers%20the%0Aperception%20accuracy%20in%20more%20vital%20areas.%20To%20address%20these%20issues%2C%20we%20propose%0ADirect-CP%2C%20a%20proactive%20and%20direction-aware%20CP%20system%20aiming%20at%20improving%20CP%20in%0Aspecific%20directions.%20Our%20key%20idea%20is%20to%20enable%20an%20ego%20vehicle%20to%20proactively%0Asignal%20its%20interested%20directions%20and%20readjust%20its%20attention%20to%20enhance%20local%0Adirectional%20CP%20performance.%20To%20achieve%20this%2C%20we%20first%20propose%20an%20RSU-aided%0Adirection%20masking%20mechanism%20that%20assists%20an%20ego%20vehicle%20in%20identifying%20vital%0Adirections.%20Additionally%2C%20we%20design%20a%20direction-aware%20selective%20attention%0Amodule%20to%20wisely%20aggregate%20pertinent%20features%20based%20on%20ego%20vehicle%27s%0Adirectional%20priorities%2C%20communication%20budget%2C%20and%20the%20positional%20data%20of%20CAVs.%0AMoreover%2C%20we%20introduce%20a%20direction-weighted%20detection%20loss%20%28DWLoss%29%20to%20capture%0Athe%20divergence%20between%20directional%20CP%20outcomes%20and%20the%20ground%20truth%2C%0Afacilitating%20effective%20model%20training.%20Extensive%20experiments%20on%20the%20V2X-Sim%202.0%0Adataset%20demonstrate%20that%20our%20approach%20achieves%2019.8%5C%25%20higher%20local%20perception%0Aaccuracy%20in%20interested%20directions%20and%202.5%5C%25%20higher%20overall%20perception%20accuracy%0Athan%20the%20state-of-the-art%20methods%20in%20collaborative%203D%20object%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08840v1&entry.124074799=Read"},
{"title": "UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with\n  Ultrasound Reflection Direction Parameterization", "author": "Ziwen Guo and Zi Fang and Zhuang Fu", "abstract": "  Three-dimensional ultrasound imaging is a critical technology widely used in\nmedical diagnostics. However, traditional 3D ultrasound imaging methods have\nlimitations such as fixed resolution, low storage efficiency, and insufficient\ncontextual connectivity, leading to poor performance in handling complex\nartifacts and reflection characteristics. Recently, techniques based on NeRF\n(Neural Radiance Fields) have made significant progress in view synthesis and\n3D reconstruction, but there remains a research gap in high-quality ultrasound\nimaging. To address these issues, we propose a new model, UlRe-NeRF, which\ncombines implicit neural networks and explicit ultrasound volume rendering into\nan ultrasound neural rendering architecture. This model incorporates reflection\ndirection parameterization and harmonic encoding, using a directional MLP\nmodule to generate view-dependent high-frequency reflection intensity\nestimates, and a spatial MLP module to produce the medium's physical property\nparameters. These parameters are used in the volume rendering process to\naccurately reproduce the propagation and reflection behavior of ultrasound\nwaves in the medium. Experimental results demonstrate that the UlRe-NeRF model\nsignificantly enhances the realism and accuracy of high-fidelity ultrasound\nimage reconstruction, especially in handling complex medium structures.\n", "link": "http://arxiv.org/abs/2408.00860v3", "date": "2024-09-13", "relevancy": 2.7029, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5416}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UlRe-NeRF%3A%203D%20Ultrasound%20Imaging%20through%20Neural%20Rendering%20with%0A%20%20Ultrasound%20Reflection%20Direction%20Parameterization&body=Title%3A%20UlRe-NeRF%3A%203D%20Ultrasound%20Imaging%20through%20Neural%20Rendering%20with%0A%20%20Ultrasound%20Reflection%20Direction%20Parameterization%0AAuthor%3A%20Ziwen%20Guo%20and%20Zi%20Fang%20and%20Zhuang%20Fu%0AAbstract%3A%20%20%20Three-dimensional%20ultrasound%20imaging%20is%20a%20critical%20technology%20widely%20used%20in%0Amedical%20diagnostics.%20However%2C%20traditional%203D%20ultrasound%20imaging%20methods%20have%0Alimitations%20such%20as%20fixed%20resolution%2C%20low%20storage%20efficiency%2C%20and%20insufficient%0Acontextual%20connectivity%2C%20leading%20to%20poor%20performance%20in%20handling%20complex%0Aartifacts%20and%20reflection%20characteristics.%20Recently%2C%20techniques%20based%20on%20NeRF%0A%28Neural%20Radiance%20Fields%29%20have%20made%20significant%20progress%20in%20view%20synthesis%20and%0A3D%20reconstruction%2C%20but%20there%20remains%20a%20research%20gap%20in%20high-quality%20ultrasound%0Aimaging.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20model%2C%20UlRe-NeRF%2C%20which%0Acombines%20implicit%20neural%20networks%20and%20explicit%20ultrasound%20volume%20rendering%20into%0Aan%20ultrasound%20neural%20rendering%20architecture.%20This%20model%20incorporates%20reflection%0Adirection%20parameterization%20and%20harmonic%20encoding%2C%20using%20a%20directional%20MLP%0Amodule%20to%20generate%20view-dependent%20high-frequency%20reflection%20intensity%0Aestimates%2C%20and%20a%20spatial%20MLP%20module%20to%20produce%20the%20medium%27s%20physical%20property%0Aparameters.%20These%20parameters%20are%20used%20in%20the%20volume%20rendering%20process%20to%0Aaccurately%20reproduce%20the%20propagation%20and%20reflection%20behavior%20of%20ultrasound%0Awaves%20in%20the%20medium.%20Experimental%20results%20demonstrate%20that%20the%20UlRe-NeRF%20model%0Asignificantly%20enhances%20the%20realism%20and%20accuracy%20of%20high-fidelity%20ultrasound%0Aimage%20reconstruction%2C%20especially%20in%20handling%20complex%20medium%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00860v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUlRe-NeRF%253A%25203D%2520Ultrasound%2520Imaging%2520through%2520Neural%2520Rendering%2520with%250A%2520%2520Ultrasound%2520Reflection%2520Direction%2520Parameterization%26entry.906535625%3DZiwen%2520Guo%2520and%2520Zi%2520Fang%2520and%2520Zhuang%2520Fu%26entry.1292438233%3D%2520%2520Three-dimensional%2520ultrasound%2520imaging%2520is%2520a%2520critical%2520technology%2520widely%2520used%2520in%250Amedical%2520diagnostics.%2520However%252C%2520traditional%25203D%2520ultrasound%2520imaging%2520methods%2520have%250Alimitations%2520such%2520as%2520fixed%2520resolution%252C%2520low%2520storage%2520efficiency%252C%2520and%2520insufficient%250Acontextual%2520connectivity%252C%2520leading%2520to%2520poor%2520performance%2520in%2520handling%2520complex%250Aartifacts%2520and%2520reflection%2520characteristics.%2520Recently%252C%2520techniques%2520based%2520on%2520NeRF%250A%2528Neural%2520Radiance%2520Fields%2529%2520have%2520made%2520significant%2520progress%2520in%2520view%2520synthesis%2520and%250A3D%2520reconstruction%252C%2520but%2520there%2520remains%2520a%2520research%2520gap%2520in%2520high-quality%2520ultrasound%250Aimaging.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520new%2520model%252C%2520UlRe-NeRF%252C%2520which%250Acombines%2520implicit%2520neural%2520networks%2520and%2520explicit%2520ultrasound%2520volume%2520rendering%2520into%250Aan%2520ultrasound%2520neural%2520rendering%2520architecture.%2520This%2520model%2520incorporates%2520reflection%250Adirection%2520parameterization%2520and%2520harmonic%2520encoding%252C%2520using%2520a%2520directional%2520MLP%250Amodule%2520to%2520generate%2520view-dependent%2520high-frequency%2520reflection%2520intensity%250Aestimates%252C%2520and%2520a%2520spatial%2520MLP%2520module%2520to%2520produce%2520the%2520medium%2527s%2520physical%2520property%250Aparameters.%2520These%2520parameters%2520are%2520used%2520in%2520the%2520volume%2520rendering%2520process%2520to%250Aaccurately%2520reproduce%2520the%2520propagation%2520and%2520reflection%2520behavior%2520of%2520ultrasound%250Awaves%2520in%2520the%2520medium.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520UlRe-NeRF%2520model%250Asignificantly%2520enhances%2520the%2520realism%2520and%2520accuracy%2520of%2520high-fidelity%2520ultrasound%250Aimage%2520reconstruction%252C%2520especially%2520in%2520handling%2520complex%2520medium%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00860v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UlRe-NeRF%3A%203D%20Ultrasound%20Imaging%20through%20Neural%20Rendering%20with%0A%20%20Ultrasound%20Reflection%20Direction%20Parameterization&entry.906535625=Ziwen%20Guo%20and%20Zi%20Fang%20and%20Zhuang%20Fu&entry.1292438233=%20%20Three-dimensional%20ultrasound%20imaging%20is%20a%20critical%20technology%20widely%20used%20in%0Amedical%20diagnostics.%20However%2C%20traditional%203D%20ultrasound%20imaging%20methods%20have%0Alimitations%20such%20as%20fixed%20resolution%2C%20low%20storage%20efficiency%2C%20and%20insufficient%0Acontextual%20connectivity%2C%20leading%20to%20poor%20performance%20in%20handling%20complex%0Aartifacts%20and%20reflection%20characteristics.%20Recently%2C%20techniques%20based%20on%20NeRF%0A%28Neural%20Radiance%20Fields%29%20have%20made%20significant%20progress%20in%20view%20synthesis%20and%0A3D%20reconstruction%2C%20but%20there%20remains%20a%20research%20gap%20in%20high-quality%20ultrasound%0Aimaging.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20model%2C%20UlRe-NeRF%2C%20which%0Acombines%20implicit%20neural%20networks%20and%20explicit%20ultrasound%20volume%20rendering%20into%0Aan%20ultrasound%20neural%20rendering%20architecture.%20This%20model%20incorporates%20reflection%0Adirection%20parameterization%20and%20harmonic%20encoding%2C%20using%20a%20directional%20MLP%0Amodule%20to%20generate%20view-dependent%20high-frequency%20reflection%20intensity%0Aestimates%2C%20and%20a%20spatial%20MLP%20module%20to%20produce%20the%20medium%27s%20physical%20property%0Aparameters.%20These%20parameters%20are%20used%20in%20the%20volume%20rendering%20process%20to%0Aaccurately%20reproduce%20the%20propagation%20and%20reflection%20behavior%20of%20ultrasound%0Awaves%20in%20the%20medium.%20Experimental%20results%20demonstrate%20that%20the%20UlRe-NeRF%20model%0Asignificantly%20enhances%20the%20realism%20and%20accuracy%20of%20high-fidelity%20ultrasound%0Aimage%20reconstruction%2C%20especially%20in%20handling%20complex%20medium%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00860v3&entry.124074799=Read"},
{"title": "Task-Specific Data Preparation for Deep Learning to Reconstruct\n  Structures of Interest from Severely Truncated CBCT Data", "author": "Yixing Huang and Fuxin Fan and Ahmed Gomaa and Andreas Maier and Rainer Fietkau and Christoph Bert and Florian Putz", "abstract": "  Cone-beam computed tomography (CBCT) is widely used in interventional\nsurgeries and radiation oncology. Due to the limited size of flat-panel\ndetectors, anatomical structures might be missing outside the limited\nfield-of-view (FOV), which restricts the clinical applications of CBCT systems.\nRecently, deep learning methods have been proposed to extend the FOV for\nmulti-slice CT systems. However, in mobile CBCT system with a smaller FOV size,\nprojection data is severely truncated and it is challenging for a network to\nrestore all missing structures outside the FOV. In some applications, only\ncertain structures outside the FOV are of interest, e.g., ribs in needle path\nplanning for liver/lung cancer diagnosis. Therefore, a task-specific data\npreparation method is proposed in this work, which automatically let the\nnetwork focus on structures of interest instead of all the structures. Our\npreliminary experiment shows that Pix2pixGAN with a conventional training has\nthe risk to reconstruct false positive and false negative rib structures from\nseverely truncated CBCT data, whereas Pix2pixGAN with the proposed\ntask-specific training can reconstruct all the ribs reliably. The proposed\nmethod is promising to empower CBCT with more clinical applications.\n", "link": "http://arxiv.org/abs/2409.08800v1", "date": "2024-09-13", "relevancy": 2.7, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5432}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5384}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Specific%20Data%20Preparation%20for%20Deep%20Learning%20to%20Reconstruct%0A%20%20Structures%20of%20Interest%20from%20Severely%20Truncated%20CBCT%20Data&body=Title%3A%20Task-Specific%20Data%20Preparation%20for%20Deep%20Learning%20to%20Reconstruct%0A%20%20Structures%20of%20Interest%20from%20Severely%20Truncated%20CBCT%20Data%0AAuthor%3A%20Yixing%20Huang%20and%20Fuxin%20Fan%20and%20Ahmed%20Gomaa%20and%20Andreas%20Maier%20and%20Rainer%20Fietkau%20and%20Christoph%20Bert%20and%20Florian%20Putz%0AAbstract%3A%20%20%20Cone-beam%20computed%20tomography%20%28CBCT%29%20is%20widely%20used%20in%20interventional%0Asurgeries%20and%20radiation%20oncology.%20Due%20to%20the%20limited%20size%20of%20flat-panel%0Adetectors%2C%20anatomical%20structures%20might%20be%20missing%20outside%20the%20limited%0Afield-of-view%20%28FOV%29%2C%20which%20restricts%20the%20clinical%20applications%20of%20CBCT%20systems.%0ARecently%2C%20deep%20learning%20methods%20have%20been%20proposed%20to%20extend%20the%20FOV%20for%0Amulti-slice%20CT%20systems.%20However%2C%20in%20mobile%20CBCT%20system%20with%20a%20smaller%20FOV%20size%2C%0Aprojection%20data%20is%20severely%20truncated%20and%20it%20is%20challenging%20for%20a%20network%20to%0Arestore%20all%20missing%20structures%20outside%20the%20FOV.%20In%20some%20applications%2C%20only%0Acertain%20structures%20outside%20the%20FOV%20are%20of%20interest%2C%20e.g.%2C%20ribs%20in%20needle%20path%0Aplanning%20for%20liver/lung%20cancer%20diagnosis.%20Therefore%2C%20a%20task-specific%20data%0Apreparation%20method%20is%20proposed%20in%20this%20work%2C%20which%20automatically%20let%20the%0Anetwork%20focus%20on%20structures%20of%20interest%20instead%20of%20all%20the%20structures.%20Our%0Apreliminary%20experiment%20shows%20that%20Pix2pixGAN%20with%20a%20conventional%20training%20has%0Athe%20risk%20to%20reconstruct%20false%20positive%20and%20false%20negative%20rib%20structures%20from%0Aseverely%20truncated%20CBCT%20data%2C%20whereas%20Pix2pixGAN%20with%20the%20proposed%0Atask-specific%20training%20can%20reconstruct%20all%20the%20ribs%20reliably.%20The%20proposed%0Amethod%20is%20promising%20to%20empower%20CBCT%20with%20more%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Specific%2520Data%2520Preparation%2520for%2520Deep%2520Learning%2520to%2520Reconstruct%250A%2520%2520Structures%2520of%2520Interest%2520from%2520Severely%2520Truncated%2520CBCT%2520Data%26entry.906535625%3DYixing%2520Huang%2520and%2520Fuxin%2520Fan%2520and%2520Ahmed%2520Gomaa%2520and%2520Andreas%2520Maier%2520and%2520Rainer%2520Fietkau%2520and%2520Christoph%2520Bert%2520and%2520Florian%2520Putz%26entry.1292438233%3D%2520%2520Cone-beam%2520computed%2520tomography%2520%2528CBCT%2529%2520is%2520widely%2520used%2520in%2520interventional%250Asurgeries%2520and%2520radiation%2520oncology.%2520Due%2520to%2520the%2520limited%2520size%2520of%2520flat-panel%250Adetectors%252C%2520anatomical%2520structures%2520might%2520be%2520missing%2520outside%2520the%2520limited%250Afield-of-view%2520%2528FOV%2529%252C%2520which%2520restricts%2520the%2520clinical%2520applications%2520of%2520CBCT%2520systems.%250ARecently%252C%2520deep%2520learning%2520methods%2520have%2520been%2520proposed%2520to%2520extend%2520the%2520FOV%2520for%250Amulti-slice%2520CT%2520systems.%2520However%252C%2520in%2520mobile%2520CBCT%2520system%2520with%2520a%2520smaller%2520FOV%2520size%252C%250Aprojection%2520data%2520is%2520severely%2520truncated%2520and%2520it%2520is%2520challenging%2520for%2520a%2520network%2520to%250Arestore%2520all%2520missing%2520structures%2520outside%2520the%2520FOV.%2520In%2520some%2520applications%252C%2520only%250Acertain%2520structures%2520outside%2520the%2520FOV%2520are%2520of%2520interest%252C%2520e.g.%252C%2520ribs%2520in%2520needle%2520path%250Aplanning%2520for%2520liver/lung%2520cancer%2520diagnosis.%2520Therefore%252C%2520a%2520task-specific%2520data%250Apreparation%2520method%2520is%2520proposed%2520in%2520this%2520work%252C%2520which%2520automatically%2520let%2520the%250Anetwork%2520focus%2520on%2520structures%2520of%2520interest%2520instead%2520of%2520all%2520the%2520structures.%2520Our%250Apreliminary%2520experiment%2520shows%2520that%2520Pix2pixGAN%2520with%2520a%2520conventional%2520training%2520has%250Athe%2520risk%2520to%2520reconstruct%2520false%2520positive%2520and%2520false%2520negative%2520rib%2520structures%2520from%250Aseverely%2520truncated%2520CBCT%2520data%252C%2520whereas%2520Pix2pixGAN%2520with%2520the%2520proposed%250Atask-specific%2520training%2520can%2520reconstruct%2520all%2520the%2520ribs%2520reliably.%2520The%2520proposed%250Amethod%2520is%2520promising%2520to%2520empower%2520CBCT%2520with%2520more%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Specific%20Data%20Preparation%20for%20Deep%20Learning%20to%20Reconstruct%0A%20%20Structures%20of%20Interest%20from%20Severely%20Truncated%20CBCT%20Data&entry.906535625=Yixing%20Huang%20and%20Fuxin%20Fan%20and%20Ahmed%20Gomaa%20and%20Andreas%20Maier%20and%20Rainer%20Fietkau%20and%20Christoph%20Bert%20and%20Florian%20Putz&entry.1292438233=%20%20Cone-beam%20computed%20tomography%20%28CBCT%29%20is%20widely%20used%20in%20interventional%0Asurgeries%20and%20radiation%20oncology.%20Due%20to%20the%20limited%20size%20of%20flat-panel%0Adetectors%2C%20anatomical%20structures%20might%20be%20missing%20outside%20the%20limited%0Afield-of-view%20%28FOV%29%2C%20which%20restricts%20the%20clinical%20applications%20of%20CBCT%20systems.%0ARecently%2C%20deep%20learning%20methods%20have%20been%20proposed%20to%20extend%20the%20FOV%20for%0Amulti-slice%20CT%20systems.%20However%2C%20in%20mobile%20CBCT%20system%20with%20a%20smaller%20FOV%20size%2C%0Aprojection%20data%20is%20severely%20truncated%20and%20it%20is%20challenging%20for%20a%20network%20to%0Arestore%20all%20missing%20structures%20outside%20the%20FOV.%20In%20some%20applications%2C%20only%0Acertain%20structures%20outside%20the%20FOV%20are%20of%20interest%2C%20e.g.%2C%20ribs%20in%20needle%20path%0Aplanning%20for%20liver/lung%20cancer%20diagnosis.%20Therefore%2C%20a%20task-specific%20data%0Apreparation%20method%20is%20proposed%20in%20this%20work%2C%20which%20automatically%20let%20the%0Anetwork%20focus%20on%20structures%20of%20interest%20instead%20of%20all%20the%20structures.%20Our%0Apreliminary%20experiment%20shows%20that%20Pix2pixGAN%20with%20a%20conventional%20training%20has%0Athe%20risk%20to%20reconstruct%20false%20positive%20and%20false%20negative%20rib%20structures%20from%0Aseverely%20truncated%20CBCT%20data%2C%20whereas%20Pix2pixGAN%20with%20the%20proposed%0Atask-specific%20training%20can%20reconstruct%20all%20the%20ribs%20reliably.%20The%20proposed%0Amethod%20is%20promising%20to%20empower%20CBCT%20with%20more%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08800v1&entry.124074799=Read"},
{"title": "Distilling Monolingual and Crosslingual Word-in-Context Representations", "author": "Yuki Arase and Tomoyuki Kajiwara", "abstract": "  In this study, we propose a method that distils representations of word\nmeaning in context from a pre-trained masked language model in both monolingual\nand crosslingual settings. Word representations are the basis for context-aware\nlexical semantics and unsupervised semantic textual similarity (STS)\nestimation. Different from existing approaches, our method does not require\nhuman-annotated corpora nor updates of the parameters of the pre-trained model.\nThe latter feature is appealing for practical scenarios where the off-the-shelf\npre-trained model is a common asset among different applications. Specifically,\nour method learns to combine the outputs of different hidden layers of the\npre-trained model using self-attention. Our auto-encoder based training only\nrequires an automatically generated corpus. To evaluate the performance of the\nproposed approach, we performed extensive experiments using various benchmark\ntasks. The results on the monolingual tasks confirmed that our representations\nexhibited a competitive performance compared to that of the previous study for\nthe context-aware lexical semantic tasks and outperformed it for STS\nestimation. The results of the crosslingual tasks revealed that the proposed\nmethod largely improved crosslingual word representations of multilingual\npre-trained models.\n", "link": "http://arxiv.org/abs/2409.08719v1", "date": "2024-09-13", "relevancy": 2.6577, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Monolingual%20and%20Crosslingual%20Word-in-Context%20Representations&body=Title%3A%20Distilling%20Monolingual%20and%20Crosslingual%20Word-in-Context%20Representations%0AAuthor%3A%20Yuki%20Arase%20and%20Tomoyuki%20Kajiwara%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20propose%20a%20method%20that%20distils%20representations%20of%20word%0Ameaning%20in%20context%20from%20a%20pre-trained%20masked%20language%20model%20in%20both%20monolingual%0Aand%20crosslingual%20settings.%20Word%20representations%20are%20the%20basis%20for%20context-aware%0Alexical%20semantics%20and%20unsupervised%20semantic%20textual%20similarity%20%28STS%29%0Aestimation.%20Different%20from%20existing%20approaches%2C%20our%20method%20does%20not%20require%0Ahuman-annotated%20corpora%20nor%20updates%20of%20the%20parameters%20of%20the%20pre-trained%20model.%0AThe%20latter%20feature%20is%20appealing%20for%20practical%20scenarios%20where%20the%20off-the-shelf%0Apre-trained%20model%20is%20a%20common%20asset%20among%20different%20applications.%20Specifically%2C%0Aour%20method%20learns%20to%20combine%20the%20outputs%20of%20different%20hidden%20layers%20of%20the%0Apre-trained%20model%20using%20self-attention.%20Our%20auto-encoder%20based%20training%20only%0Arequires%20an%20automatically%20generated%20corpus.%20To%20evaluate%20the%20performance%20of%20the%0Aproposed%20approach%2C%20we%20performed%20extensive%20experiments%20using%20various%20benchmark%0Atasks.%20The%20results%20on%20the%20monolingual%20tasks%20confirmed%20that%20our%20representations%0Aexhibited%20a%20competitive%20performance%20compared%20to%20that%20of%20the%20previous%20study%20for%0Athe%20context-aware%20lexical%20semantic%20tasks%20and%20outperformed%20it%20for%20STS%0Aestimation.%20The%20results%20of%20the%20crosslingual%20tasks%20revealed%20that%20the%20proposed%0Amethod%20largely%20improved%20crosslingual%20word%20representations%20of%20multilingual%0Apre-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Monolingual%2520and%2520Crosslingual%2520Word-in-Context%2520Representations%26entry.906535625%3DYuki%2520Arase%2520and%2520Tomoyuki%2520Kajiwara%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520method%2520that%2520distils%2520representations%2520of%2520word%250Ameaning%2520in%2520context%2520from%2520a%2520pre-trained%2520masked%2520language%2520model%2520in%2520both%2520monolingual%250Aand%2520crosslingual%2520settings.%2520Word%2520representations%2520are%2520the%2520basis%2520for%2520context-aware%250Alexical%2520semantics%2520and%2520unsupervised%2520semantic%2520textual%2520similarity%2520%2528STS%2529%250Aestimation.%2520Different%2520from%2520existing%2520approaches%252C%2520our%2520method%2520does%2520not%2520require%250Ahuman-annotated%2520corpora%2520nor%2520updates%2520of%2520the%2520parameters%2520of%2520the%2520pre-trained%2520model.%250AThe%2520latter%2520feature%2520is%2520appealing%2520for%2520practical%2520scenarios%2520where%2520the%2520off-the-shelf%250Apre-trained%2520model%2520is%2520a%2520common%2520asset%2520among%2520different%2520applications.%2520Specifically%252C%250Aour%2520method%2520learns%2520to%2520combine%2520the%2520outputs%2520of%2520different%2520hidden%2520layers%2520of%2520the%250Apre-trained%2520model%2520using%2520self-attention.%2520Our%2520auto-encoder%2520based%2520training%2520only%250Arequires%2520an%2520automatically%2520generated%2520corpus.%2520To%2520evaluate%2520the%2520performance%2520of%2520the%250Aproposed%2520approach%252C%2520we%2520performed%2520extensive%2520experiments%2520using%2520various%2520benchmark%250Atasks.%2520The%2520results%2520on%2520the%2520monolingual%2520tasks%2520confirmed%2520that%2520our%2520representations%250Aexhibited%2520a%2520competitive%2520performance%2520compared%2520to%2520that%2520of%2520the%2520previous%2520study%2520for%250Athe%2520context-aware%2520lexical%2520semantic%2520tasks%2520and%2520outperformed%2520it%2520for%2520STS%250Aestimation.%2520The%2520results%2520of%2520the%2520crosslingual%2520tasks%2520revealed%2520that%2520the%2520proposed%250Amethod%2520largely%2520improved%2520crosslingual%2520word%2520representations%2520of%2520multilingual%250Apre-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Monolingual%20and%20Crosslingual%20Word-in-Context%20Representations&entry.906535625=Yuki%20Arase%20and%20Tomoyuki%20Kajiwara&entry.1292438233=%20%20In%20this%20study%2C%20we%20propose%20a%20method%20that%20distils%20representations%20of%20word%0Ameaning%20in%20context%20from%20a%20pre-trained%20masked%20language%20model%20in%20both%20monolingual%0Aand%20crosslingual%20settings.%20Word%20representations%20are%20the%20basis%20for%20context-aware%0Alexical%20semantics%20and%20unsupervised%20semantic%20textual%20similarity%20%28STS%29%0Aestimation.%20Different%20from%20existing%20approaches%2C%20our%20method%20does%20not%20require%0Ahuman-annotated%20corpora%20nor%20updates%20of%20the%20parameters%20of%20the%20pre-trained%20model.%0AThe%20latter%20feature%20is%20appealing%20for%20practical%20scenarios%20where%20the%20off-the-shelf%0Apre-trained%20model%20is%20a%20common%20asset%20among%20different%20applications.%20Specifically%2C%0Aour%20method%20learns%20to%20combine%20the%20outputs%20of%20different%20hidden%20layers%20of%20the%0Apre-trained%20model%20using%20self-attention.%20Our%20auto-encoder%20based%20training%20only%0Arequires%20an%20automatically%20generated%20corpus.%20To%20evaluate%20the%20performance%20of%20the%0Aproposed%20approach%2C%20we%20performed%20extensive%20experiments%20using%20various%20benchmark%0Atasks.%20The%20results%20on%20the%20monolingual%20tasks%20confirmed%20that%20our%20representations%0Aexhibited%20a%20competitive%20performance%20compared%20to%20that%20of%20the%20previous%20study%20for%0Athe%20context-aware%20lexical%20semantic%20tasks%20and%20outperformed%20it%20for%20STS%0Aestimation.%20The%20results%20of%20the%20crosslingual%20tasks%20revealed%20that%20the%20proposed%0Amethod%20largely%20improved%20crosslingual%20word%20representations%20of%20multilingual%0Apre-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08719v1&entry.124074799=Read"},
{"title": "DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar\n  2D X-ray(s)", "author": "Yun Su Jeong and Hye Bin Yoo and Il Yong Chun", "abstract": "  Computational tomography (CT) provides high-resolution medical imaging, but\nit can expose patients to high radiation. X-ray scanners have low radiation\nexposure, but their resolutions are low. This paper proposes a new conditional\ndiffusion model, DX2CT, that reconstructs three-dimensional (3D) CT volumes\nfrom bi or mono-planar X-ray image(s). Proposed DX2CT consists of two key\ncomponents: 1) modulating feature maps extracted from two-dimensional (2D)\nX-ray(s) with 3D positions of CT volume using a new transformer and 2)\neffectively using the modulated 3D position-aware feature maps as conditions of\nDX2CT. In particular, the proposed transformer can provide conditions with rich\ninformation of a target CT slice to the conditional diffusion model, enabling\nhigh-quality CT reconstruction. Our experiments with the bi or mono-planar\nX-ray(s) benchmark datasets show that proposed DX2CT outperforms several\nstate-of-the-art methods. Our codes and model will be available at:\nhttps://www.github.com/intyeger/DX2CT.\n", "link": "http://arxiv.org/abs/2409.08850v1", "date": "2024-09-13", "relevancy": 2.6146, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6622}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6622}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DX2CT%3A%20Diffusion%20Model%20for%203D%20CT%20Reconstruction%20from%20Bi%20or%20Mono-planar%0A%20%202D%20X-ray%28s%29&body=Title%3A%20DX2CT%3A%20Diffusion%20Model%20for%203D%20CT%20Reconstruction%20from%20Bi%20or%20Mono-planar%0A%20%202D%20X-ray%28s%29%0AAuthor%3A%20Yun%20Su%20Jeong%20and%20Hye%20Bin%20Yoo%20and%20Il%20Yong%20Chun%0AAbstract%3A%20%20%20Computational%20tomography%20%28CT%29%20provides%20high-resolution%20medical%20imaging%2C%20but%0Ait%20can%20expose%20patients%20to%20high%20radiation.%20X-ray%20scanners%20have%20low%20radiation%0Aexposure%2C%20but%20their%20resolutions%20are%20low.%20This%20paper%20proposes%20a%20new%20conditional%0Adiffusion%20model%2C%20DX2CT%2C%20that%20reconstructs%20three-dimensional%20%283D%29%20CT%20volumes%0Afrom%20bi%20or%20mono-planar%20X-ray%20image%28s%29.%20Proposed%20DX2CT%20consists%20of%20two%20key%0Acomponents%3A%201%29%20modulating%20feature%20maps%20extracted%20from%20two-dimensional%20%282D%29%0AX-ray%28s%29%20with%203D%20positions%20of%20CT%20volume%20using%20a%20new%20transformer%20and%202%29%0Aeffectively%20using%20the%20modulated%203D%20position-aware%20feature%20maps%20as%20conditions%20of%0ADX2CT.%20In%20particular%2C%20the%20proposed%20transformer%20can%20provide%20conditions%20with%20rich%0Ainformation%20of%20a%20target%20CT%20slice%20to%20the%20conditional%20diffusion%20model%2C%20enabling%0Ahigh-quality%20CT%20reconstruction.%20Our%20experiments%20with%20the%20bi%20or%20mono-planar%0AX-ray%28s%29%20benchmark%20datasets%20show%20that%20proposed%20DX2CT%20outperforms%20several%0Astate-of-the-art%20methods.%20Our%20codes%20and%20model%20will%20be%20available%20at%3A%0Ahttps%3A//www.github.com/intyeger/DX2CT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDX2CT%253A%2520Diffusion%2520Model%2520for%25203D%2520CT%2520Reconstruction%2520from%2520Bi%2520or%2520Mono-planar%250A%2520%25202D%2520X-ray%2528s%2529%26entry.906535625%3DYun%2520Su%2520Jeong%2520and%2520Hye%2520Bin%2520Yoo%2520and%2520Il%2520Yong%2520Chun%26entry.1292438233%3D%2520%2520Computational%2520tomography%2520%2528CT%2529%2520provides%2520high-resolution%2520medical%2520imaging%252C%2520but%250Ait%2520can%2520expose%2520patients%2520to%2520high%2520radiation.%2520X-ray%2520scanners%2520have%2520low%2520radiation%250Aexposure%252C%2520but%2520their%2520resolutions%2520are%2520low.%2520This%2520paper%2520proposes%2520a%2520new%2520conditional%250Adiffusion%2520model%252C%2520DX2CT%252C%2520that%2520reconstructs%2520three-dimensional%2520%25283D%2529%2520CT%2520volumes%250Afrom%2520bi%2520or%2520mono-planar%2520X-ray%2520image%2528s%2529.%2520Proposed%2520DX2CT%2520consists%2520of%2520two%2520key%250Acomponents%253A%25201%2529%2520modulating%2520feature%2520maps%2520extracted%2520from%2520two-dimensional%2520%25282D%2529%250AX-ray%2528s%2529%2520with%25203D%2520positions%2520of%2520CT%2520volume%2520using%2520a%2520new%2520transformer%2520and%25202%2529%250Aeffectively%2520using%2520the%2520modulated%25203D%2520position-aware%2520feature%2520maps%2520as%2520conditions%2520of%250ADX2CT.%2520In%2520particular%252C%2520the%2520proposed%2520transformer%2520can%2520provide%2520conditions%2520with%2520rich%250Ainformation%2520of%2520a%2520target%2520CT%2520slice%2520to%2520the%2520conditional%2520diffusion%2520model%252C%2520enabling%250Ahigh-quality%2520CT%2520reconstruction.%2520Our%2520experiments%2520with%2520the%2520bi%2520or%2520mono-planar%250AX-ray%2528s%2529%2520benchmark%2520datasets%2520show%2520that%2520proposed%2520DX2CT%2520outperforms%2520several%250Astate-of-the-art%2520methods.%2520Our%2520codes%2520and%2520model%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//www.github.com/intyeger/DX2CT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DX2CT%3A%20Diffusion%20Model%20for%203D%20CT%20Reconstruction%20from%20Bi%20or%20Mono-planar%0A%20%202D%20X-ray%28s%29&entry.906535625=Yun%20Su%20Jeong%20and%20Hye%20Bin%20Yoo%20and%20Il%20Yong%20Chun&entry.1292438233=%20%20Computational%20tomography%20%28CT%29%20provides%20high-resolution%20medical%20imaging%2C%20but%0Ait%20can%20expose%20patients%20to%20high%20radiation.%20X-ray%20scanners%20have%20low%20radiation%0Aexposure%2C%20but%20their%20resolutions%20are%20low.%20This%20paper%20proposes%20a%20new%20conditional%0Adiffusion%20model%2C%20DX2CT%2C%20that%20reconstructs%20three-dimensional%20%283D%29%20CT%20volumes%0Afrom%20bi%20or%20mono-planar%20X-ray%20image%28s%29.%20Proposed%20DX2CT%20consists%20of%20two%20key%0Acomponents%3A%201%29%20modulating%20feature%20maps%20extracted%20from%20two-dimensional%20%282D%29%0AX-ray%28s%29%20with%203D%20positions%20of%20CT%20volume%20using%20a%20new%20transformer%20and%202%29%0Aeffectively%20using%20the%20modulated%203D%20position-aware%20feature%20maps%20as%20conditions%20of%0ADX2CT.%20In%20particular%2C%20the%20proposed%20transformer%20can%20provide%20conditions%20with%20rich%0Ainformation%20of%20a%20target%20CT%20slice%20to%20the%20conditional%20diffusion%20model%2C%20enabling%0Ahigh-quality%20CT%20reconstruction.%20Our%20experiments%20with%20the%20bi%20or%20mono-planar%0AX-ray%28s%29%20benchmark%20datasets%20show%20that%20proposed%20DX2CT%20outperforms%20several%0Astate-of-the-art%20methods.%20Our%20codes%20and%20model%20will%20be%20available%20at%3A%0Ahttps%3A//www.github.com/intyeger/DX2CT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08850v1&entry.124074799=Read"},
{"title": "Exploring the Effectiveness of Object-Centric Representations in Visual\n  Question Answering: Comparative Insights with Foundation Models", "author": "Amir Mohammad Karimi Mamaghan and Samuele Papa and Karl Henrik Johansson and Stefan Bauer and Andrea Dittadi", "abstract": "  Object-centric (OC) representations, which represent the state of a visual\nscene by modeling it as a composition of objects, have the potential to be used\nin various downstream tasks to achieve systematic compositional generalization\nand facilitate reasoning. However, these claims have not been thoroughly\nanalyzed yet. Recently, foundation models have demonstrated unparalleled\ncapabilities across diverse domains from language to computer vision, marking\nthem as a potential cornerstone of future research for a multitude of\ncomputational tasks. In this paper, we conduct an extensive empirical study on\nrepresentation learning for downstream Visual Question Answering (VQA), which\nrequires an accurate compositional understanding of the scene. We thoroughly\ninvestigate the benefits and trade-offs of OC models and alternative approaches\nincluding large pre-trained foundation models on both synthetic and real-world\ndata, and demonstrate a viable way to achieve the best of both worlds. The\nextensiveness of our study, encompassing over 800 downstream VQA models and 15\ndifferent types of upstream representations, also provides several additional\ninsights that we believe will be of interest to the community at large.\n", "link": "http://arxiv.org/abs/2407.15589v2", "date": "2024-09-13", "relevancy": 2.6132, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6714}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Effectiveness%20of%20Object-Centric%20Representations%20in%20Visual%0A%20%20Question%20Answering%3A%20Comparative%20Insights%20with%20Foundation%20Models&body=Title%3A%20Exploring%20the%20Effectiveness%20of%20Object-Centric%20Representations%20in%20Visual%0A%20%20Question%20Answering%3A%20Comparative%20Insights%20with%20Foundation%20Models%0AAuthor%3A%20Amir%20Mohammad%20Karimi%20Mamaghan%20and%20Samuele%20Papa%20and%20Karl%20Henrik%20Johansson%20and%20Stefan%20Bauer%20and%20Andrea%20Dittadi%0AAbstract%3A%20%20%20Object-centric%20%28OC%29%20representations%2C%20which%20represent%20the%20state%20of%20a%20visual%0Ascene%20by%20modeling%20it%20as%20a%20composition%20of%20objects%2C%20have%20the%20potential%20to%20be%20used%0Ain%20various%20downstream%20tasks%20to%20achieve%20systematic%20compositional%20generalization%0Aand%20facilitate%20reasoning.%20However%2C%20these%20claims%20have%20not%20been%20thoroughly%0Aanalyzed%20yet.%20Recently%2C%20foundation%20models%20have%20demonstrated%20unparalleled%0Acapabilities%20across%20diverse%20domains%20from%20language%20to%20computer%20vision%2C%20marking%0Athem%20as%20a%20potential%20cornerstone%20of%20future%20research%20for%20a%20multitude%20of%0Acomputational%20tasks.%20In%20this%20paper%2C%20we%20conduct%20an%20extensive%20empirical%20study%20on%0Arepresentation%20learning%20for%20downstream%20Visual%20Question%20Answering%20%28VQA%29%2C%20which%0Arequires%20an%20accurate%20compositional%20understanding%20of%20the%20scene.%20We%20thoroughly%0Ainvestigate%20the%20benefits%20and%20trade-offs%20of%20OC%20models%20and%20alternative%20approaches%0Aincluding%20large%20pre-trained%20foundation%20models%20on%20both%20synthetic%20and%20real-world%0Adata%2C%20and%20demonstrate%20a%20viable%20way%20to%20achieve%20the%20best%20of%20both%20worlds.%20The%0Aextensiveness%20of%20our%20study%2C%20encompassing%20over%20800%20downstream%20VQA%20models%20and%2015%0Adifferent%20types%20of%20upstream%20representations%2C%20also%20provides%20several%20additional%0Ainsights%20that%20we%20believe%20will%20be%20of%20interest%20to%20the%20community%20at%20large.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15589v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Effectiveness%2520of%2520Object-Centric%2520Representations%2520in%2520Visual%250A%2520%2520Question%2520Answering%253A%2520Comparative%2520Insights%2520with%2520Foundation%2520Models%26entry.906535625%3DAmir%2520Mohammad%2520Karimi%2520Mamaghan%2520and%2520Samuele%2520Papa%2520and%2520Karl%2520Henrik%2520Johansson%2520and%2520Stefan%2520Bauer%2520and%2520Andrea%2520Dittadi%26entry.1292438233%3D%2520%2520Object-centric%2520%2528OC%2529%2520representations%252C%2520which%2520represent%2520the%2520state%2520of%2520a%2520visual%250Ascene%2520by%2520modeling%2520it%2520as%2520a%2520composition%2520of%2520objects%252C%2520have%2520the%2520potential%2520to%2520be%2520used%250Ain%2520various%2520downstream%2520tasks%2520to%2520achieve%2520systematic%2520compositional%2520generalization%250Aand%2520facilitate%2520reasoning.%2520However%252C%2520these%2520claims%2520have%2520not%2520been%2520thoroughly%250Aanalyzed%2520yet.%2520Recently%252C%2520foundation%2520models%2520have%2520demonstrated%2520unparalleled%250Acapabilities%2520across%2520diverse%2520domains%2520from%2520language%2520to%2520computer%2520vision%252C%2520marking%250Athem%2520as%2520a%2520potential%2520cornerstone%2520of%2520future%2520research%2520for%2520a%2520multitude%2520of%250Acomputational%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520an%2520extensive%2520empirical%2520study%2520on%250Arepresentation%2520learning%2520for%2520downstream%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520which%250Arequires%2520an%2520accurate%2520compositional%2520understanding%2520of%2520the%2520scene.%2520We%2520thoroughly%250Ainvestigate%2520the%2520benefits%2520and%2520trade-offs%2520of%2520OC%2520models%2520and%2520alternative%2520approaches%250Aincluding%2520large%2520pre-trained%2520foundation%2520models%2520on%2520both%2520synthetic%2520and%2520real-world%250Adata%252C%2520and%2520demonstrate%2520a%2520viable%2520way%2520to%2520achieve%2520the%2520best%2520of%2520both%2520worlds.%2520The%250Aextensiveness%2520of%2520our%2520study%252C%2520encompassing%2520over%2520800%2520downstream%2520VQA%2520models%2520and%252015%250Adifferent%2520types%2520of%2520upstream%2520representations%252C%2520also%2520provides%2520several%2520additional%250Ainsights%2520that%2520we%2520believe%2520will%2520be%2520of%2520interest%2520to%2520the%2520community%2520at%2520large.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15589v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Effectiveness%20of%20Object-Centric%20Representations%20in%20Visual%0A%20%20Question%20Answering%3A%20Comparative%20Insights%20with%20Foundation%20Models&entry.906535625=Amir%20Mohammad%20Karimi%20Mamaghan%20and%20Samuele%20Papa%20and%20Karl%20Henrik%20Johansson%20and%20Stefan%20Bauer%20and%20Andrea%20Dittadi&entry.1292438233=%20%20Object-centric%20%28OC%29%20representations%2C%20which%20represent%20the%20state%20of%20a%20visual%0Ascene%20by%20modeling%20it%20as%20a%20composition%20of%20objects%2C%20have%20the%20potential%20to%20be%20used%0Ain%20various%20downstream%20tasks%20to%20achieve%20systematic%20compositional%20generalization%0Aand%20facilitate%20reasoning.%20However%2C%20these%20claims%20have%20not%20been%20thoroughly%0Aanalyzed%20yet.%20Recently%2C%20foundation%20models%20have%20demonstrated%20unparalleled%0Acapabilities%20across%20diverse%20domains%20from%20language%20to%20computer%20vision%2C%20marking%0Athem%20as%20a%20potential%20cornerstone%20of%20future%20research%20for%20a%20multitude%20of%0Acomputational%20tasks.%20In%20this%20paper%2C%20we%20conduct%20an%20extensive%20empirical%20study%20on%0Arepresentation%20learning%20for%20downstream%20Visual%20Question%20Answering%20%28VQA%29%2C%20which%0Arequires%20an%20accurate%20compositional%20understanding%20of%20the%20scene.%20We%20thoroughly%0Ainvestigate%20the%20benefits%20and%20trade-offs%20of%20OC%20models%20and%20alternative%20approaches%0Aincluding%20large%20pre-trained%20foundation%20models%20on%20both%20synthetic%20and%20real-world%0Adata%2C%20and%20demonstrate%20a%20viable%20way%20to%20achieve%20the%20best%20of%20both%20worlds.%20The%0Aextensiveness%20of%20our%20study%2C%20encompassing%20over%20800%20downstream%20VQA%20models%20and%2015%0Adifferent%20types%20of%20upstream%20representations%2C%20also%20provides%20several%20additional%0Ainsights%20that%20we%20believe%20will%20be%20of%20interest%20to%20the%20community%20at%20large.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15589v2&entry.124074799=Read"},
{"title": "Diverse Neural Audio Embeddings -- Bringing Features back !", "author": "Prateek Verma", "abstract": "  With the advent of modern AI architectures, a shift has happened towards\nend-to-end architectures. This pivot has led to neural architectures being\ntrained without domain-specific biases/knowledge, optimized according to the\ntask. We in this paper, learn audio embeddings via diverse feature\nrepresentations, in this case, domain-specific. For the case of audio\nclassification over hundreds of categories of sound, we learn robust separate\nembeddings for diverse audio properties such as pitch, timbre, and neural\nrepresentation, along with also learning it via an end-to-end architecture. We\nobserve handcrafted embeddings, e.g., pitch and timbre-based, although on their\nown, are not able to beat a fully end-to-end representation, yet adding these\ntogether with end-to-end embedding helps us, significantly improve performance.\nThis work would pave the way to bring some domain expertise with end-to-end\nmodels to learn robust, diverse representations, surpassing the performance of\njust training end-to-end models.\n", "link": "http://arxiv.org/abs/2309.08751v2", "date": "2024-09-13", "relevancy": 2.5872, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diverse%20Neural%20Audio%20Embeddings%20--%20Bringing%20Features%20back%20%21&body=Title%3A%20Diverse%20Neural%20Audio%20Embeddings%20--%20Bringing%20Features%20back%20%21%0AAuthor%3A%20Prateek%20Verma%0AAbstract%3A%20%20%20With%20the%20advent%20of%20modern%20AI%20architectures%2C%20a%20shift%20has%20happened%20towards%0Aend-to-end%20architectures.%20This%20pivot%20has%20led%20to%20neural%20architectures%20being%0Atrained%20without%20domain-specific%20biases/knowledge%2C%20optimized%20according%20to%20the%0Atask.%20We%20in%20this%20paper%2C%20learn%20audio%20embeddings%20via%20diverse%20feature%0Arepresentations%2C%20in%20this%20case%2C%20domain-specific.%20For%20the%20case%20of%20audio%0Aclassification%20over%20hundreds%20of%20categories%20of%20sound%2C%20we%20learn%20robust%20separate%0Aembeddings%20for%20diverse%20audio%20properties%20such%20as%20pitch%2C%20timbre%2C%20and%20neural%0Arepresentation%2C%20along%20with%20also%20learning%20it%20via%20an%20end-to-end%20architecture.%20We%0Aobserve%20handcrafted%20embeddings%2C%20e.g.%2C%20pitch%20and%20timbre-based%2C%20although%20on%20their%0Aown%2C%20are%20not%20able%20to%20beat%20a%20fully%20end-to-end%20representation%2C%20yet%20adding%20these%0Atogether%20with%20end-to-end%20embedding%20helps%20us%2C%20significantly%20improve%20performance.%0AThis%20work%20would%20pave%20the%20way%20to%20bring%20some%20domain%20expertise%20with%20end-to-end%0Amodels%20to%20learn%20robust%2C%20diverse%20representations%2C%20surpassing%20the%20performance%20of%0Ajust%20training%20end-to-end%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverse%2520Neural%2520Audio%2520Embeddings%2520--%2520Bringing%2520Features%2520back%2520%2521%26entry.906535625%3DPrateek%2520Verma%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520modern%2520AI%2520architectures%252C%2520a%2520shift%2520has%2520happened%2520towards%250Aend-to-end%2520architectures.%2520This%2520pivot%2520has%2520led%2520to%2520neural%2520architectures%2520being%250Atrained%2520without%2520domain-specific%2520biases/knowledge%252C%2520optimized%2520according%2520to%2520the%250Atask.%2520We%2520in%2520this%2520paper%252C%2520learn%2520audio%2520embeddings%2520via%2520diverse%2520feature%250Arepresentations%252C%2520in%2520this%2520case%252C%2520domain-specific.%2520For%2520the%2520case%2520of%2520audio%250Aclassification%2520over%2520hundreds%2520of%2520categories%2520of%2520sound%252C%2520we%2520learn%2520robust%2520separate%250Aembeddings%2520for%2520diverse%2520audio%2520properties%2520such%2520as%2520pitch%252C%2520timbre%252C%2520and%2520neural%250Arepresentation%252C%2520along%2520with%2520also%2520learning%2520it%2520via%2520an%2520end-to-end%2520architecture.%2520We%250Aobserve%2520handcrafted%2520embeddings%252C%2520e.g.%252C%2520pitch%2520and%2520timbre-based%252C%2520although%2520on%2520their%250Aown%252C%2520are%2520not%2520able%2520to%2520beat%2520a%2520fully%2520end-to-end%2520representation%252C%2520yet%2520adding%2520these%250Atogether%2520with%2520end-to-end%2520embedding%2520helps%2520us%252C%2520significantly%2520improve%2520performance.%250AThis%2520work%2520would%2520pave%2520the%2520way%2520to%2520bring%2520some%2520domain%2520expertise%2520with%2520end-to-end%250Amodels%2520to%2520learn%2520robust%252C%2520diverse%2520representations%252C%2520surpassing%2520the%2520performance%2520of%250Ajust%2520training%2520end-to-end%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diverse%20Neural%20Audio%20Embeddings%20--%20Bringing%20Features%20back%20%21&entry.906535625=Prateek%20Verma&entry.1292438233=%20%20With%20the%20advent%20of%20modern%20AI%20architectures%2C%20a%20shift%20has%20happened%20towards%0Aend-to-end%20architectures.%20This%20pivot%20has%20led%20to%20neural%20architectures%20being%0Atrained%20without%20domain-specific%20biases/knowledge%2C%20optimized%20according%20to%20the%0Atask.%20We%20in%20this%20paper%2C%20learn%20audio%20embeddings%20via%20diverse%20feature%0Arepresentations%2C%20in%20this%20case%2C%20domain-specific.%20For%20the%20case%20of%20audio%0Aclassification%20over%20hundreds%20of%20categories%20of%20sound%2C%20we%20learn%20robust%20separate%0Aembeddings%20for%20diverse%20audio%20properties%20such%20as%20pitch%2C%20timbre%2C%20and%20neural%0Arepresentation%2C%20along%20with%20also%20learning%20it%20via%20an%20end-to-end%20architecture.%20We%0Aobserve%20handcrafted%20embeddings%2C%20e.g.%2C%20pitch%20and%20timbre-based%2C%20although%20on%20their%0Aown%2C%20are%20not%20able%20to%20beat%20a%20fully%20end-to-end%20representation%2C%20yet%20adding%20these%0Atogether%20with%20end-to-end%20embedding%20helps%20us%2C%20significantly%20improve%20performance.%0AThis%20work%20would%20pave%20the%20way%20to%20bring%20some%20domain%20expertise%20with%20end-to-end%0Amodels%20to%20learn%20robust%2C%20diverse%20representations%2C%20surpassing%20the%20performance%20of%0Ajust%20training%20end-to-end%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08751v2&entry.124074799=Read"},
{"title": "Contactless Fingerprint Recognition Using 3D Graph Matching", "author": "Zhe Cui and Yuwei Jia and Siyang Zheng and Fei Su", "abstract": "  Contactless fingerprint is a newly developed type of fingerprint, and has\ngained lots of attention in recent fingerprint studies. However, most existing\ncontactless fingerprint algorithms treat contactless fingerprints as 2D plain\nfingerprints, and utilize similar recognition methods as traditional\ncontact-based 2D fingerprints. This recognition approach does not consider the\nmodality difference between contactless and contact fingerprints, especially\nthe intrinsic 3D characteristic of contactless fingerprints. This paper\nproposes a novel contactless fingerprint recognition algorithm that captures\nthe revealed 3D feature of contactless fingerprints rather than the plain 2D\nfeature. The proposed method first recovers 3D features from the input\ncontactless fingerprint, including the 3D shape model and 3D fingerprint\nfeature (minutiae, orientation, etc.). Then, a novel 3D graph matching is\nconducted in 3D space according to the extracted 3D feature. Our method\ncaptures the real 3D nature of contactless fingerprints as the whole feature\nextraction and matching algorithms are completed in real 3D space. Experiments\nresults on contactless fingerprint databases show that the proposed method\nsuccessfully improves the matching accuracy of contactless fingerprints.\nExceptionally, our method performs stably across multiple poses of contactless\nfingerprints due to 3D graph matching, which is a great advantage compared to\nprevious contactless fingerprint recognition algorithms.\n", "link": "http://arxiv.org/abs/2409.08782v1", "date": "2024-09-13", "relevancy": 2.5147, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5759}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contactless%20Fingerprint%20Recognition%20Using%203D%20Graph%20Matching&body=Title%3A%20Contactless%20Fingerprint%20Recognition%20Using%203D%20Graph%20Matching%0AAuthor%3A%20Zhe%20Cui%20and%20Yuwei%20Jia%20and%20Siyang%20Zheng%20and%20Fei%20Su%0AAbstract%3A%20%20%20Contactless%20fingerprint%20is%20a%20newly%20developed%20type%20of%20fingerprint%2C%20and%20has%0Agained%20lots%20of%20attention%20in%20recent%20fingerprint%20studies.%20However%2C%20most%20existing%0Acontactless%20fingerprint%20algorithms%20treat%20contactless%20fingerprints%20as%202D%20plain%0Afingerprints%2C%20and%20utilize%20similar%20recognition%20methods%20as%20traditional%0Acontact-based%202D%20fingerprints.%20This%20recognition%20approach%20does%20not%20consider%20the%0Amodality%20difference%20between%20contactless%20and%20contact%20fingerprints%2C%20especially%0Athe%20intrinsic%203D%20characteristic%20of%20contactless%20fingerprints.%20This%20paper%0Aproposes%20a%20novel%20contactless%20fingerprint%20recognition%20algorithm%20that%20captures%0Athe%20revealed%203D%20feature%20of%20contactless%20fingerprints%20rather%20than%20the%20plain%202D%0Afeature.%20The%20proposed%20method%20first%20recovers%203D%20features%20from%20the%20input%0Acontactless%20fingerprint%2C%20including%20the%203D%20shape%20model%20and%203D%20fingerprint%0Afeature%20%28minutiae%2C%20orientation%2C%20etc.%29.%20Then%2C%20a%20novel%203D%20graph%20matching%20is%0Aconducted%20in%203D%20space%20according%20to%20the%20extracted%203D%20feature.%20Our%20method%0Acaptures%20the%20real%203D%20nature%20of%20contactless%20fingerprints%20as%20the%20whole%20feature%0Aextraction%20and%20matching%20algorithms%20are%20completed%20in%20real%203D%20space.%20Experiments%0Aresults%20on%20contactless%20fingerprint%20databases%20show%20that%20the%20proposed%20method%0Asuccessfully%20improves%20the%20matching%20accuracy%20of%20contactless%20fingerprints.%0AExceptionally%2C%20our%20method%20performs%20stably%20across%20multiple%20poses%20of%20contactless%0Afingerprints%20due%20to%203D%20graph%20matching%2C%20which%20is%20a%20great%20advantage%20compared%20to%0Aprevious%20contactless%20fingerprint%20recognition%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContactless%2520Fingerprint%2520Recognition%2520Using%25203D%2520Graph%2520Matching%26entry.906535625%3DZhe%2520Cui%2520and%2520Yuwei%2520Jia%2520and%2520Siyang%2520Zheng%2520and%2520Fei%2520Su%26entry.1292438233%3D%2520%2520Contactless%2520fingerprint%2520is%2520a%2520newly%2520developed%2520type%2520of%2520fingerprint%252C%2520and%2520has%250Agained%2520lots%2520of%2520attention%2520in%2520recent%2520fingerprint%2520studies.%2520However%252C%2520most%2520existing%250Acontactless%2520fingerprint%2520algorithms%2520treat%2520contactless%2520fingerprints%2520as%25202D%2520plain%250Afingerprints%252C%2520and%2520utilize%2520similar%2520recognition%2520methods%2520as%2520traditional%250Acontact-based%25202D%2520fingerprints.%2520This%2520recognition%2520approach%2520does%2520not%2520consider%2520the%250Amodality%2520difference%2520between%2520contactless%2520and%2520contact%2520fingerprints%252C%2520especially%250Athe%2520intrinsic%25203D%2520characteristic%2520of%2520contactless%2520fingerprints.%2520This%2520paper%250Aproposes%2520a%2520novel%2520contactless%2520fingerprint%2520recognition%2520algorithm%2520that%2520captures%250Athe%2520revealed%25203D%2520feature%2520of%2520contactless%2520fingerprints%2520rather%2520than%2520the%2520plain%25202D%250Afeature.%2520The%2520proposed%2520method%2520first%2520recovers%25203D%2520features%2520from%2520the%2520input%250Acontactless%2520fingerprint%252C%2520including%2520the%25203D%2520shape%2520model%2520and%25203D%2520fingerprint%250Afeature%2520%2528minutiae%252C%2520orientation%252C%2520etc.%2529.%2520Then%252C%2520a%2520novel%25203D%2520graph%2520matching%2520is%250Aconducted%2520in%25203D%2520space%2520according%2520to%2520the%2520extracted%25203D%2520feature.%2520Our%2520method%250Acaptures%2520the%2520real%25203D%2520nature%2520of%2520contactless%2520fingerprints%2520as%2520the%2520whole%2520feature%250Aextraction%2520and%2520matching%2520algorithms%2520are%2520completed%2520in%2520real%25203D%2520space.%2520Experiments%250Aresults%2520on%2520contactless%2520fingerprint%2520databases%2520show%2520that%2520the%2520proposed%2520method%250Asuccessfully%2520improves%2520the%2520matching%2520accuracy%2520of%2520contactless%2520fingerprints.%250AExceptionally%252C%2520our%2520method%2520performs%2520stably%2520across%2520multiple%2520poses%2520of%2520contactless%250Afingerprints%2520due%2520to%25203D%2520graph%2520matching%252C%2520which%2520is%2520a%2520great%2520advantage%2520compared%2520to%250Aprevious%2520contactless%2520fingerprint%2520recognition%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contactless%20Fingerprint%20Recognition%20Using%203D%20Graph%20Matching&entry.906535625=Zhe%20Cui%20and%20Yuwei%20Jia%20and%20Siyang%20Zheng%20and%20Fei%20Su&entry.1292438233=%20%20Contactless%20fingerprint%20is%20a%20newly%20developed%20type%20of%20fingerprint%2C%20and%20has%0Agained%20lots%20of%20attention%20in%20recent%20fingerprint%20studies.%20However%2C%20most%20existing%0Acontactless%20fingerprint%20algorithms%20treat%20contactless%20fingerprints%20as%202D%20plain%0Afingerprints%2C%20and%20utilize%20similar%20recognition%20methods%20as%20traditional%0Acontact-based%202D%20fingerprints.%20This%20recognition%20approach%20does%20not%20consider%20the%0Amodality%20difference%20between%20contactless%20and%20contact%20fingerprints%2C%20especially%0Athe%20intrinsic%203D%20characteristic%20of%20contactless%20fingerprints.%20This%20paper%0Aproposes%20a%20novel%20contactless%20fingerprint%20recognition%20algorithm%20that%20captures%0Athe%20revealed%203D%20feature%20of%20contactless%20fingerprints%20rather%20than%20the%20plain%202D%0Afeature.%20The%20proposed%20method%20first%20recovers%203D%20features%20from%20the%20input%0Acontactless%20fingerprint%2C%20including%20the%203D%20shape%20model%20and%203D%20fingerprint%0Afeature%20%28minutiae%2C%20orientation%2C%20etc.%29.%20Then%2C%20a%20novel%203D%20graph%20matching%20is%0Aconducted%20in%203D%20space%20according%20to%20the%20extracted%203D%20feature.%20Our%20method%0Acaptures%20the%20real%203D%20nature%20of%20contactless%20fingerprints%20as%20the%20whole%20feature%0Aextraction%20and%20matching%20algorithms%20are%20completed%20in%20real%203D%20space.%20Experiments%0Aresults%20on%20contactless%20fingerprint%20databases%20show%20that%20the%20proposed%20method%0Asuccessfully%20improves%20the%20matching%20accuracy%20of%20contactless%20fingerprints.%0AExceptionally%2C%20our%20method%20performs%20stably%20across%20multiple%20poses%20of%20contactless%0Afingerprints%20due%20to%203D%20graph%20matching%2C%20which%20is%20a%20great%20advantage%20compared%20to%0Aprevious%20contactless%20fingerprint%20recognition%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08782v1&entry.124074799=Read"},
{"title": "Precision Aquaculture: An Integrated Computer Vision and IoT Approach\n  for Optimized Tilapia Feeding", "author": "Rania Hossam and Ahmed Heakl and Walid Gomaa", "abstract": "  Traditional fish farming practices often lead to inefficient feeding,\nresulting in environmental issues and reduced productivity. We developed an\ninnovative system combining computer vision and IoT technologies for precise\nTilapia feeding. Our solution uses real-time IoT sensors to monitor water\nquality parameters and computer vision algorithms to analyze fish size and\ncount, determining optimal feed amounts. A mobile app enables remote monitoring\nand control. We utilized YOLOv8 for keypoint detection to measure Tilapia\nweight from length, achieving \\textbf{94\\%} precision on 3,500 annotated\nimages. Pixel-based measurements were converted to centimeters using depth\nestimation for accurate feeding calculations. Our method, with data collection\nmirroring inference conditions, significantly improved results. Preliminary\nestimates suggest this approach could increase production up to 58 times\ncompared to traditional farms. Our models, code, and dataset are\nopen-source~\\footnote{The code, dataset, and models are available upon\nreasonable request.\n", "link": "http://arxiv.org/abs/2409.08695v1", "date": "2024-09-13", "relevancy": 2.4877, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5144}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4992}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Precision%20Aquaculture%3A%20An%20Integrated%20Computer%20Vision%20and%20IoT%20Approach%0A%20%20for%20Optimized%20Tilapia%20Feeding&body=Title%3A%20Precision%20Aquaculture%3A%20An%20Integrated%20Computer%20Vision%20and%20IoT%20Approach%0A%20%20for%20Optimized%20Tilapia%20Feeding%0AAuthor%3A%20Rania%20Hossam%20and%20Ahmed%20Heakl%20and%20Walid%20Gomaa%0AAbstract%3A%20%20%20Traditional%20fish%20farming%20practices%20often%20lead%20to%20inefficient%20feeding%2C%0Aresulting%20in%20environmental%20issues%20and%20reduced%20productivity.%20We%20developed%20an%0Ainnovative%20system%20combining%20computer%20vision%20and%20IoT%20technologies%20for%20precise%0ATilapia%20feeding.%20Our%20solution%20uses%20real-time%20IoT%20sensors%20to%20monitor%20water%0Aquality%20parameters%20and%20computer%20vision%20algorithms%20to%20analyze%20fish%20size%20and%0Acount%2C%20determining%20optimal%20feed%20amounts.%20A%20mobile%20app%20enables%20remote%20monitoring%0Aand%20control.%20We%20utilized%20YOLOv8%20for%20keypoint%20detection%20to%20measure%20Tilapia%0Aweight%20from%20length%2C%20achieving%20%5Ctextbf%7B94%5C%25%7D%20precision%20on%203%2C500%20annotated%0Aimages.%20Pixel-based%20measurements%20were%20converted%20to%20centimeters%20using%20depth%0Aestimation%20for%20accurate%20feeding%20calculations.%20Our%20method%2C%20with%20data%20collection%0Amirroring%20inference%20conditions%2C%20significantly%20improved%20results.%20Preliminary%0Aestimates%20suggest%20this%20approach%20could%20increase%20production%20up%20to%2058%20times%0Acompared%20to%20traditional%20farms.%20Our%20models%2C%20code%2C%20and%20dataset%20are%0Aopen-source~%5Cfootnote%7BThe%20code%2C%20dataset%2C%20and%20models%20are%20available%20upon%0Areasonable%20request.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrecision%2520Aquaculture%253A%2520An%2520Integrated%2520Computer%2520Vision%2520and%2520IoT%2520Approach%250A%2520%2520for%2520Optimized%2520Tilapia%2520Feeding%26entry.906535625%3DRania%2520Hossam%2520and%2520Ahmed%2520Heakl%2520and%2520Walid%2520Gomaa%26entry.1292438233%3D%2520%2520Traditional%2520fish%2520farming%2520practices%2520often%2520lead%2520to%2520inefficient%2520feeding%252C%250Aresulting%2520in%2520environmental%2520issues%2520and%2520reduced%2520productivity.%2520We%2520developed%2520an%250Ainnovative%2520system%2520combining%2520computer%2520vision%2520and%2520IoT%2520technologies%2520for%2520precise%250ATilapia%2520feeding.%2520Our%2520solution%2520uses%2520real-time%2520IoT%2520sensors%2520to%2520monitor%2520water%250Aquality%2520parameters%2520and%2520computer%2520vision%2520algorithms%2520to%2520analyze%2520fish%2520size%2520and%250Acount%252C%2520determining%2520optimal%2520feed%2520amounts.%2520A%2520mobile%2520app%2520enables%2520remote%2520monitoring%250Aand%2520control.%2520We%2520utilized%2520YOLOv8%2520for%2520keypoint%2520detection%2520to%2520measure%2520Tilapia%250Aweight%2520from%2520length%252C%2520achieving%2520%255Ctextbf%257B94%255C%2525%257D%2520precision%2520on%25203%252C500%2520annotated%250Aimages.%2520Pixel-based%2520measurements%2520were%2520converted%2520to%2520centimeters%2520using%2520depth%250Aestimation%2520for%2520accurate%2520feeding%2520calculations.%2520Our%2520method%252C%2520with%2520data%2520collection%250Amirroring%2520inference%2520conditions%252C%2520significantly%2520improved%2520results.%2520Preliminary%250Aestimates%2520suggest%2520this%2520approach%2520could%2520increase%2520production%2520up%2520to%252058%2520times%250Acompared%2520to%2520traditional%2520farms.%2520Our%2520models%252C%2520code%252C%2520and%2520dataset%2520are%250Aopen-source~%255Cfootnote%257BThe%2520code%252C%2520dataset%252C%2520and%2520models%2520are%2520available%2520upon%250Areasonable%2520request.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Precision%20Aquaculture%3A%20An%20Integrated%20Computer%20Vision%20and%20IoT%20Approach%0A%20%20for%20Optimized%20Tilapia%20Feeding&entry.906535625=Rania%20Hossam%20and%20Ahmed%20Heakl%20and%20Walid%20Gomaa&entry.1292438233=%20%20Traditional%20fish%20farming%20practices%20often%20lead%20to%20inefficient%20feeding%2C%0Aresulting%20in%20environmental%20issues%20and%20reduced%20productivity.%20We%20developed%20an%0Ainnovative%20system%20combining%20computer%20vision%20and%20IoT%20technologies%20for%20precise%0ATilapia%20feeding.%20Our%20solution%20uses%20real-time%20IoT%20sensors%20to%20monitor%20water%0Aquality%20parameters%20and%20computer%20vision%20algorithms%20to%20analyze%20fish%20size%20and%0Acount%2C%20determining%20optimal%20feed%20amounts.%20A%20mobile%20app%20enables%20remote%20monitoring%0Aand%20control.%20We%20utilized%20YOLOv8%20for%20keypoint%20detection%20to%20measure%20Tilapia%0Aweight%20from%20length%2C%20achieving%20%5Ctextbf%7B94%5C%25%7D%20precision%20on%203%2C500%20annotated%0Aimages.%20Pixel-based%20measurements%20were%20converted%20to%20centimeters%20using%20depth%0Aestimation%20for%20accurate%20feeding%20calculations.%20Our%20method%2C%20with%20data%20collection%0Amirroring%20inference%20conditions%2C%20significantly%20improved%20results.%20Preliminary%0Aestimates%20suggest%20this%20approach%20could%20increase%20production%20up%20to%2058%20times%0Acompared%20to%20traditional%20farms.%20Our%20models%2C%20code%2C%20and%20dataset%20are%0Aopen-source~%5Cfootnote%7BThe%20code%2C%20dataset%2C%20and%20models%20are%20available%20upon%0Areasonable%20request.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08695v1&entry.124074799=Read"},
{"title": "Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for\n  Recommender Tasks", "author": "Florian Gr\u00f6tschla and Luca Str\u00e4ssle and Luca A. Lanzend\u00f6rfer and Roger Wattenhofer", "abstract": "  Music recommender systems frequently utilize network-based models to capture\nrelationships between music pieces, artists, and users. Although these\nrelationships provide valuable insights for predictions, new music pieces or\nartists often face the cold-start problem due to insufficient initial\ninformation. To address this, one can extract content-based information\ndirectly from the music to enhance collaborative-filtering-based methods. While\nprevious approaches have relied on hand-crafted audio features for this\npurpose, we explore the use of contrastively pretrained neural audio embedding\nmodels, which offer a richer and more nuanced representation of music. Our\nexperiments demonstrate that neural embeddings, particularly those generated\nwith the Contrastive Language-Audio Pretraining (CLAP) model, present a\npromising approach to enhancing music recommendation tasks within graph-based\nframeworks.\n", "link": "http://arxiv.org/abs/2409.09026v1", "date": "2024-09-13", "relevancy": 2.4604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Leveraging%20Contrastively%20Pretrained%20Neural%20Audio%20Embeddings%20for%0A%20%20Recommender%20Tasks&body=Title%3A%20Towards%20Leveraging%20Contrastively%20Pretrained%20Neural%20Audio%20Embeddings%20for%0A%20%20Recommender%20Tasks%0AAuthor%3A%20Florian%20Gr%C3%B6tschla%20and%20Luca%20Str%C3%A4ssle%20and%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20Music%20recommender%20systems%20frequently%20utilize%20network-based%20models%20to%20capture%0Arelationships%20between%20music%20pieces%2C%20artists%2C%20and%20users.%20Although%20these%0Arelationships%20provide%20valuable%20insights%20for%20predictions%2C%20new%20music%20pieces%20or%0Aartists%20often%20face%20the%20cold-start%20problem%20due%20to%20insufficient%20initial%0Ainformation.%20To%20address%20this%2C%20one%20can%20extract%20content-based%20information%0Adirectly%20from%20the%20music%20to%20enhance%20collaborative-filtering-based%20methods.%20While%0Aprevious%20approaches%20have%20relied%20on%20hand-crafted%20audio%20features%20for%20this%0Apurpose%2C%20we%20explore%20the%20use%20of%20contrastively%20pretrained%20neural%20audio%20embedding%0Amodels%2C%20which%20offer%20a%20richer%20and%20more%20nuanced%20representation%20of%20music.%20Our%0Aexperiments%20demonstrate%20that%20neural%20embeddings%2C%20particularly%20those%20generated%0Awith%20the%20Contrastive%20Language-Audio%20Pretraining%20%28CLAP%29%20model%2C%20present%20a%0Apromising%20approach%20to%20enhancing%20music%20recommendation%20tasks%20within%20graph-based%0Aframeworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Leveraging%2520Contrastively%2520Pretrained%2520Neural%2520Audio%2520Embeddings%2520for%250A%2520%2520Recommender%2520Tasks%26entry.906535625%3DFlorian%2520Gr%25C3%25B6tschla%2520and%2520Luca%2520Str%25C3%25A4ssle%2520and%2520Luca%2520A.%2520Lanzend%25C3%25B6rfer%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520Music%2520recommender%2520systems%2520frequently%2520utilize%2520network-based%2520models%2520to%2520capture%250Arelationships%2520between%2520music%2520pieces%252C%2520artists%252C%2520and%2520users.%2520Although%2520these%250Arelationships%2520provide%2520valuable%2520insights%2520for%2520predictions%252C%2520new%2520music%2520pieces%2520or%250Aartists%2520often%2520face%2520the%2520cold-start%2520problem%2520due%2520to%2520insufficient%2520initial%250Ainformation.%2520To%2520address%2520this%252C%2520one%2520can%2520extract%2520content-based%2520information%250Adirectly%2520from%2520the%2520music%2520to%2520enhance%2520collaborative-filtering-based%2520methods.%2520While%250Aprevious%2520approaches%2520have%2520relied%2520on%2520hand-crafted%2520audio%2520features%2520for%2520this%250Apurpose%252C%2520we%2520explore%2520the%2520use%2520of%2520contrastively%2520pretrained%2520neural%2520audio%2520embedding%250Amodels%252C%2520which%2520offer%2520a%2520richer%2520and%2520more%2520nuanced%2520representation%2520of%2520music.%2520Our%250Aexperiments%2520demonstrate%2520that%2520neural%2520embeddings%252C%2520particularly%2520those%2520generated%250Awith%2520the%2520Contrastive%2520Language-Audio%2520Pretraining%2520%2528CLAP%2529%2520model%252C%2520present%2520a%250Apromising%2520approach%2520to%2520enhancing%2520music%2520recommendation%2520tasks%2520within%2520graph-based%250Aframeworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Leveraging%20Contrastively%20Pretrained%20Neural%20Audio%20Embeddings%20for%0A%20%20Recommender%20Tasks&entry.906535625=Florian%20Gr%C3%B6tschla%20and%20Luca%20Str%C3%A4ssle%20and%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20Music%20recommender%20systems%20frequently%20utilize%20network-based%20models%20to%20capture%0Arelationships%20between%20music%20pieces%2C%20artists%2C%20and%20users.%20Although%20these%0Arelationships%20provide%20valuable%20insights%20for%20predictions%2C%20new%20music%20pieces%20or%0Aartists%20often%20face%20the%20cold-start%20problem%20due%20to%20insufficient%20initial%0Ainformation.%20To%20address%20this%2C%20one%20can%20extract%20content-based%20information%0Adirectly%20from%20the%20music%20to%20enhance%20collaborative-filtering-based%20methods.%20While%0Aprevious%20approaches%20have%20relied%20on%20hand-crafted%20audio%20features%20for%20this%0Apurpose%2C%20we%20explore%20the%20use%20of%20contrastively%20pretrained%20neural%20audio%20embedding%0Amodels%2C%20which%20offer%20a%20richer%20and%20more%20nuanced%20representation%20of%20music.%20Our%0Aexperiments%20demonstrate%20that%20neural%20embeddings%2C%20particularly%20those%20generated%0Awith%20the%20Contrastive%20Language-Audio%20Pretraining%20%28CLAP%29%20model%2C%20present%20a%0Apromising%20approach%20to%20enhancing%20music%20recommendation%20tasks%20within%20graph-based%0Aframeworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09026v1&entry.124074799=Read"},
{"title": "GenMapping: Unleashing the Potential of Inverse Perspective Mapping for\n  Robust Online HD Map Construction", "author": "Siyu Li and Kailun Yang and Hao Shi and Song Wang and You Yao and Zhiyong Li", "abstract": "  Online High-Definition (HD) maps have emerged as the preferred option for\nautonomous driving, overshadowing the counterpart offline HD maps due to\nflexible update capability and lower maintenance costs. However, contemporary\nonline HD map models embed parameters of visual sensors into training,\nresulting in a significant decrease in generalization performance when applied\nto visual sensors with different parameters. Inspired by the inherent potential\nof Inverse Perspective Mapping (IPM), where camera parameters are decoupled\nfrom the training process, we have designed a universal map generation\nframework, GenMapping. The framework is established with a triadic synergy\narchitecture, including principal and dual auxiliary branches. When faced with\na coarse road image with local distortion translated via IPM, the principal\nbranch learns robust global features under the state space models. The two\nauxiliary branches are a dense perspective branch and a sparse prior branch.\nThe former exploits the correlation information between static and moving\nobjects, whereas the latter introduces the prior knowledge of OpenStreetMap\n(OSM). The triple-enhanced merging module is crafted to synergistically\nintegrate the unique spatial features from all three branches. To further\nimprove generalization capabilities, a Cross-View Map Learning (CVML) scheme is\nleveraged to realize joint learning within the common space. Additionally, a\nBidirectional Data Augmentation (BiDA) module is introduced to mitigate\nreliance on datasets concurrently. A thorough array of experimental results\nshows that the proposed model surpasses current state-of-the-art methods in\nboth semantic mapping and vectorized mapping, while also maintaining a rapid\ninference speed. The source code will be publicly available at\nhttps://github.com/lynn-yu/GenMapping.\n", "link": "http://arxiv.org/abs/2409.08688v1", "date": "2024-09-13", "relevancy": 2.4369, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6446}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5863}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenMapping%3A%20Unleashing%20the%20Potential%20of%20Inverse%20Perspective%20Mapping%20for%0A%20%20Robust%20Online%20HD%20Map%20Construction&body=Title%3A%20GenMapping%3A%20Unleashing%20the%20Potential%20of%20Inverse%20Perspective%20Mapping%20for%0A%20%20Robust%20Online%20HD%20Map%20Construction%0AAuthor%3A%20Siyu%20Li%20and%20Kailun%20Yang%20and%20Hao%20Shi%20and%20Song%20Wang%20and%20You%20Yao%20and%20Zhiyong%20Li%0AAbstract%3A%20%20%20Online%20High-Definition%20%28HD%29%20maps%20have%20emerged%20as%20the%20preferred%20option%20for%0Aautonomous%20driving%2C%20overshadowing%20the%20counterpart%20offline%20HD%20maps%20due%20to%0Aflexible%20update%20capability%20and%20lower%20maintenance%20costs.%20However%2C%20contemporary%0Aonline%20HD%20map%20models%20embed%20parameters%20of%20visual%20sensors%20into%20training%2C%0Aresulting%20in%20a%20significant%20decrease%20in%20generalization%20performance%20when%20applied%0Ato%20visual%20sensors%20with%20different%20parameters.%20Inspired%20by%20the%20inherent%20potential%0Aof%20Inverse%20Perspective%20Mapping%20%28IPM%29%2C%20where%20camera%20parameters%20are%20decoupled%0Afrom%20the%20training%20process%2C%20we%20have%20designed%20a%20universal%20map%20generation%0Aframework%2C%20GenMapping.%20The%20framework%20is%20established%20with%20a%20triadic%20synergy%0Aarchitecture%2C%20including%20principal%20and%20dual%20auxiliary%20branches.%20When%20faced%20with%0Aa%20coarse%20road%20image%20with%20local%20distortion%20translated%20via%20IPM%2C%20the%20principal%0Abranch%20learns%20robust%20global%20features%20under%20the%20state%20space%20models.%20The%20two%0Aauxiliary%20branches%20are%20a%20dense%20perspective%20branch%20and%20a%20sparse%20prior%20branch.%0AThe%20former%20exploits%20the%20correlation%20information%20between%20static%20and%20moving%0Aobjects%2C%20whereas%20the%20latter%20introduces%20the%20prior%20knowledge%20of%20OpenStreetMap%0A%28OSM%29.%20The%20triple-enhanced%20merging%20module%20is%20crafted%20to%20synergistically%0Aintegrate%20the%20unique%20spatial%20features%20from%20all%20three%20branches.%20To%20further%0Aimprove%20generalization%20capabilities%2C%20a%20Cross-View%20Map%20Learning%20%28CVML%29%20scheme%20is%0Aleveraged%20to%20realize%20joint%20learning%20within%20the%20common%20space.%20Additionally%2C%20a%0ABidirectional%20Data%20Augmentation%20%28BiDA%29%20module%20is%20introduced%20to%20mitigate%0Areliance%20on%20datasets%20concurrently.%20A%20thorough%20array%20of%20experimental%20results%0Ashows%20that%20the%20proposed%20model%20surpasses%20current%20state-of-the-art%20methods%20in%0Aboth%20semantic%20mapping%20and%20vectorized%20mapping%2C%20while%20also%20maintaining%20a%20rapid%0Ainference%20speed.%20The%20source%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/lynn-yu/GenMapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenMapping%253A%2520Unleashing%2520the%2520Potential%2520of%2520Inverse%2520Perspective%2520Mapping%2520for%250A%2520%2520Robust%2520Online%2520HD%2520Map%2520Construction%26entry.906535625%3DSiyu%2520Li%2520and%2520Kailun%2520Yang%2520and%2520Hao%2520Shi%2520and%2520Song%2520Wang%2520and%2520You%2520Yao%2520and%2520Zhiyong%2520Li%26entry.1292438233%3D%2520%2520Online%2520High-Definition%2520%2528HD%2529%2520maps%2520have%2520emerged%2520as%2520the%2520preferred%2520option%2520for%250Aautonomous%2520driving%252C%2520overshadowing%2520the%2520counterpart%2520offline%2520HD%2520maps%2520due%2520to%250Aflexible%2520update%2520capability%2520and%2520lower%2520maintenance%2520costs.%2520However%252C%2520contemporary%250Aonline%2520HD%2520map%2520models%2520embed%2520parameters%2520of%2520visual%2520sensors%2520into%2520training%252C%250Aresulting%2520in%2520a%2520significant%2520decrease%2520in%2520generalization%2520performance%2520when%2520applied%250Ato%2520visual%2520sensors%2520with%2520different%2520parameters.%2520Inspired%2520by%2520the%2520inherent%2520potential%250Aof%2520Inverse%2520Perspective%2520Mapping%2520%2528IPM%2529%252C%2520where%2520camera%2520parameters%2520are%2520decoupled%250Afrom%2520the%2520training%2520process%252C%2520we%2520have%2520designed%2520a%2520universal%2520map%2520generation%250Aframework%252C%2520GenMapping.%2520The%2520framework%2520is%2520established%2520with%2520a%2520triadic%2520synergy%250Aarchitecture%252C%2520including%2520principal%2520and%2520dual%2520auxiliary%2520branches.%2520When%2520faced%2520with%250Aa%2520coarse%2520road%2520image%2520with%2520local%2520distortion%2520translated%2520via%2520IPM%252C%2520the%2520principal%250Abranch%2520learns%2520robust%2520global%2520features%2520under%2520the%2520state%2520space%2520models.%2520The%2520two%250Aauxiliary%2520branches%2520are%2520a%2520dense%2520perspective%2520branch%2520and%2520a%2520sparse%2520prior%2520branch.%250AThe%2520former%2520exploits%2520the%2520correlation%2520information%2520between%2520static%2520and%2520moving%250Aobjects%252C%2520whereas%2520the%2520latter%2520introduces%2520the%2520prior%2520knowledge%2520of%2520OpenStreetMap%250A%2528OSM%2529.%2520The%2520triple-enhanced%2520merging%2520module%2520is%2520crafted%2520to%2520synergistically%250Aintegrate%2520the%2520unique%2520spatial%2520features%2520from%2520all%2520three%2520branches.%2520To%2520further%250Aimprove%2520generalization%2520capabilities%252C%2520a%2520Cross-View%2520Map%2520Learning%2520%2528CVML%2529%2520scheme%2520is%250Aleveraged%2520to%2520realize%2520joint%2520learning%2520within%2520the%2520common%2520space.%2520Additionally%252C%2520a%250ABidirectional%2520Data%2520Augmentation%2520%2528BiDA%2529%2520module%2520is%2520introduced%2520to%2520mitigate%250Areliance%2520on%2520datasets%2520concurrently.%2520A%2520thorough%2520array%2520of%2520experimental%2520results%250Ashows%2520that%2520the%2520proposed%2520model%2520surpasses%2520current%2520state-of-the-art%2520methods%2520in%250Aboth%2520semantic%2520mapping%2520and%2520vectorized%2520mapping%252C%2520while%2520also%2520maintaining%2520a%2520rapid%250Ainference%2520speed.%2520The%2520source%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/lynn-yu/GenMapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenMapping%3A%20Unleashing%20the%20Potential%20of%20Inverse%20Perspective%20Mapping%20for%0A%20%20Robust%20Online%20HD%20Map%20Construction&entry.906535625=Siyu%20Li%20and%20Kailun%20Yang%20and%20Hao%20Shi%20and%20Song%20Wang%20and%20You%20Yao%20and%20Zhiyong%20Li&entry.1292438233=%20%20Online%20High-Definition%20%28HD%29%20maps%20have%20emerged%20as%20the%20preferred%20option%20for%0Aautonomous%20driving%2C%20overshadowing%20the%20counterpart%20offline%20HD%20maps%20due%20to%0Aflexible%20update%20capability%20and%20lower%20maintenance%20costs.%20However%2C%20contemporary%0Aonline%20HD%20map%20models%20embed%20parameters%20of%20visual%20sensors%20into%20training%2C%0Aresulting%20in%20a%20significant%20decrease%20in%20generalization%20performance%20when%20applied%0Ato%20visual%20sensors%20with%20different%20parameters.%20Inspired%20by%20the%20inherent%20potential%0Aof%20Inverse%20Perspective%20Mapping%20%28IPM%29%2C%20where%20camera%20parameters%20are%20decoupled%0Afrom%20the%20training%20process%2C%20we%20have%20designed%20a%20universal%20map%20generation%0Aframework%2C%20GenMapping.%20The%20framework%20is%20established%20with%20a%20triadic%20synergy%0Aarchitecture%2C%20including%20principal%20and%20dual%20auxiliary%20branches.%20When%20faced%20with%0Aa%20coarse%20road%20image%20with%20local%20distortion%20translated%20via%20IPM%2C%20the%20principal%0Abranch%20learns%20robust%20global%20features%20under%20the%20state%20space%20models.%20The%20two%0Aauxiliary%20branches%20are%20a%20dense%20perspective%20branch%20and%20a%20sparse%20prior%20branch.%0AThe%20former%20exploits%20the%20correlation%20information%20between%20static%20and%20moving%0Aobjects%2C%20whereas%20the%20latter%20introduces%20the%20prior%20knowledge%20of%20OpenStreetMap%0A%28OSM%29.%20The%20triple-enhanced%20merging%20module%20is%20crafted%20to%20synergistically%0Aintegrate%20the%20unique%20spatial%20features%20from%20all%20three%20branches.%20To%20further%0Aimprove%20generalization%20capabilities%2C%20a%20Cross-View%20Map%20Learning%20%28CVML%29%20scheme%20is%0Aleveraged%20to%20realize%20joint%20learning%20within%20the%20common%20space.%20Additionally%2C%20a%0ABidirectional%20Data%20Augmentation%20%28BiDA%29%20module%20is%20introduced%20to%20mitigate%0Areliance%20on%20datasets%20concurrently.%20A%20thorough%20array%20of%20experimental%20results%0Ashows%20that%20the%20proposed%20model%20surpasses%20current%20state-of-the-art%20methods%20in%0Aboth%20semantic%20mapping%20and%20vectorized%20mapping%2C%20while%20also%20maintaining%20a%20rapid%0Ainference%20speed.%20The%20source%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/lynn-yu/GenMapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08688v1&entry.124074799=Read"},
{"title": "Adaptive Sampling for Continuous Group Equivariant Neural Networks", "author": "Berfin Inal and Gabriele Cesa", "abstract": "  Steerable networks, which process data with intrinsic symmetries, often use\nFourier-based nonlinearities that require sampling from the entire group,\nleading to a need for discretization in continuous groups. As the number of\nsamples increases, both performance and equivariance improve, yet this also\nleads to higher computational costs. To address this, we introduce an adaptive\nsampling approach that dynamically adjusts the sampling process to the\nsymmetries in the data, reducing the number of required group samples and\nlowering the computational demands. We explore various implementations and\ntheir effects on model performance, equivariance, and computational efficiency.\nOur findings demonstrate improved model performance, and a marginal increase in\nmemory efficiency.\n", "link": "http://arxiv.org/abs/2409.08741v1", "date": "2024-09-13", "relevancy": 2.413, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4941}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4775}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Sampling%20for%20Continuous%20Group%20Equivariant%20Neural%20Networks&body=Title%3A%20Adaptive%20Sampling%20for%20Continuous%20Group%20Equivariant%20Neural%20Networks%0AAuthor%3A%20Berfin%20Inal%20and%20Gabriele%20Cesa%0AAbstract%3A%20%20%20Steerable%20networks%2C%20which%20process%20data%20with%20intrinsic%20symmetries%2C%20often%20use%0AFourier-based%20nonlinearities%20that%20require%20sampling%20from%20the%20entire%20group%2C%0Aleading%20to%20a%20need%20for%20discretization%20in%20continuous%20groups.%20As%20the%20number%20of%0Asamples%20increases%2C%20both%20performance%20and%20equivariance%20improve%2C%20yet%20this%20also%0Aleads%20to%20higher%20computational%20costs.%20To%20address%20this%2C%20we%20introduce%20an%20adaptive%0Asampling%20approach%20that%20dynamically%20adjusts%20the%20sampling%20process%20to%20the%0Asymmetries%20in%20the%20data%2C%20reducing%20the%20number%20of%20required%20group%20samples%20and%0Alowering%20the%20computational%20demands.%20We%20explore%20various%20implementations%20and%0Atheir%20effects%20on%20model%20performance%2C%20equivariance%2C%20and%20computational%20efficiency.%0AOur%20findings%20demonstrate%20improved%20model%20performance%2C%20and%20a%20marginal%20increase%20in%0Amemory%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Sampling%2520for%2520Continuous%2520Group%2520Equivariant%2520Neural%2520Networks%26entry.906535625%3DBerfin%2520Inal%2520and%2520Gabriele%2520Cesa%26entry.1292438233%3D%2520%2520Steerable%2520networks%252C%2520which%2520process%2520data%2520with%2520intrinsic%2520symmetries%252C%2520often%2520use%250AFourier-based%2520nonlinearities%2520that%2520require%2520sampling%2520from%2520the%2520entire%2520group%252C%250Aleading%2520to%2520a%2520need%2520for%2520discretization%2520in%2520continuous%2520groups.%2520As%2520the%2520number%2520of%250Asamples%2520increases%252C%2520both%2520performance%2520and%2520equivariance%2520improve%252C%2520yet%2520this%2520also%250Aleads%2520to%2520higher%2520computational%2520costs.%2520To%2520address%2520this%252C%2520we%2520introduce%2520an%2520adaptive%250Asampling%2520approach%2520that%2520dynamically%2520adjusts%2520the%2520sampling%2520process%2520to%2520the%250Asymmetries%2520in%2520the%2520data%252C%2520reducing%2520the%2520number%2520of%2520required%2520group%2520samples%2520and%250Alowering%2520the%2520computational%2520demands.%2520We%2520explore%2520various%2520implementations%2520and%250Atheir%2520effects%2520on%2520model%2520performance%252C%2520equivariance%252C%2520and%2520computational%2520efficiency.%250AOur%2520findings%2520demonstrate%2520improved%2520model%2520performance%252C%2520and%2520a%2520marginal%2520increase%2520in%250Amemory%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Sampling%20for%20Continuous%20Group%20Equivariant%20Neural%20Networks&entry.906535625=Berfin%20Inal%20and%20Gabriele%20Cesa&entry.1292438233=%20%20Steerable%20networks%2C%20which%20process%20data%20with%20intrinsic%20symmetries%2C%20often%20use%0AFourier-based%20nonlinearities%20that%20require%20sampling%20from%20the%20entire%20group%2C%0Aleading%20to%20a%20need%20for%20discretization%20in%20continuous%20groups.%20As%20the%20number%20of%0Asamples%20increases%2C%20both%20performance%20and%20equivariance%20improve%2C%20yet%20this%20also%0Aleads%20to%20higher%20computational%20costs.%20To%20address%20this%2C%20we%20introduce%20an%20adaptive%0Asampling%20approach%20that%20dynamically%20adjusts%20the%20sampling%20process%20to%20the%0Asymmetries%20in%20the%20data%2C%20reducing%20the%20number%20of%20required%20group%20samples%20and%0Alowering%20the%20computational%20demands.%20We%20explore%20various%20implementations%20and%0Atheir%20effects%20on%20model%20performance%2C%20equivariance%2C%20and%20computational%20efficiency.%0AOur%20findings%20demonstrate%20improved%20model%20performance%2C%20and%20a%20marginal%20increase%20in%0Amemory%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08741v1&entry.124074799=Read"},
{"title": "Does a Neural Network Really Encode Symbolic Concepts?", "author": "Mingjie Li and Quanshi Zhang", "abstract": "  Recently, a series of studies have tried to extract interactions between\ninput variables modeled by a DNN and define such interactions as concepts\nencoded by the DNN. However, strictly speaking, there still lacks a solid\nguarantee whether such interactions indeed represent meaningful concepts.\nTherefore, in this paper, we examine the trustworthiness of interaction\nconcepts from four perspectives. Extensive empirical studies have verified that\na well-trained DNN usually encodes sparse, transferable, and discriminative\nconcepts, which is partially aligned with human intuition.\n", "link": "http://arxiv.org/abs/2302.13080v3", "date": "2024-09-13", "relevancy": 2.4081, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20a%20Neural%20Network%20Really%20Encode%20Symbolic%20Concepts%3F&body=Title%3A%20Does%20a%20Neural%20Network%20Really%20Encode%20Symbolic%20Concepts%3F%0AAuthor%3A%20Mingjie%20Li%20and%20Quanshi%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20a%20series%20of%20studies%20have%20tried%20to%20extract%20interactions%20between%0Ainput%20variables%20modeled%20by%20a%20DNN%20and%20define%20such%20interactions%20as%20concepts%0Aencoded%20by%20the%20DNN.%20However%2C%20strictly%20speaking%2C%20there%20still%20lacks%20a%20solid%0Aguarantee%20whether%20such%20interactions%20indeed%20represent%20meaningful%20concepts.%0ATherefore%2C%20in%20this%20paper%2C%20we%20examine%20the%20trustworthiness%20of%20interaction%0Aconcepts%20from%20four%20perspectives.%20Extensive%20empirical%20studies%20have%20verified%20that%0Aa%20well-trained%20DNN%20usually%20encodes%20sparse%2C%20transferable%2C%20and%20discriminative%0Aconcepts%2C%20which%20is%20partially%20aligned%20with%20human%20intuition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.13080v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520a%2520Neural%2520Network%2520Really%2520Encode%2520Symbolic%2520Concepts%253F%26entry.906535625%3DMingjie%2520Li%2520and%2520Quanshi%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520series%2520of%2520studies%2520have%2520tried%2520to%2520extract%2520interactions%2520between%250Ainput%2520variables%2520modeled%2520by%2520a%2520DNN%2520and%2520define%2520such%2520interactions%2520as%2520concepts%250Aencoded%2520by%2520the%2520DNN.%2520However%252C%2520strictly%2520speaking%252C%2520there%2520still%2520lacks%2520a%2520solid%250Aguarantee%2520whether%2520such%2520interactions%2520indeed%2520represent%2520meaningful%2520concepts.%250ATherefore%252C%2520in%2520this%2520paper%252C%2520we%2520examine%2520the%2520trustworthiness%2520of%2520interaction%250Aconcepts%2520from%2520four%2520perspectives.%2520Extensive%2520empirical%2520studies%2520have%2520verified%2520that%250Aa%2520well-trained%2520DNN%2520usually%2520encodes%2520sparse%252C%2520transferable%252C%2520and%2520discriminative%250Aconcepts%252C%2520which%2520is%2520partially%2520aligned%2520with%2520human%2520intuition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.13080v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20a%20Neural%20Network%20Really%20Encode%20Symbolic%20Concepts%3F&entry.906535625=Mingjie%20Li%20and%20Quanshi%20Zhang&entry.1292438233=%20%20Recently%2C%20a%20series%20of%20studies%20have%20tried%20to%20extract%20interactions%20between%0Ainput%20variables%20modeled%20by%20a%20DNN%20and%20define%20such%20interactions%20as%20concepts%0Aencoded%20by%20the%20DNN.%20However%2C%20strictly%20speaking%2C%20there%20still%20lacks%20a%20solid%0Aguarantee%20whether%20such%20interactions%20indeed%20represent%20meaningful%20concepts.%0ATherefore%2C%20in%20this%20paper%2C%20we%20examine%20the%20trustworthiness%20of%20interaction%0Aconcepts%20from%20four%20perspectives.%20Extensive%20empirical%20studies%20have%20verified%20that%0Aa%20well-trained%20DNN%20usually%20encodes%20sparse%2C%20transferable%2C%20and%20discriminative%0Aconcepts%2C%20which%20is%20partially%20aligned%20with%20human%20intuition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.13080v3&entry.124074799=Read"},
{"title": "SLIM: Scalable and Lightweight LiDAR Mapping in Urban Environments", "author": "Zehuan Yu and Zhijian Qiao and Wenyi Liu and Huan Yin and Shaojie Shen", "abstract": "  LiDAR point cloud maps are extensively utilized on roads for robot navigation\ndue to their high consistency. However, dense point clouds face challenges of\nhigh memory consumption and reduced maintainability for long-term operations.\nIn this study, we introduce SLIM, a scalable and lightweight mapping system for\nlong-term LiDAR mapping in urban environments. The system begins by\nparameterizing structural point clouds into lines and planes. These lightweight\nand structural representations meet the requirements of map merging, pose graph\noptimization, and bundle adjustment, ensuring incremental management and local\nconsistency. For long-term operations, a map-centric nonlinear factor recovery\nmethod is designed to sparsify poses while preserving mapping accuracy. We\nvalidate the SLIM system with multi-session real-world LiDAR data from\nclassical LiDAR mapping datasets, including KITTI, NCLT, and HeLiPR. The\nexperiments demonstrate its capabilities in mapping accuracy, lightweightness,\nand scalability. Map re-use is also verified through map-based robot\nlocalization. Ultimately, with multi-session LiDAR data, the SLIM system\nprovides a globally consistent map with low memory consumption (130 KB/km). We\nhave made our code open-source to benefit the community.\n", "link": "http://arxiv.org/abs/2409.08681v1", "date": "2024-09-13", "relevancy": 2.3887, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6171}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6075}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLIM%3A%20Scalable%20and%20Lightweight%20LiDAR%20Mapping%20in%20Urban%20Environments&body=Title%3A%20SLIM%3A%20Scalable%20and%20Lightweight%20LiDAR%20Mapping%20in%20Urban%20Environments%0AAuthor%3A%20Zehuan%20Yu%20and%20Zhijian%20Qiao%20and%20Wenyi%20Liu%20and%20Huan%20Yin%20and%20Shaojie%20Shen%0AAbstract%3A%20%20%20LiDAR%20point%20cloud%20maps%20are%20extensively%20utilized%20on%20roads%20for%20robot%20navigation%0Adue%20to%20their%20high%20consistency.%20However%2C%20dense%20point%20clouds%20face%20challenges%20of%0Ahigh%20memory%20consumption%20and%20reduced%20maintainability%20for%20long-term%20operations.%0AIn%20this%20study%2C%20we%20introduce%20SLIM%2C%20a%20scalable%20and%20lightweight%20mapping%20system%20for%0Along-term%20LiDAR%20mapping%20in%20urban%20environments.%20The%20system%20begins%20by%0Aparameterizing%20structural%20point%20clouds%20into%20lines%20and%20planes.%20These%20lightweight%0Aand%20structural%20representations%20meet%20the%20requirements%20of%20map%20merging%2C%20pose%20graph%0Aoptimization%2C%20and%20bundle%20adjustment%2C%20ensuring%20incremental%20management%20and%20local%0Aconsistency.%20For%20long-term%20operations%2C%20a%20map-centric%20nonlinear%20factor%20recovery%0Amethod%20is%20designed%20to%20sparsify%20poses%20while%20preserving%20mapping%20accuracy.%20We%0Avalidate%20the%20SLIM%20system%20with%20multi-session%20real-world%20LiDAR%20data%20from%0Aclassical%20LiDAR%20mapping%20datasets%2C%20including%20KITTI%2C%20NCLT%2C%20and%20HeLiPR.%20The%0Aexperiments%20demonstrate%20its%20capabilities%20in%20mapping%20accuracy%2C%20lightweightness%2C%0Aand%20scalability.%20Map%20re-use%20is%20also%20verified%20through%20map-based%20robot%0Alocalization.%20Ultimately%2C%20with%20multi-session%20LiDAR%20data%2C%20the%20SLIM%20system%0Aprovides%20a%20globally%20consistent%20map%20with%20low%20memory%20consumption%20%28130%20KB/km%29.%20We%0Ahave%20made%20our%20code%20open-source%20to%20benefit%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLIM%253A%2520Scalable%2520and%2520Lightweight%2520LiDAR%2520Mapping%2520in%2520Urban%2520Environments%26entry.906535625%3DZehuan%2520Yu%2520and%2520Zhijian%2520Qiao%2520and%2520Wenyi%2520Liu%2520and%2520Huan%2520Yin%2520and%2520Shaojie%2520Shen%26entry.1292438233%3D%2520%2520LiDAR%2520point%2520cloud%2520maps%2520are%2520extensively%2520utilized%2520on%2520roads%2520for%2520robot%2520navigation%250Adue%2520to%2520their%2520high%2520consistency.%2520However%252C%2520dense%2520point%2520clouds%2520face%2520challenges%2520of%250Ahigh%2520memory%2520consumption%2520and%2520reduced%2520maintainability%2520for%2520long-term%2520operations.%250AIn%2520this%2520study%252C%2520we%2520introduce%2520SLIM%252C%2520a%2520scalable%2520and%2520lightweight%2520mapping%2520system%2520for%250Along-term%2520LiDAR%2520mapping%2520in%2520urban%2520environments.%2520The%2520system%2520begins%2520by%250Aparameterizing%2520structural%2520point%2520clouds%2520into%2520lines%2520and%2520planes.%2520These%2520lightweight%250Aand%2520structural%2520representations%2520meet%2520the%2520requirements%2520of%2520map%2520merging%252C%2520pose%2520graph%250Aoptimization%252C%2520and%2520bundle%2520adjustment%252C%2520ensuring%2520incremental%2520management%2520and%2520local%250Aconsistency.%2520For%2520long-term%2520operations%252C%2520a%2520map-centric%2520nonlinear%2520factor%2520recovery%250Amethod%2520is%2520designed%2520to%2520sparsify%2520poses%2520while%2520preserving%2520mapping%2520accuracy.%2520We%250Avalidate%2520the%2520SLIM%2520system%2520with%2520multi-session%2520real-world%2520LiDAR%2520data%2520from%250Aclassical%2520LiDAR%2520mapping%2520datasets%252C%2520including%2520KITTI%252C%2520NCLT%252C%2520and%2520HeLiPR.%2520The%250Aexperiments%2520demonstrate%2520its%2520capabilities%2520in%2520mapping%2520accuracy%252C%2520lightweightness%252C%250Aand%2520scalability.%2520Map%2520re-use%2520is%2520also%2520verified%2520through%2520map-based%2520robot%250Alocalization.%2520Ultimately%252C%2520with%2520multi-session%2520LiDAR%2520data%252C%2520the%2520SLIM%2520system%250Aprovides%2520a%2520globally%2520consistent%2520map%2520with%2520low%2520memory%2520consumption%2520%2528130%2520KB/km%2529.%2520We%250Ahave%2520made%2520our%2520code%2520open-source%2520to%2520benefit%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLIM%3A%20Scalable%20and%20Lightweight%20LiDAR%20Mapping%20in%20Urban%20Environments&entry.906535625=Zehuan%20Yu%20and%20Zhijian%20Qiao%20and%20Wenyi%20Liu%20and%20Huan%20Yin%20and%20Shaojie%20Shen&entry.1292438233=%20%20LiDAR%20point%20cloud%20maps%20are%20extensively%20utilized%20on%20roads%20for%20robot%20navigation%0Adue%20to%20their%20high%20consistency.%20However%2C%20dense%20point%20clouds%20face%20challenges%20of%0Ahigh%20memory%20consumption%20and%20reduced%20maintainability%20for%20long-term%20operations.%0AIn%20this%20study%2C%20we%20introduce%20SLIM%2C%20a%20scalable%20and%20lightweight%20mapping%20system%20for%0Along-term%20LiDAR%20mapping%20in%20urban%20environments.%20The%20system%20begins%20by%0Aparameterizing%20structural%20point%20clouds%20into%20lines%20and%20planes.%20These%20lightweight%0Aand%20structural%20representations%20meet%20the%20requirements%20of%20map%20merging%2C%20pose%20graph%0Aoptimization%2C%20and%20bundle%20adjustment%2C%20ensuring%20incremental%20management%20and%20local%0Aconsistency.%20For%20long-term%20operations%2C%20a%20map-centric%20nonlinear%20factor%20recovery%0Amethod%20is%20designed%20to%20sparsify%20poses%20while%20preserving%20mapping%20accuracy.%20We%0Avalidate%20the%20SLIM%20system%20with%20multi-session%20real-world%20LiDAR%20data%20from%0Aclassical%20LiDAR%20mapping%20datasets%2C%20including%20KITTI%2C%20NCLT%2C%20and%20HeLiPR.%20The%0Aexperiments%20demonstrate%20its%20capabilities%20in%20mapping%20accuracy%2C%20lightweightness%2C%0Aand%20scalability.%20Map%20re-use%20is%20also%20verified%20through%20map-based%20robot%0Alocalization.%20Ultimately%2C%20with%20multi-session%20LiDAR%20data%2C%20the%20SLIM%20system%0Aprovides%20a%20globally%20consistent%20map%20with%20low%20memory%20consumption%20%28130%20KB/km%29.%20We%0Ahave%20made%20our%20code%20open-source%20to%20benefit%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08681v1&entry.124074799=Read"},
{"title": "A Diffusion Approach to Radiance Field Relighting using\n  Multi-Illumination Synthesis", "author": "Yohan Poirier-Ginter and Alban Gauthier and Julien Phillip and Jean-Francois Lalonde and George Drettakis", "abstract": "  Relighting radiance fields is severely underconstrained for multi-view data,\nwhich is most often captured under a single illumination condition; It is\nespecially hard for full scenes containing multiple objects. We introduce a\nmethod to create relightable radiance fields using such single-illumination\ndata by exploiting priors extracted from 2D image diffusion models. We first\nfine-tune a 2D diffusion model on a multi-illumination dataset conditioned by\nlight direction, allowing us to augment a single-illumination capture into a\nrealistic -- but possibly inconsistent -- multi-illumination dataset from\ndirectly defined light directions. We use this augmented data to create a\nrelightable radiance field represented by 3D Gaussian splats. To allow direct\ncontrol of light direction for low-frequency lighting, we represent appearance\nwith a multi-layer perceptron parameterized on light direction. To enforce\nmulti-view consistency and overcome inaccuracies we optimize a per-image\nauxiliary feature vector. We show results on synthetic and real multi-view data\nunder single illumination, demonstrating that our method successfully exploits\n2D diffusion model priors to allow realistic 3D relighting for complete scenes.\nProject site\nhttps://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/\n", "link": "http://arxiv.org/abs/2409.08947v1", "date": "2024-09-13", "relevancy": 2.3643, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5921}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5921}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Diffusion%20Approach%20to%20Radiance%20Field%20Relighting%20using%0A%20%20Multi-Illumination%20Synthesis&body=Title%3A%20A%20Diffusion%20Approach%20to%20Radiance%20Field%20Relighting%20using%0A%20%20Multi-Illumination%20Synthesis%0AAuthor%3A%20Yohan%20Poirier-Ginter%20and%20Alban%20Gauthier%20and%20Julien%20Phillip%20and%20Jean-Francois%20Lalonde%20and%20George%20Drettakis%0AAbstract%3A%20%20%20Relighting%20radiance%20fields%20is%20severely%20underconstrained%20for%20multi-view%20data%2C%0Awhich%20is%20most%20often%20captured%20under%20a%20single%20illumination%20condition%3B%20It%20is%0Aespecially%20hard%20for%20full%20scenes%20containing%20multiple%20objects.%20We%20introduce%20a%0Amethod%20to%20create%20relightable%20radiance%20fields%20using%20such%20single-illumination%0Adata%20by%20exploiting%20priors%20extracted%20from%202D%20image%20diffusion%20models.%20We%20first%0Afine-tune%20a%202D%20diffusion%20model%20on%20a%20multi-illumination%20dataset%20conditioned%20by%0Alight%20direction%2C%20allowing%20us%20to%20augment%20a%20single-illumination%20capture%20into%20a%0Arealistic%20--%20but%20possibly%20inconsistent%20--%20multi-illumination%20dataset%20from%0Adirectly%20defined%20light%20directions.%20We%20use%20this%20augmented%20data%20to%20create%20a%0Arelightable%20radiance%20field%20represented%20by%203D%20Gaussian%20splats.%20To%20allow%20direct%0Acontrol%20of%20light%20direction%20for%20low-frequency%20lighting%2C%20we%20represent%20appearance%0Awith%20a%20multi-layer%20perceptron%20parameterized%20on%20light%20direction.%20To%20enforce%0Amulti-view%20consistency%20and%20overcome%20inaccuracies%20we%20optimize%20a%20per-image%0Aauxiliary%20feature%20vector.%20We%20show%20results%20on%20synthetic%20and%20real%20multi-view%20data%0Aunder%20single%20illumination%2C%20demonstrating%20that%20our%20method%20successfully%20exploits%0A2D%20diffusion%20model%20priors%20to%20allow%20realistic%203D%20relighting%20for%20complete%20scenes.%0AProject%20site%0Ahttps%3A//repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Diffusion%2520Approach%2520to%2520Radiance%2520Field%2520Relighting%2520using%250A%2520%2520Multi-Illumination%2520Synthesis%26entry.906535625%3DYohan%2520Poirier-Ginter%2520and%2520Alban%2520Gauthier%2520and%2520Julien%2520Phillip%2520and%2520Jean-Francois%2520Lalonde%2520and%2520George%2520Drettakis%26entry.1292438233%3D%2520%2520Relighting%2520radiance%2520fields%2520is%2520severely%2520underconstrained%2520for%2520multi-view%2520data%252C%250Awhich%2520is%2520most%2520often%2520captured%2520under%2520a%2520single%2520illumination%2520condition%253B%2520It%2520is%250Aespecially%2520hard%2520for%2520full%2520scenes%2520containing%2520multiple%2520objects.%2520We%2520introduce%2520a%250Amethod%2520to%2520create%2520relightable%2520radiance%2520fields%2520using%2520such%2520single-illumination%250Adata%2520by%2520exploiting%2520priors%2520extracted%2520from%25202D%2520image%2520diffusion%2520models.%2520We%2520first%250Afine-tune%2520a%25202D%2520diffusion%2520model%2520on%2520a%2520multi-illumination%2520dataset%2520conditioned%2520by%250Alight%2520direction%252C%2520allowing%2520us%2520to%2520augment%2520a%2520single-illumination%2520capture%2520into%2520a%250Arealistic%2520--%2520but%2520possibly%2520inconsistent%2520--%2520multi-illumination%2520dataset%2520from%250Adirectly%2520defined%2520light%2520directions.%2520We%2520use%2520this%2520augmented%2520data%2520to%2520create%2520a%250Arelightable%2520radiance%2520field%2520represented%2520by%25203D%2520Gaussian%2520splats.%2520To%2520allow%2520direct%250Acontrol%2520of%2520light%2520direction%2520for%2520low-frequency%2520lighting%252C%2520we%2520represent%2520appearance%250Awith%2520a%2520multi-layer%2520perceptron%2520parameterized%2520on%2520light%2520direction.%2520To%2520enforce%250Amulti-view%2520consistency%2520and%2520overcome%2520inaccuracies%2520we%2520optimize%2520a%2520per-image%250Aauxiliary%2520feature%2520vector.%2520We%2520show%2520results%2520on%2520synthetic%2520and%2520real%2520multi-view%2520data%250Aunder%2520single%2520illumination%252C%2520demonstrating%2520that%2520our%2520method%2520successfully%2520exploits%250A2D%2520diffusion%2520model%2520priors%2520to%2520allow%2520realistic%25203D%2520relighting%2520for%2520complete%2520scenes.%250AProject%2520site%250Ahttps%253A//repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Diffusion%20Approach%20to%20Radiance%20Field%20Relighting%20using%0A%20%20Multi-Illumination%20Synthesis&entry.906535625=Yohan%20Poirier-Ginter%20and%20Alban%20Gauthier%20and%20Julien%20Phillip%20and%20Jean-Francois%20Lalonde%20and%20George%20Drettakis&entry.1292438233=%20%20Relighting%20radiance%20fields%20is%20severely%20underconstrained%20for%20multi-view%20data%2C%0Awhich%20is%20most%20often%20captured%20under%20a%20single%20illumination%20condition%3B%20It%20is%0Aespecially%20hard%20for%20full%20scenes%20containing%20multiple%20objects.%20We%20introduce%20a%0Amethod%20to%20create%20relightable%20radiance%20fields%20using%20such%20single-illumination%0Adata%20by%20exploiting%20priors%20extracted%20from%202D%20image%20diffusion%20models.%20We%20first%0Afine-tune%20a%202D%20diffusion%20model%20on%20a%20multi-illumination%20dataset%20conditioned%20by%0Alight%20direction%2C%20allowing%20us%20to%20augment%20a%20single-illumination%20capture%20into%20a%0Arealistic%20--%20but%20possibly%20inconsistent%20--%20multi-illumination%20dataset%20from%0Adirectly%20defined%20light%20directions.%20We%20use%20this%20augmented%20data%20to%20create%20a%0Arelightable%20radiance%20field%20represented%20by%203D%20Gaussian%20splats.%20To%20allow%20direct%0Acontrol%20of%20light%20direction%20for%20low-frequency%20lighting%2C%20we%20represent%20appearance%0Awith%20a%20multi-layer%20perceptron%20parameterized%20on%20light%20direction.%20To%20enforce%0Amulti-view%20consistency%20and%20overcome%20inaccuracies%20we%20optimize%20a%20per-image%0Aauxiliary%20feature%20vector.%20We%20show%20results%20on%20synthetic%20and%20real%20multi-view%20data%0Aunder%20single%20illumination%2C%20demonstrating%20that%20our%20method%20successfully%20exploits%0A2D%20diffusion%20model%20priors%20to%20allow%20realistic%203D%20relighting%20for%20complete%20scenes.%0AProject%20site%0Ahttps%3A//repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08947v1&entry.124074799=Read"},
{"title": "Uncertainty Estimation by Density Aware Evidential Deep Learning", "author": "Taeseong Yoon and Heeyoung Kim", "abstract": "  Evidential deep learning (EDL) has shown remarkable success in uncertainty\nestimation. However, there is still room for improvement, particularly in\nout-of-distribution (OOD) detection and classification tasks. The limited OOD\ndetection performance of EDL arises from its inability to reflect the distance\nbetween the testing example and training data when quantifying uncertainty,\nwhile its limited classification performance stems from its parameterization of\nthe concentration parameters. To address these limitations, we propose a novel\nmethod called Density Aware Evidential Deep Learning (DAEDL). DAEDL integrates\nthe feature space density of the testing example with the output of EDL during\nthe prediction stage, while using a novel parameterization that resolves the\nissues in the conventional parameterization. We prove that DAEDL enjoys a\nnumber of favorable theoretical properties. DAEDL demonstrates state-of-the-art\nperformance across diverse downstream tasks related to uncertainty estimation\nand classification\n", "link": "http://arxiv.org/abs/2409.08754v1", "date": "2024-09-13", "relevancy": 2.3456, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.7213}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5702}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Estimation%20by%20Density%20Aware%20Evidential%20Deep%20Learning&body=Title%3A%20Uncertainty%20Estimation%20by%20Density%20Aware%20Evidential%20Deep%20Learning%0AAuthor%3A%20Taeseong%20Yoon%20and%20Heeyoung%20Kim%0AAbstract%3A%20%20%20Evidential%20deep%20learning%20%28EDL%29%20has%20shown%20remarkable%20success%20in%20uncertainty%0Aestimation.%20However%2C%20there%20is%20still%20room%20for%20improvement%2C%20particularly%20in%0Aout-of-distribution%20%28OOD%29%20detection%20and%20classification%20tasks.%20The%20limited%20OOD%0Adetection%20performance%20of%20EDL%20arises%20from%20its%20inability%20to%20reflect%20the%20distance%0Abetween%20the%20testing%20example%20and%20training%20data%20when%20quantifying%20uncertainty%2C%0Awhile%20its%20limited%20classification%20performance%20stems%20from%20its%20parameterization%20of%0Athe%20concentration%20parameters.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20Density%20Aware%20Evidential%20Deep%20Learning%20%28DAEDL%29.%20DAEDL%20integrates%0Athe%20feature%20space%20density%20of%20the%20testing%20example%20with%20the%20output%20of%20EDL%20during%0Athe%20prediction%20stage%2C%20while%20using%20a%20novel%20parameterization%20that%20resolves%20the%0Aissues%20in%20the%20conventional%20parameterization.%20We%20prove%20that%20DAEDL%20enjoys%20a%0Anumber%20of%20favorable%20theoretical%20properties.%20DAEDL%20demonstrates%20state-of-the-art%0Aperformance%20across%20diverse%20downstream%20tasks%20related%20to%20uncertainty%20estimation%0Aand%20classification%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Estimation%2520by%2520Density%2520Aware%2520Evidential%2520Deep%2520Learning%26entry.906535625%3DTaeseong%2520Yoon%2520and%2520Heeyoung%2520Kim%26entry.1292438233%3D%2520%2520Evidential%2520deep%2520learning%2520%2528EDL%2529%2520has%2520shown%2520remarkable%2520success%2520in%2520uncertainty%250Aestimation.%2520However%252C%2520there%2520is%2520still%2520room%2520for%2520improvement%252C%2520particularly%2520in%250Aout-of-distribution%2520%2528OOD%2529%2520detection%2520and%2520classification%2520tasks.%2520The%2520limited%2520OOD%250Adetection%2520performance%2520of%2520EDL%2520arises%2520from%2520its%2520inability%2520to%2520reflect%2520the%2520distance%250Abetween%2520the%2520testing%2520example%2520and%2520training%2520data%2520when%2520quantifying%2520uncertainty%252C%250Awhile%2520its%2520limited%2520classification%2520performance%2520stems%2520from%2520its%2520parameterization%2520of%250Athe%2520concentration%2520parameters.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Amethod%2520called%2520Density%2520Aware%2520Evidential%2520Deep%2520Learning%2520%2528DAEDL%2529.%2520DAEDL%2520integrates%250Athe%2520feature%2520space%2520density%2520of%2520the%2520testing%2520example%2520with%2520the%2520output%2520of%2520EDL%2520during%250Athe%2520prediction%2520stage%252C%2520while%2520using%2520a%2520novel%2520parameterization%2520that%2520resolves%2520the%250Aissues%2520in%2520the%2520conventional%2520parameterization.%2520We%2520prove%2520that%2520DAEDL%2520enjoys%2520a%250Anumber%2520of%2520favorable%2520theoretical%2520properties.%2520DAEDL%2520demonstrates%2520state-of-the-art%250Aperformance%2520across%2520diverse%2520downstream%2520tasks%2520related%2520to%2520uncertainty%2520estimation%250Aand%2520classification%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Estimation%20by%20Density%20Aware%20Evidential%20Deep%20Learning&entry.906535625=Taeseong%20Yoon%20and%20Heeyoung%20Kim&entry.1292438233=%20%20Evidential%20deep%20learning%20%28EDL%29%20has%20shown%20remarkable%20success%20in%20uncertainty%0Aestimation.%20However%2C%20there%20is%20still%20room%20for%20improvement%2C%20particularly%20in%0Aout-of-distribution%20%28OOD%29%20detection%20and%20classification%20tasks.%20The%20limited%20OOD%0Adetection%20performance%20of%20EDL%20arises%20from%20its%20inability%20to%20reflect%20the%20distance%0Abetween%20the%20testing%20example%20and%20training%20data%20when%20quantifying%20uncertainty%2C%0Awhile%20its%20limited%20classification%20performance%20stems%20from%20its%20parameterization%20of%0Athe%20concentration%20parameters.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20Density%20Aware%20Evidential%20Deep%20Learning%20%28DAEDL%29.%20DAEDL%20integrates%0Athe%20feature%20space%20density%20of%20the%20testing%20example%20with%20the%20output%20of%20EDL%20during%0Athe%20prediction%20stage%2C%20while%20using%20a%20novel%20parameterization%20that%20resolves%20the%0Aissues%20in%20the%20conventional%20parameterization.%20We%20prove%20that%20DAEDL%20enjoys%20a%0Anumber%20of%20favorable%20theoretical%20properties.%20DAEDL%20demonstrates%20state-of-the-art%0Aperformance%20across%20diverse%20downstream%20tasks%20related%20to%20uncertainty%20estimation%0Aand%20classification%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08754v1&entry.124074799=Read"},
{"title": "Revisiting Convergence of AdaGrad with Relaxed Assumptions", "author": "Yusu Hong and Junhong Lin", "abstract": "  In this study, we revisit the convergence of AdaGrad with momentum (covering\nAdaGrad as a special case) on non-convex smooth optimization problems. We\nconsider a general noise model where the noise magnitude is controlled by the\nfunction value gap together with the gradient magnitude. This model encompasses\na broad range of noises including bounded noise, sub-Gaussian noise, affine\nvariance noise and the expected smoothness, and it has been shown to be more\nrealistic in many practical applications. Our analysis yields a probabilistic\nconvergence rate which, under the general noise, could reach at\n(\\tilde{\\mathcal{O}}(1/\\sqrt{T})). This rate does not rely on prior knowledge\nof problem-parameters and could accelerate to (\\tilde{\\mathcal{O}}(1/T)) where\n(T) denotes the total number iterations, when the noise parameters related to\nthe function value gap and noise level are sufficiently small. The convergence\nrate thus matches the lower rate for stochastic first-order methods over\nnon-convex smooth landscape up to logarithm terms [Arjevani et al., 2023]. We\nfurther derive a convergence bound for AdaGrad with mometum, considering the\ngeneralized smoothness where the local smoothness is controlled by a\nfirst-order function of the gradient norm.\n", "link": "http://arxiv.org/abs/2402.13794v2", "date": "2024-09-13", "relevancy": 2.3394, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4819}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4642}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Convergence%20of%20AdaGrad%20with%20Relaxed%20Assumptions&body=Title%3A%20Revisiting%20Convergence%20of%20AdaGrad%20with%20Relaxed%20Assumptions%0AAuthor%3A%20Yusu%20Hong%20and%20Junhong%20Lin%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20revisit%20the%20convergence%20of%20AdaGrad%20with%20momentum%20%28covering%0AAdaGrad%20as%20a%20special%20case%29%20on%20non-convex%20smooth%20optimization%20problems.%20We%0Aconsider%20a%20general%20noise%20model%20where%20the%20noise%20magnitude%20is%20controlled%20by%20the%0Afunction%20value%20gap%20together%20with%20the%20gradient%20magnitude.%20This%20model%20encompasses%0Aa%20broad%20range%20of%20noises%20including%20bounded%20noise%2C%20sub-Gaussian%20noise%2C%20affine%0Avariance%20noise%20and%20the%20expected%20smoothness%2C%20and%20it%20has%20been%20shown%20to%20be%20more%0Arealistic%20in%20many%20practical%20applications.%20Our%20analysis%20yields%20a%20probabilistic%0Aconvergence%20rate%20which%2C%20under%20the%20general%20noise%2C%20could%20reach%20at%0A%28%5Ctilde%7B%5Cmathcal%7BO%7D%7D%281/%5Csqrt%7BT%7D%29%29.%20This%20rate%20does%20not%20rely%20on%20prior%20knowledge%0Aof%20problem-parameters%20and%20could%20accelerate%20to%20%28%5Ctilde%7B%5Cmathcal%7BO%7D%7D%281/T%29%29%20where%0A%28T%29%20denotes%20the%20total%20number%20iterations%2C%20when%20the%20noise%20parameters%20related%20to%0Athe%20function%20value%20gap%20and%20noise%20level%20are%20sufficiently%20small.%20The%20convergence%0Arate%20thus%20matches%20the%20lower%20rate%20for%20stochastic%20first-order%20methods%20over%0Anon-convex%20smooth%20landscape%20up%20to%20logarithm%20terms%20%5BArjevani%20et%20al.%2C%202023%5D.%20We%0Afurther%20derive%20a%20convergence%20bound%20for%20AdaGrad%20with%20mometum%2C%20considering%20the%0Ageneralized%20smoothness%20where%20the%20local%20smoothness%20is%20controlled%20by%20a%0Afirst-order%20function%20of%20the%20gradient%20norm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Convergence%2520of%2520AdaGrad%2520with%2520Relaxed%2520Assumptions%26entry.906535625%3DYusu%2520Hong%2520and%2520Junhong%2520Lin%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520revisit%2520the%2520convergence%2520of%2520AdaGrad%2520with%2520momentum%2520%2528covering%250AAdaGrad%2520as%2520a%2520special%2520case%2529%2520on%2520non-convex%2520smooth%2520optimization%2520problems.%2520We%250Aconsider%2520a%2520general%2520noise%2520model%2520where%2520the%2520noise%2520magnitude%2520is%2520controlled%2520by%2520the%250Afunction%2520value%2520gap%2520together%2520with%2520the%2520gradient%2520magnitude.%2520This%2520model%2520encompasses%250Aa%2520broad%2520range%2520of%2520noises%2520including%2520bounded%2520noise%252C%2520sub-Gaussian%2520noise%252C%2520affine%250Avariance%2520noise%2520and%2520the%2520expected%2520smoothness%252C%2520and%2520it%2520has%2520been%2520shown%2520to%2520be%2520more%250Arealistic%2520in%2520many%2520practical%2520applications.%2520Our%2520analysis%2520yields%2520a%2520probabilistic%250Aconvergence%2520rate%2520which%252C%2520under%2520the%2520general%2520noise%252C%2520could%2520reach%2520at%250A%2528%255Ctilde%257B%255Cmathcal%257BO%257D%257D%25281/%255Csqrt%257BT%257D%2529%2529.%2520This%2520rate%2520does%2520not%2520rely%2520on%2520prior%2520knowledge%250Aof%2520problem-parameters%2520and%2520could%2520accelerate%2520to%2520%2528%255Ctilde%257B%255Cmathcal%257BO%257D%257D%25281/T%2529%2529%2520where%250A%2528T%2529%2520denotes%2520the%2520total%2520number%2520iterations%252C%2520when%2520the%2520noise%2520parameters%2520related%2520to%250Athe%2520function%2520value%2520gap%2520and%2520noise%2520level%2520are%2520sufficiently%2520small.%2520The%2520convergence%250Arate%2520thus%2520matches%2520the%2520lower%2520rate%2520for%2520stochastic%2520first-order%2520methods%2520over%250Anon-convex%2520smooth%2520landscape%2520up%2520to%2520logarithm%2520terms%2520%255BArjevani%2520et%2520al.%252C%25202023%255D.%2520We%250Afurther%2520derive%2520a%2520convergence%2520bound%2520for%2520AdaGrad%2520with%2520mometum%252C%2520considering%2520the%250Ageneralized%2520smoothness%2520where%2520the%2520local%2520smoothness%2520is%2520controlled%2520by%2520a%250Afirst-order%2520function%2520of%2520the%2520gradient%2520norm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Convergence%20of%20AdaGrad%20with%20Relaxed%20Assumptions&entry.906535625=Yusu%20Hong%20and%20Junhong%20Lin&entry.1292438233=%20%20In%20this%20study%2C%20we%20revisit%20the%20convergence%20of%20AdaGrad%20with%20momentum%20%28covering%0AAdaGrad%20as%20a%20special%20case%29%20on%20non-convex%20smooth%20optimization%20problems.%20We%0Aconsider%20a%20general%20noise%20model%20where%20the%20noise%20magnitude%20is%20controlled%20by%20the%0Afunction%20value%20gap%20together%20with%20the%20gradient%20magnitude.%20This%20model%20encompasses%0Aa%20broad%20range%20of%20noises%20including%20bounded%20noise%2C%20sub-Gaussian%20noise%2C%20affine%0Avariance%20noise%20and%20the%20expected%20smoothness%2C%20and%20it%20has%20been%20shown%20to%20be%20more%0Arealistic%20in%20many%20practical%20applications.%20Our%20analysis%20yields%20a%20probabilistic%0Aconvergence%20rate%20which%2C%20under%20the%20general%20noise%2C%20could%20reach%20at%0A%28%5Ctilde%7B%5Cmathcal%7BO%7D%7D%281/%5Csqrt%7BT%7D%29%29.%20This%20rate%20does%20not%20rely%20on%20prior%20knowledge%0Aof%20problem-parameters%20and%20could%20accelerate%20to%20%28%5Ctilde%7B%5Cmathcal%7BO%7D%7D%281/T%29%29%20where%0A%28T%29%20denotes%20the%20total%20number%20iterations%2C%20when%20the%20noise%20parameters%20related%20to%0Athe%20function%20value%20gap%20and%20noise%20level%20are%20sufficiently%20small.%20The%20convergence%0Arate%20thus%20matches%20the%20lower%20rate%20for%20stochastic%20first-order%20methods%20over%0Anon-convex%20smooth%20landscape%20up%20to%20logarithm%20terms%20%5BArjevani%20et%20al.%2C%202023%5D.%20We%0Afurther%20derive%20a%20convergence%20bound%20for%20AdaGrad%20with%20mometum%2C%20considering%20the%0Ageneralized%20smoothness%20where%20the%20local%20smoothness%20is%20controlled%20by%20a%0Afirst-order%20function%20of%20the%20gradient%20norm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13794v2&entry.124074799=Read"},
{"title": "Breaking reCAPTCHAv2", "author": "Andreas Plesner and Tobias Vontobel and Roger Wattenhofer", "abstract": "  Our work examines the efficacy of employing advanced machine learning methods\nto solve captchas from Google's reCAPTCHAv2 system. We evaluate the\neffectiveness of automated systems in solving captchas by utilizing advanced\nYOLO models for image segmentation and classification. Our main result is that\nwe can solve 100% of the captchas, while previous work only solved 68-71%.\nFurthermore, our findings suggest that there is no significant difference in\nthe number of challenges humans and bots must solve to pass the captchas in\nreCAPTCHAv2. This implies that current AI technologies can exploit advanced\nimage-based captchas. We also look under the hood of reCAPTCHAv2, and find\nevidence that reCAPTCHAv2 is heavily based on cookie and browser history data\nwhen evaluating whether a user is human or not. The code is provided alongside\nthis paper.\n", "link": "http://arxiv.org/abs/2409.08831v1", "date": "2024-09-13", "relevancy": 2.2916, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20reCAPTCHAv2&body=Title%3A%20Breaking%20reCAPTCHAv2%0AAuthor%3A%20Andreas%20Plesner%20and%20Tobias%20Vontobel%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20Our%20work%20examines%20the%20efficacy%20of%20employing%20advanced%20machine%20learning%20methods%0Ato%20solve%20captchas%20from%20Google%27s%20reCAPTCHAv2%20system.%20We%20evaluate%20the%0Aeffectiveness%20of%20automated%20systems%20in%20solving%20captchas%20by%20utilizing%20advanced%0AYOLO%20models%20for%20image%20segmentation%20and%20classification.%20Our%20main%20result%20is%20that%0Awe%20can%20solve%20100%25%20of%20the%20captchas%2C%20while%20previous%20work%20only%20solved%2068-71%25.%0AFurthermore%2C%20our%20findings%20suggest%20that%20there%20is%20no%20significant%20difference%20in%0Athe%20number%20of%20challenges%20humans%20and%20bots%20must%20solve%20to%20pass%20the%20captchas%20in%0AreCAPTCHAv2.%20This%20implies%20that%20current%20AI%20technologies%20can%20exploit%20advanced%0Aimage-based%20captchas.%20We%20also%20look%20under%20the%20hood%20of%20reCAPTCHAv2%2C%20and%20find%0Aevidence%20that%20reCAPTCHAv2%20is%20heavily%20based%20on%20cookie%20and%20browser%20history%20data%0Awhen%20evaluating%20whether%20a%20user%20is%20human%20or%20not.%20The%20code%20is%20provided%20alongside%0Athis%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520reCAPTCHAv2%26entry.906535625%3DAndreas%2520Plesner%2520and%2520Tobias%2520Vontobel%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520Our%2520work%2520examines%2520the%2520efficacy%2520of%2520employing%2520advanced%2520machine%2520learning%2520methods%250Ato%2520solve%2520captchas%2520from%2520Google%2527s%2520reCAPTCHAv2%2520system.%2520We%2520evaluate%2520the%250Aeffectiveness%2520of%2520automated%2520systems%2520in%2520solving%2520captchas%2520by%2520utilizing%2520advanced%250AYOLO%2520models%2520for%2520image%2520segmentation%2520and%2520classification.%2520Our%2520main%2520result%2520is%2520that%250Awe%2520can%2520solve%2520100%2525%2520of%2520the%2520captchas%252C%2520while%2520previous%2520work%2520only%2520solved%252068-71%2525.%250AFurthermore%252C%2520our%2520findings%2520suggest%2520that%2520there%2520is%2520no%2520significant%2520difference%2520in%250Athe%2520number%2520of%2520challenges%2520humans%2520and%2520bots%2520must%2520solve%2520to%2520pass%2520the%2520captchas%2520in%250AreCAPTCHAv2.%2520This%2520implies%2520that%2520current%2520AI%2520technologies%2520can%2520exploit%2520advanced%250Aimage-based%2520captchas.%2520We%2520also%2520look%2520under%2520the%2520hood%2520of%2520reCAPTCHAv2%252C%2520and%2520find%250Aevidence%2520that%2520reCAPTCHAv2%2520is%2520heavily%2520based%2520on%2520cookie%2520and%2520browser%2520history%2520data%250Awhen%2520evaluating%2520whether%2520a%2520user%2520is%2520human%2520or%2520not.%2520The%2520code%2520is%2520provided%2520alongside%250Athis%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20reCAPTCHAv2&entry.906535625=Andreas%20Plesner%20and%20Tobias%20Vontobel%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20Our%20work%20examines%20the%20efficacy%20of%20employing%20advanced%20machine%20learning%20methods%0Ato%20solve%20captchas%20from%20Google%27s%20reCAPTCHAv2%20system.%20We%20evaluate%20the%0Aeffectiveness%20of%20automated%20systems%20in%20solving%20captchas%20by%20utilizing%20advanced%0AYOLO%20models%20for%20image%20segmentation%20and%20classification.%20Our%20main%20result%20is%20that%0Awe%20can%20solve%20100%25%20of%20the%20captchas%2C%20while%20previous%20work%20only%20solved%2068-71%25.%0AFurthermore%2C%20our%20findings%20suggest%20that%20there%20is%20no%20significant%20difference%20in%0Athe%20number%20of%20challenges%20humans%20and%20bots%20must%20solve%20to%20pass%20the%20captchas%20in%0AreCAPTCHAv2.%20This%20implies%20that%20current%20AI%20technologies%20can%20exploit%20advanced%0Aimage-based%20captchas.%20We%20also%20look%20under%20the%20hood%20of%20reCAPTCHAv2%2C%20and%20find%0Aevidence%20that%20reCAPTCHAv2%20is%20heavily%20based%20on%20cookie%20and%20browser%20history%20data%0Awhen%20evaluating%20whether%20a%20user%20is%20human%20or%20not.%20The%20code%20is%20provided%20alongside%0Athis%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08831v1&entry.124074799=Read"},
{"title": "Pathfinder for Low-altitude Aircraft with Binary Neural Network", "author": "Kaijie Yin and Tian Gao and Hui Kong", "abstract": "  A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the\nperformance of autonomous mapping by a ground mobile robot. However, the prior\nmap is usually incomplete due to lacking labeling in partial paths. To solve\nthis problem, this paper proposes an OSM maker using airborne sensors carried\nby low-altitude aircraft, where the core of the OSM maker is a novel efficient\npathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream\nroad segmentation model. Specifically, a multi-scale feature extraction based\non the UNet architecture is implemented for images and point clouds. To reduce\nthe effect caused by the sparsity of point cloud, an attention-guided gated\nblock is designed to integrate image and point-cloud features. For enhancing\nthe efficiency of the model, we propose a binarization streamline to each model\ncomponent, including a variant of vision transformer (ViT) architecture as the\nencoder of the image branch, and new focal and perception losses to optimize\nthe model training. The experimental results on two datasets demonstrate that\nour pathfinder method achieves SOTA accuracy with high efficiency in finding\npaths from the low-level airborne sensors, and we can create complete OSM prior\nmaps based on the segmented road skeletons. Code and data are available\nat:https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder.\n", "link": "http://arxiv.org/abs/2409.08824v1", "date": "2024-09-13", "relevancy": 2.2916, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6236}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5739}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pathfinder%20for%20Low-altitude%20Aircraft%20with%20Binary%20Neural%20Network&body=Title%3A%20Pathfinder%20for%20Low-altitude%20Aircraft%20with%20Binary%20Neural%20Network%0AAuthor%3A%20Kaijie%20Yin%20and%20Tian%20Gao%20and%20Hui%20Kong%0AAbstract%3A%20%20%20A%20prior%20global%20topological%20map%20%28e.g.%2C%20the%20OpenStreetMap%2C%20OSM%29%20can%20boost%20the%0Aperformance%20of%20autonomous%20mapping%20by%20a%20ground%20mobile%20robot.%20However%2C%20the%20prior%0Amap%20is%20usually%20incomplete%20due%20to%20lacking%20labeling%20in%20partial%20paths.%20To%20solve%0Athis%20problem%2C%20this%20paper%20proposes%20an%20OSM%20maker%20using%20airborne%20sensors%20carried%0Aby%20low-altitude%20aircraft%2C%20where%20the%20core%20of%20the%20OSM%20maker%20is%20a%20novel%20efficient%0Apathfinder%20approach%20based%20on%20LiDAR%20and%20camera%20data%2C%20i.e.%2C%20a%20binary%20dual-stream%0Aroad%20segmentation%20model.%20Specifically%2C%20a%20multi-scale%20feature%20extraction%20based%0Aon%20the%20UNet%20architecture%20is%20implemented%20for%20images%20and%20point%20clouds.%20To%20reduce%0Athe%20effect%20caused%20by%20the%20sparsity%20of%20point%20cloud%2C%20an%20attention-guided%20gated%0Ablock%20is%20designed%20to%20integrate%20image%20and%20point-cloud%20features.%20For%20enhancing%0Athe%20efficiency%20of%20the%20model%2C%20we%20propose%20a%20binarization%20streamline%20to%20each%20model%0Acomponent%2C%20including%20a%20variant%20of%20vision%20transformer%20%28ViT%29%20architecture%20as%20the%0Aencoder%20of%20the%20image%20branch%2C%20and%20new%20focal%20and%20perception%20losses%20to%20optimize%0Athe%20model%20training.%20The%20experimental%20results%20on%20two%20datasets%20demonstrate%20that%0Aour%20pathfinder%20method%20achieves%20SOTA%20accuracy%20with%20high%20efficiency%20in%20finding%0Apaths%20from%20the%20low-level%20airborne%20sensors%2C%20and%20we%20can%20create%20complete%20OSM%20prior%0Amaps%20based%20on%20the%20segmented%20road%20skeletons.%20Code%20and%20data%20are%20available%0Aat%3Ahttps%3A//github.com/IMRL/Pathfinder%7D%7Bhttps%3A//github.com/IMRL/Pathfinder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathfinder%2520for%2520Low-altitude%2520Aircraft%2520with%2520Binary%2520Neural%2520Network%26entry.906535625%3DKaijie%2520Yin%2520and%2520Tian%2520Gao%2520and%2520Hui%2520Kong%26entry.1292438233%3D%2520%2520A%2520prior%2520global%2520topological%2520map%2520%2528e.g.%252C%2520the%2520OpenStreetMap%252C%2520OSM%2529%2520can%2520boost%2520the%250Aperformance%2520of%2520autonomous%2520mapping%2520by%2520a%2520ground%2520mobile%2520robot.%2520However%252C%2520the%2520prior%250Amap%2520is%2520usually%2520incomplete%2520due%2520to%2520lacking%2520labeling%2520in%2520partial%2520paths.%2520To%2520solve%250Athis%2520problem%252C%2520this%2520paper%2520proposes%2520an%2520OSM%2520maker%2520using%2520airborne%2520sensors%2520carried%250Aby%2520low-altitude%2520aircraft%252C%2520where%2520the%2520core%2520of%2520the%2520OSM%2520maker%2520is%2520a%2520novel%2520efficient%250Apathfinder%2520approach%2520based%2520on%2520LiDAR%2520and%2520camera%2520data%252C%2520i.e.%252C%2520a%2520binary%2520dual-stream%250Aroad%2520segmentation%2520model.%2520Specifically%252C%2520a%2520multi-scale%2520feature%2520extraction%2520based%250Aon%2520the%2520UNet%2520architecture%2520is%2520implemented%2520for%2520images%2520and%2520point%2520clouds.%2520To%2520reduce%250Athe%2520effect%2520caused%2520by%2520the%2520sparsity%2520of%2520point%2520cloud%252C%2520an%2520attention-guided%2520gated%250Ablock%2520is%2520designed%2520to%2520integrate%2520image%2520and%2520point-cloud%2520features.%2520For%2520enhancing%250Athe%2520efficiency%2520of%2520the%2520model%252C%2520we%2520propose%2520a%2520binarization%2520streamline%2520to%2520each%2520model%250Acomponent%252C%2520including%2520a%2520variant%2520of%2520vision%2520transformer%2520%2528ViT%2529%2520architecture%2520as%2520the%250Aencoder%2520of%2520the%2520image%2520branch%252C%2520and%2520new%2520focal%2520and%2520perception%2520losses%2520to%2520optimize%250Athe%2520model%2520training.%2520The%2520experimental%2520results%2520on%2520two%2520datasets%2520demonstrate%2520that%250Aour%2520pathfinder%2520method%2520achieves%2520SOTA%2520accuracy%2520with%2520high%2520efficiency%2520in%2520finding%250Apaths%2520from%2520the%2520low-level%2520airborne%2520sensors%252C%2520and%2520we%2520can%2520create%2520complete%2520OSM%2520prior%250Amaps%2520based%2520on%2520the%2520segmented%2520road%2520skeletons.%2520Code%2520and%2520data%2520are%2520available%250Aat%253Ahttps%253A//github.com/IMRL/Pathfinder%257D%257Bhttps%253A//github.com/IMRL/Pathfinder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pathfinder%20for%20Low-altitude%20Aircraft%20with%20Binary%20Neural%20Network&entry.906535625=Kaijie%20Yin%20and%20Tian%20Gao%20and%20Hui%20Kong&entry.1292438233=%20%20A%20prior%20global%20topological%20map%20%28e.g.%2C%20the%20OpenStreetMap%2C%20OSM%29%20can%20boost%20the%0Aperformance%20of%20autonomous%20mapping%20by%20a%20ground%20mobile%20robot.%20However%2C%20the%20prior%0Amap%20is%20usually%20incomplete%20due%20to%20lacking%20labeling%20in%20partial%20paths.%20To%20solve%0Athis%20problem%2C%20this%20paper%20proposes%20an%20OSM%20maker%20using%20airborne%20sensors%20carried%0Aby%20low-altitude%20aircraft%2C%20where%20the%20core%20of%20the%20OSM%20maker%20is%20a%20novel%20efficient%0Apathfinder%20approach%20based%20on%20LiDAR%20and%20camera%20data%2C%20i.e.%2C%20a%20binary%20dual-stream%0Aroad%20segmentation%20model.%20Specifically%2C%20a%20multi-scale%20feature%20extraction%20based%0Aon%20the%20UNet%20architecture%20is%20implemented%20for%20images%20and%20point%20clouds.%20To%20reduce%0Athe%20effect%20caused%20by%20the%20sparsity%20of%20point%20cloud%2C%20an%20attention-guided%20gated%0Ablock%20is%20designed%20to%20integrate%20image%20and%20point-cloud%20features.%20For%20enhancing%0Athe%20efficiency%20of%20the%20model%2C%20we%20propose%20a%20binarization%20streamline%20to%20each%20model%0Acomponent%2C%20including%20a%20variant%20of%20vision%20transformer%20%28ViT%29%20architecture%20as%20the%0Aencoder%20of%20the%20image%20branch%2C%20and%20new%20focal%20and%20perception%20losses%20to%20optimize%0Athe%20model%20training.%20The%20experimental%20results%20on%20two%20datasets%20demonstrate%20that%0Aour%20pathfinder%20method%20achieves%20SOTA%20accuracy%20with%20high%20efficiency%20in%20finding%0Apaths%20from%20the%20low-level%20airborne%20sensors%2C%20and%20we%20can%20create%20complete%20OSM%20prior%0Amaps%20based%20on%20the%20segmented%20road%20skeletons.%20Code%20and%20data%20are%20available%0Aat%3Ahttps%3A//github.com/IMRL/Pathfinder%7D%7Bhttps%3A//github.com/IMRL/Pathfinder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08824v1&entry.124074799=Read"},
{"title": "SpanSeq: Similarity-based sequence data splitting method for improved\n  development and assessment of deep learning projects", "author": "Alfred Ferrer Florensa and Jose Juan Almagro Armenteros and Henrik Nielsen and Frank M\u00f8ller Aarestrup and Philip Thomas Lanken Conradsen Clausen", "abstract": "  The use of deep learning models in computational biology has increased\nmassively in recent years, and it is expected to continue with the current\nadvances in the fields such as Natural Language Processing. These models,\nalthough able to draw complex relations between input and target, are also\ninclined to learn noisy deviations from the pool of data used during their\ndevelopment. In order to assess their performance on unseen data (their\ncapacity to generalize), it is common to split the available data randomly into\ndevelopment (train/validation) and test sets. This procedure, although\nstandard, has been shown to produce dubious assessments of generalization due\nto the existing similarity between samples in the databases used. In this work,\nwe present SpanSeq, a database partition method for machine learning that can\nscale to most biological sequences (genes, proteins and genomes) in order to\navoid data leakage between sets. We also explore the effect of not restraining\nsimilarity between sets by reproducing the development of two state-of-the-art\nmodels on bioinformatics, not only confirming the consequences of randomly\nsplitting databases on the model assessment, but expanding those repercussions\nto the model development. SpanSeq is available at\nhttps://github.com/genomicepidemiology/SpanSeq.\n", "link": "http://arxiv.org/abs/2402.14482v3", "date": "2024-09-13", "relevancy": 2.2502, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4528}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpanSeq%3A%20Similarity-based%20sequence%20data%20splitting%20method%20for%20improved%0A%20%20development%20and%20assessment%20of%20deep%20learning%20projects&body=Title%3A%20SpanSeq%3A%20Similarity-based%20sequence%20data%20splitting%20method%20for%20improved%0A%20%20development%20and%20assessment%20of%20deep%20learning%20projects%0AAuthor%3A%20Alfred%20Ferrer%20Florensa%20and%20Jose%20Juan%20Almagro%20Armenteros%20and%20Henrik%20Nielsen%20and%20Frank%20M%C3%B8ller%20Aarestrup%20and%20Philip%20Thomas%20Lanken%20Conradsen%20Clausen%0AAbstract%3A%20%20%20The%20use%20of%20deep%20learning%20models%20in%20computational%20biology%20has%20increased%0Amassively%20in%20recent%20years%2C%20and%20it%20is%20expected%20to%20continue%20with%20the%20current%0Aadvances%20in%20the%20fields%20such%20as%20Natural%20Language%20Processing.%20These%20models%2C%0Aalthough%20able%20to%20draw%20complex%20relations%20between%20input%20and%20target%2C%20are%20also%0Ainclined%20to%20learn%20noisy%20deviations%20from%20the%20pool%20of%20data%20used%20during%20their%0Adevelopment.%20In%20order%20to%20assess%20their%20performance%20on%20unseen%20data%20%28their%0Acapacity%20to%20generalize%29%2C%20it%20is%20common%20to%20split%20the%20available%20data%20randomly%20into%0Adevelopment%20%28train/validation%29%20and%20test%20sets.%20This%20procedure%2C%20although%0Astandard%2C%20has%20been%20shown%20to%20produce%20dubious%20assessments%20of%20generalization%20due%0Ato%20the%20existing%20similarity%20between%20samples%20in%20the%20databases%20used.%20In%20this%20work%2C%0Awe%20present%20SpanSeq%2C%20a%20database%20partition%20method%20for%20machine%20learning%20that%20can%0Ascale%20to%20most%20biological%20sequences%20%28genes%2C%20proteins%20and%20genomes%29%20in%20order%20to%0Aavoid%20data%20leakage%20between%20sets.%20We%20also%20explore%20the%20effect%20of%20not%20restraining%0Asimilarity%20between%20sets%20by%20reproducing%20the%20development%20of%20two%20state-of-the-art%0Amodels%20on%20bioinformatics%2C%20not%20only%20confirming%20the%20consequences%20of%20randomly%0Asplitting%20databases%20on%20the%20model%20assessment%2C%20but%20expanding%20those%20repercussions%0Ato%20the%20model%20development.%20SpanSeq%20is%20available%20at%0Ahttps%3A//github.com/genomicepidemiology/SpanSeq.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14482v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpanSeq%253A%2520Similarity-based%2520sequence%2520data%2520splitting%2520method%2520for%2520improved%250A%2520%2520development%2520and%2520assessment%2520of%2520deep%2520learning%2520projects%26entry.906535625%3DAlfred%2520Ferrer%2520Florensa%2520and%2520Jose%2520Juan%2520Almagro%2520Armenteros%2520and%2520Henrik%2520Nielsen%2520and%2520Frank%2520M%25C3%25B8ller%2520Aarestrup%2520and%2520Philip%2520Thomas%2520Lanken%2520Conradsen%2520Clausen%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520deep%2520learning%2520models%2520in%2520computational%2520biology%2520has%2520increased%250Amassively%2520in%2520recent%2520years%252C%2520and%2520it%2520is%2520expected%2520to%2520continue%2520with%2520the%2520current%250Aadvances%2520in%2520the%2520fields%2520such%2520as%2520Natural%2520Language%2520Processing.%2520These%2520models%252C%250Aalthough%2520able%2520to%2520draw%2520complex%2520relations%2520between%2520input%2520and%2520target%252C%2520are%2520also%250Ainclined%2520to%2520learn%2520noisy%2520deviations%2520from%2520the%2520pool%2520of%2520data%2520used%2520during%2520their%250Adevelopment.%2520In%2520order%2520to%2520assess%2520their%2520performance%2520on%2520unseen%2520data%2520%2528their%250Acapacity%2520to%2520generalize%2529%252C%2520it%2520is%2520common%2520to%2520split%2520the%2520available%2520data%2520randomly%2520into%250Adevelopment%2520%2528train/validation%2529%2520and%2520test%2520sets.%2520This%2520procedure%252C%2520although%250Astandard%252C%2520has%2520been%2520shown%2520to%2520produce%2520dubious%2520assessments%2520of%2520generalization%2520due%250Ato%2520the%2520existing%2520similarity%2520between%2520samples%2520in%2520the%2520databases%2520used.%2520In%2520this%2520work%252C%250Awe%2520present%2520SpanSeq%252C%2520a%2520database%2520partition%2520method%2520for%2520machine%2520learning%2520that%2520can%250Ascale%2520to%2520most%2520biological%2520sequences%2520%2528genes%252C%2520proteins%2520and%2520genomes%2529%2520in%2520order%2520to%250Aavoid%2520data%2520leakage%2520between%2520sets.%2520We%2520also%2520explore%2520the%2520effect%2520of%2520not%2520restraining%250Asimilarity%2520between%2520sets%2520by%2520reproducing%2520the%2520development%2520of%2520two%2520state-of-the-art%250Amodels%2520on%2520bioinformatics%252C%2520not%2520only%2520confirming%2520the%2520consequences%2520of%2520randomly%250Asplitting%2520databases%2520on%2520the%2520model%2520assessment%252C%2520but%2520expanding%2520those%2520repercussions%250Ato%2520the%2520model%2520development.%2520SpanSeq%2520is%2520available%2520at%250Ahttps%253A//github.com/genomicepidemiology/SpanSeq.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14482v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpanSeq%3A%20Similarity-based%20sequence%20data%20splitting%20method%20for%20improved%0A%20%20development%20and%20assessment%20of%20deep%20learning%20projects&entry.906535625=Alfred%20Ferrer%20Florensa%20and%20Jose%20Juan%20Almagro%20Armenteros%20and%20Henrik%20Nielsen%20and%20Frank%20M%C3%B8ller%20Aarestrup%20and%20Philip%20Thomas%20Lanken%20Conradsen%20Clausen&entry.1292438233=%20%20The%20use%20of%20deep%20learning%20models%20in%20computational%20biology%20has%20increased%0Amassively%20in%20recent%20years%2C%20and%20it%20is%20expected%20to%20continue%20with%20the%20current%0Aadvances%20in%20the%20fields%20such%20as%20Natural%20Language%20Processing.%20These%20models%2C%0Aalthough%20able%20to%20draw%20complex%20relations%20between%20input%20and%20target%2C%20are%20also%0Ainclined%20to%20learn%20noisy%20deviations%20from%20the%20pool%20of%20data%20used%20during%20their%0Adevelopment.%20In%20order%20to%20assess%20their%20performance%20on%20unseen%20data%20%28their%0Acapacity%20to%20generalize%29%2C%20it%20is%20common%20to%20split%20the%20available%20data%20randomly%20into%0Adevelopment%20%28train/validation%29%20and%20test%20sets.%20This%20procedure%2C%20although%0Astandard%2C%20has%20been%20shown%20to%20produce%20dubious%20assessments%20of%20generalization%20due%0Ato%20the%20existing%20similarity%20between%20samples%20in%20the%20databases%20used.%20In%20this%20work%2C%0Awe%20present%20SpanSeq%2C%20a%20database%20partition%20method%20for%20machine%20learning%20that%20can%0Ascale%20to%20most%20biological%20sequences%20%28genes%2C%20proteins%20and%20genomes%29%20in%20order%20to%0Aavoid%20data%20leakage%20between%20sets.%20We%20also%20explore%20the%20effect%20of%20not%20restraining%0Asimilarity%20between%20sets%20by%20reproducing%20the%20development%20of%20two%20state-of-the-art%0Amodels%20on%20bioinformatics%2C%20not%20only%20confirming%20the%20consequences%20of%20randomly%0Asplitting%20databases%20on%20the%20model%20assessment%2C%20but%20expanding%20those%20repercussions%0Ato%20the%20model%20development.%20SpanSeq%20is%20available%20at%0Ahttps%3A//github.com/genomicepidemiology/SpanSeq.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14482v3&entry.124074799=Read"},
{"title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with\n  Memoryless Stochastic Optimal Control", "author": "Carles Domingo-Enrich and Michal Drozdzal and Brian Karrer and Ricky T. Q. Chen", "abstract": "  Dynamical generative models that produce samples through an iterative\nprocess, such as Flow Matching and denoising diffusion models, have seen\nwidespread use, but there has not been many theoretically-sound methods for\nimproving these models with reward fine-tuning. In this work, we cast reward\nfine-tuning as stochastic optimal control (SOC). Critically, we prove that a\nvery specific memoryless noise schedule must be enforced during fine-tuning, in\norder to account for the dependency between the noise variable and the\ngenerated samples. We also propose a new algorithm named Adjoint Matching which\noutperforms existing SOC algorithms, by casting SOC problems as a regression\nproblem. We find that our approach significantly improves over existing methods\nfor reward fine-tuning, achieving better consistency, realism, and\ngeneralization to unseen human preference reward models, while retaining sample\ndiversity.\n", "link": "http://arxiv.org/abs/2409.08861v1", "date": "2024-09-13", "relevancy": 2.2241, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5631}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5557}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adjoint%20Matching%3A%20Fine-tuning%20Flow%20and%20Diffusion%20Generative%20Models%20with%0A%20%20Memoryless%20Stochastic%20Optimal%20Control&body=Title%3A%20Adjoint%20Matching%3A%20Fine-tuning%20Flow%20and%20Diffusion%20Generative%20Models%20with%0A%20%20Memoryless%20Stochastic%20Optimal%20Control%0AAuthor%3A%20Carles%20Domingo-Enrich%20and%20Michal%20Drozdzal%20and%20Brian%20Karrer%20and%20Ricky%20T.%20Q.%20Chen%0AAbstract%3A%20%20%20Dynamical%20generative%20models%20that%20produce%20samples%20through%20an%20iterative%0Aprocess%2C%20such%20as%20Flow%20Matching%20and%20denoising%20diffusion%20models%2C%20have%20seen%0Awidespread%20use%2C%20but%20there%20has%20not%20been%20many%20theoretically-sound%20methods%20for%0Aimproving%20these%20models%20with%20reward%20fine-tuning.%20In%20this%20work%2C%20we%20cast%20reward%0Afine-tuning%20as%20stochastic%20optimal%20control%20%28SOC%29.%20Critically%2C%20we%20prove%20that%20a%0Avery%20specific%20memoryless%20noise%20schedule%20must%20be%20enforced%20during%20fine-tuning%2C%20in%0Aorder%20to%20account%20for%20the%20dependency%20between%20the%20noise%20variable%20and%20the%0Agenerated%20samples.%20We%20also%20propose%20a%20new%20algorithm%20named%20Adjoint%20Matching%20which%0Aoutperforms%20existing%20SOC%20algorithms%2C%20by%20casting%20SOC%20problems%20as%20a%20regression%0Aproblem.%20We%20find%20that%20our%20approach%20significantly%20improves%20over%20existing%20methods%0Afor%20reward%20fine-tuning%2C%20achieving%20better%20consistency%2C%20realism%2C%20and%0Ageneralization%20to%20unseen%20human%20preference%20reward%20models%2C%20while%20retaining%20sample%0Adiversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdjoint%2520Matching%253A%2520Fine-tuning%2520Flow%2520and%2520Diffusion%2520Generative%2520Models%2520with%250A%2520%2520Memoryless%2520Stochastic%2520Optimal%2520Control%26entry.906535625%3DCarles%2520Domingo-Enrich%2520and%2520Michal%2520Drozdzal%2520and%2520Brian%2520Karrer%2520and%2520Ricky%2520T.%2520Q.%2520Chen%26entry.1292438233%3D%2520%2520Dynamical%2520generative%2520models%2520that%2520produce%2520samples%2520through%2520an%2520iterative%250Aprocess%252C%2520such%2520as%2520Flow%2520Matching%2520and%2520denoising%2520diffusion%2520models%252C%2520have%2520seen%250Awidespread%2520use%252C%2520but%2520there%2520has%2520not%2520been%2520many%2520theoretically-sound%2520methods%2520for%250Aimproving%2520these%2520models%2520with%2520reward%2520fine-tuning.%2520In%2520this%2520work%252C%2520we%2520cast%2520reward%250Afine-tuning%2520as%2520stochastic%2520optimal%2520control%2520%2528SOC%2529.%2520Critically%252C%2520we%2520prove%2520that%2520a%250Avery%2520specific%2520memoryless%2520noise%2520schedule%2520must%2520be%2520enforced%2520during%2520fine-tuning%252C%2520in%250Aorder%2520to%2520account%2520for%2520the%2520dependency%2520between%2520the%2520noise%2520variable%2520and%2520the%250Agenerated%2520samples.%2520We%2520also%2520propose%2520a%2520new%2520algorithm%2520named%2520Adjoint%2520Matching%2520which%250Aoutperforms%2520existing%2520SOC%2520algorithms%252C%2520by%2520casting%2520SOC%2520problems%2520as%2520a%2520regression%250Aproblem.%2520We%2520find%2520that%2520our%2520approach%2520significantly%2520improves%2520over%2520existing%2520methods%250Afor%2520reward%2520fine-tuning%252C%2520achieving%2520better%2520consistency%252C%2520realism%252C%2520and%250Ageneralization%2520to%2520unseen%2520human%2520preference%2520reward%2520models%252C%2520while%2520retaining%2520sample%250Adiversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adjoint%20Matching%3A%20Fine-tuning%20Flow%20and%20Diffusion%20Generative%20Models%20with%0A%20%20Memoryless%20Stochastic%20Optimal%20Control&entry.906535625=Carles%20Domingo-Enrich%20and%20Michal%20Drozdzal%20and%20Brian%20Karrer%20and%20Ricky%20T.%20Q.%20Chen&entry.1292438233=%20%20Dynamical%20generative%20models%20that%20produce%20samples%20through%20an%20iterative%0Aprocess%2C%20such%20as%20Flow%20Matching%20and%20denoising%20diffusion%20models%2C%20have%20seen%0Awidespread%20use%2C%20but%20there%20has%20not%20been%20many%20theoretically-sound%20methods%20for%0Aimproving%20these%20models%20with%20reward%20fine-tuning.%20In%20this%20work%2C%20we%20cast%20reward%0Afine-tuning%20as%20stochastic%20optimal%20control%20%28SOC%29.%20Critically%2C%20we%20prove%20that%20a%0Avery%20specific%20memoryless%20noise%20schedule%20must%20be%20enforced%20during%20fine-tuning%2C%20in%0Aorder%20to%20account%20for%20the%20dependency%20between%20the%20noise%20variable%20and%20the%0Agenerated%20samples.%20We%20also%20propose%20a%20new%20algorithm%20named%20Adjoint%20Matching%20which%0Aoutperforms%20existing%20SOC%20algorithms%2C%20by%20casting%20SOC%20problems%20as%20a%20regression%0Aproblem.%20We%20find%20that%20our%20approach%20significantly%20improves%20over%20existing%20methods%0Afor%20reward%20fine-tuning%2C%20achieving%20better%20consistency%2C%20realism%2C%20and%0Ageneralization%20to%20unseen%20human%20preference%20reward%20models%2C%20while%20retaining%20sample%0Adiversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08861v1&entry.124074799=Read"},
{"title": "Diffusion-driven lensless fiber endomicroscopic quantitative phase\n  imaging towards digital pathology", "author": "Zhaoqing Chen and Jiawei Sun and Xinyi Ye and Bin Zhao and Xuelong Li and Juergen Czarske", "abstract": "  Lensless fiber endomicroscope is an emerging tool for in-vivo microscopic\nimaging, where quantitative phase imaging (QPI) can be utilized as a label-free\nmethod to enhance image contrast. However, existing single-shot phase\nreconstruction methods through lensless fiber endomicroscope typically perform\nwell on simple images but struggle with complex microscopic structures. Here,\nwe propose a speckle-conditioned diffusion model (SpecDiffusion), which\nreconstructs phase images directly from speckles captured at the detection side\nof a multi-core fiber (MCF). Unlike conventional neural networks, SpecDiffusion\nemploys iterative phase denoising steps for speckle-driven phase\nreconstruction. The iteration scheme allows SpecDiffusion to break down the\nphase reconstruction process into multiple steps, gradually building up to the\nfinal phase image. This attribute alleviates the computation challenge at each\nstep and enables the reconstruction of rich details in complex microscopic\nimages. To validate its efficacy, we build an optical system to capture\nspeckles from MCF and construct a dataset consisting of 100,000 paired images.\nSpecDiffusion provides high-fidelity phase reconstruction results and shows\npowerful generalization capacity for unseen objects, such as test charts and\nbiological tissues, reducing the average mean absolute error of the\nreconstructed tissue images by 7 times. Furthermore, the reconstructed tissue\nimages using SpecDiffusion shows higher accuracy in zero-shot cell segmentation\ntasks compared to the conventional method, demonstrating the potential for\nfurther cell morphology analysis through the learning-based lensless fiber\nendomicroscope. SpecDiffusion offers a precise and generalized method to phase\nreconstruction through scattering media, including MCFs, opening new\nperspective in lensless fiber endomicroscopic imaging.\n", "link": "http://arxiv.org/abs/2407.18456v2", "date": "2024-09-13", "relevancy": 2.2159, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5776}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5493}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-driven%20lensless%20fiber%20endomicroscopic%20quantitative%20phase%0A%20%20imaging%20towards%20digital%20pathology&body=Title%3A%20Diffusion-driven%20lensless%20fiber%20endomicroscopic%20quantitative%20phase%0A%20%20imaging%20towards%20digital%20pathology%0AAuthor%3A%20Zhaoqing%20Chen%20and%20Jiawei%20Sun%20and%20Xinyi%20Ye%20and%20Bin%20Zhao%20and%20Xuelong%20Li%20and%20Juergen%20Czarske%0AAbstract%3A%20%20%20Lensless%20fiber%20endomicroscope%20is%20an%20emerging%20tool%20for%20in-vivo%20microscopic%0Aimaging%2C%20where%20quantitative%20phase%20imaging%20%28QPI%29%20can%20be%20utilized%20as%20a%20label-free%0Amethod%20to%20enhance%20image%20contrast.%20However%2C%20existing%20single-shot%20phase%0Areconstruction%20methods%20through%20lensless%20fiber%20endomicroscope%20typically%20perform%0Awell%20on%20simple%20images%20but%20struggle%20with%20complex%20microscopic%20structures.%20Here%2C%0Awe%20propose%20a%20speckle-conditioned%20diffusion%20model%20%28SpecDiffusion%29%2C%20which%0Areconstructs%20phase%20images%20directly%20from%20speckles%20captured%20at%20the%20detection%20side%0Aof%20a%20multi-core%20fiber%20%28MCF%29.%20Unlike%20conventional%20neural%20networks%2C%20SpecDiffusion%0Aemploys%20iterative%20phase%20denoising%20steps%20for%20speckle-driven%20phase%0Areconstruction.%20The%20iteration%20scheme%20allows%20SpecDiffusion%20to%20break%20down%20the%0Aphase%20reconstruction%20process%20into%20multiple%20steps%2C%20gradually%20building%20up%20to%20the%0Afinal%20phase%20image.%20This%20attribute%20alleviates%20the%20computation%20challenge%20at%20each%0Astep%20and%20enables%20the%20reconstruction%20of%20rich%20details%20in%20complex%20microscopic%0Aimages.%20To%20validate%20its%20efficacy%2C%20we%20build%20an%20optical%20system%20to%20capture%0Aspeckles%20from%20MCF%20and%20construct%20a%20dataset%20consisting%20of%20100%2C000%20paired%20images.%0ASpecDiffusion%20provides%20high-fidelity%20phase%20reconstruction%20results%20and%20shows%0Apowerful%20generalization%20capacity%20for%20unseen%20objects%2C%20such%20as%20test%20charts%20and%0Abiological%20tissues%2C%20reducing%20the%20average%20mean%20absolute%20error%20of%20the%0Areconstructed%20tissue%20images%20by%207%20times.%20Furthermore%2C%20the%20reconstructed%20tissue%0Aimages%20using%20SpecDiffusion%20shows%20higher%20accuracy%20in%20zero-shot%20cell%20segmentation%0Atasks%20compared%20to%20the%20conventional%20method%2C%20demonstrating%20the%20potential%20for%0Afurther%20cell%20morphology%20analysis%20through%20the%20learning-based%20lensless%20fiber%0Aendomicroscope.%20SpecDiffusion%20offers%20a%20precise%20and%20generalized%20method%20to%20phase%0Areconstruction%20through%20scattering%20media%2C%20including%20MCFs%2C%20opening%20new%0Aperspective%20in%20lensless%20fiber%20endomicroscopic%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-driven%2520lensless%2520fiber%2520endomicroscopic%2520quantitative%2520phase%250A%2520%2520imaging%2520towards%2520digital%2520pathology%26entry.906535625%3DZhaoqing%2520Chen%2520and%2520Jiawei%2520Sun%2520and%2520Xinyi%2520Ye%2520and%2520Bin%2520Zhao%2520and%2520Xuelong%2520Li%2520and%2520Juergen%2520Czarske%26entry.1292438233%3D%2520%2520Lensless%2520fiber%2520endomicroscope%2520is%2520an%2520emerging%2520tool%2520for%2520in-vivo%2520microscopic%250Aimaging%252C%2520where%2520quantitative%2520phase%2520imaging%2520%2528QPI%2529%2520can%2520be%2520utilized%2520as%2520a%2520label-free%250Amethod%2520to%2520enhance%2520image%2520contrast.%2520However%252C%2520existing%2520single-shot%2520phase%250Areconstruction%2520methods%2520through%2520lensless%2520fiber%2520endomicroscope%2520typically%2520perform%250Awell%2520on%2520simple%2520images%2520but%2520struggle%2520with%2520complex%2520microscopic%2520structures.%2520Here%252C%250Awe%2520propose%2520a%2520speckle-conditioned%2520diffusion%2520model%2520%2528SpecDiffusion%2529%252C%2520which%250Areconstructs%2520phase%2520images%2520directly%2520from%2520speckles%2520captured%2520at%2520the%2520detection%2520side%250Aof%2520a%2520multi-core%2520fiber%2520%2528MCF%2529.%2520Unlike%2520conventional%2520neural%2520networks%252C%2520SpecDiffusion%250Aemploys%2520iterative%2520phase%2520denoising%2520steps%2520for%2520speckle-driven%2520phase%250Areconstruction.%2520The%2520iteration%2520scheme%2520allows%2520SpecDiffusion%2520to%2520break%2520down%2520the%250Aphase%2520reconstruction%2520process%2520into%2520multiple%2520steps%252C%2520gradually%2520building%2520up%2520to%2520the%250Afinal%2520phase%2520image.%2520This%2520attribute%2520alleviates%2520the%2520computation%2520challenge%2520at%2520each%250Astep%2520and%2520enables%2520the%2520reconstruction%2520of%2520rich%2520details%2520in%2520complex%2520microscopic%250Aimages.%2520To%2520validate%2520its%2520efficacy%252C%2520we%2520build%2520an%2520optical%2520system%2520to%2520capture%250Aspeckles%2520from%2520MCF%2520and%2520construct%2520a%2520dataset%2520consisting%2520of%2520100%252C000%2520paired%2520images.%250ASpecDiffusion%2520provides%2520high-fidelity%2520phase%2520reconstruction%2520results%2520and%2520shows%250Apowerful%2520generalization%2520capacity%2520for%2520unseen%2520objects%252C%2520such%2520as%2520test%2520charts%2520and%250Abiological%2520tissues%252C%2520reducing%2520the%2520average%2520mean%2520absolute%2520error%2520of%2520the%250Areconstructed%2520tissue%2520images%2520by%25207%2520times.%2520Furthermore%252C%2520the%2520reconstructed%2520tissue%250Aimages%2520using%2520SpecDiffusion%2520shows%2520higher%2520accuracy%2520in%2520zero-shot%2520cell%2520segmentation%250Atasks%2520compared%2520to%2520the%2520conventional%2520method%252C%2520demonstrating%2520the%2520potential%2520for%250Afurther%2520cell%2520morphology%2520analysis%2520through%2520the%2520learning-based%2520lensless%2520fiber%250Aendomicroscope.%2520SpecDiffusion%2520offers%2520a%2520precise%2520and%2520generalized%2520method%2520to%2520phase%250Areconstruction%2520through%2520scattering%2520media%252C%2520including%2520MCFs%252C%2520opening%2520new%250Aperspective%2520in%2520lensless%2520fiber%2520endomicroscopic%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-driven%20lensless%20fiber%20endomicroscopic%20quantitative%20phase%0A%20%20imaging%20towards%20digital%20pathology&entry.906535625=Zhaoqing%20Chen%20and%20Jiawei%20Sun%20and%20Xinyi%20Ye%20and%20Bin%20Zhao%20and%20Xuelong%20Li%20and%20Juergen%20Czarske&entry.1292438233=%20%20Lensless%20fiber%20endomicroscope%20is%20an%20emerging%20tool%20for%20in-vivo%20microscopic%0Aimaging%2C%20where%20quantitative%20phase%20imaging%20%28QPI%29%20can%20be%20utilized%20as%20a%20label-free%0Amethod%20to%20enhance%20image%20contrast.%20However%2C%20existing%20single-shot%20phase%0Areconstruction%20methods%20through%20lensless%20fiber%20endomicroscope%20typically%20perform%0Awell%20on%20simple%20images%20but%20struggle%20with%20complex%20microscopic%20structures.%20Here%2C%0Awe%20propose%20a%20speckle-conditioned%20diffusion%20model%20%28SpecDiffusion%29%2C%20which%0Areconstructs%20phase%20images%20directly%20from%20speckles%20captured%20at%20the%20detection%20side%0Aof%20a%20multi-core%20fiber%20%28MCF%29.%20Unlike%20conventional%20neural%20networks%2C%20SpecDiffusion%0Aemploys%20iterative%20phase%20denoising%20steps%20for%20speckle-driven%20phase%0Areconstruction.%20The%20iteration%20scheme%20allows%20SpecDiffusion%20to%20break%20down%20the%0Aphase%20reconstruction%20process%20into%20multiple%20steps%2C%20gradually%20building%20up%20to%20the%0Afinal%20phase%20image.%20This%20attribute%20alleviates%20the%20computation%20challenge%20at%20each%0Astep%20and%20enables%20the%20reconstruction%20of%20rich%20details%20in%20complex%20microscopic%0Aimages.%20To%20validate%20its%20efficacy%2C%20we%20build%20an%20optical%20system%20to%20capture%0Aspeckles%20from%20MCF%20and%20construct%20a%20dataset%20consisting%20of%20100%2C000%20paired%20images.%0ASpecDiffusion%20provides%20high-fidelity%20phase%20reconstruction%20results%20and%20shows%0Apowerful%20generalization%20capacity%20for%20unseen%20objects%2C%20such%20as%20test%20charts%20and%0Abiological%20tissues%2C%20reducing%20the%20average%20mean%20absolute%20error%20of%20the%0Areconstructed%20tissue%20images%20by%207%20times.%20Furthermore%2C%20the%20reconstructed%20tissue%0Aimages%20using%20SpecDiffusion%20shows%20higher%20accuracy%20in%20zero-shot%20cell%20segmentation%0Atasks%20compared%20to%20the%20conventional%20method%2C%20demonstrating%20the%20potential%20for%0Afurther%20cell%20morphology%20analysis%20through%20the%20learning-based%20lensless%20fiber%0Aendomicroscope.%20SpecDiffusion%20offers%20a%20precise%20and%20generalized%20method%20to%20phase%0Areconstruction%20through%20scattering%20media%2C%20including%20MCFs%2C%20opening%20new%0Aperspective%20in%20lensless%20fiber%20endomicroscopic%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18456v2&entry.124074799=Read"},
{"title": "Defining and Extracting generalizable interaction primitives from DNNs", "author": "Lu Chen and Siyu Lou and Benhao Huang and Quanshi Zhang", "abstract": "  Faithfully summarizing the knowledge encoded by a deep neural network (DNN)\ninto a few symbolic primitive patterns without losing much information\nrepresents a core challenge in explainable AI. To this end, Ren et al. (2024)\nhave derived a series of theorems to prove that the inference score of a DNN\ncan be explained as a small set of interactions between input variables.\nHowever, the lack of generalization power makes it still hard to consider such\ninteractions as faithful primitive patterns encoded by the DNN. Therefore,\ngiven different DNNs trained for the same task, we develop a new method to\nextract interactions that are shared by these DNNs. Experiments show that the\nextracted interactions can better reflect common knowledge shared by different\nDNNs.\n", "link": "http://arxiv.org/abs/2401.16318v2", "date": "2024-09-13", "relevancy": 2.2079, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4437}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4408}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defining%20and%20Extracting%20generalizable%20interaction%20primitives%20from%20DNNs&body=Title%3A%20Defining%20and%20Extracting%20generalizable%20interaction%20primitives%20from%20DNNs%0AAuthor%3A%20Lu%20Chen%20and%20Siyu%20Lou%20and%20Benhao%20Huang%20and%20Quanshi%20Zhang%0AAbstract%3A%20%20%20Faithfully%20summarizing%20the%20knowledge%20encoded%20by%20a%20deep%20neural%20network%20%28DNN%29%0Ainto%20a%20few%20symbolic%20primitive%20patterns%20without%20losing%20much%20information%0Arepresents%20a%20core%20challenge%20in%20explainable%20AI.%20To%20this%20end%2C%20Ren%20et%20al.%20%282024%29%0Ahave%20derived%20a%20series%20of%20theorems%20to%20prove%20that%20the%20inference%20score%20of%20a%20DNN%0Acan%20be%20explained%20as%20a%20small%20set%20of%20interactions%20between%20input%20variables.%0AHowever%2C%20the%20lack%20of%20generalization%20power%20makes%20it%20still%20hard%20to%20consider%20such%0Ainteractions%20as%20faithful%20primitive%20patterns%20encoded%20by%20the%20DNN.%20Therefore%2C%0Agiven%20different%20DNNs%20trained%20for%20the%20same%20task%2C%20we%20develop%20a%20new%20method%20to%0Aextract%20interactions%20that%20are%20shared%20by%20these%20DNNs.%20Experiments%20show%20that%20the%0Aextracted%20interactions%20can%20better%20reflect%20common%20knowledge%20shared%20by%20different%0ADNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefining%2520and%2520Extracting%2520generalizable%2520interaction%2520primitives%2520from%2520DNNs%26entry.906535625%3DLu%2520Chen%2520and%2520Siyu%2520Lou%2520and%2520Benhao%2520Huang%2520and%2520Quanshi%2520Zhang%26entry.1292438233%3D%2520%2520Faithfully%2520summarizing%2520the%2520knowledge%2520encoded%2520by%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%250Ainto%2520a%2520few%2520symbolic%2520primitive%2520patterns%2520without%2520losing%2520much%2520information%250Arepresents%2520a%2520core%2520challenge%2520in%2520explainable%2520AI.%2520To%2520this%2520end%252C%2520Ren%2520et%2520al.%2520%25282024%2529%250Ahave%2520derived%2520a%2520series%2520of%2520theorems%2520to%2520prove%2520that%2520the%2520inference%2520score%2520of%2520a%2520DNN%250Acan%2520be%2520explained%2520as%2520a%2520small%2520set%2520of%2520interactions%2520between%2520input%2520variables.%250AHowever%252C%2520the%2520lack%2520of%2520generalization%2520power%2520makes%2520it%2520still%2520hard%2520to%2520consider%2520such%250Ainteractions%2520as%2520faithful%2520primitive%2520patterns%2520encoded%2520by%2520the%2520DNN.%2520Therefore%252C%250Agiven%2520different%2520DNNs%2520trained%2520for%2520the%2520same%2520task%252C%2520we%2520develop%2520a%2520new%2520method%2520to%250Aextract%2520interactions%2520that%2520are%2520shared%2520by%2520these%2520DNNs.%2520Experiments%2520show%2520that%2520the%250Aextracted%2520interactions%2520can%2520better%2520reflect%2520common%2520knowledge%2520shared%2520by%2520different%250ADNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defining%20and%20Extracting%20generalizable%20interaction%20primitives%20from%20DNNs&entry.906535625=Lu%20Chen%20and%20Siyu%20Lou%20and%20Benhao%20Huang%20and%20Quanshi%20Zhang&entry.1292438233=%20%20Faithfully%20summarizing%20the%20knowledge%20encoded%20by%20a%20deep%20neural%20network%20%28DNN%29%0Ainto%20a%20few%20symbolic%20primitive%20patterns%20without%20losing%20much%20information%0Arepresents%20a%20core%20challenge%20in%20explainable%20AI.%20To%20this%20end%2C%20Ren%20et%20al.%20%282024%29%0Ahave%20derived%20a%20series%20of%20theorems%20to%20prove%20that%20the%20inference%20score%20of%20a%20DNN%0Acan%20be%20explained%20as%20a%20small%20set%20of%20interactions%20between%20input%20variables.%0AHowever%2C%20the%20lack%20of%20generalization%20power%20makes%20it%20still%20hard%20to%20consider%20such%0Ainteractions%20as%20faithful%20primitive%20patterns%20encoded%20by%20the%20DNN.%20Therefore%2C%0Agiven%20different%20DNNs%20trained%20for%20the%20same%20task%2C%20we%20develop%20a%20new%20method%20to%0Aextract%20interactions%20that%20are%20shared%20by%20these%20DNNs.%20Experiments%20show%20that%20the%0Aextracted%20interactions%20can%20better%20reflect%20common%20knowledge%20shared%20by%20different%0ADNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16318v2&entry.124074799=Read"},
{"title": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models", "author": "Yuchen Liu and Luigi Palmieri and Sebastian Koch and Ilche Georgievski and Marco Aiello", "abstract": "  Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art.\n", "link": "http://arxiv.org/abs/2404.03275v2", "date": "2024-09-13", "relevancy": 2.1957, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DELTA%3A%20Decomposed%20Efficient%20Long-Term%20Robot%20Task%20Planning%20using%20Large%0A%20%20Language%20Models&body=Title%3A%20DELTA%3A%20Decomposed%20Efficient%20Long-Term%20Robot%20Task%20Planning%20using%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yuchen%20Liu%20and%20Luigi%20Palmieri%20and%20Sebastian%20Koch%20and%20Ilche%20Georgievski%20and%20Marco%20Aiello%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20sparked%20a%20revolution%0Aacross%20many%20research%20fields.%20In%20robotics%2C%20the%20integration%20of%20common-sense%0Aknowledge%20from%20LLMs%20into%20task%20and%20motion%20planning%20has%20drastically%20advanced%20the%0Afield%20by%20unlocking%20unprecedented%20levels%20of%20context%20awareness.%20Despite%20their%0Avast%20collection%20of%20knowledge%2C%20large%20language%20models%20may%20generate%20infeasible%0Aplans%20due%20to%20hallucinations%20or%20missing%20domain%20information.%20To%20address%20these%0Achallenges%20and%20improve%20plan%20feasibility%20and%20computational%20efficiency%2C%20we%0Aintroduce%20DELTA%2C%20a%20novel%20LLM-informed%20task%20planning%20approach.%20By%20using%20scene%0Agraphs%20as%20environment%20representations%20within%20LLMs%2C%20DELTA%20achieves%20rapid%0Ageneration%20of%20precise%20planning%20problem%20descriptions.%20To%20enhance%20planning%0Aperformance%2C%20DELTA%20decomposes%20long-term%20task%20goals%20with%20LLMs%20into%20an%0Aautoregressive%20sequence%20of%20sub-goals%2C%20enabling%20automated%20task%20planners%20to%0Aefficiently%20solve%20complex%20problems.%20In%20our%20extensive%20evaluation%2C%20we%20show%20that%0ADELTA%20enables%20an%20efficient%20and%20fully%20automatic%20task%20planning%20pipeline%2C%0Aachieving%20higher%20planning%20success%20rates%20and%20significantly%20shorter%20planning%0Atimes%20compared%20to%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDELTA%253A%2520Decomposed%2520Efficient%2520Long-Term%2520Robot%2520Task%2520Planning%2520using%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYuchen%2520Liu%2520and%2520Luigi%2520Palmieri%2520and%2520Sebastian%2520Koch%2520and%2520Ilche%2520Georgievski%2520and%2520Marco%2520Aiello%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520sparked%2520a%2520revolution%250Aacross%2520many%2520research%2520fields.%2520In%2520robotics%252C%2520the%2520integration%2520of%2520common-sense%250Aknowledge%2520from%2520LLMs%2520into%2520task%2520and%2520motion%2520planning%2520has%2520drastically%2520advanced%2520the%250Afield%2520by%2520unlocking%2520unprecedented%2520levels%2520of%2520context%2520awareness.%2520Despite%2520their%250Avast%2520collection%2520of%2520knowledge%252C%2520large%2520language%2520models%2520may%2520generate%2520infeasible%250Aplans%2520due%2520to%2520hallucinations%2520or%2520missing%2520domain%2520information.%2520To%2520address%2520these%250Achallenges%2520and%2520improve%2520plan%2520feasibility%2520and%2520computational%2520efficiency%252C%2520we%250Aintroduce%2520DELTA%252C%2520a%2520novel%2520LLM-informed%2520task%2520planning%2520approach.%2520By%2520using%2520scene%250Agraphs%2520as%2520environment%2520representations%2520within%2520LLMs%252C%2520DELTA%2520achieves%2520rapid%250Ageneration%2520of%2520precise%2520planning%2520problem%2520descriptions.%2520To%2520enhance%2520planning%250Aperformance%252C%2520DELTA%2520decomposes%2520long-term%2520task%2520goals%2520with%2520LLMs%2520into%2520an%250Aautoregressive%2520sequence%2520of%2520sub-goals%252C%2520enabling%2520automated%2520task%2520planners%2520to%250Aefficiently%2520solve%2520complex%2520problems.%2520In%2520our%2520extensive%2520evaluation%252C%2520we%2520show%2520that%250ADELTA%2520enables%2520an%2520efficient%2520and%2520fully%2520automatic%2520task%2520planning%2520pipeline%252C%250Aachieving%2520higher%2520planning%2520success%2520rates%2520and%2520significantly%2520shorter%2520planning%250Atimes%2520compared%2520to%2520the%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DELTA%3A%20Decomposed%20Efficient%20Long-Term%20Robot%20Task%20Planning%20using%20Large%0A%20%20Language%20Models&entry.906535625=Yuchen%20Liu%20and%20Luigi%20Palmieri%20and%20Sebastian%20Koch%20and%20Ilche%20Georgievski%20and%20Marco%20Aiello&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20sparked%20a%20revolution%0Aacross%20many%20research%20fields.%20In%20robotics%2C%20the%20integration%20of%20common-sense%0Aknowledge%20from%20LLMs%20into%20task%20and%20motion%20planning%20has%20drastically%20advanced%20the%0Afield%20by%20unlocking%20unprecedented%20levels%20of%20context%20awareness.%20Despite%20their%0Avast%20collection%20of%20knowledge%2C%20large%20language%20models%20may%20generate%20infeasible%0Aplans%20due%20to%20hallucinations%20or%20missing%20domain%20information.%20To%20address%20these%0Achallenges%20and%20improve%20plan%20feasibility%20and%20computational%20efficiency%2C%20we%0Aintroduce%20DELTA%2C%20a%20novel%20LLM-informed%20task%20planning%20approach.%20By%20using%20scene%0Agraphs%20as%20environment%20representations%20within%20LLMs%2C%20DELTA%20achieves%20rapid%0Ageneration%20of%20precise%20planning%20problem%20descriptions.%20To%20enhance%20planning%0Aperformance%2C%20DELTA%20decomposes%20long-term%20task%20goals%20with%20LLMs%20into%20an%0Aautoregressive%20sequence%20of%20sub-goals%2C%20enabling%20automated%20task%20planners%20to%0Aefficiently%20solve%20complex%20problems.%20In%20our%20extensive%20evaluation%2C%20we%20show%20that%0ADELTA%20enables%20an%20efficient%20and%20fully%20automatic%20task%20planning%20pipeline%2C%0Aachieving%20higher%20planning%20success%20rates%20and%20significantly%20shorter%20planning%0Atimes%20compared%20to%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03275v2&entry.124074799=Read"},
{"title": "RTF-Q: Efficient Unsupervised Domain Adaptation with Retraining-free\n  Quantization", "author": "Nanyang Du and Chen Tang and Yuxiao Jiang and Yuan Meng and Zhi Wang", "abstract": "  Performing unsupervised domain adaptation on resource-constrained edge\ndevices is challenging. Existing research typically adopts architecture\noptimization (e.g., designing slimmable networks) but requires expensive\ntraining costs. Moreover, it does not consider the considerable precision\nredundancy of parameters and activations. To address these limitations, we\npropose efficient unsupervised domain adaptation with ReTraining-Free\nQuantization (RTF-Q). Our approach uses low-precision quantization\narchitectures with varying computational costs, adapting to devices with\ndynamic computation budgets. We subtly configure subnet dimensions and leverage\nweight-sharing to optimize multiple architectures within a single set of\nweights, enabling the use of pre-trained models from open-source repositories.\nAdditionally, we introduce multi-bitwidth joint training and the SandwichQ\nrule, both of which are effective in handling multiple quantization bit-widths\nacross subnets. Experimental results demonstrate that our network achieves\ncompetitive accuracy with state-of-the-art methods across three benchmarks\nwhile significantly reducing memory and computational costs.\n", "link": "http://arxiv.org/abs/2408.05752v2", "date": "2024-09-13", "relevancy": 2.1953, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5729}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5427}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RTF-Q%3A%20Efficient%20Unsupervised%20Domain%20Adaptation%20with%20Retraining-free%0A%20%20Quantization&body=Title%3A%20RTF-Q%3A%20Efficient%20Unsupervised%20Domain%20Adaptation%20with%20Retraining-free%0A%20%20Quantization%0AAuthor%3A%20Nanyang%20Du%20and%20Chen%20Tang%20and%20Yuxiao%20Jiang%20and%20Yuan%20Meng%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20Performing%20unsupervised%20domain%20adaptation%20on%20resource-constrained%20edge%0Adevices%20is%20challenging.%20Existing%20research%20typically%20adopts%20architecture%0Aoptimization%20%28e.g.%2C%20designing%20slimmable%20networks%29%20but%20requires%20expensive%0Atraining%20costs.%20Moreover%2C%20it%20does%20not%20consider%20the%20considerable%20precision%0Aredundancy%20of%20parameters%20and%20activations.%20To%20address%20these%20limitations%2C%20we%0Apropose%20efficient%20unsupervised%20domain%20adaptation%20with%20ReTraining-Free%0AQuantization%20%28RTF-Q%29.%20Our%20approach%20uses%20low-precision%20quantization%0Aarchitectures%20with%20varying%20computational%20costs%2C%20adapting%20to%20devices%20with%0Adynamic%20computation%20budgets.%20We%20subtly%20configure%20subnet%20dimensions%20and%20leverage%0Aweight-sharing%20to%20optimize%20multiple%20architectures%20within%20a%20single%20set%20of%0Aweights%2C%20enabling%20the%20use%20of%20pre-trained%20models%20from%20open-source%20repositories.%0AAdditionally%2C%20we%20introduce%20multi-bitwidth%20joint%20training%20and%20the%20SandwichQ%0Arule%2C%20both%20of%20which%20are%20effective%20in%20handling%20multiple%20quantization%20bit-widths%0Aacross%20subnets.%20Experimental%20results%20demonstrate%20that%20our%20network%20achieves%0Acompetitive%20accuracy%20with%20state-of-the-art%20methods%20across%20three%20benchmarks%0Awhile%20significantly%20reducing%20memory%20and%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05752v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRTF-Q%253A%2520Efficient%2520Unsupervised%2520Domain%2520Adaptation%2520with%2520Retraining-free%250A%2520%2520Quantization%26entry.906535625%3DNanyang%2520Du%2520and%2520Chen%2520Tang%2520and%2520Yuxiao%2520Jiang%2520and%2520Yuan%2520Meng%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520Performing%2520unsupervised%2520domain%2520adaptation%2520on%2520resource-constrained%2520edge%250Adevices%2520is%2520challenging.%2520Existing%2520research%2520typically%2520adopts%2520architecture%250Aoptimization%2520%2528e.g.%252C%2520designing%2520slimmable%2520networks%2529%2520but%2520requires%2520expensive%250Atraining%2520costs.%2520Moreover%252C%2520it%2520does%2520not%2520consider%2520the%2520considerable%2520precision%250Aredundancy%2520of%2520parameters%2520and%2520activations.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520efficient%2520unsupervised%2520domain%2520adaptation%2520with%2520ReTraining-Free%250AQuantization%2520%2528RTF-Q%2529.%2520Our%2520approach%2520uses%2520low-precision%2520quantization%250Aarchitectures%2520with%2520varying%2520computational%2520costs%252C%2520adapting%2520to%2520devices%2520with%250Adynamic%2520computation%2520budgets.%2520We%2520subtly%2520configure%2520subnet%2520dimensions%2520and%2520leverage%250Aweight-sharing%2520to%2520optimize%2520multiple%2520architectures%2520within%2520a%2520single%2520set%2520of%250Aweights%252C%2520enabling%2520the%2520use%2520of%2520pre-trained%2520models%2520from%2520open-source%2520repositories.%250AAdditionally%252C%2520we%2520introduce%2520multi-bitwidth%2520joint%2520training%2520and%2520the%2520SandwichQ%250Arule%252C%2520both%2520of%2520which%2520are%2520effective%2520in%2520handling%2520multiple%2520quantization%2520bit-widths%250Aacross%2520subnets.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520network%2520achieves%250Acompetitive%2520accuracy%2520with%2520state-of-the-art%2520methods%2520across%2520three%2520benchmarks%250Awhile%2520significantly%2520reducing%2520memory%2520and%2520computational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05752v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTF-Q%3A%20Efficient%20Unsupervised%20Domain%20Adaptation%20with%20Retraining-free%0A%20%20Quantization&entry.906535625=Nanyang%20Du%20and%20Chen%20Tang%20and%20Yuxiao%20Jiang%20and%20Yuan%20Meng%20and%20Zhi%20Wang&entry.1292438233=%20%20Performing%20unsupervised%20domain%20adaptation%20on%20resource-constrained%20edge%0Adevices%20is%20challenging.%20Existing%20research%20typically%20adopts%20architecture%0Aoptimization%20%28e.g.%2C%20designing%20slimmable%20networks%29%20but%20requires%20expensive%0Atraining%20costs.%20Moreover%2C%20it%20does%20not%20consider%20the%20considerable%20precision%0Aredundancy%20of%20parameters%20and%20activations.%20To%20address%20these%20limitations%2C%20we%0Apropose%20efficient%20unsupervised%20domain%20adaptation%20with%20ReTraining-Free%0AQuantization%20%28RTF-Q%29.%20Our%20approach%20uses%20low-precision%20quantization%0Aarchitectures%20with%20varying%20computational%20costs%2C%20adapting%20to%20devices%20with%0Adynamic%20computation%20budgets.%20We%20subtly%20configure%20subnet%20dimensions%20and%20leverage%0Aweight-sharing%20to%20optimize%20multiple%20architectures%20within%20a%20single%20set%20of%0Aweights%2C%20enabling%20the%20use%20of%20pre-trained%20models%20from%20open-source%20repositories.%0AAdditionally%2C%20we%20introduce%20multi-bitwidth%20joint%20training%20and%20the%20SandwichQ%0Arule%2C%20both%20of%20which%20are%20effective%20in%20handling%20multiple%20quantization%20bit-widths%0Aacross%20subnets.%20Experimental%20results%20demonstrate%20that%20our%20network%20achieves%0Acompetitive%20accuracy%20with%20state-of-the-art%20methods%20across%20three%20benchmarks%0Awhile%20significantly%20reducing%20memory%20and%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05752v2&entry.124074799=Read"},
{"title": "Exploring Action-Centric Representations Through the Lens of\n  Rate-Distortion Theory", "author": "Miguel de Llanza Varona and Christopher L. Buckley and Beren Millidge", "abstract": "  Organisms have to keep track of the information in the environment that is\nrelevant for adaptive behaviour. Transmitting information in an economical and\nefficient way becomes crucial for limited-resourced agents living in\nhigh-dimensional environments. The efficient coding hypothesis claims that\norganisms seek to maximize the information about the sensory input in an\nefficient manner. Under Bayesian inference, this means that the role of the\nbrain is to efficiently allocate resources in order to make predictions about\nthe hidden states that cause sensory data. However, neither of those frameworks\naccounts for how that information is exploited downstream, leaving aside the\naction-oriented role of the perceptual system. Rate-distortion theory, which\ndefines optimal lossy compression under constraints, has gained attention as a\nformal framework to explore goal-oriented efficient coding. In this work, we\nexplore action-centric representations in the context of rate-distortion\ntheory. We also provide a mathematical definition of abstractions and we argue\nthat, as a summary of the relevant details, they can be used to fix the content\nof action-centric representations. We model action-centric representations\nusing VAEs and we find that such representations i) are efficient lossy\ncompressions of the data; ii) capture the task-dependent invariances necessary\nto achieve successful behaviour; and iii) are not in service of reconstructing\nthe data. Thus, we conclude that full reconstruction of the data is rarely\nneeded to achieve optimal behaviour, consistent with a teleological approach to\nperception.\n", "link": "http://arxiv.org/abs/2409.08892v1", "date": "2024-09-13", "relevancy": 2.1794, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Action-Centric%20Representations%20Through%20the%20Lens%20of%0A%20%20Rate-Distortion%20Theory&body=Title%3A%20Exploring%20Action-Centric%20Representations%20Through%20the%20Lens%20of%0A%20%20Rate-Distortion%20Theory%0AAuthor%3A%20Miguel%20de%20Llanza%20Varona%20and%20Christopher%20L.%20Buckley%20and%20Beren%20Millidge%0AAbstract%3A%20%20%20Organisms%20have%20to%20keep%20track%20of%20the%20information%20in%20the%20environment%20that%20is%0Arelevant%20for%20adaptive%20behaviour.%20Transmitting%20information%20in%20an%20economical%20and%0Aefficient%20way%20becomes%20crucial%20for%20limited-resourced%20agents%20living%20in%0Ahigh-dimensional%20environments.%20The%20efficient%20coding%20hypothesis%20claims%20that%0Aorganisms%20seek%20to%20maximize%20the%20information%20about%20the%20sensory%20input%20in%20an%0Aefficient%20manner.%20Under%20Bayesian%20inference%2C%20this%20means%20that%20the%20role%20of%20the%0Abrain%20is%20to%20efficiently%20allocate%20resources%20in%20order%20to%20make%20predictions%20about%0Athe%20hidden%20states%20that%20cause%20sensory%20data.%20However%2C%20neither%20of%20those%20frameworks%0Aaccounts%20for%20how%20that%20information%20is%20exploited%20downstream%2C%20leaving%20aside%20the%0Aaction-oriented%20role%20of%20the%20perceptual%20system.%20Rate-distortion%20theory%2C%20which%0Adefines%20optimal%20lossy%20compression%20under%20constraints%2C%20has%20gained%20attention%20as%20a%0Aformal%20framework%20to%20explore%20goal-oriented%20efficient%20coding.%20In%20this%20work%2C%20we%0Aexplore%20action-centric%20representations%20in%20the%20context%20of%20rate-distortion%0Atheory.%20We%20also%20provide%20a%20mathematical%20definition%20of%20abstractions%20and%20we%20argue%0Athat%2C%20as%20a%20summary%20of%20the%20relevant%20details%2C%20they%20can%20be%20used%20to%20fix%20the%20content%0Aof%20action-centric%20representations.%20We%20model%20action-centric%20representations%0Ausing%20VAEs%20and%20we%20find%20that%20such%20representations%20i%29%20are%20efficient%20lossy%0Acompressions%20of%20the%20data%3B%20ii%29%20capture%20the%20task-dependent%20invariances%20necessary%0Ato%20achieve%20successful%20behaviour%3B%20and%20iii%29%20are%20not%20in%20service%20of%20reconstructing%0Athe%20data.%20Thus%2C%20we%20conclude%20that%20full%20reconstruction%20of%20the%20data%20is%20rarely%0Aneeded%20to%20achieve%20optimal%20behaviour%2C%20consistent%20with%20a%20teleological%20approach%20to%0Aperception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Action-Centric%2520Representations%2520Through%2520the%2520Lens%2520of%250A%2520%2520Rate-Distortion%2520Theory%26entry.906535625%3DMiguel%2520de%2520Llanza%2520Varona%2520and%2520Christopher%2520L.%2520Buckley%2520and%2520Beren%2520Millidge%26entry.1292438233%3D%2520%2520Organisms%2520have%2520to%2520keep%2520track%2520of%2520the%2520information%2520in%2520the%2520environment%2520that%2520is%250Arelevant%2520for%2520adaptive%2520behaviour.%2520Transmitting%2520information%2520in%2520an%2520economical%2520and%250Aefficient%2520way%2520becomes%2520crucial%2520for%2520limited-resourced%2520agents%2520living%2520in%250Ahigh-dimensional%2520environments.%2520The%2520efficient%2520coding%2520hypothesis%2520claims%2520that%250Aorganisms%2520seek%2520to%2520maximize%2520the%2520information%2520about%2520the%2520sensory%2520input%2520in%2520an%250Aefficient%2520manner.%2520Under%2520Bayesian%2520inference%252C%2520this%2520means%2520that%2520the%2520role%2520of%2520the%250Abrain%2520is%2520to%2520efficiently%2520allocate%2520resources%2520in%2520order%2520to%2520make%2520predictions%2520about%250Athe%2520hidden%2520states%2520that%2520cause%2520sensory%2520data.%2520However%252C%2520neither%2520of%2520those%2520frameworks%250Aaccounts%2520for%2520how%2520that%2520information%2520is%2520exploited%2520downstream%252C%2520leaving%2520aside%2520the%250Aaction-oriented%2520role%2520of%2520the%2520perceptual%2520system.%2520Rate-distortion%2520theory%252C%2520which%250Adefines%2520optimal%2520lossy%2520compression%2520under%2520constraints%252C%2520has%2520gained%2520attention%2520as%2520a%250Aformal%2520framework%2520to%2520explore%2520goal-oriented%2520efficient%2520coding.%2520In%2520this%2520work%252C%2520we%250Aexplore%2520action-centric%2520representations%2520in%2520the%2520context%2520of%2520rate-distortion%250Atheory.%2520We%2520also%2520provide%2520a%2520mathematical%2520definition%2520of%2520abstractions%2520and%2520we%2520argue%250Athat%252C%2520as%2520a%2520summary%2520of%2520the%2520relevant%2520details%252C%2520they%2520can%2520be%2520used%2520to%2520fix%2520the%2520content%250Aof%2520action-centric%2520representations.%2520We%2520model%2520action-centric%2520representations%250Ausing%2520VAEs%2520and%2520we%2520find%2520that%2520such%2520representations%2520i%2529%2520are%2520efficient%2520lossy%250Acompressions%2520of%2520the%2520data%253B%2520ii%2529%2520capture%2520the%2520task-dependent%2520invariances%2520necessary%250Ato%2520achieve%2520successful%2520behaviour%253B%2520and%2520iii%2529%2520are%2520not%2520in%2520service%2520of%2520reconstructing%250Athe%2520data.%2520Thus%252C%2520we%2520conclude%2520that%2520full%2520reconstruction%2520of%2520the%2520data%2520is%2520rarely%250Aneeded%2520to%2520achieve%2520optimal%2520behaviour%252C%2520consistent%2520with%2520a%2520teleological%2520approach%2520to%250Aperception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Action-Centric%20Representations%20Through%20the%20Lens%20of%0A%20%20Rate-Distortion%20Theory&entry.906535625=Miguel%20de%20Llanza%20Varona%20and%20Christopher%20L.%20Buckley%20and%20Beren%20Millidge&entry.1292438233=%20%20Organisms%20have%20to%20keep%20track%20of%20the%20information%20in%20the%20environment%20that%20is%0Arelevant%20for%20adaptive%20behaviour.%20Transmitting%20information%20in%20an%20economical%20and%0Aefficient%20way%20becomes%20crucial%20for%20limited-resourced%20agents%20living%20in%0Ahigh-dimensional%20environments.%20The%20efficient%20coding%20hypothesis%20claims%20that%0Aorganisms%20seek%20to%20maximize%20the%20information%20about%20the%20sensory%20input%20in%20an%0Aefficient%20manner.%20Under%20Bayesian%20inference%2C%20this%20means%20that%20the%20role%20of%20the%0Abrain%20is%20to%20efficiently%20allocate%20resources%20in%20order%20to%20make%20predictions%20about%0Athe%20hidden%20states%20that%20cause%20sensory%20data.%20However%2C%20neither%20of%20those%20frameworks%0Aaccounts%20for%20how%20that%20information%20is%20exploited%20downstream%2C%20leaving%20aside%20the%0Aaction-oriented%20role%20of%20the%20perceptual%20system.%20Rate-distortion%20theory%2C%20which%0Adefines%20optimal%20lossy%20compression%20under%20constraints%2C%20has%20gained%20attention%20as%20a%0Aformal%20framework%20to%20explore%20goal-oriented%20efficient%20coding.%20In%20this%20work%2C%20we%0Aexplore%20action-centric%20representations%20in%20the%20context%20of%20rate-distortion%0Atheory.%20We%20also%20provide%20a%20mathematical%20definition%20of%20abstractions%20and%20we%20argue%0Athat%2C%20as%20a%20summary%20of%20the%20relevant%20details%2C%20they%20can%20be%20used%20to%20fix%20the%20content%0Aof%20action-centric%20representations.%20We%20model%20action-centric%20representations%0Ausing%20VAEs%20and%20we%20find%20that%20such%20representations%20i%29%20are%20efficient%20lossy%0Acompressions%20of%20the%20data%3B%20ii%29%20capture%20the%20task-dependent%20invariances%20necessary%0Ato%20achieve%20successful%20behaviour%3B%20and%20iii%29%20are%20not%20in%20service%20of%20reconstructing%0Athe%20data.%20Thus%2C%20we%20conclude%20that%20full%20reconstruction%20of%20the%20data%20is%20rarely%0Aneeded%20to%20achieve%20optimal%20behaviour%2C%20consistent%20with%20a%20teleological%20approach%20to%0Aperception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08892v1&entry.124074799=Read"},
{"title": "SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear\n  Complexity", "author": "Qitian Wu and Kai Yang and Hengrui Zhang and David Wipf and Junchi Yan", "abstract": "  Learning representations on large graphs is a long-standing challenge due to\nthe inter-dependence nature. Transformers recently have shown promising\nperformance on small graphs thanks to its global attention for capturing\nall-pair interactions beyond observed structures. Existing approaches tend to\ninherit the spirit of Transformers in language and vision tasks, and embrace\ncomplicated architectures by stacking deep attention-based propagation layers.\nIn this paper, we attempt to evaluate the necessity of adopting multi-layer\nattentions in Transformers on graphs, which considerably restricts the\nefficiency. Specifically, we analyze a generic hybrid propagation layer,\ncomprised of all-pair attention and graph-based propagation, and show that\nmulti-layer propagation can be reduced to one-layer propagation, with the same\ncapability for representation learning. It suggests a new technical path for\nbuilding powerful and efficient Transformers on graphs, particularly through\nsimplifying model architectures without sacrificing expressiveness. As\nexemplified by this work, we propose a Simplified Single-layer Graph\nTransformers (SGFormer), whose main component is a single-layer global\nattention that scales linearly w.r.t. graph sizes and requires none of any\napproximation for accommodating all-pair interactions. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M, yielding\norders-of-magnitude inference acceleration over peer Transformers on\nmedium-sized graphs, and demonstrates competitiveness with limited labeled\ndata.\n", "link": "http://arxiv.org/abs/2409.09007v1", "date": "2024-09-13", "relevancy": 2.1692, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5799}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5447}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGFormer%3A%20Single-Layer%20Graph%20Transformers%20with%20Approximation-Free%20Linear%0A%20%20Complexity&body=Title%3A%20SGFormer%3A%20Single-Layer%20Graph%20Transformers%20with%20Approximation-Free%20Linear%0A%20%20Complexity%0AAuthor%3A%20Qitian%20Wu%20and%20Kai%20Yang%20and%20Hengrui%20Zhang%20and%20David%20Wipf%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20Learning%20representations%20on%20large%20graphs%20is%20a%20long-standing%20challenge%20due%20to%0Athe%20inter-dependence%20nature.%20Transformers%20recently%20have%20shown%20promising%0Aperformance%20on%20small%20graphs%20thanks%20to%20its%20global%20attention%20for%20capturing%0Aall-pair%20interactions%20beyond%20observed%20structures.%20Existing%20approaches%20tend%20to%0Ainherit%20the%20spirit%20of%20Transformers%20in%20language%20and%20vision%20tasks%2C%20and%20embrace%0Acomplicated%20architectures%20by%20stacking%20deep%20attention-based%20propagation%20layers.%0AIn%20this%20paper%2C%20we%20attempt%20to%20evaluate%20the%20necessity%20of%20adopting%20multi-layer%0Aattentions%20in%20Transformers%20on%20graphs%2C%20which%20considerably%20restricts%20the%0Aefficiency.%20Specifically%2C%20we%20analyze%20a%20generic%20hybrid%20propagation%20layer%2C%0Acomprised%20of%20all-pair%20attention%20and%20graph-based%20propagation%2C%20and%20show%20that%0Amulti-layer%20propagation%20can%20be%20reduced%20to%20one-layer%20propagation%2C%20with%20the%20same%0Acapability%20for%20representation%20learning.%20It%20suggests%20a%20new%20technical%20path%20for%0Abuilding%20powerful%20and%20efficient%20Transformers%20on%20graphs%2C%20particularly%20through%0Asimplifying%20model%20architectures%20without%20sacrificing%20expressiveness.%20As%0Aexemplified%20by%20this%20work%2C%20we%20propose%20a%20Simplified%20Single-layer%20Graph%0ATransformers%20%28SGFormer%29%2C%20whose%20main%20component%20is%20a%20single-layer%20global%0Aattention%20that%20scales%20linearly%20w.r.t.%20graph%20sizes%20and%20requires%20none%20of%20any%0Aapproximation%20for%20accommodating%20all-pair%20interactions.%20Empirically%2C%20SGFormer%0Asuccessfully%20scales%20to%20the%20web-scale%20graph%20ogbn-papers100M%2C%20yielding%0Aorders-of-magnitude%20inference%20acceleration%20over%20peer%20Transformers%20on%0Amedium-sized%20graphs%2C%20and%20demonstrates%20competitiveness%20with%20limited%20labeled%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGFormer%253A%2520Single-Layer%2520Graph%2520Transformers%2520with%2520Approximation-Free%2520Linear%250A%2520%2520Complexity%26entry.906535625%3DQitian%2520Wu%2520and%2520Kai%2520Yang%2520and%2520Hengrui%2520Zhang%2520and%2520David%2520Wipf%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520Learning%2520representations%2520on%2520large%2520graphs%2520is%2520a%2520long-standing%2520challenge%2520due%2520to%250Athe%2520inter-dependence%2520nature.%2520Transformers%2520recently%2520have%2520shown%2520promising%250Aperformance%2520on%2520small%2520graphs%2520thanks%2520to%2520its%2520global%2520attention%2520for%2520capturing%250Aall-pair%2520interactions%2520beyond%2520observed%2520structures.%2520Existing%2520approaches%2520tend%2520to%250Ainherit%2520the%2520spirit%2520of%2520Transformers%2520in%2520language%2520and%2520vision%2520tasks%252C%2520and%2520embrace%250Acomplicated%2520architectures%2520by%2520stacking%2520deep%2520attention-based%2520propagation%2520layers.%250AIn%2520this%2520paper%252C%2520we%2520attempt%2520to%2520evaluate%2520the%2520necessity%2520of%2520adopting%2520multi-layer%250Aattentions%2520in%2520Transformers%2520on%2520graphs%252C%2520which%2520considerably%2520restricts%2520the%250Aefficiency.%2520Specifically%252C%2520we%2520analyze%2520a%2520generic%2520hybrid%2520propagation%2520layer%252C%250Acomprised%2520of%2520all-pair%2520attention%2520and%2520graph-based%2520propagation%252C%2520and%2520show%2520that%250Amulti-layer%2520propagation%2520can%2520be%2520reduced%2520to%2520one-layer%2520propagation%252C%2520with%2520the%2520same%250Acapability%2520for%2520representation%2520learning.%2520It%2520suggests%2520a%2520new%2520technical%2520path%2520for%250Abuilding%2520powerful%2520and%2520efficient%2520Transformers%2520on%2520graphs%252C%2520particularly%2520through%250Asimplifying%2520model%2520architectures%2520without%2520sacrificing%2520expressiveness.%2520As%250Aexemplified%2520by%2520this%2520work%252C%2520we%2520propose%2520a%2520Simplified%2520Single-layer%2520Graph%250ATransformers%2520%2528SGFormer%2529%252C%2520whose%2520main%2520component%2520is%2520a%2520single-layer%2520global%250Aattention%2520that%2520scales%2520linearly%2520w.r.t.%2520graph%2520sizes%2520and%2520requires%2520none%2520of%2520any%250Aapproximation%2520for%2520accommodating%2520all-pair%2520interactions.%2520Empirically%252C%2520SGFormer%250Asuccessfully%2520scales%2520to%2520the%2520web-scale%2520graph%2520ogbn-papers100M%252C%2520yielding%250Aorders-of-magnitude%2520inference%2520acceleration%2520over%2520peer%2520Transformers%2520on%250Amedium-sized%2520graphs%252C%2520and%2520demonstrates%2520competitiveness%2520with%2520limited%2520labeled%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGFormer%3A%20Single-Layer%20Graph%20Transformers%20with%20Approximation-Free%20Linear%0A%20%20Complexity&entry.906535625=Qitian%20Wu%20and%20Kai%20Yang%20and%20Hengrui%20Zhang%20and%20David%20Wipf%20and%20Junchi%20Yan&entry.1292438233=%20%20Learning%20representations%20on%20large%20graphs%20is%20a%20long-standing%20challenge%20due%20to%0Athe%20inter-dependence%20nature.%20Transformers%20recently%20have%20shown%20promising%0Aperformance%20on%20small%20graphs%20thanks%20to%20its%20global%20attention%20for%20capturing%0Aall-pair%20interactions%20beyond%20observed%20structures.%20Existing%20approaches%20tend%20to%0Ainherit%20the%20spirit%20of%20Transformers%20in%20language%20and%20vision%20tasks%2C%20and%20embrace%0Acomplicated%20architectures%20by%20stacking%20deep%20attention-based%20propagation%20layers.%0AIn%20this%20paper%2C%20we%20attempt%20to%20evaluate%20the%20necessity%20of%20adopting%20multi-layer%0Aattentions%20in%20Transformers%20on%20graphs%2C%20which%20considerably%20restricts%20the%0Aefficiency.%20Specifically%2C%20we%20analyze%20a%20generic%20hybrid%20propagation%20layer%2C%0Acomprised%20of%20all-pair%20attention%20and%20graph-based%20propagation%2C%20and%20show%20that%0Amulti-layer%20propagation%20can%20be%20reduced%20to%20one-layer%20propagation%2C%20with%20the%20same%0Acapability%20for%20representation%20learning.%20It%20suggests%20a%20new%20technical%20path%20for%0Abuilding%20powerful%20and%20efficient%20Transformers%20on%20graphs%2C%20particularly%20through%0Asimplifying%20model%20architectures%20without%20sacrificing%20expressiveness.%20As%0Aexemplified%20by%20this%20work%2C%20we%20propose%20a%20Simplified%20Single-layer%20Graph%0ATransformers%20%28SGFormer%29%2C%20whose%20main%20component%20is%20a%20single-layer%20global%0Aattention%20that%20scales%20linearly%20w.r.t.%20graph%20sizes%20and%20requires%20none%20of%20any%0Aapproximation%20for%20accommodating%20all-pair%20interactions.%20Empirically%2C%20SGFormer%0Asuccessfully%20scales%20to%20the%20web-scale%20graph%20ogbn-papers100M%2C%20yielding%0Aorders-of-magnitude%20inference%20acceleration%20over%20peer%20Transformers%20on%0Amedium-sized%20graphs%2C%20and%20demonstrates%20competitiveness%20with%20limited%20labeled%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09007v1&entry.124074799=Read"},
{"title": "Pushing Joint Image Denoising and Classification to the Edge", "author": "Thomas C Markhorst and Jan C van Gemert and Osman S Kayhan", "abstract": "  In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.\n", "link": "http://arxiv.org/abs/2409.08943v1", "date": "2024-09-13", "relevancy": 2.1663, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5507}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5398}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20Joint%20Image%20Denoising%20and%20Classification%20to%20the%20Edge&body=Title%3A%20Pushing%20Joint%20Image%20Denoising%20and%20Classification%20to%20the%20Edge%0AAuthor%3A%20Thomas%20C%20Markhorst%20and%20Jan%20C%20van%20Gemert%20and%20Osman%20S%20Kayhan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20jointly%20combine%20image%20classification%20and%20image%20denoising%2C%0Aaiming%20to%20enhance%20human%20perception%20of%20noisy%20images%20captured%20by%20edge%20devices%2C%0Alike%20low-light%20security%20cameras.%20In%20such%20settings%2C%20it%20is%20important%20to%20retain%0Athe%20ability%20of%20humans%20to%20verify%20the%20automatic%20classification%20decision%20and%20thus%0Ajointly%20denoise%20the%20image%20to%20enhance%20human%20perception.%20Since%20edge%20devices%20have%0Alittle%20computational%20power%2C%20we%20explicitly%20optimize%20for%20efficiency%20by%20proposing%0Aa%20novel%20architecture%20that%20integrates%20the%20two%20tasks.%20Additionally%2C%20we%20alter%20a%0ANeural%20Architecture%20Search%20%28NAS%29%20method%2C%20which%20searches%20for%20classifiers%20to%0Asearch%20for%20the%20integrated%20model%20while%20optimizing%20for%20a%20target%20latency%2C%0Aclassification%20accuracy%2C%20and%20denoising%20performance.%20The%20NAS%20architectures%0Aoutperform%20our%20manually%20designed%20alternatives%20in%20both%20denoising%20and%0Aclassification%2C%20offering%20a%20significant%20improvement%20to%20human%20perception.%20Our%0Aapproach%20empowers%20users%20to%20construct%20architectures%20tailored%20to%20domains%20like%0Amedical%20imaging%2C%20surveillance%20systems%2C%20and%20industrial%20inspections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520Joint%2520Image%2520Denoising%2520and%2520Classification%2520to%2520the%2520Edge%26entry.906535625%3DThomas%2520C%2520Markhorst%2520and%2520Jan%2520C%2520van%2520Gemert%2520and%2520Osman%2520S%2520Kayhan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520jointly%2520combine%2520image%2520classification%2520and%2520image%2520denoising%252C%250Aaiming%2520to%2520enhance%2520human%2520perception%2520of%2520noisy%2520images%2520captured%2520by%2520edge%2520devices%252C%250Alike%2520low-light%2520security%2520cameras.%2520In%2520such%2520settings%252C%2520it%2520is%2520important%2520to%2520retain%250Athe%2520ability%2520of%2520humans%2520to%2520verify%2520the%2520automatic%2520classification%2520decision%2520and%2520thus%250Ajointly%2520denoise%2520the%2520image%2520to%2520enhance%2520human%2520perception.%2520Since%2520edge%2520devices%2520have%250Alittle%2520computational%2520power%252C%2520we%2520explicitly%2520optimize%2520for%2520efficiency%2520by%2520proposing%250Aa%2520novel%2520architecture%2520that%2520integrates%2520the%2520two%2520tasks.%2520Additionally%252C%2520we%2520alter%2520a%250ANeural%2520Architecture%2520Search%2520%2528NAS%2529%2520method%252C%2520which%2520searches%2520for%2520classifiers%2520to%250Asearch%2520for%2520the%2520integrated%2520model%2520while%2520optimizing%2520for%2520a%2520target%2520latency%252C%250Aclassification%2520accuracy%252C%2520and%2520denoising%2520performance.%2520The%2520NAS%2520architectures%250Aoutperform%2520our%2520manually%2520designed%2520alternatives%2520in%2520both%2520denoising%2520and%250Aclassification%252C%2520offering%2520a%2520significant%2520improvement%2520to%2520human%2520perception.%2520Our%250Aapproach%2520empowers%2520users%2520to%2520construct%2520architectures%2520tailored%2520to%2520domains%2520like%250Amedical%2520imaging%252C%2520surveillance%2520systems%252C%2520and%2520industrial%2520inspections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20Joint%20Image%20Denoising%20and%20Classification%20to%20the%20Edge&entry.906535625=Thomas%20C%20Markhorst%20and%20Jan%20C%20van%20Gemert%20and%20Osman%20S%20Kayhan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20jointly%20combine%20image%20classification%20and%20image%20denoising%2C%0Aaiming%20to%20enhance%20human%20perception%20of%20noisy%20images%20captured%20by%20edge%20devices%2C%0Alike%20low-light%20security%20cameras.%20In%20such%20settings%2C%20it%20is%20important%20to%20retain%0Athe%20ability%20of%20humans%20to%20verify%20the%20automatic%20classification%20decision%20and%20thus%0Ajointly%20denoise%20the%20image%20to%20enhance%20human%20perception.%20Since%20edge%20devices%20have%0Alittle%20computational%20power%2C%20we%20explicitly%20optimize%20for%20efficiency%20by%20proposing%0Aa%20novel%20architecture%20that%20integrates%20the%20two%20tasks.%20Additionally%2C%20we%20alter%20a%0ANeural%20Architecture%20Search%20%28NAS%29%20method%2C%20which%20searches%20for%20classifiers%20to%0Asearch%20for%20the%20integrated%20model%20while%20optimizing%20for%20a%20target%20latency%2C%0Aclassification%20accuracy%2C%20and%20denoising%20performance.%20The%20NAS%20architectures%0Aoutperform%20our%20manually%20designed%20alternatives%20in%20both%20denoising%20and%0Aclassification%2C%20offering%20a%20significant%20improvement%20to%20human%20perception.%20Our%0Aapproach%20empowers%20users%20to%20construct%20architectures%20tailored%20to%20domains%20like%0Amedical%20imaging%2C%20surveillance%20systems%2C%20and%20industrial%20inspections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08943v1&entry.124074799=Read"},
{"title": "Uncertainty and Generalizability in Foundation Models for Earth\n  Observation", "author": "Raul Ramos-Pollan and Freddie Kalaitzis and Karthick Panner Selvam", "abstract": "  We take the perspective in which we want to design a downstream task (such as\nestimating vegetation coverage) on a certain area of interest (AOI) with a\nlimited labeling budget. By leveraging an existing Foundation Model (FM) we\nmust decide whether we train a downstream model on a different but label-rich\nAOI hoping it generalizes to our AOI, or we split labels in our AOI for\ntraining and validating. In either case, we face choices concerning what FM to\nuse, how to sample our AOI for labeling, etc. which affect both the performance\nand uncertainty of the results. In this work, we perform a large ablative study\nusing eight existing FMs on either Sentinel 1 or Sentinel 2 as input data, and\nthe classes from the ESA World Cover product as downstream tasks across eleven\nAOIs. We do repeated sampling and training, resulting in an ablation of some\n500K simple linear regression models. Our results show both the limits of\nspatial generalizability across AOIs and the power of FMs where we are able to\nget over 0.9 correlation coefficient between predictions and targets on\ndifferent chip level predictive tasks. And still, performance and uncertainty\nvary greatly across AOIs, tasks and FMs. We believe this is a key issue in\npractice, because there are many design decisions behind each FM and downstream\ntask (input modalities, sampling, architectures, pretraining, etc.) and usually\na downstream task designer is aware of and can decide upon a few of them.\nThrough this work, we advocate for the usage of the methodology herein\ndescribed (large ablations on reference global labels and simple probes), both\nwhen publishing new FMs, and to make informed decisions when designing\ndownstream tasks to use them.\n", "link": "http://arxiv.org/abs/2409.08744v1", "date": "2024-09-13", "relevancy": 2.1234, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5447}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20and%20Generalizability%20in%20Foundation%20Models%20for%20Earth%0A%20%20Observation&body=Title%3A%20Uncertainty%20and%20Generalizability%20in%20Foundation%20Models%20for%20Earth%0A%20%20Observation%0AAuthor%3A%20Raul%20Ramos-Pollan%20and%20Freddie%20Kalaitzis%20and%20Karthick%20Panner%20Selvam%0AAbstract%3A%20%20%20We%20take%20the%20perspective%20in%20which%20we%20want%20to%20design%20a%20downstream%20task%20%28such%20as%0Aestimating%20vegetation%20coverage%29%20on%20a%20certain%20area%20of%20interest%20%28AOI%29%20with%20a%0Alimited%20labeling%20budget.%20By%20leveraging%20an%20existing%20Foundation%20Model%20%28FM%29%20we%0Amust%20decide%20whether%20we%20train%20a%20downstream%20model%20on%20a%20different%20but%20label-rich%0AAOI%20hoping%20it%20generalizes%20to%20our%20AOI%2C%20or%20we%20split%20labels%20in%20our%20AOI%20for%0Atraining%20and%20validating.%20In%20either%20case%2C%20we%20face%20choices%20concerning%20what%20FM%20to%0Ause%2C%20how%20to%20sample%20our%20AOI%20for%20labeling%2C%20etc.%20which%20affect%20both%20the%20performance%0Aand%20uncertainty%20of%20the%20results.%20In%20this%20work%2C%20we%20perform%20a%20large%20ablative%20study%0Ausing%20eight%20existing%20FMs%20on%20either%20Sentinel%201%20or%20Sentinel%202%20as%20input%20data%2C%20and%0Athe%20classes%20from%20the%20ESA%20World%20Cover%20product%20as%20downstream%20tasks%20across%20eleven%0AAOIs.%20We%20do%20repeated%20sampling%20and%20training%2C%20resulting%20in%20an%20ablation%20of%20some%0A500K%20simple%20linear%20regression%20models.%20Our%20results%20show%20both%20the%20limits%20of%0Aspatial%20generalizability%20across%20AOIs%20and%20the%20power%20of%20FMs%20where%20we%20are%20able%20to%0Aget%20over%200.9%20correlation%20coefficient%20between%20predictions%20and%20targets%20on%0Adifferent%20chip%20level%20predictive%20tasks.%20And%20still%2C%20performance%20and%20uncertainty%0Avary%20greatly%20across%20AOIs%2C%20tasks%20and%20FMs.%20We%20believe%20this%20is%20a%20key%20issue%20in%0Apractice%2C%20because%20there%20are%20many%20design%20decisions%20behind%20each%20FM%20and%20downstream%0Atask%20%28input%20modalities%2C%20sampling%2C%20architectures%2C%20pretraining%2C%20etc.%29%20and%20usually%0Aa%20downstream%20task%20designer%20is%20aware%20of%20and%20can%20decide%20upon%20a%20few%20of%20them.%0AThrough%20this%20work%2C%20we%20advocate%20for%20the%20usage%20of%20the%20methodology%20herein%0Adescribed%20%28large%20ablations%20on%20reference%20global%20labels%20and%20simple%20probes%29%2C%20both%0Awhen%20publishing%20new%20FMs%2C%20and%20to%20make%20informed%20decisions%20when%20designing%0Adownstream%20tasks%20to%20use%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520and%2520Generalizability%2520in%2520Foundation%2520Models%2520for%2520Earth%250A%2520%2520Observation%26entry.906535625%3DRaul%2520Ramos-Pollan%2520and%2520Freddie%2520Kalaitzis%2520and%2520Karthick%2520Panner%2520Selvam%26entry.1292438233%3D%2520%2520We%2520take%2520the%2520perspective%2520in%2520which%2520we%2520want%2520to%2520design%2520a%2520downstream%2520task%2520%2528such%2520as%250Aestimating%2520vegetation%2520coverage%2529%2520on%2520a%2520certain%2520area%2520of%2520interest%2520%2528AOI%2529%2520with%2520a%250Alimited%2520labeling%2520budget.%2520By%2520leveraging%2520an%2520existing%2520Foundation%2520Model%2520%2528FM%2529%2520we%250Amust%2520decide%2520whether%2520we%2520train%2520a%2520downstream%2520model%2520on%2520a%2520different%2520but%2520label-rich%250AAOI%2520hoping%2520it%2520generalizes%2520to%2520our%2520AOI%252C%2520or%2520we%2520split%2520labels%2520in%2520our%2520AOI%2520for%250Atraining%2520and%2520validating.%2520In%2520either%2520case%252C%2520we%2520face%2520choices%2520concerning%2520what%2520FM%2520to%250Ause%252C%2520how%2520to%2520sample%2520our%2520AOI%2520for%2520labeling%252C%2520etc.%2520which%2520affect%2520both%2520the%2520performance%250Aand%2520uncertainty%2520of%2520the%2520results.%2520In%2520this%2520work%252C%2520we%2520perform%2520a%2520large%2520ablative%2520study%250Ausing%2520eight%2520existing%2520FMs%2520on%2520either%2520Sentinel%25201%2520or%2520Sentinel%25202%2520as%2520input%2520data%252C%2520and%250Athe%2520classes%2520from%2520the%2520ESA%2520World%2520Cover%2520product%2520as%2520downstream%2520tasks%2520across%2520eleven%250AAOIs.%2520We%2520do%2520repeated%2520sampling%2520and%2520training%252C%2520resulting%2520in%2520an%2520ablation%2520of%2520some%250A500K%2520simple%2520linear%2520regression%2520models.%2520Our%2520results%2520show%2520both%2520the%2520limits%2520of%250Aspatial%2520generalizability%2520across%2520AOIs%2520and%2520the%2520power%2520of%2520FMs%2520where%2520we%2520are%2520able%2520to%250Aget%2520over%25200.9%2520correlation%2520coefficient%2520between%2520predictions%2520and%2520targets%2520on%250Adifferent%2520chip%2520level%2520predictive%2520tasks.%2520And%2520still%252C%2520performance%2520and%2520uncertainty%250Avary%2520greatly%2520across%2520AOIs%252C%2520tasks%2520and%2520FMs.%2520We%2520believe%2520this%2520is%2520a%2520key%2520issue%2520in%250Apractice%252C%2520because%2520there%2520are%2520many%2520design%2520decisions%2520behind%2520each%2520FM%2520and%2520downstream%250Atask%2520%2528input%2520modalities%252C%2520sampling%252C%2520architectures%252C%2520pretraining%252C%2520etc.%2529%2520and%2520usually%250Aa%2520downstream%2520task%2520designer%2520is%2520aware%2520of%2520and%2520can%2520decide%2520upon%2520a%2520few%2520of%2520them.%250AThrough%2520this%2520work%252C%2520we%2520advocate%2520for%2520the%2520usage%2520of%2520the%2520methodology%2520herein%250Adescribed%2520%2528large%2520ablations%2520on%2520reference%2520global%2520labels%2520and%2520simple%2520probes%2529%252C%2520both%250Awhen%2520publishing%2520new%2520FMs%252C%2520and%2520to%2520make%2520informed%2520decisions%2520when%2520designing%250Adownstream%2520tasks%2520to%2520use%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20and%20Generalizability%20in%20Foundation%20Models%20for%20Earth%0A%20%20Observation&entry.906535625=Raul%20Ramos-Pollan%20and%20Freddie%20Kalaitzis%20and%20Karthick%20Panner%20Selvam&entry.1292438233=%20%20We%20take%20the%20perspective%20in%20which%20we%20want%20to%20design%20a%20downstream%20task%20%28such%20as%0Aestimating%20vegetation%20coverage%29%20on%20a%20certain%20area%20of%20interest%20%28AOI%29%20with%20a%0Alimited%20labeling%20budget.%20By%20leveraging%20an%20existing%20Foundation%20Model%20%28FM%29%20we%0Amust%20decide%20whether%20we%20train%20a%20downstream%20model%20on%20a%20different%20but%20label-rich%0AAOI%20hoping%20it%20generalizes%20to%20our%20AOI%2C%20or%20we%20split%20labels%20in%20our%20AOI%20for%0Atraining%20and%20validating.%20In%20either%20case%2C%20we%20face%20choices%20concerning%20what%20FM%20to%0Ause%2C%20how%20to%20sample%20our%20AOI%20for%20labeling%2C%20etc.%20which%20affect%20both%20the%20performance%0Aand%20uncertainty%20of%20the%20results.%20In%20this%20work%2C%20we%20perform%20a%20large%20ablative%20study%0Ausing%20eight%20existing%20FMs%20on%20either%20Sentinel%201%20or%20Sentinel%202%20as%20input%20data%2C%20and%0Athe%20classes%20from%20the%20ESA%20World%20Cover%20product%20as%20downstream%20tasks%20across%20eleven%0AAOIs.%20We%20do%20repeated%20sampling%20and%20training%2C%20resulting%20in%20an%20ablation%20of%20some%0A500K%20simple%20linear%20regression%20models.%20Our%20results%20show%20both%20the%20limits%20of%0Aspatial%20generalizability%20across%20AOIs%20and%20the%20power%20of%20FMs%20where%20we%20are%20able%20to%0Aget%20over%200.9%20correlation%20coefficient%20between%20predictions%20and%20targets%20on%0Adifferent%20chip%20level%20predictive%20tasks.%20And%20still%2C%20performance%20and%20uncertainty%0Avary%20greatly%20across%20AOIs%2C%20tasks%20and%20FMs.%20We%20believe%20this%20is%20a%20key%20issue%20in%0Apractice%2C%20because%20there%20are%20many%20design%20decisions%20behind%20each%20FM%20and%20downstream%0Atask%20%28input%20modalities%2C%20sampling%2C%20architectures%2C%20pretraining%2C%20etc.%29%20and%20usually%0Aa%20downstream%20task%20designer%20is%20aware%20of%20and%20can%20decide%20upon%20a%20few%20of%20them.%0AThrough%20this%20work%2C%20we%20advocate%20for%20the%20usage%20of%20the%20methodology%20herein%0Adescribed%20%28large%20ablations%20on%20reference%20global%20labels%20and%20simple%20probes%29%2C%20both%0Awhen%20publishing%20new%20FMs%2C%20and%20to%20make%20informed%20decisions%20when%20designing%0Adownstream%20tasks%20to%20use%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08744v1&entry.124074799=Read"},
{"title": "MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth\n  Estimation of Endoscopic Images", "author": "Zhiwei Wang and Ying Zhou and Shiquan He and Ting Li and Fan Huang and Qiang Ding and Xinxia Feng and Mei Liu and Qiang Li", "abstract": "  Photometric constraint is indispensable for self-supervised monocular depth\nestimation. It involves warping a source image onto a target view using\nestimated depth&pose, and then minimizing the difference between the warped and\ntarget images. However, the endoscopic built-in light causes significant\nbrightness fluctuations, and thus makes the photometric constraint unreliable.\nPrevious efforts only mitigate this relying on extra models to calibrate image\nbrightness. In this paper, we propose MonoPCC to address the brightness\ninconsistency radically by reshaping the photometric constraint into a cycle\nform. Instead of only warping the source image, MonoPCC constructs a closed\nloop consisting of two opposite forward-backward warping paths: from target to\nsource and then back to target. Thus, the target image finally receives an\nimage cycle-warped from itself, which naturally makes the constraint invariant\nto brightness changes. Moreover, MonoPCC transplants the source image's\nphase-frequency into the intermediate warped image to avoid structure lost, and\nalso stabilizes the training via an exponential moving average (EMA) strategy\nto avoid frequent changes in the forward warping. The comprehensive and\nextensive experimental results on four endoscopic datasets demonstrate that our\nproposed MonoPCC shows a great robustness to the brightness inconsistency, and\nexceeds other state-of-the-arts by reducing the absolute relative error by at\nleast 7.27%, 9.38%, 9.90% and 3.17%, respectively.\n", "link": "http://arxiv.org/abs/2404.16571v3", "date": "2024-09-13", "relevancy": 2.1206, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5394}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5265}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images&body=Title%3A%20MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images%0AAuthor%3A%20Zhiwei%20Wang%20and%20Ying%20Zhou%20and%20Shiquan%20He%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Qiang%20Ding%20and%20Xinxia%20Feng%20and%20Mei%20Liu%20and%20Qiang%20Li%0AAbstract%3A%20%20%20Photometric%20constraint%20is%20indispensable%20for%20self-supervised%20monocular%20depth%0Aestimation.%20It%20involves%20warping%20a%20source%20image%20onto%20a%20target%20view%20using%0Aestimated%20depth%26pose%2C%20and%20then%20minimizing%20the%20difference%20between%20the%20warped%20and%0Atarget%20images.%20However%2C%20the%20endoscopic%20built-in%20light%20causes%20significant%0Abrightness%20fluctuations%2C%20and%20thus%20makes%20the%20photometric%20constraint%20unreliable.%0APrevious%20efforts%20only%20mitigate%20this%20relying%20on%20extra%20models%20to%20calibrate%20image%0Abrightness.%20In%20this%20paper%2C%20we%20propose%20MonoPCC%20to%20address%20the%20brightness%0Ainconsistency%20radically%20by%20reshaping%20the%20photometric%20constraint%20into%20a%20cycle%0Aform.%20Instead%20of%20only%20warping%20the%20source%20image%2C%20MonoPCC%20constructs%20a%20closed%0Aloop%20consisting%20of%20two%20opposite%20forward-backward%20warping%20paths%3A%20from%20target%20to%0Asource%20and%20then%20back%20to%20target.%20Thus%2C%20the%20target%20image%20finally%20receives%20an%0Aimage%20cycle-warped%20from%20itself%2C%20which%20naturally%20makes%20the%20constraint%20invariant%0Ato%20brightness%20changes.%20Moreover%2C%20MonoPCC%20transplants%20the%20source%20image%27s%0Aphase-frequency%20into%20the%20intermediate%20warped%20image%20to%20avoid%20structure%20lost%2C%20and%0Aalso%20stabilizes%20the%20training%20via%20an%20exponential%20moving%20average%20%28EMA%29%20strategy%0Ato%20avoid%20frequent%20changes%20in%20the%20forward%20warping.%20The%20comprehensive%20and%0Aextensive%20experimental%20results%20on%20four%20endoscopic%20datasets%20demonstrate%20that%20our%0Aproposed%20MonoPCC%20shows%20a%20great%20robustness%20to%20the%20brightness%20inconsistency%2C%20and%0Aexceeds%20other%20state-of-the-arts%20by%20reducing%20the%20absolute%20relative%20error%20by%20at%0Aleast%207.27%25%2C%209.38%25%2C%209.90%25%20and%203.17%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16571v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoPCC%253A%2520Photometric-invariant%2520Cycle%2520Constraint%2520for%2520Monocular%2520Depth%250A%2520%2520Estimation%2520of%2520Endoscopic%2520Images%26entry.906535625%3DZhiwei%2520Wang%2520and%2520Ying%2520Zhou%2520and%2520Shiquan%2520He%2520and%2520Ting%2520Li%2520and%2520Fan%2520Huang%2520and%2520Qiang%2520Ding%2520and%2520Xinxia%2520Feng%2520and%2520Mei%2520Liu%2520and%2520Qiang%2520Li%26entry.1292438233%3D%2520%2520Photometric%2520constraint%2520is%2520indispensable%2520for%2520self-supervised%2520monocular%2520depth%250Aestimation.%2520It%2520involves%2520warping%2520a%2520source%2520image%2520onto%2520a%2520target%2520view%2520using%250Aestimated%2520depth%2526pose%252C%2520and%2520then%2520minimizing%2520the%2520difference%2520between%2520the%2520warped%2520and%250Atarget%2520images.%2520However%252C%2520the%2520endoscopic%2520built-in%2520light%2520causes%2520significant%250Abrightness%2520fluctuations%252C%2520and%2520thus%2520makes%2520the%2520photometric%2520constraint%2520unreliable.%250APrevious%2520efforts%2520only%2520mitigate%2520this%2520relying%2520on%2520extra%2520models%2520to%2520calibrate%2520image%250Abrightness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MonoPCC%2520to%2520address%2520the%2520brightness%250Ainconsistency%2520radically%2520by%2520reshaping%2520the%2520photometric%2520constraint%2520into%2520a%2520cycle%250Aform.%2520Instead%2520of%2520only%2520warping%2520the%2520source%2520image%252C%2520MonoPCC%2520constructs%2520a%2520closed%250Aloop%2520consisting%2520of%2520two%2520opposite%2520forward-backward%2520warping%2520paths%253A%2520from%2520target%2520to%250Asource%2520and%2520then%2520back%2520to%2520target.%2520Thus%252C%2520the%2520target%2520image%2520finally%2520receives%2520an%250Aimage%2520cycle-warped%2520from%2520itself%252C%2520which%2520naturally%2520makes%2520the%2520constraint%2520invariant%250Ato%2520brightness%2520changes.%2520Moreover%252C%2520MonoPCC%2520transplants%2520the%2520source%2520image%2527s%250Aphase-frequency%2520into%2520the%2520intermediate%2520warped%2520image%2520to%2520avoid%2520structure%2520lost%252C%2520and%250Aalso%2520stabilizes%2520the%2520training%2520via%2520an%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520strategy%250Ato%2520avoid%2520frequent%2520changes%2520in%2520the%2520forward%2520warping.%2520The%2520comprehensive%2520and%250Aextensive%2520experimental%2520results%2520on%2520four%2520endoscopic%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520MonoPCC%2520shows%2520a%2520great%2520robustness%2520to%2520the%2520brightness%2520inconsistency%252C%2520and%250Aexceeds%2520other%2520state-of-the-arts%2520by%2520reducing%2520the%2520absolute%2520relative%2520error%2520by%2520at%250Aleast%25207.27%2525%252C%25209.38%2525%252C%25209.90%2525%2520and%25203.17%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16571v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images&entry.906535625=Zhiwei%20Wang%20and%20Ying%20Zhou%20and%20Shiquan%20He%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Qiang%20Ding%20and%20Xinxia%20Feng%20and%20Mei%20Liu%20and%20Qiang%20Li&entry.1292438233=%20%20Photometric%20constraint%20is%20indispensable%20for%20self-supervised%20monocular%20depth%0Aestimation.%20It%20involves%20warping%20a%20source%20image%20onto%20a%20target%20view%20using%0Aestimated%20depth%26pose%2C%20and%20then%20minimizing%20the%20difference%20between%20the%20warped%20and%0Atarget%20images.%20However%2C%20the%20endoscopic%20built-in%20light%20causes%20significant%0Abrightness%20fluctuations%2C%20and%20thus%20makes%20the%20photometric%20constraint%20unreliable.%0APrevious%20efforts%20only%20mitigate%20this%20relying%20on%20extra%20models%20to%20calibrate%20image%0Abrightness.%20In%20this%20paper%2C%20we%20propose%20MonoPCC%20to%20address%20the%20brightness%0Ainconsistency%20radically%20by%20reshaping%20the%20photometric%20constraint%20into%20a%20cycle%0Aform.%20Instead%20of%20only%20warping%20the%20source%20image%2C%20MonoPCC%20constructs%20a%20closed%0Aloop%20consisting%20of%20two%20opposite%20forward-backward%20warping%20paths%3A%20from%20target%20to%0Asource%20and%20then%20back%20to%20target.%20Thus%2C%20the%20target%20image%20finally%20receives%20an%0Aimage%20cycle-warped%20from%20itself%2C%20which%20naturally%20makes%20the%20constraint%20invariant%0Ato%20brightness%20changes.%20Moreover%2C%20MonoPCC%20transplants%20the%20source%20image%27s%0Aphase-frequency%20into%20the%20intermediate%20warped%20image%20to%20avoid%20structure%20lost%2C%20and%0Aalso%20stabilizes%20the%20training%20via%20an%20exponential%20moving%20average%20%28EMA%29%20strategy%0Ato%20avoid%20frequent%20changes%20in%20the%20forward%20warping.%20The%20comprehensive%20and%0Aextensive%20experimental%20results%20on%20four%20endoscopic%20datasets%20demonstrate%20that%20our%0Aproposed%20MonoPCC%20shows%20a%20great%20robustness%20to%20the%20brightness%20inconsistency%2C%20and%0Aexceeds%20other%20state-of-the-arts%20by%20reducing%20the%20absolute%20relative%20error%20by%20at%0Aleast%207.27%25%2C%209.38%25%2C%209.90%25%20and%203.17%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16571v3&entry.124074799=Read"},
{"title": "QueryCAD: Grounded Question Answering for CAD Models", "author": "Claudius Kienle and Benjamin Alt and Darko Katic and Rainer J\u00e4kel", "abstract": "  CAD models are widely used in industry and are essential for robotic\nautomation processes. However, these models are rarely considered in novel\nAI-based approaches, such as the automatic synthesis of robot programs, as\nthere are no readily available methods that would allow CAD models to be\nincorporated for the analysis, interpretation, or extraction of information. To\naddress these limitations, we propose QueryCAD, the first system designed for\nCAD question answering, enabling the extraction of precise information from CAD\nmodels using natural language queries. QueryCAD incorporates SegCAD, an\nopen-vocabulary instance segmentation model we developed to identify and select\nspecific parts of the CAD model based on part descriptions. We further propose\na CAD question answering benchmark to evaluate QueryCAD and establish a\nfoundation for future research. Lastly, we integrate QueryCAD within an\nautomatic robot program synthesis framework, validating its ability to enhance\ndeep-learning solutions for robotics by enabling them to process CAD models\n(https://claudius-kienle.github.com/querycad).\n", "link": "http://arxiv.org/abs/2409.08704v1", "date": "2024-09-13", "relevancy": 2.1021, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QueryCAD%3A%20Grounded%20Question%20Answering%20for%20CAD%20Models&body=Title%3A%20QueryCAD%3A%20Grounded%20Question%20Answering%20for%20CAD%20Models%0AAuthor%3A%20Claudius%20Kienle%20and%20Benjamin%20Alt%20and%20Darko%20Katic%20and%20Rainer%20J%C3%A4kel%0AAbstract%3A%20%20%20CAD%20models%20are%20widely%20used%20in%20industry%20and%20are%20essential%20for%20robotic%0Aautomation%20processes.%20However%2C%20these%20models%20are%20rarely%20considered%20in%20novel%0AAI-based%20approaches%2C%20such%20as%20the%20automatic%20synthesis%20of%20robot%20programs%2C%20as%0Athere%20are%20no%20readily%20available%20methods%20that%20would%20allow%20CAD%20models%20to%20be%0Aincorporated%20for%20the%20analysis%2C%20interpretation%2C%20or%20extraction%20of%20information.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20QueryCAD%2C%20the%20first%20system%20designed%20for%0ACAD%20question%20answering%2C%20enabling%20the%20extraction%20of%20precise%20information%20from%20CAD%0Amodels%20using%20natural%20language%20queries.%20QueryCAD%20incorporates%20SegCAD%2C%20an%0Aopen-vocabulary%20instance%20segmentation%20model%20we%20developed%20to%20identify%20and%20select%0Aspecific%20parts%20of%20the%20CAD%20model%20based%20on%20part%20descriptions.%20We%20further%20propose%0Aa%20CAD%20question%20answering%20benchmark%20to%20evaluate%20QueryCAD%20and%20establish%20a%0Afoundation%20for%20future%20research.%20Lastly%2C%20we%20integrate%20QueryCAD%20within%20an%0Aautomatic%20robot%20program%20synthesis%20framework%2C%20validating%20its%20ability%20to%20enhance%0Adeep-learning%20solutions%20for%20robotics%20by%20enabling%20them%20to%20process%20CAD%20models%0A%28https%3A//claudius-kienle.github.com/querycad%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueryCAD%253A%2520Grounded%2520Question%2520Answering%2520for%2520CAD%2520Models%26entry.906535625%3DClaudius%2520Kienle%2520and%2520Benjamin%2520Alt%2520and%2520Darko%2520Katic%2520and%2520Rainer%2520J%25C3%25A4kel%26entry.1292438233%3D%2520%2520CAD%2520models%2520are%2520widely%2520used%2520in%2520industry%2520and%2520are%2520essential%2520for%2520robotic%250Aautomation%2520processes.%2520However%252C%2520these%2520models%2520are%2520rarely%2520considered%2520in%2520novel%250AAI-based%2520approaches%252C%2520such%2520as%2520the%2520automatic%2520synthesis%2520of%2520robot%2520programs%252C%2520as%250Athere%2520are%2520no%2520readily%2520available%2520methods%2520that%2520would%2520allow%2520CAD%2520models%2520to%2520be%250Aincorporated%2520for%2520the%2520analysis%252C%2520interpretation%252C%2520or%2520extraction%2520of%2520information.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520QueryCAD%252C%2520the%2520first%2520system%2520designed%2520for%250ACAD%2520question%2520answering%252C%2520enabling%2520the%2520extraction%2520of%2520precise%2520information%2520from%2520CAD%250Amodels%2520using%2520natural%2520language%2520queries.%2520QueryCAD%2520incorporates%2520SegCAD%252C%2520an%250Aopen-vocabulary%2520instance%2520segmentation%2520model%2520we%2520developed%2520to%2520identify%2520and%2520select%250Aspecific%2520parts%2520of%2520the%2520CAD%2520model%2520based%2520on%2520part%2520descriptions.%2520We%2520further%2520propose%250Aa%2520CAD%2520question%2520answering%2520benchmark%2520to%2520evaluate%2520QueryCAD%2520and%2520establish%2520a%250Afoundation%2520for%2520future%2520research.%2520Lastly%252C%2520we%2520integrate%2520QueryCAD%2520within%2520an%250Aautomatic%2520robot%2520program%2520synthesis%2520framework%252C%2520validating%2520its%2520ability%2520to%2520enhance%250Adeep-learning%2520solutions%2520for%2520robotics%2520by%2520enabling%2520them%2520to%2520process%2520CAD%2520models%250A%2528https%253A//claudius-kienle.github.com/querycad%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QueryCAD%3A%20Grounded%20Question%20Answering%20for%20CAD%20Models&entry.906535625=Claudius%20Kienle%20and%20Benjamin%20Alt%20and%20Darko%20Katic%20and%20Rainer%20J%C3%A4kel&entry.1292438233=%20%20CAD%20models%20are%20widely%20used%20in%20industry%20and%20are%20essential%20for%20robotic%0Aautomation%20processes.%20However%2C%20these%20models%20are%20rarely%20considered%20in%20novel%0AAI-based%20approaches%2C%20such%20as%20the%20automatic%20synthesis%20of%20robot%20programs%2C%20as%0Athere%20are%20no%20readily%20available%20methods%20that%20would%20allow%20CAD%20models%20to%20be%0Aincorporated%20for%20the%20analysis%2C%20interpretation%2C%20or%20extraction%20of%20information.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20QueryCAD%2C%20the%20first%20system%20designed%20for%0ACAD%20question%20answering%2C%20enabling%20the%20extraction%20of%20precise%20information%20from%20CAD%0Amodels%20using%20natural%20language%20queries.%20QueryCAD%20incorporates%20SegCAD%2C%20an%0Aopen-vocabulary%20instance%20segmentation%20model%20we%20developed%20to%20identify%20and%20select%0Aspecific%20parts%20of%20the%20CAD%20model%20based%20on%20part%20descriptions.%20We%20further%20propose%0Aa%20CAD%20question%20answering%20benchmark%20to%20evaluate%20QueryCAD%20and%20establish%20a%0Afoundation%20for%20future%20research.%20Lastly%2C%20we%20integrate%20QueryCAD%20within%20an%0Aautomatic%20robot%20program%20synthesis%20framework%2C%20validating%20its%20ability%20to%20enhance%0Adeep-learning%20solutions%20for%20robotics%20by%20enabling%20them%20to%20process%20CAD%20models%0A%28https%3A//claudius-kienle.github.com/querycad%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08704v1&entry.124074799=Read"},
{"title": "Kinect Calibration and Data Optimization For Anthropometric Parameters", "author": "M. S. Gokmen and M. Akbaba and O. Findik", "abstract": "  Recently, through development of several 3d vision systems, widely used in\nvarious applications, medical and biometric fields. Microsoft kinect sensor\nhave been most of used camera among 3d vision systems. Microsoft kinect sensor\ncan obtain depth images of a scene and 3d coordinates of human joints. Thus,\nanthropometric features can extractable easily. Anthropometric feature and 3d\njoint coordinate raw datas which captured from kinect sensor is unstable. The\nstrongest reason for this, datas vary by distance between joints of individual\nand location of kinect sensor. Consequently, usage of this datas without kinect\ncalibration and data optimization does not result in sufficient and healthy. In\nthis study, proposed a novel method to calibrating kinect sensor and optimizing\nskeleton features. Results indicate that the proposed method is quite effective\nand worthy of further study in more general scenarios.\n", "link": "http://arxiv.org/abs/2409.08847v1", "date": "2024-09-13", "relevancy": 2.0917, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5429}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5198}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinect%20Calibration%20and%20Data%20Optimization%20For%20Anthropometric%20Parameters&body=Title%3A%20Kinect%20Calibration%20and%20Data%20Optimization%20For%20Anthropometric%20Parameters%0AAuthor%3A%20M.%20S.%20Gokmen%20and%20M.%20Akbaba%20and%20O.%20Findik%0AAbstract%3A%20%20%20Recently%2C%20through%20development%20of%20several%203d%20vision%20systems%2C%20widely%20used%20in%0Avarious%20applications%2C%20medical%20and%20biometric%20fields.%20Microsoft%20kinect%20sensor%0Ahave%20been%20most%20of%20used%20camera%20among%203d%20vision%20systems.%20Microsoft%20kinect%20sensor%0Acan%20obtain%20depth%20images%20of%20a%20scene%20and%203d%20coordinates%20of%20human%20joints.%20Thus%2C%0Aanthropometric%20features%20can%20extractable%20easily.%20Anthropometric%20feature%20and%203d%0Ajoint%20coordinate%20raw%20datas%20which%20captured%20from%20kinect%20sensor%20is%20unstable.%20The%0Astrongest%20reason%20for%20this%2C%20datas%20vary%20by%20distance%20between%20joints%20of%20individual%0Aand%20location%20of%20kinect%20sensor.%20Consequently%2C%20usage%20of%20this%20datas%20without%20kinect%0Acalibration%20and%20data%20optimization%20does%20not%20result%20in%20sufficient%20and%20healthy.%20In%0Athis%20study%2C%20proposed%20a%20novel%20method%20to%20calibrating%20kinect%20sensor%20and%20optimizing%0Askeleton%20features.%20Results%20indicate%20that%20the%20proposed%20method%20is%20quite%20effective%0Aand%20worthy%20of%20further%20study%20in%20more%20general%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinect%2520Calibration%2520and%2520Data%2520Optimization%2520For%2520Anthropometric%2520Parameters%26entry.906535625%3DM.%2520S.%2520Gokmen%2520and%2520M.%2520Akbaba%2520and%2520O.%2520Findik%26entry.1292438233%3D%2520%2520Recently%252C%2520through%2520development%2520of%2520several%25203d%2520vision%2520systems%252C%2520widely%2520used%2520in%250Avarious%2520applications%252C%2520medical%2520and%2520biometric%2520fields.%2520Microsoft%2520kinect%2520sensor%250Ahave%2520been%2520most%2520of%2520used%2520camera%2520among%25203d%2520vision%2520systems.%2520Microsoft%2520kinect%2520sensor%250Acan%2520obtain%2520depth%2520images%2520of%2520a%2520scene%2520and%25203d%2520coordinates%2520of%2520human%2520joints.%2520Thus%252C%250Aanthropometric%2520features%2520can%2520extractable%2520easily.%2520Anthropometric%2520feature%2520and%25203d%250Ajoint%2520coordinate%2520raw%2520datas%2520which%2520captured%2520from%2520kinect%2520sensor%2520is%2520unstable.%2520The%250Astrongest%2520reason%2520for%2520this%252C%2520datas%2520vary%2520by%2520distance%2520between%2520joints%2520of%2520individual%250Aand%2520location%2520of%2520kinect%2520sensor.%2520Consequently%252C%2520usage%2520of%2520this%2520datas%2520without%2520kinect%250Acalibration%2520and%2520data%2520optimization%2520does%2520not%2520result%2520in%2520sufficient%2520and%2520healthy.%2520In%250Athis%2520study%252C%2520proposed%2520a%2520novel%2520method%2520to%2520calibrating%2520kinect%2520sensor%2520and%2520optimizing%250Askeleton%2520features.%2520Results%2520indicate%2520that%2520the%2520proposed%2520method%2520is%2520quite%2520effective%250Aand%2520worthy%2520of%2520further%2520study%2520in%2520more%2520general%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinect%20Calibration%20and%20Data%20Optimization%20For%20Anthropometric%20Parameters&entry.906535625=M.%20S.%20Gokmen%20and%20M.%20Akbaba%20and%20O.%20Findik&entry.1292438233=%20%20Recently%2C%20through%20development%20of%20several%203d%20vision%20systems%2C%20widely%20used%20in%0Avarious%20applications%2C%20medical%20and%20biometric%20fields.%20Microsoft%20kinect%20sensor%0Ahave%20been%20most%20of%20used%20camera%20among%203d%20vision%20systems.%20Microsoft%20kinect%20sensor%0Acan%20obtain%20depth%20images%20of%20a%20scene%20and%203d%20coordinates%20of%20human%20joints.%20Thus%2C%0Aanthropometric%20features%20can%20extractable%20easily.%20Anthropometric%20feature%20and%203d%0Ajoint%20coordinate%20raw%20datas%20which%20captured%20from%20kinect%20sensor%20is%20unstable.%20The%0Astrongest%20reason%20for%20this%2C%20datas%20vary%20by%20distance%20between%20joints%20of%20individual%0Aand%20location%20of%20kinect%20sensor.%20Consequently%2C%20usage%20of%20this%20datas%20without%20kinect%0Acalibration%20and%20data%20optimization%20does%20not%20result%20in%20sufficient%20and%20healthy.%20In%0Athis%20study%2C%20proposed%20a%20novel%20method%20to%20calibrating%20kinect%20sensor%20and%20optimizing%0Askeleton%20features.%20Results%20indicate%20that%20the%20proposed%20method%20is%20quite%20effective%0Aand%20worthy%20of%20further%20study%20in%20more%20general%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08847v1&entry.124074799=Read"},
{"title": "D2-MLP: Dynamic Decomposed MLP Mixer for Medical Image Segmentation", "author": "Jin Yang and Xiaobing Yu and Peijie Qiu", "abstract": "  Convolutional neural networks are widely used in various segmentation tasks\nin medical images. However, they are challenged to learn global features\nadaptively due to the inherent locality of convolutional operations. In\ncontrast, MLP Mixers are proposed as a backbone to learn global information\nacross channels with low complexity. However, they cannot capture spatial\nfeatures efficiently. Additionally, they lack effective mechanisms to fuse and\nmix features adaptively. To tackle these limitations, we propose a novel\nDynamic Decomposed Mixer module. It is designed to employ novel Mixers to\nextract features and aggregate information across different spatial locations\nand channels. Additionally, it employs novel dynamic mixing mechanisms to model\ninter-dependencies between channel and spatial feature representations and to\nfuse them adaptively. Subsequently, we incorporate it into a U-shaped\nTransformer-based architecture to generate a novel network, termed the Dynamic\nDecomposed MLP Mixer. We evaluated it for medical image segmentation on two\ndatasets, and it achieved superior segmentation performance than other\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2409.08905v1", "date": "2024-09-13", "relevancy": 2.081, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5297}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.523}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D2-MLP%3A%20Dynamic%20Decomposed%20MLP%20Mixer%20for%20Medical%20Image%20Segmentation&body=Title%3A%20D2-MLP%3A%20Dynamic%20Decomposed%20MLP%20Mixer%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Jin%20Yang%20and%20Xiaobing%20Yu%20and%20Peijie%20Qiu%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20are%20widely%20used%20in%20various%20segmentation%20tasks%0Ain%20medical%20images.%20However%2C%20they%20are%20challenged%20to%20learn%20global%20features%0Aadaptively%20due%20to%20the%20inherent%20locality%20of%20convolutional%20operations.%20In%0Acontrast%2C%20MLP%20Mixers%20are%20proposed%20as%20a%20backbone%20to%20learn%20global%20information%0Aacross%20channels%20with%20low%20complexity.%20However%2C%20they%20cannot%20capture%20spatial%0Afeatures%20efficiently.%20Additionally%2C%20they%20lack%20effective%20mechanisms%20to%20fuse%20and%0Amix%20features%20adaptively.%20To%20tackle%20these%20limitations%2C%20we%20propose%20a%20novel%0ADynamic%20Decomposed%20Mixer%20module.%20It%20is%20designed%20to%20employ%20novel%20Mixers%20to%0Aextract%20features%20and%20aggregate%20information%20across%20different%20spatial%20locations%0Aand%20channels.%20Additionally%2C%20it%20employs%20novel%20dynamic%20mixing%20mechanisms%20to%20model%0Ainter-dependencies%20between%20channel%20and%20spatial%20feature%20representations%20and%20to%0Afuse%20them%20adaptively.%20Subsequently%2C%20we%20incorporate%20it%20into%20a%20U-shaped%0ATransformer-based%20architecture%20to%20generate%20a%20novel%20network%2C%20termed%20the%20Dynamic%0ADecomposed%20MLP%20Mixer.%20We%20evaluated%20it%20for%20medical%20image%20segmentation%20on%20two%0Adatasets%2C%20and%20it%20achieved%20superior%20segmentation%20performance%20than%20other%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD2-MLP%253A%2520Dynamic%2520Decomposed%2520MLP%2520Mixer%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DJin%2520Yang%2520and%2520Xiaobing%2520Yu%2520and%2520Peijie%2520Qiu%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520are%2520widely%2520used%2520in%2520various%2520segmentation%2520tasks%250Ain%2520medical%2520images.%2520However%252C%2520they%2520are%2520challenged%2520to%2520learn%2520global%2520features%250Aadaptively%2520due%2520to%2520the%2520inherent%2520locality%2520of%2520convolutional%2520operations.%2520In%250Acontrast%252C%2520MLP%2520Mixers%2520are%2520proposed%2520as%2520a%2520backbone%2520to%2520learn%2520global%2520information%250Aacross%2520channels%2520with%2520low%2520complexity.%2520However%252C%2520they%2520cannot%2520capture%2520spatial%250Afeatures%2520efficiently.%2520Additionally%252C%2520they%2520lack%2520effective%2520mechanisms%2520to%2520fuse%2520and%250Amix%2520features%2520adaptively.%2520To%2520tackle%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250ADynamic%2520Decomposed%2520Mixer%2520module.%2520It%2520is%2520designed%2520to%2520employ%2520novel%2520Mixers%2520to%250Aextract%2520features%2520and%2520aggregate%2520information%2520across%2520different%2520spatial%2520locations%250Aand%2520channels.%2520Additionally%252C%2520it%2520employs%2520novel%2520dynamic%2520mixing%2520mechanisms%2520to%2520model%250Ainter-dependencies%2520between%2520channel%2520and%2520spatial%2520feature%2520representations%2520and%2520to%250Afuse%2520them%2520adaptively.%2520Subsequently%252C%2520we%2520incorporate%2520it%2520into%2520a%2520U-shaped%250ATransformer-based%2520architecture%2520to%2520generate%2520a%2520novel%2520network%252C%2520termed%2520the%2520Dynamic%250ADecomposed%2520MLP%2520Mixer.%2520We%2520evaluated%2520it%2520for%2520medical%2520image%2520segmentation%2520on%2520two%250Adatasets%252C%2520and%2520it%2520achieved%2520superior%2520segmentation%2520performance%2520than%2520other%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D2-MLP%3A%20Dynamic%20Decomposed%20MLP%20Mixer%20for%20Medical%20Image%20Segmentation&entry.906535625=Jin%20Yang%20and%20Xiaobing%20Yu%20and%20Peijie%20Qiu&entry.1292438233=%20%20Convolutional%20neural%20networks%20are%20widely%20used%20in%20various%20segmentation%20tasks%0Ain%20medical%20images.%20However%2C%20they%20are%20challenged%20to%20learn%20global%20features%0Aadaptively%20due%20to%20the%20inherent%20locality%20of%20convolutional%20operations.%20In%0Acontrast%2C%20MLP%20Mixers%20are%20proposed%20as%20a%20backbone%20to%20learn%20global%20information%0Aacross%20channels%20with%20low%20complexity.%20However%2C%20they%20cannot%20capture%20spatial%0Afeatures%20efficiently.%20Additionally%2C%20they%20lack%20effective%20mechanisms%20to%20fuse%20and%0Amix%20features%20adaptively.%20To%20tackle%20these%20limitations%2C%20we%20propose%20a%20novel%0ADynamic%20Decomposed%20Mixer%20module.%20It%20is%20designed%20to%20employ%20novel%20Mixers%20to%0Aextract%20features%20and%20aggregate%20information%20across%20different%20spatial%20locations%0Aand%20channels.%20Additionally%2C%20it%20employs%20novel%20dynamic%20mixing%20mechanisms%20to%20model%0Ainter-dependencies%20between%20channel%20and%20spatial%20feature%20representations%20and%20to%0Afuse%20them%20adaptively.%20Subsequently%2C%20we%20incorporate%20it%20into%20a%20U-shaped%0ATransformer-based%20architecture%20to%20generate%20a%20novel%20network%2C%20termed%20the%20Dynamic%0ADecomposed%20MLP%20Mixer.%20We%20evaluated%20it%20for%20medical%20image%20segmentation%20on%20two%0Adatasets%2C%20and%20it%20achieved%20superior%20segmentation%20performance%20than%20other%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08905v1&entry.124074799=Read"},
{"title": "Pseudo-Prompt Generating in Pre-trained Vision-Language Models for\n  Multi-Label Medical Image Classification", "author": "Yaoqin Ye and Junjie Zhang and Hongwei Shi", "abstract": "  The task of medical image recognition is notably complicated by the presence\nof varied and multiple pathological indications, presenting a unique challenge\nin multi-label classification with unseen labels. This complexity underlines\nthe need for computer-aided diagnosis methods employing multi-label zero-shot\nlearning. Recent advancements in pre-trained vision-language models (VLMs) have\nshowcased notable zero-shot classification abilities on medical images.\nHowever, these methods have limitations on leveraging extensive pre-trained\nknowledge from broader image datasets, and often depend on manual prompt\nconstruction by expert radiologists. By automating the process of prompt\ntuning, prompt learning techniques have emerged as an efficient way to adapt\nVLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in\nperforming class-specific prompts on unseen categories, limiting\ngeneralizability in fine-grained scenarios. To overcome these constraints, we\nintroduce a novel prompt generation approach inspirited by text generation in\nnatural language processing (NLP). Our method, named Pseudo-Prompt Generating\n(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring\na RNN-based decoder, PsPG autoregressively generates class-tailored embedding\nvectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label\nchest radiograph datasets affirm the superiority of our approach against\nleading medical vision-language and multi-label prompt learning methods. The\nsource code is available at https://github.com/fallingnight/PsPG\n", "link": "http://arxiv.org/abs/2405.06468v3", "date": "2024-09-13", "relevancy": 2.0724, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5234}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5171}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification&body=Title%3A%20Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification%0AAuthor%3A%20Yaoqin%20Ye%20and%20Junjie%20Zhang%20and%20Hongwei%20Shi%0AAbstract%3A%20%20%20The%20task%20of%20medical%20image%20recognition%20is%20notably%20complicated%20by%20the%20presence%0Aof%20varied%20and%20multiple%20pathological%20indications%2C%20presenting%20a%20unique%20challenge%0Ain%20multi-label%20classification%20with%20unseen%20labels.%20This%20complexity%20underlines%0Athe%20need%20for%20computer-aided%20diagnosis%20methods%20employing%20multi-label%20zero-shot%0Alearning.%20Recent%20advancements%20in%20pre-trained%20vision-language%20models%20%28VLMs%29%20have%0Ashowcased%20notable%20zero-shot%20classification%20abilities%20on%20medical%20images.%0AHowever%2C%20these%20methods%20have%20limitations%20on%20leveraging%20extensive%20pre-trained%0Aknowledge%20from%20broader%20image%20datasets%2C%20and%20often%20depend%20on%20manual%20prompt%0Aconstruction%20by%20expert%20radiologists.%20By%20automating%20the%20process%20of%20prompt%0Atuning%2C%20prompt%20learning%20techniques%20have%20emerged%20as%20an%20efficient%20way%20to%20adapt%0AVLMs%20to%20downstream%20tasks.%20Yet%2C%20existing%20CoOp-based%20strategies%20fall%20short%20in%0Aperforming%20class-specific%20prompts%20on%20unseen%20categories%2C%20limiting%0Ageneralizability%20in%20fine-grained%20scenarios.%20To%20overcome%20these%20constraints%2C%20we%0Aintroduce%20a%20novel%20prompt%20generation%20approach%20inspirited%20by%20text%20generation%20in%0Anatural%20language%20processing%20%28NLP%29.%20Our%20method%2C%20named%20Pseudo-Prompt%20Generating%0A%28PsPG%29%2C%20capitalizes%20on%20the%20priori%20knowledge%20of%20multi-modal%20features.%20Featuring%0Aa%20RNN-based%20decoder%2C%20PsPG%20autoregressively%20generates%20class-tailored%20embedding%0Avectors%2C%20i.e.%2C%20pseudo-prompts.%20Comparative%20evaluations%20on%20various%20multi-label%0Achest%20radiograph%20datasets%20affirm%20the%20superiority%20of%20our%20approach%20against%0Aleading%20medical%20vision-language%20and%20multi-label%20prompt%20learning%20methods.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/fallingnight/PsPG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06468v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Prompt%2520Generating%2520in%2520Pre-trained%2520Vision-Language%2520Models%2520for%250A%2520%2520Multi-Label%2520Medical%2520Image%2520Classification%26entry.906535625%3DYaoqin%2520Ye%2520and%2520Junjie%2520Zhang%2520and%2520Hongwei%2520Shi%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520medical%2520image%2520recognition%2520is%2520notably%2520complicated%2520by%2520the%2520presence%250Aof%2520varied%2520and%2520multiple%2520pathological%2520indications%252C%2520presenting%2520a%2520unique%2520challenge%250Ain%2520multi-label%2520classification%2520with%2520unseen%2520labels.%2520This%2520complexity%2520underlines%250Athe%2520need%2520for%2520computer-aided%2520diagnosis%2520methods%2520employing%2520multi-label%2520zero-shot%250Alearning.%2520Recent%2520advancements%2520in%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520have%250Ashowcased%2520notable%2520zero-shot%2520classification%2520abilities%2520on%2520medical%2520images.%250AHowever%252C%2520these%2520methods%2520have%2520limitations%2520on%2520leveraging%2520extensive%2520pre-trained%250Aknowledge%2520from%2520broader%2520image%2520datasets%252C%2520and%2520often%2520depend%2520on%2520manual%2520prompt%250Aconstruction%2520by%2520expert%2520radiologists.%2520By%2520automating%2520the%2520process%2520of%2520prompt%250Atuning%252C%2520prompt%2520learning%2520techniques%2520have%2520emerged%2520as%2520an%2520efficient%2520way%2520to%2520adapt%250AVLMs%2520to%2520downstream%2520tasks.%2520Yet%252C%2520existing%2520CoOp-based%2520strategies%2520fall%2520short%2520in%250Aperforming%2520class-specific%2520prompts%2520on%2520unseen%2520categories%252C%2520limiting%250Ageneralizability%2520in%2520fine-grained%2520scenarios.%2520To%2520overcome%2520these%2520constraints%252C%2520we%250Aintroduce%2520a%2520novel%2520prompt%2520generation%2520approach%2520inspirited%2520by%2520text%2520generation%2520in%250Anatural%2520language%2520processing%2520%2528NLP%2529.%2520Our%2520method%252C%2520named%2520Pseudo-Prompt%2520Generating%250A%2528PsPG%2529%252C%2520capitalizes%2520on%2520the%2520priori%2520knowledge%2520of%2520multi-modal%2520features.%2520Featuring%250Aa%2520RNN-based%2520decoder%252C%2520PsPG%2520autoregressively%2520generates%2520class-tailored%2520embedding%250Avectors%252C%2520i.e.%252C%2520pseudo-prompts.%2520Comparative%2520evaluations%2520on%2520various%2520multi-label%250Achest%2520radiograph%2520datasets%2520affirm%2520the%2520superiority%2520of%2520our%2520approach%2520against%250Aleading%2520medical%2520vision-language%2520and%2520multi-label%2520prompt%2520learning%2520methods.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/fallingnight/PsPG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06468v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification&entry.906535625=Yaoqin%20Ye%20and%20Junjie%20Zhang%20and%20Hongwei%20Shi&entry.1292438233=%20%20The%20task%20of%20medical%20image%20recognition%20is%20notably%20complicated%20by%20the%20presence%0Aof%20varied%20and%20multiple%20pathological%20indications%2C%20presenting%20a%20unique%20challenge%0Ain%20multi-label%20classification%20with%20unseen%20labels.%20This%20complexity%20underlines%0Athe%20need%20for%20computer-aided%20diagnosis%20methods%20employing%20multi-label%20zero-shot%0Alearning.%20Recent%20advancements%20in%20pre-trained%20vision-language%20models%20%28VLMs%29%20have%0Ashowcased%20notable%20zero-shot%20classification%20abilities%20on%20medical%20images.%0AHowever%2C%20these%20methods%20have%20limitations%20on%20leveraging%20extensive%20pre-trained%0Aknowledge%20from%20broader%20image%20datasets%2C%20and%20often%20depend%20on%20manual%20prompt%0Aconstruction%20by%20expert%20radiologists.%20By%20automating%20the%20process%20of%20prompt%0Atuning%2C%20prompt%20learning%20techniques%20have%20emerged%20as%20an%20efficient%20way%20to%20adapt%0AVLMs%20to%20downstream%20tasks.%20Yet%2C%20existing%20CoOp-based%20strategies%20fall%20short%20in%0Aperforming%20class-specific%20prompts%20on%20unseen%20categories%2C%20limiting%0Ageneralizability%20in%20fine-grained%20scenarios.%20To%20overcome%20these%20constraints%2C%20we%0Aintroduce%20a%20novel%20prompt%20generation%20approach%20inspirited%20by%20text%20generation%20in%0Anatural%20language%20processing%20%28NLP%29.%20Our%20method%2C%20named%20Pseudo-Prompt%20Generating%0A%28PsPG%29%2C%20capitalizes%20on%20the%20priori%20knowledge%20of%20multi-modal%20features.%20Featuring%0Aa%20RNN-based%20decoder%2C%20PsPG%20autoregressively%20generates%20class-tailored%20embedding%0Avectors%2C%20i.e.%2C%20pseudo-prompts.%20Comparative%20evaluations%20on%20various%20multi-label%0Achest%20radiograph%20datasets%20affirm%20the%20superiority%20of%20our%20approach%20against%0Aleading%20medical%20vision-language%20and%20multi-label%20prompt%20learning%20methods.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/fallingnight/PsPG%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06468v3&entry.124074799=Read"},
{"title": "Affective Computing Has Changed: The Foundation Model Disruption", "author": "Bj\u00f6rn Schuller and Adria Mallol-Ragolta and Alejandro Pe\u00f1a Almansa and Iosif Tsangko and Mostafa M. Amin and Anastasia Semertzidou and Lukas Christ and Shahin Amiriparian", "abstract": "  The dawn of Foundation Models has on the one hand revolutionised a wide range\nof research problems, and, on the other hand, democratised the access and use\nof AI-based tools by the general public. We even observe an incursion of these\nmodels into disciplines related to human psychology, such as the Affective\nComputing domain, suggesting their affective, emerging capabilities. In this\nwork, we aim to raise awareness of the power of Foundation Models in the field\nof Affective Computing by synthetically generating and analysing multimodal\naffective data, focusing on vision, linguistics, and speech (acoustics). We\nalso discuss some fundamental problems, such as ethical issues and regulatory\naspects, related to the use of Foundation Models in this research area.\n", "link": "http://arxiv.org/abs/2409.08907v1", "date": "2024-09-13", "relevancy": 2.0402, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Affective%20Computing%20Has%20Changed%3A%20The%20Foundation%20Model%20Disruption&body=Title%3A%20Affective%20Computing%20Has%20Changed%3A%20The%20Foundation%20Model%20Disruption%0AAuthor%3A%20Bj%C3%B6rn%20Schuller%20and%20Adria%20Mallol-Ragolta%20and%20Alejandro%20Pe%C3%B1a%20Almansa%20and%20Iosif%20Tsangko%20and%20Mostafa%20M.%20Amin%20and%20Anastasia%20Semertzidou%20and%20Lukas%20Christ%20and%20Shahin%20Amiriparian%0AAbstract%3A%20%20%20The%20dawn%20of%20Foundation%20Models%20has%20on%20the%20one%20hand%20revolutionised%20a%20wide%20range%0Aof%20research%20problems%2C%20and%2C%20on%20the%20other%20hand%2C%20democratised%20the%20access%20and%20use%0Aof%20AI-based%20tools%20by%20the%20general%20public.%20We%20even%20observe%20an%20incursion%20of%20these%0Amodels%20into%20disciplines%20related%20to%20human%20psychology%2C%20such%20as%20the%20Affective%0AComputing%20domain%2C%20suggesting%20their%20affective%2C%20emerging%20capabilities.%20In%20this%0Awork%2C%20we%20aim%20to%20raise%20awareness%20of%20the%20power%20of%20Foundation%20Models%20in%20the%20field%0Aof%20Affective%20Computing%20by%20synthetically%20generating%20and%20analysing%20multimodal%0Aaffective%20data%2C%20focusing%20on%20vision%2C%20linguistics%2C%20and%20speech%20%28acoustics%29.%20We%0Aalso%20discuss%20some%20fundamental%20problems%2C%20such%20as%20ethical%20issues%20and%20regulatory%0Aaspects%2C%20related%20to%20the%20use%20of%20Foundation%20Models%20in%20this%20research%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffective%2520Computing%2520Has%2520Changed%253A%2520The%2520Foundation%2520Model%2520Disruption%26entry.906535625%3DBj%25C3%25B6rn%2520Schuller%2520and%2520Adria%2520Mallol-Ragolta%2520and%2520Alejandro%2520Pe%25C3%25B1a%2520Almansa%2520and%2520Iosif%2520Tsangko%2520and%2520Mostafa%2520M.%2520Amin%2520and%2520Anastasia%2520Semertzidou%2520and%2520Lukas%2520Christ%2520and%2520Shahin%2520Amiriparian%26entry.1292438233%3D%2520%2520The%2520dawn%2520of%2520Foundation%2520Models%2520has%2520on%2520the%2520one%2520hand%2520revolutionised%2520a%2520wide%2520range%250Aof%2520research%2520problems%252C%2520and%252C%2520on%2520the%2520other%2520hand%252C%2520democratised%2520the%2520access%2520and%2520use%250Aof%2520AI-based%2520tools%2520by%2520the%2520general%2520public.%2520We%2520even%2520observe%2520an%2520incursion%2520of%2520these%250Amodels%2520into%2520disciplines%2520related%2520to%2520human%2520psychology%252C%2520such%2520as%2520the%2520Affective%250AComputing%2520domain%252C%2520suggesting%2520their%2520affective%252C%2520emerging%2520capabilities.%2520In%2520this%250Awork%252C%2520we%2520aim%2520to%2520raise%2520awareness%2520of%2520the%2520power%2520of%2520Foundation%2520Models%2520in%2520the%2520field%250Aof%2520Affective%2520Computing%2520by%2520synthetically%2520generating%2520and%2520analysing%2520multimodal%250Aaffective%2520data%252C%2520focusing%2520on%2520vision%252C%2520linguistics%252C%2520and%2520speech%2520%2528acoustics%2529.%2520We%250Aalso%2520discuss%2520some%2520fundamental%2520problems%252C%2520such%2520as%2520ethical%2520issues%2520and%2520regulatory%250Aaspects%252C%2520related%2520to%2520the%2520use%2520of%2520Foundation%2520Models%2520in%2520this%2520research%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Affective%20Computing%20Has%20Changed%3A%20The%20Foundation%20Model%20Disruption&entry.906535625=Bj%C3%B6rn%20Schuller%20and%20Adria%20Mallol-Ragolta%20and%20Alejandro%20Pe%C3%B1a%20Almansa%20and%20Iosif%20Tsangko%20and%20Mostafa%20M.%20Amin%20and%20Anastasia%20Semertzidou%20and%20Lukas%20Christ%20and%20Shahin%20Amiriparian&entry.1292438233=%20%20The%20dawn%20of%20Foundation%20Models%20has%20on%20the%20one%20hand%20revolutionised%20a%20wide%20range%0Aof%20research%20problems%2C%20and%2C%20on%20the%20other%20hand%2C%20democratised%20the%20access%20and%20use%0Aof%20AI-based%20tools%20by%20the%20general%20public.%20We%20even%20observe%20an%20incursion%20of%20these%0Amodels%20into%20disciplines%20related%20to%20human%20psychology%2C%20such%20as%20the%20Affective%0AComputing%20domain%2C%20suggesting%20their%20affective%2C%20emerging%20capabilities.%20In%20this%0Awork%2C%20we%20aim%20to%20raise%20awareness%20of%20the%20power%20of%20Foundation%20Models%20in%20the%20field%0Aof%20Affective%20Computing%20by%20synthetically%20generating%20and%20analysing%20multimodal%0Aaffective%20data%2C%20focusing%20on%20vision%2C%20linguistics%2C%20and%20speech%20%28acoustics%29.%20We%0Aalso%20discuss%20some%20fundamental%20problems%2C%20such%20as%20ethical%20issues%20and%20regulatory%0Aaspects%2C%20related%20to%20the%20use%20of%20Foundation%20Models%20in%20this%20research%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08907v1&entry.124074799=Read"},
{"title": "DELTA: Dual Consistency Delving with Topological Uncertainty for Active\n  Graph Domain Adaptation", "author": "Pengyun Wang and Yadi Cao and Chris Russell and Siyu Heng and Junyu Luo and Yanxin Shen and Xiao Luo", "abstract": "  Graph domain adaptation has recently enabled knowledge transfer across\ndifferent graphs. However, without the semantic information on target graphs,\nthe performance on target graphs is still far from satisfactory. To address the\nissue, we study the problem of active graph domain adaptation, which selects a\nsmall quantitative of informative nodes on the target graph for extra\nannotation. This problem is highly challenging due to the complicated\ntopological relationships and the distribution discrepancy across graphs. In\nthis paper, we propose a novel approach named Dual Consistency Delving with\nTopological Uncertainty (DELTA) for active graph domain adaptation. Our DELTA\nconsists of an edge-oriented graph subnetwork and a path-oriented graph\nsubnetwork, which can explore topological semantics from complementary\nperspectives. In particular, our edge-oriented graph subnetwork utilizes the\nmessage passing mechanism to learn neighborhood information, while our\npath-oriented graph subnetwork explores high-order relationships from\nsubstructures. To jointly learn from two subnetworks, we roughly select\ninformative candidate nodes with the consideration of consistency across two\nsubnetworks. Then, we aggregate local semantics from its K-hop subgraph based\non node degrees for topological uncertainty estimation. To overcome potential\ndistribution shifts, we compare target nodes and their corresponding source\nnodes for discrepancy scores as an additional component for fine selection.\nExtensive experiments on benchmark datasets demonstrate that DELTA outperforms\nvarious state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2409.08946v1", "date": "2024-09-13", "relevancy": 2.0399, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.526}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5205}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DELTA%3A%20Dual%20Consistency%20Delving%20with%20Topological%20Uncertainty%20for%20Active%0A%20%20Graph%20Domain%20Adaptation&body=Title%3A%20DELTA%3A%20Dual%20Consistency%20Delving%20with%20Topological%20Uncertainty%20for%20Active%0A%20%20Graph%20Domain%20Adaptation%0AAuthor%3A%20Pengyun%20Wang%20and%20Yadi%20Cao%20and%20Chris%20Russell%20and%20Siyu%20Heng%20and%20Junyu%20Luo%20and%20Yanxin%20Shen%20and%20Xiao%20Luo%0AAbstract%3A%20%20%20Graph%20domain%20adaptation%20has%20recently%20enabled%20knowledge%20transfer%20across%0Adifferent%20graphs.%20However%2C%20without%20the%20semantic%20information%20on%20target%20graphs%2C%0Athe%20performance%20on%20target%20graphs%20is%20still%20far%20from%20satisfactory.%20To%20address%20the%0Aissue%2C%20we%20study%20the%20problem%20of%20active%20graph%20domain%20adaptation%2C%20which%20selects%20a%0Asmall%20quantitative%20of%20informative%20nodes%20on%20the%20target%20graph%20for%20extra%0Aannotation.%20This%20problem%20is%20highly%20challenging%20due%20to%20the%20complicated%0Atopological%20relationships%20and%20the%20distribution%20discrepancy%20across%20graphs.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20approach%20named%20Dual%20Consistency%20Delving%20with%0ATopological%20Uncertainty%20%28DELTA%29%20for%20active%20graph%20domain%20adaptation.%20Our%20DELTA%0Aconsists%20of%20an%20edge-oriented%20graph%20subnetwork%20and%20a%20path-oriented%20graph%0Asubnetwork%2C%20which%20can%20explore%20topological%20semantics%20from%20complementary%0Aperspectives.%20In%20particular%2C%20our%20edge-oriented%20graph%20subnetwork%20utilizes%20the%0Amessage%20passing%20mechanism%20to%20learn%20neighborhood%20information%2C%20while%20our%0Apath-oriented%20graph%20subnetwork%20explores%20high-order%20relationships%20from%0Asubstructures.%20To%20jointly%20learn%20from%20two%20subnetworks%2C%20we%20roughly%20select%0Ainformative%20candidate%20nodes%20with%20the%20consideration%20of%20consistency%20across%20two%0Asubnetworks.%20Then%2C%20we%20aggregate%20local%20semantics%20from%20its%20K-hop%20subgraph%20based%0Aon%20node%20degrees%20for%20topological%20uncertainty%20estimation.%20To%20overcome%20potential%0Adistribution%20shifts%2C%20we%20compare%20target%20nodes%20and%20their%20corresponding%20source%0Anodes%20for%20discrepancy%20scores%20as%20an%20additional%20component%20for%20fine%20selection.%0AExtensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20DELTA%20outperforms%0Avarious%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDELTA%253A%2520Dual%2520Consistency%2520Delving%2520with%2520Topological%2520Uncertainty%2520for%2520Active%250A%2520%2520Graph%2520Domain%2520Adaptation%26entry.906535625%3DPengyun%2520Wang%2520and%2520Yadi%2520Cao%2520and%2520Chris%2520Russell%2520and%2520Siyu%2520Heng%2520and%2520Junyu%2520Luo%2520and%2520Yanxin%2520Shen%2520and%2520Xiao%2520Luo%26entry.1292438233%3D%2520%2520Graph%2520domain%2520adaptation%2520has%2520recently%2520enabled%2520knowledge%2520transfer%2520across%250Adifferent%2520graphs.%2520However%252C%2520without%2520the%2520semantic%2520information%2520on%2520target%2520graphs%252C%250Athe%2520performance%2520on%2520target%2520graphs%2520is%2520still%2520far%2520from%2520satisfactory.%2520To%2520address%2520the%250Aissue%252C%2520we%2520study%2520the%2520problem%2520of%2520active%2520graph%2520domain%2520adaptation%252C%2520which%2520selects%2520a%250Asmall%2520quantitative%2520of%2520informative%2520nodes%2520on%2520the%2520target%2520graph%2520for%2520extra%250Aannotation.%2520This%2520problem%2520is%2520highly%2520challenging%2520due%2520to%2520the%2520complicated%250Atopological%2520relationships%2520and%2520the%2520distribution%2520discrepancy%2520across%2520graphs.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520named%2520Dual%2520Consistency%2520Delving%2520with%250ATopological%2520Uncertainty%2520%2528DELTA%2529%2520for%2520active%2520graph%2520domain%2520adaptation.%2520Our%2520DELTA%250Aconsists%2520of%2520an%2520edge-oriented%2520graph%2520subnetwork%2520and%2520a%2520path-oriented%2520graph%250Asubnetwork%252C%2520which%2520can%2520explore%2520topological%2520semantics%2520from%2520complementary%250Aperspectives.%2520In%2520particular%252C%2520our%2520edge-oriented%2520graph%2520subnetwork%2520utilizes%2520the%250Amessage%2520passing%2520mechanism%2520to%2520learn%2520neighborhood%2520information%252C%2520while%2520our%250Apath-oriented%2520graph%2520subnetwork%2520explores%2520high-order%2520relationships%2520from%250Asubstructures.%2520To%2520jointly%2520learn%2520from%2520two%2520subnetworks%252C%2520we%2520roughly%2520select%250Ainformative%2520candidate%2520nodes%2520with%2520the%2520consideration%2520of%2520consistency%2520across%2520two%250Asubnetworks.%2520Then%252C%2520we%2520aggregate%2520local%2520semantics%2520from%2520its%2520K-hop%2520subgraph%2520based%250Aon%2520node%2520degrees%2520for%2520topological%2520uncertainty%2520estimation.%2520To%2520overcome%2520potential%250Adistribution%2520shifts%252C%2520we%2520compare%2520target%2520nodes%2520and%2520their%2520corresponding%2520source%250Anodes%2520for%2520discrepancy%2520scores%2520as%2520an%2520additional%2520component%2520for%2520fine%2520selection.%250AExtensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520DELTA%2520outperforms%250Avarious%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DELTA%3A%20Dual%20Consistency%20Delving%20with%20Topological%20Uncertainty%20for%20Active%0A%20%20Graph%20Domain%20Adaptation&entry.906535625=Pengyun%20Wang%20and%20Yadi%20Cao%20and%20Chris%20Russell%20and%20Siyu%20Heng%20and%20Junyu%20Luo%20and%20Yanxin%20Shen%20and%20Xiao%20Luo&entry.1292438233=%20%20Graph%20domain%20adaptation%20has%20recently%20enabled%20knowledge%20transfer%20across%0Adifferent%20graphs.%20However%2C%20without%20the%20semantic%20information%20on%20target%20graphs%2C%0Athe%20performance%20on%20target%20graphs%20is%20still%20far%20from%20satisfactory.%20To%20address%20the%0Aissue%2C%20we%20study%20the%20problem%20of%20active%20graph%20domain%20adaptation%2C%20which%20selects%20a%0Asmall%20quantitative%20of%20informative%20nodes%20on%20the%20target%20graph%20for%20extra%0Aannotation.%20This%20problem%20is%20highly%20challenging%20due%20to%20the%20complicated%0Atopological%20relationships%20and%20the%20distribution%20discrepancy%20across%20graphs.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20approach%20named%20Dual%20Consistency%20Delving%20with%0ATopological%20Uncertainty%20%28DELTA%29%20for%20active%20graph%20domain%20adaptation.%20Our%20DELTA%0Aconsists%20of%20an%20edge-oriented%20graph%20subnetwork%20and%20a%20path-oriented%20graph%0Asubnetwork%2C%20which%20can%20explore%20topological%20semantics%20from%20complementary%0Aperspectives.%20In%20particular%2C%20our%20edge-oriented%20graph%20subnetwork%20utilizes%20the%0Amessage%20passing%20mechanism%20to%20learn%20neighborhood%20information%2C%20while%20our%0Apath-oriented%20graph%20subnetwork%20explores%20high-order%20relationships%20from%0Asubstructures.%20To%20jointly%20learn%20from%20two%20subnetworks%2C%20we%20roughly%20select%0Ainformative%20candidate%20nodes%20with%20the%20consideration%20of%20consistency%20across%20two%0Asubnetworks.%20Then%2C%20we%20aggregate%20local%20semantics%20from%20its%20K-hop%20subgraph%20based%0Aon%20node%20degrees%20for%20topological%20uncertainty%20estimation.%20To%20overcome%20potential%0Adistribution%20shifts%2C%20we%20compare%20target%20nodes%20and%20their%20corresponding%20source%0Anodes%20for%20discrepancy%20scores%20as%20an%20additional%20component%20for%20fine%20selection.%0AExtensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20DELTA%20outperforms%0Avarious%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08946v1&entry.124074799=Read"},
{"title": "Contri(e)ve: Context + Retrieve for Scholarly Question Answering", "author": "Kanchan Shivashankar and Nadine Steinmetz", "abstract": "  Scholarly communication is a rapid growing field containing a wealth of\nknowledge. However, due to its unstructured and document format, it is\nchallenging to extract useful information from them through conventional\ndocument retrieval methods. Scholarly knowledge graphs solve this problem, by\nrepresenting the documents in a semantic network, providing, hidden insights,\nsummaries and ease of accessibility through queries. Naturally, question\nanswering for scholarly graphs expands the accessibility to a wider audience.\nBut some of the knowledge in this domain is still presented as unstructured\ntext, thus requiring a hybrid solution for question answering systems. In this\npaper, we present a two step solution using open source Large Language\nModel(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the\ncontext pertaining to the question from different structured and unstructured\ndata sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,\nwe implement prompt engineering to improve the information retrieval\nperformance of the LLM. Our approach achieved an F1 score of 40% and also\nobserved some anomalous responses from the LLM, that are discussed in the final\npart of the paper.\n", "link": "http://arxiv.org/abs/2409.09010v1", "date": "2024-09-13", "relevancy": 2.0391, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5147}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5147}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contri%28e%29ve%3A%20Context%20%2B%20Retrieve%20for%20Scholarly%20Question%20Answering&body=Title%3A%20Contri%28e%29ve%3A%20Context%20%2B%20Retrieve%20for%20Scholarly%20Question%20Answering%0AAuthor%3A%20Kanchan%20Shivashankar%20and%20Nadine%20Steinmetz%0AAbstract%3A%20%20%20Scholarly%20communication%20is%20a%20rapid%20growing%20field%20containing%20a%20wealth%20of%0Aknowledge.%20However%2C%20due%20to%20its%20unstructured%20and%20document%20format%2C%20it%20is%0Achallenging%20to%20extract%20useful%20information%20from%20them%20through%20conventional%0Adocument%20retrieval%20methods.%20Scholarly%20knowledge%20graphs%20solve%20this%20problem%2C%20by%0Arepresenting%20the%20documents%20in%20a%20semantic%20network%2C%20providing%2C%20hidden%20insights%2C%0Asummaries%20and%20ease%20of%20accessibility%20through%20queries.%20Naturally%2C%20question%0Aanswering%20for%20scholarly%20graphs%20expands%20the%20accessibility%20to%20a%20wider%20audience.%0ABut%20some%20of%20the%20knowledge%20in%20this%20domain%20is%20still%20presented%20as%20unstructured%0Atext%2C%20thus%20requiring%20a%20hybrid%20solution%20for%20question%20answering%20systems.%20In%20this%0Apaper%2C%20we%20present%20a%20two%20step%20solution%20using%20open%20source%20Large%20Language%0AModel%28LLM%29%3A%20Llama3.1%20for%20Scholarly-QALD%20dataset.%20Firstly%2C%20we%20extract%20the%0Acontext%20pertaining%20to%20the%20question%20from%20different%20structured%20and%20unstructured%0Adata%20sources%3A%20DBLP%2C%20SemOpenAlex%20knowledge%20graphs%20and%20Wikipedia%20text.%20Secondly%2C%0Awe%20implement%20prompt%20engineering%20to%20improve%20the%20information%20retrieval%0Aperformance%20of%20the%20LLM.%20Our%20approach%20achieved%20an%20F1%20score%20of%2040%25%20and%20also%0Aobserved%20some%20anomalous%20responses%20from%20the%20LLM%2C%20that%20are%20discussed%20in%20the%20final%0Apart%20of%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContri%2528e%2529ve%253A%2520Context%2520%252B%2520Retrieve%2520for%2520Scholarly%2520Question%2520Answering%26entry.906535625%3DKanchan%2520Shivashankar%2520and%2520Nadine%2520Steinmetz%26entry.1292438233%3D%2520%2520Scholarly%2520communication%2520is%2520a%2520rapid%2520growing%2520field%2520containing%2520a%2520wealth%2520of%250Aknowledge.%2520However%252C%2520due%2520to%2520its%2520unstructured%2520and%2520document%2520format%252C%2520it%2520is%250Achallenging%2520to%2520extract%2520useful%2520information%2520from%2520them%2520through%2520conventional%250Adocument%2520retrieval%2520methods.%2520Scholarly%2520knowledge%2520graphs%2520solve%2520this%2520problem%252C%2520by%250Arepresenting%2520the%2520documents%2520in%2520a%2520semantic%2520network%252C%2520providing%252C%2520hidden%2520insights%252C%250Asummaries%2520and%2520ease%2520of%2520accessibility%2520through%2520queries.%2520Naturally%252C%2520question%250Aanswering%2520for%2520scholarly%2520graphs%2520expands%2520the%2520accessibility%2520to%2520a%2520wider%2520audience.%250ABut%2520some%2520of%2520the%2520knowledge%2520in%2520this%2520domain%2520is%2520still%2520presented%2520as%2520unstructured%250Atext%252C%2520thus%2520requiring%2520a%2520hybrid%2520solution%2520for%2520question%2520answering%2520systems.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520two%2520step%2520solution%2520using%2520open%2520source%2520Large%2520Language%250AModel%2528LLM%2529%253A%2520Llama3.1%2520for%2520Scholarly-QALD%2520dataset.%2520Firstly%252C%2520we%2520extract%2520the%250Acontext%2520pertaining%2520to%2520the%2520question%2520from%2520different%2520structured%2520and%2520unstructured%250Adata%2520sources%253A%2520DBLP%252C%2520SemOpenAlex%2520knowledge%2520graphs%2520and%2520Wikipedia%2520text.%2520Secondly%252C%250Awe%2520implement%2520prompt%2520engineering%2520to%2520improve%2520the%2520information%2520retrieval%250Aperformance%2520of%2520the%2520LLM.%2520Our%2520approach%2520achieved%2520an%2520F1%2520score%2520of%252040%2525%2520and%2520also%250Aobserved%2520some%2520anomalous%2520responses%2520from%2520the%2520LLM%252C%2520that%2520are%2520discussed%2520in%2520the%2520final%250Apart%2520of%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contri%28e%29ve%3A%20Context%20%2B%20Retrieve%20for%20Scholarly%20Question%20Answering&entry.906535625=Kanchan%20Shivashankar%20and%20Nadine%20Steinmetz&entry.1292438233=%20%20Scholarly%20communication%20is%20a%20rapid%20growing%20field%20containing%20a%20wealth%20of%0Aknowledge.%20However%2C%20due%20to%20its%20unstructured%20and%20document%20format%2C%20it%20is%0Achallenging%20to%20extract%20useful%20information%20from%20them%20through%20conventional%0Adocument%20retrieval%20methods.%20Scholarly%20knowledge%20graphs%20solve%20this%20problem%2C%20by%0Arepresenting%20the%20documents%20in%20a%20semantic%20network%2C%20providing%2C%20hidden%20insights%2C%0Asummaries%20and%20ease%20of%20accessibility%20through%20queries.%20Naturally%2C%20question%0Aanswering%20for%20scholarly%20graphs%20expands%20the%20accessibility%20to%20a%20wider%20audience.%0ABut%20some%20of%20the%20knowledge%20in%20this%20domain%20is%20still%20presented%20as%20unstructured%0Atext%2C%20thus%20requiring%20a%20hybrid%20solution%20for%20question%20answering%20systems.%20In%20this%0Apaper%2C%20we%20present%20a%20two%20step%20solution%20using%20open%20source%20Large%20Language%0AModel%28LLM%29%3A%20Llama3.1%20for%20Scholarly-QALD%20dataset.%20Firstly%2C%20we%20extract%20the%0Acontext%20pertaining%20to%20the%20question%20from%20different%20structured%20and%20unstructured%0Adata%20sources%3A%20DBLP%2C%20SemOpenAlex%20knowledge%20graphs%20and%20Wikipedia%20text.%20Secondly%2C%0Awe%20implement%20prompt%20engineering%20to%20improve%20the%20information%20retrieval%0Aperformance%20of%20the%20LLM.%20Our%20approach%20achieved%20an%20F1%20score%20of%2040%25%20and%20also%0Aobserved%20some%20anomalous%20responses%20from%20the%20LLM%2C%20that%20are%20discussed%20in%20the%20final%0Apart%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09010v1&entry.124074799=Read"},
{"title": "Pushing the boundaries of event subsampling in event-based video\n  classification using CNNs", "author": "Hesam Araghi and Jan van Gemert and Nergis Tomen", "abstract": "  Event cameras offer low-power visual sensing capabilities ideal for\nedge-device applications. However, their high event rate, driven by high\ntemporal details, can be restrictive in terms of bandwidth and computational\nresources. In edge AI applications, determining the minimum amount of events\nfor specific tasks can allow reducing the event rate to improve bandwidth,\nmemory, and processing efficiency. In this paper, we study the effect of event\nsubsampling on the accuracy of event data classification using convolutional\nneural network (CNN) models. Surprisingly, across various datasets, the number\nof events per video can be reduced by an order of magnitude with little drop in\naccuracy, revealing the extent to which we can push the boundaries in accuracy\nvs. event rate trade-off. Additionally, we also find that lower classification\naccuracy in high subsampling rates is not solely attributable to information\nloss due to the subsampling of the events, but that the training of CNNs can be\nchallenging in highly subsampled scenarios, where the sensitivity to\nhyperparameters increases. We quantify training instability across multiple\nevent-based classification datasets using a novel metric for evaluating the\nhyperparameter sensitivity of CNNs in different subsampling settings. Finally,\nwe analyze the weight gradients of the network to gain insight into this\ninstability.\n", "link": "http://arxiv.org/abs/2409.08953v1", "date": "2024-09-13", "relevancy": 2.0049, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5097}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5048}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20the%20boundaries%20of%20event%20subsampling%20in%20event-based%20video%0A%20%20classification%20using%20CNNs&body=Title%3A%20Pushing%20the%20boundaries%20of%20event%20subsampling%20in%20event-based%20video%0A%20%20classification%20using%20CNNs%0AAuthor%3A%20Hesam%20Araghi%20and%20Jan%20van%20Gemert%20and%20Nergis%20Tomen%0AAbstract%3A%20%20%20Event%20cameras%20offer%20low-power%20visual%20sensing%20capabilities%20ideal%20for%0Aedge-device%20applications.%20However%2C%20their%20high%20event%20rate%2C%20driven%20by%20high%0Atemporal%20details%2C%20can%20be%20restrictive%20in%20terms%20of%20bandwidth%20and%20computational%0Aresources.%20In%20edge%20AI%20applications%2C%20determining%20the%20minimum%20amount%20of%20events%0Afor%20specific%20tasks%20can%20allow%20reducing%20the%20event%20rate%20to%20improve%20bandwidth%2C%0Amemory%2C%20and%20processing%20efficiency.%20In%20this%20paper%2C%20we%20study%20the%20effect%20of%20event%0Asubsampling%20on%20the%20accuracy%20of%20event%20data%20classification%20using%20convolutional%0Aneural%20network%20%28CNN%29%20models.%20Surprisingly%2C%20across%20various%20datasets%2C%20the%20number%0Aof%20events%20per%20video%20can%20be%20reduced%20by%20an%20order%20of%20magnitude%20with%20little%20drop%20in%0Aaccuracy%2C%20revealing%20the%20extent%20to%20which%20we%20can%20push%20the%20boundaries%20in%20accuracy%0Avs.%20event%20rate%20trade-off.%20Additionally%2C%20we%20also%20find%20that%20lower%20classification%0Aaccuracy%20in%20high%20subsampling%20rates%20is%20not%20solely%20attributable%20to%20information%0Aloss%20due%20to%20the%20subsampling%20of%20the%20events%2C%20but%20that%20the%20training%20of%20CNNs%20can%20be%0Achallenging%20in%20highly%20subsampled%20scenarios%2C%20where%20the%20sensitivity%20to%0Ahyperparameters%20increases.%20We%20quantify%20training%20instability%20across%20multiple%0Aevent-based%20classification%20datasets%20using%20a%20novel%20metric%20for%20evaluating%20the%0Ahyperparameter%20sensitivity%20of%20CNNs%20in%20different%20subsampling%20settings.%20Finally%2C%0Awe%20analyze%20the%20weight%20gradients%20of%20the%20network%20to%20gain%20insight%20into%20this%0Ainstability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520the%2520boundaries%2520of%2520event%2520subsampling%2520in%2520event-based%2520video%250A%2520%2520classification%2520using%2520CNNs%26entry.906535625%3DHesam%2520Araghi%2520and%2520Jan%2520van%2520Gemert%2520and%2520Nergis%2520Tomen%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520low-power%2520visual%2520sensing%2520capabilities%2520ideal%2520for%250Aedge-device%2520applications.%2520However%252C%2520their%2520high%2520event%2520rate%252C%2520driven%2520by%2520high%250Atemporal%2520details%252C%2520can%2520be%2520restrictive%2520in%2520terms%2520of%2520bandwidth%2520and%2520computational%250Aresources.%2520In%2520edge%2520AI%2520applications%252C%2520determining%2520the%2520minimum%2520amount%2520of%2520events%250Afor%2520specific%2520tasks%2520can%2520allow%2520reducing%2520the%2520event%2520rate%2520to%2520improve%2520bandwidth%252C%250Amemory%252C%2520and%2520processing%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520effect%2520of%2520event%250Asubsampling%2520on%2520the%2520accuracy%2520of%2520event%2520data%2520classification%2520using%2520convolutional%250Aneural%2520network%2520%2528CNN%2529%2520models.%2520Surprisingly%252C%2520across%2520various%2520datasets%252C%2520the%2520number%250Aof%2520events%2520per%2520video%2520can%2520be%2520reduced%2520by%2520an%2520order%2520of%2520magnitude%2520with%2520little%2520drop%2520in%250Aaccuracy%252C%2520revealing%2520the%2520extent%2520to%2520which%2520we%2520can%2520push%2520the%2520boundaries%2520in%2520accuracy%250Avs.%2520event%2520rate%2520trade-off.%2520Additionally%252C%2520we%2520also%2520find%2520that%2520lower%2520classification%250Aaccuracy%2520in%2520high%2520subsampling%2520rates%2520is%2520not%2520solely%2520attributable%2520to%2520information%250Aloss%2520due%2520to%2520the%2520subsampling%2520of%2520the%2520events%252C%2520but%2520that%2520the%2520training%2520of%2520CNNs%2520can%2520be%250Achallenging%2520in%2520highly%2520subsampled%2520scenarios%252C%2520where%2520the%2520sensitivity%2520to%250Ahyperparameters%2520increases.%2520We%2520quantify%2520training%2520instability%2520across%2520multiple%250Aevent-based%2520classification%2520datasets%2520using%2520a%2520novel%2520metric%2520for%2520evaluating%2520the%250Ahyperparameter%2520sensitivity%2520of%2520CNNs%2520in%2520different%2520subsampling%2520settings.%2520Finally%252C%250Awe%2520analyze%2520the%2520weight%2520gradients%2520of%2520the%2520network%2520to%2520gain%2520insight%2520into%2520this%250Ainstability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20the%20boundaries%20of%20event%20subsampling%20in%20event-based%20video%0A%20%20classification%20using%20CNNs&entry.906535625=Hesam%20Araghi%20and%20Jan%20van%20Gemert%20and%20Nergis%20Tomen&entry.1292438233=%20%20Event%20cameras%20offer%20low-power%20visual%20sensing%20capabilities%20ideal%20for%0Aedge-device%20applications.%20However%2C%20their%20high%20event%20rate%2C%20driven%20by%20high%0Atemporal%20details%2C%20can%20be%20restrictive%20in%20terms%20of%20bandwidth%20and%20computational%0Aresources.%20In%20edge%20AI%20applications%2C%20determining%20the%20minimum%20amount%20of%20events%0Afor%20specific%20tasks%20can%20allow%20reducing%20the%20event%20rate%20to%20improve%20bandwidth%2C%0Amemory%2C%20and%20processing%20efficiency.%20In%20this%20paper%2C%20we%20study%20the%20effect%20of%20event%0Asubsampling%20on%20the%20accuracy%20of%20event%20data%20classification%20using%20convolutional%0Aneural%20network%20%28CNN%29%20models.%20Surprisingly%2C%20across%20various%20datasets%2C%20the%20number%0Aof%20events%20per%20video%20can%20be%20reduced%20by%20an%20order%20of%20magnitude%20with%20little%20drop%20in%0Aaccuracy%2C%20revealing%20the%20extent%20to%20which%20we%20can%20push%20the%20boundaries%20in%20accuracy%0Avs.%20event%20rate%20trade-off.%20Additionally%2C%20we%20also%20find%20that%20lower%20classification%0Aaccuracy%20in%20high%20subsampling%20rates%20is%20not%20solely%20attributable%20to%20information%0Aloss%20due%20to%20the%20subsampling%20of%20the%20events%2C%20but%20that%20the%20training%20of%20CNNs%20can%20be%0Achallenging%20in%20highly%20subsampled%20scenarios%2C%20where%20the%20sensitivity%20to%0Ahyperparameters%20increases.%20We%20quantify%20training%20instability%20across%20multiple%0Aevent-based%20classification%20datasets%20using%20a%20novel%20metric%20for%20evaluating%20the%0Ahyperparameter%20sensitivity%20of%20CNNs%20in%20different%20subsampling%20settings.%20Finally%2C%0Awe%20analyze%20the%20weight%20gradients%20of%20the%20network%20to%20gain%20insight%20into%20this%0Ainstability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08953v1&entry.124074799=Read"},
{"title": "Linear Attention is Enough in Spatial-Temporal Forecasting", "author": "Xinyu Ning", "abstract": "  As the most representative scenario of spatial-temporal forecasting tasks,\nthe traffic forecasting task attracted numerous attention from machine learning\ncommunity due to its intricate correlation both in space and time dimension.\nExisting methods often treat road networks over time as spatial-temporal\ngraphs, addressing spatial and temporal representations independently. However,\nthese approaches struggle to capture the dynamic topology of road networks,\nencounter issues with message passing mechanisms and over-smoothing, and face\nchallenges in learning spatial and temporal relationships separately. To\naddress these limitations, we propose treating nodes in road networks at\ndifferent time steps as independent spatial-temporal tokens and feeding them\ninto a vanilla Transformer to learn complex spatial-temporal patterns, design\n\\textbf{STformer} achieving SOTA. Given its quadratic complexity, we introduce\na variant \\textbf{NSTformer} based on Nystr$\\ddot{o}$m method to approximate\nself-attention with linear complexity but even slightly better than former in a\nfew cases astonishingly. Extensive experimental results on traffic datasets\ndemonstrate that the proposed method achieves state-of-the-art performance at\nan affordable computational cost. Our code is available at\n\\href{https://github.com/XinyuNing/STformer-and-NSTformer}{https://github.com/XinyuNing/STformer-and-NSTformer}.\n", "link": "http://arxiv.org/abs/2408.09158v2", "date": "2024-09-13", "relevancy": 1.9962, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5158}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4979}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Attention%20is%20Enough%20in%20Spatial-Temporal%20Forecasting&body=Title%3A%20Linear%20Attention%20is%20Enough%20in%20Spatial-Temporal%20Forecasting%0AAuthor%3A%20Xinyu%20Ning%0AAbstract%3A%20%20%20As%20the%20most%20representative%20scenario%20of%20spatial-temporal%20forecasting%20tasks%2C%0Athe%20traffic%20forecasting%20task%20attracted%20numerous%20attention%20from%20machine%20learning%0Acommunity%20due%20to%20its%20intricate%20correlation%20both%20in%20space%20and%20time%20dimension.%0AExisting%20methods%20often%20treat%20road%20networks%20over%20time%20as%20spatial-temporal%0Agraphs%2C%20addressing%20spatial%20and%20temporal%20representations%20independently.%20However%2C%0Athese%20approaches%20struggle%20to%20capture%20the%20dynamic%20topology%20of%20road%20networks%2C%0Aencounter%20issues%20with%20message%20passing%20mechanisms%20and%20over-smoothing%2C%20and%20face%0Achallenges%20in%20learning%20spatial%20and%20temporal%20relationships%20separately.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20treating%20nodes%20in%20road%20networks%20at%0Adifferent%20time%20steps%20as%20independent%20spatial-temporal%20tokens%20and%20feeding%20them%0Ainto%20a%20vanilla%20Transformer%20to%20learn%20complex%20spatial-temporal%20patterns%2C%20design%0A%5Ctextbf%7BSTformer%7D%20achieving%20SOTA.%20Given%20its%20quadratic%20complexity%2C%20we%20introduce%0Aa%20variant%20%5Ctextbf%7BNSTformer%7D%20based%20on%20Nystr%24%5Cddot%7Bo%7D%24m%20method%20to%20approximate%0Aself-attention%20with%20linear%20complexity%20but%20even%20slightly%20better%20than%20former%20in%20a%0Afew%20cases%20astonishingly.%20Extensive%20experimental%20results%20on%20traffic%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20state-of-the-art%20performance%20at%0Aan%20affordable%20computational%20cost.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/XinyuNing/STformer-and-NSTformer%7D%7Bhttps%3A//github.com/XinyuNing/STformer-and-NSTformer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09158v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Attention%2520is%2520Enough%2520in%2520Spatial-Temporal%2520Forecasting%26entry.906535625%3DXinyu%2520Ning%26entry.1292438233%3D%2520%2520As%2520the%2520most%2520representative%2520scenario%2520of%2520spatial-temporal%2520forecasting%2520tasks%252C%250Athe%2520traffic%2520forecasting%2520task%2520attracted%2520numerous%2520attention%2520from%2520machine%2520learning%250Acommunity%2520due%2520to%2520its%2520intricate%2520correlation%2520both%2520in%2520space%2520and%2520time%2520dimension.%250AExisting%2520methods%2520often%2520treat%2520road%2520networks%2520over%2520time%2520as%2520spatial-temporal%250Agraphs%252C%2520addressing%2520spatial%2520and%2520temporal%2520representations%2520independently.%2520However%252C%250Athese%2520approaches%2520struggle%2520to%2520capture%2520the%2520dynamic%2520topology%2520of%2520road%2520networks%252C%250Aencounter%2520issues%2520with%2520message%2520passing%2520mechanisms%2520and%2520over-smoothing%252C%2520and%2520face%250Achallenges%2520in%2520learning%2520spatial%2520and%2520temporal%2520relationships%2520separately.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520treating%2520nodes%2520in%2520road%2520networks%2520at%250Adifferent%2520time%2520steps%2520as%2520independent%2520spatial-temporal%2520tokens%2520and%2520feeding%2520them%250Ainto%2520a%2520vanilla%2520Transformer%2520to%2520learn%2520complex%2520spatial-temporal%2520patterns%252C%2520design%250A%255Ctextbf%257BSTformer%257D%2520achieving%2520SOTA.%2520Given%2520its%2520quadratic%2520complexity%252C%2520we%2520introduce%250Aa%2520variant%2520%255Ctextbf%257BNSTformer%257D%2520based%2520on%2520Nystr%2524%255Cddot%257Bo%257D%2524m%2520method%2520to%2520approximate%250Aself-attention%2520with%2520linear%2520complexity%2520but%2520even%2520slightly%2520better%2520than%2520former%2520in%2520a%250Afew%2520cases%2520astonishingly.%2520Extensive%2520experimental%2520results%2520on%2520traffic%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520state-of-the-art%2520performance%2520at%250Aan%2520affordable%2520computational%2520cost.%2520Our%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/XinyuNing/STformer-and-NSTformer%257D%257Bhttps%253A//github.com/XinyuNing/STformer-and-NSTformer%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09158v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Attention%20is%20Enough%20in%20Spatial-Temporal%20Forecasting&entry.906535625=Xinyu%20Ning&entry.1292438233=%20%20As%20the%20most%20representative%20scenario%20of%20spatial-temporal%20forecasting%20tasks%2C%0Athe%20traffic%20forecasting%20task%20attracted%20numerous%20attention%20from%20machine%20learning%0Acommunity%20due%20to%20its%20intricate%20correlation%20both%20in%20space%20and%20time%20dimension.%0AExisting%20methods%20often%20treat%20road%20networks%20over%20time%20as%20spatial-temporal%0Agraphs%2C%20addressing%20spatial%20and%20temporal%20representations%20independently.%20However%2C%0Athese%20approaches%20struggle%20to%20capture%20the%20dynamic%20topology%20of%20road%20networks%2C%0Aencounter%20issues%20with%20message%20passing%20mechanisms%20and%20over-smoothing%2C%20and%20face%0Achallenges%20in%20learning%20spatial%20and%20temporal%20relationships%20separately.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20treating%20nodes%20in%20road%20networks%20at%0Adifferent%20time%20steps%20as%20independent%20spatial-temporal%20tokens%20and%20feeding%20them%0Ainto%20a%20vanilla%20Transformer%20to%20learn%20complex%20spatial-temporal%20patterns%2C%20design%0A%5Ctextbf%7BSTformer%7D%20achieving%20SOTA.%20Given%20its%20quadratic%20complexity%2C%20we%20introduce%0Aa%20variant%20%5Ctextbf%7BNSTformer%7D%20based%20on%20Nystr%24%5Cddot%7Bo%7D%24m%20method%20to%20approximate%0Aself-attention%20with%20linear%20complexity%20but%20even%20slightly%20better%20than%20former%20in%20a%0Afew%20cases%20astonishingly.%20Extensive%20experimental%20results%20on%20traffic%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20achieves%20state-of-the-art%20performance%20at%0Aan%20affordable%20computational%20cost.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/XinyuNing/STformer-and-NSTformer%7D%7Bhttps%3A//github.com/XinyuNing/STformer-and-NSTformer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09158v2&entry.124074799=Read"},
{"title": "XAMI -- A Benchmark Dataset for Artefact Detection in XMM-Newton Optical\n  Images", "author": "Elisabeta-Iulia Dima and Pablo G\u00f3mez and Sandor Kruk and Peter Kretschmar and Simon Rosen and C\u0103lin-Adrian Popa", "abstract": "  Reflected or scattered light produce artefacts in astronomical observations\nthat can negatively impact the scientific study. Hence, automated detection of\nthese artefacts is highly beneficial, especially with the increasing amounts of\ndata gathered. Machine learning methods are well-suited to this problem, but\ncurrently there is a lack of annotated data to train such approaches to detect\nartefacts in astronomical observations. In this work, we present a dataset of\nimages from the XMM-Newton space telescope Optical Monitoring camera showing\ndifferent types of artefacts. We hand-annotated a sample of 1000 images with\nartefacts which we use to train automated ML methods. We further demonstrate\ntechniques tailored for accurate detection and masking of artefacts using\ninstance segmentation. We adopt a hybrid approach, combining knowledge from\nboth convolutional neural networks (CNNs) and transformer-based models and use\ntheir advantages in segmentation. The presented method and dataset will advance\nartefact detection in astronomical observations by providing a reproducible\nbaseline. All code and data are made available\n(https://github.com/ESA-Datalabs/XAMI-model and\nhttps://github.com/ESA-Datalabs/XAMI-dataset).\n", "link": "http://arxiv.org/abs/2406.17323v2", "date": "2024-09-13", "relevancy": 1.9913, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5111}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XAMI%20--%20A%20Benchmark%20Dataset%20for%20Artefact%20Detection%20in%20XMM-Newton%20Optical%0A%20%20Images&body=Title%3A%20XAMI%20--%20A%20Benchmark%20Dataset%20for%20Artefact%20Detection%20in%20XMM-Newton%20Optical%0A%20%20Images%0AAuthor%3A%20Elisabeta-Iulia%20Dima%20and%20Pablo%20G%C3%B3mez%20and%20Sandor%20Kruk%20and%20Peter%20Kretschmar%20and%20Simon%20Rosen%20and%20C%C4%83lin-Adrian%20Popa%0AAbstract%3A%20%20%20Reflected%20or%20scattered%20light%20produce%20artefacts%20in%20astronomical%20observations%0Athat%20can%20negatively%20impact%20the%20scientific%20study.%20Hence%2C%20automated%20detection%20of%0Athese%20artefacts%20is%20highly%20beneficial%2C%20especially%20with%20the%20increasing%20amounts%20of%0Adata%20gathered.%20Machine%20learning%20methods%20are%20well-suited%20to%20this%20problem%2C%20but%0Acurrently%20there%20is%20a%20lack%20of%20annotated%20data%20to%20train%20such%20approaches%20to%20detect%0Aartefacts%20in%20astronomical%20observations.%20In%20this%20work%2C%20we%20present%20a%20dataset%20of%0Aimages%20from%20the%20XMM-Newton%20space%20telescope%20Optical%20Monitoring%20camera%20showing%0Adifferent%20types%20of%20artefacts.%20We%20hand-annotated%20a%20sample%20of%201000%20images%20with%0Aartefacts%20which%20we%20use%20to%20train%20automated%20ML%20methods.%20We%20further%20demonstrate%0Atechniques%20tailored%20for%20accurate%20detection%20and%20masking%20of%20artefacts%20using%0Ainstance%20segmentation.%20We%20adopt%20a%20hybrid%20approach%2C%20combining%20knowledge%20from%0Aboth%20convolutional%20neural%20networks%20%28CNNs%29%20and%20transformer-based%20models%20and%20use%0Atheir%20advantages%20in%20segmentation.%20The%20presented%20method%20and%20dataset%20will%20advance%0Aartefact%20detection%20in%20astronomical%20observations%20by%20providing%20a%20reproducible%0Abaseline.%20All%20code%20and%20data%20are%20made%20available%0A%28https%3A//github.com/ESA-Datalabs/XAMI-model%20and%0Ahttps%3A//github.com/ESA-Datalabs/XAMI-dataset%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17323v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXAMI%2520--%2520A%2520Benchmark%2520Dataset%2520for%2520Artefact%2520Detection%2520in%2520XMM-Newton%2520Optical%250A%2520%2520Images%26entry.906535625%3DElisabeta-Iulia%2520Dima%2520and%2520Pablo%2520G%25C3%25B3mez%2520and%2520Sandor%2520Kruk%2520and%2520Peter%2520Kretschmar%2520and%2520Simon%2520Rosen%2520and%2520C%25C4%2583lin-Adrian%2520Popa%26entry.1292438233%3D%2520%2520Reflected%2520or%2520scattered%2520light%2520produce%2520artefacts%2520in%2520astronomical%2520observations%250Athat%2520can%2520negatively%2520impact%2520the%2520scientific%2520study.%2520Hence%252C%2520automated%2520detection%2520of%250Athese%2520artefacts%2520is%2520highly%2520beneficial%252C%2520especially%2520with%2520the%2520increasing%2520amounts%2520of%250Adata%2520gathered.%2520Machine%2520learning%2520methods%2520are%2520well-suited%2520to%2520this%2520problem%252C%2520but%250Acurrently%2520there%2520is%2520a%2520lack%2520of%2520annotated%2520data%2520to%2520train%2520such%2520approaches%2520to%2520detect%250Aartefacts%2520in%2520astronomical%2520observations.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520dataset%2520of%250Aimages%2520from%2520the%2520XMM-Newton%2520space%2520telescope%2520Optical%2520Monitoring%2520camera%2520showing%250Adifferent%2520types%2520of%2520artefacts.%2520We%2520hand-annotated%2520a%2520sample%2520of%25201000%2520images%2520with%250Aartefacts%2520which%2520we%2520use%2520to%2520train%2520automated%2520ML%2520methods.%2520We%2520further%2520demonstrate%250Atechniques%2520tailored%2520for%2520accurate%2520detection%2520and%2520masking%2520of%2520artefacts%2520using%250Ainstance%2520segmentation.%2520We%2520adopt%2520a%2520hybrid%2520approach%252C%2520combining%2520knowledge%2520from%250Aboth%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520transformer-based%2520models%2520and%2520use%250Atheir%2520advantages%2520in%2520segmentation.%2520The%2520presented%2520method%2520and%2520dataset%2520will%2520advance%250Aartefact%2520detection%2520in%2520astronomical%2520observations%2520by%2520providing%2520a%2520reproducible%250Abaseline.%2520All%2520code%2520and%2520data%2520are%2520made%2520available%250A%2528https%253A//github.com/ESA-Datalabs/XAMI-model%2520and%250Ahttps%253A//github.com/ESA-Datalabs/XAMI-dataset%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17323v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XAMI%20--%20A%20Benchmark%20Dataset%20for%20Artefact%20Detection%20in%20XMM-Newton%20Optical%0A%20%20Images&entry.906535625=Elisabeta-Iulia%20Dima%20and%20Pablo%20G%C3%B3mez%20and%20Sandor%20Kruk%20and%20Peter%20Kretschmar%20and%20Simon%20Rosen%20and%20C%C4%83lin-Adrian%20Popa&entry.1292438233=%20%20Reflected%20or%20scattered%20light%20produce%20artefacts%20in%20astronomical%20observations%0Athat%20can%20negatively%20impact%20the%20scientific%20study.%20Hence%2C%20automated%20detection%20of%0Athese%20artefacts%20is%20highly%20beneficial%2C%20especially%20with%20the%20increasing%20amounts%20of%0Adata%20gathered.%20Machine%20learning%20methods%20are%20well-suited%20to%20this%20problem%2C%20but%0Acurrently%20there%20is%20a%20lack%20of%20annotated%20data%20to%20train%20such%20approaches%20to%20detect%0Aartefacts%20in%20astronomical%20observations.%20In%20this%20work%2C%20we%20present%20a%20dataset%20of%0Aimages%20from%20the%20XMM-Newton%20space%20telescope%20Optical%20Monitoring%20camera%20showing%0Adifferent%20types%20of%20artefacts.%20We%20hand-annotated%20a%20sample%20of%201000%20images%20with%0Aartefacts%20which%20we%20use%20to%20train%20automated%20ML%20methods.%20We%20further%20demonstrate%0Atechniques%20tailored%20for%20accurate%20detection%20and%20masking%20of%20artefacts%20using%0Ainstance%20segmentation.%20We%20adopt%20a%20hybrid%20approach%2C%20combining%20knowledge%20from%0Aboth%20convolutional%20neural%20networks%20%28CNNs%29%20and%20transformer-based%20models%20and%20use%0Atheir%20advantages%20in%20segmentation.%20The%20presented%20method%20and%20dataset%20will%20advance%0Aartefact%20detection%20in%20astronomical%20observations%20by%20providing%20a%20reproducible%0Abaseline.%20All%20code%20and%20data%20are%20made%20available%0A%28https%3A//github.com/ESA-Datalabs/XAMI-model%20and%0Ahttps%3A//github.com/ESA-Datalabs/XAMI-dataset%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17323v2&entry.124074799=Read"},
{"title": "Predicting Trust In Autonomous Vehicles: Modeling Young Adult\n  Psychosocial Traits, Risk-Benefit Attitudes, And Driving Factors With Machine\n  Learning", "author": "Robert Kaufman and Emi Lee and Manas Satish Bedmutha and David Kirsh and Nadir Weibel", "abstract": "  Low trust remains a significant barrier to Autonomous Vehicle (AV) adoption.\nTo design trustworthy AVs, we need to better understand the individual traits,\nattitudes, and experiences that impact people's trust judgements. We use\nmachine learning to understand the most important factors that contribute to\nyoung adult trust based on a comprehensive set of personal factors gathered via\nsurvey (n = 1457). Factors ranged from psychosocial and cognitive attributes to\ndriving style, experiences, and perceived AV risks and benefits. Using the\nexplainable AI technique SHAP, we found that perceptions of AV risks and\nbenefits, attitudes toward feasibility and usability, institutional trust,\nprior experience, and a person's mental model are the most important\npredictors. Surprisingly, psychosocial and many technology- and\ndriving-specific factors were not strong predictors. Results highlight the\nimportance of individual differences for designing trustworthy AVs for diverse\ngroups and lead to key implications for future design and research.\n", "link": "http://arxiv.org/abs/2409.08980v1", "date": "2024-09-13", "relevancy": 1.9842, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Trust%20In%20Autonomous%20Vehicles%3A%20Modeling%20Young%20Adult%0A%20%20Psychosocial%20Traits%2C%20Risk-Benefit%20Attitudes%2C%20And%20Driving%20Factors%20With%20Machine%0A%20%20Learning&body=Title%3A%20Predicting%20Trust%20In%20Autonomous%20Vehicles%3A%20Modeling%20Young%20Adult%0A%20%20Psychosocial%20Traits%2C%20Risk-Benefit%20Attitudes%2C%20And%20Driving%20Factors%20With%20Machine%0A%20%20Learning%0AAuthor%3A%20Robert%20Kaufman%20and%20Emi%20Lee%20and%20Manas%20Satish%20Bedmutha%20and%20David%20Kirsh%20and%20Nadir%20Weibel%0AAbstract%3A%20%20%20Low%20trust%20remains%20a%20significant%20barrier%20to%20Autonomous%20Vehicle%20%28AV%29%20adoption.%0ATo%20design%20trustworthy%20AVs%2C%20we%20need%20to%20better%20understand%20the%20individual%20traits%2C%0Aattitudes%2C%20and%20experiences%20that%20impact%20people%27s%20trust%20judgements.%20We%20use%0Amachine%20learning%20to%20understand%20the%20most%20important%20factors%20that%20contribute%20to%0Ayoung%20adult%20trust%20based%20on%20a%20comprehensive%20set%20of%20personal%20factors%20gathered%20via%0Asurvey%20%28n%20%3D%201457%29.%20Factors%20ranged%20from%20psychosocial%20and%20cognitive%20attributes%20to%0Adriving%20style%2C%20experiences%2C%20and%20perceived%20AV%20risks%20and%20benefits.%20Using%20the%0Aexplainable%20AI%20technique%20SHAP%2C%20we%20found%20that%20perceptions%20of%20AV%20risks%20and%0Abenefits%2C%20attitudes%20toward%20feasibility%20and%20usability%2C%20institutional%20trust%2C%0Aprior%20experience%2C%20and%20a%20person%27s%20mental%20model%20are%20the%20most%20important%0Apredictors.%20Surprisingly%2C%20psychosocial%20and%20many%20technology-%20and%0Adriving-specific%20factors%20were%20not%20strong%20predictors.%20Results%20highlight%20the%0Aimportance%20of%20individual%20differences%20for%20designing%20trustworthy%20AVs%20for%20diverse%0Agroups%20and%20lead%20to%20key%20implications%20for%20future%20design%20and%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Trust%2520In%2520Autonomous%2520Vehicles%253A%2520Modeling%2520Young%2520Adult%250A%2520%2520Psychosocial%2520Traits%252C%2520Risk-Benefit%2520Attitudes%252C%2520And%2520Driving%2520Factors%2520With%2520Machine%250A%2520%2520Learning%26entry.906535625%3DRobert%2520Kaufman%2520and%2520Emi%2520Lee%2520and%2520Manas%2520Satish%2520Bedmutha%2520and%2520David%2520Kirsh%2520and%2520Nadir%2520Weibel%26entry.1292438233%3D%2520%2520Low%2520trust%2520remains%2520a%2520significant%2520barrier%2520to%2520Autonomous%2520Vehicle%2520%2528AV%2529%2520adoption.%250ATo%2520design%2520trustworthy%2520AVs%252C%2520we%2520need%2520to%2520better%2520understand%2520the%2520individual%2520traits%252C%250Aattitudes%252C%2520and%2520experiences%2520that%2520impact%2520people%2527s%2520trust%2520judgements.%2520We%2520use%250Amachine%2520learning%2520to%2520understand%2520the%2520most%2520important%2520factors%2520that%2520contribute%2520to%250Ayoung%2520adult%2520trust%2520based%2520on%2520a%2520comprehensive%2520set%2520of%2520personal%2520factors%2520gathered%2520via%250Asurvey%2520%2528n%2520%253D%25201457%2529.%2520Factors%2520ranged%2520from%2520psychosocial%2520and%2520cognitive%2520attributes%2520to%250Adriving%2520style%252C%2520experiences%252C%2520and%2520perceived%2520AV%2520risks%2520and%2520benefits.%2520Using%2520the%250Aexplainable%2520AI%2520technique%2520SHAP%252C%2520we%2520found%2520that%2520perceptions%2520of%2520AV%2520risks%2520and%250Abenefits%252C%2520attitudes%2520toward%2520feasibility%2520and%2520usability%252C%2520institutional%2520trust%252C%250Aprior%2520experience%252C%2520and%2520a%2520person%2527s%2520mental%2520model%2520are%2520the%2520most%2520important%250Apredictors.%2520Surprisingly%252C%2520psychosocial%2520and%2520many%2520technology-%2520and%250Adriving-specific%2520factors%2520were%2520not%2520strong%2520predictors.%2520Results%2520highlight%2520the%250Aimportance%2520of%2520individual%2520differences%2520for%2520designing%2520trustworthy%2520AVs%2520for%2520diverse%250Agroups%2520and%2520lead%2520to%2520key%2520implications%2520for%2520future%2520design%2520and%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Trust%20In%20Autonomous%20Vehicles%3A%20Modeling%20Young%20Adult%0A%20%20Psychosocial%20Traits%2C%20Risk-Benefit%20Attitudes%2C%20And%20Driving%20Factors%20With%20Machine%0A%20%20Learning&entry.906535625=Robert%20Kaufman%20and%20Emi%20Lee%20and%20Manas%20Satish%20Bedmutha%20and%20David%20Kirsh%20and%20Nadir%20Weibel&entry.1292438233=%20%20Low%20trust%20remains%20a%20significant%20barrier%20to%20Autonomous%20Vehicle%20%28AV%29%20adoption.%0ATo%20design%20trustworthy%20AVs%2C%20we%20need%20to%20better%20understand%20the%20individual%20traits%2C%0Aattitudes%2C%20and%20experiences%20that%20impact%20people%27s%20trust%20judgements.%20We%20use%0Amachine%20learning%20to%20understand%20the%20most%20important%20factors%20that%20contribute%20to%0Ayoung%20adult%20trust%20based%20on%20a%20comprehensive%20set%20of%20personal%20factors%20gathered%20via%0Asurvey%20%28n%20%3D%201457%29.%20Factors%20ranged%20from%20psychosocial%20and%20cognitive%20attributes%20to%0Adriving%20style%2C%20experiences%2C%20and%20perceived%20AV%20risks%20and%20benefits.%20Using%20the%0Aexplainable%20AI%20technique%20SHAP%2C%20we%20found%20that%20perceptions%20of%20AV%20risks%20and%0Abenefits%2C%20attitudes%20toward%20feasibility%20and%20usability%2C%20institutional%20trust%2C%0Aprior%20experience%2C%20and%20a%20person%27s%20mental%20model%20are%20the%20most%20important%0Apredictors.%20Surprisingly%2C%20psychosocial%20and%20many%20technology-%20and%0Adriving-specific%20factors%20were%20not%20strong%20predictors.%20Results%20highlight%20the%0Aimportance%20of%20individual%20differences%20for%20designing%20trustworthy%20AVs%20for%20diverse%0Agroups%20and%20lead%20to%20key%20implications%20for%20future%20design%20and%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08980v1&entry.124074799=Read"},
{"title": "Combining Data Generation and Active Learning for Low-Resource Question\n  Answering", "author": "Maximilian Kimmich and Andrea Bartezzaghi and Jasmina Bogojeska and Cristiano Malossi and Ngoc Thang Vu", "abstract": "  Neural approaches have become very popular in Question Answering (QA),\nhowever, they require a large amount of annotated data. In this work, we\npropose a novel approach that combines data augmentation via question-answer\ngeneration with Active Learning to improve performance in low-resource\nsettings, where the target domains are diverse in terms of difficulty and\nsimilarity to the source domain. We also investigate Active Learning for\nquestion answering in different stages, overall reducing the annotation effort\nof humans. For this purpose, we consider target domains in realistic settings,\nwith an extremely low amount of annotated samples but with many unlabeled\ndocuments, which we assume can be obtained with little effort. Additionally, we\nassume a sufficient amount of labeled data from the source domain being\navailable. We perform extensive experiments to find the best setup for\nincorporating domain experts. Our findings show that our novel approach, where\nhumans are incorporated in a data generation approach, boosts performance in\nthe low-resource, domain-specific setting, allowing for low-labeling-effort\nquestion answering systems in new, specialized domains. They further\ndemonstrate how human annotation affects the performance of QA depending on the\nstage it is performed.\n", "link": "http://arxiv.org/abs/2211.14880v2", "date": "2024-09-13", "relevancy": 1.9752, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5008}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4984}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Data%20Generation%20and%20Active%20Learning%20for%20Low-Resource%20Question%0A%20%20Answering&body=Title%3A%20Combining%20Data%20Generation%20and%20Active%20Learning%20for%20Low-Resource%20Question%0A%20%20Answering%0AAuthor%3A%20Maximilian%20Kimmich%20and%20Andrea%20Bartezzaghi%20and%20Jasmina%20Bogojeska%20and%20Cristiano%20Malossi%20and%20Ngoc%20Thang%20Vu%0AAbstract%3A%20%20%20Neural%20approaches%20have%20become%20very%20popular%20in%20Question%20Answering%20%28QA%29%2C%0Ahowever%2C%20they%20require%20a%20large%20amount%20of%20annotated%20data.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20approach%20that%20combines%20data%20augmentation%20via%20question-answer%0Ageneration%20with%20Active%20Learning%20to%20improve%20performance%20in%20low-resource%0Asettings%2C%20where%20the%20target%20domains%20are%20diverse%20in%20terms%20of%20difficulty%20and%0Asimilarity%20to%20the%20source%20domain.%20We%20also%20investigate%20Active%20Learning%20for%0Aquestion%20answering%20in%20different%20stages%2C%20overall%20reducing%20the%20annotation%20effort%0Aof%20humans.%20For%20this%20purpose%2C%20we%20consider%20target%20domains%20in%20realistic%20settings%2C%0Awith%20an%20extremely%20low%20amount%20of%20annotated%20samples%20but%20with%20many%20unlabeled%0Adocuments%2C%20which%20we%20assume%20can%20be%20obtained%20with%20little%20effort.%20Additionally%2C%20we%0Aassume%20a%20sufficient%20amount%20of%20labeled%20data%20from%20the%20source%20domain%20being%0Aavailable.%20We%20perform%20extensive%20experiments%20to%20find%20the%20best%20setup%20for%0Aincorporating%20domain%20experts.%20Our%20findings%20show%20that%20our%20novel%20approach%2C%20where%0Ahumans%20are%20incorporated%20in%20a%20data%20generation%20approach%2C%20boosts%20performance%20in%0Athe%20low-resource%2C%20domain-specific%20setting%2C%20allowing%20for%20low-labeling-effort%0Aquestion%20answering%20systems%20in%20new%2C%20specialized%20domains.%20They%20further%0Ademonstrate%20how%20human%20annotation%20affects%20the%20performance%20of%20QA%20depending%20on%20the%0Astage%20it%20is%20performed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.14880v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Data%2520Generation%2520and%2520Active%2520Learning%2520for%2520Low-Resource%2520Question%250A%2520%2520Answering%26entry.906535625%3DMaximilian%2520Kimmich%2520and%2520Andrea%2520Bartezzaghi%2520and%2520Jasmina%2520Bogojeska%2520and%2520Cristiano%2520Malossi%2520and%2520Ngoc%2520Thang%2520Vu%26entry.1292438233%3D%2520%2520Neural%2520approaches%2520have%2520become%2520very%2520popular%2520in%2520Question%2520Answering%2520%2528QA%2529%252C%250Ahowever%252C%2520they%2520require%2520a%2520large%2520amount%2520of%2520annotated%2520data.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520that%2520combines%2520data%2520augmentation%2520via%2520question-answer%250Ageneration%2520with%2520Active%2520Learning%2520to%2520improve%2520performance%2520in%2520low-resource%250Asettings%252C%2520where%2520the%2520target%2520domains%2520are%2520diverse%2520in%2520terms%2520of%2520difficulty%2520and%250Asimilarity%2520to%2520the%2520source%2520domain.%2520We%2520also%2520investigate%2520Active%2520Learning%2520for%250Aquestion%2520answering%2520in%2520different%2520stages%252C%2520overall%2520reducing%2520the%2520annotation%2520effort%250Aof%2520humans.%2520For%2520this%2520purpose%252C%2520we%2520consider%2520target%2520domains%2520in%2520realistic%2520settings%252C%250Awith%2520an%2520extremely%2520low%2520amount%2520of%2520annotated%2520samples%2520but%2520with%2520many%2520unlabeled%250Adocuments%252C%2520which%2520we%2520assume%2520can%2520be%2520obtained%2520with%2520little%2520effort.%2520Additionally%252C%2520we%250Aassume%2520a%2520sufficient%2520amount%2520of%2520labeled%2520data%2520from%2520the%2520source%2520domain%2520being%250Aavailable.%2520We%2520perform%2520extensive%2520experiments%2520to%2520find%2520the%2520best%2520setup%2520for%250Aincorporating%2520domain%2520experts.%2520Our%2520findings%2520show%2520that%2520our%2520novel%2520approach%252C%2520where%250Ahumans%2520are%2520incorporated%2520in%2520a%2520data%2520generation%2520approach%252C%2520boosts%2520performance%2520in%250Athe%2520low-resource%252C%2520domain-specific%2520setting%252C%2520allowing%2520for%2520low-labeling-effort%250Aquestion%2520answering%2520systems%2520in%2520new%252C%2520specialized%2520domains.%2520They%2520further%250Ademonstrate%2520how%2520human%2520annotation%2520affects%2520the%2520performance%2520of%2520QA%2520depending%2520on%2520the%250Astage%2520it%2520is%2520performed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.14880v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Data%20Generation%20and%20Active%20Learning%20for%20Low-Resource%20Question%0A%20%20Answering&entry.906535625=Maximilian%20Kimmich%20and%20Andrea%20Bartezzaghi%20and%20Jasmina%20Bogojeska%20and%20Cristiano%20Malossi%20and%20Ngoc%20Thang%20Vu&entry.1292438233=%20%20Neural%20approaches%20have%20become%20very%20popular%20in%20Question%20Answering%20%28QA%29%2C%0Ahowever%2C%20they%20require%20a%20large%20amount%20of%20annotated%20data.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20approach%20that%20combines%20data%20augmentation%20via%20question-answer%0Ageneration%20with%20Active%20Learning%20to%20improve%20performance%20in%20low-resource%0Asettings%2C%20where%20the%20target%20domains%20are%20diverse%20in%20terms%20of%20difficulty%20and%0Asimilarity%20to%20the%20source%20domain.%20We%20also%20investigate%20Active%20Learning%20for%0Aquestion%20answering%20in%20different%20stages%2C%20overall%20reducing%20the%20annotation%20effort%0Aof%20humans.%20For%20this%20purpose%2C%20we%20consider%20target%20domains%20in%20realistic%20settings%2C%0Awith%20an%20extremely%20low%20amount%20of%20annotated%20samples%20but%20with%20many%20unlabeled%0Adocuments%2C%20which%20we%20assume%20can%20be%20obtained%20with%20little%20effort.%20Additionally%2C%20we%0Aassume%20a%20sufficient%20amount%20of%20labeled%20data%20from%20the%20source%20domain%20being%0Aavailable.%20We%20perform%20extensive%20experiments%20to%20find%20the%20best%20setup%20for%0Aincorporating%20domain%20experts.%20Our%20findings%20show%20that%20our%20novel%20approach%2C%20where%0Ahumans%20are%20incorporated%20in%20a%20data%20generation%20approach%2C%20boosts%20performance%20in%0Athe%20low-resource%2C%20domain-specific%20setting%2C%20allowing%20for%20low-labeling-effort%0Aquestion%20answering%20systems%20in%20new%2C%20specialized%20domains.%20They%20further%0Ademonstrate%20how%20human%20annotation%20affects%20the%20performance%20of%20QA%20depending%20on%20the%0Astage%20it%20is%20performed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.14880v2&entry.124074799=Read"},
{"title": "VAE Explainer: Supplement Learning Variational Autoencoders with\n  Interactive Visualization", "author": "Donald Bertucci and Alex Endert", "abstract": "  Variational Autoencoders are widespread in Machine Learning, but are\ntypically explained with dense math notation or static code examples. This\npaper presents VAE Explainer, an interactive Variational Autoencoder running in\nthe browser to supplement existing static documentation (e.g., Keras Code\nExamples). VAE Explainer adds interactions to the VAE summary with interactive\nmodel inputs, latent space, and output. VAE Explainer connects the high-level\nunderstanding with the implementation: annotated code and a live computational\ngraph. The VAE Explainer interactive visualization is live at\nhttps://xnought.github.io/vae-explainer and the code is open source at\nhttps://github.com/xnought/vae-explainer.\n", "link": "http://arxiv.org/abs/2409.09011v1", "date": "2024-09-13", "relevancy": 1.9691, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5149}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4791}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAE%20Explainer%3A%20Supplement%20Learning%20Variational%20Autoencoders%20with%0A%20%20Interactive%20Visualization&body=Title%3A%20VAE%20Explainer%3A%20Supplement%20Learning%20Variational%20Autoencoders%20with%0A%20%20Interactive%20Visualization%0AAuthor%3A%20Donald%20Bertucci%20and%20Alex%20Endert%0AAbstract%3A%20%20%20Variational%20Autoencoders%20are%20widespread%20in%20Machine%20Learning%2C%20but%20are%0Atypically%20explained%20with%20dense%20math%20notation%20or%20static%20code%20examples.%20This%0Apaper%20presents%20VAE%20Explainer%2C%20an%20interactive%20Variational%20Autoencoder%20running%20in%0Athe%20browser%20to%20supplement%20existing%20static%20documentation%20%28e.g.%2C%20Keras%20Code%0AExamples%29.%20VAE%20Explainer%20adds%20interactions%20to%20the%20VAE%20summary%20with%20interactive%0Amodel%20inputs%2C%20latent%20space%2C%20and%20output.%20VAE%20Explainer%20connects%20the%20high-level%0Aunderstanding%20with%20the%20implementation%3A%20annotated%20code%20and%20a%20live%20computational%0Agraph.%20The%20VAE%20Explainer%20interactive%20visualization%20is%20live%20at%0Ahttps%3A//xnought.github.io/vae-explainer%20and%20the%20code%20is%20open%20source%20at%0Ahttps%3A//github.com/xnought/vae-explainer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAE%2520Explainer%253A%2520Supplement%2520Learning%2520Variational%2520Autoencoders%2520with%250A%2520%2520Interactive%2520Visualization%26entry.906535625%3DDonald%2520Bertucci%2520and%2520Alex%2520Endert%26entry.1292438233%3D%2520%2520Variational%2520Autoencoders%2520are%2520widespread%2520in%2520Machine%2520Learning%252C%2520but%2520are%250Atypically%2520explained%2520with%2520dense%2520math%2520notation%2520or%2520static%2520code%2520examples.%2520This%250Apaper%2520presents%2520VAE%2520Explainer%252C%2520an%2520interactive%2520Variational%2520Autoencoder%2520running%2520in%250Athe%2520browser%2520to%2520supplement%2520existing%2520static%2520documentation%2520%2528e.g.%252C%2520Keras%2520Code%250AExamples%2529.%2520VAE%2520Explainer%2520adds%2520interactions%2520to%2520the%2520VAE%2520summary%2520with%2520interactive%250Amodel%2520inputs%252C%2520latent%2520space%252C%2520and%2520output.%2520VAE%2520Explainer%2520connects%2520the%2520high-level%250Aunderstanding%2520with%2520the%2520implementation%253A%2520annotated%2520code%2520and%2520a%2520live%2520computational%250Agraph.%2520The%2520VAE%2520Explainer%2520interactive%2520visualization%2520is%2520live%2520at%250Ahttps%253A//xnought.github.io/vae-explainer%2520and%2520the%2520code%2520is%2520open%2520source%2520at%250Ahttps%253A//github.com/xnought/vae-explainer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAE%20Explainer%3A%20Supplement%20Learning%20Variational%20Autoencoders%20with%0A%20%20Interactive%20Visualization&entry.906535625=Donald%20Bertucci%20and%20Alex%20Endert&entry.1292438233=%20%20Variational%20Autoencoders%20are%20widespread%20in%20Machine%20Learning%2C%20but%20are%0Atypically%20explained%20with%20dense%20math%20notation%20or%20static%20code%20examples.%20This%0Apaper%20presents%20VAE%20Explainer%2C%20an%20interactive%20Variational%20Autoencoder%20running%20in%0Athe%20browser%20to%20supplement%20existing%20static%20documentation%20%28e.g.%2C%20Keras%20Code%0AExamples%29.%20VAE%20Explainer%20adds%20interactions%20to%20the%20VAE%20summary%20with%20interactive%0Amodel%20inputs%2C%20latent%20space%2C%20and%20output.%20VAE%20Explainer%20connects%20the%20high-level%0Aunderstanding%20with%20the%20implementation%3A%20annotated%20code%20and%20a%20live%20computational%0Agraph.%20The%20VAE%20Explainer%20interactive%20visualization%20is%20live%20at%0Ahttps%3A//xnought.github.io/vae-explainer%20and%20the%20code%20is%20open%20source%20at%0Ahttps%3A//github.com/xnought/vae-explainer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09011v1&entry.124074799=Read"},
{"title": "L3Cube-IndicQuest: A Benchmark Questing Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context", "author": "Pritika Rohera and Chaitrali Ginimav and Akanksha Salunke and Gayatri Sawant and Raviraj Joshi", "abstract": "  Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard question-answering benchmark dataset designed to evaluate how\nwell multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp .\n", "link": "http://arxiv.org/abs/2409.08706v1", "date": "2024-09-13", "relevancy": 1.9649, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L3Cube-IndicQuest%3A%20A%20Benchmark%20Questing%20Answering%20Dataset%20for%20Evaluating%0A%20%20Knowledge%20of%20LLMs%20in%20Indic%20Context&body=Title%3A%20L3Cube-IndicQuest%3A%20A%20Benchmark%20Questing%20Answering%20Dataset%20for%20Evaluating%0A%20%20Knowledge%20of%20LLMs%20in%20Indic%20Context%0AAuthor%3A%20Pritika%20Rohera%20and%20Chaitrali%20Ginimav%20and%20Akanksha%20Salunke%20and%20Gayatri%20Sawant%20and%20Raviraj%20Joshi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20progress%20in%20incorporating%0AIndic%20languages%20within%20multilingual%20models.%20However%2C%20it%20is%20crucial%20to%0Aquantitatively%20assess%20whether%20these%20languages%20perform%20comparably%20to%20globally%0Adominant%20ones%2C%20such%20as%20English.%20Currently%2C%20there%20is%20a%20lack%20of%20benchmark%0Adatasets%20specifically%20designed%20to%20evaluate%20the%20regional%20knowledge%20of%20LLMs%20in%0Avarious%20Indic%20languages.%20In%20this%20paper%2C%20we%20present%20the%20L3Cube-IndicQuest%2C%20a%0Agold-standard%20question-answering%20benchmark%20dataset%20designed%20to%20evaluate%20how%0Awell%20multilingual%20LLMs%20capture%20regional%20knowledge%20across%20various%20Indic%0Alanguages.%20The%20dataset%20contains%20200%20question-answer%20pairs%2C%20each%20for%20English%20and%0A19%20Indic%20languages%2C%20covering%20five%20domains%20specific%20to%20the%20Indic%20region.%20We%20aim%0Afor%20this%20dataset%20to%20serve%20as%20a%20benchmark%2C%20providing%20ground%20truth%20for%20evaluating%0Athe%20performance%20of%20LLMs%20in%20understanding%20and%20representing%20knowledge%20relevant%20to%0Athe%20Indian%20context.%20The%20IndicQuest%20can%20be%20used%20for%20both%20reference-based%0Aevaluation%20and%20LLM-as-a-judge%20evaluation.%20The%20dataset%20is%20shared%20publicly%20at%0Ahttps%3A//github.com/l3cube-pune/indic-nlp%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL3Cube-IndicQuest%253A%2520A%2520Benchmark%2520Questing%2520Answering%2520Dataset%2520for%2520Evaluating%250A%2520%2520Knowledge%2520of%2520LLMs%2520in%2520Indic%2520Context%26entry.906535625%3DPritika%2520Rohera%2520and%2520Chaitrali%2520Ginimav%2520and%2520Akanksha%2520Salunke%2520and%2520Gayatri%2520Sawant%2520and%2520Raviraj%2520Joshi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520incorporating%250AIndic%2520languages%2520within%2520multilingual%2520models.%2520However%252C%2520it%2520is%2520crucial%2520to%250Aquantitatively%2520assess%2520whether%2520these%2520languages%2520perform%2520comparably%2520to%2520globally%250Adominant%2520ones%252C%2520such%2520as%2520English.%2520Currently%252C%2520there%2520is%2520a%2520lack%2520of%2520benchmark%250Adatasets%2520specifically%2520designed%2520to%2520evaluate%2520the%2520regional%2520knowledge%2520of%2520LLMs%2520in%250Avarious%2520Indic%2520languages.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520L3Cube-IndicQuest%252C%2520a%250Agold-standard%2520question-answering%2520benchmark%2520dataset%2520designed%2520to%2520evaluate%2520how%250Awell%2520multilingual%2520LLMs%2520capture%2520regional%2520knowledge%2520across%2520various%2520Indic%250Alanguages.%2520The%2520dataset%2520contains%2520200%2520question-answer%2520pairs%252C%2520each%2520for%2520English%2520and%250A19%2520Indic%2520languages%252C%2520covering%2520five%2520domains%2520specific%2520to%2520the%2520Indic%2520region.%2520We%2520aim%250Afor%2520this%2520dataset%2520to%2520serve%2520as%2520a%2520benchmark%252C%2520providing%2520ground%2520truth%2520for%2520evaluating%250Athe%2520performance%2520of%2520LLMs%2520in%2520understanding%2520and%2520representing%2520knowledge%2520relevant%2520to%250Athe%2520Indian%2520context.%2520The%2520IndicQuest%2520can%2520be%2520used%2520for%2520both%2520reference-based%250Aevaluation%2520and%2520LLM-as-a-judge%2520evaluation.%2520The%2520dataset%2520is%2520shared%2520publicly%2520at%250Ahttps%253A//github.com/l3cube-pune/indic-nlp%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L3Cube-IndicQuest%3A%20A%20Benchmark%20Questing%20Answering%20Dataset%20for%20Evaluating%0A%20%20Knowledge%20of%20LLMs%20in%20Indic%20Context&entry.906535625=Pritika%20Rohera%20and%20Chaitrali%20Ginimav%20and%20Akanksha%20Salunke%20and%20Gayatri%20Sawant%20and%20Raviraj%20Joshi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20progress%20in%20incorporating%0AIndic%20languages%20within%20multilingual%20models.%20However%2C%20it%20is%20crucial%20to%0Aquantitatively%20assess%20whether%20these%20languages%20perform%20comparably%20to%20globally%0Adominant%20ones%2C%20such%20as%20English.%20Currently%2C%20there%20is%20a%20lack%20of%20benchmark%0Adatasets%20specifically%20designed%20to%20evaluate%20the%20regional%20knowledge%20of%20LLMs%20in%0Avarious%20Indic%20languages.%20In%20this%20paper%2C%20we%20present%20the%20L3Cube-IndicQuest%2C%20a%0Agold-standard%20question-answering%20benchmark%20dataset%20designed%20to%20evaluate%20how%0Awell%20multilingual%20LLMs%20capture%20regional%20knowledge%20across%20various%20Indic%0Alanguages.%20The%20dataset%20contains%20200%20question-answer%20pairs%2C%20each%20for%20English%20and%0A19%20Indic%20languages%2C%20covering%20five%20domains%20specific%20to%20the%20Indic%20region.%20We%20aim%0Afor%20this%20dataset%20to%20serve%20as%20a%20benchmark%2C%20providing%20ground%20truth%20for%20evaluating%0Athe%20performance%20of%20LLMs%20in%20understanding%20and%20representing%20knowledge%20relevant%20to%0Athe%20Indian%20context.%20The%20IndicQuest%20can%20be%20used%20for%20both%20reference-based%0Aevaluation%20and%20LLM-as-a-judge%20evaluation.%20The%20dataset%20is%20shared%20publicly%20at%0Ahttps%3A//github.com/l3cube-pune/indic-nlp%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08706v1&entry.124074799=Read"},
{"title": "A comparative study of human inverse kinematics techniques for lower\n  limbs", "author": "Zineb Benhmidouch and Saad Moufid and Aissam Ait Omar", "abstract": "  Inverse Kinematics (IK) remains a dynamic field of research, with various\nmethods striving for speed and precision. Despite advancements, many IK\ntechniques face significant challenges, including high computational demands\nand the risk of generating unrealistic joint configurations. This paper\nconducts a comprehensive comparative analysis of leading IK methods applied to\nthe human leg, aiming to identify the most effective approach. We evaluate each\nmethod based on computational efficiency and its ability to produce realistic\npostures, while adhering to the natural range of motion and comfort zones of\nthe joints. The findings provide insights into optimizing IK solutions for\npractical applications in biomechanics and animation.\n", "link": "http://arxiv.org/abs/2302.10769v4", "date": "2024-09-13", "relevancy": 1.9435, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4985}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comparative%20study%20of%20human%20inverse%20kinematics%20techniques%20for%20lower%0A%20%20limbs&body=Title%3A%20A%20comparative%20study%20of%20human%20inverse%20kinematics%20techniques%20for%20lower%0A%20%20limbs%0AAuthor%3A%20Zineb%20Benhmidouch%20and%20Saad%20Moufid%20and%20Aissam%20Ait%20Omar%0AAbstract%3A%20%20%20Inverse%20Kinematics%20%28IK%29%20remains%20a%20dynamic%20field%20of%20research%2C%20with%20various%0Amethods%20striving%20for%20speed%20and%20precision.%20Despite%20advancements%2C%20many%20IK%0Atechniques%20face%20significant%20challenges%2C%20including%20high%20computational%20demands%0Aand%20the%20risk%20of%20generating%20unrealistic%20joint%20configurations.%20This%20paper%0Aconducts%20a%20comprehensive%20comparative%20analysis%20of%20leading%20IK%20methods%20applied%20to%0Athe%20human%20leg%2C%20aiming%20to%20identify%20the%20most%20effective%20approach.%20We%20evaluate%20each%0Amethod%20based%20on%20computational%20efficiency%20and%20its%20ability%20to%20produce%20realistic%0Apostures%2C%20while%20adhering%20to%20the%20natural%20range%20of%20motion%20and%20comfort%20zones%20of%0Athe%20joints.%20The%20findings%20provide%20insights%20into%20optimizing%20IK%20solutions%20for%0Apractical%20applications%20in%20biomechanics%20and%20animation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.10769v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comparative%2520study%2520of%2520human%2520inverse%2520kinematics%2520techniques%2520for%2520lower%250A%2520%2520limbs%26entry.906535625%3DZineb%2520Benhmidouch%2520and%2520Saad%2520Moufid%2520and%2520Aissam%2520Ait%2520Omar%26entry.1292438233%3D%2520%2520Inverse%2520Kinematics%2520%2528IK%2529%2520remains%2520a%2520dynamic%2520field%2520of%2520research%252C%2520with%2520various%250Amethods%2520striving%2520for%2520speed%2520and%2520precision.%2520Despite%2520advancements%252C%2520many%2520IK%250Atechniques%2520face%2520significant%2520challenges%252C%2520including%2520high%2520computational%2520demands%250Aand%2520the%2520risk%2520of%2520generating%2520unrealistic%2520joint%2520configurations.%2520This%2520paper%250Aconducts%2520a%2520comprehensive%2520comparative%2520analysis%2520of%2520leading%2520IK%2520methods%2520applied%2520to%250Athe%2520human%2520leg%252C%2520aiming%2520to%2520identify%2520the%2520most%2520effective%2520approach.%2520We%2520evaluate%2520each%250Amethod%2520based%2520on%2520computational%2520efficiency%2520and%2520its%2520ability%2520to%2520produce%2520realistic%250Apostures%252C%2520while%2520adhering%2520to%2520the%2520natural%2520range%2520of%2520motion%2520and%2520comfort%2520zones%2520of%250Athe%2520joints.%2520The%2520findings%2520provide%2520insights%2520into%2520optimizing%2520IK%2520solutions%2520for%250Apractical%2520applications%2520in%2520biomechanics%2520and%2520animation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.10769v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comparative%20study%20of%20human%20inverse%20kinematics%20techniques%20for%20lower%0A%20%20limbs&entry.906535625=Zineb%20Benhmidouch%20and%20Saad%20Moufid%20and%20Aissam%20Ait%20Omar&entry.1292438233=%20%20Inverse%20Kinematics%20%28IK%29%20remains%20a%20dynamic%20field%20of%20research%2C%20with%20various%0Amethods%20striving%20for%20speed%20and%20precision.%20Despite%20advancements%2C%20many%20IK%0Atechniques%20face%20significant%20challenges%2C%20including%20high%20computational%20demands%0Aand%20the%20risk%20of%20generating%20unrealistic%20joint%20configurations.%20This%20paper%0Aconducts%20a%20comprehensive%20comparative%20analysis%20of%20leading%20IK%20methods%20applied%20to%0Athe%20human%20leg%2C%20aiming%20to%20identify%20the%20most%20effective%20approach.%20We%20evaluate%20each%0Amethod%20based%20on%20computational%20efficiency%20and%20its%20ability%20to%20produce%20realistic%0Apostures%2C%20while%20adhering%20to%20the%20natural%20range%20of%20motion%20and%20comfort%20zones%20of%0Athe%20joints.%20The%20findings%20provide%20insights%20into%20optimizing%20IK%20solutions%20for%0Apractical%20applications%20in%20biomechanics%20and%20animation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.10769v4&entry.124074799=Read"},
{"title": "AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding", "author": "Chang Lei and Huan Lei", "abstract": "  Artificial intelligence for card games has long been a popular topic in AI\nresearch. In recent years, complex card games like Mahjong and Texas Hold'em\nhave been solved, with corresponding AI programs reaching the level of human\nexperts. However, the game of Doudizhu presents significant challenges due to\nits vast state/action space and unique characteristics involving reasoning\nabout competition and cooperation, making the game extremely difficult to\nsolve.The RL model Douzero, trained using the Deep Monte Carlo algorithm\nframework, has shown excellent performance in Doudizhu. However, there are\ndifferences between its simplified game environment and the actual Doudizhu\nenvironment, and its performance is still a considerable distance from that of\nhuman experts. This paper modifies the Deep Monte Carlo algorithm framework by\nusing reinforcement learning to obtain a neural network that simultaneously\nestimates win rates and expectations. The action space is pruned using\nexpectations, and strategies are generated based on win rates. The modified\nalgorithm enables the AI to perform the full range of tasks in the Doudizhu\ngame, including bidding and cardplay. The model was trained in a actual\nDoudizhu environment and achieved state-of-the-art performance among publicly\navailable models. We hope that this new framework will provide valuable\ninsights for AI development in other bidding-based games.\n", "link": "http://arxiv.org/abs/2407.10279v2", "date": "2024-09-13", "relevancy": 1.9409, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4975}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4896}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaDou%3A%20High-Performance%20End-to-End%20Doudizhu%20AI%20Integrating%20Bidding&body=Title%3A%20AlphaDou%3A%20High-Performance%20End-to-End%20Doudizhu%20AI%20Integrating%20Bidding%0AAuthor%3A%20Chang%20Lei%20and%20Huan%20Lei%0AAbstract%3A%20%20%20Artificial%20intelligence%20for%20card%20games%20has%20long%20been%20a%20popular%20topic%20in%20AI%0Aresearch.%20In%20recent%20years%2C%20complex%20card%20games%20like%20Mahjong%20and%20Texas%20Hold%27em%0Ahave%20been%20solved%2C%20with%20corresponding%20AI%20programs%20reaching%20the%20level%20of%20human%0Aexperts.%20However%2C%20the%20game%20of%20Doudizhu%20presents%20significant%20challenges%20due%20to%0Aits%20vast%20state/action%20space%20and%20unique%20characteristics%20involving%20reasoning%0Aabout%20competition%20and%20cooperation%2C%20making%20the%20game%20extremely%20difficult%20to%0Asolve.The%20RL%20model%20Douzero%2C%20trained%20using%20the%20Deep%20Monte%20Carlo%20algorithm%0Aframework%2C%20has%20shown%20excellent%20performance%20in%20Doudizhu.%20However%2C%20there%20are%0Adifferences%20between%20its%20simplified%20game%20environment%20and%20the%20actual%20Doudizhu%0Aenvironment%2C%20and%20its%20performance%20is%20still%20a%20considerable%20distance%20from%20that%20of%0Ahuman%20experts.%20This%20paper%20modifies%20the%20Deep%20Monte%20Carlo%20algorithm%20framework%20by%0Ausing%20reinforcement%20learning%20to%20obtain%20a%20neural%20network%20that%20simultaneously%0Aestimates%20win%20rates%20and%20expectations.%20The%20action%20space%20is%20pruned%20using%0Aexpectations%2C%20and%20strategies%20are%20generated%20based%20on%20win%20rates.%20The%20modified%0Aalgorithm%20enables%20the%20AI%20to%20perform%20the%20full%20range%20of%20tasks%20in%20the%20Doudizhu%0Agame%2C%20including%20bidding%20and%20cardplay.%20The%20model%20was%20trained%20in%20a%20actual%0ADoudizhu%20environment%20and%20achieved%20state-of-the-art%20performance%20among%20publicly%0Aavailable%20models.%20We%20hope%20that%20this%20new%20framework%20will%20provide%20valuable%0Ainsights%20for%20AI%20development%20in%20other%20bidding-based%20games.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaDou%253A%2520High-Performance%2520End-to-End%2520Doudizhu%2520AI%2520Integrating%2520Bidding%26entry.906535625%3DChang%2520Lei%2520and%2520Huan%2520Lei%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520for%2520card%2520games%2520has%2520long%2520been%2520a%2520popular%2520topic%2520in%2520AI%250Aresearch.%2520In%2520recent%2520years%252C%2520complex%2520card%2520games%2520like%2520Mahjong%2520and%2520Texas%2520Hold%2527em%250Ahave%2520been%2520solved%252C%2520with%2520corresponding%2520AI%2520programs%2520reaching%2520the%2520level%2520of%2520human%250Aexperts.%2520However%252C%2520the%2520game%2520of%2520Doudizhu%2520presents%2520significant%2520challenges%2520due%2520to%250Aits%2520vast%2520state/action%2520space%2520and%2520unique%2520characteristics%2520involving%2520reasoning%250Aabout%2520competition%2520and%2520cooperation%252C%2520making%2520the%2520game%2520extremely%2520difficult%2520to%250Asolve.The%2520RL%2520model%2520Douzero%252C%2520trained%2520using%2520the%2520Deep%2520Monte%2520Carlo%2520algorithm%250Aframework%252C%2520has%2520shown%2520excellent%2520performance%2520in%2520Doudizhu.%2520However%252C%2520there%2520are%250Adifferences%2520between%2520its%2520simplified%2520game%2520environment%2520and%2520the%2520actual%2520Doudizhu%250Aenvironment%252C%2520and%2520its%2520performance%2520is%2520still%2520a%2520considerable%2520distance%2520from%2520that%2520of%250Ahuman%2520experts.%2520This%2520paper%2520modifies%2520the%2520Deep%2520Monte%2520Carlo%2520algorithm%2520framework%2520by%250Ausing%2520reinforcement%2520learning%2520to%2520obtain%2520a%2520neural%2520network%2520that%2520simultaneously%250Aestimates%2520win%2520rates%2520and%2520expectations.%2520The%2520action%2520space%2520is%2520pruned%2520using%250Aexpectations%252C%2520and%2520strategies%2520are%2520generated%2520based%2520on%2520win%2520rates.%2520The%2520modified%250Aalgorithm%2520enables%2520the%2520AI%2520to%2520perform%2520the%2520full%2520range%2520of%2520tasks%2520in%2520the%2520Doudizhu%250Agame%252C%2520including%2520bidding%2520and%2520cardplay.%2520The%2520model%2520was%2520trained%2520in%2520a%2520actual%250ADoudizhu%2520environment%2520and%2520achieved%2520state-of-the-art%2520performance%2520among%2520publicly%250Aavailable%2520models.%2520We%2520hope%2520that%2520this%2520new%2520framework%2520will%2520provide%2520valuable%250Ainsights%2520for%2520AI%2520development%2520in%2520other%2520bidding-based%2520games.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaDou%3A%20High-Performance%20End-to-End%20Doudizhu%20AI%20Integrating%20Bidding&entry.906535625=Chang%20Lei%20and%20Huan%20Lei&entry.1292438233=%20%20Artificial%20intelligence%20for%20card%20games%20has%20long%20been%20a%20popular%20topic%20in%20AI%0Aresearch.%20In%20recent%20years%2C%20complex%20card%20games%20like%20Mahjong%20and%20Texas%20Hold%27em%0Ahave%20been%20solved%2C%20with%20corresponding%20AI%20programs%20reaching%20the%20level%20of%20human%0Aexperts.%20However%2C%20the%20game%20of%20Doudizhu%20presents%20significant%20challenges%20due%20to%0Aits%20vast%20state/action%20space%20and%20unique%20characteristics%20involving%20reasoning%0Aabout%20competition%20and%20cooperation%2C%20making%20the%20game%20extremely%20difficult%20to%0Asolve.The%20RL%20model%20Douzero%2C%20trained%20using%20the%20Deep%20Monte%20Carlo%20algorithm%0Aframework%2C%20has%20shown%20excellent%20performance%20in%20Doudizhu.%20However%2C%20there%20are%0Adifferences%20between%20its%20simplified%20game%20environment%20and%20the%20actual%20Doudizhu%0Aenvironment%2C%20and%20its%20performance%20is%20still%20a%20considerable%20distance%20from%20that%20of%0Ahuman%20experts.%20This%20paper%20modifies%20the%20Deep%20Monte%20Carlo%20algorithm%20framework%20by%0Ausing%20reinforcement%20learning%20to%20obtain%20a%20neural%20network%20that%20simultaneously%0Aestimates%20win%20rates%20and%20expectations.%20The%20action%20space%20is%20pruned%20using%0Aexpectations%2C%20and%20strategies%20are%20generated%20based%20on%20win%20rates.%20The%20modified%0Aalgorithm%20enables%20the%20AI%20to%20perform%20the%20full%20range%20of%20tasks%20in%20the%20Doudizhu%0Agame%2C%20including%20bidding%20and%20cardplay.%20The%20model%20was%20trained%20in%20a%20actual%0ADoudizhu%20environment%20and%20achieved%20state-of-the-art%20performance%20among%20publicly%0Aavailable%20models.%20We%20hope%20that%20this%20new%20framework%20will%20provide%20valuable%0Ainsights%20for%20AI%20development%20in%20other%20bidding-based%20games.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10279v2&entry.124074799=Read"},
{"title": "Multi-intent Aware Contrastive Learning for Sequential Recommendation", "author": "Junshu Huang and Zi Long and Xianghua Fu and Yin Chen", "abstract": "  Intent is a significant latent factor influencing user-item interaction\nsequences. Prevalent sequence recommendation models that utilize contrastive\nlearning predominantly rely on single-intent representations to direct the\ntraining process. However, this paradigm oversimplifies real-world\nrecommendation scenarios, attempting to encapsulate the diversity of intents\nwithin the single-intent level representation. SR models considering\nmulti-intent information in their framework are more likely to reflect\nreal-life recommendation scenarios accurately.\n", "link": "http://arxiv.org/abs/2409.08733v1", "date": "2024-09-13", "relevancy": 1.9395, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5037}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-intent%20Aware%20Contrastive%20Learning%20for%20Sequential%20Recommendation&body=Title%3A%20Multi-intent%20Aware%20Contrastive%20Learning%20for%20Sequential%20Recommendation%0AAuthor%3A%20Junshu%20Huang%20and%20Zi%20Long%20and%20Xianghua%20Fu%20and%20Yin%20Chen%0AAbstract%3A%20%20%20Intent%20is%20a%20significant%20latent%20factor%20influencing%20user-item%20interaction%0Asequences.%20Prevalent%20sequence%20recommendation%20models%20that%20utilize%20contrastive%0Alearning%20predominantly%20rely%20on%20single-intent%20representations%20to%20direct%20the%0Atraining%20process.%20However%2C%20this%20paradigm%20oversimplifies%20real-world%0Arecommendation%20scenarios%2C%20attempting%20to%20encapsulate%20the%20diversity%20of%20intents%0Awithin%20the%20single-intent%20level%20representation.%20SR%20models%20considering%0Amulti-intent%20information%20in%20their%20framework%20are%20more%20likely%20to%20reflect%0Areal-life%20recommendation%20scenarios%20accurately.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-intent%2520Aware%2520Contrastive%2520Learning%2520for%2520Sequential%2520Recommendation%26entry.906535625%3DJunshu%2520Huang%2520and%2520Zi%2520Long%2520and%2520Xianghua%2520Fu%2520and%2520Yin%2520Chen%26entry.1292438233%3D%2520%2520Intent%2520is%2520a%2520significant%2520latent%2520factor%2520influencing%2520user-item%2520interaction%250Asequences.%2520Prevalent%2520sequence%2520recommendation%2520models%2520that%2520utilize%2520contrastive%250Alearning%2520predominantly%2520rely%2520on%2520single-intent%2520representations%2520to%2520direct%2520the%250Atraining%2520process.%2520However%252C%2520this%2520paradigm%2520oversimplifies%2520real-world%250Arecommendation%2520scenarios%252C%2520attempting%2520to%2520encapsulate%2520the%2520diversity%2520of%2520intents%250Awithin%2520the%2520single-intent%2520level%2520representation.%2520SR%2520models%2520considering%250Amulti-intent%2520information%2520in%2520their%2520framework%2520are%2520more%2520likely%2520to%2520reflect%250Areal-life%2520recommendation%2520scenarios%2520accurately.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-intent%20Aware%20Contrastive%20Learning%20for%20Sequential%20Recommendation&entry.906535625=Junshu%20Huang%20and%20Zi%20Long%20and%20Xianghua%20Fu%20and%20Yin%20Chen&entry.1292438233=%20%20Intent%20is%20a%20significant%20latent%20factor%20influencing%20user-item%20interaction%0Asequences.%20Prevalent%20sequence%20recommendation%20models%20that%20utilize%20contrastive%0Alearning%20predominantly%20rely%20on%20single-intent%20representations%20to%20direct%20the%0Atraining%20process.%20However%2C%20this%20paradigm%20oversimplifies%20real-world%0Arecommendation%20scenarios%2C%20attempting%20to%20encapsulate%20the%20diversity%20of%20intents%0Awithin%20the%20single-intent%20level%20representation.%20SR%20models%20considering%0Amulti-intent%20information%20in%20their%20framework%20are%20more%20likely%20to%20reflect%0Areal-life%20recommendation%20scenarios%20accurately.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08733v1&entry.124074799=Read"},
{"title": "Optimization and Generalization Guarantees for Weight Normalization", "author": "Pedro Cisneros-Velarde and Zhijie Chen and Sanmi Koyejo and Arindam Banerjee", "abstract": "  Weight normalization (WeightNorm) is widely used in practice for the training\nof deep neural networks and modern deep learning libraries have built-in\nimplementations of it. In this paper, we provide the first theoretical\ncharacterizations of both optimization and generalization of deep WeightNorm\nmodels with smooth activation functions. For optimization, from the form of the\nHessian of the loss, we note that a small Hessian of the predictor leads to a\ntractable analysis. Thus, we bound the spectral norm of the Hessian of\nWeightNorm networks and show its dependence on the network width and weight\nnormalization terms--the latter being unique to networks without WeightNorm.\nThen, we use this bound to establish training convergence guarantees under\nsuitable assumptions for gradient decent. For generalization, we use WeightNorm\nto get a uniform convergence based generalization bound, which is independent\nfrom the width and depends sublinearly on the depth. Finally, we present\nexperimental results which illustrate how the normalization terms and other\nquantities of theoretical interest relate to the training of WeightNorm\nnetworks.\n", "link": "http://arxiv.org/abs/2409.08935v1", "date": "2024-09-13", "relevancy": 1.9384, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5097}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4772}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization%20and%20Generalization%20Guarantees%20for%20Weight%20Normalization&body=Title%3A%20Optimization%20and%20Generalization%20Guarantees%20for%20Weight%20Normalization%0AAuthor%3A%20Pedro%20Cisneros-Velarde%20and%20Zhijie%20Chen%20and%20Sanmi%20Koyejo%20and%20Arindam%20Banerjee%0AAbstract%3A%20%20%20Weight%20normalization%20%28WeightNorm%29%20is%20widely%20used%20in%20practice%20for%20the%20training%0Aof%20deep%20neural%20networks%20and%20modern%20deep%20learning%20libraries%20have%20built-in%0Aimplementations%20of%20it.%20In%20this%20paper%2C%20we%20provide%20the%20first%20theoretical%0Acharacterizations%20of%20both%20optimization%20and%20generalization%20of%20deep%20WeightNorm%0Amodels%20with%20smooth%20activation%20functions.%20For%20optimization%2C%20from%20the%20form%20of%20the%0AHessian%20of%20the%20loss%2C%20we%20note%20that%20a%20small%20Hessian%20of%20the%20predictor%20leads%20to%20a%0Atractable%20analysis.%20Thus%2C%20we%20bound%20the%20spectral%20norm%20of%20the%20Hessian%20of%0AWeightNorm%20networks%20and%20show%20its%20dependence%20on%20the%20network%20width%20and%20weight%0Anormalization%20terms--the%20latter%20being%20unique%20to%20networks%20without%20WeightNorm.%0AThen%2C%20we%20use%20this%20bound%20to%20establish%20training%20convergence%20guarantees%20under%0Asuitable%20assumptions%20for%20gradient%20decent.%20For%20generalization%2C%20we%20use%20WeightNorm%0Ato%20get%20a%20uniform%20convergence%20based%20generalization%20bound%2C%20which%20is%20independent%0Afrom%20the%20width%20and%20depends%20sublinearly%20on%20the%20depth.%20Finally%2C%20we%20present%0Aexperimental%20results%20which%20illustrate%20how%20the%20normalization%20terms%20and%20other%0Aquantities%20of%20theoretical%20interest%20relate%20to%20the%20training%20of%20WeightNorm%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization%2520and%2520Generalization%2520Guarantees%2520for%2520Weight%2520Normalization%26entry.906535625%3DPedro%2520Cisneros-Velarde%2520and%2520Zhijie%2520Chen%2520and%2520Sanmi%2520Koyejo%2520and%2520Arindam%2520Banerjee%26entry.1292438233%3D%2520%2520Weight%2520normalization%2520%2528WeightNorm%2529%2520is%2520widely%2520used%2520in%2520practice%2520for%2520the%2520training%250Aof%2520deep%2520neural%2520networks%2520and%2520modern%2520deep%2520learning%2520libraries%2520have%2520built-in%250Aimplementations%2520of%2520it.%2520In%2520this%2520paper%252C%2520we%2520provide%2520the%2520first%2520theoretical%250Acharacterizations%2520of%2520both%2520optimization%2520and%2520generalization%2520of%2520deep%2520WeightNorm%250Amodels%2520with%2520smooth%2520activation%2520functions.%2520For%2520optimization%252C%2520from%2520the%2520form%2520of%2520the%250AHessian%2520of%2520the%2520loss%252C%2520we%2520note%2520that%2520a%2520small%2520Hessian%2520of%2520the%2520predictor%2520leads%2520to%2520a%250Atractable%2520analysis.%2520Thus%252C%2520we%2520bound%2520the%2520spectral%2520norm%2520of%2520the%2520Hessian%2520of%250AWeightNorm%2520networks%2520and%2520show%2520its%2520dependence%2520on%2520the%2520network%2520width%2520and%2520weight%250Anormalization%2520terms--the%2520latter%2520being%2520unique%2520to%2520networks%2520without%2520WeightNorm.%250AThen%252C%2520we%2520use%2520this%2520bound%2520to%2520establish%2520training%2520convergence%2520guarantees%2520under%250Asuitable%2520assumptions%2520for%2520gradient%2520decent.%2520For%2520generalization%252C%2520we%2520use%2520WeightNorm%250Ato%2520get%2520a%2520uniform%2520convergence%2520based%2520generalization%2520bound%252C%2520which%2520is%2520independent%250Afrom%2520the%2520width%2520and%2520depends%2520sublinearly%2520on%2520the%2520depth.%2520Finally%252C%2520we%2520present%250Aexperimental%2520results%2520which%2520illustrate%2520how%2520the%2520normalization%2520terms%2520and%2520other%250Aquantities%2520of%2520theoretical%2520interest%2520relate%2520to%2520the%2520training%2520of%2520WeightNorm%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization%20and%20Generalization%20Guarantees%20for%20Weight%20Normalization&entry.906535625=Pedro%20Cisneros-Velarde%20and%20Zhijie%20Chen%20and%20Sanmi%20Koyejo%20and%20Arindam%20Banerjee&entry.1292438233=%20%20Weight%20normalization%20%28WeightNorm%29%20is%20widely%20used%20in%20practice%20for%20the%20training%0Aof%20deep%20neural%20networks%20and%20modern%20deep%20learning%20libraries%20have%20built-in%0Aimplementations%20of%20it.%20In%20this%20paper%2C%20we%20provide%20the%20first%20theoretical%0Acharacterizations%20of%20both%20optimization%20and%20generalization%20of%20deep%20WeightNorm%0Amodels%20with%20smooth%20activation%20functions.%20For%20optimization%2C%20from%20the%20form%20of%20the%0AHessian%20of%20the%20loss%2C%20we%20note%20that%20a%20small%20Hessian%20of%20the%20predictor%20leads%20to%20a%0Atractable%20analysis.%20Thus%2C%20we%20bound%20the%20spectral%20norm%20of%20the%20Hessian%20of%0AWeightNorm%20networks%20and%20show%20its%20dependence%20on%20the%20network%20width%20and%20weight%0Anormalization%20terms--the%20latter%20being%20unique%20to%20networks%20without%20WeightNorm.%0AThen%2C%20we%20use%20this%20bound%20to%20establish%20training%20convergence%20guarantees%20under%0Asuitable%20assumptions%20for%20gradient%20decent.%20For%20generalization%2C%20we%20use%20WeightNorm%0Ato%20get%20a%20uniform%20convergence%20based%20generalization%20bound%2C%20which%20is%20independent%0Afrom%20the%20width%20and%20depends%20sublinearly%20on%20the%20depth.%20Finally%2C%20we%20present%0Aexperimental%20results%20which%20illustrate%20how%20the%20normalization%20terms%20and%20other%0Aquantities%20of%20theoretical%20interest%20relate%20to%20the%20training%20of%20WeightNorm%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08935v1&entry.124074799=Read"},
{"title": "Bridging Dynamic Factor Models and Neural Controlled Differential\n  Equations for Nowcasting GDP", "author": "Seonkyu Lim and Jeongwhan Choi and Noseong Park and Sang-Ha Yoon and ShinHyuck Kang and Young-Min Kim and Hyunjoong Kang", "abstract": "  Gross domestic product (GDP) nowcasting is crucial for policy-making as GDP\ngrowth is a key indicator of economic conditions. Dynamic factor models (DFMs)\nhave been widely adopted by government agencies for GDP nowcasting due to their\nability to handle irregular or missing macroeconomic indicators and their\ninterpretability. However, DFMs face two main challenges: i) the lack of\ncapturing economic uncertainties such as sudden recessions or booms, and ii)\nthe limitation of capturing irregular dynamics from mixed-frequency data. To\naddress these challenges, we introduce NCDENow, a novel GDP nowcasting\nframework that integrates neural controlled differential equations (NCDEs) with\nDFMs. This integration effectively handles the dynamics of irregular time\nseries. NCDENow consists of 3 main modules: i) factor extraction leveraging\nDFM, ii) dynamic modeling using NCDE, and iii) GDP growth prediction through\nregression. We evaluate NCDENow against 6 baselines on 2 real-world GDP\ndatasets from South Korea and the United Kingdom, demonstrating its enhanced\npredictive capability. Our empirical results favor our method, highlighting the\nsignificant potential of integrating NCDE into nowcasting models. Our code and\ndataset are available at https://github.com/sklim84/NCDENow_CIKM2024.\n", "link": "http://arxiv.org/abs/2409.08732v1", "date": "2024-09-13", "relevancy": 1.9305, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5068}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4676}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Dynamic%20Factor%20Models%20and%20Neural%20Controlled%20Differential%0A%20%20Equations%20for%20Nowcasting%20GDP&body=Title%3A%20Bridging%20Dynamic%20Factor%20Models%20and%20Neural%20Controlled%20Differential%0A%20%20Equations%20for%20Nowcasting%20GDP%0AAuthor%3A%20Seonkyu%20Lim%20and%20Jeongwhan%20Choi%20and%20Noseong%20Park%20and%20Sang-Ha%20Yoon%20and%20ShinHyuck%20Kang%20and%20Young-Min%20Kim%20and%20Hyunjoong%20Kang%0AAbstract%3A%20%20%20Gross%20domestic%20product%20%28GDP%29%20nowcasting%20is%20crucial%20for%20policy-making%20as%20GDP%0Agrowth%20is%20a%20key%20indicator%20of%20economic%20conditions.%20Dynamic%20factor%20models%20%28DFMs%29%0Ahave%20been%20widely%20adopted%20by%20government%20agencies%20for%20GDP%20nowcasting%20due%20to%20their%0Aability%20to%20handle%20irregular%20or%20missing%20macroeconomic%20indicators%20and%20their%0Ainterpretability.%20However%2C%20DFMs%20face%20two%20main%20challenges%3A%20i%29%20the%20lack%20of%0Acapturing%20economic%20uncertainties%20such%20as%20sudden%20recessions%20or%20booms%2C%20and%20ii%29%0Athe%20limitation%20of%20capturing%20irregular%20dynamics%20from%20mixed-frequency%20data.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20NCDENow%2C%20a%20novel%20GDP%20nowcasting%0Aframework%20that%20integrates%20neural%20controlled%20differential%20equations%20%28NCDEs%29%20with%0ADFMs.%20This%20integration%20effectively%20handles%20the%20dynamics%20of%20irregular%20time%0Aseries.%20NCDENow%20consists%20of%203%20main%20modules%3A%20i%29%20factor%20extraction%20leveraging%0ADFM%2C%20ii%29%20dynamic%20modeling%20using%20NCDE%2C%20and%20iii%29%20GDP%20growth%20prediction%20through%0Aregression.%20We%20evaluate%20NCDENow%20against%206%20baselines%20on%202%20real-world%20GDP%0Adatasets%20from%20South%20Korea%20and%20the%20United%20Kingdom%2C%20demonstrating%20its%20enhanced%0Apredictive%20capability.%20Our%20empirical%20results%20favor%20our%20method%2C%20highlighting%20the%0Asignificant%20potential%20of%20integrating%20NCDE%20into%20nowcasting%20models.%20Our%20code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/sklim84/NCDENow_CIKM2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Dynamic%2520Factor%2520Models%2520and%2520Neural%2520Controlled%2520Differential%250A%2520%2520Equations%2520for%2520Nowcasting%2520GDP%26entry.906535625%3DSeonkyu%2520Lim%2520and%2520Jeongwhan%2520Choi%2520and%2520Noseong%2520Park%2520and%2520Sang-Ha%2520Yoon%2520and%2520ShinHyuck%2520Kang%2520and%2520Young-Min%2520Kim%2520and%2520Hyunjoong%2520Kang%26entry.1292438233%3D%2520%2520Gross%2520domestic%2520product%2520%2528GDP%2529%2520nowcasting%2520is%2520crucial%2520for%2520policy-making%2520as%2520GDP%250Agrowth%2520is%2520a%2520key%2520indicator%2520of%2520economic%2520conditions.%2520Dynamic%2520factor%2520models%2520%2528DFMs%2529%250Ahave%2520been%2520widely%2520adopted%2520by%2520government%2520agencies%2520for%2520GDP%2520nowcasting%2520due%2520to%2520their%250Aability%2520to%2520handle%2520irregular%2520or%2520missing%2520macroeconomic%2520indicators%2520and%2520their%250Ainterpretability.%2520However%252C%2520DFMs%2520face%2520two%2520main%2520challenges%253A%2520i%2529%2520the%2520lack%2520of%250Acapturing%2520economic%2520uncertainties%2520such%2520as%2520sudden%2520recessions%2520or%2520booms%252C%2520and%2520ii%2529%250Athe%2520limitation%2520of%2520capturing%2520irregular%2520dynamics%2520from%2520mixed-frequency%2520data.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520NCDENow%252C%2520a%2520novel%2520GDP%2520nowcasting%250Aframework%2520that%2520integrates%2520neural%2520controlled%2520differential%2520equations%2520%2528NCDEs%2529%2520with%250ADFMs.%2520This%2520integration%2520effectively%2520handles%2520the%2520dynamics%2520of%2520irregular%2520time%250Aseries.%2520NCDENow%2520consists%2520of%25203%2520main%2520modules%253A%2520i%2529%2520factor%2520extraction%2520leveraging%250ADFM%252C%2520ii%2529%2520dynamic%2520modeling%2520using%2520NCDE%252C%2520and%2520iii%2529%2520GDP%2520growth%2520prediction%2520through%250Aregression.%2520We%2520evaluate%2520NCDENow%2520against%25206%2520baselines%2520on%25202%2520real-world%2520GDP%250Adatasets%2520from%2520South%2520Korea%2520and%2520the%2520United%2520Kingdom%252C%2520demonstrating%2520its%2520enhanced%250Apredictive%2520capability.%2520Our%2520empirical%2520results%2520favor%2520our%2520method%252C%2520highlighting%2520the%250Asignificant%2520potential%2520of%2520integrating%2520NCDE%2520into%2520nowcasting%2520models.%2520Our%2520code%2520and%250Adataset%2520are%2520available%2520at%2520https%253A//github.com/sklim84/NCDENow_CIKM2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Dynamic%20Factor%20Models%20and%20Neural%20Controlled%20Differential%0A%20%20Equations%20for%20Nowcasting%20GDP&entry.906535625=Seonkyu%20Lim%20and%20Jeongwhan%20Choi%20and%20Noseong%20Park%20and%20Sang-Ha%20Yoon%20and%20ShinHyuck%20Kang%20and%20Young-Min%20Kim%20and%20Hyunjoong%20Kang&entry.1292438233=%20%20Gross%20domestic%20product%20%28GDP%29%20nowcasting%20is%20crucial%20for%20policy-making%20as%20GDP%0Agrowth%20is%20a%20key%20indicator%20of%20economic%20conditions.%20Dynamic%20factor%20models%20%28DFMs%29%0Ahave%20been%20widely%20adopted%20by%20government%20agencies%20for%20GDP%20nowcasting%20due%20to%20their%0Aability%20to%20handle%20irregular%20or%20missing%20macroeconomic%20indicators%20and%20their%0Ainterpretability.%20However%2C%20DFMs%20face%20two%20main%20challenges%3A%20i%29%20the%20lack%20of%0Acapturing%20economic%20uncertainties%20such%20as%20sudden%20recessions%20or%20booms%2C%20and%20ii%29%0Athe%20limitation%20of%20capturing%20irregular%20dynamics%20from%20mixed-frequency%20data.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20NCDENow%2C%20a%20novel%20GDP%20nowcasting%0Aframework%20that%20integrates%20neural%20controlled%20differential%20equations%20%28NCDEs%29%20with%0ADFMs.%20This%20integration%20effectively%20handles%20the%20dynamics%20of%20irregular%20time%0Aseries.%20NCDENow%20consists%20of%203%20main%20modules%3A%20i%29%20factor%20extraction%20leveraging%0ADFM%2C%20ii%29%20dynamic%20modeling%20using%20NCDE%2C%20and%20iii%29%20GDP%20growth%20prediction%20through%0Aregression.%20We%20evaluate%20NCDENow%20against%206%20baselines%20on%202%20real-world%20GDP%0Adatasets%20from%20South%20Korea%20and%20the%20United%20Kingdom%2C%20demonstrating%20its%20enhanced%0Apredictive%20capability.%20Our%20empirical%20results%20favor%20our%20method%2C%20highlighting%20the%0Asignificant%20potential%20of%20integrating%20NCDE%20into%20nowcasting%20models.%20Our%20code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/sklim84/NCDENow_CIKM2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08732v1&entry.124074799=Read"},
{"title": "Detect Fake with Fake: Leveraging Synthetic Data-driven Representation\n  for Synthetic Image Detection", "author": "Hina Otake and Yoshihiro Fukuhara and Yoshiki Kubotani and Shigeo Morishima", "abstract": "  Are general-purpose visual representations acquired solely from synthetic\ndata useful for detecting fake images? In this work, we show the effectiveness\nof synthetic data-driven representations for synthetic image detection. Upon\nanalysis, we find that vision transformers trained by the latest visual\nrepresentation learners with synthetic data can effectively distinguish fake\nfrom real images without seeing any real images during pre-training. Notably,\nusing SynCLR as the backbone in a state-of-the-art detection method\ndemonstrates a performance improvement of +10.32 mAP and +4.73% accuracy over\nthe widely used CLIP, when tested on previously unseen GAN models. Code is\navailable at https://github.com/cvpaperchallenge/detect-fake-with-fake.\n", "link": "http://arxiv.org/abs/2409.08884v1", "date": "2024-09-13", "relevancy": 1.9281, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5031}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4798}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detect%20Fake%20with%20Fake%3A%20Leveraging%20Synthetic%20Data-driven%20Representation%0A%20%20for%20Synthetic%20Image%20Detection&body=Title%3A%20Detect%20Fake%20with%20Fake%3A%20Leveraging%20Synthetic%20Data-driven%20Representation%0A%20%20for%20Synthetic%20Image%20Detection%0AAuthor%3A%20Hina%20Otake%20and%20Yoshihiro%20Fukuhara%20and%20Yoshiki%20Kubotani%20and%20Shigeo%20Morishima%0AAbstract%3A%20%20%20Are%20general-purpose%20visual%20representations%20acquired%20solely%20from%20synthetic%0Adata%20useful%20for%20detecting%20fake%20images%3F%20In%20this%20work%2C%20we%20show%20the%20effectiveness%0Aof%20synthetic%20data-driven%20representations%20for%20synthetic%20image%20detection.%20Upon%0Aanalysis%2C%20we%20find%20that%20vision%20transformers%20trained%20by%20the%20latest%20visual%0Arepresentation%20learners%20with%20synthetic%20data%20can%20effectively%20distinguish%20fake%0Afrom%20real%20images%20without%20seeing%20any%20real%20images%20during%20pre-training.%20Notably%2C%0Ausing%20SynCLR%20as%20the%20backbone%20in%20a%20state-of-the-art%20detection%20method%0Ademonstrates%20a%20performance%20improvement%20of%20%2B10.32%20mAP%20and%20%2B4.73%25%20accuracy%20over%0Athe%20widely%20used%20CLIP%2C%20when%20tested%20on%20previously%20unseen%20GAN%20models.%20Code%20is%0Aavailable%20at%20https%3A//github.com/cvpaperchallenge/detect-fake-with-fake.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetect%2520Fake%2520with%2520Fake%253A%2520Leveraging%2520Synthetic%2520Data-driven%2520Representation%250A%2520%2520for%2520Synthetic%2520Image%2520Detection%26entry.906535625%3DHina%2520Otake%2520and%2520Yoshihiro%2520Fukuhara%2520and%2520Yoshiki%2520Kubotani%2520and%2520Shigeo%2520Morishima%26entry.1292438233%3D%2520%2520Are%2520general-purpose%2520visual%2520representations%2520acquired%2520solely%2520from%2520synthetic%250Adata%2520useful%2520for%2520detecting%2520fake%2520images%253F%2520In%2520this%2520work%252C%2520we%2520show%2520the%2520effectiveness%250Aof%2520synthetic%2520data-driven%2520representations%2520for%2520synthetic%2520image%2520detection.%2520Upon%250Aanalysis%252C%2520we%2520find%2520that%2520vision%2520transformers%2520trained%2520by%2520the%2520latest%2520visual%250Arepresentation%2520learners%2520with%2520synthetic%2520data%2520can%2520effectively%2520distinguish%2520fake%250Afrom%2520real%2520images%2520without%2520seeing%2520any%2520real%2520images%2520during%2520pre-training.%2520Notably%252C%250Ausing%2520SynCLR%2520as%2520the%2520backbone%2520in%2520a%2520state-of-the-art%2520detection%2520method%250Ademonstrates%2520a%2520performance%2520improvement%2520of%2520%252B10.32%2520mAP%2520and%2520%252B4.73%2525%2520accuracy%2520over%250Athe%2520widely%2520used%2520CLIP%252C%2520when%2520tested%2520on%2520previously%2520unseen%2520GAN%2520models.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/cvpaperchallenge/detect-fake-with-fake.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detect%20Fake%20with%20Fake%3A%20Leveraging%20Synthetic%20Data-driven%20Representation%0A%20%20for%20Synthetic%20Image%20Detection&entry.906535625=Hina%20Otake%20and%20Yoshihiro%20Fukuhara%20and%20Yoshiki%20Kubotani%20and%20Shigeo%20Morishima&entry.1292438233=%20%20Are%20general-purpose%20visual%20representations%20acquired%20solely%20from%20synthetic%0Adata%20useful%20for%20detecting%20fake%20images%3F%20In%20this%20work%2C%20we%20show%20the%20effectiveness%0Aof%20synthetic%20data-driven%20representations%20for%20synthetic%20image%20detection.%20Upon%0Aanalysis%2C%20we%20find%20that%20vision%20transformers%20trained%20by%20the%20latest%20visual%0Arepresentation%20learners%20with%20synthetic%20data%20can%20effectively%20distinguish%20fake%0Afrom%20real%20images%20without%20seeing%20any%20real%20images%20during%20pre-training.%20Notably%2C%0Ausing%20SynCLR%20as%20the%20backbone%20in%20a%20state-of-the-art%20detection%20method%0Ademonstrates%20a%20performance%20improvement%20of%20%2B10.32%20mAP%20and%20%2B4.73%25%20accuracy%20over%0Athe%20widely%20used%20CLIP%2C%20when%20tested%20on%20previously%20unseen%20GAN%20models.%20Code%20is%0Aavailable%20at%20https%3A//github.com/cvpaperchallenge/detect-fake-with-fake.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08884v1&entry.124074799=Read"},
{"title": "Kinematics and Dynamics Modeling of 7 Degrees of Freedom Human Lower\n  Limb Using Dual Quaternions Algebra", "author": "Zineb Benhmidouch and Saad Moufid and Aissam Ait Omar", "abstract": "  Denavit and Hartenberg-based methods, such as Cardan, Fick, and Euler angles,\ndescribe the position and orientation of an end-effector in three-dimensional\n(3D) space. However, these methods have a significant drawback as they impose a\nwell-defined rotation order, which can lead to the generation of unrealistic\nhuman postures in joint space. To address this issue, dual quaternions can be\nused for homogeneous transformations. Quaternions are known for their\ncomputational efficiency in representing rotations, but they cannot handle\ntranslations in 3D space. Dual numbers extend quaternions to dual quaternions,\nwhich can manage both rotations and translations. This paper exploits dual\nquaternion theory to provide a fast and accurate solution for the forward and\ninverse kinematics and the recursive Newton-Euler dynamics algorithm for a\n7-degree-of-freedom (DOF) human lower limb in 3D space.\n", "link": "http://arxiv.org/abs/2302.11605v3", "date": "2024-09-13", "relevancy": 1.9233, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5118}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4922}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinematics%20and%20Dynamics%20Modeling%20of%207%20Degrees%20of%20Freedom%20Human%20Lower%0A%20%20Limb%20Using%20Dual%20Quaternions%20Algebra&body=Title%3A%20Kinematics%20and%20Dynamics%20Modeling%20of%207%20Degrees%20of%20Freedom%20Human%20Lower%0A%20%20Limb%20Using%20Dual%20Quaternions%20Algebra%0AAuthor%3A%20Zineb%20Benhmidouch%20and%20Saad%20Moufid%20and%20Aissam%20Ait%20Omar%0AAbstract%3A%20%20%20Denavit%20and%20Hartenberg-based%20methods%2C%20such%20as%20Cardan%2C%20Fick%2C%20and%20Euler%20angles%2C%0Adescribe%20the%20position%20and%20orientation%20of%20an%20end-effector%20in%20three-dimensional%0A%283D%29%20space.%20However%2C%20these%20methods%20have%20a%20significant%20drawback%20as%20they%20impose%20a%0Awell-defined%20rotation%20order%2C%20which%20can%20lead%20to%20the%20generation%20of%20unrealistic%0Ahuman%20postures%20in%20joint%20space.%20To%20address%20this%20issue%2C%20dual%20quaternions%20can%20be%0Aused%20for%20homogeneous%20transformations.%20Quaternions%20are%20known%20for%20their%0Acomputational%20efficiency%20in%20representing%20rotations%2C%20but%20they%20cannot%20handle%0Atranslations%20in%203D%20space.%20Dual%20numbers%20extend%20quaternions%20to%20dual%20quaternions%2C%0Awhich%20can%20manage%20both%20rotations%20and%20translations.%20This%20paper%20exploits%20dual%0Aquaternion%20theory%20to%20provide%20a%20fast%20and%20accurate%20solution%20for%20the%20forward%20and%0Ainverse%20kinematics%20and%20the%20recursive%20Newton-Euler%20dynamics%20algorithm%20for%20a%0A7-degree-of-freedom%20%28DOF%29%20human%20lower%20limb%20in%203D%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.11605v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinematics%2520and%2520Dynamics%2520Modeling%2520of%25207%2520Degrees%2520of%2520Freedom%2520Human%2520Lower%250A%2520%2520Limb%2520Using%2520Dual%2520Quaternions%2520Algebra%26entry.906535625%3DZineb%2520Benhmidouch%2520and%2520Saad%2520Moufid%2520and%2520Aissam%2520Ait%2520Omar%26entry.1292438233%3D%2520%2520Denavit%2520and%2520Hartenberg-based%2520methods%252C%2520such%2520as%2520Cardan%252C%2520Fick%252C%2520and%2520Euler%2520angles%252C%250Adescribe%2520the%2520position%2520and%2520orientation%2520of%2520an%2520end-effector%2520in%2520three-dimensional%250A%25283D%2529%2520space.%2520However%252C%2520these%2520methods%2520have%2520a%2520significant%2520drawback%2520as%2520they%2520impose%2520a%250Awell-defined%2520rotation%2520order%252C%2520which%2520can%2520lead%2520to%2520the%2520generation%2520of%2520unrealistic%250Ahuman%2520postures%2520in%2520joint%2520space.%2520To%2520address%2520this%2520issue%252C%2520dual%2520quaternions%2520can%2520be%250Aused%2520for%2520homogeneous%2520transformations.%2520Quaternions%2520are%2520known%2520for%2520their%250Acomputational%2520efficiency%2520in%2520representing%2520rotations%252C%2520but%2520they%2520cannot%2520handle%250Atranslations%2520in%25203D%2520space.%2520Dual%2520numbers%2520extend%2520quaternions%2520to%2520dual%2520quaternions%252C%250Awhich%2520can%2520manage%2520both%2520rotations%2520and%2520translations.%2520This%2520paper%2520exploits%2520dual%250Aquaternion%2520theory%2520to%2520provide%2520a%2520fast%2520and%2520accurate%2520solution%2520for%2520the%2520forward%2520and%250Ainverse%2520kinematics%2520and%2520the%2520recursive%2520Newton-Euler%2520dynamics%2520algorithm%2520for%2520a%250A7-degree-of-freedom%2520%2528DOF%2529%2520human%2520lower%2520limb%2520in%25203D%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.11605v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinematics%20and%20Dynamics%20Modeling%20of%207%20Degrees%20of%20Freedom%20Human%20Lower%0A%20%20Limb%20Using%20Dual%20Quaternions%20Algebra&entry.906535625=Zineb%20Benhmidouch%20and%20Saad%20Moufid%20and%20Aissam%20Ait%20Omar&entry.1292438233=%20%20Denavit%20and%20Hartenberg-based%20methods%2C%20such%20as%20Cardan%2C%20Fick%2C%20and%20Euler%20angles%2C%0Adescribe%20the%20position%20and%20orientation%20of%20an%20end-effector%20in%20three-dimensional%0A%283D%29%20space.%20However%2C%20these%20methods%20have%20a%20significant%20drawback%20as%20they%20impose%20a%0Awell-defined%20rotation%20order%2C%20which%20can%20lead%20to%20the%20generation%20of%20unrealistic%0Ahuman%20postures%20in%20joint%20space.%20To%20address%20this%20issue%2C%20dual%20quaternions%20can%20be%0Aused%20for%20homogeneous%20transformations.%20Quaternions%20are%20known%20for%20their%0Acomputational%20efficiency%20in%20representing%20rotations%2C%20but%20they%20cannot%20handle%0Atranslations%20in%203D%20space.%20Dual%20numbers%20extend%20quaternions%20to%20dual%20quaternions%2C%0Awhich%20can%20manage%20both%20rotations%20and%20translations.%20This%20paper%20exploits%20dual%0Aquaternion%20theory%20to%20provide%20a%20fast%20and%20accurate%20solution%20for%20the%20forward%20and%0Ainverse%20kinematics%20and%20the%20recursive%20Newton-Euler%20dynamics%20algorithm%20for%20a%0A7-degree-of-freedom%20%28DOF%29%20human%20lower%20limb%20in%203D%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.11605v3&entry.124074799=Read"},
{"title": "Electrocardiogram Report Generation and Question Answering via\n  Retrieval-Augmented Self-Supervised Modeling", "author": "Jialu Tang and Tong Xia and Yuan Lu and Cecilia Mascolo and Aaqib Saeed", "abstract": "  Interpreting electrocardiograms (ECGs) and generating comprehensive reports\nremain challenging tasks in cardiology, often requiring specialized expertise\nand significant time investment. To address these critical issues, we propose\nECG-ReGen, a retrieval-based approach for ECG-to-text report generation and\nquestion answering. Our method leverages a self-supervised learning for the ECG\nencoder, enabling efficient similarity searches and report retrieval. By\ncombining pre-training with dynamic retrieval and Large Language Model\n(LLM)-based refinement, ECG-ReGen effectively analyzes ECG data and answers\nrelated queries, with the potential of improving patient care. Experiments\nconducted on the PTB-XL and MIMIC-IV-ECG datasets demonstrate superior\nperformance in both in-domain and cross-domain scenarios for report generation.\nFurthermore, our approach exhibits competitive performance on ECG-QA dataset\ncompared to fully supervised methods when utilizing off-the-shelf LLMs for\nzero-shot question answering. This approach, effectively combining\nself-supervised encoder and LLMs, offers a scalable and efficient solution for\naccurate ECG interpretation, holding significant potential to enhance clinical\ndecision-making.\n", "link": "http://arxiv.org/abs/2409.08788v1", "date": "2024-09-13", "relevancy": 1.919, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4935}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.473}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Electrocardiogram%20Report%20Generation%20and%20Question%20Answering%20via%0A%20%20Retrieval-Augmented%20Self-Supervised%20Modeling&body=Title%3A%20Electrocardiogram%20Report%20Generation%20and%20Question%20Answering%20via%0A%20%20Retrieval-Augmented%20Self-Supervised%20Modeling%0AAuthor%3A%20Jialu%20Tang%20and%20Tong%20Xia%20and%20Yuan%20Lu%20and%20Cecilia%20Mascolo%20and%20Aaqib%20Saeed%0AAbstract%3A%20%20%20Interpreting%20electrocardiograms%20%28ECGs%29%20and%20generating%20comprehensive%20reports%0Aremain%20challenging%20tasks%20in%20cardiology%2C%20often%20requiring%20specialized%20expertise%0Aand%20significant%20time%20investment.%20To%20address%20these%20critical%20issues%2C%20we%20propose%0AECG-ReGen%2C%20a%20retrieval-based%20approach%20for%20ECG-to-text%20report%20generation%20and%0Aquestion%20answering.%20Our%20method%20leverages%20a%20self-supervised%20learning%20for%20the%20ECG%0Aencoder%2C%20enabling%20efficient%20similarity%20searches%20and%20report%20retrieval.%20By%0Acombining%20pre-training%20with%20dynamic%20retrieval%20and%20Large%20Language%20Model%0A%28LLM%29-based%20refinement%2C%20ECG-ReGen%20effectively%20analyzes%20ECG%20data%20and%20answers%0Arelated%20queries%2C%20with%20the%20potential%20of%20improving%20patient%20care.%20Experiments%0Aconducted%20on%20the%20PTB-XL%20and%20MIMIC-IV-ECG%20datasets%20demonstrate%20superior%0Aperformance%20in%20both%20in-domain%20and%20cross-domain%20scenarios%20for%20report%20generation.%0AFurthermore%2C%20our%20approach%20exhibits%20competitive%20performance%20on%20ECG-QA%20dataset%0Acompared%20to%20fully%20supervised%20methods%20when%20utilizing%20off-the-shelf%20LLMs%20for%0Azero-shot%20question%20answering.%20This%20approach%2C%20effectively%20combining%0Aself-supervised%20encoder%20and%20LLMs%2C%20offers%20a%20scalable%20and%20efficient%20solution%20for%0Aaccurate%20ECG%20interpretation%2C%20holding%20significant%20potential%20to%20enhance%20clinical%0Adecision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElectrocardiogram%2520Report%2520Generation%2520and%2520Question%2520Answering%2520via%250A%2520%2520Retrieval-Augmented%2520Self-Supervised%2520Modeling%26entry.906535625%3DJialu%2520Tang%2520and%2520Tong%2520Xia%2520and%2520Yuan%2520Lu%2520and%2520Cecilia%2520Mascolo%2520and%2520Aaqib%2520Saeed%26entry.1292438233%3D%2520%2520Interpreting%2520electrocardiograms%2520%2528ECGs%2529%2520and%2520generating%2520comprehensive%2520reports%250Aremain%2520challenging%2520tasks%2520in%2520cardiology%252C%2520often%2520requiring%2520specialized%2520expertise%250Aand%2520significant%2520time%2520investment.%2520To%2520address%2520these%2520critical%2520issues%252C%2520we%2520propose%250AECG-ReGen%252C%2520a%2520retrieval-based%2520approach%2520for%2520ECG-to-text%2520report%2520generation%2520and%250Aquestion%2520answering.%2520Our%2520method%2520leverages%2520a%2520self-supervised%2520learning%2520for%2520the%2520ECG%250Aencoder%252C%2520enabling%2520efficient%2520similarity%2520searches%2520and%2520report%2520retrieval.%2520By%250Acombining%2520pre-training%2520with%2520dynamic%2520retrieval%2520and%2520Large%2520Language%2520Model%250A%2528LLM%2529-based%2520refinement%252C%2520ECG-ReGen%2520effectively%2520analyzes%2520ECG%2520data%2520and%2520answers%250Arelated%2520queries%252C%2520with%2520the%2520potential%2520of%2520improving%2520patient%2520care.%2520Experiments%250Aconducted%2520on%2520the%2520PTB-XL%2520and%2520MIMIC-IV-ECG%2520datasets%2520demonstrate%2520superior%250Aperformance%2520in%2520both%2520in-domain%2520and%2520cross-domain%2520scenarios%2520for%2520report%2520generation.%250AFurthermore%252C%2520our%2520approach%2520exhibits%2520competitive%2520performance%2520on%2520ECG-QA%2520dataset%250Acompared%2520to%2520fully%2520supervised%2520methods%2520when%2520utilizing%2520off-the-shelf%2520LLMs%2520for%250Azero-shot%2520question%2520answering.%2520This%2520approach%252C%2520effectively%2520combining%250Aself-supervised%2520encoder%2520and%2520LLMs%252C%2520offers%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%250Aaccurate%2520ECG%2520interpretation%252C%2520holding%2520significant%2520potential%2520to%2520enhance%2520clinical%250Adecision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Electrocardiogram%20Report%20Generation%20and%20Question%20Answering%20via%0A%20%20Retrieval-Augmented%20Self-Supervised%20Modeling&entry.906535625=Jialu%20Tang%20and%20Tong%20Xia%20and%20Yuan%20Lu%20and%20Cecilia%20Mascolo%20and%20Aaqib%20Saeed&entry.1292438233=%20%20Interpreting%20electrocardiograms%20%28ECGs%29%20and%20generating%20comprehensive%20reports%0Aremain%20challenging%20tasks%20in%20cardiology%2C%20often%20requiring%20specialized%20expertise%0Aand%20significant%20time%20investment.%20To%20address%20these%20critical%20issues%2C%20we%20propose%0AECG-ReGen%2C%20a%20retrieval-based%20approach%20for%20ECG-to-text%20report%20generation%20and%0Aquestion%20answering.%20Our%20method%20leverages%20a%20self-supervised%20learning%20for%20the%20ECG%0Aencoder%2C%20enabling%20efficient%20similarity%20searches%20and%20report%20retrieval.%20By%0Acombining%20pre-training%20with%20dynamic%20retrieval%20and%20Large%20Language%20Model%0A%28LLM%29-based%20refinement%2C%20ECG-ReGen%20effectively%20analyzes%20ECG%20data%20and%20answers%0Arelated%20queries%2C%20with%20the%20potential%20of%20improving%20patient%20care.%20Experiments%0Aconducted%20on%20the%20PTB-XL%20and%20MIMIC-IV-ECG%20datasets%20demonstrate%20superior%0Aperformance%20in%20both%20in-domain%20and%20cross-domain%20scenarios%20for%20report%20generation.%0AFurthermore%2C%20our%20approach%20exhibits%20competitive%20performance%20on%20ECG-QA%20dataset%0Acompared%20to%20fully%20supervised%20methods%20when%20utilizing%20off-the-shelf%20LLMs%20for%0Azero-shot%20question%20answering.%20This%20approach%2C%20effectively%20combining%0Aself-supervised%20encoder%20and%20LLMs%2C%20offers%20a%20scalable%20and%20efficient%20solution%20for%0Aaccurate%20ECG%20interpretation%2C%20holding%20significant%20potential%20to%20enhance%20clinical%0Adecision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08788v1&entry.124074799=Read"},
{"title": "Deep reinforcement learning for tracking a moving target in\n  jellyfish-like swimming", "author": "Yihao Chen and Yue Yang", "abstract": "  We develop a deep reinforcement learning method for training a jellyfish-like\nswimmer to effectively track a moving target in a two-dimensional flow. This\nswimmer is a flexible object equipped with a muscle model based on torsional\nsprings. We employ a deep Q-network (DQN) that takes the swimmer's geometry and\ndynamic parameters as inputs, and outputs actions which are the forces applied\nto the swimmer. In particular, we introduce an action regulation to mitigate\nthe interference from complex fluid-structure interactions. The goal of these\nactions is to navigate the swimmer to a target point in the shortest possible\ntime. In the DQN training, the data on the swimmer's motions are obtained from\nsimulations conducted using the immersed boundary method. During tracking a\nmoving target, there is an inherent delay between the application of forces and\nthe corresponding response of the swimmer's body due to hydrodynamic\ninteractions between the shedding vortices and the swimmer's own locomotion.\nOur tests demonstrate that the swimmer, with the DQN agent and action\nregulation, is able to dynamically adjust its course based on its instantaneous\nstate. This work extends the application scope of machine learning in\ncontrolling flexible objects within fluid environments.\n", "link": "http://arxiv.org/abs/2409.08815v1", "date": "2024-09-13", "relevancy": 1.9082, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4876}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4781}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20reinforcement%20learning%20for%20tracking%20a%20moving%20target%20in%0A%20%20jellyfish-like%20swimming&body=Title%3A%20Deep%20reinforcement%20learning%20for%20tracking%20a%20moving%20target%20in%0A%20%20jellyfish-like%20swimming%0AAuthor%3A%20Yihao%20Chen%20and%20Yue%20Yang%0AAbstract%3A%20%20%20We%20develop%20a%20deep%20reinforcement%20learning%20method%20for%20training%20a%20jellyfish-like%0Aswimmer%20to%20effectively%20track%20a%20moving%20target%20in%20a%20two-dimensional%20flow.%20This%0Aswimmer%20is%20a%20flexible%20object%20equipped%20with%20a%20muscle%20model%20based%20on%20torsional%0Asprings.%20We%20employ%20a%20deep%20Q-network%20%28DQN%29%20that%20takes%20the%20swimmer%27s%20geometry%20and%0Adynamic%20parameters%20as%20inputs%2C%20and%20outputs%20actions%20which%20are%20the%20forces%20applied%0Ato%20the%20swimmer.%20In%20particular%2C%20we%20introduce%20an%20action%20regulation%20to%20mitigate%0Athe%20interference%20from%20complex%20fluid-structure%20interactions.%20The%20goal%20of%20these%0Aactions%20is%20to%20navigate%20the%20swimmer%20to%20a%20target%20point%20in%20the%20shortest%20possible%0Atime.%20In%20the%20DQN%20training%2C%20the%20data%20on%20the%20swimmer%27s%20motions%20are%20obtained%20from%0Asimulations%20conducted%20using%20the%20immersed%20boundary%20method.%20During%20tracking%20a%0Amoving%20target%2C%20there%20is%20an%20inherent%20delay%20between%20the%20application%20of%20forces%20and%0Athe%20corresponding%20response%20of%20the%20swimmer%27s%20body%20due%20to%20hydrodynamic%0Ainteractions%20between%20the%20shedding%20vortices%20and%20the%20swimmer%27s%20own%20locomotion.%0AOur%20tests%20demonstrate%20that%20the%20swimmer%2C%20with%20the%20DQN%20agent%20and%20action%0Aregulation%2C%20is%20able%20to%20dynamically%20adjust%20its%20course%20based%20on%20its%20instantaneous%0Astate.%20This%20work%20extends%20the%20application%20scope%20of%20machine%20learning%20in%0Acontrolling%20flexible%20objects%20within%20fluid%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520reinforcement%2520learning%2520for%2520tracking%2520a%2520moving%2520target%2520in%250A%2520%2520jellyfish-like%2520swimming%26entry.906535625%3DYihao%2520Chen%2520and%2520Yue%2520Yang%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520deep%2520reinforcement%2520learning%2520method%2520for%2520training%2520a%2520jellyfish-like%250Aswimmer%2520to%2520effectively%2520track%2520a%2520moving%2520target%2520in%2520a%2520two-dimensional%2520flow.%2520This%250Aswimmer%2520is%2520a%2520flexible%2520object%2520equipped%2520with%2520a%2520muscle%2520model%2520based%2520on%2520torsional%250Asprings.%2520We%2520employ%2520a%2520deep%2520Q-network%2520%2528DQN%2529%2520that%2520takes%2520the%2520swimmer%2527s%2520geometry%2520and%250Adynamic%2520parameters%2520as%2520inputs%252C%2520and%2520outputs%2520actions%2520which%2520are%2520the%2520forces%2520applied%250Ato%2520the%2520swimmer.%2520In%2520particular%252C%2520we%2520introduce%2520an%2520action%2520regulation%2520to%2520mitigate%250Athe%2520interference%2520from%2520complex%2520fluid-structure%2520interactions.%2520The%2520goal%2520of%2520these%250Aactions%2520is%2520to%2520navigate%2520the%2520swimmer%2520to%2520a%2520target%2520point%2520in%2520the%2520shortest%2520possible%250Atime.%2520In%2520the%2520DQN%2520training%252C%2520the%2520data%2520on%2520the%2520swimmer%2527s%2520motions%2520are%2520obtained%2520from%250Asimulations%2520conducted%2520using%2520the%2520immersed%2520boundary%2520method.%2520During%2520tracking%2520a%250Amoving%2520target%252C%2520there%2520is%2520an%2520inherent%2520delay%2520between%2520the%2520application%2520of%2520forces%2520and%250Athe%2520corresponding%2520response%2520of%2520the%2520swimmer%2527s%2520body%2520due%2520to%2520hydrodynamic%250Ainteractions%2520between%2520the%2520shedding%2520vortices%2520and%2520the%2520swimmer%2527s%2520own%2520locomotion.%250AOur%2520tests%2520demonstrate%2520that%2520the%2520swimmer%252C%2520with%2520the%2520DQN%2520agent%2520and%2520action%250Aregulation%252C%2520is%2520able%2520to%2520dynamically%2520adjust%2520its%2520course%2520based%2520on%2520its%2520instantaneous%250Astate.%2520This%2520work%2520extends%2520the%2520application%2520scope%2520of%2520machine%2520learning%2520in%250Acontrolling%2520flexible%2520objects%2520within%2520fluid%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20reinforcement%20learning%20for%20tracking%20a%20moving%20target%20in%0A%20%20jellyfish-like%20swimming&entry.906535625=Yihao%20Chen%20and%20Yue%20Yang&entry.1292438233=%20%20We%20develop%20a%20deep%20reinforcement%20learning%20method%20for%20training%20a%20jellyfish-like%0Aswimmer%20to%20effectively%20track%20a%20moving%20target%20in%20a%20two-dimensional%20flow.%20This%0Aswimmer%20is%20a%20flexible%20object%20equipped%20with%20a%20muscle%20model%20based%20on%20torsional%0Asprings.%20We%20employ%20a%20deep%20Q-network%20%28DQN%29%20that%20takes%20the%20swimmer%27s%20geometry%20and%0Adynamic%20parameters%20as%20inputs%2C%20and%20outputs%20actions%20which%20are%20the%20forces%20applied%0Ato%20the%20swimmer.%20In%20particular%2C%20we%20introduce%20an%20action%20regulation%20to%20mitigate%0Athe%20interference%20from%20complex%20fluid-structure%20interactions.%20The%20goal%20of%20these%0Aactions%20is%20to%20navigate%20the%20swimmer%20to%20a%20target%20point%20in%20the%20shortest%20possible%0Atime.%20In%20the%20DQN%20training%2C%20the%20data%20on%20the%20swimmer%27s%20motions%20are%20obtained%20from%0Asimulations%20conducted%20using%20the%20immersed%20boundary%20method.%20During%20tracking%20a%0Amoving%20target%2C%20there%20is%20an%20inherent%20delay%20between%20the%20application%20of%20forces%20and%0Athe%20corresponding%20response%20of%20the%20swimmer%27s%20body%20due%20to%20hydrodynamic%0Ainteractions%20between%20the%20shedding%20vortices%20and%20the%20swimmer%27s%20own%20locomotion.%0AOur%20tests%20demonstrate%20that%20the%20swimmer%2C%20with%20the%20DQN%20agent%20and%20action%0Aregulation%2C%20is%20able%20to%20dynamically%20adjust%20its%20course%20based%20on%20its%20instantaneous%0Astate.%20This%20work%20extends%20the%20application%20scope%20of%20machine%20learning%20in%0Acontrolling%20flexible%20objects%20within%20fluid%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08815v1&entry.124074799=Read"},
{"title": "Can Kans (re)discover predictive models for Direct-Drive Laser Fusion?", "author": "Rahman Ejaz and Varchas Gopalaswamy and Riccardo Betti and Aarne Lees and Christopher Kanan", "abstract": "  The domain of laser fusion presents a unique and challenging predictive\nmodeling application landscape for machine learning methods due to high problem\ncomplexity and limited training data. Data-driven approaches utilizing\nprescribed functional forms, inductive biases and physics-informed learning\n(PIL) schemes have been successful in the past for achieving desired\ngeneralization ability and model interpretation that aligns with physics\nexpectations. In complex multi-physics application domains, however, it is not\nalways obvious how architectural biases or discriminative penalties can be\nformulated. In this work, focusing on nuclear fusion energy using high powered\nlasers, we present the use of Kolmogorov-Arnold Networks (KANs) as an\nalternative to PIL for developing a new type of data-driven predictive model\nwhich is able to achieve high prediction accuracy and physics interpretability.\nA KAN based model, a MLP with PIL, and a baseline MLP model are compared in\ngeneralization ability and interpretation with a domain expert-derived symbolic\nregression model. Through empirical studies in this high physics complexity\ndomain, we show that KANs can potentially provide benefits when developing\npredictive models for data-starved physics applications.\n", "link": "http://arxiv.org/abs/2409.08832v1", "date": "2024-09-13", "relevancy": 1.9043, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4717}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Kans%20%28re%29discover%20predictive%20models%20for%20Direct-Drive%20Laser%20Fusion%3F&body=Title%3A%20Can%20Kans%20%28re%29discover%20predictive%20models%20for%20Direct-Drive%20Laser%20Fusion%3F%0AAuthor%3A%20Rahman%20Ejaz%20and%20Varchas%20Gopalaswamy%20and%20Riccardo%20Betti%20and%20Aarne%20Lees%20and%20Christopher%20Kanan%0AAbstract%3A%20%20%20The%20domain%20of%20laser%20fusion%20presents%20a%20unique%20and%20challenging%20predictive%0Amodeling%20application%20landscape%20for%20machine%20learning%20methods%20due%20to%20high%20problem%0Acomplexity%20and%20limited%20training%20data.%20Data-driven%20approaches%20utilizing%0Aprescribed%20functional%20forms%2C%20inductive%20biases%20and%20physics-informed%20learning%0A%28PIL%29%20schemes%20have%20been%20successful%20in%20the%20past%20for%20achieving%20desired%0Ageneralization%20ability%20and%20model%20interpretation%20that%20aligns%20with%20physics%0Aexpectations.%20In%20complex%20multi-physics%20application%20domains%2C%20however%2C%20it%20is%20not%0Aalways%20obvious%20how%20architectural%20biases%20or%20discriminative%20penalties%20can%20be%0Aformulated.%20In%20this%20work%2C%20focusing%20on%20nuclear%20fusion%20energy%20using%20high%20powered%0Alasers%2C%20we%20present%20the%20use%20of%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20as%20an%0Aalternative%20to%20PIL%20for%20developing%20a%20new%20type%20of%20data-driven%20predictive%20model%0Awhich%20is%20able%20to%20achieve%20high%20prediction%20accuracy%20and%20physics%20interpretability.%0AA%20KAN%20based%20model%2C%20a%20MLP%20with%20PIL%2C%20and%20a%20baseline%20MLP%20model%20are%20compared%20in%0Ageneralization%20ability%20and%20interpretation%20with%20a%20domain%20expert-derived%20symbolic%0Aregression%20model.%20Through%20empirical%20studies%20in%20this%20high%20physics%20complexity%0Adomain%2C%20we%20show%20that%20KANs%20can%20potentially%20provide%20benefits%20when%20developing%0Apredictive%20models%20for%20data-starved%20physics%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Kans%2520%2528re%2529discover%2520predictive%2520models%2520for%2520Direct-Drive%2520Laser%2520Fusion%253F%26entry.906535625%3DRahman%2520Ejaz%2520and%2520Varchas%2520Gopalaswamy%2520and%2520Riccardo%2520Betti%2520and%2520Aarne%2520Lees%2520and%2520Christopher%2520Kanan%26entry.1292438233%3D%2520%2520The%2520domain%2520of%2520laser%2520fusion%2520presents%2520a%2520unique%2520and%2520challenging%2520predictive%250Amodeling%2520application%2520landscape%2520for%2520machine%2520learning%2520methods%2520due%2520to%2520high%2520problem%250Acomplexity%2520and%2520limited%2520training%2520data.%2520Data-driven%2520approaches%2520utilizing%250Aprescribed%2520functional%2520forms%252C%2520inductive%2520biases%2520and%2520physics-informed%2520learning%250A%2528PIL%2529%2520schemes%2520have%2520been%2520successful%2520in%2520the%2520past%2520for%2520achieving%2520desired%250Ageneralization%2520ability%2520and%2520model%2520interpretation%2520that%2520aligns%2520with%2520physics%250Aexpectations.%2520In%2520complex%2520multi-physics%2520application%2520domains%252C%2520however%252C%2520it%2520is%2520not%250Aalways%2520obvious%2520how%2520architectural%2520biases%2520or%2520discriminative%2520penalties%2520can%2520be%250Aformulated.%2520In%2520this%2520work%252C%2520focusing%2520on%2520nuclear%2520fusion%2520energy%2520using%2520high%2520powered%250Alasers%252C%2520we%2520present%2520the%2520use%2520of%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520as%2520an%250Aalternative%2520to%2520PIL%2520for%2520developing%2520a%2520new%2520type%2520of%2520data-driven%2520predictive%2520model%250Awhich%2520is%2520able%2520to%2520achieve%2520high%2520prediction%2520accuracy%2520and%2520physics%2520interpretability.%250AA%2520KAN%2520based%2520model%252C%2520a%2520MLP%2520with%2520PIL%252C%2520and%2520a%2520baseline%2520MLP%2520model%2520are%2520compared%2520in%250Ageneralization%2520ability%2520and%2520interpretation%2520with%2520a%2520domain%2520expert-derived%2520symbolic%250Aregression%2520model.%2520Through%2520empirical%2520studies%2520in%2520this%2520high%2520physics%2520complexity%250Adomain%252C%2520we%2520show%2520that%2520KANs%2520can%2520potentially%2520provide%2520benefits%2520when%2520developing%250Apredictive%2520models%2520for%2520data-starved%2520physics%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Kans%20%28re%29discover%20predictive%20models%20for%20Direct-Drive%20Laser%20Fusion%3F&entry.906535625=Rahman%20Ejaz%20and%20Varchas%20Gopalaswamy%20and%20Riccardo%20Betti%20and%20Aarne%20Lees%20and%20Christopher%20Kanan&entry.1292438233=%20%20The%20domain%20of%20laser%20fusion%20presents%20a%20unique%20and%20challenging%20predictive%0Amodeling%20application%20landscape%20for%20machine%20learning%20methods%20due%20to%20high%20problem%0Acomplexity%20and%20limited%20training%20data.%20Data-driven%20approaches%20utilizing%0Aprescribed%20functional%20forms%2C%20inductive%20biases%20and%20physics-informed%20learning%0A%28PIL%29%20schemes%20have%20been%20successful%20in%20the%20past%20for%20achieving%20desired%0Ageneralization%20ability%20and%20model%20interpretation%20that%20aligns%20with%20physics%0Aexpectations.%20In%20complex%20multi-physics%20application%20domains%2C%20however%2C%20it%20is%20not%0Aalways%20obvious%20how%20architectural%20biases%20or%20discriminative%20penalties%20can%20be%0Aformulated.%20In%20this%20work%2C%20focusing%20on%20nuclear%20fusion%20energy%20using%20high%20powered%0Alasers%2C%20we%20present%20the%20use%20of%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20as%20an%0Aalternative%20to%20PIL%20for%20developing%20a%20new%20type%20of%20data-driven%20predictive%20model%0Awhich%20is%20able%20to%20achieve%20high%20prediction%20accuracy%20and%20physics%20interpretability.%0AA%20KAN%20based%20model%2C%20a%20MLP%20with%20PIL%2C%20and%20a%20baseline%20MLP%20model%20are%20compared%20in%0Ageneralization%20ability%20and%20interpretation%20with%20a%20domain%20expert-derived%20symbolic%0Aregression%20model.%20Through%20empirical%20studies%20in%20this%20high%20physics%20complexity%0Adomain%2C%20we%20show%20that%20KANs%20can%20potentially%20provide%20benefits%20when%20developing%0Apredictive%20models%20for%20data-starved%20physics%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08832v1&entry.124074799=Read"},
{"title": "TabKANet: Tabular Data Modelling with Kolmogorov-Arnold Network and\n  Transformer", "author": "Weihao Gao and Zheng Gong and Zhuo Deng and Fuju Rong and Chucheng Chen and Lan Ma", "abstract": "  Tabular data is the most common type of data in real-life scenarios. In this\nstudy, we propose a method based on the TabKANet architecture, which utilizes\nthe Kolmogorov-Arnold network to encode numerical features and merge them with\ncategorical features, enabling unified modeling of tabular data on the\nTransformer architecture. This model demonstrates outstanding performance in\nsix widely used binary classification tasks, suggesting that TabKANet has the\npotential to become a standard approach for tabular modeling, surpassing\ntraditional neural networks. Furthermore, this research reveals the significant\nadvantages of the Kolmogorov-Arnold network in encoding numerical features. The\ncode of our work is available at https://github.com/tsinghuamedgao20/TabKANet.\n", "link": "http://arxiv.org/abs/2409.08806v1", "date": "2024-09-13", "relevancy": 1.8994, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4996}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4713}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabKANet%3A%20Tabular%20Data%20Modelling%20with%20Kolmogorov-Arnold%20Network%20and%0A%20%20Transformer&body=Title%3A%20TabKANet%3A%20Tabular%20Data%20Modelling%20with%20Kolmogorov-Arnold%20Network%20and%0A%20%20Transformer%0AAuthor%3A%20Weihao%20Gao%20and%20Zheng%20Gong%20and%20Zhuo%20Deng%20and%20Fuju%20Rong%20and%20Chucheng%20Chen%20and%20Lan%20Ma%0AAbstract%3A%20%20%20Tabular%20data%20is%20the%20most%20common%20type%20of%20data%20in%20real-life%20scenarios.%20In%20this%0Astudy%2C%20we%20propose%20a%20method%20based%20on%20the%20TabKANet%20architecture%2C%20which%20utilizes%0Athe%20Kolmogorov-Arnold%20network%20to%20encode%20numerical%20features%20and%20merge%20them%20with%0Acategorical%20features%2C%20enabling%20unified%20modeling%20of%20tabular%20data%20on%20the%0ATransformer%20architecture.%20This%20model%20demonstrates%20outstanding%20performance%20in%0Asix%20widely%20used%20binary%20classification%20tasks%2C%20suggesting%20that%20TabKANet%20has%20the%0Apotential%20to%20become%20a%20standard%20approach%20for%20tabular%20modeling%2C%20surpassing%0Atraditional%20neural%20networks.%20Furthermore%2C%20this%20research%20reveals%20the%20significant%0Aadvantages%20of%20the%20Kolmogorov-Arnold%20network%20in%20encoding%20numerical%20features.%20The%0Acode%20of%20our%20work%20is%20available%20at%20https%3A//github.com/tsinghuamedgao20/TabKANet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabKANet%253A%2520Tabular%2520Data%2520Modelling%2520with%2520Kolmogorov-Arnold%2520Network%2520and%250A%2520%2520Transformer%26entry.906535625%3DWeihao%2520Gao%2520and%2520Zheng%2520Gong%2520and%2520Zhuo%2520Deng%2520and%2520Fuju%2520Rong%2520and%2520Chucheng%2520Chen%2520and%2520Lan%2520Ma%26entry.1292438233%3D%2520%2520Tabular%2520data%2520is%2520the%2520most%2520common%2520type%2520of%2520data%2520in%2520real-life%2520scenarios.%2520In%2520this%250Astudy%252C%2520we%2520propose%2520a%2520method%2520based%2520on%2520the%2520TabKANet%2520architecture%252C%2520which%2520utilizes%250Athe%2520Kolmogorov-Arnold%2520network%2520to%2520encode%2520numerical%2520features%2520and%2520merge%2520them%2520with%250Acategorical%2520features%252C%2520enabling%2520unified%2520modeling%2520of%2520tabular%2520data%2520on%2520the%250ATransformer%2520architecture.%2520This%2520model%2520demonstrates%2520outstanding%2520performance%2520in%250Asix%2520widely%2520used%2520binary%2520classification%2520tasks%252C%2520suggesting%2520that%2520TabKANet%2520has%2520the%250Apotential%2520to%2520become%2520a%2520standard%2520approach%2520for%2520tabular%2520modeling%252C%2520surpassing%250Atraditional%2520neural%2520networks.%2520Furthermore%252C%2520this%2520research%2520reveals%2520the%2520significant%250Aadvantages%2520of%2520the%2520Kolmogorov-Arnold%2520network%2520in%2520encoding%2520numerical%2520features.%2520The%250Acode%2520of%2520our%2520work%2520is%2520available%2520at%2520https%253A//github.com/tsinghuamedgao20/TabKANet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabKANet%3A%20Tabular%20Data%20Modelling%20with%20Kolmogorov-Arnold%20Network%20and%0A%20%20Transformer&entry.906535625=Weihao%20Gao%20and%20Zheng%20Gong%20and%20Zhuo%20Deng%20and%20Fuju%20Rong%20and%20Chucheng%20Chen%20and%20Lan%20Ma&entry.1292438233=%20%20Tabular%20data%20is%20the%20most%20common%20type%20of%20data%20in%20real-life%20scenarios.%20In%20this%0Astudy%2C%20we%20propose%20a%20method%20based%20on%20the%20TabKANet%20architecture%2C%20which%20utilizes%0Athe%20Kolmogorov-Arnold%20network%20to%20encode%20numerical%20features%20and%20merge%20them%20with%0Acategorical%20features%2C%20enabling%20unified%20modeling%20of%20tabular%20data%20on%20the%0ATransformer%20architecture.%20This%20model%20demonstrates%20outstanding%20performance%20in%0Asix%20widely%20used%20binary%20classification%20tasks%2C%20suggesting%20that%20TabKANet%20has%20the%0Apotential%20to%20become%20a%20standard%20approach%20for%20tabular%20modeling%2C%20surpassing%0Atraditional%20neural%20networks.%20Furthermore%2C%20this%20research%20reveals%20the%20significant%0Aadvantages%20of%20the%20Kolmogorov-Arnold%20network%20in%20encoding%20numerical%20features.%20The%0Acode%20of%20our%20work%20is%20available%20at%20https%3A//github.com/tsinghuamedgao20/TabKANet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08806v1&entry.124074799=Read"},
{"title": "Exclusive Style Removal for Cross Domain Novel Class Discovery", "author": "Yicheng Wang and Feng Liu and Junmin Liu and Kai Sun", "abstract": "  As a promising field in open-world learning, \\textit{Novel Class Discovery}\n(NCD) is usually a task to cluster unseen novel classes in an unlabeled set\nbased on the prior knowledge of labeled data within the same domain. However,\nthe performance of existing NCD methods could be severely compromised when\nnovel classes are sampled from a different distribution with the labeled ones.\nIn this paper, we explore and establish the solvability of NCD in cross domain\nsetting with the necessary condition that style information must be removed.\nBased on the theoretical analysis, we introduce an exclusive style removal\nmodule for extracting style information that is distinctive from the baseline\nfeatures, thereby facilitating inference. Moreover, this module is easy to\nintegrate with other NCD methods, acting as a plug-in to improve performance on\nnovel classes with different distributions compared to the seen labeled set.\nAdditionally, recognizing the non-negligible influence of different backbones\nand pre-training strategies on the performance of the NCD methods, we build a\nfair benchmark for future NCD research. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our proposed module.\n", "link": "http://arxiv.org/abs/2406.18140v3", "date": "2024-09-13", "relevancy": 1.8924, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5223}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exclusive%20Style%20Removal%20for%20Cross%20Domain%20Novel%20Class%20Discovery&body=Title%3A%20Exclusive%20Style%20Removal%20for%20Cross%20Domain%20Novel%20Class%20Discovery%0AAuthor%3A%20Yicheng%20Wang%20and%20Feng%20Liu%20and%20Junmin%20Liu%20and%20Kai%20Sun%0AAbstract%3A%20%20%20As%20a%20promising%20field%20in%20open-world%20learning%2C%20%5Ctextit%7BNovel%20Class%20Discovery%7D%0A%28NCD%29%20is%20usually%20a%20task%20to%20cluster%20unseen%20novel%20classes%20in%20an%20unlabeled%20set%0Abased%20on%20the%20prior%20knowledge%20of%20labeled%20data%20within%20the%20same%20domain.%20However%2C%0Athe%20performance%20of%20existing%20NCD%20methods%20could%20be%20severely%20compromised%20when%0Anovel%20classes%20are%20sampled%20from%20a%20different%20distribution%20with%20the%20labeled%20ones.%0AIn%20this%20paper%2C%20we%20explore%20and%20establish%20the%20solvability%20of%20NCD%20in%20cross%20domain%0Asetting%20with%20the%20necessary%20condition%20that%20style%20information%20must%20be%20removed.%0ABased%20on%20the%20theoretical%20analysis%2C%20we%20introduce%20an%20exclusive%20style%20removal%0Amodule%20for%20extracting%20style%20information%20that%20is%20distinctive%20from%20the%20baseline%0Afeatures%2C%20thereby%20facilitating%20inference.%20Moreover%2C%20this%20module%20is%20easy%20to%0Aintegrate%20with%20other%20NCD%20methods%2C%20acting%20as%20a%20plug-in%20to%20improve%20performance%20on%0Anovel%20classes%20with%20different%20distributions%20compared%20to%20the%20seen%20labeled%20set.%0AAdditionally%2C%20recognizing%20the%20non-negligible%20influence%20of%20different%20backbones%0Aand%20pre-training%20strategies%20on%20the%20performance%20of%20the%20NCD%20methods%2C%20we%20build%20a%0Afair%20benchmark%20for%20future%20NCD%20research.%20Extensive%20experiments%20on%20three%20common%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18140v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExclusive%2520Style%2520Removal%2520for%2520Cross%2520Domain%2520Novel%2520Class%2520Discovery%26entry.906535625%3DYicheng%2520Wang%2520and%2520Feng%2520Liu%2520and%2520Junmin%2520Liu%2520and%2520Kai%2520Sun%26entry.1292438233%3D%2520%2520As%2520a%2520promising%2520field%2520in%2520open-world%2520learning%252C%2520%255Ctextit%257BNovel%2520Class%2520Discovery%257D%250A%2528NCD%2529%2520is%2520usually%2520a%2520task%2520to%2520cluster%2520unseen%2520novel%2520classes%2520in%2520an%2520unlabeled%2520set%250Abased%2520on%2520the%2520prior%2520knowledge%2520of%2520labeled%2520data%2520within%2520the%2520same%2520domain.%2520However%252C%250Athe%2520performance%2520of%2520existing%2520NCD%2520methods%2520could%2520be%2520severely%2520compromised%2520when%250Anovel%2520classes%2520are%2520sampled%2520from%2520a%2520different%2520distribution%2520with%2520the%2520labeled%2520ones.%250AIn%2520this%2520paper%252C%2520we%2520explore%2520and%2520establish%2520the%2520solvability%2520of%2520NCD%2520in%2520cross%2520domain%250Asetting%2520with%2520the%2520necessary%2520condition%2520that%2520style%2520information%2520must%2520be%2520removed.%250ABased%2520on%2520the%2520theoretical%2520analysis%252C%2520we%2520introduce%2520an%2520exclusive%2520style%2520removal%250Amodule%2520for%2520extracting%2520style%2520information%2520that%2520is%2520distinctive%2520from%2520the%2520baseline%250Afeatures%252C%2520thereby%2520facilitating%2520inference.%2520Moreover%252C%2520this%2520module%2520is%2520easy%2520to%250Aintegrate%2520with%2520other%2520NCD%2520methods%252C%2520acting%2520as%2520a%2520plug-in%2520to%2520improve%2520performance%2520on%250Anovel%2520classes%2520with%2520different%2520distributions%2520compared%2520to%2520the%2520seen%2520labeled%2520set.%250AAdditionally%252C%2520recognizing%2520the%2520non-negligible%2520influence%2520of%2520different%2520backbones%250Aand%2520pre-training%2520strategies%2520on%2520the%2520performance%2520of%2520the%2520NCD%2520methods%252C%2520we%2520build%2520a%250Afair%2520benchmark%2520for%2520future%2520NCD%2520research.%2520Extensive%2520experiments%2520on%2520three%2520common%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18140v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exclusive%20Style%20Removal%20for%20Cross%20Domain%20Novel%20Class%20Discovery&entry.906535625=Yicheng%20Wang%20and%20Feng%20Liu%20and%20Junmin%20Liu%20and%20Kai%20Sun&entry.1292438233=%20%20As%20a%20promising%20field%20in%20open-world%20learning%2C%20%5Ctextit%7BNovel%20Class%20Discovery%7D%0A%28NCD%29%20is%20usually%20a%20task%20to%20cluster%20unseen%20novel%20classes%20in%20an%20unlabeled%20set%0Abased%20on%20the%20prior%20knowledge%20of%20labeled%20data%20within%20the%20same%20domain.%20However%2C%0Athe%20performance%20of%20existing%20NCD%20methods%20could%20be%20severely%20compromised%20when%0Anovel%20classes%20are%20sampled%20from%20a%20different%20distribution%20with%20the%20labeled%20ones.%0AIn%20this%20paper%2C%20we%20explore%20and%20establish%20the%20solvability%20of%20NCD%20in%20cross%20domain%0Asetting%20with%20the%20necessary%20condition%20that%20style%20information%20must%20be%20removed.%0ABased%20on%20the%20theoretical%20analysis%2C%20we%20introduce%20an%20exclusive%20style%20removal%0Amodule%20for%20extracting%20style%20information%20that%20is%20distinctive%20from%20the%20baseline%0Afeatures%2C%20thereby%20facilitating%20inference.%20Moreover%2C%20this%20module%20is%20easy%20to%0Aintegrate%20with%20other%20NCD%20methods%2C%20acting%20as%20a%20plug-in%20to%20improve%20performance%20on%0Anovel%20classes%20with%20different%20distributions%20compared%20to%20the%20seen%20labeled%20set.%0AAdditionally%2C%20recognizing%20the%20non-negligible%20influence%20of%20different%20backbones%0Aand%20pre-training%20strategies%20on%20the%20performance%20of%20the%20NCD%20methods%2C%20we%20build%20a%0Afair%20benchmark%20for%20future%20NCD%20research.%20Extensive%20experiments%20on%20three%20common%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18140v3&entry.124074799=Read"},
{"title": "On the Computation of BD-Rate over a Set of Videos for Fair Assessment\n  of Performance of Learned Video Codecs", "author": "M. Akin Yilmaz and Onur Kele\u015f and A. Murat Tekalp", "abstract": "  The Bj{\\o}ntegaard Delta (BD) measure is widely employed to evaluate and\nquantify the variations in the rate-distortion(RD) performance across different\ncodecs. Many researchers report the average BD value over multiple videos\nwithin a dataset for different codecs. We claim that the current practice in\nthe learned video compression community of computing the average BD value over\na dataset based on the average RD curve of multiple videos can lead to\nmisleading conclusions. We show both by analysis of a simplistic case of linear\nRD curves and experimental results with two recent learned video codecs that\naveraging RD curves can lead to a single video to disproportionately influence\nthe average BD value especially when the operating bitrate range of different\ncodecs do not exactly match. Instead, we advocate for calculating the BD\nmeasure per-video basis, as commonly done by the traditional video compression\ncommunity, followed by averaging the individual BD values over videos, to\nprovide a fair comparison of learned video codecs. Our experimental results\ndemonstrate that the comparison of two recent learned video codecs is affected\nby how we evaluate the average BD measure.\n", "link": "http://arxiv.org/abs/2409.08772v1", "date": "2024-09-13", "relevancy": 1.8901, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4952}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4583}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Computation%20of%20BD-Rate%20over%20a%20Set%20of%20Videos%20for%20Fair%20Assessment%0A%20%20of%20Performance%20of%20Learned%20Video%20Codecs&body=Title%3A%20On%20the%20Computation%20of%20BD-Rate%20over%20a%20Set%20of%20Videos%20for%20Fair%20Assessment%0A%20%20of%20Performance%20of%20Learned%20Video%20Codecs%0AAuthor%3A%20M.%20Akin%20Yilmaz%20and%20Onur%20Kele%C5%9F%20and%20A.%20Murat%20Tekalp%0AAbstract%3A%20%20%20The%20Bj%7B%5Co%7Dntegaard%20Delta%20%28BD%29%20measure%20is%20widely%20employed%20to%20evaluate%20and%0Aquantify%20the%20variations%20in%20the%20rate-distortion%28RD%29%20performance%20across%20different%0Acodecs.%20Many%20researchers%20report%20the%20average%20BD%20value%20over%20multiple%20videos%0Awithin%20a%20dataset%20for%20different%20codecs.%20We%20claim%20that%20the%20current%20practice%20in%0Athe%20learned%20video%20compression%20community%20of%20computing%20the%20average%20BD%20value%20over%0Aa%20dataset%20based%20on%20the%20average%20RD%20curve%20of%20multiple%20videos%20can%20lead%20to%0Amisleading%20conclusions.%20We%20show%20both%20by%20analysis%20of%20a%20simplistic%20case%20of%20linear%0ARD%20curves%20and%20experimental%20results%20with%20two%20recent%20learned%20video%20codecs%20that%0Aaveraging%20RD%20curves%20can%20lead%20to%20a%20single%20video%20to%20disproportionately%20influence%0Athe%20average%20BD%20value%20especially%20when%20the%20operating%20bitrate%20range%20of%20different%0Acodecs%20do%20not%20exactly%20match.%20Instead%2C%20we%20advocate%20for%20calculating%20the%20BD%0Ameasure%20per-video%20basis%2C%20as%20commonly%20done%20by%20the%20traditional%20video%20compression%0Acommunity%2C%20followed%20by%20averaging%20the%20individual%20BD%20values%20over%20videos%2C%20to%0Aprovide%20a%20fair%20comparison%20of%20learned%20video%20codecs.%20Our%20experimental%20results%0Ademonstrate%20that%20the%20comparison%20of%20two%20recent%20learned%20video%20codecs%20is%20affected%0Aby%20how%20we%20evaluate%20the%20average%20BD%20measure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Computation%2520of%2520BD-Rate%2520over%2520a%2520Set%2520of%2520Videos%2520for%2520Fair%2520Assessment%250A%2520%2520of%2520Performance%2520of%2520Learned%2520Video%2520Codecs%26entry.906535625%3DM.%2520Akin%2520Yilmaz%2520and%2520Onur%2520Kele%25C5%259F%2520and%2520A.%2520Murat%2520Tekalp%26entry.1292438233%3D%2520%2520The%2520Bj%257B%255Co%257Dntegaard%2520Delta%2520%2528BD%2529%2520measure%2520is%2520widely%2520employed%2520to%2520evaluate%2520and%250Aquantify%2520the%2520variations%2520in%2520the%2520rate-distortion%2528RD%2529%2520performance%2520across%2520different%250Acodecs.%2520Many%2520researchers%2520report%2520the%2520average%2520BD%2520value%2520over%2520multiple%2520videos%250Awithin%2520a%2520dataset%2520for%2520different%2520codecs.%2520We%2520claim%2520that%2520the%2520current%2520practice%2520in%250Athe%2520learned%2520video%2520compression%2520community%2520of%2520computing%2520the%2520average%2520BD%2520value%2520over%250Aa%2520dataset%2520based%2520on%2520the%2520average%2520RD%2520curve%2520of%2520multiple%2520videos%2520can%2520lead%2520to%250Amisleading%2520conclusions.%2520We%2520show%2520both%2520by%2520analysis%2520of%2520a%2520simplistic%2520case%2520of%2520linear%250ARD%2520curves%2520and%2520experimental%2520results%2520with%2520two%2520recent%2520learned%2520video%2520codecs%2520that%250Aaveraging%2520RD%2520curves%2520can%2520lead%2520to%2520a%2520single%2520video%2520to%2520disproportionately%2520influence%250Athe%2520average%2520BD%2520value%2520especially%2520when%2520the%2520operating%2520bitrate%2520range%2520of%2520different%250Acodecs%2520do%2520not%2520exactly%2520match.%2520Instead%252C%2520we%2520advocate%2520for%2520calculating%2520the%2520BD%250Ameasure%2520per-video%2520basis%252C%2520as%2520commonly%2520done%2520by%2520the%2520traditional%2520video%2520compression%250Acommunity%252C%2520followed%2520by%2520averaging%2520the%2520individual%2520BD%2520values%2520over%2520videos%252C%2520to%250Aprovide%2520a%2520fair%2520comparison%2520of%2520learned%2520video%2520codecs.%2520Our%2520experimental%2520results%250Ademonstrate%2520that%2520the%2520comparison%2520of%2520two%2520recent%2520learned%2520video%2520codecs%2520is%2520affected%250Aby%2520how%2520we%2520evaluate%2520the%2520average%2520BD%2520measure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Computation%20of%20BD-Rate%20over%20a%20Set%20of%20Videos%20for%20Fair%20Assessment%0A%20%20of%20Performance%20of%20Learned%20Video%20Codecs&entry.906535625=M.%20Akin%20Yilmaz%20and%20Onur%20Kele%C5%9F%20and%20A.%20Murat%20Tekalp&entry.1292438233=%20%20The%20Bj%7B%5Co%7Dntegaard%20Delta%20%28BD%29%20measure%20is%20widely%20employed%20to%20evaluate%20and%0Aquantify%20the%20variations%20in%20the%20rate-distortion%28RD%29%20performance%20across%20different%0Acodecs.%20Many%20researchers%20report%20the%20average%20BD%20value%20over%20multiple%20videos%0Awithin%20a%20dataset%20for%20different%20codecs.%20We%20claim%20that%20the%20current%20practice%20in%0Athe%20learned%20video%20compression%20community%20of%20computing%20the%20average%20BD%20value%20over%0Aa%20dataset%20based%20on%20the%20average%20RD%20curve%20of%20multiple%20videos%20can%20lead%20to%0Amisleading%20conclusions.%20We%20show%20both%20by%20analysis%20of%20a%20simplistic%20case%20of%20linear%0ARD%20curves%20and%20experimental%20results%20with%20two%20recent%20learned%20video%20codecs%20that%0Aaveraging%20RD%20curves%20can%20lead%20to%20a%20single%20video%20to%20disproportionately%20influence%0Athe%20average%20BD%20value%20especially%20when%20the%20operating%20bitrate%20range%20of%20different%0Acodecs%20do%20not%20exactly%20match.%20Instead%2C%20we%20advocate%20for%20calculating%20the%20BD%0Ameasure%20per-video%20basis%2C%20as%20commonly%20done%20by%20the%20traditional%20video%20compression%0Acommunity%2C%20followed%20by%20averaging%20the%20individual%20BD%20values%20over%20videos%2C%20to%0Aprovide%20a%20fair%20comparison%20of%20learned%20video%20codecs.%20Our%20experimental%20results%0Ademonstrate%20that%20the%20comparison%20of%20two%20recent%20learned%20video%20codecs%20is%20affected%0Aby%20how%20we%20evaluate%20the%20average%20BD%20measure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08772v1&entry.124074799=Read"},
{"title": "A Study on the Implementation Method of an Agent-Based Advanced RAG\n  System Using Graph", "author": "Cheonsu Jeong", "abstract": "  This study aims to improve knowledge-based question-answering (QA) systems by\novercoming the limitations of existing Retrieval-Augmented Generation (RAG)\nmodels and implementing an advanced RAG system based on Graph technology to\ndevelop high-quality generative AI services. While existing RAG models\ndemonstrate high accuracy and fluency by utilizing retrieved information, they\nmay suffer from accuracy degradation as they generate responses using\npre-loaded knowledge without reprocessing. Additionally, they cannot\nincorporate real-time data after the RAG configuration stage, leading to issues\nwith contextual understanding and biased information. To address these\nlimitations, this study implemented an enhanced RAG system utilizing Graph\ntechnology. This system is designed to efficiently search and utilize\ninformation. Specifically, it employs LangGraph to evaluate the reliability of\nretrieved information and synthesizes diverse data to generate more accurate\nand enhanced responses. Furthermore, the study provides a detailed explanation\nof the system's operation, key implementation steps, and examples through\nimplementation code and validation results, thereby enhancing the understanding\nof advanced RAG technology. This approach offers practical guidelines for\nimplementing advanced RAG systems in corporate services, making it a valuable\nresource for practical application.\n", "link": "http://arxiv.org/abs/2407.19994v3", "date": "2024-09-13", "relevancy": 1.8874, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.544}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4625}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20on%20the%20Implementation%20Method%20of%20an%20Agent-Based%20Advanced%20RAG%0A%20%20System%20Using%20Graph&body=Title%3A%20A%20Study%20on%20the%20Implementation%20Method%20of%20an%20Agent-Based%20Advanced%20RAG%0A%20%20System%20Using%20Graph%0AAuthor%3A%20Cheonsu%20Jeong%0AAbstract%3A%20%20%20This%20study%20aims%20to%20improve%20knowledge-based%20question-answering%20%28QA%29%20systems%20by%0Aovercoming%20the%20limitations%20of%20existing%20Retrieval-Augmented%20Generation%20%28RAG%29%0Amodels%20and%20implementing%20an%20advanced%20RAG%20system%20based%20on%20Graph%20technology%20to%0Adevelop%20high-quality%20generative%20AI%20services.%20While%20existing%20RAG%20models%0Ademonstrate%20high%20accuracy%20and%20fluency%20by%20utilizing%20retrieved%20information%2C%20they%0Amay%20suffer%20from%20accuracy%20degradation%20as%20they%20generate%20responses%20using%0Apre-loaded%20knowledge%20without%20reprocessing.%20Additionally%2C%20they%20cannot%0Aincorporate%20real-time%20data%20after%20the%20RAG%20configuration%20stage%2C%20leading%20to%20issues%0Awith%20contextual%20understanding%20and%20biased%20information.%20To%20address%20these%0Alimitations%2C%20this%20study%20implemented%20an%20enhanced%20RAG%20system%20utilizing%20Graph%0Atechnology.%20This%20system%20is%20designed%20to%20efficiently%20search%20and%20utilize%0Ainformation.%20Specifically%2C%20it%20employs%20LangGraph%20to%20evaluate%20the%20reliability%20of%0Aretrieved%20information%20and%20synthesizes%20diverse%20data%20to%20generate%20more%20accurate%0Aand%20enhanced%20responses.%20Furthermore%2C%20the%20study%20provides%20a%20detailed%20explanation%0Aof%20the%20system%27s%20operation%2C%20key%20implementation%20steps%2C%20and%20examples%20through%0Aimplementation%20code%20and%20validation%20results%2C%20thereby%20enhancing%20the%20understanding%0Aof%20advanced%20RAG%20technology.%20This%20approach%20offers%20practical%20guidelines%20for%0Aimplementing%20advanced%20RAG%20systems%20in%20corporate%20services%2C%20making%20it%20a%20valuable%0Aresource%20for%20practical%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19994v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520on%2520the%2520Implementation%2520Method%2520of%2520an%2520Agent-Based%2520Advanced%2520RAG%250A%2520%2520System%2520Using%2520Graph%26entry.906535625%3DCheonsu%2520Jeong%26entry.1292438233%3D%2520%2520This%2520study%2520aims%2520to%2520improve%2520knowledge-based%2520question-answering%2520%2528QA%2529%2520systems%2520by%250Aovercoming%2520the%2520limitations%2520of%2520existing%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%250Amodels%2520and%2520implementing%2520an%2520advanced%2520RAG%2520system%2520based%2520on%2520Graph%2520technology%2520to%250Adevelop%2520high-quality%2520generative%2520AI%2520services.%2520While%2520existing%2520RAG%2520models%250Ademonstrate%2520high%2520accuracy%2520and%2520fluency%2520by%2520utilizing%2520retrieved%2520information%252C%2520they%250Amay%2520suffer%2520from%2520accuracy%2520degradation%2520as%2520they%2520generate%2520responses%2520using%250Apre-loaded%2520knowledge%2520without%2520reprocessing.%2520Additionally%252C%2520they%2520cannot%250Aincorporate%2520real-time%2520data%2520after%2520the%2520RAG%2520configuration%2520stage%252C%2520leading%2520to%2520issues%250Awith%2520contextual%2520understanding%2520and%2520biased%2520information.%2520To%2520address%2520these%250Alimitations%252C%2520this%2520study%2520implemented%2520an%2520enhanced%2520RAG%2520system%2520utilizing%2520Graph%250Atechnology.%2520This%2520system%2520is%2520designed%2520to%2520efficiently%2520search%2520and%2520utilize%250Ainformation.%2520Specifically%252C%2520it%2520employs%2520LangGraph%2520to%2520evaluate%2520the%2520reliability%2520of%250Aretrieved%2520information%2520and%2520synthesizes%2520diverse%2520data%2520to%2520generate%2520more%2520accurate%250Aand%2520enhanced%2520responses.%2520Furthermore%252C%2520the%2520study%2520provides%2520a%2520detailed%2520explanation%250Aof%2520the%2520system%2527s%2520operation%252C%2520key%2520implementation%2520steps%252C%2520and%2520examples%2520through%250Aimplementation%2520code%2520and%2520validation%2520results%252C%2520thereby%2520enhancing%2520the%2520understanding%250Aof%2520advanced%2520RAG%2520technology.%2520This%2520approach%2520offers%2520practical%2520guidelines%2520for%250Aimplementing%2520advanced%2520RAG%2520systems%2520in%2520corporate%2520services%252C%2520making%2520it%2520a%2520valuable%250Aresource%2520for%2520practical%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19994v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20on%20the%20Implementation%20Method%20of%20an%20Agent-Based%20Advanced%20RAG%0A%20%20System%20Using%20Graph&entry.906535625=Cheonsu%20Jeong&entry.1292438233=%20%20This%20study%20aims%20to%20improve%20knowledge-based%20question-answering%20%28QA%29%20systems%20by%0Aovercoming%20the%20limitations%20of%20existing%20Retrieval-Augmented%20Generation%20%28RAG%29%0Amodels%20and%20implementing%20an%20advanced%20RAG%20system%20based%20on%20Graph%20technology%20to%0Adevelop%20high-quality%20generative%20AI%20services.%20While%20existing%20RAG%20models%0Ademonstrate%20high%20accuracy%20and%20fluency%20by%20utilizing%20retrieved%20information%2C%20they%0Amay%20suffer%20from%20accuracy%20degradation%20as%20they%20generate%20responses%20using%0Apre-loaded%20knowledge%20without%20reprocessing.%20Additionally%2C%20they%20cannot%0Aincorporate%20real-time%20data%20after%20the%20RAG%20configuration%20stage%2C%20leading%20to%20issues%0Awith%20contextual%20understanding%20and%20biased%20information.%20To%20address%20these%0Alimitations%2C%20this%20study%20implemented%20an%20enhanced%20RAG%20system%20utilizing%20Graph%0Atechnology.%20This%20system%20is%20designed%20to%20efficiently%20search%20and%20utilize%0Ainformation.%20Specifically%2C%20it%20employs%20LangGraph%20to%20evaluate%20the%20reliability%20of%0Aretrieved%20information%20and%20synthesizes%20diverse%20data%20to%20generate%20more%20accurate%0Aand%20enhanced%20responses.%20Furthermore%2C%20the%20study%20provides%20a%20detailed%20explanation%0Aof%20the%20system%27s%20operation%2C%20key%20implementation%20steps%2C%20and%20examples%20through%0Aimplementation%20code%20and%20validation%20results%2C%20thereby%20enhancing%20the%20understanding%0Aof%20advanced%20RAG%20technology.%20This%20approach%20offers%20practical%20guidelines%20for%0Aimplementing%20advanced%20RAG%20systems%20in%20corporate%20services%2C%20making%20it%20a%20valuable%0Aresource%20for%20practical%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19994v3&entry.124074799=Read"},
{"title": "DexSim2Real$^{2}$: Building Explicit World Model for Precise Articulated\n  Object Dexterous Manipulation", "author": "Taoran Jiang and Liqian Ma and Yixuan Guan and Jiaojiao Meng and Weihang Chen and Zecui Zeng and Lusong Li and Dan Wu and Jing Xu and Rui Chen", "abstract": "  Articulated object manipulation is ubiquitous in daily life. In this paper,\nwe present DexSim2Real$^{2}$, a novel robot learning framework for\ngoal-conditioned articulated object manipulation using both two-finger grippers\nand multi-finger dexterous hands. The key of our framework is constructing an\nexplicit world model of unseen articulated objects through active one-step\ninteractions. This explicit world model enables sampling-based model predictive\ncontrol to plan trajectories achieving different manipulation goals without\nneeding human demonstrations or reinforcement learning. It first predicts an\ninteraction motion using an affordance estimation network trained on\nself-supervised interaction data or videos of human manipulation from the\ninternet. After executing this interaction on the real robot, the framework\nconstructs a digital twin of the articulated object in simulation based on the\ntwo point clouds before and after the interaction. For dexterous multi-finger\nmanipulation, we propose to utilize eigengrasp to reduce the high-dimensional\naction space, enabling more efficient trajectory searching. Extensive\nexperiments validate the framework's effectiveness for precise articulated\nobject manipulation in both simulation and the real world using a two-finger\ngripper and a 16-DoF dexterous hand. The robust generalizability of the\nexplicit world model also enables advanced manipulation strategies, such as\nmanipulating with different tools.\n", "link": "http://arxiv.org/abs/2409.08750v1", "date": "2024-09-13", "relevancy": 1.8737, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.65}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexSim2Real%24%5E%7B2%7D%24%3A%20Building%20Explicit%20World%20Model%20for%20Precise%20Articulated%0A%20%20Object%20Dexterous%20Manipulation&body=Title%3A%20DexSim2Real%24%5E%7B2%7D%24%3A%20Building%20Explicit%20World%20Model%20for%20Precise%20Articulated%0A%20%20Object%20Dexterous%20Manipulation%0AAuthor%3A%20Taoran%20Jiang%20and%20Liqian%20Ma%20and%20Yixuan%20Guan%20and%20Jiaojiao%20Meng%20and%20Weihang%20Chen%20and%20Zecui%20Zeng%20and%20Lusong%20Li%20and%20Dan%20Wu%20and%20Jing%20Xu%20and%20Rui%20Chen%0AAbstract%3A%20%20%20Articulated%20object%20manipulation%20is%20ubiquitous%20in%20daily%20life.%20In%20this%20paper%2C%0Awe%20present%20DexSim2Real%24%5E%7B2%7D%24%2C%20a%20novel%20robot%20learning%20framework%20for%0Agoal-conditioned%20articulated%20object%20manipulation%20using%20both%20two-finger%20grippers%0Aand%20multi-finger%20dexterous%20hands.%20The%20key%20of%20our%20framework%20is%20constructing%20an%0Aexplicit%20world%20model%20of%20unseen%20articulated%20objects%20through%20active%20one-step%0Ainteractions.%20This%20explicit%20world%20model%20enables%20sampling-based%20model%20predictive%0Acontrol%20to%20plan%20trajectories%20achieving%20different%20manipulation%20goals%20without%0Aneeding%20human%20demonstrations%20or%20reinforcement%20learning.%20It%20first%20predicts%20an%0Ainteraction%20motion%20using%20an%20affordance%20estimation%20network%20trained%20on%0Aself-supervised%20interaction%20data%20or%20videos%20of%20human%20manipulation%20from%20the%0Ainternet.%20After%20executing%20this%20interaction%20on%20the%20real%20robot%2C%20the%20framework%0Aconstructs%20a%20digital%20twin%20of%20the%20articulated%20object%20in%20simulation%20based%20on%20the%0Atwo%20point%20clouds%20before%20and%20after%20the%20interaction.%20For%20dexterous%20multi-finger%0Amanipulation%2C%20we%20propose%20to%20utilize%20eigengrasp%20to%20reduce%20the%20high-dimensional%0Aaction%20space%2C%20enabling%20more%20efficient%20trajectory%20searching.%20Extensive%0Aexperiments%20validate%20the%20framework%27s%20effectiveness%20for%20precise%20articulated%0Aobject%20manipulation%20in%20both%20simulation%20and%20the%20real%20world%20using%20a%20two-finger%0Agripper%20and%20a%2016-DoF%20dexterous%20hand.%20The%20robust%20generalizability%20of%20the%0Aexplicit%20world%20model%20also%20enables%20advanced%20manipulation%20strategies%2C%20such%20as%0Amanipulating%20with%20different%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexSim2Real%2524%255E%257B2%257D%2524%253A%2520Building%2520Explicit%2520World%2520Model%2520for%2520Precise%2520Articulated%250A%2520%2520Object%2520Dexterous%2520Manipulation%26entry.906535625%3DTaoran%2520Jiang%2520and%2520Liqian%2520Ma%2520and%2520Yixuan%2520Guan%2520and%2520Jiaojiao%2520Meng%2520and%2520Weihang%2520Chen%2520and%2520Zecui%2520Zeng%2520and%2520Lusong%2520Li%2520and%2520Dan%2520Wu%2520and%2520Jing%2520Xu%2520and%2520Rui%2520Chen%26entry.1292438233%3D%2520%2520Articulated%2520object%2520manipulation%2520is%2520ubiquitous%2520in%2520daily%2520life.%2520In%2520this%2520paper%252C%250Awe%2520present%2520DexSim2Real%2524%255E%257B2%257D%2524%252C%2520a%2520novel%2520robot%2520learning%2520framework%2520for%250Agoal-conditioned%2520articulated%2520object%2520manipulation%2520using%2520both%2520two-finger%2520grippers%250Aand%2520multi-finger%2520dexterous%2520hands.%2520The%2520key%2520of%2520our%2520framework%2520is%2520constructing%2520an%250Aexplicit%2520world%2520model%2520of%2520unseen%2520articulated%2520objects%2520through%2520active%2520one-step%250Ainteractions.%2520This%2520explicit%2520world%2520model%2520enables%2520sampling-based%2520model%2520predictive%250Acontrol%2520to%2520plan%2520trajectories%2520achieving%2520different%2520manipulation%2520goals%2520without%250Aneeding%2520human%2520demonstrations%2520or%2520reinforcement%2520learning.%2520It%2520first%2520predicts%2520an%250Ainteraction%2520motion%2520using%2520an%2520affordance%2520estimation%2520network%2520trained%2520on%250Aself-supervised%2520interaction%2520data%2520or%2520videos%2520of%2520human%2520manipulation%2520from%2520the%250Ainternet.%2520After%2520executing%2520this%2520interaction%2520on%2520the%2520real%2520robot%252C%2520the%2520framework%250Aconstructs%2520a%2520digital%2520twin%2520of%2520the%2520articulated%2520object%2520in%2520simulation%2520based%2520on%2520the%250Atwo%2520point%2520clouds%2520before%2520and%2520after%2520the%2520interaction.%2520For%2520dexterous%2520multi-finger%250Amanipulation%252C%2520we%2520propose%2520to%2520utilize%2520eigengrasp%2520to%2520reduce%2520the%2520high-dimensional%250Aaction%2520space%252C%2520enabling%2520more%2520efficient%2520trajectory%2520searching.%2520Extensive%250Aexperiments%2520validate%2520the%2520framework%2527s%2520effectiveness%2520for%2520precise%2520articulated%250Aobject%2520manipulation%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world%2520using%2520a%2520two-finger%250Agripper%2520and%2520a%252016-DoF%2520dexterous%2520hand.%2520The%2520robust%2520generalizability%2520of%2520the%250Aexplicit%2520world%2520model%2520also%2520enables%2520advanced%2520manipulation%2520strategies%252C%2520such%2520as%250Amanipulating%2520with%2520different%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexSim2Real%24%5E%7B2%7D%24%3A%20Building%20Explicit%20World%20Model%20for%20Precise%20Articulated%0A%20%20Object%20Dexterous%20Manipulation&entry.906535625=Taoran%20Jiang%20and%20Liqian%20Ma%20and%20Yixuan%20Guan%20and%20Jiaojiao%20Meng%20and%20Weihang%20Chen%20and%20Zecui%20Zeng%20and%20Lusong%20Li%20and%20Dan%20Wu%20and%20Jing%20Xu%20and%20Rui%20Chen&entry.1292438233=%20%20Articulated%20object%20manipulation%20is%20ubiquitous%20in%20daily%20life.%20In%20this%20paper%2C%0Awe%20present%20DexSim2Real%24%5E%7B2%7D%24%2C%20a%20novel%20robot%20learning%20framework%20for%0Agoal-conditioned%20articulated%20object%20manipulation%20using%20both%20two-finger%20grippers%0Aand%20multi-finger%20dexterous%20hands.%20The%20key%20of%20our%20framework%20is%20constructing%20an%0Aexplicit%20world%20model%20of%20unseen%20articulated%20objects%20through%20active%20one-step%0Ainteractions.%20This%20explicit%20world%20model%20enables%20sampling-based%20model%20predictive%0Acontrol%20to%20plan%20trajectories%20achieving%20different%20manipulation%20goals%20without%0Aneeding%20human%20demonstrations%20or%20reinforcement%20learning.%20It%20first%20predicts%20an%0Ainteraction%20motion%20using%20an%20affordance%20estimation%20network%20trained%20on%0Aself-supervised%20interaction%20data%20or%20videos%20of%20human%20manipulation%20from%20the%0Ainternet.%20After%20executing%20this%20interaction%20on%20the%20real%20robot%2C%20the%20framework%0Aconstructs%20a%20digital%20twin%20of%20the%20articulated%20object%20in%20simulation%20based%20on%20the%0Atwo%20point%20clouds%20before%20and%20after%20the%20interaction.%20For%20dexterous%20multi-finger%0Amanipulation%2C%20we%20propose%20to%20utilize%20eigengrasp%20to%20reduce%20the%20high-dimensional%0Aaction%20space%2C%20enabling%20more%20efficient%20trajectory%20searching.%20Extensive%0Aexperiments%20validate%20the%20framework%27s%20effectiveness%20for%20precise%20articulated%0Aobject%20manipulation%20in%20both%20simulation%20and%20the%20real%20world%20using%20a%20two-finger%0Agripper%20and%20a%2016-DoF%20dexterous%20hand.%20The%20robust%20generalizability%20of%20the%0Aexplicit%20world%20model%20also%20enables%20advanced%20manipulation%20strategies%2C%20such%20as%0Amanipulating%20with%20different%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08750v1&entry.124074799=Read"},
{"title": "FP-VEC: Fingerprinting Large Language Models via Efficient Vector\n  Addition", "author": "Zhenhua Xu and Wenpeng Xing and Zhebo Wang and Chang Hu and Chen Jie and Meng Han", "abstract": "  Training Large Language Models (LLMs) requires immense computational power\nand vast amounts of data. As a result, protecting the intellectual property of\nthese models through fingerprinting is essential for ownership authentication.\nWhile adding fingerprints to LLMs through fine-tuning has been attempted, it\nremains costly and unscalable. In this paper, we introduce FP-VEC, a pilot\nstudy on using fingerprint vectors as an efficient fingerprinting method for\nLLMs. Our approach generates a fingerprint vector that represents a\nconfidential signature embedded in the model, allowing the same fingerprint to\nbe seamlessly incorporated into an unlimited number of LLMs via vector\naddition. Results on several LLMs show that FP-VEC is lightweight by running on\nCPU-only devices for fingerprinting, scalable with a single training and\nunlimited fingerprinting process, and preserves the model's normal behavior.\nThe project page is available at https://fingerprintvector.github.io .\n", "link": "http://arxiv.org/abs/2409.08846v1", "date": "2024-09-13", "relevancy": 1.872, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.469}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4683}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FP-VEC%3A%20Fingerprinting%20Large%20Language%20Models%20via%20Efficient%20Vector%0A%20%20Addition&body=Title%3A%20FP-VEC%3A%20Fingerprinting%20Large%20Language%20Models%20via%20Efficient%20Vector%0A%20%20Addition%0AAuthor%3A%20Zhenhua%20Xu%20and%20Wenpeng%20Xing%20and%20Zhebo%20Wang%20and%20Chang%20Hu%20and%20Chen%20Jie%20and%20Meng%20Han%0AAbstract%3A%20%20%20Training%20Large%20Language%20Models%20%28LLMs%29%20requires%20immense%20computational%20power%0Aand%20vast%20amounts%20of%20data.%20As%20a%20result%2C%20protecting%20the%20intellectual%20property%20of%0Athese%20models%20through%20fingerprinting%20is%20essential%20for%20ownership%20authentication.%0AWhile%20adding%20fingerprints%20to%20LLMs%20through%20fine-tuning%20has%20been%20attempted%2C%20it%0Aremains%20costly%20and%20unscalable.%20In%20this%20paper%2C%20we%20introduce%20FP-VEC%2C%20a%20pilot%0Astudy%20on%20using%20fingerprint%20vectors%20as%20an%20efficient%20fingerprinting%20method%20for%0ALLMs.%20Our%20approach%20generates%20a%20fingerprint%20vector%20that%20represents%20a%0Aconfidential%20signature%20embedded%20in%20the%20model%2C%20allowing%20the%20same%20fingerprint%20to%0Abe%20seamlessly%20incorporated%20into%20an%20unlimited%20number%20of%20LLMs%20via%20vector%0Aaddition.%20Results%20on%20several%20LLMs%20show%20that%20FP-VEC%20is%20lightweight%20by%20running%20on%0ACPU-only%20devices%20for%20fingerprinting%2C%20scalable%20with%20a%20single%20training%20and%0Aunlimited%20fingerprinting%20process%2C%20and%20preserves%20the%20model%27s%20normal%20behavior.%0AThe%20project%20page%20is%20available%20at%20https%3A//fingerprintvector.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFP-VEC%253A%2520Fingerprinting%2520Large%2520Language%2520Models%2520via%2520Efficient%2520Vector%250A%2520%2520Addition%26entry.906535625%3DZhenhua%2520Xu%2520and%2520Wenpeng%2520Xing%2520and%2520Zhebo%2520Wang%2520and%2520Chang%2520Hu%2520and%2520Chen%2520Jie%2520and%2520Meng%2520Han%26entry.1292438233%3D%2520%2520Training%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520requires%2520immense%2520computational%2520power%250Aand%2520vast%2520amounts%2520of%2520data.%2520As%2520a%2520result%252C%2520protecting%2520the%2520intellectual%2520property%2520of%250Athese%2520models%2520through%2520fingerprinting%2520is%2520essential%2520for%2520ownership%2520authentication.%250AWhile%2520adding%2520fingerprints%2520to%2520LLMs%2520through%2520fine-tuning%2520has%2520been%2520attempted%252C%2520it%250Aremains%2520costly%2520and%2520unscalable.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520FP-VEC%252C%2520a%2520pilot%250Astudy%2520on%2520using%2520fingerprint%2520vectors%2520as%2520an%2520efficient%2520fingerprinting%2520method%2520for%250ALLMs.%2520Our%2520approach%2520generates%2520a%2520fingerprint%2520vector%2520that%2520represents%2520a%250Aconfidential%2520signature%2520embedded%2520in%2520the%2520model%252C%2520allowing%2520the%2520same%2520fingerprint%2520to%250Abe%2520seamlessly%2520incorporated%2520into%2520an%2520unlimited%2520number%2520of%2520LLMs%2520via%2520vector%250Aaddition.%2520Results%2520on%2520several%2520LLMs%2520show%2520that%2520FP-VEC%2520is%2520lightweight%2520by%2520running%2520on%250ACPU-only%2520devices%2520for%2520fingerprinting%252C%2520scalable%2520with%2520a%2520single%2520training%2520and%250Aunlimited%2520fingerprinting%2520process%252C%2520and%2520preserves%2520the%2520model%2527s%2520normal%2520behavior.%250AThe%2520project%2520page%2520is%2520available%2520at%2520https%253A//fingerprintvector.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FP-VEC%3A%20Fingerprinting%20Large%20Language%20Models%20via%20Efficient%20Vector%0A%20%20Addition&entry.906535625=Zhenhua%20Xu%20and%20Wenpeng%20Xing%20and%20Zhebo%20Wang%20and%20Chang%20Hu%20and%20Chen%20Jie%20and%20Meng%20Han&entry.1292438233=%20%20Training%20Large%20Language%20Models%20%28LLMs%29%20requires%20immense%20computational%20power%0Aand%20vast%20amounts%20of%20data.%20As%20a%20result%2C%20protecting%20the%20intellectual%20property%20of%0Athese%20models%20through%20fingerprinting%20is%20essential%20for%20ownership%20authentication.%0AWhile%20adding%20fingerprints%20to%20LLMs%20through%20fine-tuning%20has%20been%20attempted%2C%20it%0Aremains%20costly%20and%20unscalable.%20In%20this%20paper%2C%20we%20introduce%20FP-VEC%2C%20a%20pilot%0Astudy%20on%20using%20fingerprint%20vectors%20as%20an%20efficient%20fingerprinting%20method%20for%0ALLMs.%20Our%20approach%20generates%20a%20fingerprint%20vector%20that%20represents%20a%0Aconfidential%20signature%20embedded%20in%20the%20model%2C%20allowing%20the%20same%20fingerprint%20to%0Abe%20seamlessly%20incorporated%20into%20an%20unlimited%20number%20of%20LLMs%20via%20vector%0Aaddition.%20Results%20on%20several%20LLMs%20show%20that%20FP-VEC%20is%20lightweight%20by%20running%20on%0ACPU-only%20devices%20for%20fingerprinting%2C%20scalable%20with%20a%20single%20training%20and%0Aunlimited%20fingerprinting%20process%2C%20and%20preserves%20the%20model%27s%20normal%20behavior.%0AThe%20project%20page%20is%20available%20at%20https%3A//fingerprintvector.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08846v1&entry.124074799=Read"},
{"title": "A RAG Approach for Generating Competency Questions in Ontology\n  Engineering", "author": "Xueli Pan and Jacco van Ossenbruggen and Victor de Boer and Zhisheng Huang", "abstract": "  Competency question (CQ) formulation is central to several ontology\ndevelopment and evaluation methodologies. Traditionally, the task of crafting\nthese competency questions heavily relies on the effort of domain experts and\nknowledge engineers which is often time-consuming and labor-intensive. With the\nemergence of Large Language Models (LLMs), there arises the possibility to\nautomate and enhance this process. Unlike other similar works which use\nexisting ontologies or knowledge graphs as input to LLMs, we present a\nretrieval-augmented generation (RAG) approach that uses LLMs for the automatic\ngeneration of CQs given a set of scientific papers considered to be a domain\nknowledge base. We investigate its performance and specifically, we study the\nimpact of different number of papers to the RAG and different temperature\nsetting of the LLM. We conduct experiments using GPT-4 on two domain ontology\nengineering tasks and compare results against ground-truth CQs constructed by\ndomain experts. Empirical assessments on the results, utilizing evaluation\nmetrics (precision and consistency), reveal that compared to zero-shot\nprompting, adding relevant domain knowledge to the RAG improves the performance\nof LLMs on generating CQs for concrete ontology engineering tasks.\n", "link": "http://arxiv.org/abs/2409.08820v1", "date": "2024-09-13", "relevancy": 1.8626, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5124}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20RAG%20Approach%20for%20Generating%20Competency%20Questions%20in%20Ontology%0A%20%20Engineering&body=Title%3A%20A%20RAG%20Approach%20for%20Generating%20Competency%20Questions%20in%20Ontology%0A%20%20Engineering%0AAuthor%3A%20Xueli%20Pan%20and%20Jacco%20van%20Ossenbruggen%20and%20Victor%20de%20Boer%20and%20Zhisheng%20Huang%0AAbstract%3A%20%20%20Competency%20question%20%28CQ%29%20formulation%20is%20central%20to%20several%20ontology%0Adevelopment%20and%20evaluation%20methodologies.%20Traditionally%2C%20the%20task%20of%20crafting%0Athese%20competency%20questions%20heavily%20relies%20on%20the%20effort%20of%20domain%20experts%20and%0Aknowledge%20engineers%20which%20is%20often%20time-consuming%20and%20labor-intensive.%20With%20the%0Aemergence%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20there%20arises%20the%20possibility%20to%0Aautomate%20and%20enhance%20this%20process.%20Unlike%20other%20similar%20works%20which%20use%0Aexisting%20ontologies%20or%20knowledge%20graphs%20as%20input%20to%20LLMs%2C%20we%20present%20a%0Aretrieval-augmented%20generation%20%28RAG%29%20approach%20that%20uses%20LLMs%20for%20the%20automatic%0Ageneration%20of%20CQs%20given%20a%20set%20of%20scientific%20papers%20considered%20to%20be%20a%20domain%0Aknowledge%20base.%20We%20investigate%20its%20performance%20and%20specifically%2C%20we%20study%20the%0Aimpact%20of%20different%20number%20of%20papers%20to%20the%20RAG%20and%20different%20temperature%0Asetting%20of%20the%20LLM.%20We%20conduct%20experiments%20using%20GPT-4%20on%20two%20domain%20ontology%0Aengineering%20tasks%20and%20compare%20results%20against%20ground-truth%20CQs%20constructed%20by%0Adomain%20experts.%20Empirical%20assessments%20on%20the%20results%2C%20utilizing%20evaluation%0Ametrics%20%28precision%20and%20consistency%29%2C%20reveal%20that%20compared%20to%20zero-shot%0Aprompting%2C%20adding%20relevant%20domain%20knowledge%20to%20the%20RAG%20improves%20the%20performance%0Aof%20LLMs%20on%20generating%20CQs%20for%20concrete%20ontology%20engineering%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520RAG%2520Approach%2520for%2520Generating%2520Competency%2520Questions%2520in%2520Ontology%250A%2520%2520Engineering%26entry.906535625%3DXueli%2520Pan%2520and%2520Jacco%2520van%2520Ossenbruggen%2520and%2520Victor%2520de%2520Boer%2520and%2520Zhisheng%2520Huang%26entry.1292438233%3D%2520%2520Competency%2520question%2520%2528CQ%2529%2520formulation%2520is%2520central%2520to%2520several%2520ontology%250Adevelopment%2520and%2520evaluation%2520methodologies.%2520Traditionally%252C%2520the%2520task%2520of%2520crafting%250Athese%2520competency%2520questions%2520heavily%2520relies%2520on%2520the%2520effort%2520of%2520domain%2520experts%2520and%250Aknowledge%2520engineers%2520which%2520is%2520often%2520time-consuming%2520and%2520labor-intensive.%2520With%2520the%250Aemergence%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520there%2520arises%2520the%2520possibility%2520to%250Aautomate%2520and%2520enhance%2520this%2520process.%2520Unlike%2520other%2520similar%2520works%2520which%2520use%250Aexisting%2520ontologies%2520or%2520knowledge%2520graphs%2520as%2520input%2520to%2520LLMs%252C%2520we%2520present%2520a%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520approach%2520that%2520uses%2520LLMs%2520for%2520the%2520automatic%250Ageneration%2520of%2520CQs%2520given%2520a%2520set%2520of%2520scientific%2520papers%2520considered%2520to%2520be%2520a%2520domain%250Aknowledge%2520base.%2520We%2520investigate%2520its%2520performance%2520and%2520specifically%252C%2520we%2520study%2520the%250Aimpact%2520of%2520different%2520number%2520of%2520papers%2520to%2520the%2520RAG%2520and%2520different%2520temperature%250Asetting%2520of%2520the%2520LLM.%2520We%2520conduct%2520experiments%2520using%2520GPT-4%2520on%2520two%2520domain%2520ontology%250Aengineering%2520tasks%2520and%2520compare%2520results%2520against%2520ground-truth%2520CQs%2520constructed%2520by%250Adomain%2520experts.%2520Empirical%2520assessments%2520on%2520the%2520results%252C%2520utilizing%2520evaluation%250Ametrics%2520%2528precision%2520and%2520consistency%2529%252C%2520reveal%2520that%2520compared%2520to%2520zero-shot%250Aprompting%252C%2520adding%2520relevant%2520domain%2520knowledge%2520to%2520the%2520RAG%2520improves%2520the%2520performance%250Aof%2520LLMs%2520on%2520generating%2520CQs%2520for%2520concrete%2520ontology%2520engineering%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20RAG%20Approach%20for%20Generating%20Competency%20Questions%20in%20Ontology%0A%20%20Engineering&entry.906535625=Xueli%20Pan%20and%20Jacco%20van%20Ossenbruggen%20and%20Victor%20de%20Boer%20and%20Zhisheng%20Huang&entry.1292438233=%20%20Competency%20question%20%28CQ%29%20formulation%20is%20central%20to%20several%20ontology%0Adevelopment%20and%20evaluation%20methodologies.%20Traditionally%2C%20the%20task%20of%20crafting%0Athese%20competency%20questions%20heavily%20relies%20on%20the%20effort%20of%20domain%20experts%20and%0Aknowledge%20engineers%20which%20is%20often%20time-consuming%20and%20labor-intensive.%20With%20the%0Aemergence%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20there%20arises%20the%20possibility%20to%0Aautomate%20and%20enhance%20this%20process.%20Unlike%20other%20similar%20works%20which%20use%0Aexisting%20ontologies%20or%20knowledge%20graphs%20as%20input%20to%20LLMs%2C%20we%20present%20a%0Aretrieval-augmented%20generation%20%28RAG%29%20approach%20that%20uses%20LLMs%20for%20the%20automatic%0Ageneration%20of%20CQs%20given%20a%20set%20of%20scientific%20papers%20considered%20to%20be%20a%20domain%0Aknowledge%20base.%20We%20investigate%20its%20performance%20and%20specifically%2C%20we%20study%20the%0Aimpact%20of%20different%20number%20of%20papers%20to%20the%20RAG%20and%20different%20temperature%0Asetting%20of%20the%20LLM.%20We%20conduct%20experiments%20using%20GPT-4%20on%20two%20domain%20ontology%0Aengineering%20tasks%20and%20compare%20results%20against%20ground-truth%20CQs%20constructed%20by%0Adomain%20experts.%20Empirical%20assessments%20on%20the%20results%2C%20utilizing%20evaluation%0Ametrics%20%28precision%20and%20consistency%29%2C%20reveal%20that%20compared%20to%20zero-shot%0Aprompting%2C%20adding%20relevant%20domain%20knowledge%20to%20the%20RAG%20improves%20the%20performance%0Aof%20LLMs%20on%20generating%20CQs%20for%20concrete%20ontology%20engineering%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08820v1&entry.124074799=Read"},
{"title": "XSub: Explanation-Driven Adversarial Attack against Blackbox Classifiers\n  via Feature Substitution", "author": "Kiana Vu and Phung Lai and Truc Nguyen", "abstract": "  Despite its significant benefits in enhancing the transparency and\ntrustworthiness of artificial intelligence (AI) systems, explainable AI (XAI)\nhas yet to reach its full potential in real-world applications. One key\nchallenge is that XAI can unintentionally provide adversaries with insights\ninto black-box models, inevitably increasing their vulnerability to various\nattacks. In this paper, we develop a novel explanation-driven adversarial\nattack against black-box classifiers based on feature substitution, called\nXSub. The key idea of XSub is to strategically replace important features\n(identified via XAI) in the original sample with corresponding important\nfeatures from a \"golden sample\" of a different label, thereby increasing the\nlikelihood of the model misclassifying the perturbed sample. The degree of\nfeature substitution is adjustable, allowing us to control how much of the\noriginal samples information is replaced. This flexibility effectively balances\na trade-off between the attacks effectiveness and its stealthiness. XSub is\nalso highly cost-effective in that the number of required queries to the\nprediction model and the explanation model in conducting the attack is in O(1).\nIn addition, XSub can be easily extended to launch backdoor attacks in case the\nattacker has access to the models training data. Our evaluation demonstrates\nthat XSub is not only effective and stealthy but also cost-effective, enabling\nits application across a wide range of AI models.\n", "link": "http://arxiv.org/abs/2409.08919v1", "date": "2024-09-13", "relevancy": 1.8623, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5198}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4324}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XSub%3A%20Explanation-Driven%20Adversarial%20Attack%20against%20Blackbox%20Classifiers%0A%20%20via%20Feature%20Substitution&body=Title%3A%20XSub%3A%20Explanation-Driven%20Adversarial%20Attack%20against%20Blackbox%20Classifiers%0A%20%20via%20Feature%20Substitution%0AAuthor%3A%20Kiana%20Vu%20and%20Phung%20Lai%20and%20Truc%20Nguyen%0AAbstract%3A%20%20%20Despite%20its%20significant%20benefits%20in%20enhancing%20the%20transparency%20and%0Atrustworthiness%20of%20artificial%20intelligence%20%28AI%29%20systems%2C%20explainable%20AI%20%28XAI%29%0Ahas%20yet%20to%20reach%20its%20full%20potential%20in%20real-world%20applications.%20One%20key%0Achallenge%20is%20that%20XAI%20can%20unintentionally%20provide%20adversaries%20with%20insights%0Ainto%20black-box%20models%2C%20inevitably%20increasing%20their%20vulnerability%20to%20various%0Aattacks.%20In%20this%20paper%2C%20we%20develop%20a%20novel%20explanation-driven%20adversarial%0Aattack%20against%20black-box%20classifiers%20based%20on%20feature%20substitution%2C%20called%0AXSub.%20The%20key%20idea%20of%20XSub%20is%20to%20strategically%20replace%20important%20features%0A%28identified%20via%20XAI%29%20in%20the%20original%20sample%20with%20corresponding%20important%0Afeatures%20from%20a%20%22golden%20sample%22%20of%20a%20different%20label%2C%20thereby%20increasing%20the%0Alikelihood%20of%20the%20model%20misclassifying%20the%20perturbed%20sample.%20The%20degree%20of%0Afeature%20substitution%20is%20adjustable%2C%20allowing%20us%20to%20control%20how%20much%20of%20the%0Aoriginal%20samples%20information%20is%20replaced.%20This%20flexibility%20effectively%20balances%0Aa%20trade-off%20between%20the%20attacks%20effectiveness%20and%20its%20stealthiness.%20XSub%20is%0Aalso%20highly%20cost-effective%20in%20that%20the%20number%20of%20required%20queries%20to%20the%0Aprediction%20model%20and%20the%20explanation%20model%20in%20conducting%20the%20attack%20is%20in%20O%281%29.%0AIn%20addition%2C%20XSub%20can%20be%20easily%20extended%20to%20launch%20backdoor%20attacks%20in%20case%20the%0Aattacker%20has%20access%20to%20the%20models%20training%20data.%20Our%20evaluation%20demonstrates%0Athat%20XSub%20is%20not%20only%20effective%20and%20stealthy%20but%20also%20cost-effective%2C%20enabling%0Aits%20application%20across%20a%20wide%20range%20of%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXSub%253A%2520Explanation-Driven%2520Adversarial%2520Attack%2520against%2520Blackbox%2520Classifiers%250A%2520%2520via%2520Feature%2520Substitution%26entry.906535625%3DKiana%2520Vu%2520and%2520Phung%2520Lai%2520and%2520Truc%2520Nguyen%26entry.1292438233%3D%2520%2520Despite%2520its%2520significant%2520benefits%2520in%2520enhancing%2520the%2520transparency%2520and%250Atrustworthiness%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%252C%2520explainable%2520AI%2520%2528XAI%2529%250Ahas%2520yet%2520to%2520reach%2520its%2520full%2520potential%2520in%2520real-world%2520applications.%2520One%2520key%250Achallenge%2520is%2520that%2520XAI%2520can%2520unintentionally%2520provide%2520adversaries%2520with%2520insights%250Ainto%2520black-box%2520models%252C%2520inevitably%2520increasing%2520their%2520vulnerability%2520to%2520various%250Aattacks.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%2520explanation-driven%2520adversarial%250Aattack%2520against%2520black-box%2520classifiers%2520based%2520on%2520feature%2520substitution%252C%2520called%250AXSub.%2520The%2520key%2520idea%2520of%2520XSub%2520is%2520to%2520strategically%2520replace%2520important%2520features%250A%2528identified%2520via%2520XAI%2529%2520in%2520the%2520original%2520sample%2520with%2520corresponding%2520important%250Afeatures%2520from%2520a%2520%2522golden%2520sample%2522%2520of%2520a%2520different%2520label%252C%2520thereby%2520increasing%2520the%250Alikelihood%2520of%2520the%2520model%2520misclassifying%2520the%2520perturbed%2520sample.%2520The%2520degree%2520of%250Afeature%2520substitution%2520is%2520adjustable%252C%2520allowing%2520us%2520to%2520control%2520how%2520much%2520of%2520the%250Aoriginal%2520samples%2520information%2520is%2520replaced.%2520This%2520flexibility%2520effectively%2520balances%250Aa%2520trade-off%2520between%2520the%2520attacks%2520effectiveness%2520and%2520its%2520stealthiness.%2520XSub%2520is%250Aalso%2520highly%2520cost-effective%2520in%2520that%2520the%2520number%2520of%2520required%2520queries%2520to%2520the%250Aprediction%2520model%2520and%2520the%2520explanation%2520model%2520in%2520conducting%2520the%2520attack%2520is%2520in%2520O%25281%2529.%250AIn%2520addition%252C%2520XSub%2520can%2520be%2520easily%2520extended%2520to%2520launch%2520backdoor%2520attacks%2520in%2520case%2520the%250Aattacker%2520has%2520access%2520to%2520the%2520models%2520training%2520data.%2520Our%2520evaluation%2520demonstrates%250Athat%2520XSub%2520is%2520not%2520only%2520effective%2520and%2520stealthy%2520but%2520also%2520cost-effective%252C%2520enabling%250Aits%2520application%2520across%2520a%2520wide%2520range%2520of%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XSub%3A%20Explanation-Driven%20Adversarial%20Attack%20against%20Blackbox%20Classifiers%0A%20%20via%20Feature%20Substitution&entry.906535625=Kiana%20Vu%20and%20Phung%20Lai%20and%20Truc%20Nguyen&entry.1292438233=%20%20Despite%20its%20significant%20benefits%20in%20enhancing%20the%20transparency%20and%0Atrustworthiness%20of%20artificial%20intelligence%20%28AI%29%20systems%2C%20explainable%20AI%20%28XAI%29%0Ahas%20yet%20to%20reach%20its%20full%20potential%20in%20real-world%20applications.%20One%20key%0Achallenge%20is%20that%20XAI%20can%20unintentionally%20provide%20adversaries%20with%20insights%0Ainto%20black-box%20models%2C%20inevitably%20increasing%20their%20vulnerability%20to%20various%0Aattacks.%20In%20this%20paper%2C%20we%20develop%20a%20novel%20explanation-driven%20adversarial%0Aattack%20against%20black-box%20classifiers%20based%20on%20feature%20substitution%2C%20called%0AXSub.%20The%20key%20idea%20of%20XSub%20is%20to%20strategically%20replace%20important%20features%0A%28identified%20via%20XAI%29%20in%20the%20original%20sample%20with%20corresponding%20important%0Afeatures%20from%20a%20%22golden%20sample%22%20of%20a%20different%20label%2C%20thereby%20increasing%20the%0Alikelihood%20of%20the%20model%20misclassifying%20the%20perturbed%20sample.%20The%20degree%20of%0Afeature%20substitution%20is%20adjustable%2C%20allowing%20us%20to%20control%20how%20much%20of%20the%0Aoriginal%20samples%20information%20is%20replaced.%20This%20flexibility%20effectively%20balances%0Aa%20trade-off%20between%20the%20attacks%20effectiveness%20and%20its%20stealthiness.%20XSub%20is%0Aalso%20highly%20cost-effective%20in%20that%20the%20number%20of%20required%20queries%20to%20the%0Aprediction%20model%20and%20the%20explanation%20model%20in%20conducting%20the%20attack%20is%20in%20O%281%29.%0AIn%20addition%2C%20XSub%20can%20be%20easily%20extended%20to%20launch%20backdoor%20attacks%20in%20case%20the%0Aattacker%20has%20access%20to%20the%20models%20training%20data.%20Our%20evaluation%20demonstrates%0Athat%20XSub%20is%20not%20only%20effective%20and%20stealthy%20but%20also%20cost-effective%2C%20enabling%0Aits%20application%20across%20a%20wide%20range%20of%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08919v1&entry.124074799=Read"},
{"title": "Four Facets of Forecast Felicity: Calibration, Predictiveness,\n  Randomness and Regret", "author": "Rabanus Derr and Robert C. Williamson", "abstract": "  Machine learning is about forecasting. Forecasts, however, obtain their\nusefulness only through their evaluation. Machine learning has traditionally\nfocused on types of losses and their corresponding regret. Currently, the\nmachine learning community regained interest in calibration. In this work, we\nshow the conceptual equivalence of calibration and regret in evaluating\nforecasts. We frame the evaluation problem as a game between a forecaster, a\ngambler and nature. Putting intuitive restrictions on gambler and forecaster,\ncalibration and regret naturally fall out of the framework. In addition, this\ngame links evaluation of forecasts to randomness of outcomes. Random outcomes\nwith respect to forecasts are equivalent to good forecasts with respect to\noutcomes. We call those dual aspects, calibration and regret, predictiveness\nand randomness, the four facets of forecast felicity.\n", "link": "http://arxiv.org/abs/2401.14483v2", "date": "2024-09-13", "relevancy": 1.8368, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4703}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Four%20Facets%20of%20Forecast%20Felicity%3A%20Calibration%2C%20Predictiveness%2C%0A%20%20Randomness%20and%20Regret&body=Title%3A%20Four%20Facets%20of%20Forecast%20Felicity%3A%20Calibration%2C%20Predictiveness%2C%0A%20%20Randomness%20and%20Regret%0AAuthor%3A%20Rabanus%20Derr%20and%20Robert%20C.%20Williamson%0AAbstract%3A%20%20%20Machine%20learning%20is%20about%20forecasting.%20Forecasts%2C%20however%2C%20obtain%20their%0Ausefulness%20only%20through%20their%20evaluation.%20Machine%20learning%20has%20traditionally%0Afocused%20on%20types%20of%20losses%20and%20their%20corresponding%20regret.%20Currently%2C%20the%0Amachine%20learning%20community%20regained%20interest%20in%20calibration.%20In%20this%20work%2C%20we%0Ashow%20the%20conceptual%20equivalence%20of%20calibration%20and%20regret%20in%20evaluating%0Aforecasts.%20We%20frame%20the%20evaluation%20problem%20as%20a%20game%20between%20a%20forecaster%2C%20a%0Agambler%20and%20nature.%20Putting%20intuitive%20restrictions%20on%20gambler%20and%20forecaster%2C%0Acalibration%20and%20regret%20naturally%20fall%20out%20of%20the%20framework.%20In%20addition%2C%20this%0Agame%20links%20evaluation%20of%20forecasts%20to%20randomness%20of%20outcomes.%20Random%20outcomes%0Awith%20respect%20to%20forecasts%20are%20equivalent%20to%20good%20forecasts%20with%20respect%20to%0Aoutcomes.%20We%20call%20those%20dual%20aspects%2C%20calibration%20and%20regret%2C%20predictiveness%0Aand%20randomness%2C%20the%20four%20facets%20of%20forecast%20felicity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFour%2520Facets%2520of%2520Forecast%2520Felicity%253A%2520Calibration%252C%2520Predictiveness%252C%250A%2520%2520Randomness%2520and%2520Regret%26entry.906535625%3DRabanus%2520Derr%2520and%2520Robert%2520C.%2520Williamson%26entry.1292438233%3D%2520%2520Machine%2520learning%2520is%2520about%2520forecasting.%2520Forecasts%252C%2520however%252C%2520obtain%2520their%250Ausefulness%2520only%2520through%2520their%2520evaluation.%2520Machine%2520learning%2520has%2520traditionally%250Afocused%2520on%2520types%2520of%2520losses%2520and%2520their%2520corresponding%2520regret.%2520Currently%252C%2520the%250Amachine%2520learning%2520community%2520regained%2520interest%2520in%2520calibration.%2520In%2520this%2520work%252C%2520we%250Ashow%2520the%2520conceptual%2520equivalence%2520of%2520calibration%2520and%2520regret%2520in%2520evaluating%250Aforecasts.%2520We%2520frame%2520the%2520evaluation%2520problem%2520as%2520a%2520game%2520between%2520a%2520forecaster%252C%2520a%250Agambler%2520and%2520nature.%2520Putting%2520intuitive%2520restrictions%2520on%2520gambler%2520and%2520forecaster%252C%250Acalibration%2520and%2520regret%2520naturally%2520fall%2520out%2520of%2520the%2520framework.%2520In%2520addition%252C%2520this%250Agame%2520links%2520evaluation%2520of%2520forecasts%2520to%2520randomness%2520of%2520outcomes.%2520Random%2520outcomes%250Awith%2520respect%2520to%2520forecasts%2520are%2520equivalent%2520to%2520good%2520forecasts%2520with%2520respect%2520to%250Aoutcomes.%2520We%2520call%2520those%2520dual%2520aspects%252C%2520calibration%2520and%2520regret%252C%2520predictiveness%250Aand%2520randomness%252C%2520the%2520four%2520facets%2520of%2520forecast%2520felicity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Four%20Facets%20of%20Forecast%20Felicity%3A%20Calibration%2C%20Predictiveness%2C%0A%20%20Randomness%20and%20Regret&entry.906535625=Rabanus%20Derr%20and%20Robert%20C.%20Williamson&entry.1292438233=%20%20Machine%20learning%20is%20about%20forecasting.%20Forecasts%2C%20however%2C%20obtain%20their%0Ausefulness%20only%20through%20their%20evaluation.%20Machine%20learning%20has%20traditionally%0Afocused%20on%20types%20of%20losses%20and%20their%20corresponding%20regret.%20Currently%2C%20the%0Amachine%20learning%20community%20regained%20interest%20in%20calibration.%20In%20this%20work%2C%20we%0Ashow%20the%20conceptual%20equivalence%20of%20calibration%20and%20regret%20in%20evaluating%0Aforecasts.%20We%20frame%20the%20evaluation%20problem%20as%20a%20game%20between%20a%20forecaster%2C%20a%0Agambler%20and%20nature.%20Putting%20intuitive%20restrictions%20on%20gambler%20and%20forecaster%2C%0Acalibration%20and%20regret%20naturally%20fall%20out%20of%20the%20framework.%20In%20addition%2C%20this%0Agame%20links%20evaluation%20of%20forecasts%20to%20randomness%20of%20outcomes.%20Random%20outcomes%0Awith%20respect%20to%20forecasts%20are%20equivalent%20to%20good%20forecasts%20with%20respect%20to%0Aoutcomes.%20We%20call%20those%20dual%20aspects%2C%20calibration%20and%20regret%2C%20predictiveness%0Aand%20randomness%2C%20the%20four%20facets%20of%20forecast%20felicity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14483v2&entry.124074799=Read"},
{"title": "Online Network Inference from Graph-Stationary Signals with Hidden Nodes", "author": "Andrei Buciulea and Madeline Navarro and Samuel Rey and Santiago Segarra and Antonio G. Marques", "abstract": "  Graph learning is the fundamental task of estimating unknown graph\nconnectivity from available data. Typical approaches assume that not only is\nall information available simultaneously but also that all nodes can be\nobserved. However, in many real-world scenarios, data can neither be known\ncompletely nor obtained all at once. We present a novel method for online graph\nestimation that accounts for the presence of hidden nodes. We consider signals\nthat are stationary on the underlying graph, which provides a model for the\nunknown connections to hidden nodes. We then formulate a convex optimization\nproblem for graph learning from streaming, incomplete graph signals. We solve\nthe proposed problem through an efficient proximal gradient algorithm that can\nrun in real-time as data arrives sequentially. Additionally, we provide\ntheoretical conditions under which our online algorithm is similar to\nbatch-wise solutions. Through experimental results on synthetic and real-world\ndata, we demonstrate the viability of our approach for online graph learning in\nthe presence of missing observations.\n", "link": "http://arxiv.org/abs/2409.08760v1", "date": "2024-09-13", "relevancy": 1.8366, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.463}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4596}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Network%20Inference%20from%20Graph-Stationary%20Signals%20with%20Hidden%20Nodes&body=Title%3A%20Online%20Network%20Inference%20from%20Graph-Stationary%20Signals%20with%20Hidden%20Nodes%0AAuthor%3A%20Andrei%20Buciulea%20and%20Madeline%20Navarro%20and%20Samuel%20Rey%20and%20Santiago%20Segarra%20and%20Antonio%20G.%20Marques%0AAbstract%3A%20%20%20Graph%20learning%20is%20the%20fundamental%20task%20of%20estimating%20unknown%20graph%0Aconnectivity%20from%20available%20data.%20Typical%20approaches%20assume%20that%20not%20only%20is%0Aall%20information%20available%20simultaneously%20but%20also%20that%20all%20nodes%20can%20be%0Aobserved.%20However%2C%20in%20many%20real-world%20scenarios%2C%20data%20can%20neither%20be%20known%0Acompletely%20nor%20obtained%20all%20at%20once.%20We%20present%20a%20novel%20method%20for%20online%20graph%0Aestimation%20that%20accounts%20for%20the%20presence%20of%20hidden%20nodes.%20We%20consider%20signals%0Athat%20are%20stationary%20on%20the%20underlying%20graph%2C%20which%20provides%20a%20model%20for%20the%0Aunknown%20connections%20to%20hidden%20nodes.%20We%20then%20formulate%20a%20convex%20optimization%0Aproblem%20for%20graph%20learning%20from%20streaming%2C%20incomplete%20graph%20signals.%20We%20solve%0Athe%20proposed%20problem%20through%20an%20efficient%20proximal%20gradient%20algorithm%20that%20can%0Arun%20in%20real-time%20as%20data%20arrives%20sequentially.%20Additionally%2C%20we%20provide%0Atheoretical%20conditions%20under%20which%20our%20online%20algorithm%20is%20similar%20to%0Abatch-wise%20solutions.%20Through%20experimental%20results%20on%20synthetic%20and%20real-world%0Adata%2C%20we%20demonstrate%20the%20viability%20of%20our%20approach%20for%20online%20graph%20learning%20in%0Athe%20presence%20of%20missing%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Network%2520Inference%2520from%2520Graph-Stationary%2520Signals%2520with%2520Hidden%2520Nodes%26entry.906535625%3DAndrei%2520Buciulea%2520and%2520Madeline%2520Navarro%2520and%2520Samuel%2520Rey%2520and%2520Santiago%2520Segarra%2520and%2520Antonio%2520G.%2520Marques%26entry.1292438233%3D%2520%2520Graph%2520learning%2520is%2520the%2520fundamental%2520task%2520of%2520estimating%2520unknown%2520graph%250Aconnectivity%2520from%2520available%2520data.%2520Typical%2520approaches%2520assume%2520that%2520not%2520only%2520is%250Aall%2520information%2520available%2520simultaneously%2520but%2520also%2520that%2520all%2520nodes%2520can%2520be%250Aobserved.%2520However%252C%2520in%2520many%2520real-world%2520scenarios%252C%2520data%2520can%2520neither%2520be%2520known%250Acompletely%2520nor%2520obtained%2520all%2520at%2520once.%2520We%2520present%2520a%2520novel%2520method%2520for%2520online%2520graph%250Aestimation%2520that%2520accounts%2520for%2520the%2520presence%2520of%2520hidden%2520nodes.%2520We%2520consider%2520signals%250Athat%2520are%2520stationary%2520on%2520the%2520underlying%2520graph%252C%2520which%2520provides%2520a%2520model%2520for%2520the%250Aunknown%2520connections%2520to%2520hidden%2520nodes.%2520We%2520then%2520formulate%2520a%2520convex%2520optimization%250Aproblem%2520for%2520graph%2520learning%2520from%2520streaming%252C%2520incomplete%2520graph%2520signals.%2520We%2520solve%250Athe%2520proposed%2520problem%2520through%2520an%2520efficient%2520proximal%2520gradient%2520algorithm%2520that%2520can%250Arun%2520in%2520real-time%2520as%2520data%2520arrives%2520sequentially.%2520Additionally%252C%2520we%2520provide%250Atheoretical%2520conditions%2520under%2520which%2520our%2520online%2520algorithm%2520is%2520similar%2520to%250Abatch-wise%2520solutions.%2520Through%2520experimental%2520results%2520on%2520synthetic%2520and%2520real-world%250Adata%252C%2520we%2520demonstrate%2520the%2520viability%2520of%2520our%2520approach%2520for%2520online%2520graph%2520learning%2520in%250Athe%2520presence%2520of%2520missing%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Network%20Inference%20from%20Graph-Stationary%20Signals%20with%20Hidden%20Nodes&entry.906535625=Andrei%20Buciulea%20and%20Madeline%20Navarro%20and%20Samuel%20Rey%20and%20Santiago%20Segarra%20and%20Antonio%20G.%20Marques&entry.1292438233=%20%20Graph%20learning%20is%20the%20fundamental%20task%20of%20estimating%20unknown%20graph%0Aconnectivity%20from%20available%20data.%20Typical%20approaches%20assume%20that%20not%20only%20is%0Aall%20information%20available%20simultaneously%20but%20also%20that%20all%20nodes%20can%20be%0Aobserved.%20However%2C%20in%20many%20real-world%20scenarios%2C%20data%20can%20neither%20be%20known%0Acompletely%20nor%20obtained%20all%20at%20once.%20We%20present%20a%20novel%20method%20for%20online%20graph%0Aestimation%20that%20accounts%20for%20the%20presence%20of%20hidden%20nodes.%20We%20consider%20signals%0Athat%20are%20stationary%20on%20the%20underlying%20graph%2C%20which%20provides%20a%20model%20for%20the%0Aunknown%20connections%20to%20hidden%20nodes.%20We%20then%20formulate%20a%20convex%20optimization%0Aproblem%20for%20graph%20learning%20from%20streaming%2C%20incomplete%20graph%20signals.%20We%20solve%0Athe%20proposed%20problem%20through%20an%20efficient%20proximal%20gradient%20algorithm%20that%20can%0Arun%20in%20real-time%20as%20data%20arrives%20sequentially.%20Additionally%2C%20we%20provide%0Atheoretical%20conditions%20under%20which%20our%20online%20algorithm%20is%20similar%20to%0Abatch-wise%20solutions.%20Through%20experimental%20results%20on%20synthetic%20and%20real-world%0Adata%2C%20we%20demonstrate%20the%20viability%20of%20our%20approach%20for%20online%20graph%20learning%20in%0Athe%20presence%20of%20missing%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08760v1&entry.124074799=Read"},
{"title": "Closed-Loop Visuomotor Control with Generative Expectation for Robotic\n  Manipulation", "author": "Qingwen Bu and Jia Zeng and Li Chen and Yanchao Yang and Guyue Zhou and Junchi Yan and Ping Luo and Heming Cui and Yi Ma and Hongyang Li", "abstract": "  Despite significant progress in robotics and embodied AI in recent years,\ndeploying robots for long-horizon tasks remains a great challenge. Majority of\nprior arts adhere to an open-loop philosophy and lack real-time feedback,\nleading to error accumulation and undesirable robustness. A handful of\napproaches have endeavored to establish feedback mechanisms leveraging\npixel-level differences or pre-trained visual representations, yet their\nefficacy and adaptability have been found to be constrained. Inspired by\nclassic closed-loop control systems, we propose CLOVER, a closed-loop\nvisuomotor control framework that incorporates feedback mechanisms to improve\nadaptive robotic control. CLOVER consists of a text-conditioned video diffusion\nmodel for generating visual plans as reference inputs, a measurable embedding\nspace for accurate error quantification, and a feedback-driven controller that\nrefines actions from feedback and initiates replans as needed. Our framework\nexhibits notable advancement in real-world robotic tasks and achieves\nstate-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop\ncounterparts. Code and checkpoints are maintained at\nhttps://github.com/OpenDriveLab/CLOVER.\n", "link": "http://arxiv.org/abs/2409.09016v1", "date": "2024-09-13", "relevancy": 1.8269, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6245}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6112}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-Loop%20Visuomotor%20Control%20with%20Generative%20Expectation%20for%20Robotic%0A%20%20Manipulation&body=Title%3A%20Closed-Loop%20Visuomotor%20Control%20with%20Generative%20Expectation%20for%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Qingwen%20Bu%20and%20Jia%20Zeng%20and%20Li%20Chen%20and%20Yanchao%20Yang%20and%20Guyue%20Zhou%20and%20Junchi%20Yan%20and%20Ping%20Luo%20and%20Heming%20Cui%20and%20Yi%20Ma%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%20robotics%20and%20embodied%20AI%20in%20recent%20years%2C%0Adeploying%20robots%20for%20long-horizon%20tasks%20remains%20a%20great%20challenge.%20Majority%20of%0Aprior%20arts%20adhere%20to%20an%20open-loop%20philosophy%20and%20lack%20real-time%20feedback%2C%0Aleading%20to%20error%20accumulation%20and%20undesirable%20robustness.%20A%20handful%20of%0Aapproaches%20have%20endeavored%20to%20establish%20feedback%20mechanisms%20leveraging%0Apixel-level%20differences%20or%20pre-trained%20visual%20representations%2C%20yet%20their%0Aefficacy%20and%20adaptability%20have%20been%20found%20to%20be%20constrained.%20Inspired%20by%0Aclassic%20closed-loop%20control%20systems%2C%20we%20propose%20CLOVER%2C%20a%20closed-loop%0Avisuomotor%20control%20framework%20that%20incorporates%20feedback%20mechanisms%20to%20improve%0Aadaptive%20robotic%20control.%20CLOVER%20consists%20of%20a%20text-conditioned%20video%20diffusion%0Amodel%20for%20generating%20visual%20plans%20as%20reference%20inputs%2C%20a%20measurable%20embedding%0Aspace%20for%20accurate%20error%20quantification%2C%20and%20a%20feedback-driven%20controller%20that%0Arefines%20actions%20from%20feedback%20and%20initiates%20replans%20as%20needed.%20Our%20framework%0Aexhibits%20notable%20advancement%20in%20real-world%20robotic%20tasks%20and%20achieves%0Astate-of-the-art%20on%20CALVIN%20benchmark%2C%20improving%20by%208%25%20over%20previous%20open-loop%0Acounterparts.%20Code%20and%20checkpoints%20are%20maintained%20at%0Ahttps%3A//github.com/OpenDriveLab/CLOVER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-Loop%2520Visuomotor%2520Control%2520with%2520Generative%2520Expectation%2520for%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DQingwen%2520Bu%2520and%2520Jia%2520Zeng%2520and%2520Li%2520Chen%2520and%2520Yanchao%2520Yang%2520and%2520Guyue%2520Zhou%2520and%2520Junchi%2520Yan%2520and%2520Ping%2520Luo%2520and%2520Heming%2520Cui%2520and%2520Yi%2520Ma%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%2520robotics%2520and%2520embodied%2520AI%2520in%2520recent%2520years%252C%250Adeploying%2520robots%2520for%2520long-horizon%2520tasks%2520remains%2520a%2520great%2520challenge.%2520Majority%2520of%250Aprior%2520arts%2520adhere%2520to%2520an%2520open-loop%2520philosophy%2520and%2520lack%2520real-time%2520feedback%252C%250Aleading%2520to%2520error%2520accumulation%2520and%2520undesirable%2520robustness.%2520A%2520handful%2520of%250Aapproaches%2520have%2520endeavored%2520to%2520establish%2520feedback%2520mechanisms%2520leveraging%250Apixel-level%2520differences%2520or%2520pre-trained%2520visual%2520representations%252C%2520yet%2520their%250Aefficacy%2520and%2520adaptability%2520have%2520been%2520found%2520to%2520be%2520constrained.%2520Inspired%2520by%250Aclassic%2520closed-loop%2520control%2520systems%252C%2520we%2520propose%2520CLOVER%252C%2520a%2520closed-loop%250Avisuomotor%2520control%2520framework%2520that%2520incorporates%2520feedback%2520mechanisms%2520to%2520improve%250Aadaptive%2520robotic%2520control.%2520CLOVER%2520consists%2520of%2520a%2520text-conditioned%2520video%2520diffusion%250Amodel%2520for%2520generating%2520visual%2520plans%2520as%2520reference%2520inputs%252C%2520a%2520measurable%2520embedding%250Aspace%2520for%2520accurate%2520error%2520quantification%252C%2520and%2520a%2520feedback-driven%2520controller%2520that%250Arefines%2520actions%2520from%2520feedback%2520and%2520initiates%2520replans%2520as%2520needed.%2520Our%2520framework%250Aexhibits%2520notable%2520advancement%2520in%2520real-world%2520robotic%2520tasks%2520and%2520achieves%250Astate-of-the-art%2520on%2520CALVIN%2520benchmark%252C%2520improving%2520by%25208%2525%2520over%2520previous%2520open-loop%250Acounterparts.%2520Code%2520and%2520checkpoints%2520are%2520maintained%2520at%250Ahttps%253A//github.com/OpenDriveLab/CLOVER.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Loop%20Visuomotor%20Control%20with%20Generative%20Expectation%20for%20Robotic%0A%20%20Manipulation&entry.906535625=Qingwen%20Bu%20and%20Jia%20Zeng%20and%20Li%20Chen%20and%20Yanchao%20Yang%20and%20Guyue%20Zhou%20and%20Junchi%20Yan%20and%20Ping%20Luo%20and%20Heming%20Cui%20and%20Yi%20Ma%20and%20Hongyang%20Li&entry.1292438233=%20%20Despite%20significant%20progress%20in%20robotics%20and%20embodied%20AI%20in%20recent%20years%2C%0Adeploying%20robots%20for%20long-horizon%20tasks%20remains%20a%20great%20challenge.%20Majority%20of%0Aprior%20arts%20adhere%20to%20an%20open-loop%20philosophy%20and%20lack%20real-time%20feedback%2C%0Aleading%20to%20error%20accumulation%20and%20undesirable%20robustness.%20A%20handful%20of%0Aapproaches%20have%20endeavored%20to%20establish%20feedback%20mechanisms%20leveraging%0Apixel-level%20differences%20or%20pre-trained%20visual%20representations%2C%20yet%20their%0Aefficacy%20and%20adaptability%20have%20been%20found%20to%20be%20constrained.%20Inspired%20by%0Aclassic%20closed-loop%20control%20systems%2C%20we%20propose%20CLOVER%2C%20a%20closed-loop%0Avisuomotor%20control%20framework%20that%20incorporates%20feedback%20mechanisms%20to%20improve%0Aadaptive%20robotic%20control.%20CLOVER%20consists%20of%20a%20text-conditioned%20video%20diffusion%0Amodel%20for%20generating%20visual%20plans%20as%20reference%20inputs%2C%20a%20measurable%20embedding%0Aspace%20for%20accurate%20error%20quantification%2C%20and%20a%20feedback-driven%20controller%20that%0Arefines%20actions%20from%20feedback%20and%20initiates%20replans%20as%20needed.%20Our%20framework%0Aexhibits%20notable%20advancement%20in%20real-world%20robotic%20tasks%20and%20achieves%0Astate-of-the-art%20on%20CALVIN%20benchmark%2C%20improving%20by%208%25%20over%20previous%20open-loop%0Acounterparts.%20Code%20and%20checkpoints%20are%20maintained%20at%0Ahttps%3A//github.com/OpenDriveLab/CLOVER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09016v1&entry.124074799=Read"},
{"title": "CoverUp: Coverage-Guided LLM-Based Test Generation", "author": "Juan Altmayer Pizzorno and Emery D. Berger", "abstract": "  Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains a challenge. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp iteratively improves test coverage, interleaving\ncoverage analysis with dialogs with the LLM that steer it to refine tests so\nthat they increase coverage of lines and branches. We evaluate our prototype\nCoverUp implementation across a benchmark of challenging code derived from\nopen-source Python projects, and show that CoverUp substantially improves on\nthe state of the art. Compared to CodaMosa, a hybrid search/LLM-based test\ngenerator, CoverUp achieves a per-module median line+branch coverage of 80%\n(vs. 47%). Compared to MuTAP, a mutation/LLM-based test generator, CoverUp\nachieves an overall line+branch coverage of 90% (vs. 77%). We show that\nCoverUp's iterative, coverage-guided approach is crucial to its effectiveness,\ncontributing to nearly 40% of its successes.\n", "link": "http://arxiv.org/abs/2403.16218v2", "date": "2024-09-13", "relevancy": 1.8217, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4706}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4533}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoverUp%3A%20Coverage-Guided%20LLM-Based%20Test%20Generation&body=Title%3A%20CoverUp%3A%20Coverage-Guided%20LLM-Based%20Test%20Generation%0AAuthor%3A%20Juan%20Altmayer%20Pizzorno%20and%20Emery%20D.%20Berger%0AAbstract%3A%20%20%20Testing%20is%20an%20essential%20part%20of%20software%20development.%20Test%20generation%20tools%0Aattempt%20to%20automate%20the%20otherwise%20labor-intensive%20task%20of%20test%20creation%2C%20but%0Agenerating%20high-coverage%20tests%20remains%20a%20challenge.%20This%20paper%20proposes%0ACoverUp%2C%20a%20novel%20approach%20to%20driving%20the%20generation%20of%20high-coverage%20Python%0Aregression%20tests.%20CoverUp%20iteratively%20improves%20test%20coverage%2C%20interleaving%0Acoverage%20analysis%20with%20dialogs%20with%20the%20LLM%20that%20steer%20it%20to%20refine%20tests%20so%0Athat%20they%20increase%20coverage%20of%20lines%20and%20branches.%20We%20evaluate%20our%20prototype%0ACoverUp%20implementation%20across%20a%20benchmark%20of%20challenging%20code%20derived%20from%0Aopen-source%20Python%20projects%2C%20and%20show%20that%20CoverUp%20substantially%20improves%20on%0Athe%20state%20of%20the%20art.%20Compared%20to%20CodaMosa%2C%20a%20hybrid%20search/LLM-based%20test%0Agenerator%2C%20CoverUp%20achieves%20a%20per-module%20median%20line%2Bbranch%20coverage%20of%2080%25%0A%28vs.%2047%25%29.%20Compared%20to%20MuTAP%2C%20a%20mutation/LLM-based%20test%20generator%2C%20CoverUp%0Aachieves%20an%20overall%20line%2Bbranch%20coverage%20of%2090%25%20%28vs.%2077%25%29.%20We%20show%20that%0ACoverUp%27s%20iterative%2C%20coverage-guided%20approach%20is%20crucial%20to%20its%20effectiveness%2C%0Acontributing%20to%20nearly%2040%25%20of%20its%20successes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16218v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoverUp%253A%2520Coverage-Guided%2520LLM-Based%2520Test%2520Generation%26entry.906535625%3DJuan%2520Altmayer%2520Pizzorno%2520and%2520Emery%2520D.%2520Berger%26entry.1292438233%3D%2520%2520Testing%2520is%2520an%2520essential%2520part%2520of%2520software%2520development.%2520Test%2520generation%2520tools%250Aattempt%2520to%2520automate%2520the%2520otherwise%2520labor-intensive%2520task%2520of%2520test%2520creation%252C%2520but%250Agenerating%2520high-coverage%2520tests%2520remains%2520a%2520challenge.%2520This%2520paper%2520proposes%250ACoverUp%252C%2520a%2520novel%2520approach%2520to%2520driving%2520the%2520generation%2520of%2520high-coverage%2520Python%250Aregression%2520tests.%2520CoverUp%2520iteratively%2520improves%2520test%2520coverage%252C%2520interleaving%250Acoverage%2520analysis%2520with%2520dialogs%2520with%2520the%2520LLM%2520that%2520steer%2520it%2520to%2520refine%2520tests%2520so%250Athat%2520they%2520increase%2520coverage%2520of%2520lines%2520and%2520branches.%2520We%2520evaluate%2520our%2520prototype%250ACoverUp%2520implementation%2520across%2520a%2520benchmark%2520of%2520challenging%2520code%2520derived%2520from%250Aopen-source%2520Python%2520projects%252C%2520and%2520show%2520that%2520CoverUp%2520substantially%2520improves%2520on%250Athe%2520state%2520of%2520the%2520art.%2520Compared%2520to%2520CodaMosa%252C%2520a%2520hybrid%2520search/LLM-based%2520test%250Agenerator%252C%2520CoverUp%2520achieves%2520a%2520per-module%2520median%2520line%252Bbranch%2520coverage%2520of%252080%2525%250A%2528vs.%252047%2525%2529.%2520Compared%2520to%2520MuTAP%252C%2520a%2520mutation/LLM-based%2520test%2520generator%252C%2520CoverUp%250Aachieves%2520an%2520overall%2520line%252Bbranch%2520coverage%2520of%252090%2525%2520%2528vs.%252077%2525%2529.%2520We%2520show%2520that%250ACoverUp%2527s%2520iterative%252C%2520coverage-guided%2520approach%2520is%2520crucial%2520to%2520its%2520effectiveness%252C%250Acontributing%2520to%2520nearly%252040%2525%2520of%2520its%2520successes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16218v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoverUp%3A%20Coverage-Guided%20LLM-Based%20Test%20Generation&entry.906535625=Juan%20Altmayer%20Pizzorno%20and%20Emery%20D.%20Berger&entry.1292438233=%20%20Testing%20is%20an%20essential%20part%20of%20software%20development.%20Test%20generation%20tools%0Aattempt%20to%20automate%20the%20otherwise%20labor-intensive%20task%20of%20test%20creation%2C%20but%0Agenerating%20high-coverage%20tests%20remains%20a%20challenge.%20This%20paper%20proposes%0ACoverUp%2C%20a%20novel%20approach%20to%20driving%20the%20generation%20of%20high-coverage%20Python%0Aregression%20tests.%20CoverUp%20iteratively%20improves%20test%20coverage%2C%20interleaving%0Acoverage%20analysis%20with%20dialogs%20with%20the%20LLM%20that%20steer%20it%20to%20refine%20tests%20so%0Athat%20they%20increase%20coverage%20of%20lines%20and%20branches.%20We%20evaluate%20our%20prototype%0ACoverUp%20implementation%20across%20a%20benchmark%20of%20challenging%20code%20derived%20from%0Aopen-source%20Python%20projects%2C%20and%20show%20that%20CoverUp%20substantially%20improves%20on%0Athe%20state%20of%20the%20art.%20Compared%20to%20CodaMosa%2C%20a%20hybrid%20search/LLM-based%20test%0Agenerator%2C%20CoverUp%20achieves%20a%20per-module%20median%20line%2Bbranch%20coverage%20of%2080%25%0A%28vs.%2047%25%29.%20Compared%20to%20MuTAP%2C%20a%20mutation/LLM-based%20test%20generator%2C%20CoverUp%0Aachieves%20an%20overall%20line%2Bbranch%20coverage%20of%2090%25%20%28vs.%2077%25%29.%20We%20show%20that%0ACoverUp%27s%20iterative%2C%20coverage-guided%20approach%20is%20crucial%20to%20its%20effectiveness%2C%0Acontributing%20to%20nearly%2040%25%20of%20its%20successes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16218v2&entry.124074799=Read"},
{"title": "Energy Consumption Trends in Sound Event Detection Systems", "author": "Constance Douwes and Romain Serizel", "abstract": "  Deep learning systems have become increasingly energy- and\ncomputation-intensive, raising concerns about their environmental impact. As\norganizers of the Detection and Classification of Acoustic Scenes and Events\n(DCASE) challenge, we recognize the importance of addressing this issue. For\nthe past three years, we have integrated energy consumption metrics into the\nevaluation of sound event detection (SED) systems. In this paper, we analyze\nthe impact of this energy criterion on the challenge results and explore the\nevolution of system complexity and energy consumption over the years. We\nhighlight a shift towards more energy-efficient approaches during training\nwithout compromising performance, while the number of operations and system\ncomplexity continue to grow. Through this analysis, we hope to promote more\nenvironmentally friendly practices within the SED community.\n", "link": "http://arxiv.org/abs/2409.08763v1", "date": "2024-09-13", "relevancy": 1.82, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy%20Consumption%20Trends%20in%20Sound%20Event%20Detection%20Systems&body=Title%3A%20Energy%20Consumption%20Trends%20in%20Sound%20Event%20Detection%20Systems%0AAuthor%3A%20Constance%20Douwes%20and%20Romain%20Serizel%0AAbstract%3A%20%20%20Deep%20learning%20systems%20have%20become%20increasingly%20energy-%20and%0Acomputation-intensive%2C%20raising%20concerns%20about%20their%20environmental%20impact.%20As%0Aorganizers%20of%20the%20Detection%20and%20Classification%20of%20Acoustic%20Scenes%20and%20Events%0A%28DCASE%29%20challenge%2C%20we%20recognize%20the%20importance%20of%20addressing%20this%20issue.%20For%0Athe%20past%20three%20years%2C%20we%20have%20integrated%20energy%20consumption%20metrics%20into%20the%0Aevaluation%20of%20sound%20event%20detection%20%28SED%29%20systems.%20In%20this%20paper%2C%20we%20analyze%0Athe%20impact%20of%20this%20energy%20criterion%20on%20the%20challenge%20results%20and%20explore%20the%0Aevolution%20of%20system%20complexity%20and%20energy%20consumption%20over%20the%20years.%20We%0Ahighlight%20a%20shift%20towards%20more%20energy-efficient%20approaches%20during%20training%0Awithout%20compromising%20performance%2C%20while%20the%20number%20of%20operations%20and%20system%0Acomplexity%20continue%20to%20grow.%20Through%20this%20analysis%2C%20we%20hope%20to%20promote%20more%0Aenvironmentally%20friendly%20practices%20within%20the%20SED%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy%2520Consumption%2520Trends%2520in%2520Sound%2520Event%2520Detection%2520Systems%26entry.906535625%3DConstance%2520Douwes%2520and%2520Romain%2520Serizel%26entry.1292438233%3D%2520%2520Deep%2520learning%2520systems%2520have%2520become%2520increasingly%2520energy-%2520and%250Acomputation-intensive%252C%2520raising%2520concerns%2520about%2520their%2520environmental%2520impact.%2520As%250Aorganizers%2520of%2520the%2520Detection%2520and%2520Classification%2520of%2520Acoustic%2520Scenes%2520and%2520Events%250A%2528DCASE%2529%2520challenge%252C%2520we%2520recognize%2520the%2520importance%2520of%2520addressing%2520this%2520issue.%2520For%250Athe%2520past%2520three%2520years%252C%2520we%2520have%2520integrated%2520energy%2520consumption%2520metrics%2520into%2520the%250Aevaluation%2520of%2520sound%2520event%2520detection%2520%2528SED%2529%2520systems.%2520In%2520this%2520paper%252C%2520we%2520analyze%250Athe%2520impact%2520of%2520this%2520energy%2520criterion%2520on%2520the%2520challenge%2520results%2520and%2520explore%2520the%250Aevolution%2520of%2520system%2520complexity%2520and%2520energy%2520consumption%2520over%2520the%2520years.%2520We%250Ahighlight%2520a%2520shift%2520towards%2520more%2520energy-efficient%2520approaches%2520during%2520training%250Awithout%2520compromising%2520performance%252C%2520while%2520the%2520number%2520of%2520operations%2520and%2520system%250Acomplexity%2520continue%2520to%2520grow.%2520Through%2520this%2520analysis%252C%2520we%2520hope%2520to%2520promote%2520more%250Aenvironmentally%2520friendly%2520practices%2520within%2520the%2520SED%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy%20Consumption%20Trends%20in%20Sound%20Event%20Detection%20Systems&entry.906535625=Constance%20Douwes%20and%20Romain%20Serizel&entry.1292438233=%20%20Deep%20learning%20systems%20have%20become%20increasingly%20energy-%20and%0Acomputation-intensive%2C%20raising%20concerns%20about%20their%20environmental%20impact.%20As%0Aorganizers%20of%20the%20Detection%20and%20Classification%20of%20Acoustic%20Scenes%20and%20Events%0A%28DCASE%29%20challenge%2C%20we%20recognize%20the%20importance%20of%20addressing%20this%20issue.%20For%0Athe%20past%20three%20years%2C%20we%20have%20integrated%20energy%20consumption%20metrics%20into%20the%0Aevaluation%20of%20sound%20event%20detection%20%28SED%29%20systems.%20In%20this%20paper%2C%20we%20analyze%0Athe%20impact%20of%20this%20energy%20criterion%20on%20the%20challenge%20results%20and%20explore%20the%0Aevolution%20of%20system%20complexity%20and%20energy%20consumption%20over%20the%20years.%20We%0Ahighlight%20a%20shift%20towards%20more%20energy-efficient%20approaches%20during%20training%0Awithout%20compromising%20performance%2C%20while%20the%20number%20of%20operations%20and%20system%0Acomplexity%20continue%20to%20grow.%20Through%20this%20analysis%2C%20we%20hope%20to%20promote%20more%0Aenvironmentally%20friendly%20practices%20within%20the%20SED%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08763v1&entry.124074799=Read"},
{"title": "Synthetic Human Memories: AI-Edited Images and Videos Can Implant False\n  Memories and Distort Recollection", "author": "Pat Pataranutaporn and Chayapatr Archiwaranguprok and Samantha W. T. Chan and Elizabeth Loftus and Pattie Maes", "abstract": "  AI is increasingly used to enhance images and videos, both intentionally and\nunintentionally. As AI editing tools become more integrated into smartphones,\nusers can modify or animate photos into realistic videos. This study examines\nthe impact of AI-altered visuals on false memories--recollections of events\nthat didn't occur or deviate from reality. In a pre-registered study, 200\nparticipants were divided into four conditions of 50 each. Participants viewed\noriginal images, completed a filler task, then saw stimuli corresponding to\ntheir assigned condition: unedited images, AI-edited images, AI-generated\nvideos, or AI-generated videos of AI-edited images. AI-edited visuals\nsignificantly increased false recollections, with AI-generated videos of\nAI-edited images having the strongest effect (2.05x compared to control).\nConfidence in false memories was also highest for this condition (1.19x\ncompared to control). We discuss potential applications in HCI, such as\ntherapeutic memory reframing, and challenges in ethical, legal, political, and\nsocietal domains.\n", "link": "http://arxiv.org/abs/2409.08895v1", "date": "2024-09-13", "relevancy": 1.8188, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4738}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4455}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Human%20Memories%3A%20AI-Edited%20Images%20and%20Videos%20Can%20Implant%20False%0A%20%20Memories%20and%20Distort%20Recollection&body=Title%3A%20Synthetic%20Human%20Memories%3A%20AI-Edited%20Images%20and%20Videos%20Can%20Implant%20False%0A%20%20Memories%20and%20Distort%20Recollection%0AAuthor%3A%20Pat%20Pataranutaporn%20and%20Chayapatr%20Archiwaranguprok%20and%20Samantha%20W.%20T.%20Chan%20and%20Elizabeth%20Loftus%20and%20Pattie%20Maes%0AAbstract%3A%20%20%20AI%20is%20increasingly%20used%20to%20enhance%20images%20and%20videos%2C%20both%20intentionally%20and%0Aunintentionally.%20As%20AI%20editing%20tools%20become%20more%20integrated%20into%20smartphones%2C%0Ausers%20can%20modify%20or%20animate%20photos%20into%20realistic%20videos.%20This%20study%20examines%0Athe%20impact%20of%20AI-altered%20visuals%20on%20false%20memories--recollections%20of%20events%0Athat%20didn%27t%20occur%20or%20deviate%20from%20reality.%20In%20a%20pre-registered%20study%2C%20200%0Aparticipants%20were%20divided%20into%20four%20conditions%20of%2050%20each.%20Participants%20viewed%0Aoriginal%20images%2C%20completed%20a%20filler%20task%2C%20then%20saw%20stimuli%20corresponding%20to%0Atheir%20assigned%20condition%3A%20unedited%20images%2C%20AI-edited%20images%2C%20AI-generated%0Avideos%2C%20or%20AI-generated%20videos%20of%20AI-edited%20images.%20AI-edited%20visuals%0Asignificantly%20increased%20false%20recollections%2C%20with%20AI-generated%20videos%20of%0AAI-edited%20images%20having%20the%20strongest%20effect%20%282.05x%20compared%20to%20control%29.%0AConfidence%20in%20false%20memories%20was%20also%20highest%20for%20this%20condition%20%281.19x%0Acompared%20to%20control%29.%20We%20discuss%20potential%20applications%20in%20HCI%2C%20such%20as%0Atherapeutic%20memory%20reframing%2C%20and%20challenges%20in%20ethical%2C%20legal%2C%20political%2C%20and%0Asocietal%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Human%2520Memories%253A%2520AI-Edited%2520Images%2520and%2520Videos%2520Can%2520Implant%2520False%250A%2520%2520Memories%2520and%2520Distort%2520Recollection%26entry.906535625%3DPat%2520Pataranutaporn%2520and%2520Chayapatr%2520Archiwaranguprok%2520and%2520Samantha%2520W.%2520T.%2520Chan%2520and%2520Elizabeth%2520Loftus%2520and%2520Pattie%2520Maes%26entry.1292438233%3D%2520%2520AI%2520is%2520increasingly%2520used%2520to%2520enhance%2520images%2520and%2520videos%252C%2520both%2520intentionally%2520and%250Aunintentionally.%2520As%2520AI%2520editing%2520tools%2520become%2520more%2520integrated%2520into%2520smartphones%252C%250Ausers%2520can%2520modify%2520or%2520animate%2520photos%2520into%2520realistic%2520videos.%2520This%2520study%2520examines%250Athe%2520impact%2520of%2520AI-altered%2520visuals%2520on%2520false%2520memories--recollections%2520of%2520events%250Athat%2520didn%2527t%2520occur%2520or%2520deviate%2520from%2520reality.%2520In%2520a%2520pre-registered%2520study%252C%2520200%250Aparticipants%2520were%2520divided%2520into%2520four%2520conditions%2520of%252050%2520each.%2520Participants%2520viewed%250Aoriginal%2520images%252C%2520completed%2520a%2520filler%2520task%252C%2520then%2520saw%2520stimuli%2520corresponding%2520to%250Atheir%2520assigned%2520condition%253A%2520unedited%2520images%252C%2520AI-edited%2520images%252C%2520AI-generated%250Avideos%252C%2520or%2520AI-generated%2520videos%2520of%2520AI-edited%2520images.%2520AI-edited%2520visuals%250Asignificantly%2520increased%2520false%2520recollections%252C%2520with%2520AI-generated%2520videos%2520of%250AAI-edited%2520images%2520having%2520the%2520strongest%2520effect%2520%25282.05x%2520compared%2520to%2520control%2529.%250AConfidence%2520in%2520false%2520memories%2520was%2520also%2520highest%2520for%2520this%2520condition%2520%25281.19x%250Acompared%2520to%2520control%2529.%2520We%2520discuss%2520potential%2520applications%2520in%2520HCI%252C%2520such%2520as%250Atherapeutic%2520memory%2520reframing%252C%2520and%2520challenges%2520in%2520ethical%252C%2520legal%252C%2520political%252C%2520and%250Asocietal%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Human%20Memories%3A%20AI-Edited%20Images%20and%20Videos%20Can%20Implant%20False%0A%20%20Memories%20and%20Distort%20Recollection&entry.906535625=Pat%20Pataranutaporn%20and%20Chayapatr%20Archiwaranguprok%20and%20Samantha%20W.%20T.%20Chan%20and%20Elizabeth%20Loftus%20and%20Pattie%20Maes&entry.1292438233=%20%20AI%20is%20increasingly%20used%20to%20enhance%20images%20and%20videos%2C%20both%20intentionally%20and%0Aunintentionally.%20As%20AI%20editing%20tools%20become%20more%20integrated%20into%20smartphones%2C%0Ausers%20can%20modify%20or%20animate%20photos%20into%20realistic%20videos.%20This%20study%20examines%0Athe%20impact%20of%20AI-altered%20visuals%20on%20false%20memories--recollections%20of%20events%0Athat%20didn%27t%20occur%20or%20deviate%20from%20reality.%20In%20a%20pre-registered%20study%2C%20200%0Aparticipants%20were%20divided%20into%20four%20conditions%20of%2050%20each.%20Participants%20viewed%0Aoriginal%20images%2C%20completed%20a%20filler%20task%2C%20then%20saw%20stimuli%20corresponding%20to%0Atheir%20assigned%20condition%3A%20unedited%20images%2C%20AI-edited%20images%2C%20AI-generated%0Avideos%2C%20or%20AI-generated%20videos%20of%20AI-edited%20images.%20AI-edited%20visuals%0Asignificantly%20increased%20false%20recollections%2C%20with%20AI-generated%20videos%20of%0AAI-edited%20images%20having%20the%20strongest%20effect%20%282.05x%20compared%20to%20control%29.%0AConfidence%20in%20false%20memories%20was%20also%20highest%20for%20this%20condition%20%281.19x%0Acompared%20to%20control%29.%20We%20discuss%20potential%20applications%20in%20HCI%2C%20such%20as%0Atherapeutic%20memory%20reframing%2C%20and%20challenges%20in%20ethical%2C%20legal%2C%20political%2C%20and%0Asocietal%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08895v1&entry.124074799=Read"},
{"title": "A Bayesian framework for active object recognition, pose estimation and\n  shape transfer learning through touch", "author": "Haodong Zheng and Andrei Jalba and Raymond H. Cuijpers and Wijnand IJsselsteijn and Sanne Schoenmakers", "abstract": "  As humans can explore and understand the world through the sense of touch,\ntactile sensing is also an important aspect of robotic perception. In\nunstructured environments, robots can encounter both known and novel objects,\nthis calls for a method to address both known and novel objects. In this study,\nwe combine a particle filter (PF) and Gaussian process implicit surface (GPIS)\nin a unified Bayesian framework. The framework can differentiate between known\nand novel objects, perform object recognition, estimate pose for known objects,\nand reconstruct shapes for unknown objects, in an active learning fashion. By\ngrounding the selection of the GPIS prior with the\nmaximum-likelihood-estimation (MLE) shape from the PF, the knowledge about\nknown objects' shapes can be transferred to learn novel shapes. An exploration\nprocedure with global shape estimation is proposed to guide active data\nacquisition and conclude the exploration when sufficient information is\nobtained. The performance of the proposed Bayesian framework is evaluated\nthrough simulations on known and novel objects, initialized with random poses.\nThe results show that the proposed exploration procedure, utilizing global\nshape estimation, achieves faster exploration than a local exploration\nprocedure based on rapidly explore random tree (RRT). Overall, our results\nindicate that the proposed framework is effective and efficient in object\nrecognition, pose estimation and shape reconstruction. Moreover, we show that a\nlearned shape can be included as a new prior and used effectively for future\nobject recognition and pose estimation.\n", "link": "http://arxiv.org/abs/2409.06912v2", "date": "2024-09-13", "relevancy": 1.7887, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6725}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6022}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bayesian%20framework%20for%20active%20object%20recognition%2C%20pose%20estimation%20and%0A%20%20shape%20transfer%20learning%20through%20touch&body=Title%3A%20A%20Bayesian%20framework%20for%20active%20object%20recognition%2C%20pose%20estimation%20and%0A%20%20shape%20transfer%20learning%20through%20touch%0AAuthor%3A%20Haodong%20Zheng%20and%20Andrei%20Jalba%20and%20Raymond%20H.%20Cuijpers%20and%20Wijnand%20IJsselsteijn%20and%20Sanne%20Schoenmakers%0AAbstract%3A%20%20%20As%20humans%20can%20explore%20and%20understand%20the%20world%20through%20the%20sense%20of%20touch%2C%0Atactile%20sensing%20is%20also%20an%20important%20aspect%20of%20robotic%20perception.%20In%0Aunstructured%20environments%2C%20robots%20can%20encounter%20both%20known%20and%20novel%20objects%2C%0Athis%20calls%20for%20a%20method%20to%20address%20both%20known%20and%20novel%20objects.%20In%20this%20study%2C%0Awe%20combine%20a%20particle%20filter%20%28PF%29%20and%20Gaussian%20process%20implicit%20surface%20%28GPIS%29%0Ain%20a%20unified%20Bayesian%20framework.%20The%20framework%20can%20differentiate%20between%20known%0Aand%20novel%20objects%2C%20perform%20object%20recognition%2C%20estimate%20pose%20for%20known%20objects%2C%0Aand%20reconstruct%20shapes%20for%20unknown%20objects%2C%20in%20an%20active%20learning%20fashion.%20By%0Agrounding%20the%20selection%20of%20the%20GPIS%20prior%20with%20the%0Amaximum-likelihood-estimation%20%28MLE%29%20shape%20from%20the%20PF%2C%20the%20knowledge%20about%0Aknown%20objects%27%20shapes%20can%20be%20transferred%20to%20learn%20novel%20shapes.%20An%20exploration%0Aprocedure%20with%20global%20shape%20estimation%20is%20proposed%20to%20guide%20active%20data%0Aacquisition%20and%20conclude%20the%20exploration%20when%20sufficient%20information%20is%0Aobtained.%20The%20performance%20of%20the%20proposed%20Bayesian%20framework%20is%20evaluated%0Athrough%20simulations%20on%20known%20and%20novel%20objects%2C%20initialized%20with%20random%20poses.%0AThe%20results%20show%20that%20the%20proposed%20exploration%20procedure%2C%20utilizing%20global%0Ashape%20estimation%2C%20achieves%20faster%20exploration%20than%20a%20local%20exploration%0Aprocedure%20based%20on%20rapidly%20explore%20random%20tree%20%28RRT%29.%20Overall%2C%20our%20results%0Aindicate%20that%20the%20proposed%20framework%20is%20effective%20and%20efficient%20in%20object%0Arecognition%2C%20pose%20estimation%20and%20shape%20reconstruction.%20Moreover%2C%20we%20show%20that%20a%0Alearned%20shape%20can%20be%20included%20as%20a%20new%20prior%20and%20used%20effectively%20for%20future%0Aobject%20recognition%20and%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bayesian%2520framework%2520for%2520active%2520object%2520recognition%252C%2520pose%2520estimation%2520and%250A%2520%2520shape%2520transfer%2520learning%2520through%2520touch%26entry.906535625%3DHaodong%2520Zheng%2520and%2520Andrei%2520Jalba%2520and%2520Raymond%2520H.%2520Cuijpers%2520and%2520Wijnand%2520IJsselsteijn%2520and%2520Sanne%2520Schoenmakers%26entry.1292438233%3D%2520%2520As%2520humans%2520can%2520explore%2520and%2520understand%2520the%2520world%2520through%2520the%2520sense%2520of%2520touch%252C%250Atactile%2520sensing%2520is%2520also%2520an%2520important%2520aspect%2520of%2520robotic%2520perception.%2520In%250Aunstructured%2520environments%252C%2520robots%2520can%2520encounter%2520both%2520known%2520and%2520novel%2520objects%252C%250Athis%2520calls%2520for%2520a%2520method%2520to%2520address%2520both%2520known%2520and%2520novel%2520objects.%2520In%2520this%2520study%252C%250Awe%2520combine%2520a%2520particle%2520filter%2520%2528PF%2529%2520and%2520Gaussian%2520process%2520implicit%2520surface%2520%2528GPIS%2529%250Ain%2520a%2520unified%2520Bayesian%2520framework.%2520The%2520framework%2520can%2520differentiate%2520between%2520known%250Aand%2520novel%2520objects%252C%2520perform%2520object%2520recognition%252C%2520estimate%2520pose%2520for%2520known%2520objects%252C%250Aand%2520reconstruct%2520shapes%2520for%2520unknown%2520objects%252C%2520in%2520an%2520active%2520learning%2520fashion.%2520By%250Agrounding%2520the%2520selection%2520of%2520the%2520GPIS%2520prior%2520with%2520the%250Amaximum-likelihood-estimation%2520%2528MLE%2529%2520shape%2520from%2520the%2520PF%252C%2520the%2520knowledge%2520about%250Aknown%2520objects%2527%2520shapes%2520can%2520be%2520transferred%2520to%2520learn%2520novel%2520shapes.%2520An%2520exploration%250Aprocedure%2520with%2520global%2520shape%2520estimation%2520is%2520proposed%2520to%2520guide%2520active%2520data%250Aacquisition%2520and%2520conclude%2520the%2520exploration%2520when%2520sufficient%2520information%2520is%250Aobtained.%2520The%2520performance%2520of%2520the%2520proposed%2520Bayesian%2520framework%2520is%2520evaluated%250Athrough%2520simulations%2520on%2520known%2520and%2520novel%2520objects%252C%2520initialized%2520with%2520random%2520poses.%250AThe%2520results%2520show%2520that%2520the%2520proposed%2520exploration%2520procedure%252C%2520utilizing%2520global%250Ashape%2520estimation%252C%2520achieves%2520faster%2520exploration%2520than%2520a%2520local%2520exploration%250Aprocedure%2520based%2520on%2520rapidly%2520explore%2520random%2520tree%2520%2528RRT%2529.%2520Overall%252C%2520our%2520results%250Aindicate%2520that%2520the%2520proposed%2520framework%2520is%2520effective%2520and%2520efficient%2520in%2520object%250Arecognition%252C%2520pose%2520estimation%2520and%2520shape%2520reconstruction.%2520Moreover%252C%2520we%2520show%2520that%2520a%250Alearned%2520shape%2520can%2520be%2520included%2520as%2520a%2520new%2520prior%2520and%2520used%2520effectively%2520for%2520future%250Aobject%2520recognition%2520and%2520pose%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bayesian%20framework%20for%20active%20object%20recognition%2C%20pose%20estimation%20and%0A%20%20shape%20transfer%20learning%20through%20touch&entry.906535625=Haodong%20Zheng%20and%20Andrei%20Jalba%20and%20Raymond%20H.%20Cuijpers%20and%20Wijnand%20IJsselsteijn%20and%20Sanne%20Schoenmakers&entry.1292438233=%20%20As%20humans%20can%20explore%20and%20understand%20the%20world%20through%20the%20sense%20of%20touch%2C%0Atactile%20sensing%20is%20also%20an%20important%20aspect%20of%20robotic%20perception.%20In%0Aunstructured%20environments%2C%20robots%20can%20encounter%20both%20known%20and%20novel%20objects%2C%0Athis%20calls%20for%20a%20method%20to%20address%20both%20known%20and%20novel%20objects.%20In%20this%20study%2C%0Awe%20combine%20a%20particle%20filter%20%28PF%29%20and%20Gaussian%20process%20implicit%20surface%20%28GPIS%29%0Ain%20a%20unified%20Bayesian%20framework.%20The%20framework%20can%20differentiate%20between%20known%0Aand%20novel%20objects%2C%20perform%20object%20recognition%2C%20estimate%20pose%20for%20known%20objects%2C%0Aand%20reconstruct%20shapes%20for%20unknown%20objects%2C%20in%20an%20active%20learning%20fashion.%20By%0Agrounding%20the%20selection%20of%20the%20GPIS%20prior%20with%20the%0Amaximum-likelihood-estimation%20%28MLE%29%20shape%20from%20the%20PF%2C%20the%20knowledge%20about%0Aknown%20objects%27%20shapes%20can%20be%20transferred%20to%20learn%20novel%20shapes.%20An%20exploration%0Aprocedure%20with%20global%20shape%20estimation%20is%20proposed%20to%20guide%20active%20data%0Aacquisition%20and%20conclude%20the%20exploration%20when%20sufficient%20information%20is%0Aobtained.%20The%20performance%20of%20the%20proposed%20Bayesian%20framework%20is%20evaluated%0Athrough%20simulations%20on%20known%20and%20novel%20objects%2C%20initialized%20with%20random%20poses.%0AThe%20results%20show%20that%20the%20proposed%20exploration%20procedure%2C%20utilizing%20global%0Ashape%20estimation%2C%20achieves%20faster%20exploration%20than%20a%20local%20exploration%0Aprocedure%20based%20on%20rapidly%20explore%20random%20tree%20%28RRT%29.%20Overall%2C%20our%20results%0Aindicate%20that%20the%20proposed%20framework%20is%20effective%20and%20efficient%20in%20object%0Arecognition%2C%20pose%20estimation%20and%20shape%20reconstruction.%20Moreover%2C%20we%20show%20that%20a%0Alearned%20shape%20can%20be%20included%20as%20a%20new%20prior%20and%20used%20effectively%20for%20future%0Aobject%20recognition%20and%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06912v2&entry.124074799=Read"},
{"title": "Clean Label Attacks against SLU Systems", "author": "Henry Li Xinyuan and Sonal Joshi and Thomas Thebaud and Jesus Villalba and Najim Dehak and Sanjeev Khudanpur", "abstract": "  Poisoning backdoor attacks involve an adversary manipulating the training\ndata to induce certain behaviors in the victim model by inserting a trigger in\nthe signal at inference time. We adapted clean label backdoor (CLBD)-data\npoisoning attacks, which do not modify the training labels, on state-of-the-art\nspeech recognition models that support/perform a Spoken Language Understanding\ntask, achieving 99.8% attack success rate by poisoning 10% of the training\ndata. We analyzed how varying the signal-strength of the poison, percent of\nsamples poisoned, and choice of trigger impact the attack. We also found that\nCLBD attacks are most successful when applied to training samples that are\ninherently hard for a proxy model. Using this strategy, we achieved an attack\nsuccess rate of 99.3% by poisoning a meager 1.5% of the training data. Finally,\nwe applied two previously developed defenses against gradient-based attacks,\nand found that they attain mixed success against poisoning.\n", "link": "http://arxiv.org/abs/2409.08985v1", "date": "2024-09-13", "relevancy": 1.7875, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4829}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4257}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clean%20Label%20Attacks%20against%20SLU%20Systems&body=Title%3A%20Clean%20Label%20Attacks%20against%20SLU%20Systems%0AAuthor%3A%20Henry%20Li%20Xinyuan%20and%20Sonal%20Joshi%20and%20Thomas%20Thebaud%20and%20Jesus%20Villalba%20and%20Najim%20Dehak%20and%20Sanjeev%20Khudanpur%0AAbstract%3A%20%20%20Poisoning%20backdoor%20attacks%20involve%20an%20adversary%20manipulating%20the%20training%0Adata%20to%20induce%20certain%20behaviors%20in%20the%20victim%20model%20by%20inserting%20a%20trigger%20in%0Athe%20signal%20at%20inference%20time.%20We%20adapted%20clean%20label%20backdoor%20%28CLBD%29-data%0Apoisoning%20attacks%2C%20which%20do%20not%20modify%20the%20training%20labels%2C%20on%20state-of-the-art%0Aspeech%20recognition%20models%20that%20support/perform%20a%20Spoken%20Language%20Understanding%0Atask%2C%20achieving%2099.8%25%20attack%20success%20rate%20by%20poisoning%2010%25%20of%20the%20training%0Adata.%20We%20analyzed%20how%20varying%20the%20signal-strength%20of%20the%20poison%2C%20percent%20of%0Asamples%20poisoned%2C%20and%20choice%20of%20trigger%20impact%20the%20attack.%20We%20also%20found%20that%0ACLBD%20attacks%20are%20most%20successful%20when%20applied%20to%20training%20samples%20that%20are%0Ainherently%20hard%20for%20a%20proxy%20model.%20Using%20this%20strategy%2C%20we%20achieved%20an%20attack%0Asuccess%20rate%20of%2099.3%25%20by%20poisoning%20a%20meager%201.5%25%20of%20the%20training%20data.%20Finally%2C%0Awe%20applied%20two%20previously%20developed%20defenses%20against%20gradient-based%20attacks%2C%0Aand%20found%20that%20they%20attain%20mixed%20success%20against%20poisoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClean%2520Label%2520Attacks%2520against%2520SLU%2520Systems%26entry.906535625%3DHenry%2520Li%2520Xinyuan%2520and%2520Sonal%2520Joshi%2520and%2520Thomas%2520Thebaud%2520and%2520Jesus%2520Villalba%2520and%2520Najim%2520Dehak%2520and%2520Sanjeev%2520Khudanpur%26entry.1292438233%3D%2520%2520Poisoning%2520backdoor%2520attacks%2520involve%2520an%2520adversary%2520manipulating%2520the%2520training%250Adata%2520to%2520induce%2520certain%2520behaviors%2520in%2520the%2520victim%2520model%2520by%2520inserting%2520a%2520trigger%2520in%250Athe%2520signal%2520at%2520inference%2520time.%2520We%2520adapted%2520clean%2520label%2520backdoor%2520%2528CLBD%2529-data%250Apoisoning%2520attacks%252C%2520which%2520do%2520not%2520modify%2520the%2520training%2520labels%252C%2520on%2520state-of-the-art%250Aspeech%2520recognition%2520models%2520that%2520support/perform%2520a%2520Spoken%2520Language%2520Understanding%250Atask%252C%2520achieving%252099.8%2525%2520attack%2520success%2520rate%2520by%2520poisoning%252010%2525%2520of%2520the%2520training%250Adata.%2520We%2520analyzed%2520how%2520varying%2520the%2520signal-strength%2520of%2520the%2520poison%252C%2520percent%2520of%250Asamples%2520poisoned%252C%2520and%2520choice%2520of%2520trigger%2520impact%2520the%2520attack.%2520We%2520also%2520found%2520that%250ACLBD%2520attacks%2520are%2520most%2520successful%2520when%2520applied%2520to%2520training%2520samples%2520that%2520are%250Ainherently%2520hard%2520for%2520a%2520proxy%2520model.%2520Using%2520this%2520strategy%252C%2520we%2520achieved%2520an%2520attack%250Asuccess%2520rate%2520of%252099.3%2525%2520by%2520poisoning%2520a%2520meager%25201.5%2525%2520of%2520the%2520training%2520data.%2520Finally%252C%250Awe%2520applied%2520two%2520previously%2520developed%2520defenses%2520against%2520gradient-based%2520attacks%252C%250Aand%2520found%2520that%2520they%2520attain%2520mixed%2520success%2520against%2520poisoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clean%20Label%20Attacks%20against%20SLU%20Systems&entry.906535625=Henry%20Li%20Xinyuan%20and%20Sonal%20Joshi%20and%20Thomas%20Thebaud%20and%20Jesus%20Villalba%20and%20Najim%20Dehak%20and%20Sanjeev%20Khudanpur&entry.1292438233=%20%20Poisoning%20backdoor%20attacks%20involve%20an%20adversary%20manipulating%20the%20training%0Adata%20to%20induce%20certain%20behaviors%20in%20the%20victim%20model%20by%20inserting%20a%20trigger%20in%0Athe%20signal%20at%20inference%20time.%20We%20adapted%20clean%20label%20backdoor%20%28CLBD%29-data%0Apoisoning%20attacks%2C%20which%20do%20not%20modify%20the%20training%20labels%2C%20on%20state-of-the-art%0Aspeech%20recognition%20models%20that%20support/perform%20a%20Spoken%20Language%20Understanding%0Atask%2C%20achieving%2099.8%25%20attack%20success%20rate%20by%20poisoning%2010%25%20of%20the%20training%0Adata.%20We%20analyzed%20how%20varying%20the%20signal-strength%20of%20the%20poison%2C%20percent%20of%0Asamples%20poisoned%2C%20and%20choice%20of%20trigger%20impact%20the%20attack.%20We%20also%20found%20that%0ACLBD%20attacks%20are%20most%20successful%20when%20applied%20to%20training%20samples%20that%20are%0Ainherently%20hard%20for%20a%20proxy%20model.%20Using%20this%20strategy%2C%20we%20achieved%20an%20attack%0Asuccess%20rate%20of%2099.3%25%20by%20poisoning%20a%20meager%201.5%25%20of%20the%20training%20data.%20Finally%2C%0Awe%20applied%20two%20previously%20developed%20defenses%20against%20gradient-based%20attacks%2C%0Aand%20found%20that%20they%20attain%20mixed%20success%20against%20poisoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08985v1&entry.124074799=Read"},
{"title": "Reading ability detection using eye-tracking data with LSTM-based\n  few-shot learning", "author": "Nanxi Li and Hongjiang Wang and Zehui Zhan", "abstract": "  Reading ability detection is important in modern educational field. In this\npaper, a method of predicting scores of reading ability is proposed, using the\neye-tracking data of a few subjects (e.g., 68 subjects). The proposed method\nbuilt a regression model for the score prediction by combining Long Short Time\nMemory (LSTM) and light-weighted neural networks. Experiments show that with\nfew-shot learning strategy, the proposed method achieved higher accuracy than\nprevious methods of score prediction in reading ability detection. The code can\nlater be downloaded at\nhttps://github.com/pumpkinLNX/LSTM-eye-tracking-pytorch.git\n", "link": "http://arxiv.org/abs/2409.08798v1", "date": "2024-09-13", "relevancy": 1.7844, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4568}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4397}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reading%20ability%20detection%20using%20eye-tracking%20data%20with%20LSTM-based%0A%20%20few-shot%20learning&body=Title%3A%20Reading%20ability%20detection%20using%20eye-tracking%20data%20with%20LSTM-based%0A%20%20few-shot%20learning%0AAuthor%3A%20Nanxi%20Li%20and%20Hongjiang%20Wang%20and%20Zehui%20Zhan%0AAbstract%3A%20%20%20Reading%20ability%20detection%20is%20important%20in%20modern%20educational%20field.%20In%20this%0Apaper%2C%20a%20method%20of%20predicting%20scores%20of%20reading%20ability%20is%20proposed%2C%20using%20the%0Aeye-tracking%20data%20of%20a%20few%20subjects%20%28e.g.%2C%2068%20subjects%29.%20The%20proposed%20method%0Abuilt%20a%20regression%20model%20for%20the%20score%20prediction%20by%20combining%20Long%20Short%20Time%0AMemory%20%28LSTM%29%20and%20light-weighted%20neural%20networks.%20Experiments%20show%20that%20with%0Afew-shot%20learning%20strategy%2C%20the%20proposed%20method%20achieved%20higher%20accuracy%20than%0Aprevious%20methods%20of%20score%20prediction%20in%20reading%20ability%20detection.%20The%20code%20can%0Alater%20be%20downloaded%20at%0Ahttps%3A//github.com/pumpkinLNX/LSTM-eye-tracking-pytorch.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReading%2520ability%2520detection%2520using%2520eye-tracking%2520data%2520with%2520LSTM-based%250A%2520%2520few-shot%2520learning%26entry.906535625%3DNanxi%2520Li%2520and%2520Hongjiang%2520Wang%2520and%2520Zehui%2520Zhan%26entry.1292438233%3D%2520%2520Reading%2520ability%2520detection%2520is%2520important%2520in%2520modern%2520educational%2520field.%2520In%2520this%250Apaper%252C%2520a%2520method%2520of%2520predicting%2520scores%2520of%2520reading%2520ability%2520is%2520proposed%252C%2520using%2520the%250Aeye-tracking%2520data%2520of%2520a%2520few%2520subjects%2520%2528e.g.%252C%252068%2520subjects%2529.%2520The%2520proposed%2520method%250Abuilt%2520a%2520regression%2520model%2520for%2520the%2520score%2520prediction%2520by%2520combining%2520Long%2520Short%2520Time%250AMemory%2520%2528LSTM%2529%2520and%2520light-weighted%2520neural%2520networks.%2520Experiments%2520show%2520that%2520with%250Afew-shot%2520learning%2520strategy%252C%2520the%2520proposed%2520method%2520achieved%2520higher%2520accuracy%2520than%250Aprevious%2520methods%2520of%2520score%2520prediction%2520in%2520reading%2520ability%2520detection.%2520The%2520code%2520can%250Alater%2520be%2520downloaded%2520at%250Ahttps%253A//github.com/pumpkinLNX/LSTM-eye-tracking-pytorch.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reading%20ability%20detection%20using%20eye-tracking%20data%20with%20LSTM-based%0A%20%20few-shot%20learning&entry.906535625=Nanxi%20Li%20and%20Hongjiang%20Wang%20and%20Zehui%20Zhan&entry.1292438233=%20%20Reading%20ability%20detection%20is%20important%20in%20modern%20educational%20field.%20In%20this%0Apaper%2C%20a%20method%20of%20predicting%20scores%20of%20reading%20ability%20is%20proposed%2C%20using%20the%0Aeye-tracking%20data%20of%20a%20few%20subjects%20%28e.g.%2C%2068%20subjects%29.%20The%20proposed%20method%0Abuilt%20a%20regression%20model%20for%20the%20score%20prediction%20by%20combining%20Long%20Short%20Time%0AMemory%20%28LSTM%29%20and%20light-weighted%20neural%20networks.%20Experiments%20show%20that%20with%0Afew-shot%20learning%20strategy%2C%20the%20proposed%20method%20achieved%20higher%20accuracy%20than%0Aprevious%20methods%20of%20score%20prediction%20in%20reading%20ability%20detection.%20The%20code%20can%0Alater%20be%20downloaded%20at%0Ahttps%3A//github.com/pumpkinLNX/LSTM-eye-tracking-pytorch.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08798v1&entry.124074799=Read"},
{"title": "PINNfluence: Influence Functions for Physics-Informed Neural Networks", "author": "Jonas R. Naujoks and Aleksander Krasowski and Moritz Weckbecker and Thomas Wiegand and Sebastian Lapuschkin and Wojciech Samek and Ren\u00e9 P. Klausen", "abstract": "  Recently, physics-informed neural networks (PINNs) have emerged as a flexible\nand promising application of deep learning to partial differential equations in\nthe physical sciences. While offering strong performance and competitive\ninference speeds on forward and inverse problems, their black-box nature limits\ninterpretability, particularly regarding alignment with expected physical\nbehavior. In the present work, we explore the application of influence\nfunctions (IFs) to validate and debug PINNs post-hoc. Specifically, we apply\nvariations of IF-based indicators to gauge the influence of different types of\ncollocation points on the prediction of PINNs applied to a 2D Navier-Stokes\nfluid flow problem. Our results demonstrate how IFs can be adapted to PINNs to\nreveal the potential for further studies.\n", "link": "http://arxiv.org/abs/2409.08958v1", "date": "2024-09-13", "relevancy": 1.7812, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4746}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4436}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PINNfluence%3A%20Influence%20Functions%20for%20Physics-Informed%20Neural%20Networks&body=Title%3A%20PINNfluence%3A%20Influence%20Functions%20for%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Jonas%20R.%20Naujoks%20and%20Aleksander%20Krasowski%20and%20Moritz%20Weckbecker%20and%20Thomas%20Wiegand%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek%20and%20Ren%C3%A9%20P.%20Klausen%0AAbstract%3A%20%20%20Recently%2C%20physics-informed%20neural%20networks%20%28PINNs%29%20have%20emerged%20as%20a%20flexible%0Aand%20promising%20application%20of%20deep%20learning%20to%20partial%20differential%20equations%20in%0Athe%20physical%20sciences.%20While%20offering%20strong%20performance%20and%20competitive%0Ainference%20speeds%20on%20forward%20and%20inverse%20problems%2C%20their%20black-box%20nature%20limits%0Ainterpretability%2C%20particularly%20regarding%20alignment%20with%20expected%20physical%0Abehavior.%20In%20the%20present%20work%2C%20we%20explore%20the%20application%20of%20influence%0Afunctions%20%28IFs%29%20to%20validate%20and%20debug%20PINNs%20post-hoc.%20Specifically%2C%20we%20apply%0Avariations%20of%20IF-based%20indicators%20to%20gauge%20the%20influence%20of%20different%20types%20of%0Acollocation%20points%20on%20the%20prediction%20of%20PINNs%20applied%20to%20a%202D%20Navier-Stokes%0Afluid%20flow%20problem.%20Our%20results%20demonstrate%20how%20IFs%20can%20be%20adapted%20to%20PINNs%20to%0Areveal%20the%20potential%20for%20further%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPINNfluence%253A%2520Influence%2520Functions%2520for%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DJonas%2520R.%2520Naujoks%2520and%2520Aleksander%2520Krasowski%2520and%2520Moritz%2520Weckbecker%2520and%2520Thomas%2520Wiegand%2520and%2520Sebastian%2520Lapuschkin%2520and%2520Wojciech%2520Samek%2520and%2520Ren%25C3%25A9%2520P.%2520Klausen%26entry.1292438233%3D%2520%2520Recently%252C%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520have%2520emerged%2520as%2520a%2520flexible%250Aand%2520promising%2520application%2520of%2520deep%2520learning%2520to%2520partial%2520differential%2520equations%2520in%250Athe%2520physical%2520sciences.%2520While%2520offering%2520strong%2520performance%2520and%2520competitive%250Ainference%2520speeds%2520on%2520forward%2520and%2520inverse%2520problems%252C%2520their%2520black-box%2520nature%2520limits%250Ainterpretability%252C%2520particularly%2520regarding%2520alignment%2520with%2520expected%2520physical%250Abehavior.%2520In%2520the%2520present%2520work%252C%2520we%2520explore%2520the%2520application%2520of%2520influence%250Afunctions%2520%2528IFs%2529%2520to%2520validate%2520and%2520debug%2520PINNs%2520post-hoc.%2520Specifically%252C%2520we%2520apply%250Avariations%2520of%2520IF-based%2520indicators%2520to%2520gauge%2520the%2520influence%2520of%2520different%2520types%2520of%250Acollocation%2520points%2520on%2520the%2520prediction%2520of%2520PINNs%2520applied%2520to%2520a%25202D%2520Navier-Stokes%250Afluid%2520flow%2520problem.%2520Our%2520results%2520demonstrate%2520how%2520IFs%2520can%2520be%2520adapted%2520to%2520PINNs%2520to%250Areveal%2520the%2520potential%2520for%2520further%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PINNfluence%3A%20Influence%20Functions%20for%20Physics-Informed%20Neural%20Networks&entry.906535625=Jonas%20R.%20Naujoks%20and%20Aleksander%20Krasowski%20and%20Moritz%20Weckbecker%20and%20Thomas%20Wiegand%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek%20and%20Ren%C3%A9%20P.%20Klausen&entry.1292438233=%20%20Recently%2C%20physics-informed%20neural%20networks%20%28PINNs%29%20have%20emerged%20as%20a%20flexible%0Aand%20promising%20application%20of%20deep%20learning%20to%20partial%20differential%20equations%20in%0Athe%20physical%20sciences.%20While%20offering%20strong%20performance%20and%20competitive%0Ainference%20speeds%20on%20forward%20and%20inverse%20problems%2C%20their%20black-box%20nature%20limits%0Ainterpretability%2C%20particularly%20regarding%20alignment%20with%20expected%20physical%0Abehavior.%20In%20the%20present%20work%2C%20we%20explore%20the%20application%20of%20influence%0Afunctions%20%28IFs%29%20to%20validate%20and%20debug%20PINNs%20post-hoc.%20Specifically%2C%20we%20apply%0Avariations%20of%20IF-based%20indicators%20to%20gauge%20the%20influence%20of%20different%20types%20of%0Acollocation%20points%20on%20the%20prediction%20of%20PINNs%20applied%20to%20a%202D%20Navier-Stokes%0Afluid%20flow%20problem.%20Our%20results%20demonstrate%20how%20IFs%20can%20be%20adapted%20to%20PINNs%20to%0Areveal%20the%20potential%20for%20further%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08958v1&entry.124074799=Read"},
{"title": "SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical\n  Records", "author": "Paloma Rabaey and Henri Arno and Stefan Heytens and Thomas Demeester", "abstract": "  We present the SynSUM benchmark, a synthetic dataset linking unstructured\nclinical notes to structured background variables. The dataset consists of\n10,000 artificial patient records containing tabular variables (like symptoms,\ndiagnoses and underlying conditions) and related notes describing the fictional\npatient encounter in the domain of respiratory diseases. The tabular portion of\nthe data is generated through a Bayesian network, where both the causal\nstructure between the variables and the conditional probabilities are proposed\nby an expert based on domain knowledge. We then prompt a large language model\n(GPT-4o) to generate a clinical note related to this patient encounter,\ndescribing the patient symptoms and additional context. The SynSUM dataset is\nprimarily designed to facilitate research on clinical information extraction in\nthe presence of tabular background variables, which can be linked through\ndomain knowledge to concepts of interest to be extracted from the text - the\nsymptoms, in the case of SynSUM. Secondary uses include research on the\nautomation of clinical reasoning over both tabular data and text, causal effect\nestimation in the presence of tabular and/or textual confounders, and\nmulti-modal synthetic data generation. The dataset can be downloaded from\nhttps://github.com/prabaey/SynSUM.\n", "link": "http://arxiv.org/abs/2409.08936v1", "date": "2024-09-13", "relevancy": 1.7811, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4519}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynSUM%20--%20Synthetic%20Benchmark%20with%20Structured%20and%20Unstructured%20Medical%0A%20%20Records&body=Title%3A%20SynSUM%20--%20Synthetic%20Benchmark%20with%20Structured%20and%20Unstructured%20Medical%0A%20%20Records%0AAuthor%3A%20Paloma%20Rabaey%20and%20Henri%20Arno%20and%20Stefan%20Heytens%20and%20Thomas%20Demeester%0AAbstract%3A%20%20%20We%20present%20the%20SynSUM%20benchmark%2C%20a%20synthetic%20dataset%20linking%20unstructured%0Aclinical%20notes%20to%20structured%20background%20variables.%20The%20dataset%20consists%20of%0A10%2C000%20artificial%20patient%20records%20containing%20tabular%20variables%20%28like%20symptoms%2C%0Adiagnoses%20and%20underlying%20conditions%29%20and%20related%20notes%20describing%20the%20fictional%0Apatient%20encounter%20in%20the%20domain%20of%20respiratory%20diseases.%20The%20tabular%20portion%20of%0Athe%20data%20is%20generated%20through%20a%20Bayesian%20network%2C%20where%20both%20the%20causal%0Astructure%20between%20the%20variables%20and%20the%20conditional%20probabilities%20are%20proposed%0Aby%20an%20expert%20based%20on%20domain%20knowledge.%20We%20then%20prompt%20a%20large%20language%20model%0A%28GPT-4o%29%20to%20generate%20a%20clinical%20note%20related%20to%20this%20patient%20encounter%2C%0Adescribing%20the%20patient%20symptoms%20and%20additional%20context.%20The%20SynSUM%20dataset%20is%0Aprimarily%20designed%20to%20facilitate%20research%20on%20clinical%20information%20extraction%20in%0Athe%20presence%20of%20tabular%20background%20variables%2C%20which%20can%20be%20linked%20through%0Adomain%20knowledge%20to%20concepts%20of%20interest%20to%20be%20extracted%20from%20the%20text%20-%20the%0Asymptoms%2C%20in%20the%20case%20of%20SynSUM.%20Secondary%20uses%20include%20research%20on%20the%0Aautomation%20of%20clinical%20reasoning%20over%20both%20tabular%20data%20and%20text%2C%20causal%20effect%0Aestimation%20in%20the%20presence%20of%20tabular%20and/or%20textual%20confounders%2C%20and%0Amulti-modal%20synthetic%20data%20generation.%20The%20dataset%20can%20be%20downloaded%20from%0Ahttps%3A//github.com/prabaey/SynSUM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynSUM%2520--%2520Synthetic%2520Benchmark%2520with%2520Structured%2520and%2520Unstructured%2520Medical%250A%2520%2520Records%26entry.906535625%3DPaloma%2520Rabaey%2520and%2520Henri%2520Arno%2520and%2520Stefan%2520Heytens%2520and%2520Thomas%2520Demeester%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520SynSUM%2520benchmark%252C%2520a%2520synthetic%2520dataset%2520linking%2520unstructured%250Aclinical%2520notes%2520to%2520structured%2520background%2520variables.%2520The%2520dataset%2520consists%2520of%250A10%252C000%2520artificial%2520patient%2520records%2520containing%2520tabular%2520variables%2520%2528like%2520symptoms%252C%250Adiagnoses%2520and%2520underlying%2520conditions%2529%2520and%2520related%2520notes%2520describing%2520the%2520fictional%250Apatient%2520encounter%2520in%2520the%2520domain%2520of%2520respiratory%2520diseases.%2520The%2520tabular%2520portion%2520of%250Athe%2520data%2520is%2520generated%2520through%2520a%2520Bayesian%2520network%252C%2520where%2520both%2520the%2520causal%250Astructure%2520between%2520the%2520variables%2520and%2520the%2520conditional%2520probabilities%2520are%2520proposed%250Aby%2520an%2520expert%2520based%2520on%2520domain%2520knowledge.%2520We%2520then%2520prompt%2520a%2520large%2520language%2520model%250A%2528GPT-4o%2529%2520to%2520generate%2520a%2520clinical%2520note%2520related%2520to%2520this%2520patient%2520encounter%252C%250Adescribing%2520the%2520patient%2520symptoms%2520and%2520additional%2520context.%2520The%2520SynSUM%2520dataset%2520is%250Aprimarily%2520designed%2520to%2520facilitate%2520research%2520on%2520clinical%2520information%2520extraction%2520in%250Athe%2520presence%2520of%2520tabular%2520background%2520variables%252C%2520which%2520can%2520be%2520linked%2520through%250Adomain%2520knowledge%2520to%2520concepts%2520of%2520interest%2520to%2520be%2520extracted%2520from%2520the%2520text%2520-%2520the%250Asymptoms%252C%2520in%2520the%2520case%2520of%2520SynSUM.%2520Secondary%2520uses%2520include%2520research%2520on%2520the%250Aautomation%2520of%2520clinical%2520reasoning%2520over%2520both%2520tabular%2520data%2520and%2520text%252C%2520causal%2520effect%250Aestimation%2520in%2520the%2520presence%2520of%2520tabular%2520and/or%2520textual%2520confounders%252C%2520and%250Amulti-modal%2520synthetic%2520data%2520generation.%2520The%2520dataset%2520can%2520be%2520downloaded%2520from%250Ahttps%253A//github.com/prabaey/SynSUM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynSUM%20--%20Synthetic%20Benchmark%20with%20Structured%20and%20Unstructured%20Medical%0A%20%20Records&entry.906535625=Paloma%20Rabaey%20and%20Henri%20Arno%20and%20Stefan%20Heytens%20and%20Thomas%20Demeester&entry.1292438233=%20%20We%20present%20the%20SynSUM%20benchmark%2C%20a%20synthetic%20dataset%20linking%20unstructured%0Aclinical%20notes%20to%20structured%20background%20variables.%20The%20dataset%20consists%20of%0A10%2C000%20artificial%20patient%20records%20containing%20tabular%20variables%20%28like%20symptoms%2C%0Adiagnoses%20and%20underlying%20conditions%29%20and%20related%20notes%20describing%20the%20fictional%0Apatient%20encounter%20in%20the%20domain%20of%20respiratory%20diseases.%20The%20tabular%20portion%20of%0Athe%20data%20is%20generated%20through%20a%20Bayesian%20network%2C%20where%20both%20the%20causal%0Astructure%20between%20the%20variables%20and%20the%20conditional%20probabilities%20are%20proposed%0Aby%20an%20expert%20based%20on%20domain%20knowledge.%20We%20then%20prompt%20a%20large%20language%20model%0A%28GPT-4o%29%20to%20generate%20a%20clinical%20note%20related%20to%20this%20patient%20encounter%2C%0Adescribing%20the%20patient%20symptoms%20and%20additional%20context.%20The%20SynSUM%20dataset%20is%0Aprimarily%20designed%20to%20facilitate%20research%20on%20clinical%20information%20extraction%20in%0Athe%20presence%20of%20tabular%20background%20variables%2C%20which%20can%20be%20linked%20through%0Adomain%20knowledge%20to%20concepts%20of%20interest%20to%20be%20extracted%20from%20the%20text%20-%20the%0Asymptoms%2C%20in%20the%20case%20of%20SynSUM.%20Secondary%20uses%20include%20research%20on%20the%0Aautomation%20of%20clinical%20reasoning%20over%20both%20tabular%20data%20and%20text%2C%20causal%20effect%0Aestimation%20in%20the%20presence%20of%20tabular%20and/or%20textual%20confounders%2C%20and%0Amulti-modal%20synthetic%20data%20generation.%20The%20dataset%20can%20be%20downloaded%20from%0Ahttps%3A//github.com/prabaey/SynSUM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08936v1&entry.124074799=Read"},
{"title": "Performance Law of Large Language Models", "author": "Chuhan Wu and Ruiming Tang", "abstract": "  Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.\n", "link": "http://arxiv.org/abs/2408.09895v4", "date": "2024-09-13", "relevancy": 1.7805, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20Law%20of%20Large%20Language%20Models&body=Title%3A%20Performance%20Law%20of%20Large%20Language%20Models%0AAuthor%3A%20Chuhan%20Wu%20and%20Ruiming%20Tang%0AAbstract%3A%20%20%20Guided%20by%20the%20belief%20of%20the%20scaling%20law%2C%20large%20language%20models%20%28LLMs%29%20have%0Aachieved%20impressive%20performance%20in%20recent%20years.%20However%2C%20scaling%20law%20only%0Agives%20a%20qualitative%20estimation%20of%20loss%2C%20which%20is%20influenced%20by%20various%20factors%0Asuch%20as%20model%20architectures%2C%20data%20distributions%2C%20tokenizers%2C%20and%20computation%0Aprecision.%20Thus%2C%20estimating%20the%20real%20performance%20of%20LLMs%20with%20different%0Atraining%20settings%20rather%20than%20loss%20may%20be%20quite%20useful%20in%20practical%0Adevelopment.%20In%20this%20article%2C%20we%20present%20an%20empirical%20equation%20named%0A%22Performance%20Law%22%20to%20directly%20predict%20the%20MMLU%20score%20of%20an%20LLM%2C%20which%20is%20a%0Awidely%20used%20metric%20to%20indicate%20the%20general%20capability%20of%20LLMs%20in%20real-world%0Aconversations%20and%20applications.%20Based%20on%20only%20a%20few%20key%20hyperparameters%20of%20the%0ALLM%20architecture%20and%20the%20size%20of%20training%20data%2C%20we%20obtain%20a%20quite%20accurate%20MMLU%0Aprediction%20of%20various%20LLMs%20with%20diverse%20sizes%20and%20architectures%20developed%20by%0Adifferent%20organizations%20in%20different%20years.%20Performance%20law%20can%20be%20used%20to%0Aguide%20the%20choice%20of%20LLM%20architecture%20and%20the%20effective%20allocation%20of%0Acomputational%20resources%20without%20extensive%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09895v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520Law%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DChuhan%2520Wu%2520and%2520Ruiming%2520Tang%26entry.1292438233%3D%2520%2520Guided%2520by%2520the%2520belief%2520of%2520the%2520scaling%2520law%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Aachieved%2520impressive%2520performance%2520in%2520recent%2520years.%2520However%252C%2520scaling%2520law%2520only%250Agives%2520a%2520qualitative%2520estimation%2520of%2520loss%252C%2520which%2520is%2520influenced%2520by%2520various%2520factors%250Asuch%2520as%2520model%2520architectures%252C%2520data%2520distributions%252C%2520tokenizers%252C%2520and%2520computation%250Aprecision.%2520Thus%252C%2520estimating%2520the%2520real%2520performance%2520of%2520LLMs%2520with%2520different%250Atraining%2520settings%2520rather%2520than%2520loss%2520may%2520be%2520quite%2520useful%2520in%2520practical%250Adevelopment.%2520In%2520this%2520article%252C%2520we%2520present%2520an%2520empirical%2520equation%2520named%250A%2522Performance%2520Law%2522%2520to%2520directly%2520predict%2520the%2520MMLU%2520score%2520of%2520an%2520LLM%252C%2520which%2520is%2520a%250Awidely%2520used%2520metric%2520to%2520indicate%2520the%2520general%2520capability%2520of%2520LLMs%2520in%2520real-world%250Aconversations%2520and%2520applications.%2520Based%2520on%2520only%2520a%2520few%2520key%2520hyperparameters%2520of%2520the%250ALLM%2520architecture%2520and%2520the%2520size%2520of%2520training%2520data%252C%2520we%2520obtain%2520a%2520quite%2520accurate%2520MMLU%250Aprediction%2520of%2520various%2520LLMs%2520with%2520diverse%2520sizes%2520and%2520architectures%2520developed%2520by%250Adifferent%2520organizations%2520in%2520different%2520years.%2520Performance%2520law%2520can%2520be%2520used%2520to%250Aguide%2520the%2520choice%2520of%2520LLM%2520architecture%2520and%2520the%2520effective%2520allocation%2520of%250Acomputational%2520resources%2520without%2520extensive%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09895v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20Law%20of%20Large%20Language%20Models&entry.906535625=Chuhan%20Wu%20and%20Ruiming%20Tang&entry.1292438233=%20%20Guided%20by%20the%20belief%20of%20the%20scaling%20law%2C%20large%20language%20models%20%28LLMs%29%20have%0Aachieved%20impressive%20performance%20in%20recent%20years.%20However%2C%20scaling%20law%20only%0Agives%20a%20qualitative%20estimation%20of%20loss%2C%20which%20is%20influenced%20by%20various%20factors%0Asuch%20as%20model%20architectures%2C%20data%20distributions%2C%20tokenizers%2C%20and%20computation%0Aprecision.%20Thus%2C%20estimating%20the%20real%20performance%20of%20LLMs%20with%20different%0Atraining%20settings%20rather%20than%20loss%20may%20be%20quite%20useful%20in%20practical%0Adevelopment.%20In%20this%20article%2C%20we%20present%20an%20empirical%20equation%20named%0A%22Performance%20Law%22%20to%20directly%20predict%20the%20MMLU%20score%20of%20an%20LLM%2C%20which%20is%20a%0Awidely%20used%20metric%20to%20indicate%20the%20general%20capability%20of%20LLMs%20in%20real-world%0Aconversations%20and%20applications.%20Based%20on%20only%20a%20few%20key%20hyperparameters%20of%20the%0ALLM%20architecture%20and%20the%20size%20of%20training%20data%2C%20we%20obtain%20a%20quite%20accurate%20MMLU%0Aprediction%20of%20various%20LLMs%20with%20diverse%20sizes%20and%20architectures%20developed%20by%0Adifferent%20organizations%20in%20different%20years.%20Performance%20law%20can%20be%20used%20to%0Aguide%20the%20choice%20of%20LLM%20architecture%20and%20the%20effective%20allocation%20of%0Acomputational%20resources%20without%20extensive%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09895v4&entry.124074799=Read"},
{"title": "NeSHFS: Neighborhood Search with Heuristic-based Feature Selection for\n  Click-Through Rate Prediction", "author": "Dogukan Aksu and Ismail Hakki Toroslu and Hasan Davulcu", "abstract": "  Click-through-rate (CTR) prediction plays an important role in online\nadvertising and ad recommender systems. In the past decade, maximizing CTR has\nbeen the main focus of model development and solution creation. Therefore,\nresearchers and practitioners have proposed various models and solutions to\nenhance the effectiveness of CTR prediction. Most of the existing literature\nfocuses on capturing either implicit or explicit feature interactions. Although\nimplicit interactions are successfully captured in some studies, explicit\ninteractions present a challenge for achieving high CTR by extracting both\nlow-order and high-order feature interactions. Unnecessary and irrelevant\nfeatures may cause high computational time and low prediction performance.\nFurthermore, certain features may perform well with specific predictive models\nwhile underperforming with others. Also, feature distribution may fluctuate due\nto traffic variations. Most importantly, in live production environments,\nresources are limited, and the time for inference is just as crucial as\ntraining time. Because of all these reasons, feature selection is one of the\nmost important factors in enhancing CTR prediction model performance. Simple\nfilter-based feature selection algorithms do not perform well and they are not\nsufficient. An effective and efficient feature selection algorithm is needed to\nconsistently filter the most useful features during live CTR prediction\nprocess. In this paper, we propose a heuristic algorithm named Neighborhood\nSearch with Heuristic-based Feature Selection (NeSHFS) to enhance CTR\nprediction performance while reducing dimensionality and training time costs.\nWe conduct comprehensive experiments on three public datasets to validate the\nefficiency and effectiveness of our proposed solution.\n", "link": "http://arxiv.org/abs/2409.08703v1", "date": "2024-09-13", "relevancy": 1.7788, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4565}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4383}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeSHFS%3A%20Neighborhood%20Search%20with%20Heuristic-based%20Feature%20Selection%20for%0A%20%20Click-Through%20Rate%20Prediction&body=Title%3A%20NeSHFS%3A%20Neighborhood%20Search%20with%20Heuristic-based%20Feature%20Selection%20for%0A%20%20Click-Through%20Rate%20Prediction%0AAuthor%3A%20Dogukan%20Aksu%20and%20Ismail%20Hakki%20Toroslu%20and%20Hasan%20Davulcu%0AAbstract%3A%20%20%20Click-through-rate%20%28CTR%29%20prediction%20plays%20an%20important%20role%20in%20online%0Aadvertising%20and%20ad%20recommender%20systems.%20In%20the%20past%20decade%2C%20maximizing%20CTR%20has%0Abeen%20the%20main%20focus%20of%20model%20development%20and%20solution%20creation.%20Therefore%2C%0Aresearchers%20and%20practitioners%20have%20proposed%20various%20models%20and%20solutions%20to%0Aenhance%20the%20effectiveness%20of%20CTR%20prediction.%20Most%20of%20the%20existing%20literature%0Afocuses%20on%20capturing%20either%20implicit%20or%20explicit%20feature%20interactions.%20Although%0Aimplicit%20interactions%20are%20successfully%20captured%20in%20some%20studies%2C%20explicit%0Ainteractions%20present%20a%20challenge%20for%20achieving%20high%20CTR%20by%20extracting%20both%0Alow-order%20and%20high-order%20feature%20interactions.%20Unnecessary%20and%20irrelevant%0Afeatures%20may%20cause%20high%20computational%20time%20and%20low%20prediction%20performance.%0AFurthermore%2C%20certain%20features%20may%20perform%20well%20with%20specific%20predictive%20models%0Awhile%20underperforming%20with%20others.%20Also%2C%20feature%20distribution%20may%20fluctuate%20due%0Ato%20traffic%20variations.%20Most%20importantly%2C%20in%20live%20production%20environments%2C%0Aresources%20are%20limited%2C%20and%20the%20time%20for%20inference%20is%20just%20as%20crucial%20as%0Atraining%20time.%20Because%20of%20all%20these%20reasons%2C%20feature%20selection%20is%20one%20of%20the%0Amost%20important%20factors%20in%20enhancing%20CTR%20prediction%20model%20performance.%20Simple%0Afilter-based%20feature%20selection%20algorithms%20do%20not%20perform%20well%20and%20they%20are%20not%0Asufficient.%20An%20effective%20and%20efficient%20feature%20selection%20algorithm%20is%20needed%20to%0Aconsistently%20filter%20the%20most%20useful%20features%20during%20live%20CTR%20prediction%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20heuristic%20algorithm%20named%20Neighborhood%0ASearch%20with%20Heuristic-based%20Feature%20Selection%20%28NeSHFS%29%20to%20enhance%20CTR%0Aprediction%20performance%20while%20reducing%20dimensionality%20and%20training%20time%20costs.%0AWe%20conduct%20comprehensive%20experiments%20on%20three%20public%20datasets%20to%20validate%20the%0Aefficiency%20and%20effectiveness%20of%20our%20proposed%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeSHFS%253A%2520Neighborhood%2520Search%2520with%2520Heuristic-based%2520Feature%2520Selection%2520for%250A%2520%2520Click-Through%2520Rate%2520Prediction%26entry.906535625%3DDogukan%2520Aksu%2520and%2520Ismail%2520Hakki%2520Toroslu%2520and%2520Hasan%2520Davulcu%26entry.1292438233%3D%2520%2520Click-through-rate%2520%2528CTR%2529%2520prediction%2520plays%2520an%2520important%2520role%2520in%2520online%250Aadvertising%2520and%2520ad%2520recommender%2520systems.%2520In%2520the%2520past%2520decade%252C%2520maximizing%2520CTR%2520has%250Abeen%2520the%2520main%2520focus%2520of%2520model%2520development%2520and%2520solution%2520creation.%2520Therefore%252C%250Aresearchers%2520and%2520practitioners%2520have%2520proposed%2520various%2520models%2520and%2520solutions%2520to%250Aenhance%2520the%2520effectiveness%2520of%2520CTR%2520prediction.%2520Most%2520of%2520the%2520existing%2520literature%250Afocuses%2520on%2520capturing%2520either%2520implicit%2520or%2520explicit%2520feature%2520interactions.%2520Although%250Aimplicit%2520interactions%2520are%2520successfully%2520captured%2520in%2520some%2520studies%252C%2520explicit%250Ainteractions%2520present%2520a%2520challenge%2520for%2520achieving%2520high%2520CTR%2520by%2520extracting%2520both%250Alow-order%2520and%2520high-order%2520feature%2520interactions.%2520Unnecessary%2520and%2520irrelevant%250Afeatures%2520may%2520cause%2520high%2520computational%2520time%2520and%2520low%2520prediction%2520performance.%250AFurthermore%252C%2520certain%2520features%2520may%2520perform%2520well%2520with%2520specific%2520predictive%2520models%250Awhile%2520underperforming%2520with%2520others.%2520Also%252C%2520feature%2520distribution%2520may%2520fluctuate%2520due%250Ato%2520traffic%2520variations.%2520Most%2520importantly%252C%2520in%2520live%2520production%2520environments%252C%250Aresources%2520are%2520limited%252C%2520and%2520the%2520time%2520for%2520inference%2520is%2520just%2520as%2520crucial%2520as%250Atraining%2520time.%2520Because%2520of%2520all%2520these%2520reasons%252C%2520feature%2520selection%2520is%2520one%2520of%2520the%250Amost%2520important%2520factors%2520in%2520enhancing%2520CTR%2520prediction%2520model%2520performance.%2520Simple%250Afilter-based%2520feature%2520selection%2520algorithms%2520do%2520not%2520perform%2520well%2520and%2520they%2520are%2520not%250Asufficient.%2520An%2520effective%2520and%2520efficient%2520feature%2520selection%2520algorithm%2520is%2520needed%2520to%250Aconsistently%2520filter%2520the%2520most%2520useful%2520features%2520during%2520live%2520CTR%2520prediction%250Aprocess.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520heuristic%2520algorithm%2520named%2520Neighborhood%250ASearch%2520with%2520Heuristic-based%2520Feature%2520Selection%2520%2528NeSHFS%2529%2520to%2520enhance%2520CTR%250Aprediction%2520performance%2520while%2520reducing%2520dimensionality%2520and%2520training%2520time%2520costs.%250AWe%2520conduct%2520comprehensive%2520experiments%2520on%2520three%2520public%2520datasets%2520to%2520validate%2520the%250Aefficiency%2520and%2520effectiveness%2520of%2520our%2520proposed%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeSHFS%3A%20Neighborhood%20Search%20with%20Heuristic-based%20Feature%20Selection%20for%0A%20%20Click-Through%20Rate%20Prediction&entry.906535625=Dogukan%20Aksu%20and%20Ismail%20Hakki%20Toroslu%20and%20Hasan%20Davulcu&entry.1292438233=%20%20Click-through-rate%20%28CTR%29%20prediction%20plays%20an%20important%20role%20in%20online%0Aadvertising%20and%20ad%20recommender%20systems.%20In%20the%20past%20decade%2C%20maximizing%20CTR%20has%0Abeen%20the%20main%20focus%20of%20model%20development%20and%20solution%20creation.%20Therefore%2C%0Aresearchers%20and%20practitioners%20have%20proposed%20various%20models%20and%20solutions%20to%0Aenhance%20the%20effectiveness%20of%20CTR%20prediction.%20Most%20of%20the%20existing%20literature%0Afocuses%20on%20capturing%20either%20implicit%20or%20explicit%20feature%20interactions.%20Although%0Aimplicit%20interactions%20are%20successfully%20captured%20in%20some%20studies%2C%20explicit%0Ainteractions%20present%20a%20challenge%20for%20achieving%20high%20CTR%20by%20extracting%20both%0Alow-order%20and%20high-order%20feature%20interactions.%20Unnecessary%20and%20irrelevant%0Afeatures%20may%20cause%20high%20computational%20time%20and%20low%20prediction%20performance.%0AFurthermore%2C%20certain%20features%20may%20perform%20well%20with%20specific%20predictive%20models%0Awhile%20underperforming%20with%20others.%20Also%2C%20feature%20distribution%20may%20fluctuate%20due%0Ato%20traffic%20variations.%20Most%20importantly%2C%20in%20live%20production%20environments%2C%0Aresources%20are%20limited%2C%20and%20the%20time%20for%20inference%20is%20just%20as%20crucial%20as%0Atraining%20time.%20Because%20of%20all%20these%20reasons%2C%20feature%20selection%20is%20one%20of%20the%0Amost%20important%20factors%20in%20enhancing%20CTR%20prediction%20model%20performance.%20Simple%0Afilter-based%20feature%20selection%20algorithms%20do%20not%20perform%20well%20and%20they%20are%20not%0Asufficient.%20An%20effective%20and%20efficient%20feature%20selection%20algorithm%20is%20needed%20to%0Aconsistently%20filter%20the%20most%20useful%20features%20during%20live%20CTR%20prediction%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20heuristic%20algorithm%20named%20Neighborhood%0ASearch%20with%20Heuristic-based%20Feature%20Selection%20%28NeSHFS%29%20to%20enhance%20CTR%0Aprediction%20performance%20while%20reducing%20dimensionality%20and%20training%20time%20costs.%0AWe%20conduct%20comprehensive%20experiments%20on%20three%20public%20datasets%20to%20validate%20the%0Aefficiency%20and%20effectiveness%20of%20our%20proposed%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08703v1&entry.124074799=Read"},
{"title": "In-depth Analysis of Low-rank Matrix Factorisation in a Federated\n  Setting", "author": "Constantin Philippenko and Kevin Scaman and Laurent Massouli\u00e9", "abstract": "  We analyze a distributed algorithm to compute a low-rank matrix factorization\non $N$ clients, each holding a local dataset $\\mathbf{S}^i \\in \\mathbb{R}^{n_i\n\\times d}$, mathematically, we seek to solve $min_{\\mathbf{U}^i \\in\n\\mathbb{R}^{n_i\\times r}, \\mathbf{V}\\in \\mathbb{R}^{d \\times r} } \\frac{1}{2}\n\\sum_{i=1}^N \\|\\mathbf{S}^i - \\mathbf{U}^i \\mathbf{V}^\\top\\|^2_{\\text{F}}$.\nConsidering a power initialization of $\\mathbf{V}$, we rewrite the previous\nsmooth non-convex problem into a smooth strongly-convex problem that we solve\nusing a parallel Nesterov gradient descent potentially requiring a single step\nof communication at the initialization step. For any client $i$ in $\\{1, \\dots,\nN\\}$, we obtain a global $\\mathbf{V}$ in $\\mathbb{R}^{d \\times r}$ common to\nall clients and a local variable $\\mathbf{U}^i$ in $\\mathbb{R}^{n_i \\times r}$.\nWe provide a linear rate of convergence of the excess loss which depends on\n$\\sigma_{\\max} / \\sigma_{r}$, where $\\sigma_{r}$ is the $r^{\\mathrm{th}}$\nsingular value of the concatenation $\\mathbf{S}$ of the matrices\n$(\\mathbf{S}^i)_{i=1}^N$. This result improves the rates of convergence given\nin the literature, which depend on $\\sigma_{\\max}^2 / \\sigma_{\\min}^2$. We\nprovide an upper bound on the Frobenius-norm error of reconstruction under the\npower initialization strategy. We complete our analysis with experiments on\nboth synthetic and real data.\n", "link": "http://arxiv.org/abs/2409.08771v1", "date": "2024-09-13", "relevancy": 1.7777, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4502}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4411}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-depth%20Analysis%20of%20Low-rank%20Matrix%20Factorisation%20in%20a%20Federated%0A%20%20Setting&body=Title%3A%20In-depth%20Analysis%20of%20Low-rank%20Matrix%20Factorisation%20in%20a%20Federated%0A%20%20Setting%0AAuthor%3A%20Constantin%20Philippenko%20and%20Kevin%20Scaman%20and%20Laurent%20Massouli%C3%A9%0AAbstract%3A%20%20%20We%20analyze%20a%20distributed%20algorithm%20to%20compute%20a%20low-rank%20matrix%20factorization%0Aon%20%24N%24%20clients%2C%20each%20holding%20a%20local%20dataset%20%24%5Cmathbf%7BS%7D%5Ei%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn_i%0A%5Ctimes%20d%7D%24%2C%20mathematically%2C%20we%20seek%20to%20solve%20%24min_%7B%5Cmathbf%7BU%7D%5Ei%20%5Cin%0A%5Cmathbb%7BR%7D%5E%7Bn_i%5Ctimes%20r%7D%2C%20%5Cmathbf%7BV%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20r%7D%20%7D%20%5Cfrac%7B1%7D%7B2%7D%0A%5Csum_%7Bi%3D1%7D%5EN%20%5C%7C%5Cmathbf%7BS%7D%5Ei%20-%20%5Cmathbf%7BU%7D%5Ei%20%5Cmathbf%7BV%7D%5E%5Ctop%5C%7C%5E2_%7B%5Ctext%7BF%7D%7D%24.%0AConsidering%20a%20power%20initialization%20of%20%24%5Cmathbf%7BV%7D%24%2C%20we%20rewrite%20the%20previous%0Asmooth%20non-convex%20problem%20into%20a%20smooth%20strongly-convex%20problem%20that%20we%20solve%0Ausing%20a%20parallel%20Nesterov%20gradient%20descent%20potentially%20requiring%20a%20single%20step%0Aof%20communication%20at%20the%20initialization%20step.%20For%20any%20client%20%24i%24%20in%20%24%5C%7B1%2C%20%5Cdots%2C%0AN%5C%7D%24%2C%20we%20obtain%20a%20global%20%24%5Cmathbf%7BV%7D%24%20in%20%24%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20r%7D%24%20common%20to%0Aall%20clients%20and%20a%20local%20variable%20%24%5Cmathbf%7BU%7D%5Ei%24%20in%20%24%5Cmathbb%7BR%7D%5E%7Bn_i%20%5Ctimes%20r%7D%24.%0AWe%20provide%20a%20linear%20rate%20of%20convergence%20of%20the%20excess%20loss%20which%20depends%20on%0A%24%5Csigma_%7B%5Cmax%7D%20/%20%5Csigma_%7Br%7D%24%2C%20where%20%24%5Csigma_%7Br%7D%24%20is%20the%20%24r%5E%7B%5Cmathrm%7Bth%7D%7D%24%0Asingular%20value%20of%20the%20concatenation%20%24%5Cmathbf%7BS%7D%24%20of%20the%20matrices%0A%24%28%5Cmathbf%7BS%7D%5Ei%29_%7Bi%3D1%7D%5EN%24.%20This%20result%20improves%20the%20rates%20of%20convergence%20given%0Ain%20the%20literature%2C%20which%20depend%20on%20%24%5Csigma_%7B%5Cmax%7D%5E2%20/%20%5Csigma_%7B%5Cmin%7D%5E2%24.%20We%0Aprovide%20an%20upper%20bound%20on%20the%20Frobenius-norm%20error%20of%20reconstruction%20under%20the%0Apower%20initialization%20strategy.%20We%20complete%20our%20analysis%20with%20experiments%20on%0Aboth%20synthetic%20and%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-depth%2520Analysis%2520of%2520Low-rank%2520Matrix%2520Factorisation%2520in%2520a%2520Federated%250A%2520%2520Setting%26entry.906535625%3DConstantin%2520Philippenko%2520and%2520Kevin%2520Scaman%2520and%2520Laurent%2520Massouli%25C3%25A9%26entry.1292438233%3D%2520%2520We%2520analyze%2520a%2520distributed%2520algorithm%2520to%2520compute%2520a%2520low-rank%2520matrix%2520factorization%250Aon%2520%2524N%2524%2520clients%252C%2520each%2520holding%2520a%2520local%2520dataset%2520%2524%255Cmathbf%257BS%257D%255Ei%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bn_i%250A%255Ctimes%2520d%257D%2524%252C%2520mathematically%252C%2520we%2520seek%2520to%2520solve%2520%2524min_%257B%255Cmathbf%257BU%257D%255Ei%2520%255Cin%250A%255Cmathbb%257BR%257D%255E%257Bn_i%255Ctimes%2520r%257D%252C%2520%255Cmathbf%257BV%257D%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bd%2520%255Ctimes%2520r%257D%2520%257D%2520%255Cfrac%257B1%257D%257B2%257D%250A%255Csum_%257Bi%253D1%257D%255EN%2520%255C%257C%255Cmathbf%257BS%257D%255Ei%2520-%2520%255Cmathbf%257BU%257D%255Ei%2520%255Cmathbf%257BV%257D%255E%255Ctop%255C%257C%255E2_%257B%255Ctext%257BF%257D%257D%2524.%250AConsidering%2520a%2520power%2520initialization%2520of%2520%2524%255Cmathbf%257BV%257D%2524%252C%2520we%2520rewrite%2520the%2520previous%250Asmooth%2520non-convex%2520problem%2520into%2520a%2520smooth%2520strongly-convex%2520problem%2520that%2520we%2520solve%250Ausing%2520a%2520parallel%2520Nesterov%2520gradient%2520descent%2520potentially%2520requiring%2520a%2520single%2520step%250Aof%2520communication%2520at%2520the%2520initialization%2520step.%2520For%2520any%2520client%2520%2524i%2524%2520in%2520%2524%255C%257B1%252C%2520%255Cdots%252C%250AN%255C%257D%2524%252C%2520we%2520obtain%2520a%2520global%2520%2524%255Cmathbf%257BV%257D%2524%2520in%2520%2524%255Cmathbb%257BR%257D%255E%257Bd%2520%255Ctimes%2520r%257D%2524%2520common%2520to%250Aall%2520clients%2520and%2520a%2520local%2520variable%2520%2524%255Cmathbf%257BU%257D%255Ei%2524%2520in%2520%2524%255Cmathbb%257BR%257D%255E%257Bn_i%2520%255Ctimes%2520r%257D%2524.%250AWe%2520provide%2520a%2520linear%2520rate%2520of%2520convergence%2520of%2520the%2520excess%2520loss%2520which%2520depends%2520on%250A%2524%255Csigma_%257B%255Cmax%257D%2520/%2520%255Csigma_%257Br%257D%2524%252C%2520where%2520%2524%255Csigma_%257Br%257D%2524%2520is%2520the%2520%2524r%255E%257B%255Cmathrm%257Bth%257D%257D%2524%250Asingular%2520value%2520of%2520the%2520concatenation%2520%2524%255Cmathbf%257BS%257D%2524%2520of%2520the%2520matrices%250A%2524%2528%255Cmathbf%257BS%257D%255Ei%2529_%257Bi%253D1%257D%255EN%2524.%2520This%2520result%2520improves%2520the%2520rates%2520of%2520convergence%2520given%250Ain%2520the%2520literature%252C%2520which%2520depend%2520on%2520%2524%255Csigma_%257B%255Cmax%257D%255E2%2520/%2520%255Csigma_%257B%255Cmin%257D%255E2%2524.%2520We%250Aprovide%2520an%2520upper%2520bound%2520on%2520the%2520Frobenius-norm%2520error%2520of%2520reconstruction%2520under%2520the%250Apower%2520initialization%2520strategy.%2520We%2520complete%2520our%2520analysis%2520with%2520experiments%2520on%250Aboth%2520synthetic%2520and%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-depth%20Analysis%20of%20Low-rank%20Matrix%20Factorisation%20in%20a%20Federated%0A%20%20Setting&entry.906535625=Constantin%20Philippenko%20and%20Kevin%20Scaman%20and%20Laurent%20Massouli%C3%A9&entry.1292438233=%20%20We%20analyze%20a%20distributed%20algorithm%20to%20compute%20a%20low-rank%20matrix%20factorization%0Aon%20%24N%24%20clients%2C%20each%20holding%20a%20local%20dataset%20%24%5Cmathbf%7BS%7D%5Ei%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn_i%0A%5Ctimes%20d%7D%24%2C%20mathematically%2C%20we%20seek%20to%20solve%20%24min_%7B%5Cmathbf%7BU%7D%5Ei%20%5Cin%0A%5Cmathbb%7BR%7D%5E%7Bn_i%5Ctimes%20r%7D%2C%20%5Cmathbf%7BV%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20r%7D%20%7D%20%5Cfrac%7B1%7D%7B2%7D%0A%5Csum_%7Bi%3D1%7D%5EN%20%5C%7C%5Cmathbf%7BS%7D%5Ei%20-%20%5Cmathbf%7BU%7D%5Ei%20%5Cmathbf%7BV%7D%5E%5Ctop%5C%7C%5E2_%7B%5Ctext%7BF%7D%7D%24.%0AConsidering%20a%20power%20initialization%20of%20%24%5Cmathbf%7BV%7D%24%2C%20we%20rewrite%20the%20previous%0Asmooth%20non-convex%20problem%20into%20a%20smooth%20strongly-convex%20problem%20that%20we%20solve%0Ausing%20a%20parallel%20Nesterov%20gradient%20descent%20potentially%20requiring%20a%20single%20step%0Aof%20communication%20at%20the%20initialization%20step.%20For%20any%20client%20%24i%24%20in%20%24%5C%7B1%2C%20%5Cdots%2C%0AN%5C%7D%24%2C%20we%20obtain%20a%20global%20%24%5Cmathbf%7BV%7D%24%20in%20%24%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20r%7D%24%20common%20to%0Aall%20clients%20and%20a%20local%20variable%20%24%5Cmathbf%7BU%7D%5Ei%24%20in%20%24%5Cmathbb%7BR%7D%5E%7Bn_i%20%5Ctimes%20r%7D%24.%0AWe%20provide%20a%20linear%20rate%20of%20convergence%20of%20the%20excess%20loss%20which%20depends%20on%0A%24%5Csigma_%7B%5Cmax%7D%20/%20%5Csigma_%7Br%7D%24%2C%20where%20%24%5Csigma_%7Br%7D%24%20is%20the%20%24r%5E%7B%5Cmathrm%7Bth%7D%7D%24%0Asingular%20value%20of%20the%20concatenation%20%24%5Cmathbf%7BS%7D%24%20of%20the%20matrices%0A%24%28%5Cmathbf%7BS%7D%5Ei%29_%7Bi%3D1%7D%5EN%24.%20This%20result%20improves%20the%20rates%20of%20convergence%20given%0Ain%20the%20literature%2C%20which%20depend%20on%20%24%5Csigma_%7B%5Cmax%7D%5E2%20/%20%5Csigma_%7B%5Cmin%7D%5E2%24.%20We%0Aprovide%20an%20upper%20bound%20on%20the%20Frobenius-norm%20error%20of%20reconstruction%20under%20the%0Apower%20initialization%20strategy.%20We%20complete%20our%20analysis%20with%20experiments%20on%0Aboth%20synthetic%20and%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08771v1&entry.124074799=Read"},
{"title": "Journalists, Emotions, and the Introduction of Generative AI Chatbots: A\n  Large-Scale Analysis of Tweets Before and After the Launch of ChatGPT", "author": "Seth C. Lewis and David M. Markowitz and Jon Benedik Bunquin", "abstract": "  As part of a broader look at the impact of generative AI, this study\ninvestigated the emotional responses of journalists to the release of ChatGPT\nat the time of its launch. By analyzing nearly 1 million Tweets from\njournalists at major U.S. news outlets, we tracked changes in emotional tone\nand sentiment before and after the introduction of ChatGPT in November 2022.\nUsing various computational and natural language processing techniques to\nmeasure emotional shifts in response to ChatGPT's release, we found an increase\nin positive emotion and a more favorable tone post-launch, suggesting initial\noptimism toward AI's potential. This research underscores the pivotal role of\njournalists as interpreters of technological innovation and disruption,\nhighlighting how their emotional reactions may shape public narratives around\nemerging technologies. The study contributes to understanding the intersection\nof journalism, emotion, and AI, offering insights into the broader societal\nimpact of generative AI tools.\n", "link": "http://arxiv.org/abs/2409.08761v1", "date": "2024-09-13", "relevancy": 1.7739, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4633}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4408}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Journalists%2C%20Emotions%2C%20and%20the%20Introduction%20of%20Generative%20AI%20Chatbots%3A%20A%0A%20%20Large-Scale%20Analysis%20of%20Tweets%20Before%20and%20After%20the%20Launch%20of%20ChatGPT&body=Title%3A%20Journalists%2C%20Emotions%2C%20and%20the%20Introduction%20of%20Generative%20AI%20Chatbots%3A%20A%0A%20%20Large-Scale%20Analysis%20of%20Tweets%20Before%20and%20After%20the%20Launch%20of%20ChatGPT%0AAuthor%3A%20Seth%20C.%20Lewis%20and%20David%20M.%20Markowitz%20and%20Jon%20Benedik%20Bunquin%0AAbstract%3A%20%20%20As%20part%20of%20a%20broader%20look%20at%20the%20impact%20of%20generative%20AI%2C%20this%20study%0Ainvestigated%20the%20emotional%20responses%20of%20journalists%20to%20the%20release%20of%20ChatGPT%0Aat%20the%20time%20of%20its%20launch.%20By%20analyzing%20nearly%201%20million%20Tweets%20from%0Ajournalists%20at%20major%20U.S.%20news%20outlets%2C%20we%20tracked%20changes%20in%20emotional%20tone%0Aand%20sentiment%20before%20and%20after%20the%20introduction%20of%20ChatGPT%20in%20November%202022.%0AUsing%20various%20computational%20and%20natural%20language%20processing%20techniques%20to%0Ameasure%20emotional%20shifts%20in%20response%20to%20ChatGPT%27s%20release%2C%20we%20found%20an%20increase%0Ain%20positive%20emotion%20and%20a%20more%20favorable%20tone%20post-launch%2C%20suggesting%20initial%0Aoptimism%20toward%20AI%27s%20potential.%20This%20research%20underscores%20the%20pivotal%20role%20of%0Ajournalists%20as%20interpreters%20of%20technological%20innovation%20and%20disruption%2C%0Ahighlighting%20how%20their%20emotional%20reactions%20may%20shape%20public%20narratives%20around%0Aemerging%20technologies.%20The%20study%20contributes%20to%20understanding%20the%20intersection%0Aof%20journalism%2C%20emotion%2C%20and%20AI%2C%20offering%20insights%20into%20the%20broader%20societal%0Aimpact%20of%20generative%20AI%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJournalists%252C%2520Emotions%252C%2520and%2520the%2520Introduction%2520of%2520Generative%2520AI%2520Chatbots%253A%2520A%250A%2520%2520Large-Scale%2520Analysis%2520of%2520Tweets%2520Before%2520and%2520After%2520the%2520Launch%2520of%2520ChatGPT%26entry.906535625%3DSeth%2520C.%2520Lewis%2520and%2520David%2520M.%2520Markowitz%2520and%2520Jon%2520Benedik%2520Bunquin%26entry.1292438233%3D%2520%2520As%2520part%2520of%2520a%2520broader%2520look%2520at%2520the%2520impact%2520of%2520generative%2520AI%252C%2520this%2520study%250Ainvestigated%2520the%2520emotional%2520responses%2520of%2520journalists%2520to%2520the%2520release%2520of%2520ChatGPT%250Aat%2520the%2520time%2520of%2520its%2520launch.%2520By%2520analyzing%2520nearly%25201%2520million%2520Tweets%2520from%250Ajournalists%2520at%2520major%2520U.S.%2520news%2520outlets%252C%2520we%2520tracked%2520changes%2520in%2520emotional%2520tone%250Aand%2520sentiment%2520before%2520and%2520after%2520the%2520introduction%2520of%2520ChatGPT%2520in%2520November%25202022.%250AUsing%2520various%2520computational%2520and%2520natural%2520language%2520processing%2520techniques%2520to%250Ameasure%2520emotional%2520shifts%2520in%2520response%2520to%2520ChatGPT%2527s%2520release%252C%2520we%2520found%2520an%2520increase%250Ain%2520positive%2520emotion%2520and%2520a%2520more%2520favorable%2520tone%2520post-launch%252C%2520suggesting%2520initial%250Aoptimism%2520toward%2520AI%2527s%2520potential.%2520This%2520research%2520underscores%2520the%2520pivotal%2520role%2520of%250Ajournalists%2520as%2520interpreters%2520of%2520technological%2520innovation%2520and%2520disruption%252C%250Ahighlighting%2520how%2520their%2520emotional%2520reactions%2520may%2520shape%2520public%2520narratives%2520around%250Aemerging%2520technologies.%2520The%2520study%2520contributes%2520to%2520understanding%2520the%2520intersection%250Aof%2520journalism%252C%2520emotion%252C%2520and%2520AI%252C%2520offering%2520insights%2520into%2520the%2520broader%2520societal%250Aimpact%2520of%2520generative%2520AI%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Journalists%2C%20Emotions%2C%20and%20the%20Introduction%20of%20Generative%20AI%20Chatbots%3A%20A%0A%20%20Large-Scale%20Analysis%20of%20Tweets%20Before%20and%20After%20the%20Launch%20of%20ChatGPT&entry.906535625=Seth%20C.%20Lewis%20and%20David%20M.%20Markowitz%20and%20Jon%20Benedik%20Bunquin&entry.1292438233=%20%20As%20part%20of%20a%20broader%20look%20at%20the%20impact%20of%20generative%20AI%2C%20this%20study%0Ainvestigated%20the%20emotional%20responses%20of%20journalists%20to%20the%20release%20of%20ChatGPT%0Aat%20the%20time%20of%20its%20launch.%20By%20analyzing%20nearly%201%20million%20Tweets%20from%0Ajournalists%20at%20major%20U.S.%20news%20outlets%2C%20we%20tracked%20changes%20in%20emotional%20tone%0Aand%20sentiment%20before%20and%20after%20the%20introduction%20of%20ChatGPT%20in%20November%202022.%0AUsing%20various%20computational%20and%20natural%20language%20processing%20techniques%20to%0Ameasure%20emotional%20shifts%20in%20response%20to%20ChatGPT%27s%20release%2C%20we%20found%20an%20increase%0Ain%20positive%20emotion%20and%20a%20more%20favorable%20tone%20post-launch%2C%20suggesting%20initial%0Aoptimism%20toward%20AI%27s%20potential.%20This%20research%20underscores%20the%20pivotal%20role%20of%0Ajournalists%20as%20interpreters%20of%20technological%20innovation%20and%20disruption%2C%0Ahighlighting%20how%20their%20emotional%20reactions%20may%20shape%20public%20narratives%20around%0Aemerging%20technologies.%20The%20study%20contributes%20to%20understanding%20the%20intersection%0Aof%20journalism%2C%20emotion%2C%20and%20AI%2C%20offering%20insights%20into%20the%20broader%20societal%0Aimpact%20of%20generative%20AI%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08761v1&entry.124074799=Read"},
{"title": "Increasing Both Batch Size and Learning Rate Accelerates Stochastic\n  Gradient Descent", "author": "Hikaru Umeda and Hideaki Iiduka", "abstract": "  The performance of mini-batch stochastic gradient descent (SGD) strongly\ndepends on setting the batch size and learning rate to minimize the empirical\nloss in training the deep neural network. In this paper, we present theoretical\nanalyses of mini-batch SGD with four schedulers: (i) constant batch size and\ndecaying learning rate scheduler, (ii) increasing batch size and decaying\nlearning rate scheduler, (iii) increasing batch size and increasing learning\nrate scheduler, and (iv) increasing batch size and warm-up decaying learning\nrate scheduler. We show that mini-batch SGD using scheduler (i) does not always\nminimize the expectation of the full gradient norm of the empirical loss,\nwhereas it does using any of schedulers (ii), (iii), and (iv). Furthermore,\nschedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides\nnumerical results of supporting analyses showing that using scheduler (iii) or\n(iv) minimizes the full gradient norm of the empirical loss faster than using\nscheduler (i) or (ii).\n", "link": "http://arxiv.org/abs/2409.08770v1", "date": "2024-09-13", "relevancy": 1.7614, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4525}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4455}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Increasing%20Both%20Batch%20Size%20and%20Learning%20Rate%20Accelerates%20Stochastic%0A%20%20Gradient%20Descent&body=Title%3A%20Increasing%20Both%20Batch%20Size%20and%20Learning%20Rate%20Accelerates%20Stochastic%0A%20%20Gradient%20Descent%0AAuthor%3A%20Hikaru%20Umeda%20and%20Hideaki%20Iiduka%0AAbstract%3A%20%20%20The%20performance%20of%20mini-batch%20stochastic%20gradient%20descent%20%28SGD%29%20strongly%0Adepends%20on%20setting%20the%20batch%20size%20and%20learning%20rate%20to%20minimize%20the%20empirical%0Aloss%20in%20training%20the%20deep%20neural%20network.%20In%20this%20paper%2C%20we%20present%20theoretical%0Aanalyses%20of%20mini-batch%20SGD%20with%20four%20schedulers%3A%20%28i%29%20constant%20batch%20size%20and%0Adecaying%20learning%20rate%20scheduler%2C%20%28ii%29%20increasing%20batch%20size%20and%20decaying%0Alearning%20rate%20scheduler%2C%20%28iii%29%20increasing%20batch%20size%20and%20increasing%20learning%0Arate%20scheduler%2C%20and%20%28iv%29%20increasing%20batch%20size%20and%20warm-up%20decaying%20learning%0Arate%20scheduler.%20We%20show%20that%20mini-batch%20SGD%20using%20scheduler%20%28i%29%20does%20not%20always%0Aminimize%20the%20expectation%20of%20the%20full%20gradient%20norm%20of%20the%20empirical%20loss%2C%0Awhereas%20it%20does%20using%20any%20of%20schedulers%20%28ii%29%2C%20%28iii%29%2C%20and%20%28iv%29.%20Furthermore%2C%0Aschedulers%20%28iii%29%20and%20%28iv%29%20accelerate%20mini-batch%20SGD.%20The%20paper%20also%20provides%0Anumerical%20results%20of%20supporting%20analyses%20showing%20that%20using%20scheduler%20%28iii%29%20or%0A%28iv%29%20minimizes%20the%20full%20gradient%20norm%20of%20the%20empirical%20loss%20faster%20than%20using%0Ascheduler%20%28i%29%20or%20%28ii%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncreasing%2520Both%2520Batch%2520Size%2520and%2520Learning%2520Rate%2520Accelerates%2520Stochastic%250A%2520%2520Gradient%2520Descent%26entry.906535625%3DHikaru%2520Umeda%2520and%2520Hideaki%2520Iiduka%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520mini-batch%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520strongly%250Adepends%2520on%2520setting%2520the%2520batch%2520size%2520and%2520learning%2520rate%2520to%2520minimize%2520the%2520empirical%250Aloss%2520in%2520training%2520the%2520deep%2520neural%2520network.%2520In%2520this%2520paper%252C%2520we%2520present%2520theoretical%250Aanalyses%2520of%2520mini-batch%2520SGD%2520with%2520four%2520schedulers%253A%2520%2528i%2529%2520constant%2520batch%2520size%2520and%250Adecaying%2520learning%2520rate%2520scheduler%252C%2520%2528ii%2529%2520increasing%2520batch%2520size%2520and%2520decaying%250Alearning%2520rate%2520scheduler%252C%2520%2528iii%2529%2520increasing%2520batch%2520size%2520and%2520increasing%2520learning%250Arate%2520scheduler%252C%2520and%2520%2528iv%2529%2520increasing%2520batch%2520size%2520and%2520warm-up%2520decaying%2520learning%250Arate%2520scheduler.%2520We%2520show%2520that%2520mini-batch%2520SGD%2520using%2520scheduler%2520%2528i%2529%2520does%2520not%2520always%250Aminimize%2520the%2520expectation%2520of%2520the%2520full%2520gradient%2520norm%2520of%2520the%2520empirical%2520loss%252C%250Awhereas%2520it%2520does%2520using%2520any%2520of%2520schedulers%2520%2528ii%2529%252C%2520%2528iii%2529%252C%2520and%2520%2528iv%2529.%2520Furthermore%252C%250Aschedulers%2520%2528iii%2529%2520and%2520%2528iv%2529%2520accelerate%2520mini-batch%2520SGD.%2520The%2520paper%2520also%2520provides%250Anumerical%2520results%2520of%2520supporting%2520analyses%2520showing%2520that%2520using%2520scheduler%2520%2528iii%2529%2520or%250A%2528iv%2529%2520minimizes%2520the%2520full%2520gradient%2520norm%2520of%2520the%2520empirical%2520loss%2520faster%2520than%2520using%250Ascheduler%2520%2528i%2529%2520or%2520%2528ii%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Increasing%20Both%20Batch%20Size%20and%20Learning%20Rate%20Accelerates%20Stochastic%0A%20%20Gradient%20Descent&entry.906535625=Hikaru%20Umeda%20and%20Hideaki%20Iiduka&entry.1292438233=%20%20The%20performance%20of%20mini-batch%20stochastic%20gradient%20descent%20%28SGD%29%20strongly%0Adepends%20on%20setting%20the%20batch%20size%20and%20learning%20rate%20to%20minimize%20the%20empirical%0Aloss%20in%20training%20the%20deep%20neural%20network.%20In%20this%20paper%2C%20we%20present%20theoretical%0Aanalyses%20of%20mini-batch%20SGD%20with%20four%20schedulers%3A%20%28i%29%20constant%20batch%20size%20and%0Adecaying%20learning%20rate%20scheduler%2C%20%28ii%29%20increasing%20batch%20size%20and%20decaying%0Alearning%20rate%20scheduler%2C%20%28iii%29%20increasing%20batch%20size%20and%20increasing%20learning%0Arate%20scheduler%2C%20and%20%28iv%29%20increasing%20batch%20size%20and%20warm-up%20decaying%20learning%0Arate%20scheduler.%20We%20show%20that%20mini-batch%20SGD%20using%20scheduler%20%28i%29%20does%20not%20always%0Aminimize%20the%20expectation%20of%20the%20full%20gradient%20norm%20of%20the%20empirical%20loss%2C%0Awhereas%20it%20does%20using%20any%20of%20schedulers%20%28ii%29%2C%20%28iii%29%2C%20and%20%28iv%29.%20Furthermore%2C%0Aschedulers%20%28iii%29%20and%20%28iv%29%20accelerate%20mini-batch%20SGD.%20The%20paper%20also%20provides%0Anumerical%20results%20of%20supporting%20analyses%20showing%20that%20using%20scheduler%20%28iii%29%20or%0A%28iv%29%20minimizes%20the%20full%20gradient%20norm%20of%20the%20empirical%20loss%20faster%20than%20using%0Ascheduler%20%28i%29%20or%20%28ii%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08770v1&entry.124074799=Read"},
{"title": "Gaussian is All You Need: A Unified Framework for Solving Inverse\n  Problems via Diffusion Posterior Sampling", "author": "Nebiyou Yismaw and Ulugbek S. Kamilov and M. Salman Asif", "abstract": "  Diffusion models can generate a variety of high-quality images by modeling\ncomplex data distributions. Trained diffusion models can also be very effective\nimage priors for solving inverse problems. Most of the existing diffusion-based\nmethods integrate data consistency steps within the diffusion reverse sampling\nprocess. The data consistency steps rely on an approximate likelihood function.\nIn this paper, we show that the existing approximations are either insufficient\nor computationally inefficient. To address these issues, we propose a unified\nlikelihood approximation method that incorporates a covariance correction term\nto enhance the performance and avoids propagating gradients through the\ndiffusion model. The correction term, when integrated into the reverse\ndiffusion sampling process, achieves better convergence towards the true data\nposterior for selected distributions and improves performance on real-world\nnatural image datasets. Furthermore, we present an efficient way to factorize\nand invert the covariance matrix of the likelihood function for several inverse\nproblems. We present comprehensive experiments to demonstrate the effectiveness\nof our method over several existing approaches.\n", "link": "http://arxiv.org/abs/2409.08906v1", "date": "2024-09-13", "relevancy": 1.7598, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.609}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5808}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20is%20All%20You%20Need%3A%20A%20Unified%20Framework%20for%20Solving%20Inverse%0A%20%20Problems%20via%20Diffusion%20Posterior%20Sampling&body=Title%3A%20Gaussian%20is%20All%20You%20Need%3A%20A%20Unified%20Framework%20for%20Solving%20Inverse%0A%20%20Problems%20via%20Diffusion%20Posterior%20Sampling%0AAuthor%3A%20Nebiyou%20Yismaw%20and%20Ulugbek%20S.%20Kamilov%20and%20M.%20Salman%20Asif%0AAbstract%3A%20%20%20Diffusion%20models%20can%20generate%20a%20variety%20of%20high-quality%20images%20by%20modeling%0Acomplex%20data%20distributions.%20Trained%20diffusion%20models%20can%20also%20be%20very%20effective%0Aimage%20priors%20for%20solving%20inverse%20problems.%20Most%20of%20the%20existing%20diffusion-based%0Amethods%20integrate%20data%20consistency%20steps%20within%20the%20diffusion%20reverse%20sampling%0Aprocess.%20The%20data%20consistency%20steps%20rely%20on%20an%20approximate%20likelihood%20function.%0AIn%20this%20paper%2C%20we%20show%20that%20the%20existing%20approximations%20are%20either%20insufficient%0Aor%20computationally%20inefficient.%20To%20address%20these%20issues%2C%20we%20propose%20a%20unified%0Alikelihood%20approximation%20method%20that%20incorporates%20a%20covariance%20correction%20term%0Ato%20enhance%20the%20performance%20and%20avoids%20propagating%20gradients%20through%20the%0Adiffusion%20model.%20The%20correction%20term%2C%20when%20integrated%20into%20the%20reverse%0Adiffusion%20sampling%20process%2C%20achieves%20better%20convergence%20towards%20the%20true%20data%0Aposterior%20for%20selected%20distributions%20and%20improves%20performance%20on%20real-world%0Anatural%20image%20datasets.%20Furthermore%2C%20we%20present%20an%20efficient%20way%20to%20factorize%0Aand%20invert%20the%20covariance%20matrix%20of%20the%20likelihood%20function%20for%20several%20inverse%0Aproblems.%20We%20present%20comprehensive%20experiments%20to%20demonstrate%20the%20effectiveness%0Aof%20our%20method%20over%20several%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520is%2520All%2520You%2520Need%253A%2520A%2520Unified%2520Framework%2520for%2520Solving%2520Inverse%250A%2520%2520Problems%2520via%2520Diffusion%2520Posterior%2520Sampling%26entry.906535625%3DNebiyou%2520Yismaw%2520and%2520Ulugbek%2520S.%2520Kamilov%2520and%2520M.%2520Salman%2520Asif%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520can%2520generate%2520a%2520variety%2520of%2520high-quality%2520images%2520by%2520modeling%250Acomplex%2520data%2520distributions.%2520Trained%2520diffusion%2520models%2520can%2520also%2520be%2520very%2520effective%250Aimage%2520priors%2520for%2520solving%2520inverse%2520problems.%2520Most%2520of%2520the%2520existing%2520diffusion-based%250Amethods%2520integrate%2520data%2520consistency%2520steps%2520within%2520the%2520diffusion%2520reverse%2520sampling%250Aprocess.%2520The%2520data%2520consistency%2520steps%2520rely%2520on%2520an%2520approximate%2520likelihood%2520function.%250AIn%2520this%2520paper%252C%2520we%2520show%2520that%2520the%2520existing%2520approximations%2520are%2520either%2520insufficient%250Aor%2520computationally%2520inefficient.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520unified%250Alikelihood%2520approximation%2520method%2520that%2520incorporates%2520a%2520covariance%2520correction%2520term%250Ato%2520enhance%2520the%2520performance%2520and%2520avoids%2520propagating%2520gradients%2520through%2520the%250Adiffusion%2520model.%2520The%2520correction%2520term%252C%2520when%2520integrated%2520into%2520the%2520reverse%250Adiffusion%2520sampling%2520process%252C%2520achieves%2520better%2520convergence%2520towards%2520the%2520true%2520data%250Aposterior%2520for%2520selected%2520distributions%2520and%2520improves%2520performance%2520on%2520real-world%250Anatural%2520image%2520datasets.%2520Furthermore%252C%2520we%2520present%2520an%2520efficient%2520way%2520to%2520factorize%250Aand%2520invert%2520the%2520covariance%2520matrix%2520of%2520the%2520likelihood%2520function%2520for%2520several%2520inverse%250Aproblems.%2520We%2520present%2520comprehensive%2520experiments%2520to%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520method%2520over%2520several%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20is%20All%20You%20Need%3A%20A%20Unified%20Framework%20for%20Solving%20Inverse%0A%20%20Problems%20via%20Diffusion%20Posterior%20Sampling&entry.906535625=Nebiyou%20Yismaw%20and%20Ulugbek%20S.%20Kamilov%20and%20M.%20Salman%20Asif&entry.1292438233=%20%20Diffusion%20models%20can%20generate%20a%20variety%20of%20high-quality%20images%20by%20modeling%0Acomplex%20data%20distributions.%20Trained%20diffusion%20models%20can%20also%20be%20very%20effective%0Aimage%20priors%20for%20solving%20inverse%20problems.%20Most%20of%20the%20existing%20diffusion-based%0Amethods%20integrate%20data%20consistency%20steps%20within%20the%20diffusion%20reverse%20sampling%0Aprocess.%20The%20data%20consistency%20steps%20rely%20on%20an%20approximate%20likelihood%20function.%0AIn%20this%20paper%2C%20we%20show%20that%20the%20existing%20approximations%20are%20either%20insufficient%0Aor%20computationally%20inefficient.%20To%20address%20these%20issues%2C%20we%20propose%20a%20unified%0Alikelihood%20approximation%20method%20that%20incorporates%20a%20covariance%20correction%20term%0Ato%20enhance%20the%20performance%20and%20avoids%20propagating%20gradients%20through%20the%0Adiffusion%20model.%20The%20correction%20term%2C%20when%20integrated%20into%20the%20reverse%0Adiffusion%20sampling%20process%2C%20achieves%20better%20convergence%20towards%20the%20true%20data%0Aposterior%20for%20selected%20distributions%20and%20improves%20performance%20on%20real-world%0Anatural%20image%20datasets.%20Furthermore%2C%20we%20present%20an%20efficient%20way%20to%20factorize%0Aand%20invert%20the%20covariance%20matrix%20of%20the%20likelihood%20function%20for%20several%20inverse%0Aproblems.%20We%20present%20comprehensive%20experiments%20to%20demonstrate%20the%20effectiveness%0Aof%20our%20method%20over%20several%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08906v1&entry.124074799=Read"},
{"title": "Farmer.Chat: Scaling AI-Powered Agricultural Services for Smallholder\n  Farmers", "author": "Namita Singh and Jacqueline Wang'ombe and Nereah Okanga and Tetyana Zelenska and Jona Repishti and Jayasankar G K and Sanjeev Mishra and Rajsekar Manokaran and Vineet Singh and Mohammed Irfan Rafiq and Rikin Gandhi and Akshay Nambi", "abstract": "  Small and medium-sized agricultural holders face challenges like limited\naccess to localized, timely information, impacting productivity and\nsustainability. Traditional extension services, which rely on in-person agents,\nstruggle with scalability and timely delivery, especially in remote areas. We\nintroduce Farmer.Chat, a generative AI-powered chatbot designed to address\nthese issues. Leveraging Generative AI, Farmer.Chat offers personalized,\nreliable, and contextually relevant advice, overcoming limitations of previous\nchatbots in deterministic dialogue flows, language support, and unstructured\ndata processing. Deployed in four countries, Farmer.Chat has engaged over\n15,000 farmers and answered over 300,000 queries. This paper highlights how\nFarmer.Chat's innovative use of GenAI enhances agricultural service scalability\nand effectiveness. Our evaluation, combining quantitative analysis and\nqualitative insights, highlights Farmer.Chat's effectiveness in improving\nfarming practices, enhancing trust, response quality, and user engagement.\n", "link": "http://arxiv.org/abs/2409.08916v1", "date": "2024-09-13", "relevancy": 1.7511, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4526}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4292}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Farmer.Chat%3A%20Scaling%20AI-Powered%20Agricultural%20Services%20for%20Smallholder%0A%20%20Farmers&body=Title%3A%20Farmer.Chat%3A%20Scaling%20AI-Powered%20Agricultural%20Services%20for%20Smallholder%0A%20%20Farmers%0AAuthor%3A%20Namita%20Singh%20and%20Jacqueline%20Wang%27ombe%20and%20Nereah%20Okanga%20and%20Tetyana%20Zelenska%20and%20Jona%20Repishti%20and%20Jayasankar%20G%20K%20and%20Sanjeev%20Mishra%20and%20Rajsekar%20Manokaran%20and%20Vineet%20Singh%20and%20Mohammed%20Irfan%20Rafiq%20and%20Rikin%20Gandhi%20and%20Akshay%20Nambi%0AAbstract%3A%20%20%20Small%20and%20medium-sized%20agricultural%20holders%20face%20challenges%20like%20limited%0Aaccess%20to%20localized%2C%20timely%20information%2C%20impacting%20productivity%20and%0Asustainability.%20Traditional%20extension%20services%2C%20which%20rely%20on%20in-person%20agents%2C%0Astruggle%20with%20scalability%20and%20timely%20delivery%2C%20especially%20in%20remote%20areas.%20We%0Aintroduce%20Farmer.Chat%2C%20a%20generative%20AI-powered%20chatbot%20designed%20to%20address%0Athese%20issues.%20Leveraging%20Generative%20AI%2C%20Farmer.Chat%20offers%20personalized%2C%0Areliable%2C%20and%20contextually%20relevant%20advice%2C%20overcoming%20limitations%20of%20previous%0Achatbots%20in%20deterministic%20dialogue%20flows%2C%20language%20support%2C%20and%20unstructured%0Adata%20processing.%20Deployed%20in%20four%20countries%2C%20Farmer.Chat%20has%20engaged%20over%0A15%2C000%20farmers%20and%20answered%20over%20300%2C000%20queries.%20This%20paper%20highlights%20how%0AFarmer.Chat%27s%20innovative%20use%20of%20GenAI%20enhances%20agricultural%20service%20scalability%0Aand%20effectiveness.%20Our%20evaluation%2C%20combining%20quantitative%20analysis%20and%0Aqualitative%20insights%2C%20highlights%20Farmer.Chat%27s%20effectiveness%20in%20improving%0Afarming%20practices%2C%20enhancing%20trust%2C%20response%20quality%2C%20and%20user%20engagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFarmer.Chat%253A%2520Scaling%2520AI-Powered%2520Agricultural%2520Services%2520for%2520Smallholder%250A%2520%2520Farmers%26entry.906535625%3DNamita%2520Singh%2520and%2520Jacqueline%2520Wang%2527ombe%2520and%2520Nereah%2520Okanga%2520and%2520Tetyana%2520Zelenska%2520and%2520Jona%2520Repishti%2520and%2520Jayasankar%2520G%2520K%2520and%2520Sanjeev%2520Mishra%2520and%2520Rajsekar%2520Manokaran%2520and%2520Vineet%2520Singh%2520and%2520Mohammed%2520Irfan%2520Rafiq%2520and%2520Rikin%2520Gandhi%2520and%2520Akshay%2520Nambi%26entry.1292438233%3D%2520%2520Small%2520and%2520medium-sized%2520agricultural%2520holders%2520face%2520challenges%2520like%2520limited%250Aaccess%2520to%2520localized%252C%2520timely%2520information%252C%2520impacting%2520productivity%2520and%250Asustainability.%2520Traditional%2520extension%2520services%252C%2520which%2520rely%2520on%2520in-person%2520agents%252C%250Astruggle%2520with%2520scalability%2520and%2520timely%2520delivery%252C%2520especially%2520in%2520remote%2520areas.%2520We%250Aintroduce%2520Farmer.Chat%252C%2520a%2520generative%2520AI-powered%2520chatbot%2520designed%2520to%2520address%250Athese%2520issues.%2520Leveraging%2520Generative%2520AI%252C%2520Farmer.Chat%2520offers%2520personalized%252C%250Areliable%252C%2520and%2520contextually%2520relevant%2520advice%252C%2520overcoming%2520limitations%2520of%2520previous%250Achatbots%2520in%2520deterministic%2520dialogue%2520flows%252C%2520language%2520support%252C%2520and%2520unstructured%250Adata%2520processing.%2520Deployed%2520in%2520four%2520countries%252C%2520Farmer.Chat%2520has%2520engaged%2520over%250A15%252C000%2520farmers%2520and%2520answered%2520over%2520300%252C000%2520queries.%2520This%2520paper%2520highlights%2520how%250AFarmer.Chat%2527s%2520innovative%2520use%2520of%2520GenAI%2520enhances%2520agricultural%2520service%2520scalability%250Aand%2520effectiveness.%2520Our%2520evaluation%252C%2520combining%2520quantitative%2520analysis%2520and%250Aqualitative%2520insights%252C%2520highlights%2520Farmer.Chat%2527s%2520effectiveness%2520in%2520improving%250Afarming%2520practices%252C%2520enhancing%2520trust%252C%2520response%2520quality%252C%2520and%2520user%2520engagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Farmer.Chat%3A%20Scaling%20AI-Powered%20Agricultural%20Services%20for%20Smallholder%0A%20%20Farmers&entry.906535625=Namita%20Singh%20and%20Jacqueline%20Wang%27ombe%20and%20Nereah%20Okanga%20and%20Tetyana%20Zelenska%20and%20Jona%20Repishti%20and%20Jayasankar%20G%20K%20and%20Sanjeev%20Mishra%20and%20Rajsekar%20Manokaran%20and%20Vineet%20Singh%20and%20Mohammed%20Irfan%20Rafiq%20and%20Rikin%20Gandhi%20and%20Akshay%20Nambi&entry.1292438233=%20%20Small%20and%20medium-sized%20agricultural%20holders%20face%20challenges%20like%20limited%0Aaccess%20to%20localized%2C%20timely%20information%2C%20impacting%20productivity%20and%0Asustainability.%20Traditional%20extension%20services%2C%20which%20rely%20on%20in-person%20agents%2C%0Astruggle%20with%20scalability%20and%20timely%20delivery%2C%20especially%20in%20remote%20areas.%20We%0Aintroduce%20Farmer.Chat%2C%20a%20generative%20AI-powered%20chatbot%20designed%20to%20address%0Athese%20issues.%20Leveraging%20Generative%20AI%2C%20Farmer.Chat%20offers%20personalized%2C%0Areliable%2C%20and%20contextually%20relevant%20advice%2C%20overcoming%20limitations%20of%20previous%0Achatbots%20in%20deterministic%20dialogue%20flows%2C%20language%20support%2C%20and%20unstructured%0Adata%20processing.%20Deployed%20in%20four%20countries%2C%20Farmer.Chat%20has%20engaged%20over%0A15%2C000%20farmers%20and%20answered%20over%20300%2C000%20queries.%20This%20paper%20highlights%20how%0AFarmer.Chat%27s%20innovative%20use%20of%20GenAI%20enhances%20agricultural%20service%20scalability%0Aand%20effectiveness.%20Our%20evaluation%2C%20combining%20quantitative%20analysis%20and%0Aqualitative%20insights%2C%20highlights%20Farmer.Chat%27s%20effectiveness%20in%20improving%0Afarming%20practices%2C%20enhancing%20trust%2C%20response%20quality%2C%20and%20user%20engagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08916v1&entry.124074799=Read"},
{"title": "Model-independent variable selection via the rule-based variable priorit", "author": "Min Lu and Hemant Ishwaran", "abstract": "  While achieving high prediction accuracy is a fundamental goal in machine\nlearning, an equally important task is finding a small number of features with\nhigh explanatory power. One popular selection technique is permutation\nimportance, which assesses a variable's impact by measuring the change in\nprediction error after permuting the variable. However, this can be problematic\ndue to the need to create artificial data, a problem shared by other methods as\nwell. Another problem is that variable selection methods can be limited by\nbeing model-specific. We introduce a new model-independent approach, Variable\nPriority (VarPro), which works by utilizing rules without the need to generate\nartificial data or evaluate prediction error. The method is relatively easy to\nuse, requiring only the calculation of sample averages of simple statistics,\nand can be applied to many data settings, including regression, classification,\nand survival. We investigate the asymptotic properties of VarPro and show,\namong other things, that VarPro has a consistent filtering property for noise\nvariables. Empirical studies using synthetic and real-world data show the\nmethod achieves a balanced performance and compares favorably to many\nstate-of-the-art procedures currently used for variable selection.\n", "link": "http://arxiv.org/abs/2409.09003v1", "date": "2024-09-13", "relevancy": 1.7508, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4594}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-independent%20variable%20selection%20via%20the%20rule-based%20variable%20priorit&body=Title%3A%20Model-independent%20variable%20selection%20via%20the%20rule-based%20variable%20priorit%0AAuthor%3A%20Min%20Lu%20and%20Hemant%20Ishwaran%0AAbstract%3A%20%20%20While%20achieving%20high%20prediction%20accuracy%20is%20a%20fundamental%20goal%20in%20machine%0Alearning%2C%20an%20equally%20important%20task%20is%20finding%20a%20small%20number%20of%20features%20with%0Ahigh%20explanatory%20power.%20One%20popular%20selection%20technique%20is%20permutation%0Aimportance%2C%20which%20assesses%20a%20variable%27s%20impact%20by%20measuring%20the%20change%20in%0Aprediction%20error%20after%20permuting%20the%20variable.%20However%2C%20this%20can%20be%20problematic%0Adue%20to%20the%20need%20to%20create%20artificial%20data%2C%20a%20problem%20shared%20by%20other%20methods%20as%0Awell.%20Another%20problem%20is%20that%20variable%20selection%20methods%20can%20be%20limited%20by%0Abeing%20model-specific.%20We%20introduce%20a%20new%20model-independent%20approach%2C%20Variable%0APriority%20%28VarPro%29%2C%20which%20works%20by%20utilizing%20rules%20without%20the%20need%20to%20generate%0Aartificial%20data%20or%20evaluate%20prediction%20error.%20The%20method%20is%20relatively%20easy%20to%0Ause%2C%20requiring%20only%20the%20calculation%20of%20sample%20averages%20of%20simple%20statistics%2C%0Aand%20can%20be%20applied%20to%20many%20data%20settings%2C%20including%20regression%2C%20classification%2C%0Aand%20survival.%20We%20investigate%20the%20asymptotic%20properties%20of%20VarPro%20and%20show%2C%0Aamong%20other%20things%2C%20that%20VarPro%20has%20a%20consistent%20filtering%20property%20for%20noise%0Avariables.%20Empirical%20studies%20using%20synthetic%20and%20real-world%20data%20show%20the%0Amethod%20achieves%20a%20balanced%20performance%20and%20compares%20favorably%20to%20many%0Astate-of-the-art%20procedures%20currently%20used%20for%20variable%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-independent%2520variable%2520selection%2520via%2520the%2520rule-based%2520variable%2520priorit%26entry.906535625%3DMin%2520Lu%2520and%2520Hemant%2520Ishwaran%26entry.1292438233%3D%2520%2520While%2520achieving%2520high%2520prediction%2520accuracy%2520is%2520a%2520fundamental%2520goal%2520in%2520machine%250Alearning%252C%2520an%2520equally%2520important%2520task%2520is%2520finding%2520a%2520small%2520number%2520of%2520features%2520with%250Ahigh%2520explanatory%2520power.%2520One%2520popular%2520selection%2520technique%2520is%2520permutation%250Aimportance%252C%2520which%2520assesses%2520a%2520variable%2527s%2520impact%2520by%2520measuring%2520the%2520change%2520in%250Aprediction%2520error%2520after%2520permuting%2520the%2520variable.%2520However%252C%2520this%2520can%2520be%2520problematic%250Adue%2520to%2520the%2520need%2520to%2520create%2520artificial%2520data%252C%2520a%2520problem%2520shared%2520by%2520other%2520methods%2520as%250Awell.%2520Another%2520problem%2520is%2520that%2520variable%2520selection%2520methods%2520can%2520be%2520limited%2520by%250Abeing%2520model-specific.%2520We%2520introduce%2520a%2520new%2520model-independent%2520approach%252C%2520Variable%250APriority%2520%2528VarPro%2529%252C%2520which%2520works%2520by%2520utilizing%2520rules%2520without%2520the%2520need%2520to%2520generate%250Aartificial%2520data%2520or%2520evaluate%2520prediction%2520error.%2520The%2520method%2520is%2520relatively%2520easy%2520to%250Ause%252C%2520requiring%2520only%2520the%2520calculation%2520of%2520sample%2520averages%2520of%2520simple%2520statistics%252C%250Aand%2520can%2520be%2520applied%2520to%2520many%2520data%2520settings%252C%2520including%2520regression%252C%2520classification%252C%250Aand%2520survival.%2520We%2520investigate%2520the%2520asymptotic%2520properties%2520of%2520VarPro%2520and%2520show%252C%250Aamong%2520other%2520things%252C%2520that%2520VarPro%2520has%2520a%2520consistent%2520filtering%2520property%2520for%2520noise%250Avariables.%2520Empirical%2520studies%2520using%2520synthetic%2520and%2520real-world%2520data%2520show%2520the%250Amethod%2520achieves%2520a%2520balanced%2520performance%2520and%2520compares%2520favorably%2520to%2520many%250Astate-of-the-art%2520procedures%2520currently%2520used%2520for%2520variable%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-independent%20variable%20selection%20via%20the%20rule-based%20variable%20priorit&entry.906535625=Min%20Lu%20and%20Hemant%20Ishwaran&entry.1292438233=%20%20While%20achieving%20high%20prediction%20accuracy%20is%20a%20fundamental%20goal%20in%20machine%0Alearning%2C%20an%20equally%20important%20task%20is%20finding%20a%20small%20number%20of%20features%20with%0Ahigh%20explanatory%20power.%20One%20popular%20selection%20technique%20is%20permutation%0Aimportance%2C%20which%20assesses%20a%20variable%27s%20impact%20by%20measuring%20the%20change%20in%0Aprediction%20error%20after%20permuting%20the%20variable.%20However%2C%20this%20can%20be%20problematic%0Adue%20to%20the%20need%20to%20create%20artificial%20data%2C%20a%20problem%20shared%20by%20other%20methods%20as%0Awell.%20Another%20problem%20is%20that%20variable%20selection%20methods%20can%20be%20limited%20by%0Abeing%20model-specific.%20We%20introduce%20a%20new%20model-independent%20approach%2C%20Variable%0APriority%20%28VarPro%29%2C%20which%20works%20by%20utilizing%20rules%20without%20the%20need%20to%20generate%0Aartificial%20data%20or%20evaluate%20prediction%20error.%20The%20method%20is%20relatively%20easy%20to%0Ause%2C%20requiring%20only%20the%20calculation%20of%20sample%20averages%20of%20simple%20statistics%2C%0Aand%20can%20be%20applied%20to%20many%20data%20settings%2C%20including%20regression%2C%20classification%2C%0Aand%20survival.%20We%20investigate%20the%20asymptotic%20properties%20of%20VarPro%20and%20show%2C%0Aamong%20other%20things%2C%20that%20VarPro%20has%20a%20consistent%20filtering%20property%20for%20noise%0Avariables.%20Empirical%20studies%20using%20synthetic%20and%20real-world%20data%20show%20the%0Amethod%20achieves%20a%20balanced%20performance%20and%20compares%20favorably%20to%20many%0Astate-of-the-art%20procedures%20currently%20used%20for%20variable%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09003v1&entry.124074799=Read"},
{"title": "Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification\n  with Snoring Usecase", "author": "Md Rakibul Hasan and Shreya Ghosh and Pradyumna Agrawal and Zhixi Cai and Abhinav Dhall and Tom Gedeon", "abstract": "  This paper proposes a feedback mechanism to change behavioural patterns using\nthe Pavlok device. Pavlok utilises beeps, vibration and shocks as a mode of\naversion technique to help individuals with behaviour modification. While the\ndevice can be useful in certain periodic daily life situations, like alarms and\nexercise notifications, the device relies on manual operations that limit its\nusage. To automate behaviour modification, we propose a framework that first\ndetects targeted behaviours through a lightweight deep learning model and\nsubsequently nudges the user through Pavlok. Our proposed solution is\nimplemented and verified in the context of snoring, which captures audio from\nthe environment following a prediction of whether the audio content is a snore\nor not using a 1D convolutional neural network. Based on the prediction, we use\nPavlok to nudge users for preventive measures, such as a change in sleeping\nposture. We believe that this simple solution can help people to change their\natomic habits, which may lead to long-term health benefits. Our proposed\nreal-time, lightweight model (99.8% less parameters over SOTA; 1,278,049 -->\n1337) achieves SOTA performance (test accuracy of 0.99) on a public domain\nbenchmark. The code and model are publicly available at\nhttps://github.com/hasan-rakibul/pavlok-nudge-snore.\n", "link": "http://arxiv.org/abs/2305.06110v3", "date": "2024-09-13", "relevancy": 1.7411, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.437}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4342}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pavlok-Nudge%3A%20A%20Feedback%20Mechanism%20for%20Atomic%20Behaviour%20Modification%0A%20%20with%20Snoring%20Usecase&body=Title%3A%20Pavlok-Nudge%3A%20A%20Feedback%20Mechanism%20for%20Atomic%20Behaviour%20Modification%0A%20%20with%20Snoring%20Usecase%0AAuthor%3A%20Md%20Rakibul%20Hasan%20and%20Shreya%20Ghosh%20and%20Pradyumna%20Agrawal%20and%20Zhixi%20Cai%20and%20Abhinav%20Dhall%20and%20Tom%20Gedeon%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20feedback%20mechanism%20to%20change%20behavioural%20patterns%20using%0Athe%20Pavlok%20device.%20Pavlok%20utilises%20beeps%2C%20vibration%20and%20shocks%20as%20a%20mode%20of%0Aaversion%20technique%20to%20help%20individuals%20with%20behaviour%20modification.%20While%20the%0Adevice%20can%20be%20useful%20in%20certain%20periodic%20daily%20life%20situations%2C%20like%20alarms%20and%0Aexercise%20notifications%2C%20the%20device%20relies%20on%20manual%20operations%20that%20limit%20its%0Ausage.%20To%20automate%20behaviour%20modification%2C%20we%20propose%20a%20framework%20that%20first%0Adetects%20targeted%20behaviours%20through%20a%20lightweight%20deep%20learning%20model%20and%0Asubsequently%20nudges%20the%20user%20through%20Pavlok.%20Our%20proposed%20solution%20is%0Aimplemented%20and%20verified%20in%20the%20context%20of%20snoring%2C%20which%20captures%20audio%20from%0Athe%20environment%20following%20a%20prediction%20of%20whether%20the%20audio%20content%20is%20a%20snore%0Aor%20not%20using%20a%201D%20convolutional%20neural%20network.%20Based%20on%20the%20prediction%2C%20we%20use%0APavlok%20to%20nudge%20users%20for%20preventive%20measures%2C%20such%20as%20a%20change%20in%20sleeping%0Aposture.%20We%20believe%20that%20this%20simple%20solution%20can%20help%20people%20to%20change%20their%0Aatomic%20habits%2C%20which%20may%20lead%20to%20long-term%20health%20benefits.%20Our%20proposed%0Areal-time%2C%20lightweight%20model%20%2899.8%25%20less%20parameters%20over%20SOTA%3B%201%2C278%2C049%20--%3E%0A1337%29%20achieves%20SOTA%20performance%20%28test%20accuracy%20of%200.99%29%20on%20a%20public%20domain%0Abenchmark.%20The%20code%20and%20model%20are%20publicly%20available%20at%0Ahttps%3A//github.com/hasan-rakibul/pavlok-nudge-snore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.06110v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPavlok-Nudge%253A%2520A%2520Feedback%2520Mechanism%2520for%2520Atomic%2520Behaviour%2520Modification%250A%2520%2520with%2520Snoring%2520Usecase%26entry.906535625%3DMd%2520Rakibul%2520Hasan%2520and%2520Shreya%2520Ghosh%2520and%2520Pradyumna%2520Agrawal%2520and%2520Zhixi%2520Cai%2520and%2520Abhinav%2520Dhall%2520and%2520Tom%2520Gedeon%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520feedback%2520mechanism%2520to%2520change%2520behavioural%2520patterns%2520using%250Athe%2520Pavlok%2520device.%2520Pavlok%2520utilises%2520beeps%252C%2520vibration%2520and%2520shocks%2520as%2520a%2520mode%2520of%250Aaversion%2520technique%2520to%2520help%2520individuals%2520with%2520behaviour%2520modification.%2520While%2520the%250Adevice%2520can%2520be%2520useful%2520in%2520certain%2520periodic%2520daily%2520life%2520situations%252C%2520like%2520alarms%2520and%250Aexercise%2520notifications%252C%2520the%2520device%2520relies%2520on%2520manual%2520operations%2520that%2520limit%2520its%250Ausage.%2520To%2520automate%2520behaviour%2520modification%252C%2520we%2520propose%2520a%2520framework%2520that%2520first%250Adetects%2520targeted%2520behaviours%2520through%2520a%2520lightweight%2520deep%2520learning%2520model%2520and%250Asubsequently%2520nudges%2520the%2520user%2520through%2520Pavlok.%2520Our%2520proposed%2520solution%2520is%250Aimplemented%2520and%2520verified%2520in%2520the%2520context%2520of%2520snoring%252C%2520which%2520captures%2520audio%2520from%250Athe%2520environment%2520following%2520a%2520prediction%2520of%2520whether%2520the%2520audio%2520content%2520is%2520a%2520snore%250Aor%2520not%2520using%2520a%25201D%2520convolutional%2520neural%2520network.%2520Based%2520on%2520the%2520prediction%252C%2520we%2520use%250APavlok%2520to%2520nudge%2520users%2520for%2520preventive%2520measures%252C%2520such%2520as%2520a%2520change%2520in%2520sleeping%250Aposture.%2520We%2520believe%2520that%2520this%2520simple%2520solution%2520can%2520help%2520people%2520to%2520change%2520their%250Aatomic%2520habits%252C%2520which%2520may%2520lead%2520to%2520long-term%2520health%2520benefits.%2520Our%2520proposed%250Areal-time%252C%2520lightweight%2520model%2520%252899.8%2525%2520less%2520parameters%2520over%2520SOTA%253B%25201%252C278%252C049%2520--%253E%250A1337%2529%2520achieves%2520SOTA%2520performance%2520%2528test%2520accuracy%2520of%25200.99%2529%2520on%2520a%2520public%2520domain%250Abenchmark.%2520The%2520code%2520and%2520model%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/hasan-rakibul/pavlok-nudge-snore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.06110v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pavlok-Nudge%3A%20A%20Feedback%20Mechanism%20for%20Atomic%20Behaviour%20Modification%0A%20%20with%20Snoring%20Usecase&entry.906535625=Md%20Rakibul%20Hasan%20and%20Shreya%20Ghosh%20and%20Pradyumna%20Agrawal%20and%20Zhixi%20Cai%20and%20Abhinav%20Dhall%20and%20Tom%20Gedeon&entry.1292438233=%20%20This%20paper%20proposes%20a%20feedback%20mechanism%20to%20change%20behavioural%20patterns%20using%0Athe%20Pavlok%20device.%20Pavlok%20utilises%20beeps%2C%20vibration%20and%20shocks%20as%20a%20mode%20of%0Aaversion%20technique%20to%20help%20individuals%20with%20behaviour%20modification.%20While%20the%0Adevice%20can%20be%20useful%20in%20certain%20periodic%20daily%20life%20situations%2C%20like%20alarms%20and%0Aexercise%20notifications%2C%20the%20device%20relies%20on%20manual%20operations%20that%20limit%20its%0Ausage.%20To%20automate%20behaviour%20modification%2C%20we%20propose%20a%20framework%20that%20first%0Adetects%20targeted%20behaviours%20through%20a%20lightweight%20deep%20learning%20model%20and%0Asubsequently%20nudges%20the%20user%20through%20Pavlok.%20Our%20proposed%20solution%20is%0Aimplemented%20and%20verified%20in%20the%20context%20of%20snoring%2C%20which%20captures%20audio%20from%0Athe%20environment%20following%20a%20prediction%20of%20whether%20the%20audio%20content%20is%20a%20snore%0Aor%20not%20using%20a%201D%20convolutional%20neural%20network.%20Based%20on%20the%20prediction%2C%20we%20use%0APavlok%20to%20nudge%20users%20for%20preventive%20measures%2C%20such%20as%20a%20change%20in%20sleeping%0Aposture.%20We%20believe%20that%20this%20simple%20solution%20can%20help%20people%20to%20change%20their%0Aatomic%20habits%2C%20which%20may%20lead%20to%20long-term%20health%20benefits.%20Our%20proposed%0Areal-time%2C%20lightweight%20model%20%2899.8%25%20less%20parameters%20over%20SOTA%3B%201%2C278%2C049%20--%3E%0A1337%29%20achieves%20SOTA%20performance%20%28test%20accuracy%20of%200.99%29%20on%20a%20public%20domain%0Abenchmark.%20The%20code%20and%20model%20are%20publicly%20available%20at%0Ahttps%3A//github.com/hasan-rakibul/pavlok-nudge-snore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.06110v3&entry.124074799=Read"},
{"title": "Interactive Masked Image Modeling for Multimodal Object Detection in\n  Remote Sensing", "author": "Minh-Duc Vu and Zuheng Ming and Fangchen Feng and Bissmella Bahaduri and Anissa Mokraoui", "abstract": "  Object detection in remote sensing imagery plays a vital role in various\nEarth observation applications. However, unlike object detection in natural\nscene images, this task is particularly challenging due to the abundance of\nsmall, often barely visible objects across diverse terrains. To address these\nchallenges, multimodal learning can be used to integrate features from\ndifferent data modalities, thereby improving detection accuracy. Nonetheless,\nthe performance of multimodal learning is often constrained by the limited size\nof labeled datasets. In this paper, we propose to use Masked Image Modeling\n(MIM) as a pre-training technique, leveraging self-supervised learning on\nunlabeled data to enhance detection performance. However, conventional MIM such\nas MAE which uses masked tokens without any contextual information, struggles\nto capture the fine-grained details due to a lack of interactions with other\nparts of image. To address this, we propose a new interactive MIM method that\ncan establish interactions between different tokens, which is particularly\nbeneficial for object detection in remote sensing. The extensive ablation\nstudies and evluation demonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2409.08885v1", "date": "2024-09-13", "relevancy": 1.735, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5846}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5811}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Masked%20Image%20Modeling%20for%20Multimodal%20Object%20Detection%20in%0A%20%20Remote%20Sensing&body=Title%3A%20Interactive%20Masked%20Image%20Modeling%20for%20Multimodal%20Object%20Detection%20in%0A%20%20Remote%20Sensing%0AAuthor%3A%20Minh-Duc%20Vu%20and%20Zuheng%20Ming%20and%20Fangchen%20Feng%20and%20Bissmella%20Bahaduri%20and%20Anissa%20Mokraoui%0AAbstract%3A%20%20%20Object%20detection%20in%20remote%20sensing%20imagery%20plays%20a%20vital%20role%20in%20various%0AEarth%20observation%20applications.%20However%2C%20unlike%20object%20detection%20in%20natural%0Ascene%20images%2C%20this%20task%20is%20particularly%20challenging%20due%20to%20the%20abundance%20of%0Asmall%2C%20often%20barely%20visible%20objects%20across%20diverse%20terrains.%20To%20address%20these%0Achallenges%2C%20multimodal%20learning%20can%20be%20used%20to%20integrate%20features%20from%0Adifferent%20data%20modalities%2C%20thereby%20improving%20detection%20accuracy.%20Nonetheless%2C%0Athe%20performance%20of%20multimodal%20learning%20is%20often%20constrained%20by%20the%20limited%20size%0Aof%20labeled%20datasets.%20In%20this%20paper%2C%20we%20propose%20to%20use%20Masked%20Image%20Modeling%0A%28MIM%29%20as%20a%20pre-training%20technique%2C%20leveraging%20self-supervised%20learning%20on%0Aunlabeled%20data%20to%20enhance%20detection%20performance.%20However%2C%20conventional%20MIM%20such%0Aas%20MAE%20which%20uses%20masked%20tokens%20without%20any%20contextual%20information%2C%20struggles%0Ato%20capture%20the%20fine-grained%20details%20due%20to%20a%20lack%20of%20interactions%20with%20other%0Aparts%20of%20image.%20To%20address%20this%2C%20we%20propose%20a%20new%20interactive%20MIM%20method%20that%0Acan%20establish%20interactions%20between%20different%20tokens%2C%20which%20is%20particularly%0Abeneficial%20for%20object%20detection%20in%20remote%20sensing.%20The%20extensive%20ablation%0Astudies%20and%20evluation%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Masked%2520Image%2520Modeling%2520for%2520Multimodal%2520Object%2520Detection%2520in%250A%2520%2520Remote%2520Sensing%26entry.906535625%3DMinh-Duc%2520Vu%2520and%2520Zuheng%2520Ming%2520and%2520Fangchen%2520Feng%2520and%2520Bissmella%2520Bahaduri%2520and%2520Anissa%2520Mokraoui%26entry.1292438233%3D%2520%2520Object%2520detection%2520in%2520remote%2520sensing%2520imagery%2520plays%2520a%2520vital%2520role%2520in%2520various%250AEarth%2520observation%2520applications.%2520However%252C%2520unlike%2520object%2520detection%2520in%2520natural%250Ascene%2520images%252C%2520this%2520task%2520is%2520particularly%2520challenging%2520due%2520to%2520the%2520abundance%2520of%250Asmall%252C%2520often%2520barely%2520visible%2520objects%2520across%2520diverse%2520terrains.%2520To%2520address%2520these%250Achallenges%252C%2520multimodal%2520learning%2520can%2520be%2520used%2520to%2520integrate%2520features%2520from%250Adifferent%2520data%2520modalities%252C%2520thereby%2520improving%2520detection%2520accuracy.%2520Nonetheless%252C%250Athe%2520performance%2520of%2520multimodal%2520learning%2520is%2520often%2520constrained%2520by%2520the%2520limited%2520size%250Aof%2520labeled%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520use%2520Masked%2520Image%2520Modeling%250A%2528MIM%2529%2520as%2520a%2520pre-training%2520technique%252C%2520leveraging%2520self-supervised%2520learning%2520on%250Aunlabeled%2520data%2520to%2520enhance%2520detection%2520performance.%2520However%252C%2520conventional%2520MIM%2520such%250Aas%2520MAE%2520which%2520uses%2520masked%2520tokens%2520without%2520any%2520contextual%2520information%252C%2520struggles%250Ato%2520capture%2520the%2520fine-grained%2520details%2520due%2520to%2520a%2520lack%2520of%2520interactions%2520with%2520other%250Aparts%2520of%2520image.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520new%2520interactive%2520MIM%2520method%2520that%250Acan%2520establish%2520interactions%2520between%2520different%2520tokens%252C%2520which%2520is%2520particularly%250Abeneficial%2520for%2520object%2520detection%2520in%2520remote%2520sensing.%2520The%2520extensive%2520ablation%250Astudies%2520and%2520evluation%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Masked%20Image%20Modeling%20for%20Multimodal%20Object%20Detection%20in%0A%20%20Remote%20Sensing&entry.906535625=Minh-Duc%20Vu%20and%20Zuheng%20Ming%20and%20Fangchen%20Feng%20and%20Bissmella%20Bahaduri%20and%20Anissa%20Mokraoui&entry.1292438233=%20%20Object%20detection%20in%20remote%20sensing%20imagery%20plays%20a%20vital%20role%20in%20various%0AEarth%20observation%20applications.%20However%2C%20unlike%20object%20detection%20in%20natural%0Ascene%20images%2C%20this%20task%20is%20particularly%20challenging%20due%20to%20the%20abundance%20of%0Asmall%2C%20often%20barely%20visible%20objects%20across%20diverse%20terrains.%20To%20address%20these%0Achallenges%2C%20multimodal%20learning%20can%20be%20used%20to%20integrate%20features%20from%0Adifferent%20data%20modalities%2C%20thereby%20improving%20detection%20accuracy.%20Nonetheless%2C%0Athe%20performance%20of%20multimodal%20learning%20is%20often%20constrained%20by%20the%20limited%20size%0Aof%20labeled%20datasets.%20In%20this%20paper%2C%20we%20propose%20to%20use%20Masked%20Image%20Modeling%0A%28MIM%29%20as%20a%20pre-training%20technique%2C%20leveraging%20self-supervised%20learning%20on%0Aunlabeled%20data%20to%20enhance%20detection%20performance.%20However%2C%20conventional%20MIM%20such%0Aas%20MAE%20which%20uses%20masked%20tokens%20without%20any%20contextual%20information%2C%20struggles%0Ato%20capture%20the%20fine-grained%20details%20due%20to%20a%20lack%20of%20interactions%20with%20other%0Aparts%20of%20image.%20To%20address%20this%2C%20we%20propose%20a%20new%20interactive%20MIM%20method%20that%0Acan%20establish%20interactions%20between%20different%20tokens%2C%20which%20is%20particularly%0Abeneficial%20for%20object%20detection%20in%20remote%20sensing.%20The%20extensive%20ablation%0Astudies%20and%20evluation%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08885v1&entry.124074799=Read"},
{"title": "Multi forests: Variable importance for multi-class outcomes", "author": "Roman Hornung and Alexander Hapfelmeier", "abstract": "  In prediction tasks with multi-class outcomes, identifying covariates\nspecifically associated with one or more outcome classes can be important.\nConventional variable importance measures (VIMs) from random forests (RFs),\nlike permutation and Gini importance, focus on overall predictive performance\nor node purity, without differentiating between the classes. Therefore, they\ncan be expected to fail to distinguish class-associated covariates from\ncovariates that only distinguish between groups of classes. We introduce a VIM\ncalled multi-class VIM, tailored for identifying exclusively class-associated\ncovariates, via a novel RF variant called multi forests (MuFs). The trees in\nMuFs use both multi-way and binary splitting. The multi-way splits generate\nchild nodes for each class, using a split criterion that evaluates how well\nthese nodes represent their respective classes. This setup forms the basis of\nthe multi-class VIM, which measures the discriminatory ability of the splits\nperformed in the respective covariates with regard to this split criterion.\nAlongside the multi-class VIM, we introduce a second VIM, the discriminatory\nVIM. This measure, based on the binary splits, assesses the strength of the\ngeneral influence of the covariates, irrespective of their\nclass-associatedness. Simulation studies demonstrate that the multi-class VIM\nspecifically ranks class-associated covariates highly, unlike conventional VIMs\nwhich also rank other types of covariates highly. Analyses of 121 datasets\nreveal that MuFs often have slightly lower predictive performance compared to\nconventional RFs. This is, however, not a limiting factor given the algorithm's\nprimary purpose of calculating the multi-class VIM.\n", "link": "http://arxiv.org/abs/2409.08925v1", "date": "2024-09-13", "relevancy": 1.7305, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4736}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4265}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi%20forests%3A%20Variable%20importance%20for%20multi-class%20outcomes&body=Title%3A%20Multi%20forests%3A%20Variable%20importance%20for%20multi-class%20outcomes%0AAuthor%3A%20Roman%20Hornung%20and%20Alexander%20Hapfelmeier%0AAbstract%3A%20%20%20In%20prediction%20tasks%20with%20multi-class%20outcomes%2C%20identifying%20covariates%0Aspecifically%20associated%20with%20one%20or%20more%20outcome%20classes%20can%20be%20important.%0AConventional%20variable%20importance%20measures%20%28VIMs%29%20from%20random%20forests%20%28RFs%29%2C%0Alike%20permutation%20and%20Gini%20importance%2C%20focus%20on%20overall%20predictive%20performance%0Aor%20node%20purity%2C%20without%20differentiating%20between%20the%20classes.%20Therefore%2C%20they%0Acan%20be%20expected%20to%20fail%20to%20distinguish%20class-associated%20covariates%20from%0Acovariates%20that%20only%20distinguish%20between%20groups%20of%20classes.%20We%20introduce%20a%20VIM%0Acalled%20multi-class%20VIM%2C%20tailored%20for%20identifying%20exclusively%20class-associated%0Acovariates%2C%20via%20a%20novel%20RF%20variant%20called%20multi%20forests%20%28MuFs%29.%20The%20trees%20in%0AMuFs%20use%20both%20multi-way%20and%20binary%20splitting.%20The%20multi-way%20splits%20generate%0Achild%20nodes%20for%20each%20class%2C%20using%20a%20split%20criterion%20that%20evaluates%20how%20well%0Athese%20nodes%20represent%20their%20respective%20classes.%20This%20setup%20forms%20the%20basis%20of%0Athe%20multi-class%20VIM%2C%20which%20measures%20the%20discriminatory%20ability%20of%20the%20splits%0Aperformed%20in%20the%20respective%20covariates%20with%20regard%20to%20this%20split%20criterion.%0AAlongside%20the%20multi-class%20VIM%2C%20we%20introduce%20a%20second%20VIM%2C%20the%20discriminatory%0AVIM.%20This%20measure%2C%20based%20on%20the%20binary%20splits%2C%20assesses%20the%20strength%20of%20the%0Ageneral%20influence%20of%20the%20covariates%2C%20irrespective%20of%20their%0Aclass-associatedness.%20Simulation%20studies%20demonstrate%20that%20the%20multi-class%20VIM%0Aspecifically%20ranks%20class-associated%20covariates%20highly%2C%20unlike%20conventional%20VIMs%0Awhich%20also%20rank%20other%20types%20of%20covariates%20highly.%20Analyses%20of%20121%20datasets%0Areveal%20that%20MuFs%20often%20have%20slightly%20lower%20predictive%20performance%20compared%20to%0Aconventional%20RFs.%20This%20is%2C%20however%2C%20not%20a%20limiting%20factor%20given%20the%20algorithm%27s%0Aprimary%20purpose%20of%20calculating%20the%20multi-class%20VIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti%2520forests%253A%2520Variable%2520importance%2520for%2520multi-class%2520outcomes%26entry.906535625%3DRoman%2520Hornung%2520and%2520Alexander%2520Hapfelmeier%26entry.1292438233%3D%2520%2520In%2520prediction%2520tasks%2520with%2520multi-class%2520outcomes%252C%2520identifying%2520covariates%250Aspecifically%2520associated%2520with%2520one%2520or%2520more%2520outcome%2520classes%2520can%2520be%2520important.%250AConventional%2520variable%2520importance%2520measures%2520%2528VIMs%2529%2520from%2520random%2520forests%2520%2528RFs%2529%252C%250Alike%2520permutation%2520and%2520Gini%2520importance%252C%2520focus%2520on%2520overall%2520predictive%2520performance%250Aor%2520node%2520purity%252C%2520without%2520differentiating%2520between%2520the%2520classes.%2520Therefore%252C%2520they%250Acan%2520be%2520expected%2520to%2520fail%2520to%2520distinguish%2520class-associated%2520covariates%2520from%250Acovariates%2520that%2520only%2520distinguish%2520between%2520groups%2520of%2520classes.%2520We%2520introduce%2520a%2520VIM%250Acalled%2520multi-class%2520VIM%252C%2520tailored%2520for%2520identifying%2520exclusively%2520class-associated%250Acovariates%252C%2520via%2520a%2520novel%2520RF%2520variant%2520called%2520multi%2520forests%2520%2528MuFs%2529.%2520The%2520trees%2520in%250AMuFs%2520use%2520both%2520multi-way%2520and%2520binary%2520splitting.%2520The%2520multi-way%2520splits%2520generate%250Achild%2520nodes%2520for%2520each%2520class%252C%2520using%2520a%2520split%2520criterion%2520that%2520evaluates%2520how%2520well%250Athese%2520nodes%2520represent%2520their%2520respective%2520classes.%2520This%2520setup%2520forms%2520the%2520basis%2520of%250Athe%2520multi-class%2520VIM%252C%2520which%2520measures%2520the%2520discriminatory%2520ability%2520of%2520the%2520splits%250Aperformed%2520in%2520the%2520respective%2520covariates%2520with%2520regard%2520to%2520this%2520split%2520criterion.%250AAlongside%2520the%2520multi-class%2520VIM%252C%2520we%2520introduce%2520a%2520second%2520VIM%252C%2520the%2520discriminatory%250AVIM.%2520This%2520measure%252C%2520based%2520on%2520the%2520binary%2520splits%252C%2520assesses%2520the%2520strength%2520of%2520the%250Ageneral%2520influence%2520of%2520the%2520covariates%252C%2520irrespective%2520of%2520their%250Aclass-associatedness.%2520Simulation%2520studies%2520demonstrate%2520that%2520the%2520multi-class%2520VIM%250Aspecifically%2520ranks%2520class-associated%2520covariates%2520highly%252C%2520unlike%2520conventional%2520VIMs%250Awhich%2520also%2520rank%2520other%2520types%2520of%2520covariates%2520highly.%2520Analyses%2520of%2520121%2520datasets%250Areveal%2520that%2520MuFs%2520often%2520have%2520slightly%2520lower%2520predictive%2520performance%2520compared%2520to%250Aconventional%2520RFs.%2520This%2520is%252C%2520however%252C%2520not%2520a%2520limiting%2520factor%2520given%2520the%2520algorithm%2527s%250Aprimary%2520purpose%2520of%2520calculating%2520the%2520multi-class%2520VIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi%20forests%3A%20Variable%20importance%20for%20multi-class%20outcomes&entry.906535625=Roman%20Hornung%20and%20Alexander%20Hapfelmeier&entry.1292438233=%20%20In%20prediction%20tasks%20with%20multi-class%20outcomes%2C%20identifying%20covariates%0Aspecifically%20associated%20with%20one%20or%20more%20outcome%20classes%20can%20be%20important.%0AConventional%20variable%20importance%20measures%20%28VIMs%29%20from%20random%20forests%20%28RFs%29%2C%0Alike%20permutation%20and%20Gini%20importance%2C%20focus%20on%20overall%20predictive%20performance%0Aor%20node%20purity%2C%20without%20differentiating%20between%20the%20classes.%20Therefore%2C%20they%0Acan%20be%20expected%20to%20fail%20to%20distinguish%20class-associated%20covariates%20from%0Acovariates%20that%20only%20distinguish%20between%20groups%20of%20classes.%20We%20introduce%20a%20VIM%0Acalled%20multi-class%20VIM%2C%20tailored%20for%20identifying%20exclusively%20class-associated%0Acovariates%2C%20via%20a%20novel%20RF%20variant%20called%20multi%20forests%20%28MuFs%29.%20The%20trees%20in%0AMuFs%20use%20both%20multi-way%20and%20binary%20splitting.%20The%20multi-way%20splits%20generate%0Achild%20nodes%20for%20each%20class%2C%20using%20a%20split%20criterion%20that%20evaluates%20how%20well%0Athese%20nodes%20represent%20their%20respective%20classes.%20This%20setup%20forms%20the%20basis%20of%0Athe%20multi-class%20VIM%2C%20which%20measures%20the%20discriminatory%20ability%20of%20the%20splits%0Aperformed%20in%20the%20respective%20covariates%20with%20regard%20to%20this%20split%20criterion.%0AAlongside%20the%20multi-class%20VIM%2C%20we%20introduce%20a%20second%20VIM%2C%20the%20discriminatory%0AVIM.%20This%20measure%2C%20based%20on%20the%20binary%20splits%2C%20assesses%20the%20strength%20of%20the%0Ageneral%20influence%20of%20the%20covariates%2C%20irrespective%20of%20their%0Aclass-associatedness.%20Simulation%20studies%20demonstrate%20that%20the%20multi-class%20VIM%0Aspecifically%20ranks%20class-associated%20covariates%20highly%2C%20unlike%20conventional%20VIMs%0Awhich%20also%20rank%20other%20types%20of%20covariates%20highly.%20Analyses%20of%20121%20datasets%0Areveal%20that%20MuFs%20often%20have%20slightly%20lower%20predictive%20performance%20compared%20to%0Aconventional%20RFs.%20This%20is%2C%20however%2C%20not%20a%20limiting%20factor%20given%20the%20algorithm%27s%0Aprimary%20purpose%20of%20calculating%20the%20multi-class%20VIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08925v1&entry.124074799=Read"},
{"title": "A Bayesian Approach to Clustering via the Proper Bayesian Bootstrap: the\n  Bayesian Bagged Clustering (BBC) algorithm", "author": "Federico Maria Quetti and Silvia Figini and Elena ballante", "abstract": "  The paper presents a novel approach for unsupervised techniques in the field\nof clustering. A new method is proposed to enhance existing literature models\nusing the proper Bayesian bootstrap to improve results in terms of robustness\nand interpretability. Our approach is organized in two steps: k-means\nclustering is used for prior elicitation, then proper Bayesian bootstrap is\napplied as resampling method in an ensemble clustering approach. Results are\nanalyzed introducing measures of uncertainty based on Shannon entropy. The\nproposal provides clear indication on the optimal number of clusters, as well\nas a better representation of the clustered data. Empirical results are\nprovided on simulated data showing the methodological and empirical advances\nobtained.\n", "link": "http://arxiv.org/abs/2409.08954v1", "date": "2024-09-13", "relevancy": 1.676, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4561}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4286}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bayesian%20Approach%20to%20Clustering%20via%20the%20Proper%20Bayesian%20Bootstrap%3A%20the%0A%20%20Bayesian%20Bagged%20Clustering%20%28BBC%29%20algorithm&body=Title%3A%20A%20Bayesian%20Approach%20to%20Clustering%20via%20the%20Proper%20Bayesian%20Bootstrap%3A%20the%0A%20%20Bayesian%20Bagged%20Clustering%20%28BBC%29%20algorithm%0AAuthor%3A%20Federico%20Maria%20Quetti%20and%20Silvia%20Figini%20and%20Elena%20ballante%0AAbstract%3A%20%20%20The%20paper%20presents%20a%20novel%20approach%20for%20unsupervised%20techniques%20in%20the%20field%0Aof%20clustering.%20A%20new%20method%20is%20proposed%20to%20enhance%20existing%20literature%20models%0Ausing%20the%20proper%20Bayesian%20bootstrap%20to%20improve%20results%20in%20terms%20of%20robustness%0Aand%20interpretability.%20Our%20approach%20is%20organized%20in%20two%20steps%3A%20k-means%0Aclustering%20is%20used%20for%20prior%20elicitation%2C%20then%20proper%20Bayesian%20bootstrap%20is%0Aapplied%20as%20resampling%20method%20in%20an%20ensemble%20clustering%20approach.%20Results%20are%0Aanalyzed%20introducing%20measures%20of%20uncertainty%20based%20on%20Shannon%20entropy.%20The%0Aproposal%20provides%20clear%20indication%20on%20the%20optimal%20number%20of%20clusters%2C%20as%20well%0Aas%20a%20better%20representation%20of%20the%20clustered%20data.%20Empirical%20results%20are%0Aprovided%20on%20simulated%20data%20showing%20the%20methodological%20and%20empirical%20advances%0Aobtained.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bayesian%2520Approach%2520to%2520Clustering%2520via%2520the%2520Proper%2520Bayesian%2520Bootstrap%253A%2520the%250A%2520%2520Bayesian%2520Bagged%2520Clustering%2520%2528BBC%2529%2520algorithm%26entry.906535625%3DFederico%2520Maria%2520Quetti%2520and%2520Silvia%2520Figini%2520and%2520Elena%2520ballante%26entry.1292438233%3D%2520%2520The%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520unsupervised%2520techniques%2520in%2520the%2520field%250Aof%2520clustering.%2520A%2520new%2520method%2520is%2520proposed%2520to%2520enhance%2520existing%2520literature%2520models%250Ausing%2520the%2520proper%2520Bayesian%2520bootstrap%2520to%2520improve%2520results%2520in%2520terms%2520of%2520robustness%250Aand%2520interpretability.%2520Our%2520approach%2520is%2520organized%2520in%2520two%2520steps%253A%2520k-means%250Aclustering%2520is%2520used%2520for%2520prior%2520elicitation%252C%2520then%2520proper%2520Bayesian%2520bootstrap%2520is%250Aapplied%2520as%2520resampling%2520method%2520in%2520an%2520ensemble%2520clustering%2520approach.%2520Results%2520are%250Aanalyzed%2520introducing%2520measures%2520of%2520uncertainty%2520based%2520on%2520Shannon%2520entropy.%2520The%250Aproposal%2520provides%2520clear%2520indication%2520on%2520the%2520optimal%2520number%2520of%2520clusters%252C%2520as%2520well%250Aas%2520a%2520better%2520representation%2520of%2520the%2520clustered%2520data.%2520Empirical%2520results%2520are%250Aprovided%2520on%2520simulated%2520data%2520showing%2520the%2520methodological%2520and%2520empirical%2520advances%250Aobtained.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bayesian%20Approach%20to%20Clustering%20via%20the%20Proper%20Bayesian%20Bootstrap%3A%20the%0A%20%20Bayesian%20Bagged%20Clustering%20%28BBC%29%20algorithm&entry.906535625=Federico%20Maria%20Quetti%20and%20Silvia%20Figini%20and%20Elena%20ballante&entry.1292438233=%20%20The%20paper%20presents%20a%20novel%20approach%20for%20unsupervised%20techniques%20in%20the%20field%0Aof%20clustering.%20A%20new%20method%20is%20proposed%20to%20enhance%20existing%20literature%20models%0Ausing%20the%20proper%20Bayesian%20bootstrap%20to%20improve%20results%20in%20terms%20of%20robustness%0Aand%20interpretability.%20Our%20approach%20is%20organized%20in%20two%20steps%3A%20k-means%0Aclustering%20is%20used%20for%20prior%20elicitation%2C%20then%20proper%20Bayesian%20bootstrap%20is%0Aapplied%20as%20resampling%20method%20in%20an%20ensemble%20clustering%20approach.%20Results%20are%0Aanalyzed%20introducing%20measures%20of%20uncertainty%20based%20on%20Shannon%20entropy.%20The%0Aproposal%20provides%20clear%20indication%20on%20the%20optimal%20number%20of%20clusters%2C%20as%20well%0Aas%20a%20better%20representation%20of%20the%20clustered%20data.%20Empirical%20results%20are%0Aprovided%20on%20simulated%20data%20showing%20the%20methodological%20and%20empirical%20advances%0Aobtained.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08954v1&entry.124074799=Read"},
{"title": "Measure-Theoretic Time-Delay Embedding", "author": "Jonah Botvinick-Greenhouse and Maria Oprea and Romit Maulik and Yunan Yang", "abstract": "  The celebrated Takens' embedding theorem provides a theoretical foundation\nfor reconstructing the full state of a dynamical system from partial\nobservations. However, the classical theorem assumes that the underlying system\nis deterministic and that observations are noise-free, limiting its\napplicability in real-world scenarios. Motivated by these limitations, we\nrigorously establish a measure-theoretic generalization that adopts an Eulerian\ndescription of the dynamics and recasts the embedding as a pushforward map\nbetween probability spaces. Our mathematical results leverage recent advances\nin optimal transportation theory. Building on our novel measure-theoretic\ntime-delay embedding theory, we have developed a new computational framework\nthat forecasts the full state of a dynamical system from time-lagged partial\nobservations, engineered with better robustness to handle sparse and noisy\ndata. We showcase the efficacy and versatility of our approach through several\nnumerical examples, ranging from the classic Lorenz-63 system to large-scale,\nreal-world applications such as NOAA sea surface temperature forecasting and\nERA5 wind field reconstruction.\n", "link": "http://arxiv.org/abs/2409.08768v1", "date": "2024-09-13", "relevancy": 1.3205, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4625}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4502}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measure-Theoretic%20Time-Delay%20Embedding&body=Title%3A%20Measure-Theoretic%20Time-Delay%20Embedding%0AAuthor%3A%20Jonah%20Botvinick-Greenhouse%20and%20Maria%20Oprea%20and%20Romit%20Maulik%20and%20Yunan%20Yang%0AAbstract%3A%20%20%20The%20celebrated%20Takens%27%20embedding%20theorem%20provides%20a%20theoretical%20foundation%0Afor%20reconstructing%20the%20full%20state%20of%20a%20dynamical%20system%20from%20partial%0Aobservations.%20However%2C%20the%20classical%20theorem%20assumes%20that%20the%20underlying%20system%0Ais%20deterministic%20and%20that%20observations%20are%20noise-free%2C%20limiting%20its%0Aapplicability%20in%20real-world%20scenarios.%20Motivated%20by%20these%20limitations%2C%20we%0Arigorously%20establish%20a%20measure-theoretic%20generalization%20that%20adopts%20an%20Eulerian%0Adescription%20of%20the%20dynamics%20and%20recasts%20the%20embedding%20as%20a%20pushforward%20map%0Abetween%20probability%20spaces.%20Our%20mathematical%20results%20leverage%20recent%20advances%0Ain%20optimal%20transportation%20theory.%20Building%20on%20our%20novel%20measure-theoretic%0Atime-delay%20embedding%20theory%2C%20we%20have%20developed%20a%20new%20computational%20framework%0Athat%20forecasts%20the%20full%20state%20of%20a%20dynamical%20system%20from%20time-lagged%20partial%0Aobservations%2C%20engineered%20with%20better%20robustness%20to%20handle%20sparse%20and%20noisy%0Adata.%20We%20showcase%20the%20efficacy%20and%20versatility%20of%20our%20approach%20through%20several%0Anumerical%20examples%2C%20ranging%20from%20the%20classic%20Lorenz-63%20system%20to%20large-scale%2C%0Areal-world%20applications%20such%20as%20NOAA%20sea%20surface%20temperature%20forecasting%20and%0AERA5%20wind%20field%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasure-Theoretic%2520Time-Delay%2520Embedding%26entry.906535625%3DJonah%2520Botvinick-Greenhouse%2520and%2520Maria%2520Oprea%2520and%2520Romit%2520Maulik%2520and%2520Yunan%2520Yang%26entry.1292438233%3D%2520%2520The%2520celebrated%2520Takens%2527%2520embedding%2520theorem%2520provides%2520a%2520theoretical%2520foundation%250Afor%2520reconstructing%2520the%2520full%2520state%2520of%2520a%2520dynamical%2520system%2520from%2520partial%250Aobservations.%2520However%252C%2520the%2520classical%2520theorem%2520assumes%2520that%2520the%2520underlying%2520system%250Ais%2520deterministic%2520and%2520that%2520observations%2520are%2520noise-free%252C%2520limiting%2520its%250Aapplicability%2520in%2520real-world%2520scenarios.%2520Motivated%2520by%2520these%2520limitations%252C%2520we%250Arigorously%2520establish%2520a%2520measure-theoretic%2520generalization%2520that%2520adopts%2520an%2520Eulerian%250Adescription%2520of%2520the%2520dynamics%2520and%2520recasts%2520the%2520embedding%2520as%2520a%2520pushforward%2520map%250Abetween%2520probability%2520spaces.%2520Our%2520mathematical%2520results%2520leverage%2520recent%2520advances%250Ain%2520optimal%2520transportation%2520theory.%2520Building%2520on%2520our%2520novel%2520measure-theoretic%250Atime-delay%2520embedding%2520theory%252C%2520we%2520have%2520developed%2520a%2520new%2520computational%2520framework%250Athat%2520forecasts%2520the%2520full%2520state%2520of%2520a%2520dynamical%2520system%2520from%2520time-lagged%2520partial%250Aobservations%252C%2520engineered%2520with%2520better%2520robustness%2520to%2520handle%2520sparse%2520and%2520noisy%250Adata.%2520We%2520showcase%2520the%2520efficacy%2520and%2520versatility%2520of%2520our%2520approach%2520through%2520several%250Anumerical%2520examples%252C%2520ranging%2520from%2520the%2520classic%2520Lorenz-63%2520system%2520to%2520large-scale%252C%250Areal-world%2520applications%2520such%2520as%2520NOAA%2520sea%2520surface%2520temperature%2520forecasting%2520and%250AERA5%2520wind%2520field%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measure-Theoretic%20Time-Delay%20Embedding&entry.906535625=Jonah%20Botvinick-Greenhouse%20and%20Maria%20Oprea%20and%20Romit%20Maulik%20and%20Yunan%20Yang&entry.1292438233=%20%20The%20celebrated%20Takens%27%20embedding%20theorem%20provides%20a%20theoretical%20foundation%0Afor%20reconstructing%20the%20full%20state%20of%20a%20dynamical%20system%20from%20partial%0Aobservations.%20However%2C%20the%20classical%20theorem%20assumes%20that%20the%20underlying%20system%0Ais%20deterministic%20and%20that%20observations%20are%20noise-free%2C%20limiting%20its%0Aapplicability%20in%20real-world%20scenarios.%20Motivated%20by%20these%20limitations%2C%20we%0Arigorously%20establish%20a%20measure-theoretic%20generalization%20that%20adopts%20an%20Eulerian%0Adescription%20of%20the%20dynamics%20and%20recasts%20the%20embedding%20as%20a%20pushforward%20map%0Abetween%20probability%20spaces.%20Our%20mathematical%20results%20leverage%20recent%20advances%0Ain%20optimal%20transportation%20theory.%20Building%20on%20our%20novel%20measure-theoretic%0Atime-delay%20embedding%20theory%2C%20we%20have%20developed%20a%20new%20computational%20framework%0Athat%20forecasts%20the%20full%20state%20of%20a%20dynamical%20system%20from%20time-lagged%20partial%0Aobservations%2C%20engineered%20with%20better%20robustness%20to%20handle%20sparse%20and%20noisy%0Adata.%20We%20showcase%20the%20efficacy%20and%20versatility%20of%20our%20approach%20through%20several%0Anumerical%20examples%2C%20ranging%20from%20the%20classic%20Lorenz-63%20system%20to%20large-scale%2C%0Areal-world%20applications%20such%20as%20NOAA%20sea%20surface%20temperature%20forecasting%20and%0AERA5%20wind%20field%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08768v1&entry.124074799=Read"},
{"title": "A call for embodied AI", "author": "Giuseppe Paolo and Jonas Gonzalez-Billandon and Bal\u00e1zs K\u00e9gl", "abstract": "  We propose Embodied AI as the next fundamental step in the pursuit of\nArtificial General Intelligence, juxtaposing it against current AI\nadvancements, particularly Large Language Models. We traverse the evolution of\nthe embodiment concept across diverse fields - philosophy, psychology,\nneuroscience, and robotics - to highlight how EAI distinguishes itself from the\nclassical paradigm of static learning. By broadening the scope of Embodied AI,\nwe introduce a theoretical framework based on cognitive architectures,\nemphasizing perception, action, memory, and learning as essential components of\nan embodied agent. This framework is aligned with Friston's active inference\nprinciple, offering a comprehensive approach to EAI development. Despite the\nprogress made in the field of AI, substantial challenges, such as the\nformulation of a novel AI learning theory and the innovation of advanced\nhardware, persist. Our discussion lays down a foundational guideline for future\nEmbodied AI research. Highlighting the importance of creating Embodied AI\nagents capable of seamless communication, collaboration, and coexistence with\nhumans and other intelligent entities within real-world environments, we aim to\nsteer the AI community towards addressing the multifaceted challenges and\nseizing the opportunities that lie ahead in the quest for AGI.\n", "link": "http://arxiv.org/abs/2402.03824v4", "date": "2024-09-13", "relevancy": 1.4045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4715}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4673}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20call%20for%20embodied%20AI&body=Title%3A%20A%20call%20for%20embodied%20AI%0AAuthor%3A%20Giuseppe%20Paolo%20and%20Jonas%20Gonzalez-Billandon%20and%20Bal%C3%A1zs%20K%C3%A9gl%0AAbstract%3A%20%20%20We%20propose%20Embodied%20AI%20as%20the%20next%20fundamental%20step%20in%20the%20pursuit%20of%0AArtificial%20General%20Intelligence%2C%20juxtaposing%20it%20against%20current%20AI%0Aadvancements%2C%20particularly%20Large%20Language%20Models.%20We%20traverse%20the%20evolution%20of%0Athe%20embodiment%20concept%20across%20diverse%20fields%20-%20philosophy%2C%20psychology%2C%0Aneuroscience%2C%20and%20robotics%20-%20to%20highlight%20how%20EAI%20distinguishes%20itself%20from%20the%0Aclassical%20paradigm%20of%20static%20learning.%20By%20broadening%20the%20scope%20of%20Embodied%20AI%2C%0Awe%20introduce%20a%20theoretical%20framework%20based%20on%20cognitive%20architectures%2C%0Aemphasizing%20perception%2C%20action%2C%20memory%2C%20and%20learning%20as%20essential%20components%20of%0Aan%20embodied%20agent.%20This%20framework%20is%20aligned%20with%20Friston%27s%20active%20inference%0Aprinciple%2C%20offering%20a%20comprehensive%20approach%20to%20EAI%20development.%20Despite%20the%0Aprogress%20made%20in%20the%20field%20of%20AI%2C%20substantial%20challenges%2C%20such%20as%20the%0Aformulation%20of%20a%20novel%20AI%20learning%20theory%20and%20the%20innovation%20of%20advanced%0Ahardware%2C%20persist.%20Our%20discussion%20lays%20down%20a%20foundational%20guideline%20for%20future%0AEmbodied%20AI%20research.%20Highlighting%20the%20importance%20of%20creating%20Embodied%20AI%0Aagents%20capable%20of%20seamless%20communication%2C%20collaboration%2C%20and%20coexistence%20with%0Ahumans%20and%20other%20intelligent%20entities%20within%20real-world%20environments%2C%20we%20aim%20to%0Asteer%20the%20AI%20community%20towards%20addressing%20the%20multifaceted%20challenges%20and%0Aseizing%20the%20opportunities%20that%20lie%20ahead%20in%20the%20quest%20for%20AGI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03824v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520call%2520for%2520embodied%2520AI%26entry.906535625%3DGiuseppe%2520Paolo%2520and%2520Jonas%2520Gonzalez-Billandon%2520and%2520Bal%25C3%25A1zs%2520K%25C3%25A9gl%26entry.1292438233%3D%2520%2520We%2520propose%2520Embodied%2520AI%2520as%2520the%2520next%2520fundamental%2520step%2520in%2520the%2520pursuit%2520of%250AArtificial%2520General%2520Intelligence%252C%2520juxtaposing%2520it%2520against%2520current%2520AI%250Aadvancements%252C%2520particularly%2520Large%2520Language%2520Models.%2520We%2520traverse%2520the%2520evolution%2520of%250Athe%2520embodiment%2520concept%2520across%2520diverse%2520fields%2520-%2520philosophy%252C%2520psychology%252C%250Aneuroscience%252C%2520and%2520robotics%2520-%2520to%2520highlight%2520how%2520EAI%2520distinguishes%2520itself%2520from%2520the%250Aclassical%2520paradigm%2520of%2520static%2520learning.%2520By%2520broadening%2520the%2520scope%2520of%2520Embodied%2520AI%252C%250Awe%2520introduce%2520a%2520theoretical%2520framework%2520based%2520on%2520cognitive%2520architectures%252C%250Aemphasizing%2520perception%252C%2520action%252C%2520memory%252C%2520and%2520learning%2520as%2520essential%2520components%2520of%250Aan%2520embodied%2520agent.%2520This%2520framework%2520is%2520aligned%2520with%2520Friston%2527s%2520active%2520inference%250Aprinciple%252C%2520offering%2520a%2520comprehensive%2520approach%2520to%2520EAI%2520development.%2520Despite%2520the%250Aprogress%2520made%2520in%2520the%2520field%2520of%2520AI%252C%2520substantial%2520challenges%252C%2520such%2520as%2520the%250Aformulation%2520of%2520a%2520novel%2520AI%2520learning%2520theory%2520and%2520the%2520innovation%2520of%2520advanced%250Ahardware%252C%2520persist.%2520Our%2520discussion%2520lays%2520down%2520a%2520foundational%2520guideline%2520for%2520future%250AEmbodied%2520AI%2520research.%2520Highlighting%2520the%2520importance%2520of%2520creating%2520Embodied%2520AI%250Aagents%2520capable%2520of%2520seamless%2520communication%252C%2520collaboration%252C%2520and%2520coexistence%2520with%250Ahumans%2520and%2520other%2520intelligent%2520entities%2520within%2520real-world%2520environments%252C%2520we%2520aim%2520to%250Asteer%2520the%2520AI%2520community%2520towards%2520addressing%2520the%2520multifaceted%2520challenges%2520and%250Aseizing%2520the%2520opportunities%2520that%2520lie%2520ahead%2520in%2520the%2520quest%2520for%2520AGI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03824v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20call%20for%20embodied%20AI&entry.906535625=Giuseppe%20Paolo%20and%20Jonas%20Gonzalez-Billandon%20and%20Bal%C3%A1zs%20K%C3%A9gl&entry.1292438233=%20%20We%20propose%20Embodied%20AI%20as%20the%20next%20fundamental%20step%20in%20the%20pursuit%20of%0AArtificial%20General%20Intelligence%2C%20juxtaposing%20it%20against%20current%20AI%0Aadvancements%2C%20particularly%20Large%20Language%20Models.%20We%20traverse%20the%20evolution%20of%0Athe%20embodiment%20concept%20across%20diverse%20fields%20-%20philosophy%2C%20psychology%2C%0Aneuroscience%2C%20and%20robotics%20-%20to%20highlight%20how%20EAI%20distinguishes%20itself%20from%20the%0Aclassical%20paradigm%20of%20static%20learning.%20By%20broadening%20the%20scope%20of%20Embodied%20AI%2C%0Awe%20introduce%20a%20theoretical%20framework%20based%20on%20cognitive%20architectures%2C%0Aemphasizing%20perception%2C%20action%2C%20memory%2C%20and%20learning%20as%20essential%20components%20of%0Aan%20embodied%20agent.%20This%20framework%20is%20aligned%20with%20Friston%27s%20active%20inference%0Aprinciple%2C%20offering%20a%20comprehensive%20approach%20to%20EAI%20development.%20Despite%20the%0Aprogress%20made%20in%20the%20field%20of%20AI%2C%20substantial%20challenges%2C%20such%20as%20the%0Aformulation%20of%20a%20novel%20AI%20learning%20theory%20and%20the%20innovation%20of%20advanced%0Ahardware%2C%20persist.%20Our%20discussion%20lays%20down%20a%20foundational%20guideline%20for%20future%0AEmbodied%20AI%20research.%20Highlighting%20the%20importance%20of%20creating%20Embodied%20AI%0Aagents%20capable%20of%20seamless%20communication%2C%20collaboration%2C%20and%20coexistence%20with%0Ahumans%20and%20other%20intelligent%20entities%20within%20real-world%20environments%2C%20we%20aim%20to%0Asteer%20the%20AI%20community%20towards%20addressing%20the%20multifaceted%20challenges%20and%0Aseizing%20the%20opportunities%20that%20lie%20ahead%20in%20the%20quest%20for%20AGI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03824v4&entry.124074799=Read"},
{"title": "E2MoCase: A Dataset for Emotional, Event and Moral Observations in News\n  Articles on High-impact Legal Cases", "author": "Candida M. Greco and Lorenzo Zangari and Davide Picca and Andrea Tagarelli", "abstract": "  The way media reports on legal cases can significantly shape public opinion,\noften embedding subtle biases that influence societal views on justice and\nmorality. Analyzing these biases requires a holistic approach that captures the\nemotional tone, moral framing, and specific events within the narratives. In\nthis work we introduce E2MoCase, a novel dataset designed to facilitate the\nintegrated analysis of emotions, moral values, and events within legal\nnarratives and media coverage. By leveraging advanced models for emotion\ndetection, moral value identification, and event extraction, E2MoCase offers a\nmulti-dimensional perspective on how legal cases are portrayed in news\narticles.\n", "link": "http://arxiv.org/abs/2409.09001v1", "date": "2024-09-13", "relevancy": 0.8595, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.439}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4282}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E2MoCase%3A%20A%20Dataset%20for%20Emotional%2C%20Event%20and%20Moral%20Observations%20in%20News%0A%20%20Articles%20on%20High-impact%20Legal%20Cases&body=Title%3A%20E2MoCase%3A%20A%20Dataset%20for%20Emotional%2C%20Event%20and%20Moral%20Observations%20in%20News%0A%20%20Articles%20on%20High-impact%20Legal%20Cases%0AAuthor%3A%20Candida%20M.%20Greco%20and%20Lorenzo%20Zangari%20and%20Davide%20Picca%20and%20Andrea%20Tagarelli%0AAbstract%3A%20%20%20The%20way%20media%20reports%20on%20legal%20cases%20can%20significantly%20shape%20public%20opinion%2C%0Aoften%20embedding%20subtle%20biases%20that%20influence%20societal%20views%20on%20justice%20and%0Amorality.%20Analyzing%20these%20biases%20requires%20a%20holistic%20approach%20that%20captures%20the%0Aemotional%20tone%2C%20moral%20framing%2C%20and%20specific%20events%20within%20the%20narratives.%20In%0Athis%20work%20we%20introduce%20E2MoCase%2C%20a%20novel%20dataset%20designed%20to%20facilitate%20the%0Aintegrated%20analysis%20of%20emotions%2C%20moral%20values%2C%20and%20events%20within%20legal%0Anarratives%20and%20media%20coverage.%20By%20leveraging%20advanced%20models%20for%20emotion%0Adetection%2C%20moral%20value%20identification%2C%20and%20event%20extraction%2C%20E2MoCase%20offers%20a%0Amulti-dimensional%20perspective%20on%20how%20legal%20cases%20are%20portrayed%20in%20news%0Aarticles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE2MoCase%253A%2520A%2520Dataset%2520for%2520Emotional%252C%2520Event%2520and%2520Moral%2520Observations%2520in%2520News%250A%2520%2520Articles%2520on%2520High-impact%2520Legal%2520Cases%26entry.906535625%3DCandida%2520M.%2520Greco%2520and%2520Lorenzo%2520Zangari%2520and%2520Davide%2520Picca%2520and%2520Andrea%2520Tagarelli%26entry.1292438233%3D%2520%2520The%2520way%2520media%2520reports%2520on%2520legal%2520cases%2520can%2520significantly%2520shape%2520public%2520opinion%252C%250Aoften%2520embedding%2520subtle%2520biases%2520that%2520influence%2520societal%2520views%2520on%2520justice%2520and%250Amorality.%2520Analyzing%2520these%2520biases%2520requires%2520a%2520holistic%2520approach%2520that%2520captures%2520the%250Aemotional%2520tone%252C%2520moral%2520framing%252C%2520and%2520specific%2520events%2520within%2520the%2520narratives.%2520In%250Athis%2520work%2520we%2520introduce%2520E2MoCase%252C%2520a%2520novel%2520dataset%2520designed%2520to%2520facilitate%2520the%250Aintegrated%2520analysis%2520of%2520emotions%252C%2520moral%2520values%252C%2520and%2520events%2520within%2520legal%250Anarratives%2520and%2520media%2520coverage.%2520By%2520leveraging%2520advanced%2520models%2520for%2520emotion%250Adetection%252C%2520moral%2520value%2520identification%252C%2520and%2520event%2520extraction%252C%2520E2MoCase%2520offers%2520a%250Amulti-dimensional%2520perspective%2520on%2520how%2520legal%2520cases%2520are%2520portrayed%2520in%2520news%250Aarticles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E2MoCase%3A%20A%20Dataset%20for%20Emotional%2C%20Event%20and%20Moral%20Observations%20in%20News%0A%20%20Articles%20on%20High-impact%20Legal%20Cases&entry.906535625=Candida%20M.%20Greco%20and%20Lorenzo%20Zangari%20and%20Davide%20Picca%20and%20Andrea%20Tagarelli&entry.1292438233=%20%20The%20way%20media%20reports%20on%20legal%20cases%20can%20significantly%20shape%20public%20opinion%2C%0Aoften%20embedding%20subtle%20biases%20that%20influence%20societal%20views%20on%20justice%20and%0Amorality.%20Analyzing%20these%20biases%20requires%20a%20holistic%20approach%20that%20captures%20the%0Aemotional%20tone%2C%20moral%20framing%2C%20and%20specific%20events%20within%20the%20narratives.%20In%0Athis%20work%20we%20introduce%20E2MoCase%2C%20a%20novel%20dataset%20designed%20to%20facilitate%20the%0Aintegrated%20analysis%20of%20emotions%2C%20moral%20values%2C%20and%20events%20within%20legal%0Anarratives%20and%20media%20coverage.%20By%20leveraging%20advanced%20models%20for%20emotion%0Adetection%2C%20moral%20value%20identification%2C%20and%20event%20extraction%2C%20E2MoCase%20offers%20a%0Amulti-dimensional%20perspective%20on%20how%20legal%20cases%20are%20portrayed%20in%20news%0Aarticles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09001v1&entry.124074799=Read"},
{"title": "A Methodology to Study the Impact of Spiking Neural Network Parameters\n  considering Event-Based Automotive Data", "author": "Iqra Bano and Rachmad Vidya Wicaksana Putra and Alberto Marchisio and Muhammad Shafique", "abstract": "  Autonomous Driving (AD) systems are considered as the future of human\nmobility and transportation. Solving computer vision tasks such as image\nclassification and object detection/segmentation, with high accuracy and low\npower/energy consumption, is highly needed to realize AD systems in real life.\nThese requirements can potentially be satisfied by Spiking Neural Networks\n(SNNs). However, the state-of-the-art works in SNN-based AD systems still focus\non proposing network models that can achieve high accuracy, and they have not\nsystematically studied the roles of SNN parameters when used for learning\nevent-based automotive data. Therefore, we still lack understanding of how to\neffectively develop SNN models for AD systems. Toward this, we propose a novel\nmethodology to systematically study and analyze the impact of SNN parameters\nconsidering event-based automotive data, then leverage this analysis for\nenhancing SNN developments. To do this, we first explore different settings of\nSNN parameters that directly affect the learning mechanism (i.e., batch size,\nlearning rate, neuron threshold potential, and weight decay), then analyze the\naccuracy results. Afterward, we propose techniques that jointly improve SNN\naccuracy and reduce training time. Experimental results show that our\nmethodology can improve the SNN models for AD systems than the\nstate-of-the-art, as it achieves higher accuracy (i.e., 86%) for the NCARS\ndataset, and it can also achieve iso-accuracy (i.e., ~85% with standard\ndeviation less than 0.5%) while speeding up the training time by 1.9x. In this\nmanner, our research work provides a set of guidelines for SNN parameter\nenhancements, thereby enabling the practical developments of SNN-based AD\nsystems.\n", "link": "http://arxiv.org/abs/2404.03493v3", "date": "2024-09-13", "relevancy": 1.552, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5309}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Methodology%20to%20Study%20the%20Impact%20of%20Spiking%20Neural%20Network%20Parameters%0A%20%20considering%20Event-Based%20Automotive%20Data&body=Title%3A%20A%20Methodology%20to%20Study%20the%20Impact%20of%20Spiking%20Neural%20Network%20Parameters%0A%20%20considering%20Event-Based%20Automotive%20Data%0AAuthor%3A%20Iqra%20Bano%20and%20Rachmad%20Vidya%20Wicaksana%20Putra%20and%20Alberto%20Marchisio%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20Autonomous%20Driving%20%28AD%29%20systems%20are%20considered%20as%20the%20future%20of%20human%0Amobility%20and%20transportation.%20Solving%20computer%20vision%20tasks%20such%20as%20image%0Aclassification%20and%20object%20detection/segmentation%2C%20with%20high%20accuracy%20and%20low%0Apower/energy%20consumption%2C%20is%20highly%20needed%20to%20realize%20AD%20systems%20in%20real%20life.%0AThese%20requirements%20can%20potentially%20be%20satisfied%20by%20Spiking%20Neural%20Networks%0A%28SNNs%29.%20However%2C%20the%20state-of-the-art%20works%20in%20SNN-based%20AD%20systems%20still%20focus%0Aon%20proposing%20network%20models%20that%20can%20achieve%20high%20accuracy%2C%20and%20they%20have%20not%0Asystematically%20studied%20the%20roles%20of%20SNN%20parameters%20when%20used%20for%20learning%0Aevent-based%20automotive%20data.%20Therefore%2C%20we%20still%20lack%20understanding%20of%20how%20to%0Aeffectively%20develop%20SNN%20models%20for%20AD%20systems.%20Toward%20this%2C%20we%20propose%20a%20novel%0Amethodology%20to%20systematically%20study%20and%20analyze%20the%20impact%20of%20SNN%20parameters%0Aconsidering%20event-based%20automotive%20data%2C%20then%20leverage%20this%20analysis%20for%0Aenhancing%20SNN%20developments.%20To%20do%20this%2C%20we%20first%20explore%20different%20settings%20of%0ASNN%20parameters%20that%20directly%20affect%20the%20learning%20mechanism%20%28i.e.%2C%20batch%20size%2C%0Alearning%20rate%2C%20neuron%20threshold%20potential%2C%20and%20weight%20decay%29%2C%20then%20analyze%20the%0Aaccuracy%20results.%20Afterward%2C%20we%20propose%20techniques%20that%20jointly%20improve%20SNN%0Aaccuracy%20and%20reduce%20training%20time.%20Experimental%20results%20show%20that%20our%0Amethodology%20can%20improve%20the%20SNN%20models%20for%20AD%20systems%20than%20the%0Astate-of-the-art%2C%20as%20it%20achieves%20higher%20accuracy%20%28i.e.%2C%2086%25%29%20for%20the%20NCARS%0Adataset%2C%20and%20it%20can%20also%20achieve%20iso-accuracy%20%28i.e.%2C%20~85%25%20with%20standard%0Adeviation%20less%20than%200.5%25%29%20while%20speeding%20up%20the%20training%20time%20by%201.9x.%20In%20this%0Amanner%2C%20our%20research%20work%20provides%20a%20set%20of%20guidelines%20for%20SNN%20parameter%0Aenhancements%2C%20thereby%20enabling%20the%20practical%20developments%20of%20SNN-based%20AD%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Methodology%2520to%2520Study%2520the%2520Impact%2520of%2520Spiking%2520Neural%2520Network%2520Parameters%250A%2520%2520considering%2520Event-Based%2520Automotive%2520Data%26entry.906535625%3DIqra%2520Bano%2520and%2520Rachmad%2520Vidya%2520Wicaksana%2520Putra%2520and%2520Alberto%2520Marchisio%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3D%2520%2520Autonomous%2520Driving%2520%2528AD%2529%2520systems%2520are%2520considered%2520as%2520the%2520future%2520of%2520human%250Amobility%2520and%2520transportation.%2520Solving%2520computer%2520vision%2520tasks%2520such%2520as%2520image%250Aclassification%2520and%2520object%2520detection/segmentation%252C%2520with%2520high%2520accuracy%2520and%2520low%250Apower/energy%2520consumption%252C%2520is%2520highly%2520needed%2520to%2520realize%2520AD%2520systems%2520in%2520real%2520life.%250AThese%2520requirements%2520can%2520potentially%2520be%2520satisfied%2520by%2520Spiking%2520Neural%2520Networks%250A%2528SNNs%2529.%2520However%252C%2520the%2520state-of-the-art%2520works%2520in%2520SNN-based%2520AD%2520systems%2520still%2520focus%250Aon%2520proposing%2520network%2520models%2520that%2520can%2520achieve%2520high%2520accuracy%252C%2520and%2520they%2520have%2520not%250Asystematically%2520studied%2520the%2520roles%2520of%2520SNN%2520parameters%2520when%2520used%2520for%2520learning%250Aevent-based%2520automotive%2520data.%2520Therefore%252C%2520we%2520still%2520lack%2520understanding%2520of%2520how%2520to%250Aeffectively%2520develop%2520SNN%2520models%2520for%2520AD%2520systems.%2520Toward%2520this%252C%2520we%2520propose%2520a%2520novel%250Amethodology%2520to%2520systematically%2520study%2520and%2520analyze%2520the%2520impact%2520of%2520SNN%2520parameters%250Aconsidering%2520event-based%2520automotive%2520data%252C%2520then%2520leverage%2520this%2520analysis%2520for%250Aenhancing%2520SNN%2520developments.%2520To%2520do%2520this%252C%2520we%2520first%2520explore%2520different%2520settings%2520of%250ASNN%2520parameters%2520that%2520directly%2520affect%2520the%2520learning%2520mechanism%2520%2528i.e.%252C%2520batch%2520size%252C%250Alearning%2520rate%252C%2520neuron%2520threshold%2520potential%252C%2520and%2520weight%2520decay%2529%252C%2520then%2520analyze%2520the%250Aaccuracy%2520results.%2520Afterward%252C%2520we%2520propose%2520techniques%2520that%2520jointly%2520improve%2520SNN%250Aaccuracy%2520and%2520reduce%2520training%2520time.%2520Experimental%2520results%2520show%2520that%2520our%250Amethodology%2520can%2520improve%2520the%2520SNN%2520models%2520for%2520AD%2520systems%2520than%2520the%250Astate-of-the-art%252C%2520as%2520it%2520achieves%2520higher%2520accuracy%2520%2528i.e.%252C%252086%2525%2529%2520for%2520the%2520NCARS%250Adataset%252C%2520and%2520it%2520can%2520also%2520achieve%2520iso-accuracy%2520%2528i.e.%252C%2520~85%2525%2520with%2520standard%250Adeviation%2520less%2520than%25200.5%2525%2529%2520while%2520speeding%2520up%2520the%2520training%2520time%2520by%25201.9x.%2520In%2520this%250Amanner%252C%2520our%2520research%2520work%2520provides%2520a%2520set%2520of%2520guidelines%2520for%2520SNN%2520parameter%250Aenhancements%252C%2520thereby%2520enabling%2520the%2520practical%2520developments%2520of%2520SNN-based%2520AD%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Methodology%20to%20Study%20the%20Impact%20of%20Spiking%20Neural%20Network%20Parameters%0A%20%20considering%20Event-Based%20Automotive%20Data&entry.906535625=Iqra%20Bano%20and%20Rachmad%20Vidya%20Wicaksana%20Putra%20and%20Alberto%20Marchisio%20and%20Muhammad%20Shafique&entry.1292438233=%20%20Autonomous%20Driving%20%28AD%29%20systems%20are%20considered%20as%20the%20future%20of%20human%0Amobility%20and%20transportation.%20Solving%20computer%20vision%20tasks%20such%20as%20image%0Aclassification%20and%20object%20detection/segmentation%2C%20with%20high%20accuracy%20and%20low%0Apower/energy%20consumption%2C%20is%20highly%20needed%20to%20realize%20AD%20systems%20in%20real%20life.%0AThese%20requirements%20can%20potentially%20be%20satisfied%20by%20Spiking%20Neural%20Networks%0A%28SNNs%29.%20However%2C%20the%20state-of-the-art%20works%20in%20SNN-based%20AD%20systems%20still%20focus%0Aon%20proposing%20network%20models%20that%20can%20achieve%20high%20accuracy%2C%20and%20they%20have%20not%0Asystematically%20studied%20the%20roles%20of%20SNN%20parameters%20when%20used%20for%20learning%0Aevent-based%20automotive%20data.%20Therefore%2C%20we%20still%20lack%20understanding%20of%20how%20to%0Aeffectively%20develop%20SNN%20models%20for%20AD%20systems.%20Toward%20this%2C%20we%20propose%20a%20novel%0Amethodology%20to%20systematically%20study%20and%20analyze%20the%20impact%20of%20SNN%20parameters%0Aconsidering%20event-based%20automotive%20data%2C%20then%20leverage%20this%20analysis%20for%0Aenhancing%20SNN%20developments.%20To%20do%20this%2C%20we%20first%20explore%20different%20settings%20of%0ASNN%20parameters%20that%20directly%20affect%20the%20learning%20mechanism%20%28i.e.%2C%20batch%20size%2C%0Alearning%20rate%2C%20neuron%20threshold%20potential%2C%20and%20weight%20decay%29%2C%20then%20analyze%20the%0Aaccuracy%20results.%20Afterward%2C%20we%20propose%20techniques%20that%20jointly%20improve%20SNN%0Aaccuracy%20and%20reduce%20training%20time.%20Experimental%20results%20show%20that%20our%0Amethodology%20can%20improve%20the%20SNN%20models%20for%20AD%20systems%20than%20the%0Astate-of-the-art%2C%20as%20it%20achieves%20higher%20accuracy%20%28i.e.%2C%2086%25%29%20for%20the%20NCARS%0Adataset%2C%20and%20it%20can%20also%20achieve%20iso-accuracy%20%28i.e.%2C%20~85%25%20with%20standard%0Adeviation%20less%20than%200.5%25%29%20while%20speeding%20up%20the%20training%20time%20by%201.9x.%20In%20this%0Amanner%2C%20our%20research%20work%20provides%20a%20set%20of%20guidelines%20for%20SNN%20parameter%0Aenhancements%2C%20thereby%20enabling%20the%20practical%20developments%20of%20SNN-based%20AD%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03493v3&entry.124074799=Read"},
{"title": "Deep Learning-based Codes for Wiretap Fading Channels", "author": "Daniel Seifert and Onur G\u00fcnl\u00fc and Rafael F. Schaefer", "abstract": "  The wiretap channel is a well-studied problem in the physical layer security\n(PLS) literature. Although it is proven that the decoding error probability and\ninformation leakage can be made arbitrarily small in the asymptotic regime,\nfurther research on finite-blocklength codes is required on the path towards\npractical, secure communications systems. This work provides the first\nexperimental characterization of a deep learning-based, finite-blocklength code\nconstruction for multi-tap fading wiretap channels without channel state\ninformation (CSI). In addition to the evaluation of the average probability of\nerror and information leakage, we illustrate the influence of (i) the number of\nfading taps, (ii) differing variances of the fading coefficients and (iii) the\nseed selection for the hash function-based security layer.\n", "link": "http://arxiv.org/abs/2409.08786v1", "date": "2024-09-13", "relevancy": 1.6427, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4409}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.407}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-based%20Codes%20for%20Wiretap%20Fading%20Channels&body=Title%3A%20Deep%20Learning-based%20Codes%20for%20Wiretap%20Fading%20Channels%0AAuthor%3A%20Daniel%20Seifert%20and%20Onur%20G%C3%BCnl%C3%BC%20and%20Rafael%20F.%20Schaefer%0AAbstract%3A%20%20%20The%20wiretap%20channel%20is%20a%20well-studied%20problem%20in%20the%20physical%20layer%20security%0A%28PLS%29%20literature.%20Although%20it%20is%20proven%20that%20the%20decoding%20error%20probability%20and%0Ainformation%20leakage%20can%20be%20made%20arbitrarily%20small%20in%20the%20asymptotic%20regime%2C%0Afurther%20research%20on%20finite-blocklength%20codes%20is%20required%20on%20the%20path%20towards%0Apractical%2C%20secure%20communications%20systems.%20This%20work%20provides%20the%20first%0Aexperimental%20characterization%20of%20a%20deep%20learning-based%2C%20finite-blocklength%20code%0Aconstruction%20for%20multi-tap%20fading%20wiretap%20channels%20without%20channel%20state%0Ainformation%20%28CSI%29.%20In%20addition%20to%20the%20evaluation%20of%20the%20average%20probability%20of%0Aerror%20and%20information%20leakage%2C%20we%20illustrate%20the%20influence%20of%20%28i%29%20the%20number%20of%0Afading%20taps%2C%20%28ii%29%20differing%20variances%20of%20the%20fading%20coefficients%20and%20%28iii%29%20the%0Aseed%20selection%20for%20the%20hash%20function-based%20security%20layer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-based%2520Codes%2520for%2520Wiretap%2520Fading%2520Channels%26entry.906535625%3DDaniel%2520Seifert%2520and%2520Onur%2520G%25C3%25BCnl%25C3%25BC%2520and%2520Rafael%2520F.%2520Schaefer%26entry.1292438233%3D%2520%2520The%2520wiretap%2520channel%2520is%2520a%2520well-studied%2520problem%2520in%2520the%2520physical%2520layer%2520security%250A%2528PLS%2529%2520literature.%2520Although%2520it%2520is%2520proven%2520that%2520the%2520decoding%2520error%2520probability%2520and%250Ainformation%2520leakage%2520can%2520be%2520made%2520arbitrarily%2520small%2520in%2520the%2520asymptotic%2520regime%252C%250Afurther%2520research%2520on%2520finite-blocklength%2520codes%2520is%2520required%2520on%2520the%2520path%2520towards%250Apractical%252C%2520secure%2520communications%2520systems.%2520This%2520work%2520provides%2520the%2520first%250Aexperimental%2520characterization%2520of%2520a%2520deep%2520learning-based%252C%2520finite-blocklength%2520code%250Aconstruction%2520for%2520multi-tap%2520fading%2520wiretap%2520channels%2520without%2520channel%2520state%250Ainformation%2520%2528CSI%2529.%2520In%2520addition%2520to%2520the%2520evaluation%2520of%2520the%2520average%2520probability%2520of%250Aerror%2520and%2520information%2520leakage%252C%2520we%2520illustrate%2520the%2520influence%2520of%2520%2528i%2529%2520the%2520number%2520of%250Afading%2520taps%252C%2520%2528ii%2529%2520differing%2520variances%2520of%2520the%2520fading%2520coefficients%2520and%2520%2528iii%2529%2520the%250Aseed%2520selection%2520for%2520the%2520hash%2520function-based%2520security%2520layer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-based%20Codes%20for%20Wiretap%20Fading%20Channels&entry.906535625=Daniel%20Seifert%20and%20Onur%20G%C3%BCnl%C3%BC%20and%20Rafael%20F.%20Schaefer&entry.1292438233=%20%20The%20wiretap%20channel%20is%20a%20well-studied%20problem%20in%20the%20physical%20layer%20security%0A%28PLS%29%20literature.%20Although%20it%20is%20proven%20that%20the%20decoding%20error%20probability%20and%0Ainformation%20leakage%20can%20be%20made%20arbitrarily%20small%20in%20the%20asymptotic%20regime%2C%0Afurther%20research%20on%20finite-blocklength%20codes%20is%20required%20on%20the%20path%20towards%0Apractical%2C%20secure%20communications%20systems.%20This%20work%20provides%20the%20first%0Aexperimental%20characterization%20of%20a%20deep%20learning-based%2C%20finite-blocklength%20code%0Aconstruction%20for%20multi-tap%20fading%20wiretap%20channels%20without%20channel%20state%0Ainformation%20%28CSI%29.%20In%20addition%20to%20the%20evaluation%20of%20the%20average%20probability%20of%0Aerror%20and%20information%20leakage%2C%20we%20illustrate%20the%20influence%20of%20%28i%29%20the%20number%20of%0Afading%20taps%2C%20%28ii%29%20differing%20variances%20of%20the%20fading%20coefficients%20and%20%28iii%29%20the%0Aseed%20selection%20for%20the%20hash%20function-based%20security%20layer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08786v1&entry.124074799=Read"},
{"title": "Exploring the Links between the Fundamental Lemma and Kernel Regression", "author": "Oleksii Molodchyk and Timm Faulwasser", "abstract": "  Generalizations and variations of the fundamental lemma by Willems et al. are\nan active topic of recent research. In this note, we explore and formalize the\nlinks between kernel regression and some known nonlinear extensions of the\nfundamental lemma. Applying a transformation to the usual linear equation in\nHankel matrices, we arrive at an alternative implicit kernel representation of\nthe system trajectories while keeping the requirements on persistency of\nexcitation. We show that this representation is equivalent to the solution of a\nspecific kernel regression problem. We explore the possible structures of the\nunderlying kernel as well as the system classes to which they correspond.\n", "link": "http://arxiv.org/abs/2403.05368v2", "date": "2024-09-13", "relevancy": 1.5379, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4113}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3868}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Links%20between%20the%20Fundamental%20Lemma%20and%20Kernel%20Regression&body=Title%3A%20Exploring%20the%20Links%20between%20the%20Fundamental%20Lemma%20and%20Kernel%20Regression%0AAuthor%3A%20Oleksii%20Molodchyk%20and%20Timm%20Faulwasser%0AAbstract%3A%20%20%20Generalizations%20and%20variations%20of%20the%20fundamental%20lemma%20by%20Willems%20et%20al.%20are%0Aan%20active%20topic%20of%20recent%20research.%20In%20this%20note%2C%20we%20explore%20and%20formalize%20the%0Alinks%20between%20kernel%20regression%20and%20some%20known%20nonlinear%20extensions%20of%20the%0Afundamental%20lemma.%20Applying%20a%20transformation%20to%20the%20usual%20linear%20equation%20in%0AHankel%20matrices%2C%20we%20arrive%20at%20an%20alternative%20implicit%20kernel%20representation%20of%0Athe%20system%20trajectories%20while%20keeping%20the%20requirements%20on%20persistency%20of%0Aexcitation.%20We%20show%20that%20this%20representation%20is%20equivalent%20to%20the%20solution%20of%20a%0Aspecific%20kernel%20regression%20problem.%20We%20explore%20the%20possible%20structures%20of%20the%0Aunderlying%20kernel%20as%20well%20as%20the%20system%20classes%20to%20which%20they%20correspond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Links%2520between%2520the%2520Fundamental%2520Lemma%2520and%2520Kernel%2520Regression%26entry.906535625%3DOleksii%2520Molodchyk%2520and%2520Timm%2520Faulwasser%26entry.1292438233%3D%2520%2520Generalizations%2520and%2520variations%2520of%2520the%2520fundamental%2520lemma%2520by%2520Willems%2520et%2520al.%2520are%250Aan%2520active%2520topic%2520of%2520recent%2520research.%2520In%2520this%2520note%252C%2520we%2520explore%2520and%2520formalize%2520the%250Alinks%2520between%2520kernel%2520regression%2520and%2520some%2520known%2520nonlinear%2520extensions%2520of%2520the%250Afundamental%2520lemma.%2520Applying%2520a%2520transformation%2520to%2520the%2520usual%2520linear%2520equation%2520in%250AHankel%2520matrices%252C%2520we%2520arrive%2520at%2520an%2520alternative%2520implicit%2520kernel%2520representation%2520of%250Athe%2520system%2520trajectories%2520while%2520keeping%2520the%2520requirements%2520on%2520persistency%2520of%250Aexcitation.%2520We%2520show%2520that%2520this%2520representation%2520is%2520equivalent%2520to%2520the%2520solution%2520of%2520a%250Aspecific%2520kernel%2520regression%2520problem.%2520We%2520explore%2520the%2520possible%2520structures%2520of%2520the%250Aunderlying%2520kernel%2520as%2520well%2520as%2520the%2520system%2520classes%2520to%2520which%2520they%2520correspond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Links%20between%20the%20Fundamental%20Lemma%20and%20Kernel%20Regression&entry.906535625=Oleksii%20Molodchyk%20and%20Timm%20Faulwasser&entry.1292438233=%20%20Generalizations%20and%20variations%20of%20the%20fundamental%20lemma%20by%20Willems%20et%20al.%20are%0Aan%20active%20topic%20of%20recent%20research.%20In%20this%20note%2C%20we%20explore%20and%20formalize%20the%0Alinks%20between%20kernel%20regression%20and%20some%20known%20nonlinear%20extensions%20of%20the%0Afundamental%20lemma.%20Applying%20a%20transformation%20to%20the%20usual%20linear%20equation%20in%0AHankel%20matrices%2C%20we%20arrive%20at%20an%20alternative%20implicit%20kernel%20representation%20of%0Athe%20system%20trajectories%20while%20keeping%20the%20requirements%20on%20persistency%20of%0Aexcitation.%20We%20show%20that%20this%20representation%20is%20equivalent%20to%20the%20solution%20of%20a%0Aspecific%20kernel%20regression%20problem.%20We%20explore%20the%20possible%20structures%20of%20the%0Aunderlying%20kernel%20as%20well%20as%20the%20system%20classes%20to%20which%20they%20correspond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05368v2&entry.124074799=Read"},
{"title": "xTED: Cross-Domain Policy Adaptation via Diffusion-Based Trajectory\n  Editing", "author": "Haoyi Niu and Qimao Chen and Tenglong Liu and Jianxiong Li and Guyue Zhou and Yi Zhang and Jianming Hu and Xianyuan Zhan", "abstract": "  Reusing pre-collected data from different domains is an attractive solution\nin decision-making tasks where the accessible data is insufficient in the\ntarget domain but relatively abundant in other related domains. Existing\ncross-domain policy transfer methods mostly aim at learning domain\ncorrespondences or corrections to facilitate policy learning, which requires\nlearning domain/task-specific model components, representations, or policies\nthat are inflexible or not fully reusable to accommodate arbitrary domains and\ntasks. These issues make us wonder: can we directly bridge the domain gap at\nthe data (trajectory) level, instead of devising complicated, domain-specific\npolicy transfer models? In this study, we propose a Cross-Domain Trajectory\nEDiting (xTED) framework with a new diffusion transformer model (Decision\nDiffusion Transformer, DDiT) that captures the trajectory distribution from the\ntarget dataset as a prior. The proposed diffusion transformer backbone captures\nthe intricate dependencies among state, action, and reward sequences, as well\nas the transition dynamics within the target data trajectories. With the above\npre-trained diffusion prior, source data trajectories with domain gaps can be\ntransformed into edited trajectories that closely resemble the target data\ndistribution through the diffusion-based editing process, which implicitly\ncorrects the underlying domain gaps, enhancing the state realism and dynamics\nreliability in source trajectory data, while enabling flexible choices of\ndownstream policy learning methods. Despite its simplicity, xTED demonstrates\nsuperior performance against other baselines in extensive simulation and\nreal-robot experiments.\n", "link": "http://arxiv.org/abs/2409.08687v1", "date": "2024-09-13", "relevancy": 1.043, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.539}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5141}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xTED%3A%20Cross-Domain%20Policy%20Adaptation%20via%20Diffusion-Based%20Trajectory%0A%20%20Editing&body=Title%3A%20xTED%3A%20Cross-Domain%20Policy%20Adaptation%20via%20Diffusion-Based%20Trajectory%0A%20%20Editing%0AAuthor%3A%20Haoyi%20Niu%20and%20Qimao%20Chen%20and%20Tenglong%20Liu%20and%20Jianxiong%20Li%20and%20Guyue%20Zhou%20and%20Yi%20Zhang%20and%20Jianming%20Hu%20and%20Xianyuan%20Zhan%0AAbstract%3A%20%20%20Reusing%20pre-collected%20data%20from%20different%20domains%20is%20an%20attractive%20solution%0Ain%20decision-making%20tasks%20where%20the%20accessible%20data%20is%20insufficient%20in%20the%0Atarget%20domain%20but%20relatively%20abundant%20in%20other%20related%20domains.%20Existing%0Across-domain%20policy%20transfer%20methods%20mostly%20aim%20at%20learning%20domain%0Acorrespondences%20or%20corrections%20to%20facilitate%20policy%20learning%2C%20which%20requires%0Alearning%20domain/task-specific%20model%20components%2C%20representations%2C%20or%20policies%0Athat%20are%20inflexible%20or%20not%20fully%20reusable%20to%20accommodate%20arbitrary%20domains%20and%0Atasks.%20These%20issues%20make%20us%20wonder%3A%20can%20we%20directly%20bridge%20the%20domain%20gap%20at%0Athe%20data%20%28trajectory%29%20level%2C%20instead%20of%20devising%20complicated%2C%20domain-specific%0Apolicy%20transfer%20models%3F%20In%20this%20study%2C%20we%20propose%20a%20Cross-Domain%20Trajectory%0AEDiting%20%28xTED%29%20framework%20with%20a%20new%20diffusion%20transformer%20model%20%28Decision%0ADiffusion%20Transformer%2C%20DDiT%29%20that%20captures%20the%20trajectory%20distribution%20from%20the%0Atarget%20dataset%20as%20a%20prior.%20The%20proposed%20diffusion%20transformer%20backbone%20captures%0Athe%20intricate%20dependencies%20among%20state%2C%20action%2C%20and%20reward%20sequences%2C%20as%20well%0Aas%20the%20transition%20dynamics%20within%20the%20target%20data%20trajectories.%20With%20the%20above%0Apre-trained%20diffusion%20prior%2C%20source%20data%20trajectories%20with%20domain%20gaps%20can%20be%0Atransformed%20into%20edited%20trajectories%20that%20closely%20resemble%20the%20target%20data%0Adistribution%20through%20the%20diffusion-based%20editing%20process%2C%20which%20implicitly%0Acorrects%20the%20underlying%20domain%20gaps%2C%20enhancing%20the%20state%20realism%20and%20dynamics%0Areliability%20in%20source%20trajectory%20data%2C%20while%20enabling%20flexible%20choices%20of%0Adownstream%20policy%20learning%20methods.%20Despite%20its%20simplicity%2C%20xTED%20demonstrates%0Asuperior%20performance%20against%20other%20baselines%20in%20extensive%20simulation%20and%0Areal-robot%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxTED%253A%2520Cross-Domain%2520Policy%2520Adaptation%2520via%2520Diffusion-Based%2520Trajectory%250A%2520%2520Editing%26entry.906535625%3DHaoyi%2520Niu%2520and%2520Qimao%2520Chen%2520and%2520Tenglong%2520Liu%2520and%2520Jianxiong%2520Li%2520and%2520Guyue%2520Zhou%2520and%2520Yi%2520Zhang%2520and%2520Jianming%2520Hu%2520and%2520Xianyuan%2520Zhan%26entry.1292438233%3D%2520%2520Reusing%2520pre-collected%2520data%2520from%2520different%2520domains%2520is%2520an%2520attractive%2520solution%250Ain%2520decision-making%2520tasks%2520where%2520the%2520accessible%2520data%2520is%2520insufficient%2520in%2520the%250Atarget%2520domain%2520but%2520relatively%2520abundant%2520in%2520other%2520related%2520domains.%2520Existing%250Across-domain%2520policy%2520transfer%2520methods%2520mostly%2520aim%2520at%2520learning%2520domain%250Acorrespondences%2520or%2520corrections%2520to%2520facilitate%2520policy%2520learning%252C%2520which%2520requires%250Alearning%2520domain/task-specific%2520model%2520components%252C%2520representations%252C%2520or%2520policies%250Athat%2520are%2520inflexible%2520or%2520not%2520fully%2520reusable%2520to%2520accommodate%2520arbitrary%2520domains%2520and%250Atasks.%2520These%2520issues%2520make%2520us%2520wonder%253A%2520can%2520we%2520directly%2520bridge%2520the%2520domain%2520gap%2520at%250Athe%2520data%2520%2528trajectory%2529%2520level%252C%2520instead%2520of%2520devising%2520complicated%252C%2520domain-specific%250Apolicy%2520transfer%2520models%253F%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520Cross-Domain%2520Trajectory%250AEDiting%2520%2528xTED%2529%2520framework%2520with%2520a%2520new%2520diffusion%2520transformer%2520model%2520%2528Decision%250ADiffusion%2520Transformer%252C%2520DDiT%2529%2520that%2520captures%2520the%2520trajectory%2520distribution%2520from%2520the%250Atarget%2520dataset%2520as%2520a%2520prior.%2520The%2520proposed%2520diffusion%2520transformer%2520backbone%2520captures%250Athe%2520intricate%2520dependencies%2520among%2520state%252C%2520action%252C%2520and%2520reward%2520sequences%252C%2520as%2520well%250Aas%2520the%2520transition%2520dynamics%2520within%2520the%2520target%2520data%2520trajectories.%2520With%2520the%2520above%250Apre-trained%2520diffusion%2520prior%252C%2520source%2520data%2520trajectories%2520with%2520domain%2520gaps%2520can%2520be%250Atransformed%2520into%2520edited%2520trajectories%2520that%2520closely%2520resemble%2520the%2520target%2520data%250Adistribution%2520through%2520the%2520diffusion-based%2520editing%2520process%252C%2520which%2520implicitly%250Acorrects%2520the%2520underlying%2520domain%2520gaps%252C%2520enhancing%2520the%2520state%2520realism%2520and%2520dynamics%250Areliability%2520in%2520source%2520trajectory%2520data%252C%2520while%2520enabling%2520flexible%2520choices%2520of%250Adownstream%2520policy%2520learning%2520methods.%2520Despite%2520its%2520simplicity%252C%2520xTED%2520demonstrates%250Asuperior%2520performance%2520against%2520other%2520baselines%2520in%2520extensive%2520simulation%2520and%250Areal-robot%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xTED%3A%20Cross-Domain%20Policy%20Adaptation%20via%20Diffusion-Based%20Trajectory%0A%20%20Editing&entry.906535625=Haoyi%20Niu%20and%20Qimao%20Chen%20and%20Tenglong%20Liu%20and%20Jianxiong%20Li%20and%20Guyue%20Zhou%20and%20Yi%20Zhang%20and%20Jianming%20Hu%20and%20Xianyuan%20Zhan&entry.1292438233=%20%20Reusing%20pre-collected%20data%20from%20different%20domains%20is%20an%20attractive%20solution%0Ain%20decision-making%20tasks%20where%20the%20accessible%20data%20is%20insufficient%20in%20the%0Atarget%20domain%20but%20relatively%20abundant%20in%20other%20related%20domains.%20Existing%0Across-domain%20policy%20transfer%20methods%20mostly%20aim%20at%20learning%20domain%0Acorrespondences%20or%20corrections%20to%20facilitate%20policy%20learning%2C%20which%20requires%0Alearning%20domain/task-specific%20model%20components%2C%20representations%2C%20or%20policies%0Athat%20are%20inflexible%20or%20not%20fully%20reusable%20to%20accommodate%20arbitrary%20domains%20and%0Atasks.%20These%20issues%20make%20us%20wonder%3A%20can%20we%20directly%20bridge%20the%20domain%20gap%20at%0Athe%20data%20%28trajectory%29%20level%2C%20instead%20of%20devising%20complicated%2C%20domain-specific%0Apolicy%20transfer%20models%3F%20In%20this%20study%2C%20we%20propose%20a%20Cross-Domain%20Trajectory%0AEDiting%20%28xTED%29%20framework%20with%20a%20new%20diffusion%20transformer%20model%20%28Decision%0ADiffusion%20Transformer%2C%20DDiT%29%20that%20captures%20the%20trajectory%20distribution%20from%20the%0Atarget%20dataset%20as%20a%20prior.%20The%20proposed%20diffusion%20transformer%20backbone%20captures%0Athe%20intricate%20dependencies%20among%20state%2C%20action%2C%20and%20reward%20sequences%2C%20as%20well%0Aas%20the%20transition%20dynamics%20within%20the%20target%20data%20trajectories.%20With%20the%20above%0Apre-trained%20diffusion%20prior%2C%20source%20data%20trajectories%20with%20domain%20gaps%20can%20be%0Atransformed%20into%20edited%20trajectories%20that%20closely%20resemble%20the%20target%20data%0Adistribution%20through%20the%20diffusion-based%20editing%20process%2C%20which%20implicitly%0Acorrects%20the%20underlying%20domain%20gaps%2C%20enhancing%20the%20state%20realism%20and%20dynamics%0Areliability%20in%20source%20trajectory%20data%2C%20while%20enabling%20flexible%20choices%20of%0Adownstream%20policy%20learning%20methods.%20Despite%20its%20simplicity%2C%20xTED%20demonstrates%0Asuperior%20performance%20against%20other%20baselines%20in%20extensive%20simulation%20and%0Areal-robot%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08687v1&entry.124074799=Read"},
{"title": "Disentangling the sources of cyber risk premia", "author": "Lo\u00efc Mar\u00e9chal and Nathan Monnet", "abstract": "  We use a methodology based on a machine learning algorithm to quantify firms'\ncyber risks based on their disclosures and a dedicated cyber corpus. The model\ncan identify paragraphs related to determined cyber-threat types and\naccordingly attribute several related cyber scores to the firm. The cyber\nscores are unrelated to other firms' characteristics. Stocks with high cyber\nscores significantly outperform other stocks. The long-short cyber risk factors\nhave positive risk premia, are robust to all factors' benchmarks, and help\nprice returns. Furthermore, we suggest the market does not distinguish between\ndifferent types of cyber risks but instead views them as a single, aggregate\ncyber risk.\n", "link": "http://arxiv.org/abs/2409.08728v1", "date": "2024-09-13", "relevancy": 1.0957, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3765}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20the%20sources%20of%20cyber%20risk%20premia&body=Title%3A%20Disentangling%20the%20sources%20of%20cyber%20risk%20premia%0AAuthor%3A%20Lo%C3%AFc%20Mar%C3%A9chal%20and%20Nathan%20Monnet%0AAbstract%3A%20%20%20We%20use%20a%20methodology%20based%20on%20a%20machine%20learning%20algorithm%20to%20quantify%20firms%27%0Acyber%20risks%20based%20on%20their%20disclosures%20and%20a%20dedicated%20cyber%20corpus.%20The%20model%0Acan%20identify%20paragraphs%20related%20to%20determined%20cyber-threat%20types%20and%0Aaccordingly%20attribute%20several%20related%20cyber%20scores%20to%20the%20firm.%20The%20cyber%0Ascores%20are%20unrelated%20to%20other%20firms%27%20characteristics.%20Stocks%20with%20high%20cyber%0Ascores%20significantly%20outperform%20other%20stocks.%20The%20long-short%20cyber%20risk%20factors%0Ahave%20positive%20risk%20premia%2C%20are%20robust%20to%20all%20factors%27%20benchmarks%2C%20and%20help%0Aprice%20returns.%20Furthermore%2C%20we%20suggest%20the%20market%20does%20not%20distinguish%20between%0Adifferent%20types%20of%20cyber%20risks%20but%20instead%20views%20them%20as%20a%20single%2C%20aggregate%0Acyber%20risk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520the%2520sources%2520of%2520cyber%2520risk%2520premia%26entry.906535625%3DLo%25C3%25AFc%2520Mar%25C3%25A9chal%2520and%2520Nathan%2520Monnet%26entry.1292438233%3D%2520%2520We%2520use%2520a%2520methodology%2520based%2520on%2520a%2520machine%2520learning%2520algorithm%2520to%2520quantify%2520firms%2527%250Acyber%2520risks%2520based%2520on%2520their%2520disclosures%2520and%2520a%2520dedicated%2520cyber%2520corpus.%2520The%2520model%250Acan%2520identify%2520paragraphs%2520related%2520to%2520determined%2520cyber-threat%2520types%2520and%250Aaccordingly%2520attribute%2520several%2520related%2520cyber%2520scores%2520to%2520the%2520firm.%2520The%2520cyber%250Ascores%2520are%2520unrelated%2520to%2520other%2520firms%2527%2520characteristics.%2520Stocks%2520with%2520high%2520cyber%250Ascores%2520significantly%2520outperform%2520other%2520stocks.%2520The%2520long-short%2520cyber%2520risk%2520factors%250Ahave%2520positive%2520risk%2520premia%252C%2520are%2520robust%2520to%2520all%2520factors%2527%2520benchmarks%252C%2520and%2520help%250Aprice%2520returns.%2520Furthermore%252C%2520we%2520suggest%2520the%2520market%2520does%2520not%2520distinguish%2520between%250Adifferent%2520types%2520of%2520cyber%2520risks%2520but%2520instead%2520views%2520them%2520as%2520a%2520single%252C%2520aggregate%250Acyber%2520risk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20the%20sources%20of%20cyber%20risk%20premia&entry.906535625=Lo%C3%AFc%20Mar%C3%A9chal%20and%20Nathan%20Monnet&entry.1292438233=%20%20We%20use%20a%20methodology%20based%20on%20a%20machine%20learning%20algorithm%20to%20quantify%20firms%27%0Acyber%20risks%20based%20on%20their%20disclosures%20and%20a%20dedicated%20cyber%20corpus.%20The%20model%0Acan%20identify%20paragraphs%20related%20to%20determined%20cyber-threat%20types%20and%0Aaccordingly%20attribute%20several%20related%20cyber%20scores%20to%20the%20firm.%20The%20cyber%0Ascores%20are%20unrelated%20to%20other%20firms%27%20characteristics.%20Stocks%20with%20high%20cyber%0Ascores%20significantly%20outperform%20other%20stocks.%20The%20long-short%20cyber%20risk%20factors%0Ahave%20positive%20risk%20premia%2C%20are%20robust%20to%20all%20factors%27%20benchmarks%2C%20and%20help%0Aprice%20returns.%20Furthermore%2C%20we%20suggest%20the%20market%20does%20not%20distinguish%20between%0Adifferent%20types%20of%20cyber%20risks%20but%20instead%20views%20them%20as%20a%20single%2C%20aggregate%0Acyber%20risk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08728v1&entry.124074799=Read"},
{"title": "Algorithmic Decision-Making under Agents with Persistent Improvement", "author": "Tian Xie and Xuwei Tan and Xueru Zhang", "abstract": "  This paper studies algorithmic decision-making under human's strategic\nbehavior, where a decision maker uses an algorithm to make decisions about\nhuman agents, and the latter with information about the algorithm may exert\neffort strategically and improve to receive favorable decisions. Unlike prior\nworks that assume agents benefit from their efforts immediately, we consider\nrealistic scenarios where the impacts of these efforts are persistent and\nagents benefit from efforts by making improvements gradually. We first develop\na dynamic model to characterize persistent improvements and based on this\nconstruct a Stackelberg game to model the interplay between agents and the\ndecision-maker. We analytically characterize the equilibrium strategies and\nidentify conditions under which agents have incentives to improve. With the\ndynamics, we then study how the decision-maker can design an optimal policy to\nincentivize the largest improvements inside the agent population. We also\nextend the model to settings where 1) agents may be dishonest and game the\nalgorithm into making favorable but erroneous decisions; 2) honest efforts are\nforgettable and not sufficient to guarantee persistent improvements. With the\nextended models, we further examine conditions under which agents prefer honest\nefforts over dishonest behavior and the impacts of forgettable efforts.\n", "link": "http://arxiv.org/abs/2405.01807v3", "date": "2024-09-13", "relevancy": 1.4018, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4723}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4648}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Algorithmic%20Decision-Making%20under%20Agents%20with%20Persistent%20Improvement&body=Title%3A%20Algorithmic%20Decision-Making%20under%20Agents%20with%20Persistent%20Improvement%0AAuthor%3A%20Tian%20Xie%20and%20Xuwei%20Tan%20and%20Xueru%20Zhang%0AAbstract%3A%20%20%20This%20paper%20studies%20algorithmic%20decision-making%20under%20human%27s%20strategic%0Abehavior%2C%20where%20a%20decision%20maker%20uses%20an%20algorithm%20to%20make%20decisions%20about%0Ahuman%20agents%2C%20and%20the%20latter%20with%20information%20about%20the%20algorithm%20may%20exert%0Aeffort%20strategically%20and%20improve%20to%20receive%20favorable%20decisions.%20Unlike%20prior%0Aworks%20that%20assume%20agents%20benefit%20from%20their%20efforts%20immediately%2C%20we%20consider%0Arealistic%20scenarios%20where%20the%20impacts%20of%20these%20efforts%20are%20persistent%20and%0Aagents%20benefit%20from%20efforts%20by%20making%20improvements%20gradually.%20We%20first%20develop%0Aa%20dynamic%20model%20to%20characterize%20persistent%20improvements%20and%20based%20on%20this%0Aconstruct%20a%20Stackelberg%20game%20to%20model%20the%20interplay%20between%20agents%20and%20the%0Adecision-maker.%20We%20analytically%20characterize%20the%20equilibrium%20strategies%20and%0Aidentify%20conditions%20under%20which%20agents%20have%20incentives%20to%20improve.%20With%20the%0Adynamics%2C%20we%20then%20study%20how%20the%20decision-maker%20can%20design%20an%20optimal%20policy%20to%0Aincentivize%20the%20largest%20improvements%20inside%20the%20agent%20population.%20We%20also%0Aextend%20the%20model%20to%20settings%20where%201%29%20agents%20may%20be%20dishonest%20and%20game%20the%0Aalgorithm%20into%20making%20favorable%20but%20erroneous%20decisions%3B%202%29%20honest%20efforts%20are%0Aforgettable%20and%20not%20sufficient%20to%20guarantee%20persistent%20improvements.%20With%20the%0Aextended%20models%2C%20we%20further%20examine%20conditions%20under%20which%20agents%20prefer%20honest%0Aefforts%20over%20dishonest%20behavior%20and%20the%20impacts%20of%20forgettable%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01807v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgorithmic%2520Decision-Making%2520under%2520Agents%2520with%2520Persistent%2520Improvement%26entry.906535625%3DTian%2520Xie%2520and%2520Xuwei%2520Tan%2520and%2520Xueru%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520algorithmic%2520decision-making%2520under%2520human%2527s%2520strategic%250Abehavior%252C%2520where%2520a%2520decision%2520maker%2520uses%2520an%2520algorithm%2520to%2520make%2520decisions%2520about%250Ahuman%2520agents%252C%2520and%2520the%2520latter%2520with%2520information%2520about%2520the%2520algorithm%2520may%2520exert%250Aeffort%2520strategically%2520and%2520improve%2520to%2520receive%2520favorable%2520decisions.%2520Unlike%2520prior%250Aworks%2520that%2520assume%2520agents%2520benefit%2520from%2520their%2520efforts%2520immediately%252C%2520we%2520consider%250Arealistic%2520scenarios%2520where%2520the%2520impacts%2520of%2520these%2520efforts%2520are%2520persistent%2520and%250Aagents%2520benefit%2520from%2520efforts%2520by%2520making%2520improvements%2520gradually.%2520We%2520first%2520develop%250Aa%2520dynamic%2520model%2520to%2520characterize%2520persistent%2520improvements%2520and%2520based%2520on%2520this%250Aconstruct%2520a%2520Stackelberg%2520game%2520to%2520model%2520the%2520interplay%2520between%2520agents%2520and%2520the%250Adecision-maker.%2520We%2520analytically%2520characterize%2520the%2520equilibrium%2520strategies%2520and%250Aidentify%2520conditions%2520under%2520which%2520agents%2520have%2520incentives%2520to%2520improve.%2520With%2520the%250Adynamics%252C%2520we%2520then%2520study%2520how%2520the%2520decision-maker%2520can%2520design%2520an%2520optimal%2520policy%2520to%250Aincentivize%2520the%2520largest%2520improvements%2520inside%2520the%2520agent%2520population.%2520We%2520also%250Aextend%2520the%2520model%2520to%2520settings%2520where%25201%2529%2520agents%2520may%2520be%2520dishonest%2520and%2520game%2520the%250Aalgorithm%2520into%2520making%2520favorable%2520but%2520erroneous%2520decisions%253B%25202%2529%2520honest%2520efforts%2520are%250Aforgettable%2520and%2520not%2520sufficient%2520to%2520guarantee%2520persistent%2520improvements.%2520With%2520the%250Aextended%2520models%252C%2520we%2520further%2520examine%2520conditions%2520under%2520which%2520agents%2520prefer%2520honest%250Aefforts%2520over%2520dishonest%2520behavior%2520and%2520the%2520impacts%2520of%2520forgettable%2520efforts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01807v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithmic%20Decision-Making%20under%20Agents%20with%20Persistent%20Improvement&entry.906535625=Tian%20Xie%20and%20Xuwei%20Tan%20and%20Xueru%20Zhang&entry.1292438233=%20%20This%20paper%20studies%20algorithmic%20decision-making%20under%20human%27s%20strategic%0Abehavior%2C%20where%20a%20decision%20maker%20uses%20an%20algorithm%20to%20make%20decisions%20about%0Ahuman%20agents%2C%20and%20the%20latter%20with%20information%20about%20the%20algorithm%20may%20exert%0Aeffort%20strategically%20and%20improve%20to%20receive%20favorable%20decisions.%20Unlike%20prior%0Aworks%20that%20assume%20agents%20benefit%20from%20their%20efforts%20immediately%2C%20we%20consider%0Arealistic%20scenarios%20where%20the%20impacts%20of%20these%20efforts%20are%20persistent%20and%0Aagents%20benefit%20from%20efforts%20by%20making%20improvements%20gradually.%20We%20first%20develop%0Aa%20dynamic%20model%20to%20characterize%20persistent%20improvements%20and%20based%20on%20this%0Aconstruct%20a%20Stackelberg%20game%20to%20model%20the%20interplay%20between%20agents%20and%20the%0Adecision-maker.%20We%20analytically%20characterize%20the%20equilibrium%20strategies%20and%0Aidentify%20conditions%20under%20which%20agents%20have%20incentives%20to%20improve.%20With%20the%0Adynamics%2C%20we%20then%20study%20how%20the%20decision-maker%20can%20design%20an%20optimal%20policy%20to%0Aincentivize%20the%20largest%20improvements%20inside%20the%20agent%20population.%20We%20also%0Aextend%20the%20model%20to%20settings%20where%201%29%20agents%20may%20be%20dishonest%20and%20game%20the%0Aalgorithm%20into%20making%20favorable%20but%20erroneous%20decisions%3B%202%29%20honest%20efforts%20are%0Aforgettable%20and%20not%20sufficient%20to%20guarantee%20persistent%20improvements.%20With%20the%0Aextended%20models%2C%20we%20further%20examine%20conditions%20under%20which%20agents%20prefer%20honest%0Aefforts%20over%20dishonest%20behavior%20and%20the%20impacts%20of%20forgettable%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01807v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


