<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250714.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions", "author": "Shivangi Aneja and Sebastian Weiss and Irene Baeza and Prashanth Chandran and Gaspard Zoss and Matthias Nie\u00dfner and Derek Bradley", "abstract": "  Generating high-fidelity real-time animated sequences of photorealistic 3D\nhead avatars is important for many graphics applications, including immersive\ntelepresence and movies. This is a challenging problem particularly when\nrendering digital avatar close-ups for showing character's facial microfeatures\nand expressions. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nlocally-defined facial expressions with 3D Gaussian splatting to enable\ncreating ultra-high fidelity, expressive and photorealistic 3D head avatars. In\ncontrast to previous works that operate on a global expression space, we\ncondition our avatar's dynamics on patch-based local expression features and\nsynthesize 3D Gaussians at a patch level. In particular, we leverage a\npatch-based geometric 3D face model to extract patch expressions and learn how\nto translate these into local dynamic skin appearance and motion by coupling\nthe patches with anchor points of Scaffold-GS, a recent hierarchical scene\nrepresentation. These anchors are then used to synthesize 3D Gaussians\non-the-fly, conditioned by patch-expressions and viewing direction. We employ\ncolor-based densification and progressive training to obtain high-quality\nresults and faster convergence for high resolution 3K training images. By\nleveraging patch-level expressions, ScaffoldAvatar consistently achieves\nstate-of-the-art performance with visually natural motion, while encompassing\ndiverse facial expressions and styles in real time.\n", "link": "http://arxiv.org/abs/2507.10542v1", "date": "2025-07-14", "relevancy": 3.6559, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7692}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7692}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScaffoldAvatar%3A%20High-Fidelity%20Gaussian%20Avatars%20with%20Patch%20Expressions&body=Title%3A%20ScaffoldAvatar%3A%20High-Fidelity%20Gaussian%20Avatars%20with%20Patch%20Expressions%0AAuthor%3A%20Shivangi%20Aneja%20and%20Sebastian%20Weiss%20and%20Irene%20Baeza%20and%20Prashanth%20Chandran%20and%20Gaspard%20Zoss%20and%20Matthias%20Nie%C3%9Fner%20and%20Derek%20Bradley%0AAbstract%3A%20%20%20Generating%20high-fidelity%20real-time%20animated%20sequences%20of%20photorealistic%203D%0Ahead%20avatars%20is%20important%20for%20many%20graphics%20applications%2C%20including%20immersive%0Atelepresence%20and%20movies.%20This%20is%20a%20challenging%20problem%20particularly%20when%0Arendering%20digital%20avatar%20close-ups%20for%20showing%20character%27s%20facial%20microfeatures%0Aand%20expressions.%20To%20capture%20the%20expressive%2C%20detailed%20nature%20of%20human%20heads%2C%0Aincluding%20skin%20furrowing%20and%20finer-scale%20facial%20movements%2C%20we%20propose%20to%20couple%0Alocally-defined%20facial%20expressions%20with%203D%20Gaussian%20splatting%20to%20enable%0Acreating%20ultra-high%20fidelity%2C%20expressive%20and%20photorealistic%203D%20head%20avatars.%20In%0Acontrast%20to%20previous%20works%20that%20operate%20on%20a%20global%20expression%20space%2C%20we%0Acondition%20our%20avatar%27s%20dynamics%20on%20patch-based%20local%20expression%20features%20and%0Asynthesize%203D%20Gaussians%20at%20a%20patch%20level.%20In%20particular%2C%20we%20leverage%20a%0Apatch-based%20geometric%203D%20face%20model%20to%20extract%20patch%20expressions%20and%20learn%20how%0Ato%20translate%20these%20into%20local%20dynamic%20skin%20appearance%20and%20motion%20by%20coupling%0Athe%20patches%20with%20anchor%20points%20of%20Scaffold-GS%2C%20a%20recent%20hierarchical%20scene%0Arepresentation.%20These%20anchors%20are%20then%20used%20to%20synthesize%203D%20Gaussians%0Aon-the-fly%2C%20conditioned%20by%20patch-expressions%20and%20viewing%20direction.%20We%20employ%0Acolor-based%20densification%20and%20progressive%20training%20to%20obtain%20high-quality%0Aresults%20and%20faster%20convergence%20for%20high%20resolution%203K%20training%20images.%20By%0Aleveraging%20patch-level%20expressions%2C%20ScaffoldAvatar%20consistently%20achieves%0Astate-of-the-art%20performance%20with%20visually%20natural%20motion%2C%20while%20encompassing%0Adiverse%20facial%20expressions%20and%20styles%20in%20real%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaffoldAvatar%253A%2520High-Fidelity%2520Gaussian%2520Avatars%2520with%2520Patch%2520Expressions%26entry.906535625%3DShivangi%2520Aneja%2520and%2520Sebastian%2520Weiss%2520and%2520Irene%2520Baeza%2520and%2520Prashanth%2520Chandran%2520and%2520Gaspard%2520Zoss%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Derek%2520Bradley%26entry.1292438233%3D%2520%2520Generating%2520high-fidelity%2520real-time%2520animated%2520sequences%2520of%2520photorealistic%25203D%250Ahead%2520avatars%2520is%2520important%2520for%2520many%2520graphics%2520applications%252C%2520including%2520immersive%250Atelepresence%2520and%2520movies.%2520This%2520is%2520a%2520challenging%2520problem%2520particularly%2520when%250Arendering%2520digital%2520avatar%2520close-ups%2520for%2520showing%2520character%2527s%2520facial%2520microfeatures%250Aand%2520expressions.%2520To%2520capture%2520the%2520expressive%252C%2520detailed%2520nature%2520of%2520human%2520heads%252C%250Aincluding%2520skin%2520furrowing%2520and%2520finer-scale%2520facial%2520movements%252C%2520we%2520propose%2520to%2520couple%250Alocally-defined%2520facial%2520expressions%2520with%25203D%2520Gaussian%2520splatting%2520to%2520enable%250Acreating%2520ultra-high%2520fidelity%252C%2520expressive%2520and%2520photorealistic%25203D%2520head%2520avatars.%2520In%250Acontrast%2520to%2520previous%2520works%2520that%2520operate%2520on%2520a%2520global%2520expression%2520space%252C%2520we%250Acondition%2520our%2520avatar%2527s%2520dynamics%2520on%2520patch-based%2520local%2520expression%2520features%2520and%250Asynthesize%25203D%2520Gaussians%2520at%2520a%2520patch%2520level.%2520In%2520particular%252C%2520we%2520leverage%2520a%250Apatch-based%2520geometric%25203D%2520face%2520model%2520to%2520extract%2520patch%2520expressions%2520and%2520learn%2520how%250Ato%2520translate%2520these%2520into%2520local%2520dynamic%2520skin%2520appearance%2520and%2520motion%2520by%2520coupling%250Athe%2520patches%2520with%2520anchor%2520points%2520of%2520Scaffold-GS%252C%2520a%2520recent%2520hierarchical%2520scene%250Arepresentation.%2520These%2520anchors%2520are%2520then%2520used%2520to%2520synthesize%25203D%2520Gaussians%250Aon-the-fly%252C%2520conditioned%2520by%2520patch-expressions%2520and%2520viewing%2520direction.%2520We%2520employ%250Acolor-based%2520densification%2520and%2520progressive%2520training%2520to%2520obtain%2520high-quality%250Aresults%2520and%2520faster%2520convergence%2520for%2520high%2520resolution%25203K%2520training%2520images.%2520By%250Aleveraging%2520patch-level%2520expressions%252C%2520ScaffoldAvatar%2520consistently%2520achieves%250Astate-of-the-art%2520performance%2520with%2520visually%2520natural%2520motion%252C%2520while%2520encompassing%250Adiverse%2520facial%2520expressions%2520and%2520styles%2520in%2520real%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScaffoldAvatar%3A%20High-Fidelity%20Gaussian%20Avatars%20with%20Patch%20Expressions&entry.906535625=Shivangi%20Aneja%20and%20Sebastian%20Weiss%20and%20Irene%20Baeza%20and%20Prashanth%20Chandran%20and%20Gaspard%20Zoss%20and%20Matthias%20Nie%C3%9Fner%20and%20Derek%20Bradley&entry.1292438233=%20%20Generating%20high-fidelity%20real-time%20animated%20sequences%20of%20photorealistic%203D%0Ahead%20avatars%20is%20important%20for%20many%20graphics%20applications%2C%20including%20immersive%0Atelepresence%20and%20movies.%20This%20is%20a%20challenging%20problem%20particularly%20when%0Arendering%20digital%20avatar%20close-ups%20for%20showing%20character%27s%20facial%20microfeatures%0Aand%20expressions.%20To%20capture%20the%20expressive%2C%20detailed%20nature%20of%20human%20heads%2C%0Aincluding%20skin%20furrowing%20and%20finer-scale%20facial%20movements%2C%20we%20propose%20to%20couple%0Alocally-defined%20facial%20expressions%20with%203D%20Gaussian%20splatting%20to%20enable%0Acreating%20ultra-high%20fidelity%2C%20expressive%20and%20photorealistic%203D%20head%20avatars.%20In%0Acontrast%20to%20previous%20works%20that%20operate%20on%20a%20global%20expression%20space%2C%20we%0Acondition%20our%20avatar%27s%20dynamics%20on%20patch-based%20local%20expression%20features%20and%0Asynthesize%203D%20Gaussians%20at%20a%20patch%20level.%20In%20particular%2C%20we%20leverage%20a%0Apatch-based%20geometric%203D%20face%20model%20to%20extract%20patch%20expressions%20and%20learn%20how%0Ato%20translate%20these%20into%20local%20dynamic%20skin%20appearance%20and%20motion%20by%20coupling%0Athe%20patches%20with%20anchor%20points%20of%20Scaffold-GS%2C%20a%20recent%20hierarchical%20scene%0Arepresentation.%20These%20anchors%20are%20then%20used%20to%20synthesize%203D%20Gaussians%0Aon-the-fly%2C%20conditioned%20by%20patch-expressions%20and%20viewing%20direction.%20We%20employ%0Acolor-based%20densification%20and%20progressive%20training%20to%20obtain%20high-quality%0Aresults%20and%20faster%20convergence%20for%20high%20resolution%203K%20training%20images.%20By%0Aleveraging%20patch-level%20expressions%2C%20ScaffoldAvatar%20consistently%20achieves%0Astate-of-the-art%20performance%20with%20visually%20natural%20motion%2C%20while%20encompassing%0Adiverse%20facial%20expressions%20and%20styles%20in%20real%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10542v1&entry.124074799=Read"},
{"title": "GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation\n  with Gaussian Splatting", "author": "Wanshui Gan and Fang Liu and Hongbin Xu and Ningkai Mo and Naoto Yokoya", "abstract": "  We introduce GaussianOcc, a systematic method that investigates the two\nusages of Gaussian splatting for fully self-supervised and efficient 3D\noccupancy estimation in surround views. First, traditional methods for\nself-supervised 3D occupancy estimation still require ground truth 6D poses\nfrom sensors during training. To address this limitation, we propose Gaussian\nSplatting for Projection (GSP) module to provide accurate scale information for\nfully self-supervised training from adjacent view projection. Additionally,\nexisting methods rely on volume rendering for final 3D voxel representation\nlearning using 2D signals (depth maps, semantic maps), which is both\ntime-consuming and less effective. We propose Gaussian Splatting from Voxel\nspace (GSV) to leverage the fast rendering properties of Gaussian splatting. As\na result, the proposed GaussianOcc method enables fully self-supervised (no\nground truth pose) 3D occupancy estimation in competitive performance with low\ncomputational cost (2.7 times faster in training and 5 times faster in\nrendering). The relevant code is available in\nhttps://github.com/GANWANSHUI/GaussianOcc.git.\n", "link": "http://arxiv.org/abs/2408.11447v4", "date": "2025-07-14", "relevancy": 3.3942, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7048}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7041}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianOcc%3A%20Fully%20Self-supervised%20and%20Efficient%203D%20Occupancy%20Estimation%0A%20%20with%20Gaussian%20Splatting&body=Title%3A%20GaussianOcc%3A%20Fully%20Self-supervised%20and%20Efficient%203D%20Occupancy%20Estimation%0A%20%20with%20Gaussian%20Splatting%0AAuthor%3A%20Wanshui%20Gan%20and%20Fang%20Liu%20and%20Hongbin%20Xu%20and%20Ningkai%20Mo%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20We%20introduce%20GaussianOcc%2C%20a%20systematic%20method%20that%20investigates%20the%20two%0Ausages%20of%20Gaussian%20splatting%20for%20fully%20self-supervised%20and%20efficient%203D%0Aoccupancy%20estimation%20in%20surround%20views.%20First%2C%20traditional%20methods%20for%0Aself-supervised%203D%20occupancy%20estimation%20still%20require%20ground%20truth%206D%20poses%0Afrom%20sensors%20during%20training.%20To%20address%20this%20limitation%2C%20we%20propose%20Gaussian%0ASplatting%20for%20Projection%20%28GSP%29%20module%20to%20provide%20accurate%20scale%20information%20for%0Afully%20self-supervised%20training%20from%20adjacent%20view%20projection.%20Additionally%2C%0Aexisting%20methods%20rely%20on%20volume%20rendering%20for%20final%203D%20voxel%20representation%0Alearning%20using%202D%20signals%20%28depth%20maps%2C%20semantic%20maps%29%2C%20which%20is%20both%0Atime-consuming%20and%20less%20effective.%20We%20propose%20Gaussian%20Splatting%20from%20Voxel%0Aspace%20%28GSV%29%20to%20leverage%20the%20fast%20rendering%20properties%20of%20Gaussian%20splatting.%20As%0Aa%20result%2C%20the%20proposed%20GaussianOcc%20method%20enables%20fully%20self-supervised%20%28no%0Aground%20truth%20pose%29%203D%20occupancy%20estimation%20in%20competitive%20performance%20with%20low%0Acomputational%20cost%20%282.7%20times%20faster%20in%20training%20and%205%20times%20faster%20in%0Arendering%29.%20The%20relevant%20code%20is%20available%20in%0Ahttps%3A//github.com/GANWANSHUI/GaussianOcc.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11447v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianOcc%253A%2520Fully%2520Self-supervised%2520and%2520Efficient%25203D%2520Occupancy%2520Estimation%250A%2520%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DWanshui%2520Gan%2520and%2520Fang%2520Liu%2520and%2520Hongbin%2520Xu%2520and%2520Ningkai%2520Mo%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520We%2520introduce%2520GaussianOcc%252C%2520a%2520systematic%2520method%2520that%2520investigates%2520the%2520two%250Ausages%2520of%2520Gaussian%2520splatting%2520for%2520fully%2520self-supervised%2520and%2520efficient%25203D%250Aoccupancy%2520estimation%2520in%2520surround%2520views.%2520First%252C%2520traditional%2520methods%2520for%250Aself-supervised%25203D%2520occupancy%2520estimation%2520still%2520require%2520ground%2520truth%25206D%2520poses%250Afrom%2520sensors%2520during%2520training.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Gaussian%250ASplatting%2520for%2520Projection%2520%2528GSP%2529%2520module%2520to%2520provide%2520accurate%2520scale%2520information%2520for%250Afully%2520self-supervised%2520training%2520from%2520adjacent%2520view%2520projection.%2520Additionally%252C%250Aexisting%2520methods%2520rely%2520on%2520volume%2520rendering%2520for%2520final%25203D%2520voxel%2520representation%250Alearning%2520using%25202D%2520signals%2520%2528depth%2520maps%252C%2520semantic%2520maps%2529%252C%2520which%2520is%2520both%250Atime-consuming%2520and%2520less%2520effective.%2520We%2520propose%2520Gaussian%2520Splatting%2520from%2520Voxel%250Aspace%2520%2528GSV%2529%2520to%2520leverage%2520the%2520fast%2520rendering%2520properties%2520of%2520Gaussian%2520splatting.%2520As%250Aa%2520result%252C%2520the%2520proposed%2520GaussianOcc%2520method%2520enables%2520fully%2520self-supervised%2520%2528no%250Aground%2520truth%2520pose%2529%25203D%2520occupancy%2520estimation%2520in%2520competitive%2520performance%2520with%2520low%250Acomputational%2520cost%2520%25282.7%2520times%2520faster%2520in%2520training%2520and%25205%2520times%2520faster%2520in%250Arendering%2529.%2520The%2520relevant%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/GANWANSHUI/GaussianOcc.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11447v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianOcc%3A%20Fully%20Self-supervised%20and%20Efficient%203D%20Occupancy%20Estimation%0A%20%20with%20Gaussian%20Splatting&entry.906535625=Wanshui%20Gan%20and%20Fang%20Liu%20and%20Hongbin%20Xu%20and%20Ningkai%20Mo%20and%20Naoto%20Yokoya&entry.1292438233=%20%20We%20introduce%20GaussianOcc%2C%20a%20systematic%20method%20that%20investigates%20the%20two%0Ausages%20of%20Gaussian%20splatting%20for%20fully%20self-supervised%20and%20efficient%203D%0Aoccupancy%20estimation%20in%20surround%20views.%20First%2C%20traditional%20methods%20for%0Aself-supervised%203D%20occupancy%20estimation%20still%20require%20ground%20truth%206D%20poses%0Afrom%20sensors%20during%20training.%20To%20address%20this%20limitation%2C%20we%20propose%20Gaussian%0ASplatting%20for%20Projection%20%28GSP%29%20module%20to%20provide%20accurate%20scale%20information%20for%0Afully%20self-supervised%20training%20from%20adjacent%20view%20projection.%20Additionally%2C%0Aexisting%20methods%20rely%20on%20volume%20rendering%20for%20final%203D%20voxel%20representation%0Alearning%20using%202D%20signals%20%28depth%20maps%2C%20semantic%20maps%29%2C%20which%20is%20both%0Atime-consuming%20and%20less%20effective.%20We%20propose%20Gaussian%20Splatting%20from%20Voxel%0Aspace%20%28GSV%29%20to%20leverage%20the%20fast%20rendering%20properties%20of%20Gaussian%20splatting.%20As%0Aa%20result%2C%20the%20proposed%20GaussianOcc%20method%20enables%20fully%20self-supervised%20%28no%0Aground%20truth%20pose%29%203D%20occupancy%20estimation%20in%20competitive%20performance%20with%20low%0Acomputational%20cost%20%282.7%20times%20faster%20in%20training%20and%205%20times%20faster%20in%0Arendering%29.%20The%20relevant%20code%20is%20available%20in%0Ahttps%3A//github.com/GANWANSHUI/GaussianOcc.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11447v4&entry.124074799=Read"},
{"title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery", "author": "Shubhendu Jena and Amine Ouasfi and Mae Younes and Adnane Boukhayma", "abstract": "  We present a method for Sparse view reconstruction with surface element\nsplatting that runs within 3 minutes on a consumer grade GPU. While few methods\naddress sparse radiance field learning from noisy or unposed sparse cameras,\nshape recovery remains relatively underexplored in this setting. Several\nradiance and shape learning test-time optimization methods address the sparse\nposed setting by learning data priors or using combinations of external\nmonocular geometry priors. Differently, we propose an efficient and simple\npipeline harnessing a single recent 3D foundation model. We leverage its\nvarious task heads, notably point maps and camera initializations to\ninstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image\ncorrespondences to guide camera optimization midst 2DGS training. Key to our\ncontribution is a novel formulation of splatted color variance along rays,\nwhich can be computed efficiently. Reducing this moment in training leads to\nmore accurate shape reconstructions. We demonstrate state-of-the-art\nperformances in the sparse uncalibrated setting in reconstruction and novel\nview benchmarks based on established multi-view datasets.\n", "link": "http://arxiv.org/abs/2505.02178v2", "date": "2025-07-14", "relevancy": 3.3845, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7362}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6745}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.62}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparfels%3A%20Fast%20Reconstruction%20from%20Sparse%20Unposed%20Imagery&body=Title%3A%20Sparfels%3A%20Fast%20Reconstruction%20from%20Sparse%20Unposed%20Imagery%0AAuthor%3A%20Shubhendu%20Jena%20and%20Amine%20Ouasfi%20and%20Mae%20Younes%20and%20Adnane%20Boukhayma%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20Sparse%20view%20reconstruction%20with%20surface%20element%0Asplatting%20that%20runs%20within%203%20minutes%20on%20a%20consumer%20grade%20GPU.%20While%20few%20methods%0Aaddress%20sparse%20radiance%20field%20learning%20from%20noisy%20or%20unposed%20sparse%20cameras%2C%0Ashape%20recovery%20remains%20relatively%20underexplored%20in%20this%20setting.%20Several%0Aradiance%20and%20shape%20learning%20test-time%20optimization%20methods%20address%20the%20sparse%0Aposed%20setting%20by%20learning%20data%20priors%20or%20using%20combinations%20of%20external%0Amonocular%20geometry%20priors.%20Differently%2C%20we%20propose%20an%20efficient%20and%20simple%0Apipeline%20harnessing%20a%20single%20recent%203D%20foundation%20model.%20We%20leverage%20its%0Avarious%20task%20heads%2C%20notably%20point%20maps%20and%20camera%20initializations%20to%0Ainstantiate%20a%20bundle%20adjusting%202D%20Gaussian%20Splatting%20%282DGS%29%20model%2C%20and%20image%0Acorrespondences%20to%20guide%20camera%20optimization%20midst%202DGS%20training.%20Key%20to%20our%0Acontribution%20is%20a%20novel%20formulation%20of%20splatted%20color%20variance%20along%20rays%2C%0Awhich%20can%20be%20computed%20efficiently.%20Reducing%20this%20moment%20in%20training%20leads%20to%0Amore%20accurate%20shape%20reconstructions.%20We%20demonstrate%20state-of-the-art%0Aperformances%20in%20the%20sparse%20uncalibrated%20setting%20in%20reconstruction%20and%20novel%0Aview%20benchmarks%20based%20on%20established%20multi-view%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparfels%253A%2520Fast%2520Reconstruction%2520from%2520Sparse%2520Unposed%2520Imagery%26entry.906535625%3DShubhendu%2520Jena%2520and%2520Amine%2520Ouasfi%2520and%2520Mae%2520Younes%2520and%2520Adnane%2520Boukhayma%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520Sparse%2520view%2520reconstruction%2520with%2520surface%2520element%250Asplatting%2520that%2520runs%2520within%25203%2520minutes%2520on%2520a%2520consumer%2520grade%2520GPU.%2520While%2520few%2520methods%250Aaddress%2520sparse%2520radiance%2520field%2520learning%2520from%2520noisy%2520or%2520unposed%2520sparse%2520cameras%252C%250Ashape%2520recovery%2520remains%2520relatively%2520underexplored%2520in%2520this%2520setting.%2520Several%250Aradiance%2520and%2520shape%2520learning%2520test-time%2520optimization%2520methods%2520address%2520the%2520sparse%250Aposed%2520setting%2520by%2520learning%2520data%2520priors%2520or%2520using%2520combinations%2520of%2520external%250Amonocular%2520geometry%2520priors.%2520Differently%252C%2520we%2520propose%2520an%2520efficient%2520and%2520simple%250Apipeline%2520harnessing%2520a%2520single%2520recent%25203D%2520foundation%2520model.%2520We%2520leverage%2520its%250Avarious%2520task%2520heads%252C%2520notably%2520point%2520maps%2520and%2520camera%2520initializations%2520to%250Ainstantiate%2520a%2520bundle%2520adjusting%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520model%252C%2520and%2520image%250Acorrespondences%2520to%2520guide%2520camera%2520optimization%2520midst%25202DGS%2520training.%2520Key%2520to%2520our%250Acontribution%2520is%2520a%2520novel%2520formulation%2520of%2520splatted%2520color%2520variance%2520along%2520rays%252C%250Awhich%2520can%2520be%2520computed%2520efficiently.%2520Reducing%2520this%2520moment%2520in%2520training%2520leads%2520to%250Amore%2520accurate%2520shape%2520reconstructions.%2520We%2520demonstrate%2520state-of-the-art%250Aperformances%2520in%2520the%2520sparse%2520uncalibrated%2520setting%2520in%2520reconstruction%2520and%2520novel%250Aview%2520benchmarks%2520based%2520on%2520established%2520multi-view%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparfels%3A%20Fast%20Reconstruction%20from%20Sparse%20Unposed%20Imagery&entry.906535625=Shubhendu%20Jena%20and%20Amine%20Ouasfi%20and%20Mae%20Younes%20and%20Adnane%20Boukhayma&entry.1292438233=%20%20We%20present%20a%20method%20for%20Sparse%20view%20reconstruction%20with%20surface%20element%0Asplatting%20that%20runs%20within%203%20minutes%20on%20a%20consumer%20grade%20GPU.%20While%20few%20methods%0Aaddress%20sparse%20radiance%20field%20learning%20from%20noisy%20or%20unposed%20sparse%20cameras%2C%0Ashape%20recovery%20remains%20relatively%20underexplored%20in%20this%20setting.%20Several%0Aradiance%20and%20shape%20learning%20test-time%20optimization%20methods%20address%20the%20sparse%0Aposed%20setting%20by%20learning%20data%20priors%20or%20using%20combinations%20of%20external%0Amonocular%20geometry%20priors.%20Differently%2C%20we%20propose%20an%20efficient%20and%20simple%0Apipeline%20harnessing%20a%20single%20recent%203D%20foundation%20model.%20We%20leverage%20its%0Avarious%20task%20heads%2C%20notably%20point%20maps%20and%20camera%20initializations%20to%0Ainstantiate%20a%20bundle%20adjusting%202D%20Gaussian%20Splatting%20%282DGS%29%20model%2C%20and%20image%0Acorrespondences%20to%20guide%20camera%20optimization%20midst%202DGS%20training.%20Key%20to%20our%0Acontribution%20is%20a%20novel%20formulation%20of%20splatted%20color%20variance%20along%20rays%2C%0Awhich%20can%20be%20computed%20efficiently.%20Reducing%20this%20moment%20in%20training%20leads%20to%0Amore%20accurate%20shape%20reconstructions.%20We%20demonstrate%20state-of-the-art%0Aperformances%20in%20the%20sparse%20uncalibrated%20setting%20in%20reconstruction%20and%20novel%0Aview%20benchmarks%20based%20on%20established%20multi-view%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02178v2&entry.124074799=Read"},
{"title": "SLGaussian: Fast Language Gaussian Splatting in Sparse Views", "author": "Kangjie Chen and BingQuan Dai and Minghan Qin and Dongbin Zhang and Peihao Li and Yingshuang Zou and Haoqian Wang", "abstract": "  3D semantic field learning is crucial for applications like autonomous\nnavigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from\nlimited viewpoints is essential. Existing methods struggle under sparse view\nconditions, relying on inefficient per-scene multi-view optimizations, which\nare impractical for many real-world tasks. To address this, we propose\nSLGaussian, a feed-forward method for constructing 3D semantic fields from\nsparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring\nconsistent SAM segmentations through video tracking and using low-dimensional\nindexing for high-dimensional CLIP features, SLGaussian efficiently embeds\nlanguage information in 3D space, offering a robust solution for accurate 3D\nscene understanding under sparse view conditions. In experiments on two-view\nsparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,\nSLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,\nand mIoU. Moreover, our model achieves scene inference in under 30 seconds and\nopen-vocabulary querying in just 0.011 seconds per query.\n", "link": "http://arxiv.org/abs/2412.08331v2", "date": "2025-07-14", "relevancy": 3.3714, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7255}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.669}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLGaussian%3A%20Fast%20Language%20Gaussian%20Splatting%20in%20Sparse%20Views&body=Title%3A%20SLGaussian%3A%20Fast%20Language%20Gaussian%20Splatting%20in%20Sparse%20Views%0AAuthor%3A%20Kangjie%20Chen%20and%20BingQuan%20Dai%20and%20Minghan%20Qin%20and%20Dongbin%20Zhang%20and%20Peihao%20Li%20and%20Yingshuang%20Zou%20and%20Haoqian%20Wang%0AAbstract%3A%20%20%203D%20semantic%20field%20learning%20is%20crucial%20for%20applications%20like%20autonomous%0Anavigation%2C%20AR/VR%2C%20and%20robotics%2C%20where%20accurate%20comprehension%20of%203D%20scenes%20from%0Alimited%20viewpoints%20is%20essential.%20Existing%20methods%20struggle%20under%20sparse%20view%0Aconditions%2C%20relying%20on%20inefficient%20per-scene%20multi-view%20optimizations%2C%20which%0Aare%20impractical%20for%20many%20real-world%20tasks.%20To%20address%20this%2C%20we%20propose%0ASLGaussian%2C%20a%20feed-forward%20method%20for%20constructing%203D%20semantic%20fields%20from%0Asparse%20viewpoints%2C%20allowing%20direct%20inference%20of%203DGS-based%20scenes.%20By%20ensuring%0Aconsistent%20SAM%20segmentations%20through%20video%20tracking%20and%20using%20low-dimensional%0Aindexing%20for%20high-dimensional%20CLIP%20features%2C%20SLGaussian%20efficiently%20embeds%0Alanguage%20information%20in%203D%20space%2C%20offering%20a%20robust%20solution%20for%20accurate%203D%0Ascene%20understanding%20under%20sparse%20view%20conditions.%20In%20experiments%20on%20two-view%0Asparse%203D%20object%20querying%20and%20segmentation%20in%20the%20LERF%20and%203D-OVS%20datasets%2C%0ASLGaussian%20outperforms%20existing%20methods%20in%20chosen%20IoU%2C%20Localization%20Accuracy%2C%0Aand%20mIoU.%20Moreover%2C%20our%20model%20achieves%20scene%20inference%20in%20under%2030%20seconds%20and%0Aopen-vocabulary%20querying%20in%20just%200.011%20seconds%20per%20query.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08331v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLGaussian%253A%2520Fast%2520Language%2520Gaussian%2520Splatting%2520in%2520Sparse%2520Views%26entry.906535625%3DKangjie%2520Chen%2520and%2520BingQuan%2520Dai%2520and%2520Minghan%2520Qin%2520and%2520Dongbin%2520Zhang%2520and%2520Peihao%2520Li%2520and%2520Yingshuang%2520Zou%2520and%2520Haoqian%2520Wang%26entry.1292438233%3D%2520%25203D%2520semantic%2520field%2520learning%2520is%2520crucial%2520for%2520applications%2520like%2520autonomous%250Anavigation%252C%2520AR/VR%252C%2520and%2520robotics%252C%2520where%2520accurate%2520comprehension%2520of%25203D%2520scenes%2520from%250Alimited%2520viewpoints%2520is%2520essential.%2520Existing%2520methods%2520struggle%2520under%2520sparse%2520view%250Aconditions%252C%2520relying%2520on%2520inefficient%2520per-scene%2520multi-view%2520optimizations%252C%2520which%250Aare%2520impractical%2520for%2520many%2520real-world%2520tasks.%2520To%2520address%2520this%252C%2520we%2520propose%250ASLGaussian%252C%2520a%2520feed-forward%2520method%2520for%2520constructing%25203D%2520semantic%2520fields%2520from%250Asparse%2520viewpoints%252C%2520allowing%2520direct%2520inference%2520of%25203DGS-based%2520scenes.%2520By%2520ensuring%250Aconsistent%2520SAM%2520segmentations%2520through%2520video%2520tracking%2520and%2520using%2520low-dimensional%250Aindexing%2520for%2520high-dimensional%2520CLIP%2520features%252C%2520SLGaussian%2520efficiently%2520embeds%250Alanguage%2520information%2520in%25203D%2520space%252C%2520offering%2520a%2520robust%2520solution%2520for%2520accurate%25203D%250Ascene%2520understanding%2520under%2520sparse%2520view%2520conditions.%2520In%2520experiments%2520on%2520two-view%250Asparse%25203D%2520object%2520querying%2520and%2520segmentation%2520in%2520the%2520LERF%2520and%25203D-OVS%2520datasets%252C%250ASLGaussian%2520outperforms%2520existing%2520methods%2520in%2520chosen%2520IoU%252C%2520Localization%2520Accuracy%252C%250Aand%2520mIoU.%2520Moreover%252C%2520our%2520model%2520achieves%2520scene%2520inference%2520in%2520under%252030%2520seconds%2520and%250Aopen-vocabulary%2520querying%2520in%2520just%25200.011%2520seconds%2520per%2520query.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08331v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLGaussian%3A%20Fast%20Language%20Gaussian%20Splatting%20in%20Sparse%20Views&entry.906535625=Kangjie%20Chen%20and%20BingQuan%20Dai%20and%20Minghan%20Qin%20and%20Dongbin%20Zhang%20and%20Peihao%20Li%20and%20Yingshuang%20Zou%20and%20Haoqian%20Wang&entry.1292438233=%20%203D%20semantic%20field%20learning%20is%20crucial%20for%20applications%20like%20autonomous%0Anavigation%2C%20AR/VR%2C%20and%20robotics%2C%20where%20accurate%20comprehension%20of%203D%20scenes%20from%0Alimited%20viewpoints%20is%20essential.%20Existing%20methods%20struggle%20under%20sparse%20view%0Aconditions%2C%20relying%20on%20inefficient%20per-scene%20multi-view%20optimizations%2C%20which%0Aare%20impractical%20for%20many%20real-world%20tasks.%20To%20address%20this%2C%20we%20propose%0ASLGaussian%2C%20a%20feed-forward%20method%20for%20constructing%203D%20semantic%20fields%20from%0Asparse%20viewpoints%2C%20allowing%20direct%20inference%20of%203DGS-based%20scenes.%20By%20ensuring%0Aconsistent%20SAM%20segmentations%20through%20video%20tracking%20and%20using%20low-dimensional%0Aindexing%20for%20high-dimensional%20CLIP%20features%2C%20SLGaussian%20efficiently%20embeds%0Alanguage%20information%20in%203D%20space%2C%20offering%20a%20robust%20solution%20for%20accurate%203D%0Ascene%20understanding%20under%20sparse%20view%20conditions.%20In%20experiments%20on%20two-view%0Asparse%203D%20object%20querying%20and%20segmentation%20in%20the%20LERF%20and%203D-OVS%20datasets%2C%0ASLGaussian%20outperforms%20existing%20methods%20in%20chosen%20IoU%2C%20Localization%20Accuracy%2C%0Aand%20mIoU.%20Moreover%2C%20our%20model%20achieves%20scene%20inference%20in%20under%2030%20seconds%20and%0Aopen-vocabulary%20querying%20in%20just%200.011%20seconds%20per%20query.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08331v2&entry.124074799=Read"},
{"title": "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from\n  Motion-Blurred Images", "author": "Jungho Lee and Donghyeong Kim and Dogyoon Lee and Suhwan Cho and Minhyeok Lee and Wonjoon Lee and Taeoh Kim and Dongyoon Wee and Sangyoun Lee", "abstract": "  3D Gaussian Splatting (3DGS) has gained significant attention due to its\nhigh-quality novel view rendering, motivating research to address real-world\nchallenges. A critical issue is the camera motion blur caused by movement\nduring exposure, which hinders accurate 3D scene reconstruction. In this study,\nwe propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that\nreconstructs precise 3D scenes from motion-blurred images while maintaining\nreal-time rendering speed. Considering the complex motion patterns inherent in\nreal-world camera movements, we predict continuous camera trajectories using\nneural ordinary differential equations (ODEs). To ensure accurate modeling, we\nemploy rigid body transformations, preserving the shape and size of the object\nbut rely on the discrete integration of sampled frames. To better approximate\nthe continuous nature of motion blur, we introduce a continuous motion\nrefinement (CMR) transformation that refines rigid transformations by\nincorporating additional learnable parameters. By revisiting fundamental camera\ntheory and leveraging advanced neural ODE techniques, we achieve precise\nmodeling of continuous camera trajectories, leading to improved reconstruction\naccuracy. Extensive experiments demonstrate state-of-the-art performance both\nquantitatively and qualitatively on benchmark datasets, which include a wide\nrange of motion blur scenarios, from moderate to extreme blur.\n", "link": "http://arxiv.org/abs/2503.05332v2", "date": "2025-07-14", "relevancy": 3.3342, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6845}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6587}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoMoGaussian%3A%20Continuous%20Motion-Aware%20Gaussian%20Splatting%20from%0A%20%20Motion-Blurred%20Images&body=Title%3A%20CoMoGaussian%3A%20Continuous%20Motion-Aware%20Gaussian%20Splatting%20from%0A%20%20Motion-Blurred%20Images%0AAuthor%3A%20Jungho%20Lee%20and%20Donghyeong%20Kim%20and%20Dogyoon%20Lee%20and%20Suhwan%20Cho%20and%20Minhyeok%20Lee%20and%20Wonjoon%20Lee%20and%20Taeoh%20Kim%20and%20Dongyoon%20Wee%20and%20Sangyoun%20Lee%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20gained%20significant%20attention%20due%20to%20its%0Ahigh-quality%20novel%20view%20rendering%2C%20motivating%20research%20to%20address%20real-world%0Achallenges.%20A%20critical%20issue%20is%20the%20camera%20motion%20blur%20caused%20by%20movement%0Aduring%20exposure%2C%20which%20hinders%20accurate%203D%20scene%20reconstruction.%20In%20this%20study%2C%0Awe%20propose%20CoMoGaussian%2C%20a%20Continuous%20Motion-Aware%20Gaussian%20Splatting%20that%0Areconstructs%20precise%203D%20scenes%20from%20motion-blurred%20images%20while%20maintaining%0Areal-time%20rendering%20speed.%20Considering%20the%20complex%20motion%20patterns%20inherent%20in%0Areal-world%20camera%20movements%2C%20we%20predict%20continuous%20camera%20trajectories%20using%0Aneural%20ordinary%20differential%20equations%20%28ODEs%29.%20To%20ensure%20accurate%20modeling%2C%20we%0Aemploy%20rigid%20body%20transformations%2C%20preserving%20the%20shape%20and%20size%20of%20the%20object%0Abut%20rely%20on%20the%20discrete%20integration%20of%20sampled%20frames.%20To%20better%20approximate%0Athe%20continuous%20nature%20of%20motion%20blur%2C%20we%20introduce%20a%20continuous%20motion%0Arefinement%20%28CMR%29%20transformation%20that%20refines%20rigid%20transformations%20by%0Aincorporating%20additional%20learnable%20parameters.%20By%20revisiting%20fundamental%20camera%0Atheory%20and%20leveraging%20advanced%20neural%20ODE%20techniques%2C%20we%20achieve%20precise%0Amodeling%20of%20continuous%20camera%20trajectories%2C%20leading%20to%20improved%20reconstruction%0Aaccuracy.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20performance%20both%0Aquantitatively%20and%20qualitatively%20on%20benchmark%20datasets%2C%20which%20include%20a%20wide%0Arange%20of%20motion%20blur%20scenarios%2C%20from%20moderate%20to%20extreme%20blur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05332v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoMoGaussian%253A%2520Continuous%2520Motion-Aware%2520Gaussian%2520Splatting%2520from%250A%2520%2520Motion-Blurred%2520Images%26entry.906535625%3DJungho%2520Lee%2520and%2520Donghyeong%2520Kim%2520and%2520Dogyoon%2520Lee%2520and%2520Suhwan%2520Cho%2520and%2520Minhyeok%2520Lee%2520and%2520Wonjoon%2520Lee%2520and%2520Taeoh%2520Kim%2520and%2520Dongyoon%2520Wee%2520and%2520Sangyoun%2520Lee%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520gained%2520significant%2520attention%2520due%2520to%2520its%250Ahigh-quality%2520novel%2520view%2520rendering%252C%2520motivating%2520research%2520to%2520address%2520real-world%250Achallenges.%2520A%2520critical%2520issue%2520is%2520the%2520camera%2520motion%2520blur%2520caused%2520by%2520movement%250Aduring%2520exposure%252C%2520which%2520hinders%2520accurate%25203D%2520scene%2520reconstruction.%2520In%2520this%2520study%252C%250Awe%2520propose%2520CoMoGaussian%252C%2520a%2520Continuous%2520Motion-Aware%2520Gaussian%2520Splatting%2520that%250Areconstructs%2520precise%25203D%2520scenes%2520from%2520motion-blurred%2520images%2520while%2520maintaining%250Areal-time%2520rendering%2520speed.%2520Considering%2520the%2520complex%2520motion%2520patterns%2520inherent%2520in%250Areal-world%2520camera%2520movements%252C%2520we%2520predict%2520continuous%2520camera%2520trajectories%2520using%250Aneural%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529.%2520To%2520ensure%2520accurate%2520modeling%252C%2520we%250Aemploy%2520rigid%2520body%2520transformations%252C%2520preserving%2520the%2520shape%2520and%2520size%2520of%2520the%2520object%250Abut%2520rely%2520on%2520the%2520discrete%2520integration%2520of%2520sampled%2520frames.%2520To%2520better%2520approximate%250Athe%2520continuous%2520nature%2520of%2520motion%2520blur%252C%2520we%2520introduce%2520a%2520continuous%2520motion%250Arefinement%2520%2528CMR%2529%2520transformation%2520that%2520refines%2520rigid%2520transformations%2520by%250Aincorporating%2520additional%2520learnable%2520parameters.%2520By%2520revisiting%2520fundamental%2520camera%250Atheory%2520and%2520leveraging%2520advanced%2520neural%2520ODE%2520techniques%252C%2520we%2520achieve%2520precise%250Amodeling%2520of%2520continuous%2520camera%2520trajectories%252C%2520leading%2520to%2520improved%2520reconstruction%250Aaccuracy.%2520Extensive%2520experiments%2520demonstrate%2520state-of-the-art%2520performance%2520both%250Aquantitatively%2520and%2520qualitatively%2520on%2520benchmark%2520datasets%252C%2520which%2520include%2520a%2520wide%250Arange%2520of%2520motion%2520blur%2520scenarios%252C%2520from%2520moderate%2520to%2520extreme%2520blur.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05332v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoMoGaussian%3A%20Continuous%20Motion-Aware%20Gaussian%20Splatting%20from%0A%20%20Motion-Blurred%20Images&entry.906535625=Jungho%20Lee%20and%20Donghyeong%20Kim%20and%20Dogyoon%20Lee%20and%20Suhwan%20Cho%20and%20Minhyeok%20Lee%20and%20Wonjoon%20Lee%20and%20Taeoh%20Kim%20and%20Dongyoon%20Wee%20and%20Sangyoun%20Lee&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20gained%20significant%20attention%20due%20to%20its%0Ahigh-quality%20novel%20view%20rendering%2C%20motivating%20research%20to%20address%20real-world%0Achallenges.%20A%20critical%20issue%20is%20the%20camera%20motion%20blur%20caused%20by%20movement%0Aduring%20exposure%2C%20which%20hinders%20accurate%203D%20scene%20reconstruction.%20In%20this%20study%2C%0Awe%20propose%20CoMoGaussian%2C%20a%20Continuous%20Motion-Aware%20Gaussian%20Splatting%20that%0Areconstructs%20precise%203D%20scenes%20from%20motion-blurred%20images%20while%20maintaining%0Areal-time%20rendering%20speed.%20Considering%20the%20complex%20motion%20patterns%20inherent%20in%0Areal-world%20camera%20movements%2C%20we%20predict%20continuous%20camera%20trajectories%20using%0Aneural%20ordinary%20differential%20equations%20%28ODEs%29.%20To%20ensure%20accurate%20modeling%2C%20we%0Aemploy%20rigid%20body%20transformations%2C%20preserving%20the%20shape%20and%20size%20of%20the%20object%0Abut%20rely%20on%20the%20discrete%20integration%20of%20sampled%20frames.%20To%20better%20approximate%0Athe%20continuous%20nature%20of%20motion%20blur%2C%20we%20introduce%20a%20continuous%20motion%0Arefinement%20%28CMR%29%20transformation%20that%20refines%20rigid%20transformations%20by%0Aincorporating%20additional%20learnable%20parameters.%20By%20revisiting%20fundamental%20camera%0Atheory%20and%20leveraging%20advanced%20neural%20ODE%20techniques%2C%20we%20achieve%20precise%0Amodeling%20of%20continuous%20camera%20trajectories%2C%20leading%20to%20improved%20reconstruction%0Aaccuracy.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20performance%20both%0Aquantitatively%20and%20qualitatively%20on%20benchmark%20datasets%2C%20which%20include%20a%20wide%0Arange%20of%20motion%20blur%20scenarios%2C%20from%20moderate%20to%20extreme%20blur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05332v2&entry.124074799=Read"},
{"title": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and\n  Alternating Optimization for Talking-head Generation", "author": "Kui Jiang and Shiyu Liu and Junjun Jiang and Xin Yang and Hongxun Yao and Xiaopeng Fan", "abstract": "  Audio-driven talking head generation holds significant potential for film\nproduction. While existing 3D methods have advanced motion modeling and content\nsynthesis, they often produce rendering artifacts, such as motion blur,\ntemporal jitter, and local penetration, due to limitations in representing\nstable, fine-grained motion fields. Through systematic analysis, we reformulate\ntalking head generation into a unified framework comprising three steps: video\npreprocessing, motion representation, and rendering reconstruction. This\nframework underpins our proposed M2DAO-Talker, which addresses current\nlimitations via multi-granular motion decoupling and alternating optimization.\nSpecifically, we devise a novel 2D portrait preprocessing pipeline to extract\nframe-wise deformation control conditions (motion region segmentation masks,\nand camera parameters) to facilitate motion representation. To ameliorate\nmotion modeling, we elaborate a multi-granular motion decoupling strategy,\nwhich independently models non-rigid (oral and facial) and rigid (head) motions\nfor improved reconstruction accuracy. Meanwhile, a motion consistency\nconstraint is developed to ensure head-torso kinematic consistency, thereby\nmitigating penetration artifacts caused by motion aliasing. In addition, an\nalternating optimization strategy is designed to iteratively refine facial and\noral motion parameters, enabling more realistic video generation. Experiments\nacross multiple datasets show that M2DAO-Talker achieves state-of-the-art\nperformance, with the 2.43 dB PSNR improvement in generation quality and 0.64\ngain in user-evaluated video realness versus TalkingGaussian while with 150 FPS\ninference speed. Our project homepage is\nhttps://m2dao-talker.github.io/M2DAO-Talk.github.io.\n", "link": "http://arxiv.org/abs/2507.08307v2", "date": "2025-07-14", "relevancy": 3.022, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6314}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5909}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M2DAO-Talker%3A%20Harmonizing%20Multi-granular%20Motion%20Decoupling%20and%0A%20%20Alternating%20Optimization%20for%20Talking-head%20Generation&body=Title%3A%20M2DAO-Talker%3A%20Harmonizing%20Multi-granular%20Motion%20Decoupling%20and%0A%20%20Alternating%20Optimization%20for%20Talking-head%20Generation%0AAuthor%3A%20Kui%20Jiang%20and%20Shiyu%20Liu%20and%20Junjun%20Jiang%20and%20Xin%20Yang%20and%20Hongxun%20Yao%20and%20Xiaopeng%20Fan%0AAbstract%3A%20%20%20Audio-driven%20talking%20head%20generation%20holds%20significant%20potential%20for%20film%0Aproduction.%20While%20existing%203D%20methods%20have%20advanced%20motion%20modeling%20and%20content%0Asynthesis%2C%20they%20often%20produce%20rendering%20artifacts%2C%20such%20as%20motion%20blur%2C%0Atemporal%20jitter%2C%20and%20local%20penetration%2C%20due%20to%20limitations%20in%20representing%0Astable%2C%20fine-grained%20motion%20fields.%20Through%20systematic%20analysis%2C%20we%20reformulate%0Atalking%20head%20generation%20into%20a%20unified%20framework%20comprising%20three%20steps%3A%20video%0Apreprocessing%2C%20motion%20representation%2C%20and%20rendering%20reconstruction.%20This%0Aframework%20underpins%20our%20proposed%20M2DAO-Talker%2C%20which%20addresses%20current%0Alimitations%20via%20multi-granular%20motion%20decoupling%20and%20alternating%20optimization.%0ASpecifically%2C%20we%20devise%20a%20novel%202D%20portrait%20preprocessing%20pipeline%20to%20extract%0Aframe-wise%20deformation%20control%20conditions%20%28motion%20region%20segmentation%20masks%2C%0Aand%20camera%20parameters%29%20to%20facilitate%20motion%20representation.%20To%20ameliorate%0Amotion%20modeling%2C%20we%20elaborate%20a%20multi-granular%20motion%20decoupling%20strategy%2C%0Awhich%20independently%20models%20non-rigid%20%28oral%20and%20facial%29%20and%20rigid%20%28head%29%20motions%0Afor%20improved%20reconstruction%20accuracy.%20Meanwhile%2C%20a%20motion%20consistency%0Aconstraint%20is%20developed%20to%20ensure%20head-torso%20kinematic%20consistency%2C%20thereby%0Amitigating%20penetration%20artifacts%20caused%20by%20motion%20aliasing.%20In%20addition%2C%20an%0Aalternating%20optimization%20strategy%20is%20designed%20to%20iteratively%20refine%20facial%20and%0Aoral%20motion%20parameters%2C%20enabling%20more%20realistic%20video%20generation.%20Experiments%0Aacross%20multiple%20datasets%20show%20that%20M2DAO-Talker%20achieves%20state-of-the-art%0Aperformance%2C%20with%20the%202.43%20dB%20PSNR%20improvement%20in%20generation%20quality%20and%200.64%0Again%20in%20user-evaluated%20video%20realness%20versus%20TalkingGaussian%20while%20with%20150%20FPS%0Ainference%20speed.%20Our%20project%20homepage%20is%0Ahttps%3A//m2dao-talker.github.io/M2DAO-Talk.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08307v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM2DAO-Talker%253A%2520Harmonizing%2520Multi-granular%2520Motion%2520Decoupling%2520and%250A%2520%2520Alternating%2520Optimization%2520for%2520Talking-head%2520Generation%26entry.906535625%3DKui%2520Jiang%2520and%2520Shiyu%2520Liu%2520and%2520Junjun%2520Jiang%2520and%2520Xin%2520Yang%2520and%2520Hongxun%2520Yao%2520and%2520Xiaopeng%2520Fan%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520head%2520generation%2520holds%2520significant%2520potential%2520for%2520film%250Aproduction.%2520While%2520existing%25203D%2520methods%2520have%2520advanced%2520motion%2520modeling%2520and%2520content%250Asynthesis%252C%2520they%2520often%2520produce%2520rendering%2520artifacts%252C%2520such%2520as%2520motion%2520blur%252C%250Atemporal%2520jitter%252C%2520and%2520local%2520penetration%252C%2520due%2520to%2520limitations%2520in%2520representing%250Astable%252C%2520fine-grained%2520motion%2520fields.%2520Through%2520systematic%2520analysis%252C%2520we%2520reformulate%250Atalking%2520head%2520generation%2520into%2520a%2520unified%2520framework%2520comprising%2520three%2520steps%253A%2520video%250Apreprocessing%252C%2520motion%2520representation%252C%2520and%2520rendering%2520reconstruction.%2520This%250Aframework%2520underpins%2520our%2520proposed%2520M2DAO-Talker%252C%2520which%2520addresses%2520current%250Alimitations%2520via%2520multi-granular%2520motion%2520decoupling%2520and%2520alternating%2520optimization.%250ASpecifically%252C%2520we%2520devise%2520a%2520novel%25202D%2520portrait%2520preprocessing%2520pipeline%2520to%2520extract%250Aframe-wise%2520deformation%2520control%2520conditions%2520%2528motion%2520region%2520segmentation%2520masks%252C%250Aand%2520camera%2520parameters%2529%2520to%2520facilitate%2520motion%2520representation.%2520To%2520ameliorate%250Amotion%2520modeling%252C%2520we%2520elaborate%2520a%2520multi-granular%2520motion%2520decoupling%2520strategy%252C%250Awhich%2520independently%2520models%2520non-rigid%2520%2528oral%2520and%2520facial%2529%2520and%2520rigid%2520%2528head%2529%2520motions%250Afor%2520improved%2520reconstruction%2520accuracy.%2520Meanwhile%252C%2520a%2520motion%2520consistency%250Aconstraint%2520is%2520developed%2520to%2520ensure%2520head-torso%2520kinematic%2520consistency%252C%2520thereby%250Amitigating%2520penetration%2520artifacts%2520caused%2520by%2520motion%2520aliasing.%2520In%2520addition%252C%2520an%250Aalternating%2520optimization%2520strategy%2520is%2520designed%2520to%2520iteratively%2520refine%2520facial%2520and%250Aoral%2520motion%2520parameters%252C%2520enabling%2520more%2520realistic%2520video%2520generation.%2520Experiments%250Aacross%2520multiple%2520datasets%2520show%2520that%2520M2DAO-Talker%2520achieves%2520state-of-the-art%250Aperformance%252C%2520with%2520the%25202.43%2520dB%2520PSNR%2520improvement%2520in%2520generation%2520quality%2520and%25200.64%250Again%2520in%2520user-evaluated%2520video%2520realness%2520versus%2520TalkingGaussian%2520while%2520with%2520150%2520FPS%250Ainference%2520speed.%2520Our%2520project%2520homepage%2520is%250Ahttps%253A//m2dao-talker.github.io/M2DAO-Talk.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08307v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M2DAO-Talker%3A%20Harmonizing%20Multi-granular%20Motion%20Decoupling%20and%0A%20%20Alternating%20Optimization%20for%20Talking-head%20Generation&entry.906535625=Kui%20Jiang%20and%20Shiyu%20Liu%20and%20Junjun%20Jiang%20and%20Xin%20Yang%20and%20Hongxun%20Yao%20and%20Xiaopeng%20Fan&entry.1292438233=%20%20Audio-driven%20talking%20head%20generation%20holds%20significant%20potential%20for%20film%0Aproduction.%20While%20existing%203D%20methods%20have%20advanced%20motion%20modeling%20and%20content%0Asynthesis%2C%20they%20often%20produce%20rendering%20artifacts%2C%20such%20as%20motion%20blur%2C%0Atemporal%20jitter%2C%20and%20local%20penetration%2C%20due%20to%20limitations%20in%20representing%0Astable%2C%20fine-grained%20motion%20fields.%20Through%20systematic%20analysis%2C%20we%20reformulate%0Atalking%20head%20generation%20into%20a%20unified%20framework%20comprising%20three%20steps%3A%20video%0Apreprocessing%2C%20motion%20representation%2C%20and%20rendering%20reconstruction.%20This%0Aframework%20underpins%20our%20proposed%20M2DAO-Talker%2C%20which%20addresses%20current%0Alimitations%20via%20multi-granular%20motion%20decoupling%20and%20alternating%20optimization.%0ASpecifically%2C%20we%20devise%20a%20novel%202D%20portrait%20preprocessing%20pipeline%20to%20extract%0Aframe-wise%20deformation%20control%20conditions%20%28motion%20region%20segmentation%20masks%2C%0Aand%20camera%20parameters%29%20to%20facilitate%20motion%20representation.%20To%20ameliorate%0Amotion%20modeling%2C%20we%20elaborate%20a%20multi-granular%20motion%20decoupling%20strategy%2C%0Awhich%20independently%20models%20non-rigid%20%28oral%20and%20facial%29%20and%20rigid%20%28head%29%20motions%0Afor%20improved%20reconstruction%20accuracy.%20Meanwhile%2C%20a%20motion%20consistency%0Aconstraint%20is%20developed%20to%20ensure%20head-torso%20kinematic%20consistency%2C%20thereby%0Amitigating%20penetration%20artifacts%20caused%20by%20motion%20aliasing.%20In%20addition%2C%20an%0Aalternating%20optimization%20strategy%20is%20designed%20to%20iteratively%20refine%20facial%20and%0Aoral%20motion%20parameters%2C%20enabling%20more%20realistic%20video%20generation.%20Experiments%0Aacross%20multiple%20datasets%20show%20that%20M2DAO-Talker%20achieves%20state-of-the-art%0Aperformance%2C%20with%20the%202.43%20dB%20PSNR%20improvement%20in%20generation%20quality%20and%200.64%0Again%20in%20user-evaluated%20video%20realness%20versus%20TalkingGaussian%20while%20with%20150%20FPS%0Ainference%20speed.%20Our%20project%20homepage%20is%0Ahttps%3A//m2dao-talker.github.io/M2DAO-Talk.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08307v2&entry.124074799=Read"},
{"title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos", "author": "Shanshan Zhong and Jiawei Peng and Zehan Zheng and Zhongzhan Huang and Wufei Ma and Guofeng Zhang and Qihao Liu and Alan Yuille and Jieneng Chen", "abstract": "  Existing methods for reconstructing animatable 3D animals from videos\ntypically rely on sparse semantic keypoints to fit parametric models. However,\nobtaining such keypoints is labor-intensive, and keypoint detectors trained on\nlimited animal data are often unreliable. To address this, we propose\n4D-Animal, a novel framework that reconstructs animatable 3D animals from\nvideos without requiring sparse keypoint annotations. Our approach introduces a\ndense feature network that maps 2D representations to SMAL parameters,\nenhancing both the efficiency and stability of the fitting process.\nFurthermore, we develop a hierarchical alignment strategy that integrates\nsilhouette, part-level, pixel-level, and temporal cues from pre-trained 2D\nvisual models to produce accurate and temporally coherent reconstructions\nacross frames. Extensive experiments demonstrate that 4D-Animal outperforms\nboth model-based and model-free baselines. Moreover, the high-quality 3D assets\ngenerated by our method can benefit other 3D tasks, underscoring its potential\nfor large-scale applications. The code is released at\nhttps://github.com/zhongshsh/4D-Animal.\n", "link": "http://arxiv.org/abs/2507.10437v1", "date": "2025-07-14", "relevancy": 2.9953, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6288}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.588}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D-Animal%3A%20Freely%20Reconstructing%20Animatable%203D%20Animals%20from%20Videos&body=Title%3A%204D-Animal%3A%20Freely%20Reconstructing%20Animatable%203D%20Animals%20from%20Videos%0AAuthor%3A%20Shanshan%20Zhong%20and%20Jiawei%20Peng%20and%20Zehan%20Zheng%20and%20Zhongzhan%20Huang%20and%20Wufei%20Ma%20and%20Guofeng%20Zhang%20and%20Qihao%20Liu%20and%20Alan%20Yuille%20and%20Jieneng%20Chen%0AAbstract%3A%20%20%20Existing%20methods%20for%20reconstructing%20animatable%203D%20animals%20from%20videos%0Atypically%20rely%20on%20sparse%20semantic%20keypoints%20to%20fit%20parametric%20models.%20However%2C%0Aobtaining%20such%20keypoints%20is%20labor-intensive%2C%20and%20keypoint%20detectors%20trained%20on%0Alimited%20animal%20data%20are%20often%20unreliable.%20To%20address%20this%2C%20we%20propose%0A4D-Animal%2C%20a%20novel%20framework%20that%20reconstructs%20animatable%203D%20animals%20from%0Avideos%20without%20requiring%20sparse%20keypoint%20annotations.%20Our%20approach%20introduces%20a%0Adense%20feature%20network%20that%20maps%202D%20representations%20to%20SMAL%20parameters%2C%0Aenhancing%20both%20the%20efficiency%20and%20stability%20of%20the%20fitting%20process.%0AFurthermore%2C%20we%20develop%20a%20hierarchical%20alignment%20strategy%20that%20integrates%0Asilhouette%2C%20part-level%2C%20pixel-level%2C%20and%20temporal%20cues%20from%20pre-trained%202D%0Avisual%20models%20to%20produce%20accurate%20and%20temporally%20coherent%20reconstructions%0Aacross%20frames.%20Extensive%20experiments%20demonstrate%20that%204D-Animal%20outperforms%0Aboth%20model-based%20and%20model-free%20baselines.%20Moreover%2C%20the%20high-quality%203D%20assets%0Agenerated%20by%20our%20method%20can%20benefit%20other%203D%20tasks%2C%20underscoring%20its%20potential%0Afor%20large-scale%20applications.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/zhongshsh/4D-Animal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D-Animal%253A%2520Freely%2520Reconstructing%2520Animatable%25203D%2520Animals%2520from%2520Videos%26entry.906535625%3DShanshan%2520Zhong%2520and%2520Jiawei%2520Peng%2520and%2520Zehan%2520Zheng%2520and%2520Zhongzhan%2520Huang%2520and%2520Wufei%2520Ma%2520and%2520Guofeng%2520Zhang%2520and%2520Qihao%2520Liu%2520and%2520Alan%2520Yuille%2520and%2520Jieneng%2520Chen%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520reconstructing%2520animatable%25203D%2520animals%2520from%2520videos%250Atypically%2520rely%2520on%2520sparse%2520semantic%2520keypoints%2520to%2520fit%2520parametric%2520models.%2520However%252C%250Aobtaining%2520such%2520keypoints%2520is%2520labor-intensive%252C%2520and%2520keypoint%2520detectors%2520trained%2520on%250Alimited%2520animal%2520data%2520are%2520often%2520unreliable.%2520To%2520address%2520this%252C%2520we%2520propose%250A4D-Animal%252C%2520a%2520novel%2520framework%2520that%2520reconstructs%2520animatable%25203D%2520animals%2520from%250Avideos%2520without%2520requiring%2520sparse%2520keypoint%2520annotations.%2520Our%2520approach%2520introduces%2520a%250Adense%2520feature%2520network%2520that%2520maps%25202D%2520representations%2520to%2520SMAL%2520parameters%252C%250Aenhancing%2520both%2520the%2520efficiency%2520and%2520stability%2520of%2520the%2520fitting%2520process.%250AFurthermore%252C%2520we%2520develop%2520a%2520hierarchical%2520alignment%2520strategy%2520that%2520integrates%250Asilhouette%252C%2520part-level%252C%2520pixel-level%252C%2520and%2520temporal%2520cues%2520from%2520pre-trained%25202D%250Avisual%2520models%2520to%2520produce%2520accurate%2520and%2520temporally%2520coherent%2520reconstructions%250Aacross%2520frames.%2520Extensive%2520experiments%2520demonstrate%2520that%25204D-Animal%2520outperforms%250Aboth%2520model-based%2520and%2520model-free%2520baselines.%2520Moreover%252C%2520the%2520high-quality%25203D%2520assets%250Agenerated%2520by%2520our%2520method%2520can%2520benefit%2520other%25203D%2520tasks%252C%2520underscoring%2520its%2520potential%250Afor%2520large-scale%2520applications.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/zhongshsh/4D-Animal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D-Animal%3A%20Freely%20Reconstructing%20Animatable%203D%20Animals%20from%20Videos&entry.906535625=Shanshan%20Zhong%20and%20Jiawei%20Peng%20and%20Zehan%20Zheng%20and%20Zhongzhan%20Huang%20and%20Wufei%20Ma%20and%20Guofeng%20Zhang%20and%20Qihao%20Liu%20and%20Alan%20Yuille%20and%20Jieneng%20Chen&entry.1292438233=%20%20Existing%20methods%20for%20reconstructing%20animatable%203D%20animals%20from%20videos%0Atypically%20rely%20on%20sparse%20semantic%20keypoints%20to%20fit%20parametric%20models.%20However%2C%0Aobtaining%20such%20keypoints%20is%20labor-intensive%2C%20and%20keypoint%20detectors%20trained%20on%0Alimited%20animal%20data%20are%20often%20unreliable.%20To%20address%20this%2C%20we%20propose%0A4D-Animal%2C%20a%20novel%20framework%20that%20reconstructs%20animatable%203D%20animals%20from%0Avideos%20without%20requiring%20sparse%20keypoint%20annotations.%20Our%20approach%20introduces%20a%0Adense%20feature%20network%20that%20maps%202D%20representations%20to%20SMAL%20parameters%2C%0Aenhancing%20both%20the%20efficiency%20and%20stability%20of%20the%20fitting%20process.%0AFurthermore%2C%20we%20develop%20a%20hierarchical%20alignment%20strategy%20that%20integrates%0Asilhouette%2C%20part-level%2C%20pixel-level%2C%20and%20temporal%20cues%20from%20pre-trained%202D%0Avisual%20models%20to%20produce%20accurate%20and%20temporally%20coherent%20reconstructions%0Aacross%20frames.%20Extensive%20experiments%20demonstrate%20that%204D-Animal%20outperforms%0Aboth%20model-based%20and%20model-free%20baselines.%20Moreover%2C%20the%20high-quality%203D%20assets%0Agenerated%20by%20our%20method%20can%20benefit%20other%203D%20tasks%2C%20underscoring%20its%20potential%0Afor%20large-scale%20applications.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/zhongshsh/4D-Animal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10437v1&entry.124074799=Read"},
{"title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs", "author": "Jiahe Zhao and Rongkun Zheng and Yi Wang and Helin Wang and Hengshuang Zhao", "abstract": "  In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo.\n", "link": "http://arxiv.org/abs/2507.10302v1", "date": "2025-07-14", "relevancy": 2.9692, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6074}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCo%3A%20Towards%20Distinct%20and%20Coherent%20Visual%20Encapsulation%20in%20Video%20MLLMs&body=Title%3A%20DisCo%3A%20Towards%20Distinct%20and%20Coherent%20Visual%20Encapsulation%20in%20Video%20MLLMs%0AAuthor%3A%20Jiahe%20Zhao%20and%20Rongkun%20Zheng%20and%20Yi%20Wang%20and%20Helin%20Wang%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20In%20video%20Multimodal%20Large%20Language%20Models%20%28video%20MLLMs%29%2C%20the%20visual%0Aencapsulation%20process%20plays%20a%20pivotal%20role%20in%20converting%20video%20contents%20into%0Arepresentative%20tokens%20for%20LLM%20input.%20While%20linear%20projectors%20are%20widely%0Aemployed%20for%20encapsulation%2C%20they%20introduce%20semantic%20indistinctness%20and%20temporal%0Aincoherence%20when%20applied%20to%20videos.%20Conversely%2C%20the%20structure%20of%20resamplers%0Ashows%20promise%20in%20tackling%20these%20challenges%2C%20but%20an%20effective%20solution%20remains%0Aunexplored.%20Drawing%20inspiration%20from%20resampler%20structures%2C%20we%20introduce%20DisCo%2C%0Aa%20novel%20visual%20encapsulation%20method%20designed%20to%20yield%20semantically%20distinct%20and%0Atemporally%20coherent%20visual%20tokens%20for%20video%20MLLMs.%20DisCo%20integrates%20two%20key%0Acomponents%3A%20%281%29%20A%20Visual%20Concept%20Discriminator%20%28VCD%29%20module%2C%20assigning%20unique%0Asemantics%20for%20visual%20tokens%20by%20associating%20them%20in%20pair%20with%20discriminative%0Aconcepts%20in%20the%20video.%20%282%29%20A%20Temporal%20Focus%20Calibrator%20%28TFC%29%20module%2C%20ensuring%0Aconsistent%20temporal%20focus%20of%20visual%20tokens%20to%20video%20elements%20across%20every%20video%0Aframe.%20Through%20extensive%20experiments%20on%20multiple%20video%20MLLM%20frameworks%2C%20we%0Ademonstrate%20that%20DisCo%20remarkably%20outperforms%20previous%20state-of-the-art%20methods%0Aacross%20a%20variety%20of%20video%20understanding%20benchmarks%2C%20while%20also%20achieving%20higher%0Atoken%20efficiency%20thanks%20to%20the%20reduction%20of%20semantic%20indistinctness.%20The%20code%3A%0Ahttps%3A//github.com/ZJHTerry18/DisCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCo%253A%2520Towards%2520Distinct%2520and%2520Coherent%2520Visual%2520Encapsulation%2520in%2520Video%2520MLLMs%26entry.906535625%3DJiahe%2520Zhao%2520and%2520Rongkun%2520Zheng%2520and%2520Yi%2520Wang%2520and%2520Helin%2520Wang%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520In%2520video%2520Multimodal%2520Large%2520Language%2520Models%2520%2528video%2520MLLMs%2529%252C%2520the%2520visual%250Aencapsulation%2520process%2520plays%2520a%2520pivotal%2520role%2520in%2520converting%2520video%2520contents%2520into%250Arepresentative%2520tokens%2520for%2520LLM%2520input.%2520While%2520linear%2520projectors%2520are%2520widely%250Aemployed%2520for%2520encapsulation%252C%2520they%2520introduce%2520semantic%2520indistinctness%2520and%2520temporal%250Aincoherence%2520when%2520applied%2520to%2520videos.%2520Conversely%252C%2520the%2520structure%2520of%2520resamplers%250Ashows%2520promise%2520in%2520tackling%2520these%2520challenges%252C%2520but%2520an%2520effective%2520solution%2520remains%250Aunexplored.%2520Drawing%2520inspiration%2520from%2520resampler%2520structures%252C%2520we%2520introduce%2520DisCo%252C%250Aa%2520novel%2520visual%2520encapsulation%2520method%2520designed%2520to%2520yield%2520semantically%2520distinct%2520and%250Atemporally%2520coherent%2520visual%2520tokens%2520for%2520video%2520MLLMs.%2520DisCo%2520integrates%2520two%2520key%250Acomponents%253A%2520%25281%2529%2520A%2520Visual%2520Concept%2520Discriminator%2520%2528VCD%2529%2520module%252C%2520assigning%2520unique%250Asemantics%2520for%2520visual%2520tokens%2520by%2520associating%2520them%2520in%2520pair%2520with%2520discriminative%250Aconcepts%2520in%2520the%2520video.%2520%25282%2529%2520A%2520Temporal%2520Focus%2520Calibrator%2520%2528TFC%2529%2520module%252C%2520ensuring%250Aconsistent%2520temporal%2520focus%2520of%2520visual%2520tokens%2520to%2520video%2520elements%2520across%2520every%2520video%250Aframe.%2520Through%2520extensive%2520experiments%2520on%2520multiple%2520video%2520MLLM%2520frameworks%252C%2520we%250Ademonstrate%2520that%2520DisCo%2520remarkably%2520outperforms%2520previous%2520state-of-the-art%2520methods%250Aacross%2520a%2520variety%2520of%2520video%2520understanding%2520benchmarks%252C%2520while%2520also%2520achieving%2520higher%250Atoken%2520efficiency%2520thanks%2520to%2520the%2520reduction%2520of%2520semantic%2520indistinctness.%2520The%2520code%253A%250Ahttps%253A//github.com/ZJHTerry18/DisCo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCo%3A%20Towards%20Distinct%20and%20Coherent%20Visual%20Encapsulation%20in%20Video%20MLLMs&entry.906535625=Jiahe%20Zhao%20and%20Rongkun%20Zheng%20and%20Yi%20Wang%20and%20Helin%20Wang%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20In%20video%20Multimodal%20Large%20Language%20Models%20%28video%20MLLMs%29%2C%20the%20visual%0Aencapsulation%20process%20plays%20a%20pivotal%20role%20in%20converting%20video%20contents%20into%0Arepresentative%20tokens%20for%20LLM%20input.%20While%20linear%20projectors%20are%20widely%0Aemployed%20for%20encapsulation%2C%20they%20introduce%20semantic%20indistinctness%20and%20temporal%0Aincoherence%20when%20applied%20to%20videos.%20Conversely%2C%20the%20structure%20of%20resamplers%0Ashows%20promise%20in%20tackling%20these%20challenges%2C%20but%20an%20effective%20solution%20remains%0Aunexplored.%20Drawing%20inspiration%20from%20resampler%20structures%2C%20we%20introduce%20DisCo%2C%0Aa%20novel%20visual%20encapsulation%20method%20designed%20to%20yield%20semantically%20distinct%20and%0Atemporally%20coherent%20visual%20tokens%20for%20video%20MLLMs.%20DisCo%20integrates%20two%20key%0Acomponents%3A%20%281%29%20A%20Visual%20Concept%20Discriminator%20%28VCD%29%20module%2C%20assigning%20unique%0Asemantics%20for%20visual%20tokens%20by%20associating%20them%20in%20pair%20with%20discriminative%0Aconcepts%20in%20the%20video.%20%282%29%20A%20Temporal%20Focus%20Calibrator%20%28TFC%29%20module%2C%20ensuring%0Aconsistent%20temporal%20focus%20of%20visual%20tokens%20to%20video%20elements%20across%20every%20video%0Aframe.%20Through%20extensive%20experiments%20on%20multiple%20video%20MLLM%20frameworks%2C%20we%0Ademonstrate%20that%20DisCo%20remarkably%20outperforms%20previous%20state-of-the-art%20methods%0Aacross%20a%20variety%20of%20video%20understanding%20benchmarks%2C%20while%20also%20achieving%20higher%0Atoken%20efficiency%20thanks%20to%20the%20reduction%20of%20semantic%20indistinctness.%20The%20code%3A%0Ahttps%3A//github.com/ZJHTerry18/DisCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10302v1&entry.124074799=Read"},
{"title": "Test-Time Canonicalization by Foundation Models for Robust Perception", "author": "Utkarsh Singhal and Ryan Feng and Stella X. Yu and Atul Prakash", "abstract": "  Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal.\n", "link": "http://arxiv.org/abs/2507.10375v1", "date": "2025-07-14", "relevancy": 2.9365, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Canonicalization%20by%20Foundation%20Models%20for%20Robust%20Perception&body=Title%3A%20Test-Time%20Canonicalization%20by%20Foundation%20Models%20for%20Robust%20Perception%0AAuthor%3A%20Utkarsh%20Singhal%20and%20Ryan%20Feng%20and%20Stella%20X.%20Yu%20and%20Atul%20Prakash%0AAbstract%3A%20%20%20Real-world%20visual%20perception%20requires%20invariance%20to%20diverse%20transformations%2C%0Ayet%20current%20methods%20rely%20heavily%20on%20specialized%20architectures%20or%20training%20on%0Apredefined%20augmentations%2C%20limiting%20generalization.%20We%20propose%20FOCAL%2C%20a%0Atest-time%2C%20data-driven%20framework%20that%20achieves%20robust%20perception%20by%20leveraging%0Ainternet-scale%20visual%20priors%20from%20foundation%20models.%20By%20generating%20and%0Aoptimizing%20candidate%20transformations%20toward%20visually%20typical%2C%20%22canonical%22%0Aviews%2C%20FOCAL%20enhances%20robustness%20without%20re-training%20or%20architectural%20changes.%0AOur%20experiments%20demonstrate%20improved%20robustness%20of%20CLIP%20and%20SAM%20across%0Achallenging%20transformations%2C%20including%202D/3D%20rotations%2C%20illumination%20shifts%0A%28contrast%20and%20color%29%2C%20and%20day-night%20variations.%20We%20also%20highlight%20potential%0Aapplications%20in%20active%20vision.%20Our%20approach%20challenges%20the%20assumption%20that%0Atransform-specific%20training%20is%20necessary%2C%20instead%20offering%20a%20scalable%20path%20to%0Ainvariance.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/sutkarsh/focal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Canonicalization%2520by%2520Foundation%2520Models%2520for%2520Robust%2520Perception%26entry.906535625%3DUtkarsh%2520Singhal%2520and%2520Ryan%2520Feng%2520and%2520Stella%2520X.%2520Yu%2520and%2520Atul%2520Prakash%26entry.1292438233%3D%2520%2520Real-world%2520visual%2520perception%2520requires%2520invariance%2520to%2520diverse%2520transformations%252C%250Ayet%2520current%2520methods%2520rely%2520heavily%2520on%2520specialized%2520architectures%2520or%2520training%2520on%250Apredefined%2520augmentations%252C%2520limiting%2520generalization.%2520We%2520propose%2520FOCAL%252C%2520a%250Atest-time%252C%2520data-driven%2520framework%2520that%2520achieves%2520robust%2520perception%2520by%2520leveraging%250Ainternet-scale%2520visual%2520priors%2520from%2520foundation%2520models.%2520By%2520generating%2520and%250Aoptimizing%2520candidate%2520transformations%2520toward%2520visually%2520typical%252C%2520%2522canonical%2522%250Aviews%252C%2520FOCAL%2520enhances%2520robustness%2520without%2520re-training%2520or%2520architectural%2520changes.%250AOur%2520experiments%2520demonstrate%2520improved%2520robustness%2520of%2520CLIP%2520and%2520SAM%2520across%250Achallenging%2520transformations%252C%2520including%25202D/3D%2520rotations%252C%2520illumination%2520shifts%250A%2528contrast%2520and%2520color%2529%252C%2520and%2520day-night%2520variations.%2520We%2520also%2520highlight%2520potential%250Aapplications%2520in%2520active%2520vision.%2520Our%2520approach%2520challenges%2520the%2520assumption%2520that%250Atransform-specific%2520training%2520is%2520necessary%252C%2520instead%2520offering%2520a%2520scalable%2520path%2520to%250Ainvariance.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/sutkarsh/focal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Canonicalization%20by%20Foundation%20Models%20for%20Robust%20Perception&entry.906535625=Utkarsh%20Singhal%20and%20Ryan%20Feng%20and%20Stella%20X.%20Yu%20and%20Atul%20Prakash&entry.1292438233=%20%20Real-world%20visual%20perception%20requires%20invariance%20to%20diverse%20transformations%2C%0Ayet%20current%20methods%20rely%20heavily%20on%20specialized%20architectures%20or%20training%20on%0Apredefined%20augmentations%2C%20limiting%20generalization.%20We%20propose%20FOCAL%2C%20a%0Atest-time%2C%20data-driven%20framework%20that%20achieves%20robust%20perception%20by%20leveraging%0Ainternet-scale%20visual%20priors%20from%20foundation%20models.%20By%20generating%20and%0Aoptimizing%20candidate%20transformations%20toward%20visually%20typical%2C%20%22canonical%22%0Aviews%2C%20FOCAL%20enhances%20robustness%20without%20re-training%20or%20architectural%20changes.%0AOur%20experiments%20demonstrate%20improved%20robustness%20of%20CLIP%20and%20SAM%20across%0Achallenging%20transformations%2C%20including%202D/3D%20rotations%2C%20illumination%20shifts%0A%28contrast%20and%20color%29%2C%20and%20day-night%20variations.%20We%20also%20highlight%20potential%0Aapplications%20in%20active%20vision.%20Our%20approach%20challenges%20the%20assumption%20that%0Atransform-specific%20training%20is%20necessary%2C%20instead%20offering%20a%20scalable%20path%20to%0Ainvariance.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/sutkarsh/focal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10375v1&entry.124074799=Read"},
{"title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature\n  Matching", "author": "Yuhan Liu and Jingwen Fu and Yang Wu and Kangyi Wu and Pengna Li and Jiayi Wu and Sanping Zhou and Jingmin Xin", "abstract": "  Leveraging the vision foundation models has emerged as a mainstream paradigm\nthat improves the performance of image feature matching. However, previous\nworks have ignored the misalignment when introducing the foundation models into\nfeature matching. The misalignment arises from the discrepancy between the\nfoundation models focusing on single-image understanding and the cross-image\nunderstanding requirement of feature matching. Specifically, 1) the embeddings\nderived from commonly used foundation models exhibit discrepancies with the\noptimal embeddings required for feature matching; 2) lacking an effective\nmechanism to leverage the single-image understanding ability into cross-image\nunderstanding. A significant consequence of the misalignment is they struggle\nwhen addressing multi-instance feature matching problems. To address this, we\nintroduce a simple but effective framework, called IMD (Image feature Matching\nwith a pre-trained Diffusion model) with two parts: 1) Unlike the dominant\nsolutions employing contrastive-learning based foundation models that emphasize\nglobal semantics, we integrate the generative-based diffusion models to\neffectively capture instance-level details. 2) We leverage the prompt mechanism\nin generative model as a natural tunnel, propose a novel cross-image\ninteraction prompting module to facilitate bidirectional information\ninteraction between image pairs. To more accurately measure the misalignment,\nwe propose a new benchmark called IMIM, which focuses on multi-instance\nscenarios. Our proposed IMD establishes a new state-of-the-art in commonly\nevaluated benchmarks, and the superior improvement 12% in IMIM indicates our\nmethod efficiently mitigates the misalignment.\n", "link": "http://arxiv.org/abs/2507.10318v1", "date": "2025-07-14", "relevancy": 2.8636, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%3A%20Aligning%20Vision%20Foundation%20Models%20to%20Image%20Feature%0A%20%20Matching&body=Title%3A%20Mind%20the%20Gap%3A%20Aligning%20Vision%20Foundation%20Models%20to%20Image%20Feature%0A%20%20Matching%0AAuthor%3A%20Yuhan%20Liu%20and%20Jingwen%20Fu%20and%20Yang%20Wu%20and%20Kangyi%20Wu%20and%20Pengna%20Li%20and%20Jiayi%20Wu%20and%20Sanping%20Zhou%20and%20Jingmin%20Xin%0AAbstract%3A%20%20%20Leveraging%20the%20vision%20foundation%20models%20has%20emerged%20as%20a%20mainstream%20paradigm%0Athat%20improves%20the%20performance%20of%20image%20feature%20matching.%20However%2C%20previous%0Aworks%20have%20ignored%20the%20misalignment%20when%20introducing%20the%20foundation%20models%20into%0Afeature%20matching.%20The%20misalignment%20arises%20from%20the%20discrepancy%20between%20the%0Afoundation%20models%20focusing%20on%20single-image%20understanding%20and%20the%20cross-image%0Aunderstanding%20requirement%20of%20feature%20matching.%20Specifically%2C%201%29%20the%20embeddings%0Aderived%20from%20commonly%20used%20foundation%20models%20exhibit%20discrepancies%20with%20the%0Aoptimal%20embeddings%20required%20for%20feature%20matching%3B%202%29%20lacking%20an%20effective%0Amechanism%20to%20leverage%20the%20single-image%20understanding%20ability%20into%20cross-image%0Aunderstanding.%20A%20significant%20consequence%20of%20the%20misalignment%20is%20they%20struggle%0Awhen%20addressing%20multi-instance%20feature%20matching%20problems.%20To%20address%20this%2C%20we%0Aintroduce%20a%20simple%20but%20effective%20framework%2C%20called%20IMD%20%28Image%20feature%20Matching%0Awith%20a%20pre-trained%20Diffusion%20model%29%20with%20two%20parts%3A%201%29%20Unlike%20the%20dominant%0Asolutions%20employing%20contrastive-learning%20based%20foundation%20models%20that%20emphasize%0Aglobal%20semantics%2C%20we%20integrate%20the%20generative-based%20diffusion%20models%20to%0Aeffectively%20capture%20instance-level%20details.%202%29%20We%20leverage%20the%20prompt%20mechanism%0Ain%20generative%20model%20as%20a%20natural%20tunnel%2C%20propose%20a%20novel%20cross-image%0Ainteraction%20prompting%20module%20to%20facilitate%20bidirectional%20information%0Ainteraction%20between%20image%20pairs.%20To%20more%20accurately%20measure%20the%20misalignment%2C%0Awe%20propose%20a%20new%20benchmark%20called%20IMIM%2C%20which%20focuses%20on%20multi-instance%0Ascenarios.%20Our%20proposed%20IMD%20establishes%20a%20new%20state-of-the-art%20in%20commonly%0Aevaluated%20benchmarks%2C%20and%20the%20superior%20improvement%2012%25%20in%20IMIM%20indicates%20our%0Amethod%20efficiently%20mitigates%20the%20misalignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%253A%2520Aligning%2520Vision%2520Foundation%2520Models%2520to%2520Image%2520Feature%250A%2520%2520Matching%26entry.906535625%3DYuhan%2520Liu%2520and%2520Jingwen%2520Fu%2520and%2520Yang%2520Wu%2520and%2520Kangyi%2520Wu%2520and%2520Pengna%2520Li%2520and%2520Jiayi%2520Wu%2520and%2520Sanping%2520Zhou%2520and%2520Jingmin%2520Xin%26entry.1292438233%3D%2520%2520Leveraging%2520the%2520vision%2520foundation%2520models%2520has%2520emerged%2520as%2520a%2520mainstream%2520paradigm%250Athat%2520improves%2520the%2520performance%2520of%2520image%2520feature%2520matching.%2520However%252C%2520previous%250Aworks%2520have%2520ignored%2520the%2520misalignment%2520when%2520introducing%2520the%2520foundation%2520models%2520into%250Afeature%2520matching.%2520The%2520misalignment%2520arises%2520from%2520the%2520discrepancy%2520between%2520the%250Afoundation%2520models%2520focusing%2520on%2520single-image%2520understanding%2520and%2520the%2520cross-image%250Aunderstanding%2520requirement%2520of%2520feature%2520matching.%2520Specifically%252C%25201%2529%2520the%2520embeddings%250Aderived%2520from%2520commonly%2520used%2520foundation%2520models%2520exhibit%2520discrepancies%2520with%2520the%250Aoptimal%2520embeddings%2520required%2520for%2520feature%2520matching%253B%25202%2529%2520lacking%2520an%2520effective%250Amechanism%2520to%2520leverage%2520the%2520single-image%2520understanding%2520ability%2520into%2520cross-image%250Aunderstanding.%2520A%2520significant%2520consequence%2520of%2520the%2520misalignment%2520is%2520they%2520struggle%250Awhen%2520addressing%2520multi-instance%2520feature%2520matching%2520problems.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520simple%2520but%2520effective%2520framework%252C%2520called%2520IMD%2520%2528Image%2520feature%2520Matching%250Awith%2520a%2520pre-trained%2520Diffusion%2520model%2529%2520with%2520two%2520parts%253A%25201%2529%2520Unlike%2520the%2520dominant%250Asolutions%2520employing%2520contrastive-learning%2520based%2520foundation%2520models%2520that%2520emphasize%250Aglobal%2520semantics%252C%2520we%2520integrate%2520the%2520generative-based%2520diffusion%2520models%2520to%250Aeffectively%2520capture%2520instance-level%2520details.%25202%2529%2520We%2520leverage%2520the%2520prompt%2520mechanism%250Ain%2520generative%2520model%2520as%2520a%2520natural%2520tunnel%252C%2520propose%2520a%2520novel%2520cross-image%250Ainteraction%2520prompting%2520module%2520to%2520facilitate%2520bidirectional%2520information%250Ainteraction%2520between%2520image%2520pairs.%2520To%2520more%2520accurately%2520measure%2520the%2520misalignment%252C%250Awe%2520propose%2520a%2520new%2520benchmark%2520called%2520IMIM%252C%2520which%2520focuses%2520on%2520multi-instance%250Ascenarios.%2520Our%2520proposed%2520IMD%2520establishes%2520a%2520new%2520state-of-the-art%2520in%2520commonly%250Aevaluated%2520benchmarks%252C%2520and%2520the%2520superior%2520improvement%252012%2525%2520in%2520IMIM%2520indicates%2520our%250Amethod%2520efficiently%2520mitigates%2520the%2520misalignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%3A%20Aligning%20Vision%20Foundation%20Models%20to%20Image%20Feature%0A%20%20Matching&entry.906535625=Yuhan%20Liu%20and%20Jingwen%20Fu%20and%20Yang%20Wu%20and%20Kangyi%20Wu%20and%20Pengna%20Li%20and%20Jiayi%20Wu%20and%20Sanping%20Zhou%20and%20Jingmin%20Xin&entry.1292438233=%20%20Leveraging%20the%20vision%20foundation%20models%20has%20emerged%20as%20a%20mainstream%20paradigm%0Athat%20improves%20the%20performance%20of%20image%20feature%20matching.%20However%2C%20previous%0Aworks%20have%20ignored%20the%20misalignment%20when%20introducing%20the%20foundation%20models%20into%0Afeature%20matching.%20The%20misalignment%20arises%20from%20the%20discrepancy%20between%20the%0Afoundation%20models%20focusing%20on%20single-image%20understanding%20and%20the%20cross-image%0Aunderstanding%20requirement%20of%20feature%20matching.%20Specifically%2C%201%29%20the%20embeddings%0Aderived%20from%20commonly%20used%20foundation%20models%20exhibit%20discrepancies%20with%20the%0Aoptimal%20embeddings%20required%20for%20feature%20matching%3B%202%29%20lacking%20an%20effective%0Amechanism%20to%20leverage%20the%20single-image%20understanding%20ability%20into%20cross-image%0Aunderstanding.%20A%20significant%20consequence%20of%20the%20misalignment%20is%20they%20struggle%0Awhen%20addressing%20multi-instance%20feature%20matching%20problems.%20To%20address%20this%2C%20we%0Aintroduce%20a%20simple%20but%20effective%20framework%2C%20called%20IMD%20%28Image%20feature%20Matching%0Awith%20a%20pre-trained%20Diffusion%20model%29%20with%20two%20parts%3A%201%29%20Unlike%20the%20dominant%0Asolutions%20employing%20contrastive-learning%20based%20foundation%20models%20that%20emphasize%0Aglobal%20semantics%2C%20we%20integrate%20the%20generative-based%20diffusion%20models%20to%0Aeffectively%20capture%20instance-level%20details.%202%29%20We%20leverage%20the%20prompt%20mechanism%0Ain%20generative%20model%20as%20a%20natural%20tunnel%2C%20propose%20a%20novel%20cross-image%0Ainteraction%20prompting%20module%20to%20facilitate%20bidirectional%20information%0Ainteraction%20between%20image%20pairs.%20To%20more%20accurately%20measure%20the%20misalignment%2C%0Awe%20propose%20a%20new%20benchmark%20called%20IMIM%2C%20which%20focuses%20on%20multi-instance%0Ascenarios.%20Our%20proposed%20IMD%20establishes%20a%20new%20state-of-the-art%20in%20commonly%0Aevaluated%20benchmarks%2C%20and%20the%20superior%20improvement%2012%25%20in%20IMIM%20indicates%20our%0Amethod%20efficiently%20mitigates%20the%20misalignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10318v1&entry.124074799=Read"},
{"title": "Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware\n  Point Cloud Registration in the Wild", "author": "Turcan Tuna and Julian Nubert and Patrick Pfreundschuh and Cesar Cadena and Shehryar Khattak and Marco Hutter", "abstract": "  The ICP registration algorithm has been a preferred method for LiDAR-based\nrobot localization for nearly a decade. However, even in modern SLAM solutions,\nICP can degrade and become unreliable in geometrically ill-conditioned\nenvironments. Current solutions primarily focus on utilizing additional sources\nof information, such as external odometry, to either replace the degenerate\ndirections of the optimization solution or add additional constraints in a\nsensor-fusion setup afterward.\n  In response, this work investigates and compares new and existing degeneracy\nmitigation methods for robust LiDAR-based localization and analyzes the\nefficacy of these approaches in degenerate environments for the first time in\nthe literature at this scale. Specifically, this work investigates i) the\neffect of using active or passive degeneracy mitigation methods for the problem\nof ill-conditioned ICP in LiDAR degenerate environments, ii) the evaluation of\nTSVD, inequality constraints, and linear/non-linear Tikhonov regularization for\nthe application of degenerate point cloud registration for the first time.\nFurthermore, a sensitivity analysis for least-squares minimization step of the\nICP problem is carried out to better understand how each method affects the\noptimization and what to expect from each method. The results of the analysis\nare validated through multiple real-world robotic field and simulated\nexperiments. The analysis demonstrates that active optimization degeneracy\nmitigation is necessary and advantageous in the absence of reliable external\nestimate assistance for LiDAR-SLAM, and soft-constrained methods can provide\nbetter results in complex ill-conditioned scenarios with heuristic fine-tuned\nparameters.\n", "link": "http://arxiv.org/abs/2408.11809v3", "date": "2025-07-14", "relevancy": 2.8031, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5883}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5487}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild&body=Title%3A%20Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild%0AAuthor%3A%20Turcan%20Tuna%20and%20Julian%20Nubert%20and%20Patrick%20Pfreundschuh%20and%20Cesar%20Cadena%20and%20Shehryar%20Khattak%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20The%20ICP%20registration%20algorithm%20has%20been%20a%20preferred%20method%20for%20LiDAR-based%0Arobot%20localization%20for%20nearly%20a%20decade.%20However%2C%20even%20in%20modern%20SLAM%20solutions%2C%0AICP%20can%20degrade%20and%20become%20unreliable%20in%20geometrically%20ill-conditioned%0Aenvironments.%20Current%20solutions%20primarily%20focus%20on%20utilizing%20additional%20sources%0Aof%20information%2C%20such%20as%20external%20odometry%2C%20to%20either%20replace%20the%20degenerate%0Adirections%20of%20the%20optimization%20solution%20or%20add%20additional%20constraints%20in%20a%0Asensor-fusion%20setup%20afterward.%0A%20%20In%20response%2C%20this%20work%20investigates%20and%20compares%20new%20and%20existing%20degeneracy%0Amitigation%20methods%20for%20robust%20LiDAR-based%20localization%20and%20analyzes%20the%0Aefficacy%20of%20these%20approaches%20in%20degenerate%20environments%20for%20the%20first%20time%20in%0Athe%20literature%20at%20this%20scale.%20Specifically%2C%20this%20work%20investigates%20i%29%20the%0Aeffect%20of%20using%20active%20or%20passive%20degeneracy%20mitigation%20methods%20for%20the%20problem%0Aof%20ill-conditioned%20ICP%20in%20LiDAR%20degenerate%20environments%2C%20ii%29%20the%20evaluation%20of%0ATSVD%2C%20inequality%20constraints%2C%20and%20linear/non-linear%20Tikhonov%20regularization%20for%0Athe%20application%20of%20degenerate%20point%20cloud%20registration%20for%20the%20first%20time.%0AFurthermore%2C%20a%20sensitivity%20analysis%20for%20least-squares%20minimization%20step%20of%20the%0AICP%20problem%20is%20carried%20out%20to%20better%20understand%20how%20each%20method%20affects%20the%0Aoptimization%20and%20what%20to%20expect%20from%20each%20method.%20The%20results%20of%20the%20analysis%0Aare%20validated%20through%20multiple%20real-world%20robotic%20field%20and%20simulated%0Aexperiments.%20The%20analysis%20demonstrates%20that%20active%20optimization%20degeneracy%0Amitigation%20is%20necessary%20and%20advantageous%20in%20the%20absence%20of%20reliable%20external%0Aestimate%20assistance%20for%20LiDAR-SLAM%2C%20and%20soft-constrained%20methods%20can%20provide%0Abetter%20results%20in%20complex%20ill-conditioned%20scenarios%20with%20heuristic%20fine-tuned%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11809v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformed%252C%2520Constrained%252C%2520Aligned%253A%2520A%2520Field%2520Analysis%2520on%2520Degeneracy-aware%250A%2520%2520Point%2520Cloud%2520Registration%2520in%2520the%2520Wild%26entry.906535625%3DTurcan%2520Tuna%2520and%2520Julian%2520Nubert%2520and%2520Patrick%2520Pfreundschuh%2520and%2520Cesar%2520Cadena%2520and%2520Shehryar%2520Khattak%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520The%2520ICP%2520registration%2520algorithm%2520has%2520been%2520a%2520preferred%2520method%2520for%2520LiDAR-based%250Arobot%2520localization%2520for%2520nearly%2520a%2520decade.%2520However%252C%2520even%2520in%2520modern%2520SLAM%2520solutions%252C%250AICP%2520can%2520degrade%2520and%2520become%2520unreliable%2520in%2520geometrically%2520ill-conditioned%250Aenvironments.%2520Current%2520solutions%2520primarily%2520focus%2520on%2520utilizing%2520additional%2520sources%250Aof%2520information%252C%2520such%2520as%2520external%2520odometry%252C%2520to%2520either%2520replace%2520the%2520degenerate%250Adirections%2520of%2520the%2520optimization%2520solution%2520or%2520add%2520additional%2520constraints%2520in%2520a%250Asensor-fusion%2520setup%2520afterward.%250A%2520%2520In%2520response%252C%2520this%2520work%2520investigates%2520and%2520compares%2520new%2520and%2520existing%2520degeneracy%250Amitigation%2520methods%2520for%2520robust%2520LiDAR-based%2520localization%2520and%2520analyzes%2520the%250Aefficacy%2520of%2520these%2520approaches%2520in%2520degenerate%2520environments%2520for%2520the%2520first%2520time%2520in%250Athe%2520literature%2520at%2520this%2520scale.%2520Specifically%252C%2520this%2520work%2520investigates%2520i%2529%2520the%250Aeffect%2520of%2520using%2520active%2520or%2520passive%2520degeneracy%2520mitigation%2520methods%2520for%2520the%2520problem%250Aof%2520ill-conditioned%2520ICP%2520in%2520LiDAR%2520degenerate%2520environments%252C%2520ii%2529%2520the%2520evaluation%2520of%250ATSVD%252C%2520inequality%2520constraints%252C%2520and%2520linear/non-linear%2520Tikhonov%2520regularization%2520for%250Athe%2520application%2520of%2520degenerate%2520point%2520cloud%2520registration%2520for%2520the%2520first%2520time.%250AFurthermore%252C%2520a%2520sensitivity%2520analysis%2520for%2520least-squares%2520minimization%2520step%2520of%2520the%250AICP%2520problem%2520is%2520carried%2520out%2520to%2520better%2520understand%2520how%2520each%2520method%2520affects%2520the%250Aoptimization%2520and%2520what%2520to%2520expect%2520from%2520each%2520method.%2520The%2520results%2520of%2520the%2520analysis%250Aare%2520validated%2520through%2520multiple%2520real-world%2520robotic%2520field%2520and%2520simulated%250Aexperiments.%2520The%2520analysis%2520demonstrates%2520that%2520active%2520optimization%2520degeneracy%250Amitigation%2520is%2520necessary%2520and%2520advantageous%2520in%2520the%2520absence%2520of%2520reliable%2520external%250Aestimate%2520assistance%2520for%2520LiDAR-SLAM%252C%2520and%2520soft-constrained%2520methods%2520can%2520provide%250Abetter%2520results%2520in%2520complex%2520ill-conditioned%2520scenarios%2520with%2520heuristic%2520fine-tuned%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11809v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild&entry.906535625=Turcan%20Tuna%20and%20Julian%20Nubert%20and%20Patrick%20Pfreundschuh%20and%20Cesar%20Cadena%20and%20Shehryar%20Khattak%20and%20Marco%20Hutter&entry.1292438233=%20%20The%20ICP%20registration%20algorithm%20has%20been%20a%20preferred%20method%20for%20LiDAR-based%0Arobot%20localization%20for%20nearly%20a%20decade.%20However%2C%20even%20in%20modern%20SLAM%20solutions%2C%0AICP%20can%20degrade%20and%20become%20unreliable%20in%20geometrically%20ill-conditioned%0Aenvironments.%20Current%20solutions%20primarily%20focus%20on%20utilizing%20additional%20sources%0Aof%20information%2C%20such%20as%20external%20odometry%2C%20to%20either%20replace%20the%20degenerate%0Adirections%20of%20the%20optimization%20solution%20or%20add%20additional%20constraints%20in%20a%0Asensor-fusion%20setup%20afterward.%0A%20%20In%20response%2C%20this%20work%20investigates%20and%20compares%20new%20and%20existing%20degeneracy%0Amitigation%20methods%20for%20robust%20LiDAR-based%20localization%20and%20analyzes%20the%0Aefficacy%20of%20these%20approaches%20in%20degenerate%20environments%20for%20the%20first%20time%20in%0Athe%20literature%20at%20this%20scale.%20Specifically%2C%20this%20work%20investigates%20i%29%20the%0Aeffect%20of%20using%20active%20or%20passive%20degeneracy%20mitigation%20methods%20for%20the%20problem%0Aof%20ill-conditioned%20ICP%20in%20LiDAR%20degenerate%20environments%2C%20ii%29%20the%20evaluation%20of%0ATSVD%2C%20inequality%20constraints%2C%20and%20linear/non-linear%20Tikhonov%20regularization%20for%0Athe%20application%20of%20degenerate%20point%20cloud%20registration%20for%20the%20first%20time.%0AFurthermore%2C%20a%20sensitivity%20analysis%20for%20least-squares%20minimization%20step%20of%20the%0AICP%20problem%20is%20carried%20out%20to%20better%20understand%20how%20each%20method%20affects%20the%0Aoptimization%20and%20what%20to%20expect%20from%20each%20method.%20The%20results%20of%20the%20analysis%0Aare%20validated%20through%20multiple%20real-world%20robotic%20field%20and%20simulated%0Aexperiments.%20The%20analysis%20demonstrates%20that%20active%20optimization%20degeneracy%0Amitigation%20is%20necessary%20and%20advantageous%20in%20the%20absence%20of%20reliable%20external%0Aestimate%20assistance%20for%20LiDAR-SLAM%2C%20and%20soft-constrained%20methods%20can%20provide%0Abetter%20results%20in%20complex%20ill-conditioned%20scenarios%20with%20heuristic%20fine-tuned%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11809v3&entry.124074799=Read"},
{"title": "Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign\n  Language Translation", "author": "Ozge Mercanoglu Sincan and Richard Bowden", "abstract": "  Sign Language Translation (SLT) aims to convert sign language videos into\nspoken or written text. While early systems relied on gloss annotations as an\nintermediate supervision, such annotations are costly to obtain and often fail\nto capture the full complexity of continuous signing. In this work, we propose\na two-phase, dual visual encoder framework for gloss-free SLT, leveraging\ncontrastive visual-language pretraining. During pretraining, our approach\nemploys two complementary visual backbones whose outputs are jointly aligned\nwith each other and with sentence-level text embeddings via a contrastive\nobjective. During the downstream SLT task, we fuse the visual features and\ninput them into an encoder-decoder model. On the Phoenix-2014T benchmark, our\ndual encoder architecture consistently outperforms its single stream variants\nand achieves the highest BLEU-4 score among existing gloss-free SLT approaches.\n", "link": "http://arxiv.org/abs/2507.10306v1", "date": "2025-07-14", "relevancy": 2.7735, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Pretraining%20with%20Dual%20Visual%20Encoders%20for%20Gloss-Free%20Sign%0A%20%20Language%20Translation&body=Title%3A%20Contrastive%20Pretraining%20with%20Dual%20Visual%20Encoders%20for%20Gloss-Free%20Sign%0A%20%20Language%20Translation%0AAuthor%3A%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Sign%20Language%20Translation%20%28SLT%29%20aims%20to%20convert%20sign%20language%20videos%20into%0Aspoken%20or%20written%20text.%20While%20early%20systems%20relied%20on%20gloss%20annotations%20as%20an%0Aintermediate%20supervision%2C%20such%20annotations%20are%20costly%20to%20obtain%20and%20often%20fail%0Ato%20capture%20the%20full%20complexity%20of%20continuous%20signing.%20In%20this%20work%2C%20we%20propose%0Aa%20two-phase%2C%20dual%20visual%20encoder%20framework%20for%20gloss-free%20SLT%2C%20leveraging%0Acontrastive%20visual-language%20pretraining.%20During%20pretraining%2C%20our%20approach%0Aemploys%20two%20complementary%20visual%20backbones%20whose%20outputs%20are%20jointly%20aligned%0Awith%20each%20other%20and%20with%20sentence-level%20text%20embeddings%20via%20a%20contrastive%0Aobjective.%20During%20the%20downstream%20SLT%20task%2C%20we%20fuse%20the%20visual%20features%20and%0Ainput%20them%20into%20an%20encoder-decoder%20model.%20On%20the%20Phoenix-2014T%20benchmark%2C%20our%0Adual%20encoder%20architecture%20consistently%20outperforms%20its%20single%20stream%20variants%0Aand%20achieves%20the%20highest%20BLEU-4%20score%20among%20existing%20gloss-free%20SLT%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Pretraining%2520with%2520Dual%2520Visual%2520Encoders%2520for%2520Gloss-Free%2520Sign%250A%2520%2520Language%2520Translation%26entry.906535625%3DOzge%2520Mercanoglu%2520Sincan%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%2520aims%2520to%2520convert%2520sign%2520language%2520videos%2520into%250Aspoken%2520or%2520written%2520text.%2520While%2520early%2520systems%2520relied%2520on%2520gloss%2520annotations%2520as%2520an%250Aintermediate%2520supervision%252C%2520such%2520annotations%2520are%2520costly%2520to%2520obtain%2520and%2520often%2520fail%250Ato%2520capture%2520the%2520full%2520complexity%2520of%2520continuous%2520signing.%2520In%2520this%2520work%252C%2520we%2520propose%250Aa%2520two-phase%252C%2520dual%2520visual%2520encoder%2520framework%2520for%2520gloss-free%2520SLT%252C%2520leveraging%250Acontrastive%2520visual-language%2520pretraining.%2520During%2520pretraining%252C%2520our%2520approach%250Aemploys%2520two%2520complementary%2520visual%2520backbones%2520whose%2520outputs%2520are%2520jointly%2520aligned%250Awith%2520each%2520other%2520and%2520with%2520sentence-level%2520text%2520embeddings%2520via%2520a%2520contrastive%250Aobjective.%2520During%2520the%2520downstream%2520SLT%2520task%252C%2520we%2520fuse%2520the%2520visual%2520features%2520and%250Ainput%2520them%2520into%2520an%2520encoder-decoder%2520model.%2520On%2520the%2520Phoenix-2014T%2520benchmark%252C%2520our%250Adual%2520encoder%2520architecture%2520consistently%2520outperforms%2520its%2520single%2520stream%2520variants%250Aand%2520achieves%2520the%2520highest%2520BLEU-4%2520score%2520among%2520existing%2520gloss-free%2520SLT%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Pretraining%20with%20Dual%20Visual%20Encoders%20for%20Gloss-Free%20Sign%0A%20%20Language%20Translation&entry.906535625=Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden&entry.1292438233=%20%20Sign%20Language%20Translation%20%28SLT%29%20aims%20to%20convert%20sign%20language%20videos%20into%0Aspoken%20or%20written%20text.%20While%20early%20systems%20relied%20on%20gloss%20annotations%20as%20an%0Aintermediate%20supervision%2C%20such%20annotations%20are%20costly%20to%20obtain%20and%20often%20fail%0Ato%20capture%20the%20full%20complexity%20of%20continuous%20signing.%20In%20this%20work%2C%20we%20propose%0Aa%20two-phase%2C%20dual%20visual%20encoder%20framework%20for%20gloss-free%20SLT%2C%20leveraging%0Acontrastive%20visual-language%20pretraining.%20During%20pretraining%2C%20our%20approach%0Aemploys%20two%20complementary%20visual%20backbones%20whose%20outputs%20are%20jointly%20aligned%0Awith%20each%20other%20and%20with%20sentence-level%20text%20embeddings%20via%20a%20contrastive%0Aobjective.%20During%20the%20downstream%20SLT%20task%2C%20we%20fuse%20the%20visual%20features%20and%0Ainput%20them%20into%20an%20encoder-decoder%20model.%20On%20the%20Phoenix-2014T%20benchmark%2C%20our%0Adual%20encoder%20architecture%20consistently%20outperforms%20its%20single%20stream%20variants%0Aand%20achieves%20the%20highest%20BLEU-4%20score%20among%20existing%20gloss-free%20SLT%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10306v1&entry.124074799=Read"},
{"title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding\n  Space", "author": "David G. Shatwell and Ishan Rajendrakumar Dave and Sirnam Swetha and Mubarak Shah", "abstract": "  Timestamp prediction aims to determine when an image was captured using only\nvisual information, supporting applications such as metadata correction,\nretrieval, and digital forensics. In outdoor scenarios, hourly estimates rely\non cues like brightness, hue, and shadow positioning, while seasonal changes\nand weather inform date estimation. However, these visual cues significantly\ndepend on geographic context, closely linking timestamp prediction to\ngeo-localization. To address this interdependence, we introduce GT-Loc, a novel\nretrieval-based method that jointly predicts the capture time (hour and month)\nand geo-location (GPS coordinates) of an image. Our approach employs separate\nencoders for images, time, and location, aligning their embeddings within a\nshared high-dimensional feature space. Recognizing the cyclical nature of time,\ninstead of conventional contrastive learning with hard positives and negatives,\nwe propose a temporal metric-learning objective providing soft targets by\nmodeling pairwise time differences over a cyclical toroidal surface. We present\nnew benchmarks demonstrating that our joint optimization surpasses previous\ntime prediction methods, even those using the ground-truth geo-location as an\ninput during inference. Additionally, our approach achieves competitive results\non standard geo-localization tasks, and the unified embedding space facilitates\ncompositional and text-based image retrieval.\n", "link": "http://arxiv.org/abs/2507.10473v1", "date": "2025-07-14", "relevancy": 2.7507, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5673}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5482}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GT-Loc%3A%20Unifying%20When%20and%20Where%20in%20Images%20Through%20a%20Joint%20Embedding%0A%20%20Space&body=Title%3A%20GT-Loc%3A%20Unifying%20When%20and%20Where%20in%20Images%20Through%20a%20Joint%20Embedding%0A%20%20Space%0AAuthor%3A%20David%20G.%20Shatwell%20and%20Ishan%20Rajendrakumar%20Dave%20and%20Sirnam%20Swetha%20and%20Mubarak%20Shah%0AAbstract%3A%20%20%20Timestamp%20prediction%20aims%20to%20determine%20when%20an%20image%20was%20captured%20using%20only%0Avisual%20information%2C%20supporting%20applications%20such%20as%20metadata%20correction%2C%0Aretrieval%2C%20and%20digital%20forensics.%20In%20outdoor%20scenarios%2C%20hourly%20estimates%20rely%0Aon%20cues%20like%20brightness%2C%20hue%2C%20and%20shadow%20positioning%2C%20while%20seasonal%20changes%0Aand%20weather%20inform%20date%20estimation.%20However%2C%20these%20visual%20cues%20significantly%0Adepend%20on%20geographic%20context%2C%20closely%20linking%20timestamp%20prediction%20to%0Ageo-localization.%20To%20address%20this%20interdependence%2C%20we%20introduce%20GT-Loc%2C%20a%20novel%0Aretrieval-based%20method%20that%20jointly%20predicts%20the%20capture%20time%20%28hour%20and%20month%29%0Aand%20geo-location%20%28GPS%20coordinates%29%20of%20an%20image.%20Our%20approach%20employs%20separate%0Aencoders%20for%20images%2C%20time%2C%20and%20location%2C%20aligning%20their%20embeddings%20within%20a%0Ashared%20high-dimensional%20feature%20space.%20Recognizing%20the%20cyclical%20nature%20of%20time%2C%0Ainstead%20of%20conventional%20contrastive%20learning%20with%20hard%20positives%20and%20negatives%2C%0Awe%20propose%20a%20temporal%20metric-learning%20objective%20providing%20soft%20targets%20by%0Amodeling%20pairwise%20time%20differences%20over%20a%20cyclical%20toroidal%20surface.%20We%20present%0Anew%20benchmarks%20demonstrating%20that%20our%20joint%20optimization%20surpasses%20previous%0Atime%20prediction%20methods%2C%20even%20those%20using%20the%20ground-truth%20geo-location%20as%20an%0Ainput%20during%20inference.%20Additionally%2C%20our%20approach%20achieves%20competitive%20results%0Aon%20standard%20geo-localization%20tasks%2C%20and%20the%20unified%20embedding%20space%20facilitates%0Acompositional%20and%20text-based%20image%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGT-Loc%253A%2520Unifying%2520When%2520and%2520Where%2520in%2520Images%2520Through%2520a%2520Joint%2520Embedding%250A%2520%2520Space%26entry.906535625%3DDavid%2520G.%2520Shatwell%2520and%2520Ishan%2520Rajendrakumar%2520Dave%2520and%2520Sirnam%2520Swetha%2520and%2520Mubarak%2520Shah%26entry.1292438233%3D%2520%2520Timestamp%2520prediction%2520aims%2520to%2520determine%2520when%2520an%2520image%2520was%2520captured%2520using%2520only%250Avisual%2520information%252C%2520supporting%2520applications%2520such%2520as%2520metadata%2520correction%252C%250Aretrieval%252C%2520and%2520digital%2520forensics.%2520In%2520outdoor%2520scenarios%252C%2520hourly%2520estimates%2520rely%250Aon%2520cues%2520like%2520brightness%252C%2520hue%252C%2520and%2520shadow%2520positioning%252C%2520while%2520seasonal%2520changes%250Aand%2520weather%2520inform%2520date%2520estimation.%2520However%252C%2520these%2520visual%2520cues%2520significantly%250Adepend%2520on%2520geographic%2520context%252C%2520closely%2520linking%2520timestamp%2520prediction%2520to%250Ageo-localization.%2520To%2520address%2520this%2520interdependence%252C%2520we%2520introduce%2520GT-Loc%252C%2520a%2520novel%250Aretrieval-based%2520method%2520that%2520jointly%2520predicts%2520the%2520capture%2520time%2520%2528hour%2520and%2520month%2529%250Aand%2520geo-location%2520%2528GPS%2520coordinates%2529%2520of%2520an%2520image.%2520Our%2520approach%2520employs%2520separate%250Aencoders%2520for%2520images%252C%2520time%252C%2520and%2520location%252C%2520aligning%2520their%2520embeddings%2520within%2520a%250Ashared%2520high-dimensional%2520feature%2520space.%2520Recognizing%2520the%2520cyclical%2520nature%2520of%2520time%252C%250Ainstead%2520of%2520conventional%2520contrastive%2520learning%2520with%2520hard%2520positives%2520and%2520negatives%252C%250Awe%2520propose%2520a%2520temporal%2520metric-learning%2520objective%2520providing%2520soft%2520targets%2520by%250Amodeling%2520pairwise%2520time%2520differences%2520over%2520a%2520cyclical%2520toroidal%2520surface.%2520We%2520present%250Anew%2520benchmarks%2520demonstrating%2520that%2520our%2520joint%2520optimization%2520surpasses%2520previous%250Atime%2520prediction%2520methods%252C%2520even%2520those%2520using%2520the%2520ground-truth%2520geo-location%2520as%2520an%250Ainput%2520during%2520inference.%2520Additionally%252C%2520our%2520approach%2520achieves%2520competitive%2520results%250Aon%2520standard%2520geo-localization%2520tasks%252C%2520and%2520the%2520unified%2520embedding%2520space%2520facilitates%250Acompositional%2520and%2520text-based%2520image%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GT-Loc%3A%20Unifying%20When%20and%20Where%20in%20Images%20Through%20a%20Joint%20Embedding%0A%20%20Space&entry.906535625=David%20G.%20Shatwell%20and%20Ishan%20Rajendrakumar%20Dave%20and%20Sirnam%20Swetha%20and%20Mubarak%20Shah&entry.1292438233=%20%20Timestamp%20prediction%20aims%20to%20determine%20when%20an%20image%20was%20captured%20using%20only%0Avisual%20information%2C%20supporting%20applications%20such%20as%20metadata%20correction%2C%0Aretrieval%2C%20and%20digital%20forensics.%20In%20outdoor%20scenarios%2C%20hourly%20estimates%20rely%0Aon%20cues%20like%20brightness%2C%20hue%2C%20and%20shadow%20positioning%2C%20while%20seasonal%20changes%0Aand%20weather%20inform%20date%20estimation.%20However%2C%20these%20visual%20cues%20significantly%0Adepend%20on%20geographic%20context%2C%20closely%20linking%20timestamp%20prediction%20to%0Ageo-localization.%20To%20address%20this%20interdependence%2C%20we%20introduce%20GT-Loc%2C%20a%20novel%0Aretrieval-based%20method%20that%20jointly%20predicts%20the%20capture%20time%20%28hour%20and%20month%29%0Aand%20geo-location%20%28GPS%20coordinates%29%20of%20an%20image.%20Our%20approach%20employs%20separate%0Aencoders%20for%20images%2C%20time%2C%20and%20location%2C%20aligning%20their%20embeddings%20within%20a%0Ashared%20high-dimensional%20feature%20space.%20Recognizing%20the%20cyclical%20nature%20of%20time%2C%0Ainstead%20of%20conventional%20contrastive%20learning%20with%20hard%20positives%20and%20negatives%2C%0Awe%20propose%20a%20temporal%20metric-learning%20objective%20providing%20soft%20targets%20by%0Amodeling%20pairwise%20time%20differences%20over%20a%20cyclical%20toroidal%20surface.%20We%20present%0Anew%20benchmarks%20demonstrating%20that%20our%20joint%20optimization%20surpasses%20previous%0Atime%20prediction%20methods%2C%20even%20those%20using%20the%20ground-truth%20geo-location%20as%20an%0Ainput%20during%20inference.%20Additionally%2C%20our%20approach%20achieves%20competitive%20results%0Aon%20standard%20geo-localization%20tasks%2C%20and%20the%20unified%20embedding%20space%20facilitates%0Acompositional%20and%20text-based%20image%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10473v1&entry.124074799=Read"},
{"title": "RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust\n  Keypoint Extraction", "author": "Johannes K\u00fcnzel and Anna Hilsmann and Peter Eisert", "abstract": "  We introduce RIPE, an innovative reinforcement learning-based framework for\nweakly-supervised training of a keypoint extractor that excels in both\ndetection and description tasks. In contrast to conventional training regimes\nthat depend heavily on artificial transformations, pre-generated models, or 3D\ndata, RIPE requires only a binary label indicating whether paired images\nrepresent the same scene. This minimal supervision significantly expands the\npool of training data, enabling the creation of a highly generalized and robust\nkeypoint extractor.\n  RIPE utilizes the encoder's intermediate layers for the description of the\nkeypoints with a hyper-column approach to integrate information from different\nscales. Additionally, we propose an auxiliary loss to enhance the\ndiscriminative capability of the learned descriptors.\n  Comprehensive evaluations on standard benchmarks demonstrate that RIPE\nsimplifies data preparation while achieving competitive performance compared to\nstate-of-the-art techniques, marking a significant advancement in robust\nkeypoint extraction and description. To support further research, we have made\nour code publicly available at https://github.com/fraunhoferhhi/RIPE.\n", "link": "http://arxiv.org/abs/2507.04839v2", "date": "2025-07-14", "relevancy": 2.7308, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.576}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.541}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIPE%3A%20Reinforcement%20Learning%20on%20Unlabeled%20Image%20Pairs%20for%20Robust%0A%20%20Keypoint%20Extraction&body=Title%3A%20RIPE%3A%20Reinforcement%20Learning%20on%20Unlabeled%20Image%20Pairs%20for%20Robust%0A%20%20Keypoint%20Extraction%0AAuthor%3A%20Johannes%20K%C3%BCnzel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20%20%20We%20introduce%20RIPE%2C%20an%20innovative%20reinforcement%20learning-based%20framework%20for%0Aweakly-supervised%20training%20of%20a%20keypoint%20extractor%20that%20excels%20in%20both%0Adetection%20and%20description%20tasks.%20In%20contrast%20to%20conventional%20training%20regimes%0Athat%20depend%20heavily%20on%20artificial%20transformations%2C%20pre-generated%20models%2C%20or%203D%0Adata%2C%20RIPE%20requires%20only%20a%20binary%20label%20indicating%20whether%20paired%20images%0Arepresent%20the%20same%20scene.%20This%20minimal%20supervision%20significantly%20expands%20the%0Apool%20of%20training%20data%2C%20enabling%20the%20creation%20of%20a%20highly%20generalized%20and%20robust%0Akeypoint%20extractor.%0A%20%20RIPE%20utilizes%20the%20encoder%27s%20intermediate%20layers%20for%20the%20description%20of%20the%0Akeypoints%20with%20a%20hyper-column%20approach%20to%20integrate%20information%20from%20different%0Ascales.%20Additionally%2C%20we%20propose%20an%20auxiliary%20loss%20to%20enhance%20the%0Adiscriminative%20capability%20of%20the%20learned%20descriptors.%0A%20%20Comprehensive%20evaluations%20on%20standard%20benchmarks%20demonstrate%20that%20RIPE%0Asimplifies%20data%20preparation%20while%20achieving%20competitive%20performance%20compared%20to%0Astate-of-the-art%20techniques%2C%20marking%20a%20significant%20advancement%20in%20robust%0Akeypoint%20extraction%20and%20description.%20To%20support%20further%20research%2C%20we%20have%20made%0Aour%20code%20publicly%20available%20at%20https%3A//github.com/fraunhoferhhi/RIPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIPE%253A%2520Reinforcement%2520Learning%2520on%2520Unlabeled%2520Image%2520Pairs%2520for%2520Robust%250A%2520%2520Keypoint%2520Extraction%26entry.906535625%3DJohannes%2520K%25C3%25BCnzel%2520and%2520Anna%2520Hilsmann%2520and%2520Peter%2520Eisert%26entry.1292438233%3D%2520%2520We%2520introduce%2520RIPE%252C%2520an%2520innovative%2520reinforcement%2520learning-based%2520framework%2520for%250Aweakly-supervised%2520training%2520of%2520a%2520keypoint%2520extractor%2520that%2520excels%2520in%2520both%250Adetection%2520and%2520description%2520tasks.%2520In%2520contrast%2520to%2520conventional%2520training%2520regimes%250Athat%2520depend%2520heavily%2520on%2520artificial%2520transformations%252C%2520pre-generated%2520models%252C%2520or%25203D%250Adata%252C%2520RIPE%2520requires%2520only%2520a%2520binary%2520label%2520indicating%2520whether%2520paired%2520images%250Arepresent%2520the%2520same%2520scene.%2520This%2520minimal%2520supervision%2520significantly%2520expands%2520the%250Apool%2520of%2520training%2520data%252C%2520enabling%2520the%2520creation%2520of%2520a%2520highly%2520generalized%2520and%2520robust%250Akeypoint%2520extractor.%250A%2520%2520RIPE%2520utilizes%2520the%2520encoder%2527s%2520intermediate%2520layers%2520for%2520the%2520description%2520of%2520the%250Akeypoints%2520with%2520a%2520hyper-column%2520approach%2520to%2520integrate%2520information%2520from%2520different%250Ascales.%2520Additionally%252C%2520we%2520propose%2520an%2520auxiliary%2520loss%2520to%2520enhance%2520the%250Adiscriminative%2520capability%2520of%2520the%2520learned%2520descriptors.%250A%2520%2520Comprehensive%2520evaluations%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520RIPE%250Asimplifies%2520data%2520preparation%2520while%2520achieving%2520competitive%2520performance%2520compared%2520to%250Astate-of-the-art%2520techniques%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520robust%250Akeypoint%2520extraction%2520and%2520description.%2520To%2520support%2520further%2520research%252C%2520we%2520have%2520made%250Aour%2520code%2520publicly%2520available%2520at%2520https%253A//github.com/fraunhoferhhi/RIPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIPE%3A%20Reinforcement%20Learning%20on%20Unlabeled%20Image%20Pairs%20for%20Robust%0A%20%20Keypoint%20Extraction&entry.906535625=Johannes%20K%C3%BCnzel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=%20%20We%20introduce%20RIPE%2C%20an%20innovative%20reinforcement%20learning-based%20framework%20for%0Aweakly-supervised%20training%20of%20a%20keypoint%20extractor%20that%20excels%20in%20both%0Adetection%20and%20description%20tasks.%20In%20contrast%20to%20conventional%20training%20regimes%0Athat%20depend%20heavily%20on%20artificial%20transformations%2C%20pre-generated%20models%2C%20or%203D%0Adata%2C%20RIPE%20requires%20only%20a%20binary%20label%20indicating%20whether%20paired%20images%0Arepresent%20the%20same%20scene.%20This%20minimal%20supervision%20significantly%20expands%20the%0Apool%20of%20training%20data%2C%20enabling%20the%20creation%20of%20a%20highly%20generalized%20and%20robust%0Akeypoint%20extractor.%0A%20%20RIPE%20utilizes%20the%20encoder%27s%20intermediate%20layers%20for%20the%20description%20of%20the%0Akeypoints%20with%20a%20hyper-column%20approach%20to%20integrate%20information%20from%20different%0Ascales.%20Additionally%2C%20we%20propose%20an%20auxiliary%20loss%20to%20enhance%20the%0Adiscriminative%20capability%20of%20the%20learned%20descriptors.%0A%20%20Comprehensive%20evaluations%20on%20standard%20benchmarks%20demonstrate%20that%20RIPE%0Asimplifies%20data%20preparation%20while%20achieving%20competitive%20performance%20compared%20to%0Astate-of-the-art%20techniques%2C%20marking%20a%20significant%20advancement%20in%20robust%0Akeypoint%20extraction%20and%20description.%20To%20support%20further%20research%2C%20we%20have%20made%0Aour%20code%20publicly%20available%20at%20https%3A//github.com/fraunhoferhhi/RIPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04839v2&entry.124074799=Read"},
{"title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment", "author": "Qiang Li and Qingsen Yan and Haojian Huang and Peng Wu and Haokui Zhang and Yanning Zhang", "abstract": "  With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.\n", "link": "http://arxiv.org/abs/2507.10432v1", "date": "2025-07-14", "relevancy": 2.7174, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5562}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5425}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment&body=Title%3A%20Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment%0AAuthor%3A%20Qiang%20Li%20and%20Qingsen%20Yan%20and%20Haojian%20Huang%20and%20Peng%20Wu%20and%20Haokui%20Zhang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20With%20the%20rapid%20advancements%20in%20Artificial%20Intelligence%20Generated%20Image%20%28AGI%29%0Atechnology%2C%20the%20accurate%20assessment%20of%20their%20quality%20has%20become%20an%20increasingly%0Avital%20requirement.%20Prevailing%20methods%20typically%20rely%20on%20cross-modal%20models%20like%0ACLIP%20or%20BLIP%20to%20evaluate%20text-image%20alignment%20and%20visual%20quality.%20However%2C%20when%0Aapplied%20to%20AGIs%2C%20these%20methods%20encounter%20two%20primary%20challenges%3A%20semantic%0Amisalignment%20and%20details%20perception%20missing.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment%0A%28SC-AGIQA%29%2C%20a%20unified%20framework%20that%20leverages%20text-visual%20semantic%20constraints%0Ato%20significantly%20enhance%20the%20comprehensive%20evaluation%20of%20both%20text-image%0Aconsistency%20and%20perceptual%20distortion%20in%20AI-generated%20images.%20Our%20approach%0Aintegrates%20key%20capabilities%20from%20multiple%20models%20and%20tackles%20the%20aforementioned%0Achallenges%20by%20introducing%20two%20core%20modules%3A%20the%20Text-assisted%20Semantic%0AAlignment%20Module%20%28TSAM%29%2C%20which%20leverages%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20to%20bridge%20the%20semantic%20gap%20by%20generating%20an%20image%20description%20and%0Acomparing%20it%20against%20the%20original%20prompt%20for%20a%20refined%20consistency%20check%2C%20and%0Athe%20Frequency-domain%20Fine-Grained%20Degradation%20Perception%20Module%20%28FFDPM%29%2C%20which%0Adraws%20inspiration%20from%20Human%20Visual%20System%20%28HVS%29%20properties%20by%20employing%0Afrequency%20domain%20analysis%20combined%20with%20perceptual%20sensitivity%20weighting%20to%0Abetter%20quantify%20subtle%20visual%20distortions%20and%20enhance%20the%20capture%20of%0Afine-grained%20visual%20quality%20details%20in%20images.%20Extensive%20experiments%20conducted%0Aon%20multiple%20benchmark%20datasets%20demonstrate%20that%20SC-AGIQA%20outperforms%20existing%0Astate-of-the-art%20methods.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/mozhu1/SC-AGIQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Visual%2520Semantic%2520Constrained%2520AI-Generated%2520Image%2520Quality%2520Assessment%26entry.906535625%3DQiang%2520Li%2520and%2520Qingsen%2520Yan%2520and%2520Haojian%2520Huang%2520and%2520Peng%2520Wu%2520and%2520Haokui%2520Zhang%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancements%2520in%2520Artificial%2520Intelligence%2520Generated%2520Image%2520%2528AGI%2529%250Atechnology%252C%2520the%2520accurate%2520assessment%2520of%2520their%2520quality%2520has%2520become%2520an%2520increasingly%250Avital%2520requirement.%2520Prevailing%2520methods%2520typically%2520rely%2520on%2520cross-modal%2520models%2520like%250ACLIP%2520or%2520BLIP%2520to%2520evaluate%2520text-image%2520alignment%2520and%2520visual%2520quality.%2520However%252C%2520when%250Aapplied%2520to%2520AGIs%252C%2520these%2520methods%2520encounter%2520two%2520primary%2520challenges%253A%2520semantic%250Amisalignment%2520and%2520details%2520perception%2520missing.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520Text-Visual%2520Semantic%2520Constrained%2520AI-Generated%2520Image%2520Quality%2520Assessment%250A%2528SC-AGIQA%2529%252C%2520a%2520unified%2520framework%2520that%2520leverages%2520text-visual%2520semantic%2520constraints%250Ato%2520significantly%2520enhance%2520the%2520comprehensive%2520evaluation%2520of%2520both%2520text-image%250Aconsistency%2520and%2520perceptual%2520distortion%2520in%2520AI-generated%2520images.%2520Our%2520approach%250Aintegrates%2520key%2520capabilities%2520from%2520multiple%2520models%2520and%2520tackles%2520the%2520aforementioned%250Achallenges%2520by%2520introducing%2520two%2520core%2520modules%253A%2520the%2520Text-assisted%2520Semantic%250AAlignment%2520Module%2520%2528TSAM%2529%252C%2520which%2520leverages%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520to%2520bridge%2520the%2520semantic%2520gap%2520by%2520generating%2520an%2520image%2520description%2520and%250Acomparing%2520it%2520against%2520the%2520original%2520prompt%2520for%2520a%2520refined%2520consistency%2520check%252C%2520and%250Athe%2520Frequency-domain%2520Fine-Grained%2520Degradation%2520Perception%2520Module%2520%2528FFDPM%2529%252C%2520which%250Adraws%2520inspiration%2520from%2520Human%2520Visual%2520System%2520%2528HVS%2529%2520properties%2520by%2520employing%250Afrequency%2520domain%2520analysis%2520combined%2520with%2520perceptual%2520sensitivity%2520weighting%2520to%250Abetter%2520quantify%2520subtle%2520visual%2520distortions%2520and%2520enhance%2520the%2520capture%2520of%250Afine-grained%2520visual%2520quality%2520details%2520in%2520images.%2520Extensive%2520experiments%2520conducted%250Aon%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520that%2520SC-AGIQA%2520outperforms%2520existing%250Astate-of-the-art%2520methods.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/mozhu1/SC-AGIQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment&entry.906535625=Qiang%20Li%20and%20Qingsen%20Yan%20and%20Haojian%20Huang%20and%20Peng%20Wu%20and%20Haokui%20Zhang%20and%20Yanning%20Zhang&entry.1292438233=%20%20With%20the%20rapid%20advancements%20in%20Artificial%20Intelligence%20Generated%20Image%20%28AGI%29%0Atechnology%2C%20the%20accurate%20assessment%20of%20their%20quality%20has%20become%20an%20increasingly%0Avital%20requirement.%20Prevailing%20methods%20typically%20rely%20on%20cross-modal%20models%20like%0ACLIP%20or%20BLIP%20to%20evaluate%20text-image%20alignment%20and%20visual%20quality.%20However%2C%20when%0Aapplied%20to%20AGIs%2C%20these%20methods%20encounter%20two%20primary%20challenges%3A%20semantic%0Amisalignment%20and%20details%20perception%20missing.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment%0A%28SC-AGIQA%29%2C%20a%20unified%20framework%20that%20leverages%20text-visual%20semantic%20constraints%0Ato%20significantly%20enhance%20the%20comprehensive%20evaluation%20of%20both%20text-image%0Aconsistency%20and%20perceptual%20distortion%20in%20AI-generated%20images.%20Our%20approach%0Aintegrates%20key%20capabilities%20from%20multiple%20models%20and%20tackles%20the%20aforementioned%0Achallenges%20by%20introducing%20two%20core%20modules%3A%20the%20Text-assisted%20Semantic%0AAlignment%20Module%20%28TSAM%29%2C%20which%20leverages%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20to%20bridge%20the%20semantic%20gap%20by%20generating%20an%20image%20description%20and%0Acomparing%20it%20against%20the%20original%20prompt%20for%20a%20refined%20consistency%20check%2C%20and%0Athe%20Frequency-domain%20Fine-Grained%20Degradation%20Perception%20Module%20%28FFDPM%29%2C%20which%0Adraws%20inspiration%20from%20Human%20Visual%20System%20%28HVS%29%20properties%20by%20employing%0Afrequency%20domain%20analysis%20combined%20with%20perceptual%20sensitivity%20weighting%20to%0Abetter%20quantify%20subtle%20visual%20distortions%20and%20enhance%20the%20capture%20of%0Afine-grained%20visual%20quality%20details%20in%20images.%20Extensive%20experiments%20conducted%0Aon%20multiple%20benchmark%20datasets%20demonstrate%20that%20SC-AGIQA%20outperforms%20existing%0Astate-of-the-art%20methods.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/mozhu1/SC-AGIQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10432v1&entry.124074799=Read"},
{"title": "MGA-Net: A Novel Mask-Guided Attention Neural Network for Precision\n  Neonatal Brain Imaging", "author": "Bahram Jafrasteh and Simon Pedro Lubian-Lopez and Emiliano Trimarco and Macarena Roman Ruiz and Carmen Rodriguez Barrios and Yolanda Marin Almagro and Isabel Benavente-Fernandez", "abstract": "  In this study, we introduce MGA-Net, a novel mask-guided attention neural\nnetwork, which extends the U-net model for precision neonatal brain imaging.\nMGA-Net is designed to extract the brain from other structures and reconstruct\nhigh-quality brain images. The network employs a common encoder and two\ndecoders: one for brain mask extraction and the other for brain region\nreconstruction. A key feature of MGA-Net is its high-level mask-guided\nattention module, which leverages features from the brain mask decoder to\nenhance image reconstruction. To enable the same encoder and decoder to process\nboth MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional\nencoding. This encoding assigns distinct positional values to MRI and US\nimages, allowing the model to effectively learn from both modalities.\nConsequently, features learned from a single modality can aid in learning a\nmodality with less available data, such as US. We extensively validated the\nproposed MGA-Net on diverse and independent datasets from varied clinical\nsettings and neonatal age groups. The metrics used for assessment included the\nDICE similarity coefficient, recall, and accuracy for image segmentation;\nstructural similarity for image reconstruction; and root mean squared error for\ntotal brain volume estimation from 3D ultrasound images. Our results\ndemonstrate that MGA-Net significantly outperforms traditional methods,\noffering superior performance in brain extraction and segmentation while\nachieving high precision in image reconstruction and volumetric analysis. Thus,\nMGA-Net represents a robust and effective preprocessing tool for MRI and 3D\nultrasound images, marking a significant advance in neuroimaging that enhances\nboth research and clinical diagnostics in the neonatal period and beyond.Our\ncode is available at https://github.com/BahramJafrasteh/MGA-Net\n", "link": "http://arxiv.org/abs/2406.17709v3", "date": "2025-07-14", "relevancy": 2.6991, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5422}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.542}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGA-Net%3A%20A%20Novel%20Mask-Guided%20Attention%20Neural%20Network%20for%20Precision%0A%20%20Neonatal%20Brain%20Imaging&body=Title%3A%20MGA-Net%3A%20A%20Novel%20Mask-Guided%20Attention%20Neural%20Network%20for%20Precision%0A%20%20Neonatal%20Brain%20Imaging%0AAuthor%3A%20Bahram%20Jafrasteh%20and%20Simon%20Pedro%20Lubian-Lopez%20and%20Emiliano%20Trimarco%20and%20Macarena%20Roman%20Ruiz%20and%20Carmen%20Rodriguez%20Barrios%20and%20Yolanda%20Marin%20Almagro%20and%20Isabel%20Benavente-Fernandez%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20MGA-Net%2C%20a%20novel%20mask-guided%20attention%20neural%0Anetwork%2C%20which%20extends%20the%20U-net%20model%20for%20precision%20neonatal%20brain%20imaging.%0AMGA-Net%20is%20designed%20to%20extract%20the%20brain%20from%20other%20structures%20and%20reconstruct%0Ahigh-quality%20brain%20images.%20The%20network%20employs%20a%20common%20encoder%20and%20two%0Adecoders%3A%20one%20for%20brain%20mask%20extraction%20and%20the%20other%20for%20brain%20region%0Areconstruction.%20A%20key%20feature%20of%20MGA-Net%20is%20its%20high-level%20mask-guided%0Aattention%20module%2C%20which%20leverages%20features%20from%20the%20brain%20mask%20decoder%20to%0Aenhance%20image%20reconstruction.%20To%20enable%20the%20same%20encoder%20and%20decoder%20to%20process%0Aboth%20MRI%20and%20ultrasound%20%28US%29%20images%2C%20MGA-Net%20integrates%20sinusoidal%20positional%0Aencoding.%20This%20encoding%20assigns%20distinct%20positional%20values%20to%20MRI%20and%20US%0Aimages%2C%20allowing%20the%20model%20to%20effectively%20learn%20from%20both%20modalities.%0AConsequently%2C%20features%20learned%20from%20a%20single%20modality%20can%20aid%20in%20learning%20a%0Amodality%20with%20less%20available%20data%2C%20such%20as%20US.%20We%20extensively%20validated%20the%0Aproposed%20MGA-Net%20on%20diverse%20and%20independent%20datasets%20from%20varied%20clinical%0Asettings%20and%20neonatal%20age%20groups.%20The%20metrics%20used%20for%20assessment%20included%20the%0ADICE%20similarity%20coefficient%2C%20recall%2C%20and%20accuracy%20for%20image%20segmentation%3B%0Astructural%20similarity%20for%20image%20reconstruction%3B%20and%20root%20mean%20squared%20error%20for%0Atotal%20brain%20volume%20estimation%20from%203D%20ultrasound%20images.%20Our%20results%0Ademonstrate%20that%20MGA-Net%20significantly%20outperforms%20traditional%20methods%2C%0Aoffering%20superior%20performance%20in%20brain%20extraction%20and%20segmentation%20while%0Aachieving%20high%20precision%20in%20image%20reconstruction%20and%20volumetric%20analysis.%20Thus%2C%0AMGA-Net%20represents%20a%20robust%20and%20effective%20preprocessing%20tool%20for%20MRI%20and%203D%0Aultrasound%20images%2C%20marking%20a%20significant%20advance%20in%20neuroimaging%20that%20enhances%0Aboth%20research%20and%20clinical%20diagnostics%20in%20the%20neonatal%20period%20and%20beyond.Our%0Acode%20is%20available%20at%20https%3A//github.com/BahramJafrasteh/MGA-Net%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17709v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGA-Net%253A%2520A%2520Novel%2520Mask-Guided%2520Attention%2520Neural%2520Network%2520for%2520Precision%250A%2520%2520Neonatal%2520Brain%2520Imaging%26entry.906535625%3DBahram%2520Jafrasteh%2520and%2520Simon%2520Pedro%2520Lubian-Lopez%2520and%2520Emiliano%2520Trimarco%2520and%2520Macarena%2520Roman%2520Ruiz%2520and%2520Carmen%2520Rodriguez%2520Barrios%2520and%2520Yolanda%2520Marin%2520Almagro%2520and%2520Isabel%2520Benavente-Fernandez%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520MGA-Net%252C%2520a%2520novel%2520mask-guided%2520attention%2520neural%250Anetwork%252C%2520which%2520extends%2520the%2520U-net%2520model%2520for%2520precision%2520neonatal%2520brain%2520imaging.%250AMGA-Net%2520is%2520designed%2520to%2520extract%2520the%2520brain%2520from%2520other%2520structures%2520and%2520reconstruct%250Ahigh-quality%2520brain%2520images.%2520The%2520network%2520employs%2520a%2520common%2520encoder%2520and%2520two%250Adecoders%253A%2520one%2520for%2520brain%2520mask%2520extraction%2520and%2520the%2520other%2520for%2520brain%2520region%250Areconstruction.%2520A%2520key%2520feature%2520of%2520MGA-Net%2520is%2520its%2520high-level%2520mask-guided%250Aattention%2520module%252C%2520which%2520leverages%2520features%2520from%2520the%2520brain%2520mask%2520decoder%2520to%250Aenhance%2520image%2520reconstruction.%2520To%2520enable%2520the%2520same%2520encoder%2520and%2520decoder%2520to%2520process%250Aboth%2520MRI%2520and%2520ultrasound%2520%2528US%2529%2520images%252C%2520MGA-Net%2520integrates%2520sinusoidal%2520positional%250Aencoding.%2520This%2520encoding%2520assigns%2520distinct%2520positional%2520values%2520to%2520MRI%2520and%2520US%250Aimages%252C%2520allowing%2520the%2520model%2520to%2520effectively%2520learn%2520from%2520both%2520modalities.%250AConsequently%252C%2520features%2520learned%2520from%2520a%2520single%2520modality%2520can%2520aid%2520in%2520learning%2520a%250Amodality%2520with%2520less%2520available%2520data%252C%2520such%2520as%2520US.%2520We%2520extensively%2520validated%2520the%250Aproposed%2520MGA-Net%2520on%2520diverse%2520and%2520independent%2520datasets%2520from%2520varied%2520clinical%250Asettings%2520and%2520neonatal%2520age%2520groups.%2520The%2520metrics%2520used%2520for%2520assessment%2520included%2520the%250ADICE%2520similarity%2520coefficient%252C%2520recall%252C%2520and%2520accuracy%2520for%2520image%2520segmentation%253B%250Astructural%2520similarity%2520for%2520image%2520reconstruction%253B%2520and%2520root%2520mean%2520squared%2520error%2520for%250Atotal%2520brain%2520volume%2520estimation%2520from%25203D%2520ultrasound%2520images.%2520Our%2520results%250Ademonstrate%2520that%2520MGA-Net%2520significantly%2520outperforms%2520traditional%2520methods%252C%250Aoffering%2520superior%2520performance%2520in%2520brain%2520extraction%2520and%2520segmentation%2520while%250Aachieving%2520high%2520precision%2520in%2520image%2520reconstruction%2520and%2520volumetric%2520analysis.%2520Thus%252C%250AMGA-Net%2520represents%2520a%2520robust%2520and%2520effective%2520preprocessing%2520tool%2520for%2520MRI%2520and%25203D%250Aultrasound%2520images%252C%2520marking%2520a%2520significant%2520advance%2520in%2520neuroimaging%2520that%2520enhances%250Aboth%2520research%2520and%2520clinical%2520diagnostics%2520in%2520the%2520neonatal%2520period%2520and%2520beyond.Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/BahramJafrasteh/MGA-Net%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17709v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGA-Net%3A%20A%20Novel%20Mask-Guided%20Attention%20Neural%20Network%20for%20Precision%0A%20%20Neonatal%20Brain%20Imaging&entry.906535625=Bahram%20Jafrasteh%20and%20Simon%20Pedro%20Lubian-Lopez%20and%20Emiliano%20Trimarco%20and%20Macarena%20Roman%20Ruiz%20and%20Carmen%20Rodriguez%20Barrios%20and%20Yolanda%20Marin%20Almagro%20and%20Isabel%20Benavente-Fernandez&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20MGA-Net%2C%20a%20novel%20mask-guided%20attention%20neural%0Anetwork%2C%20which%20extends%20the%20U-net%20model%20for%20precision%20neonatal%20brain%20imaging.%0AMGA-Net%20is%20designed%20to%20extract%20the%20brain%20from%20other%20structures%20and%20reconstruct%0Ahigh-quality%20brain%20images.%20The%20network%20employs%20a%20common%20encoder%20and%20two%0Adecoders%3A%20one%20for%20brain%20mask%20extraction%20and%20the%20other%20for%20brain%20region%0Areconstruction.%20A%20key%20feature%20of%20MGA-Net%20is%20its%20high-level%20mask-guided%0Aattention%20module%2C%20which%20leverages%20features%20from%20the%20brain%20mask%20decoder%20to%0Aenhance%20image%20reconstruction.%20To%20enable%20the%20same%20encoder%20and%20decoder%20to%20process%0Aboth%20MRI%20and%20ultrasound%20%28US%29%20images%2C%20MGA-Net%20integrates%20sinusoidal%20positional%0Aencoding.%20This%20encoding%20assigns%20distinct%20positional%20values%20to%20MRI%20and%20US%0Aimages%2C%20allowing%20the%20model%20to%20effectively%20learn%20from%20both%20modalities.%0AConsequently%2C%20features%20learned%20from%20a%20single%20modality%20can%20aid%20in%20learning%20a%0Amodality%20with%20less%20available%20data%2C%20such%20as%20US.%20We%20extensively%20validated%20the%0Aproposed%20MGA-Net%20on%20diverse%20and%20independent%20datasets%20from%20varied%20clinical%0Asettings%20and%20neonatal%20age%20groups.%20The%20metrics%20used%20for%20assessment%20included%20the%0ADICE%20similarity%20coefficient%2C%20recall%2C%20and%20accuracy%20for%20image%20segmentation%3B%0Astructural%20similarity%20for%20image%20reconstruction%3B%20and%20root%20mean%20squared%20error%20for%0Atotal%20brain%20volume%20estimation%20from%203D%20ultrasound%20images.%20Our%20results%0Ademonstrate%20that%20MGA-Net%20significantly%20outperforms%20traditional%20methods%2C%0Aoffering%20superior%20performance%20in%20brain%20extraction%20and%20segmentation%20while%0Aachieving%20high%20precision%20in%20image%20reconstruction%20and%20volumetric%20analysis.%20Thus%2C%0AMGA-Net%20represents%20a%20robust%20and%20effective%20preprocessing%20tool%20for%20MRI%20and%203D%0Aultrasound%20images%2C%20marking%20a%20significant%20advance%20in%20neuroimaging%20that%20enhances%0Aboth%20research%20and%20clinical%20diagnostics%20in%20the%20neonatal%20period%20and%20beyond.Our%0Acode%20is%20available%20at%20https%3A//github.com/BahramJafrasteh/MGA-Net%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17709v3&entry.124074799=Read"},
{"title": "FaceLLM: A Multimodal Large Language Model for Face Understanding", "author": "Hatef Otroshi Shahreza and S\u00e9bastien Marcel", "abstract": "  Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.\n", "link": "http://arxiv.org/abs/2507.10300v1", "date": "2025-07-14", "relevancy": 2.6852, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceLLM%3A%20A%20Multimodal%20Large%20Language%20Model%20for%20Face%20Understanding&body=Title%3A%20FaceLLM%3A%20A%20Multimodal%20Large%20Language%20Model%20for%20Face%20Understanding%0AAuthor%3A%20Hatef%20Otroshi%20Shahreza%20and%20S%C3%A9bastien%20Marcel%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20remarkable%20performance%20in%0Avision-language%20tasks.%20However%2C%20existing%20MLLMs%20are%20primarily%20trained%20on%20generic%0Adatasets%2C%20limiting%20their%20ability%20to%20reason%20on%20domain-specific%20visual%20cues%20such%0Aas%20those%20in%20facial%20images.%20In%20particular%2C%20tasks%20that%20require%20detailed%0Aunderstanding%20of%20facial%20structure%2C%20expression%2C%20emotion%2C%20and%20demographic%0Afeatures%20remain%20underexplored%20by%20MLLMs%20due%20to%20the%20lack%20of%20large-scale%20annotated%0Aface%20image-text%20datasets.%20In%20this%20work%2C%20we%20introduce%20FaceLLM%2C%20a%20multimodal%0Alarge%20language%20model%20trained%20specifically%20for%20facial%20image%20understanding.%20To%0Aconstruct%20the%20training%20data%2C%20we%20propose%20a%20novel%20weakly%20supervised%20pipeline%20that%0Auses%20ChatGPT%20with%20attribute-aware%20prompts%20to%20generate%20high-quality%0Aquestion-answer%20pairs%20based%20on%20images%20from%20the%20FairFace%20dataset.%20The%20resulting%0Acorpus%2C%20called%20FairFaceGPT%2C%20covers%20a%20diverse%20set%20of%20attributes%20including%0Aexpression%2C%20pose%2C%20skin%20texture%2C%20and%20forensic%20information.%20Our%20experiments%0Ademonstrate%20that%20FaceLLM%20improves%20the%20performance%20of%20MLLMs%20on%20various%0Aface-centric%20tasks%20and%20achieves%20state-of-the-art%20performance.%20This%20work%0Ahighlights%20the%20potential%20of%20synthetic%20supervision%20via%20language%20models%20for%0Abuilding%20domain-specialized%20MLLMs%2C%20and%20sets%20a%20precedent%20for%20trustworthy%2C%0Ahuman-centric%20multimodal%20AI%20systems.%20FairFaceGPT%20dataset%20and%20pretrained%20FaceLLM%0Amodels%20are%20publicly%20available%20in%20the%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceLLM%253A%2520A%2520Multimodal%2520Large%2520Language%2520Model%2520for%2520Face%2520Understanding%26entry.906535625%3DHatef%2520Otroshi%2520Shahreza%2520and%2520S%25C3%25A9bastien%2520Marcel%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%250Avision-language%2520tasks.%2520However%252C%2520existing%2520MLLMs%2520are%2520primarily%2520trained%2520on%2520generic%250Adatasets%252C%2520limiting%2520their%2520ability%2520to%2520reason%2520on%2520domain-specific%2520visual%2520cues%2520such%250Aas%2520those%2520in%2520facial%2520images.%2520In%2520particular%252C%2520tasks%2520that%2520require%2520detailed%250Aunderstanding%2520of%2520facial%2520structure%252C%2520expression%252C%2520emotion%252C%2520and%2520demographic%250Afeatures%2520remain%2520underexplored%2520by%2520MLLMs%2520due%2520to%2520the%2520lack%2520of%2520large-scale%2520annotated%250Aface%2520image-text%2520datasets.%2520In%2520this%2520work%252C%2520we%2520introduce%2520FaceLLM%252C%2520a%2520multimodal%250Alarge%2520language%2520model%2520trained%2520specifically%2520for%2520facial%2520image%2520understanding.%2520To%250Aconstruct%2520the%2520training%2520data%252C%2520we%2520propose%2520a%2520novel%2520weakly%2520supervised%2520pipeline%2520that%250Auses%2520ChatGPT%2520with%2520attribute-aware%2520prompts%2520to%2520generate%2520high-quality%250Aquestion-answer%2520pairs%2520based%2520on%2520images%2520from%2520the%2520FairFace%2520dataset.%2520The%2520resulting%250Acorpus%252C%2520called%2520FairFaceGPT%252C%2520covers%2520a%2520diverse%2520set%2520of%2520attributes%2520including%250Aexpression%252C%2520pose%252C%2520skin%2520texture%252C%2520and%2520forensic%2520information.%2520Our%2520experiments%250Ademonstrate%2520that%2520FaceLLM%2520improves%2520the%2520performance%2520of%2520MLLMs%2520on%2520various%250Aface-centric%2520tasks%2520and%2520achieves%2520state-of-the-art%2520performance.%2520This%2520work%250Ahighlights%2520the%2520potential%2520of%2520synthetic%2520supervision%2520via%2520language%2520models%2520for%250Abuilding%2520domain-specialized%2520MLLMs%252C%2520and%2520sets%2520a%2520precedent%2520for%2520trustworthy%252C%250Ahuman-centric%2520multimodal%2520AI%2520systems.%2520FairFaceGPT%2520dataset%2520and%2520pretrained%2520FaceLLM%250Amodels%2520are%2520publicly%2520available%2520in%2520the%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceLLM%3A%20A%20Multimodal%20Large%20Language%20Model%20for%20Face%20Understanding&entry.906535625=Hatef%20Otroshi%20Shahreza%20and%20S%C3%A9bastien%20Marcel&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20remarkable%20performance%20in%0Avision-language%20tasks.%20However%2C%20existing%20MLLMs%20are%20primarily%20trained%20on%20generic%0Adatasets%2C%20limiting%20their%20ability%20to%20reason%20on%20domain-specific%20visual%20cues%20such%0Aas%20those%20in%20facial%20images.%20In%20particular%2C%20tasks%20that%20require%20detailed%0Aunderstanding%20of%20facial%20structure%2C%20expression%2C%20emotion%2C%20and%20demographic%0Afeatures%20remain%20underexplored%20by%20MLLMs%20due%20to%20the%20lack%20of%20large-scale%20annotated%0Aface%20image-text%20datasets.%20In%20this%20work%2C%20we%20introduce%20FaceLLM%2C%20a%20multimodal%0Alarge%20language%20model%20trained%20specifically%20for%20facial%20image%20understanding.%20To%0Aconstruct%20the%20training%20data%2C%20we%20propose%20a%20novel%20weakly%20supervised%20pipeline%20that%0Auses%20ChatGPT%20with%20attribute-aware%20prompts%20to%20generate%20high-quality%0Aquestion-answer%20pairs%20based%20on%20images%20from%20the%20FairFace%20dataset.%20The%20resulting%0Acorpus%2C%20called%20FairFaceGPT%2C%20covers%20a%20diverse%20set%20of%20attributes%20including%0Aexpression%2C%20pose%2C%20skin%20texture%2C%20and%20forensic%20information.%20Our%20experiments%0Ademonstrate%20that%20FaceLLM%20improves%20the%20performance%20of%20MLLMs%20on%20various%0Aface-centric%20tasks%20and%20achieves%20state-of-the-art%20performance.%20This%20work%0Ahighlights%20the%20potential%20of%20synthetic%20supervision%20via%20language%20models%20for%0Abuilding%20domain-specialized%20MLLMs%2C%20and%20sets%20a%20precedent%20for%20trustworthy%2C%0Ahuman-centric%20multimodal%20AI%20systems.%20FairFaceGPT%20dataset%20and%20pretrained%20FaceLLM%0Amodels%20are%20publicly%20available%20in%20the%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10300v1&entry.124074799=Read"},
{"title": "Minimizing the Pretraining Gap: Domain-aligned Text-Based Person\n  Retrieval", "author": "Shuyu Yang and Yaxiong Wang and Yongrui Li and Li Zhu and Zhedong Zheng", "abstract": "  In this work, we focus on text-based person retrieval, which aims to identify\nindividuals based on textual descriptions. Given the significant privacy issues\nand the high cost associated with manual annotation, synthetic data has become\na popular choice for pretraining models, leading to notable advancements.\nHowever, the considerable domain gap between synthetic pretraining datasets and\nreal-world target datasets, characterized by differences in lighting, color,\nand viewpoint, remains a critical obstacle that hinders the effectiveness of\nthe pretrain-finetune paradigm. To bridge this gap, we introduce a unified\ntext-based person retrieval pipeline considering domain adaptation at both\nimage and region levels. In particular, it contains two primary components,\ni.e., Domain-aware Diffusion (DaD) for image-level adaptation and\nMulti-granularity Relation Alignment (MRA) for region-level adaptation. As the\nname implies, Domain-aware Diffusion is to migrate the distribution of images\nfrom the pretraining dataset domain to the target real-world dataset domain,\ne.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level\nalignment by establishing correspondences between visual regions and their\ndescriptive sentences, thereby addressing disparities at a finer granularity.\nExtensive experiments show that our dual-level adaptation method has achieved\nstate-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,\noutperforming existing methodologies. The dataset, model, and code are\navailable at https://github.com/Shuyu-XJTU/MRA.\n", "link": "http://arxiv.org/abs/2507.10195v1", "date": "2025-07-14", "relevancy": 2.682, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5739}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5211}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimizing%20the%20Pretraining%20Gap%3A%20Domain-aligned%20Text-Based%20Person%0A%20%20Retrieval&body=Title%3A%20Minimizing%20the%20Pretraining%20Gap%3A%20Domain-aligned%20Text-Based%20Person%0A%20%20Retrieval%0AAuthor%3A%20Shuyu%20Yang%20and%20Yaxiong%20Wang%20and%20Yongrui%20Li%20and%20Li%20Zhu%20and%20Zhedong%20Zheng%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20focus%20on%20text-based%20person%20retrieval%2C%20which%20aims%20to%20identify%0Aindividuals%20based%20on%20textual%20descriptions.%20Given%20the%20significant%20privacy%20issues%0Aand%20the%20high%20cost%20associated%20with%20manual%20annotation%2C%20synthetic%20data%20has%20become%0Aa%20popular%20choice%20for%20pretraining%20models%2C%20leading%20to%20notable%20advancements.%0AHowever%2C%20the%20considerable%20domain%20gap%20between%20synthetic%20pretraining%20datasets%20and%0Areal-world%20target%20datasets%2C%20characterized%20by%20differences%20in%20lighting%2C%20color%2C%0Aand%20viewpoint%2C%20remains%20a%20critical%20obstacle%20that%20hinders%20the%20effectiveness%20of%0Athe%20pretrain-finetune%20paradigm.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20unified%0Atext-based%20person%20retrieval%20pipeline%20considering%20domain%20adaptation%20at%20both%0Aimage%20and%20region%20levels.%20In%20particular%2C%20it%20contains%20two%20primary%20components%2C%0Ai.e.%2C%20Domain-aware%20Diffusion%20%28DaD%29%20for%20image-level%20adaptation%20and%0AMulti-granularity%20Relation%20Alignment%20%28MRA%29%20for%20region-level%20adaptation.%20As%20the%0Aname%20implies%2C%20Domain-aware%20Diffusion%20is%20to%20migrate%20the%20distribution%20of%20images%0Afrom%20the%20pretraining%20dataset%20domain%20to%20the%20target%20real-world%20dataset%20domain%2C%0Ae.g.%2C%20CUHK-PEDES.%20Subsequently%2C%20MRA%20performs%20a%20meticulous%20region-level%0Aalignment%20by%20establishing%20correspondences%20between%20visual%20regions%20and%20their%0Adescriptive%20sentences%2C%20thereby%20addressing%20disparities%20at%20a%20finer%20granularity.%0AExtensive%20experiments%20show%20that%20our%20dual-level%20adaptation%20method%20has%20achieved%0Astate-of-the-art%20results%20on%20the%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20and%20RSTPReid%20datasets%2C%0Aoutperforming%20existing%20methodologies.%20The%20dataset%2C%20model%2C%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/Shuyu-XJTU/MRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimizing%2520the%2520Pretraining%2520Gap%253A%2520Domain-aligned%2520Text-Based%2520Person%250A%2520%2520Retrieval%26entry.906535625%3DShuyu%2520Yang%2520and%2520Yaxiong%2520Wang%2520and%2520Yongrui%2520Li%2520and%2520Li%2520Zhu%2520and%2520Zhedong%2520Zheng%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520text-based%2520person%2520retrieval%252C%2520which%2520aims%2520to%2520identify%250Aindividuals%2520based%2520on%2520textual%2520descriptions.%2520Given%2520the%2520significant%2520privacy%2520issues%250Aand%2520the%2520high%2520cost%2520associated%2520with%2520manual%2520annotation%252C%2520synthetic%2520data%2520has%2520become%250Aa%2520popular%2520choice%2520for%2520pretraining%2520models%252C%2520leading%2520to%2520notable%2520advancements.%250AHowever%252C%2520the%2520considerable%2520domain%2520gap%2520between%2520synthetic%2520pretraining%2520datasets%2520and%250Areal-world%2520target%2520datasets%252C%2520characterized%2520by%2520differences%2520in%2520lighting%252C%2520color%252C%250Aand%2520viewpoint%252C%2520remains%2520a%2520critical%2520obstacle%2520that%2520hinders%2520the%2520effectiveness%2520of%250Athe%2520pretrain-finetune%2520paradigm.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520unified%250Atext-based%2520person%2520retrieval%2520pipeline%2520considering%2520domain%2520adaptation%2520at%2520both%250Aimage%2520and%2520region%2520levels.%2520In%2520particular%252C%2520it%2520contains%2520two%2520primary%2520components%252C%250Ai.e.%252C%2520Domain-aware%2520Diffusion%2520%2528DaD%2529%2520for%2520image-level%2520adaptation%2520and%250AMulti-granularity%2520Relation%2520Alignment%2520%2528MRA%2529%2520for%2520region-level%2520adaptation.%2520As%2520the%250Aname%2520implies%252C%2520Domain-aware%2520Diffusion%2520is%2520to%2520migrate%2520the%2520distribution%2520of%2520images%250Afrom%2520the%2520pretraining%2520dataset%2520domain%2520to%2520the%2520target%2520real-world%2520dataset%2520domain%252C%250Ae.g.%252C%2520CUHK-PEDES.%2520Subsequently%252C%2520MRA%2520performs%2520a%2520meticulous%2520region-level%250Aalignment%2520by%2520establishing%2520correspondences%2520between%2520visual%2520regions%2520and%2520their%250Adescriptive%2520sentences%252C%2520thereby%2520addressing%2520disparities%2520at%2520a%2520finer%2520granularity.%250AExtensive%2520experiments%2520show%2520that%2520our%2520dual-level%2520adaptation%2520method%2520has%2520achieved%250Astate-of-the-art%2520results%2520on%2520the%2520CUHK-PEDES%252C%2520ICFG-PEDES%252C%2520and%2520RSTPReid%2520datasets%252C%250Aoutperforming%2520existing%2520methodologies.%2520The%2520dataset%252C%2520model%252C%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//github.com/Shuyu-XJTU/MRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimizing%20the%20Pretraining%20Gap%3A%20Domain-aligned%20Text-Based%20Person%0A%20%20Retrieval&entry.906535625=Shuyu%20Yang%20and%20Yaxiong%20Wang%20and%20Yongrui%20Li%20and%20Li%20Zhu%20and%20Zhedong%20Zheng&entry.1292438233=%20%20In%20this%20work%2C%20we%20focus%20on%20text-based%20person%20retrieval%2C%20which%20aims%20to%20identify%0Aindividuals%20based%20on%20textual%20descriptions.%20Given%20the%20significant%20privacy%20issues%0Aand%20the%20high%20cost%20associated%20with%20manual%20annotation%2C%20synthetic%20data%20has%20become%0Aa%20popular%20choice%20for%20pretraining%20models%2C%20leading%20to%20notable%20advancements.%0AHowever%2C%20the%20considerable%20domain%20gap%20between%20synthetic%20pretraining%20datasets%20and%0Areal-world%20target%20datasets%2C%20characterized%20by%20differences%20in%20lighting%2C%20color%2C%0Aand%20viewpoint%2C%20remains%20a%20critical%20obstacle%20that%20hinders%20the%20effectiveness%20of%0Athe%20pretrain-finetune%20paradigm.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20unified%0Atext-based%20person%20retrieval%20pipeline%20considering%20domain%20adaptation%20at%20both%0Aimage%20and%20region%20levels.%20In%20particular%2C%20it%20contains%20two%20primary%20components%2C%0Ai.e.%2C%20Domain-aware%20Diffusion%20%28DaD%29%20for%20image-level%20adaptation%20and%0AMulti-granularity%20Relation%20Alignment%20%28MRA%29%20for%20region-level%20adaptation.%20As%20the%0Aname%20implies%2C%20Domain-aware%20Diffusion%20is%20to%20migrate%20the%20distribution%20of%20images%0Afrom%20the%20pretraining%20dataset%20domain%20to%20the%20target%20real-world%20dataset%20domain%2C%0Ae.g.%2C%20CUHK-PEDES.%20Subsequently%2C%20MRA%20performs%20a%20meticulous%20region-level%0Aalignment%20by%20establishing%20correspondences%20between%20visual%20regions%20and%20their%0Adescriptive%20sentences%2C%20thereby%20addressing%20disparities%20at%20a%20finer%20granularity.%0AExtensive%20experiments%20show%20that%20our%20dual-level%20adaptation%20method%20has%20achieved%0Astate-of-the-art%20results%20on%20the%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20and%20RSTPReid%20datasets%2C%0Aoutperforming%20existing%20methodologies.%20The%20dataset%2C%20model%2C%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/Shuyu-XJTU/MRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10195v1&entry.124074799=Read"},
{"title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling", "author": "Victor Letzelter and Hugo Malard and Mathieu Fontaine and Ga\u00ebl Richard and Slim Essid and Andrei Bursuc and Patrick P\u00e9rez", "abstract": "  We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.\n", "link": "http://arxiv.org/abs/2507.10419v1", "date": "2025-07-14", "relevancy": 2.6672, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple%20Choice%20Learning%20of%20Low%20Rank%20Adapters%20for%20Language%20Modeling&body=Title%3A%20Multiple%20Choice%20Learning%20of%20Low%20Rank%20Adapters%20for%20Language%20Modeling%0AAuthor%3A%20Victor%20Letzelter%20and%20Hugo%20Malard%20and%20Mathieu%20Fontaine%20and%20Ga%C3%ABl%20Richard%20and%20Slim%20Essid%20and%20Andrei%20Bursuc%20and%20Patrick%20P%C3%A9rez%0AAbstract%3A%20%20%20We%20propose%20LoRA-MCL%2C%20a%20training%20scheme%20that%20extends%20next-token%20prediction%20in%0Alanguage%20models%20with%20a%20method%20designed%20to%20decode%20diverse%2C%20plausible%20sentence%0Acontinuations%20at%20inference%20time.%20Traditional%20language%20modeling%20is%20an%0Aintrinsically%20ill-posed%20problem%3A%20given%20a%20context%2C%20multiple%20futures%20may%20be%0Aequally%20plausible.%20Our%20approach%20leverages%20Multiple%20Choice%20Learning%20%28MCL%29%20and%0Athe%20Winner-Takes-All%20%28WTA%29%20loss%20to%20efficiently%20handle%20ambiguity%20through%0ALow-Rank%20Adaptation%20%28LoRA%29.%20We%20provide%20a%20theoretical%20interpretation%20of%20applying%0AMultiple%20Choice%20Learning%20to%20Language%20Modeling%2C%20assuming%20the%20data%20is%20generated%0Afrom%20a%20mixture%20of%20distributions.%20To%20illustrate%20the%20proposed%20approach%2C%20we%20use%0Adata%20sampled%20from%20mixtures%20of%20Markov%20chains.%20We%20then%20demonstrate%20with%20extensive%0Aexperiments%20on%20real-world%20visual%20and%20audio%20captioning%20tasks%20that%20our%20method%0Aachieves%20high%20diversity%20and%20relevance%20in%20generated%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple%2520Choice%2520Learning%2520of%2520Low%2520Rank%2520Adapters%2520for%2520Language%2520Modeling%26entry.906535625%3DVictor%2520Letzelter%2520and%2520Hugo%2520Malard%2520and%2520Mathieu%2520Fontaine%2520and%2520Ga%25C3%25ABl%2520Richard%2520and%2520Slim%2520Essid%2520and%2520Andrei%2520Bursuc%2520and%2520Patrick%2520P%25C3%25A9rez%26entry.1292438233%3D%2520%2520We%2520propose%2520LoRA-MCL%252C%2520a%2520training%2520scheme%2520that%2520extends%2520next-token%2520prediction%2520in%250Alanguage%2520models%2520with%2520a%2520method%2520designed%2520to%2520decode%2520diverse%252C%2520plausible%2520sentence%250Acontinuations%2520at%2520inference%2520time.%2520Traditional%2520language%2520modeling%2520is%2520an%250Aintrinsically%2520ill-posed%2520problem%253A%2520given%2520a%2520context%252C%2520multiple%2520futures%2520may%2520be%250Aequally%2520plausible.%2520Our%2520approach%2520leverages%2520Multiple%2520Choice%2520Learning%2520%2528MCL%2529%2520and%250Athe%2520Winner-Takes-All%2520%2528WTA%2529%2520loss%2520to%2520efficiently%2520handle%2520ambiguity%2520through%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529.%2520We%2520provide%2520a%2520theoretical%2520interpretation%2520of%2520applying%250AMultiple%2520Choice%2520Learning%2520to%2520Language%2520Modeling%252C%2520assuming%2520the%2520data%2520is%2520generated%250Afrom%2520a%2520mixture%2520of%2520distributions.%2520To%2520illustrate%2520the%2520proposed%2520approach%252C%2520we%2520use%250Adata%2520sampled%2520from%2520mixtures%2520of%2520Markov%2520chains.%2520We%2520then%2520demonstrate%2520with%2520extensive%250Aexperiments%2520on%2520real-world%2520visual%2520and%2520audio%2520captioning%2520tasks%2520that%2520our%2520method%250Aachieves%2520high%2520diversity%2520and%2520relevance%2520in%2520generated%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple%20Choice%20Learning%20of%20Low%20Rank%20Adapters%20for%20Language%20Modeling&entry.906535625=Victor%20Letzelter%20and%20Hugo%20Malard%20and%20Mathieu%20Fontaine%20and%20Ga%C3%ABl%20Richard%20and%20Slim%20Essid%20and%20Andrei%20Bursuc%20and%20Patrick%20P%C3%A9rez&entry.1292438233=%20%20We%20propose%20LoRA-MCL%2C%20a%20training%20scheme%20that%20extends%20next-token%20prediction%20in%0Alanguage%20models%20with%20a%20method%20designed%20to%20decode%20diverse%2C%20plausible%20sentence%0Acontinuations%20at%20inference%20time.%20Traditional%20language%20modeling%20is%20an%0Aintrinsically%20ill-posed%20problem%3A%20given%20a%20context%2C%20multiple%20futures%20may%20be%0Aequally%20plausible.%20Our%20approach%20leverages%20Multiple%20Choice%20Learning%20%28MCL%29%20and%0Athe%20Winner-Takes-All%20%28WTA%29%20loss%20to%20efficiently%20handle%20ambiguity%20through%0ALow-Rank%20Adaptation%20%28LoRA%29.%20We%20provide%20a%20theoretical%20interpretation%20of%20applying%0AMultiple%20Choice%20Learning%20to%20Language%20Modeling%2C%20assuming%20the%20data%20is%20generated%0Afrom%20a%20mixture%20of%20distributions.%20To%20illustrate%20the%20proposed%20approach%2C%20we%20use%0Adata%20sampled%20from%20mixtures%20of%20Markov%20chains.%20We%20then%20demonstrate%20with%20extensive%0Aexperiments%20on%20real-world%20visual%20and%20audio%20captioning%20tasks%20that%20our%20method%0Aachieves%20high%20diversity%20and%20relevance%20in%20generated%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10419v1&entry.124074799=Read"},
{"title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "author": "Daniele Rege Cambrin and Lorenzo Vaiani and Giuseppe Gallipoli and Luca Cagliero and Paolo Garza", "abstract": "  Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.\n", "link": "http://arxiv.org/abs/2507.10403v1", "date": "2025-07-14", "relevancy": 2.6458, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5331}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Remote-Sensing-Image%20Retrieval%20beyond%20RGB%20Sources&body=Title%3A%20Text-to-Remote-Sensing-Image%20Retrieval%20beyond%20RGB%20Sources%0AAuthor%3A%20Daniele%20Rege%20Cambrin%20and%20Lorenzo%20Vaiani%20and%20Giuseppe%20Gallipoli%20and%20Luca%20Cagliero%20and%20Paolo%20Garza%0AAbstract%3A%20%20%20Retrieving%20relevant%20imagery%20from%20vast%20satellite%20archives%20is%20crucial%20for%0Aapplications%20like%20disaster%20response%20and%20long-term%20climate%20monitoring.%20However%2C%0Amost%20text-to-image%20retrieval%20systems%20are%20limited%20to%20RGB%20data%2C%20failing%20to%0Aexploit%20the%20unique%20physical%20information%20captured%20by%20other%20sensors%2C%20such%20as%20the%0Aall-weather%20structural%20sensitivity%20of%20Synthetic%20Aperture%20Radar%20%28SAR%29%20or%20the%0Aspectral%20signatures%20in%20optical%20multispectral%20data.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20CrisisLandMark%2C%20a%20new%20large-scale%20corpus%20of%20over%20647%2C000%20Sentinel-1%0ASAR%20and%20Sentinel-2%20multispectral%20images%20paired%20with%20structured%20textual%0Aannotations%20for%20land%20cover%2C%20land%20use%2C%20and%20crisis%20events%20harmonized%20from%0Aauthoritative%20land%20cover%20systems%20%28CORINE%20and%20Dynamic%20World%29%20and%20crisis-specific%0Asources.%20We%20then%20present%20CLOSP%20%28Contrastive%20Language%20Optical%20SAR%20Pretraining%29%2C%0Aa%20novel%20framework%20that%20uses%20text%20as%20a%20bridge%20to%20align%20unpaired%20optical%20and%20SAR%0Aimages%20into%20a%20unified%20embedding%20space.%20Our%20experiments%20show%20that%20CLOSP%20achieves%0Aa%20new%20state-of-the-art%2C%20improving%20retrieval%20nDGC%20by%2054%25%20over%20existing%20models.%0AAdditionally%2C%20we%20find%20that%20the%20unified%20training%20strategy%20overcomes%20the%20inherent%0Adifficulty%20of%20interpreting%20SAR%20imagery%20by%20transferring%20rich%20semantic%20knowledge%0Afrom%20the%20optical%20domain%20with%20indirect%20interaction.%20Furthermore%2C%20GeoCLOSP%2C%20which%0Aintegrates%20geographic%20coordinates%20into%20our%20framework%2C%20creates%20a%20powerful%0Atrade-off%20between%20generality%20and%20specificity%3A%20while%20the%20CLOSP%20excels%20at%20general%0Asemantic%20tasks%2C%20the%20GeoCLOSP%20becomes%20a%20specialized%20expert%20for%20retrieving%0Alocation-dependent%20crisis%20events%20and%20rare%20geographic%20features.%20This%20work%0Ahighlights%20that%20the%20integration%20of%20diverse%20sensor%20data%20and%20geographic%20context%0Ais%20essential%20for%20unlocking%20the%20full%20potential%20of%20remote%20sensing%20archives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Remote-Sensing-Image%2520Retrieval%2520beyond%2520RGB%2520Sources%26entry.906535625%3DDaniele%2520Rege%2520Cambrin%2520and%2520Lorenzo%2520Vaiani%2520and%2520Giuseppe%2520Gallipoli%2520and%2520Luca%2520Cagliero%2520and%2520Paolo%2520Garza%26entry.1292438233%3D%2520%2520Retrieving%2520relevant%2520imagery%2520from%2520vast%2520satellite%2520archives%2520is%2520crucial%2520for%250Aapplications%2520like%2520disaster%2520response%2520and%2520long-term%2520climate%2520monitoring.%2520However%252C%250Amost%2520text-to-image%2520retrieval%2520systems%2520are%2520limited%2520to%2520RGB%2520data%252C%2520failing%2520to%250Aexploit%2520the%2520unique%2520physical%2520information%2520captured%2520by%2520other%2520sensors%252C%2520such%2520as%2520the%250Aall-weather%2520structural%2520sensitivity%2520of%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520or%2520the%250Aspectral%2520signatures%2520in%2520optical%2520multispectral%2520data.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520CrisisLandMark%252C%2520a%2520new%2520large-scale%2520corpus%2520of%2520over%2520647%252C000%2520Sentinel-1%250ASAR%2520and%2520Sentinel-2%2520multispectral%2520images%2520paired%2520with%2520structured%2520textual%250Aannotations%2520for%2520land%2520cover%252C%2520land%2520use%252C%2520and%2520crisis%2520events%2520harmonized%2520from%250Aauthoritative%2520land%2520cover%2520systems%2520%2528CORINE%2520and%2520Dynamic%2520World%2529%2520and%2520crisis-specific%250Asources.%2520We%2520then%2520present%2520CLOSP%2520%2528Contrastive%2520Language%2520Optical%2520SAR%2520Pretraining%2529%252C%250Aa%2520novel%2520framework%2520that%2520uses%2520text%2520as%2520a%2520bridge%2520to%2520align%2520unpaired%2520optical%2520and%2520SAR%250Aimages%2520into%2520a%2520unified%2520embedding%2520space.%2520Our%2520experiments%2520show%2520that%2520CLOSP%2520achieves%250Aa%2520new%2520state-of-the-art%252C%2520improving%2520retrieval%2520nDGC%2520by%252054%2525%2520over%2520existing%2520models.%250AAdditionally%252C%2520we%2520find%2520that%2520the%2520unified%2520training%2520strategy%2520overcomes%2520the%2520inherent%250Adifficulty%2520of%2520interpreting%2520SAR%2520imagery%2520by%2520transferring%2520rich%2520semantic%2520knowledge%250Afrom%2520the%2520optical%2520domain%2520with%2520indirect%2520interaction.%2520Furthermore%252C%2520GeoCLOSP%252C%2520which%250Aintegrates%2520geographic%2520coordinates%2520into%2520our%2520framework%252C%2520creates%2520a%2520powerful%250Atrade-off%2520between%2520generality%2520and%2520specificity%253A%2520while%2520the%2520CLOSP%2520excels%2520at%2520general%250Asemantic%2520tasks%252C%2520the%2520GeoCLOSP%2520becomes%2520a%2520specialized%2520expert%2520for%2520retrieving%250Alocation-dependent%2520crisis%2520events%2520and%2520rare%2520geographic%2520features.%2520This%2520work%250Ahighlights%2520that%2520the%2520integration%2520of%2520diverse%2520sensor%2520data%2520and%2520geographic%2520context%250Ais%2520essential%2520for%2520unlocking%2520the%2520full%2520potential%2520of%2520remote%2520sensing%2520archives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Remote-Sensing-Image%20Retrieval%20beyond%20RGB%20Sources&entry.906535625=Daniele%20Rege%20Cambrin%20and%20Lorenzo%20Vaiani%20and%20Giuseppe%20Gallipoli%20and%20Luca%20Cagliero%20and%20Paolo%20Garza&entry.1292438233=%20%20Retrieving%20relevant%20imagery%20from%20vast%20satellite%20archives%20is%20crucial%20for%0Aapplications%20like%20disaster%20response%20and%20long-term%20climate%20monitoring.%20However%2C%0Amost%20text-to-image%20retrieval%20systems%20are%20limited%20to%20RGB%20data%2C%20failing%20to%0Aexploit%20the%20unique%20physical%20information%20captured%20by%20other%20sensors%2C%20such%20as%20the%0Aall-weather%20structural%20sensitivity%20of%20Synthetic%20Aperture%20Radar%20%28SAR%29%20or%20the%0Aspectral%20signatures%20in%20optical%20multispectral%20data.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20CrisisLandMark%2C%20a%20new%20large-scale%20corpus%20of%20over%20647%2C000%20Sentinel-1%0ASAR%20and%20Sentinel-2%20multispectral%20images%20paired%20with%20structured%20textual%0Aannotations%20for%20land%20cover%2C%20land%20use%2C%20and%20crisis%20events%20harmonized%20from%0Aauthoritative%20land%20cover%20systems%20%28CORINE%20and%20Dynamic%20World%29%20and%20crisis-specific%0Asources.%20We%20then%20present%20CLOSP%20%28Contrastive%20Language%20Optical%20SAR%20Pretraining%29%2C%0Aa%20novel%20framework%20that%20uses%20text%20as%20a%20bridge%20to%20align%20unpaired%20optical%20and%20SAR%0Aimages%20into%20a%20unified%20embedding%20space.%20Our%20experiments%20show%20that%20CLOSP%20achieves%0Aa%20new%20state-of-the-art%2C%20improving%20retrieval%20nDGC%20by%2054%25%20over%20existing%20models.%0AAdditionally%2C%20we%20find%20that%20the%20unified%20training%20strategy%20overcomes%20the%20inherent%0Adifficulty%20of%20interpreting%20SAR%20imagery%20by%20transferring%20rich%20semantic%20knowledge%0Afrom%20the%20optical%20domain%20with%20indirect%20interaction.%20Furthermore%2C%20GeoCLOSP%2C%20which%0Aintegrates%20geographic%20coordinates%20into%20our%20framework%2C%20creates%20a%20powerful%0Atrade-off%20between%20generality%20and%20specificity%3A%20while%20the%20CLOSP%20excels%20at%20general%0Asemantic%20tasks%2C%20the%20GeoCLOSP%20becomes%20a%20specialized%20expert%20for%20retrieving%0Alocation-dependent%20crisis%20events%20and%20rare%20geographic%20features.%20This%20work%0Ahighlights%20that%20the%20integration%20of%20diverse%20sensor%20data%20and%20geographic%20context%0Ais%20essential%20for%20unlocking%20the%20full%20potential%20of%20remote%20sensing%20archives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10403v1&entry.124074799=Read"},
{"title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation", "author": "Ao Wang and Hui Chen and Jiaxin Li and Jianchao Tan and Kefeng Zhang and Xunliang Cai and Zijia Lin and Jungong Han and Guiguang Ding", "abstract": "  Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.\n", "link": "http://arxiv.org/abs/2412.03409v3", "date": "2025-07-14", "relevancy": 2.6373, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrefixKV%3A%20Adaptive%20Prefix%20KV%20Cache%20is%20What%20Vision%20Instruction-Following%0A%20%20Models%20Need%20for%20Efficient%20Generation&body=Title%3A%20PrefixKV%3A%20Adaptive%20Prefix%20KV%20Cache%20is%20What%20Vision%20Instruction-Following%0A%20%20Models%20Need%20for%20Efficient%20Generation%0AAuthor%3A%20Ao%20Wang%20and%20Hui%20Chen%20and%20Jiaxin%20Li%20and%20Jianchao%20Tan%20and%20Kefeng%20Zhang%20and%20Xunliang%20Cai%20and%20Zijia%20Lin%20and%20Jungong%20Han%20and%20Guiguang%20Ding%0AAbstract%3A%20%20%20Recently%2C%20large%20vision-language%20models%20%28LVLMs%29%20have%20rapidly%20gained%20popularity%0Afor%20their%20strong%20generation%20and%20reasoning%20capabilities%20given%20diverse%20multimodal%0Ainputs.%20However%2C%20these%20models%20incur%20significant%20computational%20and%20memory%0Aoverhead%20during%20inference%2C%20which%20greatly%20hinders%20the%20efficient%20deployment%20in%0Apractical%20scenarios.%20The%20extensive%20key-value%20%28KV%29%20cache%2C%20necessitated%20by%20the%0Alengthy%20input%20and%20output%20sequences%2C%20notably%20contributes%20to%20the%20high%20inference%0Acost.%20Based%20on%20this%2C%20recent%20works%20have%20investigated%20ways%20to%20reduce%20the%20KV%20cache%0Asize%20for%20higher%20efficiency.%20Although%20effective%2C%20they%20generally%20overlook%20the%0Adistinct%20importance%20distributions%20of%20KV%20vectors%20across%20layers%20and%20maintain%20the%0Asame%20cache%20size%20for%20each%20layer%20during%20the%20next%20token%20prediction.%20This%20results%0Ain%20the%20significant%20contextual%20information%20loss%20for%20certain%20layers%2C%20leading%20to%0Anotable%20performance%20decline.%20To%20address%20this%2C%20we%20present%20PrefixKV.%20It%20reframes%0Athe%20challenge%20of%20determining%20KV%20cache%20sizes%20for%20all%20layers%20into%20the%20task%20of%0Asearching%20for%20the%20optimal%20global%20prefix%20configuration.%20With%20an%20adaptive%0Alayer-wise%20KV%20retention%20recipe%20based%20on%20binary%20search%2C%20the%20maximum%20contextual%0Ainformation%20can%20thus%20be%20preserved%20in%20each%20layer%2C%20facilitating%20the%20generation.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20the%20state-of-the-art%0Aperformance%20compared%20with%20others.%20It%20exhibits%20superior%20inference%20efficiency%20and%0Ageneration%20quality%20trade-offs%2C%20showing%20promising%20potential%20for%20practical%0Aapplications.%20Code%20is%20available%20at%20https%3A//github.com/THU-MIG/PrefixKV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03409v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefixKV%253A%2520Adaptive%2520Prefix%2520KV%2520Cache%2520is%2520What%2520Vision%2520Instruction-Following%250A%2520%2520Models%2520Need%2520for%2520Efficient%2520Generation%26entry.906535625%3DAo%2520Wang%2520and%2520Hui%2520Chen%2520and%2520Jiaxin%2520Li%2520and%2520Jianchao%2520Tan%2520and%2520Kefeng%2520Zhang%2520and%2520Xunliang%2520Cai%2520and%2520Zijia%2520Lin%2520and%2520Jungong%2520Han%2520and%2520Guiguang%2520Ding%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520rapidly%2520gained%2520popularity%250Afor%2520their%2520strong%2520generation%2520and%2520reasoning%2520capabilities%2520given%2520diverse%2520multimodal%250Ainputs.%2520However%252C%2520these%2520models%2520incur%2520significant%2520computational%2520and%2520memory%250Aoverhead%2520during%2520inference%252C%2520which%2520greatly%2520hinders%2520the%2520efficient%2520deployment%2520in%250Apractical%2520scenarios.%2520The%2520extensive%2520key-value%2520%2528KV%2529%2520cache%252C%2520necessitated%2520by%2520the%250Alengthy%2520input%2520and%2520output%2520sequences%252C%2520notably%2520contributes%2520to%2520the%2520high%2520inference%250Acost.%2520Based%2520on%2520this%252C%2520recent%2520works%2520have%2520investigated%2520ways%2520to%2520reduce%2520the%2520KV%2520cache%250Asize%2520for%2520higher%2520efficiency.%2520Although%2520effective%252C%2520they%2520generally%2520overlook%2520the%250Adistinct%2520importance%2520distributions%2520of%2520KV%2520vectors%2520across%2520layers%2520and%2520maintain%2520the%250Asame%2520cache%2520size%2520for%2520each%2520layer%2520during%2520the%2520next%2520token%2520prediction.%2520This%2520results%250Ain%2520the%2520significant%2520contextual%2520information%2520loss%2520for%2520certain%2520layers%252C%2520leading%2520to%250Anotable%2520performance%2520decline.%2520To%2520address%2520this%252C%2520we%2520present%2520PrefixKV.%2520It%2520reframes%250Athe%2520challenge%2520of%2520determining%2520KV%2520cache%2520sizes%2520for%2520all%2520layers%2520into%2520the%2520task%2520of%250Asearching%2520for%2520the%2520optimal%2520global%2520prefix%2520configuration.%2520With%2520an%2520adaptive%250Alayer-wise%2520KV%2520retention%2520recipe%2520based%2520on%2520binary%2520search%252C%2520the%2520maximum%2520contextual%250Ainformation%2520can%2520thus%2520be%2520preserved%2520in%2520each%2520layer%252C%2520facilitating%2520the%2520generation.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520the%2520state-of-the-art%250Aperformance%2520compared%2520with%2520others.%2520It%2520exhibits%2520superior%2520inference%2520efficiency%2520and%250Ageneration%2520quality%2520trade-offs%252C%2520showing%2520promising%2520potential%2520for%2520practical%250Aapplications.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/THU-MIG/PrefixKV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03409v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrefixKV%3A%20Adaptive%20Prefix%20KV%20Cache%20is%20What%20Vision%20Instruction-Following%0A%20%20Models%20Need%20for%20Efficient%20Generation&entry.906535625=Ao%20Wang%20and%20Hui%20Chen%20and%20Jiaxin%20Li%20and%20Jianchao%20Tan%20and%20Kefeng%20Zhang%20and%20Xunliang%20Cai%20and%20Zijia%20Lin%20and%20Jungong%20Han%20and%20Guiguang%20Ding&entry.1292438233=%20%20Recently%2C%20large%20vision-language%20models%20%28LVLMs%29%20have%20rapidly%20gained%20popularity%0Afor%20their%20strong%20generation%20and%20reasoning%20capabilities%20given%20diverse%20multimodal%0Ainputs.%20However%2C%20these%20models%20incur%20significant%20computational%20and%20memory%0Aoverhead%20during%20inference%2C%20which%20greatly%20hinders%20the%20efficient%20deployment%20in%0Apractical%20scenarios.%20The%20extensive%20key-value%20%28KV%29%20cache%2C%20necessitated%20by%20the%0Alengthy%20input%20and%20output%20sequences%2C%20notably%20contributes%20to%20the%20high%20inference%0Acost.%20Based%20on%20this%2C%20recent%20works%20have%20investigated%20ways%20to%20reduce%20the%20KV%20cache%0Asize%20for%20higher%20efficiency.%20Although%20effective%2C%20they%20generally%20overlook%20the%0Adistinct%20importance%20distributions%20of%20KV%20vectors%20across%20layers%20and%20maintain%20the%0Asame%20cache%20size%20for%20each%20layer%20during%20the%20next%20token%20prediction.%20This%20results%0Ain%20the%20significant%20contextual%20information%20loss%20for%20certain%20layers%2C%20leading%20to%0Anotable%20performance%20decline.%20To%20address%20this%2C%20we%20present%20PrefixKV.%20It%20reframes%0Athe%20challenge%20of%20determining%20KV%20cache%20sizes%20for%20all%20layers%20into%20the%20task%20of%0Asearching%20for%20the%20optimal%20global%20prefix%20configuration.%20With%20an%20adaptive%0Alayer-wise%20KV%20retention%20recipe%20based%20on%20binary%20search%2C%20the%20maximum%20contextual%0Ainformation%20can%20thus%20be%20preserved%20in%20each%20layer%2C%20facilitating%20the%20generation.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20the%20state-of-the-art%0Aperformance%20compared%20with%20others.%20It%20exhibits%20superior%20inference%20efficiency%20and%0Ageneration%20quality%20trade-offs%2C%20showing%20promising%20potential%20for%20practical%0Aapplications.%20Code%20is%20available%20at%20https%3A//github.com/THU-MIG/PrefixKV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03409v3&entry.124074799=Read"},
{"title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments", "author": "Mingxian Lin and Wei Huang and Yitang Li and Chengjie Jiang and Kui Wu and Fangwei Zhong and Shengju Qian and Xin Wang and Xiaojuan Qi", "abstract": "  Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.\n", "link": "http://arxiv.org/abs/2507.10548v1", "date": "2025-07-14", "relevancy": 2.5985, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6513}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmbRACE-3K%3A%20Embodied%20Reasoning%20and%20Action%20in%20Complex%20Environments&body=Title%3A%20EmbRACE-3K%3A%20Embodied%20Reasoning%20and%20Action%20in%20Complex%20Environments%0AAuthor%3A%20Mingxian%20Lin%20and%20Wei%20Huang%20and%20Yitang%20Li%20and%20Chengjie%20Jiang%20and%20Kui%20Wu%20and%20Fangwei%20Zhong%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Recent%20advanced%20vision-language%20models%28VLMs%29%20have%20demonstrated%20strong%0Aperformance%20on%20passive%2C%20offline%20image%20and%20video%20understanding%20tasks.%20However%2C%0Atheir%20effectiveness%20in%20embodied%20settings%2C%20which%20require%20online%20interaction%20and%0Aactive%20scene%20understanding%20remains%20limited.%20In%20such%20scenarios%2C%20an%20agent%0Aperceives%20the%20environment%20from%20a%20first-person%20perspective%2C%20with%20each%20action%0Adynamically%20shaping%20subsequent%20observations.%20Even%20state-of-the-art%20models%20such%0Aas%20GPT-4o%2C%20Claude%203.5%20Sonnet%2C%20and%20Gemini%202.5%20Pro%20struggle%20in%20open-environment%0Ainteractions%2C%20exhibiting%20clear%20limitations%20in%20spatial%20reasoning%20and%0Along-horizon%20planning.%20To%20address%20this%20gap%2C%20we%20introduce%20EmRACE-3K%2C%20a%20dataset%0Aof%20over%203%2C000%20language-guided%20tasks%20situated%20in%20diverse%2C%20photorealistic%0Aenvironments%20constructed%20using%20Unreal%20Engine%20and%20the%20UnrealCV-Zoo%20framework.%0AThe%20tasks%20encompass%20a%20wide%20range%20of%20embodied%20challenges%2C%20including%20navigation%2C%0Aobject%20manipulation%2C%20and%20multi-stage%20goal%20execution.%20Each%20task%20unfolds%20as%20a%0Amulti-step%20trajectory%2C%20pairing%20first-person%20visual%20observations%20with%20high-level%0Ainstructions%2C%20grounded%20actions%2C%20and%20natural%20language%20rationales%20that%20express%0Athe%20agent%27s%20intent%20at%20every%20step.%20Using%20EmRACE-3K%2C%20we%20establish%20a%20benchmark%20to%0Aevaluate%20the%20embodied%20reasoning%20capabilities%20of%20VLMs%20across%20three%20key%0Adimensions%3A%20Exploration%2C%20Dynamic%20Spatial-Semantic%20Reasoning%2C%20and%20Multi-stage%0AGoal%20Execution.%20In%20zero-shot%20settings%2C%20all%20models%20achieve%20success%20rates%20below%0A20%25%2C%20underscoring%20the%20challenge%20posed%20by%20our%20benchmark%20and%20the%20current%0Alimitations%20of%20VLMs%20in%20interactive%20environments.%20To%20demonstrate%20the%20utility%20of%0AEmRACE-3K%2C%20we%20further%20fine-tune%20Qwen2.5-VL-7B%20using%20supervised%20learning%0Afollowed%20by%20reinforcement%20learning.%20This%20approach%20yields%20substantial%0Aimprovements%20across%20all%20three%20challenge%20categories%2C%20highlighting%20the%20dataset%27s%0Aeffectiveness%20in%20enabling%20the%20development%20of%20embodied%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbRACE-3K%253A%2520Embodied%2520Reasoning%2520and%2520Action%2520in%2520Complex%2520Environments%26entry.906535625%3DMingxian%2520Lin%2520and%2520Wei%2520Huang%2520and%2520Yitang%2520Li%2520and%2520Chengjie%2520Jiang%2520and%2520Kui%2520Wu%2520and%2520Fangwei%2520Zhong%2520and%2520Shengju%2520Qian%2520and%2520Xin%2520Wang%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Recent%2520advanced%2520vision-language%2520models%2528VLMs%2529%2520have%2520demonstrated%2520strong%250Aperformance%2520on%2520passive%252C%2520offline%2520image%2520and%2520video%2520understanding%2520tasks.%2520However%252C%250Atheir%2520effectiveness%2520in%2520embodied%2520settings%252C%2520which%2520require%2520online%2520interaction%2520and%250Aactive%2520scene%2520understanding%2520remains%2520limited.%2520In%2520such%2520scenarios%252C%2520an%2520agent%250Aperceives%2520the%2520environment%2520from%2520a%2520first-person%2520perspective%252C%2520with%2520each%2520action%250Adynamically%2520shaping%2520subsequent%2520observations.%2520Even%2520state-of-the-art%2520models%2520such%250Aas%2520GPT-4o%252C%2520Claude%25203.5%2520Sonnet%252C%2520and%2520Gemini%25202.5%2520Pro%2520struggle%2520in%2520open-environment%250Ainteractions%252C%2520exhibiting%2520clear%2520limitations%2520in%2520spatial%2520reasoning%2520and%250Along-horizon%2520planning.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520EmRACE-3K%252C%2520a%2520dataset%250Aof%2520over%25203%252C000%2520language-guided%2520tasks%2520situated%2520in%2520diverse%252C%2520photorealistic%250Aenvironments%2520constructed%2520using%2520Unreal%2520Engine%2520and%2520the%2520UnrealCV-Zoo%2520framework.%250AThe%2520tasks%2520encompass%2520a%2520wide%2520range%2520of%2520embodied%2520challenges%252C%2520including%2520navigation%252C%250Aobject%2520manipulation%252C%2520and%2520multi-stage%2520goal%2520execution.%2520Each%2520task%2520unfolds%2520as%2520a%250Amulti-step%2520trajectory%252C%2520pairing%2520first-person%2520visual%2520observations%2520with%2520high-level%250Ainstructions%252C%2520grounded%2520actions%252C%2520and%2520natural%2520language%2520rationales%2520that%2520express%250Athe%2520agent%2527s%2520intent%2520at%2520every%2520step.%2520Using%2520EmRACE-3K%252C%2520we%2520establish%2520a%2520benchmark%2520to%250Aevaluate%2520the%2520embodied%2520reasoning%2520capabilities%2520of%2520VLMs%2520across%2520three%2520key%250Adimensions%253A%2520Exploration%252C%2520Dynamic%2520Spatial-Semantic%2520Reasoning%252C%2520and%2520Multi-stage%250AGoal%2520Execution.%2520In%2520zero-shot%2520settings%252C%2520all%2520models%2520achieve%2520success%2520rates%2520below%250A20%2525%252C%2520underscoring%2520the%2520challenge%2520posed%2520by%2520our%2520benchmark%2520and%2520the%2520current%250Alimitations%2520of%2520VLMs%2520in%2520interactive%2520environments.%2520To%2520demonstrate%2520the%2520utility%2520of%250AEmRACE-3K%252C%2520we%2520further%2520fine-tune%2520Qwen2.5-VL-7B%2520using%2520supervised%2520learning%250Afollowed%2520by%2520reinforcement%2520learning.%2520This%2520approach%2520yields%2520substantial%250Aimprovements%2520across%2520all%2520three%2520challenge%2520categories%252C%2520highlighting%2520the%2520dataset%2527s%250Aeffectiveness%2520in%2520enabling%2520the%2520development%2520of%2520embodied%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmbRACE-3K%3A%20Embodied%20Reasoning%20and%20Action%20in%20Complex%20Environments&entry.906535625=Mingxian%20Lin%20and%20Wei%20Huang%20and%20Yitang%20Li%20and%20Chengjie%20Jiang%20and%20Kui%20Wu%20and%20Fangwei%20Zhong%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Recent%20advanced%20vision-language%20models%28VLMs%29%20have%20demonstrated%20strong%0Aperformance%20on%20passive%2C%20offline%20image%20and%20video%20understanding%20tasks.%20However%2C%0Atheir%20effectiveness%20in%20embodied%20settings%2C%20which%20require%20online%20interaction%20and%0Aactive%20scene%20understanding%20remains%20limited.%20In%20such%20scenarios%2C%20an%20agent%0Aperceives%20the%20environment%20from%20a%20first-person%20perspective%2C%20with%20each%20action%0Adynamically%20shaping%20subsequent%20observations.%20Even%20state-of-the-art%20models%20such%0Aas%20GPT-4o%2C%20Claude%203.5%20Sonnet%2C%20and%20Gemini%202.5%20Pro%20struggle%20in%20open-environment%0Ainteractions%2C%20exhibiting%20clear%20limitations%20in%20spatial%20reasoning%20and%0Along-horizon%20planning.%20To%20address%20this%20gap%2C%20we%20introduce%20EmRACE-3K%2C%20a%20dataset%0Aof%20over%203%2C000%20language-guided%20tasks%20situated%20in%20diverse%2C%20photorealistic%0Aenvironments%20constructed%20using%20Unreal%20Engine%20and%20the%20UnrealCV-Zoo%20framework.%0AThe%20tasks%20encompass%20a%20wide%20range%20of%20embodied%20challenges%2C%20including%20navigation%2C%0Aobject%20manipulation%2C%20and%20multi-stage%20goal%20execution.%20Each%20task%20unfolds%20as%20a%0Amulti-step%20trajectory%2C%20pairing%20first-person%20visual%20observations%20with%20high-level%0Ainstructions%2C%20grounded%20actions%2C%20and%20natural%20language%20rationales%20that%20express%0Athe%20agent%27s%20intent%20at%20every%20step.%20Using%20EmRACE-3K%2C%20we%20establish%20a%20benchmark%20to%0Aevaluate%20the%20embodied%20reasoning%20capabilities%20of%20VLMs%20across%20three%20key%0Adimensions%3A%20Exploration%2C%20Dynamic%20Spatial-Semantic%20Reasoning%2C%20and%20Multi-stage%0AGoal%20Execution.%20In%20zero-shot%20settings%2C%20all%20models%20achieve%20success%20rates%20below%0A20%25%2C%20underscoring%20the%20challenge%20posed%20by%20our%20benchmark%20and%20the%20current%0Alimitations%20of%20VLMs%20in%20interactive%20environments.%20To%20demonstrate%20the%20utility%20of%0AEmRACE-3K%2C%20we%20further%20fine-tune%20Qwen2.5-VL-7B%20using%20supervised%20learning%0Afollowed%20by%20reinforcement%20learning.%20This%20approach%20yields%20substantial%0Aimprovements%20across%20all%20three%20challenge%20categories%2C%20highlighting%20the%20dataset%27s%0Aeffectiveness%20in%20enabling%20the%20development%20of%20embodied%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10548v1&entry.124074799=Read"},
{"title": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in\n  Histopathology", "author": "Ashkan Shakarami and Lorenzo Nicole and Rocco Cappellesso and Angelo Paolo Dei Tos and Stefano Ghidoni", "abstract": "  Accurate and timely cancer diagnosis from histopathological slides is vital\nfor effective clinical decision-making. This paper introduces DepViT-CAD, a\ndeployable AI system for multi-class cancer diagnosis in histopathology. At its\ncore is MAViT, a novel Multi-Attention Vision Transformer designed to capture\nfine-grained morphological patterns across diverse tumor types. MAViT was\ntrained on expert-annotated patches from 1008 whole-slide images, covering 11\ndiagnostic categories, including 10 major cancers and non-tumor tissue.\nDepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer\nGenome Atlas and 50 routine clinical cases from pathology labs, achieving\ndiagnostic sensitivities of 94.11% and 92%, respectively. By combining\nstate-of-the-art transformer architecture with large-scale real-world\nvalidation, DepViT-CAD offers a robust and scalable approach for AI-assisted\ncancer diagnostics. To support transparency and reproducibility, software and\ncode will be made publicly available at GitHub.\n", "link": "http://arxiv.org/abs/2507.10250v1", "date": "2025-07-14", "relevancy": 2.573, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5182}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepViT-CAD%3A%20Deployable%20Vision%20Transformer-Based%20Cancer%20Diagnosis%20in%0A%20%20Histopathology&body=Title%3A%20DepViT-CAD%3A%20Deployable%20Vision%20Transformer-Based%20Cancer%20Diagnosis%20in%0A%20%20Histopathology%0AAuthor%3A%20Ashkan%20Shakarami%20and%20Lorenzo%20Nicole%20and%20Rocco%20Cappellesso%20and%20Angelo%20Paolo%20Dei%20Tos%20and%20Stefano%20Ghidoni%0AAbstract%3A%20%20%20Accurate%20and%20timely%20cancer%20diagnosis%20from%20histopathological%20slides%20is%20vital%0Afor%20effective%20clinical%20decision-making.%20This%20paper%20introduces%20DepViT-CAD%2C%20a%0Adeployable%20AI%20system%20for%20multi-class%20cancer%20diagnosis%20in%20histopathology.%20At%20its%0Acore%20is%20MAViT%2C%20a%20novel%20Multi-Attention%20Vision%20Transformer%20designed%20to%20capture%0Afine-grained%20morphological%20patterns%20across%20diverse%20tumor%20types.%20MAViT%20was%0Atrained%20on%20expert-annotated%20patches%20from%201008%20whole-slide%20images%2C%20covering%2011%0Adiagnostic%20categories%2C%20including%2010%20major%20cancers%20and%20non-tumor%20tissue.%0ADepViT-CAD%20was%20validated%20on%20two%20independent%20cohorts%3A%20275%20WSIs%20from%20The%20Cancer%0AGenome%20Atlas%20and%2050%20routine%20clinical%20cases%20from%20pathology%20labs%2C%20achieving%0Adiagnostic%20sensitivities%20of%2094.11%25%20and%2092%25%2C%20respectively.%20By%20combining%0Astate-of-the-art%20transformer%20architecture%20with%20large-scale%20real-world%0Avalidation%2C%20DepViT-CAD%20offers%20a%20robust%20and%20scalable%20approach%20for%20AI-assisted%0Acancer%20diagnostics.%20To%20support%20transparency%20and%20reproducibility%2C%20software%20and%0Acode%20will%20be%20made%20publicly%20available%20at%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepViT-CAD%253A%2520Deployable%2520Vision%2520Transformer-Based%2520Cancer%2520Diagnosis%2520in%250A%2520%2520Histopathology%26entry.906535625%3DAshkan%2520Shakarami%2520and%2520Lorenzo%2520Nicole%2520and%2520Rocco%2520Cappellesso%2520and%2520Angelo%2520Paolo%2520Dei%2520Tos%2520and%2520Stefano%2520Ghidoni%26entry.1292438233%3D%2520%2520Accurate%2520and%2520timely%2520cancer%2520diagnosis%2520from%2520histopathological%2520slides%2520is%2520vital%250Afor%2520effective%2520clinical%2520decision-making.%2520This%2520paper%2520introduces%2520DepViT-CAD%252C%2520a%250Adeployable%2520AI%2520system%2520for%2520multi-class%2520cancer%2520diagnosis%2520in%2520histopathology.%2520At%2520its%250Acore%2520is%2520MAViT%252C%2520a%2520novel%2520Multi-Attention%2520Vision%2520Transformer%2520designed%2520to%2520capture%250Afine-grained%2520morphological%2520patterns%2520across%2520diverse%2520tumor%2520types.%2520MAViT%2520was%250Atrained%2520on%2520expert-annotated%2520patches%2520from%25201008%2520whole-slide%2520images%252C%2520covering%252011%250Adiagnostic%2520categories%252C%2520including%252010%2520major%2520cancers%2520and%2520non-tumor%2520tissue.%250ADepViT-CAD%2520was%2520validated%2520on%2520two%2520independent%2520cohorts%253A%2520275%2520WSIs%2520from%2520The%2520Cancer%250AGenome%2520Atlas%2520and%252050%2520routine%2520clinical%2520cases%2520from%2520pathology%2520labs%252C%2520achieving%250Adiagnostic%2520sensitivities%2520of%252094.11%2525%2520and%252092%2525%252C%2520respectively.%2520By%2520combining%250Astate-of-the-art%2520transformer%2520architecture%2520with%2520large-scale%2520real-world%250Avalidation%252C%2520DepViT-CAD%2520offers%2520a%2520robust%2520and%2520scalable%2520approach%2520for%2520AI-assisted%250Acancer%2520diagnostics.%2520To%2520support%2520transparency%2520and%2520reproducibility%252C%2520software%2520and%250Acode%2520will%2520be%2520made%2520publicly%2520available%2520at%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepViT-CAD%3A%20Deployable%20Vision%20Transformer-Based%20Cancer%20Diagnosis%20in%0A%20%20Histopathology&entry.906535625=Ashkan%20Shakarami%20and%20Lorenzo%20Nicole%20and%20Rocco%20Cappellesso%20and%20Angelo%20Paolo%20Dei%20Tos%20and%20Stefano%20Ghidoni&entry.1292438233=%20%20Accurate%20and%20timely%20cancer%20diagnosis%20from%20histopathological%20slides%20is%20vital%0Afor%20effective%20clinical%20decision-making.%20This%20paper%20introduces%20DepViT-CAD%2C%20a%0Adeployable%20AI%20system%20for%20multi-class%20cancer%20diagnosis%20in%20histopathology.%20At%20its%0Acore%20is%20MAViT%2C%20a%20novel%20Multi-Attention%20Vision%20Transformer%20designed%20to%20capture%0Afine-grained%20morphological%20patterns%20across%20diverse%20tumor%20types.%20MAViT%20was%0Atrained%20on%20expert-annotated%20patches%20from%201008%20whole-slide%20images%2C%20covering%2011%0Adiagnostic%20categories%2C%20including%2010%20major%20cancers%20and%20non-tumor%20tissue.%0ADepViT-CAD%20was%20validated%20on%20two%20independent%20cohorts%3A%20275%20WSIs%20from%20The%20Cancer%0AGenome%20Atlas%20and%2050%20routine%20clinical%20cases%20from%20pathology%20labs%2C%20achieving%0Adiagnostic%20sensitivities%20of%2094.11%25%20and%2092%25%2C%20respectively.%20By%20combining%0Astate-of-the-art%20transformer%20architecture%20with%20large-scale%20real-world%0Avalidation%2C%20DepViT-CAD%20offers%20a%20robust%20and%20scalable%20approach%20for%20AI-assisted%0Acancer%20diagnostics.%20To%20support%20transparency%20and%20reproducibility%2C%20software%20and%0Acode%20will%20be%20made%20publicly%20available%20at%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10250v1&entry.124074799=Read"},
{"title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans", "author": "Hugo Norrby and Gabriel F\u00e4rm and Kevin Hernandez-Diaz and Fernando Alonso-Fernandez", "abstract": "  We introduce FGSSNet, a novel multi-headed feature-guided semantic\nsegmentation (FGSS) architecture designed to improve the generalization ability\nof wall segmentation on floorplans. FGSSNet features a U-Net segmentation\nbackbone with a multi-headed dedicated feature extractor used to extract\ndomain-specific feature maps which are injected into the latent space of U-Net\nto guide the segmentation process. This dedicated feature extractor is trained\nas an encoder-decoder with selected wall patches, representative of the walls\npresent in the input floorplan, to produce a compressed latent representation\nof wall patches while jointly trained to predict the wall width. In doing so,\nwe expect that the feature extractor encodes texture and width features of wall\npatches that are useful to guide the wall segmentation process. Our experiments\nshow increased performance by the use of such injected features in comparison\nto the vanilla U-Net, highlighting the validity of the proposed approach.\n", "link": "http://arxiv.org/abs/2507.10343v1", "date": "2025-07-14", "relevancy": 2.557, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5314}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5093}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FGSSNet%3A%20Feature-Guided%20Semantic%20Segmentation%20of%20Real%20World%20Floorplans&body=Title%3A%20FGSSNet%3A%20Feature-Guided%20Semantic%20Segmentation%20of%20Real%20World%20Floorplans%0AAuthor%3A%20Hugo%20Norrby%20and%20Gabriel%20F%C3%A4rm%20and%20Kevin%20Hernandez-Diaz%20and%20Fernando%20Alonso-Fernandez%0AAbstract%3A%20%20%20We%20introduce%20FGSSNet%2C%20a%20novel%20multi-headed%20feature-guided%20semantic%0Asegmentation%20%28FGSS%29%20architecture%20designed%20to%20improve%20the%20generalization%20ability%0Aof%20wall%20segmentation%20on%20floorplans.%20FGSSNet%20features%20a%20U-Net%20segmentation%0Abackbone%20with%20a%20multi-headed%20dedicated%20feature%20extractor%20used%20to%20extract%0Adomain-specific%20feature%20maps%20which%20are%20injected%20into%20the%20latent%20space%20of%20U-Net%0Ato%20guide%20the%20segmentation%20process.%20This%20dedicated%20feature%20extractor%20is%20trained%0Aas%20an%20encoder-decoder%20with%20selected%20wall%20patches%2C%20representative%20of%20the%20walls%0Apresent%20in%20the%20input%20floorplan%2C%20to%20produce%20a%20compressed%20latent%20representation%0Aof%20wall%20patches%20while%20jointly%20trained%20to%20predict%20the%20wall%20width.%20In%20doing%20so%2C%0Awe%20expect%20that%20the%20feature%20extractor%20encodes%20texture%20and%20width%20features%20of%20wall%0Apatches%20that%20are%20useful%20to%20guide%20the%20wall%20segmentation%20process.%20Our%20experiments%0Ashow%20increased%20performance%20by%20the%20use%20of%20such%20injected%20features%20in%20comparison%0Ato%20the%20vanilla%20U-Net%2C%20highlighting%20the%20validity%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFGSSNet%253A%2520Feature-Guided%2520Semantic%2520Segmentation%2520of%2520Real%2520World%2520Floorplans%26entry.906535625%3DHugo%2520Norrby%2520and%2520Gabriel%2520F%25C3%25A4rm%2520and%2520Kevin%2520Hernandez-Diaz%2520and%2520Fernando%2520Alonso-Fernandez%26entry.1292438233%3D%2520%2520We%2520introduce%2520FGSSNet%252C%2520a%2520novel%2520multi-headed%2520feature-guided%2520semantic%250Asegmentation%2520%2528FGSS%2529%2520architecture%2520designed%2520to%2520improve%2520the%2520generalization%2520ability%250Aof%2520wall%2520segmentation%2520on%2520floorplans.%2520FGSSNet%2520features%2520a%2520U-Net%2520segmentation%250Abackbone%2520with%2520a%2520multi-headed%2520dedicated%2520feature%2520extractor%2520used%2520to%2520extract%250Adomain-specific%2520feature%2520maps%2520which%2520are%2520injected%2520into%2520the%2520latent%2520space%2520of%2520U-Net%250Ato%2520guide%2520the%2520segmentation%2520process.%2520This%2520dedicated%2520feature%2520extractor%2520is%2520trained%250Aas%2520an%2520encoder-decoder%2520with%2520selected%2520wall%2520patches%252C%2520representative%2520of%2520the%2520walls%250Apresent%2520in%2520the%2520input%2520floorplan%252C%2520to%2520produce%2520a%2520compressed%2520latent%2520representation%250Aof%2520wall%2520patches%2520while%2520jointly%2520trained%2520to%2520predict%2520the%2520wall%2520width.%2520In%2520doing%2520so%252C%250Awe%2520expect%2520that%2520the%2520feature%2520extractor%2520encodes%2520texture%2520and%2520width%2520features%2520of%2520wall%250Apatches%2520that%2520are%2520useful%2520to%2520guide%2520the%2520wall%2520segmentation%2520process.%2520Our%2520experiments%250Ashow%2520increased%2520performance%2520by%2520the%2520use%2520of%2520such%2520injected%2520features%2520in%2520comparison%250Ato%2520the%2520vanilla%2520U-Net%252C%2520highlighting%2520the%2520validity%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FGSSNet%3A%20Feature-Guided%20Semantic%20Segmentation%20of%20Real%20World%20Floorplans&entry.906535625=Hugo%20Norrby%20and%20Gabriel%20F%C3%A4rm%20and%20Kevin%20Hernandez-Diaz%20and%20Fernando%20Alonso-Fernandez&entry.1292438233=%20%20We%20introduce%20FGSSNet%2C%20a%20novel%20multi-headed%20feature-guided%20semantic%0Asegmentation%20%28FGSS%29%20architecture%20designed%20to%20improve%20the%20generalization%20ability%0Aof%20wall%20segmentation%20on%20floorplans.%20FGSSNet%20features%20a%20U-Net%20segmentation%0Abackbone%20with%20a%20multi-headed%20dedicated%20feature%20extractor%20used%20to%20extract%0Adomain-specific%20feature%20maps%20which%20are%20injected%20into%20the%20latent%20space%20of%20U-Net%0Ato%20guide%20the%20segmentation%20process.%20This%20dedicated%20feature%20extractor%20is%20trained%0Aas%20an%20encoder-decoder%20with%20selected%20wall%20patches%2C%20representative%20of%20the%20walls%0Apresent%20in%20the%20input%20floorplan%2C%20to%20produce%20a%20compressed%20latent%20representation%0Aof%20wall%20patches%20while%20jointly%20trained%20to%20predict%20the%20wall%20width.%20In%20doing%20so%2C%0Awe%20expect%20that%20the%20feature%20extractor%20encodes%20texture%20and%20width%20features%20of%20wall%0Apatches%20that%20are%20useful%20to%20guide%20the%20wall%20segmentation%20process.%20Our%20experiments%0Ashow%20increased%20performance%20by%20the%20use%20of%20such%20injected%20features%20in%20comparison%0Ato%20the%20vanilla%20U-Net%2C%20highlighting%20the%20validity%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10343v1&entry.124074799=Read"},
{"title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef\n  Image Understanding", "author": "Hongyong Han and Wei Wang and Gaowei Zhang and Mingjie Li and Yi Wang", "abstract": "  Coral reefs are vital yet vulnerable ecosystems that require continuous\nmonitoring to support conservation. While coral reef images provide essential\ninformation in coral monitoring, interpreting such images remains challenging\ndue to the need for domain expertise. Visual Question Answering (VQA), powered\nby Large Vision-Language Models (LVLMs), has great potential in user-friendly\ninteraction with coral reef images. However, applying VQA to coral imagery\ndemands a dedicated dataset that addresses two key challenges: domain-specific\nannotations and multidimensional questions. In this work, we introduce\nCoralVQA, the first large-scale VQA dataset for coral reef analysis. It\ncontains 12,805 real-world coral images from 67 coral genera collected from 3\noceans, along with 277,653 question-answer pairs that comprehensively assess\necological and health-related conditions. To construct this dataset, we develop\na semi-automatic data construction pipeline in collaboration with marine\nbiologists to ensure both scalability and professional-grade data quality.\nCoralVQA presents novel challenges and provides a comprehensive benchmark for\nstudying vision-language reasoning in the context of coral reef images. By\nevaluating several state-of-the-art LVLMs, we reveal key limitations and\nopportunities. These insights form a foundation for future LVLM development,\nwith a particular emphasis on supporting coral conservation efforts.\n", "link": "http://arxiv.org/abs/2507.10449v1", "date": "2025-07-14", "relevancy": 2.5516, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoralVQA%3A%20A%20Large-Scale%20Visual%20Question%20Answering%20Dataset%20for%20Coral%20Reef%0A%20%20Image%20Understanding&body=Title%3A%20CoralVQA%3A%20A%20Large-Scale%20Visual%20Question%20Answering%20Dataset%20for%20Coral%20Reef%0A%20%20Image%20Understanding%0AAuthor%3A%20Hongyong%20Han%20and%20Wei%20Wang%20and%20Gaowei%20Zhang%20and%20Mingjie%20Li%20and%20Yi%20Wang%0AAbstract%3A%20%20%20Coral%20reefs%20are%20vital%20yet%20vulnerable%20ecosystems%20that%20require%20continuous%0Amonitoring%20to%20support%20conservation.%20While%20coral%20reef%20images%20provide%20essential%0Ainformation%20in%20coral%20monitoring%2C%20interpreting%20such%20images%20remains%20challenging%0Adue%20to%20the%20need%20for%20domain%20expertise.%20Visual%20Question%20Answering%20%28VQA%29%2C%20powered%0Aby%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20has%20great%20potential%20in%20user-friendly%0Ainteraction%20with%20coral%20reef%20images.%20However%2C%20applying%20VQA%20to%20coral%20imagery%0Ademands%20a%20dedicated%20dataset%20that%20addresses%20two%20key%20challenges%3A%20domain-specific%0Aannotations%20and%20multidimensional%20questions.%20In%20this%20work%2C%20we%20introduce%0ACoralVQA%2C%20the%20first%20large-scale%20VQA%20dataset%20for%20coral%20reef%20analysis.%20It%0Acontains%2012%2C805%20real-world%20coral%20images%20from%2067%20coral%20genera%20collected%20from%203%0Aoceans%2C%20along%20with%20277%2C653%20question-answer%20pairs%20that%20comprehensively%20assess%0Aecological%20and%20health-related%20conditions.%20To%20construct%20this%20dataset%2C%20we%20develop%0Aa%20semi-automatic%20data%20construction%20pipeline%20in%20collaboration%20with%20marine%0Abiologists%20to%20ensure%20both%20scalability%20and%20professional-grade%20data%20quality.%0ACoralVQA%20presents%20novel%20challenges%20and%20provides%20a%20comprehensive%20benchmark%20for%0Astudying%20vision-language%20reasoning%20in%20the%20context%20of%20coral%20reef%20images.%20By%0Aevaluating%20several%20state-of-the-art%20LVLMs%2C%20we%20reveal%20key%20limitations%20and%0Aopportunities.%20These%20insights%20form%20a%20foundation%20for%20future%20LVLM%20development%2C%0Awith%20a%20particular%20emphasis%20on%20supporting%20coral%20conservation%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoralVQA%253A%2520A%2520Large-Scale%2520Visual%2520Question%2520Answering%2520Dataset%2520for%2520Coral%2520Reef%250A%2520%2520Image%2520Understanding%26entry.906535625%3DHongyong%2520Han%2520and%2520Wei%2520Wang%2520and%2520Gaowei%2520Zhang%2520and%2520Mingjie%2520Li%2520and%2520Yi%2520Wang%26entry.1292438233%3D%2520%2520Coral%2520reefs%2520are%2520vital%2520yet%2520vulnerable%2520ecosystems%2520that%2520require%2520continuous%250Amonitoring%2520to%2520support%2520conservation.%2520While%2520coral%2520reef%2520images%2520provide%2520essential%250Ainformation%2520in%2520coral%2520monitoring%252C%2520interpreting%2520such%2520images%2520remains%2520challenging%250Adue%2520to%2520the%2520need%2520for%2520domain%2520expertise.%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520powered%250Aby%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%2520has%2520great%2520potential%2520in%2520user-friendly%250Ainteraction%2520with%2520coral%2520reef%2520images.%2520However%252C%2520applying%2520VQA%2520to%2520coral%2520imagery%250Ademands%2520a%2520dedicated%2520dataset%2520that%2520addresses%2520two%2520key%2520challenges%253A%2520domain-specific%250Aannotations%2520and%2520multidimensional%2520questions.%2520In%2520this%2520work%252C%2520we%2520introduce%250ACoralVQA%252C%2520the%2520first%2520large-scale%2520VQA%2520dataset%2520for%2520coral%2520reef%2520analysis.%2520It%250Acontains%252012%252C805%2520real-world%2520coral%2520images%2520from%252067%2520coral%2520genera%2520collected%2520from%25203%250Aoceans%252C%2520along%2520with%2520277%252C653%2520question-answer%2520pairs%2520that%2520comprehensively%2520assess%250Aecological%2520and%2520health-related%2520conditions.%2520To%2520construct%2520this%2520dataset%252C%2520we%2520develop%250Aa%2520semi-automatic%2520data%2520construction%2520pipeline%2520in%2520collaboration%2520with%2520marine%250Abiologists%2520to%2520ensure%2520both%2520scalability%2520and%2520professional-grade%2520data%2520quality.%250ACoralVQA%2520presents%2520novel%2520challenges%2520and%2520provides%2520a%2520comprehensive%2520benchmark%2520for%250Astudying%2520vision-language%2520reasoning%2520in%2520the%2520context%2520of%2520coral%2520reef%2520images.%2520By%250Aevaluating%2520several%2520state-of-the-art%2520LVLMs%252C%2520we%2520reveal%2520key%2520limitations%2520and%250Aopportunities.%2520These%2520insights%2520form%2520a%2520foundation%2520for%2520future%2520LVLM%2520development%252C%250Awith%2520a%2520particular%2520emphasis%2520on%2520supporting%2520coral%2520conservation%2520efforts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoralVQA%3A%20A%20Large-Scale%20Visual%20Question%20Answering%20Dataset%20for%20Coral%20Reef%0A%20%20Image%20Understanding&entry.906535625=Hongyong%20Han%20and%20Wei%20Wang%20and%20Gaowei%20Zhang%20and%20Mingjie%20Li%20and%20Yi%20Wang&entry.1292438233=%20%20Coral%20reefs%20are%20vital%20yet%20vulnerable%20ecosystems%20that%20require%20continuous%0Amonitoring%20to%20support%20conservation.%20While%20coral%20reef%20images%20provide%20essential%0Ainformation%20in%20coral%20monitoring%2C%20interpreting%20such%20images%20remains%20challenging%0Adue%20to%20the%20need%20for%20domain%20expertise.%20Visual%20Question%20Answering%20%28VQA%29%2C%20powered%0Aby%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20has%20great%20potential%20in%20user-friendly%0Ainteraction%20with%20coral%20reef%20images.%20However%2C%20applying%20VQA%20to%20coral%20imagery%0Ademands%20a%20dedicated%20dataset%20that%20addresses%20two%20key%20challenges%3A%20domain-specific%0Aannotations%20and%20multidimensional%20questions.%20In%20this%20work%2C%20we%20introduce%0ACoralVQA%2C%20the%20first%20large-scale%20VQA%20dataset%20for%20coral%20reef%20analysis.%20It%0Acontains%2012%2C805%20real-world%20coral%20images%20from%2067%20coral%20genera%20collected%20from%203%0Aoceans%2C%20along%20with%20277%2C653%20question-answer%20pairs%20that%20comprehensively%20assess%0Aecological%20and%20health-related%20conditions.%20To%20construct%20this%20dataset%2C%20we%20develop%0Aa%20semi-automatic%20data%20construction%20pipeline%20in%20collaboration%20with%20marine%0Abiologists%20to%20ensure%20both%20scalability%20and%20professional-grade%20data%20quality.%0ACoralVQA%20presents%20novel%20challenges%20and%20provides%20a%20comprehensive%20benchmark%20for%0Astudying%20vision-language%20reasoning%20in%20the%20context%20of%20coral%20reef%20images.%20By%0Aevaluating%20several%20state-of-the-art%20LVLMs%2C%20we%20reveal%20key%20limitations%20and%0Aopportunities.%20These%20insights%20form%20a%20foundation%20for%20future%20LVLM%20development%2C%0Awith%20a%20particular%20emphasis%20on%20supporting%20coral%20conservation%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10449v1&entry.124074799=Read"},
{"title": "AudioMAE++: learning better masked audio representations with SwiGLU\n  FFNs", "author": "Sarthak Yadav and Sergios Theodoridis and Zheng-Hua Tan", "abstract": "  Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged\nas a prominent approach for learning self-supervised audio representations.\nWhile several recent papers have evaluated key aspects of training MAEs on\naudio data, the majority of these approaches still leverage vanilla transformer\nbuilding blocks, whereas the transformer community has seen steady integration\nof newer architectural advancements. In this work, we propose AudioMAE++, a\nrevamped audio masked autoencoder with two such enhancements, namely\nmacaron-style transformer blocks with gated linear units. When pretrained on\nthe AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE\nbased approaches on 10 diverse downstream tasks, demonstrating excellent\nperformance on audio classification and speech-based benchmarks. The proposed\nAudioMAE++ models also demonstrate excellent scaling characteristics,\noutperforming directly comparable standard MAE baselines with up to 4x more\nparameters.\n", "link": "http://arxiv.org/abs/2507.10464v1", "date": "2025-07-14", "relevancy": 2.5476, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5114}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5089}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioMAE%2B%2B%3A%20learning%20better%20masked%20audio%20representations%20with%20SwiGLU%0A%20%20FFNs&body=Title%3A%20AudioMAE%2B%2B%3A%20learning%20better%20masked%20audio%20representations%20with%20SwiGLU%0A%20%20FFNs%0AAuthor%3A%20Sarthak%20Yadav%20and%20Sergios%20Theodoridis%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20Masked%20Autoencoders%20%28MAEs%29%20trained%20on%20audio%20spectrogram%20patches%20have%20emerged%0Aas%20a%20prominent%20approach%20for%20learning%20self-supervised%20audio%20representations.%0AWhile%20several%20recent%20papers%20have%20evaluated%20key%20aspects%20of%20training%20MAEs%20on%0Aaudio%20data%2C%20the%20majority%20of%20these%20approaches%20still%20leverage%20vanilla%20transformer%0Abuilding%20blocks%2C%20whereas%20the%20transformer%20community%20has%20seen%20steady%20integration%0Aof%20newer%20architectural%20advancements.%20In%20this%20work%2C%20we%20propose%20AudioMAE%2B%2B%2C%20a%0Arevamped%20audio%20masked%20autoencoder%20with%20two%20such%20enhancements%2C%20namely%0Amacaron-style%20transformer%20blocks%20with%20gated%20linear%20units.%20When%20pretrained%20on%0Athe%20AudioSet%20dataset%2C%20the%20proposed%20AudioMAE%2B%2B%20models%20outperform%20existing%20MAE%0Abased%20approaches%20on%2010%20diverse%20downstream%20tasks%2C%20demonstrating%20excellent%0Aperformance%20on%20audio%20classification%20and%20speech-based%20benchmarks.%20The%20proposed%0AAudioMAE%2B%2B%20models%20also%20demonstrate%20excellent%20scaling%20characteristics%2C%0Aoutperforming%20directly%20comparable%20standard%20MAE%20baselines%20with%20up%20to%204x%20more%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioMAE%252B%252B%253A%2520learning%2520better%2520masked%2520audio%2520representations%2520with%2520SwiGLU%250A%2520%2520FFNs%26entry.906535625%3DSarthak%2520Yadav%2520and%2520Sergios%2520Theodoridis%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520Masked%2520Autoencoders%2520%2528MAEs%2529%2520trained%2520on%2520audio%2520spectrogram%2520patches%2520have%2520emerged%250Aas%2520a%2520prominent%2520approach%2520for%2520learning%2520self-supervised%2520audio%2520representations.%250AWhile%2520several%2520recent%2520papers%2520have%2520evaluated%2520key%2520aspects%2520of%2520training%2520MAEs%2520on%250Aaudio%2520data%252C%2520the%2520majority%2520of%2520these%2520approaches%2520still%2520leverage%2520vanilla%2520transformer%250Abuilding%2520blocks%252C%2520whereas%2520the%2520transformer%2520community%2520has%2520seen%2520steady%2520integration%250Aof%2520newer%2520architectural%2520advancements.%2520In%2520this%2520work%252C%2520we%2520propose%2520AudioMAE%252B%252B%252C%2520a%250Arevamped%2520audio%2520masked%2520autoencoder%2520with%2520two%2520such%2520enhancements%252C%2520namely%250Amacaron-style%2520transformer%2520blocks%2520with%2520gated%2520linear%2520units.%2520When%2520pretrained%2520on%250Athe%2520AudioSet%2520dataset%252C%2520the%2520proposed%2520AudioMAE%252B%252B%2520models%2520outperform%2520existing%2520MAE%250Abased%2520approaches%2520on%252010%2520diverse%2520downstream%2520tasks%252C%2520demonstrating%2520excellent%250Aperformance%2520on%2520audio%2520classification%2520and%2520speech-based%2520benchmarks.%2520The%2520proposed%250AAudioMAE%252B%252B%2520models%2520also%2520demonstrate%2520excellent%2520scaling%2520characteristics%252C%250Aoutperforming%2520directly%2520comparable%2520standard%2520MAE%2520baselines%2520with%2520up%2520to%25204x%2520more%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioMAE%2B%2B%3A%20learning%20better%20masked%20audio%20representations%20with%20SwiGLU%0A%20%20FFNs&entry.906535625=Sarthak%20Yadav%20and%20Sergios%20Theodoridis%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20Masked%20Autoencoders%20%28MAEs%29%20trained%20on%20audio%20spectrogram%20patches%20have%20emerged%0Aas%20a%20prominent%20approach%20for%20learning%20self-supervised%20audio%20representations.%0AWhile%20several%20recent%20papers%20have%20evaluated%20key%20aspects%20of%20training%20MAEs%20on%0Aaudio%20data%2C%20the%20majority%20of%20these%20approaches%20still%20leverage%20vanilla%20transformer%0Abuilding%20blocks%2C%20whereas%20the%20transformer%20community%20has%20seen%20steady%20integration%0Aof%20newer%20architectural%20advancements.%20In%20this%20work%2C%20we%20propose%20AudioMAE%2B%2B%2C%20a%0Arevamped%20audio%20masked%20autoencoder%20with%20two%20such%20enhancements%2C%20namely%0Amacaron-style%20transformer%20blocks%20with%20gated%20linear%20units.%20When%20pretrained%20on%0Athe%20AudioSet%20dataset%2C%20the%20proposed%20AudioMAE%2B%2B%20models%20outperform%20existing%20MAE%0Abased%20approaches%20on%2010%20diverse%20downstream%20tasks%2C%20demonstrating%20excellent%0Aperformance%20on%20audio%20classification%20and%20speech-based%20benchmarks.%20The%20proposed%0AAudioMAE%2B%2B%20models%20also%20demonstrate%20excellent%20scaling%20characteristics%2C%0Aoutperforming%20directly%20comparable%20standard%20MAE%20baselines%20with%20up%20to%204x%20more%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10464v1&entry.124074799=Read"},
{"title": "Show and Polish: Reference-Guided Identity Preservation in Face Video\n  Restoration", "author": "Wenkang Han and Wang Lin and Yiyun Zhou and Qi Liu and Shulei Wang and Chang Yao and Jingyuan Chen", "abstract": "  Face Video Restoration (FVR) aims to recover high-quality face videos from\ndegraded versions. Traditional methods struggle to preserve fine-grained,\nidentity-specific features when degradation is severe, often producing\naverage-looking faces that lack individual characteristics. To address these\nchallenges, we introduce IP-FVR, a novel method that leverages a high-quality\nreference face image as a visual prompt to provide identity conditioning during\nthe denoising process. IP-FVR incorporates semantically rich identity\ninformation from the reference image using decoupled cross-attention\nmechanisms, ensuring detailed and identity consistent results. For intra-clip\nidentity drift (within 24 frames), we introduce an identity-preserving feedback\nlearning method that combines cosine similarity-based reward signals with\nsuffix-weighted temporal aggregation. This approach effectively minimizes drift\nwithin sequences of frames. For inter-clip identity drift, we develop an\nexponential blending strategy that aligns identities across clips by\niteratively blending frames from previous clips during the denoising process.\nThis method ensures consistent identity representation across different clips.\nAdditionally, we enhance the restoration process with a multi-stream negative\nprompt, guiding the model's attention to relevant facial attributes and\nminimizing the generation of low-quality or incorrect features. Extensive\nexperiments on both synthetic and real-world datasets demonstrate that IP-FVR\noutperforms existing methods in both quality and identity preservation,\nshowcasing its substantial potential for practical applications in face video\nrestoration.\n", "link": "http://arxiv.org/abs/2507.10293v1", "date": "2025-07-14", "relevancy": 2.5265, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6519}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6309}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Show%20and%20Polish%3A%20Reference-Guided%20Identity%20Preservation%20in%20Face%20Video%0A%20%20Restoration&body=Title%3A%20Show%20and%20Polish%3A%20Reference-Guided%20Identity%20Preservation%20in%20Face%20Video%0A%20%20Restoration%0AAuthor%3A%20Wenkang%20Han%20and%20Wang%20Lin%20and%20Yiyun%20Zhou%20and%20Qi%20Liu%20and%20Shulei%20Wang%20and%20Chang%20Yao%20and%20Jingyuan%20Chen%0AAbstract%3A%20%20%20Face%20Video%20Restoration%20%28FVR%29%20aims%20to%20recover%20high-quality%20face%20videos%20from%0Adegraded%20versions.%20Traditional%20methods%20struggle%20to%20preserve%20fine-grained%2C%0Aidentity-specific%20features%20when%20degradation%20is%20severe%2C%20often%20producing%0Aaverage-looking%20faces%20that%20lack%20individual%20characteristics.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20IP-FVR%2C%20a%20novel%20method%20that%20leverages%20a%20high-quality%0Areference%20face%20image%20as%20a%20visual%20prompt%20to%20provide%20identity%20conditioning%20during%0Athe%20denoising%20process.%20IP-FVR%20incorporates%20semantically%20rich%20identity%0Ainformation%20from%20the%20reference%20image%20using%20decoupled%20cross-attention%0Amechanisms%2C%20ensuring%20detailed%20and%20identity%20consistent%20results.%20For%20intra-clip%0Aidentity%20drift%20%28within%2024%20frames%29%2C%20we%20introduce%20an%20identity-preserving%20feedback%0Alearning%20method%20that%20combines%20cosine%20similarity-based%20reward%20signals%20with%0Asuffix-weighted%20temporal%20aggregation.%20This%20approach%20effectively%20minimizes%20drift%0Awithin%20sequences%20of%20frames.%20For%20inter-clip%20identity%20drift%2C%20we%20develop%20an%0Aexponential%20blending%20strategy%20that%20aligns%20identities%20across%20clips%20by%0Aiteratively%20blending%20frames%20from%20previous%20clips%20during%20the%20denoising%20process.%0AThis%20method%20ensures%20consistent%20identity%20representation%20across%20different%20clips.%0AAdditionally%2C%20we%20enhance%20the%20restoration%20process%20with%20a%20multi-stream%20negative%0Aprompt%2C%20guiding%20the%20model%27s%20attention%20to%20relevant%20facial%20attributes%20and%0Aminimizing%20the%20generation%20of%20low-quality%20or%20incorrect%20features.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20IP-FVR%0Aoutperforms%20existing%20methods%20in%20both%20quality%20and%20identity%20preservation%2C%0Ashowcasing%20its%20substantial%20potential%20for%20practical%20applications%20in%20face%20video%0Arestoration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShow%2520and%2520Polish%253A%2520Reference-Guided%2520Identity%2520Preservation%2520in%2520Face%2520Video%250A%2520%2520Restoration%26entry.906535625%3DWenkang%2520Han%2520and%2520Wang%2520Lin%2520and%2520Yiyun%2520Zhou%2520and%2520Qi%2520Liu%2520and%2520Shulei%2520Wang%2520and%2520Chang%2520Yao%2520and%2520Jingyuan%2520Chen%26entry.1292438233%3D%2520%2520Face%2520Video%2520Restoration%2520%2528FVR%2529%2520aims%2520to%2520recover%2520high-quality%2520face%2520videos%2520from%250Adegraded%2520versions.%2520Traditional%2520methods%2520struggle%2520to%2520preserve%2520fine-grained%252C%250Aidentity-specific%2520features%2520when%2520degradation%2520is%2520severe%252C%2520often%2520producing%250Aaverage-looking%2520faces%2520that%2520lack%2520individual%2520characteristics.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520IP-FVR%252C%2520a%2520novel%2520method%2520that%2520leverages%2520a%2520high-quality%250Areference%2520face%2520image%2520as%2520a%2520visual%2520prompt%2520to%2520provide%2520identity%2520conditioning%2520during%250Athe%2520denoising%2520process.%2520IP-FVR%2520incorporates%2520semantically%2520rich%2520identity%250Ainformation%2520from%2520the%2520reference%2520image%2520using%2520decoupled%2520cross-attention%250Amechanisms%252C%2520ensuring%2520detailed%2520and%2520identity%2520consistent%2520results.%2520For%2520intra-clip%250Aidentity%2520drift%2520%2528within%252024%2520frames%2529%252C%2520we%2520introduce%2520an%2520identity-preserving%2520feedback%250Alearning%2520method%2520that%2520combines%2520cosine%2520similarity-based%2520reward%2520signals%2520with%250Asuffix-weighted%2520temporal%2520aggregation.%2520This%2520approach%2520effectively%2520minimizes%2520drift%250Awithin%2520sequences%2520of%2520frames.%2520For%2520inter-clip%2520identity%2520drift%252C%2520we%2520develop%2520an%250Aexponential%2520blending%2520strategy%2520that%2520aligns%2520identities%2520across%2520clips%2520by%250Aiteratively%2520blending%2520frames%2520from%2520previous%2520clips%2520during%2520the%2520denoising%2520process.%250AThis%2520method%2520ensures%2520consistent%2520identity%2520representation%2520across%2520different%2520clips.%250AAdditionally%252C%2520we%2520enhance%2520the%2520restoration%2520process%2520with%2520a%2520multi-stream%2520negative%250Aprompt%252C%2520guiding%2520the%2520model%2527s%2520attention%2520to%2520relevant%2520facial%2520attributes%2520and%250Aminimizing%2520the%2520generation%2520of%2520low-quality%2520or%2520incorrect%2520features.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520IP-FVR%250Aoutperforms%2520existing%2520methods%2520in%2520both%2520quality%2520and%2520identity%2520preservation%252C%250Ashowcasing%2520its%2520substantial%2520potential%2520for%2520practical%2520applications%2520in%2520face%2520video%250Arestoration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Show%20and%20Polish%3A%20Reference-Guided%20Identity%20Preservation%20in%20Face%20Video%0A%20%20Restoration&entry.906535625=Wenkang%20Han%20and%20Wang%20Lin%20and%20Yiyun%20Zhou%20and%20Qi%20Liu%20and%20Shulei%20Wang%20and%20Chang%20Yao%20and%20Jingyuan%20Chen&entry.1292438233=%20%20Face%20Video%20Restoration%20%28FVR%29%20aims%20to%20recover%20high-quality%20face%20videos%20from%0Adegraded%20versions.%20Traditional%20methods%20struggle%20to%20preserve%20fine-grained%2C%0Aidentity-specific%20features%20when%20degradation%20is%20severe%2C%20often%20producing%0Aaverage-looking%20faces%20that%20lack%20individual%20characteristics.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20IP-FVR%2C%20a%20novel%20method%20that%20leverages%20a%20high-quality%0Areference%20face%20image%20as%20a%20visual%20prompt%20to%20provide%20identity%20conditioning%20during%0Athe%20denoising%20process.%20IP-FVR%20incorporates%20semantically%20rich%20identity%0Ainformation%20from%20the%20reference%20image%20using%20decoupled%20cross-attention%0Amechanisms%2C%20ensuring%20detailed%20and%20identity%20consistent%20results.%20For%20intra-clip%0Aidentity%20drift%20%28within%2024%20frames%29%2C%20we%20introduce%20an%20identity-preserving%20feedback%0Alearning%20method%20that%20combines%20cosine%20similarity-based%20reward%20signals%20with%0Asuffix-weighted%20temporal%20aggregation.%20This%20approach%20effectively%20minimizes%20drift%0Awithin%20sequences%20of%20frames.%20For%20inter-clip%20identity%20drift%2C%20we%20develop%20an%0Aexponential%20blending%20strategy%20that%20aligns%20identities%20across%20clips%20by%0Aiteratively%20blending%20frames%20from%20previous%20clips%20during%20the%20denoising%20process.%0AThis%20method%20ensures%20consistent%20identity%20representation%20across%20different%20clips.%0AAdditionally%2C%20we%20enhance%20the%20restoration%20process%20with%20a%20multi-stream%20negative%0Aprompt%2C%20guiding%20the%20model%27s%20attention%20to%20relevant%20facial%20attributes%20and%0Aminimizing%20the%20generation%20of%20low-quality%20or%20incorrect%20features.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20IP-FVR%0Aoutperforms%20existing%20methods%20in%20both%20quality%20and%20identity%20preservation%2C%0Ashowcasing%20its%20substantial%20potential%20for%20practical%20applications%20in%20face%20video%0Arestoration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10293v1&entry.124074799=Read"},
{"title": "CLA: Latent Alignment for Online Continual Self-Supervised Learning", "author": "Giacomo Cignoni and Andrea Cossu and Alexandra Gomez-Villa and Joost van de Weijer and Antonio Carta", "abstract": "  Self-supervised learning (SSL) is able to build latent representations that\ngeneralize well to unseen data. However, only a few SSL techniques exist for\nthe online CL setting, where data arrives in small minibatches, the model must\ncomply with a fixed computational budget, and task boundaries are absent. We\nintroduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL\nthat aligns the representations learned by the current model with past\nrepresentations to mitigate forgetting. We found that our CLA is able to speed\nup the convergence of the training process in the online scenario,\noutperforming state-of-the-art approaches under the same computational budget.\nSurprisingly, we also discovered that using CLA as a pretraining protocol in\nthe early stages of pretraining leads to a better final performance when\ncompared to a full i.i.d. pretraining.\n", "link": "http://arxiv.org/abs/2507.10434v1", "date": "2025-07-14", "relevancy": 2.5081, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5248}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5102}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLA%3A%20Latent%20Alignment%20for%20Online%20Continual%20Self-Supervised%20Learning&body=Title%3A%20CLA%3A%20Latent%20Alignment%20for%20Online%20Continual%20Self-Supervised%20Learning%0AAuthor%3A%20Giacomo%20Cignoni%20and%20Andrea%20Cossu%20and%20Alexandra%20Gomez-Villa%20and%20Joost%20van%20de%20Weijer%20and%20Antonio%20Carta%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20is%20able%20to%20build%20latent%20representations%20that%0Ageneralize%20well%20to%20unseen%20data.%20However%2C%20only%20a%20few%20SSL%20techniques%20exist%20for%0Athe%20online%20CL%20setting%2C%20where%20data%20arrives%20in%20small%20minibatches%2C%20the%20model%20must%0Acomply%20with%20a%20fixed%20computational%20budget%2C%20and%20task%20boundaries%20are%20absent.%20We%0Aintroduce%20Continual%20Latent%20Alignment%20%28CLA%29%2C%20a%20novel%20SSL%20strategy%20for%20Online%20CL%0Athat%20aligns%20the%20representations%20learned%20by%20the%20current%20model%20with%20past%0Arepresentations%20to%20mitigate%20forgetting.%20We%20found%20that%20our%20CLA%20is%20able%20to%20speed%0Aup%20the%20convergence%20of%20the%20training%20process%20in%20the%20online%20scenario%2C%0Aoutperforming%20state-of-the-art%20approaches%20under%20the%20same%20computational%20budget.%0ASurprisingly%2C%20we%20also%20discovered%20that%20using%20CLA%20as%20a%20pretraining%20protocol%20in%0Athe%20early%20stages%20of%20pretraining%20leads%20to%20a%20better%20final%20performance%20when%0Acompared%20to%20a%20full%20i.i.d.%20pretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLA%253A%2520Latent%2520Alignment%2520for%2520Online%2520Continual%2520Self-Supervised%2520Learning%26entry.906535625%3DGiacomo%2520Cignoni%2520and%2520Andrea%2520Cossu%2520and%2520Alexandra%2520Gomez-Villa%2520and%2520Joost%2520van%2520de%2520Weijer%2520and%2520Antonio%2520Carta%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520is%2520able%2520to%2520build%2520latent%2520representations%2520that%250Ageneralize%2520well%2520to%2520unseen%2520data.%2520However%252C%2520only%2520a%2520few%2520SSL%2520techniques%2520exist%2520for%250Athe%2520online%2520CL%2520setting%252C%2520where%2520data%2520arrives%2520in%2520small%2520minibatches%252C%2520the%2520model%2520must%250Acomply%2520with%2520a%2520fixed%2520computational%2520budget%252C%2520and%2520task%2520boundaries%2520are%2520absent.%2520We%250Aintroduce%2520Continual%2520Latent%2520Alignment%2520%2528CLA%2529%252C%2520a%2520novel%2520SSL%2520strategy%2520for%2520Online%2520CL%250Athat%2520aligns%2520the%2520representations%2520learned%2520by%2520the%2520current%2520model%2520with%2520past%250Arepresentations%2520to%2520mitigate%2520forgetting.%2520We%2520found%2520that%2520our%2520CLA%2520is%2520able%2520to%2520speed%250Aup%2520the%2520convergence%2520of%2520the%2520training%2520process%2520in%2520the%2520online%2520scenario%252C%250Aoutperforming%2520state-of-the-art%2520approaches%2520under%2520the%2520same%2520computational%2520budget.%250ASurprisingly%252C%2520we%2520also%2520discovered%2520that%2520using%2520CLA%2520as%2520a%2520pretraining%2520protocol%2520in%250Athe%2520early%2520stages%2520of%2520pretraining%2520leads%2520to%2520a%2520better%2520final%2520performance%2520when%250Acompared%2520to%2520a%2520full%2520i.i.d.%2520pretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLA%3A%20Latent%20Alignment%20for%20Online%20Continual%20Self-Supervised%20Learning&entry.906535625=Giacomo%20Cignoni%20and%20Andrea%20Cossu%20and%20Alexandra%20Gomez-Villa%20and%20Joost%20van%20de%20Weijer%20and%20Antonio%20Carta&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20is%20able%20to%20build%20latent%20representations%20that%0Ageneralize%20well%20to%20unseen%20data.%20However%2C%20only%20a%20few%20SSL%20techniques%20exist%20for%0Athe%20online%20CL%20setting%2C%20where%20data%20arrives%20in%20small%20minibatches%2C%20the%20model%20must%0Acomply%20with%20a%20fixed%20computational%20budget%2C%20and%20task%20boundaries%20are%20absent.%20We%0Aintroduce%20Continual%20Latent%20Alignment%20%28CLA%29%2C%20a%20novel%20SSL%20strategy%20for%20Online%20CL%0Athat%20aligns%20the%20representations%20learned%20by%20the%20current%20model%20with%20past%0Arepresentations%20to%20mitigate%20forgetting.%20We%20found%20that%20our%20CLA%20is%20able%20to%20speed%0Aup%20the%20convergence%20of%20the%20training%20process%20in%20the%20online%20scenario%2C%0Aoutperforming%20state-of-the-art%20approaches%20under%20the%20same%20computational%20budget.%0ASurprisingly%2C%20we%20also%20discovered%20that%20using%20CLA%20as%20a%20pretraining%20protocol%20in%0Athe%20early%20stages%20of%20pretraining%20leads%20to%20a%20better%20final%20performance%20when%0Acompared%20to%20a%20full%20i.i.d.%20pretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10434v1&entry.124074799=Read"},
{"title": "DEARLi: Decoupled Enhancement of Recognition and Localization for\n  Semi-supervised Panoptic Segmentation", "author": "Ivan Martinovi\u0107 and Josip \u0160ari\u0107 and Marin Or\u0161i\u0107 and Matej Kristan and Sini\u0161a \u0160egvi\u0107", "abstract": "  Pixel-level annotation is expensive and time-consuming. Semi-supervised\nsegmentation methods address this challenge by learning models on few labeled\nimages alongside a large corpus of unlabeled images. Although foundation models\ncould further account for label scarcity, effective mechanisms for their\nexploitation remain underexplored. We address this by devising a novel\nsemi-supervised panoptic approach fueled by two dedicated foundation models. We\nenhance recognition by complementing unsupervised mask-transformer consistency\nwith zero-shot classification of CLIP features. We enhance localization by\nclass-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting\ndecoupled enhancement of recognition and localization (DEARLi) particularly\nexcels in the most challenging semi-supervised scenarios with large taxonomies\nand limited labeled data. Moreover, DEARLi outperforms the state of the art in\nsemi-supervised semantic segmentation by a large margin while requiring 8x less\nGPU memory, in spite of being trained only for the panoptic objective. We\nobserve 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The\nsource code is available at https://github.com/helen1c/DEARLi.\n", "link": "http://arxiv.org/abs/2507.10118v1", "date": "2025-07-14", "relevancy": 2.4723, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6245}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6187}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEARLi%3A%20Decoupled%20Enhancement%20of%20Recognition%20and%20Localization%20for%0A%20%20Semi-supervised%20Panoptic%20Segmentation&body=Title%3A%20DEARLi%3A%20Decoupled%20Enhancement%20of%20Recognition%20and%20Localization%20for%0A%20%20Semi-supervised%20Panoptic%20Segmentation%0AAuthor%3A%20Ivan%20Martinovi%C4%87%20and%20Josip%20%C5%A0ari%C4%87%20and%20Marin%20Or%C5%A1i%C4%87%20and%20Matej%20Kristan%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87%0AAbstract%3A%20%20%20Pixel-level%20annotation%20is%20expensive%20and%20time-consuming.%20Semi-supervised%0Asegmentation%20methods%20address%20this%20challenge%20by%20learning%20models%20on%20few%20labeled%0Aimages%20alongside%20a%20large%20corpus%20of%20unlabeled%20images.%20Although%20foundation%20models%0Acould%20further%20account%20for%20label%20scarcity%2C%20effective%20mechanisms%20for%20their%0Aexploitation%20remain%20underexplored.%20We%20address%20this%20by%20devising%20a%20novel%0Asemi-supervised%20panoptic%20approach%20fueled%20by%20two%20dedicated%20foundation%20models.%20We%0Aenhance%20recognition%20by%20complementing%20unsupervised%20mask-transformer%20consistency%0Awith%20zero-shot%20classification%20of%20CLIP%20features.%20We%20enhance%20localization%20by%0Aclass-agnostic%20decoder%20warm-up%20with%20respect%20to%20SAM%20pseudo-labels.%20The%20resulting%0Adecoupled%20enhancement%20of%20recognition%20and%20localization%20%28DEARLi%29%20particularly%0Aexcels%20in%20the%20most%20challenging%20semi-supervised%20scenarios%20with%20large%20taxonomies%0Aand%20limited%20labeled%20data.%20Moreover%2C%20DEARLi%20outperforms%20the%20state%20of%20the%20art%20in%0Asemi-supervised%20semantic%20segmentation%20by%20a%20large%20margin%20while%20requiring%208x%20less%0AGPU%20memory%2C%20in%20spite%20of%20being%20trained%20only%20for%20the%20panoptic%20objective.%20We%0Aobserve%2029.9%20PQ%20and%2038.9%20mIoU%20on%20ADE20K%20with%20only%20158%20labeled%20images.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/helen1c/DEARLi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEARLi%253A%2520Decoupled%2520Enhancement%2520of%2520Recognition%2520and%2520Localization%2520for%250A%2520%2520Semi-supervised%2520Panoptic%2520Segmentation%26entry.906535625%3DIvan%2520Martinovi%25C4%2587%2520and%2520Josip%2520%25C5%25A0ari%25C4%2587%2520and%2520Marin%2520Or%25C5%25A1i%25C4%2587%2520and%2520Matej%2520Kristan%2520and%2520Sini%25C5%25A1a%2520%25C5%25A0egvi%25C4%2587%26entry.1292438233%3D%2520%2520Pixel-level%2520annotation%2520is%2520expensive%2520and%2520time-consuming.%2520Semi-supervised%250Asegmentation%2520methods%2520address%2520this%2520challenge%2520by%2520learning%2520models%2520on%2520few%2520labeled%250Aimages%2520alongside%2520a%2520large%2520corpus%2520of%2520unlabeled%2520images.%2520Although%2520foundation%2520models%250Acould%2520further%2520account%2520for%2520label%2520scarcity%252C%2520effective%2520mechanisms%2520for%2520their%250Aexploitation%2520remain%2520underexplored.%2520We%2520address%2520this%2520by%2520devising%2520a%2520novel%250Asemi-supervised%2520panoptic%2520approach%2520fueled%2520by%2520two%2520dedicated%2520foundation%2520models.%2520We%250Aenhance%2520recognition%2520by%2520complementing%2520unsupervised%2520mask-transformer%2520consistency%250Awith%2520zero-shot%2520classification%2520of%2520CLIP%2520features.%2520We%2520enhance%2520localization%2520by%250Aclass-agnostic%2520decoder%2520warm-up%2520with%2520respect%2520to%2520SAM%2520pseudo-labels.%2520The%2520resulting%250Adecoupled%2520enhancement%2520of%2520recognition%2520and%2520localization%2520%2528DEARLi%2529%2520particularly%250Aexcels%2520in%2520the%2520most%2520challenging%2520semi-supervised%2520scenarios%2520with%2520large%2520taxonomies%250Aand%2520limited%2520labeled%2520data.%2520Moreover%252C%2520DEARLi%2520outperforms%2520the%2520state%2520of%2520the%2520art%2520in%250Asemi-supervised%2520semantic%2520segmentation%2520by%2520a%2520large%2520margin%2520while%2520requiring%25208x%2520less%250AGPU%2520memory%252C%2520in%2520spite%2520of%2520being%2520trained%2520only%2520for%2520the%2520panoptic%2520objective.%2520We%250Aobserve%252029.9%2520PQ%2520and%252038.9%2520mIoU%2520on%2520ADE20K%2520with%2520only%2520158%2520labeled%2520images.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/helen1c/DEARLi.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEARLi%3A%20Decoupled%20Enhancement%20of%20Recognition%20and%20Localization%20for%0A%20%20Semi-supervised%20Panoptic%20Segmentation&entry.906535625=Ivan%20Martinovi%C4%87%20and%20Josip%20%C5%A0ari%C4%87%20and%20Marin%20Or%C5%A1i%C4%87%20and%20Matej%20Kristan%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87&entry.1292438233=%20%20Pixel-level%20annotation%20is%20expensive%20and%20time-consuming.%20Semi-supervised%0Asegmentation%20methods%20address%20this%20challenge%20by%20learning%20models%20on%20few%20labeled%0Aimages%20alongside%20a%20large%20corpus%20of%20unlabeled%20images.%20Although%20foundation%20models%0Acould%20further%20account%20for%20label%20scarcity%2C%20effective%20mechanisms%20for%20their%0Aexploitation%20remain%20underexplored.%20We%20address%20this%20by%20devising%20a%20novel%0Asemi-supervised%20panoptic%20approach%20fueled%20by%20two%20dedicated%20foundation%20models.%20We%0Aenhance%20recognition%20by%20complementing%20unsupervised%20mask-transformer%20consistency%0Awith%20zero-shot%20classification%20of%20CLIP%20features.%20We%20enhance%20localization%20by%0Aclass-agnostic%20decoder%20warm-up%20with%20respect%20to%20SAM%20pseudo-labels.%20The%20resulting%0Adecoupled%20enhancement%20of%20recognition%20and%20localization%20%28DEARLi%29%20particularly%0Aexcels%20in%20the%20most%20challenging%20semi-supervised%20scenarios%20with%20large%20taxonomies%0Aand%20limited%20labeled%20data.%20Moreover%2C%20DEARLi%20outperforms%20the%20state%20of%20the%20art%20in%0Asemi-supervised%20semantic%20segmentation%20by%20a%20large%20margin%20while%20requiring%208x%20less%0AGPU%20memory%2C%20in%20spite%20of%20being%20trained%20only%20for%20the%20panoptic%20objective.%20We%0Aobserve%2029.9%20PQ%20and%2038.9%20mIoU%20on%20ADE20K%20with%20only%20158%20labeled%20images.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/helen1c/DEARLi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10118v1&entry.124074799=Read"},
{"title": "Enhanced DeepONet for 1-D consolidation operator learning: an\n  architectural investigation", "author": "Yongjin Choi and Chenying Liu and Jorge Macedo", "abstract": "  Deep Operator Networks (DeepONets) have emerged as a powerful surrogate\nmodeling framework for learning solution operators in PDE-governed systems.\nWhile their use is expanding across engineering disciplines, applications in\ngeotechnical engineering remain limited. This study systematically evaluates\nseveral DeepONet architectures for the one-dimensional consolidation problem.\nWe initially consider three architectures: a standard DeepONet with the\ncoefficient of consolidation embedded in the branch net (Models 1 and 2), and a\nphysics-inspired architecture with the coefficient embedded in the trunk net\n(Model 3). Results show that Model 3 outperforms the standard configurations\n(Models 1 and 2) but still has limitations when the target solution (excess\npore pressures) exhibits significant variation. To overcome this limitation, we\npropose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses\nthe identified limitations by capturing rapidly varying functions. All proposed\narchitectures achieve speedups ranging from 1.5 to 100 times over traditional\nexplicit and implicit solvers, with Model 4 being the most efficient. Larger\ncomputational savings are expected for more complex systems than the explored\n1D case, which is promising. Overall, the study highlights the potential of\nDeepONets to enable efficient, generalizable surrogate modeling in geotechnical\napplications, advancing the integration of scientific machine learning in\ngeotechnics, which is at an early stage.\n", "link": "http://arxiv.org/abs/2507.10368v1", "date": "2025-07-14", "relevancy": 2.4513, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20DeepONet%20for%201-D%20consolidation%20operator%20learning%3A%20an%0A%20%20architectural%20investigation&body=Title%3A%20Enhanced%20DeepONet%20for%201-D%20consolidation%20operator%20learning%3A%20an%0A%20%20architectural%20investigation%0AAuthor%3A%20Yongjin%20Choi%20and%20Chenying%20Liu%20and%20Jorge%20Macedo%0AAbstract%3A%20%20%20Deep%20Operator%20Networks%20%28DeepONets%29%20have%20emerged%20as%20a%20powerful%20surrogate%0Amodeling%20framework%20for%20learning%20solution%20operators%20in%20PDE-governed%20systems.%0AWhile%20their%20use%20is%20expanding%20across%20engineering%20disciplines%2C%20applications%20in%0Ageotechnical%20engineering%20remain%20limited.%20This%20study%20systematically%20evaluates%0Aseveral%20DeepONet%20architectures%20for%20the%20one-dimensional%20consolidation%20problem.%0AWe%20initially%20consider%20three%20architectures%3A%20a%20standard%20DeepONet%20with%20the%0Acoefficient%20of%20consolidation%20embedded%20in%20the%20branch%20net%20%28Models%201%20and%202%29%2C%20and%20a%0Aphysics-inspired%20architecture%20with%20the%20coefficient%20embedded%20in%20the%20trunk%20net%0A%28Model%203%29.%20Results%20show%20that%20Model%203%20outperforms%20the%20standard%20configurations%0A%28Models%201%20and%202%29%20but%20still%20has%20limitations%20when%20the%20target%20solution%20%28excess%0Apore%20pressures%29%20exhibits%20significant%20variation.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20Trunknet%20Fourier%20feature-enhanced%20DeepONet%20%28Model%204%29%20that%20addresses%0Athe%20identified%20limitations%20by%20capturing%20rapidly%20varying%20functions.%20All%20proposed%0Aarchitectures%20achieve%20speedups%20ranging%20from%201.5%20to%20100%20times%20over%20traditional%0Aexplicit%20and%20implicit%20solvers%2C%20with%20Model%204%20being%20the%20most%20efficient.%20Larger%0Acomputational%20savings%20are%20expected%20for%20more%20complex%20systems%20than%20the%20explored%0A1D%20case%2C%20which%20is%20promising.%20Overall%2C%20the%20study%20highlights%20the%20potential%20of%0ADeepONets%20to%20enable%20efficient%2C%20generalizable%20surrogate%20modeling%20in%20geotechnical%0Aapplications%2C%20advancing%20the%20integration%20of%20scientific%20machine%20learning%20in%0Ageotechnics%2C%20which%20is%20at%20an%20early%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520DeepONet%2520for%25201-D%2520consolidation%2520operator%2520learning%253A%2520an%250A%2520%2520architectural%2520investigation%26entry.906535625%3DYongjin%2520Choi%2520and%2520Chenying%2520Liu%2520and%2520Jorge%2520Macedo%26entry.1292438233%3D%2520%2520Deep%2520Operator%2520Networks%2520%2528DeepONets%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520surrogate%250Amodeling%2520framework%2520for%2520learning%2520solution%2520operators%2520in%2520PDE-governed%2520systems.%250AWhile%2520their%2520use%2520is%2520expanding%2520across%2520engineering%2520disciplines%252C%2520applications%2520in%250Ageotechnical%2520engineering%2520remain%2520limited.%2520This%2520study%2520systematically%2520evaluates%250Aseveral%2520DeepONet%2520architectures%2520for%2520the%2520one-dimensional%2520consolidation%2520problem.%250AWe%2520initially%2520consider%2520three%2520architectures%253A%2520a%2520standard%2520DeepONet%2520with%2520the%250Acoefficient%2520of%2520consolidation%2520embedded%2520in%2520the%2520branch%2520net%2520%2528Models%25201%2520and%25202%2529%252C%2520and%2520a%250Aphysics-inspired%2520architecture%2520with%2520the%2520coefficient%2520embedded%2520in%2520the%2520trunk%2520net%250A%2528Model%25203%2529.%2520Results%2520show%2520that%2520Model%25203%2520outperforms%2520the%2520standard%2520configurations%250A%2528Models%25201%2520and%25202%2529%2520but%2520still%2520has%2520limitations%2520when%2520the%2520target%2520solution%2520%2528excess%250Apore%2520pressures%2529%2520exhibits%2520significant%2520variation.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520a%2520Trunknet%2520Fourier%2520feature-enhanced%2520DeepONet%2520%2528Model%25204%2529%2520that%2520addresses%250Athe%2520identified%2520limitations%2520by%2520capturing%2520rapidly%2520varying%2520functions.%2520All%2520proposed%250Aarchitectures%2520achieve%2520speedups%2520ranging%2520from%25201.5%2520to%2520100%2520times%2520over%2520traditional%250Aexplicit%2520and%2520implicit%2520solvers%252C%2520with%2520Model%25204%2520being%2520the%2520most%2520efficient.%2520Larger%250Acomputational%2520savings%2520are%2520expected%2520for%2520more%2520complex%2520systems%2520than%2520the%2520explored%250A1D%2520case%252C%2520which%2520is%2520promising.%2520Overall%252C%2520the%2520study%2520highlights%2520the%2520potential%2520of%250ADeepONets%2520to%2520enable%2520efficient%252C%2520generalizable%2520surrogate%2520modeling%2520in%2520geotechnical%250Aapplications%252C%2520advancing%2520the%2520integration%2520of%2520scientific%2520machine%2520learning%2520in%250Ageotechnics%252C%2520which%2520is%2520at%2520an%2520early%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20DeepONet%20for%201-D%20consolidation%20operator%20learning%3A%20an%0A%20%20architectural%20investigation&entry.906535625=Yongjin%20Choi%20and%20Chenying%20Liu%20and%20Jorge%20Macedo&entry.1292438233=%20%20Deep%20Operator%20Networks%20%28DeepONets%29%20have%20emerged%20as%20a%20powerful%20surrogate%0Amodeling%20framework%20for%20learning%20solution%20operators%20in%20PDE-governed%20systems.%0AWhile%20their%20use%20is%20expanding%20across%20engineering%20disciplines%2C%20applications%20in%0Ageotechnical%20engineering%20remain%20limited.%20This%20study%20systematically%20evaluates%0Aseveral%20DeepONet%20architectures%20for%20the%20one-dimensional%20consolidation%20problem.%0AWe%20initially%20consider%20three%20architectures%3A%20a%20standard%20DeepONet%20with%20the%0Acoefficient%20of%20consolidation%20embedded%20in%20the%20branch%20net%20%28Models%201%20and%202%29%2C%20and%20a%0Aphysics-inspired%20architecture%20with%20the%20coefficient%20embedded%20in%20the%20trunk%20net%0A%28Model%203%29.%20Results%20show%20that%20Model%203%20outperforms%20the%20standard%20configurations%0A%28Models%201%20and%202%29%20but%20still%20has%20limitations%20when%20the%20target%20solution%20%28excess%0Apore%20pressures%29%20exhibits%20significant%20variation.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20Trunknet%20Fourier%20feature-enhanced%20DeepONet%20%28Model%204%29%20that%20addresses%0Athe%20identified%20limitations%20by%20capturing%20rapidly%20varying%20functions.%20All%20proposed%0Aarchitectures%20achieve%20speedups%20ranging%20from%201.5%20to%20100%20times%20over%20traditional%0Aexplicit%20and%20implicit%20solvers%2C%20with%20Model%204%20being%20the%20most%20efficient.%20Larger%0Acomputational%20savings%20are%20expected%20for%20more%20complex%20systems%20than%20the%20explored%0A1D%20case%2C%20which%20is%20promising.%20Overall%2C%20the%20study%20highlights%20the%20potential%20of%0ADeepONets%20to%20enable%20efficient%2C%20generalizable%20surrogate%20modeling%20in%20geotechnical%0Aapplications%2C%20advancing%20the%20integration%20of%20scientific%20machine%20learning%20in%0Ageotechnics%2C%20which%20is%20at%20an%20early%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10368v1&entry.124074799=Read"},
{"title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance\n  on High-Resolution Images", "author": "Jaeseong Lee and Yeeun Choi and Heechan Choi and Hanjung Kim and Seonjoo Kim", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language understanding, reasoning, and generation.\nHowever, they struggle with tasks requiring fine-grained localization and\nreasoning in high-resolution images. This constraint stems from the fact that\nMLLMs are fine-tuned with fixed image resolution to align with the pre-trained\nimage encoder used in MLLM. Consequently, feeding high-resolution images\ndirectly into MLLMs leads to poor generalization due to a train-test resolution\ndiscrepancy, while downsampling these images-although ensuring\nconsistency-compromises fine-grained visual details and ultimately degrades\nperformance. To address this challenge, we propose Extract Candidate then\nPredict (ECP), a novel training-free, task-agnostic two-stage framework\ndesigned to enhance MLLM performance on high-resolution images. The key\nintuition behind ECP is that while MLLMs struggle with high-resolution images,\ntheir predictions on downsampled images still contain implicit localization\ncues. By first identifying candidate region using the coarse prediction and\nthen predicting the final output based on candidate region, ECP effectively\npreserves fine-grained details while mitigating the challenges posed by\nhigh-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K\nMLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared\nto baseline respectively, demonstrating its effectiveness. Code is available at\nhttps://github.com/yenncye/ECP.\n", "link": "http://arxiv.org/abs/2507.10202v1", "date": "2025-07-14", "relevancy": 2.4348, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6173}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6141}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Training-Free%2C%20Task-Agnostic%20Framework%20for%20Enhancing%20MLLM%20Performance%0A%20%20on%20High-Resolution%20Images&body=Title%3A%20A%20Training-Free%2C%20Task-Agnostic%20Framework%20for%20Enhancing%20MLLM%20Performance%0A%20%20on%20High-Resolution%20Images%0AAuthor%3A%20Jaeseong%20Lee%20and%20Yeeun%20Choi%20and%20Heechan%20Choi%20and%20Hanjung%20Kim%20and%20Seonjoo%20Kim%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20vision-language%20understanding%2C%20reasoning%2C%20and%20generation.%0AHowever%2C%20they%20struggle%20with%20tasks%20requiring%20fine-grained%20localization%20and%0Areasoning%20in%20high-resolution%20images.%20This%20constraint%20stems%20from%20the%20fact%20that%0AMLLMs%20are%20fine-tuned%20with%20fixed%20image%20resolution%20to%20align%20with%20the%20pre-trained%0Aimage%20encoder%20used%20in%20MLLM.%20Consequently%2C%20feeding%20high-resolution%20images%0Adirectly%20into%20MLLMs%20leads%20to%20poor%20generalization%20due%20to%20a%20train-test%20resolution%0Adiscrepancy%2C%20while%20downsampling%20these%20images-although%20ensuring%0Aconsistency-compromises%20fine-grained%20visual%20details%20and%20ultimately%20degrades%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20Extract%20Candidate%20then%0APredict%20%28ECP%29%2C%20a%20novel%20training-free%2C%20task-agnostic%20two-stage%20framework%0Adesigned%20to%20enhance%20MLLM%20performance%20on%20high-resolution%20images.%20The%20key%0Aintuition%20behind%20ECP%20is%20that%20while%20MLLMs%20struggle%20with%20high-resolution%20images%2C%0Atheir%20predictions%20on%20downsampled%20images%20still%20contain%20implicit%20localization%0Acues.%20By%20first%20identifying%20candidate%20region%20using%20the%20coarse%20prediction%20and%0Athen%20predicting%20the%20final%20output%20based%20on%20candidate%20region%2C%20ECP%20effectively%0Apreserves%20fine-grained%20details%20while%20mitigating%20the%20challenges%20posed%20by%0Ahigh-resolution%20data.%20We%20validate%20our%20framework%20on%204K%20GUI%20grounding%20and%204K%2C%208K%0AMLLM%20perception%2C%20achieving%20%2B21.3%25%2C%20%2B5.8%25%2C%20%2B5.2%25%20absolute%20improvement%20compared%0Ato%20baseline%20respectively%2C%20demonstrating%20its%20effectiveness.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yenncye/ECP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Training-Free%252C%2520Task-Agnostic%2520Framework%2520for%2520Enhancing%2520MLLM%2520Performance%250A%2520%2520on%2520High-Resolution%2520Images%26entry.906535625%3DJaeseong%2520Lee%2520and%2520Yeeun%2520Choi%2520and%2520Heechan%2520Choi%2520and%2520Hanjung%2520Kim%2520and%2520Seonjoo%2520Kim%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520vision-language%2520understanding%252C%2520reasoning%252C%2520and%2520generation.%250AHowever%252C%2520they%2520struggle%2520with%2520tasks%2520requiring%2520fine-grained%2520localization%2520and%250Areasoning%2520in%2520high-resolution%2520images.%2520This%2520constraint%2520stems%2520from%2520the%2520fact%2520that%250AMLLMs%2520are%2520fine-tuned%2520with%2520fixed%2520image%2520resolution%2520to%2520align%2520with%2520the%2520pre-trained%250Aimage%2520encoder%2520used%2520in%2520MLLM.%2520Consequently%252C%2520feeding%2520high-resolution%2520images%250Adirectly%2520into%2520MLLMs%2520leads%2520to%2520poor%2520generalization%2520due%2520to%2520a%2520train-test%2520resolution%250Adiscrepancy%252C%2520while%2520downsampling%2520these%2520images-although%2520ensuring%250Aconsistency-compromises%2520fine-grained%2520visual%2520details%2520and%2520ultimately%2520degrades%250Aperformance.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Extract%2520Candidate%2520then%250APredict%2520%2528ECP%2529%252C%2520a%2520novel%2520training-free%252C%2520task-agnostic%2520two-stage%2520framework%250Adesigned%2520to%2520enhance%2520MLLM%2520performance%2520on%2520high-resolution%2520images.%2520The%2520key%250Aintuition%2520behind%2520ECP%2520is%2520that%2520while%2520MLLMs%2520struggle%2520with%2520high-resolution%2520images%252C%250Atheir%2520predictions%2520on%2520downsampled%2520images%2520still%2520contain%2520implicit%2520localization%250Acues.%2520By%2520first%2520identifying%2520candidate%2520region%2520using%2520the%2520coarse%2520prediction%2520and%250Athen%2520predicting%2520the%2520final%2520output%2520based%2520on%2520candidate%2520region%252C%2520ECP%2520effectively%250Apreserves%2520fine-grained%2520details%2520while%2520mitigating%2520the%2520challenges%2520posed%2520by%250Ahigh-resolution%2520data.%2520We%2520validate%2520our%2520framework%2520on%25204K%2520GUI%2520grounding%2520and%25204K%252C%25208K%250AMLLM%2520perception%252C%2520achieving%2520%252B21.3%2525%252C%2520%252B5.8%2525%252C%2520%252B5.2%2525%2520absolute%2520improvement%2520compared%250Ato%2520baseline%2520respectively%252C%2520demonstrating%2520its%2520effectiveness.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/yenncye/ECP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Training-Free%2C%20Task-Agnostic%20Framework%20for%20Enhancing%20MLLM%20Performance%0A%20%20on%20High-Resolution%20Images&entry.906535625=Jaeseong%20Lee%20and%20Yeeun%20Choi%20and%20Heechan%20Choi%20and%20Hanjung%20Kim%20and%20Seonjoo%20Kim&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20vision-language%20understanding%2C%20reasoning%2C%20and%20generation.%0AHowever%2C%20they%20struggle%20with%20tasks%20requiring%20fine-grained%20localization%20and%0Areasoning%20in%20high-resolution%20images.%20This%20constraint%20stems%20from%20the%20fact%20that%0AMLLMs%20are%20fine-tuned%20with%20fixed%20image%20resolution%20to%20align%20with%20the%20pre-trained%0Aimage%20encoder%20used%20in%20MLLM.%20Consequently%2C%20feeding%20high-resolution%20images%0Adirectly%20into%20MLLMs%20leads%20to%20poor%20generalization%20due%20to%20a%20train-test%20resolution%0Adiscrepancy%2C%20while%20downsampling%20these%20images-although%20ensuring%0Aconsistency-compromises%20fine-grained%20visual%20details%20and%20ultimately%20degrades%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20Extract%20Candidate%20then%0APredict%20%28ECP%29%2C%20a%20novel%20training-free%2C%20task-agnostic%20two-stage%20framework%0Adesigned%20to%20enhance%20MLLM%20performance%20on%20high-resolution%20images.%20The%20key%0Aintuition%20behind%20ECP%20is%20that%20while%20MLLMs%20struggle%20with%20high-resolution%20images%2C%0Atheir%20predictions%20on%20downsampled%20images%20still%20contain%20implicit%20localization%0Acues.%20By%20first%20identifying%20candidate%20region%20using%20the%20coarse%20prediction%20and%0Athen%20predicting%20the%20final%20output%20based%20on%20candidate%20region%2C%20ECP%20effectively%0Apreserves%20fine-grained%20details%20while%20mitigating%20the%20challenges%20posed%20by%0Ahigh-resolution%20data.%20We%20validate%20our%20framework%20on%204K%20GUI%20grounding%20and%204K%2C%208K%0AMLLM%20perception%2C%20achieving%20%2B21.3%25%2C%20%2B5.8%25%2C%20%2B5.2%25%20absolute%20improvement%20compared%0Ato%20baseline%20respectively%2C%20demonstrating%20its%20effectiveness.%20Code%20is%20available%20at%0Ahttps%3A//github.com/yenncye/ECP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10202v1&entry.124074799=Read"},
{"title": "Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy\n  Distribution", "author": "Lukas Sablica and Kurt Hornik", "abstract": "  We propose a novel variational autoencoder (VAE) architecture that employs a\nspherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian\nlatent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy\nprovides a more natural hyperspherical representation of latent variables,\nbetter capturing directional data while maintaining flexibility. Its\nheavy-tailed nature prevents over-regularization, ensuring efficient latent\nspace utilization while offering a more expressive representation.\nAdditionally, spCauchy circumvents the numerical instabilities inherent to vMF,\nwhich arise from computing normalization constants involving Bessel functions.\nInstead, it enables a fully differentiable and efficient reparameterization\ntrick via M\\\"obius transformations, allowing for stable and scalable training.\nThe KL divergence can be computed through a rapidly converging power series,\neliminating concerns of underflow or overflow associated with evaluation of\nratios of hypergeometric functions. These properties make spCauchy a compelling\nalternative for VAEs, offering both theoretical advantages and practical\nefficiency in high-dimensional generative modeling.\n", "link": "http://arxiv.org/abs/2506.21278v2", "date": "2025-07-14", "relevancy": 2.4346, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5284}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4808}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspherical%20Variational%20Autoencoders%20Using%20Efficient%20Spherical%20Cauchy%0A%20%20Distribution&body=Title%3A%20Hyperspherical%20Variational%20Autoencoders%20Using%20Efficient%20Spherical%20Cauchy%0A%20%20Distribution%0AAuthor%3A%20Lukas%20Sablica%20and%20Kurt%20Hornik%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20variational%20autoencoder%20%28VAE%29%20architecture%20that%20employs%20a%0Aspherical%20Cauchy%20%28spCauchy%29%20latent%20distribution.%20Unlike%20traditional%20Gaussian%0Alatent%20spaces%20or%20the%20widely%20used%20von%20Mises-Fisher%20%28vMF%29%20distribution%2C%20spCauchy%0Aprovides%20a%20more%20natural%20hyperspherical%20representation%20of%20latent%20variables%2C%0Abetter%20capturing%20directional%20data%20while%20maintaining%20flexibility.%20Its%0Aheavy-tailed%20nature%20prevents%20over-regularization%2C%20ensuring%20efficient%20latent%0Aspace%20utilization%20while%20offering%20a%20more%20expressive%20representation.%0AAdditionally%2C%20spCauchy%20circumvents%20the%20numerical%20instabilities%20inherent%20to%20vMF%2C%0Awhich%20arise%20from%20computing%20normalization%20constants%20involving%20Bessel%20functions.%0AInstead%2C%20it%20enables%20a%20fully%20differentiable%20and%20efficient%20reparameterization%0Atrick%20via%20M%5C%22obius%20transformations%2C%20allowing%20for%20stable%20and%20scalable%20training.%0AThe%20KL%20divergence%20can%20be%20computed%20through%20a%20rapidly%20converging%20power%20series%2C%0Aeliminating%20concerns%20of%20underflow%20or%20overflow%20associated%20with%20evaluation%20of%0Aratios%20of%20hypergeometric%20functions.%20These%20properties%20make%20spCauchy%20a%20compelling%0Aalternative%20for%20VAEs%2C%20offering%20both%20theoretical%20advantages%20and%20practical%0Aefficiency%20in%20high-dimensional%20generative%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspherical%2520Variational%2520Autoencoders%2520Using%2520Efficient%2520Spherical%2520Cauchy%250A%2520%2520Distribution%26entry.906535625%3DLukas%2520Sablica%2520and%2520Kurt%2520Hornik%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520variational%2520autoencoder%2520%2528VAE%2529%2520architecture%2520that%2520employs%2520a%250Aspherical%2520Cauchy%2520%2528spCauchy%2529%2520latent%2520distribution.%2520Unlike%2520traditional%2520Gaussian%250Alatent%2520spaces%2520or%2520the%2520widely%2520used%2520von%2520Mises-Fisher%2520%2528vMF%2529%2520distribution%252C%2520spCauchy%250Aprovides%2520a%2520more%2520natural%2520hyperspherical%2520representation%2520of%2520latent%2520variables%252C%250Abetter%2520capturing%2520directional%2520data%2520while%2520maintaining%2520flexibility.%2520Its%250Aheavy-tailed%2520nature%2520prevents%2520over-regularization%252C%2520ensuring%2520efficient%2520latent%250Aspace%2520utilization%2520while%2520offering%2520a%2520more%2520expressive%2520representation.%250AAdditionally%252C%2520spCauchy%2520circumvents%2520the%2520numerical%2520instabilities%2520inherent%2520to%2520vMF%252C%250Awhich%2520arise%2520from%2520computing%2520normalization%2520constants%2520involving%2520Bessel%2520functions.%250AInstead%252C%2520it%2520enables%2520a%2520fully%2520differentiable%2520and%2520efficient%2520reparameterization%250Atrick%2520via%2520M%255C%2522obius%2520transformations%252C%2520allowing%2520for%2520stable%2520and%2520scalable%2520training.%250AThe%2520KL%2520divergence%2520can%2520be%2520computed%2520through%2520a%2520rapidly%2520converging%2520power%2520series%252C%250Aeliminating%2520concerns%2520of%2520underflow%2520or%2520overflow%2520associated%2520with%2520evaluation%2520of%250Aratios%2520of%2520hypergeometric%2520functions.%2520These%2520properties%2520make%2520spCauchy%2520a%2520compelling%250Aalternative%2520for%2520VAEs%252C%2520offering%2520both%2520theoretical%2520advantages%2520and%2520practical%250Aefficiency%2520in%2520high-dimensional%2520generative%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspherical%20Variational%20Autoencoders%20Using%20Efficient%20Spherical%20Cauchy%0A%20%20Distribution&entry.906535625=Lukas%20Sablica%20and%20Kurt%20Hornik&entry.1292438233=%20%20We%20propose%20a%20novel%20variational%20autoencoder%20%28VAE%29%20architecture%20that%20employs%20a%0Aspherical%20Cauchy%20%28spCauchy%29%20latent%20distribution.%20Unlike%20traditional%20Gaussian%0Alatent%20spaces%20or%20the%20widely%20used%20von%20Mises-Fisher%20%28vMF%29%20distribution%2C%20spCauchy%0Aprovides%20a%20more%20natural%20hyperspherical%20representation%20of%20latent%20variables%2C%0Abetter%20capturing%20directional%20data%20while%20maintaining%20flexibility.%20Its%0Aheavy-tailed%20nature%20prevents%20over-regularization%2C%20ensuring%20efficient%20latent%0Aspace%20utilization%20while%20offering%20a%20more%20expressive%20representation.%0AAdditionally%2C%20spCauchy%20circumvents%20the%20numerical%20instabilities%20inherent%20to%20vMF%2C%0Awhich%20arise%20from%20computing%20normalization%20constants%20involving%20Bessel%20functions.%0AInstead%2C%20it%20enables%20a%20fully%20differentiable%20and%20efficient%20reparameterization%0Atrick%20via%20M%5C%22obius%20transformations%2C%20allowing%20for%20stable%20and%20scalable%20training.%0AThe%20KL%20divergence%20can%20be%20computed%20through%20a%20rapidly%20converging%20power%20series%2C%0Aeliminating%20concerns%20of%20underflow%20or%20overflow%20associated%20with%20evaluation%20of%0Aratios%20of%20hypergeometric%20functions.%20These%20properties%20make%20spCauchy%20a%20compelling%0Aalternative%20for%20VAEs%2C%20offering%20both%20theoretical%20advantages%20and%20practical%0Aefficiency%20in%20high-dimensional%20generative%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21278v2&entry.124074799=Read"},
{"title": "On the Robustness Tradeoff in Fine-Tuning", "author": "Kunyang Li and Jean-Charles Noirot Ferrand and Ryan Sheatsley and Blaine Hoak and Yohan Beugin and Eric Pauley and Patrick McDaniel", "abstract": "  Fine-tuning has become the standard practice for adapting pre-trained models\nto downstream tasks. However, the impact on model robustness is not well\nunderstood. In this work, we characterize the robustness-accuracy trade-off in\nfine-tuning. We evaluate the robustness and accuracy of fine-tuned models over\n6 benchmark datasets and 7 different fine-tuning strategies. We observe a\nconsistent trade-off between adversarial robustness and accuracy. Peripheral\nupdates such as BitFit are more effective for simple tasks -- over 75% above\nthe average measured by the area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex tasks\n-- 57.5% and 34.6% above the average on Caltech-256 and CUB-200, respectively.\nLastly, we observe that the robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.\n", "link": "http://arxiv.org/abs/2503.14836v2", "date": "2025-07-14", "relevancy": 2.433, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5022}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4896}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Robustness%20Tradeoff%20in%20Fine-Tuning&body=Title%3A%20On%20the%20Robustness%20Tradeoff%20in%20Fine-Tuning%0AAuthor%3A%20Kunyang%20Li%20and%20Jean-Charles%20Noirot%20Ferrand%20and%20Ryan%20Sheatsley%20and%20Blaine%20Hoak%20and%20Yohan%20Beugin%20and%20Eric%20Pauley%20and%20Patrick%20McDaniel%0AAbstract%3A%20%20%20Fine-tuning%20has%20become%20the%20standard%20practice%20for%20adapting%20pre-trained%20models%0Ato%20downstream%20tasks.%20However%2C%20the%20impact%20on%20model%20robustness%20is%20not%20well%0Aunderstood.%20In%20this%20work%2C%20we%20characterize%20the%20robustness-accuracy%20trade-off%20in%0Afine-tuning.%20We%20evaluate%20the%20robustness%20and%20accuracy%20of%20fine-tuned%20models%20over%0A6%20benchmark%20datasets%20and%207%20different%20fine-tuning%20strategies.%20We%20observe%20a%0Aconsistent%20trade-off%20between%20adversarial%20robustness%20and%20accuracy.%20Peripheral%0Aupdates%20such%20as%20BitFit%20are%20more%20effective%20for%20simple%20tasks%20--%20over%2075%25%20above%0Athe%20average%20measured%20by%20the%20area%20under%20the%20Pareto%20frontiers%20on%20CIFAR-10%20and%0ACIFAR-100.%20In%20contrast%2C%20fine-tuning%20information-heavy%20layers%2C%20such%20as%20attention%0Alayers%20via%20Compacter%2C%20achieves%20a%20better%20Pareto%20frontier%20on%20more%20complex%20tasks%0A--%2057.5%25%20and%2034.6%25%20above%20the%20average%20on%20Caltech-256%20and%20CUB-200%2C%20respectively.%0ALastly%2C%20we%20observe%20that%20the%20robustness%20of%20fine-tuning%20against%0Aout-of-distribution%20data%20closely%20tracks%20accuracy.%20These%20insights%20emphasize%20the%0Aneed%20for%20robustness-aware%20fine-tuning%20to%20ensure%20reliable%20real-world%0Adeployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Robustness%2520Tradeoff%2520in%2520Fine-Tuning%26entry.906535625%3DKunyang%2520Li%2520and%2520Jean-Charles%2520Noirot%2520Ferrand%2520and%2520Ryan%2520Sheatsley%2520and%2520Blaine%2520Hoak%2520and%2520Yohan%2520Beugin%2520and%2520Eric%2520Pauley%2520and%2520Patrick%2520McDaniel%26entry.1292438233%3D%2520%2520Fine-tuning%2520has%2520become%2520the%2520standard%2520practice%2520for%2520adapting%2520pre-trained%2520models%250Ato%2520downstream%2520tasks.%2520However%252C%2520the%2520impact%2520on%2520model%2520robustness%2520is%2520not%2520well%250Aunderstood.%2520In%2520this%2520work%252C%2520we%2520characterize%2520the%2520robustness-accuracy%2520trade-off%2520in%250Afine-tuning.%2520We%2520evaluate%2520the%2520robustness%2520and%2520accuracy%2520of%2520fine-tuned%2520models%2520over%250A6%2520benchmark%2520datasets%2520and%25207%2520different%2520fine-tuning%2520strategies.%2520We%2520observe%2520a%250Aconsistent%2520trade-off%2520between%2520adversarial%2520robustness%2520and%2520accuracy.%2520Peripheral%250Aupdates%2520such%2520as%2520BitFit%2520are%2520more%2520effective%2520for%2520simple%2520tasks%2520--%2520over%252075%2525%2520above%250Athe%2520average%2520measured%2520by%2520the%2520area%2520under%2520the%2520Pareto%2520frontiers%2520on%2520CIFAR-10%2520and%250ACIFAR-100.%2520In%2520contrast%252C%2520fine-tuning%2520information-heavy%2520layers%252C%2520such%2520as%2520attention%250Alayers%2520via%2520Compacter%252C%2520achieves%2520a%2520better%2520Pareto%2520frontier%2520on%2520more%2520complex%2520tasks%250A--%252057.5%2525%2520and%252034.6%2525%2520above%2520the%2520average%2520on%2520Caltech-256%2520and%2520CUB-200%252C%2520respectively.%250ALastly%252C%2520we%2520observe%2520that%2520the%2520robustness%2520of%2520fine-tuning%2520against%250Aout-of-distribution%2520data%2520closely%2520tracks%2520accuracy.%2520These%2520insights%2520emphasize%2520the%250Aneed%2520for%2520robustness-aware%2520fine-tuning%2520to%2520ensure%2520reliable%2520real-world%250Adeployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Robustness%20Tradeoff%20in%20Fine-Tuning&entry.906535625=Kunyang%20Li%20and%20Jean-Charles%20Noirot%20Ferrand%20and%20Ryan%20Sheatsley%20and%20Blaine%20Hoak%20and%20Yohan%20Beugin%20and%20Eric%20Pauley%20and%20Patrick%20McDaniel&entry.1292438233=%20%20Fine-tuning%20has%20become%20the%20standard%20practice%20for%20adapting%20pre-trained%20models%0Ato%20downstream%20tasks.%20However%2C%20the%20impact%20on%20model%20robustness%20is%20not%20well%0Aunderstood.%20In%20this%20work%2C%20we%20characterize%20the%20robustness-accuracy%20trade-off%20in%0Afine-tuning.%20We%20evaluate%20the%20robustness%20and%20accuracy%20of%20fine-tuned%20models%20over%0A6%20benchmark%20datasets%20and%207%20different%20fine-tuning%20strategies.%20We%20observe%20a%0Aconsistent%20trade-off%20between%20adversarial%20robustness%20and%20accuracy.%20Peripheral%0Aupdates%20such%20as%20BitFit%20are%20more%20effective%20for%20simple%20tasks%20--%20over%2075%25%20above%0Athe%20average%20measured%20by%20the%20area%20under%20the%20Pareto%20frontiers%20on%20CIFAR-10%20and%0ACIFAR-100.%20In%20contrast%2C%20fine-tuning%20information-heavy%20layers%2C%20such%20as%20attention%0Alayers%20via%20Compacter%2C%20achieves%20a%20better%20Pareto%20frontier%20on%20more%20complex%20tasks%0A--%2057.5%25%20and%2034.6%25%20above%20the%20average%20on%20Caltech-256%20and%20CUB-200%2C%20respectively.%0ALastly%2C%20we%20observe%20that%20the%20robustness%20of%20fine-tuning%20against%0Aout-of-distribution%20data%20closely%20tracks%20accuracy.%20These%20insights%20emphasize%20the%0Aneed%20for%20robustness-aware%20fine-tuning%20to%20ensure%20reliable%20real-world%0Adeployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14836v2&entry.124074799=Read"},
{"title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination", "author": "Mingqi Wu and Zhihao Zhang and Qiaole Dong and Zhiheng Xi and Jun Zhao and Senjie Jin and Xiaoran Fan and Yuhao Zhou and Yanwei Fu and Qin Liu and Songyang Zhang and Qi Zhang", "abstract": "  The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.\n", "link": "http://arxiv.org/abs/2507.10532v1", "date": "2025-07-14", "relevancy": 2.4318, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20or%20Memorization%3F%20Unreliable%20Results%20of%20Reinforcement%20Learning%0A%20%20Due%20to%20Data%20Contamination&body=Title%3A%20Reasoning%20or%20Memorization%3F%20Unreliable%20Results%20of%20Reinforcement%20Learning%0A%20%20Due%20to%20Data%20Contamination%0AAuthor%3A%20Mingqi%20Wu%20and%20Zhihao%20Zhang%20and%20Qiaole%20Dong%20and%20Zhiheng%20Xi%20and%20Jun%20Zhao%20and%20Senjie%20Jin%20and%20Xiaoran%20Fan%20and%20Yuhao%20Zhou%20and%20Yanwei%20Fu%20and%20Qin%20Liu%20and%20Songyang%20Zhang%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20The%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%20been%20a%0Alongstanding%20focus%20of%20research.%20Recent%20works%20have%20further%20enhanced%20these%0Acapabilities%20using%20reinforcement%20learning%20%28RL%29%2C%20with%20many%20new%20methods%20claiming%0Asignificant%20improvements%20with%20minimal%20or%20no%20external%20supervision.%20Surprisingly%2C%0Asome%20studies%20even%20suggest%20that%20random%20or%20incorrect%20reward%20signals%20can%20enhance%0Areasoning%20performance.%20However%2C%20these%20breakthroughs%20are%20mostly%20reported%20on%20the%0AQwen2.5%20model%20family%20and%20evaluated%20on%20well-known%20benchmarks%20such%20as%20MATH-500%2C%0AAMC%2C%20and%20AIME%2C%20while%20failing%20to%20achieve%20similar%20gains%20on%20other%20models%20like%0ALlama%2C%20which%20warrants%20further%20investigation.%20Our%20analysis%20shows%20that%20although%0AQwen2.5%20achieves%20strong%20mathematical%20reasoning%20performance%2C%20its%20pretraining%20on%0Alarge-scale%20web%20corpora%20makes%20it%20vulnerable%20to%20data%20contamination%20in%20popular%0Abenchmarks.%20As%20a%20result%2C%20results%20derived%20from%20these%20benchmarks%20may%20be%0Aunreliable.%20To%20address%20this%2C%20we%20introduce%20a%20generator%20that%20produces%20fully%0Asynthetic%20arithmetic%20problems%20of%20arbitrary%20length%20and%20difficulty%2C%20yielding%20a%0Aclean%20dataset%20we%20call%20RandomCalculation.%20Using%20these%20leakage-free%20datasets%2C%20we%0Ashow%20that%20only%20accurate%20reward%20signals%20consistently%20improve%20performance%2C%20while%0Anoisy%20or%20incorrect%20signals%20do%20not.%20We%20advocate%20for%20evaluating%20RL%20methods%20on%0Auncontaminated%20benchmarks%20and%20across%20diverse%20model%20families%20to%20ensure%0Atrustworthy%20conclusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520or%2520Memorization%253F%2520Unreliable%2520Results%2520of%2520Reinforcement%2520Learning%250A%2520%2520Due%2520to%2520Data%2520Contamination%26entry.906535625%3DMingqi%2520Wu%2520and%2520Zhihao%2520Zhang%2520and%2520Qiaole%2520Dong%2520and%2520Zhiheng%2520Xi%2520and%2520Jun%2520Zhao%2520and%2520Senjie%2520Jin%2520and%2520Xiaoran%2520Fan%2520and%2520Yuhao%2520Zhou%2520and%2520Yanwei%2520Fu%2520and%2520Qin%2520Liu%2520and%2520Songyang%2520Zhang%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520The%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520a%250Alongstanding%2520focus%2520of%2520research.%2520Recent%2520works%2520have%2520further%2520enhanced%2520these%250Acapabilities%2520using%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520with%2520many%2520new%2520methods%2520claiming%250Asignificant%2520improvements%2520with%2520minimal%2520or%2520no%2520external%2520supervision.%2520Surprisingly%252C%250Asome%2520studies%2520even%2520suggest%2520that%2520random%2520or%2520incorrect%2520reward%2520signals%2520can%2520enhance%250Areasoning%2520performance.%2520However%252C%2520these%2520breakthroughs%2520are%2520mostly%2520reported%2520on%2520the%250AQwen2.5%2520model%2520family%2520and%2520evaluated%2520on%2520well-known%2520benchmarks%2520such%2520as%2520MATH-500%252C%250AAMC%252C%2520and%2520AIME%252C%2520while%2520failing%2520to%2520achieve%2520similar%2520gains%2520on%2520other%2520models%2520like%250ALlama%252C%2520which%2520warrants%2520further%2520investigation.%2520Our%2520analysis%2520shows%2520that%2520although%250AQwen2.5%2520achieves%2520strong%2520mathematical%2520reasoning%2520performance%252C%2520its%2520pretraining%2520on%250Alarge-scale%2520web%2520corpora%2520makes%2520it%2520vulnerable%2520to%2520data%2520contamination%2520in%2520popular%250Abenchmarks.%2520As%2520a%2520result%252C%2520results%2520derived%2520from%2520these%2520benchmarks%2520may%2520be%250Aunreliable.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520generator%2520that%2520produces%2520fully%250Asynthetic%2520arithmetic%2520problems%2520of%2520arbitrary%2520length%2520and%2520difficulty%252C%2520yielding%2520a%250Aclean%2520dataset%2520we%2520call%2520RandomCalculation.%2520Using%2520these%2520leakage-free%2520datasets%252C%2520we%250Ashow%2520that%2520only%2520accurate%2520reward%2520signals%2520consistently%2520improve%2520performance%252C%2520while%250Anoisy%2520or%2520incorrect%2520signals%2520do%2520not.%2520We%2520advocate%2520for%2520evaluating%2520RL%2520methods%2520on%250Auncontaminated%2520benchmarks%2520and%2520across%2520diverse%2520model%2520families%2520to%2520ensure%250Atrustworthy%2520conclusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20or%20Memorization%3F%20Unreliable%20Results%20of%20Reinforcement%20Learning%0A%20%20Due%20to%20Data%20Contamination&entry.906535625=Mingqi%20Wu%20and%20Zhihao%20Zhang%20and%20Qiaole%20Dong%20and%20Zhiheng%20Xi%20and%20Jun%20Zhao%20and%20Senjie%20Jin%20and%20Xiaoran%20Fan%20and%20Yuhao%20Zhou%20and%20Yanwei%20Fu%20and%20Qin%20Liu%20and%20Songyang%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20The%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%20been%20a%0Alongstanding%20focus%20of%20research.%20Recent%20works%20have%20further%20enhanced%20these%0Acapabilities%20using%20reinforcement%20learning%20%28RL%29%2C%20with%20many%20new%20methods%20claiming%0Asignificant%20improvements%20with%20minimal%20or%20no%20external%20supervision.%20Surprisingly%2C%0Asome%20studies%20even%20suggest%20that%20random%20or%20incorrect%20reward%20signals%20can%20enhance%0Areasoning%20performance.%20However%2C%20these%20breakthroughs%20are%20mostly%20reported%20on%20the%0AQwen2.5%20model%20family%20and%20evaluated%20on%20well-known%20benchmarks%20such%20as%20MATH-500%2C%0AAMC%2C%20and%20AIME%2C%20while%20failing%20to%20achieve%20similar%20gains%20on%20other%20models%20like%0ALlama%2C%20which%20warrants%20further%20investigation.%20Our%20analysis%20shows%20that%20although%0AQwen2.5%20achieves%20strong%20mathematical%20reasoning%20performance%2C%20its%20pretraining%20on%0Alarge-scale%20web%20corpora%20makes%20it%20vulnerable%20to%20data%20contamination%20in%20popular%0Abenchmarks.%20As%20a%20result%2C%20results%20derived%20from%20these%20benchmarks%20may%20be%0Aunreliable.%20To%20address%20this%2C%20we%20introduce%20a%20generator%20that%20produces%20fully%0Asynthetic%20arithmetic%20problems%20of%20arbitrary%20length%20and%20difficulty%2C%20yielding%20a%0Aclean%20dataset%20we%20call%20RandomCalculation.%20Using%20these%20leakage-free%20datasets%2C%20we%0Ashow%20that%20only%20accurate%20reward%20signals%20consistently%20improve%20performance%2C%20while%0Anoisy%20or%20incorrect%20signals%20do%20not.%20We%20advocate%20for%20evaluating%20RL%20methods%20on%0Auncontaminated%20benchmarks%20and%20across%20diverse%20model%20families%20to%20ensure%0Atrustworthy%20conclusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10532v1&entry.124074799=Read"},
{"title": "Cameras as Relative Positional Encoding", "author": "Ruilong Li and Brent Yi and Junchen Liu and Hang Gao and Yi Ma and Angjoo Kanazawa", "abstract": "  Transformers are increasingly prevalent for multi-view computer vision tasks,\nwhere geometric relationships between viewpoints are critical for 3D\nperception. To leverage these relationships, multi-view transformers must use\ncamera geometry to ground visual tokens in 3D space. In this work, we compare\ntechniques for conditioning transformers on cameras: token-level raymap\nencodings, attention-level relative pose encodings, and a new relative encoding\nwe propose -- Projective Positional Encoding (PRoPE) -- that captures complete\ncamera frustums, both intrinsics and extrinsics, as a relative positional\nencoding. Our experiments begin by showing how relative camera conditioning\nimproves performance in feedforward novel view synthesis, with further gains\nfrom PRoPE. This holds across settings: scenes with both shared and varying\nintrinsics, when combining token- and attention-level conditioning, and for\ngeneralization to inputs with out-of-distribution sequence lengths and camera\nintrinsics. We then verify that these benefits persist for different tasks,\nstereo depth estimation and discriminative spatial cognition, as well as larger\nmodel sizes.\n", "link": "http://arxiv.org/abs/2507.10496v1", "date": "2025-07-14", "relevancy": 2.4297, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cameras%20as%20Relative%20Positional%20Encoding&body=Title%3A%20Cameras%20as%20Relative%20Positional%20Encoding%0AAuthor%3A%20Ruilong%20Li%20and%20Brent%20Yi%20and%20Junchen%20Liu%20and%20Hang%20Gao%20and%20Yi%20Ma%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20Transformers%20are%20increasingly%20prevalent%20for%20multi-view%20computer%20vision%20tasks%2C%0Awhere%20geometric%20relationships%20between%20viewpoints%20are%20critical%20for%203D%0Aperception.%20To%20leverage%20these%20relationships%2C%20multi-view%20transformers%20must%20use%0Acamera%20geometry%20to%20ground%20visual%20tokens%20in%203D%20space.%20In%20this%20work%2C%20we%20compare%0Atechniques%20for%20conditioning%20transformers%20on%20cameras%3A%20token-level%20raymap%0Aencodings%2C%20attention-level%20relative%20pose%20encodings%2C%20and%20a%20new%20relative%20encoding%0Awe%20propose%20--%20Projective%20Positional%20Encoding%20%28PRoPE%29%20--%20that%20captures%20complete%0Acamera%20frustums%2C%20both%20intrinsics%20and%20extrinsics%2C%20as%20a%20relative%20positional%0Aencoding.%20Our%20experiments%20begin%20by%20showing%20how%20relative%20camera%20conditioning%0Aimproves%20performance%20in%20feedforward%20novel%20view%20synthesis%2C%20with%20further%20gains%0Afrom%20PRoPE.%20This%20holds%20across%20settings%3A%20scenes%20with%20both%20shared%20and%20varying%0Aintrinsics%2C%20when%20combining%20token-%20and%20attention-level%20conditioning%2C%20and%20for%0Ageneralization%20to%20inputs%20with%20out-of-distribution%20sequence%20lengths%20and%20camera%0Aintrinsics.%20We%20then%20verify%20that%20these%20benefits%20persist%20for%20different%20tasks%2C%0Astereo%20depth%20estimation%20and%20discriminative%20spatial%20cognition%2C%20as%20well%20as%20larger%0Amodel%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCameras%2520as%2520Relative%2520Positional%2520Encoding%26entry.906535625%3DRuilong%2520Li%2520and%2520Brent%2520Yi%2520and%2520Junchen%2520Liu%2520and%2520Hang%2520Gao%2520and%2520Yi%2520Ma%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520Transformers%2520are%2520increasingly%2520prevalent%2520for%2520multi-view%2520computer%2520vision%2520tasks%252C%250Awhere%2520geometric%2520relationships%2520between%2520viewpoints%2520are%2520critical%2520for%25203D%250Aperception.%2520To%2520leverage%2520these%2520relationships%252C%2520multi-view%2520transformers%2520must%2520use%250Acamera%2520geometry%2520to%2520ground%2520visual%2520tokens%2520in%25203D%2520space.%2520In%2520this%2520work%252C%2520we%2520compare%250Atechniques%2520for%2520conditioning%2520transformers%2520on%2520cameras%253A%2520token-level%2520raymap%250Aencodings%252C%2520attention-level%2520relative%2520pose%2520encodings%252C%2520and%2520a%2520new%2520relative%2520encoding%250Awe%2520propose%2520--%2520Projective%2520Positional%2520Encoding%2520%2528PRoPE%2529%2520--%2520that%2520captures%2520complete%250Acamera%2520frustums%252C%2520both%2520intrinsics%2520and%2520extrinsics%252C%2520as%2520a%2520relative%2520positional%250Aencoding.%2520Our%2520experiments%2520begin%2520by%2520showing%2520how%2520relative%2520camera%2520conditioning%250Aimproves%2520performance%2520in%2520feedforward%2520novel%2520view%2520synthesis%252C%2520with%2520further%2520gains%250Afrom%2520PRoPE.%2520This%2520holds%2520across%2520settings%253A%2520scenes%2520with%2520both%2520shared%2520and%2520varying%250Aintrinsics%252C%2520when%2520combining%2520token-%2520and%2520attention-level%2520conditioning%252C%2520and%2520for%250Ageneralization%2520to%2520inputs%2520with%2520out-of-distribution%2520sequence%2520lengths%2520and%2520camera%250Aintrinsics.%2520We%2520then%2520verify%2520that%2520these%2520benefits%2520persist%2520for%2520different%2520tasks%252C%250Astereo%2520depth%2520estimation%2520and%2520discriminative%2520spatial%2520cognition%252C%2520as%2520well%2520as%2520larger%250Amodel%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cameras%20as%20Relative%20Positional%20Encoding&entry.906535625=Ruilong%20Li%20and%20Brent%20Yi%20and%20Junchen%20Liu%20and%20Hang%20Gao%20and%20Yi%20Ma%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20Transformers%20are%20increasingly%20prevalent%20for%20multi-view%20computer%20vision%20tasks%2C%0Awhere%20geometric%20relationships%20between%20viewpoints%20are%20critical%20for%203D%0Aperception.%20To%20leverage%20these%20relationships%2C%20multi-view%20transformers%20must%20use%0Acamera%20geometry%20to%20ground%20visual%20tokens%20in%203D%20space.%20In%20this%20work%2C%20we%20compare%0Atechniques%20for%20conditioning%20transformers%20on%20cameras%3A%20token-level%20raymap%0Aencodings%2C%20attention-level%20relative%20pose%20encodings%2C%20and%20a%20new%20relative%20encoding%0Awe%20propose%20--%20Projective%20Positional%20Encoding%20%28PRoPE%29%20--%20that%20captures%20complete%0Acamera%20frustums%2C%20both%20intrinsics%20and%20extrinsics%2C%20as%20a%20relative%20positional%0Aencoding.%20Our%20experiments%20begin%20by%20showing%20how%20relative%20camera%20conditioning%0Aimproves%20performance%20in%20feedforward%20novel%20view%20synthesis%2C%20with%20further%20gains%0Afrom%20PRoPE.%20This%20holds%20across%20settings%3A%20scenes%20with%20both%20shared%20and%20varying%0Aintrinsics%2C%20when%20combining%20token-%20and%20attention-level%20conditioning%2C%20and%20for%0Ageneralization%20to%20inputs%20with%20out-of-distribution%20sequence%20lengths%20and%20camera%0Aintrinsics.%20We%20then%20verify%20that%20these%20benefits%20persist%20for%20different%20tasks%2C%0Astereo%20depth%20estimation%20and%20discriminative%20spatial%20cognition%2C%20as%20well%20as%20larger%0Amodel%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10496v1&entry.124074799=Read"},
{"title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level\n  Controllable Human Image Generation", "author": "Jeongho Kim and Sunghyun Park and Hyoungwoo Park and Sungrack Yun and Jaegul Choo and Seokeon Cho", "abstract": "  Recent diffusion models achieve personalization by learning specific\nsubjects, allowing learned attributes to be integrated into generated images.\nHowever, personalized human image generation remains challenging due to the\nneed for precise and consistent attribute preservation (e.g., identity,\nclothing details). Existing subject-driven image generation methods often\nrequire either (1) inference-time fine-tuning with few images for each new\nsubject or (2) large-scale dataset training for generalization. Both approaches\nare computationally expensive and impractical for real-time applications. To\naddress these limitations, we present Wardrobe Polyptych LoRA, a novel\npart-level controllable model for personalized human image generation. By\ntraining only LoRA layers, our method removes the computational burden at\ninference while ensuring high-fidelity synthesis of unseen subjects. Our key\nidea is to condition the generation on the subject's wardrobe and leverage\nspatial references to reduce information loss, thereby improving fidelity and\nconsistency. Additionally, we introduce a selective subject region loss, which\nencourages the model to disregard some of reference images during training. Our\nloss ensures that generated images better align with text prompts while\nmaintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no\nadditional parameters at the inference stage and performs generation using a\nsingle model trained on a few training samples. We construct a new dataset and\nbenchmark tailored for personalized human image generation. Extensive\nexperiments show that our approach significantly outperforms existing\ntechniques in fidelity and consistency, enabling realistic and\nidentity-preserving full-body synthesis.\n", "link": "http://arxiv.org/abs/2507.10217v1", "date": "2025-07-14", "relevancy": 2.4057, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6061}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6058}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Wardrobe%20to%20Canvas%3A%20Wardrobe%20Polyptych%20LoRA%20for%20Part-level%0A%20%20Controllable%20Human%20Image%20Generation&body=Title%3A%20From%20Wardrobe%20to%20Canvas%3A%20Wardrobe%20Polyptych%20LoRA%20for%20Part-level%0A%20%20Controllable%20Human%20Image%20Generation%0AAuthor%3A%20Jeongho%20Kim%20and%20Sunghyun%20Park%20and%20Hyoungwoo%20Park%20and%20Sungrack%20Yun%20and%20Jaegul%20Choo%20and%20Seokeon%20Cho%0AAbstract%3A%20%20%20Recent%20diffusion%20models%20achieve%20personalization%20by%20learning%20specific%0Asubjects%2C%20allowing%20learned%20attributes%20to%20be%20integrated%20into%20generated%20images.%0AHowever%2C%20personalized%20human%20image%20generation%20remains%20challenging%20due%20to%20the%0Aneed%20for%20precise%20and%20consistent%20attribute%20preservation%20%28e.g.%2C%20identity%2C%0Aclothing%20details%29.%20Existing%20subject-driven%20image%20generation%20methods%20often%0Arequire%20either%20%281%29%20inference-time%20fine-tuning%20with%20few%20images%20for%20each%20new%0Asubject%20or%20%282%29%20large-scale%20dataset%20training%20for%20generalization.%20Both%20approaches%0Aare%20computationally%20expensive%20and%20impractical%20for%20real-time%20applications.%20To%0Aaddress%20these%20limitations%2C%20we%20present%20Wardrobe%20Polyptych%20LoRA%2C%20a%20novel%0Apart-level%20controllable%20model%20for%20personalized%20human%20image%20generation.%20By%0Atraining%20only%20LoRA%20layers%2C%20our%20method%20removes%20the%20computational%20burden%20at%0Ainference%20while%20ensuring%20high-fidelity%20synthesis%20of%20unseen%20subjects.%20Our%20key%0Aidea%20is%20to%20condition%20the%20generation%20on%20the%20subject%27s%20wardrobe%20and%20leverage%0Aspatial%20references%20to%20reduce%20information%20loss%2C%20thereby%20improving%20fidelity%20and%0Aconsistency.%20Additionally%2C%20we%20introduce%20a%20selective%20subject%20region%20loss%2C%20which%0Aencourages%20the%20model%20to%20disregard%20some%20of%20reference%20images%20during%20training.%20Our%0Aloss%20ensures%20that%20generated%20images%20better%20align%20with%20text%20prompts%20while%0Amaintaining%20subject%20integrity.%20Notably%2C%20our%20Wardrobe%20Polyptych%20LoRA%20requires%20no%0Aadditional%20parameters%20at%20the%20inference%20stage%20and%20performs%20generation%20using%20a%0Asingle%20model%20trained%20on%20a%20few%20training%20samples.%20We%20construct%20a%20new%20dataset%20and%0Abenchmark%20tailored%20for%20personalized%20human%20image%20generation.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20significantly%20outperforms%20existing%0Atechniques%20in%20fidelity%20and%20consistency%2C%20enabling%20realistic%20and%0Aidentity-preserving%20full-body%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Wardrobe%2520to%2520Canvas%253A%2520Wardrobe%2520Polyptych%2520LoRA%2520for%2520Part-level%250A%2520%2520Controllable%2520Human%2520Image%2520Generation%26entry.906535625%3DJeongho%2520Kim%2520and%2520Sunghyun%2520Park%2520and%2520Hyoungwoo%2520Park%2520and%2520Sungrack%2520Yun%2520and%2520Jaegul%2520Choo%2520and%2520Seokeon%2520Cho%26entry.1292438233%3D%2520%2520Recent%2520diffusion%2520models%2520achieve%2520personalization%2520by%2520learning%2520specific%250Asubjects%252C%2520allowing%2520learned%2520attributes%2520to%2520be%2520integrated%2520into%2520generated%2520images.%250AHowever%252C%2520personalized%2520human%2520image%2520generation%2520remains%2520challenging%2520due%2520to%2520the%250Aneed%2520for%2520precise%2520and%2520consistent%2520attribute%2520preservation%2520%2528e.g.%252C%2520identity%252C%250Aclothing%2520details%2529.%2520Existing%2520subject-driven%2520image%2520generation%2520methods%2520often%250Arequire%2520either%2520%25281%2529%2520inference-time%2520fine-tuning%2520with%2520few%2520images%2520for%2520each%2520new%250Asubject%2520or%2520%25282%2529%2520large-scale%2520dataset%2520training%2520for%2520generalization.%2520Both%2520approaches%250Aare%2520computationally%2520expensive%2520and%2520impractical%2520for%2520real-time%2520applications.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520present%2520Wardrobe%2520Polyptych%2520LoRA%252C%2520a%2520novel%250Apart-level%2520controllable%2520model%2520for%2520personalized%2520human%2520image%2520generation.%2520By%250Atraining%2520only%2520LoRA%2520layers%252C%2520our%2520method%2520removes%2520the%2520computational%2520burden%2520at%250Ainference%2520while%2520ensuring%2520high-fidelity%2520synthesis%2520of%2520unseen%2520subjects.%2520Our%2520key%250Aidea%2520is%2520to%2520condition%2520the%2520generation%2520on%2520the%2520subject%2527s%2520wardrobe%2520and%2520leverage%250Aspatial%2520references%2520to%2520reduce%2520information%2520loss%252C%2520thereby%2520improving%2520fidelity%2520and%250Aconsistency.%2520Additionally%252C%2520we%2520introduce%2520a%2520selective%2520subject%2520region%2520loss%252C%2520which%250Aencourages%2520the%2520model%2520to%2520disregard%2520some%2520of%2520reference%2520images%2520during%2520training.%2520Our%250Aloss%2520ensures%2520that%2520generated%2520images%2520better%2520align%2520with%2520text%2520prompts%2520while%250Amaintaining%2520subject%2520integrity.%2520Notably%252C%2520our%2520Wardrobe%2520Polyptych%2520LoRA%2520requires%2520no%250Aadditional%2520parameters%2520at%2520the%2520inference%2520stage%2520and%2520performs%2520generation%2520using%2520a%250Asingle%2520model%2520trained%2520on%2520a%2520few%2520training%2520samples.%2520We%2520construct%2520a%2520new%2520dataset%2520and%250Abenchmark%2520tailored%2520for%2520personalized%2520human%2520image%2520generation.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%250Atechniques%2520in%2520fidelity%2520and%2520consistency%252C%2520enabling%2520realistic%2520and%250Aidentity-preserving%2520full-body%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Wardrobe%20to%20Canvas%3A%20Wardrobe%20Polyptych%20LoRA%20for%20Part-level%0A%20%20Controllable%20Human%20Image%20Generation&entry.906535625=Jeongho%20Kim%20and%20Sunghyun%20Park%20and%20Hyoungwoo%20Park%20and%20Sungrack%20Yun%20and%20Jaegul%20Choo%20and%20Seokeon%20Cho&entry.1292438233=%20%20Recent%20diffusion%20models%20achieve%20personalization%20by%20learning%20specific%0Asubjects%2C%20allowing%20learned%20attributes%20to%20be%20integrated%20into%20generated%20images.%0AHowever%2C%20personalized%20human%20image%20generation%20remains%20challenging%20due%20to%20the%0Aneed%20for%20precise%20and%20consistent%20attribute%20preservation%20%28e.g.%2C%20identity%2C%0Aclothing%20details%29.%20Existing%20subject-driven%20image%20generation%20methods%20often%0Arequire%20either%20%281%29%20inference-time%20fine-tuning%20with%20few%20images%20for%20each%20new%0Asubject%20or%20%282%29%20large-scale%20dataset%20training%20for%20generalization.%20Both%20approaches%0Aare%20computationally%20expensive%20and%20impractical%20for%20real-time%20applications.%20To%0Aaddress%20these%20limitations%2C%20we%20present%20Wardrobe%20Polyptych%20LoRA%2C%20a%20novel%0Apart-level%20controllable%20model%20for%20personalized%20human%20image%20generation.%20By%0Atraining%20only%20LoRA%20layers%2C%20our%20method%20removes%20the%20computational%20burden%20at%0Ainference%20while%20ensuring%20high-fidelity%20synthesis%20of%20unseen%20subjects.%20Our%20key%0Aidea%20is%20to%20condition%20the%20generation%20on%20the%20subject%27s%20wardrobe%20and%20leverage%0Aspatial%20references%20to%20reduce%20information%20loss%2C%20thereby%20improving%20fidelity%20and%0Aconsistency.%20Additionally%2C%20we%20introduce%20a%20selective%20subject%20region%20loss%2C%20which%0Aencourages%20the%20model%20to%20disregard%20some%20of%20reference%20images%20during%20training.%20Our%0Aloss%20ensures%20that%20generated%20images%20better%20align%20with%20text%20prompts%20while%0Amaintaining%20subject%20integrity.%20Notably%2C%20our%20Wardrobe%20Polyptych%20LoRA%20requires%20no%0Aadditional%20parameters%20at%20the%20inference%20stage%20and%20performs%20generation%20using%20a%0Asingle%20model%20trained%20on%20a%20few%20training%20samples.%20We%20construct%20a%20new%20dataset%20and%0Abenchmark%20tailored%20for%20personalized%20human%20image%20generation.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20significantly%20outperforms%20existing%0Atechniques%20in%20fidelity%20and%20consistency%2C%20enabling%20realistic%20and%0Aidentity-preserving%20full-body%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10217v1&entry.124074799=Read"},
{"title": "Overcoming catastrophic forgetting in neural networks", "author": "Brandon Shuen Yi Loke and Filippo Quadri and Gabriel Vivanco and Maximilian Casagrande and Sa\u00fal Fenollosa", "abstract": "  Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.\n", "link": "http://arxiv.org/abs/2507.10485v1", "date": "2025-07-14", "relevancy": 2.3995, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5185}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4609}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks&body=Title%3A%20Overcoming%20catastrophic%20forgetting%20in%20neural%20networks%0AAuthor%3A%20Brandon%20Shuen%20Yi%20Loke%20and%20Filippo%20Quadri%20and%20Gabriel%20Vivanco%20and%20Maximilian%20Casagrande%20and%20Sa%C3%BAl%20Fenollosa%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20is%20the%20primary%20challenge%20that%20hinders%20continual%0Alearning%2C%20which%20refers%20to%20a%20neural%20network%20ability%20to%20sequentially%20learn%0Amultiple%20tasks%20while%20retaining%20previously%20acquired%20knowledge.%20Elastic%20Weight%0AConsolidation%2C%20a%20regularization-based%20approach%20inspired%20by%20synaptic%0Aconsolidation%20in%20biological%20neural%20systems%2C%20has%20been%20used%20to%20overcome%20this%0Aproblem.%20In%20this%20study%20prior%20research%20is%20replicated%20and%20extended%20by%20evaluating%0AEWC%20in%20supervised%20learning%20settings%20using%20the%20PermutedMNIST%20and%20RotatedMNIST%0Abenchmarks.%20Through%20systematic%20comparisons%20with%20L2%20regularization%20and%0Astochastic%20gradient%20descent%20%28SGD%29%20without%20regularization%2C%20we%20analyze%20how%0Adifferent%20approaches%20balance%20knowledge%20retention%20and%20adaptability.%20Our%20results%0Aconfirm%20what%20was%20shown%20in%20previous%20research%2C%20showing%20that%20EWC%20significantly%0Areduces%20forgetting%20compared%20to%20naive%20training%20while%20slightly%20compromising%0Alearning%20efficiency%20on%20new%20tasks.%20Moreover%2C%20we%20investigate%20the%20impact%20of%0Adropout%20regularization%20and%20varying%20hyperparameters%2C%20offering%20insights%20into%20the%0Ageneralization%20of%20EWC%20across%20diverse%20learning%20scenarios.%20These%20results%0Aunderscore%20EWC%27s%20potential%20as%20a%20viable%20solution%20for%20lifelong%20learning%20in%20neural%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520catastrophic%2520forgetting%2520in%2520neural%2520networks%26entry.906535625%3DBrandon%2520Shuen%2520Yi%2520Loke%2520and%2520Filippo%2520Quadri%2520and%2520Gabriel%2520Vivanco%2520and%2520Maximilian%2520Casagrande%2520and%2520Sa%25C3%25BAl%2520Fenollosa%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520is%2520the%2520primary%2520challenge%2520that%2520hinders%2520continual%250Alearning%252C%2520which%2520refers%2520to%2520a%2520neural%2520network%2520ability%2520to%2520sequentially%2520learn%250Amultiple%2520tasks%2520while%2520retaining%2520previously%2520acquired%2520knowledge.%2520Elastic%2520Weight%250AConsolidation%252C%2520a%2520regularization-based%2520approach%2520inspired%2520by%2520synaptic%250Aconsolidation%2520in%2520biological%2520neural%2520systems%252C%2520has%2520been%2520used%2520to%2520overcome%2520this%250Aproblem.%2520In%2520this%2520study%2520prior%2520research%2520is%2520replicated%2520and%2520extended%2520by%2520evaluating%250AEWC%2520in%2520supervised%2520learning%2520settings%2520using%2520the%2520PermutedMNIST%2520and%2520RotatedMNIST%250Abenchmarks.%2520Through%2520systematic%2520comparisons%2520with%2520L2%2520regularization%2520and%250Astochastic%2520gradient%2520descent%2520%2528SGD%2529%2520without%2520regularization%252C%2520we%2520analyze%2520how%250Adifferent%2520approaches%2520balance%2520knowledge%2520retention%2520and%2520adaptability.%2520Our%2520results%250Aconfirm%2520what%2520was%2520shown%2520in%2520previous%2520research%252C%2520showing%2520that%2520EWC%2520significantly%250Areduces%2520forgetting%2520compared%2520to%2520naive%2520training%2520while%2520slightly%2520compromising%250Alearning%2520efficiency%2520on%2520new%2520tasks.%2520Moreover%252C%2520we%2520investigate%2520the%2520impact%2520of%250Adropout%2520regularization%2520and%2520varying%2520hyperparameters%252C%2520offering%2520insights%2520into%2520the%250Ageneralization%2520of%2520EWC%2520across%2520diverse%2520learning%2520scenarios.%2520These%2520results%250Aunderscore%2520EWC%2527s%2520potential%2520as%2520a%2520viable%2520solution%2520for%2520lifelong%2520learning%2520in%2520neural%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20catastrophic%20forgetting%20in%20neural%20networks&entry.906535625=Brandon%20Shuen%20Yi%20Loke%20and%20Filippo%20Quadri%20and%20Gabriel%20Vivanco%20and%20Maximilian%20Casagrande%20and%20Sa%C3%BAl%20Fenollosa&entry.1292438233=%20%20Catastrophic%20forgetting%20is%20the%20primary%20challenge%20that%20hinders%20continual%0Alearning%2C%20which%20refers%20to%20a%20neural%20network%20ability%20to%20sequentially%20learn%0Amultiple%20tasks%20while%20retaining%20previously%20acquired%20knowledge.%20Elastic%20Weight%0AConsolidation%2C%20a%20regularization-based%20approach%20inspired%20by%20synaptic%0Aconsolidation%20in%20biological%20neural%20systems%2C%20has%20been%20used%20to%20overcome%20this%0Aproblem.%20In%20this%20study%20prior%20research%20is%20replicated%20and%20extended%20by%20evaluating%0AEWC%20in%20supervised%20learning%20settings%20using%20the%20PermutedMNIST%20and%20RotatedMNIST%0Abenchmarks.%20Through%20systematic%20comparisons%20with%20L2%20regularization%20and%0Astochastic%20gradient%20descent%20%28SGD%29%20without%20regularization%2C%20we%20analyze%20how%0Adifferent%20approaches%20balance%20knowledge%20retention%20and%20adaptability.%20Our%20results%0Aconfirm%20what%20was%20shown%20in%20previous%20research%2C%20showing%20that%20EWC%20significantly%0Areduces%20forgetting%20compared%20to%20naive%20training%20while%20slightly%20compromising%0Alearning%20efficiency%20on%20new%20tasks.%20Moreover%2C%20we%20investigate%20the%20impact%20of%0Adropout%20regularization%20and%20varying%20hyperparameters%2C%20offering%20insights%20into%20the%0Ageneralization%20of%20EWC%20across%20diverse%20learning%20scenarios.%20These%20results%0Aunderscore%20EWC%27s%20potential%20as%20a%20viable%20solution%20for%20lifelong%20learning%20in%20neural%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10485v1&entry.124074799=Read"},
{"title": "On the Performance of Differentially Private Optimization with\n  Heavy-Tail Class Imbalance", "author": "Qiaoyue Tang and Alain Zhiyanov and Mathias L\u00e9cuyer", "abstract": "  In this work, we analyze the optimization behaviour of common private\nlearning optimization algorithms under heavy-tail class imbalanced\ndistribution. We show that, in a stylized model, optimizing with Gradient\nDescent with differential privacy (DP-GD) suffers when learning low-frequency\nclasses, whereas optimization algorithms that estimate second-order information\ndo not. In particular, DP-AdamBC that removes the DP bias from estimating loss\ncurvature is a crucial component to avoid the ill-condition caused by\nheavy-tail class imbalance, and empirically fits the data better with\n$\\approx8\\%$ and $\\approx5\\%$ increase in training accuracy when learning the\nleast frequent classes on both controlled experiments and real data\nrespectively.\n", "link": "http://arxiv.org/abs/2507.10536v1", "date": "2025-07-14", "relevancy": 2.3811, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.499}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4969}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Performance%20of%20Differentially%20Private%20Optimization%20with%0A%20%20Heavy-Tail%20Class%20Imbalance&body=Title%3A%20On%20the%20Performance%20of%20Differentially%20Private%20Optimization%20with%0A%20%20Heavy-Tail%20Class%20Imbalance%0AAuthor%3A%20Qiaoyue%20Tang%20and%20Alain%20Zhiyanov%20and%20Mathias%20L%C3%A9cuyer%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20analyze%20the%20optimization%20behaviour%20of%20common%20private%0Alearning%20optimization%20algorithms%20under%20heavy-tail%20class%20imbalanced%0Adistribution.%20We%20show%20that%2C%20in%20a%20stylized%20model%2C%20optimizing%20with%20Gradient%0ADescent%20with%20differential%20privacy%20%28DP-GD%29%20suffers%20when%20learning%20low-frequency%0Aclasses%2C%20whereas%20optimization%20algorithms%20that%20estimate%20second-order%20information%0Ado%20not.%20In%20particular%2C%20DP-AdamBC%20that%20removes%20the%20DP%20bias%20from%20estimating%20loss%0Acurvature%20is%20a%20crucial%20component%20to%20avoid%20the%20ill-condition%20caused%20by%0Aheavy-tail%20class%20imbalance%2C%20and%20empirically%20fits%20the%20data%20better%20with%0A%24%5Capprox8%5C%25%24%20and%20%24%5Capprox5%5C%25%24%20increase%20in%20training%20accuracy%20when%20learning%20the%0Aleast%20frequent%20classes%20on%20both%20controlled%20experiments%20and%20real%20data%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Performance%2520of%2520Differentially%2520Private%2520Optimization%2520with%250A%2520%2520Heavy-Tail%2520Class%2520Imbalance%26entry.906535625%3DQiaoyue%2520Tang%2520and%2520Alain%2520Zhiyanov%2520and%2520Mathias%2520L%25C3%25A9cuyer%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520analyze%2520the%2520optimization%2520behaviour%2520of%2520common%2520private%250Alearning%2520optimization%2520algorithms%2520under%2520heavy-tail%2520class%2520imbalanced%250Adistribution.%2520We%2520show%2520that%252C%2520in%2520a%2520stylized%2520model%252C%2520optimizing%2520with%2520Gradient%250ADescent%2520with%2520differential%2520privacy%2520%2528DP-GD%2529%2520suffers%2520when%2520learning%2520low-frequency%250Aclasses%252C%2520whereas%2520optimization%2520algorithms%2520that%2520estimate%2520second-order%2520information%250Ado%2520not.%2520In%2520particular%252C%2520DP-AdamBC%2520that%2520removes%2520the%2520DP%2520bias%2520from%2520estimating%2520loss%250Acurvature%2520is%2520a%2520crucial%2520component%2520to%2520avoid%2520the%2520ill-condition%2520caused%2520by%250Aheavy-tail%2520class%2520imbalance%252C%2520and%2520empirically%2520fits%2520the%2520data%2520better%2520with%250A%2524%255Capprox8%255C%2525%2524%2520and%2520%2524%255Capprox5%255C%2525%2524%2520increase%2520in%2520training%2520accuracy%2520when%2520learning%2520the%250Aleast%2520frequent%2520classes%2520on%2520both%2520controlled%2520experiments%2520and%2520real%2520data%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Performance%20of%20Differentially%20Private%20Optimization%20with%0A%20%20Heavy-Tail%20Class%20Imbalance&entry.906535625=Qiaoyue%20Tang%20and%20Alain%20Zhiyanov%20and%20Mathias%20L%C3%A9cuyer&entry.1292438233=%20%20In%20this%20work%2C%20we%20analyze%20the%20optimization%20behaviour%20of%20common%20private%0Alearning%20optimization%20algorithms%20under%20heavy-tail%20class%20imbalanced%0Adistribution.%20We%20show%20that%2C%20in%20a%20stylized%20model%2C%20optimizing%20with%20Gradient%0ADescent%20with%20differential%20privacy%20%28DP-GD%29%20suffers%20when%20learning%20low-frequency%0Aclasses%2C%20whereas%20optimization%20algorithms%20that%20estimate%20second-order%20information%0Ado%20not.%20In%20particular%2C%20DP-AdamBC%20that%20removes%20the%20DP%20bias%20from%20estimating%20loss%0Acurvature%20is%20a%20crucial%20component%20to%20avoid%20the%20ill-condition%20caused%20by%0Aheavy-tail%20class%20imbalance%2C%20and%20empirically%20fits%20the%20data%20better%20with%0A%24%5Capprox8%5C%25%24%20and%20%24%5Capprox5%5C%25%24%20increase%20in%20training%20accuracy%20when%20learning%20the%0Aleast%20frequent%20classes%20on%20both%20controlled%20experiments%20and%20real%20data%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10536v1&entry.124074799=Read"},
{"title": "Dynamical stability for dense patterns in discrete attractor neural\n  networks", "author": "Uri Cohen and M\u00e1t\u00e9 Lengyel", "abstract": "  Neural networks storing multiple discrete attractors are canonical models of\nbiological memory. Previously, the dynamical stability of such networks could\nonly be guaranteed under highly restrictive conditions. Here, we derive a\ntheory of the local stability of discrete fixed points in a broad class of\nnetworks with graded neural activities and in the presence of noise. By\ndirectly analyzing the bulk and outliers of the Jacobian spectrum, we show that\nall fixed points are stable below a critical load that is distinct from the\nclassical \\textit{critical capacity} and depends on the statistics of neural\nactivities in the fixed points as well as the single-neuron activation\nfunction. Our analysis highlights the computational benefits of\nthreshold-linear activation and sparse-like patterns.\n", "link": "http://arxiv.org/abs/2507.10383v1", "date": "2025-07-14", "relevancy": 2.338, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4844}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4816}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%0A%20%20networks&body=Title%3A%20Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%0A%20%20networks%0AAuthor%3A%20Uri%20Cohen%20and%20M%C3%A1t%C3%A9%20Lengyel%0AAbstract%3A%20%20%20Neural%20networks%20storing%20multiple%20discrete%20attractors%20are%20canonical%20models%20of%0Abiological%20memory.%20Previously%2C%20the%20dynamical%20stability%20of%20such%20networks%20could%0Aonly%20be%20guaranteed%20under%20highly%20restrictive%20conditions.%20Here%2C%20we%20derive%20a%0Atheory%20of%20the%20local%20stability%20of%20discrete%20fixed%20points%20in%20a%20broad%20class%20of%0Anetworks%20with%20graded%20neural%20activities%20and%20in%20the%20presence%20of%20noise.%20By%0Adirectly%20analyzing%20the%20bulk%20and%20outliers%20of%20the%20Jacobian%20spectrum%2C%20we%20show%20that%0Aall%20fixed%20points%20are%20stable%20below%20a%20critical%20load%20that%20is%20distinct%20from%20the%0Aclassical%20%5Ctextit%7Bcritical%20capacity%7D%20and%20depends%20on%20the%20statistics%20of%20neural%0Aactivities%20in%20the%20fixed%20points%20as%20well%20as%20the%20single-neuron%20activation%0Afunction.%20Our%20analysis%20highlights%20the%20computational%20benefits%20of%0Athreshold-linear%20activation%20and%20sparse-like%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamical%2520stability%2520for%2520dense%2520patterns%2520in%2520discrete%2520attractor%2520neural%250A%2520%2520networks%26entry.906535625%3DUri%2520Cohen%2520and%2520M%25C3%25A1t%25C3%25A9%2520Lengyel%26entry.1292438233%3D%2520%2520Neural%2520networks%2520storing%2520multiple%2520discrete%2520attractors%2520are%2520canonical%2520models%2520of%250Abiological%2520memory.%2520Previously%252C%2520the%2520dynamical%2520stability%2520of%2520such%2520networks%2520could%250Aonly%2520be%2520guaranteed%2520under%2520highly%2520restrictive%2520conditions.%2520Here%252C%2520we%2520derive%2520a%250Atheory%2520of%2520the%2520local%2520stability%2520of%2520discrete%2520fixed%2520points%2520in%2520a%2520broad%2520class%2520of%250Anetworks%2520with%2520graded%2520neural%2520activities%2520and%2520in%2520the%2520presence%2520of%2520noise.%2520By%250Adirectly%2520analyzing%2520the%2520bulk%2520and%2520outliers%2520of%2520the%2520Jacobian%2520spectrum%252C%2520we%2520show%2520that%250Aall%2520fixed%2520points%2520are%2520stable%2520below%2520a%2520critical%2520load%2520that%2520is%2520distinct%2520from%2520the%250Aclassical%2520%255Ctextit%257Bcritical%2520capacity%257D%2520and%2520depends%2520on%2520the%2520statistics%2520of%2520neural%250Aactivities%2520in%2520the%2520fixed%2520points%2520as%2520well%2520as%2520the%2520single-neuron%2520activation%250Afunction.%2520Our%2520analysis%2520highlights%2520the%2520computational%2520benefits%2520of%250Athreshold-linear%2520activation%2520and%2520sparse-like%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%0A%20%20networks&entry.906535625=Uri%20Cohen%20and%20M%C3%A1t%C3%A9%20Lengyel&entry.1292438233=%20%20Neural%20networks%20storing%20multiple%20discrete%20attractors%20are%20canonical%20models%20of%0Abiological%20memory.%20Previously%2C%20the%20dynamical%20stability%20of%20such%20networks%20could%0Aonly%20be%20guaranteed%20under%20highly%20restrictive%20conditions.%20Here%2C%20we%20derive%20a%0Atheory%20of%20the%20local%20stability%20of%20discrete%20fixed%20points%20in%20a%20broad%20class%20of%0Anetworks%20with%20graded%20neural%20activities%20and%20in%20the%20presence%20of%20noise.%20By%0Adirectly%20analyzing%20the%20bulk%20and%20outliers%20of%20the%20Jacobian%20spectrum%2C%20we%20show%20that%0Aall%20fixed%20points%20are%20stable%20below%20a%20critical%20load%20that%20is%20distinct%20from%20the%0Aclassical%20%5Ctextit%7Bcritical%20capacity%7D%20and%20depends%20on%20the%20statistics%20of%20neural%0Aactivities%20in%20the%20fixed%20points%20as%20well%20as%20the%20single-neuron%20activation%0Afunction.%20Our%20analysis%20highlights%20the%20computational%20benefits%20of%0Athreshold-linear%20activation%20and%20sparse-like%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10383v1&entry.124074799=Read"},
{"title": "DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning", "author": "Kaixiang Zhao and Joseph Yousry Attalla and Qian Lou and Yushun Dong", "abstract": "  Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph-based learning tasks. However, enabling privacy-preserving GNNs\nin encrypted domains, such as under Fully Homomorphic Encryption (FHE),\ntypically incurs substantial computational overhead, rendering real-time and\nprivacy-preserving inference impractical. In this work, we propose DESIGN\n(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel\nframework for efficient encrypted GNN inference. DESIGN tackles the critical\nefficiency limitations of existing FHE GNN approaches, which often overlook\ninput data redundancy and apply uniform computational strategies. Our framework\nachieves significant performance gains through a hierarchical optimization\nstrategy executed entirely on the server: first, FHE-compatible node importance\nscores (based on encrypted degree statistics) are computed from the encrypted\ngraph. These scores then guide a homomorphic partitioning process, generating\nmulti-level importance masks directly under FHE. This dynamically generated\nmask facilitates both input graph pruning (by logically removing unimportant\nelements) and a novel adaptive polynomial activation scheme, where activation\ncomplexity is tailored to node importance levels. Empirical evaluations\ndemonstrate that DESIGN substantially accelerates FHE GNN inference compared to\nstate-of-the-art methods while maintaining competitive model accuracy,\npresenting a robust solution for secure graph analytics. Our implementation is\npublicly available at https://github.com/LabRAI/DESIGN.\n", "link": "http://arxiv.org/abs/2507.05649v2", "date": "2025-07-14", "relevancy": 2.3098, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.466}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4614}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DESIGN%3A%20Encrypted%20GNN%20Inference%20via%20Server-Side%20Input%20Graph%20Pruning&body=Title%3A%20DESIGN%3A%20Encrypted%20GNN%20Inference%20via%20Server-Side%20Input%20Graph%20Pruning%0AAuthor%3A%20Kaixiang%20Zhao%20and%20Joseph%20Yousry%20Attalla%20and%20Qian%20Lou%20and%20Yushun%20Dong%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20state-of-the-art%20performance%20in%0Avarious%20graph-based%20learning%20tasks.%20However%2C%20enabling%20privacy-preserving%20GNNs%0Ain%20encrypted%20domains%2C%20such%20as%20under%20Fully%20Homomorphic%20Encryption%20%28FHE%29%2C%0Atypically%20incurs%20substantial%20computational%20overhead%2C%20rendering%20real-time%20and%0Aprivacy-preserving%20inference%20impractical.%20In%20this%20work%2C%20we%20propose%20DESIGN%0A%28EncrypteD%20GNN%20Inference%20via%20sErver-Side%20Input%20Graph%20pruNing%29%2C%20a%20novel%0Aframework%20for%20efficient%20encrypted%20GNN%20inference.%20DESIGN%20tackles%20the%20critical%0Aefficiency%20limitations%20of%20existing%20FHE%20GNN%20approaches%2C%20which%20often%20overlook%0Ainput%20data%20redundancy%20and%20apply%20uniform%20computational%20strategies.%20Our%20framework%0Aachieves%20significant%20performance%20gains%20through%20a%20hierarchical%20optimization%0Astrategy%20executed%20entirely%20on%20the%20server%3A%20first%2C%20FHE-compatible%20node%20importance%0Ascores%20%28based%20on%20encrypted%20degree%20statistics%29%20are%20computed%20from%20the%20encrypted%0Agraph.%20These%20scores%20then%20guide%20a%20homomorphic%20partitioning%20process%2C%20generating%0Amulti-level%20importance%20masks%20directly%20under%20FHE.%20This%20dynamically%20generated%0Amask%20facilitates%20both%20input%20graph%20pruning%20%28by%20logically%20removing%20unimportant%0Aelements%29%20and%20a%20novel%20adaptive%20polynomial%20activation%20scheme%2C%20where%20activation%0Acomplexity%20is%20tailored%20to%20node%20importance%20levels.%20Empirical%20evaluations%0Ademonstrate%20that%20DESIGN%20substantially%20accelerates%20FHE%20GNN%20inference%20compared%20to%0Astate-of-the-art%20methods%20while%20maintaining%20competitive%20model%20accuracy%2C%0Apresenting%20a%20robust%20solution%20for%20secure%20graph%20analytics.%20Our%20implementation%20is%0Apublicly%20available%20at%20https%3A//github.com/LabRAI/DESIGN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05649v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDESIGN%253A%2520Encrypted%2520GNN%2520Inference%2520via%2520Server-Side%2520Input%2520Graph%2520Pruning%26entry.906535625%3DKaixiang%2520Zhao%2520and%2520Joseph%2520Yousry%2520Attalla%2520and%2520Qian%2520Lou%2520and%2520Yushun%2520Dong%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520state-of-the-art%2520performance%2520in%250Avarious%2520graph-based%2520learning%2520tasks.%2520However%252C%2520enabling%2520privacy-preserving%2520GNNs%250Ain%2520encrypted%2520domains%252C%2520such%2520as%2520under%2520Fully%2520Homomorphic%2520Encryption%2520%2528FHE%2529%252C%250Atypically%2520incurs%2520substantial%2520computational%2520overhead%252C%2520rendering%2520real-time%2520and%250Aprivacy-preserving%2520inference%2520impractical.%2520In%2520this%2520work%252C%2520we%2520propose%2520DESIGN%250A%2528EncrypteD%2520GNN%2520Inference%2520via%2520sErver-Side%2520Input%2520Graph%2520pruNing%2529%252C%2520a%2520novel%250Aframework%2520for%2520efficient%2520encrypted%2520GNN%2520inference.%2520DESIGN%2520tackles%2520the%2520critical%250Aefficiency%2520limitations%2520of%2520existing%2520FHE%2520GNN%2520approaches%252C%2520which%2520often%2520overlook%250Ainput%2520data%2520redundancy%2520and%2520apply%2520uniform%2520computational%2520strategies.%2520Our%2520framework%250Aachieves%2520significant%2520performance%2520gains%2520through%2520a%2520hierarchical%2520optimization%250Astrategy%2520executed%2520entirely%2520on%2520the%2520server%253A%2520first%252C%2520FHE-compatible%2520node%2520importance%250Ascores%2520%2528based%2520on%2520encrypted%2520degree%2520statistics%2529%2520are%2520computed%2520from%2520the%2520encrypted%250Agraph.%2520These%2520scores%2520then%2520guide%2520a%2520homomorphic%2520partitioning%2520process%252C%2520generating%250Amulti-level%2520importance%2520masks%2520directly%2520under%2520FHE.%2520This%2520dynamically%2520generated%250Amask%2520facilitates%2520both%2520input%2520graph%2520pruning%2520%2528by%2520logically%2520removing%2520unimportant%250Aelements%2529%2520and%2520a%2520novel%2520adaptive%2520polynomial%2520activation%2520scheme%252C%2520where%2520activation%250Acomplexity%2520is%2520tailored%2520to%2520node%2520importance%2520levels.%2520Empirical%2520evaluations%250Ademonstrate%2520that%2520DESIGN%2520substantially%2520accelerates%2520FHE%2520GNN%2520inference%2520compared%2520to%250Astate-of-the-art%2520methods%2520while%2520maintaining%2520competitive%2520model%2520accuracy%252C%250Apresenting%2520a%2520robust%2520solution%2520for%2520secure%2520graph%2520analytics.%2520Our%2520implementation%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/LabRAI/DESIGN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05649v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DESIGN%3A%20Encrypted%20GNN%20Inference%20via%20Server-Side%20Input%20Graph%20Pruning&entry.906535625=Kaixiang%20Zhao%20and%20Joseph%20Yousry%20Attalla%20and%20Qian%20Lou%20and%20Yushun%20Dong&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20state-of-the-art%20performance%20in%0Avarious%20graph-based%20learning%20tasks.%20However%2C%20enabling%20privacy-preserving%20GNNs%0Ain%20encrypted%20domains%2C%20such%20as%20under%20Fully%20Homomorphic%20Encryption%20%28FHE%29%2C%0Atypically%20incurs%20substantial%20computational%20overhead%2C%20rendering%20real-time%20and%0Aprivacy-preserving%20inference%20impractical.%20In%20this%20work%2C%20we%20propose%20DESIGN%0A%28EncrypteD%20GNN%20Inference%20via%20sErver-Side%20Input%20Graph%20pruNing%29%2C%20a%20novel%0Aframework%20for%20efficient%20encrypted%20GNN%20inference.%20DESIGN%20tackles%20the%20critical%0Aefficiency%20limitations%20of%20existing%20FHE%20GNN%20approaches%2C%20which%20often%20overlook%0Ainput%20data%20redundancy%20and%20apply%20uniform%20computational%20strategies.%20Our%20framework%0Aachieves%20significant%20performance%20gains%20through%20a%20hierarchical%20optimization%0Astrategy%20executed%20entirely%20on%20the%20server%3A%20first%2C%20FHE-compatible%20node%20importance%0Ascores%20%28based%20on%20encrypted%20degree%20statistics%29%20are%20computed%20from%20the%20encrypted%0Agraph.%20These%20scores%20then%20guide%20a%20homomorphic%20partitioning%20process%2C%20generating%0Amulti-level%20importance%20masks%20directly%20under%20FHE.%20This%20dynamically%20generated%0Amask%20facilitates%20both%20input%20graph%20pruning%20%28by%20logically%20removing%20unimportant%0Aelements%29%20and%20a%20novel%20adaptive%20polynomial%20activation%20scheme%2C%20where%20activation%0Acomplexity%20is%20tailored%20to%20node%20importance%20levels.%20Empirical%20evaluations%0Ademonstrate%20that%20DESIGN%20substantially%20accelerates%20FHE%20GNN%20inference%20compared%20to%0Astate-of-the-art%20methods%20while%20maintaining%20competitive%20model%20accuracy%2C%0Apresenting%20a%20robust%20solution%20for%20secure%20graph%20analytics.%20Our%20implementation%20is%0Apublicly%20available%20at%20https%3A//github.com/LabRAI/DESIGN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05649v2&entry.124074799=Read"},
{"title": "Token-based Audio Inpainting via Discrete Diffusion", "author": "Tali Dror and Iftach Shoham and Moshe Buchris and Oren Gal and Haim Permuter and Gilad Katz and Eliya Nachmani", "abstract": "  Audio inpainting refers to the task of reconstructing missing segments in\ncorrupted audio recordings. While prior approaches-including waveform and\nspectrogram-based diffusion models-have shown promising results for short gaps,\nthey often degrade in quality when gaps exceed 100 milliseconds (ms). In this\nwork, we introduce a novel inpainting method based on discrete diffusion\nmodeling, which operates over tokenized audio representations produced by a\npre-trained audio tokenizer. Our approach models the generative process\ndirectly in the discrete latent space, enabling stable and semantically\ncoherent reconstruction of missing audio. We evaluate the method on the\nMusicNet dataset using both objective and perceptual metrics across gap\ndurations up to 300 ms. We further evaluated our approach on the MTG dataset,\nextending the gap duration to 500 ms. Experimental results demonstrate that our\nmethod achieves competitive or superior performance compared to existing\nbaselines, particularly for longer gaps, offering a robust solution for\nrestoring degraded musical recordings. Audio examples of our proposed method\ncan be found at https://iftach21.github.io/\n", "link": "http://arxiv.org/abs/2507.08333v2", "date": "2025-07-14", "relevancy": 2.2959, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5966}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5602}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-based%20Audio%20Inpainting%20via%20Discrete%20Diffusion&body=Title%3A%20Token-based%20Audio%20Inpainting%20via%20Discrete%20Diffusion%0AAuthor%3A%20Tali%20Dror%20and%20Iftach%20Shoham%20and%20Moshe%20Buchris%20and%20Oren%20Gal%20and%20Haim%20Permuter%20and%20Gilad%20Katz%20and%20Eliya%20Nachmani%0AAbstract%3A%20%20%20Audio%20inpainting%20refers%20to%20the%20task%20of%20reconstructing%20missing%20segments%20in%0Acorrupted%20audio%20recordings.%20While%20prior%20approaches-including%20waveform%20and%0Aspectrogram-based%20diffusion%20models-have%20shown%20promising%20results%20for%20short%20gaps%2C%0Athey%20often%20degrade%20in%20quality%20when%20gaps%20exceed%20100%20milliseconds%20%28ms%29.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20inpainting%20method%20based%20on%20discrete%20diffusion%0Amodeling%2C%20which%20operates%20over%20tokenized%20audio%20representations%20produced%20by%20a%0Apre-trained%20audio%20tokenizer.%20Our%20approach%20models%20the%20generative%20process%0Adirectly%20in%20the%20discrete%20latent%20space%2C%20enabling%20stable%20and%20semantically%0Acoherent%20reconstruction%20of%20missing%20audio.%20We%20evaluate%20the%20method%20on%20the%0AMusicNet%20dataset%20using%20both%20objective%20and%20perceptual%20metrics%20across%20gap%0Adurations%20up%20to%20300%20ms.%20We%20further%20evaluated%20our%20approach%20on%20the%20MTG%20dataset%2C%0Aextending%20the%20gap%20duration%20to%20500%20ms.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20competitive%20or%20superior%20performance%20compared%20to%20existing%0Abaselines%2C%20particularly%20for%20longer%20gaps%2C%20offering%20a%20robust%20solution%20for%0Arestoring%20degraded%20musical%20recordings.%20Audio%20examples%20of%20our%20proposed%20method%0Acan%20be%20found%20at%20https%3A//iftach21.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-based%2520Audio%2520Inpainting%2520via%2520Discrete%2520Diffusion%26entry.906535625%3DTali%2520Dror%2520and%2520Iftach%2520Shoham%2520and%2520Moshe%2520Buchris%2520and%2520Oren%2520Gal%2520and%2520Haim%2520Permuter%2520and%2520Gilad%2520Katz%2520and%2520Eliya%2520Nachmani%26entry.1292438233%3D%2520%2520Audio%2520inpainting%2520refers%2520to%2520the%2520task%2520of%2520reconstructing%2520missing%2520segments%2520in%250Acorrupted%2520audio%2520recordings.%2520While%2520prior%2520approaches-including%2520waveform%2520and%250Aspectrogram-based%2520diffusion%2520models-have%2520shown%2520promising%2520results%2520for%2520short%2520gaps%252C%250Athey%2520often%2520degrade%2520in%2520quality%2520when%2520gaps%2520exceed%2520100%2520milliseconds%2520%2528ms%2529.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520inpainting%2520method%2520based%2520on%2520discrete%2520diffusion%250Amodeling%252C%2520which%2520operates%2520over%2520tokenized%2520audio%2520representations%2520produced%2520by%2520a%250Apre-trained%2520audio%2520tokenizer.%2520Our%2520approach%2520models%2520the%2520generative%2520process%250Adirectly%2520in%2520the%2520discrete%2520latent%2520space%252C%2520enabling%2520stable%2520and%2520semantically%250Acoherent%2520reconstruction%2520of%2520missing%2520audio.%2520We%2520evaluate%2520the%2520method%2520on%2520the%250AMusicNet%2520dataset%2520using%2520both%2520objective%2520and%2520perceptual%2520metrics%2520across%2520gap%250Adurations%2520up%2520to%2520300%2520ms.%2520We%2520further%2520evaluated%2520our%2520approach%2520on%2520the%2520MTG%2520dataset%252C%250Aextending%2520the%2520gap%2520duration%2520to%2520500%2520ms.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520competitive%2520or%2520superior%2520performance%2520compared%2520to%2520existing%250Abaselines%252C%2520particularly%2520for%2520longer%2520gaps%252C%2520offering%2520a%2520robust%2520solution%2520for%250Arestoring%2520degraded%2520musical%2520recordings.%2520Audio%2520examples%2520of%2520our%2520proposed%2520method%250Acan%2520be%2520found%2520at%2520https%253A//iftach21.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-based%20Audio%20Inpainting%20via%20Discrete%20Diffusion&entry.906535625=Tali%20Dror%20and%20Iftach%20Shoham%20and%20Moshe%20Buchris%20and%20Oren%20Gal%20and%20Haim%20Permuter%20and%20Gilad%20Katz%20and%20Eliya%20Nachmani&entry.1292438233=%20%20Audio%20inpainting%20refers%20to%20the%20task%20of%20reconstructing%20missing%20segments%20in%0Acorrupted%20audio%20recordings.%20While%20prior%20approaches-including%20waveform%20and%0Aspectrogram-based%20diffusion%20models-have%20shown%20promising%20results%20for%20short%20gaps%2C%0Athey%20often%20degrade%20in%20quality%20when%20gaps%20exceed%20100%20milliseconds%20%28ms%29.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20inpainting%20method%20based%20on%20discrete%20diffusion%0Amodeling%2C%20which%20operates%20over%20tokenized%20audio%20representations%20produced%20by%20a%0Apre-trained%20audio%20tokenizer.%20Our%20approach%20models%20the%20generative%20process%0Adirectly%20in%20the%20discrete%20latent%20space%2C%20enabling%20stable%20and%20semantically%0Acoherent%20reconstruction%20of%20missing%20audio.%20We%20evaluate%20the%20method%20on%20the%0AMusicNet%20dataset%20using%20both%20objective%20and%20perceptual%20metrics%20across%20gap%0Adurations%20up%20to%20300%20ms.%20We%20further%20evaluated%20our%20approach%20on%20the%20MTG%20dataset%2C%0Aextending%20the%20gap%20duration%20to%20500%20ms.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20competitive%20or%20superior%20performance%20compared%20to%20existing%0Abaselines%2C%20particularly%20for%20longer%20gaps%2C%20offering%20a%20robust%20solution%20for%0Arestoring%20degraded%20musical%20recordings.%20Audio%20examples%20of%20our%20proposed%20method%0Acan%20be%20found%20at%20https%3A//iftach21.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08333v2&entry.124074799=Read"},
{"title": "Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions", "author": "Mohammadhossein Talebi and Pragyan Dahal and Davide Possenti and Stefano Arrigoni and Francesco Braghin", "abstract": "  Autonomous driving systems are highly dependent on sensors like cameras,\nLiDAR, and inertial measurement units (IMU) to perceive the environment and\nestimate their motion. Among these sensors, perception-based sensors are not\nprotected from harsh weather and technical failures. Although existing methods\nshow robustness against common technical issues like rotational misalignment\nand disconnection, they often degrade when faced with dynamic environmental\nfactors like weather conditions. To address these problems, this research\nintroduces a novel deep learning-based motion estimator that integrates visual,\ninertial, and millimeter-wave radar data, utilizing each sensor strengths to\nimprove odometry estimation accuracy and reliability under adverse\nenvironmental conditions such as snow, rain, and varying light. The proposed\nmodel uses advanced sensor fusion techniques that dynamically adjust the\ncontributions of each sensor based on the current environmental condition, with\nradar compensating for visual sensor limitations in poor visibility. This work\nexplores recent advancements in radar-based odometry and highlights that radar\nrobustness in different weather conditions makes it a valuable component for\npose estimation systems, specifically when visual sensors are degraded.\nExperimental results, conducted on the Boreas dataset, showcase the robustness\nand effectiveness of the model in both clear and degraded environments.\n", "link": "http://arxiv.org/abs/2507.10376v1", "date": "2025-07-14", "relevancy": 2.2943, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6124}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5994}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Raci-Net%3A%20Ego-vehicle%20Odometry%20Estimation%20in%20Adverse%20Weather%20Conditions&body=Title%3A%20Raci-Net%3A%20Ego-vehicle%20Odometry%20Estimation%20in%20Adverse%20Weather%20Conditions%0AAuthor%3A%20Mohammadhossein%20Talebi%20and%20Pragyan%20Dahal%20and%20Davide%20Possenti%20and%20Stefano%20Arrigoni%20and%20Francesco%20Braghin%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20are%20highly%20dependent%20on%20sensors%20like%20cameras%2C%0ALiDAR%2C%20and%20inertial%20measurement%20units%20%28IMU%29%20to%20perceive%20the%20environment%20and%0Aestimate%20their%20motion.%20Among%20these%20sensors%2C%20perception-based%20sensors%20are%20not%0Aprotected%20from%20harsh%20weather%20and%20technical%20failures.%20Although%20existing%20methods%0Ashow%20robustness%20against%20common%20technical%20issues%20like%20rotational%20misalignment%0Aand%20disconnection%2C%20they%20often%20degrade%20when%20faced%20with%20dynamic%20environmental%0Afactors%20like%20weather%20conditions.%20To%20address%20these%20problems%2C%20this%20research%0Aintroduces%20a%20novel%20deep%20learning-based%20motion%20estimator%20that%20integrates%20visual%2C%0Ainertial%2C%20and%20millimeter-wave%20radar%20data%2C%20utilizing%20each%20sensor%20strengths%20to%0Aimprove%20odometry%20estimation%20accuracy%20and%20reliability%20under%20adverse%0Aenvironmental%20conditions%20such%20as%20snow%2C%20rain%2C%20and%20varying%20light.%20The%20proposed%0Amodel%20uses%20advanced%20sensor%20fusion%20techniques%20that%20dynamically%20adjust%20the%0Acontributions%20of%20each%20sensor%20based%20on%20the%20current%20environmental%20condition%2C%20with%0Aradar%20compensating%20for%20visual%20sensor%20limitations%20in%20poor%20visibility.%20This%20work%0Aexplores%20recent%20advancements%20in%20radar-based%20odometry%20and%20highlights%20that%20radar%0Arobustness%20in%20different%20weather%20conditions%20makes%20it%20a%20valuable%20component%20for%0Apose%20estimation%20systems%2C%20specifically%20when%20visual%20sensors%20are%20degraded.%0AExperimental%20results%2C%20conducted%20on%20the%20Boreas%20dataset%2C%20showcase%20the%20robustness%0Aand%20effectiveness%20of%20the%20model%20in%20both%20clear%20and%20degraded%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaci-Net%253A%2520Ego-vehicle%2520Odometry%2520Estimation%2520in%2520Adverse%2520Weather%2520Conditions%26entry.906535625%3DMohammadhossein%2520Talebi%2520and%2520Pragyan%2520Dahal%2520and%2520Davide%2520Possenti%2520and%2520Stefano%2520Arrigoni%2520and%2520Francesco%2520Braghin%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520are%2520highly%2520dependent%2520on%2520sensors%2520like%2520cameras%252C%250ALiDAR%252C%2520and%2520inertial%2520measurement%2520units%2520%2528IMU%2529%2520to%2520perceive%2520the%2520environment%2520and%250Aestimate%2520their%2520motion.%2520Among%2520these%2520sensors%252C%2520perception-based%2520sensors%2520are%2520not%250Aprotected%2520from%2520harsh%2520weather%2520and%2520technical%2520failures.%2520Although%2520existing%2520methods%250Ashow%2520robustness%2520against%2520common%2520technical%2520issues%2520like%2520rotational%2520misalignment%250Aand%2520disconnection%252C%2520they%2520often%2520degrade%2520when%2520faced%2520with%2520dynamic%2520environmental%250Afactors%2520like%2520weather%2520conditions.%2520To%2520address%2520these%2520problems%252C%2520this%2520research%250Aintroduces%2520a%2520novel%2520deep%2520learning-based%2520motion%2520estimator%2520that%2520integrates%2520visual%252C%250Ainertial%252C%2520and%2520millimeter-wave%2520radar%2520data%252C%2520utilizing%2520each%2520sensor%2520strengths%2520to%250Aimprove%2520odometry%2520estimation%2520accuracy%2520and%2520reliability%2520under%2520adverse%250Aenvironmental%2520conditions%2520such%2520as%2520snow%252C%2520rain%252C%2520and%2520varying%2520light.%2520The%2520proposed%250Amodel%2520uses%2520advanced%2520sensor%2520fusion%2520techniques%2520that%2520dynamically%2520adjust%2520the%250Acontributions%2520of%2520each%2520sensor%2520based%2520on%2520the%2520current%2520environmental%2520condition%252C%2520with%250Aradar%2520compensating%2520for%2520visual%2520sensor%2520limitations%2520in%2520poor%2520visibility.%2520This%2520work%250Aexplores%2520recent%2520advancements%2520in%2520radar-based%2520odometry%2520and%2520highlights%2520that%2520radar%250Arobustness%2520in%2520different%2520weather%2520conditions%2520makes%2520it%2520a%2520valuable%2520component%2520for%250Apose%2520estimation%2520systems%252C%2520specifically%2520when%2520visual%2520sensors%2520are%2520degraded.%250AExperimental%2520results%252C%2520conducted%2520on%2520the%2520Boreas%2520dataset%252C%2520showcase%2520the%2520robustness%250Aand%2520effectiveness%2520of%2520the%2520model%2520in%2520both%2520clear%2520and%2520degraded%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raci-Net%3A%20Ego-vehicle%20Odometry%20Estimation%20in%20Adverse%20Weather%20Conditions&entry.906535625=Mohammadhossein%20Talebi%20and%20Pragyan%20Dahal%20and%20Davide%20Possenti%20and%20Stefano%20Arrigoni%20and%20Francesco%20Braghin&entry.1292438233=%20%20Autonomous%20driving%20systems%20are%20highly%20dependent%20on%20sensors%20like%20cameras%2C%0ALiDAR%2C%20and%20inertial%20measurement%20units%20%28IMU%29%20to%20perceive%20the%20environment%20and%0Aestimate%20their%20motion.%20Among%20these%20sensors%2C%20perception-based%20sensors%20are%20not%0Aprotected%20from%20harsh%20weather%20and%20technical%20failures.%20Although%20existing%20methods%0Ashow%20robustness%20against%20common%20technical%20issues%20like%20rotational%20misalignment%0Aand%20disconnection%2C%20they%20often%20degrade%20when%20faced%20with%20dynamic%20environmental%0Afactors%20like%20weather%20conditions.%20To%20address%20these%20problems%2C%20this%20research%0Aintroduces%20a%20novel%20deep%20learning-based%20motion%20estimator%20that%20integrates%20visual%2C%0Ainertial%2C%20and%20millimeter-wave%20radar%20data%2C%20utilizing%20each%20sensor%20strengths%20to%0Aimprove%20odometry%20estimation%20accuracy%20and%20reliability%20under%20adverse%0Aenvironmental%20conditions%20such%20as%20snow%2C%20rain%2C%20and%20varying%20light.%20The%20proposed%0Amodel%20uses%20advanced%20sensor%20fusion%20techniques%20that%20dynamically%20adjust%20the%0Acontributions%20of%20each%20sensor%20based%20on%20the%20current%20environmental%20condition%2C%20with%0Aradar%20compensating%20for%20visual%20sensor%20limitations%20in%20poor%20visibility.%20This%20work%0Aexplores%20recent%20advancements%20in%20radar-based%20odometry%20and%20highlights%20that%20radar%0Arobustness%20in%20different%20weather%20conditions%20makes%20it%20a%20valuable%20component%20for%0Apose%20estimation%20systems%2C%20specifically%20when%20visual%20sensors%20are%20degraded.%0AExperimental%20results%2C%20conducted%20on%20the%20Boreas%20dataset%2C%20showcase%20the%20robustness%0Aand%20effectiveness%20of%20the%20model%20in%20both%20clear%20and%20degraded%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10376v1&entry.124074799=Read"},
{"title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping", "author": "Obaidullah Zaland and Erik Elmroth and Monowar Bhuyan", "abstract": "  Federated Learning (FL) is a promising machine learning paradigm that enables\nparticipating devices to train privacy-preserved and collaborative models. FL\nhas proven its benefits for robotic manipulation tasks. However, grasping tasks\nlack exploration in such settings where robots train a global model without\nmoving data and ensuring data privacy. The main challenge is that each robot\nlearns from data that is nonindependent and identically distributed (non-IID)\nand of low quantity. This exhibits performance degradation, particularly in\nrobotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL\napproach for robotic grasping, acknowledging the unique challenges posed by the\nnon-IID data distribution across robots, including quantitative skewness.\nMTF-Grasp harnesses data quality and quantity across robots to select a set of\n\"top-level\" robots with better data distribution and higher sample count. It\nthen utilizes top-level robots to train initial seed models and distribute them\nto the remaining \"low-level\" robots, reducing the risk of model performance\ndegradation in low-level robots. Our approach outperforms the conventional FL\nsetup by up to 8% on the quantity-skewed Cornell and Jacquard grasping\ndatasets.\n", "link": "http://arxiv.org/abs/2507.10158v1", "date": "2025-07-14", "relevancy": 2.2697, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6074}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTF-Grasp%3A%20A%20Multi-tier%20Federated%20Learning%20Approach%20for%20Robotic%20Grasping&body=Title%3A%20MTF-Grasp%3A%20A%20Multi-tier%20Federated%20Learning%20Approach%20for%20Robotic%20Grasping%0AAuthor%3A%20Obaidullah%20Zaland%20and%20Erik%20Elmroth%20and%20Monowar%20Bhuyan%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20promising%20machine%20learning%20paradigm%20that%20enables%0Aparticipating%20devices%20to%20train%20privacy-preserved%20and%20collaborative%20models.%20FL%0Ahas%20proven%20its%20benefits%20for%20robotic%20manipulation%20tasks.%20However%2C%20grasping%20tasks%0Alack%20exploration%20in%20such%20settings%20where%20robots%20train%20a%20global%20model%20without%0Amoving%20data%20and%20ensuring%20data%20privacy.%20The%20main%20challenge%20is%20that%20each%20robot%0Alearns%20from%20data%20that%20is%20nonindependent%20and%20identically%20distributed%20%28non-IID%29%0Aand%20of%20low%20quantity.%20This%20exhibits%20performance%20degradation%2C%20particularly%20in%0Arobotic%20grasping.%20Thus%2C%20in%20this%20work%2C%20we%20propose%20MTF-Grasp%2C%20a%20multi-tier%20FL%0Aapproach%20for%20robotic%20grasping%2C%20acknowledging%20the%20unique%20challenges%20posed%20by%20the%0Anon-IID%20data%20distribution%20across%20robots%2C%20including%20quantitative%20skewness.%0AMTF-Grasp%20harnesses%20data%20quality%20and%20quantity%20across%20robots%20to%20select%20a%20set%20of%0A%22top-level%22%20robots%20with%20better%20data%20distribution%20and%20higher%20sample%20count.%20It%0Athen%20utilizes%20top-level%20robots%20to%20train%20initial%20seed%20models%20and%20distribute%20them%0Ato%20the%20remaining%20%22low-level%22%20robots%2C%20reducing%20the%20risk%20of%20model%20performance%0Adegradation%20in%20low-level%20robots.%20Our%20approach%20outperforms%20the%20conventional%20FL%0Asetup%20by%20up%20to%208%25%20on%20the%20quantity-skewed%20Cornell%20and%20Jacquard%20grasping%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTF-Grasp%253A%2520A%2520Multi-tier%2520Federated%2520Learning%2520Approach%2520for%2520Robotic%2520Grasping%26entry.906535625%3DObaidullah%2520Zaland%2520and%2520Erik%2520Elmroth%2520and%2520Monowar%2520Bhuyan%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520promising%2520machine%2520learning%2520paradigm%2520that%2520enables%250Aparticipating%2520devices%2520to%2520train%2520privacy-preserved%2520and%2520collaborative%2520models.%2520FL%250Ahas%2520proven%2520its%2520benefits%2520for%2520robotic%2520manipulation%2520tasks.%2520However%252C%2520grasping%2520tasks%250Alack%2520exploration%2520in%2520such%2520settings%2520where%2520robots%2520train%2520a%2520global%2520model%2520without%250Amoving%2520data%2520and%2520ensuring%2520data%2520privacy.%2520The%2520main%2520challenge%2520is%2520that%2520each%2520robot%250Alearns%2520from%2520data%2520that%2520is%2520nonindependent%2520and%2520identically%2520distributed%2520%2528non-IID%2529%250Aand%2520of%2520low%2520quantity.%2520This%2520exhibits%2520performance%2520degradation%252C%2520particularly%2520in%250Arobotic%2520grasping.%2520Thus%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520MTF-Grasp%252C%2520a%2520multi-tier%2520FL%250Aapproach%2520for%2520robotic%2520grasping%252C%2520acknowledging%2520the%2520unique%2520challenges%2520posed%2520by%2520the%250Anon-IID%2520data%2520distribution%2520across%2520robots%252C%2520including%2520quantitative%2520skewness.%250AMTF-Grasp%2520harnesses%2520data%2520quality%2520and%2520quantity%2520across%2520robots%2520to%2520select%2520a%2520set%2520of%250A%2522top-level%2522%2520robots%2520with%2520better%2520data%2520distribution%2520and%2520higher%2520sample%2520count.%2520It%250Athen%2520utilizes%2520top-level%2520robots%2520to%2520train%2520initial%2520seed%2520models%2520and%2520distribute%2520them%250Ato%2520the%2520remaining%2520%2522low-level%2522%2520robots%252C%2520reducing%2520the%2520risk%2520of%2520model%2520performance%250Adegradation%2520in%2520low-level%2520robots.%2520Our%2520approach%2520outperforms%2520the%2520conventional%2520FL%250Asetup%2520by%2520up%2520to%25208%2525%2520on%2520the%2520quantity-skewed%2520Cornell%2520and%2520Jacquard%2520grasping%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTF-Grasp%3A%20A%20Multi-tier%20Federated%20Learning%20Approach%20for%20Robotic%20Grasping&entry.906535625=Obaidullah%20Zaland%20and%20Erik%20Elmroth%20and%20Monowar%20Bhuyan&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20promising%20machine%20learning%20paradigm%20that%20enables%0Aparticipating%20devices%20to%20train%20privacy-preserved%20and%20collaborative%20models.%20FL%0Ahas%20proven%20its%20benefits%20for%20robotic%20manipulation%20tasks.%20However%2C%20grasping%20tasks%0Alack%20exploration%20in%20such%20settings%20where%20robots%20train%20a%20global%20model%20without%0Amoving%20data%20and%20ensuring%20data%20privacy.%20The%20main%20challenge%20is%20that%20each%20robot%0Alearns%20from%20data%20that%20is%20nonindependent%20and%20identically%20distributed%20%28non-IID%29%0Aand%20of%20low%20quantity.%20This%20exhibits%20performance%20degradation%2C%20particularly%20in%0Arobotic%20grasping.%20Thus%2C%20in%20this%20work%2C%20we%20propose%20MTF-Grasp%2C%20a%20multi-tier%20FL%0Aapproach%20for%20robotic%20grasping%2C%20acknowledging%20the%20unique%20challenges%20posed%20by%20the%0Anon-IID%20data%20distribution%20across%20robots%2C%20including%20quantitative%20skewness.%0AMTF-Grasp%20harnesses%20data%20quality%20and%20quantity%20across%20robots%20to%20select%20a%20set%20of%0A%22top-level%22%20robots%20with%20better%20data%20distribution%20and%20higher%20sample%20count.%20It%0Athen%20utilizes%20top-level%20robots%20to%20train%20initial%20seed%20models%20and%20distribute%20them%0Ato%20the%20remaining%20%22low-level%22%20robots%2C%20reducing%20the%20risk%20of%20model%20performance%0Adegradation%20in%20low-level%20robots.%20Our%20approach%20outperforms%20the%20conventional%20FL%0Asetup%20by%20up%20to%208%25%20on%20the%20quantity-skewed%20Cornell%20and%20Jacquard%20grasping%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10158v1&entry.124074799=Read"},
{"title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive\n  CVaR Barrier Functions", "author": "Xinyi Wang and Taekyung Kim and Bardh Hoxha and Georgios Fainekos and Dimitra Panagou", "abstract": "  Robot navigation in dynamic, crowded environments poses a significant\nchallenge due to the inherent uncertainties in the obstacle model. In this\nwork, we propose a risk-adaptive approach based on the Conditional\nValue-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically\nadjusted to accept the minimum necessary risk, achieving a good performance in\nterms of safety and optimization feasibility under uncertainty. Additionally,\nwe introduce a dynamic zone-based barrier function which characterizes the\ncollision likelihood by evaluating the relative state between the robot and the\nobstacle. By integrating risk adaptation with this new function, our approach\nadaptively expands the safety margin, enabling the robot to proactively avoid\nobstacles in highly dynamic environments. Comparisons and ablation studies\ndemonstrate that our method outperforms existing social navigation approaches,\nand validate the effectiveness of our proposed framework.\n", "link": "http://arxiv.org/abs/2504.06513v3", "date": "2025-07-14", "relevancy": 2.2486, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5853}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Navigation%20in%20Uncertain%20Crowded%20Environments%20Using%20Risk%20Adaptive%0A%20%20CVaR%20Barrier%20Functions&body=Title%3A%20Safe%20Navigation%20in%20Uncertain%20Crowded%20Environments%20Using%20Risk%20Adaptive%0A%20%20CVaR%20Barrier%20Functions%0AAuthor%3A%20Xinyi%20Wang%20and%20Taekyung%20Kim%20and%20Bardh%20Hoxha%20and%20Georgios%20Fainekos%20and%20Dimitra%20Panagou%0AAbstract%3A%20%20%20Robot%20navigation%20in%20dynamic%2C%20crowded%20environments%20poses%20a%20significant%0Achallenge%20due%20to%20the%20inherent%20uncertainties%20in%20the%20obstacle%20model.%20In%20this%0Awork%2C%20we%20propose%20a%20risk-adaptive%20approach%20based%20on%20the%20Conditional%0AValue-at-Risk%20Barrier%20Function%20%28CVaR-BF%29%2C%20where%20the%20risk%20level%20is%20automatically%0Aadjusted%20to%20accept%20the%20minimum%20necessary%20risk%2C%20achieving%20a%20good%20performance%20in%0Aterms%20of%20safety%20and%20optimization%20feasibility%20under%20uncertainty.%20Additionally%2C%0Awe%20introduce%20a%20dynamic%20zone-based%20barrier%20function%20which%20characterizes%20the%0Acollision%20likelihood%20by%20evaluating%20the%20relative%20state%20between%20the%20robot%20and%20the%0Aobstacle.%20By%20integrating%20risk%20adaptation%20with%20this%20new%20function%2C%20our%20approach%0Aadaptively%20expands%20the%20safety%20margin%2C%20enabling%20the%20robot%20to%20proactively%20avoid%0Aobstacles%20in%20highly%20dynamic%20environments.%20Comparisons%20and%20ablation%20studies%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20social%20navigation%20approaches%2C%0Aand%20validate%20the%20effectiveness%20of%20our%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06513v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Navigation%2520in%2520Uncertain%2520Crowded%2520Environments%2520Using%2520Risk%2520Adaptive%250A%2520%2520CVaR%2520Barrier%2520Functions%26entry.906535625%3DXinyi%2520Wang%2520and%2520Taekyung%2520Kim%2520and%2520Bardh%2520Hoxha%2520and%2520Georgios%2520Fainekos%2520and%2520Dimitra%2520Panagou%26entry.1292438233%3D%2520%2520Robot%2520navigation%2520in%2520dynamic%252C%2520crowded%2520environments%2520poses%2520a%2520significant%250Achallenge%2520due%2520to%2520the%2520inherent%2520uncertainties%2520in%2520the%2520obstacle%2520model.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520risk-adaptive%2520approach%2520based%2520on%2520the%2520Conditional%250AValue-at-Risk%2520Barrier%2520Function%2520%2528CVaR-BF%2529%252C%2520where%2520the%2520risk%2520level%2520is%2520automatically%250Aadjusted%2520to%2520accept%2520the%2520minimum%2520necessary%2520risk%252C%2520achieving%2520a%2520good%2520performance%2520in%250Aterms%2520of%2520safety%2520and%2520optimization%2520feasibility%2520under%2520uncertainty.%2520Additionally%252C%250Awe%2520introduce%2520a%2520dynamic%2520zone-based%2520barrier%2520function%2520which%2520characterizes%2520the%250Acollision%2520likelihood%2520by%2520evaluating%2520the%2520relative%2520state%2520between%2520the%2520robot%2520and%2520the%250Aobstacle.%2520By%2520integrating%2520risk%2520adaptation%2520with%2520this%2520new%2520function%252C%2520our%2520approach%250Aadaptively%2520expands%2520the%2520safety%2520margin%252C%2520enabling%2520the%2520robot%2520to%2520proactively%2520avoid%250Aobstacles%2520in%2520highly%2520dynamic%2520environments.%2520Comparisons%2520and%2520ablation%2520studies%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520social%2520navigation%2520approaches%252C%250Aand%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06513v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Navigation%20in%20Uncertain%20Crowded%20Environments%20Using%20Risk%20Adaptive%0A%20%20CVaR%20Barrier%20Functions&entry.906535625=Xinyi%20Wang%20and%20Taekyung%20Kim%20and%20Bardh%20Hoxha%20and%20Georgios%20Fainekos%20and%20Dimitra%20Panagou&entry.1292438233=%20%20Robot%20navigation%20in%20dynamic%2C%20crowded%20environments%20poses%20a%20significant%0Achallenge%20due%20to%20the%20inherent%20uncertainties%20in%20the%20obstacle%20model.%20In%20this%0Awork%2C%20we%20propose%20a%20risk-adaptive%20approach%20based%20on%20the%20Conditional%0AValue-at-Risk%20Barrier%20Function%20%28CVaR-BF%29%2C%20where%20the%20risk%20level%20is%20automatically%0Aadjusted%20to%20accept%20the%20minimum%20necessary%20risk%2C%20achieving%20a%20good%20performance%20in%0Aterms%20of%20safety%20and%20optimization%20feasibility%20under%20uncertainty.%20Additionally%2C%0Awe%20introduce%20a%20dynamic%20zone-based%20barrier%20function%20which%20characterizes%20the%0Acollision%20likelihood%20by%20evaluating%20the%20relative%20state%20between%20the%20robot%20and%20the%0Aobstacle.%20By%20integrating%20risk%20adaptation%20with%20this%20new%20function%2C%20our%20approach%0Aadaptively%20expands%20the%20safety%20margin%2C%20enabling%20the%20robot%20to%20proactively%20avoid%0Aobstacles%20in%20highly%20dynamic%20environments.%20Comparisons%20and%20ablation%20studies%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20social%20navigation%20approaches%2C%0Aand%20validate%20the%20effectiveness%20of%20our%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06513v3&entry.124074799=Read"},
{"title": "Boosting Multimodal Learning via Disentangled Gradient Learning", "author": "Shicai Wei and Chunbo Luo and Yang Luo", "abstract": "  Multimodal learning often encounters the under-optimized problem and may have\nworse performance than unimodal learning. Existing methods attribute this\nproblem to the imbalanced learning between modalities and rebalance them\nthrough gradient modulation. However, they fail to explain why the dominant\nmodality in multimodal models also underperforms that in unimodal learning. In\nthis work, we reveal the optimization conflict between the modality encoder and\nmodality fusion module in multimodal models. Specifically, we prove that the\ncross-modal fusion in multimodal models decreases the gradient passed back to\neach modality encoder compared with unimodal models. Consequently, the\nperformance of each modality in the multimodal model is inferior to that in the\nunimodal model. To this end, we propose a disentangled gradient learning (DGL)\nframework to decouple the optimization of the modality encoder and modality\nfusion module in the multimodal model. DGL truncates the gradient\nback-propagated from the multimodal loss to the modality encoder and replaces\nit with the gradient from unimodal loss. Besides, DGL removes the gradient\nback-propagated from the unimodal loss to the modality fusion module. This\nhelps eliminate the gradient interference between the modality encoder and\nmodality fusion module while ensuring their respective optimization processes.\nFinally, extensive experiments on multiple types of modalities, tasks, and\nframeworks with dense cross-modal interaction demonstrate the effectiveness and\nversatility of the proposed DGL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}\n", "link": "http://arxiv.org/abs/2507.10213v1", "date": "2025-07-14", "relevancy": 2.2409, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Multimodal%20Learning%20via%20Disentangled%20Gradient%20Learning&body=Title%3A%20Boosting%20Multimodal%20Learning%20via%20Disentangled%20Gradient%20Learning%0AAuthor%3A%20Shicai%20Wei%20and%20Chunbo%20Luo%20and%20Yang%20Luo%0AAbstract%3A%20%20%20Multimodal%20learning%20often%20encounters%20the%20under-optimized%20problem%20and%20may%20have%0Aworse%20performance%20than%20unimodal%20learning.%20Existing%20methods%20attribute%20this%0Aproblem%20to%20the%20imbalanced%20learning%20between%20modalities%20and%20rebalance%20them%0Athrough%20gradient%20modulation.%20However%2C%20they%20fail%20to%20explain%20why%20the%20dominant%0Amodality%20in%20multimodal%20models%20also%20underperforms%20that%20in%20unimodal%20learning.%20In%0Athis%20work%2C%20we%20reveal%20the%20optimization%20conflict%20between%20the%20modality%20encoder%20and%0Amodality%20fusion%20module%20in%20multimodal%20models.%20Specifically%2C%20we%20prove%20that%20the%0Across-modal%20fusion%20in%20multimodal%20models%20decreases%20the%20gradient%20passed%20back%20to%0Aeach%20modality%20encoder%20compared%20with%20unimodal%20models.%20Consequently%2C%20the%0Aperformance%20of%20each%20modality%20in%20the%20multimodal%20model%20is%20inferior%20to%20that%20in%20the%0Aunimodal%20model.%20To%20this%20end%2C%20we%20propose%20a%20disentangled%20gradient%20learning%20%28DGL%29%0Aframework%20to%20decouple%20the%20optimization%20of%20the%20modality%20encoder%20and%20modality%0Afusion%20module%20in%20the%20multimodal%20model.%20DGL%20truncates%20the%20gradient%0Aback-propagated%20from%20the%20multimodal%20loss%20to%20the%20modality%20encoder%20and%20replaces%0Ait%20with%20the%20gradient%20from%20unimodal%20loss.%20Besides%2C%20DGL%20removes%20the%20gradient%0Aback-propagated%20from%20the%20unimodal%20loss%20to%20the%20modality%20fusion%20module.%20This%0Ahelps%20eliminate%20the%20gradient%20interference%20between%20the%20modality%20encoder%20and%0Amodality%20fusion%20module%20while%20ensuring%20their%20respective%20optimization%20processes.%0AFinally%2C%20extensive%20experiments%20on%20multiple%20types%20of%20modalities%2C%20tasks%2C%20and%0Aframeworks%20with%20dense%20cross-modal%20interaction%20demonstrate%20the%20effectiveness%20and%0Aversatility%20of%20the%20proposed%20DGL.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/shicaiwei123/ICCV2025-GDL%7D%7Bhttps%3A//github.com/shicaiwei123/ICCV2025-GDL%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Multimodal%2520Learning%2520via%2520Disentangled%2520Gradient%2520Learning%26entry.906535625%3DShicai%2520Wei%2520and%2520Chunbo%2520Luo%2520and%2520Yang%2520Luo%26entry.1292438233%3D%2520%2520Multimodal%2520learning%2520often%2520encounters%2520the%2520under-optimized%2520problem%2520and%2520may%2520have%250Aworse%2520performance%2520than%2520unimodal%2520learning.%2520Existing%2520methods%2520attribute%2520this%250Aproblem%2520to%2520the%2520imbalanced%2520learning%2520between%2520modalities%2520and%2520rebalance%2520them%250Athrough%2520gradient%2520modulation.%2520However%252C%2520they%2520fail%2520to%2520explain%2520why%2520the%2520dominant%250Amodality%2520in%2520multimodal%2520models%2520also%2520underperforms%2520that%2520in%2520unimodal%2520learning.%2520In%250Athis%2520work%252C%2520we%2520reveal%2520the%2520optimization%2520conflict%2520between%2520the%2520modality%2520encoder%2520and%250Amodality%2520fusion%2520module%2520in%2520multimodal%2520models.%2520Specifically%252C%2520we%2520prove%2520that%2520the%250Across-modal%2520fusion%2520in%2520multimodal%2520models%2520decreases%2520the%2520gradient%2520passed%2520back%2520to%250Aeach%2520modality%2520encoder%2520compared%2520with%2520unimodal%2520models.%2520Consequently%252C%2520the%250Aperformance%2520of%2520each%2520modality%2520in%2520the%2520multimodal%2520model%2520is%2520inferior%2520to%2520that%2520in%2520the%250Aunimodal%2520model.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520disentangled%2520gradient%2520learning%2520%2528DGL%2529%250Aframework%2520to%2520decouple%2520the%2520optimization%2520of%2520the%2520modality%2520encoder%2520and%2520modality%250Afusion%2520module%2520in%2520the%2520multimodal%2520model.%2520DGL%2520truncates%2520the%2520gradient%250Aback-propagated%2520from%2520the%2520multimodal%2520loss%2520to%2520the%2520modality%2520encoder%2520and%2520replaces%250Ait%2520with%2520the%2520gradient%2520from%2520unimodal%2520loss.%2520Besides%252C%2520DGL%2520removes%2520the%2520gradient%250Aback-propagated%2520from%2520the%2520unimodal%2520loss%2520to%2520the%2520modality%2520fusion%2520module.%2520This%250Ahelps%2520eliminate%2520the%2520gradient%2520interference%2520between%2520the%2520modality%2520encoder%2520and%250Amodality%2520fusion%2520module%2520while%2520ensuring%2520their%2520respective%2520optimization%2520processes.%250AFinally%252C%2520extensive%2520experiments%2520on%2520multiple%2520types%2520of%2520modalities%252C%2520tasks%252C%2520and%250Aframeworks%2520with%2520dense%2520cross-modal%2520interaction%2520demonstrate%2520the%2520effectiveness%2520and%250Aversatility%2520of%2520the%2520proposed%2520DGL.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/shicaiwei123/ICCV2025-GDL%257D%257Bhttps%253A//github.com/shicaiwei123/ICCV2025-GDL%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Multimodal%20Learning%20via%20Disentangled%20Gradient%20Learning&entry.906535625=Shicai%20Wei%20and%20Chunbo%20Luo%20and%20Yang%20Luo&entry.1292438233=%20%20Multimodal%20learning%20often%20encounters%20the%20under-optimized%20problem%20and%20may%20have%0Aworse%20performance%20than%20unimodal%20learning.%20Existing%20methods%20attribute%20this%0Aproblem%20to%20the%20imbalanced%20learning%20between%20modalities%20and%20rebalance%20them%0Athrough%20gradient%20modulation.%20However%2C%20they%20fail%20to%20explain%20why%20the%20dominant%0Amodality%20in%20multimodal%20models%20also%20underperforms%20that%20in%20unimodal%20learning.%20In%0Athis%20work%2C%20we%20reveal%20the%20optimization%20conflict%20between%20the%20modality%20encoder%20and%0Amodality%20fusion%20module%20in%20multimodal%20models.%20Specifically%2C%20we%20prove%20that%20the%0Across-modal%20fusion%20in%20multimodal%20models%20decreases%20the%20gradient%20passed%20back%20to%0Aeach%20modality%20encoder%20compared%20with%20unimodal%20models.%20Consequently%2C%20the%0Aperformance%20of%20each%20modality%20in%20the%20multimodal%20model%20is%20inferior%20to%20that%20in%20the%0Aunimodal%20model.%20To%20this%20end%2C%20we%20propose%20a%20disentangled%20gradient%20learning%20%28DGL%29%0Aframework%20to%20decouple%20the%20optimization%20of%20the%20modality%20encoder%20and%20modality%0Afusion%20module%20in%20the%20multimodal%20model.%20DGL%20truncates%20the%20gradient%0Aback-propagated%20from%20the%20multimodal%20loss%20to%20the%20modality%20encoder%20and%20replaces%0Ait%20with%20the%20gradient%20from%20unimodal%20loss.%20Besides%2C%20DGL%20removes%20the%20gradient%0Aback-propagated%20from%20the%20unimodal%20loss%20to%20the%20modality%20fusion%20module.%20This%0Ahelps%20eliminate%20the%20gradient%20interference%20between%20the%20modality%20encoder%20and%0Amodality%20fusion%20module%20while%20ensuring%20their%20respective%20optimization%20processes.%0AFinally%2C%20extensive%20experiments%20on%20multiple%20types%20of%20modalities%2C%20tasks%2C%20and%0Aframeworks%20with%20dense%20cross-modal%20interaction%20demonstrate%20the%20effectiveness%20and%0Aversatility%20of%20the%20proposed%20DGL.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/shicaiwei123/ICCV2025-GDL%7D%7Bhttps%3A//github.com/shicaiwei123/ICCV2025-GDL%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10213v1&entry.124074799=Read"},
{"title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation", "author": "Sangmin Bae and Yujin Kim and Reza Bayat and Sungnyun Kim and Jiyoun Ha and Tal Schuster and Adam Fisch and Hrayr Harutyunyan and Ziwei Ji and Aaron Courville and Se-Young Yun", "abstract": "  Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.\n", "link": "http://arxiv.org/abs/2507.10524v1", "date": "2025-07-14", "relevancy": 2.2282, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5871}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5617}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture-of-Recursions%3A%20Learning%20Dynamic%20Recursive%20Depths%20for%20Adaptive%0A%20%20Token-Level%20Computation&body=Title%3A%20Mixture-of-Recursions%3A%20Learning%20Dynamic%20Recursive%20Depths%20for%20Adaptive%0A%20%20Token-Level%20Computation%0AAuthor%3A%20Sangmin%20Bae%20and%20Yujin%20Kim%20and%20Reza%20Bayat%20and%20Sungnyun%20Kim%20and%20Jiyoun%20Ha%20and%20Tal%20Schuster%20and%20Adam%20Fisch%20and%20Hrayr%20Harutyunyan%20and%20Ziwei%20Ji%20and%20Aaron%20Courville%20and%20Se-Young%20Yun%0AAbstract%3A%20%20%20Scaling%20language%20models%20unlocks%20impressive%20capabilities%2C%20but%20the%20accompanying%0Acomputational%20and%20memory%20demands%20make%20both%20training%20and%20deployment%20expensive.%0AExisting%20efficiency%20efforts%20typically%20target%20either%20parameter%20sharing%20or%0Aadaptive%20computation%2C%20leaving%20open%20the%20question%20of%20how%20to%20attain%20both%0Asimultaneously.%20We%20introduce%20Mixture-of-Recursions%20%28MoR%29%2C%20a%20unified%20framework%0Athat%20combines%20the%20two%20axes%20of%20efficiency%20inside%20a%20single%20Recursive%20Transformer.%0AMoR%20reuses%20a%20shared%20stack%20of%20layers%20across%20recursion%20steps%20to%20achieve%20parameter%0Aefficiency%2C%20while%20lightweight%20routers%20enable%20adaptive%20token-level%20thinking%20by%0Adynamically%20assigning%20different%20recursion%20depths%20to%20individual%20tokens.%20This%0Aallows%20MoR%20to%20focus%20quadratic%20attention%20computation%20only%20among%20tokens%20still%0Aactive%20at%20a%20given%20recursion%20depth%2C%20further%20improving%20memory%20access%20efficiency%0Aby%20selectively%20caching%20only%20their%20key-value%20pairs.%20Beyond%20these%20core%0Amechanisms%2C%20we%20also%20propose%20a%20KV%20sharing%20variant%20that%20reuses%20KV%20pairs%20from%20the%0Afirst%20recursion%2C%20specifically%20designed%20to%20decrease%20prefill%20latency%20and%20memory%0Afootprint.%20Across%20model%20scales%20ranging%20from%20135M%20to%201.7B%20parameters%2C%20MoR%20forms%0Aa%20new%20Pareto%20frontier%3A%20at%20equal%20training%20FLOPs%20and%20smaller%20model%20sizes%2C%20it%0Asignificantly%20lowers%20validation%20perplexity%20and%20improves%20few-shot%20accuracy%2C%0Awhile%20delivering%20higher%20throughput%20compared%20with%20vanilla%20and%20existing%20recursive%0Abaselines.%20These%20gains%20demonstrate%20that%20MoR%20is%20an%20effective%20path%20towards%0Alarge-model%20quality%20without%20incurring%20large-model%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture-of-Recursions%253A%2520Learning%2520Dynamic%2520Recursive%2520Depths%2520for%2520Adaptive%250A%2520%2520Token-Level%2520Computation%26entry.906535625%3DSangmin%2520Bae%2520and%2520Yujin%2520Kim%2520and%2520Reza%2520Bayat%2520and%2520Sungnyun%2520Kim%2520and%2520Jiyoun%2520Ha%2520and%2520Tal%2520Schuster%2520and%2520Adam%2520Fisch%2520and%2520Hrayr%2520Harutyunyan%2520and%2520Ziwei%2520Ji%2520and%2520Aaron%2520Courville%2520and%2520Se-Young%2520Yun%26entry.1292438233%3D%2520%2520Scaling%2520language%2520models%2520unlocks%2520impressive%2520capabilities%252C%2520but%2520the%2520accompanying%250Acomputational%2520and%2520memory%2520demands%2520make%2520both%2520training%2520and%2520deployment%2520expensive.%250AExisting%2520efficiency%2520efforts%2520typically%2520target%2520either%2520parameter%2520sharing%2520or%250Aadaptive%2520computation%252C%2520leaving%2520open%2520the%2520question%2520of%2520how%2520to%2520attain%2520both%250Asimultaneously.%2520We%2520introduce%2520Mixture-of-Recursions%2520%2528MoR%2529%252C%2520a%2520unified%2520framework%250Athat%2520combines%2520the%2520two%2520axes%2520of%2520efficiency%2520inside%2520a%2520single%2520Recursive%2520Transformer.%250AMoR%2520reuses%2520a%2520shared%2520stack%2520of%2520layers%2520across%2520recursion%2520steps%2520to%2520achieve%2520parameter%250Aefficiency%252C%2520while%2520lightweight%2520routers%2520enable%2520adaptive%2520token-level%2520thinking%2520by%250Adynamically%2520assigning%2520different%2520recursion%2520depths%2520to%2520individual%2520tokens.%2520This%250Aallows%2520MoR%2520to%2520focus%2520quadratic%2520attention%2520computation%2520only%2520among%2520tokens%2520still%250Aactive%2520at%2520a%2520given%2520recursion%2520depth%252C%2520further%2520improving%2520memory%2520access%2520efficiency%250Aby%2520selectively%2520caching%2520only%2520their%2520key-value%2520pairs.%2520Beyond%2520these%2520core%250Amechanisms%252C%2520we%2520also%2520propose%2520a%2520KV%2520sharing%2520variant%2520that%2520reuses%2520KV%2520pairs%2520from%2520the%250Afirst%2520recursion%252C%2520specifically%2520designed%2520to%2520decrease%2520prefill%2520latency%2520and%2520memory%250Afootprint.%2520Across%2520model%2520scales%2520ranging%2520from%2520135M%2520to%25201.7B%2520parameters%252C%2520MoR%2520forms%250Aa%2520new%2520Pareto%2520frontier%253A%2520at%2520equal%2520training%2520FLOPs%2520and%2520smaller%2520model%2520sizes%252C%2520it%250Asignificantly%2520lowers%2520validation%2520perplexity%2520and%2520improves%2520few-shot%2520accuracy%252C%250Awhile%2520delivering%2520higher%2520throughput%2520compared%2520with%2520vanilla%2520and%2520existing%2520recursive%250Abaselines.%2520These%2520gains%2520demonstrate%2520that%2520MoR%2520is%2520an%2520effective%2520path%2520towards%250Alarge-model%2520quality%2520without%2520incurring%2520large-model%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture-of-Recursions%3A%20Learning%20Dynamic%20Recursive%20Depths%20for%20Adaptive%0A%20%20Token-Level%20Computation&entry.906535625=Sangmin%20Bae%20and%20Yujin%20Kim%20and%20Reza%20Bayat%20and%20Sungnyun%20Kim%20and%20Jiyoun%20Ha%20and%20Tal%20Schuster%20and%20Adam%20Fisch%20and%20Hrayr%20Harutyunyan%20and%20Ziwei%20Ji%20and%20Aaron%20Courville%20and%20Se-Young%20Yun&entry.1292438233=%20%20Scaling%20language%20models%20unlocks%20impressive%20capabilities%2C%20but%20the%20accompanying%0Acomputational%20and%20memory%20demands%20make%20both%20training%20and%20deployment%20expensive.%0AExisting%20efficiency%20efforts%20typically%20target%20either%20parameter%20sharing%20or%0Aadaptive%20computation%2C%20leaving%20open%20the%20question%20of%20how%20to%20attain%20both%0Asimultaneously.%20We%20introduce%20Mixture-of-Recursions%20%28MoR%29%2C%20a%20unified%20framework%0Athat%20combines%20the%20two%20axes%20of%20efficiency%20inside%20a%20single%20Recursive%20Transformer.%0AMoR%20reuses%20a%20shared%20stack%20of%20layers%20across%20recursion%20steps%20to%20achieve%20parameter%0Aefficiency%2C%20while%20lightweight%20routers%20enable%20adaptive%20token-level%20thinking%20by%0Adynamically%20assigning%20different%20recursion%20depths%20to%20individual%20tokens.%20This%0Aallows%20MoR%20to%20focus%20quadratic%20attention%20computation%20only%20among%20tokens%20still%0Aactive%20at%20a%20given%20recursion%20depth%2C%20further%20improving%20memory%20access%20efficiency%0Aby%20selectively%20caching%20only%20their%20key-value%20pairs.%20Beyond%20these%20core%0Amechanisms%2C%20we%20also%20propose%20a%20KV%20sharing%20variant%20that%20reuses%20KV%20pairs%20from%20the%0Afirst%20recursion%2C%20specifically%20designed%20to%20decrease%20prefill%20latency%20and%20memory%0Afootprint.%20Across%20model%20scales%20ranging%20from%20135M%20to%201.7B%20parameters%2C%20MoR%20forms%0Aa%20new%20Pareto%20frontier%3A%20at%20equal%20training%20FLOPs%20and%20smaller%20model%20sizes%2C%20it%0Asignificantly%20lowers%20validation%20perplexity%20and%20improves%20few-shot%20accuracy%2C%0Awhile%20delivering%20higher%20throughput%20compared%20with%20vanilla%20and%20existing%20recursive%0Abaselines.%20These%20gains%20demonstrate%20that%20MoR%20is%20an%20effective%20path%20towards%0Alarge-model%20quality%20without%20incurring%20large-model%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10524v1&entry.124074799=Read"},
{"title": "Quantize-then-Rectify: Efficient VQ-VAE Training", "author": "Borui Zhang and Qihang Rao and Wenzhao Zheng and Jie Zhou and Jiwen Lu", "abstract": "  Visual tokenizers are pivotal in multimodal large models, acting as bridges\nbetween continuous inputs and discrete tokens. Nevertheless, training\nhigh-compression-rate VQ-VAEs remains computationally demanding, often\nnecessitating thousands of GPU hours. This work demonstrates that a pre-trained\nVAE can be efficiently transformed into a VQ-VAE by controlling quantization\nnoise within the VAE's tolerance threshold. We present\n\\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs\nto enable rapid VQ-VAE training with minimal computational overhead. By\nintegrating \\textbf{channel multi-group quantization} to enlarge codebook\ncapacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ\ncompresses ImageNet images into at most 512 tokens while sustaining competitive\nreconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training\ncosts by over two orders of magnitude relative to state-of-the-art approaches:\nReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,\nwhereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental\nresults show that ReVQ achieves superior efficiency-reconstruction trade-offs.\n", "link": "http://arxiv.org/abs/2507.10547v1", "date": "2025-07-14", "relevancy": 2.2185, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5633}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5554}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantize-then-Rectify%3A%20Efficient%20VQ-VAE%20Training&body=Title%3A%20Quantize-then-Rectify%3A%20Efficient%20VQ-VAE%20Training%0AAuthor%3A%20Borui%20Zhang%20and%20Qihang%20Rao%20and%20Wenzhao%20Zheng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Visual%20tokenizers%20are%20pivotal%20in%20multimodal%20large%20models%2C%20acting%20as%20bridges%0Abetween%20continuous%20inputs%20and%20discrete%20tokens.%20Nevertheless%2C%20training%0Ahigh-compression-rate%20VQ-VAEs%20remains%20computationally%20demanding%2C%20often%0Anecessitating%20thousands%20of%20GPU%20hours.%20This%20work%20demonstrates%20that%20a%20pre-trained%0AVAE%20can%20be%20efficiently%20transformed%20into%20a%20VQ-VAE%20by%20controlling%20quantization%0Anoise%20within%20the%20VAE%27s%20tolerance%20threshold.%20We%20present%0A%5Ctextbf%7BQuantize-then-Rectify%20%28ReVQ%29%7D%2C%20a%20framework%20leveraging%20pre-trained%20VAEs%0Ato%20enable%20rapid%20VQ-VAE%20training%20with%20minimal%20computational%20overhead.%20By%0Aintegrating%20%5Ctextbf%7Bchannel%20multi-group%20quantization%7D%20to%20enlarge%20codebook%0Acapacity%20and%20a%20%5Ctextbf%7Bpost%20rectifier%7D%20to%20mitigate%20quantization%20errors%2C%20ReVQ%0Acompresses%20ImageNet%20images%20into%20at%20most%20512%20tokens%20while%20sustaining%20competitive%0Areconstruction%20quality%20%28rFID%20%3D%201.06%29.%20Significantly%2C%20ReVQ%20reduces%20training%0Acosts%20by%20over%20two%20orders%20of%20magnitude%20relative%20to%20state-of-the-art%20approaches%3A%0AReVQ%20finishes%20full%20training%20on%20a%20single%20NVIDIA%204090%20in%20approximately%2022%20hours%2C%0Awhereas%20comparable%20methods%20require%204.5%20days%20on%2032%20A100%20GPUs.%20Experimental%0Aresults%20show%20that%20ReVQ%20achieves%20superior%20efficiency-reconstruction%20trade-offs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantize-then-Rectify%253A%2520Efficient%2520VQ-VAE%2520Training%26entry.906535625%3DBorui%2520Zhang%2520and%2520Qihang%2520Rao%2520and%2520Wenzhao%2520Zheng%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Visual%2520tokenizers%2520are%2520pivotal%2520in%2520multimodal%2520large%2520models%252C%2520acting%2520as%2520bridges%250Abetween%2520continuous%2520inputs%2520and%2520discrete%2520tokens.%2520Nevertheless%252C%2520training%250Ahigh-compression-rate%2520VQ-VAEs%2520remains%2520computationally%2520demanding%252C%2520often%250Anecessitating%2520thousands%2520of%2520GPU%2520hours.%2520This%2520work%2520demonstrates%2520that%2520a%2520pre-trained%250AVAE%2520can%2520be%2520efficiently%2520transformed%2520into%2520a%2520VQ-VAE%2520by%2520controlling%2520quantization%250Anoise%2520within%2520the%2520VAE%2527s%2520tolerance%2520threshold.%2520We%2520present%250A%255Ctextbf%257BQuantize-then-Rectify%2520%2528ReVQ%2529%257D%252C%2520a%2520framework%2520leveraging%2520pre-trained%2520VAEs%250Ato%2520enable%2520rapid%2520VQ-VAE%2520training%2520with%2520minimal%2520computational%2520overhead.%2520By%250Aintegrating%2520%255Ctextbf%257Bchannel%2520multi-group%2520quantization%257D%2520to%2520enlarge%2520codebook%250Acapacity%2520and%2520a%2520%255Ctextbf%257Bpost%2520rectifier%257D%2520to%2520mitigate%2520quantization%2520errors%252C%2520ReVQ%250Acompresses%2520ImageNet%2520images%2520into%2520at%2520most%2520512%2520tokens%2520while%2520sustaining%2520competitive%250Areconstruction%2520quality%2520%2528rFID%2520%253D%25201.06%2529.%2520Significantly%252C%2520ReVQ%2520reduces%2520training%250Acosts%2520by%2520over%2520two%2520orders%2520of%2520magnitude%2520relative%2520to%2520state-of-the-art%2520approaches%253A%250AReVQ%2520finishes%2520full%2520training%2520on%2520a%2520single%2520NVIDIA%25204090%2520in%2520approximately%252022%2520hours%252C%250Awhereas%2520comparable%2520methods%2520require%25204.5%2520days%2520on%252032%2520A100%2520GPUs.%2520Experimental%250Aresults%2520show%2520that%2520ReVQ%2520achieves%2520superior%2520efficiency-reconstruction%2520trade-offs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantize-then-Rectify%3A%20Efficient%20VQ-VAE%20Training&entry.906535625=Borui%20Zhang%20and%20Qihang%20Rao%20and%20Wenzhao%20Zheng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Visual%20tokenizers%20are%20pivotal%20in%20multimodal%20large%20models%2C%20acting%20as%20bridges%0Abetween%20continuous%20inputs%20and%20discrete%20tokens.%20Nevertheless%2C%20training%0Ahigh-compression-rate%20VQ-VAEs%20remains%20computationally%20demanding%2C%20often%0Anecessitating%20thousands%20of%20GPU%20hours.%20This%20work%20demonstrates%20that%20a%20pre-trained%0AVAE%20can%20be%20efficiently%20transformed%20into%20a%20VQ-VAE%20by%20controlling%20quantization%0Anoise%20within%20the%20VAE%27s%20tolerance%20threshold.%20We%20present%0A%5Ctextbf%7BQuantize-then-Rectify%20%28ReVQ%29%7D%2C%20a%20framework%20leveraging%20pre-trained%20VAEs%0Ato%20enable%20rapid%20VQ-VAE%20training%20with%20minimal%20computational%20overhead.%20By%0Aintegrating%20%5Ctextbf%7Bchannel%20multi-group%20quantization%7D%20to%20enlarge%20codebook%0Acapacity%20and%20a%20%5Ctextbf%7Bpost%20rectifier%7D%20to%20mitigate%20quantization%20errors%2C%20ReVQ%0Acompresses%20ImageNet%20images%20into%20at%20most%20512%20tokens%20while%20sustaining%20competitive%0Areconstruction%20quality%20%28rFID%20%3D%201.06%29.%20Significantly%2C%20ReVQ%20reduces%20training%0Acosts%20by%20over%20two%20orders%20of%20magnitude%20relative%20to%20state-of-the-art%20approaches%3A%0AReVQ%20finishes%20full%20training%20on%20a%20single%20NVIDIA%204090%20in%20approximately%2022%20hours%2C%0Awhereas%20comparable%20methods%20require%204.5%20days%20on%2032%20A100%20GPUs.%20Experimental%0Aresults%20show%20that%20ReVQ%20achieves%20superior%20efficiency-reconstruction%20trade-offs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10547v1&entry.124074799=Read"},
{"title": "Glance-MCMT: A General MCMT Framework with Glance Initialization and\n  Progressive Association", "author": "Hamidreza Hashempoor", "abstract": "  We propose a multi-camera multi-target (MCMT) tracking framework that ensures\nconsistent global identity assignment across views using trajectory and\nappearance cues. The pipeline starts with BoT-SORT-based single-camera\ntracking, followed by an initial glance phase to initialize global IDs via\ntrajectory-feature matching. In later frames, new tracklets are matched to\nexisting global identities through a prioritized global matching strategy. New\nglobal IDs are only introduced when no sufficiently similar trajectory or\nfeature match is found. 3D positions are estimated using depth maps and\ncalibration for spatial validation.\n", "link": "http://arxiv.org/abs/2507.10115v1", "date": "2025-07-14", "relevancy": 2.2133, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glance-MCMT%3A%20A%20General%20MCMT%20Framework%20with%20Glance%20Initialization%20and%0A%20%20Progressive%20Association&body=Title%3A%20Glance-MCMT%3A%20A%20General%20MCMT%20Framework%20with%20Glance%20Initialization%20and%0A%20%20Progressive%20Association%0AAuthor%3A%20Hamidreza%20Hashempoor%0AAbstract%3A%20%20%20We%20propose%20a%20multi-camera%20multi-target%20%28MCMT%29%20tracking%20framework%20that%20ensures%0Aconsistent%20global%20identity%20assignment%20across%20views%20using%20trajectory%20and%0Aappearance%20cues.%20The%20pipeline%20starts%20with%20BoT-SORT-based%20single-camera%0Atracking%2C%20followed%20by%20an%20initial%20glance%20phase%20to%20initialize%20global%20IDs%20via%0Atrajectory-feature%20matching.%20In%20later%20frames%2C%20new%20tracklets%20are%20matched%20to%0Aexisting%20global%20identities%20through%20a%20prioritized%20global%20matching%20strategy.%20New%0Aglobal%20IDs%20are%20only%20introduced%20when%20no%20sufficiently%20similar%20trajectory%20or%0Afeature%20match%20is%20found.%203D%20positions%20are%20estimated%20using%20depth%20maps%20and%0Acalibration%20for%20spatial%20validation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlance-MCMT%253A%2520A%2520General%2520MCMT%2520Framework%2520with%2520Glance%2520Initialization%2520and%250A%2520%2520Progressive%2520Association%26entry.906535625%3DHamidreza%2520Hashempoor%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520multi-camera%2520multi-target%2520%2528MCMT%2529%2520tracking%2520framework%2520that%2520ensures%250Aconsistent%2520global%2520identity%2520assignment%2520across%2520views%2520using%2520trajectory%2520and%250Aappearance%2520cues.%2520The%2520pipeline%2520starts%2520with%2520BoT-SORT-based%2520single-camera%250Atracking%252C%2520followed%2520by%2520an%2520initial%2520glance%2520phase%2520to%2520initialize%2520global%2520IDs%2520via%250Atrajectory-feature%2520matching.%2520In%2520later%2520frames%252C%2520new%2520tracklets%2520are%2520matched%2520to%250Aexisting%2520global%2520identities%2520through%2520a%2520prioritized%2520global%2520matching%2520strategy.%2520New%250Aglobal%2520IDs%2520are%2520only%2520introduced%2520when%2520no%2520sufficiently%2520similar%2520trajectory%2520or%250Afeature%2520match%2520is%2520found.%25203D%2520positions%2520are%2520estimated%2520using%2520depth%2520maps%2520and%250Acalibration%2520for%2520spatial%2520validation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glance-MCMT%3A%20A%20General%20MCMT%20Framework%20with%20Glance%20Initialization%20and%0A%20%20Progressive%20Association&entry.906535625=Hamidreza%20Hashempoor&entry.1292438233=%20%20We%20propose%20a%20multi-camera%20multi-target%20%28MCMT%29%20tracking%20framework%20that%20ensures%0Aconsistent%20global%20identity%20assignment%20across%20views%20using%20trajectory%20and%0Aappearance%20cues.%20The%20pipeline%20starts%20with%20BoT-SORT-based%20single-camera%0Atracking%2C%20followed%20by%20an%20initial%20glance%20phase%20to%20initialize%20global%20IDs%20via%0Atrajectory-feature%20matching.%20In%20later%20frames%2C%20new%20tracklets%20are%20matched%20to%0Aexisting%20global%20identities%20through%20a%20prioritized%20global%20matching%20strategy.%20New%0Aglobal%20IDs%20are%20only%20introduced%20when%20no%20sufficiently%20similar%20trajectory%20or%0Afeature%20match%20is%20found.%203D%20positions%20are%20estimated%20using%20depth%20maps%20and%0Acalibration%20for%20spatial%20validation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10115v1&entry.124074799=Read"},
{"title": "Spatial Lifting for Dense Prediction", "author": "Mingzhi Xu and Yizhe Zhang", "abstract": "  We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision.\n", "link": "http://arxiv.org/abs/2507.10222v1", "date": "2025-07-14", "relevancy": 2.2059, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5641}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5638}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Lifting%20for%20Dense%20Prediction&body=Title%3A%20Spatial%20Lifting%20for%20Dense%20Prediction%0AAuthor%3A%20Mingzhi%20Xu%20and%20Yizhe%20Zhang%0AAbstract%3A%20%20%20We%20present%20Spatial%20Lifting%20%28SL%29%2C%20a%20novel%20methodology%20for%20dense%20prediction%0Atasks.%20SL%20operates%20by%20lifting%20standard%20inputs%2C%20such%20as%202D%20images%2C%20into%20a%0Ahigher-dimensional%20space%20and%20subsequently%20processing%20them%20using%20networks%0Adesigned%20for%20that%20higher%20dimension%2C%20such%20as%20a%203D%20U-Net.%20Counterintuitively%2C%0Athis%20dimensionality%20lifting%20allows%20us%20to%20achieve%20good%20performance%20on%20benchmark%0Atasks%20compared%20to%20conventional%20approaches%2C%20while%20reducing%20inference%20costs%20and%0Asignificantly%20lowering%20the%20number%20of%20model%20parameters.%20The%20SL%20framework%0Aproduces%20intrinsically%20structured%20outputs%20along%20the%20lifted%20dimension.%20This%0Aemergent%20structure%20facilitates%20dense%20supervision%20during%20training%20and%20enables%0Arobust%2C%20near-zero-additional-cost%20prediction%20quality%20assessment%20at%20test%20time.%0AWe%20validate%20our%20approach%20across%2019%20benchmark%20datasets%20%2813%20for%20semantic%0Asegmentation%20and%206%20for%20depth%20estimation%29%2C%20demonstrating%20competitive%20dense%0Aprediction%20performance%20while%20reducing%20the%20model%20parameter%20count%20by%20over%2098%25%20%28in%0Athe%20U-Net%20case%29%20and%20lowering%20inference%20costs.%20Spatial%20Lifting%20introduces%20a%20new%0Avision%20modeling%20paradigm%20that%20offers%20a%20promising%20path%20toward%20more%20efficient%2C%0Aaccurate%2C%20and%20reliable%20deep%20networks%20for%20dense%20prediction%20tasks%20in%20vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Lifting%2520for%2520Dense%2520Prediction%26entry.906535625%3DMingzhi%2520Xu%2520and%2520Yizhe%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520Spatial%2520Lifting%2520%2528SL%2529%252C%2520a%2520novel%2520methodology%2520for%2520dense%2520prediction%250Atasks.%2520SL%2520operates%2520by%2520lifting%2520standard%2520inputs%252C%2520such%2520as%25202D%2520images%252C%2520into%2520a%250Ahigher-dimensional%2520space%2520and%2520subsequently%2520processing%2520them%2520using%2520networks%250Adesigned%2520for%2520that%2520higher%2520dimension%252C%2520such%2520as%2520a%25203D%2520U-Net.%2520Counterintuitively%252C%250Athis%2520dimensionality%2520lifting%2520allows%2520us%2520to%2520achieve%2520good%2520performance%2520on%2520benchmark%250Atasks%2520compared%2520to%2520conventional%2520approaches%252C%2520while%2520reducing%2520inference%2520costs%2520and%250Asignificantly%2520lowering%2520the%2520number%2520of%2520model%2520parameters.%2520The%2520SL%2520framework%250Aproduces%2520intrinsically%2520structured%2520outputs%2520along%2520the%2520lifted%2520dimension.%2520This%250Aemergent%2520structure%2520facilitates%2520dense%2520supervision%2520during%2520training%2520and%2520enables%250Arobust%252C%2520near-zero-additional-cost%2520prediction%2520quality%2520assessment%2520at%2520test%2520time.%250AWe%2520validate%2520our%2520approach%2520across%252019%2520benchmark%2520datasets%2520%252813%2520for%2520semantic%250Asegmentation%2520and%25206%2520for%2520depth%2520estimation%2529%252C%2520demonstrating%2520competitive%2520dense%250Aprediction%2520performance%2520while%2520reducing%2520the%2520model%2520parameter%2520count%2520by%2520over%252098%2525%2520%2528in%250Athe%2520U-Net%2520case%2529%2520and%2520lowering%2520inference%2520costs.%2520Spatial%2520Lifting%2520introduces%2520a%2520new%250Avision%2520modeling%2520paradigm%2520that%2520offers%2520a%2520promising%2520path%2520toward%2520more%2520efficient%252C%250Aaccurate%252C%2520and%2520reliable%2520deep%2520networks%2520for%2520dense%2520prediction%2520tasks%2520in%2520vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Lifting%20for%20Dense%20Prediction&entry.906535625=Mingzhi%20Xu%20and%20Yizhe%20Zhang&entry.1292438233=%20%20We%20present%20Spatial%20Lifting%20%28SL%29%2C%20a%20novel%20methodology%20for%20dense%20prediction%0Atasks.%20SL%20operates%20by%20lifting%20standard%20inputs%2C%20such%20as%202D%20images%2C%20into%20a%0Ahigher-dimensional%20space%20and%20subsequently%20processing%20them%20using%20networks%0Adesigned%20for%20that%20higher%20dimension%2C%20such%20as%20a%203D%20U-Net.%20Counterintuitively%2C%0Athis%20dimensionality%20lifting%20allows%20us%20to%20achieve%20good%20performance%20on%20benchmark%0Atasks%20compared%20to%20conventional%20approaches%2C%20while%20reducing%20inference%20costs%20and%0Asignificantly%20lowering%20the%20number%20of%20model%20parameters.%20The%20SL%20framework%0Aproduces%20intrinsically%20structured%20outputs%20along%20the%20lifted%20dimension.%20This%0Aemergent%20structure%20facilitates%20dense%20supervision%20during%20training%20and%20enables%0Arobust%2C%20near-zero-additional-cost%20prediction%20quality%20assessment%20at%20test%20time.%0AWe%20validate%20our%20approach%20across%2019%20benchmark%20datasets%20%2813%20for%20semantic%0Asegmentation%20and%206%20for%20depth%20estimation%29%2C%20demonstrating%20competitive%20dense%0Aprediction%20performance%20while%20reducing%20the%20model%20parameter%20count%20by%20over%2098%25%20%28in%0Athe%20U-Net%20case%29%20and%20lowering%20inference%20costs.%20Spatial%20Lifting%20introduces%20a%20new%0Avision%20modeling%20paradigm%20that%20offers%20a%20promising%20path%20toward%20more%20efficient%2C%0Aaccurate%2C%20and%20reliable%20deep%20networks%20for%20dense%20prediction%20tasks%20in%20vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10222v1&entry.124074799=Read"},
{"title": "UniQA: Unified Vision-Language Pre-training for Image Quality and\n  Aesthetic Assessment", "author": "Hantao Zhou and Longxiang Tang and Rui Yang and Guanyi Qin and Yan Zhang and Yutao Li and Xiu Li and Runze Hu and Guangtao Zhai", "abstract": "  Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA) aim to\nsimulate human subjective perception of image visual quality and aesthetic\nappeal. Despite distinct learning objectives, they have underlying\ninterconnectedness due to consistent human assessment perception. In this\npaper, we propose Unified vision-language pre-training of Quality and\nAesthetics (UniQA}), to extract useful and common representations from two\ntasks, thereby benefiting them simultaneously. However, the lack of text in the\nIQA datasets and the textual noise in the IAA datasets pose severe challenges\nfor multimodal pre-training. To address this, we (1) utilize multimodal large\nlanguage models (MLLMs) to generate high-quality text descriptions; (2) use the\ngenerated text for IAA as metadata to purify noisy IAA data. To effectively\nadapt the pre-trained UniQA to downstream tasks, we further propose a\nlightweight adapter that utilizes versatile cues to fully exploit the extensive\nknowledge of the pre-trained model. UniQA demonstrates high competitiveness in\nvarious image assessment tasks, including classical IQA and IAA tasks,\nfew-label IQA, and other downstream tasks, showing promise as a foundational\nassessment model. Codes are available at https://github.com/zht8506/UniQA.\n", "link": "http://arxiv.org/abs/2406.01069v2", "date": "2025-07-14", "relevancy": 2.2055, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniQA%3A%20Unified%20Vision-Language%20Pre-training%20for%20Image%20Quality%20and%0A%20%20Aesthetic%20Assessment&body=Title%3A%20UniQA%3A%20Unified%20Vision-Language%20Pre-training%20for%20Image%20Quality%20and%0A%20%20Aesthetic%20Assessment%0AAuthor%3A%20Hantao%20Zhou%20and%20Longxiang%20Tang%20and%20Rui%20Yang%20and%20Guanyi%20Qin%20and%20Yan%20Zhang%20and%20Yutao%20Li%20and%20Xiu%20Li%20and%20Runze%20Hu%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20Image%20Quality%20Assessment%20%28IQA%29%20and%20Image%20Aesthetic%20Assessment%20%28IAA%29%20aim%20to%0Asimulate%20human%20subjective%20perception%20of%20image%20visual%20quality%20and%20aesthetic%0Aappeal.%20Despite%20distinct%20learning%20objectives%2C%20they%20have%20underlying%0Ainterconnectedness%20due%20to%20consistent%20human%20assessment%20perception.%20In%20this%0Apaper%2C%20we%20propose%20Unified%20vision-language%20pre-training%20of%20Quality%20and%0AAesthetics%20%28UniQA%7D%29%2C%20to%20extract%20useful%20and%20common%20representations%20from%20two%0Atasks%2C%20thereby%20benefiting%20them%20simultaneously.%20However%2C%20the%20lack%20of%20text%20in%20the%0AIQA%20datasets%20and%20the%20textual%20noise%20in%20the%20IAA%20datasets%20pose%20severe%20challenges%0Afor%20multimodal%20pre-training.%20To%20address%20this%2C%20we%20%281%29%20utilize%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20to%20generate%20high-quality%20text%20descriptions%3B%20%282%29%20use%20the%0Agenerated%20text%20for%20IAA%20as%20metadata%20to%20purify%20noisy%20IAA%20data.%20To%20effectively%0Aadapt%20the%20pre-trained%20UniQA%20to%20downstream%20tasks%2C%20we%20further%20propose%20a%0Alightweight%20adapter%20that%20utilizes%20versatile%20cues%20to%20fully%20exploit%20the%20extensive%0Aknowledge%20of%20the%20pre-trained%20model.%20UniQA%20demonstrates%20high%20competitiveness%20in%0Avarious%20image%20assessment%20tasks%2C%20including%20classical%20IQA%20and%20IAA%20tasks%2C%0Afew-label%20IQA%2C%20and%20other%20downstream%20tasks%2C%20showing%20promise%20as%20a%20foundational%0Aassessment%20model.%20Codes%20are%20available%20at%20https%3A//github.com/zht8506/UniQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniQA%253A%2520Unified%2520Vision-Language%2520Pre-training%2520for%2520Image%2520Quality%2520and%250A%2520%2520Aesthetic%2520Assessment%26entry.906535625%3DHantao%2520Zhou%2520and%2520Longxiang%2520Tang%2520and%2520Rui%2520Yang%2520and%2520Guanyi%2520Qin%2520and%2520Yan%2520Zhang%2520and%2520Yutao%2520Li%2520and%2520Xiu%2520Li%2520and%2520Runze%2520Hu%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520and%2520Image%2520Aesthetic%2520Assessment%2520%2528IAA%2529%2520aim%2520to%250Asimulate%2520human%2520subjective%2520perception%2520of%2520image%2520visual%2520quality%2520and%2520aesthetic%250Aappeal.%2520Despite%2520distinct%2520learning%2520objectives%252C%2520they%2520have%2520underlying%250Ainterconnectedness%2520due%2520to%2520consistent%2520human%2520assessment%2520perception.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520Unified%2520vision-language%2520pre-training%2520of%2520Quality%2520and%250AAesthetics%2520%2528UniQA%257D%2529%252C%2520to%2520extract%2520useful%2520and%2520common%2520representations%2520from%2520two%250Atasks%252C%2520thereby%2520benefiting%2520them%2520simultaneously.%2520However%252C%2520the%2520lack%2520of%2520text%2520in%2520the%250AIQA%2520datasets%2520and%2520the%2520textual%2520noise%2520in%2520the%2520IAA%2520datasets%2520pose%2520severe%2520challenges%250Afor%2520multimodal%2520pre-training.%2520To%2520address%2520this%252C%2520we%2520%25281%2529%2520utilize%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520to%2520generate%2520high-quality%2520text%2520descriptions%253B%2520%25282%2529%2520use%2520the%250Agenerated%2520text%2520for%2520IAA%2520as%2520metadata%2520to%2520purify%2520noisy%2520IAA%2520data.%2520To%2520effectively%250Aadapt%2520the%2520pre-trained%2520UniQA%2520to%2520downstream%2520tasks%252C%2520we%2520further%2520propose%2520a%250Alightweight%2520adapter%2520that%2520utilizes%2520versatile%2520cues%2520to%2520fully%2520exploit%2520the%2520extensive%250Aknowledge%2520of%2520the%2520pre-trained%2520model.%2520UniQA%2520demonstrates%2520high%2520competitiveness%2520in%250Avarious%2520image%2520assessment%2520tasks%252C%2520including%2520classical%2520IQA%2520and%2520IAA%2520tasks%252C%250Afew-label%2520IQA%252C%2520and%2520other%2520downstream%2520tasks%252C%2520showing%2520promise%2520as%2520a%2520foundational%250Aassessment%2520model.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/zht8506/UniQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniQA%3A%20Unified%20Vision-Language%20Pre-training%20for%20Image%20Quality%20and%0A%20%20Aesthetic%20Assessment&entry.906535625=Hantao%20Zhou%20and%20Longxiang%20Tang%20and%20Rui%20Yang%20and%20Guanyi%20Qin%20and%20Yan%20Zhang%20and%20Yutao%20Li%20and%20Xiu%20Li%20and%20Runze%20Hu%20and%20Guangtao%20Zhai&entry.1292438233=%20%20Image%20Quality%20Assessment%20%28IQA%29%20and%20Image%20Aesthetic%20Assessment%20%28IAA%29%20aim%20to%0Asimulate%20human%20subjective%20perception%20of%20image%20visual%20quality%20and%20aesthetic%0Aappeal.%20Despite%20distinct%20learning%20objectives%2C%20they%20have%20underlying%0Ainterconnectedness%20due%20to%20consistent%20human%20assessment%20perception.%20In%20this%0Apaper%2C%20we%20propose%20Unified%20vision-language%20pre-training%20of%20Quality%20and%0AAesthetics%20%28UniQA%7D%29%2C%20to%20extract%20useful%20and%20common%20representations%20from%20two%0Atasks%2C%20thereby%20benefiting%20them%20simultaneously.%20However%2C%20the%20lack%20of%20text%20in%20the%0AIQA%20datasets%20and%20the%20textual%20noise%20in%20the%20IAA%20datasets%20pose%20severe%20challenges%0Afor%20multimodal%20pre-training.%20To%20address%20this%2C%20we%20%281%29%20utilize%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20to%20generate%20high-quality%20text%20descriptions%3B%20%282%29%20use%20the%0Agenerated%20text%20for%20IAA%20as%20metadata%20to%20purify%20noisy%20IAA%20data.%20To%20effectively%0Aadapt%20the%20pre-trained%20UniQA%20to%20downstream%20tasks%2C%20we%20further%20propose%20a%0Alightweight%20adapter%20that%20utilizes%20versatile%20cues%20to%20fully%20exploit%20the%20extensive%0Aknowledge%20of%20the%20pre-trained%20model.%20UniQA%20demonstrates%20high%20competitiveness%20in%0Avarious%20image%20assessment%20tasks%2C%20including%20classical%20IQA%20and%20IAA%20tasks%2C%0Afew-label%20IQA%2C%20and%20other%20downstream%20tasks%2C%20showing%20promise%20as%20a%20foundational%0Aassessment%20model.%20Codes%20are%20available%20at%20https%3A//github.com/zht8506/UniQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01069v2&entry.124074799=Read"},
{"title": "Advancing Automatic Photovoltaic Defect Detection using Semi-Supervised\n  Semantic Segmentation of Electroluminescence Images", "author": "Abhishek Jha and Yogesh Rawat and Shruti Vyas", "abstract": "  Photovoltaic (PV) systems allow us to tap into all abundant solar energy,\nhowever they require regular maintenance for high efficiency and to prevent\ndegradation. Traditional manual health check, using Electroluminescence (EL)\nimaging, is expensive and logistically challenging which makes automated defect\ndetection essential. Current automation approaches require extensive manual\nexpert labeling, which is time-consuming, expensive, and prone to errors. We\npropose PV-S3 (Photovoltaic-Semi-supervised Semantic Segmentation), a\nSemi-Supervised Learning approach for semantic segmentation of defects in EL\nimages that reduces reliance on extensive labeling. PV-S3 is an artificial\nintelligence (AI) model trained using a few labeled images along with numerous\nunlabeled images. We introduce a novel Semi Cross-Entropy loss function to deal\nwith class imbalance. We evaluate PV-S3 on multiple datasets and demonstrate\nits effectiveness and adaptability. With merely 20% labeled samples, we achieve\nan absolute improvement of 9.7% in mean Intersection-over-Union (mIoU), 13.5%\nin Precision, 29.15% in Recall, and 20.42% in F1-Score over prior\nstate-of-the-art supervised method (which uses 100% labeled samples) on\nUniversity of Central Florida-Electroluminescence (UCF-EL) dataset (largest\ndataset available for semantic segmentation of EL images) showing improvement\nin performance while reducing the annotation costs by 80%. For more details,\nvisit our GitHub repository: https://github.com/abj247/PV-S3.\n", "link": "http://arxiv.org/abs/2404.13693v4", "date": "2025-07-14", "relevancy": 2.2035, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5658}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5544}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Automatic%20Photovoltaic%20Defect%20Detection%20using%20Semi-Supervised%0A%20%20Semantic%20Segmentation%20of%20Electroluminescence%20Images&body=Title%3A%20Advancing%20Automatic%20Photovoltaic%20Defect%20Detection%20using%20Semi-Supervised%0A%20%20Semantic%20Segmentation%20of%20Electroluminescence%20Images%0AAuthor%3A%20Abhishek%20Jha%20and%20Yogesh%20Rawat%20and%20Shruti%20Vyas%0AAbstract%3A%20%20%20Photovoltaic%20%28PV%29%20systems%20allow%20us%20to%20tap%20into%20all%20abundant%20solar%20energy%2C%0Ahowever%20they%20require%20regular%20maintenance%20for%20high%20efficiency%20and%20to%20prevent%0Adegradation.%20Traditional%20manual%20health%20check%2C%20using%20Electroluminescence%20%28EL%29%0Aimaging%2C%20is%20expensive%20and%20logistically%20challenging%20which%20makes%20automated%20defect%0Adetection%20essential.%20Current%20automation%20approaches%20require%20extensive%20manual%0Aexpert%20labeling%2C%20which%20is%20time-consuming%2C%20expensive%2C%20and%20prone%20to%20errors.%20We%0Apropose%20PV-S3%20%28Photovoltaic-Semi-supervised%20Semantic%20Segmentation%29%2C%20a%0ASemi-Supervised%20Learning%20approach%20for%20semantic%20segmentation%20of%20defects%20in%20EL%0Aimages%20that%20reduces%20reliance%20on%20extensive%20labeling.%20PV-S3%20is%20an%20artificial%0Aintelligence%20%28AI%29%20model%20trained%20using%20a%20few%20labeled%20images%20along%20with%20numerous%0Aunlabeled%20images.%20We%20introduce%20a%20novel%20Semi%20Cross-Entropy%20loss%20function%20to%20deal%0Awith%20class%20imbalance.%20We%20evaluate%20PV-S3%20on%20multiple%20datasets%20and%20demonstrate%0Aits%20effectiveness%20and%20adaptability.%20With%20merely%2020%25%20labeled%20samples%2C%20we%20achieve%0Aan%20absolute%20improvement%20of%209.7%25%20in%20mean%20Intersection-over-Union%20%28mIoU%29%2C%2013.5%25%0Ain%20Precision%2C%2029.15%25%20in%20Recall%2C%20and%2020.42%25%20in%20F1-Score%20over%20prior%0Astate-of-the-art%20supervised%20method%20%28which%20uses%20100%25%20labeled%20samples%29%20on%0AUniversity%20of%20Central%20Florida-Electroluminescence%20%28UCF-EL%29%20dataset%20%28largest%0Adataset%20available%20for%20semantic%20segmentation%20of%20EL%20images%29%20showing%20improvement%0Ain%20performance%20while%20reducing%20the%20annotation%20costs%20by%2080%25.%20For%20more%20details%2C%0Avisit%20our%20GitHub%20repository%3A%20https%3A//github.com/abj247/PV-S3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13693v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Automatic%2520Photovoltaic%2520Defect%2520Detection%2520using%2520Semi-Supervised%250A%2520%2520Semantic%2520Segmentation%2520of%2520Electroluminescence%2520Images%26entry.906535625%3DAbhishek%2520Jha%2520and%2520Yogesh%2520Rawat%2520and%2520Shruti%2520Vyas%26entry.1292438233%3D%2520%2520Photovoltaic%2520%2528PV%2529%2520systems%2520allow%2520us%2520to%2520tap%2520into%2520all%2520abundant%2520solar%2520energy%252C%250Ahowever%2520they%2520require%2520regular%2520maintenance%2520for%2520high%2520efficiency%2520and%2520to%2520prevent%250Adegradation.%2520Traditional%2520manual%2520health%2520check%252C%2520using%2520Electroluminescence%2520%2528EL%2529%250Aimaging%252C%2520is%2520expensive%2520and%2520logistically%2520challenging%2520which%2520makes%2520automated%2520defect%250Adetection%2520essential.%2520Current%2520automation%2520approaches%2520require%2520extensive%2520manual%250Aexpert%2520labeling%252C%2520which%2520is%2520time-consuming%252C%2520expensive%252C%2520and%2520prone%2520to%2520errors.%2520We%250Apropose%2520PV-S3%2520%2528Photovoltaic-Semi-supervised%2520Semantic%2520Segmentation%2529%252C%2520a%250ASemi-Supervised%2520Learning%2520approach%2520for%2520semantic%2520segmentation%2520of%2520defects%2520in%2520EL%250Aimages%2520that%2520reduces%2520reliance%2520on%2520extensive%2520labeling.%2520PV-S3%2520is%2520an%2520artificial%250Aintelligence%2520%2528AI%2529%2520model%2520trained%2520using%2520a%2520few%2520labeled%2520images%2520along%2520with%2520numerous%250Aunlabeled%2520images.%2520We%2520introduce%2520a%2520novel%2520Semi%2520Cross-Entropy%2520loss%2520function%2520to%2520deal%250Awith%2520class%2520imbalance.%2520We%2520evaluate%2520PV-S3%2520on%2520multiple%2520datasets%2520and%2520demonstrate%250Aits%2520effectiveness%2520and%2520adaptability.%2520With%2520merely%252020%2525%2520labeled%2520samples%252C%2520we%2520achieve%250Aan%2520absolute%2520improvement%2520of%25209.7%2525%2520in%2520mean%2520Intersection-over-Union%2520%2528mIoU%2529%252C%252013.5%2525%250Ain%2520Precision%252C%252029.15%2525%2520in%2520Recall%252C%2520and%252020.42%2525%2520in%2520F1-Score%2520over%2520prior%250Astate-of-the-art%2520supervised%2520method%2520%2528which%2520uses%2520100%2525%2520labeled%2520samples%2529%2520on%250AUniversity%2520of%2520Central%2520Florida-Electroluminescence%2520%2528UCF-EL%2529%2520dataset%2520%2528largest%250Adataset%2520available%2520for%2520semantic%2520segmentation%2520of%2520EL%2520images%2529%2520showing%2520improvement%250Ain%2520performance%2520while%2520reducing%2520the%2520annotation%2520costs%2520by%252080%2525.%2520For%2520more%2520details%252C%250Avisit%2520our%2520GitHub%2520repository%253A%2520https%253A//github.com/abj247/PV-S3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13693v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Automatic%20Photovoltaic%20Defect%20Detection%20using%20Semi-Supervised%0A%20%20Semantic%20Segmentation%20of%20Electroluminescence%20Images&entry.906535625=Abhishek%20Jha%20and%20Yogesh%20Rawat%20and%20Shruti%20Vyas&entry.1292438233=%20%20Photovoltaic%20%28PV%29%20systems%20allow%20us%20to%20tap%20into%20all%20abundant%20solar%20energy%2C%0Ahowever%20they%20require%20regular%20maintenance%20for%20high%20efficiency%20and%20to%20prevent%0Adegradation.%20Traditional%20manual%20health%20check%2C%20using%20Electroluminescence%20%28EL%29%0Aimaging%2C%20is%20expensive%20and%20logistically%20challenging%20which%20makes%20automated%20defect%0Adetection%20essential.%20Current%20automation%20approaches%20require%20extensive%20manual%0Aexpert%20labeling%2C%20which%20is%20time-consuming%2C%20expensive%2C%20and%20prone%20to%20errors.%20We%0Apropose%20PV-S3%20%28Photovoltaic-Semi-supervised%20Semantic%20Segmentation%29%2C%20a%0ASemi-Supervised%20Learning%20approach%20for%20semantic%20segmentation%20of%20defects%20in%20EL%0Aimages%20that%20reduces%20reliance%20on%20extensive%20labeling.%20PV-S3%20is%20an%20artificial%0Aintelligence%20%28AI%29%20model%20trained%20using%20a%20few%20labeled%20images%20along%20with%20numerous%0Aunlabeled%20images.%20We%20introduce%20a%20novel%20Semi%20Cross-Entropy%20loss%20function%20to%20deal%0Awith%20class%20imbalance.%20We%20evaluate%20PV-S3%20on%20multiple%20datasets%20and%20demonstrate%0Aits%20effectiveness%20and%20adaptability.%20With%20merely%2020%25%20labeled%20samples%2C%20we%20achieve%0Aan%20absolute%20improvement%20of%209.7%25%20in%20mean%20Intersection-over-Union%20%28mIoU%29%2C%2013.5%25%0Ain%20Precision%2C%2029.15%25%20in%20Recall%2C%20and%2020.42%25%20in%20F1-Score%20over%20prior%0Astate-of-the-art%20supervised%20method%20%28which%20uses%20100%25%20labeled%20samples%29%20on%0AUniversity%20of%20Central%20Florida-Electroluminescence%20%28UCF-EL%29%20dataset%20%28largest%0Adataset%20available%20for%20semantic%20segmentation%20of%20EL%20images%29%20showing%20improvement%0Ain%20performance%20while%20reducing%20the%20annotation%20costs%20by%2080%25.%20For%20more%20details%2C%0Avisit%20our%20GitHub%20repository%3A%20https%3A//github.com/abj247/PV-S3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13693v4&entry.124074799=Read"},
{"title": "FTCFormer: Fuzzy Token Clustering Transformer for Image Classification", "author": "Muyi Bao and Changyu Zeng and Yifan Wang and Zhengni Yang and Zimu Wang and Guangliang Cheng and Jun Qi and Wei Wang", "abstract": "  Transformer-based deep neural networks have achieved remarkable success\nacross various computer vision tasks, largely attributed to their long-range\nself-attention mechanism and scalability. However, most transformer\narchitectures embed images into uniform, grid-based vision tokens, neglecting\nthe underlying semantic meanings of image regions, resulting in suboptimal\nfeature representations. To address this issue, we propose Fuzzy Token\nClustering Transformer (FTCFormer), which incorporates a novel clustering-based\ndownsampling module to dynamically generate vision tokens based on the semantic\nmeanings instead of spatial positions. It allocates fewer tokens to less\ninformative regions and more to represent semantically important regions,\nregardless of their spatial adjacency or shape irregularity. To further enhance\nfeature extraction and representation, we propose a Density Peak\nClustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center\ndetermination, a Spatial Connectivity Score (SCS) for token assignment, and a\nchannel-wise merging (Cmerge) strategy for token merging. Extensive experiments\non 32 datasets across diverse domains validate the effectiveness of FTCFormer\non image classification, showing consistent improvements over the TCFormer\nbaseline, achieving gains of improving 1.43% on five fine-grained datasets,\n1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%\non four remote sensing datasets. The code is available at:\nhttps://github.com/BaoBao0926/FTCFormer/tree/main.\n", "link": "http://arxiv.org/abs/2507.10283v1", "date": "2025-07-14", "relevancy": 2.1988, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5686}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5393}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FTCFormer%3A%20Fuzzy%20Token%20Clustering%20Transformer%20for%20Image%20Classification&body=Title%3A%20FTCFormer%3A%20Fuzzy%20Token%20Clustering%20Transformer%20for%20Image%20Classification%0AAuthor%3A%20Muyi%20Bao%20and%20Changyu%20Zeng%20and%20Yifan%20Wang%20and%20Zhengni%20Yang%20and%20Zimu%20Wang%20and%20Guangliang%20Cheng%20and%20Jun%20Qi%20and%20Wei%20Wang%0AAbstract%3A%20%20%20Transformer-based%20deep%20neural%20networks%20have%20achieved%20remarkable%20success%0Aacross%20various%20computer%20vision%20tasks%2C%20largely%20attributed%20to%20their%20long-range%0Aself-attention%20mechanism%20and%20scalability.%20However%2C%20most%20transformer%0Aarchitectures%20embed%20images%20into%20uniform%2C%20grid-based%20vision%20tokens%2C%20neglecting%0Athe%20underlying%20semantic%20meanings%20of%20image%20regions%2C%20resulting%20in%20suboptimal%0Afeature%20representations.%20To%20address%20this%20issue%2C%20we%20propose%20Fuzzy%20Token%0AClustering%20Transformer%20%28FTCFormer%29%2C%20which%20incorporates%20a%20novel%20clustering-based%0Adownsampling%20module%20to%20dynamically%20generate%20vision%20tokens%20based%20on%20the%20semantic%0Ameanings%20instead%20of%20spatial%20positions.%20It%20allocates%20fewer%20tokens%20to%20less%0Ainformative%20regions%20and%20more%20to%20represent%20semantically%20important%20regions%2C%0Aregardless%20of%20their%20spatial%20adjacency%20or%20shape%20irregularity.%20To%20further%20enhance%0Afeature%20extraction%20and%20representation%2C%20we%20propose%20a%20Density%20Peak%0AClustering-Fuzzy%20K-Nearest%20Neighbor%20%28DPC-FKNN%29%20mechanism%20for%20clustering%20center%0Adetermination%2C%20a%20Spatial%20Connectivity%20Score%20%28SCS%29%20for%20token%20assignment%2C%20and%20a%0Achannel-wise%20merging%20%28Cmerge%29%20strategy%20for%20token%20merging.%20Extensive%20experiments%0Aon%2032%20datasets%20across%20diverse%20domains%20validate%20the%20effectiveness%20of%20FTCFormer%0Aon%20image%20classification%2C%20showing%20consistent%20improvements%20over%20the%20TCFormer%0Abaseline%2C%20achieving%20gains%20of%20improving%201.43%25%20on%20five%20fine-grained%20datasets%2C%0A1.09%25%20on%20six%20natural%20image%20datasets%2C%200.97%25%20on%20three%20medical%20datasets%20and%200.55%25%0Aon%20four%20remote%20sensing%20datasets.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/BaoBao0926/FTCFormer/tree/main.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFTCFormer%253A%2520Fuzzy%2520Token%2520Clustering%2520Transformer%2520for%2520Image%2520Classification%26entry.906535625%3DMuyi%2520Bao%2520and%2520Changyu%2520Zeng%2520and%2520Yifan%2520Wang%2520and%2520Zhengni%2520Yang%2520and%2520Zimu%2520Wang%2520and%2520Guangliang%2520Cheng%2520and%2520Jun%2520Qi%2520and%2520Wei%2520Wang%26entry.1292438233%3D%2520%2520Transformer-based%2520deep%2520neural%2520networks%2520have%2520achieved%2520remarkable%2520success%250Aacross%2520various%2520computer%2520vision%2520tasks%252C%2520largely%2520attributed%2520to%2520their%2520long-range%250Aself-attention%2520mechanism%2520and%2520scalability.%2520However%252C%2520most%2520transformer%250Aarchitectures%2520embed%2520images%2520into%2520uniform%252C%2520grid-based%2520vision%2520tokens%252C%2520neglecting%250Athe%2520underlying%2520semantic%2520meanings%2520of%2520image%2520regions%252C%2520resulting%2520in%2520suboptimal%250Afeature%2520representations.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Fuzzy%2520Token%250AClustering%2520Transformer%2520%2528FTCFormer%2529%252C%2520which%2520incorporates%2520a%2520novel%2520clustering-based%250Adownsampling%2520module%2520to%2520dynamically%2520generate%2520vision%2520tokens%2520based%2520on%2520the%2520semantic%250Ameanings%2520instead%2520of%2520spatial%2520positions.%2520It%2520allocates%2520fewer%2520tokens%2520to%2520less%250Ainformative%2520regions%2520and%2520more%2520to%2520represent%2520semantically%2520important%2520regions%252C%250Aregardless%2520of%2520their%2520spatial%2520adjacency%2520or%2520shape%2520irregularity.%2520To%2520further%2520enhance%250Afeature%2520extraction%2520and%2520representation%252C%2520we%2520propose%2520a%2520Density%2520Peak%250AClustering-Fuzzy%2520K-Nearest%2520Neighbor%2520%2528DPC-FKNN%2529%2520mechanism%2520for%2520clustering%2520center%250Adetermination%252C%2520a%2520Spatial%2520Connectivity%2520Score%2520%2528SCS%2529%2520for%2520token%2520assignment%252C%2520and%2520a%250Achannel-wise%2520merging%2520%2528Cmerge%2529%2520strategy%2520for%2520token%2520merging.%2520Extensive%2520experiments%250Aon%252032%2520datasets%2520across%2520diverse%2520domains%2520validate%2520the%2520effectiveness%2520of%2520FTCFormer%250Aon%2520image%2520classification%252C%2520showing%2520consistent%2520improvements%2520over%2520the%2520TCFormer%250Abaseline%252C%2520achieving%2520gains%2520of%2520improving%25201.43%2525%2520on%2520five%2520fine-grained%2520datasets%252C%250A1.09%2525%2520on%2520six%2520natural%2520image%2520datasets%252C%25200.97%2525%2520on%2520three%2520medical%2520datasets%2520and%25200.55%2525%250Aon%2520four%2520remote%2520sensing%2520datasets.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/BaoBao0926/FTCFormer/tree/main.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FTCFormer%3A%20Fuzzy%20Token%20Clustering%20Transformer%20for%20Image%20Classification&entry.906535625=Muyi%20Bao%20and%20Changyu%20Zeng%20and%20Yifan%20Wang%20and%20Zhengni%20Yang%20and%20Zimu%20Wang%20and%20Guangliang%20Cheng%20and%20Jun%20Qi%20and%20Wei%20Wang&entry.1292438233=%20%20Transformer-based%20deep%20neural%20networks%20have%20achieved%20remarkable%20success%0Aacross%20various%20computer%20vision%20tasks%2C%20largely%20attributed%20to%20their%20long-range%0Aself-attention%20mechanism%20and%20scalability.%20However%2C%20most%20transformer%0Aarchitectures%20embed%20images%20into%20uniform%2C%20grid-based%20vision%20tokens%2C%20neglecting%0Athe%20underlying%20semantic%20meanings%20of%20image%20regions%2C%20resulting%20in%20suboptimal%0Afeature%20representations.%20To%20address%20this%20issue%2C%20we%20propose%20Fuzzy%20Token%0AClustering%20Transformer%20%28FTCFormer%29%2C%20which%20incorporates%20a%20novel%20clustering-based%0Adownsampling%20module%20to%20dynamically%20generate%20vision%20tokens%20based%20on%20the%20semantic%0Ameanings%20instead%20of%20spatial%20positions.%20It%20allocates%20fewer%20tokens%20to%20less%0Ainformative%20regions%20and%20more%20to%20represent%20semantically%20important%20regions%2C%0Aregardless%20of%20their%20spatial%20adjacency%20or%20shape%20irregularity.%20To%20further%20enhance%0Afeature%20extraction%20and%20representation%2C%20we%20propose%20a%20Density%20Peak%0AClustering-Fuzzy%20K-Nearest%20Neighbor%20%28DPC-FKNN%29%20mechanism%20for%20clustering%20center%0Adetermination%2C%20a%20Spatial%20Connectivity%20Score%20%28SCS%29%20for%20token%20assignment%2C%20and%20a%0Achannel-wise%20merging%20%28Cmerge%29%20strategy%20for%20token%20merging.%20Extensive%20experiments%0Aon%2032%20datasets%20across%20diverse%20domains%20validate%20the%20effectiveness%20of%20FTCFormer%0Aon%20image%20classification%2C%20showing%20consistent%20improvements%20over%20the%20TCFormer%0Abaseline%2C%20achieving%20gains%20of%20improving%201.43%25%20on%20five%20fine-grained%20datasets%2C%0A1.09%25%20on%20six%20natural%20image%20datasets%2C%200.97%25%20on%20three%20medical%20datasets%20and%200.55%25%0Aon%20four%20remote%20sensing%20datasets.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/BaoBao0926/FTCFormer/tree/main.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10283v1&entry.124074799=Read"},
{"title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and\n  Recommendations from the CZI Virtual Cells Workshop", "author": "Elizabeth Fahsbender and Alma Andersson and Jeremy Ash and Polina Binder and Daniel Burkhardt and Benjamin Chang and Georg K. Gerber and Anthony Gitter and Patrick Godau and Ankit Gupta and Genevieve Haliburton and Siyu He and Trey Ideker and Ivana Jelic and Aly Khan and Yang-Joon Kim and Aditi Krishnapriyan and Jon M. Laurent and Tianyu Liu 28 and Emma Lundberg and Shalin B. Mehta and Rob Moccia and Angela Oliveira Pisco and Katherine S. Pollard and Suresh Ramani and Julio Saez-Rodriguez and Yasin Senbabaoglu and Elana Simon and Srinivasan Sivanandan and Gustavo Stolovitzky and Marc Valer and Bo Wang and Xikun Zhang and James Zou and Katrina Kalantar", "abstract": "  Artificial intelligence holds immense promise for transforming biology, yet a\nlack of standardized, cross domain, benchmarks undermines our ability to build\nrobust, trustworthy models. Here, we present insights from a recent workshop\nthat convened machine learning and computational biology experts across\nimaging, transcriptomics, proteomics, and genomics to tackle this gap. We\nidentify major technical and systemic bottlenecks such as data heterogeneity\nand noise, reproducibility challenges, biases, and the fragmented ecosystem of\npublicly available resources and propose a set of recommendations for building\nbenchmarking frameworks that can efficiently compare ML models of biological\nsystems across tasks and data modalities. By promoting high quality data\ncuration, standardized tooling, comprehensive evaluation metrics, and open,\ncollaborative platforms, we aim to accelerate the development of robust\nbenchmarks for AI driven Virtual Cells. These benchmarks are crucial for\nensuring rigor, reproducibility, and biological relevance, and will ultimately\nadvance the field toward integrated models that drive new discoveries,\ntherapeutic insights, and a deeper understanding of cellular systems.\n", "link": "http://arxiv.org/abs/2507.10502v1", "date": "2025-07-14", "relevancy": 2.1973, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4447}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20and%20Evaluation%20of%20AI%20Models%20in%20Biology%3A%20Outcomes%20and%0A%20%20Recommendations%20from%20the%20CZI%20Virtual%20Cells%20Workshop&body=Title%3A%20Benchmarking%20and%20Evaluation%20of%20AI%20Models%20in%20Biology%3A%20Outcomes%20and%0A%20%20Recommendations%20from%20the%20CZI%20Virtual%20Cells%20Workshop%0AAuthor%3A%20Elizabeth%20Fahsbender%20and%20Alma%20Andersson%20and%20Jeremy%20Ash%20and%20Polina%20Binder%20and%20Daniel%20Burkhardt%20and%20Benjamin%20Chang%20and%20Georg%20K.%20Gerber%20and%20Anthony%20Gitter%20and%20Patrick%20Godau%20and%20Ankit%20Gupta%20and%20Genevieve%20Haliburton%20and%20Siyu%20He%20and%20Trey%20Ideker%20and%20Ivana%20Jelic%20and%20Aly%20Khan%20and%20Yang-Joon%20Kim%20and%20Aditi%20Krishnapriyan%20and%20Jon%20M.%20Laurent%20and%20Tianyu%20Liu%2028%20and%20Emma%20Lundberg%20and%20Shalin%20B.%20Mehta%20and%20Rob%20Moccia%20and%20Angela%20Oliveira%20Pisco%20and%20Katherine%20S.%20Pollard%20and%20Suresh%20Ramani%20and%20Julio%20Saez-Rodriguez%20and%20Yasin%20Senbabaoglu%20and%20Elana%20Simon%20and%20Srinivasan%20Sivanandan%20and%20Gustavo%20Stolovitzky%20and%20Marc%20Valer%20and%20Bo%20Wang%20and%20Xikun%20Zhang%20and%20James%20Zou%20and%20Katrina%20Kalantar%0AAbstract%3A%20%20%20Artificial%20intelligence%20holds%20immense%20promise%20for%20transforming%20biology%2C%20yet%20a%0Alack%20of%20standardized%2C%20cross%20domain%2C%20benchmarks%20undermines%20our%20ability%20to%20build%0Arobust%2C%20trustworthy%20models.%20Here%2C%20we%20present%20insights%20from%20a%20recent%20workshop%0Athat%20convened%20machine%20learning%20and%20computational%20biology%20experts%20across%0Aimaging%2C%20transcriptomics%2C%20proteomics%2C%20and%20genomics%20to%20tackle%20this%20gap.%20We%0Aidentify%20major%20technical%20and%20systemic%20bottlenecks%20such%20as%20data%20heterogeneity%0Aand%20noise%2C%20reproducibility%20challenges%2C%20biases%2C%20and%20the%20fragmented%20ecosystem%20of%0Apublicly%20available%20resources%20and%20propose%20a%20set%20of%20recommendations%20for%20building%0Abenchmarking%20frameworks%20that%20can%20efficiently%20compare%20ML%20models%20of%20biological%0Asystems%20across%20tasks%20and%20data%20modalities.%20By%20promoting%20high%20quality%20data%0Acuration%2C%20standardized%20tooling%2C%20comprehensive%20evaluation%20metrics%2C%20and%20open%2C%0Acollaborative%20platforms%2C%20we%20aim%20to%20accelerate%20the%20development%20of%20robust%0Abenchmarks%20for%20AI%20driven%20Virtual%20Cells.%20These%20benchmarks%20are%20crucial%20for%0Aensuring%20rigor%2C%20reproducibility%2C%20and%20biological%20relevance%2C%20and%20will%20ultimately%0Aadvance%20the%20field%20toward%20integrated%20models%20that%20drive%20new%20discoveries%2C%0Atherapeutic%20insights%2C%20and%20a%20deeper%20understanding%20of%20cellular%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520and%2520Evaluation%2520of%2520AI%2520Models%2520in%2520Biology%253A%2520Outcomes%2520and%250A%2520%2520Recommendations%2520from%2520the%2520CZI%2520Virtual%2520Cells%2520Workshop%26entry.906535625%3DElizabeth%2520Fahsbender%2520and%2520Alma%2520Andersson%2520and%2520Jeremy%2520Ash%2520and%2520Polina%2520Binder%2520and%2520Daniel%2520Burkhardt%2520and%2520Benjamin%2520Chang%2520and%2520Georg%2520K.%2520Gerber%2520and%2520Anthony%2520Gitter%2520and%2520Patrick%2520Godau%2520and%2520Ankit%2520Gupta%2520and%2520Genevieve%2520Haliburton%2520and%2520Siyu%2520He%2520and%2520Trey%2520Ideker%2520and%2520Ivana%2520Jelic%2520and%2520Aly%2520Khan%2520and%2520Yang-Joon%2520Kim%2520and%2520Aditi%2520Krishnapriyan%2520and%2520Jon%2520M.%2520Laurent%2520and%2520Tianyu%2520Liu%252028%2520and%2520Emma%2520Lundberg%2520and%2520Shalin%2520B.%2520Mehta%2520and%2520Rob%2520Moccia%2520and%2520Angela%2520Oliveira%2520Pisco%2520and%2520Katherine%2520S.%2520Pollard%2520and%2520Suresh%2520Ramani%2520and%2520Julio%2520Saez-Rodriguez%2520and%2520Yasin%2520Senbabaoglu%2520and%2520Elana%2520Simon%2520and%2520Srinivasan%2520Sivanandan%2520and%2520Gustavo%2520Stolovitzky%2520and%2520Marc%2520Valer%2520and%2520Bo%2520Wang%2520and%2520Xikun%2520Zhang%2520and%2520James%2520Zou%2520and%2520Katrina%2520Kalantar%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520holds%2520immense%2520promise%2520for%2520transforming%2520biology%252C%2520yet%2520a%250Alack%2520of%2520standardized%252C%2520cross%2520domain%252C%2520benchmarks%2520undermines%2520our%2520ability%2520to%2520build%250Arobust%252C%2520trustworthy%2520models.%2520Here%252C%2520we%2520present%2520insights%2520from%2520a%2520recent%2520workshop%250Athat%2520convened%2520machine%2520learning%2520and%2520computational%2520biology%2520experts%2520across%250Aimaging%252C%2520transcriptomics%252C%2520proteomics%252C%2520and%2520genomics%2520to%2520tackle%2520this%2520gap.%2520We%250Aidentify%2520major%2520technical%2520and%2520systemic%2520bottlenecks%2520such%2520as%2520data%2520heterogeneity%250Aand%2520noise%252C%2520reproducibility%2520challenges%252C%2520biases%252C%2520and%2520the%2520fragmented%2520ecosystem%2520of%250Apublicly%2520available%2520resources%2520and%2520propose%2520a%2520set%2520of%2520recommendations%2520for%2520building%250Abenchmarking%2520frameworks%2520that%2520can%2520efficiently%2520compare%2520ML%2520models%2520of%2520biological%250Asystems%2520across%2520tasks%2520and%2520data%2520modalities.%2520By%2520promoting%2520high%2520quality%2520data%250Acuration%252C%2520standardized%2520tooling%252C%2520comprehensive%2520evaluation%2520metrics%252C%2520and%2520open%252C%250Acollaborative%2520platforms%252C%2520we%2520aim%2520to%2520accelerate%2520the%2520development%2520of%2520robust%250Abenchmarks%2520for%2520AI%2520driven%2520Virtual%2520Cells.%2520These%2520benchmarks%2520are%2520crucial%2520for%250Aensuring%2520rigor%252C%2520reproducibility%252C%2520and%2520biological%2520relevance%252C%2520and%2520will%2520ultimately%250Aadvance%2520the%2520field%2520toward%2520integrated%2520models%2520that%2520drive%2520new%2520discoveries%252C%250Atherapeutic%2520insights%252C%2520and%2520a%2520deeper%2520understanding%2520of%2520cellular%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20and%20Evaluation%20of%20AI%20Models%20in%20Biology%3A%20Outcomes%20and%0A%20%20Recommendations%20from%20the%20CZI%20Virtual%20Cells%20Workshop&entry.906535625=Elizabeth%20Fahsbender%20and%20Alma%20Andersson%20and%20Jeremy%20Ash%20and%20Polina%20Binder%20and%20Daniel%20Burkhardt%20and%20Benjamin%20Chang%20and%20Georg%20K.%20Gerber%20and%20Anthony%20Gitter%20and%20Patrick%20Godau%20and%20Ankit%20Gupta%20and%20Genevieve%20Haliburton%20and%20Siyu%20He%20and%20Trey%20Ideker%20and%20Ivana%20Jelic%20and%20Aly%20Khan%20and%20Yang-Joon%20Kim%20and%20Aditi%20Krishnapriyan%20and%20Jon%20M.%20Laurent%20and%20Tianyu%20Liu%2028%20and%20Emma%20Lundberg%20and%20Shalin%20B.%20Mehta%20and%20Rob%20Moccia%20and%20Angela%20Oliveira%20Pisco%20and%20Katherine%20S.%20Pollard%20and%20Suresh%20Ramani%20and%20Julio%20Saez-Rodriguez%20and%20Yasin%20Senbabaoglu%20and%20Elana%20Simon%20and%20Srinivasan%20Sivanandan%20and%20Gustavo%20Stolovitzky%20and%20Marc%20Valer%20and%20Bo%20Wang%20and%20Xikun%20Zhang%20and%20James%20Zou%20and%20Katrina%20Kalantar&entry.1292438233=%20%20Artificial%20intelligence%20holds%20immense%20promise%20for%20transforming%20biology%2C%20yet%20a%0Alack%20of%20standardized%2C%20cross%20domain%2C%20benchmarks%20undermines%20our%20ability%20to%20build%0Arobust%2C%20trustworthy%20models.%20Here%2C%20we%20present%20insights%20from%20a%20recent%20workshop%0Athat%20convened%20machine%20learning%20and%20computational%20biology%20experts%20across%0Aimaging%2C%20transcriptomics%2C%20proteomics%2C%20and%20genomics%20to%20tackle%20this%20gap.%20We%0Aidentify%20major%20technical%20and%20systemic%20bottlenecks%20such%20as%20data%20heterogeneity%0Aand%20noise%2C%20reproducibility%20challenges%2C%20biases%2C%20and%20the%20fragmented%20ecosystem%20of%0Apublicly%20available%20resources%20and%20propose%20a%20set%20of%20recommendations%20for%20building%0Abenchmarking%20frameworks%20that%20can%20efficiently%20compare%20ML%20models%20of%20biological%0Asystems%20across%20tasks%20and%20data%20modalities.%20By%20promoting%20high%20quality%20data%0Acuration%2C%20standardized%20tooling%2C%20comprehensive%20evaluation%20metrics%2C%20and%20open%2C%0Acollaborative%20platforms%2C%20we%20aim%20to%20accelerate%20the%20development%20of%20robust%0Abenchmarks%20for%20AI%20driven%20Virtual%20Cells.%20These%20benchmarks%20are%20crucial%20for%0Aensuring%20rigor%2C%20reproducibility%2C%20and%20biological%20relevance%2C%20and%20will%20ultimately%0Aadvance%20the%20field%20toward%20integrated%20models%20that%20drive%20new%20discoveries%2C%0Atherapeutic%20insights%2C%20and%20a%20deeper%20understanding%20of%20cellular%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10502v1&entry.124074799=Read"},
{"title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral\n  Prosthesis Users", "author": "Xiangyu Yin and Boyuan Yang and Weichen Liu and Qiyao Xue and Abrar Alamri and Goeran Fiedler and Wei Gao", "abstract": "  Prosthetic legs play a pivotal role in clinical rehabilitation, allowing\nindividuals with lower-limb amputations the ability to regain mobility and\nimprove their quality of life. Gait analysis is fundamental for optimizing\nprosthesis design and alignment, directly impacting the mobility and life\nquality of individuals with lower-limb amputations. Vision-based machine\nlearning (ML) methods offer a scalable and non-invasive solution to gait\nanalysis, but face challenges in correctly detecting and analyzing prosthesis,\ndue to their unique appearances and new movement patterns. In this paper, we\naim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,\nto support multiple vision tasks including Video Object Segmentation, 2D Human\nPose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from\nfour above-knee amputees when testing multiple newly-fitted prosthetic legs\nthrough walking trials, and depicts the presence, contours, poses, and gait\npatterns of human subjects with transfemoral prosthetic legs. Alongside the\ndataset itself, we also present benchmark tasks and fine-tuned baseline models\nto illustrate the practical application and performance of the ProGait dataset.\nWe compared our baseline models against pre-trained vision models,\ndemonstrating improved generalizability when applying the ProGait dataset for\nprosthesis-specific tasks. Our code is available at\nhttps://github.com/pittisl/ProGait and dataset at\nhttps://huggingface.co/datasets/ericyxy98/ProGait.\n", "link": "http://arxiv.org/abs/2507.10223v1", "date": "2025-07-14", "relevancy": 2.1966, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5807}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5479}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProGait%3A%20A%20Multi-Purpose%20Video%20Dataset%20and%20Benchmark%20for%20Transfemoral%0A%20%20Prosthesis%20Users&body=Title%3A%20ProGait%3A%20A%20Multi-Purpose%20Video%20Dataset%20and%20Benchmark%20for%20Transfemoral%0A%20%20Prosthesis%20Users%0AAuthor%3A%20Xiangyu%20Yin%20and%20Boyuan%20Yang%20and%20Weichen%20Liu%20and%20Qiyao%20Xue%20and%20Abrar%20Alamri%20and%20Goeran%20Fiedler%20and%20Wei%20Gao%0AAbstract%3A%20%20%20Prosthetic%20legs%20play%20a%20pivotal%20role%20in%20clinical%20rehabilitation%2C%20allowing%0Aindividuals%20with%20lower-limb%20amputations%20the%20ability%20to%20regain%20mobility%20and%0Aimprove%20their%20quality%20of%20life.%20Gait%20analysis%20is%20fundamental%20for%20optimizing%0Aprosthesis%20design%20and%20alignment%2C%20directly%20impacting%20the%20mobility%20and%20life%0Aquality%20of%20individuals%20with%20lower-limb%20amputations.%20Vision-based%20machine%0Alearning%20%28ML%29%20methods%20offer%20a%20scalable%20and%20non-invasive%20solution%20to%20gait%0Aanalysis%2C%20but%20face%20challenges%20in%20correctly%20detecting%20and%20analyzing%20prosthesis%2C%0Adue%20to%20their%20unique%20appearances%20and%20new%20movement%20patterns.%20In%20this%20paper%2C%20we%0Aaim%20to%20bridge%20this%20gap%20by%20introducing%20a%20multi-purpose%20dataset%2C%20namely%20ProGait%2C%0Ato%20support%20multiple%20vision%20tasks%20including%20Video%20Object%20Segmentation%2C%202D%20Human%0APose%20Estimation%2C%20and%20Gait%20Analysis%20%28GA%29.%20ProGait%20provides%20412%20video%20clips%20from%0Afour%20above-knee%20amputees%20when%20testing%20multiple%20newly-fitted%20prosthetic%20legs%0Athrough%20walking%20trials%2C%20and%20depicts%20the%20presence%2C%20contours%2C%20poses%2C%20and%20gait%0Apatterns%20of%20human%20subjects%20with%20transfemoral%20prosthetic%20legs.%20Alongside%20the%0Adataset%20itself%2C%20we%20also%20present%20benchmark%20tasks%20and%20fine-tuned%20baseline%20models%0Ato%20illustrate%20the%20practical%20application%20and%20performance%20of%20the%20ProGait%20dataset.%0AWe%20compared%20our%20baseline%20models%20against%20pre-trained%20vision%20models%2C%0Ademonstrating%20improved%20generalizability%20when%20applying%20the%20ProGait%20dataset%20for%0Aprosthesis-specific%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/pittisl/ProGait%20and%20dataset%20at%0Ahttps%3A//huggingface.co/datasets/ericyxy98/ProGait.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProGait%253A%2520A%2520Multi-Purpose%2520Video%2520Dataset%2520and%2520Benchmark%2520for%2520Transfemoral%250A%2520%2520Prosthesis%2520Users%26entry.906535625%3DXiangyu%2520Yin%2520and%2520Boyuan%2520Yang%2520and%2520Weichen%2520Liu%2520and%2520Qiyao%2520Xue%2520and%2520Abrar%2520Alamri%2520and%2520Goeran%2520Fiedler%2520and%2520Wei%2520Gao%26entry.1292438233%3D%2520%2520Prosthetic%2520legs%2520play%2520a%2520pivotal%2520role%2520in%2520clinical%2520rehabilitation%252C%2520allowing%250Aindividuals%2520with%2520lower-limb%2520amputations%2520the%2520ability%2520to%2520regain%2520mobility%2520and%250Aimprove%2520their%2520quality%2520of%2520life.%2520Gait%2520analysis%2520is%2520fundamental%2520for%2520optimizing%250Aprosthesis%2520design%2520and%2520alignment%252C%2520directly%2520impacting%2520the%2520mobility%2520and%2520life%250Aquality%2520of%2520individuals%2520with%2520lower-limb%2520amputations.%2520Vision-based%2520machine%250Alearning%2520%2528ML%2529%2520methods%2520offer%2520a%2520scalable%2520and%2520non-invasive%2520solution%2520to%2520gait%250Aanalysis%252C%2520but%2520face%2520challenges%2520in%2520correctly%2520detecting%2520and%2520analyzing%2520prosthesis%252C%250Adue%2520to%2520their%2520unique%2520appearances%2520and%2520new%2520movement%2520patterns.%2520In%2520this%2520paper%252C%2520we%250Aaim%2520to%2520bridge%2520this%2520gap%2520by%2520introducing%2520a%2520multi-purpose%2520dataset%252C%2520namely%2520ProGait%252C%250Ato%2520support%2520multiple%2520vision%2520tasks%2520including%2520Video%2520Object%2520Segmentation%252C%25202D%2520Human%250APose%2520Estimation%252C%2520and%2520Gait%2520Analysis%2520%2528GA%2529.%2520ProGait%2520provides%2520412%2520video%2520clips%2520from%250Afour%2520above-knee%2520amputees%2520when%2520testing%2520multiple%2520newly-fitted%2520prosthetic%2520legs%250Athrough%2520walking%2520trials%252C%2520and%2520depicts%2520the%2520presence%252C%2520contours%252C%2520poses%252C%2520and%2520gait%250Apatterns%2520of%2520human%2520subjects%2520with%2520transfemoral%2520prosthetic%2520legs.%2520Alongside%2520the%250Adataset%2520itself%252C%2520we%2520also%2520present%2520benchmark%2520tasks%2520and%2520fine-tuned%2520baseline%2520models%250Ato%2520illustrate%2520the%2520practical%2520application%2520and%2520performance%2520of%2520the%2520ProGait%2520dataset.%250AWe%2520compared%2520our%2520baseline%2520models%2520against%2520pre-trained%2520vision%2520models%252C%250Ademonstrating%2520improved%2520generalizability%2520when%2520applying%2520the%2520ProGait%2520dataset%2520for%250Aprosthesis-specific%2520tasks.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/pittisl/ProGait%2520and%2520dataset%2520at%250Ahttps%253A//huggingface.co/datasets/ericyxy98/ProGait.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProGait%3A%20A%20Multi-Purpose%20Video%20Dataset%20and%20Benchmark%20for%20Transfemoral%0A%20%20Prosthesis%20Users&entry.906535625=Xiangyu%20Yin%20and%20Boyuan%20Yang%20and%20Weichen%20Liu%20and%20Qiyao%20Xue%20and%20Abrar%20Alamri%20and%20Goeran%20Fiedler%20and%20Wei%20Gao&entry.1292438233=%20%20Prosthetic%20legs%20play%20a%20pivotal%20role%20in%20clinical%20rehabilitation%2C%20allowing%0Aindividuals%20with%20lower-limb%20amputations%20the%20ability%20to%20regain%20mobility%20and%0Aimprove%20their%20quality%20of%20life.%20Gait%20analysis%20is%20fundamental%20for%20optimizing%0Aprosthesis%20design%20and%20alignment%2C%20directly%20impacting%20the%20mobility%20and%20life%0Aquality%20of%20individuals%20with%20lower-limb%20amputations.%20Vision-based%20machine%0Alearning%20%28ML%29%20methods%20offer%20a%20scalable%20and%20non-invasive%20solution%20to%20gait%0Aanalysis%2C%20but%20face%20challenges%20in%20correctly%20detecting%20and%20analyzing%20prosthesis%2C%0Adue%20to%20their%20unique%20appearances%20and%20new%20movement%20patterns.%20In%20this%20paper%2C%20we%0Aaim%20to%20bridge%20this%20gap%20by%20introducing%20a%20multi-purpose%20dataset%2C%20namely%20ProGait%2C%0Ato%20support%20multiple%20vision%20tasks%20including%20Video%20Object%20Segmentation%2C%202D%20Human%0APose%20Estimation%2C%20and%20Gait%20Analysis%20%28GA%29.%20ProGait%20provides%20412%20video%20clips%20from%0Afour%20above-knee%20amputees%20when%20testing%20multiple%20newly-fitted%20prosthetic%20legs%0Athrough%20walking%20trials%2C%20and%20depicts%20the%20presence%2C%20contours%2C%20poses%2C%20and%20gait%0Apatterns%20of%20human%20subjects%20with%20transfemoral%20prosthetic%20legs.%20Alongside%20the%0Adataset%20itself%2C%20we%20also%20present%20benchmark%20tasks%20and%20fine-tuned%20baseline%20models%0Ato%20illustrate%20the%20practical%20application%20and%20performance%20of%20the%20ProGait%20dataset.%0AWe%20compared%20our%20baseline%20models%20against%20pre-trained%20vision%20models%2C%0Ademonstrating%20improved%20generalizability%20when%20applying%20the%20ProGait%20dataset%20for%0Aprosthesis-specific%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/pittisl/ProGait%20and%20dataset%20at%0Ahttps%3A//huggingface.co/datasets/ericyxy98/ProGait.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10223v1&entry.124074799=Read"},
{"title": "TKAN: Temporal Kolmogorov-Arnold Networks", "author": "Remi Genet and Hugo Inzirillo", "abstract": "  Recurrent Neural Networks (RNNs) have revolutionized many areas of machine\nlearning, particularly in natural language and data sequence processing. Long\nShort-Term Memory (LSTM) has demonstrated its ability to capture long-term\ndependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks\n(KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed\na new neural networks architecture inspired by KAN and the LSTM, the Temporal\nKolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both\nnetworks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers\nembedding memory management. This innovation enables us to perform multi-step\ntime series forecasting with enhanced accuracy and efficiency. By addressing\nthe limitations of traditional models in handling complex sequential patterns,\nthe TKAN architecture offers significant potential for advancements in fields\nrequiring more than one step ahead forecasting.\n", "link": "http://arxiv.org/abs/2405.07344v4", "date": "2025-07-14", "relevancy": 2.1911, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4429}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4381}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks&body=Title%3A%20TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Remi%20Genet%20and%20Hugo%20Inzirillo%0AAbstract%3A%20%20%20Recurrent%20Neural%20Networks%20%28RNNs%29%20have%20revolutionized%20many%20areas%20of%20machine%0Alearning%2C%20particularly%20in%20natural%20language%20and%20data%20sequence%20processing.%20Long%0AShort-Term%20Memory%20%28LSTM%29%20has%20demonstrated%20its%20ability%20to%20capture%20long-term%0Adependencies%20in%20sequential%20data.%20Inspired%20by%20the%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%20a%20promising%20alternatives%20to%20Multi-Layer%20Perceptrons%20%28MLPs%29%2C%20we%20proposed%0Aa%20new%20neural%20networks%20architecture%20inspired%20by%20KAN%20and%20the%20LSTM%2C%20the%20Temporal%0AKolomogorov-Arnold%20Networks%20%28TKANs%29.%20TKANs%20combined%20the%20strenght%20of%20both%0Anetworks%2C%20it%20is%20composed%20of%20Recurring%20Kolmogorov-Arnold%20Networks%20%28RKANs%29%20Layers%0Aembedding%20memory%20management.%20This%20innovation%20enables%20us%20to%20perform%20multi-step%0Atime%20series%20forecasting%20with%20enhanced%20accuracy%20and%20efficiency.%20By%20addressing%0Athe%20limitations%20of%20traditional%20models%20in%20handling%20complex%20sequential%20patterns%2C%0Athe%20TKAN%20architecture%20offers%20significant%20potential%20for%20advancements%20in%20fields%0Arequiring%20more%20than%20one%20step%20ahead%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07344v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTKAN%253A%2520Temporal%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DRemi%2520Genet%2520and%2520Hugo%2520Inzirillo%26entry.1292438233%3D%2520%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520have%2520revolutionized%2520many%2520areas%2520of%2520machine%250Alearning%252C%2520particularly%2520in%2520natural%2520language%2520and%2520data%2520sequence%2520processing.%2520Long%250AShort-Term%2520Memory%2520%2528LSTM%2529%2520has%2520demonstrated%2520its%2520ability%2520to%2520capture%2520long-term%250Adependencies%2520in%2520sequential%2520data.%2520Inspired%2520by%2520the%2520Kolmogorov-Arnold%2520Networks%250A%2528KANs%2529%2520a%2520promising%2520alternatives%2520to%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%252C%2520we%2520proposed%250Aa%2520new%2520neural%2520networks%2520architecture%2520inspired%2520by%2520KAN%2520and%2520the%2520LSTM%252C%2520the%2520Temporal%250AKolomogorov-Arnold%2520Networks%2520%2528TKANs%2529.%2520TKANs%2520combined%2520the%2520strenght%2520of%2520both%250Anetworks%252C%2520it%2520is%2520composed%2520of%2520Recurring%2520Kolmogorov-Arnold%2520Networks%2520%2528RKANs%2529%2520Layers%250Aembedding%2520memory%2520management.%2520This%2520innovation%2520enables%2520us%2520to%2520perform%2520multi-step%250Atime%2520series%2520forecasting%2520with%2520enhanced%2520accuracy%2520and%2520efficiency.%2520By%2520addressing%250Athe%2520limitations%2520of%2520traditional%2520models%2520in%2520handling%2520complex%2520sequential%2520patterns%252C%250Athe%2520TKAN%2520architecture%2520offers%2520significant%2520potential%2520for%2520advancements%2520in%2520fields%250Arequiring%2520more%2520than%2520one%2520step%2520ahead%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07344v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks&entry.906535625=Remi%20Genet%20and%20Hugo%20Inzirillo&entry.1292438233=%20%20Recurrent%20Neural%20Networks%20%28RNNs%29%20have%20revolutionized%20many%20areas%20of%20machine%0Alearning%2C%20particularly%20in%20natural%20language%20and%20data%20sequence%20processing.%20Long%0AShort-Term%20Memory%20%28LSTM%29%20has%20demonstrated%20its%20ability%20to%20capture%20long-term%0Adependencies%20in%20sequential%20data.%20Inspired%20by%20the%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%20a%20promising%20alternatives%20to%20Multi-Layer%20Perceptrons%20%28MLPs%29%2C%20we%20proposed%0Aa%20new%20neural%20networks%20architecture%20inspired%20by%20KAN%20and%20the%20LSTM%2C%20the%20Temporal%0AKolomogorov-Arnold%20Networks%20%28TKANs%29.%20TKANs%20combined%20the%20strenght%20of%20both%0Anetworks%2C%20it%20is%20composed%20of%20Recurring%20Kolmogorov-Arnold%20Networks%20%28RKANs%29%20Layers%0Aembedding%20memory%20management.%20This%20innovation%20enables%20us%20to%20perform%20multi-step%0Atime%20series%20forecasting%20with%20enhanced%20accuracy%20and%20efficiency.%20By%20addressing%0Athe%20limitations%20of%20traditional%20models%20in%20handling%20complex%20sequential%20patterns%2C%0Athe%20TKAN%20architecture%20offers%20significant%20potential%20for%20advancements%20in%20fields%0Arequiring%20more%20than%20one%20step%20ahead%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07344v4&entry.124074799=Read"},
{"title": "Parallel Sampling of Diffusion Models on $SO(3)$", "author": "Yan-Ting Chen and Hao-Wei Chen and Tsu-Ching Hsiao and Chun-Yi Lee", "abstract": "  In this paper, we design an algorithm to accelerate the diffusion process on\nthe $SO(3)$ manifold. The inherently sequential nature of diffusion models\nnecessitates substantial time for denoising perturbed data. To overcome this\nlimitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$\nspace. We demonstrate our algorithm on an existing method that employs\ndiffusion models to address the pose ambiguity problem. Moreover, we show that\nthis acceleration advantage occurs without any measurable degradation in task\nreward. The experiments reveal that our algorithm achieves a speed-up of up to\n4.9$\\times$, significantly reducing the latency for generating a single sample.\n", "link": "http://arxiv.org/abs/2507.10347v1", "date": "2025-07-14", "relevancy": 2.1864, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.57}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20Sampling%20of%20Diffusion%20Models%20on%20%24SO%283%29%24&body=Title%3A%20Parallel%20Sampling%20of%20Diffusion%20Models%20on%20%24SO%283%29%24%0AAuthor%3A%20Yan-Ting%20Chen%20and%20Hao-Wei%20Chen%20and%20Tsu-Ching%20Hsiao%20and%20Chun-Yi%20Lee%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20design%20an%20algorithm%20to%20accelerate%20the%20diffusion%20process%20on%0Athe%20%24SO%283%29%24%20manifold.%20The%20inherently%20sequential%20nature%20of%20diffusion%20models%0Anecessitates%20substantial%20time%20for%20denoising%20perturbed%20data.%20To%20overcome%20this%0Alimitation%2C%20we%20proposed%20to%20adapt%20the%20numerical%20Picard%20iteration%20for%20the%20%24SO%283%29%24%0Aspace.%20We%20demonstrate%20our%20algorithm%20on%20an%20existing%20method%20that%20employs%0Adiffusion%20models%20to%20address%20the%20pose%20ambiguity%20problem.%20Moreover%2C%20we%20show%20that%0Athis%20acceleration%20advantage%20occurs%20without%20any%20measurable%20degradation%20in%20task%0Areward.%20The%20experiments%20reveal%20that%20our%20algorithm%20achieves%20a%20speed-up%20of%20up%20to%0A4.9%24%5Ctimes%24%2C%20significantly%20reducing%20the%20latency%20for%20generating%20a%20single%20sample.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520Sampling%2520of%2520Diffusion%2520Models%2520on%2520%2524SO%25283%2529%2524%26entry.906535625%3DYan-Ting%2520Chen%2520and%2520Hao-Wei%2520Chen%2520and%2520Tsu-Ching%2520Hsiao%2520and%2520Chun-Yi%2520Lee%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520design%2520an%2520algorithm%2520to%2520accelerate%2520the%2520diffusion%2520process%2520on%250Athe%2520%2524SO%25283%2529%2524%2520manifold.%2520The%2520inherently%2520sequential%2520nature%2520of%2520diffusion%2520models%250Anecessitates%2520substantial%2520time%2520for%2520denoising%2520perturbed%2520data.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520proposed%2520to%2520adapt%2520the%2520numerical%2520Picard%2520iteration%2520for%2520the%2520%2524SO%25283%2529%2524%250Aspace.%2520We%2520demonstrate%2520our%2520algorithm%2520on%2520an%2520existing%2520method%2520that%2520employs%250Adiffusion%2520models%2520to%2520address%2520the%2520pose%2520ambiguity%2520problem.%2520Moreover%252C%2520we%2520show%2520that%250Athis%2520acceleration%2520advantage%2520occurs%2520without%2520any%2520measurable%2520degradation%2520in%2520task%250Areward.%2520The%2520experiments%2520reveal%2520that%2520our%2520algorithm%2520achieves%2520a%2520speed-up%2520of%2520up%2520to%250A4.9%2524%255Ctimes%2524%252C%2520significantly%2520reducing%2520the%2520latency%2520for%2520generating%2520a%2520single%2520sample.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20Sampling%20of%20Diffusion%20Models%20on%20%24SO%283%29%24&entry.906535625=Yan-Ting%20Chen%20and%20Hao-Wei%20Chen%20and%20Tsu-Ching%20Hsiao%20and%20Chun-Yi%20Lee&entry.1292438233=%20%20In%20this%20paper%2C%20we%20design%20an%20algorithm%20to%20accelerate%20the%20diffusion%20process%20on%0Athe%20%24SO%283%29%24%20manifold.%20The%20inherently%20sequential%20nature%20of%20diffusion%20models%0Anecessitates%20substantial%20time%20for%20denoising%20perturbed%20data.%20To%20overcome%20this%0Alimitation%2C%20we%20proposed%20to%20adapt%20the%20numerical%20Picard%20iteration%20for%20the%20%24SO%283%29%24%0Aspace.%20We%20demonstrate%20our%20algorithm%20on%20an%20existing%20method%20that%20employs%0Adiffusion%20models%20to%20address%20the%20pose%20ambiguity%20problem.%20Moreover%2C%20we%20show%20that%0Athis%20acceleration%20advantage%20occurs%20without%20any%20measurable%20degradation%20in%20task%0Areward.%20The%20experiments%20reveal%20that%20our%20algorithm%20achieves%20a%20speed-up%20of%20up%20to%0A4.9%24%5Ctimes%24%2C%20significantly%20reducing%20the%20latency%20for%20generating%20a%20single%20sample.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10347v1&entry.124074799=Read"},
{"title": "Devanagari Handwritten Character Recognition using Convolutional Neural\n  Network", "author": "Diksha Mehta and Prateek Mehta", "abstract": "  Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.\n", "link": "http://arxiv.org/abs/2507.10398v1", "date": "2025-07-14", "relevancy": 2.175, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4414}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4355}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Devanagari%20Handwritten%20Character%20Recognition%20using%20Convolutional%20Neural%0A%20%20Network&body=Title%3A%20Devanagari%20Handwritten%20Character%20Recognition%20using%20Convolutional%20Neural%0A%20%20Network%0AAuthor%3A%20Diksha%20Mehta%20and%20Prateek%20Mehta%0AAbstract%3A%20%20%20Handwritten%20character%20recognition%20is%20getting%20popular%20among%20researchers%0Abecause%20of%20its%20possible%20applications%20in%20facilitating%20technological%20search%0Aengines%2C%20social%20media%2C%20recommender%20systems%2C%20etc.%20The%20Devanagari%20script%20is%20one%0Aof%20the%20oldest%20language%20scripts%20in%20India%20that%20does%20not%20have%20proper%20digitization%0Atools.%20With%20the%20advancement%20of%20computing%20and%20technology%2C%20the%20task%20of%20this%0Aresearch%20is%20to%20extract%20handwritten%20Hindi%20characters%20from%20an%20image%20of%20Devanagari%0Ascript%20with%20an%20automated%20approach%20to%20save%20time%20and%20obsolete%20data.%20In%20this%0Apaper%2C%20we%20present%20a%20technique%20to%20recognize%20handwritten%20Devanagari%20characters%0Ausing%20two%20deep%20convolutional%20neural%20network%20layers.%20This%20work%20employs%20a%0Amethodology%20that%20is%20useful%20to%20enhance%20the%20recognition%20rate%20and%20configures%20a%0Aconvolutional%20neural%20network%20for%20effective%20Devanagari%20handwritten%20text%0Arecognition%20%28DHTR%29.%20This%20approach%20uses%20the%20Devanagari%20handwritten%20character%0Adataset%20%28DHCD%29%2C%20an%20open%20dataset%20with%2036%20classes%20of%20Devanagari%20characters.%20Each%0Aof%20these%20classes%20has%201700%20images%20for%20training%20and%20testing%20purposes.%20This%0Aapproach%20obtains%20promising%20results%20in%20terms%20of%20accuracy%20by%20achieving%2096.36%25%0Aaccuracy%20in%20testing%20and%2099.55%25%20in%20training%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevanagari%2520Handwritten%2520Character%2520Recognition%2520using%2520Convolutional%2520Neural%250A%2520%2520Network%26entry.906535625%3DDiksha%2520Mehta%2520and%2520Prateek%2520Mehta%26entry.1292438233%3D%2520%2520Handwritten%2520character%2520recognition%2520is%2520getting%2520popular%2520among%2520researchers%250Abecause%2520of%2520its%2520possible%2520applications%2520in%2520facilitating%2520technological%2520search%250Aengines%252C%2520social%2520media%252C%2520recommender%2520systems%252C%2520etc.%2520The%2520Devanagari%2520script%2520is%2520one%250Aof%2520the%2520oldest%2520language%2520scripts%2520in%2520India%2520that%2520does%2520not%2520have%2520proper%2520digitization%250Atools.%2520With%2520the%2520advancement%2520of%2520computing%2520and%2520technology%252C%2520the%2520task%2520of%2520this%250Aresearch%2520is%2520to%2520extract%2520handwritten%2520Hindi%2520characters%2520from%2520an%2520image%2520of%2520Devanagari%250Ascript%2520with%2520an%2520automated%2520approach%2520to%2520save%2520time%2520and%2520obsolete%2520data.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520technique%2520to%2520recognize%2520handwritten%2520Devanagari%2520characters%250Ausing%2520two%2520deep%2520convolutional%2520neural%2520network%2520layers.%2520This%2520work%2520employs%2520a%250Amethodology%2520that%2520is%2520useful%2520to%2520enhance%2520the%2520recognition%2520rate%2520and%2520configures%2520a%250Aconvolutional%2520neural%2520network%2520for%2520effective%2520Devanagari%2520handwritten%2520text%250Arecognition%2520%2528DHTR%2529.%2520This%2520approach%2520uses%2520the%2520Devanagari%2520handwritten%2520character%250Adataset%2520%2528DHCD%2529%252C%2520an%2520open%2520dataset%2520with%252036%2520classes%2520of%2520Devanagari%2520characters.%2520Each%250Aof%2520these%2520classes%2520has%25201700%2520images%2520for%2520training%2520and%2520testing%2520purposes.%2520This%250Aapproach%2520obtains%2520promising%2520results%2520in%2520terms%2520of%2520accuracy%2520by%2520achieving%252096.36%2525%250Aaccuracy%2520in%2520testing%2520and%252099.55%2525%2520in%2520training%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Devanagari%20Handwritten%20Character%20Recognition%20using%20Convolutional%20Neural%0A%20%20Network&entry.906535625=Diksha%20Mehta%20and%20Prateek%20Mehta&entry.1292438233=%20%20Handwritten%20character%20recognition%20is%20getting%20popular%20among%20researchers%0Abecause%20of%20its%20possible%20applications%20in%20facilitating%20technological%20search%0Aengines%2C%20social%20media%2C%20recommender%20systems%2C%20etc.%20The%20Devanagari%20script%20is%20one%0Aof%20the%20oldest%20language%20scripts%20in%20India%20that%20does%20not%20have%20proper%20digitization%0Atools.%20With%20the%20advancement%20of%20computing%20and%20technology%2C%20the%20task%20of%20this%0Aresearch%20is%20to%20extract%20handwritten%20Hindi%20characters%20from%20an%20image%20of%20Devanagari%0Ascript%20with%20an%20automated%20approach%20to%20save%20time%20and%20obsolete%20data.%20In%20this%0Apaper%2C%20we%20present%20a%20technique%20to%20recognize%20handwritten%20Devanagari%20characters%0Ausing%20two%20deep%20convolutional%20neural%20network%20layers.%20This%20work%20employs%20a%0Amethodology%20that%20is%20useful%20to%20enhance%20the%20recognition%20rate%20and%20configures%20a%0Aconvolutional%20neural%20network%20for%20effective%20Devanagari%20handwritten%20text%0Arecognition%20%28DHTR%29.%20This%20approach%20uses%20the%20Devanagari%20handwritten%20character%0Adataset%20%28DHCD%29%2C%20an%20open%20dataset%20with%2036%20classes%20of%20Devanagari%20characters.%20Each%0Aof%20these%20classes%20has%201700%20images%20for%20training%20and%20testing%20purposes.%20This%0Aapproach%20obtains%20promising%20results%20in%20terms%20of%20accuracy%20by%20achieving%2096.36%25%0Aaccuracy%20in%20testing%20and%2099.55%25%20in%20training%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10398v1&entry.124074799=Read"},
{"title": "Large-Scale Graph Building in Dynamic Environments: Low Latency and High\n  Quality", "author": "Filipe Miguel Gon\u00e7alves de Almeida and CJ Carey and Hendrik Fichtenberger and Jonathan Halcrow and Silvio Lattanzi and Andr\u00e9 Linhares and Tao Meng and Ashkan Norouzi-Fard and Nikos Parotsidis and Bryan Perozzi and David Simcha", "abstract": "  Learning and constructing large-scale graphs has attracted attention in\nrecent decades, resulting in a rich literature that introduced various systems,\ntools, and algorithms. Grale is one of such tools that is designed for offline\nenvironments and is deployed in more than 50 different industrial settings at\nGoogle. Grale is widely applicable because of its ability to efficiently learn\nand construct a graph on datasets with multiple types of features. However, it\nis often the case that applications require the underlying data to evolve\ncontinuously and rapidly and the updated graph needs to be available with low\nlatency. Such setting make the use of Grale prohibitive. While there are\nApproximate Nearest Neighbor (ANN) systems that handle dynamic updates with low\nlatency, they are mostly limited to similarities over a single embedding.\n  In this work, we introduce a system that inherits the advantages and the\nquality of Grale, and maintains a graph construction in a dynamic setting with\ntens of milliseconds of latency per request. We call the system Dynamic Grale\nUsing ScaNN (Dynamic GUS). Our system has a wide range of applications with\nover 10 deployments at Google. One of the applications is in Android Security\nand Privacy, where Dynamic Grale Using ScaNN enables capturing harmful\napplications 4 times faster, before they can reach users.\n", "link": "http://arxiv.org/abs/2507.10139v1", "date": "2025-07-14", "relevancy": 2.1725, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5552}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5442}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Scale%20Graph%20Building%20in%20Dynamic%20Environments%3A%20Low%20Latency%20and%20High%0A%20%20Quality&body=Title%3A%20Large-Scale%20Graph%20Building%20in%20Dynamic%20Environments%3A%20Low%20Latency%20and%20High%0A%20%20Quality%0AAuthor%3A%20Filipe%20Miguel%20Gon%C3%A7alves%20de%20Almeida%20and%20CJ%20Carey%20and%20Hendrik%20Fichtenberger%20and%20Jonathan%20Halcrow%20and%20Silvio%20Lattanzi%20and%20Andr%C3%A9%20Linhares%20and%20Tao%20Meng%20and%20Ashkan%20Norouzi-Fard%20and%20Nikos%20Parotsidis%20and%20Bryan%20Perozzi%20and%20David%20Simcha%0AAbstract%3A%20%20%20Learning%20and%20constructing%20large-scale%20graphs%20has%20attracted%20attention%20in%0Arecent%20decades%2C%20resulting%20in%20a%20rich%20literature%20that%20introduced%20various%20systems%2C%0Atools%2C%20and%20algorithms.%20Grale%20is%20one%20of%20such%20tools%20that%20is%20designed%20for%20offline%0Aenvironments%20and%20is%20deployed%20in%20more%20than%2050%20different%20industrial%20settings%20at%0AGoogle.%20Grale%20is%20widely%20applicable%20because%20of%20its%20ability%20to%20efficiently%20learn%0Aand%20construct%20a%20graph%20on%20datasets%20with%20multiple%20types%20of%20features.%20However%2C%20it%0Ais%20often%20the%20case%20that%20applications%20require%20the%20underlying%20data%20to%20evolve%0Acontinuously%20and%20rapidly%20and%20the%20updated%20graph%20needs%20to%20be%20available%20with%20low%0Alatency.%20Such%20setting%20make%20the%20use%20of%20Grale%20prohibitive.%20While%20there%20are%0AApproximate%20Nearest%20Neighbor%20%28ANN%29%20systems%20that%20handle%20dynamic%20updates%20with%20low%0Alatency%2C%20they%20are%20mostly%20limited%20to%20similarities%20over%20a%20single%20embedding.%0A%20%20In%20this%20work%2C%20we%20introduce%20a%20system%20that%20inherits%20the%20advantages%20and%20the%0Aquality%20of%20Grale%2C%20and%20maintains%20a%20graph%20construction%20in%20a%20dynamic%20setting%20with%0Atens%20of%20milliseconds%20of%20latency%20per%20request.%20We%20call%20the%20system%20Dynamic%20Grale%0AUsing%20ScaNN%20%28Dynamic%20GUS%29.%20Our%20system%20has%20a%20wide%20range%20of%20applications%20with%0Aover%2010%20deployments%20at%20Google.%20One%20of%20the%20applications%20is%20in%20Android%20Security%0Aand%20Privacy%2C%20where%20Dynamic%20Grale%20Using%20ScaNN%20enables%20capturing%20harmful%0Aapplications%204%20times%20faster%2C%20before%20they%20can%20reach%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Scale%2520Graph%2520Building%2520in%2520Dynamic%2520Environments%253A%2520Low%2520Latency%2520and%2520High%250A%2520%2520Quality%26entry.906535625%3DFilipe%2520Miguel%2520Gon%25C3%25A7alves%2520de%2520Almeida%2520and%2520CJ%2520Carey%2520and%2520Hendrik%2520Fichtenberger%2520and%2520Jonathan%2520Halcrow%2520and%2520Silvio%2520Lattanzi%2520and%2520Andr%25C3%25A9%2520Linhares%2520and%2520Tao%2520Meng%2520and%2520Ashkan%2520Norouzi-Fard%2520and%2520Nikos%2520Parotsidis%2520and%2520Bryan%2520Perozzi%2520and%2520David%2520Simcha%26entry.1292438233%3D%2520%2520Learning%2520and%2520constructing%2520large-scale%2520graphs%2520has%2520attracted%2520attention%2520in%250Arecent%2520decades%252C%2520resulting%2520in%2520a%2520rich%2520literature%2520that%2520introduced%2520various%2520systems%252C%250Atools%252C%2520and%2520algorithms.%2520Grale%2520is%2520one%2520of%2520such%2520tools%2520that%2520is%2520designed%2520for%2520offline%250Aenvironments%2520and%2520is%2520deployed%2520in%2520more%2520than%252050%2520different%2520industrial%2520settings%2520at%250AGoogle.%2520Grale%2520is%2520widely%2520applicable%2520because%2520of%2520its%2520ability%2520to%2520efficiently%2520learn%250Aand%2520construct%2520a%2520graph%2520on%2520datasets%2520with%2520multiple%2520types%2520of%2520features.%2520However%252C%2520it%250Ais%2520often%2520the%2520case%2520that%2520applications%2520require%2520the%2520underlying%2520data%2520to%2520evolve%250Acontinuously%2520and%2520rapidly%2520and%2520the%2520updated%2520graph%2520needs%2520to%2520be%2520available%2520with%2520low%250Alatency.%2520Such%2520setting%2520make%2520the%2520use%2520of%2520Grale%2520prohibitive.%2520While%2520there%2520are%250AApproximate%2520Nearest%2520Neighbor%2520%2528ANN%2529%2520systems%2520that%2520handle%2520dynamic%2520updates%2520with%2520low%250Alatency%252C%2520they%2520are%2520mostly%2520limited%2520to%2520similarities%2520over%2520a%2520single%2520embedding.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520system%2520that%2520inherits%2520the%2520advantages%2520and%2520the%250Aquality%2520of%2520Grale%252C%2520and%2520maintains%2520a%2520graph%2520construction%2520in%2520a%2520dynamic%2520setting%2520with%250Atens%2520of%2520milliseconds%2520of%2520latency%2520per%2520request.%2520We%2520call%2520the%2520system%2520Dynamic%2520Grale%250AUsing%2520ScaNN%2520%2528Dynamic%2520GUS%2529.%2520Our%2520system%2520has%2520a%2520wide%2520range%2520of%2520applications%2520with%250Aover%252010%2520deployments%2520at%2520Google.%2520One%2520of%2520the%2520applications%2520is%2520in%2520Android%2520Security%250Aand%2520Privacy%252C%2520where%2520Dynamic%2520Grale%2520Using%2520ScaNN%2520enables%2520capturing%2520harmful%250Aapplications%25204%2520times%2520faster%252C%2520before%2520they%2520can%2520reach%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Scale%20Graph%20Building%20in%20Dynamic%20Environments%3A%20Low%20Latency%20and%20High%0A%20%20Quality&entry.906535625=Filipe%20Miguel%20Gon%C3%A7alves%20de%20Almeida%20and%20CJ%20Carey%20and%20Hendrik%20Fichtenberger%20and%20Jonathan%20Halcrow%20and%20Silvio%20Lattanzi%20and%20Andr%C3%A9%20Linhares%20and%20Tao%20Meng%20and%20Ashkan%20Norouzi-Fard%20and%20Nikos%20Parotsidis%20and%20Bryan%20Perozzi%20and%20David%20Simcha&entry.1292438233=%20%20Learning%20and%20constructing%20large-scale%20graphs%20has%20attracted%20attention%20in%0Arecent%20decades%2C%20resulting%20in%20a%20rich%20literature%20that%20introduced%20various%20systems%2C%0Atools%2C%20and%20algorithms.%20Grale%20is%20one%20of%20such%20tools%20that%20is%20designed%20for%20offline%0Aenvironments%20and%20is%20deployed%20in%20more%20than%2050%20different%20industrial%20settings%20at%0AGoogle.%20Grale%20is%20widely%20applicable%20because%20of%20its%20ability%20to%20efficiently%20learn%0Aand%20construct%20a%20graph%20on%20datasets%20with%20multiple%20types%20of%20features.%20However%2C%20it%0Ais%20often%20the%20case%20that%20applications%20require%20the%20underlying%20data%20to%20evolve%0Acontinuously%20and%20rapidly%20and%20the%20updated%20graph%20needs%20to%20be%20available%20with%20low%0Alatency.%20Such%20setting%20make%20the%20use%20of%20Grale%20prohibitive.%20While%20there%20are%0AApproximate%20Nearest%20Neighbor%20%28ANN%29%20systems%20that%20handle%20dynamic%20updates%20with%20low%0Alatency%2C%20they%20are%20mostly%20limited%20to%20similarities%20over%20a%20single%20embedding.%0A%20%20In%20this%20work%2C%20we%20introduce%20a%20system%20that%20inherits%20the%20advantages%20and%20the%0Aquality%20of%20Grale%2C%20and%20maintains%20a%20graph%20construction%20in%20a%20dynamic%20setting%20with%0Atens%20of%20milliseconds%20of%20latency%20per%20request.%20We%20call%20the%20system%20Dynamic%20Grale%0AUsing%20ScaNN%20%28Dynamic%20GUS%29.%20Our%20system%20has%20a%20wide%20range%20of%20applications%20with%0Aover%2010%20deployments%20at%20Google.%20One%20of%20the%20applications%20is%20in%20Android%20Security%0Aand%20Privacy%2C%20where%20Dynamic%20Grale%20Using%20ScaNN%20enables%20capturing%20harmful%0Aapplications%204%20times%20faster%2C%20before%20they%20can%20reach%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10139v1&entry.124074799=Read"},
{"title": "Navigating the Challenges of AI-Generated Image Detection in the Wild:\n  What Truly Matters?", "author": "Despina Konstantinidou and Dimitrios Karageorgiou and Christos Koutlis and Olga Papadopoulou and Emmanouil Schinas and Symeon Papadopoulos", "abstract": "  The rapid advancement of generative technologies presents both unprecedented\ncreative opportunities and significant challenges, particularly in maintaining\nsocial trust and ensuring the integrity of digital information. Following these\nconcerns, the challenge of AI-Generated Image Detection (AID) becomes\nincreasingly critical. As these technologies become more sophisticated, the\nquality of AI-generated images has reached a level that can easily deceive even\nthe most discerning observers. Our systematic evaluation highlights a critical\nweakness in current AI-Generated Image Detection models: while they perform\nexceptionally well on controlled benchmark datasets, they struggle\nsignificantly with real-world variations. To assess this, we introduce ITW-SM,\na new dataset of real and AI-generated images collected from major social media\nplatforms. In this paper, we identify four key factors that influence AID\nperformance in real-world scenarios: backbone architecture, training data\ncomposition, pre-processing strategies and data augmentation combinations. By\nsystematically analyzing these components, we shed light on their impact on\ndetection efficacy. Our modifications result in an average AUC improvement of\n26.87% across various AID models under real-world conditions.\n", "link": "http://arxiv.org/abs/2507.10236v1", "date": "2025-07-14", "relevancy": 2.1703, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5597}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5306}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20the%20Challenges%20of%20AI-Generated%20Image%20Detection%20in%20the%20Wild%3A%0A%20%20What%20Truly%20Matters%3F&body=Title%3A%20Navigating%20the%20Challenges%20of%20AI-Generated%20Image%20Detection%20in%20the%20Wild%3A%0A%20%20What%20Truly%20Matters%3F%0AAuthor%3A%20Despina%20Konstantinidou%20and%20Dimitrios%20Karageorgiou%20and%20Christos%20Koutlis%20and%20Olga%20Papadopoulou%20and%20Emmanouil%20Schinas%20and%20Symeon%20Papadopoulos%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20generative%20technologies%20presents%20both%20unprecedented%0Acreative%20opportunities%20and%20significant%20challenges%2C%20particularly%20in%20maintaining%0Asocial%20trust%20and%20ensuring%20the%20integrity%20of%20digital%20information.%20Following%20these%0Aconcerns%2C%20the%20challenge%20of%20AI-Generated%20Image%20Detection%20%28AID%29%20becomes%0Aincreasingly%20critical.%20As%20these%20technologies%20become%20more%20sophisticated%2C%20the%0Aquality%20of%20AI-generated%20images%20has%20reached%20a%20level%20that%20can%20easily%20deceive%20even%0Athe%20most%20discerning%20observers.%20Our%20systematic%20evaluation%20highlights%20a%20critical%0Aweakness%20in%20current%20AI-Generated%20Image%20Detection%20models%3A%20while%20they%20perform%0Aexceptionally%20well%20on%20controlled%20benchmark%20datasets%2C%20they%20struggle%0Asignificantly%20with%20real-world%20variations.%20To%20assess%20this%2C%20we%20introduce%20ITW-SM%2C%0Aa%20new%20dataset%20of%20real%20and%20AI-generated%20images%20collected%20from%20major%20social%20media%0Aplatforms.%20In%20this%20paper%2C%20we%20identify%20four%20key%20factors%20that%20influence%20AID%0Aperformance%20in%20real-world%20scenarios%3A%20backbone%20architecture%2C%20training%20data%0Acomposition%2C%20pre-processing%20strategies%20and%20data%20augmentation%20combinations.%20By%0Asystematically%20analyzing%20these%20components%2C%20we%20shed%20light%20on%20their%20impact%20on%0Adetection%20efficacy.%20Our%20modifications%20result%20in%20an%20average%20AUC%20improvement%20of%0A26.87%25%20across%20various%20AID%20models%20under%20real-world%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520the%2520Challenges%2520of%2520AI-Generated%2520Image%2520Detection%2520in%2520the%2520Wild%253A%250A%2520%2520What%2520Truly%2520Matters%253F%26entry.906535625%3DDespina%2520Konstantinidou%2520and%2520Dimitrios%2520Karageorgiou%2520and%2520Christos%2520Koutlis%2520and%2520Olga%2520Papadopoulou%2520and%2520Emmanouil%2520Schinas%2520and%2520Symeon%2520Papadopoulos%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520generative%2520technologies%2520presents%2520both%2520unprecedented%250Acreative%2520opportunities%2520and%2520significant%2520challenges%252C%2520particularly%2520in%2520maintaining%250Asocial%2520trust%2520and%2520ensuring%2520the%2520integrity%2520of%2520digital%2520information.%2520Following%2520these%250Aconcerns%252C%2520the%2520challenge%2520of%2520AI-Generated%2520Image%2520Detection%2520%2528AID%2529%2520becomes%250Aincreasingly%2520critical.%2520As%2520these%2520technologies%2520become%2520more%2520sophisticated%252C%2520the%250Aquality%2520of%2520AI-generated%2520images%2520has%2520reached%2520a%2520level%2520that%2520can%2520easily%2520deceive%2520even%250Athe%2520most%2520discerning%2520observers.%2520Our%2520systematic%2520evaluation%2520highlights%2520a%2520critical%250Aweakness%2520in%2520current%2520AI-Generated%2520Image%2520Detection%2520models%253A%2520while%2520they%2520perform%250Aexceptionally%2520well%2520on%2520controlled%2520benchmark%2520datasets%252C%2520they%2520struggle%250Asignificantly%2520with%2520real-world%2520variations.%2520To%2520assess%2520this%252C%2520we%2520introduce%2520ITW-SM%252C%250Aa%2520new%2520dataset%2520of%2520real%2520and%2520AI-generated%2520images%2520collected%2520from%2520major%2520social%2520media%250Aplatforms.%2520In%2520this%2520paper%252C%2520we%2520identify%2520four%2520key%2520factors%2520that%2520influence%2520AID%250Aperformance%2520in%2520real-world%2520scenarios%253A%2520backbone%2520architecture%252C%2520training%2520data%250Acomposition%252C%2520pre-processing%2520strategies%2520and%2520data%2520augmentation%2520combinations.%2520By%250Asystematically%2520analyzing%2520these%2520components%252C%2520we%2520shed%2520light%2520on%2520their%2520impact%2520on%250Adetection%2520efficacy.%2520Our%2520modifications%2520result%2520in%2520an%2520average%2520AUC%2520improvement%2520of%250A26.87%2525%2520across%2520various%2520AID%2520models%2520under%2520real-world%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20the%20Challenges%20of%20AI-Generated%20Image%20Detection%20in%20the%20Wild%3A%0A%20%20What%20Truly%20Matters%3F&entry.906535625=Despina%20Konstantinidou%20and%20Dimitrios%20Karageorgiou%20and%20Christos%20Koutlis%20and%20Olga%20Papadopoulou%20and%20Emmanouil%20Schinas%20and%20Symeon%20Papadopoulos&entry.1292438233=%20%20The%20rapid%20advancement%20of%20generative%20technologies%20presents%20both%20unprecedented%0Acreative%20opportunities%20and%20significant%20challenges%2C%20particularly%20in%20maintaining%0Asocial%20trust%20and%20ensuring%20the%20integrity%20of%20digital%20information.%20Following%20these%0Aconcerns%2C%20the%20challenge%20of%20AI-Generated%20Image%20Detection%20%28AID%29%20becomes%0Aincreasingly%20critical.%20As%20these%20technologies%20become%20more%20sophisticated%2C%20the%0Aquality%20of%20AI-generated%20images%20has%20reached%20a%20level%20that%20can%20easily%20deceive%20even%0Athe%20most%20discerning%20observers.%20Our%20systematic%20evaluation%20highlights%20a%20critical%0Aweakness%20in%20current%20AI-Generated%20Image%20Detection%20models%3A%20while%20they%20perform%0Aexceptionally%20well%20on%20controlled%20benchmark%20datasets%2C%20they%20struggle%0Asignificantly%20with%20real-world%20variations.%20To%20assess%20this%2C%20we%20introduce%20ITW-SM%2C%0Aa%20new%20dataset%20of%20real%20and%20AI-generated%20images%20collected%20from%20major%20social%20media%0Aplatforms.%20In%20this%20paper%2C%20we%20identify%20four%20key%20factors%20that%20influence%20AID%0Aperformance%20in%20real-world%20scenarios%3A%20backbone%20architecture%2C%20training%20data%0Acomposition%2C%20pre-processing%20strategies%20and%20data%20augmentation%20combinations.%20By%0Asystematically%20analyzing%20these%20components%2C%20we%20shed%20light%20on%20their%20impact%20on%0Adetection%20efficacy.%20Our%20modifications%20result%20in%20an%20average%20AUC%20improvement%20of%0A26.87%25%20across%20various%20AID%20models%20under%20real-world%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10236v1&entry.124074799=Read"},
{"title": "Gamma: Toward Generic Image Assessment with Mixture of Assessment\n  Experts", "author": "Hantao Zhou and Rui Yang and Longxiang Tang and Guanyi Qin and Runze Hu and Xiu Li", "abstract": "  Image assessment aims to evaluate the quality and aesthetics of images and\nhas been applied across various scenarios, such as natural and AIGC scenes.\nExisting methods mostly address these sub-tasks or scenes individually. While\nsome works attempt to develop unified image assessment models, they have\nstruggled to achieve satisfactory performance or cover a broad spectrum of\nassessment scenarios. In this paper, we present \\textbf{Gamma}, a\n\\textbf{G}eneric im\\textbf{A}ge assess\\textbf{M}ent model using\n\\textbf{M}ixture of \\textbf{A}ssessment Experts, which can effectively assess\nimages from diverse scenes through mixed-dataset training. Achieving unified\ntraining in image assessment presents significant challenges due to annotation\nbiases across different datasets. To address this issue, we first propose a\nMixture of Assessment Experts (MoAE) module, which employs shared and adaptive\nexperts to dynamically learn common and specific knowledge for different\ndatasets, respectively. In addition, we introduce a Scene-based Differential\nPrompt (SDP) strategy, which uses scene-specific prompts to provide prior\nknowledge and guidance during the learning process, further boosting adaptation\nfor various scenes. Our Gamma model is trained and evaluated on 12 datasets\nspanning 6 image assessment scenarios. Extensive experiments show that our\nunified Gamma outperforms other state-of-the-art mixed-training methods by\nsignificant margins while covering more scenes. Codes are available at\nhttps://github.com/zht8506/Gamma.\n", "link": "http://arxiv.org/abs/2503.06678v2", "date": "2025-07-14", "relevancy": 2.1669, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5532}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5336}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gamma%3A%20Toward%20Generic%20Image%20Assessment%20with%20Mixture%20of%20Assessment%0A%20%20Experts&body=Title%3A%20Gamma%3A%20Toward%20Generic%20Image%20Assessment%20with%20Mixture%20of%20Assessment%0A%20%20Experts%0AAuthor%3A%20Hantao%20Zhou%20and%20Rui%20Yang%20and%20Longxiang%20Tang%20and%20Guanyi%20Qin%20and%20Runze%20Hu%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Image%20assessment%20aims%20to%20evaluate%20the%20quality%20and%20aesthetics%20of%20images%20and%0Ahas%20been%20applied%20across%20various%20scenarios%2C%20such%20as%20natural%20and%20AIGC%20scenes.%0AExisting%20methods%20mostly%20address%20these%20sub-tasks%20or%20scenes%20individually.%20While%0Asome%20works%20attempt%20to%20develop%20unified%20image%20assessment%20models%2C%20they%20have%0Astruggled%20to%20achieve%20satisfactory%20performance%20or%20cover%20a%20broad%20spectrum%20of%0Aassessment%20scenarios.%20In%20this%20paper%2C%20we%20present%20%5Ctextbf%7BGamma%7D%2C%20a%0A%5Ctextbf%7BG%7Deneric%20im%5Ctextbf%7BA%7Dge%20assess%5Ctextbf%7BM%7Dent%20model%20using%0A%5Ctextbf%7BM%7Dixture%20of%20%5Ctextbf%7BA%7Dssessment%20Experts%2C%20which%20can%20effectively%20assess%0Aimages%20from%20diverse%20scenes%20through%20mixed-dataset%20training.%20Achieving%20unified%0Atraining%20in%20image%20assessment%20presents%20significant%20challenges%20due%20to%20annotation%0Abiases%20across%20different%20datasets.%20To%20address%20this%20issue%2C%20we%20first%20propose%20a%0AMixture%20of%20Assessment%20Experts%20%28MoAE%29%20module%2C%20which%20employs%20shared%20and%20adaptive%0Aexperts%20to%20dynamically%20learn%20common%20and%20specific%20knowledge%20for%20different%0Adatasets%2C%20respectively.%20In%20addition%2C%20we%20introduce%20a%20Scene-based%20Differential%0APrompt%20%28SDP%29%20strategy%2C%20which%20uses%20scene-specific%20prompts%20to%20provide%20prior%0Aknowledge%20and%20guidance%20during%20the%20learning%20process%2C%20further%20boosting%20adaptation%0Afor%20various%20scenes.%20Our%20Gamma%20model%20is%20trained%20and%20evaluated%20on%2012%20datasets%0Aspanning%206%20image%20assessment%20scenarios.%20Extensive%20experiments%20show%20that%20our%0Aunified%20Gamma%20outperforms%20other%20state-of-the-art%20mixed-training%20methods%20by%0Asignificant%20margins%20while%20covering%20more%20scenes.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/zht8506/Gamma.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGamma%253A%2520Toward%2520Generic%2520Image%2520Assessment%2520with%2520Mixture%2520of%2520Assessment%250A%2520%2520Experts%26entry.906535625%3DHantao%2520Zhou%2520and%2520Rui%2520Yang%2520and%2520Longxiang%2520Tang%2520and%2520Guanyi%2520Qin%2520and%2520Runze%2520Hu%2520and%2520Xiu%2520Li%26entry.1292438233%3D%2520%2520Image%2520assessment%2520aims%2520to%2520evaluate%2520the%2520quality%2520and%2520aesthetics%2520of%2520images%2520and%250Ahas%2520been%2520applied%2520across%2520various%2520scenarios%252C%2520such%2520as%2520natural%2520and%2520AIGC%2520scenes.%250AExisting%2520methods%2520mostly%2520address%2520these%2520sub-tasks%2520or%2520scenes%2520individually.%2520While%250Asome%2520works%2520attempt%2520to%2520develop%2520unified%2520image%2520assessment%2520models%252C%2520they%2520have%250Astruggled%2520to%2520achieve%2520satisfactory%2520performance%2520or%2520cover%2520a%2520broad%2520spectrum%2520of%250Aassessment%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520present%2520%255Ctextbf%257BGamma%257D%252C%2520a%250A%255Ctextbf%257BG%257Deneric%2520im%255Ctextbf%257BA%257Dge%2520assess%255Ctextbf%257BM%257Dent%2520model%2520using%250A%255Ctextbf%257BM%257Dixture%2520of%2520%255Ctextbf%257BA%257Dssessment%2520Experts%252C%2520which%2520can%2520effectively%2520assess%250Aimages%2520from%2520diverse%2520scenes%2520through%2520mixed-dataset%2520training.%2520Achieving%2520unified%250Atraining%2520in%2520image%2520assessment%2520presents%2520significant%2520challenges%2520due%2520to%2520annotation%250Abiases%2520across%2520different%2520datasets.%2520To%2520address%2520this%2520issue%252C%2520we%2520first%2520propose%2520a%250AMixture%2520of%2520Assessment%2520Experts%2520%2528MoAE%2529%2520module%252C%2520which%2520employs%2520shared%2520and%2520adaptive%250Aexperts%2520to%2520dynamically%2520learn%2520common%2520and%2520specific%2520knowledge%2520for%2520different%250Adatasets%252C%2520respectively.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520Scene-based%2520Differential%250APrompt%2520%2528SDP%2529%2520strategy%252C%2520which%2520uses%2520scene-specific%2520prompts%2520to%2520provide%2520prior%250Aknowledge%2520and%2520guidance%2520during%2520the%2520learning%2520process%252C%2520further%2520boosting%2520adaptation%250Afor%2520various%2520scenes.%2520Our%2520Gamma%2520model%2520is%2520trained%2520and%2520evaluated%2520on%252012%2520datasets%250Aspanning%25206%2520image%2520assessment%2520scenarios.%2520Extensive%2520experiments%2520show%2520that%2520our%250Aunified%2520Gamma%2520outperforms%2520other%2520state-of-the-art%2520mixed-training%2520methods%2520by%250Asignificant%2520margins%2520while%2520covering%2520more%2520scenes.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/zht8506/Gamma.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gamma%3A%20Toward%20Generic%20Image%20Assessment%20with%20Mixture%20of%20Assessment%0A%20%20Experts&entry.906535625=Hantao%20Zhou%20and%20Rui%20Yang%20and%20Longxiang%20Tang%20and%20Guanyi%20Qin%20and%20Runze%20Hu%20and%20Xiu%20Li&entry.1292438233=%20%20Image%20assessment%20aims%20to%20evaluate%20the%20quality%20and%20aesthetics%20of%20images%20and%0Ahas%20been%20applied%20across%20various%20scenarios%2C%20such%20as%20natural%20and%20AIGC%20scenes.%0AExisting%20methods%20mostly%20address%20these%20sub-tasks%20or%20scenes%20individually.%20While%0Asome%20works%20attempt%20to%20develop%20unified%20image%20assessment%20models%2C%20they%20have%0Astruggled%20to%20achieve%20satisfactory%20performance%20or%20cover%20a%20broad%20spectrum%20of%0Aassessment%20scenarios.%20In%20this%20paper%2C%20we%20present%20%5Ctextbf%7BGamma%7D%2C%20a%0A%5Ctextbf%7BG%7Deneric%20im%5Ctextbf%7BA%7Dge%20assess%5Ctextbf%7BM%7Dent%20model%20using%0A%5Ctextbf%7BM%7Dixture%20of%20%5Ctextbf%7BA%7Dssessment%20Experts%2C%20which%20can%20effectively%20assess%0Aimages%20from%20diverse%20scenes%20through%20mixed-dataset%20training.%20Achieving%20unified%0Atraining%20in%20image%20assessment%20presents%20significant%20challenges%20due%20to%20annotation%0Abiases%20across%20different%20datasets.%20To%20address%20this%20issue%2C%20we%20first%20propose%20a%0AMixture%20of%20Assessment%20Experts%20%28MoAE%29%20module%2C%20which%20employs%20shared%20and%20adaptive%0Aexperts%20to%20dynamically%20learn%20common%20and%20specific%20knowledge%20for%20different%0Adatasets%2C%20respectively.%20In%20addition%2C%20we%20introduce%20a%20Scene-based%20Differential%0APrompt%20%28SDP%29%20strategy%2C%20which%20uses%20scene-specific%20prompts%20to%20provide%20prior%0Aknowledge%20and%20guidance%20during%20the%20learning%20process%2C%20further%20boosting%20adaptation%0Afor%20various%20scenes.%20Our%20Gamma%20model%20is%20trained%20and%20evaluated%20on%2012%20datasets%0Aspanning%206%20image%20assessment%20scenarios.%20Extensive%20experiments%20show%20that%20our%0Aunified%20Gamma%20outperforms%20other%20state-of-the-art%20mixed-training%20methods%20by%0Asignificant%20margins%20while%20covering%20more%20scenes.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/zht8506/Gamma.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06678v2&entry.124074799=Read"},
{"title": "Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong\n  Error Lower Bounds", "author": "Rishikesh Srinivasan and Dheeraj Nagaraj", "abstract": "  We study the problem of sampling from strongly log-concave distributions over\n$\\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the\nrandomized midpoint method) for overdamped/underdamped Langevin dynamics. We\nprove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic\nspeedup in dependence on the target accuracy ($\\epsilon$) over the\nEuler-Maruyama discretization, surpassing existing bounds for randomized\nmidpoint methods. Notably, in the case of underdamped Langevin dynamics, we\ndemonstrate the complexity of $W_2$ convergence is much smaller than the\ncomplexity lower bounds for convergence in $L^2$ strong error established in\nthe literature.\n", "link": "http://arxiv.org/abs/2506.07614v2", "date": "2025-07-14", "relevancy": 2.1597, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4474}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.426}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poisson%20Midpoint%20Method%20for%20Log%20Concave%20Sampling%3A%20Beyond%20the%20Strong%0A%20%20Error%20Lower%20Bounds&body=Title%3A%20Poisson%20Midpoint%20Method%20for%20Log%20Concave%20Sampling%3A%20Beyond%20the%20Strong%0A%20%20Error%20Lower%20Bounds%0AAuthor%3A%20Rishikesh%20Srinivasan%20and%20Dheeraj%20Nagaraj%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20sampling%20from%20strongly%20log-concave%20distributions%20over%0A%24%5Cmathbb%7BR%7D%5Ed%24%20using%20the%20Poisson%20midpoint%20discretization%20%28a%20variant%20of%20the%0Arandomized%20midpoint%20method%29%20for%20overdamped/underdamped%20Langevin%20dynamics.%20We%0Aprove%20its%20convergence%20in%20the%202-Wasserstein%20distance%20%28%24W_2%24%29%2C%20achieving%20a%20cubic%0Aspeedup%20in%20dependence%20on%20the%20target%20accuracy%20%28%24%5Cepsilon%24%29%20over%20the%0AEuler-Maruyama%20discretization%2C%20surpassing%20existing%20bounds%20for%20randomized%0Amidpoint%20methods.%20Notably%2C%20in%20the%20case%20of%20underdamped%20Langevin%20dynamics%2C%20we%0Ademonstrate%20the%20complexity%20of%20%24W_2%24%20convergence%20is%20much%20smaller%20than%20the%0Acomplexity%20lower%20bounds%20for%20convergence%20in%20%24L%5E2%24%20strong%20error%20established%20in%0Athe%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoisson%2520Midpoint%2520Method%2520for%2520Log%2520Concave%2520Sampling%253A%2520Beyond%2520the%2520Strong%250A%2520%2520Error%2520Lower%2520Bounds%26entry.906535625%3DRishikesh%2520Srinivasan%2520and%2520Dheeraj%2520Nagaraj%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520sampling%2520from%2520strongly%2520log-concave%2520distributions%2520over%250A%2524%255Cmathbb%257BR%257D%255Ed%2524%2520using%2520the%2520Poisson%2520midpoint%2520discretization%2520%2528a%2520variant%2520of%2520the%250Arandomized%2520midpoint%2520method%2529%2520for%2520overdamped/underdamped%2520Langevin%2520dynamics.%2520We%250Aprove%2520its%2520convergence%2520in%2520the%25202-Wasserstein%2520distance%2520%2528%2524W_2%2524%2529%252C%2520achieving%2520a%2520cubic%250Aspeedup%2520in%2520dependence%2520on%2520the%2520target%2520accuracy%2520%2528%2524%255Cepsilon%2524%2529%2520over%2520the%250AEuler-Maruyama%2520discretization%252C%2520surpassing%2520existing%2520bounds%2520for%2520randomized%250Amidpoint%2520methods.%2520Notably%252C%2520in%2520the%2520case%2520of%2520underdamped%2520Langevin%2520dynamics%252C%2520we%250Ademonstrate%2520the%2520complexity%2520of%2520%2524W_2%2524%2520convergence%2520is%2520much%2520smaller%2520than%2520the%250Acomplexity%2520lower%2520bounds%2520for%2520convergence%2520in%2520%2524L%255E2%2524%2520strong%2520error%2520established%2520in%250Athe%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poisson%20Midpoint%20Method%20for%20Log%20Concave%20Sampling%3A%20Beyond%20the%20Strong%0A%20%20Error%20Lower%20Bounds&entry.906535625=Rishikesh%20Srinivasan%20and%20Dheeraj%20Nagaraj&entry.1292438233=%20%20We%20study%20the%20problem%20of%20sampling%20from%20strongly%20log-concave%20distributions%20over%0A%24%5Cmathbb%7BR%7D%5Ed%24%20using%20the%20Poisson%20midpoint%20discretization%20%28a%20variant%20of%20the%0Arandomized%20midpoint%20method%29%20for%20overdamped/underdamped%20Langevin%20dynamics.%20We%0Aprove%20its%20convergence%20in%20the%202-Wasserstein%20distance%20%28%24W_2%24%29%2C%20achieving%20a%20cubic%0Aspeedup%20in%20dependence%20on%20the%20target%20accuracy%20%28%24%5Cepsilon%24%29%20over%20the%0AEuler-Maruyama%20discretization%2C%20surpassing%20existing%20bounds%20for%20randomized%0Amidpoint%20methods.%20Notably%2C%20in%20the%20case%20of%20underdamped%20Langevin%20dynamics%2C%20we%0Ademonstrate%20the%20complexity%20of%20%24W_2%24%20convergence%20is%20much%20smaller%20than%20the%0Acomplexity%20lower%20bounds%20for%20convergence%20in%20%24L%5E2%24%20strong%20error%20established%20in%0Athe%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07614v2&entry.124074799=Read"},
{"title": "Improving Remote Sensing Classification using Topological Data Analysis\n  and Convolutional Neural Networks", "author": "Aaryam Sharma", "abstract": "  Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.\n", "link": "http://arxiv.org/abs/2507.10381v1", "date": "2025-07-14", "relevancy": 2.1508, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5566}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5279}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Remote%20Sensing%20Classification%20using%20Topological%20Data%20Analysis%0A%20%20and%20Convolutional%20Neural%20Networks&body=Title%3A%20Improving%20Remote%20Sensing%20Classification%20using%20Topological%20Data%20Analysis%0A%20%20and%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Aaryam%20Sharma%0AAbstract%3A%20%20%20Topological%20data%20analysis%20%28TDA%29%20is%20a%20relatively%20new%20field%20that%20is%20gaining%0Arapid%20adoption%20due%20to%20its%20robustness%20and%20ability%20to%20effectively%20describe%0Acomplex%20datasets%20by%20quantifying%20geometric%20information.%20In%20imaging%20contexts%2C%20TDA%0Atypically%20models%20data%20as%20filtered%20cubical%20complexes%20from%20which%20we%20can%20extract%0Adiscriminative%20features%20using%20persistence%20homology.%20Meanwhile%2C%20convolutional%0Aneural%20networks%20%28CNNs%29%20have%20been%20shown%20to%20be%20biased%20towards%20texture%20based%20local%0Afeatures.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20TDA%20feature%20engineering%0Apipeline%20and%20a%20simple%20method%20to%20integrate%20topological%20features%20with%20deep%0Alearning%20models%20on%20remote%20sensing%20classification.%20Our%20method%20improves%20the%0Aperformance%20of%20a%20ResNet18%20model%20on%20the%20EuroSAT%20dataset%20by%201.44%25%20achieving%0A99.33%25%20accuracy%2C%20which%20surpasses%20all%20previously%20reported%20single-model%0Aaccuracies%2C%20including%20those%20with%20larger%20architectures%2C%20such%20as%20ResNet50%20%282x%0Alarger%29%20and%20XL%20Vision%20Transformers%20%28197x%20larger%29.%20We%20additionally%20show%20that%20our%0Amethod%27s%20accuracy%20is%201.82%25%20higher%20than%20our%20ResNet18%20baseline%20on%20the%20RESISC45%0Adataset.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%20TDA%20features%20in%0Asatellite%20scene%20classification%20with%20deep%20learning.%20This%20demonstrates%20that%20TDA%0Afeatures%20can%20be%20integrated%20with%20deep%20learning%20models%2C%20even%20on%20datasets%20without%0Aexplicit%20topological%20structures%2C%20thereby%20increasing%20the%20applicability%20of%20TDA.%20A%0Aclean%20implementation%20of%20our%20method%20will%20be%20made%20publicly%20available%20upon%0Apublication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Remote%2520Sensing%2520Classification%2520using%2520Topological%2520Data%2520Analysis%250A%2520%2520and%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DAaryam%2520Sharma%26entry.1292438233%3D%2520%2520Topological%2520data%2520analysis%2520%2528TDA%2529%2520is%2520a%2520relatively%2520new%2520field%2520that%2520is%2520gaining%250Arapid%2520adoption%2520due%2520to%2520its%2520robustness%2520and%2520ability%2520to%2520effectively%2520describe%250Acomplex%2520datasets%2520by%2520quantifying%2520geometric%2520information.%2520In%2520imaging%2520contexts%252C%2520TDA%250Atypically%2520models%2520data%2520as%2520filtered%2520cubical%2520complexes%2520from%2520which%2520we%2520can%2520extract%250Adiscriminative%2520features%2520using%2520persistence%2520homology.%2520Meanwhile%252C%2520convolutional%250Aneural%2520networks%2520%2528CNNs%2529%2520have%2520been%2520shown%2520to%2520be%2520biased%2520towards%2520texture%2520based%2520local%250Afeatures.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520TDA%2520feature%2520engineering%250Apipeline%2520and%2520a%2520simple%2520method%2520to%2520integrate%2520topological%2520features%2520with%2520deep%250Alearning%2520models%2520on%2520remote%2520sensing%2520classification.%2520Our%2520method%2520improves%2520the%250Aperformance%2520of%2520a%2520ResNet18%2520model%2520on%2520the%2520EuroSAT%2520dataset%2520by%25201.44%2525%2520achieving%250A99.33%2525%2520accuracy%252C%2520which%2520surpasses%2520all%2520previously%2520reported%2520single-model%250Aaccuracies%252C%2520including%2520those%2520with%2520larger%2520architectures%252C%2520such%2520as%2520ResNet50%2520%25282x%250Alarger%2529%2520and%2520XL%2520Vision%2520Transformers%2520%2528197x%2520larger%2529.%2520We%2520additionally%2520show%2520that%2520our%250Amethod%2527s%2520accuracy%2520is%25201.82%2525%2520higher%2520than%2520our%2520ResNet18%2520baseline%2520on%2520the%2520RESISC45%250Adataset.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520application%2520of%2520TDA%2520features%2520in%250Asatellite%2520scene%2520classification%2520with%2520deep%2520learning.%2520This%2520demonstrates%2520that%2520TDA%250Afeatures%2520can%2520be%2520integrated%2520with%2520deep%2520learning%2520models%252C%2520even%2520on%2520datasets%2520without%250Aexplicit%2520topological%2520structures%252C%2520thereby%2520increasing%2520the%2520applicability%2520of%2520TDA.%2520A%250Aclean%2520implementation%2520of%2520our%2520method%2520will%2520be%2520made%2520publicly%2520available%2520upon%250Apublication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Remote%20Sensing%20Classification%20using%20Topological%20Data%20Analysis%0A%20%20and%20Convolutional%20Neural%20Networks&entry.906535625=Aaryam%20Sharma&entry.1292438233=%20%20Topological%20data%20analysis%20%28TDA%29%20is%20a%20relatively%20new%20field%20that%20is%20gaining%0Arapid%20adoption%20due%20to%20its%20robustness%20and%20ability%20to%20effectively%20describe%0Acomplex%20datasets%20by%20quantifying%20geometric%20information.%20In%20imaging%20contexts%2C%20TDA%0Atypically%20models%20data%20as%20filtered%20cubical%20complexes%20from%20which%20we%20can%20extract%0Adiscriminative%20features%20using%20persistence%20homology.%20Meanwhile%2C%20convolutional%0Aneural%20networks%20%28CNNs%29%20have%20been%20shown%20to%20be%20biased%20towards%20texture%20based%20local%0Afeatures.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20TDA%20feature%20engineering%0Apipeline%20and%20a%20simple%20method%20to%20integrate%20topological%20features%20with%20deep%0Alearning%20models%20on%20remote%20sensing%20classification.%20Our%20method%20improves%20the%0Aperformance%20of%20a%20ResNet18%20model%20on%20the%20EuroSAT%20dataset%20by%201.44%25%20achieving%0A99.33%25%20accuracy%2C%20which%20surpasses%20all%20previously%20reported%20single-model%0Aaccuracies%2C%20including%20those%20with%20larger%20architectures%2C%20such%20as%20ResNet50%20%282x%0Alarger%29%20and%20XL%20Vision%20Transformers%20%28197x%20larger%29.%20We%20additionally%20show%20that%20our%0Amethod%27s%20accuracy%20is%201.82%25%20higher%20than%20our%20ResNet18%20baseline%20on%20the%20RESISC45%0Adataset.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%20TDA%20features%20in%0Asatellite%20scene%20classification%20with%20deep%20learning.%20This%20demonstrates%20that%20TDA%0Afeatures%20can%20be%20integrated%20with%20deep%20learning%20models%2C%20even%20on%20datasets%20without%0Aexplicit%20topological%20structures%2C%20thereby%20increasing%20the%20applicability%20of%20TDA.%20A%0Aclean%20implementation%20of%20our%20method%20will%20be%20made%20publicly%20available%20upon%0Apublication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10381v1&entry.124074799=Read"},
{"title": "Self-supervised Learning on Camera Trap Footage Yields a Strong\n  Universal Face Embedder", "author": "Vladimir Iashin and Horace Lee and Dan Schofield and Andrew Zisserman", "abstract": "  Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.\n", "link": "http://arxiv.org/abs/2507.10552v1", "date": "2025-07-14", "relevancy": 2.1495, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5352}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Learning%20on%20Camera%20Trap%20Footage%20Yields%20a%20Strong%0A%20%20Universal%20Face%20Embedder&body=Title%3A%20Self-supervised%20Learning%20on%20Camera%20Trap%20Footage%20Yields%20a%20Strong%0A%20%20Universal%20Face%20Embedder%0AAuthor%3A%20Vladimir%20Iashin%20and%20Horace%20Lee%20and%20Dan%20Schofield%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Camera%20traps%20are%20revolutionising%20wildlife%20monitoring%20by%20capturing%20vast%0Aamounts%20of%20visual%20data%3B%20however%2C%20the%20manual%20identification%20of%20individual%0Aanimals%20remains%20a%20significant%20bottleneck.%20This%20study%20introduces%20a%20fully%0Aself-supervised%20approach%20to%20learning%20robust%20chimpanzee%20face%20embeddings%20from%0Aunlabeled%20camera-trap%20footage.%20Leveraging%20the%20DINOv2%20framework%2C%20we%20train%20Vision%0ATransformers%20on%20automatically%20mined%20face%20crops%2C%20eliminating%20the%20need%20for%0Aidentity%20labels.%20Our%20method%20demonstrates%20strong%20open-set%20re-identification%0Aperformance%2C%20surpassing%20supervised%20baselines%20on%20challenging%20benchmarks%20such%20as%0ABossou%2C%20despite%20utilising%20no%20labelled%20data%20during%20training.%20This%20work%0Aunderscores%20the%20potential%20of%20self-supervised%20learning%20in%20biodiversity%0Amonitoring%20and%20paves%20the%20way%20for%20scalable%2C%20non-invasive%20population%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Learning%2520on%2520Camera%2520Trap%2520Footage%2520Yields%2520a%2520Strong%250A%2520%2520Universal%2520Face%2520Embedder%26entry.906535625%3DVladimir%2520Iashin%2520and%2520Horace%2520Lee%2520and%2520Dan%2520Schofield%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Camera%2520traps%2520are%2520revolutionising%2520wildlife%2520monitoring%2520by%2520capturing%2520vast%250Aamounts%2520of%2520visual%2520data%253B%2520however%252C%2520the%2520manual%2520identification%2520of%2520individual%250Aanimals%2520remains%2520a%2520significant%2520bottleneck.%2520This%2520study%2520introduces%2520a%2520fully%250Aself-supervised%2520approach%2520to%2520learning%2520robust%2520chimpanzee%2520face%2520embeddings%2520from%250Aunlabeled%2520camera-trap%2520footage.%2520Leveraging%2520the%2520DINOv2%2520framework%252C%2520we%2520train%2520Vision%250ATransformers%2520on%2520automatically%2520mined%2520face%2520crops%252C%2520eliminating%2520the%2520need%2520for%250Aidentity%2520labels.%2520Our%2520method%2520demonstrates%2520strong%2520open-set%2520re-identification%250Aperformance%252C%2520surpassing%2520supervised%2520baselines%2520on%2520challenging%2520benchmarks%2520such%2520as%250ABossou%252C%2520despite%2520utilising%2520no%2520labelled%2520data%2520during%2520training.%2520This%2520work%250Aunderscores%2520the%2520potential%2520of%2520self-supervised%2520learning%2520in%2520biodiversity%250Amonitoring%2520and%2520paves%2520the%2520way%2520for%2520scalable%252C%2520non-invasive%2520population%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Learning%20on%20Camera%20Trap%20Footage%20Yields%20a%20Strong%0A%20%20Universal%20Face%20Embedder&entry.906535625=Vladimir%20Iashin%20and%20Horace%20Lee%20and%20Dan%20Schofield%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Camera%20traps%20are%20revolutionising%20wildlife%20monitoring%20by%20capturing%20vast%0Aamounts%20of%20visual%20data%3B%20however%2C%20the%20manual%20identification%20of%20individual%0Aanimals%20remains%20a%20significant%20bottleneck.%20This%20study%20introduces%20a%20fully%0Aself-supervised%20approach%20to%20learning%20robust%20chimpanzee%20face%20embeddings%20from%0Aunlabeled%20camera-trap%20footage.%20Leveraging%20the%20DINOv2%20framework%2C%20we%20train%20Vision%0ATransformers%20on%20automatically%20mined%20face%20crops%2C%20eliminating%20the%20need%20for%0Aidentity%20labels.%20Our%20method%20demonstrates%20strong%20open-set%20re-identification%0Aperformance%2C%20surpassing%20supervised%20baselines%20on%20challenging%20benchmarks%20such%20as%0ABossou%2C%20despite%20utilising%20no%20labelled%20data%20during%20training.%20This%20work%0Aunderscores%20the%20potential%20of%20self-supervised%20learning%20in%20biodiversity%0Amonitoring%20and%20paves%20the%20way%20for%20scalable%2C%20non-invasive%20population%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10552v1&entry.124074799=Read"},
{"title": "Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction", "author": "Vaishnavh Nagarajan and Chen Henry Wu and Charles Ding and Aditi Raghunathan", "abstract": "  We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic; multi-token approaches,\nnamely teacherless training and diffusion models, comparatively excel in\nproducing diverse and original output. Secondly, to elicit randomness without\nhurting coherence, we find that injecting noise at the input layer (dubbed\nseed-conditioning) works surprisingly as well as (and in some conditions,\nbetter than) temperature sampling from the output layer. Thus, our work offers\na principled, minimal test-bed for analyzing open-ended creative skills, and\noffers new arguments for going beyond next-token learning and temperature\nsampling. We make part of the code available under\nhttps://github.com/chenwu98/algorithmic-creativity\n", "link": "http://arxiv.org/abs/2504.15266v3", "date": "2025-07-14", "relevancy": 2.1488, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5441}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.541}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Roll%20the%20dice%20%26%20look%20before%20you%20leap%3A%20Going%20beyond%20the%20creative%20limits%0A%20%20of%20next-token%20prediction&body=Title%3A%20Roll%20the%20dice%20%26%20look%20before%20you%20leap%3A%20Going%20beyond%20the%20creative%20limits%0A%20%20of%20next-token%20prediction%0AAuthor%3A%20Vaishnavh%20Nagarajan%20and%20Chen%20Henry%20Wu%20and%20Charles%20Ding%20and%20Aditi%20Raghunathan%0AAbstract%3A%20%20%20We%20design%20a%20suite%20of%20minimal%20algorithmic%20tasks%20that%20are%20a%20loose%20abstraction%0Aof%20open-ended%20real-world%20tasks.%20This%20allows%20us%20to%20cleanly%20and%20controllably%0Aquantify%20the%20creative%20limits%20of%20the%20present-day%20language%20model.%20Much%20like%0Areal-world%20tasks%20that%20require%20a%20creative%2C%20far-sighted%20leap%20of%20thought%2C%20our%0Atasks%20require%20an%20implicit%2C%20open-ended%20stochastic%20planning%20step%20that%20either%20%28a%29%0Adiscovers%20new%20connections%20in%20an%20abstract%20knowledge%20graph%20%28like%20in%20wordplay%2C%0Adrawing%20analogies%2C%20or%20research%29%20or%20%28b%29%20constructs%20new%20patterns%20%28like%20in%0Adesigning%20math%20problems%20or%20new%20proteins%29.%20In%20these%20tasks%2C%20we%20empirically%20and%0Aconceptually%20argue%20how%20next-token%20learning%20is%20myopic%3B%20multi-token%20approaches%2C%0Anamely%20teacherless%20training%20and%20diffusion%20models%2C%20comparatively%20excel%20in%0Aproducing%20diverse%20and%20original%20output.%20Secondly%2C%20to%20elicit%20randomness%20without%0Ahurting%20coherence%2C%20we%20find%20that%20injecting%20noise%20at%20the%20input%20layer%20%28dubbed%0Aseed-conditioning%29%20works%20surprisingly%20as%20well%20as%20%28and%20in%20some%20conditions%2C%0Abetter%20than%29%20temperature%20sampling%20from%20the%20output%20layer.%20Thus%2C%20our%20work%20offers%0Aa%20principled%2C%20minimal%20test-bed%20for%20analyzing%20open-ended%20creative%20skills%2C%20and%0Aoffers%20new%20arguments%20for%20going%20beyond%20next-token%20learning%20and%20temperature%0Asampling.%20We%20make%20part%20of%20the%20code%20available%20under%0Ahttps%3A//github.com/chenwu98/algorithmic-creativity%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15266v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoll%2520the%2520dice%2520%2526%2520look%2520before%2520you%2520leap%253A%2520Going%2520beyond%2520the%2520creative%2520limits%250A%2520%2520of%2520next-token%2520prediction%26entry.906535625%3DVaishnavh%2520Nagarajan%2520and%2520Chen%2520Henry%2520Wu%2520and%2520Charles%2520Ding%2520and%2520Aditi%2520Raghunathan%26entry.1292438233%3D%2520%2520We%2520design%2520a%2520suite%2520of%2520minimal%2520algorithmic%2520tasks%2520that%2520are%2520a%2520loose%2520abstraction%250Aof%2520open-ended%2520real-world%2520tasks.%2520This%2520allows%2520us%2520to%2520cleanly%2520and%2520controllably%250Aquantify%2520the%2520creative%2520limits%2520of%2520the%2520present-day%2520language%2520model.%2520Much%2520like%250Areal-world%2520tasks%2520that%2520require%2520a%2520creative%252C%2520far-sighted%2520leap%2520of%2520thought%252C%2520our%250Atasks%2520require%2520an%2520implicit%252C%2520open-ended%2520stochastic%2520planning%2520step%2520that%2520either%2520%2528a%2529%250Adiscovers%2520new%2520connections%2520in%2520an%2520abstract%2520knowledge%2520graph%2520%2528like%2520in%2520wordplay%252C%250Adrawing%2520analogies%252C%2520or%2520research%2529%2520or%2520%2528b%2529%2520constructs%2520new%2520patterns%2520%2528like%2520in%250Adesigning%2520math%2520problems%2520or%2520new%2520proteins%2529.%2520In%2520these%2520tasks%252C%2520we%2520empirically%2520and%250Aconceptually%2520argue%2520how%2520next-token%2520learning%2520is%2520myopic%253B%2520multi-token%2520approaches%252C%250Anamely%2520teacherless%2520training%2520and%2520diffusion%2520models%252C%2520comparatively%2520excel%2520in%250Aproducing%2520diverse%2520and%2520original%2520output.%2520Secondly%252C%2520to%2520elicit%2520randomness%2520without%250Ahurting%2520coherence%252C%2520we%2520find%2520that%2520injecting%2520noise%2520at%2520the%2520input%2520layer%2520%2528dubbed%250Aseed-conditioning%2529%2520works%2520surprisingly%2520as%2520well%2520as%2520%2528and%2520in%2520some%2520conditions%252C%250Abetter%2520than%2529%2520temperature%2520sampling%2520from%2520the%2520output%2520layer.%2520Thus%252C%2520our%2520work%2520offers%250Aa%2520principled%252C%2520minimal%2520test-bed%2520for%2520analyzing%2520open-ended%2520creative%2520skills%252C%2520and%250Aoffers%2520new%2520arguments%2520for%2520going%2520beyond%2520next-token%2520learning%2520and%2520temperature%250Asampling.%2520We%2520make%2520part%2520of%2520the%2520code%2520available%2520under%250Ahttps%253A//github.com/chenwu98/algorithmic-creativity%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15266v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Roll%20the%20dice%20%26%20look%20before%20you%20leap%3A%20Going%20beyond%20the%20creative%20limits%0A%20%20of%20next-token%20prediction&entry.906535625=Vaishnavh%20Nagarajan%20and%20Chen%20Henry%20Wu%20and%20Charles%20Ding%20and%20Aditi%20Raghunathan&entry.1292438233=%20%20We%20design%20a%20suite%20of%20minimal%20algorithmic%20tasks%20that%20are%20a%20loose%20abstraction%0Aof%20open-ended%20real-world%20tasks.%20This%20allows%20us%20to%20cleanly%20and%20controllably%0Aquantify%20the%20creative%20limits%20of%20the%20present-day%20language%20model.%20Much%20like%0Areal-world%20tasks%20that%20require%20a%20creative%2C%20far-sighted%20leap%20of%20thought%2C%20our%0Atasks%20require%20an%20implicit%2C%20open-ended%20stochastic%20planning%20step%20that%20either%20%28a%29%0Adiscovers%20new%20connections%20in%20an%20abstract%20knowledge%20graph%20%28like%20in%20wordplay%2C%0Adrawing%20analogies%2C%20or%20research%29%20or%20%28b%29%20constructs%20new%20patterns%20%28like%20in%0Adesigning%20math%20problems%20or%20new%20proteins%29.%20In%20these%20tasks%2C%20we%20empirically%20and%0Aconceptually%20argue%20how%20next-token%20learning%20is%20myopic%3B%20multi-token%20approaches%2C%0Anamely%20teacherless%20training%20and%20diffusion%20models%2C%20comparatively%20excel%20in%0Aproducing%20diverse%20and%20original%20output.%20Secondly%2C%20to%20elicit%20randomness%20without%0Ahurting%20coherence%2C%20we%20find%20that%20injecting%20noise%20at%20the%20input%20layer%20%28dubbed%0Aseed-conditioning%29%20works%20surprisingly%20as%20well%20as%20%28and%20in%20some%20conditions%2C%0Abetter%20than%29%20temperature%20sampling%20from%20the%20output%20layer.%20Thus%2C%20our%20work%20offers%0Aa%20principled%2C%20minimal%20test-bed%20for%20analyzing%20open-ended%20creative%20skills%2C%20and%0Aoffers%20new%20arguments%20for%20going%20beyond%20next-token%20learning%20and%20temperature%0Asampling.%20We%20make%20part%20of%20the%20code%20available%20under%0Ahttps%3A//github.com/chenwu98/algorithmic-creativity%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15266v3&entry.124074799=Read"},
{"title": "Mechanistic Indicators of Understanding in Large Language Models", "author": "Pierre Beckmann and Matthieu Queloz", "abstract": "  Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.\n", "link": "http://arxiv.org/abs/2507.08017v2", "date": "2025-07-14", "relevancy": 2.1477, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanistic%20Indicators%20of%20Understanding%20in%20Large%20Language%20Models&body=Title%3A%20Mechanistic%20Indicators%20of%20Understanding%20in%20Large%20Language%20Models%0AAuthor%3A%20Pierre%20Beckmann%20and%20Matthieu%20Queloz%0AAbstract%3A%20%20%20Recent%20findings%20in%20mechanistic%20interpretability%20%28MI%29%2C%20the%20field%20probing%20the%0Ainner%20workings%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20challenge%20the%20view%20that%20these%0Amodels%20rely%20solely%20on%20superficial%20statistics.%20We%20offer%20an%20accessible%20synthesis%0Aof%20these%20findings%20that%20doubles%20as%20an%20introduction%20to%20MI%20while%20integrating%20these%0Afindings%20within%20a%20novel%20theoretical%20framework%20for%20thinking%20about%20machine%0Aunderstanding.%20We%20argue%20that%20LLMs%20develop%20internal%20structures%20that%20are%0Afunctionally%20analogous%20to%20the%20kind%20of%20understanding%20that%20consists%20in%20seeing%0Aconnections.%20To%20sharpen%20this%20idea%2C%20we%20propose%20a%20three-tiered%20conception%20of%0Aunderstanding.%20First%2C%20conceptual%20understanding%20emerges%20when%20a%20model%20forms%0A%22features%22%20as%20directions%20in%20latent%20space%2C%20learning%20the%20connections%20between%0Adiverse%20manifestations%20of%20something.%20Second%2C%20state-of-the-world%20understanding%0Aemerges%20when%20a%20model%20learns%20contingent%20factual%20connections%20between%20features%20and%0Adynamically%20tracks%20changes%20in%20the%20world.%20Third%2C%20principled%20understanding%0Aemerges%20when%20a%20model%20ceases%20to%20rely%20on%20a%20collection%20of%20memorized%20facts%20and%0Adiscovers%20a%20%22circuit%22%20connecting%20these%20facts.%20However%2C%20these%20forms%20of%0Aunderstanding%20remain%20radically%20different%20from%20human%20understanding%2C%20as%20the%0Aphenomenon%20of%20%22parallel%20mechanisms%22%20shows.%20We%20conclude%20that%20the%20debate%20should%0Amove%20beyond%20the%20yes-or-no%20question%20of%20whether%20LLMs%20understand%20to%20investigate%0Ahow%20their%20strange%20minds%20work%20and%20forge%20conceptions%20that%20fit%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08017v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanistic%2520Indicators%2520of%2520Understanding%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DPierre%2520Beckmann%2520and%2520Matthieu%2520Queloz%26entry.1292438233%3D%2520%2520Recent%2520findings%2520in%2520mechanistic%2520interpretability%2520%2528MI%2529%252C%2520the%2520field%2520probing%2520the%250Ainner%2520workings%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520challenge%2520the%2520view%2520that%2520these%250Amodels%2520rely%2520solely%2520on%2520superficial%2520statistics.%2520We%2520offer%2520an%2520accessible%2520synthesis%250Aof%2520these%2520findings%2520that%2520doubles%2520as%2520an%2520introduction%2520to%2520MI%2520while%2520integrating%2520these%250Afindings%2520within%2520a%2520novel%2520theoretical%2520framework%2520for%2520thinking%2520about%2520machine%250Aunderstanding.%2520We%2520argue%2520that%2520LLMs%2520develop%2520internal%2520structures%2520that%2520are%250Afunctionally%2520analogous%2520to%2520the%2520kind%2520of%2520understanding%2520that%2520consists%2520in%2520seeing%250Aconnections.%2520To%2520sharpen%2520this%2520idea%252C%2520we%2520propose%2520a%2520three-tiered%2520conception%2520of%250Aunderstanding.%2520First%252C%2520conceptual%2520understanding%2520emerges%2520when%2520a%2520model%2520forms%250A%2522features%2522%2520as%2520directions%2520in%2520latent%2520space%252C%2520learning%2520the%2520connections%2520between%250Adiverse%2520manifestations%2520of%2520something.%2520Second%252C%2520state-of-the-world%2520understanding%250Aemerges%2520when%2520a%2520model%2520learns%2520contingent%2520factual%2520connections%2520between%2520features%2520and%250Adynamically%2520tracks%2520changes%2520in%2520the%2520world.%2520Third%252C%2520principled%2520understanding%250Aemerges%2520when%2520a%2520model%2520ceases%2520to%2520rely%2520on%2520a%2520collection%2520of%2520memorized%2520facts%2520and%250Adiscovers%2520a%2520%2522circuit%2522%2520connecting%2520these%2520facts.%2520However%252C%2520these%2520forms%2520of%250Aunderstanding%2520remain%2520radically%2520different%2520from%2520human%2520understanding%252C%2520as%2520the%250Aphenomenon%2520of%2520%2522parallel%2520mechanisms%2522%2520shows.%2520We%2520conclude%2520that%2520the%2520debate%2520should%250Amove%2520beyond%2520the%2520yes-or-no%2520question%2520of%2520whether%2520LLMs%2520understand%2520to%2520investigate%250Ahow%2520their%2520strange%2520minds%2520work%2520and%2520forge%2520conceptions%2520that%2520fit%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08017v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistic%20Indicators%20of%20Understanding%20in%20Large%20Language%20Models&entry.906535625=Pierre%20Beckmann%20and%20Matthieu%20Queloz&entry.1292438233=%20%20Recent%20findings%20in%20mechanistic%20interpretability%20%28MI%29%2C%20the%20field%20probing%20the%0Ainner%20workings%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20challenge%20the%20view%20that%20these%0Amodels%20rely%20solely%20on%20superficial%20statistics.%20We%20offer%20an%20accessible%20synthesis%0Aof%20these%20findings%20that%20doubles%20as%20an%20introduction%20to%20MI%20while%20integrating%20these%0Afindings%20within%20a%20novel%20theoretical%20framework%20for%20thinking%20about%20machine%0Aunderstanding.%20We%20argue%20that%20LLMs%20develop%20internal%20structures%20that%20are%0Afunctionally%20analogous%20to%20the%20kind%20of%20understanding%20that%20consists%20in%20seeing%0Aconnections.%20To%20sharpen%20this%20idea%2C%20we%20propose%20a%20three-tiered%20conception%20of%0Aunderstanding.%20First%2C%20conceptual%20understanding%20emerges%20when%20a%20model%20forms%0A%22features%22%20as%20directions%20in%20latent%20space%2C%20learning%20the%20connections%20between%0Adiverse%20manifestations%20of%20something.%20Second%2C%20state-of-the-world%20understanding%0Aemerges%20when%20a%20model%20learns%20contingent%20factual%20connections%20between%20features%20and%0Adynamically%20tracks%20changes%20in%20the%20world.%20Third%2C%20principled%20understanding%0Aemerges%20when%20a%20model%20ceases%20to%20rely%20on%20a%20collection%20of%20memorized%20facts%20and%0Adiscovers%20a%20%22circuit%22%20connecting%20these%20facts.%20However%2C%20these%20forms%20of%0Aunderstanding%20remain%20radically%20different%20from%20human%20understanding%2C%20as%20the%0Aphenomenon%20of%20%22parallel%20mechanisms%22%20shows.%20We%20conclude%20that%20the%20debate%20should%0Amove%20beyond%20the%20yes-or-no%20question%20of%20whether%20LLMs%20understand%20to%20investigate%0Ahow%20their%20strange%20minds%20work%20and%20forge%20conceptions%20that%20fit%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08017v2&entry.124074799=Read"},
{"title": "WASABI: A Metric for Evaluating Morphometric Plausibility of Synthetic\n  Brain MRIs", "author": "Bahram Jafrasteh and Wei Peng and Cheng Wan and Yimin Luo and Ehsan Adeli and Qingyu Zhao", "abstract": "  Generative models enhance neuroimaging through data augmentation, quality\nimprovement, and rare condition studies. Despite advances in realistic\nsynthetic MRIs, evaluations focus on texture and perception, lacking\nsensitivity to crucial anatomical fidelity. This study proposes a new metric,\ncalled WASABI (Wasserstein-Based Anatomical Brain Index), to assess the\nanatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg},\na deep learning-based brain parcellation tool, to derive volumetric measures of\nbrain regions in each MRI and uses the multivariate Wasserstein distance to\ncompare distributions between real and synthetic anatomies. Based on controlled\nexperiments on two real datasets and synthetic MRIs from five generative\nmodels, WASABI demonstrates higher sensitivity in quantifying anatomical\ndiscrepancies compared to traditional image-level metrics, even when synthetic\nimages achieve near-perfect visual quality. Our findings advocate for shifting\nthe evaluation paradigm beyond visual inspection and conventional metrics,\nemphasizing anatomical fidelity as a crucial benchmark for clinically\nmeaningful brain MRI synthesis. Our code is available at\nhttps://github.com/BahramJafrasteh/wasabi-mri.\n", "link": "http://arxiv.org/abs/2504.21771v3", "date": "2025-07-14", "relevancy": 2.1445, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WASABI%3A%20A%20Metric%20for%20Evaluating%20Morphometric%20Plausibility%20of%20Synthetic%0A%20%20Brain%20MRIs&body=Title%3A%20WASABI%3A%20A%20Metric%20for%20Evaluating%20Morphometric%20Plausibility%20of%20Synthetic%0A%20%20Brain%20MRIs%0AAuthor%3A%20Bahram%20Jafrasteh%20and%20Wei%20Peng%20and%20Cheng%20Wan%20and%20Yimin%20Luo%20and%20Ehsan%20Adeli%20and%20Qingyu%20Zhao%0AAbstract%3A%20%20%20Generative%20models%20enhance%20neuroimaging%20through%20data%20augmentation%2C%20quality%0Aimprovement%2C%20and%20rare%20condition%20studies.%20Despite%20advances%20in%20realistic%0Asynthetic%20MRIs%2C%20evaluations%20focus%20on%20texture%20and%20perception%2C%20lacking%0Asensitivity%20to%20crucial%20anatomical%20fidelity.%20This%20study%20proposes%20a%20new%20metric%2C%0Acalled%20WASABI%20%28Wasserstein-Based%20Anatomical%20Brain%20Index%29%2C%20to%20assess%20the%0Aanatomical%20realism%20of%20synthetic%20brain%20MRIs.%20WASABI%20leverages%20%5Ctextit%7BSynthSeg%7D%2C%0Aa%20deep%20learning-based%20brain%20parcellation%20tool%2C%20to%20derive%20volumetric%20measures%20of%0Abrain%20regions%20in%20each%20MRI%20and%20uses%20the%20multivariate%20Wasserstein%20distance%20to%0Acompare%20distributions%20between%20real%20and%20synthetic%20anatomies.%20Based%20on%20controlled%0Aexperiments%20on%20two%20real%20datasets%20and%20synthetic%20MRIs%20from%20five%20generative%0Amodels%2C%20WASABI%20demonstrates%20higher%20sensitivity%20in%20quantifying%20anatomical%0Adiscrepancies%20compared%20to%20traditional%20image-level%20metrics%2C%20even%20when%20synthetic%0Aimages%20achieve%20near-perfect%20visual%20quality.%20Our%20findings%20advocate%20for%20shifting%0Athe%20evaluation%20paradigm%20beyond%20visual%20inspection%20and%20conventional%20metrics%2C%0Aemphasizing%20anatomical%20fidelity%20as%20a%20crucial%20benchmark%20for%20clinically%0Ameaningful%20brain%20MRI%20synthesis.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/BahramJafrasteh/wasabi-mri.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21771v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWASABI%253A%2520A%2520Metric%2520for%2520Evaluating%2520Morphometric%2520Plausibility%2520of%2520Synthetic%250A%2520%2520Brain%2520MRIs%26entry.906535625%3DBahram%2520Jafrasteh%2520and%2520Wei%2520Peng%2520and%2520Cheng%2520Wan%2520and%2520Yimin%2520Luo%2520and%2520Ehsan%2520Adeli%2520and%2520Qingyu%2520Zhao%26entry.1292438233%3D%2520%2520Generative%2520models%2520enhance%2520neuroimaging%2520through%2520data%2520augmentation%252C%2520quality%250Aimprovement%252C%2520and%2520rare%2520condition%2520studies.%2520Despite%2520advances%2520in%2520realistic%250Asynthetic%2520MRIs%252C%2520evaluations%2520focus%2520on%2520texture%2520and%2520perception%252C%2520lacking%250Asensitivity%2520to%2520crucial%2520anatomical%2520fidelity.%2520This%2520study%2520proposes%2520a%2520new%2520metric%252C%250Acalled%2520WASABI%2520%2528Wasserstein-Based%2520Anatomical%2520Brain%2520Index%2529%252C%2520to%2520assess%2520the%250Aanatomical%2520realism%2520of%2520synthetic%2520brain%2520MRIs.%2520WASABI%2520leverages%2520%255Ctextit%257BSynthSeg%257D%252C%250Aa%2520deep%2520learning-based%2520brain%2520parcellation%2520tool%252C%2520to%2520derive%2520volumetric%2520measures%2520of%250Abrain%2520regions%2520in%2520each%2520MRI%2520and%2520uses%2520the%2520multivariate%2520Wasserstein%2520distance%2520to%250Acompare%2520distributions%2520between%2520real%2520and%2520synthetic%2520anatomies.%2520Based%2520on%2520controlled%250Aexperiments%2520on%2520two%2520real%2520datasets%2520and%2520synthetic%2520MRIs%2520from%2520five%2520generative%250Amodels%252C%2520WASABI%2520demonstrates%2520higher%2520sensitivity%2520in%2520quantifying%2520anatomical%250Adiscrepancies%2520compared%2520to%2520traditional%2520image-level%2520metrics%252C%2520even%2520when%2520synthetic%250Aimages%2520achieve%2520near-perfect%2520visual%2520quality.%2520Our%2520findings%2520advocate%2520for%2520shifting%250Athe%2520evaluation%2520paradigm%2520beyond%2520visual%2520inspection%2520and%2520conventional%2520metrics%252C%250Aemphasizing%2520anatomical%2520fidelity%2520as%2520a%2520crucial%2520benchmark%2520for%2520clinically%250Ameaningful%2520brain%2520MRI%2520synthesis.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/BahramJafrasteh/wasabi-mri.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21771v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WASABI%3A%20A%20Metric%20for%20Evaluating%20Morphometric%20Plausibility%20of%20Synthetic%0A%20%20Brain%20MRIs&entry.906535625=Bahram%20Jafrasteh%20and%20Wei%20Peng%20and%20Cheng%20Wan%20and%20Yimin%20Luo%20and%20Ehsan%20Adeli%20and%20Qingyu%20Zhao&entry.1292438233=%20%20Generative%20models%20enhance%20neuroimaging%20through%20data%20augmentation%2C%20quality%0Aimprovement%2C%20and%20rare%20condition%20studies.%20Despite%20advances%20in%20realistic%0Asynthetic%20MRIs%2C%20evaluations%20focus%20on%20texture%20and%20perception%2C%20lacking%0Asensitivity%20to%20crucial%20anatomical%20fidelity.%20This%20study%20proposes%20a%20new%20metric%2C%0Acalled%20WASABI%20%28Wasserstein-Based%20Anatomical%20Brain%20Index%29%2C%20to%20assess%20the%0Aanatomical%20realism%20of%20synthetic%20brain%20MRIs.%20WASABI%20leverages%20%5Ctextit%7BSynthSeg%7D%2C%0Aa%20deep%20learning-based%20brain%20parcellation%20tool%2C%20to%20derive%20volumetric%20measures%20of%0Abrain%20regions%20in%20each%20MRI%20and%20uses%20the%20multivariate%20Wasserstein%20distance%20to%0Acompare%20distributions%20between%20real%20and%20synthetic%20anatomies.%20Based%20on%20controlled%0Aexperiments%20on%20two%20real%20datasets%20and%20synthetic%20MRIs%20from%20five%20generative%0Amodels%2C%20WASABI%20demonstrates%20higher%20sensitivity%20in%20quantifying%20anatomical%0Adiscrepancies%20compared%20to%20traditional%20image-level%20metrics%2C%20even%20when%20synthetic%0Aimages%20achieve%20near-perfect%20visual%20quality.%20Our%20findings%20advocate%20for%20shifting%0Athe%20evaluation%20paradigm%20beyond%20visual%20inspection%20and%20conventional%20metrics%2C%0Aemphasizing%20anatomical%20fidelity%20as%20a%20crucial%20benchmark%20for%20clinically%0Ameaningful%20brain%20MRI%20synthesis.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/BahramJafrasteh/wasabi-mri.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21771v3&entry.124074799=Read"},
{"title": "EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration", "author": "Allen Nie and Yi Su and Bo Chang and Jonathan N. Lee and Ed H. Chi and Quoc V. Le and Minmin Chen", "abstract": "  Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm.\n", "link": "http://arxiv.org/abs/2410.06238v2", "date": "2025-07-14", "relevancy": 2.1433, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVOLvE%3A%20Evaluating%20and%20Optimizing%20LLMs%20For%20In-Context%20Exploration&body=Title%3A%20EVOLvE%3A%20Evaluating%20and%20Optimizing%20LLMs%20For%20In-Context%20Exploration%0AAuthor%3A%20Allen%20Nie%20and%20Yi%20Su%20and%20Bo%20Chang%20and%20Jonathan%20N.%20Lee%20and%20Ed%20H.%20Chi%20and%20Quoc%20V.%20Le%20and%20Minmin%20Chen%0AAbstract%3A%20%20%20Despite%20their%20success%20in%20many%20domains%2C%20large%20language%20models%20%28LLMs%29%20remain%0Aunder-studied%20in%20scenarios%20requiring%20optimal%20decision-making%20under%20uncertainty.%0AThis%20is%20crucial%20as%20many%20real-world%20applications%2C%20ranging%20from%20personalized%0Arecommendations%20to%20healthcare%20interventions%2C%20demand%20that%20LLMs%20not%20only%20predict%0Abut%20also%20actively%20learn%20to%20make%20optimal%20decisions%20through%20exploration.%20In%20this%0Awork%2C%20we%20measure%20LLMs%27%20%28in%29ability%20to%20make%20optimal%20decisions%20in%20bandits%2C%20a%0Astate-less%20reinforcement%20learning%20setting%20relevant%20to%20many%20applications.%20We%0Adevelop%20a%20comprehensive%20suite%20of%20environments%2C%20including%20both%20context-free%20and%0Acontextual%20bandits%20with%20varying%20task%20difficulties%2C%20to%20benchmark%20LLMs%27%0Aperformance.%20Motivated%20by%20the%20existence%20of%20optimal%20exploration%20algorithms%2C%20we%0Apropose%20efficient%20ways%20to%20integrate%20this%20algorithmic%20knowledge%20into%20LLMs%3A%20by%0Aproviding%20explicit%20algorithm-guided%20support%20during%20inference%3B%20and%20through%0Aalgorithm%20distillation%20via%20in-context%20demonstrations%20and%20fine-tuning%2C%20using%0Asynthetic%20data%20generated%20from%20these%20algorithms.%20Impressively%2C%20these%20techniques%0Aallow%20us%20to%20achieve%20superior%20exploration%20performance%20with%20smaller%20models%2C%0Asurpassing%20larger%20models%20on%20various%20tasks.%20We%20conducted%20an%20extensive%20ablation%0Astudy%20to%20shed%20light%20on%20various%20factors%2C%20such%20as%20task%20difficulty%20and%20data%0Arepresentation%2C%20that%20influence%20the%20efficiency%20of%20LLM%20exploration.%20Additionally%2C%0Awe%20conduct%20a%20rigorous%20analysis%20of%20the%20LLM%27s%20exploration%20efficiency%20using%20the%0Aconcept%20of%20regret%2C%20linking%20its%20ability%20to%20explore%20to%20the%20model%20size%20and%0Aunderlying%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVOLvE%253A%2520Evaluating%2520and%2520Optimizing%2520LLMs%2520For%2520In-Context%2520Exploration%26entry.906535625%3DAllen%2520Nie%2520and%2520Yi%2520Su%2520and%2520Bo%2520Chang%2520and%2520Jonathan%2520N.%2520Lee%2520and%2520Ed%2520H.%2520Chi%2520and%2520Quoc%2520V.%2520Le%2520and%2520Minmin%2520Chen%26entry.1292438233%3D%2520%2520Despite%2520their%2520success%2520in%2520many%2520domains%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520remain%250Aunder-studied%2520in%2520scenarios%2520requiring%2520optimal%2520decision-making%2520under%2520uncertainty.%250AThis%2520is%2520crucial%2520as%2520many%2520real-world%2520applications%252C%2520ranging%2520from%2520personalized%250Arecommendations%2520to%2520healthcare%2520interventions%252C%2520demand%2520that%2520LLMs%2520not%2520only%2520predict%250Abut%2520also%2520actively%2520learn%2520to%2520make%2520optimal%2520decisions%2520through%2520exploration.%2520In%2520this%250Awork%252C%2520we%2520measure%2520LLMs%2527%2520%2528in%2529ability%2520to%2520make%2520optimal%2520decisions%2520in%2520bandits%252C%2520a%250Astate-less%2520reinforcement%2520learning%2520setting%2520relevant%2520to%2520many%2520applications.%2520We%250Adevelop%2520a%2520comprehensive%2520suite%2520of%2520environments%252C%2520including%2520both%2520context-free%2520and%250Acontextual%2520bandits%2520with%2520varying%2520task%2520difficulties%252C%2520to%2520benchmark%2520LLMs%2527%250Aperformance.%2520Motivated%2520by%2520the%2520existence%2520of%2520optimal%2520exploration%2520algorithms%252C%2520we%250Apropose%2520efficient%2520ways%2520to%2520integrate%2520this%2520algorithmic%2520knowledge%2520into%2520LLMs%253A%2520by%250Aproviding%2520explicit%2520algorithm-guided%2520support%2520during%2520inference%253B%2520and%2520through%250Aalgorithm%2520distillation%2520via%2520in-context%2520demonstrations%2520and%2520fine-tuning%252C%2520using%250Asynthetic%2520data%2520generated%2520from%2520these%2520algorithms.%2520Impressively%252C%2520these%2520techniques%250Aallow%2520us%2520to%2520achieve%2520superior%2520exploration%2520performance%2520with%2520smaller%2520models%252C%250Asurpassing%2520larger%2520models%2520on%2520various%2520tasks.%2520We%2520conducted%2520an%2520extensive%2520ablation%250Astudy%2520to%2520shed%2520light%2520on%2520various%2520factors%252C%2520such%2520as%2520task%2520difficulty%2520and%2520data%250Arepresentation%252C%2520that%2520influence%2520the%2520efficiency%2520of%2520LLM%2520exploration.%2520Additionally%252C%250Awe%2520conduct%2520a%2520rigorous%2520analysis%2520of%2520the%2520LLM%2527s%2520exploration%2520efficiency%2520using%2520the%250Aconcept%2520of%2520regret%252C%2520linking%2520its%2520ability%2520to%2520explore%2520to%2520the%2520model%2520size%2520and%250Aunderlying%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVOLvE%3A%20Evaluating%20and%20Optimizing%20LLMs%20For%20In-Context%20Exploration&entry.906535625=Allen%20Nie%20and%20Yi%20Su%20and%20Bo%20Chang%20and%20Jonathan%20N.%20Lee%20and%20Ed%20H.%20Chi%20and%20Quoc%20V.%20Le%20and%20Minmin%20Chen&entry.1292438233=%20%20Despite%20their%20success%20in%20many%20domains%2C%20large%20language%20models%20%28LLMs%29%20remain%0Aunder-studied%20in%20scenarios%20requiring%20optimal%20decision-making%20under%20uncertainty.%0AThis%20is%20crucial%20as%20many%20real-world%20applications%2C%20ranging%20from%20personalized%0Arecommendations%20to%20healthcare%20interventions%2C%20demand%20that%20LLMs%20not%20only%20predict%0Abut%20also%20actively%20learn%20to%20make%20optimal%20decisions%20through%20exploration.%20In%20this%0Awork%2C%20we%20measure%20LLMs%27%20%28in%29ability%20to%20make%20optimal%20decisions%20in%20bandits%2C%20a%0Astate-less%20reinforcement%20learning%20setting%20relevant%20to%20many%20applications.%20We%0Adevelop%20a%20comprehensive%20suite%20of%20environments%2C%20including%20both%20context-free%20and%0Acontextual%20bandits%20with%20varying%20task%20difficulties%2C%20to%20benchmark%20LLMs%27%0Aperformance.%20Motivated%20by%20the%20existence%20of%20optimal%20exploration%20algorithms%2C%20we%0Apropose%20efficient%20ways%20to%20integrate%20this%20algorithmic%20knowledge%20into%20LLMs%3A%20by%0Aproviding%20explicit%20algorithm-guided%20support%20during%20inference%3B%20and%20through%0Aalgorithm%20distillation%20via%20in-context%20demonstrations%20and%20fine-tuning%2C%20using%0Asynthetic%20data%20generated%20from%20these%20algorithms.%20Impressively%2C%20these%20techniques%0Aallow%20us%20to%20achieve%20superior%20exploration%20performance%20with%20smaller%20models%2C%0Asurpassing%20larger%20models%20on%20various%20tasks.%20We%20conducted%20an%20extensive%20ablation%0Astudy%20to%20shed%20light%20on%20various%20factors%2C%20such%20as%20task%20difficulty%20and%20data%0Arepresentation%2C%20that%20influence%20the%20efficiency%20of%20LLM%20exploration.%20Additionally%2C%0Awe%20conduct%20a%20rigorous%20analysis%20of%20the%20LLM%27s%20exploration%20efficiency%20using%20the%0Aconcept%20of%20regret%2C%20linking%20its%20ability%20to%20explore%20to%20the%20model%20size%20and%0Aunderlying%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06238v2&entry.124074799=Read"},
{"title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for\n  Pansharpening", "author": "Tao Tang and Chengxu Yang", "abstract": "  Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.\n", "link": "http://arxiv.org/abs/2507.10461v1", "date": "2025-07-14", "relevancy": 2.1413, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5849}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5066}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAPNet%3A%20A%20Receptive-Field%20Adaptive%20Convolutional%20Neural%20Network%20for%0A%20%20Pansharpening&body=Title%3A%20RAPNet%3A%20A%20Receptive-Field%20Adaptive%20Convolutional%20Neural%20Network%20for%0A%20%20Pansharpening%0AAuthor%3A%20Tao%20Tang%20and%20Chengxu%20Yang%0AAbstract%3A%20%20%20Pansharpening%20refers%20to%20the%20process%20of%20integrating%20a%20high%20resolution%0Apanchromatic%20%28PAN%29%20image%20with%20a%20lower%20resolution%20multispectral%20%28MS%29%20image%20to%0Agenerate%20a%20fused%20product%2C%20which%20is%20pivotal%20in%20remote%20sensing.%20Despite%20the%0Aeffectiveness%20of%20CNNs%20in%20addressing%20this%20challenge%2C%20they%20are%20inherently%0Aconstrained%20by%20the%20uniform%20application%20of%20convolutional%20kernels%20across%20all%0Aspatial%20positions%2C%20overlooking%20local%20content%20variations.%20To%20overcome%20this%0Aissue%2C%20we%20introduce%20RAPNet%2C%20a%20new%20architecture%20that%20leverages%20content-adaptive%0Aconvolution.%20At%20its%20core%2C%20RAPNet%20employs%20the%20Receptive-field%20Adaptive%0APansharpening%20Convolution%20%28RAPConv%29%2C%20designed%20to%20produce%20spatially%20adaptive%0Akernels%20responsive%20to%20local%20feature%20context%2C%20thereby%20enhancing%20the%20precision%20of%0Aspatial%20detail%20extraction.%20Additionally%2C%20the%20network%20integrates%20the%0APansharpening%20Dynamic%20Feature%20Fusion%20%28PAN-DFF%29%20module%2C%20which%20incorporates%20an%0Aattention%20mechanism%20to%20achieve%20an%20optimal%20balance%20between%20spatial%20detail%0Aenhancement%20and%20spectral%20fidelity.%20Comprehensive%20evaluations%20on%20publicly%0Aavailable%20datasets%20confirm%20that%20RAPNet%20delivers%20superior%20performance%20compared%0Ato%20existing%20approaches%2C%20as%20demonstrated%20by%20both%20quantitative%20metrics%20and%0Aqualitative%20assessments.%20Ablation%20analyses%20further%20substantiate%20the%0Aeffectiveness%20of%20the%20proposed%20adaptive%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAPNet%253A%2520A%2520Receptive-Field%2520Adaptive%2520Convolutional%2520Neural%2520Network%2520for%250A%2520%2520Pansharpening%26entry.906535625%3DTao%2520Tang%2520and%2520Chengxu%2520Yang%26entry.1292438233%3D%2520%2520Pansharpening%2520refers%2520to%2520the%2520process%2520of%2520integrating%2520a%2520high%2520resolution%250Apanchromatic%2520%2528PAN%2529%2520image%2520with%2520a%2520lower%2520resolution%2520multispectral%2520%2528MS%2529%2520image%2520to%250Agenerate%2520a%2520fused%2520product%252C%2520which%2520is%2520pivotal%2520in%2520remote%2520sensing.%2520Despite%2520the%250Aeffectiveness%2520of%2520CNNs%2520in%2520addressing%2520this%2520challenge%252C%2520they%2520are%2520inherently%250Aconstrained%2520by%2520the%2520uniform%2520application%2520of%2520convolutional%2520kernels%2520across%2520all%250Aspatial%2520positions%252C%2520overlooking%2520local%2520content%2520variations.%2520To%2520overcome%2520this%250Aissue%252C%2520we%2520introduce%2520RAPNet%252C%2520a%2520new%2520architecture%2520that%2520leverages%2520content-adaptive%250Aconvolution.%2520At%2520its%2520core%252C%2520RAPNet%2520employs%2520the%2520Receptive-field%2520Adaptive%250APansharpening%2520Convolution%2520%2528RAPConv%2529%252C%2520designed%2520to%2520produce%2520spatially%2520adaptive%250Akernels%2520responsive%2520to%2520local%2520feature%2520context%252C%2520thereby%2520enhancing%2520the%2520precision%2520of%250Aspatial%2520detail%2520extraction.%2520Additionally%252C%2520the%2520network%2520integrates%2520the%250APansharpening%2520Dynamic%2520Feature%2520Fusion%2520%2528PAN-DFF%2529%2520module%252C%2520which%2520incorporates%2520an%250Aattention%2520mechanism%2520to%2520achieve%2520an%2520optimal%2520balance%2520between%2520spatial%2520detail%250Aenhancement%2520and%2520spectral%2520fidelity.%2520Comprehensive%2520evaluations%2520on%2520publicly%250Aavailable%2520datasets%2520confirm%2520that%2520RAPNet%2520delivers%2520superior%2520performance%2520compared%250Ato%2520existing%2520approaches%252C%2520as%2520demonstrated%2520by%2520both%2520quantitative%2520metrics%2520and%250Aqualitative%2520assessments.%2520Ablation%2520analyses%2520further%2520substantiate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520adaptive%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAPNet%3A%20A%20Receptive-Field%20Adaptive%20Convolutional%20Neural%20Network%20for%0A%20%20Pansharpening&entry.906535625=Tao%20Tang%20and%20Chengxu%20Yang&entry.1292438233=%20%20Pansharpening%20refers%20to%20the%20process%20of%20integrating%20a%20high%20resolution%0Apanchromatic%20%28PAN%29%20image%20with%20a%20lower%20resolution%20multispectral%20%28MS%29%20image%20to%0Agenerate%20a%20fused%20product%2C%20which%20is%20pivotal%20in%20remote%20sensing.%20Despite%20the%0Aeffectiveness%20of%20CNNs%20in%20addressing%20this%20challenge%2C%20they%20are%20inherently%0Aconstrained%20by%20the%20uniform%20application%20of%20convolutional%20kernels%20across%20all%0Aspatial%20positions%2C%20overlooking%20local%20content%20variations.%20To%20overcome%20this%0Aissue%2C%20we%20introduce%20RAPNet%2C%20a%20new%20architecture%20that%20leverages%20content-adaptive%0Aconvolution.%20At%20its%20core%2C%20RAPNet%20employs%20the%20Receptive-field%20Adaptive%0APansharpening%20Convolution%20%28RAPConv%29%2C%20designed%20to%20produce%20spatially%20adaptive%0Akernels%20responsive%20to%20local%20feature%20context%2C%20thereby%20enhancing%20the%20precision%20of%0Aspatial%20detail%20extraction.%20Additionally%2C%20the%20network%20integrates%20the%0APansharpening%20Dynamic%20Feature%20Fusion%20%28PAN-DFF%29%20module%2C%20which%20incorporates%20an%0Aattention%20mechanism%20to%20achieve%20an%20optimal%20balance%20between%20spatial%20detail%0Aenhancement%20and%20spectral%20fidelity.%20Comprehensive%20evaluations%20on%20publicly%0Aavailable%20datasets%20confirm%20that%20RAPNet%20delivers%20superior%20performance%20compared%0Ato%20existing%20approaches%2C%20as%20demonstrated%20by%20both%20quantitative%20metrics%20and%0Aqualitative%20assessments.%20Ablation%20analyses%20further%20substantiate%20the%0Aeffectiveness%20of%20the%20proposed%20adaptive%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10461v1&entry.124074799=Read"},
{"title": "Improving Multimodal Learning via Imbalanced Learning", "author": "Shicai Wei and Chunbo Luo and Yang Luo", "abstract": "  Multimodal learning often encounters the under-optimized problem and may\nperform worse than unimodal learning. Existing approaches attribute this issue\nto imbalanced learning across modalities and tend to address it through\ngradient balancing. However, this paper argues that balanced learning is not\nthe optimal setting for multimodal learning. With bias-variance analysis, we\nprove that imbalanced dependency on each modality obeying the inverse ratio of\ntheir variances contributes to optimal performance. To this end, we propose the\nAsymmetric Representation Learning(ARL) strategy to assist multimodal learning\nvia imbalanced optimization. ARL introduces auxiliary regularizers for each\nmodality encoder to calculate their prediction variance. ARL then calculates\ncoefficients via the unimodal variance to re-weight the optimization of each\nmodality, forcing the modality dependence ratio to be inversely proportional to\nthe modality variance ratio. Moreover, to minimize the generalization error,\nARL further introduces the prediction bias of each modality and jointly\noptimizes them with multimodal loss. Notably, all auxiliary regularizers share\nparameters with the multimodal model and rely only on the modality\nrepresentation. Thus the proposed ARL strategy introduces no extra parameters\nand is independent of the structures and fusion methods of the multimodal\nmodel. Finally, extensive experiments on various datasets validate the\neffectiveness and versatility of ARL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}\n", "link": "http://arxiv.org/abs/2507.10203v1", "date": "2025-07-14", "relevancy": 2.1359, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Multimodal%20Learning%20via%20Imbalanced%20Learning&body=Title%3A%20Improving%20Multimodal%20Learning%20via%20Imbalanced%20Learning%0AAuthor%3A%20Shicai%20Wei%20and%20Chunbo%20Luo%20and%20Yang%20Luo%0AAbstract%3A%20%20%20Multimodal%20learning%20often%20encounters%20the%20under-optimized%20problem%20and%20may%0Aperform%20worse%20than%20unimodal%20learning.%20Existing%20approaches%20attribute%20this%20issue%0Ato%20imbalanced%20learning%20across%20modalities%20and%20tend%20to%20address%20it%20through%0Agradient%20balancing.%20However%2C%20this%20paper%20argues%20that%20balanced%20learning%20is%20not%0Athe%20optimal%20setting%20for%20multimodal%20learning.%20With%20bias-variance%20analysis%2C%20we%0Aprove%20that%20imbalanced%20dependency%20on%20each%20modality%20obeying%20the%20inverse%20ratio%20of%0Atheir%20variances%20contributes%20to%20optimal%20performance.%20To%20this%20end%2C%20we%20propose%20the%0AAsymmetric%20Representation%20Learning%28ARL%29%20strategy%20to%20assist%20multimodal%20learning%0Avia%20imbalanced%20optimization.%20ARL%20introduces%20auxiliary%20regularizers%20for%20each%0Amodality%20encoder%20to%20calculate%20their%20prediction%20variance.%20ARL%20then%20calculates%0Acoefficients%20via%20the%20unimodal%20variance%20to%20re-weight%20the%20optimization%20of%20each%0Amodality%2C%20forcing%20the%20modality%20dependence%20ratio%20to%20be%20inversely%20proportional%20to%0Athe%20modality%20variance%20ratio.%20Moreover%2C%20to%20minimize%20the%20generalization%20error%2C%0AARL%20further%20introduces%20the%20prediction%20bias%20of%20each%20modality%20and%20jointly%0Aoptimizes%20them%20with%20multimodal%20loss.%20Notably%2C%20all%20auxiliary%20regularizers%20share%0Aparameters%20with%20the%20multimodal%20model%20and%20rely%20only%20on%20the%20modality%0Arepresentation.%20Thus%20the%20proposed%20ARL%20strategy%20introduces%20no%20extra%20parameters%0Aand%20is%20independent%20of%20the%20structures%20and%20fusion%20methods%20of%20the%20multimodal%0Amodel.%20Finally%2C%20extensive%20experiments%20on%20various%20datasets%20validate%20the%0Aeffectiveness%20and%20versatility%20of%20ARL.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/shicaiwei123/ICCV2025-ARL%7D%7Bhttps%3A//github.com/shicaiwei123/ICCV2025-ARL%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Multimodal%2520Learning%2520via%2520Imbalanced%2520Learning%26entry.906535625%3DShicai%2520Wei%2520and%2520Chunbo%2520Luo%2520and%2520Yang%2520Luo%26entry.1292438233%3D%2520%2520Multimodal%2520learning%2520often%2520encounters%2520the%2520under-optimized%2520problem%2520and%2520may%250Aperform%2520worse%2520than%2520unimodal%2520learning.%2520Existing%2520approaches%2520attribute%2520this%2520issue%250Ato%2520imbalanced%2520learning%2520across%2520modalities%2520and%2520tend%2520to%2520address%2520it%2520through%250Agradient%2520balancing.%2520However%252C%2520this%2520paper%2520argues%2520that%2520balanced%2520learning%2520is%2520not%250Athe%2520optimal%2520setting%2520for%2520multimodal%2520learning.%2520With%2520bias-variance%2520analysis%252C%2520we%250Aprove%2520that%2520imbalanced%2520dependency%2520on%2520each%2520modality%2520obeying%2520the%2520inverse%2520ratio%2520of%250Atheir%2520variances%2520contributes%2520to%2520optimal%2520performance.%2520To%2520this%2520end%252C%2520we%2520propose%2520the%250AAsymmetric%2520Representation%2520Learning%2528ARL%2529%2520strategy%2520to%2520assist%2520multimodal%2520learning%250Avia%2520imbalanced%2520optimization.%2520ARL%2520introduces%2520auxiliary%2520regularizers%2520for%2520each%250Amodality%2520encoder%2520to%2520calculate%2520their%2520prediction%2520variance.%2520ARL%2520then%2520calculates%250Acoefficients%2520via%2520the%2520unimodal%2520variance%2520to%2520re-weight%2520the%2520optimization%2520of%2520each%250Amodality%252C%2520forcing%2520the%2520modality%2520dependence%2520ratio%2520to%2520be%2520inversely%2520proportional%2520to%250Athe%2520modality%2520variance%2520ratio.%2520Moreover%252C%2520to%2520minimize%2520the%2520generalization%2520error%252C%250AARL%2520further%2520introduces%2520the%2520prediction%2520bias%2520of%2520each%2520modality%2520and%2520jointly%250Aoptimizes%2520them%2520with%2520multimodal%2520loss.%2520Notably%252C%2520all%2520auxiliary%2520regularizers%2520share%250Aparameters%2520with%2520the%2520multimodal%2520model%2520and%2520rely%2520only%2520on%2520the%2520modality%250Arepresentation.%2520Thus%2520the%2520proposed%2520ARL%2520strategy%2520introduces%2520no%2520extra%2520parameters%250Aand%2520is%2520independent%2520of%2520the%2520structures%2520and%2520fusion%2520methods%2520of%2520the%2520multimodal%250Amodel.%2520Finally%252C%2520extensive%2520experiments%2520on%2520various%2520datasets%2520validate%2520the%250Aeffectiveness%2520and%2520versatility%2520of%2520ARL.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/shicaiwei123/ICCV2025-ARL%257D%257Bhttps%253A//github.com/shicaiwei123/ICCV2025-ARL%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Multimodal%20Learning%20via%20Imbalanced%20Learning&entry.906535625=Shicai%20Wei%20and%20Chunbo%20Luo%20and%20Yang%20Luo&entry.1292438233=%20%20Multimodal%20learning%20often%20encounters%20the%20under-optimized%20problem%20and%20may%0Aperform%20worse%20than%20unimodal%20learning.%20Existing%20approaches%20attribute%20this%20issue%0Ato%20imbalanced%20learning%20across%20modalities%20and%20tend%20to%20address%20it%20through%0Agradient%20balancing.%20However%2C%20this%20paper%20argues%20that%20balanced%20learning%20is%20not%0Athe%20optimal%20setting%20for%20multimodal%20learning.%20With%20bias-variance%20analysis%2C%20we%0Aprove%20that%20imbalanced%20dependency%20on%20each%20modality%20obeying%20the%20inverse%20ratio%20of%0Atheir%20variances%20contributes%20to%20optimal%20performance.%20To%20this%20end%2C%20we%20propose%20the%0AAsymmetric%20Representation%20Learning%28ARL%29%20strategy%20to%20assist%20multimodal%20learning%0Avia%20imbalanced%20optimization.%20ARL%20introduces%20auxiliary%20regularizers%20for%20each%0Amodality%20encoder%20to%20calculate%20their%20prediction%20variance.%20ARL%20then%20calculates%0Acoefficients%20via%20the%20unimodal%20variance%20to%20re-weight%20the%20optimization%20of%20each%0Amodality%2C%20forcing%20the%20modality%20dependence%20ratio%20to%20be%20inversely%20proportional%20to%0Athe%20modality%20variance%20ratio.%20Moreover%2C%20to%20minimize%20the%20generalization%20error%2C%0AARL%20further%20introduces%20the%20prediction%20bias%20of%20each%20modality%20and%20jointly%0Aoptimizes%20them%20with%20multimodal%20loss.%20Notably%2C%20all%20auxiliary%20regularizers%20share%0Aparameters%20with%20the%20multimodal%20model%20and%20rely%20only%20on%20the%20modality%0Arepresentation.%20Thus%20the%20proposed%20ARL%20strategy%20introduces%20no%20extra%20parameters%0Aand%20is%20independent%20of%20the%20structures%20and%20fusion%20methods%20of%20the%20multimodal%0Amodel.%20Finally%2C%20extensive%20experiments%20on%20various%20datasets%20validate%20the%0Aeffectiveness%20and%20versatility%20of%20ARL.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/shicaiwei123/ICCV2025-ARL%7D%7Bhttps%3A//github.com/shicaiwei123/ICCV2025-ARL%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10203v1&entry.124074799=Read"},
{"title": "Polygonal Obstacle Avoidance Combining Model Predictive Control and\n  Fuzzy Logic", "author": "Michael Schr\u00f6der and Eric Sch\u00f6neberg and Daniel G\u00f6rges and Hans D. Schotten", "abstract": "  In practice, navigation of mobile robots in confined environments is often\ndone using a spatially discrete cost-map to represent obstacles. Path following\nis a typical use case for model predictive control (MPC), but formulating\nconstraints for obstacle avoidance is challenging in this case. Typically the\ncost and constraints of an MPC problem are defined as closed-form functions and\ntypical solvers work best with continuously differentiable functions. This is\ncontrary to spatially discrete occupancy grid maps, in which a grid's value\ndefines the cost associated with occupancy. This paper presents a way to\novercome this compatibility issue by re-formulating occupancy grid maps to\ncontinuously differentiable functions to be embedded into the MPC scheme as\nconstraints. Each obstacle is defined as a polygon -- an intersection of\nhalf-spaces. Any half-space is a linear inequality representing one edge of a\npolygon. Using AND and OR operators, the combined set of all obstacles and\ntherefore the obstacle avoidance constraints can be described. The key\ncontribution of this paper is the use of fuzzy logic to re-formulate such\nconstraints that include logical operators as inequality constraints which are\ncompatible with standard MPC formulation. The resulting MPC-based trajectory\nplanner is successfully tested in simulation. This concept is also applicable\noutside of navigation tasks to implement logical or verbal constraints in MPC.\n", "link": "http://arxiv.org/abs/2507.10310v1", "date": "2025-07-14", "relevancy": 2.1312, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5635}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5396}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polygonal%20Obstacle%20Avoidance%20Combining%20Model%20Predictive%20Control%20and%0A%20%20Fuzzy%20Logic&body=Title%3A%20Polygonal%20Obstacle%20Avoidance%20Combining%20Model%20Predictive%20Control%20and%0A%20%20Fuzzy%20Logic%0AAuthor%3A%20Michael%20Schr%C3%B6der%20and%20Eric%20Sch%C3%B6neberg%20and%20Daniel%20G%C3%B6rges%20and%20Hans%20D.%20Schotten%0AAbstract%3A%20%20%20In%20practice%2C%20navigation%20of%20mobile%20robots%20in%20confined%20environments%20is%20often%0Adone%20using%20a%20spatially%20discrete%20cost-map%20to%20represent%20obstacles.%20Path%20following%0Ais%20a%20typical%20use%20case%20for%20model%20predictive%20control%20%28MPC%29%2C%20but%20formulating%0Aconstraints%20for%20obstacle%20avoidance%20is%20challenging%20in%20this%20case.%20Typically%20the%0Acost%20and%20constraints%20of%20an%20MPC%20problem%20are%20defined%20as%20closed-form%20functions%20and%0Atypical%20solvers%20work%20best%20with%20continuously%20differentiable%20functions.%20This%20is%0Acontrary%20to%20spatially%20discrete%20occupancy%20grid%20maps%2C%20in%20which%20a%20grid%27s%20value%0Adefines%20the%20cost%20associated%20with%20occupancy.%20This%20paper%20presents%20a%20way%20to%0Aovercome%20this%20compatibility%20issue%20by%20re-formulating%20occupancy%20grid%20maps%20to%0Acontinuously%20differentiable%20functions%20to%20be%20embedded%20into%20the%20MPC%20scheme%20as%0Aconstraints.%20Each%20obstacle%20is%20defined%20as%20a%20polygon%20--%20an%20intersection%20of%0Ahalf-spaces.%20Any%20half-space%20is%20a%20linear%20inequality%20representing%20one%20edge%20of%20a%0Apolygon.%20Using%20AND%20and%20OR%20operators%2C%20the%20combined%20set%20of%20all%20obstacles%20and%0Atherefore%20the%20obstacle%20avoidance%20constraints%20can%20be%20described.%20The%20key%0Acontribution%20of%20this%20paper%20is%20the%20use%20of%20fuzzy%20logic%20to%20re-formulate%20such%0Aconstraints%20that%20include%20logical%20operators%20as%20inequality%20constraints%20which%20are%0Acompatible%20with%20standard%20MPC%20formulation.%20The%20resulting%20MPC-based%20trajectory%0Aplanner%20is%20successfully%20tested%20in%20simulation.%20This%20concept%20is%20also%20applicable%0Aoutside%20of%20navigation%20tasks%20to%20implement%20logical%20or%20verbal%20constraints%20in%20MPC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolygonal%2520Obstacle%2520Avoidance%2520Combining%2520Model%2520Predictive%2520Control%2520and%250A%2520%2520Fuzzy%2520Logic%26entry.906535625%3DMichael%2520Schr%25C3%25B6der%2520and%2520Eric%2520Sch%25C3%25B6neberg%2520and%2520Daniel%2520G%25C3%25B6rges%2520and%2520Hans%2520D.%2520Schotten%26entry.1292438233%3D%2520%2520In%2520practice%252C%2520navigation%2520of%2520mobile%2520robots%2520in%2520confined%2520environments%2520is%2520often%250Adone%2520using%2520a%2520spatially%2520discrete%2520cost-map%2520to%2520represent%2520obstacles.%2520Path%2520following%250Ais%2520a%2520typical%2520use%2520case%2520for%2520model%2520predictive%2520control%2520%2528MPC%2529%252C%2520but%2520formulating%250Aconstraints%2520for%2520obstacle%2520avoidance%2520is%2520challenging%2520in%2520this%2520case.%2520Typically%2520the%250Acost%2520and%2520constraints%2520of%2520an%2520MPC%2520problem%2520are%2520defined%2520as%2520closed-form%2520functions%2520and%250Atypical%2520solvers%2520work%2520best%2520with%2520continuously%2520differentiable%2520functions.%2520This%2520is%250Acontrary%2520to%2520spatially%2520discrete%2520occupancy%2520grid%2520maps%252C%2520in%2520which%2520a%2520grid%2527s%2520value%250Adefines%2520the%2520cost%2520associated%2520with%2520occupancy.%2520This%2520paper%2520presents%2520a%2520way%2520to%250Aovercome%2520this%2520compatibility%2520issue%2520by%2520re-formulating%2520occupancy%2520grid%2520maps%2520to%250Acontinuously%2520differentiable%2520functions%2520to%2520be%2520embedded%2520into%2520the%2520MPC%2520scheme%2520as%250Aconstraints.%2520Each%2520obstacle%2520is%2520defined%2520as%2520a%2520polygon%2520--%2520an%2520intersection%2520of%250Ahalf-spaces.%2520Any%2520half-space%2520is%2520a%2520linear%2520inequality%2520representing%2520one%2520edge%2520of%2520a%250Apolygon.%2520Using%2520AND%2520and%2520OR%2520operators%252C%2520the%2520combined%2520set%2520of%2520all%2520obstacles%2520and%250Atherefore%2520the%2520obstacle%2520avoidance%2520constraints%2520can%2520be%2520described.%2520The%2520key%250Acontribution%2520of%2520this%2520paper%2520is%2520the%2520use%2520of%2520fuzzy%2520logic%2520to%2520re-formulate%2520such%250Aconstraints%2520that%2520include%2520logical%2520operators%2520as%2520inequality%2520constraints%2520which%2520are%250Acompatible%2520with%2520standard%2520MPC%2520formulation.%2520The%2520resulting%2520MPC-based%2520trajectory%250Aplanner%2520is%2520successfully%2520tested%2520in%2520simulation.%2520This%2520concept%2520is%2520also%2520applicable%250Aoutside%2520of%2520navigation%2520tasks%2520to%2520implement%2520logical%2520or%2520verbal%2520constraints%2520in%2520MPC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polygonal%20Obstacle%20Avoidance%20Combining%20Model%20Predictive%20Control%20and%0A%20%20Fuzzy%20Logic&entry.906535625=Michael%20Schr%C3%B6der%20and%20Eric%20Sch%C3%B6neberg%20and%20Daniel%20G%C3%B6rges%20and%20Hans%20D.%20Schotten&entry.1292438233=%20%20In%20practice%2C%20navigation%20of%20mobile%20robots%20in%20confined%20environments%20is%20often%0Adone%20using%20a%20spatially%20discrete%20cost-map%20to%20represent%20obstacles.%20Path%20following%0Ais%20a%20typical%20use%20case%20for%20model%20predictive%20control%20%28MPC%29%2C%20but%20formulating%0Aconstraints%20for%20obstacle%20avoidance%20is%20challenging%20in%20this%20case.%20Typically%20the%0Acost%20and%20constraints%20of%20an%20MPC%20problem%20are%20defined%20as%20closed-form%20functions%20and%0Atypical%20solvers%20work%20best%20with%20continuously%20differentiable%20functions.%20This%20is%0Acontrary%20to%20spatially%20discrete%20occupancy%20grid%20maps%2C%20in%20which%20a%20grid%27s%20value%0Adefines%20the%20cost%20associated%20with%20occupancy.%20This%20paper%20presents%20a%20way%20to%0Aovercome%20this%20compatibility%20issue%20by%20re-formulating%20occupancy%20grid%20maps%20to%0Acontinuously%20differentiable%20functions%20to%20be%20embedded%20into%20the%20MPC%20scheme%20as%0Aconstraints.%20Each%20obstacle%20is%20defined%20as%20a%20polygon%20--%20an%20intersection%20of%0Ahalf-spaces.%20Any%20half-space%20is%20a%20linear%20inequality%20representing%20one%20edge%20of%20a%0Apolygon.%20Using%20AND%20and%20OR%20operators%2C%20the%20combined%20set%20of%20all%20obstacles%20and%0Atherefore%20the%20obstacle%20avoidance%20constraints%20can%20be%20described.%20The%20key%0Acontribution%20of%20this%20paper%20is%20the%20use%20of%20fuzzy%20logic%20to%20re-formulate%20such%0Aconstraints%20that%20include%20logical%20operators%20as%20inequality%20constraints%20which%20are%0Acompatible%20with%20standard%20MPC%20formulation.%20The%20resulting%20MPC-based%20trajectory%0Aplanner%20is%20successfully%20tested%20in%20simulation.%20This%20concept%20is%20also%20applicable%0Aoutside%20of%20navigation%20tasks%20to%20implement%20logical%20or%20verbal%20constraints%20in%20MPC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10310v1&entry.124074799=Read"},
{"title": "Low Resource Reconstruction Attacks Through Benign Prompts", "author": "Sol Yarkoni and Roi Livni", "abstract": "  The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts.\n", "link": "http://arxiv.org/abs/2507.07947v2", "date": "2025-07-14", "relevancy": 2.1278, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5422}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5313}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low%20Resource%20Reconstruction%20Attacks%20Through%20Benign%20Prompts&body=Title%3A%20Low%20Resource%20Reconstruction%20Attacks%20Through%20Benign%20Prompts%0AAuthor%3A%20Sol%20Yarkoni%20and%20Roi%20Livni%0AAbstract%3A%20%20%20The%20recent%20advances%20in%20generative%20models%20such%20as%20diffusion%20models%20have%20raised%0Aseveral%20risks%20and%20concerns%20related%20to%20privacy%2C%20copyright%20infringements%20and%20data%0Astewardship.%20To%20better%20understand%20and%20control%20the%20risks%2C%20various%20researchers%0Ahave%20created%20techniques%2C%20experiments%20and%20attacks%20that%20reconstruct%20images%2C%20or%0Apart%20of%20images%2C%20from%20the%20training%20set.%20While%20these%20techniques%20already%20establish%0Athat%20data%20from%20the%20training%20set%20can%20be%20reconstructed%2C%20they%20often%20rely%20on%0Ahigh-resources%2C%20excess%20to%20the%20training%20set%20as%20well%20as%20well-engineered%20and%0Adesigned%20prompts.%0A%20%20In%20this%20work%2C%20we%20devise%20a%20new%20attack%20that%20requires%20low%20resources%2C%20assumes%0Alittle%20to%20no%20access%20to%20the%20actual%20training%20set%2C%20and%20identifies%2C%20seemingly%2C%0Abenign%20prompts%20that%20lead%20to%20potentially-risky%20image%20reconstruction.%20This%0Ahighlights%20the%20risk%20that%20images%20might%20even%20be%20reconstructed%20by%20an%20uninformed%0Auser%20and%20unintentionally.%20For%20example%2C%20we%20identified%20that%2C%20with%20regard%20to%20one%0Aexisting%20model%2C%20the%20prompt%20%60%60blue%20Unisex%20T-Shirt%27%27%20can%20generate%20the%20face%20of%20a%0Areal-life%20human%20model.%20Our%20method%20builds%20on%20an%20intuition%20from%20previous%20works%0Awhich%20leverages%20domain%20knowledge%20and%20identifies%20a%20fundamental%20vulnerability%0Athat%20stems%20from%20the%20use%20of%20scraped%20data%20from%20e-commerce%20platforms%2C%20where%0Atemplated%20layouts%20and%20images%20are%20tied%20to%20pattern-like%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07947v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow%2520Resource%2520Reconstruction%2520Attacks%2520Through%2520Benign%2520Prompts%26entry.906535625%3DSol%2520Yarkoni%2520and%2520Roi%2520Livni%26entry.1292438233%3D%2520%2520The%2520recent%2520advances%2520in%2520generative%2520models%2520such%2520as%2520diffusion%2520models%2520have%2520raised%250Aseveral%2520risks%2520and%2520concerns%2520related%2520to%2520privacy%252C%2520copyright%2520infringements%2520and%2520data%250Astewardship.%2520To%2520better%2520understand%2520and%2520control%2520the%2520risks%252C%2520various%2520researchers%250Ahave%2520created%2520techniques%252C%2520experiments%2520and%2520attacks%2520that%2520reconstruct%2520images%252C%2520or%250Apart%2520of%2520images%252C%2520from%2520the%2520training%2520set.%2520While%2520these%2520techniques%2520already%2520establish%250Athat%2520data%2520from%2520the%2520training%2520set%2520can%2520be%2520reconstructed%252C%2520they%2520often%2520rely%2520on%250Ahigh-resources%252C%2520excess%2520to%2520the%2520training%2520set%2520as%2520well%2520as%2520well-engineered%2520and%250Adesigned%2520prompts.%250A%2520%2520In%2520this%2520work%252C%2520we%2520devise%2520a%2520new%2520attack%2520that%2520requires%2520low%2520resources%252C%2520assumes%250Alittle%2520to%2520no%2520access%2520to%2520the%2520actual%2520training%2520set%252C%2520and%2520identifies%252C%2520seemingly%252C%250Abenign%2520prompts%2520that%2520lead%2520to%2520potentially-risky%2520image%2520reconstruction.%2520This%250Ahighlights%2520the%2520risk%2520that%2520images%2520might%2520even%2520be%2520reconstructed%2520by%2520an%2520uninformed%250Auser%2520and%2520unintentionally.%2520For%2520example%252C%2520we%2520identified%2520that%252C%2520with%2520regard%2520to%2520one%250Aexisting%2520model%252C%2520the%2520prompt%2520%2560%2560blue%2520Unisex%2520T-Shirt%2527%2527%2520can%2520generate%2520the%2520face%2520of%2520a%250Areal-life%2520human%2520model.%2520Our%2520method%2520builds%2520on%2520an%2520intuition%2520from%2520previous%2520works%250Awhich%2520leverages%2520domain%2520knowledge%2520and%2520identifies%2520a%2520fundamental%2520vulnerability%250Athat%2520stems%2520from%2520the%2520use%2520of%2520scraped%2520data%2520from%2520e-commerce%2520platforms%252C%2520where%250Atemplated%2520layouts%2520and%2520images%2520are%2520tied%2520to%2520pattern-like%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07947v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Resource%20Reconstruction%20Attacks%20Through%20Benign%20Prompts&entry.906535625=Sol%20Yarkoni%20and%20Roi%20Livni&entry.1292438233=%20%20The%20recent%20advances%20in%20generative%20models%20such%20as%20diffusion%20models%20have%20raised%0Aseveral%20risks%20and%20concerns%20related%20to%20privacy%2C%20copyright%20infringements%20and%20data%0Astewardship.%20To%20better%20understand%20and%20control%20the%20risks%2C%20various%20researchers%0Ahave%20created%20techniques%2C%20experiments%20and%20attacks%20that%20reconstruct%20images%2C%20or%0Apart%20of%20images%2C%20from%20the%20training%20set.%20While%20these%20techniques%20already%20establish%0Athat%20data%20from%20the%20training%20set%20can%20be%20reconstructed%2C%20they%20often%20rely%20on%0Ahigh-resources%2C%20excess%20to%20the%20training%20set%20as%20well%20as%20well-engineered%20and%0Adesigned%20prompts.%0A%20%20In%20this%20work%2C%20we%20devise%20a%20new%20attack%20that%20requires%20low%20resources%2C%20assumes%0Alittle%20to%20no%20access%20to%20the%20actual%20training%20set%2C%20and%20identifies%2C%20seemingly%2C%0Abenign%20prompts%20that%20lead%20to%20potentially-risky%20image%20reconstruction.%20This%0Ahighlights%20the%20risk%20that%20images%20might%20even%20be%20reconstructed%20by%20an%20uninformed%0Auser%20and%20unintentionally.%20For%20example%2C%20we%20identified%20that%2C%20with%20regard%20to%20one%0Aexisting%20model%2C%20the%20prompt%20%60%60blue%20Unisex%20T-Shirt%27%27%20can%20generate%20the%20face%20of%20a%0Areal-life%20human%20model.%20Our%20method%20builds%20on%20an%20intuition%20from%20previous%20works%0Awhich%20leverages%20domain%20knowledge%20and%20identifies%20a%20fundamental%20vulnerability%0Athat%20stems%20from%20the%20use%20of%20scraped%20data%20from%20e-commerce%20platforms%2C%20where%0Atemplated%20layouts%20and%20images%20are%20tied%20to%20pattern-like%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07947v2&entry.124074799=Read"},
{"title": "Average Calibration Error: A Differentiable Loss for Improved\n  Reliability in Image Segmentation", "author": "Theodore Barfoot and Luis Garcia-Peraza-Herrera and Ben Glocker and Tom Vercauteren", "abstract": "  Deep neural networks for medical image segmentation often produce\noverconfident results misaligned with empirical observations. Such\nmiscalibration, challenges their clinical translation. We propose to use\nmarginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss\nfunction to improve pixel-wise calibration without compromising segmentation\nquality. We show that this loss, despite using hard binning, is directly\ndifferentiable, bypassing the need for approximate but differentiable surrogate\nor soft binning approaches. Our work also introduces the concept of dataset\nreliability histograms which generalises standard reliability diagrams for\nrefined visual assessment of calibration in semantic segmentation aggregated at\nthe dataset level. Using mL1-ACE, we reduce average and maximum calibration\nerror by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS\n2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS\n", "link": "http://arxiv.org/abs/2403.06759v4", "date": "2025-07-14", "relevancy": 2.125, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5661}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5507}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Average%20Calibration%20Error%3A%20A%20Differentiable%20Loss%20for%20Improved%0A%20%20Reliability%20in%20Image%20Segmentation&body=Title%3A%20Average%20Calibration%20Error%3A%20A%20Differentiable%20Loss%20for%20Improved%0A%20%20Reliability%20in%20Image%20Segmentation%0AAuthor%3A%20Theodore%20Barfoot%20and%20Luis%20Garcia-Peraza-Herrera%20and%20Ben%20Glocker%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20Deep%20neural%20networks%20for%20medical%20image%20segmentation%20often%20produce%0Aoverconfident%20results%20misaligned%20with%20empirical%20observations.%20Such%0Amiscalibration%2C%20challenges%20their%20clinical%20translation.%20We%20propose%20to%20use%0Amarginal%20L1%20average%20calibration%20error%20%28mL1-ACE%29%20as%20a%20novel%20auxiliary%20loss%0Afunction%20to%20improve%20pixel-wise%20calibration%20without%20compromising%20segmentation%0Aquality.%20We%20show%20that%20this%20loss%2C%20despite%20using%20hard%20binning%2C%20is%20directly%0Adifferentiable%2C%20bypassing%20the%20need%20for%20approximate%20but%20differentiable%20surrogate%0Aor%20soft%20binning%20approaches.%20Our%20work%20also%20introduces%20the%20concept%20of%20dataset%0Areliability%20histograms%20which%20generalises%20standard%20reliability%20diagrams%20for%0Arefined%20visual%20assessment%20of%20calibration%20in%20semantic%20segmentation%20aggregated%20at%0Athe%20dataset%20level.%20Using%20mL1-ACE%2C%20we%20reduce%20average%20and%20maximum%20calibration%0Aerror%20by%2045%25%20and%2055%25%20respectively%2C%20maintaining%20a%20Dice%20score%20of%2087%25%20on%20the%20BraTS%0A2021%20dataset.%20We%20share%20our%20code%20here%3A%20https%3A//github.com/cai4cai/ACE-DLIRIS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06759v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAverage%2520Calibration%2520Error%253A%2520A%2520Differentiable%2520Loss%2520for%2520Improved%250A%2520%2520Reliability%2520in%2520Image%2520Segmentation%26entry.906535625%3DTheodore%2520Barfoot%2520and%2520Luis%2520Garcia-Peraza-Herrera%2520and%2520Ben%2520Glocker%2520and%2520Tom%2520Vercauteren%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520for%2520medical%2520image%2520segmentation%2520often%2520produce%250Aoverconfident%2520results%2520misaligned%2520with%2520empirical%2520observations.%2520Such%250Amiscalibration%252C%2520challenges%2520their%2520clinical%2520translation.%2520We%2520propose%2520to%2520use%250Amarginal%2520L1%2520average%2520calibration%2520error%2520%2528mL1-ACE%2529%2520as%2520a%2520novel%2520auxiliary%2520loss%250Afunction%2520to%2520improve%2520pixel-wise%2520calibration%2520without%2520compromising%2520segmentation%250Aquality.%2520We%2520show%2520that%2520this%2520loss%252C%2520despite%2520using%2520hard%2520binning%252C%2520is%2520directly%250Adifferentiable%252C%2520bypassing%2520the%2520need%2520for%2520approximate%2520but%2520differentiable%2520surrogate%250Aor%2520soft%2520binning%2520approaches.%2520Our%2520work%2520also%2520introduces%2520the%2520concept%2520of%2520dataset%250Areliability%2520histograms%2520which%2520generalises%2520standard%2520reliability%2520diagrams%2520for%250Arefined%2520visual%2520assessment%2520of%2520calibration%2520in%2520semantic%2520segmentation%2520aggregated%2520at%250Athe%2520dataset%2520level.%2520Using%2520mL1-ACE%252C%2520we%2520reduce%2520average%2520and%2520maximum%2520calibration%250Aerror%2520by%252045%2525%2520and%252055%2525%2520respectively%252C%2520maintaining%2520a%2520Dice%2520score%2520of%252087%2525%2520on%2520the%2520BraTS%250A2021%2520dataset.%2520We%2520share%2520our%2520code%2520here%253A%2520https%253A//github.com/cai4cai/ACE-DLIRIS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06759v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Average%20Calibration%20Error%3A%20A%20Differentiable%20Loss%20for%20Improved%0A%20%20Reliability%20in%20Image%20Segmentation&entry.906535625=Theodore%20Barfoot%20and%20Luis%20Garcia-Peraza-Herrera%20and%20Ben%20Glocker%20and%20Tom%20Vercauteren&entry.1292438233=%20%20Deep%20neural%20networks%20for%20medical%20image%20segmentation%20often%20produce%0Aoverconfident%20results%20misaligned%20with%20empirical%20observations.%20Such%0Amiscalibration%2C%20challenges%20their%20clinical%20translation.%20We%20propose%20to%20use%0Amarginal%20L1%20average%20calibration%20error%20%28mL1-ACE%29%20as%20a%20novel%20auxiliary%20loss%0Afunction%20to%20improve%20pixel-wise%20calibration%20without%20compromising%20segmentation%0Aquality.%20We%20show%20that%20this%20loss%2C%20despite%20using%20hard%20binning%2C%20is%20directly%0Adifferentiable%2C%20bypassing%20the%20need%20for%20approximate%20but%20differentiable%20surrogate%0Aor%20soft%20binning%20approaches.%20Our%20work%20also%20introduces%20the%20concept%20of%20dataset%0Areliability%20histograms%20which%20generalises%20standard%20reliability%20diagrams%20for%0Arefined%20visual%20assessment%20of%20calibration%20in%20semantic%20segmentation%20aggregated%20at%0Athe%20dataset%20level.%20Using%20mL1-ACE%2C%20we%20reduce%20average%20and%20maximum%20calibration%0Aerror%20by%2045%25%20and%2055%25%20respectively%2C%20maintaining%20a%20Dice%20score%20of%2087%25%20on%20the%20BraTS%0A2021%20dataset.%20We%20share%20our%20code%20here%3A%20https%3A//github.com/cai4cai/ACE-DLIRIS%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06759v4&entry.124074799=Read"},
{"title": "Fast Bilateral Teleoperation and Imitation Learning Using Sensorless\n  Force Control via Accurate Dynamics Model", "author": "Koki Yamane and Yunhan Li and Masashi Konosu and Koki Inami and Junji Oaki and Sho Sakaino and Toshiaki Tsuji", "abstract": "  In recent years, the advancement of imitation learning has led to increased\ninterest in teleoperating low-cost manipulators to collect demonstration data.\nHowever, most existing systems rely on unilateral control, which only transmits\ntarget position values. While this approach is easy to implement and suitable\nfor slow, non-contact tasks, it struggles with fast or contact-rich operations\ndue to the absence of force feedback. This work demonstrates that fast\nteleoperation with force feedback is feasible even with force-sensorless,\nlow-cost manipulators by leveraging 4-channel bilateral control. Based on\naccurately identified manipulator dynamics, our method integrates nonlinear\nterms compensation, velocity and external force estimation, and variable gain\ncorresponding to inertial variation. Furthermore, using data collected by\n4-channel bilateral control, we show that incorporating force information into\nboth the input and output of learned policies improves performance in imitation\nlearning. These results highlight the practical effectiveness of our system for\nhigh-fidelity teleoperation and data collection on affordable hardware.\n", "link": "http://arxiv.org/abs/2507.06174v2", "date": "2025-07-14", "relevancy": 2.1225, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5301}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model&body=Title%3A%20Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model%0AAuthor%3A%20Koki%20Yamane%20and%20Yunhan%20Li%20and%20Masashi%20Konosu%20and%20Koki%20Inami%20and%20Junji%20Oaki%20and%20Sho%20Sakaino%20and%20Toshiaki%20Tsuji%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20advancement%20of%20imitation%20learning%20has%20led%20to%20increased%0Ainterest%20in%20teleoperating%20low-cost%20manipulators%20to%20collect%20demonstration%20data.%0AHowever%2C%20most%20existing%20systems%20rely%20on%20unilateral%20control%2C%20which%20only%20transmits%0Atarget%20position%20values.%20While%20this%20approach%20is%20easy%20to%20implement%20and%20suitable%0Afor%20slow%2C%20non-contact%20tasks%2C%20it%20struggles%20with%20fast%20or%20contact-rich%20operations%0Adue%20to%20the%20absence%20of%20force%20feedback.%20This%20work%20demonstrates%20that%20fast%0Ateleoperation%20with%20force%20feedback%20is%20feasible%20even%20with%20force-sensorless%2C%0Alow-cost%20manipulators%20by%20leveraging%204-channel%20bilateral%20control.%20Based%20on%0Aaccurately%20identified%20manipulator%20dynamics%2C%20our%20method%20integrates%20nonlinear%0Aterms%20compensation%2C%20velocity%20and%20external%20force%20estimation%2C%20and%20variable%20gain%0Acorresponding%20to%20inertial%20variation.%20Furthermore%2C%20using%20data%20collected%20by%0A4-channel%20bilateral%20control%2C%20we%20show%20that%20incorporating%20force%20information%20into%0Aboth%20the%20input%20and%20output%20of%20learned%20policies%20improves%20performance%20in%20imitation%0Alearning.%20These%20results%20highlight%20the%20practical%20effectiveness%20of%20our%20system%20for%0Ahigh-fidelity%20teleoperation%20and%20data%20collection%20on%20affordable%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Bilateral%2520Teleoperation%2520and%2520Imitation%2520Learning%2520Using%2520Sensorless%250A%2520%2520Force%2520Control%2520via%2520Accurate%2520Dynamics%2520Model%26entry.906535625%3DKoki%2520Yamane%2520and%2520Yunhan%2520Li%2520and%2520Masashi%2520Konosu%2520and%2520Koki%2520Inami%2520and%2520Junji%2520Oaki%2520and%2520Sho%2520Sakaino%2520and%2520Toshiaki%2520Tsuji%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520advancement%2520of%2520imitation%2520learning%2520has%2520led%2520to%2520increased%250Ainterest%2520in%2520teleoperating%2520low-cost%2520manipulators%2520to%2520collect%2520demonstration%2520data.%250AHowever%252C%2520most%2520existing%2520systems%2520rely%2520on%2520unilateral%2520control%252C%2520which%2520only%2520transmits%250Atarget%2520position%2520values.%2520While%2520this%2520approach%2520is%2520easy%2520to%2520implement%2520and%2520suitable%250Afor%2520slow%252C%2520non-contact%2520tasks%252C%2520it%2520struggles%2520with%2520fast%2520or%2520contact-rich%2520operations%250Adue%2520to%2520the%2520absence%2520of%2520force%2520feedback.%2520This%2520work%2520demonstrates%2520that%2520fast%250Ateleoperation%2520with%2520force%2520feedback%2520is%2520feasible%2520even%2520with%2520force-sensorless%252C%250Alow-cost%2520manipulators%2520by%2520leveraging%25204-channel%2520bilateral%2520control.%2520Based%2520on%250Aaccurately%2520identified%2520manipulator%2520dynamics%252C%2520our%2520method%2520integrates%2520nonlinear%250Aterms%2520compensation%252C%2520velocity%2520and%2520external%2520force%2520estimation%252C%2520and%2520variable%2520gain%250Acorresponding%2520to%2520inertial%2520variation.%2520Furthermore%252C%2520using%2520data%2520collected%2520by%250A4-channel%2520bilateral%2520control%252C%2520we%2520show%2520that%2520incorporating%2520force%2520information%2520into%250Aboth%2520the%2520input%2520and%2520output%2520of%2520learned%2520policies%2520improves%2520performance%2520in%2520imitation%250Alearning.%2520These%2520results%2520highlight%2520the%2520practical%2520effectiveness%2520of%2520our%2520system%2520for%250Ahigh-fidelity%2520teleoperation%2520and%2520data%2520collection%2520on%2520affordable%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Bilateral%20Teleoperation%20and%20Imitation%20Learning%20Using%20Sensorless%0A%20%20Force%20Control%20via%20Accurate%20Dynamics%20Model&entry.906535625=Koki%20Yamane%20and%20Yunhan%20Li%20and%20Masashi%20Konosu%20and%20Koki%20Inami%20and%20Junji%20Oaki%20and%20Sho%20Sakaino%20and%20Toshiaki%20Tsuji&entry.1292438233=%20%20In%20recent%20years%2C%20the%20advancement%20of%20imitation%20learning%20has%20led%20to%20increased%0Ainterest%20in%20teleoperating%20low-cost%20manipulators%20to%20collect%20demonstration%20data.%0AHowever%2C%20most%20existing%20systems%20rely%20on%20unilateral%20control%2C%20which%20only%20transmits%0Atarget%20position%20values.%20While%20this%20approach%20is%20easy%20to%20implement%20and%20suitable%0Afor%20slow%2C%20non-contact%20tasks%2C%20it%20struggles%20with%20fast%20or%20contact-rich%20operations%0Adue%20to%20the%20absence%20of%20force%20feedback.%20This%20work%20demonstrates%20that%20fast%0Ateleoperation%20with%20force%20feedback%20is%20feasible%20even%20with%20force-sensorless%2C%0Alow-cost%20manipulators%20by%20leveraging%204-channel%20bilateral%20control.%20Based%20on%0Aaccurately%20identified%20manipulator%20dynamics%2C%20our%20method%20integrates%20nonlinear%0Aterms%20compensation%2C%20velocity%20and%20external%20force%20estimation%2C%20and%20variable%20gain%0Acorresponding%20to%20inertial%20variation.%20Furthermore%2C%20using%20data%20collected%20by%0A4-channel%20bilateral%20control%2C%20we%20show%20that%20incorporating%20force%20information%20into%0Aboth%20the%20input%20and%20output%20of%20learned%20policies%20improves%20performance%20in%20imitation%0Alearning.%20These%20results%20highlight%20the%20practical%20effectiveness%20of%20our%20system%20for%0Ahigh-fidelity%20teleoperation%20and%20data%20collection%20on%20affordable%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06174v2&entry.124074799=Read"},
{"title": "Screen Them All: High-Throughput Pan-Cancer Genetic and Phenotypic\n  Biomarker Screening from H&E Whole Slide Images", "author": "Yi Kan Wang and Ludmila Tydlitatova and Jeremy D. Kunz and Gerard Oakley and Bonnie Kar Bo Chow and Ran A. Godrich and Matthew C. H. Lee and Hamed Aghdam and Alican Bozkurt and Michal Zelechowski and Chad Vanderbilt and Christopher Kanan and Juan A. Retamero and Peter Hamilton and Razik Yousfi and Thomas J. Fuchs and David S. Klimstra and Siqi Liu", "abstract": "  Molecular assays are standard of care for detecting genomic alterations in\ncancer prognosis and therapy selection but are costly, tissue-destructive and\ntime-consuming. Artificial intelligence (AI) applied to routine hematoxylin and\neosin (H&E)-stained whole slide images (WSIs) offers a fast and economical\nalternative for screening molecular biomarkers. We introduce OmniScreen, a\nhigh-throughput AI-based system leveraging Virchow2 embeddings extracted from\n60,529 cancer patients with paired 489-gene MSK-IMPACT targeted biomarker panel\nand WSIs. Unlike conventional approaches that train separate models for each\nbiomarker, OmniScreen employs a unified model to predict a broad range of\nclinically relevant biomarkers across cancers, including low-prevalence targets\nimpractical to model individually. OmniScreen reliably identifies therapeutic\ntargets and shared phenotypic features across common and rare tumors. We\ninvestigate the biomarker prediction probabilities and accuracies of OmniScreen\nin relation to tumor area, cohort size, histologic subtype alignment, and\npathway-level morphological patterns. These findings underscore the potential\nof OmniScreen for routine clinical screening.\n", "link": "http://arxiv.org/abs/2408.09554v4", "date": "2025-07-14", "relevancy": 2.1203, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4292}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Screen%20Them%20All%3A%20High-Throughput%20Pan-Cancer%20Genetic%20and%20Phenotypic%0A%20%20Biomarker%20Screening%20from%20H%26E%20Whole%20Slide%20Images&body=Title%3A%20Screen%20Them%20All%3A%20High-Throughput%20Pan-Cancer%20Genetic%20and%20Phenotypic%0A%20%20Biomarker%20Screening%20from%20H%26E%20Whole%20Slide%20Images%0AAuthor%3A%20Yi%20Kan%20Wang%20and%20Ludmila%20Tydlitatova%20and%20Jeremy%20D.%20Kunz%20and%20Gerard%20Oakley%20and%20Bonnie%20Kar%20Bo%20Chow%20and%20Ran%20A.%20Godrich%20and%20Matthew%20C.%20H.%20Lee%20and%20Hamed%20Aghdam%20and%20Alican%20Bozkurt%20and%20Michal%20Zelechowski%20and%20Chad%20Vanderbilt%20and%20Christopher%20Kanan%20and%20Juan%20A.%20Retamero%20and%20Peter%20Hamilton%20and%20Razik%20Yousfi%20and%20Thomas%20J.%20Fuchs%20and%20David%20S.%20Klimstra%20and%20Siqi%20Liu%0AAbstract%3A%20%20%20Molecular%20assays%20are%20standard%20of%20care%20for%20detecting%20genomic%20alterations%20in%0Acancer%20prognosis%20and%20therapy%20selection%20but%20are%20costly%2C%20tissue-destructive%20and%0Atime-consuming.%20Artificial%20intelligence%20%28AI%29%20applied%20to%20routine%20hematoxylin%20and%0Aeosin%20%28H%26E%29-stained%20whole%20slide%20images%20%28WSIs%29%20offers%20a%20fast%20and%20economical%0Aalternative%20for%20screening%20molecular%20biomarkers.%20We%20introduce%20OmniScreen%2C%20a%0Ahigh-throughput%20AI-based%20system%20leveraging%20Virchow2%20embeddings%20extracted%20from%0A60%2C529%20cancer%20patients%20with%20paired%20489-gene%20MSK-IMPACT%20targeted%20biomarker%20panel%0Aand%20WSIs.%20Unlike%20conventional%20approaches%20that%20train%20separate%20models%20for%20each%0Abiomarker%2C%20OmniScreen%20employs%20a%20unified%20model%20to%20predict%20a%20broad%20range%20of%0Aclinically%20relevant%20biomarkers%20across%20cancers%2C%20including%20low-prevalence%20targets%0Aimpractical%20to%20model%20individually.%20OmniScreen%20reliably%20identifies%20therapeutic%0Atargets%20and%20shared%20phenotypic%20features%20across%20common%20and%20rare%20tumors.%20We%0Ainvestigate%20the%20biomarker%20prediction%20probabilities%20and%20accuracies%20of%20OmniScreen%0Ain%20relation%20to%20tumor%20area%2C%20cohort%20size%2C%20histologic%20subtype%20alignment%2C%20and%0Apathway-level%20morphological%20patterns.%20These%20findings%20underscore%20the%20potential%0Aof%20OmniScreen%20for%20routine%20clinical%20screening.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09554v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScreen%2520Them%2520All%253A%2520High-Throughput%2520Pan-Cancer%2520Genetic%2520and%2520Phenotypic%250A%2520%2520Biomarker%2520Screening%2520from%2520H%2526E%2520Whole%2520Slide%2520Images%26entry.906535625%3DYi%2520Kan%2520Wang%2520and%2520Ludmila%2520Tydlitatova%2520and%2520Jeremy%2520D.%2520Kunz%2520and%2520Gerard%2520Oakley%2520and%2520Bonnie%2520Kar%2520Bo%2520Chow%2520and%2520Ran%2520A.%2520Godrich%2520and%2520Matthew%2520C.%2520H.%2520Lee%2520and%2520Hamed%2520Aghdam%2520and%2520Alican%2520Bozkurt%2520and%2520Michal%2520Zelechowski%2520and%2520Chad%2520Vanderbilt%2520and%2520Christopher%2520Kanan%2520and%2520Juan%2520A.%2520Retamero%2520and%2520Peter%2520Hamilton%2520and%2520Razik%2520Yousfi%2520and%2520Thomas%2520J.%2520Fuchs%2520and%2520David%2520S.%2520Klimstra%2520and%2520Siqi%2520Liu%26entry.1292438233%3D%2520%2520Molecular%2520assays%2520are%2520standard%2520of%2520care%2520for%2520detecting%2520genomic%2520alterations%2520in%250Acancer%2520prognosis%2520and%2520therapy%2520selection%2520but%2520are%2520costly%252C%2520tissue-destructive%2520and%250Atime-consuming.%2520Artificial%2520intelligence%2520%2528AI%2529%2520applied%2520to%2520routine%2520hematoxylin%2520and%250Aeosin%2520%2528H%2526E%2529-stained%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520offers%2520a%2520fast%2520and%2520economical%250Aalternative%2520for%2520screening%2520molecular%2520biomarkers.%2520We%2520introduce%2520OmniScreen%252C%2520a%250Ahigh-throughput%2520AI-based%2520system%2520leveraging%2520Virchow2%2520embeddings%2520extracted%2520from%250A60%252C529%2520cancer%2520patients%2520with%2520paired%2520489-gene%2520MSK-IMPACT%2520targeted%2520biomarker%2520panel%250Aand%2520WSIs.%2520Unlike%2520conventional%2520approaches%2520that%2520train%2520separate%2520models%2520for%2520each%250Abiomarker%252C%2520OmniScreen%2520employs%2520a%2520unified%2520model%2520to%2520predict%2520a%2520broad%2520range%2520of%250Aclinically%2520relevant%2520biomarkers%2520across%2520cancers%252C%2520including%2520low-prevalence%2520targets%250Aimpractical%2520to%2520model%2520individually.%2520OmniScreen%2520reliably%2520identifies%2520therapeutic%250Atargets%2520and%2520shared%2520phenotypic%2520features%2520across%2520common%2520and%2520rare%2520tumors.%2520We%250Ainvestigate%2520the%2520biomarker%2520prediction%2520probabilities%2520and%2520accuracies%2520of%2520OmniScreen%250Ain%2520relation%2520to%2520tumor%2520area%252C%2520cohort%2520size%252C%2520histologic%2520subtype%2520alignment%252C%2520and%250Apathway-level%2520morphological%2520patterns.%2520These%2520findings%2520underscore%2520the%2520potential%250Aof%2520OmniScreen%2520for%2520routine%2520clinical%2520screening.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09554v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Screen%20Them%20All%3A%20High-Throughput%20Pan-Cancer%20Genetic%20and%20Phenotypic%0A%20%20Biomarker%20Screening%20from%20H%26E%20Whole%20Slide%20Images&entry.906535625=Yi%20Kan%20Wang%20and%20Ludmila%20Tydlitatova%20and%20Jeremy%20D.%20Kunz%20and%20Gerard%20Oakley%20and%20Bonnie%20Kar%20Bo%20Chow%20and%20Ran%20A.%20Godrich%20and%20Matthew%20C.%20H.%20Lee%20and%20Hamed%20Aghdam%20and%20Alican%20Bozkurt%20and%20Michal%20Zelechowski%20and%20Chad%20Vanderbilt%20and%20Christopher%20Kanan%20and%20Juan%20A.%20Retamero%20and%20Peter%20Hamilton%20and%20Razik%20Yousfi%20and%20Thomas%20J.%20Fuchs%20and%20David%20S.%20Klimstra%20and%20Siqi%20Liu&entry.1292438233=%20%20Molecular%20assays%20are%20standard%20of%20care%20for%20detecting%20genomic%20alterations%20in%0Acancer%20prognosis%20and%20therapy%20selection%20but%20are%20costly%2C%20tissue-destructive%20and%0Atime-consuming.%20Artificial%20intelligence%20%28AI%29%20applied%20to%20routine%20hematoxylin%20and%0Aeosin%20%28H%26E%29-stained%20whole%20slide%20images%20%28WSIs%29%20offers%20a%20fast%20and%20economical%0Aalternative%20for%20screening%20molecular%20biomarkers.%20We%20introduce%20OmniScreen%2C%20a%0Ahigh-throughput%20AI-based%20system%20leveraging%20Virchow2%20embeddings%20extracted%20from%0A60%2C529%20cancer%20patients%20with%20paired%20489-gene%20MSK-IMPACT%20targeted%20biomarker%20panel%0Aand%20WSIs.%20Unlike%20conventional%20approaches%20that%20train%20separate%20models%20for%20each%0Abiomarker%2C%20OmniScreen%20employs%20a%20unified%20model%20to%20predict%20a%20broad%20range%20of%0Aclinically%20relevant%20biomarkers%20across%20cancers%2C%20including%20low-prevalence%20targets%0Aimpractical%20to%20model%20individually.%20OmniScreen%20reliably%20identifies%20therapeutic%0Atargets%20and%20shared%20phenotypic%20features%20across%20common%20and%20rare%20tumors.%20We%0Ainvestigate%20the%20biomarker%20prediction%20probabilities%20and%20accuracies%20of%20OmniScreen%0Ain%20relation%20to%20tumor%20area%2C%20cohort%20size%2C%20histologic%20subtype%20alignment%2C%20and%0Apathway-level%20morphological%20patterns.%20These%20findings%20underscore%20the%20potential%0Aof%20OmniScreen%20for%20routine%20clinical%20screening.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09554v4&entry.124074799=Read"},
{"title": "Graph World Model", "author": "Tao Feng and Yexin Wu and Guanyu Lin and Jiaxuan You", "abstract": "  World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM.\n", "link": "http://arxiv.org/abs/2507.10539v1", "date": "2025-07-14", "relevancy": 2.1188, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5471}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5215}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20World%20Model&body=Title%3A%20Graph%20World%20Model%0AAuthor%3A%20Tao%20Feng%20and%20Yexin%20Wu%20and%20Guanyu%20Lin%20and%20Jiaxuan%20You%0AAbstract%3A%20%20%20World%20models%20%28WMs%29%20demonstrate%20strong%20capabilities%20in%20prediction%2C%20generation%2C%0Aand%20planning%20tasks.%20Existing%20WMs%20primarily%20focus%20on%20unstructured%20data%20and%0Acannot%20leverage%20the%20ubiquitous%20structured%20data%2C%20often%20represented%20as%20graphs%2C%20in%0Athe%20digital%20world.%20While%20multiple%20graph%20foundation%20models%20have%20been%20proposed%2C%0Athey%20focus%20on%20graph%20learning%20tasks%20and%20cannot%20extend%20to%20diverse%20multi-modal%0Adata%20and%20interdisciplinary%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20the%0AGraph%20World%20Model%20%28GWM%29%2C%20a%20world%20model%20that%20supports%20both%20unstructured%20and%0Agraph-structured%20states%20with%20multi-modal%20information%20and%20represents%20diverse%0Atasks%20as%20actions.%20The%20core%20of%20a%20GWM%20is%20a%20generic%20message-passing%20algorithm%20to%0Aaggregate%20structured%20information%2C%20either%20over%20a%20unified%20multi-modal%20token%20space%0Aby%20converting%20multi-modal%20data%20into%20text%20%28GWM-T%29%20or%20a%20unified%20multi-modal%0Aembedding%20space%20by%20modality-specific%20encoders%20%28GWM-E%29.%20Notably%2C%20GWM%20introduces%0Aaction%20nodes%20to%20support%20diverse%20tasks%2C%20where%20action%20nodes%20are%20linked%20to%20other%0Anodes%20via%20direct%20reference%20or%20similarity%20computation.%20Extensive%20experiments%20on%0Asix%20tasks%20from%20diverse%20domains%2C%20including%20multi-modal%20generation%20and%20matching%2C%0Arecommendation%2C%20graph%20prediction%2C%20multi-agent%2C%20retrieval-augmented%20generation%2C%0Aand%20planning%20and%20optimization%2C%20show%20that%20the%20same%20GWM%20outperforms%20or%20matches%0Adomain-specific%20baselines%27%20performance%2C%20benefits%20from%20multi-hop%20structures%2C%20and%0Ademonstrates%20strong%20zero-shot/few-shot%20capabilities%20on%20unseen%20new%20tasks.%20Our%0Acode%20for%20GWM%20is%20released%20at%20https%3A//github.com/ulab-uiuc/GWM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520World%2520Model%26entry.906535625%3DTao%2520Feng%2520and%2520Yexin%2520Wu%2520and%2520Guanyu%2520Lin%2520and%2520Jiaxuan%2520You%26entry.1292438233%3D%2520%2520World%2520models%2520%2528WMs%2529%2520demonstrate%2520strong%2520capabilities%2520in%2520prediction%252C%2520generation%252C%250Aand%2520planning%2520tasks.%2520Existing%2520WMs%2520primarily%2520focus%2520on%2520unstructured%2520data%2520and%250Acannot%2520leverage%2520the%2520ubiquitous%2520structured%2520data%252C%2520often%2520represented%2520as%2520graphs%252C%2520in%250Athe%2520digital%2520world.%2520While%2520multiple%2520graph%2520foundation%2520models%2520have%2520been%2520proposed%252C%250Athey%2520focus%2520on%2520graph%2520learning%2520tasks%2520and%2520cannot%2520extend%2520to%2520diverse%2520multi-modal%250Adata%2520and%2520interdisciplinary%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%250AGraph%2520World%2520Model%2520%2528GWM%2529%252C%2520a%2520world%2520model%2520that%2520supports%2520both%2520unstructured%2520and%250Agraph-structured%2520states%2520with%2520multi-modal%2520information%2520and%2520represents%2520diverse%250Atasks%2520as%2520actions.%2520The%2520core%2520of%2520a%2520GWM%2520is%2520a%2520generic%2520message-passing%2520algorithm%2520to%250Aaggregate%2520structured%2520information%252C%2520either%2520over%2520a%2520unified%2520multi-modal%2520token%2520space%250Aby%2520converting%2520multi-modal%2520data%2520into%2520text%2520%2528GWM-T%2529%2520or%2520a%2520unified%2520multi-modal%250Aembedding%2520space%2520by%2520modality-specific%2520encoders%2520%2528GWM-E%2529.%2520Notably%252C%2520GWM%2520introduces%250Aaction%2520nodes%2520to%2520support%2520diverse%2520tasks%252C%2520where%2520action%2520nodes%2520are%2520linked%2520to%2520other%250Anodes%2520via%2520direct%2520reference%2520or%2520similarity%2520computation.%2520Extensive%2520experiments%2520on%250Asix%2520tasks%2520from%2520diverse%2520domains%252C%2520including%2520multi-modal%2520generation%2520and%2520matching%252C%250Arecommendation%252C%2520graph%2520prediction%252C%2520multi-agent%252C%2520retrieval-augmented%2520generation%252C%250Aand%2520planning%2520and%2520optimization%252C%2520show%2520that%2520the%2520same%2520GWM%2520outperforms%2520or%2520matches%250Adomain-specific%2520baselines%2527%2520performance%252C%2520benefits%2520from%2520multi-hop%2520structures%252C%2520and%250Ademonstrates%2520strong%2520zero-shot/few-shot%2520capabilities%2520on%2520unseen%2520new%2520tasks.%2520Our%250Acode%2520for%2520GWM%2520is%2520released%2520at%2520https%253A//github.com/ulab-uiuc/GWM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20World%20Model&entry.906535625=Tao%20Feng%20and%20Yexin%20Wu%20and%20Guanyu%20Lin%20and%20Jiaxuan%20You&entry.1292438233=%20%20World%20models%20%28WMs%29%20demonstrate%20strong%20capabilities%20in%20prediction%2C%20generation%2C%0Aand%20planning%20tasks.%20Existing%20WMs%20primarily%20focus%20on%20unstructured%20data%20and%0Acannot%20leverage%20the%20ubiquitous%20structured%20data%2C%20often%20represented%20as%20graphs%2C%20in%0Athe%20digital%20world.%20While%20multiple%20graph%20foundation%20models%20have%20been%20proposed%2C%0Athey%20focus%20on%20graph%20learning%20tasks%20and%20cannot%20extend%20to%20diverse%20multi-modal%0Adata%20and%20interdisciplinary%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20the%0AGraph%20World%20Model%20%28GWM%29%2C%20a%20world%20model%20that%20supports%20both%20unstructured%20and%0Agraph-structured%20states%20with%20multi-modal%20information%20and%20represents%20diverse%0Atasks%20as%20actions.%20The%20core%20of%20a%20GWM%20is%20a%20generic%20message-passing%20algorithm%20to%0Aaggregate%20structured%20information%2C%20either%20over%20a%20unified%20multi-modal%20token%20space%0Aby%20converting%20multi-modal%20data%20into%20text%20%28GWM-T%29%20or%20a%20unified%20multi-modal%0Aembedding%20space%20by%20modality-specific%20encoders%20%28GWM-E%29.%20Notably%2C%20GWM%20introduces%0Aaction%20nodes%20to%20support%20diverse%20tasks%2C%20where%20action%20nodes%20are%20linked%20to%20other%0Anodes%20via%20direct%20reference%20or%20similarity%20computation.%20Extensive%20experiments%20on%0Asix%20tasks%20from%20diverse%20domains%2C%20including%20multi-modal%20generation%20and%20matching%2C%0Arecommendation%2C%20graph%20prediction%2C%20multi-agent%2C%20retrieval-augmented%20generation%2C%0Aand%20planning%20and%20optimization%2C%20show%20that%20the%20same%20GWM%20outperforms%20or%20matches%0Adomain-specific%20baselines%27%20performance%2C%20benefits%20from%20multi-hop%20structures%2C%20and%0Ademonstrates%20strong%20zero-shot/few-shot%20capabilities%20on%20unseen%20new%20tasks.%20Our%0Acode%20for%20GWM%20is%20released%20at%20https%3A//github.com/ulab-uiuc/GWM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10539v1&entry.124074799=Read"},
{"title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via\n  Impartial Motion", "author": "Md Abulkalam Azad and John Nyberg and H\u00e5vard Dalen and Bj\u00f8rnar Grenne and Lasse Lovstakken and Andreas \u00d8stvik", "abstract": "  Accurate motion estimation for tracking deformable tissues in\nechocardiography is essential for precise cardiac function measurements. While\ntraditional methods like block matching or optical flow struggle with intricate\ncardiac motion, modern point tracking approaches remain largely underexplored\nin this domain. This work investigates the potential of state-of-the-art (SOTA)\npoint tracking methods for ultrasound, with a focus on echocardiography.\nAlthough these novel approaches demonstrate strong performance in general\nvideos, their effectiveness and generalizability in echocardiography remain\nlimited. By analyzing cardiac motion throughout the heart cycle in real B-mode\nultrasound videos, we identify that a directional motion bias across different\nviews is affecting the existing training strategies. To mitigate this, we\nrefine the training procedure and incorporate a set of tailored augmentations\nto reduce the bias and enhance tracking robustness and generalization through\nimpartial cardiac motion. We also propose a lightweight network leveraging\nmulti-scale cost volumes from spatial context alone to challenge the advanced\nspatiotemporal point tracking models. Experiments demonstrate that fine-tuning\nwith our strategies significantly improves models' performances over their\nbaselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker\nboosts overall position accuracy by 60.7% and reduces median trajectory error\nby 61.5% across heart cycle phases. Interestingly, several point tracking\nmodels fail to outperform our proposed simple model in terms of tracking\naccuracy and generalization, reflecting their limitations when applied to\nechocardiography. Nevertheless, clinical evaluation reveals that these methods\nimprove GLS measurements, aligning more closely with expert-validated,\nsemi-automated tools and thus demonstrating better reproducibility in\nreal-world applications.\n", "link": "http://arxiv.org/abs/2507.10127v1", "date": "2025-07-14", "relevancy": 2.117, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5393}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5297}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Modern%20Point%20Tracking%20for%20Speckle%20Tracking%20Echocardiography%20via%0A%20%20Impartial%20Motion&body=Title%3A%20Taming%20Modern%20Point%20Tracking%20for%20Speckle%20Tracking%20Echocardiography%20via%0A%20%20Impartial%20Motion%0AAuthor%3A%20Md%20Abulkalam%20Azad%20and%20John%20Nyberg%20and%20H%C3%A5vard%20Dalen%20and%20Bj%C3%B8rnar%20Grenne%20and%20Lasse%20Lovstakken%20and%20Andreas%20%C3%98stvik%0AAbstract%3A%20%20%20Accurate%20motion%20estimation%20for%20tracking%20deformable%20tissues%20in%0Aechocardiography%20is%20essential%20for%20precise%20cardiac%20function%20measurements.%20While%0Atraditional%20methods%20like%20block%20matching%20or%20optical%20flow%20struggle%20with%20intricate%0Acardiac%20motion%2C%20modern%20point%20tracking%20approaches%20remain%20largely%20underexplored%0Ain%20this%20domain.%20This%20work%20investigates%20the%20potential%20of%20state-of-the-art%20%28SOTA%29%0Apoint%20tracking%20methods%20for%20ultrasound%2C%20with%20a%20focus%20on%20echocardiography.%0AAlthough%20these%20novel%20approaches%20demonstrate%20strong%20performance%20in%20general%0Avideos%2C%20their%20effectiveness%20and%20generalizability%20in%20echocardiography%20remain%0Alimited.%20By%20analyzing%20cardiac%20motion%20throughout%20the%20heart%20cycle%20in%20real%20B-mode%0Aultrasound%20videos%2C%20we%20identify%20that%20a%20directional%20motion%20bias%20across%20different%0Aviews%20is%20affecting%20the%20existing%20training%20strategies.%20To%20mitigate%20this%2C%20we%0Arefine%20the%20training%20procedure%20and%20incorporate%20a%20set%20of%20tailored%20augmentations%0Ato%20reduce%20the%20bias%20and%20enhance%20tracking%20robustness%20and%20generalization%20through%0Aimpartial%20cardiac%20motion.%20We%20also%20propose%20a%20lightweight%20network%20leveraging%0Amulti-scale%20cost%20volumes%20from%20spatial%20context%20alone%20to%20challenge%20the%20advanced%0Aspatiotemporal%20point%20tracking%20models.%20Experiments%20demonstrate%20that%20fine-tuning%0Awith%20our%20strategies%20significantly%20improves%20models%27%20performances%20over%20their%0Abaselines%2C%20even%20for%20out-of-distribution%20%28OOD%29%20cases.%20For%20instance%2C%20EchoTracker%0Aboosts%20overall%20position%20accuracy%20by%2060.7%25%20and%20reduces%20median%20trajectory%20error%0Aby%2061.5%25%20across%20heart%20cycle%20phases.%20Interestingly%2C%20several%20point%20tracking%0Amodels%20fail%20to%20outperform%20our%20proposed%20simple%20model%20in%20terms%20of%20tracking%0Aaccuracy%20and%20generalization%2C%20reflecting%20their%20limitations%20when%20applied%20to%0Aechocardiography.%20Nevertheless%2C%20clinical%20evaluation%20reveals%20that%20these%20methods%0Aimprove%20GLS%20measurements%2C%20aligning%20more%20closely%20with%20expert-validated%2C%0Asemi-automated%20tools%20and%20thus%20demonstrating%20better%20reproducibility%20in%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Modern%2520Point%2520Tracking%2520for%2520Speckle%2520Tracking%2520Echocardiography%2520via%250A%2520%2520Impartial%2520Motion%26entry.906535625%3DMd%2520Abulkalam%2520Azad%2520and%2520John%2520Nyberg%2520and%2520H%25C3%25A5vard%2520Dalen%2520and%2520Bj%25C3%25B8rnar%2520Grenne%2520and%2520Lasse%2520Lovstakken%2520and%2520Andreas%2520%25C3%2598stvik%26entry.1292438233%3D%2520%2520Accurate%2520motion%2520estimation%2520for%2520tracking%2520deformable%2520tissues%2520in%250Aechocardiography%2520is%2520essential%2520for%2520precise%2520cardiac%2520function%2520measurements.%2520While%250Atraditional%2520methods%2520like%2520block%2520matching%2520or%2520optical%2520flow%2520struggle%2520with%2520intricate%250Acardiac%2520motion%252C%2520modern%2520point%2520tracking%2520approaches%2520remain%2520largely%2520underexplored%250Ain%2520this%2520domain.%2520This%2520work%2520investigates%2520the%2520potential%2520of%2520state-of-the-art%2520%2528SOTA%2529%250Apoint%2520tracking%2520methods%2520for%2520ultrasound%252C%2520with%2520a%2520focus%2520on%2520echocardiography.%250AAlthough%2520these%2520novel%2520approaches%2520demonstrate%2520strong%2520performance%2520in%2520general%250Avideos%252C%2520their%2520effectiveness%2520and%2520generalizability%2520in%2520echocardiography%2520remain%250Alimited.%2520By%2520analyzing%2520cardiac%2520motion%2520throughout%2520the%2520heart%2520cycle%2520in%2520real%2520B-mode%250Aultrasound%2520videos%252C%2520we%2520identify%2520that%2520a%2520directional%2520motion%2520bias%2520across%2520different%250Aviews%2520is%2520affecting%2520the%2520existing%2520training%2520strategies.%2520To%2520mitigate%2520this%252C%2520we%250Arefine%2520the%2520training%2520procedure%2520and%2520incorporate%2520a%2520set%2520of%2520tailored%2520augmentations%250Ato%2520reduce%2520the%2520bias%2520and%2520enhance%2520tracking%2520robustness%2520and%2520generalization%2520through%250Aimpartial%2520cardiac%2520motion.%2520We%2520also%2520propose%2520a%2520lightweight%2520network%2520leveraging%250Amulti-scale%2520cost%2520volumes%2520from%2520spatial%2520context%2520alone%2520to%2520challenge%2520the%2520advanced%250Aspatiotemporal%2520point%2520tracking%2520models.%2520Experiments%2520demonstrate%2520that%2520fine-tuning%250Awith%2520our%2520strategies%2520significantly%2520improves%2520models%2527%2520performances%2520over%2520their%250Abaselines%252C%2520even%2520for%2520out-of-distribution%2520%2528OOD%2529%2520cases.%2520For%2520instance%252C%2520EchoTracker%250Aboosts%2520overall%2520position%2520accuracy%2520by%252060.7%2525%2520and%2520reduces%2520median%2520trajectory%2520error%250Aby%252061.5%2525%2520across%2520heart%2520cycle%2520phases.%2520Interestingly%252C%2520several%2520point%2520tracking%250Amodels%2520fail%2520to%2520outperform%2520our%2520proposed%2520simple%2520model%2520in%2520terms%2520of%2520tracking%250Aaccuracy%2520and%2520generalization%252C%2520reflecting%2520their%2520limitations%2520when%2520applied%2520to%250Aechocardiography.%2520Nevertheless%252C%2520clinical%2520evaluation%2520reveals%2520that%2520these%2520methods%250Aimprove%2520GLS%2520measurements%252C%2520aligning%2520more%2520closely%2520with%2520expert-validated%252C%250Asemi-automated%2520tools%2520and%2520thus%2520demonstrating%2520better%2520reproducibility%2520in%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Modern%20Point%20Tracking%20for%20Speckle%20Tracking%20Echocardiography%20via%0A%20%20Impartial%20Motion&entry.906535625=Md%20Abulkalam%20Azad%20and%20John%20Nyberg%20and%20H%C3%A5vard%20Dalen%20and%20Bj%C3%B8rnar%20Grenne%20and%20Lasse%20Lovstakken%20and%20Andreas%20%C3%98stvik&entry.1292438233=%20%20Accurate%20motion%20estimation%20for%20tracking%20deformable%20tissues%20in%0Aechocardiography%20is%20essential%20for%20precise%20cardiac%20function%20measurements.%20While%0Atraditional%20methods%20like%20block%20matching%20or%20optical%20flow%20struggle%20with%20intricate%0Acardiac%20motion%2C%20modern%20point%20tracking%20approaches%20remain%20largely%20underexplored%0Ain%20this%20domain.%20This%20work%20investigates%20the%20potential%20of%20state-of-the-art%20%28SOTA%29%0Apoint%20tracking%20methods%20for%20ultrasound%2C%20with%20a%20focus%20on%20echocardiography.%0AAlthough%20these%20novel%20approaches%20demonstrate%20strong%20performance%20in%20general%0Avideos%2C%20their%20effectiveness%20and%20generalizability%20in%20echocardiography%20remain%0Alimited.%20By%20analyzing%20cardiac%20motion%20throughout%20the%20heart%20cycle%20in%20real%20B-mode%0Aultrasound%20videos%2C%20we%20identify%20that%20a%20directional%20motion%20bias%20across%20different%0Aviews%20is%20affecting%20the%20existing%20training%20strategies.%20To%20mitigate%20this%2C%20we%0Arefine%20the%20training%20procedure%20and%20incorporate%20a%20set%20of%20tailored%20augmentations%0Ato%20reduce%20the%20bias%20and%20enhance%20tracking%20robustness%20and%20generalization%20through%0Aimpartial%20cardiac%20motion.%20We%20also%20propose%20a%20lightweight%20network%20leveraging%0Amulti-scale%20cost%20volumes%20from%20spatial%20context%20alone%20to%20challenge%20the%20advanced%0Aspatiotemporal%20point%20tracking%20models.%20Experiments%20demonstrate%20that%20fine-tuning%0Awith%20our%20strategies%20significantly%20improves%20models%27%20performances%20over%20their%0Abaselines%2C%20even%20for%20out-of-distribution%20%28OOD%29%20cases.%20For%20instance%2C%20EchoTracker%0Aboosts%20overall%20position%20accuracy%20by%2060.7%25%20and%20reduces%20median%20trajectory%20error%0Aby%2061.5%25%20across%20heart%20cycle%20phases.%20Interestingly%2C%20several%20point%20tracking%0Amodels%20fail%20to%20outperform%20our%20proposed%20simple%20model%20in%20terms%20of%20tracking%0Aaccuracy%20and%20generalization%2C%20reflecting%20their%20limitations%20when%20applied%20to%0Aechocardiography.%20Nevertheless%2C%20clinical%20evaluation%20reveals%20that%20these%20methods%0Aimprove%20GLS%20measurements%2C%20aligning%20more%20closely%20with%20expert-validated%2C%0Asemi-automated%20tools%20and%20thus%20demonstrating%20better%20reproducibility%20in%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10127v1&entry.124074799=Read"},
{"title": "Efficient Federated Learning with Heterogeneous Data and Adaptive\n  Dropout", "author": "Ji Liu and Beichen Ma and Yang Zhou and Jingbo Zhou and Ruoming Jin and Dejing Dou and Huaiyu Dai and Haixun Wang and Patrick Valduriez", "abstract": "  Federated Learning (FL) is a promising distributed machine learning approach\nthat enables collaborative training of a global model using multiple edge\ndevices. The data distributed among the edge devices is highly heterogeneous.\nThus, FL faces the challenge of data distribution and heterogeneity, where\nnon-Independent and Identically Distributed (non-IID) data across edge devices\nmay yield in significant accuracy drop. Furthermore, the limited computation\nand communication capabilities of edge devices increase the likelihood of\nstragglers, thus leading to slow model convergence. In this paper, we propose\nthe FedDHAD FL framework, which comes with two novel methods: Dynamic\nHeterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH\ndynamically adjusts the weights of each local model within the model\naggregation process based on the non-IID degree of heterogeneous data to deal\nwith the statistical data heterogeneity. FedAD performs neuron-adaptive\noperations in response to heterogeneous devices to improve accuracy while\nachieving superb efficiency. The combination of these two methods makes FedDHAD\nsignificantly outperform state-of-the-art solutions in terms of accuracy (up to\n6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to\n15.0% smaller).\n", "link": "http://arxiv.org/abs/2507.10430v1", "date": "2025-07-14", "relevancy": 2.1096, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5542}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5162}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Federated%20Learning%20with%20Heterogeneous%20Data%20and%20Adaptive%0A%20%20Dropout&body=Title%3A%20Efficient%20Federated%20Learning%20with%20Heterogeneous%20Data%20and%20Adaptive%0A%20%20Dropout%0AAuthor%3A%20Ji%20Liu%20and%20Beichen%20Ma%20and%20Yang%20Zhou%20and%20Jingbo%20Zhou%20and%20Ruoming%20Jin%20and%20Dejing%20Dou%20and%20Huaiyu%20Dai%20and%20Haixun%20Wang%20and%20Patrick%20Valduriez%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20promising%20distributed%20machine%20learning%20approach%0Athat%20enables%20collaborative%20training%20of%20a%20global%20model%20using%20multiple%20edge%0Adevices.%20The%20data%20distributed%20among%20the%20edge%20devices%20is%20highly%20heterogeneous.%0AThus%2C%20FL%20faces%20the%20challenge%20of%20data%20distribution%20and%20heterogeneity%2C%20where%0Anon-Independent%20and%20Identically%20Distributed%20%28non-IID%29%20data%20across%20edge%20devices%0Amay%20yield%20in%20significant%20accuracy%20drop.%20Furthermore%2C%20the%20limited%20computation%0Aand%20communication%20capabilities%20of%20edge%20devices%20increase%20the%20likelihood%20of%0Astragglers%2C%20thus%20leading%20to%20slow%20model%20convergence.%20In%20this%20paper%2C%20we%20propose%0Athe%20FedDHAD%20FL%20framework%2C%20which%20comes%20with%20two%20novel%20methods%3A%20Dynamic%0AHeterogeneous%20model%20aggregation%20%28FedDH%29%20and%20Adaptive%20Dropout%20%28FedAD%29.%20FedDH%0Adynamically%20adjusts%20the%20weights%20of%20each%20local%20model%20within%20the%20model%0Aaggregation%20process%20based%20on%20the%20non-IID%20degree%20of%20heterogeneous%20data%20to%20deal%0Awith%20the%20statistical%20data%20heterogeneity.%20FedAD%20performs%20neuron-adaptive%0Aoperations%20in%20response%20to%20heterogeneous%20devices%20to%20improve%20accuracy%20while%0Aachieving%20superb%20efficiency.%20The%20combination%20of%20these%20two%20methods%20makes%20FedDHAD%0Asignificantly%20outperform%20state-of-the-art%20solutions%20in%20terms%20of%20accuracy%20%28up%20to%0A6.7%25%20higher%29%2C%20efficiency%20%28up%20to%202.02%20times%20faster%29%2C%20and%20computation%20cost%20%28up%20to%0A15.0%25%20smaller%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Federated%2520Learning%2520with%2520Heterogeneous%2520Data%2520and%2520Adaptive%250A%2520%2520Dropout%26entry.906535625%3DJi%2520Liu%2520and%2520Beichen%2520Ma%2520and%2520Yang%2520Zhou%2520and%2520Jingbo%2520Zhou%2520and%2520Ruoming%2520Jin%2520and%2520Dejing%2520Dou%2520and%2520Huaiyu%2520Dai%2520and%2520Haixun%2520Wang%2520and%2520Patrick%2520Valduriez%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520promising%2520distributed%2520machine%2520learning%2520approach%250Athat%2520enables%2520collaborative%2520training%2520of%2520a%2520global%2520model%2520using%2520multiple%2520edge%250Adevices.%2520The%2520data%2520distributed%2520among%2520the%2520edge%2520devices%2520is%2520highly%2520heterogeneous.%250AThus%252C%2520FL%2520faces%2520the%2520challenge%2520of%2520data%2520distribution%2520and%2520heterogeneity%252C%2520where%250Anon-Independent%2520and%2520Identically%2520Distributed%2520%2528non-IID%2529%2520data%2520across%2520edge%2520devices%250Amay%2520yield%2520in%2520significant%2520accuracy%2520drop.%2520Furthermore%252C%2520the%2520limited%2520computation%250Aand%2520communication%2520capabilities%2520of%2520edge%2520devices%2520increase%2520the%2520likelihood%2520of%250Astragglers%252C%2520thus%2520leading%2520to%2520slow%2520model%2520convergence.%2520In%2520this%2520paper%252C%2520we%2520propose%250Athe%2520FedDHAD%2520FL%2520framework%252C%2520which%2520comes%2520with%2520two%2520novel%2520methods%253A%2520Dynamic%250AHeterogeneous%2520model%2520aggregation%2520%2528FedDH%2529%2520and%2520Adaptive%2520Dropout%2520%2528FedAD%2529.%2520FedDH%250Adynamically%2520adjusts%2520the%2520weights%2520of%2520each%2520local%2520model%2520within%2520the%2520model%250Aaggregation%2520process%2520based%2520on%2520the%2520non-IID%2520degree%2520of%2520heterogeneous%2520data%2520to%2520deal%250Awith%2520the%2520statistical%2520data%2520heterogeneity.%2520FedAD%2520performs%2520neuron-adaptive%250Aoperations%2520in%2520response%2520to%2520heterogeneous%2520devices%2520to%2520improve%2520accuracy%2520while%250Aachieving%2520superb%2520efficiency.%2520The%2520combination%2520of%2520these%2520two%2520methods%2520makes%2520FedDHAD%250Asignificantly%2520outperform%2520state-of-the-art%2520solutions%2520in%2520terms%2520of%2520accuracy%2520%2528up%2520to%250A6.7%2525%2520higher%2529%252C%2520efficiency%2520%2528up%2520to%25202.02%2520times%2520faster%2529%252C%2520and%2520computation%2520cost%2520%2528up%2520to%250A15.0%2525%2520smaller%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Federated%20Learning%20with%20Heterogeneous%20Data%20and%20Adaptive%0A%20%20Dropout&entry.906535625=Ji%20Liu%20and%20Beichen%20Ma%20and%20Yang%20Zhou%20and%20Jingbo%20Zhou%20and%20Ruoming%20Jin%20and%20Dejing%20Dou%20and%20Huaiyu%20Dai%20and%20Haixun%20Wang%20and%20Patrick%20Valduriez&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20promising%20distributed%20machine%20learning%20approach%0Athat%20enables%20collaborative%20training%20of%20a%20global%20model%20using%20multiple%20edge%0Adevices.%20The%20data%20distributed%20among%20the%20edge%20devices%20is%20highly%20heterogeneous.%0AThus%2C%20FL%20faces%20the%20challenge%20of%20data%20distribution%20and%20heterogeneity%2C%20where%0Anon-Independent%20and%20Identically%20Distributed%20%28non-IID%29%20data%20across%20edge%20devices%0Amay%20yield%20in%20significant%20accuracy%20drop.%20Furthermore%2C%20the%20limited%20computation%0Aand%20communication%20capabilities%20of%20edge%20devices%20increase%20the%20likelihood%20of%0Astragglers%2C%20thus%20leading%20to%20slow%20model%20convergence.%20In%20this%20paper%2C%20we%20propose%0Athe%20FedDHAD%20FL%20framework%2C%20which%20comes%20with%20two%20novel%20methods%3A%20Dynamic%0AHeterogeneous%20model%20aggregation%20%28FedDH%29%20and%20Adaptive%20Dropout%20%28FedAD%29.%20FedDH%0Adynamically%20adjusts%20the%20weights%20of%20each%20local%20model%20within%20the%20model%0Aaggregation%20process%20based%20on%20the%20non-IID%20degree%20of%20heterogeneous%20data%20to%20deal%0Awith%20the%20statistical%20data%20heterogeneity.%20FedAD%20performs%20neuron-adaptive%0Aoperations%20in%20response%20to%20heterogeneous%20devices%20to%20improve%20accuracy%20while%0Aachieving%20superb%20efficiency.%20The%20combination%20of%20these%20two%20methods%20makes%20FedDHAD%0Asignificantly%20outperform%20state-of-the-art%20solutions%20in%20terms%20of%20accuracy%20%28up%20to%0A6.7%25%20higher%29%2C%20efficiency%20%28up%20to%202.02%20times%20faster%29%2C%20and%20computation%20cost%20%28up%20to%0A15.0%25%20smaller%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10430v1&entry.124074799=Read"},
{"title": "Disentangling Neural Disjunctive Normal Form Models", "author": "Kexin Gu Baugh and Vincent Perreault and Matthew Baugh and Luke Dickens and Katsumi Inoue and Alessandra Russo", "abstract": "  Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.\n", "link": "http://arxiv.org/abs/2507.10546v1", "date": "2025-07-14", "relevancy": 2.1092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Neural%20Disjunctive%20Normal%20Form%20Models&body=Title%3A%20Disentangling%20Neural%20Disjunctive%20Normal%20Form%20Models%0AAuthor%3A%20Kexin%20Gu%20Baugh%20and%20Vincent%20Perreault%20and%20Matthew%20Baugh%20and%20Luke%20Dickens%20and%20Katsumi%20Inoue%20and%20Alessandra%20Russo%0AAbstract%3A%20%20%20Neural%20Disjunctive%20Normal%20Form%20%28DNF%29%20based%20models%20are%20powerful%20and%0Ainterpretable%20approaches%20to%20neuro-symbolic%20learning%20and%20have%20shown%20promising%0Aresults%20in%20classification%20and%20reinforcement%20learning%20settings%20without%20prior%0Aknowledge%20of%20the%20tasks.%20However%2C%20their%20performance%20is%20degraded%20by%20the%0Athresholding%20of%20the%20post-training%20symbolic%20translation%20process.%20We%20show%20here%0Athat%20part%20of%20the%20performance%20degradation%20during%20translation%20is%20due%20to%20its%0Afailure%20to%20disentangle%20the%20learned%20knowledge%20represented%20in%20the%20form%20of%20the%0Anetworks%27%20weights.%20We%20address%20this%20issue%20by%20proposing%20a%20new%20disentanglement%0Amethod%3B%20by%20splitting%20nodes%20that%20encode%20nested%20rules%20into%20smaller%20independent%0Anodes%2C%20we%20are%20able%20to%20better%20preserve%20the%20models%27%20performance.%20Through%0Aexperiments%20on%20binary%2C%20multiclass%2C%20and%20multilabel%20classification%20tasks%0A%28including%20those%20requiring%20predicate%20invention%29%2C%20we%20demonstrate%20that%20our%0Adisentanglement%20method%20provides%20compact%20and%20interpretable%20logical%0Arepresentations%20for%20the%20neural%20DNF-based%20models%2C%20with%20performance%20closer%20to%0Athat%20of%20their%20pre-translation%20counterparts.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/kittykg/disentangling-ndnf-classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Neural%2520Disjunctive%2520Normal%2520Form%2520Models%26entry.906535625%3DKexin%2520Gu%2520Baugh%2520and%2520Vincent%2520Perreault%2520and%2520Matthew%2520Baugh%2520and%2520Luke%2520Dickens%2520and%2520Katsumi%2520Inoue%2520and%2520Alessandra%2520Russo%26entry.1292438233%3D%2520%2520Neural%2520Disjunctive%2520Normal%2520Form%2520%2528DNF%2529%2520based%2520models%2520are%2520powerful%2520and%250Ainterpretable%2520approaches%2520to%2520neuro-symbolic%2520learning%2520and%2520have%2520shown%2520promising%250Aresults%2520in%2520classification%2520and%2520reinforcement%2520learning%2520settings%2520without%2520prior%250Aknowledge%2520of%2520the%2520tasks.%2520However%252C%2520their%2520performance%2520is%2520degraded%2520by%2520the%250Athresholding%2520of%2520the%2520post-training%2520symbolic%2520translation%2520process.%2520We%2520show%2520here%250Athat%2520part%2520of%2520the%2520performance%2520degradation%2520during%2520translation%2520is%2520due%2520to%2520its%250Afailure%2520to%2520disentangle%2520the%2520learned%2520knowledge%2520represented%2520in%2520the%2520form%2520of%2520the%250Anetworks%2527%2520weights.%2520We%2520address%2520this%2520issue%2520by%2520proposing%2520a%2520new%2520disentanglement%250Amethod%253B%2520by%2520splitting%2520nodes%2520that%2520encode%2520nested%2520rules%2520into%2520smaller%2520independent%250Anodes%252C%2520we%2520are%2520able%2520to%2520better%2520preserve%2520the%2520models%2527%2520performance.%2520Through%250Aexperiments%2520on%2520binary%252C%2520multiclass%252C%2520and%2520multilabel%2520classification%2520tasks%250A%2528including%2520those%2520requiring%2520predicate%2520invention%2529%252C%2520we%2520demonstrate%2520that%2520our%250Adisentanglement%2520method%2520provides%2520compact%2520and%2520interpretable%2520logical%250Arepresentations%2520for%2520the%2520neural%2520DNF-based%2520models%252C%2520with%2520performance%2520closer%2520to%250Athat%2520of%2520their%2520pre-translation%2520counterparts.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/kittykg/disentangling-ndnf-classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Neural%20Disjunctive%20Normal%20Form%20Models&entry.906535625=Kexin%20Gu%20Baugh%20and%20Vincent%20Perreault%20and%20Matthew%20Baugh%20and%20Luke%20Dickens%20and%20Katsumi%20Inoue%20and%20Alessandra%20Russo&entry.1292438233=%20%20Neural%20Disjunctive%20Normal%20Form%20%28DNF%29%20based%20models%20are%20powerful%20and%0Ainterpretable%20approaches%20to%20neuro-symbolic%20learning%20and%20have%20shown%20promising%0Aresults%20in%20classification%20and%20reinforcement%20learning%20settings%20without%20prior%0Aknowledge%20of%20the%20tasks.%20However%2C%20their%20performance%20is%20degraded%20by%20the%0Athresholding%20of%20the%20post-training%20symbolic%20translation%20process.%20We%20show%20here%0Athat%20part%20of%20the%20performance%20degradation%20during%20translation%20is%20due%20to%20its%0Afailure%20to%20disentangle%20the%20learned%20knowledge%20represented%20in%20the%20form%20of%20the%0Anetworks%27%20weights.%20We%20address%20this%20issue%20by%20proposing%20a%20new%20disentanglement%0Amethod%3B%20by%20splitting%20nodes%20that%20encode%20nested%20rules%20into%20smaller%20independent%0Anodes%2C%20we%20are%20able%20to%20better%20preserve%20the%20models%27%20performance.%20Through%0Aexperiments%20on%20binary%2C%20multiclass%2C%20and%20multilabel%20classification%20tasks%0A%28including%20those%20requiring%20predicate%20invention%29%2C%20we%20demonstrate%20that%20our%0Adisentanglement%20method%20provides%20compact%20and%20interpretable%20logical%0Arepresentations%20for%20the%20neural%20DNF-based%20models%2C%20with%20performance%20closer%20to%0Athat%20of%20their%20pre-translation%20counterparts.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/kittykg/disentangling-ndnf-classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10546v1&entry.124074799=Read"},
{"title": "Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI", "author": "Jiangkai Wu and Zhiyuan Ren and Liming Liu and Xinggong Zhang", "abstract": "  AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.\n", "link": "http://arxiv.org/abs/2507.10510v1", "date": "2025-07-14", "relevancy": 2.1064, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5431}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5332}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chat%20with%20AI%3A%20The%20Surprising%20Turn%20of%20Real-time%20Video%20Communication%20from%0A%20%20Human%20to%20AI&body=Title%3A%20Chat%20with%20AI%3A%20The%20Surprising%20Turn%20of%20Real-time%20Video%20Communication%20from%0A%20%20Human%20to%20AI%0AAuthor%3A%20Jiangkai%20Wu%20and%20Zhiyuan%20Ren%20and%20Liming%20Liu%20and%20Xinggong%20Zhang%0AAbstract%3A%20%20%20AI%20Video%20Chat%20emerges%20as%20a%20new%20paradigm%20for%20Real-time%20Communication%20%28RTC%29%2C%0Awhere%20one%20peer%20is%20not%20a%20human%2C%20but%20a%20Multimodal%20Large%20Language%20Model%20%28MLLM%29.%0AThis%20makes%20interaction%20between%20humans%20and%20AI%20more%20intuitive%2C%20as%20if%20chatting%0Aface-to-face%20with%20a%20real%20person.%20However%2C%20this%20poses%20significant%20challenges%20to%0Alatency%2C%20because%20the%20MLLM%20inference%20takes%20up%20most%20of%20the%20response%20time%2C%20leaving%0Avery%20little%20time%20for%20video%20streaming.%20Due%20to%20network%20uncertainty%20and%0Ainstability%2C%20transmission%20latency%20becomes%20a%20critical%20bottleneck%20preventing%20AI%0Afrom%20being%20like%20a%20real%20person.%20To%20address%20this%2C%20we%20propose%20Artic%2C%20an%0AAI-oriented%20Real-time%20Communication%20framework%2C%20exploring%20the%20network%0Arequirement%20shift%20from%20%22humans%20watching%20video%22%20to%20%22AI%20understanding%20video%22.%20To%0Areduce%20bitrate%20dramatically%20while%20maintaining%20MLLM%20accuracy%2C%20we%20propose%0AContext-Aware%20Video%20Streaming%20that%20recognizes%20the%20importance%20of%20each%20video%0Aregion%20for%20chat%20and%20allocates%20bitrate%20almost%20exclusively%20to%20chat-important%0Aregions.%20To%20avoid%20packet%20retransmission%2C%20we%20propose%20Loss-Resilient%20Adaptive%0AFrame%20Rate%20that%20leverages%20previous%20frames%20to%20substitute%20for%20lost/delayed%20frames%0Awhile%20avoiding%20bitrate%20waste.%20To%20evaluate%20the%20impact%20of%20video%20streaming%20quality%0Aon%20MLLM%20accuracy%2C%20we%20build%20the%20first%20benchmark%2C%20named%20Degraded%20Video%0AUnderstanding%20Benchmark%20%28DeViBench%29.%20Finally%2C%20we%20discuss%20some%20open%20questions%0Aand%20ongoing%20solutions%20for%20AI%20Video%20Chat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChat%2520with%2520AI%253A%2520The%2520Surprising%2520Turn%2520of%2520Real-time%2520Video%2520Communication%2520from%250A%2520%2520Human%2520to%2520AI%26entry.906535625%3DJiangkai%2520Wu%2520and%2520Zhiyuan%2520Ren%2520and%2520Liming%2520Liu%2520and%2520Xinggong%2520Zhang%26entry.1292438233%3D%2520%2520AI%2520Video%2520Chat%2520emerges%2520as%2520a%2520new%2520paradigm%2520for%2520Real-time%2520Communication%2520%2528RTC%2529%252C%250Awhere%2520one%2520peer%2520is%2520not%2520a%2520human%252C%2520but%2520a%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529.%250AThis%2520makes%2520interaction%2520between%2520humans%2520and%2520AI%2520more%2520intuitive%252C%2520as%2520if%2520chatting%250Aface-to-face%2520with%2520a%2520real%2520person.%2520However%252C%2520this%2520poses%2520significant%2520challenges%2520to%250Alatency%252C%2520because%2520the%2520MLLM%2520inference%2520takes%2520up%2520most%2520of%2520the%2520response%2520time%252C%2520leaving%250Avery%2520little%2520time%2520for%2520video%2520streaming.%2520Due%2520to%2520network%2520uncertainty%2520and%250Ainstability%252C%2520transmission%2520latency%2520becomes%2520a%2520critical%2520bottleneck%2520preventing%2520AI%250Afrom%2520being%2520like%2520a%2520real%2520person.%2520To%2520address%2520this%252C%2520we%2520propose%2520Artic%252C%2520an%250AAI-oriented%2520Real-time%2520Communication%2520framework%252C%2520exploring%2520the%2520network%250Arequirement%2520shift%2520from%2520%2522humans%2520watching%2520video%2522%2520to%2520%2522AI%2520understanding%2520video%2522.%2520To%250Areduce%2520bitrate%2520dramatically%2520while%2520maintaining%2520MLLM%2520accuracy%252C%2520we%2520propose%250AContext-Aware%2520Video%2520Streaming%2520that%2520recognizes%2520the%2520importance%2520of%2520each%2520video%250Aregion%2520for%2520chat%2520and%2520allocates%2520bitrate%2520almost%2520exclusively%2520to%2520chat-important%250Aregions.%2520To%2520avoid%2520packet%2520retransmission%252C%2520we%2520propose%2520Loss-Resilient%2520Adaptive%250AFrame%2520Rate%2520that%2520leverages%2520previous%2520frames%2520to%2520substitute%2520for%2520lost/delayed%2520frames%250Awhile%2520avoiding%2520bitrate%2520waste.%2520To%2520evaluate%2520the%2520impact%2520of%2520video%2520streaming%2520quality%250Aon%2520MLLM%2520accuracy%252C%2520we%2520build%2520the%2520first%2520benchmark%252C%2520named%2520Degraded%2520Video%250AUnderstanding%2520Benchmark%2520%2528DeViBench%2529.%2520Finally%252C%2520we%2520discuss%2520some%2520open%2520questions%250Aand%2520ongoing%2520solutions%2520for%2520AI%2520Video%2520Chat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chat%20with%20AI%3A%20The%20Surprising%20Turn%20of%20Real-time%20Video%20Communication%20from%0A%20%20Human%20to%20AI&entry.906535625=Jiangkai%20Wu%20and%20Zhiyuan%20Ren%20and%20Liming%20Liu%20and%20Xinggong%20Zhang&entry.1292438233=%20%20AI%20Video%20Chat%20emerges%20as%20a%20new%20paradigm%20for%20Real-time%20Communication%20%28RTC%29%2C%0Awhere%20one%20peer%20is%20not%20a%20human%2C%20but%20a%20Multimodal%20Large%20Language%20Model%20%28MLLM%29.%0AThis%20makes%20interaction%20between%20humans%20and%20AI%20more%20intuitive%2C%20as%20if%20chatting%0Aface-to-face%20with%20a%20real%20person.%20However%2C%20this%20poses%20significant%20challenges%20to%0Alatency%2C%20because%20the%20MLLM%20inference%20takes%20up%20most%20of%20the%20response%20time%2C%20leaving%0Avery%20little%20time%20for%20video%20streaming.%20Due%20to%20network%20uncertainty%20and%0Ainstability%2C%20transmission%20latency%20becomes%20a%20critical%20bottleneck%20preventing%20AI%0Afrom%20being%20like%20a%20real%20person.%20To%20address%20this%2C%20we%20propose%20Artic%2C%20an%0AAI-oriented%20Real-time%20Communication%20framework%2C%20exploring%20the%20network%0Arequirement%20shift%20from%20%22humans%20watching%20video%22%20to%20%22AI%20understanding%20video%22.%20To%0Areduce%20bitrate%20dramatically%20while%20maintaining%20MLLM%20accuracy%2C%20we%20propose%0AContext-Aware%20Video%20Streaming%20that%20recognizes%20the%20importance%20of%20each%20video%0Aregion%20for%20chat%20and%20allocates%20bitrate%20almost%20exclusively%20to%20chat-important%0Aregions.%20To%20avoid%20packet%20retransmission%2C%20we%20propose%20Loss-Resilient%20Adaptive%0AFrame%20Rate%20that%20leverages%20previous%20frames%20to%20substitute%20for%20lost/delayed%20frames%0Awhile%20avoiding%20bitrate%20waste.%20To%20evaluate%20the%20impact%20of%20video%20streaming%20quality%0Aon%20MLLM%20accuracy%2C%20we%20build%20the%20first%20benchmark%2C%20named%20Degraded%20Video%0AUnderstanding%20Benchmark%20%28DeViBench%29.%20Finally%2C%20we%20discuss%20some%20open%20questions%0Aand%20ongoing%20solutions%20for%20AI%20Video%20Chat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10510v1&entry.124074799=Read"},
{"title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "author": "Yu Fan and Jingwei Ni and Jakob Merane and Etienne Salimbeni and Yang Tian and Yoan Hermstr\u00fcwer and Yinya Huang and Mubashara Akhtar and Florian Geering and Oliver Dreyer and Daniel Brunner and Markus Leippold and Mrinmaya Sachan and Alexander Stremitzer and Christoph Engel and Elliott Ash and Joel Niklaus", "abstract": "  Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/\n", "link": "http://arxiv.org/abs/2505.12864v3", "date": "2025-07-14", "relevancy": 2.0883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEXam%3A%20Benchmarking%20Legal%20Reasoning%20on%20340%20Law%20Exams&body=Title%3A%20LEXam%3A%20Benchmarking%20Legal%20Reasoning%20on%20340%20Law%20Exams%0AAuthor%3A%20Yu%20Fan%20and%20Jingwei%20Ni%20and%20Jakob%20Merane%20and%20Etienne%20Salimbeni%20and%20Yang%20Tian%20and%20Yoan%20Hermstr%C3%BCwer%20and%20Yinya%20Huang%20and%20Mubashara%20Akhtar%20and%20Florian%20Geering%20and%20Oliver%20Dreyer%20and%20Daniel%20Brunner%20and%20Markus%20Leippold%20and%20Mrinmaya%20Sachan%20and%20Alexander%20Stremitzer%20and%20Christoph%20Engel%20and%20Elliott%20Ash%20and%20Joel%20Niklaus%0AAbstract%3A%20%20%20Long-form%20legal%20reasoning%20remains%20a%20key%20challenge%20for%20large%20language%20models%0A%28LLMs%29%20in%20spite%20of%20recent%20advances%20in%20test-time%20scaling.%20We%20introduce%20LEXam%2C%20a%0Anovel%20benchmark%20derived%20from%20340%20law%20exams%20spanning%20116%20law%20school%20courses%0Aacross%20a%20range%20of%20subjects%20and%20degree%20levels.%20The%20dataset%20comprises%204%2C886%20law%0Aexam%20questions%20in%20English%20and%20German%2C%20including%202%2C841%20long-form%2C%20open-ended%0Aquestions%20and%202%2C045%20multiple-choice%20questions.%20Besides%20reference%20answers%2C%20the%0Aopen%20questions%20are%20also%20accompanied%20by%20explicit%20guidance%20outlining%20the%20expected%0Alegal%20reasoning%20approach%20such%20as%20issue%20spotting%2C%20rule%20recall%2C%20or%20rule%0Aapplication.%20Our%20evaluation%20on%20both%20open-ended%20and%20multiple-choice%20questions%0Apresent%20significant%20challenges%20for%20current%20LLMs%3B%20in%20particular%2C%20they%20notably%0Astruggle%20with%20open%20questions%20that%20require%20structured%2C%20multi-step%20legal%0Areasoning.%20Moreover%2C%20our%20results%20underscore%20the%20effectiveness%20of%20the%20dataset%20in%0Adifferentiating%20between%20models%20with%20varying%20capabilities.%20Adopting%20an%0ALLM-as-a-Judge%20paradigm%20with%20rigorous%20human%20expert%20validation%2C%20we%20demonstrate%0Ahow%20model-generated%20reasoning%20steps%20can%20be%20evaluated%20consistently%20and%0Aaccurately.%20Our%20evaluation%20setup%20provides%20a%20scalable%20method%20to%20assess%20legal%0Areasoning%20quality%20beyond%20simple%20accuracy%20metrics.%20Project%20page%3A%0Ahttps%3A//lexam-benchmark.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12864v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEXam%253A%2520Benchmarking%2520Legal%2520Reasoning%2520on%2520340%2520Law%2520Exams%26entry.906535625%3DYu%2520Fan%2520and%2520Jingwei%2520Ni%2520and%2520Jakob%2520Merane%2520and%2520Etienne%2520Salimbeni%2520and%2520Yang%2520Tian%2520and%2520Yoan%2520Hermstr%25C3%25BCwer%2520and%2520Yinya%2520Huang%2520and%2520Mubashara%2520Akhtar%2520and%2520Florian%2520Geering%2520and%2520Oliver%2520Dreyer%2520and%2520Daniel%2520Brunner%2520and%2520Markus%2520Leippold%2520and%2520Mrinmaya%2520Sachan%2520and%2520Alexander%2520Stremitzer%2520and%2520Christoph%2520Engel%2520and%2520Elliott%2520Ash%2520and%2520Joel%2520Niklaus%26entry.1292438233%3D%2520%2520Long-form%2520legal%2520reasoning%2520remains%2520a%2520key%2520challenge%2520for%2520large%2520language%2520models%250A%2528LLMs%2529%2520in%2520spite%2520of%2520recent%2520advances%2520in%2520test-time%2520scaling.%2520We%2520introduce%2520LEXam%252C%2520a%250Anovel%2520benchmark%2520derived%2520from%2520340%2520law%2520exams%2520spanning%2520116%2520law%2520school%2520courses%250Aacross%2520a%2520range%2520of%2520subjects%2520and%2520degree%2520levels.%2520The%2520dataset%2520comprises%25204%252C886%2520law%250Aexam%2520questions%2520in%2520English%2520and%2520German%252C%2520including%25202%252C841%2520long-form%252C%2520open-ended%250Aquestions%2520and%25202%252C045%2520multiple-choice%2520questions.%2520Besides%2520reference%2520answers%252C%2520the%250Aopen%2520questions%2520are%2520also%2520accompanied%2520by%2520explicit%2520guidance%2520outlining%2520the%2520expected%250Alegal%2520reasoning%2520approach%2520such%2520as%2520issue%2520spotting%252C%2520rule%2520recall%252C%2520or%2520rule%250Aapplication.%2520Our%2520evaluation%2520on%2520both%2520open-ended%2520and%2520multiple-choice%2520questions%250Apresent%2520significant%2520challenges%2520for%2520current%2520LLMs%253B%2520in%2520particular%252C%2520they%2520notably%250Astruggle%2520with%2520open%2520questions%2520that%2520require%2520structured%252C%2520multi-step%2520legal%250Areasoning.%2520Moreover%252C%2520our%2520results%2520underscore%2520the%2520effectiveness%2520of%2520the%2520dataset%2520in%250Adifferentiating%2520between%2520models%2520with%2520varying%2520capabilities.%2520Adopting%2520an%250ALLM-as-a-Judge%2520paradigm%2520with%2520rigorous%2520human%2520expert%2520validation%252C%2520we%2520demonstrate%250Ahow%2520model-generated%2520reasoning%2520steps%2520can%2520be%2520evaluated%2520consistently%2520and%250Aaccurately.%2520Our%2520evaluation%2520setup%2520provides%2520a%2520scalable%2520method%2520to%2520assess%2520legal%250Areasoning%2520quality%2520beyond%2520simple%2520accuracy%2520metrics.%2520Project%2520page%253A%250Ahttps%253A//lexam-benchmark.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12864v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEXam%3A%20Benchmarking%20Legal%20Reasoning%20on%20340%20Law%20Exams&entry.906535625=Yu%20Fan%20and%20Jingwei%20Ni%20and%20Jakob%20Merane%20and%20Etienne%20Salimbeni%20and%20Yang%20Tian%20and%20Yoan%20Hermstr%C3%BCwer%20and%20Yinya%20Huang%20and%20Mubashara%20Akhtar%20and%20Florian%20Geering%20and%20Oliver%20Dreyer%20and%20Daniel%20Brunner%20and%20Markus%20Leippold%20and%20Mrinmaya%20Sachan%20and%20Alexander%20Stremitzer%20and%20Christoph%20Engel%20and%20Elliott%20Ash%20and%20Joel%20Niklaus&entry.1292438233=%20%20Long-form%20legal%20reasoning%20remains%20a%20key%20challenge%20for%20large%20language%20models%0A%28LLMs%29%20in%20spite%20of%20recent%20advances%20in%20test-time%20scaling.%20We%20introduce%20LEXam%2C%20a%0Anovel%20benchmark%20derived%20from%20340%20law%20exams%20spanning%20116%20law%20school%20courses%0Aacross%20a%20range%20of%20subjects%20and%20degree%20levels.%20The%20dataset%20comprises%204%2C886%20law%0Aexam%20questions%20in%20English%20and%20German%2C%20including%202%2C841%20long-form%2C%20open-ended%0Aquestions%20and%202%2C045%20multiple-choice%20questions.%20Besides%20reference%20answers%2C%20the%0Aopen%20questions%20are%20also%20accompanied%20by%20explicit%20guidance%20outlining%20the%20expected%0Alegal%20reasoning%20approach%20such%20as%20issue%20spotting%2C%20rule%20recall%2C%20or%20rule%0Aapplication.%20Our%20evaluation%20on%20both%20open-ended%20and%20multiple-choice%20questions%0Apresent%20significant%20challenges%20for%20current%20LLMs%3B%20in%20particular%2C%20they%20notably%0Astruggle%20with%20open%20questions%20that%20require%20structured%2C%20multi-step%20legal%0Areasoning.%20Moreover%2C%20our%20results%20underscore%20the%20effectiveness%20of%20the%20dataset%20in%0Adifferentiating%20between%20models%20with%20varying%20capabilities.%20Adopting%20an%0ALLM-as-a-Judge%20paradigm%20with%20rigorous%20human%20expert%20validation%2C%20we%20demonstrate%0Ahow%20model-generated%20reasoning%20steps%20can%20be%20evaluated%20consistently%20and%0Aaccurately.%20Our%20evaluation%20setup%20provides%20a%20scalable%20method%20to%20assess%20legal%0Areasoning%20quality%20beyond%20simple%20accuracy%20metrics.%20Project%20page%3A%0Ahttps%3A//lexam-benchmark.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12864v3&entry.124074799=Read"},
{"title": "GO-VMP: Global Optimization for View Motion Planning in Fruit Mapping", "author": "Allen Isaac Jose and Sicong Pan and Tobias Zaenker and Rohit Menon and Sebastian Houben and Maren Bennewitz", "abstract": "  Automating labor-intensive tasks such as crop monitoring with robots is\nessential for enhancing production and conserving resources. However,\nautonomously monitoring horticulture crops remains challenging due to their\ncomplex structures, which often result in fruit occlusions. Existing view\nplanning methods attempt to reduce occlusions but either struggle to achieve\nadequate coverage or incur high robot motion costs. We introduce a global\noptimization approach for view motion planning that aims to minimize robot\nmotion costs while maximizing fruit coverage. To this end, we leverage coverage\nconstraints derived from the set covering problem (SCP) within a shortest\nHamiltonian path problem (SHPP) formulation. While both SCP and SHPP are\nwell-established, their tailored integration enables a unified framework that\ncomputes a global view path with minimized motion while ensuring full coverage\nof selected targets. Given the NP-hard nature of the problem, we employ a\nregion-prior-based selection of coverage targets and a sparse graph structure\nto achieve effective optimization outcomes within a limited time. Experiments\nin simulation demonstrate that our method detects more fruits, enhances surface\ncoverage, and achieves higher volume accuracy than the motion-efficient\nbaseline with a moderate increase in motion cost, while significantly reducing\nmotion costs compared to the coverage-focused baseline. Real-world experiments\nfurther confirm the practical applicability of our approach.\n", "link": "http://arxiv.org/abs/2503.03912v2", "date": "2025-07-14", "relevancy": 2.0838, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5405}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5319}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GO-VMP%3A%20Global%20Optimization%20for%20View%20Motion%20Planning%20in%20Fruit%20Mapping&body=Title%3A%20GO-VMP%3A%20Global%20Optimization%20for%20View%20Motion%20Planning%20in%20Fruit%20Mapping%0AAuthor%3A%20Allen%20Isaac%20Jose%20and%20Sicong%20Pan%20and%20Tobias%20Zaenker%20and%20Rohit%20Menon%20and%20Sebastian%20Houben%20and%20Maren%20Bennewitz%0AAbstract%3A%20%20%20Automating%20labor-intensive%20tasks%20such%20as%20crop%20monitoring%20with%20robots%20is%0Aessential%20for%20enhancing%20production%20and%20conserving%20resources.%20However%2C%0Aautonomously%20monitoring%20horticulture%20crops%20remains%20challenging%20due%20to%20their%0Acomplex%20structures%2C%20which%20often%20result%20in%20fruit%20occlusions.%20Existing%20view%0Aplanning%20methods%20attempt%20to%20reduce%20occlusions%20but%20either%20struggle%20to%20achieve%0Aadequate%20coverage%20or%20incur%20high%20robot%20motion%20costs.%20We%20introduce%20a%20global%0Aoptimization%20approach%20for%20view%20motion%20planning%20that%20aims%20to%20minimize%20robot%0Amotion%20costs%20while%20maximizing%20fruit%20coverage.%20To%20this%20end%2C%20we%20leverage%20coverage%0Aconstraints%20derived%20from%20the%20set%20covering%20problem%20%28SCP%29%20within%20a%20shortest%0AHamiltonian%20path%20problem%20%28SHPP%29%20formulation.%20While%20both%20SCP%20and%20SHPP%20are%0Awell-established%2C%20their%20tailored%20integration%20enables%20a%20unified%20framework%20that%0Acomputes%20a%20global%20view%20path%20with%20minimized%20motion%20while%20ensuring%20full%20coverage%0Aof%20selected%20targets.%20Given%20the%20NP-hard%20nature%20of%20the%20problem%2C%20we%20employ%20a%0Aregion-prior-based%20selection%20of%20coverage%20targets%20and%20a%20sparse%20graph%20structure%0Ato%20achieve%20effective%20optimization%20outcomes%20within%20a%20limited%20time.%20Experiments%0Ain%20simulation%20demonstrate%20that%20our%20method%20detects%20more%20fruits%2C%20enhances%20surface%0Acoverage%2C%20and%20achieves%20higher%20volume%20accuracy%20than%20the%20motion-efficient%0Abaseline%20with%20a%20moderate%20increase%20in%20motion%20cost%2C%20while%20significantly%20reducing%0Amotion%20costs%20compared%20to%20the%20coverage-focused%20baseline.%20Real-world%20experiments%0Afurther%20confirm%20the%20practical%20applicability%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGO-VMP%253A%2520Global%2520Optimization%2520for%2520View%2520Motion%2520Planning%2520in%2520Fruit%2520Mapping%26entry.906535625%3DAllen%2520Isaac%2520Jose%2520and%2520Sicong%2520Pan%2520and%2520Tobias%2520Zaenker%2520and%2520Rohit%2520Menon%2520and%2520Sebastian%2520Houben%2520and%2520Maren%2520Bennewitz%26entry.1292438233%3D%2520%2520Automating%2520labor-intensive%2520tasks%2520such%2520as%2520crop%2520monitoring%2520with%2520robots%2520is%250Aessential%2520for%2520enhancing%2520production%2520and%2520conserving%2520resources.%2520However%252C%250Aautonomously%2520monitoring%2520horticulture%2520crops%2520remains%2520challenging%2520due%2520to%2520their%250Acomplex%2520structures%252C%2520which%2520often%2520result%2520in%2520fruit%2520occlusions.%2520Existing%2520view%250Aplanning%2520methods%2520attempt%2520to%2520reduce%2520occlusions%2520but%2520either%2520struggle%2520to%2520achieve%250Aadequate%2520coverage%2520or%2520incur%2520high%2520robot%2520motion%2520costs.%2520We%2520introduce%2520a%2520global%250Aoptimization%2520approach%2520for%2520view%2520motion%2520planning%2520that%2520aims%2520to%2520minimize%2520robot%250Amotion%2520costs%2520while%2520maximizing%2520fruit%2520coverage.%2520To%2520this%2520end%252C%2520we%2520leverage%2520coverage%250Aconstraints%2520derived%2520from%2520the%2520set%2520covering%2520problem%2520%2528SCP%2529%2520within%2520a%2520shortest%250AHamiltonian%2520path%2520problem%2520%2528SHPP%2529%2520formulation.%2520While%2520both%2520SCP%2520and%2520SHPP%2520are%250Awell-established%252C%2520their%2520tailored%2520integration%2520enables%2520a%2520unified%2520framework%2520that%250Acomputes%2520a%2520global%2520view%2520path%2520with%2520minimized%2520motion%2520while%2520ensuring%2520full%2520coverage%250Aof%2520selected%2520targets.%2520Given%2520the%2520NP-hard%2520nature%2520of%2520the%2520problem%252C%2520we%2520employ%2520a%250Aregion-prior-based%2520selection%2520of%2520coverage%2520targets%2520and%2520a%2520sparse%2520graph%2520structure%250Ato%2520achieve%2520effective%2520optimization%2520outcomes%2520within%2520a%2520limited%2520time.%2520Experiments%250Ain%2520simulation%2520demonstrate%2520that%2520our%2520method%2520detects%2520more%2520fruits%252C%2520enhances%2520surface%250Acoverage%252C%2520and%2520achieves%2520higher%2520volume%2520accuracy%2520than%2520the%2520motion-efficient%250Abaseline%2520with%2520a%2520moderate%2520increase%2520in%2520motion%2520cost%252C%2520while%2520significantly%2520reducing%250Amotion%2520costs%2520compared%2520to%2520the%2520coverage-focused%2520baseline.%2520Real-world%2520experiments%250Afurther%2520confirm%2520the%2520practical%2520applicability%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GO-VMP%3A%20Global%20Optimization%20for%20View%20Motion%20Planning%20in%20Fruit%20Mapping&entry.906535625=Allen%20Isaac%20Jose%20and%20Sicong%20Pan%20and%20Tobias%20Zaenker%20and%20Rohit%20Menon%20and%20Sebastian%20Houben%20and%20Maren%20Bennewitz&entry.1292438233=%20%20Automating%20labor-intensive%20tasks%20such%20as%20crop%20monitoring%20with%20robots%20is%0Aessential%20for%20enhancing%20production%20and%20conserving%20resources.%20However%2C%0Aautonomously%20monitoring%20horticulture%20crops%20remains%20challenging%20due%20to%20their%0Acomplex%20structures%2C%20which%20often%20result%20in%20fruit%20occlusions.%20Existing%20view%0Aplanning%20methods%20attempt%20to%20reduce%20occlusions%20but%20either%20struggle%20to%20achieve%0Aadequate%20coverage%20or%20incur%20high%20robot%20motion%20costs.%20We%20introduce%20a%20global%0Aoptimization%20approach%20for%20view%20motion%20planning%20that%20aims%20to%20minimize%20robot%0Amotion%20costs%20while%20maximizing%20fruit%20coverage.%20To%20this%20end%2C%20we%20leverage%20coverage%0Aconstraints%20derived%20from%20the%20set%20covering%20problem%20%28SCP%29%20within%20a%20shortest%0AHamiltonian%20path%20problem%20%28SHPP%29%20formulation.%20While%20both%20SCP%20and%20SHPP%20are%0Awell-established%2C%20their%20tailored%20integration%20enables%20a%20unified%20framework%20that%0Acomputes%20a%20global%20view%20path%20with%20minimized%20motion%20while%20ensuring%20full%20coverage%0Aof%20selected%20targets.%20Given%20the%20NP-hard%20nature%20of%20the%20problem%2C%20we%20employ%20a%0Aregion-prior-based%20selection%20of%20coverage%20targets%20and%20a%20sparse%20graph%20structure%0Ato%20achieve%20effective%20optimization%20outcomes%20within%20a%20limited%20time.%20Experiments%0Ain%20simulation%20demonstrate%20that%20our%20method%20detects%20more%20fruits%2C%20enhances%20surface%0Acoverage%2C%20and%20achieves%20higher%20volume%20accuracy%20than%20the%20motion-efficient%0Abaseline%20with%20a%20moderate%20increase%20in%20motion%20cost%2C%20while%20significantly%20reducing%0Amotion%20costs%20compared%20to%20the%20coverage-focused%20baseline.%20Real-world%20experiments%0Afurther%20confirm%20the%20practical%20applicability%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03912v2&entry.124074799=Read"},
{"title": "Expert-level validation of AI-generated medical text with scalable\n  language models", "author": "Asad Aali and Vasiliki Bikia and Maya Varma and Nicole Chiou and Sophie Ostmeier and Arnav Singhvi and Magdalini Paschali and Ashwin Kumar and Andrew Johnston and Karimar Amador-Martinez and Eduardo Juan Perez Guerrero and Paola Naovi Cruz Rivera and Sergios Gatidis and Christian Bluethgen and Eduardo Pontes Reis and Eddy D. Zandee van Rilland and Poonam Laxmappa Hosamani and Kevin R Keet and Minjoung Go and Evelyn Ling and David B. Larson and Curtis Langlotz and Roxana Daneshjou and Jason Hom and Sanmi Koyejo and Emily Alsentzer and Akshay S. Chaudhari", "abstract": "  With the growing use of language models (LMs) in clinical environments, there\nis an immediate need to evaluate the accuracy and safety of LM-generated\nmedical text. Currently, such evaluation relies solely on manual physician\nreview. However, detecting errors in LM-generated text is challenging because\n1) manual review is costly and 2) expert-composed reference outputs are often\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\nsubtle but clinically significant errors. To address these challenges, we\npropose MedVAL, a self-supervised framework that leverages synthetic data to\ntrain evaluator LMs to assess whether LM-generated medical outputs are\nfactually consistent with inputs, without requiring physician labels or\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\ndataset containing 840 outputs annotated by physicians, following a\nphysician-defined taxonomy of risk levels and error categories. Across 6\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\nincreasing average F1 scores from 66% to 83%, with per-sample safety\nclassification scores up to 86%. MedVAL improves the performance of even the\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\n(https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench\n(https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B\n(https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performing\nopen-source LM. Our research provides the first evidence of LMs approaching\nexpert-level validation ability for medical text.\n", "link": "http://arxiv.org/abs/2507.03152v2", "date": "2025-07-14", "relevancy": 2.0786, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expert-level%20validation%20of%20AI-generated%20medical%20text%20with%20scalable%0A%20%20language%20models&body=Title%3A%20Expert-level%20validation%20of%20AI-generated%20medical%20text%20with%20scalable%0A%20%20language%20models%0AAuthor%3A%20Asad%20Aali%20and%20Vasiliki%20Bikia%20and%20Maya%20Varma%20and%20Nicole%20Chiou%20and%20Sophie%20Ostmeier%20and%20Arnav%20Singhvi%20and%20Magdalini%20Paschali%20and%20Ashwin%20Kumar%20and%20Andrew%20Johnston%20and%20Karimar%20Amador-Martinez%20and%20Eduardo%20Juan%20Perez%20Guerrero%20and%20Paola%20Naovi%20Cruz%20Rivera%20and%20Sergios%20Gatidis%20and%20Christian%20Bluethgen%20and%20Eduardo%20Pontes%20Reis%20and%20Eddy%20D.%20Zandee%20van%20Rilland%20and%20Poonam%20Laxmappa%20Hosamani%20and%20Kevin%20R%20Keet%20and%20Minjoung%20Go%20and%20Evelyn%20Ling%20and%20David%20B.%20Larson%20and%20Curtis%20Langlotz%20and%20Roxana%20Daneshjou%20and%20Jason%20Hom%20and%20Sanmi%20Koyejo%20and%20Emily%20Alsentzer%20and%20Akshay%20S.%20Chaudhari%0AAbstract%3A%20%20%20With%20the%20growing%20use%20of%20language%20models%20%28LMs%29%20in%20clinical%20environments%2C%20there%0Ais%20an%20immediate%20need%20to%20evaluate%20the%20accuracy%20and%20safety%20of%20LM-generated%0Amedical%20text.%20Currently%2C%20such%20evaluation%20relies%20solely%20on%20manual%20physician%0Areview.%20However%2C%20detecting%20errors%20in%20LM-generated%20text%20is%20challenging%20because%0A1%29%20manual%20review%20is%20costly%20and%202%29%20expert-composed%20reference%20outputs%20are%20often%0Aunavailable%20in%20real-world%20settings.%20While%20the%20%22LM-as-judge%22%20paradigm%20%28a%20LM%0Aevaluating%20another%20LM%29%20offers%20scalable%20evaluation%2C%20even%20frontier%20LMs%20can%20miss%0Asubtle%20but%20clinically%20significant%20errors.%20To%20address%20these%20challenges%2C%20we%0Apropose%20MedVAL%2C%20a%20self-supervised%20framework%20that%20leverages%20synthetic%20data%20to%0Atrain%20evaluator%20LMs%20to%20assess%20whether%20LM-generated%20medical%20outputs%20are%0Afactually%20consistent%20with%20inputs%2C%20without%20requiring%20physician%20labels%20or%0Areference%20outputs.%20To%20evaluate%20LM%20performance%2C%20we%20introduce%20MedVAL-Bench%2C%20a%0Adataset%20containing%20840%20outputs%20annotated%20by%20physicians%2C%20following%20a%0Aphysician-defined%20taxonomy%20of%20risk%20levels%20and%20error%20categories.%20Across%206%0Adiverse%20medical%20tasks%20and%2010%20state-of-the-art%20LMs%20spanning%20open-source%2C%0Aproprietary%2C%20and%20medically%20adapted%20models%2C%20MedVAL%20fine-tuning%20significantly%0Aimproves%20%28p%20%3C%200.001%29%20alignment%20with%20physicians%20on%20both%20seen%20and%20unseen%20tasks%2C%0Aincreasing%20average%20F1%20scores%20from%2066%25%20to%2083%25%2C%20with%20per-sample%20safety%0Aclassification%20scores%20up%20to%2086%25.%20MedVAL%20improves%20the%20performance%20of%20even%20the%0Abest-performing%20proprietary%20LM%20%28GPT-4o%29%20by%208%25.%20To%20support%20a%20scalable%2C%0Arisk-aware%20pathway%20towards%20clinical%20integration%2C%20we%20open-source%20the%201%29%20codebase%0A%28https%3A//github.com/StanfordMIMI/MedVAL%29%2C%202%29%20MedVAL-Bench%0A%28https%3A//huggingface.co/datasets/stanfordmimi/MedVAL-Bench%29%2C%20and%203%29%20MedVAL-4B%0A%28https%3A//huggingface.co/stanfordmimi/MedVAL-4B%29%2C%20the%20best-performing%0Aopen-source%20LM.%20Our%20research%20provides%20the%20first%20evidence%20of%20LMs%20approaching%0Aexpert-level%20validation%20ability%20for%20medical%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03152v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpert-level%2520validation%2520of%2520AI-generated%2520medical%2520text%2520with%2520scalable%250A%2520%2520language%2520models%26entry.906535625%3DAsad%2520Aali%2520and%2520Vasiliki%2520Bikia%2520and%2520Maya%2520Varma%2520and%2520Nicole%2520Chiou%2520and%2520Sophie%2520Ostmeier%2520and%2520Arnav%2520Singhvi%2520and%2520Magdalini%2520Paschali%2520and%2520Ashwin%2520Kumar%2520and%2520Andrew%2520Johnston%2520and%2520Karimar%2520Amador-Martinez%2520and%2520Eduardo%2520Juan%2520Perez%2520Guerrero%2520and%2520Paola%2520Naovi%2520Cruz%2520Rivera%2520and%2520Sergios%2520Gatidis%2520and%2520Christian%2520Bluethgen%2520and%2520Eduardo%2520Pontes%2520Reis%2520and%2520Eddy%2520D.%2520Zandee%2520van%2520Rilland%2520and%2520Poonam%2520Laxmappa%2520Hosamani%2520and%2520Kevin%2520R%2520Keet%2520and%2520Minjoung%2520Go%2520and%2520Evelyn%2520Ling%2520and%2520David%2520B.%2520Larson%2520and%2520Curtis%2520Langlotz%2520and%2520Roxana%2520Daneshjou%2520and%2520Jason%2520Hom%2520and%2520Sanmi%2520Koyejo%2520and%2520Emily%2520Alsentzer%2520and%2520Akshay%2520S.%2520Chaudhari%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520use%2520of%2520language%2520models%2520%2528LMs%2529%2520in%2520clinical%2520environments%252C%2520there%250Ais%2520an%2520immediate%2520need%2520to%2520evaluate%2520the%2520accuracy%2520and%2520safety%2520of%2520LM-generated%250Amedical%2520text.%2520Currently%252C%2520such%2520evaluation%2520relies%2520solely%2520on%2520manual%2520physician%250Areview.%2520However%252C%2520detecting%2520errors%2520in%2520LM-generated%2520text%2520is%2520challenging%2520because%250A1%2529%2520manual%2520review%2520is%2520costly%2520and%25202%2529%2520expert-composed%2520reference%2520outputs%2520are%2520often%250Aunavailable%2520in%2520real-world%2520settings.%2520While%2520the%2520%2522LM-as-judge%2522%2520paradigm%2520%2528a%2520LM%250Aevaluating%2520another%2520LM%2529%2520offers%2520scalable%2520evaluation%252C%2520even%2520frontier%2520LMs%2520can%2520miss%250Asubtle%2520but%2520clinically%2520significant%2520errors.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520MedVAL%252C%2520a%2520self-supervised%2520framework%2520that%2520leverages%2520synthetic%2520data%2520to%250Atrain%2520evaluator%2520LMs%2520to%2520assess%2520whether%2520LM-generated%2520medical%2520outputs%2520are%250Afactually%2520consistent%2520with%2520inputs%252C%2520without%2520requiring%2520physician%2520labels%2520or%250Areference%2520outputs.%2520To%2520evaluate%2520LM%2520performance%252C%2520we%2520introduce%2520MedVAL-Bench%252C%2520a%250Adataset%2520containing%2520840%2520outputs%2520annotated%2520by%2520physicians%252C%2520following%2520a%250Aphysician-defined%2520taxonomy%2520of%2520risk%2520levels%2520and%2520error%2520categories.%2520Across%25206%250Adiverse%2520medical%2520tasks%2520and%252010%2520state-of-the-art%2520LMs%2520spanning%2520open-source%252C%250Aproprietary%252C%2520and%2520medically%2520adapted%2520models%252C%2520MedVAL%2520fine-tuning%2520significantly%250Aimproves%2520%2528p%2520%253C%25200.001%2529%2520alignment%2520with%2520physicians%2520on%2520both%2520seen%2520and%2520unseen%2520tasks%252C%250Aincreasing%2520average%2520F1%2520scores%2520from%252066%2525%2520to%252083%2525%252C%2520with%2520per-sample%2520safety%250Aclassification%2520scores%2520up%2520to%252086%2525.%2520MedVAL%2520improves%2520the%2520performance%2520of%2520even%2520the%250Abest-performing%2520proprietary%2520LM%2520%2528GPT-4o%2529%2520by%25208%2525.%2520To%2520support%2520a%2520scalable%252C%250Arisk-aware%2520pathway%2520towards%2520clinical%2520integration%252C%2520we%2520open-source%2520the%25201%2529%2520codebase%250A%2528https%253A//github.com/StanfordMIMI/MedVAL%2529%252C%25202%2529%2520MedVAL-Bench%250A%2528https%253A//huggingface.co/datasets/stanfordmimi/MedVAL-Bench%2529%252C%2520and%25203%2529%2520MedVAL-4B%250A%2528https%253A//huggingface.co/stanfordmimi/MedVAL-4B%2529%252C%2520the%2520best-performing%250Aopen-source%2520LM.%2520Our%2520research%2520provides%2520the%2520first%2520evidence%2520of%2520LMs%2520approaching%250Aexpert-level%2520validation%2520ability%2520for%2520medical%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03152v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expert-level%20validation%20of%20AI-generated%20medical%20text%20with%20scalable%0A%20%20language%20models&entry.906535625=Asad%20Aali%20and%20Vasiliki%20Bikia%20and%20Maya%20Varma%20and%20Nicole%20Chiou%20and%20Sophie%20Ostmeier%20and%20Arnav%20Singhvi%20and%20Magdalini%20Paschali%20and%20Ashwin%20Kumar%20and%20Andrew%20Johnston%20and%20Karimar%20Amador-Martinez%20and%20Eduardo%20Juan%20Perez%20Guerrero%20and%20Paola%20Naovi%20Cruz%20Rivera%20and%20Sergios%20Gatidis%20and%20Christian%20Bluethgen%20and%20Eduardo%20Pontes%20Reis%20and%20Eddy%20D.%20Zandee%20van%20Rilland%20and%20Poonam%20Laxmappa%20Hosamani%20and%20Kevin%20R%20Keet%20and%20Minjoung%20Go%20and%20Evelyn%20Ling%20and%20David%20B.%20Larson%20and%20Curtis%20Langlotz%20and%20Roxana%20Daneshjou%20and%20Jason%20Hom%20and%20Sanmi%20Koyejo%20and%20Emily%20Alsentzer%20and%20Akshay%20S.%20Chaudhari&entry.1292438233=%20%20With%20the%20growing%20use%20of%20language%20models%20%28LMs%29%20in%20clinical%20environments%2C%20there%0Ais%20an%20immediate%20need%20to%20evaluate%20the%20accuracy%20and%20safety%20of%20LM-generated%0Amedical%20text.%20Currently%2C%20such%20evaluation%20relies%20solely%20on%20manual%20physician%0Areview.%20However%2C%20detecting%20errors%20in%20LM-generated%20text%20is%20challenging%20because%0A1%29%20manual%20review%20is%20costly%20and%202%29%20expert-composed%20reference%20outputs%20are%20often%0Aunavailable%20in%20real-world%20settings.%20While%20the%20%22LM-as-judge%22%20paradigm%20%28a%20LM%0Aevaluating%20another%20LM%29%20offers%20scalable%20evaluation%2C%20even%20frontier%20LMs%20can%20miss%0Asubtle%20but%20clinically%20significant%20errors.%20To%20address%20these%20challenges%2C%20we%0Apropose%20MedVAL%2C%20a%20self-supervised%20framework%20that%20leverages%20synthetic%20data%20to%0Atrain%20evaluator%20LMs%20to%20assess%20whether%20LM-generated%20medical%20outputs%20are%0Afactually%20consistent%20with%20inputs%2C%20without%20requiring%20physician%20labels%20or%0Areference%20outputs.%20To%20evaluate%20LM%20performance%2C%20we%20introduce%20MedVAL-Bench%2C%20a%0Adataset%20containing%20840%20outputs%20annotated%20by%20physicians%2C%20following%20a%0Aphysician-defined%20taxonomy%20of%20risk%20levels%20and%20error%20categories.%20Across%206%0Adiverse%20medical%20tasks%20and%2010%20state-of-the-art%20LMs%20spanning%20open-source%2C%0Aproprietary%2C%20and%20medically%20adapted%20models%2C%20MedVAL%20fine-tuning%20significantly%0Aimproves%20%28p%20%3C%200.001%29%20alignment%20with%20physicians%20on%20both%20seen%20and%20unseen%20tasks%2C%0Aincreasing%20average%20F1%20scores%20from%2066%25%20to%2083%25%2C%20with%20per-sample%20safety%0Aclassification%20scores%20up%20to%2086%25.%20MedVAL%20improves%20the%20performance%20of%20even%20the%0Abest-performing%20proprietary%20LM%20%28GPT-4o%29%20by%208%25.%20To%20support%20a%20scalable%2C%0Arisk-aware%20pathway%20towards%20clinical%20integration%2C%20we%20open-source%20the%201%29%20codebase%0A%28https%3A//github.com/StanfordMIMI/MedVAL%29%2C%202%29%20MedVAL-Bench%0A%28https%3A//huggingface.co/datasets/stanfordmimi/MedVAL-Bench%29%2C%20and%203%29%20MedVAL-4B%0A%28https%3A//huggingface.co/stanfordmimi/MedVAL-4B%29%2C%20the%20best-performing%0Aopen-source%20LM.%20Our%20research%20provides%20the%20first%20evidence%20of%20LMs%20approaching%0Aexpert-level%20validation%20ability%20for%20medical%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03152v2&entry.124074799=Read"},
{"title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning", "author": "Zhaohui Yang and Yuxiao Ye and Shilei Jiang and Chen Hu and Linjing Li and Shihong Deng and Daxin Jiang", "abstract": "  Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.\n", "link": "http://arxiv.org/abs/2505.14403v3", "date": "2025-07-14", "relevancy": 2.0763, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unearthing%20Gems%20from%20Stones%3A%20Policy%20Optimization%20with%20Negative%20Sample%0A%20%20Augmentation%20for%20LLM%20Reasoning&body=Title%3A%20Unearthing%20Gems%20from%20Stones%3A%20Policy%20Optimization%20with%20Negative%20Sample%0A%20%20Augmentation%20for%20LLM%20Reasoning%0AAuthor%3A%20Zhaohui%20Yang%20and%20Yuxiao%20Ye%20and%20Shilei%20Jiang%20and%20Chen%20Hu%20and%20Linjing%20Li%20and%20Shihong%20Deng%20and%20Daxin%20Jiang%0AAbstract%3A%20%20%20Recent%20advances%20in%20reasoning%20language%20models%20have%20witnessed%20a%20paradigm%20shift%0Afrom%20short%20to%20long%20CoT%20pattern.%20Given%20the%20substantial%20computational%20cost%20of%0Arollouts%20in%20long%20CoT%20models%2C%20maximizing%20the%20utility%20of%20fixed%20training%20datasets%0Abecomes%20crucial.%20Our%20analysis%20reveals%20that%20negative%20responses%20contain%20valuable%0Acomponents%20such%20as%20self-reflection%20and%20error-correction%20steps%2C%20yet%20primary%0Aexisting%20methods%20either%20completely%20discard%20negative%20samples%20%28RFT%29%20or%20apply%0Aequal%20penalization%20across%20all%20tokens%20%28RL%29%2C%20failing%20to%20leverage%20these%20potential%0Alearning%20signals.%20In%20light%20of%20this%2C%20we%20propose%20Behavior%20Constrained%20Policy%0AGradient%20with%20Negative%20Sample%20Augmentation%20%28BCPG-NSA%29%2C%20a%20fine-grained%20offline%0ARL%20framework%20that%20encompasses%20three%20stages%3A%201%29%20sample%20segmentation%2C%202%29%0Aconsensus-based%20step%20correctness%20assessment%20combining%20LLM%20and%20PRM%20judgers%2C%20and%0A3%29%20policy%20optimization%20with%20NSA%20designed%20to%20effectively%20mine%20positive%20steps%0Awithin%20negative%20samples.%20Experimental%20results%20show%20that%20BCPG-NSA%20outperforms%0Abaselines%20on%20several%20challenging%20math/coding%20reasoning%20benchmarks%20using%20the%0Asame%20training%20dataset%2C%20achieving%20improved%20sample%20efficiency%20and%20demonstrating%0Arobustness%20and%20scalability%20when%20extended%20to%20multiple%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14403v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnearthing%2520Gems%2520from%2520Stones%253A%2520Policy%2520Optimization%2520with%2520Negative%2520Sample%250A%2520%2520Augmentation%2520for%2520LLM%2520Reasoning%26entry.906535625%3DZhaohui%2520Yang%2520and%2520Yuxiao%2520Ye%2520and%2520Shilei%2520Jiang%2520and%2520Chen%2520Hu%2520and%2520Linjing%2520Li%2520and%2520Shihong%2520Deng%2520and%2520Daxin%2520Jiang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reasoning%2520language%2520models%2520have%2520witnessed%2520a%2520paradigm%2520shift%250Afrom%2520short%2520to%2520long%2520CoT%2520pattern.%2520Given%2520the%2520substantial%2520computational%2520cost%2520of%250Arollouts%2520in%2520long%2520CoT%2520models%252C%2520maximizing%2520the%2520utility%2520of%2520fixed%2520training%2520datasets%250Abecomes%2520crucial.%2520Our%2520analysis%2520reveals%2520that%2520negative%2520responses%2520contain%2520valuable%250Acomponents%2520such%2520as%2520self-reflection%2520and%2520error-correction%2520steps%252C%2520yet%2520primary%250Aexisting%2520methods%2520either%2520completely%2520discard%2520negative%2520samples%2520%2528RFT%2529%2520or%2520apply%250Aequal%2520penalization%2520across%2520all%2520tokens%2520%2528RL%2529%252C%2520failing%2520to%2520leverage%2520these%2520potential%250Alearning%2520signals.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520Behavior%2520Constrained%2520Policy%250AGradient%2520with%2520Negative%2520Sample%2520Augmentation%2520%2528BCPG-NSA%2529%252C%2520a%2520fine-grained%2520offline%250ARL%2520framework%2520that%2520encompasses%2520three%2520stages%253A%25201%2529%2520sample%2520segmentation%252C%25202%2529%250Aconsensus-based%2520step%2520correctness%2520assessment%2520combining%2520LLM%2520and%2520PRM%2520judgers%252C%2520and%250A3%2529%2520policy%2520optimization%2520with%2520NSA%2520designed%2520to%2520effectively%2520mine%2520positive%2520steps%250Awithin%2520negative%2520samples.%2520Experimental%2520results%2520show%2520that%2520BCPG-NSA%2520outperforms%250Abaselines%2520on%2520several%2520challenging%2520math/coding%2520reasoning%2520benchmarks%2520using%2520the%250Asame%2520training%2520dataset%252C%2520achieving%2520improved%2520sample%2520efficiency%2520and%2520demonstrating%250Arobustness%2520and%2520scalability%2520when%2520extended%2520to%2520multiple%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14403v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unearthing%20Gems%20from%20Stones%3A%20Policy%20Optimization%20with%20Negative%20Sample%0A%20%20Augmentation%20for%20LLM%20Reasoning&entry.906535625=Zhaohui%20Yang%20and%20Yuxiao%20Ye%20and%20Shilei%20Jiang%20and%20Chen%20Hu%20and%20Linjing%20Li%20and%20Shihong%20Deng%20and%20Daxin%20Jiang&entry.1292438233=%20%20Recent%20advances%20in%20reasoning%20language%20models%20have%20witnessed%20a%20paradigm%20shift%0Afrom%20short%20to%20long%20CoT%20pattern.%20Given%20the%20substantial%20computational%20cost%20of%0Arollouts%20in%20long%20CoT%20models%2C%20maximizing%20the%20utility%20of%20fixed%20training%20datasets%0Abecomes%20crucial.%20Our%20analysis%20reveals%20that%20negative%20responses%20contain%20valuable%0Acomponents%20such%20as%20self-reflection%20and%20error-correction%20steps%2C%20yet%20primary%0Aexisting%20methods%20either%20completely%20discard%20negative%20samples%20%28RFT%29%20or%20apply%0Aequal%20penalization%20across%20all%20tokens%20%28RL%29%2C%20failing%20to%20leverage%20these%20potential%0Alearning%20signals.%20In%20light%20of%20this%2C%20we%20propose%20Behavior%20Constrained%20Policy%0AGradient%20with%20Negative%20Sample%20Augmentation%20%28BCPG-NSA%29%2C%20a%20fine-grained%20offline%0ARL%20framework%20that%20encompasses%20three%20stages%3A%201%29%20sample%20segmentation%2C%202%29%0Aconsensus-based%20step%20correctness%20assessment%20combining%20LLM%20and%20PRM%20judgers%2C%20and%0A3%29%20policy%20optimization%20with%20NSA%20designed%20to%20effectively%20mine%20positive%20steps%0Awithin%20negative%20samples.%20Experimental%20results%20show%20that%20BCPG-NSA%20outperforms%0Abaselines%20on%20several%20challenging%20math/coding%20reasoning%20benchmarks%20using%20the%0Asame%20training%20dataset%2C%20achieving%20improved%20sample%20efficiency%20and%20demonstrating%0Arobustness%20and%20scalability%20when%20extended%20to%20multiple%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14403v3&entry.124074799=Read"},
{"title": "Privacy-Preserving Multi-Stage Fall Detection Framework with\n  Semi-supervised Federated Learning and Robotic Vision Confirmation", "author": "Seyed Alireza Rahimi Azghadi and Truong-Thanh-Hung Nguyen and Helene Fournier and Monica Wachowicz and Rene Richard and Francis Palma and Hung Cao", "abstract": "  The aging population is growing rapidly, and so is the danger of falls in\nolder adults. A major cause of injury is falling, and detection in time can\ngreatly save medical expenses and recovery time. However, to provide timely\nintervention and avoid unnecessary alarms, detection systems must be effective\nand reliable while addressing privacy concerns regarding the user. In this\nwork, we propose a framework for detecting falls using several complementary\nsystems: a semi-supervised federated learning-based fall detection system\n(SF2D), an indoor localization and navigation system, and a vision-based human\nfall recognition system. A wearable device and an edge device identify a fall\nscenario in the first system. On top of that, the second system uses an indoor\nlocalization technique first to localize the fall location and then navigate a\nrobot to inspect the scenario. A vision-based detection system running on an\nedge device with a mounted camera on a robot is used to recognize fallen\npeople. Each of the systems of this proposed framework achieves different\naccuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to\n99.19% accuracy, while the vision-based fallen people detection achieves 96.3%\naccuracy. However, when we combine the accuracy of these two systems with the\naccuracy of the navigation system (95% success rate), our proposed framework\ncreates a highly reliable performance for fall detection, with an overall\naccuracy of 99.99%. Not only is the proposed framework safe for older adults,\nbut it is also a privacy-preserving solution for detecting falls.\n", "link": "http://arxiv.org/abs/2507.10474v1", "date": "2025-07-14", "relevancy": 2.0756, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5595}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5134}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy-Preserving%20Multi-Stage%20Fall%20Detection%20Framework%20with%0A%20%20Semi-supervised%20Federated%20Learning%20and%20Robotic%20Vision%20Confirmation&body=Title%3A%20Privacy-Preserving%20Multi-Stage%20Fall%20Detection%20Framework%20with%0A%20%20Semi-supervised%20Federated%20Learning%20and%20Robotic%20Vision%20Confirmation%0AAuthor%3A%20Seyed%20Alireza%20Rahimi%20Azghadi%20and%20Truong-Thanh-Hung%20Nguyen%20and%20Helene%20Fournier%20and%20Monica%20Wachowicz%20and%20Rene%20Richard%20and%20Francis%20Palma%20and%20Hung%20Cao%0AAbstract%3A%20%20%20The%20aging%20population%20is%20growing%20rapidly%2C%20and%20so%20is%20the%20danger%20of%20falls%20in%0Aolder%20adults.%20A%20major%20cause%20of%20injury%20is%20falling%2C%20and%20detection%20in%20time%20can%0Agreatly%20save%20medical%20expenses%20and%20recovery%20time.%20However%2C%20to%20provide%20timely%0Aintervention%20and%20avoid%20unnecessary%20alarms%2C%20detection%20systems%20must%20be%20effective%0Aand%20reliable%20while%20addressing%20privacy%20concerns%20regarding%20the%20user.%20In%20this%0Awork%2C%20we%20propose%20a%20framework%20for%20detecting%20falls%20using%20several%20complementary%0Asystems%3A%20a%20semi-supervised%20federated%20learning-based%20fall%20detection%20system%0A%28SF2D%29%2C%20an%20indoor%20localization%20and%20navigation%20system%2C%20and%20a%20vision-based%20human%0Afall%20recognition%20system.%20A%20wearable%20device%20and%20an%20edge%20device%20identify%20a%20fall%0Ascenario%20in%20the%20first%20system.%20On%20top%20of%20that%2C%20the%20second%20system%20uses%20an%20indoor%0Alocalization%20technique%20first%20to%20localize%20the%20fall%20location%20and%20then%20navigate%20a%0Arobot%20to%20inspect%20the%20scenario.%20A%20vision-based%20detection%20system%20running%20on%20an%0Aedge%20device%20with%20a%20mounted%20camera%20on%20a%20robot%20is%20used%20to%20recognize%20fallen%0Apeople.%20Each%20of%20the%20systems%20of%20this%20proposed%20framework%20achieves%20different%0Aaccuracy%20rates.%20Specifically%2C%20the%20SF2D%20has%20a%200.81%25%20failure%20rate%20equivalent%20to%0A99.19%25%20accuracy%2C%20while%20the%20vision-based%20fallen%20people%20detection%20achieves%2096.3%25%0Aaccuracy.%20However%2C%20when%20we%20combine%20the%20accuracy%20of%20these%20two%20systems%20with%20the%0Aaccuracy%20of%20the%20navigation%20system%20%2895%25%20success%20rate%29%2C%20our%20proposed%20framework%0Acreates%20a%20highly%20reliable%20performance%20for%20fall%20detection%2C%20with%20an%20overall%0Aaccuracy%20of%2099.99%25.%20Not%20only%20is%20the%20proposed%20framework%20safe%20for%20older%20adults%2C%0Abut%20it%20is%20also%20a%20privacy-preserving%20solution%20for%20detecting%20falls.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy-Preserving%2520Multi-Stage%2520Fall%2520Detection%2520Framework%2520with%250A%2520%2520Semi-supervised%2520Federated%2520Learning%2520and%2520Robotic%2520Vision%2520Confirmation%26entry.906535625%3DSeyed%2520Alireza%2520Rahimi%2520Azghadi%2520and%2520Truong-Thanh-Hung%2520Nguyen%2520and%2520Helene%2520Fournier%2520and%2520Monica%2520Wachowicz%2520and%2520Rene%2520Richard%2520and%2520Francis%2520Palma%2520and%2520Hung%2520Cao%26entry.1292438233%3D%2520%2520The%2520aging%2520population%2520is%2520growing%2520rapidly%252C%2520and%2520so%2520is%2520the%2520danger%2520of%2520falls%2520in%250Aolder%2520adults.%2520A%2520major%2520cause%2520of%2520injury%2520is%2520falling%252C%2520and%2520detection%2520in%2520time%2520can%250Agreatly%2520save%2520medical%2520expenses%2520and%2520recovery%2520time.%2520However%252C%2520to%2520provide%2520timely%250Aintervention%2520and%2520avoid%2520unnecessary%2520alarms%252C%2520detection%2520systems%2520must%2520be%2520effective%250Aand%2520reliable%2520while%2520addressing%2520privacy%2520concerns%2520regarding%2520the%2520user.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520framework%2520for%2520detecting%2520falls%2520using%2520several%2520complementary%250Asystems%253A%2520a%2520semi-supervised%2520federated%2520learning-based%2520fall%2520detection%2520system%250A%2528SF2D%2529%252C%2520an%2520indoor%2520localization%2520and%2520navigation%2520system%252C%2520and%2520a%2520vision-based%2520human%250Afall%2520recognition%2520system.%2520A%2520wearable%2520device%2520and%2520an%2520edge%2520device%2520identify%2520a%2520fall%250Ascenario%2520in%2520the%2520first%2520system.%2520On%2520top%2520of%2520that%252C%2520the%2520second%2520system%2520uses%2520an%2520indoor%250Alocalization%2520technique%2520first%2520to%2520localize%2520the%2520fall%2520location%2520and%2520then%2520navigate%2520a%250Arobot%2520to%2520inspect%2520the%2520scenario.%2520A%2520vision-based%2520detection%2520system%2520running%2520on%2520an%250Aedge%2520device%2520with%2520a%2520mounted%2520camera%2520on%2520a%2520robot%2520is%2520used%2520to%2520recognize%2520fallen%250Apeople.%2520Each%2520of%2520the%2520systems%2520of%2520this%2520proposed%2520framework%2520achieves%2520different%250Aaccuracy%2520rates.%2520Specifically%252C%2520the%2520SF2D%2520has%2520a%25200.81%2525%2520failure%2520rate%2520equivalent%2520to%250A99.19%2525%2520accuracy%252C%2520while%2520the%2520vision-based%2520fallen%2520people%2520detection%2520achieves%252096.3%2525%250Aaccuracy.%2520However%252C%2520when%2520we%2520combine%2520the%2520accuracy%2520of%2520these%2520two%2520systems%2520with%2520the%250Aaccuracy%2520of%2520the%2520navigation%2520system%2520%252895%2525%2520success%2520rate%2529%252C%2520our%2520proposed%2520framework%250Acreates%2520a%2520highly%2520reliable%2520performance%2520for%2520fall%2520detection%252C%2520with%2520an%2520overall%250Aaccuracy%2520of%252099.99%2525.%2520Not%2520only%2520is%2520the%2520proposed%2520framework%2520safe%2520for%2520older%2520adults%252C%250Abut%2520it%2520is%2520also%2520a%2520privacy-preserving%2520solution%2520for%2520detecting%2520falls.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-Preserving%20Multi-Stage%20Fall%20Detection%20Framework%20with%0A%20%20Semi-supervised%20Federated%20Learning%20and%20Robotic%20Vision%20Confirmation&entry.906535625=Seyed%20Alireza%20Rahimi%20Azghadi%20and%20Truong-Thanh-Hung%20Nguyen%20and%20Helene%20Fournier%20and%20Monica%20Wachowicz%20and%20Rene%20Richard%20and%20Francis%20Palma%20and%20Hung%20Cao&entry.1292438233=%20%20The%20aging%20population%20is%20growing%20rapidly%2C%20and%20so%20is%20the%20danger%20of%20falls%20in%0Aolder%20adults.%20A%20major%20cause%20of%20injury%20is%20falling%2C%20and%20detection%20in%20time%20can%0Agreatly%20save%20medical%20expenses%20and%20recovery%20time.%20However%2C%20to%20provide%20timely%0Aintervention%20and%20avoid%20unnecessary%20alarms%2C%20detection%20systems%20must%20be%20effective%0Aand%20reliable%20while%20addressing%20privacy%20concerns%20regarding%20the%20user.%20In%20this%0Awork%2C%20we%20propose%20a%20framework%20for%20detecting%20falls%20using%20several%20complementary%0Asystems%3A%20a%20semi-supervised%20federated%20learning-based%20fall%20detection%20system%0A%28SF2D%29%2C%20an%20indoor%20localization%20and%20navigation%20system%2C%20and%20a%20vision-based%20human%0Afall%20recognition%20system.%20A%20wearable%20device%20and%20an%20edge%20device%20identify%20a%20fall%0Ascenario%20in%20the%20first%20system.%20On%20top%20of%20that%2C%20the%20second%20system%20uses%20an%20indoor%0Alocalization%20technique%20first%20to%20localize%20the%20fall%20location%20and%20then%20navigate%20a%0Arobot%20to%20inspect%20the%20scenario.%20A%20vision-based%20detection%20system%20running%20on%20an%0Aedge%20device%20with%20a%20mounted%20camera%20on%20a%20robot%20is%20used%20to%20recognize%20fallen%0Apeople.%20Each%20of%20the%20systems%20of%20this%20proposed%20framework%20achieves%20different%0Aaccuracy%20rates.%20Specifically%2C%20the%20SF2D%20has%20a%200.81%25%20failure%20rate%20equivalent%20to%0A99.19%25%20accuracy%2C%20while%20the%20vision-based%20fallen%20people%20detection%20achieves%2096.3%25%0Aaccuracy.%20However%2C%20when%20we%20combine%20the%20accuracy%20of%20these%20two%20systems%20with%20the%0Aaccuracy%20of%20the%20navigation%20system%20%2895%25%20success%20rate%29%2C%20our%20proposed%20framework%0Acreates%20a%20highly%20reliable%20performance%20for%20fall%20detection%2C%20with%20an%20overall%0Aaccuracy%20of%2099.99%25.%20Not%20only%20is%20the%20proposed%20framework%20safe%20for%20older%20adults%2C%0Abut%20it%20is%20also%20a%20privacy-preserving%20solution%20for%20detecting%20falls.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10474v1&entry.124074799=Read"},
{"title": "Visual Test-time Scaling for GUI Agent Grounding", "author": "Tiange Luo and Lajanugen Logeswaran and Justin Johnson and Honglak Lee", "abstract": "  We introduce RegionFocus, a visual test-time scaling approach for Vision\nLanguage Model Agents. Understanding webpages is challenging due to the visual\ncomplexity of GUI images and the large number of interface elements, making\naccurate action selection difficult. Our approach dynamically zooms in on\nrelevant regions, reducing background clutter and improving grounding accuracy.\nTo support this process, we propose an image-as-map mechanism that visualizes\nkey landmarks at each step, providing a transparent action record and enables\nthe agent to effectively choose among action candidates. Even with a simple\nregion selection strategy, we observe significant performance gains of 28+\\% on\nScreenspot-pro and 24+\\% on WebVoyager benchmarks on top of two\nstate-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,\nhighlighting the effectiveness of visual test-time scaling in interactive\nsettings. We achieve a new state-of-the-art grounding performance of 61.6\\% on\nthe ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.\nOur code will be released publicly at https://github.com/tiangeluo/RegionFocus.\n", "link": "http://arxiv.org/abs/2505.00684v2", "date": "2025-07-14", "relevancy": 2.0718, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Test-time%20Scaling%20for%20GUI%20Agent%20Grounding&body=Title%3A%20Visual%20Test-time%20Scaling%20for%20GUI%20Agent%20Grounding%0AAuthor%3A%20Tiange%20Luo%20and%20Lajanugen%20Logeswaran%20and%20Justin%20Johnson%20and%20Honglak%20Lee%0AAbstract%3A%20%20%20We%20introduce%20RegionFocus%2C%20a%20visual%20test-time%20scaling%20approach%20for%20Vision%0ALanguage%20Model%20Agents.%20Understanding%20webpages%20is%20challenging%20due%20to%20the%20visual%0Acomplexity%20of%20GUI%20images%20and%20the%20large%20number%20of%20interface%20elements%2C%20making%0Aaccurate%20action%20selection%20difficult.%20Our%20approach%20dynamically%20zooms%20in%20on%0Arelevant%20regions%2C%20reducing%20background%20clutter%20and%20improving%20grounding%20accuracy.%0ATo%20support%20this%20process%2C%20we%20propose%20an%20image-as-map%20mechanism%20that%20visualizes%0Akey%20landmarks%20at%20each%20step%2C%20providing%20a%20transparent%20action%20record%20and%20enables%0Athe%20agent%20to%20effectively%20choose%20among%20action%20candidates.%20Even%20with%20a%20simple%0Aregion%20selection%20strategy%2C%20we%20observe%20significant%20performance%20gains%20of%2028%2B%5C%25%20on%0AScreenspot-pro%20and%2024%2B%5C%25%20on%20WebVoyager%20benchmarks%20on%20top%20of%20two%0Astate-of-the-art%20open%20vision%20language%20model%20agents%2C%20UI-TARS%20and%20Qwen2.5-VL%2C%0Ahighlighting%20the%20effectiveness%20of%20visual%20test-time%20scaling%20in%20interactive%0Asettings.%20We%20achieve%20a%20new%20state-of-the-art%20grounding%20performance%20of%2061.6%5C%25%20on%0Athe%20ScreenSpot-Pro%20benchmark%20by%20applying%20RegionFocus%20to%20a%20Qwen2.5-VL-72B%20model.%0AOur%20code%20will%20be%20released%20publicly%20at%20https%3A//github.com/tiangeluo/RegionFocus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Test-time%2520Scaling%2520for%2520GUI%2520Agent%2520Grounding%26entry.906535625%3DTiange%2520Luo%2520and%2520Lajanugen%2520Logeswaran%2520and%2520Justin%2520Johnson%2520and%2520Honglak%2520Lee%26entry.1292438233%3D%2520%2520We%2520introduce%2520RegionFocus%252C%2520a%2520visual%2520test-time%2520scaling%2520approach%2520for%2520Vision%250ALanguage%2520Model%2520Agents.%2520Understanding%2520webpages%2520is%2520challenging%2520due%2520to%2520the%2520visual%250Acomplexity%2520of%2520GUI%2520images%2520and%2520the%2520large%2520number%2520of%2520interface%2520elements%252C%2520making%250Aaccurate%2520action%2520selection%2520difficult.%2520Our%2520approach%2520dynamically%2520zooms%2520in%2520on%250Arelevant%2520regions%252C%2520reducing%2520background%2520clutter%2520and%2520improving%2520grounding%2520accuracy.%250ATo%2520support%2520this%2520process%252C%2520we%2520propose%2520an%2520image-as-map%2520mechanism%2520that%2520visualizes%250Akey%2520landmarks%2520at%2520each%2520step%252C%2520providing%2520a%2520transparent%2520action%2520record%2520and%2520enables%250Athe%2520agent%2520to%2520effectively%2520choose%2520among%2520action%2520candidates.%2520Even%2520with%2520a%2520simple%250Aregion%2520selection%2520strategy%252C%2520we%2520observe%2520significant%2520performance%2520gains%2520of%252028%252B%255C%2525%2520on%250AScreenspot-pro%2520and%252024%252B%255C%2525%2520on%2520WebVoyager%2520benchmarks%2520on%2520top%2520of%2520two%250Astate-of-the-art%2520open%2520vision%2520language%2520model%2520agents%252C%2520UI-TARS%2520and%2520Qwen2.5-VL%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520visual%2520test-time%2520scaling%2520in%2520interactive%250Asettings.%2520We%2520achieve%2520a%2520new%2520state-of-the-art%2520grounding%2520performance%2520of%252061.6%255C%2525%2520on%250Athe%2520ScreenSpot-Pro%2520benchmark%2520by%2520applying%2520RegionFocus%2520to%2520a%2520Qwen2.5-VL-72B%2520model.%250AOur%2520code%2520will%2520be%2520released%2520publicly%2520at%2520https%253A//github.com/tiangeluo/RegionFocus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Test-time%20Scaling%20for%20GUI%20Agent%20Grounding&entry.906535625=Tiange%20Luo%20and%20Lajanugen%20Logeswaran%20and%20Justin%20Johnson%20and%20Honglak%20Lee&entry.1292438233=%20%20We%20introduce%20RegionFocus%2C%20a%20visual%20test-time%20scaling%20approach%20for%20Vision%0ALanguage%20Model%20Agents.%20Understanding%20webpages%20is%20challenging%20due%20to%20the%20visual%0Acomplexity%20of%20GUI%20images%20and%20the%20large%20number%20of%20interface%20elements%2C%20making%0Aaccurate%20action%20selection%20difficult.%20Our%20approach%20dynamically%20zooms%20in%20on%0Arelevant%20regions%2C%20reducing%20background%20clutter%20and%20improving%20grounding%20accuracy.%0ATo%20support%20this%20process%2C%20we%20propose%20an%20image-as-map%20mechanism%20that%20visualizes%0Akey%20landmarks%20at%20each%20step%2C%20providing%20a%20transparent%20action%20record%20and%20enables%0Athe%20agent%20to%20effectively%20choose%20among%20action%20candidates.%20Even%20with%20a%20simple%0Aregion%20selection%20strategy%2C%20we%20observe%20significant%20performance%20gains%20of%2028%2B%5C%25%20on%0AScreenspot-pro%20and%2024%2B%5C%25%20on%20WebVoyager%20benchmarks%20on%20top%20of%20two%0Astate-of-the-art%20open%20vision%20language%20model%20agents%2C%20UI-TARS%20and%20Qwen2.5-VL%2C%0Ahighlighting%20the%20effectiveness%20of%20visual%20test-time%20scaling%20in%20interactive%0Asettings.%20We%20achieve%20a%20new%20state-of-the-art%20grounding%20performance%20of%2061.6%5C%25%20on%0Athe%20ScreenSpot-Pro%20benchmark%20by%20applying%20RegionFocus%20to%20a%20Qwen2.5-VL-72B%20model.%0AOur%20code%20will%20be%20released%20publicly%20at%20https%3A//github.com/tiangeluo/RegionFocus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00684v2&entry.124074799=Read"},
{"title": "RefSTAR: Blind Facial Image Restoration with Reference Selection,\n  Transfer, and Reconstruction", "author": "Zhicun Yin and Junjie Chen and Ming Liu and Zhixin Wang and Fan Li and Renjing Pei and Xiaoming Li and Rynson W. H. Lau and Wangmeng Zuo", "abstract": "  Blind facial image restoration is highly challenging due to unknown complex\ndegradations and the sensitivity of humans to faces. Although existing methods\nintroduce auxiliary information from generative priors or high-quality\nreference images, they still struggle with identity preservation problems,\nmainly due to improper feature introduction on detailed textures. In this\npaper, we focus on effectively incorporating appropriate features from\nhigh-quality reference images, presenting a novel blind facial image\nrestoration method that considers reference selection, transfer, and\nreconstruction (RefSTAR). In terms of selection, we construct a reference\nselection (RefSel) module. For training the RefSel module, we construct a\nRefSel-HQ dataset through a mask generation pipeline, which contains annotating\nmasks for 10,000 ground truth-reference pairs. As for the transfer, due to the\ntrivial solution in vanilla cross-attention operations, a feature fusion\nparadigm is designed to force the features from the reference to be integrated.\nFinally, we propose a reference image reconstruction mechanism that further\nensures the presence of reference image features in the output image. The cycle\nconsistency loss is also redesigned in conjunction with the mask. Extensive\nexperiments on various backbone models demonstrate superior performance,\nshowing better identity preservation ability and reference feature transfer\nquality. Source code, dataset, and pre-trained models are available at\nhttps://github.com/yinzhicun/RefSTAR.\n", "link": "http://arxiv.org/abs/2507.10470v1", "date": "2025-07-14", "relevancy": 2.0648, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.518}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5176}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefSTAR%3A%20Blind%20Facial%20Image%20Restoration%20with%20Reference%20Selection%2C%0A%20%20Transfer%2C%20and%20Reconstruction&body=Title%3A%20RefSTAR%3A%20Blind%20Facial%20Image%20Restoration%20with%20Reference%20Selection%2C%0A%20%20Transfer%2C%20and%20Reconstruction%0AAuthor%3A%20Zhicun%20Yin%20and%20Junjie%20Chen%20and%20Ming%20Liu%20and%20Zhixin%20Wang%20and%20Fan%20Li%20and%20Renjing%20Pei%20and%20Xiaoming%20Li%20and%20Rynson%20W.%20H.%20Lau%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Blind%20facial%20image%20restoration%20is%20highly%20challenging%20due%20to%20unknown%20complex%0Adegradations%20and%20the%20sensitivity%20of%20humans%20to%20faces.%20Although%20existing%20methods%0Aintroduce%20auxiliary%20information%20from%20generative%20priors%20or%20high-quality%0Areference%20images%2C%20they%20still%20struggle%20with%20identity%20preservation%20problems%2C%0Amainly%20due%20to%20improper%20feature%20introduction%20on%20detailed%20textures.%20In%20this%0Apaper%2C%20we%20focus%20on%20effectively%20incorporating%20appropriate%20features%20from%0Ahigh-quality%20reference%20images%2C%20presenting%20a%20novel%20blind%20facial%20image%0Arestoration%20method%20that%20considers%20reference%20selection%2C%20transfer%2C%20and%0Areconstruction%20%28RefSTAR%29.%20In%20terms%20of%20selection%2C%20we%20construct%20a%20reference%0Aselection%20%28RefSel%29%20module.%20For%20training%20the%20RefSel%20module%2C%20we%20construct%20a%0ARefSel-HQ%20dataset%20through%20a%20mask%20generation%20pipeline%2C%20which%20contains%20annotating%0Amasks%20for%2010%2C000%20ground%20truth-reference%20pairs.%20As%20for%20the%20transfer%2C%20due%20to%20the%0Atrivial%20solution%20in%20vanilla%20cross-attention%20operations%2C%20a%20feature%20fusion%0Aparadigm%20is%20designed%20to%20force%20the%20features%20from%20the%20reference%20to%20be%20integrated.%0AFinally%2C%20we%20propose%20a%20reference%20image%20reconstruction%20mechanism%20that%20further%0Aensures%20the%20presence%20of%20reference%20image%20features%20in%20the%20output%20image.%20The%20cycle%0Aconsistency%20loss%20is%20also%20redesigned%20in%20conjunction%20with%20the%20mask.%20Extensive%0Aexperiments%20on%20various%20backbone%20models%20demonstrate%20superior%20performance%2C%0Ashowing%20better%20identity%20preservation%20ability%20and%20reference%20feature%20transfer%0Aquality.%20Source%20code%2C%20dataset%2C%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/yinzhicun/RefSTAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefSTAR%253A%2520Blind%2520Facial%2520Image%2520Restoration%2520with%2520Reference%2520Selection%252C%250A%2520%2520Transfer%252C%2520and%2520Reconstruction%26entry.906535625%3DZhicun%2520Yin%2520and%2520Junjie%2520Chen%2520and%2520Ming%2520Liu%2520and%2520Zhixin%2520Wang%2520and%2520Fan%2520Li%2520and%2520Renjing%2520Pei%2520and%2520Xiaoming%2520Li%2520and%2520Rynson%2520W.%2520H.%2520Lau%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Blind%2520facial%2520image%2520restoration%2520is%2520highly%2520challenging%2520due%2520to%2520unknown%2520complex%250Adegradations%2520and%2520the%2520sensitivity%2520of%2520humans%2520to%2520faces.%2520Although%2520existing%2520methods%250Aintroduce%2520auxiliary%2520information%2520from%2520generative%2520priors%2520or%2520high-quality%250Areference%2520images%252C%2520they%2520still%2520struggle%2520with%2520identity%2520preservation%2520problems%252C%250Amainly%2520due%2520to%2520improper%2520feature%2520introduction%2520on%2520detailed%2520textures.%2520In%2520this%250Apaper%252C%2520we%2520focus%2520on%2520effectively%2520incorporating%2520appropriate%2520features%2520from%250Ahigh-quality%2520reference%2520images%252C%2520presenting%2520a%2520novel%2520blind%2520facial%2520image%250Arestoration%2520method%2520that%2520considers%2520reference%2520selection%252C%2520transfer%252C%2520and%250Areconstruction%2520%2528RefSTAR%2529.%2520In%2520terms%2520of%2520selection%252C%2520we%2520construct%2520a%2520reference%250Aselection%2520%2528RefSel%2529%2520module.%2520For%2520training%2520the%2520RefSel%2520module%252C%2520we%2520construct%2520a%250ARefSel-HQ%2520dataset%2520through%2520a%2520mask%2520generation%2520pipeline%252C%2520which%2520contains%2520annotating%250Amasks%2520for%252010%252C000%2520ground%2520truth-reference%2520pairs.%2520As%2520for%2520the%2520transfer%252C%2520due%2520to%2520the%250Atrivial%2520solution%2520in%2520vanilla%2520cross-attention%2520operations%252C%2520a%2520feature%2520fusion%250Aparadigm%2520is%2520designed%2520to%2520force%2520the%2520features%2520from%2520the%2520reference%2520to%2520be%2520integrated.%250AFinally%252C%2520we%2520propose%2520a%2520reference%2520image%2520reconstruction%2520mechanism%2520that%2520further%250Aensures%2520the%2520presence%2520of%2520reference%2520image%2520features%2520in%2520the%2520output%2520image.%2520The%2520cycle%250Aconsistency%2520loss%2520is%2520also%2520redesigned%2520in%2520conjunction%2520with%2520the%2520mask.%2520Extensive%250Aexperiments%2520on%2520various%2520backbone%2520models%2520demonstrate%2520superior%2520performance%252C%250Ashowing%2520better%2520identity%2520preservation%2520ability%2520and%2520reference%2520feature%2520transfer%250Aquality.%2520Source%2520code%252C%2520dataset%252C%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/yinzhicun/RefSTAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefSTAR%3A%20Blind%20Facial%20Image%20Restoration%20with%20Reference%20Selection%2C%0A%20%20Transfer%2C%20and%20Reconstruction&entry.906535625=Zhicun%20Yin%20and%20Junjie%20Chen%20and%20Ming%20Liu%20and%20Zhixin%20Wang%20and%20Fan%20Li%20and%20Renjing%20Pei%20and%20Xiaoming%20Li%20and%20Rynson%20W.%20H.%20Lau%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Blind%20facial%20image%20restoration%20is%20highly%20challenging%20due%20to%20unknown%20complex%0Adegradations%20and%20the%20sensitivity%20of%20humans%20to%20faces.%20Although%20existing%20methods%0Aintroduce%20auxiliary%20information%20from%20generative%20priors%20or%20high-quality%0Areference%20images%2C%20they%20still%20struggle%20with%20identity%20preservation%20problems%2C%0Amainly%20due%20to%20improper%20feature%20introduction%20on%20detailed%20textures.%20In%20this%0Apaper%2C%20we%20focus%20on%20effectively%20incorporating%20appropriate%20features%20from%0Ahigh-quality%20reference%20images%2C%20presenting%20a%20novel%20blind%20facial%20image%0Arestoration%20method%20that%20considers%20reference%20selection%2C%20transfer%2C%20and%0Areconstruction%20%28RefSTAR%29.%20In%20terms%20of%20selection%2C%20we%20construct%20a%20reference%0Aselection%20%28RefSel%29%20module.%20For%20training%20the%20RefSel%20module%2C%20we%20construct%20a%0ARefSel-HQ%20dataset%20through%20a%20mask%20generation%20pipeline%2C%20which%20contains%20annotating%0Amasks%20for%2010%2C000%20ground%20truth-reference%20pairs.%20As%20for%20the%20transfer%2C%20due%20to%20the%0Atrivial%20solution%20in%20vanilla%20cross-attention%20operations%2C%20a%20feature%20fusion%0Aparadigm%20is%20designed%20to%20force%20the%20features%20from%20the%20reference%20to%20be%20integrated.%0AFinally%2C%20we%20propose%20a%20reference%20image%20reconstruction%20mechanism%20that%20further%0Aensures%20the%20presence%20of%20reference%20image%20features%20in%20the%20output%20image.%20The%20cycle%0Aconsistency%20loss%20is%20also%20redesigned%20in%20conjunction%20with%20the%20mask.%20Extensive%0Aexperiments%20on%20various%20backbone%20models%20demonstrate%20superior%20performance%2C%0Ashowing%20better%20identity%20preservation%20ability%20and%20reference%20feature%20transfer%0Aquality.%20Source%20code%2C%20dataset%2C%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/yinzhicun/RefSTAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10470v1&entry.124074799=Read"},
{"title": "MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of\n  Imputation Methods for IMU-based Motion Capture Data", "author": "Mahmoud Bekhit and Ahmad Salah and Ahmed Salim Alrawahi and Tarek Attia and Ahmed Ali and Esraa Eldesokey and Ahmed Fathalla", "abstract": "  Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)\nis vital for applications in sports science, but its utility is often\ncompromised by missing data. Despite numerous imputation techniques, a\nsystematic performance evaluation for IMU-derived MoCap time-series data is\nlacking. We address this gap by conducting a comprehensive comparative analysis\nof statistical, machine learning, and deep learning imputation methods. Our\nevaluation considers three distinct contexts: univariate time-series,\nmultivariate across subjects, and multivariate across kinematic angles. To\nfacilitate this benchmark, we introduce the first publicly available MoCap\ndataset designed specifically for imputation, featuring data from 53 karate\npractitioners. We simulate three controlled missingness mechanisms: missing\ncompletely at random (MCAR), block missingness, and a novel value-dependent\npattern at signal transition points. Our experiments, conducted on 39 kinematic\nvariables across all subjects, reveal that multivariate imputation frameworks\nconsistently outperform univariate approaches, particularly for complex\nmissingness. For instance, multivariate methods achieve up to a 50% mean\nabsolute error reduction (MAE from 10.8 to 5.8) compared to univariate\ntechniques for transition point missingness. Advanced models like Generative\nAdversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the\nhighest accuracy in these challenging scenarios. This work provides a critical\nbaseline for future research and offers practical recommendations for improving\nthe integrity and robustness of Mo-Cap data analysis.\n", "link": "http://arxiv.org/abs/2507.10334v1", "date": "2025-07-14", "relevancy": 2.0633, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5604}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5137}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoCap-Impute%3A%20A%20Comprehensive%20Benchmark%20and%20Comparative%20Analysis%20of%0A%20%20Imputation%20Methods%20for%20IMU-based%20Motion%20Capture%20Data&body=Title%3A%20MoCap-Impute%3A%20A%20Comprehensive%20Benchmark%20and%20Comparative%20Analysis%20of%0A%20%20Imputation%20Methods%20for%20IMU-based%20Motion%20Capture%20Data%0AAuthor%3A%20Mahmoud%20Bekhit%20and%20Ahmad%20Salah%20and%20Ahmed%20Salim%20Alrawahi%20and%20Tarek%20Attia%20and%20Ahmed%20Ali%20and%20Esraa%20Eldesokey%20and%20Ahmed%20Fathalla%0AAbstract%3A%20%20%20Motion%20capture%20%28MoCap%29%20data%20from%20wearable%20Inertial%20Measurement%20Units%20%28IMUs%29%0Ais%20vital%20for%20applications%20in%20sports%20science%2C%20but%20its%20utility%20is%20often%0Acompromised%20by%20missing%20data.%20Despite%20numerous%20imputation%20techniques%2C%20a%0Asystematic%20performance%20evaluation%20for%20IMU-derived%20MoCap%20time-series%20data%20is%0Alacking.%20We%20address%20this%20gap%20by%20conducting%20a%20comprehensive%20comparative%20analysis%0Aof%20statistical%2C%20machine%20learning%2C%20and%20deep%20learning%20imputation%20methods.%20Our%0Aevaluation%20considers%20three%20distinct%20contexts%3A%20univariate%20time-series%2C%0Amultivariate%20across%20subjects%2C%20and%20multivariate%20across%20kinematic%20angles.%20To%0Afacilitate%20this%20benchmark%2C%20we%20introduce%20the%20first%20publicly%20available%20MoCap%0Adataset%20designed%20specifically%20for%20imputation%2C%20featuring%20data%20from%2053%20karate%0Apractitioners.%20We%20simulate%20three%20controlled%20missingness%20mechanisms%3A%20missing%0Acompletely%20at%20random%20%28MCAR%29%2C%20block%20missingness%2C%20and%20a%20novel%20value-dependent%0Apattern%20at%20signal%20transition%20points.%20Our%20experiments%2C%20conducted%20on%2039%20kinematic%0Avariables%20across%20all%20subjects%2C%20reveal%20that%20multivariate%20imputation%20frameworks%0Aconsistently%20outperform%20univariate%20approaches%2C%20particularly%20for%20complex%0Amissingness.%20For%20instance%2C%20multivariate%20methods%20achieve%20up%20to%20a%2050%25%20mean%0Aabsolute%20error%20reduction%20%28MAE%20from%2010.8%20to%205.8%29%20compared%20to%20univariate%0Atechniques%20for%20transition%20point%20missingness.%20Advanced%20models%20like%20Generative%0AAdversarial%20Imputation%20Networks%20%28GAIN%29%20and%20Iterative%20Imputers%20demonstrate%20the%0Ahighest%20accuracy%20in%20these%20challenging%20scenarios.%20This%20work%20provides%20a%20critical%0Abaseline%20for%20future%20research%20and%20offers%20practical%20recommendations%20for%20improving%0Athe%20integrity%20and%20robustness%20of%20Mo-Cap%20data%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoCap-Impute%253A%2520A%2520Comprehensive%2520Benchmark%2520and%2520Comparative%2520Analysis%2520of%250A%2520%2520Imputation%2520Methods%2520for%2520IMU-based%2520Motion%2520Capture%2520Data%26entry.906535625%3DMahmoud%2520Bekhit%2520and%2520Ahmad%2520Salah%2520and%2520Ahmed%2520Salim%2520Alrawahi%2520and%2520Tarek%2520Attia%2520and%2520Ahmed%2520Ali%2520and%2520Esraa%2520Eldesokey%2520and%2520Ahmed%2520Fathalla%26entry.1292438233%3D%2520%2520Motion%2520capture%2520%2528MoCap%2529%2520data%2520from%2520wearable%2520Inertial%2520Measurement%2520Units%2520%2528IMUs%2529%250Ais%2520vital%2520for%2520applications%2520in%2520sports%2520science%252C%2520but%2520its%2520utility%2520is%2520often%250Acompromised%2520by%2520missing%2520data.%2520Despite%2520numerous%2520imputation%2520techniques%252C%2520a%250Asystematic%2520performance%2520evaluation%2520for%2520IMU-derived%2520MoCap%2520time-series%2520data%2520is%250Alacking.%2520We%2520address%2520this%2520gap%2520by%2520conducting%2520a%2520comprehensive%2520comparative%2520analysis%250Aof%2520statistical%252C%2520machine%2520learning%252C%2520and%2520deep%2520learning%2520imputation%2520methods.%2520Our%250Aevaluation%2520considers%2520three%2520distinct%2520contexts%253A%2520univariate%2520time-series%252C%250Amultivariate%2520across%2520subjects%252C%2520and%2520multivariate%2520across%2520kinematic%2520angles.%2520To%250Afacilitate%2520this%2520benchmark%252C%2520we%2520introduce%2520the%2520first%2520publicly%2520available%2520MoCap%250Adataset%2520designed%2520specifically%2520for%2520imputation%252C%2520featuring%2520data%2520from%252053%2520karate%250Apractitioners.%2520We%2520simulate%2520three%2520controlled%2520missingness%2520mechanisms%253A%2520missing%250Acompletely%2520at%2520random%2520%2528MCAR%2529%252C%2520block%2520missingness%252C%2520and%2520a%2520novel%2520value-dependent%250Apattern%2520at%2520signal%2520transition%2520points.%2520Our%2520experiments%252C%2520conducted%2520on%252039%2520kinematic%250Avariables%2520across%2520all%2520subjects%252C%2520reveal%2520that%2520multivariate%2520imputation%2520frameworks%250Aconsistently%2520outperform%2520univariate%2520approaches%252C%2520particularly%2520for%2520complex%250Amissingness.%2520For%2520instance%252C%2520multivariate%2520methods%2520achieve%2520up%2520to%2520a%252050%2525%2520mean%250Aabsolute%2520error%2520reduction%2520%2528MAE%2520from%252010.8%2520to%25205.8%2529%2520compared%2520to%2520univariate%250Atechniques%2520for%2520transition%2520point%2520missingness.%2520Advanced%2520models%2520like%2520Generative%250AAdversarial%2520Imputation%2520Networks%2520%2528GAIN%2529%2520and%2520Iterative%2520Imputers%2520demonstrate%2520the%250Ahighest%2520accuracy%2520in%2520these%2520challenging%2520scenarios.%2520This%2520work%2520provides%2520a%2520critical%250Abaseline%2520for%2520future%2520research%2520and%2520offers%2520practical%2520recommendations%2520for%2520improving%250Athe%2520integrity%2520and%2520robustness%2520of%2520Mo-Cap%2520data%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoCap-Impute%3A%20A%20Comprehensive%20Benchmark%20and%20Comparative%20Analysis%20of%0A%20%20Imputation%20Methods%20for%20IMU-based%20Motion%20Capture%20Data&entry.906535625=Mahmoud%20Bekhit%20and%20Ahmad%20Salah%20and%20Ahmed%20Salim%20Alrawahi%20and%20Tarek%20Attia%20and%20Ahmed%20Ali%20and%20Esraa%20Eldesokey%20and%20Ahmed%20Fathalla&entry.1292438233=%20%20Motion%20capture%20%28MoCap%29%20data%20from%20wearable%20Inertial%20Measurement%20Units%20%28IMUs%29%0Ais%20vital%20for%20applications%20in%20sports%20science%2C%20but%20its%20utility%20is%20often%0Acompromised%20by%20missing%20data.%20Despite%20numerous%20imputation%20techniques%2C%20a%0Asystematic%20performance%20evaluation%20for%20IMU-derived%20MoCap%20time-series%20data%20is%0Alacking.%20We%20address%20this%20gap%20by%20conducting%20a%20comprehensive%20comparative%20analysis%0Aof%20statistical%2C%20machine%20learning%2C%20and%20deep%20learning%20imputation%20methods.%20Our%0Aevaluation%20considers%20three%20distinct%20contexts%3A%20univariate%20time-series%2C%0Amultivariate%20across%20subjects%2C%20and%20multivariate%20across%20kinematic%20angles.%20To%0Afacilitate%20this%20benchmark%2C%20we%20introduce%20the%20first%20publicly%20available%20MoCap%0Adataset%20designed%20specifically%20for%20imputation%2C%20featuring%20data%20from%2053%20karate%0Apractitioners.%20We%20simulate%20three%20controlled%20missingness%20mechanisms%3A%20missing%0Acompletely%20at%20random%20%28MCAR%29%2C%20block%20missingness%2C%20and%20a%20novel%20value-dependent%0Apattern%20at%20signal%20transition%20points.%20Our%20experiments%2C%20conducted%20on%2039%20kinematic%0Avariables%20across%20all%20subjects%2C%20reveal%20that%20multivariate%20imputation%20frameworks%0Aconsistently%20outperform%20univariate%20approaches%2C%20particularly%20for%20complex%0Amissingness.%20For%20instance%2C%20multivariate%20methods%20achieve%20up%20to%20a%2050%25%20mean%0Aabsolute%20error%20reduction%20%28MAE%20from%2010.8%20to%205.8%29%20compared%20to%20univariate%0Atechniques%20for%20transition%20point%20missingness.%20Advanced%20models%20like%20Generative%0AAdversarial%20Imputation%20Networks%20%28GAIN%29%20and%20Iterative%20Imputers%20demonstrate%20the%0Ahighest%20accuracy%20in%20these%20challenging%20scenarios.%20This%20work%20provides%20a%20critical%0Abaseline%20for%20future%20research%20and%20offers%20practical%20recommendations%20for%20improving%0Athe%20integrity%20and%20robustness%20of%20Mo-Cap%20data%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10334v1&entry.124074799=Read"},
{"title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with\n  Multi-Fold Radial Symmetry Textures", "author": "Xinlong Ding and Hongwei Yu and Jiawei Li and Feifan Li and Yu Shang and Bochao Zou and Huimin Ma and Jiansheng Chen", "abstract": "  Camera pose estimation is a fundamental computer vision task that is\nessential for applications like visual localization and multi-view stereo\nreconstruction. In the object-centric scenarios with sparse inputs, the\naccuracy of pose estimation can be significantly influenced by background\ntextures that occupy major portions of the images across different viewpoints.\nIn light of this, we introduce the Kaleidoscopic Background Attack (KBA), which\nuses identical segments to form discs with multi-fold radial symmetry. These\ndiscs maintain high similarity across different viewpoints, enabling effective\nattacks on pose estimation models even with natural texture segments.\nAdditionally, a projected orientation consistency loss is proposed to optimize\nthe kaleidoscopic segments, leading to significant enhancement in the attack\neffectiveness. Experimental results show that optimized adversarial\nkaleidoscopic backgrounds can effectively attack various camera pose estimation\nmodels.\n", "link": "http://arxiv.org/abs/2507.10265v1", "date": "2025-07-14", "relevancy": 2.0576, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5223}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5134}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kaleidoscopic%20Background%20Attack%3A%20Disrupting%20Pose%20Estimation%20with%0A%20%20Multi-Fold%20Radial%20Symmetry%20Textures&body=Title%3A%20Kaleidoscopic%20Background%20Attack%3A%20Disrupting%20Pose%20Estimation%20with%0A%20%20Multi-Fold%20Radial%20Symmetry%20Textures%0AAuthor%3A%20Xinlong%20Ding%20and%20Hongwei%20Yu%20and%20Jiawei%20Li%20and%20Feifan%20Li%20and%20Yu%20Shang%20and%20Bochao%20Zou%20and%20Huimin%20Ma%20and%20Jiansheng%20Chen%0AAbstract%3A%20%20%20Camera%20pose%20estimation%20is%20a%20fundamental%20computer%20vision%20task%20that%20is%0Aessential%20for%20applications%20like%20visual%20localization%20and%20multi-view%20stereo%0Areconstruction.%20In%20the%20object-centric%20scenarios%20with%20sparse%20inputs%2C%20the%0Aaccuracy%20of%20pose%20estimation%20can%20be%20significantly%20influenced%20by%20background%0Atextures%20that%20occupy%20major%20portions%20of%20the%20images%20across%20different%20viewpoints.%0AIn%20light%20of%20this%2C%20we%20introduce%20the%20Kaleidoscopic%20Background%20Attack%20%28KBA%29%2C%20which%0Auses%20identical%20segments%20to%20form%20discs%20with%20multi-fold%20radial%20symmetry.%20These%0Adiscs%20maintain%20high%20similarity%20across%20different%20viewpoints%2C%20enabling%20effective%0Aattacks%20on%20pose%20estimation%20models%20even%20with%20natural%20texture%20segments.%0AAdditionally%2C%20a%20projected%20orientation%20consistency%20loss%20is%20proposed%20to%20optimize%0Athe%20kaleidoscopic%20segments%2C%20leading%20to%20significant%20enhancement%20in%20the%20attack%0Aeffectiveness.%20Experimental%20results%20show%20that%20optimized%20adversarial%0Akaleidoscopic%20backgrounds%20can%20effectively%20attack%20various%20camera%20pose%20estimation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKaleidoscopic%2520Background%2520Attack%253A%2520Disrupting%2520Pose%2520Estimation%2520with%250A%2520%2520Multi-Fold%2520Radial%2520Symmetry%2520Textures%26entry.906535625%3DXinlong%2520Ding%2520and%2520Hongwei%2520Yu%2520and%2520Jiawei%2520Li%2520and%2520Feifan%2520Li%2520and%2520Yu%2520Shang%2520and%2520Bochao%2520Zou%2520and%2520Huimin%2520Ma%2520and%2520Jiansheng%2520Chen%26entry.1292438233%3D%2520%2520Camera%2520pose%2520estimation%2520is%2520a%2520fundamental%2520computer%2520vision%2520task%2520that%2520is%250Aessential%2520for%2520applications%2520like%2520visual%2520localization%2520and%2520multi-view%2520stereo%250Areconstruction.%2520In%2520the%2520object-centric%2520scenarios%2520with%2520sparse%2520inputs%252C%2520the%250Aaccuracy%2520of%2520pose%2520estimation%2520can%2520be%2520significantly%2520influenced%2520by%2520background%250Atextures%2520that%2520occupy%2520major%2520portions%2520of%2520the%2520images%2520across%2520different%2520viewpoints.%250AIn%2520light%2520of%2520this%252C%2520we%2520introduce%2520the%2520Kaleidoscopic%2520Background%2520Attack%2520%2528KBA%2529%252C%2520which%250Auses%2520identical%2520segments%2520to%2520form%2520discs%2520with%2520multi-fold%2520radial%2520symmetry.%2520These%250Adiscs%2520maintain%2520high%2520similarity%2520across%2520different%2520viewpoints%252C%2520enabling%2520effective%250Aattacks%2520on%2520pose%2520estimation%2520models%2520even%2520with%2520natural%2520texture%2520segments.%250AAdditionally%252C%2520a%2520projected%2520orientation%2520consistency%2520loss%2520is%2520proposed%2520to%2520optimize%250Athe%2520kaleidoscopic%2520segments%252C%2520leading%2520to%2520significant%2520enhancement%2520in%2520the%2520attack%250Aeffectiveness.%2520Experimental%2520results%2520show%2520that%2520optimized%2520adversarial%250Akaleidoscopic%2520backgrounds%2520can%2520effectively%2520attack%2520various%2520camera%2520pose%2520estimation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kaleidoscopic%20Background%20Attack%3A%20Disrupting%20Pose%20Estimation%20with%0A%20%20Multi-Fold%20Radial%20Symmetry%20Textures&entry.906535625=Xinlong%20Ding%20and%20Hongwei%20Yu%20and%20Jiawei%20Li%20and%20Feifan%20Li%20and%20Yu%20Shang%20and%20Bochao%20Zou%20and%20Huimin%20Ma%20and%20Jiansheng%20Chen&entry.1292438233=%20%20Camera%20pose%20estimation%20is%20a%20fundamental%20computer%20vision%20task%20that%20is%0Aessential%20for%20applications%20like%20visual%20localization%20and%20multi-view%20stereo%0Areconstruction.%20In%20the%20object-centric%20scenarios%20with%20sparse%20inputs%2C%20the%0Aaccuracy%20of%20pose%20estimation%20can%20be%20significantly%20influenced%20by%20background%0Atextures%20that%20occupy%20major%20portions%20of%20the%20images%20across%20different%20viewpoints.%0AIn%20light%20of%20this%2C%20we%20introduce%20the%20Kaleidoscopic%20Background%20Attack%20%28KBA%29%2C%20which%0Auses%20identical%20segments%20to%20form%20discs%20with%20multi-fold%20radial%20symmetry.%20These%0Adiscs%20maintain%20high%20similarity%20across%20different%20viewpoints%2C%20enabling%20effective%0Aattacks%20on%20pose%20estimation%20models%20even%20with%20natural%20texture%20segments.%0AAdditionally%2C%20a%20projected%20orientation%20consistency%20loss%20is%20proposed%20to%20optimize%0Athe%20kaleidoscopic%20segments%2C%20leading%20to%20significant%20enhancement%20in%20the%20attack%0Aeffectiveness.%20Experimental%20results%20show%20that%20optimized%20adversarial%0Akaleidoscopic%20backgrounds%20can%20effectively%20attack%20various%20camera%20pose%20estimation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10265v1&entry.124074799=Read"},
{"title": "SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial\n  Examples", "author": "Dren Fazlija and Monty-Maximilian Z\u00fchlke and Johanna Schrader and Arkadij Orlov and Clara Stein and Iyiola E. Olatunji and Daniel Kudenko", "abstract": "  Unrestricted adversarial attacks aim to fool computer vision models without\nbeing constrained by $\\ell_p$-norm bounds to remain imperceptible to humans,\nfor example, by changing an object's color. This allows attackers to circumvent\ntraditional, norm-bounded defense strategies such as adversarial training or\ncertified defense strategies. However, due to their unrestricted nature, there\nare also no guarantees of norm-based imperceptibility, necessitating human\nevaluations to verify just how authentic these adversarial examples look. While\nsome related work assesses this vital quality of adversarial attacks, none\nprovide statistically significant insights. This issue necessitates a unified\nframework that supports and streamlines such an assessment for evaluating and\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\nopen-source, statistically powered framework for evaluating unrestricted\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\ncrowd-study power, compensation, and Likert equivalence bounds to measure\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\nacross 346 human participants showing that three color-space attacks and three\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\nonly consistently detects adversarial examples for four out of six tested\nattacks; $(iii)$ open-source software tools, including a browser-based task\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\nexamples, and over 34K human ratings. Our findings demonstrate that automated\nvision systems do not align with human perception, reinforcing the need for a\nground-truth SCOOTER benchmark.\n", "link": "http://arxiv.org/abs/2507.07776v2", "date": "2025-07-14", "relevancy": 2.0419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCOOTER%3A%20A%20Human%20Evaluation%20Framework%20for%20Unrestricted%20Adversarial%0A%20%20Examples&body=Title%3A%20SCOOTER%3A%20A%20Human%20Evaluation%20Framework%20for%20Unrestricted%20Adversarial%0A%20%20Examples%0AAuthor%3A%20Dren%20Fazlija%20and%20Monty-Maximilian%20Z%C3%BChlke%20and%20Johanna%20Schrader%20and%20Arkadij%20Orlov%20and%20Clara%20Stein%20and%20Iyiola%20E.%20Olatunji%20and%20Daniel%20Kudenko%0AAbstract%3A%20%20%20Unrestricted%20adversarial%20attacks%20aim%20to%20fool%20computer%20vision%20models%20without%0Abeing%20constrained%20by%20%24%5Cell_p%24-norm%20bounds%20to%20remain%20imperceptible%20to%20humans%2C%0Afor%20example%2C%20by%20changing%20an%20object%27s%20color.%20This%20allows%20attackers%20to%20circumvent%0Atraditional%2C%20norm-bounded%20defense%20strategies%20such%20as%20adversarial%20training%20or%0Acertified%20defense%20strategies.%20However%2C%20due%20to%20their%20unrestricted%20nature%2C%20there%0Aare%20also%20no%20guarantees%20of%20norm-based%20imperceptibility%2C%20necessitating%20human%0Aevaluations%20to%20verify%20just%20how%20authentic%20these%20adversarial%20examples%20look.%20While%0Asome%20related%20work%20assesses%20this%20vital%20quality%20of%20adversarial%20attacks%2C%20none%0Aprovide%20statistically%20significant%20insights.%20This%20issue%20necessitates%20a%20unified%0Aframework%20that%20supports%20and%20streamlines%20such%20an%20assessment%20for%20evaluating%20and%0Acomparing%20unrestricted%20attacks.%20To%20close%20this%20gap%2C%20we%20introduce%20SCOOTER%20-%20an%0Aopen-source%2C%20statistically%20powered%20framework%20for%20evaluating%20unrestricted%0Aadversarial%20examples.%20Our%20contributions%20are%3A%20%24%28i%29%24%20best-practice%20guidelines%20for%0Acrowd-study%20power%2C%20compensation%2C%20and%20Likert%20equivalence%20bounds%20to%20measure%0Aimperceptibility%3B%20%24%28ii%29%24%20the%20first%20large-scale%20human%20vs.%20model%20comparison%0Aacross%20346%20human%20participants%20showing%20that%20three%20color-space%20attacks%20and%20three%0Adiffusion-based%20attacks%20fail%20to%20produce%20imperceptible%20images.%20Furthermore%2C%20we%0Afound%20that%20GPT-4o%20can%20serve%20as%20a%20preliminary%20test%20for%20imperceptibility%2C%20but%20it%0Aonly%20consistently%20detects%20adversarial%20examples%20for%20four%20out%20of%20six%20tested%0Aattacks%3B%20%24%28iii%29%24%20open-source%20software%20tools%2C%20including%20a%20browser-based%20task%0Atemplate%20to%20collect%20annotations%20and%20analysis%20scripts%20in%20Python%20and%20R%3B%20%24%28iv%29%24%20an%0AImageNet-derived%20benchmark%20dataset%20containing%203K%20real%20images%2C%207K%20adversarial%0Aexamples%2C%20and%20over%2034K%20human%20ratings.%20Our%20findings%20demonstrate%20that%20automated%0Avision%20systems%20do%20not%20align%20with%20human%20perception%2C%20reinforcing%20the%20need%20for%20a%0Aground-truth%20SCOOTER%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07776v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCOOTER%253A%2520A%2520Human%2520Evaluation%2520Framework%2520for%2520Unrestricted%2520Adversarial%250A%2520%2520Examples%26entry.906535625%3DDren%2520Fazlija%2520and%2520Monty-Maximilian%2520Z%25C3%25BChlke%2520and%2520Johanna%2520Schrader%2520and%2520Arkadij%2520Orlov%2520and%2520Clara%2520Stein%2520and%2520Iyiola%2520E.%2520Olatunji%2520and%2520Daniel%2520Kudenko%26entry.1292438233%3D%2520%2520Unrestricted%2520adversarial%2520attacks%2520aim%2520to%2520fool%2520computer%2520vision%2520models%2520without%250Abeing%2520constrained%2520by%2520%2524%255Cell_p%2524-norm%2520bounds%2520to%2520remain%2520imperceptible%2520to%2520humans%252C%250Afor%2520example%252C%2520by%2520changing%2520an%2520object%2527s%2520color.%2520This%2520allows%2520attackers%2520to%2520circumvent%250Atraditional%252C%2520norm-bounded%2520defense%2520strategies%2520such%2520as%2520adversarial%2520training%2520or%250Acertified%2520defense%2520strategies.%2520However%252C%2520due%2520to%2520their%2520unrestricted%2520nature%252C%2520there%250Aare%2520also%2520no%2520guarantees%2520of%2520norm-based%2520imperceptibility%252C%2520necessitating%2520human%250Aevaluations%2520to%2520verify%2520just%2520how%2520authentic%2520these%2520adversarial%2520examples%2520look.%2520While%250Asome%2520related%2520work%2520assesses%2520this%2520vital%2520quality%2520of%2520adversarial%2520attacks%252C%2520none%250Aprovide%2520statistically%2520significant%2520insights.%2520This%2520issue%2520necessitates%2520a%2520unified%250Aframework%2520that%2520supports%2520and%2520streamlines%2520such%2520an%2520assessment%2520for%2520evaluating%2520and%250Acomparing%2520unrestricted%2520attacks.%2520To%2520close%2520this%2520gap%252C%2520we%2520introduce%2520SCOOTER%2520-%2520an%250Aopen-source%252C%2520statistically%2520powered%2520framework%2520for%2520evaluating%2520unrestricted%250Aadversarial%2520examples.%2520Our%2520contributions%2520are%253A%2520%2524%2528i%2529%2524%2520best-practice%2520guidelines%2520for%250Acrowd-study%2520power%252C%2520compensation%252C%2520and%2520Likert%2520equivalence%2520bounds%2520to%2520measure%250Aimperceptibility%253B%2520%2524%2528ii%2529%2524%2520the%2520first%2520large-scale%2520human%2520vs.%2520model%2520comparison%250Aacross%2520346%2520human%2520participants%2520showing%2520that%2520three%2520color-space%2520attacks%2520and%2520three%250Adiffusion-based%2520attacks%2520fail%2520to%2520produce%2520imperceptible%2520images.%2520Furthermore%252C%2520we%250Afound%2520that%2520GPT-4o%2520can%2520serve%2520as%2520a%2520preliminary%2520test%2520for%2520imperceptibility%252C%2520but%2520it%250Aonly%2520consistently%2520detects%2520adversarial%2520examples%2520for%2520four%2520out%2520of%2520six%2520tested%250Aattacks%253B%2520%2524%2528iii%2529%2524%2520open-source%2520software%2520tools%252C%2520including%2520a%2520browser-based%2520task%250Atemplate%2520to%2520collect%2520annotations%2520and%2520analysis%2520scripts%2520in%2520Python%2520and%2520R%253B%2520%2524%2528iv%2529%2524%2520an%250AImageNet-derived%2520benchmark%2520dataset%2520containing%25203K%2520real%2520images%252C%25207K%2520adversarial%250Aexamples%252C%2520and%2520over%252034K%2520human%2520ratings.%2520Our%2520findings%2520demonstrate%2520that%2520automated%250Avision%2520systems%2520do%2520not%2520align%2520with%2520human%2520perception%252C%2520reinforcing%2520the%2520need%2520for%2520a%250Aground-truth%2520SCOOTER%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07776v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCOOTER%3A%20A%20Human%20Evaluation%20Framework%20for%20Unrestricted%20Adversarial%0A%20%20Examples&entry.906535625=Dren%20Fazlija%20and%20Monty-Maximilian%20Z%C3%BChlke%20and%20Johanna%20Schrader%20and%20Arkadij%20Orlov%20and%20Clara%20Stein%20and%20Iyiola%20E.%20Olatunji%20and%20Daniel%20Kudenko&entry.1292438233=%20%20Unrestricted%20adversarial%20attacks%20aim%20to%20fool%20computer%20vision%20models%20without%0Abeing%20constrained%20by%20%24%5Cell_p%24-norm%20bounds%20to%20remain%20imperceptible%20to%20humans%2C%0Afor%20example%2C%20by%20changing%20an%20object%27s%20color.%20This%20allows%20attackers%20to%20circumvent%0Atraditional%2C%20norm-bounded%20defense%20strategies%20such%20as%20adversarial%20training%20or%0Acertified%20defense%20strategies.%20However%2C%20due%20to%20their%20unrestricted%20nature%2C%20there%0Aare%20also%20no%20guarantees%20of%20norm-based%20imperceptibility%2C%20necessitating%20human%0Aevaluations%20to%20verify%20just%20how%20authentic%20these%20adversarial%20examples%20look.%20While%0Asome%20related%20work%20assesses%20this%20vital%20quality%20of%20adversarial%20attacks%2C%20none%0Aprovide%20statistically%20significant%20insights.%20This%20issue%20necessitates%20a%20unified%0Aframework%20that%20supports%20and%20streamlines%20such%20an%20assessment%20for%20evaluating%20and%0Acomparing%20unrestricted%20attacks.%20To%20close%20this%20gap%2C%20we%20introduce%20SCOOTER%20-%20an%0Aopen-source%2C%20statistically%20powered%20framework%20for%20evaluating%20unrestricted%0Aadversarial%20examples.%20Our%20contributions%20are%3A%20%24%28i%29%24%20best-practice%20guidelines%20for%0Acrowd-study%20power%2C%20compensation%2C%20and%20Likert%20equivalence%20bounds%20to%20measure%0Aimperceptibility%3B%20%24%28ii%29%24%20the%20first%20large-scale%20human%20vs.%20model%20comparison%0Aacross%20346%20human%20participants%20showing%20that%20three%20color-space%20attacks%20and%20three%0Adiffusion-based%20attacks%20fail%20to%20produce%20imperceptible%20images.%20Furthermore%2C%20we%0Afound%20that%20GPT-4o%20can%20serve%20as%20a%20preliminary%20test%20for%20imperceptibility%2C%20but%20it%0Aonly%20consistently%20detects%20adversarial%20examples%20for%20four%20out%20of%20six%20tested%0Aattacks%3B%20%24%28iii%29%24%20open-source%20software%20tools%2C%20including%20a%20browser-based%20task%0Atemplate%20to%20collect%20annotations%20and%20analysis%20scripts%20in%20Python%20and%20R%3B%20%24%28iv%29%24%20an%0AImageNet-derived%20benchmark%20dataset%20containing%203K%20real%20images%2C%207K%20adversarial%0Aexamples%2C%20and%20over%2034K%20human%20ratings.%20Our%20findings%20demonstrate%20that%20automated%0Avision%20systems%20do%20not%20align%20with%20human%20perception%2C%20reinforcing%20the%20need%20for%20a%0Aground-truth%20SCOOTER%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07776v2&entry.124074799=Read"},
{"title": "Zero-Shot Cyclic Peptide Design via Composable Geometric Constraints", "author": "Dapeng Jiang and Xiangzhe Kong and Jiaqi Han and Mingyu Li and Rui Jiao and Wenbing Huang and Stefano Ermon and Jianzhu Ma and Yang Liu", "abstract": "  Cyclic peptides, characterized by geometric constraints absent in linear\npeptides, offer enhanced biochemical properties, presenting new opportunities\nto address unmet medical needs. However, designing target-specific cyclic\npeptides remains underexplored due to limited training data. To bridge the gap,\nwe propose CP-Composer, a novel generative framework that enables zero-shot\ncyclic peptide generation via composable geometric constraints. Our approach\ndecomposes complex cyclization patterns into unit constraints, which are\nincorporated into a diffusion model through geometric conditioning on nodes and\nedges. During training, the model learns from unit constraints and their random\ncombinations in linear peptides, while at inference, novel constraint\ncombinations required for cyclization are imposed as input. Experiments show\nthat our model, despite trained with linear peptides, is capable of generating\ndiverse target-binding cyclic peptides, reaching success rates from 38% to 84%\non different cyclization strategies.\n", "link": "http://arxiv.org/abs/2507.04225v2", "date": "2025-07-14", "relevancy": 2.0312, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5574}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5107}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Cyclic%20Peptide%20Design%20via%20Composable%20Geometric%20Constraints&body=Title%3A%20Zero-Shot%20Cyclic%20Peptide%20Design%20via%20Composable%20Geometric%20Constraints%0AAuthor%3A%20Dapeng%20Jiang%20and%20Xiangzhe%20Kong%20and%20Jiaqi%20Han%20and%20Mingyu%20Li%20and%20Rui%20Jiao%20and%20Wenbing%20Huang%20and%20Stefano%20Ermon%20and%20Jianzhu%20Ma%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Cyclic%20peptides%2C%20characterized%20by%20geometric%20constraints%20absent%20in%20linear%0Apeptides%2C%20offer%20enhanced%20biochemical%20properties%2C%20presenting%20new%20opportunities%0Ato%20address%20unmet%20medical%20needs.%20However%2C%20designing%20target-specific%20cyclic%0Apeptides%20remains%20underexplored%20due%20to%20limited%20training%20data.%20To%20bridge%20the%20gap%2C%0Awe%20propose%20CP-Composer%2C%20a%20novel%20generative%20framework%20that%20enables%20zero-shot%0Acyclic%20peptide%20generation%20via%20composable%20geometric%20constraints.%20Our%20approach%0Adecomposes%20complex%20cyclization%20patterns%20into%20unit%20constraints%2C%20which%20are%0Aincorporated%20into%20a%20diffusion%20model%20through%20geometric%20conditioning%20on%20nodes%20and%0Aedges.%20During%20training%2C%20the%20model%20learns%20from%20unit%20constraints%20and%20their%20random%0Acombinations%20in%20linear%20peptides%2C%20while%20at%20inference%2C%20novel%20constraint%0Acombinations%20required%20for%20cyclization%20are%20imposed%20as%20input.%20Experiments%20show%0Athat%20our%20model%2C%20despite%20trained%20with%20linear%20peptides%2C%20is%20capable%20of%20generating%0Adiverse%20target-binding%20cyclic%20peptides%2C%20reaching%20success%20rates%20from%2038%25%20to%2084%25%0Aon%20different%20cyclization%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Cyclic%2520Peptide%2520Design%2520via%2520Composable%2520Geometric%2520Constraints%26entry.906535625%3DDapeng%2520Jiang%2520and%2520Xiangzhe%2520Kong%2520and%2520Jiaqi%2520Han%2520and%2520Mingyu%2520Li%2520and%2520Rui%2520Jiao%2520and%2520Wenbing%2520Huang%2520and%2520Stefano%2520Ermon%2520and%2520Jianzhu%2520Ma%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Cyclic%2520peptides%252C%2520characterized%2520by%2520geometric%2520constraints%2520absent%2520in%2520linear%250Apeptides%252C%2520offer%2520enhanced%2520biochemical%2520properties%252C%2520presenting%2520new%2520opportunities%250Ato%2520address%2520unmet%2520medical%2520needs.%2520However%252C%2520designing%2520target-specific%2520cyclic%250Apeptides%2520remains%2520underexplored%2520due%2520to%2520limited%2520training%2520data.%2520To%2520bridge%2520the%2520gap%252C%250Awe%2520propose%2520CP-Composer%252C%2520a%2520novel%2520generative%2520framework%2520that%2520enables%2520zero-shot%250Acyclic%2520peptide%2520generation%2520via%2520composable%2520geometric%2520constraints.%2520Our%2520approach%250Adecomposes%2520complex%2520cyclization%2520patterns%2520into%2520unit%2520constraints%252C%2520which%2520are%250Aincorporated%2520into%2520a%2520diffusion%2520model%2520through%2520geometric%2520conditioning%2520on%2520nodes%2520and%250Aedges.%2520During%2520training%252C%2520the%2520model%2520learns%2520from%2520unit%2520constraints%2520and%2520their%2520random%250Acombinations%2520in%2520linear%2520peptides%252C%2520while%2520at%2520inference%252C%2520novel%2520constraint%250Acombinations%2520required%2520for%2520cyclization%2520are%2520imposed%2520as%2520input.%2520Experiments%2520show%250Athat%2520our%2520model%252C%2520despite%2520trained%2520with%2520linear%2520peptides%252C%2520is%2520capable%2520of%2520generating%250Adiverse%2520target-binding%2520cyclic%2520peptides%252C%2520reaching%2520success%2520rates%2520from%252038%2525%2520to%252084%2525%250Aon%2520different%2520cyclization%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Cyclic%20Peptide%20Design%20via%20Composable%20Geometric%20Constraints&entry.906535625=Dapeng%20Jiang%20and%20Xiangzhe%20Kong%20and%20Jiaqi%20Han%20and%20Mingyu%20Li%20and%20Rui%20Jiao%20and%20Wenbing%20Huang%20and%20Stefano%20Ermon%20and%20Jianzhu%20Ma%20and%20Yang%20Liu&entry.1292438233=%20%20Cyclic%20peptides%2C%20characterized%20by%20geometric%20constraints%20absent%20in%20linear%0Apeptides%2C%20offer%20enhanced%20biochemical%20properties%2C%20presenting%20new%20opportunities%0Ato%20address%20unmet%20medical%20needs.%20However%2C%20designing%20target-specific%20cyclic%0Apeptides%20remains%20underexplored%20due%20to%20limited%20training%20data.%20To%20bridge%20the%20gap%2C%0Awe%20propose%20CP-Composer%2C%20a%20novel%20generative%20framework%20that%20enables%20zero-shot%0Acyclic%20peptide%20generation%20via%20composable%20geometric%20constraints.%20Our%20approach%0Adecomposes%20complex%20cyclization%20patterns%20into%20unit%20constraints%2C%20which%20are%0Aincorporated%20into%20a%20diffusion%20model%20through%20geometric%20conditioning%20on%20nodes%20and%0Aedges.%20During%20training%2C%20the%20model%20learns%20from%20unit%20constraints%20and%20their%20random%0Acombinations%20in%20linear%20peptides%2C%20while%20at%20inference%2C%20novel%20constraint%0Acombinations%20required%20for%20cyclization%20are%20imposed%20as%20input.%20Experiments%20show%0Athat%20our%20model%2C%20despite%20trained%20with%20linear%20peptides%2C%20is%20capable%20of%20generating%0Adiverse%20target-binding%20cyclic%20peptides%2C%20reaching%20success%20rates%20from%2038%25%20to%2084%25%0Aon%20different%20cyclization%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04225v2&entry.124074799=Read"},
{"title": "TOP: Trajectory Optimization via Parallel Optimization towards Constant\n  Time Complexity", "author": "Jiajun Yu and Nanhe Chen and Guodong Liu and Chao Xu and Fei Gao and Yanjun Cao", "abstract": "  Optimization has been widely used to generate smooth trajectories for motion\nplanning. However, existing trajectory optimization methods show weakness when\ndealing with large-scale long trajectories. Recent advances in parallel\ncomputing have accelerated optimization in some fields, but how to efficiently\nsolve trajectory optimization via parallelism remains an open question. In this\npaper, we propose a novel trajectory optimization framework based on the\nConsensus Alternating Direction Method of Multipliers (CADMM) algorithm, which\ndecomposes the trajectory into multiple segments and solves the subproblems in\nparallel. The proposed framework reduces the time complexity to O(1) per\niteration to the number of segments, compared to O(N) of the state-of-the-art\n(SOTA) approaches. Furthermore, we introduce a closed-form solution that\nintegrates convex linear and quadratic constraints to speed up the\noptimization, and we also present numerical solutions for general inequality\nconstraints. A series of simulations and experiments demonstrate that our\napproach outperforms the SOTA approach in terms of efficiency and smoothness.\nEspecially for a large-scale trajectory, with one hundred segments, achieving\nover a tenfold speedup. To fully explore the potential of our algorithm on\nmodern parallel computing architectures, we deploy our framework on a GPU and\nshow high performance with thousands of segments.\n", "link": "http://arxiv.org/abs/2507.10290v1", "date": "2025-07-14", "relevancy": 1.9149, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4914}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4789}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TOP%3A%20Trajectory%20Optimization%20via%20Parallel%20Optimization%20towards%20Constant%0A%20%20Time%20Complexity&body=Title%3A%20TOP%3A%20Trajectory%20Optimization%20via%20Parallel%20Optimization%20towards%20Constant%0A%20%20Time%20Complexity%0AAuthor%3A%20Jiajun%20Yu%20and%20Nanhe%20Chen%20and%20Guodong%20Liu%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao%0AAbstract%3A%20%20%20Optimization%20has%20been%20widely%20used%20to%20generate%20smooth%20trajectories%20for%20motion%0Aplanning.%20However%2C%20existing%20trajectory%20optimization%20methods%20show%20weakness%20when%0Adealing%20with%20large-scale%20long%20trajectories.%20Recent%20advances%20in%20parallel%0Acomputing%20have%20accelerated%20optimization%20in%20some%20fields%2C%20but%20how%20to%20efficiently%0Asolve%20trajectory%20optimization%20via%20parallelism%20remains%20an%20open%20question.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20trajectory%20optimization%20framework%20based%20on%20the%0AConsensus%20Alternating%20Direction%20Method%20of%20Multipliers%20%28CADMM%29%20algorithm%2C%20which%0Adecomposes%20the%20trajectory%20into%20multiple%20segments%20and%20solves%20the%20subproblems%20in%0Aparallel.%20The%20proposed%20framework%20reduces%20the%20time%20complexity%20to%20O%281%29%20per%0Aiteration%20to%20the%20number%20of%20segments%2C%20compared%20to%20O%28N%29%20of%20the%20state-of-the-art%0A%28SOTA%29%20approaches.%20Furthermore%2C%20we%20introduce%20a%20closed-form%20solution%20that%0Aintegrates%20convex%20linear%20and%20quadratic%20constraints%20to%20speed%20up%20the%0Aoptimization%2C%20and%20we%20also%20present%20numerical%20solutions%20for%20general%20inequality%0Aconstraints.%20A%20series%20of%20simulations%20and%20experiments%20demonstrate%20that%20our%0Aapproach%20outperforms%20the%20SOTA%20approach%20in%20terms%20of%20efficiency%20and%20smoothness.%0AEspecially%20for%20a%20large-scale%20trajectory%2C%20with%20one%20hundred%20segments%2C%20achieving%0Aover%20a%20tenfold%20speedup.%20To%20fully%20explore%20the%20potential%20of%20our%20algorithm%20on%0Amodern%20parallel%20computing%20architectures%2C%20we%20deploy%20our%20framework%20on%20a%20GPU%20and%0Ashow%20high%20performance%20with%20thousands%20of%20segments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTOP%253A%2520Trajectory%2520Optimization%2520via%2520Parallel%2520Optimization%2520towards%2520Constant%250A%2520%2520Time%2520Complexity%26entry.906535625%3DJiajun%2520Yu%2520and%2520Nanhe%2520Chen%2520and%2520Guodong%2520Liu%2520and%2520Chao%2520Xu%2520and%2520Fei%2520Gao%2520and%2520Yanjun%2520Cao%26entry.1292438233%3D%2520%2520Optimization%2520has%2520been%2520widely%2520used%2520to%2520generate%2520smooth%2520trajectories%2520for%2520motion%250Aplanning.%2520However%252C%2520existing%2520trajectory%2520optimization%2520methods%2520show%2520weakness%2520when%250Adealing%2520with%2520large-scale%2520long%2520trajectories.%2520Recent%2520advances%2520in%2520parallel%250Acomputing%2520have%2520accelerated%2520optimization%2520in%2520some%2520fields%252C%2520but%2520how%2520to%2520efficiently%250Asolve%2520trajectory%2520optimization%2520via%2520parallelism%2520remains%2520an%2520open%2520question.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520trajectory%2520optimization%2520framework%2520based%2520on%2520the%250AConsensus%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%2520%2528CADMM%2529%2520algorithm%252C%2520which%250Adecomposes%2520the%2520trajectory%2520into%2520multiple%2520segments%2520and%2520solves%2520the%2520subproblems%2520in%250Aparallel.%2520The%2520proposed%2520framework%2520reduces%2520the%2520time%2520complexity%2520to%2520O%25281%2529%2520per%250Aiteration%2520to%2520the%2520number%2520of%2520segments%252C%2520compared%2520to%2520O%2528N%2529%2520of%2520the%2520state-of-the-art%250A%2528SOTA%2529%2520approaches.%2520Furthermore%252C%2520we%2520introduce%2520a%2520closed-form%2520solution%2520that%250Aintegrates%2520convex%2520linear%2520and%2520quadratic%2520constraints%2520to%2520speed%2520up%2520the%250Aoptimization%252C%2520and%2520we%2520also%2520present%2520numerical%2520solutions%2520for%2520general%2520inequality%250Aconstraints.%2520A%2520series%2520of%2520simulations%2520and%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520outperforms%2520the%2520SOTA%2520approach%2520in%2520terms%2520of%2520efficiency%2520and%2520smoothness.%250AEspecially%2520for%2520a%2520large-scale%2520trajectory%252C%2520with%2520one%2520hundred%2520segments%252C%2520achieving%250Aover%2520a%2520tenfold%2520speedup.%2520To%2520fully%2520explore%2520the%2520potential%2520of%2520our%2520algorithm%2520on%250Amodern%2520parallel%2520computing%2520architectures%252C%2520we%2520deploy%2520our%2520framework%2520on%2520a%2520GPU%2520and%250Ashow%2520high%2520performance%2520with%2520thousands%2520of%2520segments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TOP%3A%20Trajectory%20Optimization%20via%20Parallel%20Optimization%20towards%20Constant%0A%20%20Time%20Complexity&entry.906535625=Jiajun%20Yu%20and%20Nanhe%20Chen%20and%20Guodong%20Liu%20and%20Chao%20Xu%20and%20Fei%20Gao%20and%20Yanjun%20Cao&entry.1292438233=%20%20Optimization%20has%20been%20widely%20used%20to%20generate%20smooth%20trajectories%20for%20motion%0Aplanning.%20However%2C%20existing%20trajectory%20optimization%20methods%20show%20weakness%20when%0Adealing%20with%20large-scale%20long%20trajectories.%20Recent%20advances%20in%20parallel%0Acomputing%20have%20accelerated%20optimization%20in%20some%20fields%2C%20but%20how%20to%20efficiently%0Asolve%20trajectory%20optimization%20via%20parallelism%20remains%20an%20open%20question.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20trajectory%20optimization%20framework%20based%20on%20the%0AConsensus%20Alternating%20Direction%20Method%20of%20Multipliers%20%28CADMM%29%20algorithm%2C%20which%0Adecomposes%20the%20trajectory%20into%20multiple%20segments%20and%20solves%20the%20subproblems%20in%0Aparallel.%20The%20proposed%20framework%20reduces%20the%20time%20complexity%20to%20O%281%29%20per%0Aiteration%20to%20the%20number%20of%20segments%2C%20compared%20to%20O%28N%29%20of%20the%20state-of-the-art%0A%28SOTA%29%20approaches.%20Furthermore%2C%20we%20introduce%20a%20closed-form%20solution%20that%0Aintegrates%20convex%20linear%20and%20quadratic%20constraints%20to%20speed%20up%20the%0Aoptimization%2C%20and%20we%20also%20present%20numerical%20solutions%20for%20general%20inequality%0Aconstraints.%20A%20series%20of%20simulations%20and%20experiments%20demonstrate%20that%20our%0Aapproach%20outperforms%20the%20SOTA%20approach%20in%20terms%20of%20efficiency%20and%20smoothness.%0AEspecially%20for%20a%20large-scale%20trajectory%2C%20with%20one%20hundred%20segments%2C%20achieving%0Aover%20a%20tenfold%20speedup.%20To%20fully%20explore%20the%20potential%20of%20our%20algorithm%20on%0Amodern%20parallel%20computing%20architectures%2C%20we%20deploy%20our%20framework%20on%20a%20GPU%20and%0Ashow%20high%20performance%20with%20thousands%20of%20segments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10290v1&entry.124074799=Read"},
{"title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout\n  in Distance Learning", "author": "Meriem Zerkouk and Miloud Mihoubi and Belkacem Chikhaoui", "abstract": "  School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance\n", "link": "http://arxiv.org/abs/2507.10421v1", "date": "2025-07-14", "relevancy": 1.5107, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5134}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4915}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SentiDrop%3A%20A%20Multi%20Modal%20Machine%20Learning%20model%20for%20Predicting%20Dropout%0A%20%20in%20Distance%20Learning&body=Title%3A%20SentiDrop%3A%20A%20Multi%20Modal%20Machine%20Learning%20model%20for%20Predicting%20Dropout%0A%20%20in%20Distance%20Learning%0AAuthor%3A%20Meriem%20Zerkouk%20and%20Miloud%20Mihoubi%20and%20Belkacem%20Chikhaoui%0AAbstract%3A%20%20%20School%20dropout%20is%20a%20serious%20problem%20in%20distance%20learning%2C%20where%20early%0Adetection%20is%20crucial%20for%20effective%20intervention%20and%20student%20perseverance.%0APredicting%20student%20dropout%20using%20available%20educational%20data%20is%20a%20widely%0Aresearched%20topic%20in%20learning%20analytics.%20Our%20partner%27s%20distance%20learning%0Aplatform%20highlights%20the%20importance%20of%20integrating%20diverse%20data%20sources%2C%0Aincluding%20socio-demographic%20data%2C%20behavioral%20data%2C%20and%20sentiment%20analysis%2C%20to%0Aaccurately%20predict%20dropout%20risks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20model%0Athat%20combines%20sentiment%20analysis%20of%20student%20comments%20using%20the%20Bidirectional%0AEncoder%20Representations%20from%20Transformers%20%28BERT%29%20model%20with%20socio-demographic%0Aand%20behavioral%20data%20analyzed%20through%20Extreme%20Gradient%20Boosting%20%28XGBoost%29.%20We%0Afine-tuned%20BERT%20on%20student%20comments%20to%20capture%20nuanced%20sentiments%2C%20which%20were%0Athen%20merged%20with%20key%20features%20selected%20using%20feature%20importance%20techniques%20in%0AXGBoost.%20Our%20model%20was%20tested%20on%20unseen%20data%20from%20the%20next%20academic%20year%2C%0Aachieving%20an%20accuracy%20of%2084%5C%25%2C%20compared%20to%2082%5C%25%20for%20the%20baseline%20model.%0AAdditionally%2C%20the%20model%20demonstrated%20superior%20performance%20in%20other%20metrics%2C%0Asuch%20as%20precision%20and%20F1-score.%20The%20proposed%20method%20could%20be%20a%20vital%20tool%20in%0Adeveloping%20personalized%20strategies%20to%20reduce%20dropout%20rates%20and%20encourage%0Astudent%20perseverance%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSentiDrop%253A%2520A%2520Multi%2520Modal%2520Machine%2520Learning%2520model%2520for%2520Predicting%2520Dropout%250A%2520%2520in%2520Distance%2520Learning%26entry.906535625%3DMeriem%2520Zerkouk%2520and%2520Miloud%2520Mihoubi%2520and%2520Belkacem%2520Chikhaoui%26entry.1292438233%3D%2520%2520School%2520dropout%2520is%2520a%2520serious%2520problem%2520in%2520distance%2520learning%252C%2520where%2520early%250Adetection%2520is%2520crucial%2520for%2520effective%2520intervention%2520and%2520student%2520perseverance.%250APredicting%2520student%2520dropout%2520using%2520available%2520educational%2520data%2520is%2520a%2520widely%250Aresearched%2520topic%2520in%2520learning%2520analytics.%2520Our%2520partner%2527s%2520distance%2520learning%250Aplatform%2520highlights%2520the%2520importance%2520of%2520integrating%2520diverse%2520data%2520sources%252C%250Aincluding%2520socio-demographic%2520data%252C%2520behavioral%2520data%252C%2520and%2520sentiment%2520analysis%252C%2520to%250Aaccurately%2520predict%2520dropout%2520risks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520model%250Athat%2520combines%2520sentiment%2520analysis%2520of%2520student%2520comments%2520using%2520the%2520Bidirectional%250AEncoder%2520Representations%2520from%2520Transformers%2520%2528BERT%2529%2520model%2520with%2520socio-demographic%250Aand%2520behavioral%2520data%2520analyzed%2520through%2520Extreme%2520Gradient%2520Boosting%2520%2528XGBoost%2529.%2520We%250Afine-tuned%2520BERT%2520on%2520student%2520comments%2520to%2520capture%2520nuanced%2520sentiments%252C%2520which%2520were%250Athen%2520merged%2520with%2520key%2520features%2520selected%2520using%2520feature%2520importance%2520techniques%2520in%250AXGBoost.%2520Our%2520model%2520was%2520tested%2520on%2520unseen%2520data%2520from%2520the%2520next%2520academic%2520year%252C%250Aachieving%2520an%2520accuracy%2520of%252084%255C%2525%252C%2520compared%2520to%252082%255C%2525%2520for%2520the%2520baseline%2520model.%250AAdditionally%252C%2520the%2520model%2520demonstrated%2520superior%2520performance%2520in%2520other%2520metrics%252C%250Asuch%2520as%2520precision%2520and%2520F1-score.%2520The%2520proposed%2520method%2520could%2520be%2520a%2520vital%2520tool%2520in%250Adeveloping%2520personalized%2520strategies%2520to%2520reduce%2520dropout%2520rates%2520and%2520encourage%250Astudent%2520perseverance%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SentiDrop%3A%20A%20Multi%20Modal%20Machine%20Learning%20model%20for%20Predicting%20Dropout%0A%20%20in%20Distance%20Learning&entry.906535625=Meriem%20Zerkouk%20and%20Miloud%20Mihoubi%20and%20Belkacem%20Chikhaoui&entry.1292438233=%20%20School%20dropout%20is%20a%20serious%20problem%20in%20distance%20learning%2C%20where%20early%0Adetection%20is%20crucial%20for%20effective%20intervention%20and%20student%20perseverance.%0APredicting%20student%20dropout%20using%20available%20educational%20data%20is%20a%20widely%0Aresearched%20topic%20in%20learning%20analytics.%20Our%20partner%27s%20distance%20learning%0Aplatform%20highlights%20the%20importance%20of%20integrating%20diverse%20data%20sources%2C%0Aincluding%20socio-demographic%20data%2C%20behavioral%20data%2C%20and%20sentiment%20analysis%2C%20to%0Aaccurately%20predict%20dropout%20risks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20model%0Athat%20combines%20sentiment%20analysis%20of%20student%20comments%20using%20the%20Bidirectional%0AEncoder%20Representations%20from%20Transformers%20%28BERT%29%20model%20with%20socio-demographic%0Aand%20behavioral%20data%20analyzed%20through%20Extreme%20Gradient%20Boosting%20%28XGBoost%29.%20We%0Afine-tuned%20BERT%20on%20student%20comments%20to%20capture%20nuanced%20sentiments%2C%20which%20were%0Athen%20merged%20with%20key%20features%20selected%20using%20feature%20importance%20techniques%20in%0AXGBoost.%20Our%20model%20was%20tested%20on%20unseen%20data%20from%20the%20next%20academic%20year%2C%0Aachieving%20an%20accuracy%20of%2084%5C%25%2C%20compared%20to%2082%5C%25%20for%20the%20baseline%20model.%0AAdditionally%2C%20the%20model%20demonstrated%20superior%20performance%20in%20other%20metrics%2C%0Asuch%20as%20precision%20and%20F1-score.%20The%20proposed%20method%20could%20be%20a%20vital%20tool%20in%0Adeveloping%20personalized%20strategies%20to%20reduce%20dropout%20rates%20and%20encourage%0Astudent%20perseverance%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10421v1&entry.124074799=Read"},
{"title": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs", "author": "Alireza Dizaji and Benedict Aaron Tjandra and Mehrab Hamidi and Shenyang Huang and Guillaume Rabusseau", "abstract": "  Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB.\n", "link": "http://arxiv.org/abs/2507.10183v1", "date": "2025-07-14", "relevancy": 1.909, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.469}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-GRAB%3A%20A%20Synthetic%20Diagnostic%20Benchmark%20for%20Learning%20on%20Temporal%20Graphs&body=Title%3A%20T-GRAB%3A%20A%20Synthetic%20Diagnostic%20Benchmark%20for%20Learning%20on%20Temporal%20Graphs%0AAuthor%3A%20Alireza%20Dizaji%20and%20Benedict%20Aaron%20Tjandra%20and%20Mehrab%20Hamidi%20and%20Shenyang%20Huang%20and%20Guillaume%20Rabusseau%0AAbstract%3A%20%20%20Dynamic%20graph%20learning%20methods%20have%20recently%20emerged%20as%20powerful%20tools%20for%0Amodelling%20relational%20data%20evolving%20through%20time.%20However%2C%20despite%20extensive%0Abenchmarking%20efforts%2C%20it%20remains%20unclear%20whether%20current%20Temporal%20Graph%20Neural%0ANetworks%20%28TGNNs%29%20effectively%20capture%20core%20temporal%20patterns%20such%20as%0Aperiodicity%2C%20cause-and-effect%2C%20and%20long-range%20dependencies.%20In%20this%20work%2C%20we%0Aintroduce%20the%20Temporal%20Graph%20Reasoning%20Benchmark%20%28T-GRAB%29%2C%20a%20comprehensive%20set%0Aof%20synthetic%20tasks%20designed%20to%20systematically%20probe%20the%20capabilities%20of%20TGNNs%0Ato%20reason%20across%20time.%20T-GRAB%20provides%20controlled%2C%20interpretable%20tasks%20that%0Aisolate%20key%20temporal%20skills%3A%20counting/memorizing%20periodic%20repetitions%2C%0Ainferring%20delayed%20causal%20effects%2C%20and%20capturing%20long-range%20dependencies%20over%0Aboth%20spatial%20and%20temporal%20dimensions.%20We%20evaluate%2011%20temporal%20graph%20learning%0Amethods%20on%20these%20tasks%2C%20revealing%20fundamental%20shortcomings%20in%20their%20ability%20to%0Ageneralize%20temporal%20patterns.%20Our%20findings%20offer%20actionable%20insights%20into%20the%0Alimitations%20of%20current%20models%2C%20highlight%20challenges%20hidden%20by%20traditional%0Areal-world%20benchmarks%2C%20and%20motivate%20the%20development%20of%20architectures%20with%0Astronger%20temporal%20reasoning%20abilities.%20The%20code%20for%20T-GRAB%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/alirezadizaji/T-GRAB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-GRAB%253A%2520A%2520Synthetic%2520Diagnostic%2520Benchmark%2520for%2520Learning%2520on%2520Temporal%2520Graphs%26entry.906535625%3DAlireza%2520Dizaji%2520and%2520Benedict%2520Aaron%2520Tjandra%2520and%2520Mehrab%2520Hamidi%2520and%2520Shenyang%2520Huang%2520and%2520Guillaume%2520Rabusseau%26entry.1292438233%3D%2520%2520Dynamic%2520graph%2520learning%2520methods%2520have%2520recently%2520emerged%2520as%2520powerful%2520tools%2520for%250Amodelling%2520relational%2520data%2520evolving%2520through%2520time.%2520However%252C%2520despite%2520extensive%250Abenchmarking%2520efforts%252C%2520it%2520remains%2520unclear%2520whether%2520current%2520Temporal%2520Graph%2520Neural%250ANetworks%2520%2528TGNNs%2529%2520effectively%2520capture%2520core%2520temporal%2520patterns%2520such%2520as%250Aperiodicity%252C%2520cause-and-effect%252C%2520and%2520long-range%2520dependencies.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520Temporal%2520Graph%2520Reasoning%2520Benchmark%2520%2528T-GRAB%2529%252C%2520a%2520comprehensive%2520set%250Aof%2520synthetic%2520tasks%2520designed%2520to%2520systematically%2520probe%2520the%2520capabilities%2520of%2520TGNNs%250Ato%2520reason%2520across%2520time.%2520T-GRAB%2520provides%2520controlled%252C%2520interpretable%2520tasks%2520that%250Aisolate%2520key%2520temporal%2520skills%253A%2520counting/memorizing%2520periodic%2520repetitions%252C%250Ainferring%2520delayed%2520causal%2520effects%252C%2520and%2520capturing%2520long-range%2520dependencies%2520over%250Aboth%2520spatial%2520and%2520temporal%2520dimensions.%2520We%2520evaluate%252011%2520temporal%2520graph%2520learning%250Amethods%2520on%2520these%2520tasks%252C%2520revealing%2520fundamental%2520shortcomings%2520in%2520their%2520ability%2520to%250Ageneralize%2520temporal%2520patterns.%2520Our%2520findings%2520offer%2520actionable%2520insights%2520into%2520the%250Alimitations%2520of%2520current%2520models%252C%2520highlight%2520challenges%2520hidden%2520by%2520traditional%250Areal-world%2520benchmarks%252C%2520and%2520motivate%2520the%2520development%2520of%2520architectures%2520with%250Astronger%2520temporal%2520reasoning%2520abilities.%2520The%2520code%2520for%2520T-GRAB%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/alirezadizaji/T-GRAB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-GRAB%3A%20A%20Synthetic%20Diagnostic%20Benchmark%20for%20Learning%20on%20Temporal%20Graphs&entry.906535625=Alireza%20Dizaji%20and%20Benedict%20Aaron%20Tjandra%20and%20Mehrab%20Hamidi%20and%20Shenyang%20Huang%20and%20Guillaume%20Rabusseau&entry.1292438233=%20%20Dynamic%20graph%20learning%20methods%20have%20recently%20emerged%20as%20powerful%20tools%20for%0Amodelling%20relational%20data%20evolving%20through%20time.%20However%2C%20despite%20extensive%0Abenchmarking%20efforts%2C%20it%20remains%20unclear%20whether%20current%20Temporal%20Graph%20Neural%0ANetworks%20%28TGNNs%29%20effectively%20capture%20core%20temporal%20patterns%20such%20as%0Aperiodicity%2C%20cause-and-effect%2C%20and%20long-range%20dependencies.%20In%20this%20work%2C%20we%0Aintroduce%20the%20Temporal%20Graph%20Reasoning%20Benchmark%20%28T-GRAB%29%2C%20a%20comprehensive%20set%0Aof%20synthetic%20tasks%20designed%20to%20systematically%20probe%20the%20capabilities%20of%20TGNNs%0Ato%20reason%20across%20time.%20T-GRAB%20provides%20controlled%2C%20interpretable%20tasks%20that%0Aisolate%20key%20temporal%20skills%3A%20counting/memorizing%20periodic%20repetitions%2C%0Ainferring%20delayed%20causal%20effects%2C%20and%20capturing%20long-range%20dependencies%20over%0Aboth%20spatial%20and%20temporal%20dimensions.%20We%20evaluate%2011%20temporal%20graph%20learning%0Amethods%20on%20these%20tasks%2C%20revealing%20fundamental%20shortcomings%20in%20their%20ability%20to%0Ageneralize%20temporal%20patterns.%20Our%20findings%20offer%20actionable%20insights%20into%20the%0Alimitations%20of%20current%20models%2C%20highlight%20challenges%20hidden%20by%20traditional%0Areal-world%20benchmarks%2C%20and%20motivate%20the%20development%20of%20architectures%20with%0Astronger%20temporal%20reasoning%20abilities.%20The%20code%20for%20T-GRAB%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/alirezadizaji/T-GRAB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10183v1&entry.124074799=Read"},
{"title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel\n  Dataset and Method", "author": "Han Wang and Shengyang Li and Jian Yang and Yuxuan Liu and Yixuan Lv and Zhuang Zhou", "abstract": "  Detecting and tracking ground objects using earth observation imagery remains\na significant challenge in the field of remote sensing. Continuous maritime\nship tracking is crucial for applications such as maritime search and rescue,\nlaw enforcement, and shipping analysis. However, most current ship tracking\nmethods rely on geostationary satellites or video satellites. The former offer\nlow resolution and are susceptible to weather conditions, while the latter have\nshort filming durations and limited coverage areas, making them less suitable\nfor the real-world requirements of ship tracking. To address these limitations,\nwe present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship\nRe-Identification Dataset (HOSS ReID dataset), designed to evaluate the\neffectiveness of ship tracking using low-Earth orbit constellations of optical\nand SAR sensors. This approach ensures shorter re-imaging cycles and enables\nall-weather tracking. HOSS ReID dataset includes images of the same ship\ncaptured over extended periods under diverse conditions, using different\nsatellites of different modalities at varying times and angles. Furthermore, we\npropose a baseline method for cross-modal ship re-identification, TransOSS,\nwhich is built on the Vision Transformer architecture. It refines the patch\nembedding structure to better accommodate cross-modal tasks, incorporates\nadditional embeddings to introduce more reference information, and employs\ncontrastive learning to pre-train on large-scale optical-SAR image pairs,\nensuring the model's ability to extract modality-invariant features. Our\ndataset and baseline method are publicly available on\nhttps://github.com/Alioth2000/Hoss-ReID.\n", "link": "http://arxiv.org/abs/2506.22027v2", "date": "2025-07-14", "relevancy": 2.0263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5202}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-modal%20Ship%20Re-Identification%20via%20Optical%20and%20SAR%20Imagery%3A%20A%20Novel%0A%20%20Dataset%20and%20Method&body=Title%3A%20Cross-modal%20Ship%20Re-Identification%20via%20Optical%20and%20SAR%20Imagery%3A%20A%20Novel%0A%20%20Dataset%20and%20Method%0AAuthor%3A%20Han%20Wang%20and%20Shengyang%20Li%20and%20Jian%20Yang%20and%20Yuxuan%20Liu%20and%20Yixuan%20Lv%20and%20Zhuang%20Zhou%0AAbstract%3A%20%20%20Detecting%20and%20tracking%20ground%20objects%20using%20earth%20observation%20imagery%20remains%0Aa%20significant%20challenge%20in%20the%20field%20of%20remote%20sensing.%20Continuous%20maritime%0Aship%20tracking%20is%20crucial%20for%20applications%20such%20as%20maritime%20search%20and%20rescue%2C%0Alaw%20enforcement%2C%20and%20shipping%20analysis.%20However%2C%20most%20current%20ship%20tracking%0Amethods%20rely%20on%20geostationary%20satellites%20or%20video%20satellites.%20The%20former%20offer%0Alow%20resolution%20and%20are%20susceptible%20to%20weather%20conditions%2C%20while%20the%20latter%20have%0Ashort%20filming%20durations%20and%20limited%20coverage%20areas%2C%20making%20them%20less%20suitable%0Afor%20the%20real-world%20requirements%20of%20ship%20tracking.%20To%20address%20these%20limitations%2C%0Awe%20present%20the%20Hybrid%20Optical%20and%20Synthetic%20Aperture%20Radar%20%28SAR%29%20Ship%0ARe-Identification%20Dataset%20%28HOSS%20ReID%20dataset%29%2C%20designed%20to%20evaluate%20the%0Aeffectiveness%20of%20ship%20tracking%20using%20low-Earth%20orbit%20constellations%20of%20optical%0Aand%20SAR%20sensors.%20This%20approach%20ensures%20shorter%20re-imaging%20cycles%20and%20enables%0Aall-weather%20tracking.%20HOSS%20ReID%20dataset%20includes%20images%20of%20the%20same%20ship%0Acaptured%20over%20extended%20periods%20under%20diverse%20conditions%2C%20using%20different%0Asatellites%20of%20different%20modalities%20at%20varying%20times%20and%20angles.%20Furthermore%2C%20we%0Apropose%20a%20baseline%20method%20for%20cross-modal%20ship%20re-identification%2C%20TransOSS%2C%0Awhich%20is%20built%20on%20the%20Vision%20Transformer%20architecture.%20It%20refines%20the%20patch%0Aembedding%20structure%20to%20better%20accommodate%20cross-modal%20tasks%2C%20incorporates%0Aadditional%20embeddings%20to%20introduce%20more%20reference%20information%2C%20and%20employs%0Acontrastive%20learning%20to%20pre-train%20on%20large-scale%20optical-SAR%20image%20pairs%2C%0Aensuring%20the%20model%27s%20ability%20to%20extract%20modality-invariant%20features.%20Our%0Adataset%20and%20baseline%20method%20are%20publicly%20available%20on%0Ahttps%3A//github.com/Alioth2000/Hoss-ReID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-modal%2520Ship%2520Re-Identification%2520via%2520Optical%2520and%2520SAR%2520Imagery%253A%2520A%2520Novel%250A%2520%2520Dataset%2520and%2520Method%26entry.906535625%3DHan%2520Wang%2520and%2520Shengyang%2520Li%2520and%2520Jian%2520Yang%2520and%2520Yuxuan%2520Liu%2520and%2520Yixuan%2520Lv%2520and%2520Zhuang%2520Zhou%26entry.1292438233%3D%2520%2520Detecting%2520and%2520tracking%2520ground%2520objects%2520using%2520earth%2520observation%2520imagery%2520remains%250Aa%2520significant%2520challenge%2520in%2520the%2520field%2520of%2520remote%2520sensing.%2520Continuous%2520maritime%250Aship%2520tracking%2520is%2520crucial%2520for%2520applications%2520such%2520as%2520maritime%2520search%2520and%2520rescue%252C%250Alaw%2520enforcement%252C%2520and%2520shipping%2520analysis.%2520However%252C%2520most%2520current%2520ship%2520tracking%250Amethods%2520rely%2520on%2520geostationary%2520satellites%2520or%2520video%2520satellites.%2520The%2520former%2520offer%250Alow%2520resolution%2520and%2520are%2520susceptible%2520to%2520weather%2520conditions%252C%2520while%2520the%2520latter%2520have%250Ashort%2520filming%2520durations%2520and%2520limited%2520coverage%2520areas%252C%2520making%2520them%2520less%2520suitable%250Afor%2520the%2520real-world%2520requirements%2520of%2520ship%2520tracking.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520present%2520the%2520Hybrid%2520Optical%2520and%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520Ship%250ARe-Identification%2520Dataset%2520%2528HOSS%2520ReID%2520dataset%2529%252C%2520designed%2520to%2520evaluate%2520the%250Aeffectiveness%2520of%2520ship%2520tracking%2520using%2520low-Earth%2520orbit%2520constellations%2520of%2520optical%250Aand%2520SAR%2520sensors.%2520This%2520approach%2520ensures%2520shorter%2520re-imaging%2520cycles%2520and%2520enables%250Aall-weather%2520tracking.%2520HOSS%2520ReID%2520dataset%2520includes%2520images%2520of%2520the%2520same%2520ship%250Acaptured%2520over%2520extended%2520periods%2520under%2520diverse%2520conditions%252C%2520using%2520different%250Asatellites%2520of%2520different%2520modalities%2520at%2520varying%2520times%2520and%2520angles.%2520Furthermore%252C%2520we%250Apropose%2520a%2520baseline%2520method%2520for%2520cross-modal%2520ship%2520re-identification%252C%2520TransOSS%252C%250Awhich%2520is%2520built%2520on%2520the%2520Vision%2520Transformer%2520architecture.%2520It%2520refines%2520the%2520patch%250Aembedding%2520structure%2520to%2520better%2520accommodate%2520cross-modal%2520tasks%252C%2520incorporates%250Aadditional%2520embeddings%2520to%2520introduce%2520more%2520reference%2520information%252C%2520and%2520employs%250Acontrastive%2520learning%2520to%2520pre-train%2520on%2520large-scale%2520optical-SAR%2520image%2520pairs%252C%250Aensuring%2520the%2520model%2527s%2520ability%2520to%2520extract%2520modality-invariant%2520features.%2520Our%250Adataset%2520and%2520baseline%2520method%2520are%2520publicly%2520available%2520on%250Ahttps%253A//github.com/Alioth2000/Hoss-ReID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modal%20Ship%20Re-Identification%20via%20Optical%20and%20SAR%20Imagery%3A%20A%20Novel%0A%20%20Dataset%20and%20Method&entry.906535625=Han%20Wang%20and%20Shengyang%20Li%20and%20Jian%20Yang%20and%20Yuxuan%20Liu%20and%20Yixuan%20Lv%20and%20Zhuang%20Zhou&entry.1292438233=%20%20Detecting%20and%20tracking%20ground%20objects%20using%20earth%20observation%20imagery%20remains%0Aa%20significant%20challenge%20in%20the%20field%20of%20remote%20sensing.%20Continuous%20maritime%0Aship%20tracking%20is%20crucial%20for%20applications%20such%20as%20maritime%20search%20and%20rescue%2C%0Alaw%20enforcement%2C%20and%20shipping%20analysis.%20However%2C%20most%20current%20ship%20tracking%0Amethods%20rely%20on%20geostationary%20satellites%20or%20video%20satellites.%20The%20former%20offer%0Alow%20resolution%20and%20are%20susceptible%20to%20weather%20conditions%2C%20while%20the%20latter%20have%0Ashort%20filming%20durations%20and%20limited%20coverage%20areas%2C%20making%20them%20less%20suitable%0Afor%20the%20real-world%20requirements%20of%20ship%20tracking.%20To%20address%20these%20limitations%2C%0Awe%20present%20the%20Hybrid%20Optical%20and%20Synthetic%20Aperture%20Radar%20%28SAR%29%20Ship%0ARe-Identification%20Dataset%20%28HOSS%20ReID%20dataset%29%2C%20designed%20to%20evaluate%20the%0Aeffectiveness%20of%20ship%20tracking%20using%20low-Earth%20orbit%20constellations%20of%20optical%0Aand%20SAR%20sensors.%20This%20approach%20ensures%20shorter%20re-imaging%20cycles%20and%20enables%0Aall-weather%20tracking.%20HOSS%20ReID%20dataset%20includes%20images%20of%20the%20same%20ship%0Acaptured%20over%20extended%20periods%20under%20diverse%20conditions%2C%20using%20different%0Asatellites%20of%20different%20modalities%20at%20varying%20times%20and%20angles.%20Furthermore%2C%20we%0Apropose%20a%20baseline%20method%20for%20cross-modal%20ship%20re-identification%2C%20TransOSS%2C%0Awhich%20is%20built%20on%20the%20Vision%20Transformer%20architecture.%20It%20refines%20the%20patch%0Aembedding%20structure%20to%20better%20accommodate%20cross-modal%20tasks%2C%20incorporates%0Aadditional%20embeddings%20to%20introduce%20more%20reference%20information%2C%20and%20employs%0Acontrastive%20learning%20to%20pre-train%20on%20large-scale%20optical-SAR%20image%20pairs%2C%0Aensuring%20the%20model%27s%20ability%20to%20extract%20modality-invariant%20features.%20Our%0Adataset%20and%20baseline%20method%20are%20publicly%20available%20on%0Ahttps%3A//github.com/Alioth2000/Hoss-ReID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22027v2&entry.124074799=Read"},
{"title": "A Variance-Reduced Cubic-Regularized Newton for Policy Optimization", "author": "Cheng Sun and Zhen Zhang and Shaofu Yang", "abstract": "  In this paper, we study a second-order approach to policy optimization in\nreinforcement learning. Existing second-order methods often suffer from\nsuboptimal sample complexity or rely on unrealistic assumptions about\nimportance sampling. To overcome these limitations, we propose VR-CR-PN, a\nvariance-reduced cubic-regularized policy Newton algorithm. To the best of our\nknowledge, this is the first algorithm that integrates Hessian-aided variance\nreduction with second-order policy optimization, effectively addressing the\ndistribution shift problem and achieving best-known sample complexity under\ngeneral nonconvex conditions but without the need for importance sampling. We\ntheoretically establish that VR-CR-PN achieves a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ to reach an $\\epsilon$-second-order\nstationary point, significantly improving upon the previous best result of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ under comparable assumptions. As an\nadditional contribution, we introduce a novel Hessian estimator for the\nexpected return function, which admits a uniform upper bound independent of the\nhorizon length $H$, allowing the algorithm to achieve horizon-independent\nsample complexity.\n", "link": "http://arxiv.org/abs/2507.10120v1", "date": "2025-07-14", "relevancy": 1.6649, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4444}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4188}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Variance-Reduced%20Cubic-Regularized%20Newton%20for%20Policy%20Optimization&body=Title%3A%20A%20Variance-Reduced%20Cubic-Regularized%20Newton%20for%20Policy%20Optimization%0AAuthor%3A%20Cheng%20Sun%20and%20Zhen%20Zhang%20and%20Shaofu%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20a%20second-order%20approach%20to%20policy%20optimization%20in%0Areinforcement%20learning.%20Existing%20second-order%20methods%20often%20suffer%20from%0Asuboptimal%20sample%20complexity%20or%20rely%20on%20unrealistic%20assumptions%20about%0Aimportance%20sampling.%20To%20overcome%20these%20limitations%2C%20we%20propose%20VR-CR-PN%2C%20a%0Avariance-reduced%20cubic-regularized%20policy%20Newton%20algorithm.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20algorithm%20that%20integrates%20Hessian-aided%20variance%0Areduction%20with%20second-order%20policy%20optimization%2C%20effectively%20addressing%20the%0Adistribution%20shift%20problem%20and%20achieving%20best-known%20sample%20complexity%20under%0Ageneral%20nonconvex%20conditions%20but%20without%20the%20need%20for%20importance%20sampling.%20We%0Atheoretically%20establish%20that%20VR-CR-PN%20achieves%20a%20sample%20complexity%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-3%7D%29%24%20to%20reach%20an%20%24%5Cepsilon%24-second-order%0Astationary%20point%2C%20significantly%20improving%20upon%20the%20previous%20best%20result%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-3.5%7D%29%24%20under%20comparable%20assumptions.%20As%20an%0Aadditional%20contribution%2C%20we%20introduce%20a%20novel%20Hessian%20estimator%20for%20the%0Aexpected%20return%20function%2C%20which%20admits%20a%20uniform%20upper%20bound%20independent%20of%20the%0Ahorizon%20length%20%24H%24%2C%20allowing%20the%20algorithm%20to%20achieve%20horizon-independent%0Asample%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Variance-Reduced%2520Cubic-Regularized%2520Newton%2520for%2520Policy%2520Optimization%26entry.906535625%3DCheng%2520Sun%2520and%2520Zhen%2520Zhang%2520and%2520Shaofu%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520a%2520second-order%2520approach%2520to%2520policy%2520optimization%2520in%250Areinforcement%2520learning.%2520Existing%2520second-order%2520methods%2520often%2520suffer%2520from%250Asuboptimal%2520sample%2520complexity%2520or%2520rely%2520on%2520unrealistic%2520assumptions%2520about%250Aimportance%2520sampling.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520VR-CR-PN%252C%2520a%250Avariance-reduced%2520cubic-regularized%2520policy%2520Newton%2520algorithm.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520algorithm%2520that%2520integrates%2520Hessian-aided%2520variance%250Areduction%2520with%2520second-order%2520policy%2520optimization%252C%2520effectively%2520addressing%2520the%250Adistribution%2520shift%2520problem%2520and%2520achieving%2520best-known%2520sample%2520complexity%2520under%250Ageneral%2520nonconvex%2520conditions%2520but%2520without%2520the%2520need%2520for%2520importance%2520sampling.%2520We%250Atheoretically%2520establish%2520that%2520VR-CR-PN%2520achieves%2520a%2520sample%2520complexity%2520of%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cepsilon%255E%257B-3%257D%2529%2524%2520to%2520reach%2520an%2520%2524%255Cepsilon%2524-second-order%250Astationary%2520point%252C%2520significantly%2520improving%2520upon%2520the%2520previous%2520best%2520result%2520of%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cepsilon%255E%257B-3.5%257D%2529%2524%2520under%2520comparable%2520assumptions.%2520As%2520an%250Aadditional%2520contribution%252C%2520we%2520introduce%2520a%2520novel%2520Hessian%2520estimator%2520for%2520the%250Aexpected%2520return%2520function%252C%2520which%2520admits%2520a%2520uniform%2520upper%2520bound%2520independent%2520of%2520the%250Ahorizon%2520length%2520%2524H%2524%252C%2520allowing%2520the%2520algorithm%2520to%2520achieve%2520horizon-independent%250Asample%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Variance-Reduced%20Cubic-Regularized%20Newton%20for%20Policy%20Optimization&entry.906535625=Cheng%20Sun%20and%20Zhen%20Zhang%20and%20Shaofu%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20a%20second-order%20approach%20to%20policy%20optimization%20in%0Areinforcement%20learning.%20Existing%20second-order%20methods%20often%20suffer%20from%0Asuboptimal%20sample%20complexity%20or%20rely%20on%20unrealistic%20assumptions%20about%0Aimportance%20sampling.%20To%20overcome%20these%20limitations%2C%20we%20propose%20VR-CR-PN%2C%20a%0Avariance-reduced%20cubic-regularized%20policy%20Newton%20algorithm.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20algorithm%20that%20integrates%20Hessian-aided%20variance%0Areduction%20with%20second-order%20policy%20optimization%2C%20effectively%20addressing%20the%0Adistribution%20shift%20problem%20and%20achieving%20best-known%20sample%20complexity%20under%0Ageneral%20nonconvex%20conditions%20but%20without%20the%20need%20for%20importance%20sampling.%20We%0Atheoretically%20establish%20that%20VR-CR-PN%20achieves%20a%20sample%20complexity%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-3%7D%29%24%20to%20reach%20an%20%24%5Cepsilon%24-second-order%0Astationary%20point%2C%20significantly%20improving%20upon%20the%20previous%20best%20result%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-3.5%7D%29%24%20under%20comparable%20assumptions.%20As%20an%0Aadditional%20contribution%2C%20we%20introduce%20a%20novel%20Hessian%20estimator%20for%20the%0Aexpected%20return%20function%2C%20which%20admits%20a%20uniform%20upper%20bound%20independent%20of%20the%0Ahorizon%20length%20%24H%24%2C%20allowing%20the%20algorithm%20to%20achieve%20horizon-independent%0Asample%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10120v1&entry.124074799=Read"},
{"title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task\n  Frameworks", "author": "Hamzah Ziadeh and Hendrik Knoche", "abstract": "  Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design.\n", "link": "http://arxiv.org/abs/2507.10208v1", "date": "2025-07-14", "relevancy": 1.9481, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20for%20Categorising%20Explainable%20AI%20Studies%20Using%20Data%20Analysis%20Task%0A%20%20Frameworks&body=Title%3A%20Survey%20for%20Categorising%20Explainable%20AI%20Studies%20Using%20Data%20Analysis%20Task%0A%20%20Frameworks%0AAuthor%3A%20Hamzah%20Ziadeh%20and%20Hendrik%20Knoche%0AAbstract%3A%20%20%20Research%20into%20explainable%20artificial%20intelligence%20%28XAI%29%20for%20data%20analysis%0Atasks%20suffer%20from%20a%20large%20number%20of%20contradictions%20and%20lack%20of%20concrete%20design%0Arecommendations%20stemming%20from%20gaps%20in%20understanding%20the%20tasks%20that%20require%20AI%0Aassistance.%20In%20this%20paper%2C%20we%20drew%20on%20multiple%20fields%20such%20as%20visual%20analytics%2C%0Acognition%2C%20and%20dashboard%20design%20to%20propose%20a%20method%20for%20categorising%20and%0Acomparing%20XAI%20studies%20under%20three%20dimensions%3A%20what%2C%20why%2C%20and%20who.%20We%20identified%0Athe%20main%20problems%20as%3A%20inadequate%20descriptions%20of%20tasks%2C%20context-free%20studies%2C%0Aand%20insufficient%20testing%20with%20target%20users.%20We%20propose%20that%20studies%20should%0Aspecifically%20report%20on%20their%20users%27%20domain%2C%20AI%2C%20and%20data%20analysis%20expertise%20to%0Aillustrate%20the%20generalisability%20of%20their%20findings.%20We%20also%20propose%20study%0Aguidelines%20for%20designing%20and%20reporting%20XAI%20tasks%20to%20improve%20the%20XAI%20community%27s%0Aability%20to%20parse%20the%20rapidly%20growing%20field.%20We%20hope%20that%20our%20contribution%20can%0Ahelp%20researchers%20and%20designers%20better%20identify%20which%20studies%20are%20most%20relevant%0Ato%20their%20work%2C%20what%20gaps%20exist%20in%20the%20research%2C%20and%20how%20to%20handle%20contradictory%0Aresults%20regarding%20XAI%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520for%2520Categorising%2520Explainable%2520AI%2520Studies%2520Using%2520Data%2520Analysis%2520Task%250A%2520%2520Frameworks%26entry.906535625%3DHamzah%2520Ziadeh%2520and%2520Hendrik%2520Knoche%26entry.1292438233%3D%2520%2520Research%2520into%2520explainable%2520artificial%2520intelligence%2520%2528XAI%2529%2520for%2520data%2520analysis%250Atasks%2520suffer%2520from%2520a%2520large%2520number%2520of%2520contradictions%2520and%2520lack%2520of%2520concrete%2520design%250Arecommendations%2520stemming%2520from%2520gaps%2520in%2520understanding%2520the%2520tasks%2520that%2520require%2520AI%250Aassistance.%2520In%2520this%2520paper%252C%2520we%2520drew%2520on%2520multiple%2520fields%2520such%2520as%2520visual%2520analytics%252C%250Acognition%252C%2520and%2520dashboard%2520design%2520to%2520propose%2520a%2520method%2520for%2520categorising%2520and%250Acomparing%2520XAI%2520studies%2520under%2520three%2520dimensions%253A%2520what%252C%2520why%252C%2520and%2520who.%2520We%2520identified%250Athe%2520main%2520problems%2520as%253A%2520inadequate%2520descriptions%2520of%2520tasks%252C%2520context-free%2520studies%252C%250Aand%2520insufficient%2520testing%2520with%2520target%2520users.%2520We%2520propose%2520that%2520studies%2520should%250Aspecifically%2520report%2520on%2520their%2520users%2527%2520domain%252C%2520AI%252C%2520and%2520data%2520analysis%2520expertise%2520to%250Aillustrate%2520the%2520generalisability%2520of%2520their%2520findings.%2520We%2520also%2520propose%2520study%250Aguidelines%2520for%2520designing%2520and%2520reporting%2520XAI%2520tasks%2520to%2520improve%2520the%2520XAI%2520community%2527s%250Aability%2520to%2520parse%2520the%2520rapidly%2520growing%2520field.%2520We%2520hope%2520that%2520our%2520contribution%2520can%250Ahelp%2520researchers%2520and%2520designers%2520better%2520identify%2520which%2520studies%2520are%2520most%2520relevant%250Ato%2520their%2520work%252C%2520what%2520gaps%2520exist%2520in%2520the%2520research%252C%2520and%2520how%2520to%2520handle%2520contradictory%250Aresults%2520regarding%2520XAI%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20for%20Categorising%20Explainable%20AI%20Studies%20Using%20Data%20Analysis%20Task%0A%20%20Frameworks&entry.906535625=Hamzah%20Ziadeh%20and%20Hendrik%20Knoche&entry.1292438233=%20%20Research%20into%20explainable%20artificial%20intelligence%20%28XAI%29%20for%20data%20analysis%0Atasks%20suffer%20from%20a%20large%20number%20of%20contradictions%20and%20lack%20of%20concrete%20design%0Arecommendations%20stemming%20from%20gaps%20in%20understanding%20the%20tasks%20that%20require%20AI%0Aassistance.%20In%20this%20paper%2C%20we%20drew%20on%20multiple%20fields%20such%20as%20visual%20analytics%2C%0Acognition%2C%20and%20dashboard%20design%20to%20propose%20a%20method%20for%20categorising%20and%0Acomparing%20XAI%20studies%20under%20three%20dimensions%3A%20what%2C%20why%2C%20and%20who.%20We%20identified%0Athe%20main%20problems%20as%3A%20inadequate%20descriptions%20of%20tasks%2C%20context-free%20studies%2C%0Aand%20insufficient%20testing%20with%20target%20users.%20We%20propose%20that%20studies%20should%0Aspecifically%20report%20on%20their%20users%27%20domain%2C%20AI%2C%20and%20data%20analysis%20expertise%20to%0Aillustrate%20the%20generalisability%20of%20their%20findings.%20We%20also%20propose%20study%0Aguidelines%20for%20designing%20and%20reporting%20XAI%20tasks%20to%20improve%20the%20XAI%20community%27s%0Aability%20to%20parse%20the%20rapidly%20growing%20field.%20We%20hope%20that%20our%20contribution%20can%0Ahelp%20researchers%20and%20designers%20better%20identify%20which%20studies%20are%20most%20relevant%0Ato%20their%20work%2C%20what%20gaps%20exist%20in%20the%20research%2C%20and%20how%20to%20handle%20contradictory%0Aresults%20regarding%20XAI%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10208v1&entry.124074799=Read"},
{"title": "Some remarks on gradient dominance and LQR policy optimization", "author": "Eduardo D. Sontag", "abstract": "  Solutions of optimization problems, including policy optimization in\nreinforcement learning, typically rely upon some variant of gradient descent.\nThere has been much recent work in the machine learning, control, and\noptimization communities applying the Polyak-{\\L}ojasiewicz Inequality (PLI) to\nsuch problems in order to establish an exponential rate of convergence (a.k.a.\n``linear convergence'' in the local-iteration language of numerical analysis)\nof loss functions to their minima under the gradient flow. Often, as is the\ncase of policy iteration for the continuous-time LQR problem, this rate\nvanishes for large initial conditions, resulting in a mixed globally linear /\nlocally exponential behavior. This is in sharp contrast with the discrete-time\nLQR problem, where there is global exponential convergence. That gap between CT\nand DT behaviors motivates the search for various generalized PLI-like\nconditions, and this talk will address that topic. Moreover, these\ngeneralizations are key to understanding the transient and asymptotic effects\nof errors in the estimation of the gradient, errors which might arise from\nadversarial attacks, wrong evaluation by an oracle, early stopping of a\nsimulation, inaccurate and very approximate digital twins, stochastic\ncomputations (algorithm ``reproducibility''), or learning by sampling from\nlimited data. We describe an ``input to state stability'' (ISS) analysis of\nthis issue. The lecture also discussed convergence and PLI-like properties of\n``linear feedforward neural networks'' in feedback control, but this arXiv\nskips that part (to be updated). Much of the work described here was done in\ncollaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping\nJiang, and Milad Siami.\n", "link": "http://arxiv.org/abs/2507.10452v1", "date": "2025-07-14", "relevancy": 1.9044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4845}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4757}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Some%20remarks%20on%20gradient%20dominance%20and%20LQR%20policy%20optimization&body=Title%3A%20Some%20remarks%20on%20gradient%20dominance%20and%20LQR%20policy%20optimization%0AAuthor%3A%20Eduardo%20D.%20Sontag%0AAbstract%3A%20%20%20Solutions%20of%20optimization%20problems%2C%20including%20policy%20optimization%20in%0Areinforcement%20learning%2C%20typically%20rely%20upon%20some%20variant%20of%20gradient%20descent.%0AThere%20has%20been%20much%20recent%20work%20in%20the%20machine%20learning%2C%20control%2C%20and%0Aoptimization%20communities%20applying%20the%20Polyak-%7B%5CL%7Dojasiewicz%20Inequality%20%28PLI%29%20to%0Asuch%20problems%20in%20order%20to%20establish%20an%20exponential%20rate%20of%20convergence%20%28a.k.a.%0A%60%60linear%20convergence%27%27%20in%20the%20local-iteration%20language%20of%20numerical%20analysis%29%0Aof%20loss%20functions%20to%20their%20minima%20under%20the%20gradient%20flow.%20Often%2C%20as%20is%20the%0Acase%20of%20policy%20iteration%20for%20the%20continuous-time%20LQR%20problem%2C%20this%20rate%0Avanishes%20for%20large%20initial%20conditions%2C%20resulting%20in%20a%20mixed%20globally%20linear%20/%0Alocally%20exponential%20behavior.%20This%20is%20in%20sharp%20contrast%20with%20the%20discrete-time%0ALQR%20problem%2C%20where%20there%20is%20global%20exponential%20convergence.%20That%20gap%20between%20CT%0Aand%20DT%20behaviors%20motivates%20the%20search%20for%20various%20generalized%20PLI-like%0Aconditions%2C%20and%20this%20talk%20will%20address%20that%20topic.%20Moreover%2C%20these%0Ageneralizations%20are%20key%20to%20understanding%20the%20transient%20and%20asymptotic%20effects%0Aof%20errors%20in%20the%20estimation%20of%20the%20gradient%2C%20errors%20which%20might%20arise%20from%0Aadversarial%20attacks%2C%20wrong%20evaluation%20by%20an%20oracle%2C%20early%20stopping%20of%20a%0Asimulation%2C%20inaccurate%20and%20very%20approximate%20digital%20twins%2C%20stochastic%0Acomputations%20%28algorithm%20%60%60reproducibility%27%27%29%2C%20or%20learning%20by%20sampling%20from%0Alimited%20data.%20We%20describe%20an%20%60%60input%20to%20state%20stability%27%27%20%28ISS%29%20analysis%20of%0Athis%20issue.%20The%20lecture%20also%20discussed%20convergence%20and%20PLI-like%20properties%20of%0A%60%60linear%20feedforward%20neural%20networks%27%27%20in%20feedback%20control%2C%20but%20this%20arXiv%0Askips%20that%20part%20%28to%20be%20updated%29.%20Much%20of%20the%20work%20described%20here%20was%20done%20in%0Acollaboration%20with%20Arthur%20Castello%20B.%20de%20Oliveira%2C%20Leilei%20Cui%2C%20Zhong-Ping%0AJiang%2C%20and%20Milad%20Siami.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSome%2520remarks%2520on%2520gradient%2520dominance%2520and%2520LQR%2520policy%2520optimization%26entry.906535625%3DEduardo%2520D.%2520Sontag%26entry.1292438233%3D%2520%2520Solutions%2520of%2520optimization%2520problems%252C%2520including%2520policy%2520optimization%2520in%250Areinforcement%2520learning%252C%2520typically%2520rely%2520upon%2520some%2520variant%2520of%2520gradient%2520descent.%250AThere%2520has%2520been%2520much%2520recent%2520work%2520in%2520the%2520machine%2520learning%252C%2520control%252C%2520and%250Aoptimization%2520communities%2520applying%2520the%2520Polyak-%257B%255CL%257Dojasiewicz%2520Inequality%2520%2528PLI%2529%2520to%250Asuch%2520problems%2520in%2520order%2520to%2520establish%2520an%2520exponential%2520rate%2520of%2520convergence%2520%2528a.k.a.%250A%2560%2560linear%2520convergence%2527%2527%2520in%2520the%2520local-iteration%2520language%2520of%2520numerical%2520analysis%2529%250Aof%2520loss%2520functions%2520to%2520their%2520minima%2520under%2520the%2520gradient%2520flow.%2520Often%252C%2520as%2520is%2520the%250Acase%2520of%2520policy%2520iteration%2520for%2520the%2520continuous-time%2520LQR%2520problem%252C%2520this%2520rate%250Avanishes%2520for%2520large%2520initial%2520conditions%252C%2520resulting%2520in%2520a%2520mixed%2520globally%2520linear%2520/%250Alocally%2520exponential%2520behavior.%2520This%2520is%2520in%2520sharp%2520contrast%2520with%2520the%2520discrete-time%250ALQR%2520problem%252C%2520where%2520there%2520is%2520global%2520exponential%2520convergence.%2520That%2520gap%2520between%2520CT%250Aand%2520DT%2520behaviors%2520motivates%2520the%2520search%2520for%2520various%2520generalized%2520PLI-like%250Aconditions%252C%2520and%2520this%2520talk%2520will%2520address%2520that%2520topic.%2520Moreover%252C%2520these%250Ageneralizations%2520are%2520key%2520to%2520understanding%2520the%2520transient%2520and%2520asymptotic%2520effects%250Aof%2520errors%2520in%2520the%2520estimation%2520of%2520the%2520gradient%252C%2520errors%2520which%2520might%2520arise%2520from%250Aadversarial%2520attacks%252C%2520wrong%2520evaluation%2520by%2520an%2520oracle%252C%2520early%2520stopping%2520of%2520a%250Asimulation%252C%2520inaccurate%2520and%2520very%2520approximate%2520digital%2520twins%252C%2520stochastic%250Acomputations%2520%2528algorithm%2520%2560%2560reproducibility%2527%2527%2529%252C%2520or%2520learning%2520by%2520sampling%2520from%250Alimited%2520data.%2520We%2520describe%2520an%2520%2560%2560input%2520to%2520state%2520stability%2527%2527%2520%2528ISS%2529%2520analysis%2520of%250Athis%2520issue.%2520The%2520lecture%2520also%2520discussed%2520convergence%2520and%2520PLI-like%2520properties%2520of%250A%2560%2560linear%2520feedforward%2520neural%2520networks%2527%2527%2520in%2520feedback%2520control%252C%2520but%2520this%2520arXiv%250Askips%2520that%2520part%2520%2528to%2520be%2520updated%2529.%2520Much%2520of%2520the%2520work%2520described%2520here%2520was%2520done%2520in%250Acollaboration%2520with%2520Arthur%2520Castello%2520B.%2520de%2520Oliveira%252C%2520Leilei%2520Cui%252C%2520Zhong-Ping%250AJiang%252C%2520and%2520Milad%2520Siami.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Some%20remarks%20on%20gradient%20dominance%20and%20LQR%20policy%20optimization&entry.906535625=Eduardo%20D.%20Sontag&entry.1292438233=%20%20Solutions%20of%20optimization%20problems%2C%20including%20policy%20optimization%20in%0Areinforcement%20learning%2C%20typically%20rely%20upon%20some%20variant%20of%20gradient%20descent.%0AThere%20has%20been%20much%20recent%20work%20in%20the%20machine%20learning%2C%20control%2C%20and%0Aoptimization%20communities%20applying%20the%20Polyak-%7B%5CL%7Dojasiewicz%20Inequality%20%28PLI%29%20to%0Asuch%20problems%20in%20order%20to%20establish%20an%20exponential%20rate%20of%20convergence%20%28a.k.a.%0A%60%60linear%20convergence%27%27%20in%20the%20local-iteration%20language%20of%20numerical%20analysis%29%0Aof%20loss%20functions%20to%20their%20minima%20under%20the%20gradient%20flow.%20Often%2C%20as%20is%20the%0Acase%20of%20policy%20iteration%20for%20the%20continuous-time%20LQR%20problem%2C%20this%20rate%0Avanishes%20for%20large%20initial%20conditions%2C%20resulting%20in%20a%20mixed%20globally%20linear%20/%0Alocally%20exponential%20behavior.%20This%20is%20in%20sharp%20contrast%20with%20the%20discrete-time%0ALQR%20problem%2C%20where%20there%20is%20global%20exponential%20convergence.%20That%20gap%20between%20CT%0Aand%20DT%20behaviors%20motivates%20the%20search%20for%20various%20generalized%20PLI-like%0Aconditions%2C%20and%20this%20talk%20will%20address%20that%20topic.%20Moreover%2C%20these%0Ageneralizations%20are%20key%20to%20understanding%20the%20transient%20and%20asymptotic%20effects%0Aof%20errors%20in%20the%20estimation%20of%20the%20gradient%2C%20errors%20which%20might%20arise%20from%0Aadversarial%20attacks%2C%20wrong%20evaluation%20by%20an%20oracle%2C%20early%20stopping%20of%20a%0Asimulation%2C%20inaccurate%20and%20very%20approximate%20digital%20twins%2C%20stochastic%0Acomputations%20%28algorithm%20%60%60reproducibility%27%27%29%2C%20or%20learning%20by%20sampling%20from%0Alimited%20data.%20We%20describe%20an%20%60%60input%20to%20state%20stability%27%27%20%28ISS%29%20analysis%20of%0Athis%20issue.%20The%20lecture%20also%20discussed%20convergence%20and%20PLI-like%20properties%20of%0A%60%60linear%20feedforward%20neural%20networks%27%27%20in%20feedback%20control%2C%20but%20this%20arXiv%0Askips%20that%20part%20%28to%20be%20updated%29.%20Much%20of%20the%20work%20described%20here%20was%20done%20in%0Acollaboration%20with%20Arthur%20Castello%20B.%20de%20Oliveira%2C%20Leilei%20Cui%2C%20Zhong-Ping%0AJiang%2C%20and%20Milad%20Siami.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10452v1&entry.124074799=Read"},
{"title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for\n  Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "author": "Yousef Emami and Hao Zhou and Miguel Gutierrez Gaitan and Kai Li and Luis Almeida", "abstract": "  Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.\n", "link": "http://arxiv.org/abs/2507.10134v1", "date": "2025-07-14", "relevancy": 1.5003, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5205}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4907}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRSICL%3A%20LLM-Enabled%20In-Context%20Learning%20Flight%20Resource%20Allocation%20for%0A%20%20Fresh%20Data%20Collection%20in%20UAV-Assisted%20Wildfire%20Monitoring&body=Title%3A%20FRSICL%3A%20LLM-Enabled%20In-Context%20Learning%20Flight%20Resource%20Allocation%20for%0A%20%20Fresh%20Data%20Collection%20in%20UAV-Assisted%20Wildfire%20Monitoring%0AAuthor%3A%20Yousef%20Emami%20and%20Hao%20Zhou%20and%20Miguel%20Gutierrez%20Gaitan%20and%20Kai%20Li%20and%20Luis%20Almeida%0AAbstract%3A%20%20%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20vital%20for%20public%20safety%2C%20particularly%20in%0Awildfire%20monitoring%2C%20where%20early%20detection%20minimizes%20environmental%20impact.%20In%0AUAV-Assisted%20Wildfire%20Monitoring%20%28UAWM%29%20systems%2C%20joint%20optimization%20of%20sensor%0Atransmission%20scheduling%20and%20velocity%20is%20critical%20for%20minimizing%20Age%20of%0AInformation%20%28AoI%29%20from%20stale%20sensor%20data.%20Deep%20Reinforcement%20Learning%20%28DRL%29%20has%0Abeen%20used%20for%20such%20optimization%3B%20however%2C%20its%20limitations%20such%20as%20low%20sampling%0Aefficiency%2C%20simulation-to-reality%20gaps%2C%20and%20complex%20training%20render%20it%0Aunsuitable%20for%20time-critical%20applications%20like%20wildfire%20monitoring.%20This%20paper%0Aintroduces%20a%20new%20online%20Flight%20Resource%20Allocation%20scheme%20based%20on%20LLM-Enabled%0AIn-Context%20Learning%20%28FRSICL%29%20to%20jointly%20optimize%20the%20UAV%27s%20flight%20control%20and%0Adata%20collection%20schedule%20along%20the%20trajectory%20in%20real%20time%2C%20thereby%0Aasymptotically%20minimizing%20the%20average%20AoI%20across%20ground%20sensors.%20In%20contrast%20to%0ADRL%2C%20FRSICL%20generates%20data%20collection%20schedules%20and%20controls%20velocity%20using%0Anatural%20language%20task%20descriptions%20and%20feedback%20from%20the%20environment%2C%20enabling%0Adynamic%20decision-making%20without%20extensive%20retraining.%20Simulation%20results%0Aconfirm%20the%20effectiveness%20of%20the%20proposed%20FRSICL%20compared%20to%20Proximal%20Policy%0AOptimization%20%28PPO%29%20and%20Nearest-Neighbor%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRSICL%253A%2520LLM-Enabled%2520In-Context%2520Learning%2520Flight%2520Resource%2520Allocation%2520for%250A%2520%2520Fresh%2520Data%2520Collection%2520in%2520UAV-Assisted%2520Wildfire%2520Monitoring%26entry.906535625%3DYousef%2520Emami%2520and%2520Hao%2520Zhou%2520and%2520Miguel%2520Gutierrez%2520Gaitan%2520and%2520Kai%2520Li%2520and%2520Luis%2520Almeida%26entry.1292438233%3D%2520%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520are%2520vital%2520for%2520public%2520safety%252C%2520particularly%2520in%250Awildfire%2520monitoring%252C%2520where%2520early%2520detection%2520minimizes%2520environmental%2520impact.%2520In%250AUAV-Assisted%2520Wildfire%2520Monitoring%2520%2528UAWM%2529%2520systems%252C%2520joint%2520optimization%2520of%2520sensor%250Atransmission%2520scheduling%2520and%2520velocity%2520is%2520critical%2520for%2520minimizing%2520Age%2520of%250AInformation%2520%2528AoI%2529%2520from%2520stale%2520sensor%2520data.%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520has%250Abeen%2520used%2520for%2520such%2520optimization%253B%2520however%252C%2520its%2520limitations%2520such%2520as%2520low%2520sampling%250Aefficiency%252C%2520simulation-to-reality%2520gaps%252C%2520and%2520complex%2520training%2520render%2520it%250Aunsuitable%2520for%2520time-critical%2520applications%2520like%2520wildfire%2520monitoring.%2520This%2520paper%250Aintroduces%2520a%2520new%2520online%2520Flight%2520Resource%2520Allocation%2520scheme%2520based%2520on%2520LLM-Enabled%250AIn-Context%2520Learning%2520%2528FRSICL%2529%2520to%2520jointly%2520optimize%2520the%2520UAV%2527s%2520flight%2520control%2520and%250Adata%2520collection%2520schedule%2520along%2520the%2520trajectory%2520in%2520real%2520time%252C%2520thereby%250Aasymptotically%2520minimizing%2520the%2520average%2520AoI%2520across%2520ground%2520sensors.%2520In%2520contrast%2520to%250ADRL%252C%2520FRSICL%2520generates%2520data%2520collection%2520schedules%2520and%2520controls%2520velocity%2520using%250Anatural%2520language%2520task%2520descriptions%2520and%2520feedback%2520from%2520the%2520environment%252C%2520enabling%250Adynamic%2520decision-making%2520without%2520extensive%2520retraining.%2520Simulation%2520results%250Aconfirm%2520the%2520effectiveness%2520of%2520the%2520proposed%2520FRSICL%2520compared%2520to%2520Proximal%2520Policy%250AOptimization%2520%2528PPO%2529%2520and%2520Nearest-Neighbor%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRSICL%3A%20LLM-Enabled%20In-Context%20Learning%20Flight%20Resource%20Allocation%20for%0A%20%20Fresh%20Data%20Collection%20in%20UAV-Assisted%20Wildfire%20Monitoring&entry.906535625=Yousef%20Emami%20and%20Hao%20Zhou%20and%20Miguel%20Gutierrez%20Gaitan%20and%20Kai%20Li%20and%20Luis%20Almeida&entry.1292438233=%20%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20vital%20for%20public%20safety%2C%20particularly%20in%0Awildfire%20monitoring%2C%20where%20early%20detection%20minimizes%20environmental%20impact.%20In%0AUAV-Assisted%20Wildfire%20Monitoring%20%28UAWM%29%20systems%2C%20joint%20optimization%20of%20sensor%0Atransmission%20scheduling%20and%20velocity%20is%20critical%20for%20minimizing%20Age%20of%0AInformation%20%28AoI%29%20from%20stale%20sensor%20data.%20Deep%20Reinforcement%20Learning%20%28DRL%29%20has%0Abeen%20used%20for%20such%20optimization%3B%20however%2C%20its%20limitations%20such%20as%20low%20sampling%0Aefficiency%2C%20simulation-to-reality%20gaps%2C%20and%20complex%20training%20render%20it%0Aunsuitable%20for%20time-critical%20applications%20like%20wildfire%20monitoring.%20This%20paper%0Aintroduces%20a%20new%20online%20Flight%20Resource%20Allocation%20scheme%20based%20on%20LLM-Enabled%0AIn-Context%20Learning%20%28FRSICL%29%20to%20jointly%20optimize%20the%20UAV%27s%20flight%20control%20and%0Adata%20collection%20schedule%20along%20the%20trajectory%20in%20real%20time%2C%20thereby%0Aasymptotically%20minimizing%20the%20average%20AoI%20across%20ground%20sensors.%20In%20contrast%20to%0ADRL%2C%20FRSICL%20generates%20data%20collection%20schedules%20and%20controls%20velocity%20using%0Anatural%20language%20task%20descriptions%20and%20feedback%20from%20the%20environment%2C%20enabling%0Adynamic%20decision-making%20without%20extensive%20retraining.%20Simulation%20results%0Aconfirm%20the%20effectiveness%20of%20the%20proposed%20FRSICL%20compared%20to%20Proximal%20Policy%0AOptimization%20%28PPO%29%20and%20Nearest-Neighbor%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10134v1&entry.124074799=Read"},
{"title": "Breaking the Myth: Can Small Models Infer Postconditions Too?", "author": "Gehao Zhang and Zhenting Wang and Juan Zhai", "abstract": "  Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration.\n", "link": "http://arxiv.org/abs/2507.10182v1", "date": "2025-07-14", "relevancy": 1.9172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Myth%3A%20Can%20Small%20Models%20Infer%20Postconditions%20Too%3F&body=Title%3A%20Breaking%20the%20Myth%3A%20Can%20Small%20Models%20Infer%20Postconditions%20Too%3F%0AAuthor%3A%20Gehao%20Zhang%20and%20Zhenting%20Wang%20and%20Juan%20Zhai%0AAbstract%3A%20%20%20Formal%20specifications%20are%20essential%20for%20ensuring%20software%20correctness%2C%20yet%0Amanually%20writing%20them%20is%20tedious%20and%20error-prone.%20Large%20Language%20Models%20%28LLMs%29%0Ahave%20shown%20promise%20in%20generating%20such%20specifications%20from%20natural%20language%0Aintents%2C%20but%20the%20giant%20model%20size%20and%20high%20computational%20demands%20raise%20a%0Afundamental%20question%3A%20Do%20we%20really%20need%20large%20models%20for%20this%20task%3F%20In%20this%0Apaper%2C%20we%20show%20that%20a%20small%2C%20fine-tuned%20language%20model%20can%20achieve%20high-quality%0Apostcondition%20generation%20with%20much%20lower%20computational%20costs.%20We%20construct%20a%0Aspecialized%20dataset%20of%20prompts%2C%20reasoning%20logs%2C%20and%20postconditions%2C%20then%0Asupervise%20the%20fine-tuning%20of%20a%20%247%24B-parameter%20code%20model.%20Our%20approach%20tackles%0Areal-world%20repository%20dependencies%20and%20preserves%20pre-state%20information%2C%0Aallowing%20for%20expressive%20and%20accurate%20specifications.%20We%20evaluate%20the%20model%20on%20a%0Abenchmark%20of%20real-world%20Java%20bugs%20%28Defects4J%29%20and%20compare%20against%20both%0Aproprietary%20giants%20%28e.g.%2C%20GPT-4o%29%20and%20open-source%20large%20models.%20Empirical%0Aresults%20demonstrate%20that%20our%20compact%20model%20matches%20or%20outperforms%20significantly%0Alarger%20counterparts%20in%20syntax%20correctness%2C%20semantic%20correctness%2C%20and%0Abug-distinguishing%20capability.%20These%20findings%20highlight%20that%20targeted%0Afine-tuning%20on%20a%20modest%20dataset%20can%20enable%20small%20models%20to%20achieve%20results%0Aformerly%20seen%20only%20in%20massive%2C%20resource-heavy%20LLMs%2C%20offering%20a%20practical%20and%0Aefficient%20path%20for%20the%20real-world%20adoption%20of%20automated%20specification%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Myth%253A%2520Can%2520Small%2520Models%2520Infer%2520Postconditions%2520Too%253F%26entry.906535625%3DGehao%2520Zhang%2520and%2520Zhenting%2520Wang%2520and%2520Juan%2520Zhai%26entry.1292438233%3D%2520%2520Formal%2520specifications%2520are%2520essential%2520for%2520ensuring%2520software%2520correctness%252C%2520yet%250Amanually%2520writing%2520them%2520is%2520tedious%2520and%2520error-prone.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Ahave%2520shown%2520promise%2520in%2520generating%2520such%2520specifications%2520from%2520natural%2520language%250Aintents%252C%2520but%2520the%2520giant%2520model%2520size%2520and%2520high%2520computational%2520demands%2520raise%2520a%250Afundamental%2520question%253A%2520Do%2520we%2520really%2520need%2520large%2520models%2520for%2520this%2520task%253F%2520In%2520this%250Apaper%252C%2520we%2520show%2520that%2520a%2520small%252C%2520fine-tuned%2520language%2520model%2520can%2520achieve%2520high-quality%250Apostcondition%2520generation%2520with%2520much%2520lower%2520computational%2520costs.%2520We%2520construct%2520a%250Aspecialized%2520dataset%2520of%2520prompts%252C%2520reasoning%2520logs%252C%2520and%2520postconditions%252C%2520then%250Asupervise%2520the%2520fine-tuning%2520of%2520a%2520%25247%2524B-parameter%2520code%2520model.%2520Our%2520approach%2520tackles%250Areal-world%2520repository%2520dependencies%2520and%2520preserves%2520pre-state%2520information%252C%250Aallowing%2520for%2520expressive%2520and%2520accurate%2520specifications.%2520We%2520evaluate%2520the%2520model%2520on%2520a%250Abenchmark%2520of%2520real-world%2520Java%2520bugs%2520%2528Defects4J%2529%2520and%2520compare%2520against%2520both%250Aproprietary%2520giants%2520%2528e.g.%252C%2520GPT-4o%2529%2520and%2520open-source%2520large%2520models.%2520Empirical%250Aresults%2520demonstrate%2520that%2520our%2520compact%2520model%2520matches%2520or%2520outperforms%2520significantly%250Alarger%2520counterparts%2520in%2520syntax%2520correctness%252C%2520semantic%2520correctness%252C%2520and%250Abug-distinguishing%2520capability.%2520These%2520findings%2520highlight%2520that%2520targeted%250Afine-tuning%2520on%2520a%2520modest%2520dataset%2520can%2520enable%2520small%2520models%2520to%2520achieve%2520results%250Aformerly%2520seen%2520only%2520in%2520massive%252C%2520resource-heavy%2520LLMs%252C%2520offering%2520a%2520practical%2520and%250Aefficient%2520path%2520for%2520the%2520real-world%2520adoption%2520of%2520automated%2520specification%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Myth%3A%20Can%20Small%20Models%20Infer%20Postconditions%20Too%3F&entry.906535625=Gehao%20Zhang%20and%20Zhenting%20Wang%20and%20Juan%20Zhai&entry.1292438233=%20%20Formal%20specifications%20are%20essential%20for%20ensuring%20software%20correctness%2C%20yet%0Amanually%20writing%20them%20is%20tedious%20and%20error-prone.%20Large%20Language%20Models%20%28LLMs%29%0Ahave%20shown%20promise%20in%20generating%20such%20specifications%20from%20natural%20language%0Aintents%2C%20but%20the%20giant%20model%20size%20and%20high%20computational%20demands%20raise%20a%0Afundamental%20question%3A%20Do%20we%20really%20need%20large%20models%20for%20this%20task%3F%20In%20this%0Apaper%2C%20we%20show%20that%20a%20small%2C%20fine-tuned%20language%20model%20can%20achieve%20high-quality%0Apostcondition%20generation%20with%20much%20lower%20computational%20costs.%20We%20construct%20a%0Aspecialized%20dataset%20of%20prompts%2C%20reasoning%20logs%2C%20and%20postconditions%2C%20then%0Asupervise%20the%20fine-tuning%20of%20a%20%247%24B-parameter%20code%20model.%20Our%20approach%20tackles%0Areal-world%20repository%20dependencies%20and%20preserves%20pre-state%20information%2C%0Aallowing%20for%20expressive%20and%20accurate%20specifications.%20We%20evaluate%20the%20model%20on%20a%0Abenchmark%20of%20real-world%20Java%20bugs%20%28Defects4J%29%20and%20compare%20against%20both%0Aproprietary%20giants%20%28e.g.%2C%20GPT-4o%29%20and%20open-source%20large%20models.%20Empirical%0Aresults%20demonstrate%20that%20our%20compact%20model%20matches%20or%20outperforms%20significantly%0Alarger%20counterparts%20in%20syntax%20correctness%2C%20semantic%20correctness%2C%20and%0Abug-distinguishing%20capability.%20These%20findings%20highlight%20that%20targeted%0Afine-tuning%20on%20a%20modest%20dataset%20can%20enable%20small%20models%20to%20achieve%20results%0Aformerly%20seen%20only%20in%20massive%2C%20resource-heavy%20LLMs%2C%20offering%20a%20practical%20and%0Aefficient%20path%20for%20the%20real-world%20adoption%20of%20automated%20specification%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10182v1&entry.124074799=Read"},
{"title": "Measuring Scientific Capabilities of Language Models with a Systems\n  Biology Dry Lab", "author": "Haonan Duan and Stephen Zhewen Lu and Caitlin Fiona Harrigan and Nishkrit Desai and Jiarui Lu and Micha\u0142 Koziarski and Leonardo Cotta and Chris J. Maddison", "abstract": "  Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents.\n", "link": "http://arxiv.org/abs/2507.02083v2", "date": "2025-07-14", "relevancy": 2.016, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Scientific%20Capabilities%20of%20Language%20Models%20with%20a%20Systems%0A%20%20Biology%20Dry%20Lab&body=Title%3A%20Measuring%20Scientific%20Capabilities%20of%20Language%20Models%20with%20a%20Systems%0A%20%20Biology%20Dry%20Lab%0AAuthor%3A%20Haonan%20Duan%20and%20Stephen%20Zhewen%20Lu%20and%20Caitlin%20Fiona%20Harrigan%20and%20Nishkrit%20Desai%20and%20Jiarui%20Lu%20and%20Micha%C5%82%20Koziarski%20and%20Leonardo%20Cotta%20and%20Chris%20J.%20Maddison%0AAbstract%3A%20%20%20Designing%20experiments%20and%20result%20interpretations%20are%20core%20scientific%0Acompetencies%2C%20particularly%20in%20biology%2C%20where%20researchers%20perturb%20complex%0Asystems%20to%20uncover%20the%20underlying%20systems.%20Recent%20efforts%20to%20evaluate%20the%0Ascientific%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20fail%20to%20test%20these%0Acompetencies%20because%20wet-lab%20experimentation%20is%20prohibitively%20expensive%3A%20in%0Aexpertise%2C%20time%20and%20equipment.%20We%20introduce%20SciGym%2C%20a%20first-in-class%20benchmark%0Athat%20assesses%20LLMs%27%20iterative%20experiment%20design%20and%20analysis%20abilities%20in%0Aopen-ended%20scientific%20discovery%20tasks.%20SciGym%20overcomes%20the%20challenge%20of%0Awet-lab%20costs%20by%20running%20a%20dry%20lab%20of%20biological%20systems.%20These%20models%2C%20encoded%0Ain%20Systems%20Biology%20Markup%20Language%2C%20are%20efficient%20for%20generating%20simulated%0Adata%2C%20making%20them%20ideal%20testbeds%20for%20experimentation%20on%20realistically%20complex%0Asystems.%20We%20evaluated%20six%20frontier%20LLMs%20on%20137%20small%20systems%2C%20and%20released%20a%0Atotal%20of%20350%20systems.%20Our%20evaluation%20shows%20that%20while%20more%20capable%20models%0Ademonstrated%20superior%20performance%2C%20all%20models%27%20performance%20declined%0Asignificantly%20as%20system%20complexity%20increased%2C%20suggesting%20substantial%20room%20for%0Aimprovement%20in%20the%20scientific%20capabilities%20of%20LLM%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Scientific%2520Capabilities%2520of%2520Language%2520Models%2520with%2520a%2520Systems%250A%2520%2520Biology%2520Dry%2520Lab%26entry.906535625%3DHaonan%2520Duan%2520and%2520Stephen%2520Zhewen%2520Lu%2520and%2520Caitlin%2520Fiona%2520Harrigan%2520and%2520Nishkrit%2520Desai%2520and%2520Jiarui%2520Lu%2520and%2520Micha%25C5%2582%2520Koziarski%2520and%2520Leonardo%2520Cotta%2520and%2520Chris%2520J.%2520Maddison%26entry.1292438233%3D%2520%2520Designing%2520experiments%2520and%2520result%2520interpretations%2520are%2520core%2520scientific%250Acompetencies%252C%2520particularly%2520in%2520biology%252C%2520where%2520researchers%2520perturb%2520complex%250Asystems%2520to%2520uncover%2520the%2520underlying%2520systems.%2520Recent%2520efforts%2520to%2520evaluate%2520the%250Ascientific%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520fail%2520to%2520test%2520these%250Acompetencies%2520because%2520wet-lab%2520experimentation%2520is%2520prohibitively%2520expensive%253A%2520in%250Aexpertise%252C%2520time%2520and%2520equipment.%2520We%2520introduce%2520SciGym%252C%2520a%2520first-in-class%2520benchmark%250Athat%2520assesses%2520LLMs%2527%2520iterative%2520experiment%2520design%2520and%2520analysis%2520abilities%2520in%250Aopen-ended%2520scientific%2520discovery%2520tasks.%2520SciGym%2520overcomes%2520the%2520challenge%2520of%250Awet-lab%2520costs%2520by%2520running%2520a%2520dry%2520lab%2520of%2520biological%2520systems.%2520These%2520models%252C%2520encoded%250Ain%2520Systems%2520Biology%2520Markup%2520Language%252C%2520are%2520efficient%2520for%2520generating%2520simulated%250Adata%252C%2520making%2520them%2520ideal%2520testbeds%2520for%2520experimentation%2520on%2520realistically%2520complex%250Asystems.%2520We%2520evaluated%2520six%2520frontier%2520LLMs%2520on%2520137%2520small%2520systems%252C%2520and%2520released%2520a%250Atotal%2520of%2520350%2520systems.%2520Our%2520evaluation%2520shows%2520that%2520while%2520more%2520capable%2520models%250Ademonstrated%2520superior%2520performance%252C%2520all%2520models%2527%2520performance%2520declined%250Asignificantly%2520as%2520system%2520complexity%2520increased%252C%2520suggesting%2520substantial%2520room%2520for%250Aimprovement%2520in%2520the%2520scientific%2520capabilities%2520of%2520LLM%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Scientific%20Capabilities%20of%20Language%20Models%20with%20a%20Systems%0A%20%20Biology%20Dry%20Lab&entry.906535625=Haonan%20Duan%20and%20Stephen%20Zhewen%20Lu%20and%20Caitlin%20Fiona%20Harrigan%20and%20Nishkrit%20Desai%20and%20Jiarui%20Lu%20and%20Micha%C5%82%20Koziarski%20and%20Leonardo%20Cotta%20and%20Chris%20J.%20Maddison&entry.1292438233=%20%20Designing%20experiments%20and%20result%20interpretations%20are%20core%20scientific%0Acompetencies%2C%20particularly%20in%20biology%2C%20where%20researchers%20perturb%20complex%0Asystems%20to%20uncover%20the%20underlying%20systems.%20Recent%20efforts%20to%20evaluate%20the%0Ascientific%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20fail%20to%20test%20these%0Acompetencies%20because%20wet-lab%20experimentation%20is%20prohibitively%20expensive%3A%20in%0Aexpertise%2C%20time%20and%20equipment.%20We%20introduce%20SciGym%2C%20a%20first-in-class%20benchmark%0Athat%20assesses%20LLMs%27%20iterative%20experiment%20design%20and%20analysis%20abilities%20in%0Aopen-ended%20scientific%20discovery%20tasks.%20SciGym%20overcomes%20the%20challenge%20of%0Awet-lab%20costs%20by%20running%20a%20dry%20lab%20of%20biological%20systems.%20These%20models%2C%20encoded%0Ain%20Systems%20Biology%20Markup%20Language%2C%20are%20efficient%20for%20generating%20simulated%0Adata%2C%20making%20them%20ideal%20testbeds%20for%20experimentation%20on%20realistically%20complex%0Asystems.%20We%20evaluated%20six%20frontier%20LLMs%20on%20137%20small%20systems%2C%20and%20released%20a%0Atotal%20of%20350%20systems.%20Our%20evaluation%20shows%20that%20while%20more%20capable%20models%0Ademonstrated%20superior%20performance%2C%20all%20models%27%20performance%20declined%0Asignificantly%20as%20system%20complexity%20increased%2C%20suggesting%20substantial%20room%20for%0Aimprovement%20in%20the%20scientific%20capabilities%20of%20LLM%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02083v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


