<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features\n  and Rendered Novel Views", "author": "Francis Engelmann and Fabian Manhardt and Michael Niemeyer and Keisuke Tateno and Marc Pollefeys and Federico Tombari", "abstract": "  Large visual-language models (VLMs), like CLIP, enable open-set image\nsegmentation to segment arbitrary concepts from an image in a zero-shot manner.\nThis goes beyond the traditional closed-set assumption, i.e., where models can\nonly segment classes from a pre-defined training set. More recently, first\nworks on open-set segmentation in 3D scenes have appeared in the literature.\nThese methods are heavily influenced by closed-set 3D convolutional approaches\nthat process point clouds or polygon meshes. However, these 3D scene\nrepresentations do not align well with the image-based nature of the\nvisual-language models. Indeed, point cloud and 3D meshes typically have a\nlower resolution than images and the reconstructed 3D scene geometry might not\nproject well to the underlying 2D image sequences used to compute pixel-aligned\nCLIP features. To address these challenges, we propose OpenNeRF which naturally\noperates on posed images and directly encodes the VLM features within the NeRF.\nThis is similar in spirit to LERF, however our work shows that using pixel-wise\nVLM features (instead of global CLIP features) results in an overall less\ncomplex architecture without the need for additional DINO regularization. Our\nOpenNeRF further leverages NeRF's ability to render novel views and extract\nopen-set VLM features from areas that are not well observed in the initial\nposed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF\noutperforms recent open-vocabulary methods such as LERF and OpenScene by at\nleast +4.9 mIoU.\n", "link": "http://arxiv.org/abs/2404.03650v1", "date": "2024-04-04", "relevancy": 2.8312, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5843}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5558}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OpenNeRF%3A%20Open%20Set%203D%20Neural%20Scene%20Segmentation%20with%20Pixel-Wise%20Features%0A%20%20and%20Rendered%20Novel%20Views&body=Title%3A%20OpenNeRF%3A%20Open%20Set%203D%20Neural%20Scene%20Segmentation%20with%20Pixel-Wise%20Features%0A%20%20and%20Rendered%20Novel%20Views%0AAuthor%3A%20Francis%20Engelmann%20and%20Fabian%20Manhardt%20and%20Michael%20Niemeyer%20and%20Keisuke%20Tateno%20and%20Marc%20Pollefeys%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Large%20visual-language%20models%20%28VLMs%29%2C%20like%20CLIP%2C%20enable%20open-set%20image%0Asegmentation%20to%20segment%20arbitrary%20concepts%20from%20an%20image%20in%20a%20zero-shot%20manner.%0AThis%20goes%20beyond%20the%20traditional%20closed-set%20assumption%2C%20i.e.%2C%20where%20models%20can%0Aonly%20segment%20classes%20from%20a%20pre-defined%20training%20set.%20More%20recently%2C%20first%0Aworks%20on%20open-set%20segmentation%20in%203D%20scenes%20have%20appeared%20in%20the%20literature.%0AThese%20methods%20are%20heavily%20influenced%20by%20closed-set%203D%20convolutional%20approaches%0Athat%20process%20point%20clouds%20or%20polygon%20meshes.%20However%2C%20these%203D%20scene%0Arepresentations%20do%20not%20align%20well%20with%20the%20image-based%20nature%20of%20the%0Avisual-language%20models.%20Indeed%2C%20point%20cloud%20and%203D%20meshes%20typically%20have%20a%0Alower%20resolution%20than%20images%20and%20the%20reconstructed%203D%20scene%20geometry%20might%20not%0Aproject%20well%20to%20the%20underlying%202D%20image%20sequences%20used%20to%20compute%20pixel-aligned%0ACLIP%20features.%20To%20address%20these%20challenges%2C%20we%20propose%20OpenNeRF%20which%20naturally%0Aoperates%20on%20posed%20images%20and%20directly%20encodes%20the%20VLM%20features%20within%20the%20NeRF.%0AThis%20is%20similar%20in%20spirit%20to%20LERF%2C%20however%20our%20work%20shows%20that%20using%20pixel-wise%0AVLM%20features%20%28instead%20of%20global%20CLIP%20features%29%20results%20in%20an%20overall%20less%0Acomplex%20architecture%20without%20the%20need%20for%20additional%20DINO%20regularization.%20Our%0AOpenNeRF%20further%20leverages%20NeRF%27s%20ability%20to%20render%20novel%20views%20and%20extract%0Aopen-set%20VLM%20features%20from%20areas%20that%20are%20not%20well%20observed%20in%20the%20initial%0Aposed%20images.%20For%203D%20point%20cloud%20segmentation%20on%20the%20Replica%20dataset%2C%20OpenNeRF%0Aoutperforms%20recent%20open-vocabulary%20methods%20such%20as%20LERF%20and%20OpenScene%20by%20at%0Aleast%20%2B4.9%20mIoU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03650v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenNeRF%3A%20Open%20Set%203D%20Neural%20Scene%20Segmentation%20with%20Pixel-Wise%20Features%0A%20%20and%20Rendered%20Novel%20Views&entry.906535625=Francis%20Engelmann%20and%20Fabian%20Manhardt%20and%20Michael%20Niemeyer%20and%20Keisuke%20Tateno%20and%20Marc%20Pollefeys%20and%20Federico%20Tombari&entry.1292438233=%20%20Large%20visual-language%20models%20%28VLMs%29%2C%20like%20CLIP%2C%20enable%20open-set%20image%0Asegmentation%20to%20segment%20arbitrary%20concepts%20from%20an%20image%20in%20a%20zero-shot%20manner.%0AThis%20goes%20beyond%20the%20traditional%20closed-set%20assumption%2C%20i.e.%2C%20where%20models%20can%0Aonly%20segment%20classes%20from%20a%20pre-defined%20training%20set.%20More%20recently%2C%20first%0Aworks%20on%20open-set%20segmentation%20in%203D%20scenes%20have%20appeared%20in%20the%20literature.%0AThese%20methods%20are%20heavily%20influenced%20by%20closed-set%203D%20convolutional%20approaches%0Athat%20process%20point%20clouds%20or%20polygon%20meshes.%20However%2C%20these%203D%20scene%0Arepresentations%20do%20not%20align%20well%20with%20the%20image-based%20nature%20of%20the%0Avisual-language%20models.%20Indeed%2C%20point%20cloud%20and%203D%20meshes%20typically%20have%20a%0Alower%20resolution%20than%20images%20and%20the%20reconstructed%203D%20scene%20geometry%20might%20not%0Aproject%20well%20to%20the%20underlying%202D%20image%20sequences%20used%20to%20compute%20pixel-aligned%0ACLIP%20features.%20To%20address%20these%20challenges%2C%20we%20propose%20OpenNeRF%20which%20naturally%0Aoperates%20on%20posed%20images%20and%20directly%20encodes%20the%20VLM%20features%20within%20the%20NeRF.%0AThis%20is%20similar%20in%20spirit%20to%20LERF%2C%20however%20our%20work%20shows%20that%20using%20pixel-wise%0AVLM%20features%20%28instead%20of%20global%20CLIP%20features%29%20results%20in%20an%20overall%20less%0Acomplex%20architecture%20without%20the%20need%20for%20additional%20DINO%20regularization.%20Our%0AOpenNeRF%20further%20leverages%20NeRF%27s%20ability%20to%20render%20novel%20views%20and%20extract%0Aopen-set%20VLM%20features%20from%20areas%20that%20are%20not%20well%20observed%20in%20the%20initial%0Aposed%20images.%20For%203D%20point%20cloud%20segmentation%20on%20the%20Replica%20dataset%2C%20OpenNeRF%0Aoutperforms%20recent%20open-vocabulary%20methods%20such%20as%20LERF%20and%20OpenScene%20by%20at%0Aleast%20%2B4.9%20mIoU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03650v1&entry.124074799=Read"},
{"title": "ILPO-NET: Network for the invariant recognition of arbitrary volumetric\n  patterns in 3D", "author": "Dmitrii Zhemchuzhnikov and Sergei Grudinin", "abstract": "  Effective recognition of spatial patterns and learning their hierarchy is\ncrucial in modern spatial data analysis. Volumetric data applications seek\ntechniques ensuring invariance not only to shifts but also to pattern\nrotations. While traditional methods can readily achieve translational\ninvariance, rotational invariance possesses multiple challenges and remains an\nactive area of research. Here, we present ILPO-Net (Invariant to Local Patterns\nOrientation Network), a novel approach that handles arbitrarily shaped patterns\nwith the convolutional operation inherently invariant to local spatial pattern\norientations using the Wigner matrix expansions. Our architecture seamlessly\nintegrates the new convolution operator and, when benchmarked on diverse\nvolumetric datasets such as MedMNIST and CATH, demonstrates superior\nperformance over the baselines with significantly reduced parameter counts - up\nto 1000 times fewer in the case of MedMNIST. Beyond these demonstrations,\nILPO-Net's rotational invariance paves the way for other applications across\nmultiple disciplines. Our code is publicly available at\nhttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.\n", "link": "http://arxiv.org/abs/2403.19612v2", "date": "2024-04-04", "relevancy": 2.7784, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5666}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5606}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5398}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ILPO-NET%3A%20Network%20for%20the%20invariant%20recognition%20of%20arbitrary%20volumetric%0A%20%20patterns%20in%203D&body=Title%3A%20ILPO-NET%3A%20Network%20for%20the%20invariant%20recognition%20of%20arbitrary%20volumetric%0A%20%20patterns%20in%203D%0AAuthor%3A%20Dmitrii%20Zhemchuzhnikov%20and%20Sergei%20Grudinin%0AAbstract%3A%20%20%20Effective%20recognition%20of%20spatial%20patterns%20and%20learning%20their%20hierarchy%20is%0Acrucial%20in%20modern%20spatial%20data%20analysis.%20Volumetric%20data%20applications%20seek%0Atechniques%20ensuring%20invariance%20not%20only%20to%20shifts%20but%20also%20to%20pattern%0Arotations.%20While%20traditional%20methods%20can%20readily%20achieve%20translational%0Ainvariance%2C%20rotational%20invariance%20possesses%20multiple%20challenges%20and%20remains%20an%0Aactive%20area%20of%20research.%20Here%2C%20we%20present%20ILPO-Net%20%28Invariant%20to%20Local%20Patterns%0AOrientation%20Network%29%2C%20a%20novel%20approach%20that%20handles%20arbitrarily%20shaped%20patterns%0Awith%20the%20convolutional%20operation%20inherently%20invariant%20to%20local%20spatial%20pattern%0Aorientations%20using%20the%20Wigner%20matrix%20expansions.%20Our%20architecture%20seamlessly%0Aintegrates%20the%20new%20convolution%20operator%20and%2C%20when%20benchmarked%20on%20diverse%0Avolumetric%20datasets%20such%20as%20MedMNIST%20and%20CATH%2C%20demonstrates%20superior%0Aperformance%20over%20the%20baselines%20with%20significantly%20reduced%20parameter%20counts%20-%20up%0Ato%201000%20times%20fewer%20in%20the%20case%20of%20MedMNIST.%20Beyond%20these%20demonstrations%2C%0AILPO-Net%27s%20rotational%20invariance%20paves%20the%20way%20for%20other%20applications%20across%0Amultiple%20disciplines.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19612v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ILPO-NET%3A%20Network%20for%20the%20invariant%20recognition%20of%20arbitrary%20volumetric%0A%20%20patterns%20in%203D&entry.906535625=Dmitrii%20Zhemchuzhnikov%20and%20Sergei%20Grudinin&entry.1292438233=%20%20Effective%20recognition%20of%20spatial%20patterns%20and%20learning%20their%20hierarchy%20is%0Acrucial%20in%20modern%20spatial%20data%20analysis.%20Volumetric%20data%20applications%20seek%0Atechniques%20ensuring%20invariance%20not%20only%20to%20shifts%20but%20also%20to%20pattern%0Arotations.%20While%20traditional%20methods%20can%20readily%20achieve%20translational%0Ainvariance%2C%20rotational%20invariance%20possesses%20multiple%20challenges%20and%20remains%20an%0Aactive%20area%20of%20research.%20Here%2C%20we%20present%20ILPO-Net%20%28Invariant%20to%20Local%20Patterns%0AOrientation%20Network%29%2C%20a%20novel%20approach%20that%20handles%20arbitrarily%20shaped%20patterns%0Awith%20the%20convolutional%20operation%20inherently%20invariant%20to%20local%20spatial%20pattern%0Aorientations%20using%20the%20Wigner%20matrix%20expansions.%20Our%20architecture%20seamlessly%0Aintegrates%20the%20new%20convolution%20operator%20and%2C%20when%20benchmarked%20on%20diverse%0Avolumetric%20datasets%20such%20as%20MedMNIST%20and%20CATH%2C%20demonstrates%20superior%0Aperformance%20over%20the%20baselines%20with%20significantly%20reduced%20parameter%20counts%20-%20up%0Ato%201000%20times%20fewer%20in%20the%20case%20of%20MedMNIST.%20Beyond%20these%20demonstrations%2C%0AILPO-Net%27s%20rotational%20invariance%20paves%20the%20way%20for%20other%20applications%20across%0Amultiple%20disciplines.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19612v2&entry.124074799=Read"},
{"title": "Reference-Based 3D-Aware Image Editing with Triplane", "author": "Bahri Batuhan Bilecen and Yigit Yalin and Ning Yu and Aysegul Dundar", "abstract": "  Generative Adversarial Networks (GANs) have emerged as powerful tools not\nonly for high-quality image generation but also for real image editing through\nmanipulation of their interpretable latent spaces. Recent advancements in GANs\ninclude the development of 3D-aware models such as EG3D, characterized by\nefficient triplane-based architectures enabling the reconstruction of 3D\ngeometry from single images. However, scant attention has been devoted to\nproviding an integrated framework for high-quality reference-based 3D-aware\nimage editing within this domain. This study addresses this gap by exploring\nand demonstrating the effectiveness of EG3D's triplane space for achieving\nadvanced reference-based edits, presenting a unique perspective on 3D-aware\nimage editing through our novel pipeline. Our approach integrates the encoding\nof triplane features, spatial disentanglement and automatic localization of\nfeatures in the triplane domain, and fusion learning for desired image editing.\nMoreover, our framework demonstrates versatility across domains, extending its\neffectiveness to animal face edits and partial stylization of cartoon\nportraits. The method shows significant improvements over relevant 3D-aware\nlatent editing and 2D reference-based editing methods, both qualitatively and\nquantitatively. Project page: https://three-bee.github.io/triplane_edit\n", "link": "http://arxiv.org/abs/2404.03632v1", "date": "2024-04-04", "relevancy": 2.7214, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5873}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5387}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5068}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reference-Based%203D-Aware%20Image%20Editing%20with%20Triplane&body=Title%3A%20Reference-Based%203D-Aware%20Image%20Editing%20with%20Triplane%0AAuthor%3A%20Bahri%20Batuhan%20Bilecen%20and%20Yigit%20Yalin%20and%20Ning%20Yu%20and%20Aysegul%20Dundar%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20emerged%20as%20powerful%20tools%20not%0Aonly%20for%20high-quality%20image%20generation%20but%20also%20for%20real%20image%20editing%20through%0Amanipulation%20of%20their%20interpretable%20latent%20spaces.%20Recent%20advancements%20in%20GANs%0Ainclude%20the%20development%20of%203D-aware%20models%20such%20as%20EG3D%2C%20characterized%20by%0Aefficient%20triplane-based%20architectures%20enabling%20the%20reconstruction%20of%203D%0Ageometry%20from%20single%20images.%20However%2C%20scant%20attention%20has%20been%20devoted%20to%0Aproviding%20an%20integrated%20framework%20for%20high-quality%20reference-based%203D-aware%0Aimage%20editing%20within%20this%20domain.%20This%20study%20addresses%20this%20gap%20by%20exploring%0Aand%20demonstrating%20the%20effectiveness%20of%20EG3D%27s%20triplane%20space%20for%20achieving%0Aadvanced%20reference-based%20edits%2C%20presenting%20a%20unique%20perspective%20on%203D-aware%0Aimage%20editing%20through%20our%20novel%20pipeline.%20Our%20approach%20integrates%20the%20encoding%0Aof%20triplane%20features%2C%20spatial%20disentanglement%20and%20automatic%20localization%20of%0Afeatures%20in%20the%20triplane%20domain%2C%20and%20fusion%20learning%20for%20desired%20image%20editing.%0AMoreover%2C%20our%20framework%20demonstrates%20versatility%20across%20domains%2C%20extending%20its%0Aeffectiveness%20to%20animal%20face%20edits%20and%20partial%20stylization%20of%20cartoon%0Aportraits.%20The%20method%20shows%20significant%20improvements%20over%20relevant%203D-aware%0Alatent%20editing%20and%202D%20reference-based%20editing%20methods%2C%20both%20qualitatively%20and%0Aquantitatively.%20Project%20page%3A%20https%3A//three-bee.github.io/triplane_edit%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03632v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reference-Based%203D-Aware%20Image%20Editing%20with%20Triplane&entry.906535625=Bahri%20Batuhan%20Bilecen%20and%20Yigit%20Yalin%20and%20Ning%20Yu%20and%20Aysegul%20Dundar&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20emerged%20as%20powerful%20tools%20not%0Aonly%20for%20high-quality%20image%20generation%20but%20also%20for%20real%20image%20editing%20through%0Amanipulation%20of%20their%20interpretable%20latent%20spaces.%20Recent%20advancements%20in%20GANs%0Ainclude%20the%20development%20of%203D-aware%20models%20such%20as%20EG3D%2C%20characterized%20by%0Aefficient%20triplane-based%20architectures%20enabling%20the%20reconstruction%20of%203D%0Ageometry%20from%20single%20images.%20However%2C%20scant%20attention%20has%20been%20devoted%20to%0Aproviding%20an%20integrated%20framework%20for%20high-quality%20reference-based%203D-aware%0Aimage%20editing%20within%20this%20domain.%20This%20study%20addresses%20this%20gap%20by%20exploring%0Aand%20demonstrating%20the%20effectiveness%20of%20EG3D%27s%20triplane%20space%20for%20achieving%0Aadvanced%20reference-based%20edits%2C%20presenting%20a%20unique%20perspective%20on%203D-aware%0Aimage%20editing%20through%20our%20novel%20pipeline.%20Our%20approach%20integrates%20the%20encoding%0Aof%20triplane%20features%2C%20spatial%20disentanglement%20and%20automatic%20localization%20of%0Afeatures%20in%20the%20triplane%20domain%2C%20and%20fusion%20learning%20for%20desired%20image%20editing.%0AMoreover%2C%20our%20framework%20demonstrates%20versatility%20across%20domains%2C%20extending%20its%0Aeffectiveness%20to%20animal%20face%20edits%20and%20partial%20stylization%20of%20cartoon%0Aportraits.%20The%20method%20shows%20significant%20improvements%20over%20relevant%203D-aware%0Alatent%20editing%20and%202D%20reference-based%20editing%20methods%2C%20both%20qualitatively%20and%0Aquantitatively.%20Project%20page%3A%20https%3A//three-bee.github.io/triplane_edit%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03632v1&entry.124074799=Read"},
{"title": "Background Noise Reduction of Attention Map for Weakly Supervised\n  Semantic Segmentation", "author": "Izumi Fujimori and Masaki Oono and Masami Shishibori", "abstract": "  In weakly-supervised semantic segmentation (WSSS) using only image-level\nclass labels, a problem with CNN-based Class Activation Maps (CAM) is that they\ntend to activate the most discriminative local regions of objects. On the other\nhand, methods based on Transformers learn global features but suffer from the\nissue of background noise contamination. This paper focuses on addressing the\nissue of background noise in attention weights within the existing WSSS method\nbased on Conformer, known as TransCAM. The proposed method successfully reduces\nbackground noise, leading to improved accuracy of pseudo labels. Experimental\nresults demonstrate that our model achieves segmentation performance of 70.5%\non the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS\nCOCO 2014 data, outperforming TransCAM in terms of segmentation performance.\n", "link": "http://arxiv.org/abs/2404.03394v1", "date": "2024-04-04", "relevancy": 2.6917, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5415}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5397}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5338}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Background%20Noise%20Reduction%20of%20Attention%20Map%20for%20Weakly%20Supervised%0A%20%20Semantic%20Segmentation&body=Title%3A%20Background%20Noise%20Reduction%20of%20Attention%20Map%20for%20Weakly%20Supervised%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Izumi%20Fujimori%20and%20Masaki%20Oono%20and%20Masami%20Shishibori%0AAbstract%3A%20%20%20In%20weakly-supervised%20semantic%20segmentation%20%28WSSS%29%20using%20only%20image-level%0Aclass%20labels%2C%20a%20problem%20with%20CNN-based%20Class%20Activation%20Maps%20%28CAM%29%20is%20that%20they%0Atend%20to%20activate%20the%20most%20discriminative%20local%20regions%20of%20objects.%20On%20the%20other%0Ahand%2C%20methods%20based%20on%20Transformers%20learn%20global%20features%20but%20suffer%20from%20the%0Aissue%20of%20background%20noise%20contamination.%20This%20paper%20focuses%20on%20addressing%20the%0Aissue%20of%20background%20noise%20in%20attention%20weights%20within%20the%20existing%20WSSS%20method%0Abased%20on%20Conformer%2C%20known%20as%20TransCAM.%20The%20proposed%20method%20successfully%20reduces%0Abackground%20noise%2C%20leading%20to%20improved%20accuracy%20of%20pseudo%20labels.%20Experimental%0Aresults%20demonstrate%20that%20our%20model%20achieves%20segmentation%20performance%20of%2070.5%25%0Aon%20the%20PASCAL%20VOC%202012%20validation%20data%2C%2071.1%25%20on%20the%20test%20data%2C%20and%2045.9%25%20on%20MS%0ACOCO%202014%20data%2C%20outperforming%20TransCAM%20in%20terms%20of%20segmentation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03394v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Background%20Noise%20Reduction%20of%20Attention%20Map%20for%20Weakly%20Supervised%0A%20%20Semantic%20Segmentation&entry.906535625=Izumi%20Fujimori%20and%20Masaki%20Oono%20and%20Masami%20Shishibori&entry.1292438233=%20%20In%20weakly-supervised%20semantic%20segmentation%20%28WSSS%29%20using%20only%20image-level%0Aclass%20labels%2C%20a%20problem%20with%20CNN-based%20Class%20Activation%20Maps%20%28CAM%29%20is%20that%20they%0Atend%20to%20activate%20the%20most%20discriminative%20local%20regions%20of%20objects.%20On%20the%20other%0Ahand%2C%20methods%20based%20on%20Transformers%20learn%20global%20features%20but%20suffer%20from%20the%0Aissue%20of%20background%20noise%20contamination.%20This%20paper%20focuses%20on%20addressing%20the%0Aissue%20of%20background%20noise%20in%20attention%20weights%20within%20the%20existing%20WSSS%20method%0Abased%20on%20Conformer%2C%20known%20as%20TransCAM.%20The%20proposed%20method%20successfully%20reduces%0Abackground%20noise%2C%20leading%20to%20improved%20accuracy%20of%20pseudo%20labels.%20Experimental%0Aresults%20demonstrate%20that%20our%20model%20achieves%20segmentation%20performance%20of%2070.5%25%0Aon%20the%20PASCAL%20VOC%202012%20validation%20data%2C%2071.1%25%20on%20the%20test%20data%2C%20and%2045.9%25%20on%20MS%0ACOCO%202014%20data%2C%20outperforming%20TransCAM%20in%20terms%20of%20segmentation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03394v1&entry.124074799=Read"},
{"title": "UINav: A Practical Approach to Train On-Device Automation Agents", "author": "Wei Li and Fu-Lin Hsu and Will Bishop and Folawiyo Campbell-Ajala and Max Lin and Oriana Riva", "abstract": "  Automation systems that can autonomously drive application user interfaces to\ncomplete user tasks are of great benefit, especially when users are\nsituationally or permanently impaired. Prior automation systems do not produce\ngeneralizable models while AI-based automation agents work reliably only in\nsimple, hand-crafted applications or incur high computation costs. We propose\nUINav, a demonstration-based approach to train automation agents that fit\nmobile devices, yet achieving high success rates with modest numbers of\ndemonstrations. To reduce the demonstration overhead, UINav uses a referee\nmodel that provides users with immediate feedback on tasks where the agent\nfails, and automatically augments human demonstrations to increase diversity in\ntraining data. Our evaluation shows that with only 10 demonstrations UINav can\nachieve 70% accuracy, and that with enough demonstrations it can surpass 90%\naccuracy.\n", "link": "http://arxiv.org/abs/2312.10170v3", "date": "2024-04-04", "relevancy": 2.6502, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5552}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5274}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5074}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents&body=Title%3A%20UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents%0AAuthor%3A%20Wei%20Li%20and%20Fu-Lin%20Hsu%20and%20Will%20Bishop%20and%20Folawiyo%20Campbell-Ajala%20and%20Max%20Lin%20and%20Oriana%20Riva%0AAbstract%3A%20%20%20Automation%20systems%20that%20can%20autonomously%20drive%20application%20user%20interfaces%20to%0Acomplete%20user%20tasks%20are%20of%20great%20benefit%2C%20especially%20when%20users%20are%0Asituationally%20or%20permanently%20impaired.%20Prior%20automation%20systems%20do%20not%20produce%0Ageneralizable%20models%20while%20AI-based%20automation%20agents%20work%20reliably%20only%20in%0Asimple%2C%20hand-crafted%20applications%20or%20incur%20high%20computation%20costs.%20We%20propose%0AUINav%2C%20a%20demonstration-based%20approach%20to%20train%20automation%20agents%20that%20fit%0Amobile%20devices%2C%20yet%20achieving%20high%20success%20rates%20with%20modest%20numbers%20of%0Ademonstrations.%20To%20reduce%20the%20demonstration%20overhead%2C%20UINav%20uses%20a%20referee%0Amodel%20that%20provides%20users%20with%20immediate%20feedback%20on%20tasks%20where%20the%20agent%0Afails%2C%20and%20automatically%20augments%20human%20demonstrations%20to%20increase%20diversity%20in%0Atraining%20data.%20Our%20evaluation%20shows%20that%20with%20only%2010%20demonstrations%20UINav%20can%0Aachieve%2070%25%20accuracy%2C%20and%20that%20with%20enough%20demonstrations%20it%20can%20surpass%2090%25%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10170v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UINav%3A%20A%20Practical%20Approach%20to%20Train%20On-Device%20Automation%20Agents&entry.906535625=Wei%20Li%20and%20Fu-Lin%20Hsu%20and%20Will%20Bishop%20and%20Folawiyo%20Campbell-Ajala%20and%20Max%20Lin%20and%20Oriana%20Riva&entry.1292438233=%20%20Automation%20systems%20that%20can%20autonomously%20drive%20application%20user%20interfaces%20to%0Acomplete%20user%20tasks%20are%20of%20great%20benefit%2C%20especially%20when%20users%20are%0Asituationally%20or%20permanently%20impaired.%20Prior%20automation%20systems%20do%20not%20produce%0Ageneralizable%20models%20while%20AI-based%20automation%20agents%20work%20reliably%20only%20in%0Asimple%2C%20hand-crafted%20applications%20or%20incur%20high%20computation%20costs.%20We%20propose%0AUINav%2C%20a%20demonstration-based%20approach%20to%20train%20automation%20agents%20that%20fit%0Amobile%20devices%2C%20yet%20achieving%20high%20success%20rates%20with%20modest%20numbers%20of%0Ademonstrations.%20To%20reduce%20the%20demonstration%20overhead%2C%20UINav%20uses%20a%20referee%0Amodel%20that%20provides%20users%20with%20immediate%20feedback%20on%20tasks%20where%20the%20agent%0Afails%2C%20and%20automatically%20augments%20human%20demonstrations%20to%20increase%20diversity%20in%0Atraining%20data.%20Our%20evaluation%20shows%20that%20with%20only%2010%20demonstrations%20UINav%20can%0Aachieve%2070%25%20accuracy%2C%20and%20that%20with%20enough%20demonstrations%20it%20can%20surpass%2090%25%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10170v3&entry.124074799=Read"},
{"title": "AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment", "author": "Chunyi Li and Tengchuan Kou and Yixuan Gao and Yuqin Cao and Wei Sun and Zicheng Zhang and Yingjie Zhou and Zhichao Zhang and Weixia Zhang and Haoning Wu and Xiaohong Liu and Xiongkuo Min and Guangtao Zhai", "abstract": "  With the rapid advancements in AI-Generated Content (AIGC), AI-Generated\nImages (AIGIs) have been widely applied in entertainment, education, and social\nmedia. However, due to the significant variance in quality among different\nAIGIs, there is an urgent need for models that consistently match human\nsubjective ratings. To address this issue, we organized a challenge towards\nAIGC quality assessment on NTIRE 2024 that extensively considers 15 popular\ngenerative models, utilizing dynamic hyper-parameters (including\nclassifier-free guidance, iteration epochs, and output image resolution), and\ngather subjective scores that consider perceptual quality and text-to-image\nalignment altogether comprehensively involving 21 subjects. This approach\nculminates in the creation of the largest fine-grained AIGI subjective quality\ndatabase to date with 20,000 AIGIs and 420,000 subjective ratings, known as\nAIGIQA-20K. Furthermore, we conduct benchmark experiments on this database to\nassess the correspondence between 16 mainstream AIGI quality models and human\nperception. We anticipate that this large-scale quality database will inspire\nrobust quality indicators for AIGIs and propel the evolution of AIGC for\nvision. The database is released on\nhttps://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image.\n", "link": "http://arxiv.org/abs/2404.03407v1", "date": "2024-04-04", "relevancy": 2.544, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5323}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5009}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4932}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AIGIQA-20K%3A%20A%20Large%20Database%20for%20AI-Generated%20Image%20Quality%20Assessment&body=Title%3A%20AIGIQA-20K%3A%20A%20Large%20Database%20for%20AI-Generated%20Image%20Quality%20Assessment%0AAuthor%3A%20Chunyi%20Li%20and%20Tengchuan%20Kou%20and%20Yixuan%20Gao%20and%20Yuqin%20Cao%20and%20Wei%20Sun%20and%20Zicheng%20Zhang%20and%20Yingjie%20Zhou%20and%20Zhichao%20Zhang%20and%20Weixia%20Zhang%20and%20Haoning%20Wu%20and%20Xiaohong%20Liu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20With%20the%20rapid%20advancements%20in%20AI-Generated%20Content%20%28AIGC%29%2C%20AI-Generated%0AImages%20%28AIGIs%29%20have%20been%20widely%20applied%20in%20entertainment%2C%20education%2C%20and%20social%0Amedia.%20However%2C%20due%20to%20the%20significant%20variance%20in%20quality%20among%20different%0AAIGIs%2C%20there%20is%20an%20urgent%20need%20for%20models%20that%20consistently%20match%20human%0Asubjective%20ratings.%20To%20address%20this%20issue%2C%20we%20organized%20a%20challenge%20towards%0AAIGC%20quality%20assessment%20on%20NTIRE%202024%20that%20extensively%20considers%2015%20popular%0Agenerative%20models%2C%20utilizing%20dynamic%20hyper-parameters%20%28including%0Aclassifier-free%20guidance%2C%20iteration%20epochs%2C%20and%20output%20image%20resolution%29%2C%20and%0Agather%20subjective%20scores%20that%20consider%20perceptual%20quality%20and%20text-to-image%0Aalignment%20altogether%20comprehensively%20involving%2021%20subjects.%20This%20approach%0Aculminates%20in%20the%20creation%20of%20the%20largest%20fine-grained%20AIGI%20subjective%20quality%0Adatabase%20to%20date%20with%2020%2C000%20AIGIs%20and%20420%2C000%20subjective%20ratings%2C%20known%20as%0AAIGIQA-20K.%20Furthermore%2C%20we%20conduct%20benchmark%20experiments%20on%20this%20database%20to%0Aassess%20the%20correspondence%20between%2016%20mainstream%20AIGI%20quality%20models%20and%20human%0Aperception.%20We%20anticipate%20that%20this%20large-scale%20quality%20database%20will%20inspire%0Arobust%20quality%20indicators%20for%20AIGIs%20and%20propel%20the%20evolution%20of%20AIGC%20for%0Avision.%20The%20database%20is%20released%20on%0Ahttps%3A//www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03407v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIGIQA-20K%3A%20A%20Large%20Database%20for%20AI-Generated%20Image%20Quality%20Assessment&entry.906535625=Chunyi%20Li%20and%20Tengchuan%20Kou%20and%20Yixuan%20Gao%20and%20Yuqin%20Cao%20and%20Wei%20Sun%20and%20Zicheng%20Zhang%20and%20Yingjie%20Zhou%20and%20Zhichao%20Zhang%20and%20Weixia%20Zhang%20and%20Haoning%20Wu%20and%20Xiaohong%20Liu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20With%20the%20rapid%20advancements%20in%20AI-Generated%20Content%20%28AIGC%29%2C%20AI-Generated%0AImages%20%28AIGIs%29%20have%20been%20widely%20applied%20in%20entertainment%2C%20education%2C%20and%20social%0Amedia.%20However%2C%20due%20to%20the%20significant%20variance%20in%20quality%20among%20different%0AAIGIs%2C%20there%20is%20an%20urgent%20need%20for%20models%20that%20consistently%20match%20human%0Asubjective%20ratings.%20To%20address%20this%20issue%2C%20we%20organized%20a%20challenge%20towards%0AAIGC%20quality%20assessment%20on%20NTIRE%202024%20that%20extensively%20considers%2015%20popular%0Agenerative%20models%2C%20utilizing%20dynamic%20hyper-parameters%20%28including%0Aclassifier-free%20guidance%2C%20iteration%20epochs%2C%20and%20output%20image%20resolution%29%2C%20and%0Agather%20subjective%20scores%20that%20consider%20perceptual%20quality%20and%20text-to-image%0Aalignment%20altogether%20comprehensively%20involving%2021%20subjects.%20This%20approach%0Aculminates%20in%20the%20creation%20of%20the%20largest%20fine-grained%20AIGI%20subjective%20quality%0Adatabase%20to%20date%20with%2020%2C000%20AIGIs%20and%20420%2C000%20subjective%20ratings%2C%20known%20as%0AAIGIQA-20K.%20Furthermore%2C%20we%20conduct%20benchmark%20experiments%20on%20this%20database%20to%0Aassess%20the%20correspondence%20between%2016%20mainstream%20AIGI%20quality%20models%20and%20human%0Aperception.%20We%20anticipate%20that%20this%20large-scale%20quality%20database%20will%20inspire%0Arobust%20quality%20indicators%20for%20AIGIs%20and%20propel%20the%20evolution%20of%20AIGC%20for%0Avision.%20The%20database%20is%20released%20on%0Ahttps%3A//www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03407v1&entry.124074799=Read"},
{"title": "Part-Attention Based Model Make Occluded Person Re-Identification\n  Stronger", "author": "Zhihao Chen and Yiyuan Ge", "abstract": "  The goal of occluded person re-identification (ReID) is to retrieve specific\npedestrians in occluded situations. However, occluded person ReID still suffers\nfrom background clutter and low-quality local feature representations, which\nlimits model performance. In our research, we introduce a new framework called\nPAB-ReID, which is a novel ReID model incorporating part-attention mechanisms\nto tackle the aforementioned issues effectively. Firstly, we introduce the\nhuman parsing label to guide the generation of more accurate human part\nattention maps. In addition, we propose a fine-grained feature focuser for\ngenerating fine-grained human local feature representations while suppressing\nbackground interference. Moreover, We also design a part triplet loss to\nsupervise the learning of human local features, which optimizes\nintra/inter-class distance. We conducted extensive experiments on specialized\nocclusion and regular ReID datasets, showcasing that our approach outperforms\nthe existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2404.03443v1", "date": "2024-04-04", "relevancy": 2.5424, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5095}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5094}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5065}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Part-Attention%20Based%20Model%20Make%20Occluded%20Person%20Re-Identification%0A%20%20Stronger&body=Title%3A%20Part-Attention%20Based%20Model%20Make%20Occluded%20Person%20Re-Identification%0A%20%20Stronger%0AAuthor%3A%20Zhihao%20Chen%20and%20Yiyuan%20Ge%0AAbstract%3A%20%20%20The%20goal%20of%20occluded%20person%20re-identification%20%28ReID%29%20is%20to%20retrieve%20specific%0Apedestrians%20in%20occluded%20situations.%20However%2C%20occluded%20person%20ReID%20still%20suffers%0Afrom%20background%20clutter%20and%20low-quality%20local%20feature%20representations%2C%20which%0Alimits%20model%20performance.%20In%20our%20research%2C%20we%20introduce%20a%20new%20framework%20called%0APAB-ReID%2C%20which%20is%20a%20novel%20ReID%20model%20incorporating%20part-attention%20mechanisms%0Ato%20tackle%20the%20aforementioned%20issues%20effectively.%20Firstly%2C%20we%20introduce%20the%0Ahuman%20parsing%20label%20to%20guide%20the%20generation%20of%20more%20accurate%20human%20part%0Aattention%20maps.%20In%20addition%2C%20we%20propose%20a%20fine-grained%20feature%20focuser%20for%0Agenerating%20fine-grained%20human%20local%20feature%20representations%20while%20suppressing%0Abackground%20interference.%20Moreover%2C%20We%20also%20design%20a%20part%20triplet%20loss%20to%0Asupervise%20the%20learning%20of%20human%20local%20features%2C%20which%20optimizes%0Aintra/inter-class%20distance.%20We%20conducted%20extensive%20experiments%20on%20specialized%0Aocclusion%20and%20regular%20ReID%20datasets%2C%20showcasing%20that%20our%20approach%20outperforms%0Athe%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03443v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part-Attention%20Based%20Model%20Make%20Occluded%20Person%20Re-Identification%0A%20%20Stronger&entry.906535625=Zhihao%20Chen%20and%20Yiyuan%20Ge&entry.1292438233=%20%20The%20goal%20of%20occluded%20person%20re-identification%20%28ReID%29%20is%20to%20retrieve%20specific%0Apedestrians%20in%20occluded%20situations.%20However%2C%20occluded%20person%20ReID%20still%20suffers%0Afrom%20background%20clutter%20and%20low-quality%20local%20feature%20representations%2C%20which%0Alimits%20model%20performance.%20In%20our%20research%2C%20we%20introduce%20a%20new%20framework%20called%0APAB-ReID%2C%20which%20is%20a%20novel%20ReID%20model%20incorporating%20part-attention%20mechanisms%0Ato%20tackle%20the%20aforementioned%20issues%20effectively.%20Firstly%2C%20we%20introduce%20the%0Ahuman%20parsing%20label%20to%20guide%20the%20generation%20of%20more%20accurate%20human%20part%0Aattention%20maps.%20In%20addition%2C%20we%20propose%20a%20fine-grained%20feature%20focuser%20for%0Agenerating%20fine-grained%20human%20local%20feature%20representations%20while%20suppressing%0Abackground%20interference.%20Moreover%2C%20We%20also%20design%20a%20part%20triplet%20loss%20to%0Asupervise%20the%20learning%20of%20human%20local%20features%2C%20which%20optimizes%0Aintra/inter-class%20distance.%20We%20conducted%20extensive%20experiments%20on%20specialized%0Aocclusion%20and%20regular%20ReID%20datasets%2C%20showcasing%20that%20our%20approach%20outperforms%0Athe%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03443v1&entry.124074799=Read"},
{"title": "Generative Semi-supervised Graph Anomaly Detection", "author": "Hezhe Qiao and Qingsong Wen and Xiaoli Li and Ee-Peng Lim and Guansong Pang", "abstract": "  This work considers a practical semi-supervised graph anomaly detection (GAD)\nscenario, where part of the nodes in a graph are known to be normal,\ncontrasting to the unsupervised setting in most GAD studies with a fully\nunlabeled graph. As expected, we find that having access to these normal nodes\nhelps enhance the detection performance of existing unsupervised GAD methods\nwhen they are adapted to the semi-supervised setting. However, their\nutilization of these normal nodes is limited. In this paper, we propose a novel\nGenerative GAD approach (GGAD) for the semi-supervised scenario to better\nexploit the normal nodes. The key idea is to generate outlier nodes that\nassimilate anomaly nodes in both local structure and node representations for\nproviding effective negative node samples in training a discriminative\none-class classifier. There have been many generative anomaly detection\napproaches, but they are designed for non-graph data, and as a result, they\nfail to take account of the graph structure information. Our approach tackles\nthis problem by generating graph structure-aware outlier nodes that have\nasymmetric affinity separability from normal nodes while being enforced to\nachieve egocentric closeness to normal nodes in the node representation space.\nComprehensive experiments on four real-world datasets are performed to\nestablish a benchmark for semi-supervised GAD and show that GGAD substantially\noutperforms state-of-the-art unsupervised and semi-supervised GAD methods with\nvarying numbers of training normal nodes. Code will be made available at\nhttps://github.com/mala-lab/GGAD.\n", "link": "http://arxiv.org/abs/2402.11887v3", "date": "2024-04-04", "relevancy": 2.5379, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5162}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5063}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5003}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generative%20Semi-supervised%20Graph%20Anomaly%20Detection&body=Title%3A%20Generative%20Semi-supervised%20Graph%20Anomaly%20Detection%0AAuthor%3A%20Hezhe%20Qiao%20and%20Qingsong%20Wen%20and%20Xiaoli%20Li%20and%20Ee-Peng%20Lim%20and%20Guansong%20Pang%0AAbstract%3A%20%20%20This%20work%20considers%20a%20practical%20semi-supervised%20graph%20anomaly%20detection%20%28GAD%29%0Ascenario%2C%20where%20part%20of%20the%20nodes%20in%20a%20graph%20are%20known%20to%20be%20normal%2C%0Acontrasting%20to%20the%20unsupervised%20setting%20in%20most%20GAD%20studies%20with%20a%20fully%0Aunlabeled%20graph.%20As%20expected%2C%20we%20find%20that%20having%20access%20to%20these%20normal%20nodes%0Ahelps%20enhance%20the%20detection%20performance%20of%20existing%20unsupervised%20GAD%20methods%0Awhen%20they%20are%20adapted%20to%20the%20semi-supervised%20setting.%20However%2C%20their%0Autilization%20of%20these%20normal%20nodes%20is%20limited.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AGenerative%20GAD%20approach%20%28GGAD%29%20for%20the%20semi-supervised%20scenario%20to%20better%0Aexploit%20the%20normal%20nodes.%20The%20key%20idea%20is%20to%20generate%20outlier%20nodes%20that%0Aassimilate%20anomaly%20nodes%20in%20both%20local%20structure%20and%20node%20representations%20for%0Aproviding%20effective%20negative%20node%20samples%20in%20training%20a%20discriminative%0Aone-class%20classifier.%20There%20have%20been%20many%20generative%20anomaly%20detection%0Aapproaches%2C%20but%20they%20are%20designed%20for%20non-graph%20data%2C%20and%20as%20a%20result%2C%20they%0Afail%20to%20take%20account%20of%20the%20graph%20structure%20information.%20Our%20approach%20tackles%0Athis%20problem%20by%20generating%20graph%20structure-aware%20outlier%20nodes%20that%20have%0Aasymmetric%20affinity%20separability%20from%20normal%20nodes%20while%20being%20enforced%20to%0Aachieve%20egocentric%20closeness%20to%20normal%20nodes%20in%20the%20node%20representation%20space.%0AComprehensive%20experiments%20on%20four%20real-world%20datasets%20are%20performed%20to%0Aestablish%20a%20benchmark%20for%20semi-supervised%20GAD%20and%20show%20that%20GGAD%20substantially%0Aoutperforms%20state-of-the-art%20unsupervised%20and%20semi-supervised%20GAD%20methods%20with%0Avarying%20numbers%20of%20training%20normal%20nodes.%20Code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/mala-lab/GGAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11887v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Semi-supervised%20Graph%20Anomaly%20Detection&entry.906535625=Hezhe%20Qiao%20and%20Qingsong%20Wen%20and%20Xiaoli%20Li%20and%20Ee-Peng%20Lim%20and%20Guansong%20Pang&entry.1292438233=%20%20This%20work%20considers%20a%20practical%20semi-supervised%20graph%20anomaly%20detection%20%28GAD%29%0Ascenario%2C%20where%20part%20of%20the%20nodes%20in%20a%20graph%20are%20known%20to%20be%20normal%2C%0Acontrasting%20to%20the%20unsupervised%20setting%20in%20most%20GAD%20studies%20with%20a%20fully%0Aunlabeled%20graph.%20As%20expected%2C%20we%20find%20that%20having%20access%20to%20these%20normal%20nodes%0Ahelps%20enhance%20the%20detection%20performance%20of%20existing%20unsupervised%20GAD%20methods%0Awhen%20they%20are%20adapted%20to%20the%20semi-supervised%20setting.%20However%2C%20their%0Autilization%20of%20these%20normal%20nodes%20is%20limited.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AGenerative%20GAD%20approach%20%28GGAD%29%20for%20the%20semi-supervised%20scenario%20to%20better%0Aexploit%20the%20normal%20nodes.%20The%20key%20idea%20is%20to%20generate%20outlier%20nodes%20that%0Aassimilate%20anomaly%20nodes%20in%20both%20local%20structure%20and%20node%20representations%20for%0Aproviding%20effective%20negative%20node%20samples%20in%20training%20a%20discriminative%0Aone-class%20classifier.%20There%20have%20been%20many%20generative%20anomaly%20detection%0Aapproaches%2C%20but%20they%20are%20designed%20for%20non-graph%20data%2C%20and%20as%20a%20result%2C%20they%0Afail%20to%20take%20account%20of%20the%20graph%20structure%20information.%20Our%20approach%20tackles%0Athis%20problem%20by%20generating%20graph%20structure-aware%20outlier%20nodes%20that%20have%0Aasymmetric%20affinity%20separability%20from%20normal%20nodes%20while%20being%20enforced%20to%0Aachieve%20egocentric%20closeness%20to%20normal%20nodes%20in%20the%20node%20representation%20space.%0AComprehensive%20experiments%20on%20four%20real-world%20datasets%20are%20performed%20to%0Aestablish%20a%20benchmark%20for%20semi-supervised%20GAD%20and%20show%20that%20GGAD%20substantially%0Aoutperforms%20state-of-the-art%20unsupervised%20and%20semi-supervised%20GAD%20methods%20with%0Avarying%20numbers%20of%20training%20normal%20nodes.%20Code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/mala-lab/GGAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11887v3&entry.124074799=Read"},
{"title": "Non-negative Subspace Feature Representation for Few-shot Learning in\n  Medical Imaging", "author": "Keqiang Fan and Xiaohao Cai and Mahesan Niranjan", "abstract": "  Unlike typical visual scene recognition domains, in which massive datasets\nare accessible to deep neural networks, medical image interpretations are often\nobstructed by the paucity of data. In this paper, we investigate the\neffectiveness of data-based few-shot learning in medical imaging by exploring\ndifferent data attribute representations in a low-dimensional space. We\nintroduce different types of non-negative matrix factorization (NMF) in\nfew-shot learning, addressing the data scarcity issue in medical image\nclassification. Extensive empirical studies are conducted in terms of\nvalidating the effectiveness of NMF, especially its supervised variants (e.g.,\ndiscriminative NMF, and supervised and constrained NMF with sparseness), and\nthe comparison with principal component analysis (PCA), i.e., the collaborative\nrepresentation-based dimensionality reduction technique derived from\neigenvectors. With 14 different datasets covering 11 distinct illness\ncategories, thorough experimental results and comparison with related\ntechniques demonstrate that NMF is a competitive alternative to PCA for\nfew-shot learning in medical imaging, and the supervised NMF algorithms are\nmore discriminative in the subspace with greater effectiveness. Furthermore, we\nshow that the part-based representation of NMF, especially its supervised\nvariants, is dramatically impactful in detecting lesion areas in medical\nimaging with limited samples.\n", "link": "http://arxiv.org/abs/2404.02656v2", "date": "2024-04-04", "relevancy": 2.5323, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.521}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Non-negative%20Subspace%20Feature%20Representation%20for%20Few-shot%20Learning%20in%0A%20%20Medical%20Imaging&body=Title%3A%20Non-negative%20Subspace%20Feature%20Representation%20for%20Few-shot%20Learning%20in%0A%20%20Medical%20Imaging%0AAuthor%3A%20Keqiang%20Fan%20and%20Xiaohao%20Cai%20and%20Mahesan%20Niranjan%0AAbstract%3A%20%20%20Unlike%20typical%20visual%20scene%20recognition%20domains%2C%20in%20which%20massive%20datasets%0Aare%20accessible%20to%20deep%20neural%20networks%2C%20medical%20image%20interpretations%20are%20often%0Aobstructed%20by%20the%20paucity%20of%20data.%20In%20this%20paper%2C%20we%20investigate%20the%0Aeffectiveness%20of%20data-based%20few-shot%20learning%20in%20medical%20imaging%20by%20exploring%0Adifferent%20data%20attribute%20representations%20in%20a%20low-dimensional%20space.%20We%0Aintroduce%20different%20types%20of%20non-negative%20matrix%20factorization%20%28NMF%29%20in%0Afew-shot%20learning%2C%20addressing%20the%20data%20scarcity%20issue%20in%20medical%20image%0Aclassification.%20Extensive%20empirical%20studies%20are%20conducted%20in%20terms%20of%0Avalidating%20the%20effectiveness%20of%20NMF%2C%20especially%20its%20supervised%20variants%20%28e.g.%2C%0Adiscriminative%20NMF%2C%20and%20supervised%20and%20constrained%20NMF%20with%20sparseness%29%2C%20and%0Athe%20comparison%20with%20principal%20component%20analysis%20%28PCA%29%2C%20i.e.%2C%20the%20collaborative%0Arepresentation-based%20dimensionality%20reduction%20technique%20derived%20from%0Aeigenvectors.%20With%2014%20different%20datasets%20covering%2011%20distinct%20illness%0Acategories%2C%20thorough%20experimental%20results%20and%20comparison%20with%20related%0Atechniques%20demonstrate%20that%20NMF%20is%20a%20competitive%20alternative%20to%20PCA%20for%0Afew-shot%20learning%20in%20medical%20imaging%2C%20and%20the%20supervised%20NMF%20algorithms%20are%0Amore%20discriminative%20in%20the%20subspace%20with%20greater%20effectiveness.%20Furthermore%2C%20we%0Ashow%20that%20the%20part-based%20representation%20of%20NMF%2C%20especially%20its%20supervised%0Avariants%2C%20is%20dramatically%20impactful%20in%20detecting%20lesion%20areas%20in%20medical%0Aimaging%20with%20limited%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02656v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-negative%20Subspace%20Feature%20Representation%20for%20Few-shot%20Learning%20in%0A%20%20Medical%20Imaging&entry.906535625=Keqiang%20Fan%20and%20Xiaohao%20Cai%20and%20Mahesan%20Niranjan&entry.1292438233=%20%20Unlike%20typical%20visual%20scene%20recognition%20domains%2C%20in%20which%20massive%20datasets%0Aare%20accessible%20to%20deep%20neural%20networks%2C%20medical%20image%20interpretations%20are%20often%0Aobstructed%20by%20the%20paucity%20of%20data.%20In%20this%20paper%2C%20we%20investigate%20the%0Aeffectiveness%20of%20data-based%20few-shot%20learning%20in%20medical%20imaging%20by%20exploring%0Adifferent%20data%20attribute%20representations%20in%20a%20low-dimensional%20space.%20We%0Aintroduce%20different%20types%20of%20non-negative%20matrix%20factorization%20%28NMF%29%20in%0Afew-shot%20learning%2C%20addressing%20the%20data%20scarcity%20issue%20in%20medical%20image%0Aclassification.%20Extensive%20empirical%20studies%20are%20conducted%20in%20terms%20of%0Avalidating%20the%20effectiveness%20of%20NMF%2C%20especially%20its%20supervised%20variants%20%28e.g.%2C%0Adiscriminative%20NMF%2C%20and%20supervised%20and%20constrained%20NMF%20with%20sparseness%29%2C%20and%0Athe%20comparison%20with%20principal%20component%20analysis%20%28PCA%29%2C%20i.e.%2C%20the%20collaborative%0Arepresentation-based%20dimensionality%20reduction%20technique%20derived%20from%0Aeigenvectors.%20With%2014%20different%20datasets%20covering%2011%20distinct%20illness%0Acategories%2C%20thorough%20experimental%20results%20and%20comparison%20with%20related%0Atechniques%20demonstrate%20that%20NMF%20is%20a%20competitive%20alternative%20to%20PCA%20for%0Afew-shot%20learning%20in%20medical%20imaging%2C%20and%20the%20supervised%20NMF%20algorithms%20are%0Amore%20discriminative%20in%20the%20subspace%20with%20greater%20effectiveness.%20Furthermore%2C%20we%0Ashow%20that%20the%20part-based%20representation%20of%20NMF%2C%20especially%20its%20supervised%0Avariants%2C%20is%20dramatically%20impactful%20in%20detecting%20lesion%20areas%20in%20medical%0Aimaging%20with%20limited%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02656v2&entry.124074799=Read"},
{"title": "Decoding Natural Images from EEG for Object Recognition", "author": "Yonghao Song and Bingchuan Liu and Xiang Li and Nanlin Shi and Yijun Wang and Xiaorong Gao", "abstract": "  Electroencephalography (EEG) signals, known for convenient non-invasive\nacquisition but low signal-to-noise ratio, have recently gained substantial\nattention due to the potential to decode natural images. This paper presents a\nself-supervised framework to demonstrate the feasibility of learning image\nrepresentations from EEG signals, particularly for object recognition. The\nframework utilizes image and EEG encoders to extract features from paired image\nstimuli and EEG responses. Contrastive learning aligns these two modalities by\nconstraining their similarity. With the framework, we attain significantly\nabove-chance results on a comprehensive EEG-image dataset, achieving a top-1\naccuracy of 15.6% and a top-5 accuracy of 42.8% in challenging 200-way\nzero-shot tasks. Moreover, we perform extensive experiments to explore the\nbiological plausibility by resolving the temporal, spatial, spectral, and\nsemantic aspects of EEG signals. Besides, we introduce attention modules to\ncapture spatial correlations, providing implicit evidence of the brain activity\nperceived from EEG data. These findings yield valuable insights for neural\ndecoding and brain-computer interfaces in real-world scenarios. The code will\nbe released on https://github.com/eeyhsong/NICE-EEG.\n", "link": "http://arxiv.org/abs/2308.13234v3", "date": "2024-04-04", "relevancy": 2.5298, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5212}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5065}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4901}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Decoding%20Natural%20Images%20from%20EEG%20for%20Object%20Recognition&body=Title%3A%20Decoding%20Natural%20Images%20from%20EEG%20for%20Object%20Recognition%0AAuthor%3A%20Yonghao%20Song%20and%20Bingchuan%20Liu%20and%20Xiang%20Li%20and%20Nanlin%20Shi%20and%20Yijun%20Wang%20and%20Xiaorong%20Gao%0AAbstract%3A%20%20%20Electroencephalography%20%28EEG%29%20signals%2C%20known%20for%20convenient%20non-invasive%0Aacquisition%20but%20low%20signal-to-noise%20ratio%2C%20have%20recently%20gained%20substantial%0Aattention%20due%20to%20the%20potential%20to%20decode%20natural%20images.%20This%20paper%20presents%20a%0Aself-supervised%20framework%20to%20demonstrate%20the%20feasibility%20of%20learning%20image%0Arepresentations%20from%20EEG%20signals%2C%20particularly%20for%20object%20recognition.%20The%0Aframework%20utilizes%20image%20and%20EEG%20encoders%20to%20extract%20features%20from%20paired%20image%0Astimuli%20and%20EEG%20responses.%20Contrastive%20learning%20aligns%20these%20two%20modalities%20by%0Aconstraining%20their%20similarity.%20With%20the%20framework%2C%20we%20attain%20significantly%0Aabove-chance%20results%20on%20a%20comprehensive%20EEG-image%20dataset%2C%20achieving%20a%20top-1%0Aaccuracy%20of%2015.6%25%20and%20a%20top-5%20accuracy%20of%2042.8%25%20in%20challenging%20200-way%0Azero-shot%20tasks.%20Moreover%2C%20we%20perform%20extensive%20experiments%20to%20explore%20the%0Abiological%20plausibility%20by%20resolving%20the%20temporal%2C%20spatial%2C%20spectral%2C%20and%0Asemantic%20aspects%20of%20EEG%20signals.%20Besides%2C%20we%20introduce%20attention%20modules%20to%0Acapture%20spatial%20correlations%2C%20providing%20implicit%20evidence%20of%20the%20brain%20activity%0Aperceived%20from%20EEG%20data.%20These%20findings%20yield%20valuable%20insights%20for%20neural%0Adecoding%20and%20brain-computer%20interfaces%20in%20real-world%20scenarios.%20The%20code%20will%0Abe%20released%20on%20https%3A//github.com/eeyhsong/NICE-EEG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13234v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Natural%20Images%20from%20EEG%20for%20Object%20Recognition&entry.906535625=Yonghao%20Song%20and%20Bingchuan%20Liu%20and%20Xiang%20Li%20and%20Nanlin%20Shi%20and%20Yijun%20Wang%20and%20Xiaorong%20Gao&entry.1292438233=%20%20Electroencephalography%20%28EEG%29%20signals%2C%20known%20for%20convenient%20non-invasive%0Aacquisition%20but%20low%20signal-to-noise%20ratio%2C%20have%20recently%20gained%20substantial%0Aattention%20due%20to%20the%20potential%20to%20decode%20natural%20images.%20This%20paper%20presents%20a%0Aself-supervised%20framework%20to%20demonstrate%20the%20feasibility%20of%20learning%20image%0Arepresentations%20from%20EEG%20signals%2C%20particularly%20for%20object%20recognition.%20The%0Aframework%20utilizes%20image%20and%20EEG%20encoders%20to%20extract%20features%20from%20paired%20image%0Astimuli%20and%20EEG%20responses.%20Contrastive%20learning%20aligns%20these%20two%20modalities%20by%0Aconstraining%20their%20similarity.%20With%20the%20framework%2C%20we%20attain%20significantly%0Aabove-chance%20results%20on%20a%20comprehensive%20EEG-image%20dataset%2C%20achieving%20a%20top-1%0Aaccuracy%20of%2015.6%25%20and%20a%20top-5%20accuracy%20of%2042.8%25%20in%20challenging%20200-way%0Azero-shot%20tasks.%20Moreover%2C%20we%20perform%20extensive%20experiments%20to%20explore%20the%0Abiological%20plausibility%20by%20resolving%20the%20temporal%2C%20spatial%2C%20spectral%2C%20and%0Asemantic%20aspects%20of%20EEG%20signals.%20Besides%2C%20we%20introduce%20attention%20modules%20to%0Acapture%20spatial%20correlations%2C%20providing%20implicit%20evidence%20of%20the%20brain%20activity%0Aperceived%20from%20EEG%20data.%20These%20findings%20yield%20valuable%20insights%20for%20neural%0Adecoding%20and%20brain-computer%20interfaces%20in%20real-world%20scenarios.%20The%20code%20will%0Abe%20released%20on%20https%3A//github.com/eeyhsong/NICE-EEG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13234v3&entry.124074799=Read"},
{"title": "VF-NeRF: Viewshed Fields for Rigid NeRF Registration", "author": "Leo Segre and Shai Avidan", "abstract": "  3D scene registration is a fundamental problem in computer vision that seeks\nthe best 6-DoF alignment between two scenes. This problem was extensively\ninvestigated in the case of point clouds and meshes, but there has been\nrelatively limited work regarding Neural Radiance Fields (NeRF). In this paper,\nwe consider the problem of rigid registration between two NeRFs when the\nposition of the original cameras is not given. Our key novelty is the\nintroduction of Viewshed Fields (VF), an implicit function that determines, for\neach 3D point, how likely it is to be viewed by the original cameras. We\ndemonstrate how VF can help in the various stages of NeRF registration, with an\nextensive evaluation showing that VF-NeRF achieves SOTA results on various\ndatasets with different capturing approaches such as LLFF and Objaverese.\n", "link": "http://arxiv.org/abs/2404.03349v1", "date": "2024-04-04", "relevancy": 2.523, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5113}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5035}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4989}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VF-NeRF%3A%20Viewshed%20Fields%20for%20Rigid%20NeRF%20Registration&body=Title%3A%20VF-NeRF%3A%20Viewshed%20Fields%20for%20Rigid%20NeRF%20Registration%0AAuthor%3A%20Leo%20Segre%20and%20Shai%20Avidan%0AAbstract%3A%20%20%203D%20scene%20registration%20is%20a%20fundamental%20problem%20in%20computer%20vision%20that%20seeks%0Athe%20best%206-DoF%20alignment%20between%20two%20scenes.%20This%20problem%20was%20extensively%0Ainvestigated%20in%20the%20case%20of%20point%20clouds%20and%20meshes%2C%20but%20there%20has%20been%0Arelatively%20limited%20work%20regarding%20Neural%20Radiance%20Fields%20%28NeRF%29.%20In%20this%20paper%2C%0Awe%20consider%20the%20problem%20of%20rigid%20registration%20between%20two%20NeRFs%20when%20the%0Aposition%20of%20the%20original%20cameras%20is%20not%20given.%20Our%20key%20novelty%20is%20the%0Aintroduction%20of%20Viewshed%20Fields%20%28VF%29%2C%20an%20implicit%20function%20that%20determines%2C%20for%0Aeach%203D%20point%2C%20how%20likely%20it%20is%20to%20be%20viewed%20by%20the%20original%20cameras.%20We%0Ademonstrate%20how%20VF%20can%20help%20in%20the%20various%20stages%20of%20NeRF%20registration%2C%20with%20an%0Aextensive%20evaluation%20showing%20that%20VF-NeRF%20achieves%20SOTA%20results%20on%20various%0Adatasets%20with%20different%20capturing%20approaches%20such%20as%20LLFF%20and%20Objaverese.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03349v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VF-NeRF%3A%20Viewshed%20Fields%20for%20Rigid%20NeRF%20Registration&entry.906535625=Leo%20Segre%20and%20Shai%20Avidan&entry.1292438233=%20%203D%20scene%20registration%20is%20a%20fundamental%20problem%20in%20computer%20vision%20that%20seeks%0Athe%20best%206-DoF%20alignment%20between%20two%20scenes.%20This%20problem%20was%20extensively%0Ainvestigated%20in%20the%20case%20of%20point%20clouds%20and%20meshes%2C%20but%20there%20has%20been%0Arelatively%20limited%20work%20regarding%20Neural%20Radiance%20Fields%20%28NeRF%29.%20In%20this%20paper%2C%0Awe%20consider%20the%20problem%20of%20rigid%20registration%20between%20two%20NeRFs%20when%20the%0Aposition%20of%20the%20original%20cameras%20is%20not%20given.%20Our%20key%20novelty%20is%20the%0Aintroduction%20of%20Viewshed%20Fields%20%28VF%29%2C%20an%20implicit%20function%20that%20determines%2C%20for%0Aeach%203D%20point%2C%20how%20likely%20it%20is%20to%20be%20viewed%20by%20the%20original%20cameras.%20We%0Ademonstrate%20how%20VF%20can%20help%20in%20the%20various%20stages%20of%20NeRF%20registration%2C%20with%20an%0Aextensive%20evaluation%20showing%20that%20VF-NeRF%20achieves%20SOTA%20results%20on%20various%0Adatasets%20with%20different%20capturing%20approaches%20such%20as%20LLFF%20and%20Objaverese.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03349v1&entry.124074799=Read"},
{"title": "LCM-Lookahead for Encoder-based Text-to-Image Personalization", "author": "Rinon Gal and Or Lichter and Elad Richardson and Or Patashnik and Amit H. Bermano and Gal Chechik and Daniel Cohen-Or", "abstract": "  Recent advancements in diffusion models have introduced fast sampling methods\nthat can effectively produce high-quality images in just one or a few denoising\nsteps. Interestingly, when these are distilled from existing diffusion models,\nthey often maintain alignment with the original model, retaining similar\noutputs for similar prompts and seeds. These properties present opportunities\nto leverage fast sampling methods as a shortcut-mechanism, using them to create\na preview of denoised outputs through which we can backpropagate image-space\nlosses. In this work, we explore the potential of using such\nshortcut-mechanisms to guide the personalization of text-to-image models to\nspecific facial identities. We focus on encoder-based personalization\napproaches, and demonstrate that by tuning them with a lookahead identity loss,\nwe can achieve higher identity fidelity, without sacrificing layout diversity\nor prompt alignment. We further explore the use of attention sharing mechanisms\nand consistent data generation for the task of personalization, and find that\nencoder training can benefit from both.\n", "link": "http://arxiv.org/abs/2404.03620v1", "date": "2024-04-04", "relevancy": 2.5136, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6664}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6443}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5973}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LCM-Lookahead%20for%20Encoder-based%20Text-to-Image%20Personalization&body=Title%3A%20LCM-Lookahead%20for%20Encoder-based%20Text-to-Image%20Personalization%0AAuthor%3A%20Rinon%20Gal%20and%20Or%20Lichter%20and%20Elad%20Richardson%20and%20Or%20Patashnik%20and%20Amit%20H.%20Bermano%20and%20Gal%20Chechik%20and%20Daniel%20Cohen-Or%0AAbstract%3A%20%20%20Recent%20advancements%20in%20diffusion%20models%20have%20introduced%20fast%20sampling%20methods%0Athat%20can%20effectively%20produce%20high-quality%20images%20in%20just%20one%20or%20a%20few%20denoising%0Asteps.%20Interestingly%2C%20when%20these%20are%20distilled%20from%20existing%20diffusion%20models%2C%0Athey%20often%20maintain%20alignment%20with%20the%20original%20model%2C%20retaining%20similar%0Aoutputs%20for%20similar%20prompts%20and%20seeds.%20These%20properties%20present%20opportunities%0Ato%20leverage%20fast%20sampling%20methods%20as%20a%20shortcut-mechanism%2C%20using%20them%20to%20create%0Aa%20preview%20of%20denoised%20outputs%20through%20which%20we%20can%20backpropagate%20image-space%0Alosses.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20using%20such%0Ashortcut-mechanisms%20to%20guide%20the%20personalization%20of%20text-to-image%20models%20to%0Aspecific%20facial%20identities.%20We%20focus%20on%20encoder-based%20personalization%0Aapproaches%2C%20and%20demonstrate%20that%20by%20tuning%20them%20with%20a%20lookahead%20identity%20loss%2C%0Awe%20can%20achieve%20higher%20identity%20fidelity%2C%20without%20sacrificing%20layout%20diversity%0Aor%20prompt%20alignment.%20We%20further%20explore%20the%20use%20of%20attention%20sharing%20mechanisms%0Aand%20consistent%20data%20generation%20for%20the%20task%20of%20personalization%2C%20and%20find%20that%0Aencoder%20training%20can%20benefit%20from%20both.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03620v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCM-Lookahead%20for%20Encoder-based%20Text-to-Image%20Personalization&entry.906535625=Rinon%20Gal%20and%20Or%20Lichter%20and%20Elad%20Richardson%20and%20Or%20Patashnik%20and%20Amit%20H.%20Bermano%20and%20Gal%20Chechik%20and%20Daniel%20Cohen-Or&entry.1292438233=%20%20Recent%20advancements%20in%20diffusion%20models%20have%20introduced%20fast%20sampling%20methods%0Athat%20can%20effectively%20produce%20high-quality%20images%20in%20just%20one%20or%20a%20few%20denoising%0Asteps.%20Interestingly%2C%20when%20these%20are%20distilled%20from%20existing%20diffusion%20models%2C%0Athey%20often%20maintain%20alignment%20with%20the%20original%20model%2C%20retaining%20similar%0Aoutputs%20for%20similar%20prompts%20and%20seeds.%20These%20properties%20present%20opportunities%0Ato%20leverage%20fast%20sampling%20methods%20as%20a%20shortcut-mechanism%2C%20using%20them%20to%20create%0Aa%20preview%20of%20denoised%20outputs%20through%20which%20we%20can%20backpropagate%20image-space%0Alosses.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20using%20such%0Ashortcut-mechanisms%20to%20guide%20the%20personalization%20of%20text-to-image%20models%20to%0Aspecific%20facial%20identities.%20We%20focus%20on%20encoder-based%20personalization%0Aapproaches%2C%20and%20demonstrate%20that%20by%20tuning%20them%20with%20a%20lookahead%20identity%20loss%2C%0Awe%20can%20achieve%20higher%20identity%20fidelity%2C%20without%20sacrificing%20layout%20diversity%0Aor%20prompt%20alignment.%20We%20further%20explore%20the%20use%20of%20attention%20sharing%20mechanisms%0Aand%20consistent%20data%20generation%20for%20the%20task%20of%20personalization%2C%20and%20find%20that%0Aencoder%20training%20can%20benefit%20from%20both.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03620v1&entry.124074799=Read"},
{"title": "Truncated Affinity Maximization: One-class Homophily Modeling for Graph\n  Anomaly Detection", "author": "Hezhe Qiao and Guansong Pang", "abstract": "  We reveal a one-class homophily phenomenon, which is one prevalent property\nwe find empirically in real-world graph anomaly detection (GAD) datasets, i.e.,\nnormal nodes tend to have strong connection/affinity with each other, while the\nhomophily in abnormal nodes is significantly weaker than normal nodes. However,\nthis anomaly-discriminative property is ignored by existing GAD methods that\nare typically built using a conventional anomaly detection objective, such as\ndata reconstruction. In this work, we explore this property to introduce a\nnovel unsupervised anomaly scoring measure for GAD, local node affinity, that\nassigns a larger anomaly score to nodes that are less affiliated with their\nneighbors, with the affinity defined as similarity on node\nattributes/representations. We further propose Truncated Affinity Maximization\n(TAM) that learns tailored node representations for our anomaly measure by\nmaximizing the local affinity of nodes to their neighbors. Optimizing on the\noriginal graph structure can be biased by nonhomophily edges (i.e., edges\nconnecting normal and abnormal nodes). Thus, TAM is instead optimized on\ntruncated graphs where non-homophily edges are removed iteratively to mitigate\nthis bias. The learned representations result in significantly stronger local\naffinity for normal nodes than abnormal nodes. Extensive empirical results on\n10 real-world GAD datasets show that TAM substantially outperforms seven\ncompeting models, achieving over 10% increase in AUROC/AUPRC compared to the\nbest contenders on challenging datasets. Our code is available at\nhttps://github.com/mala-lab/TAM-master/.\n", "link": "http://arxiv.org/abs/2306.00006v5", "date": "2024-04-04", "relevancy": 2.4759, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.521}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5068}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4578}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Truncated%20Affinity%20Maximization%3A%20One-class%20Homophily%20Modeling%20for%20Graph%0A%20%20Anomaly%20Detection&body=Title%3A%20Truncated%20Affinity%20Maximization%3A%20One-class%20Homophily%20Modeling%20for%20Graph%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Hezhe%20Qiao%20and%20Guansong%20Pang%0AAbstract%3A%20%20%20We%20reveal%20a%20one-class%20homophily%20phenomenon%2C%20which%20is%20one%20prevalent%20property%0Awe%20find%20empirically%20in%20real-world%20graph%20anomaly%20detection%20%28GAD%29%20datasets%2C%20i.e.%2C%0Anormal%20nodes%20tend%20to%20have%20strong%20connection/affinity%20with%20each%20other%2C%20while%20the%0Ahomophily%20in%20abnormal%20nodes%20is%20significantly%20weaker%20than%20normal%20nodes.%20However%2C%0Athis%20anomaly-discriminative%20property%20is%20ignored%20by%20existing%20GAD%20methods%20that%0Aare%20typically%20built%20using%20a%20conventional%20anomaly%20detection%20objective%2C%20such%20as%0Adata%20reconstruction.%20In%20this%20work%2C%20we%20explore%20this%20property%20to%20introduce%20a%0Anovel%20unsupervised%20anomaly%20scoring%20measure%20for%20GAD%2C%20local%20node%20affinity%2C%20that%0Aassigns%20a%20larger%20anomaly%20score%20to%20nodes%20that%20are%20less%20affiliated%20with%20their%0Aneighbors%2C%20with%20the%20affinity%20defined%20as%20similarity%20on%20node%0Aattributes/representations.%20We%20further%20propose%20Truncated%20Affinity%20Maximization%0A%28TAM%29%20that%20learns%20tailored%20node%20representations%20for%20our%20anomaly%20measure%20by%0Amaximizing%20the%20local%20affinity%20of%20nodes%20to%20their%20neighbors.%20Optimizing%20on%20the%0Aoriginal%20graph%20structure%20can%20be%20biased%20by%20nonhomophily%20edges%20%28i.e.%2C%20edges%0Aconnecting%20normal%20and%20abnormal%20nodes%29.%20Thus%2C%20TAM%20is%20instead%20optimized%20on%0Atruncated%20graphs%20where%20non-homophily%20edges%20are%20removed%20iteratively%20to%20mitigate%0Athis%20bias.%20The%20learned%20representations%20result%20in%20significantly%20stronger%20local%0Aaffinity%20for%20normal%20nodes%20than%20abnormal%20nodes.%20Extensive%20empirical%20results%20on%0A10%20real-world%20GAD%20datasets%20show%20that%20TAM%20substantially%20outperforms%20seven%0Acompeting%20models%2C%20achieving%20over%2010%25%20increase%20in%20AUROC/AUPRC%20compared%20to%20the%0Abest%20contenders%20on%20challenging%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mala-lab/TAM-master/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00006v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truncated%20Affinity%20Maximization%3A%20One-class%20Homophily%20Modeling%20for%20Graph%0A%20%20Anomaly%20Detection&entry.906535625=Hezhe%20Qiao%20and%20Guansong%20Pang&entry.1292438233=%20%20We%20reveal%20a%20one-class%20homophily%20phenomenon%2C%20which%20is%20one%20prevalent%20property%0Awe%20find%20empirically%20in%20real-world%20graph%20anomaly%20detection%20%28GAD%29%20datasets%2C%20i.e.%2C%0Anormal%20nodes%20tend%20to%20have%20strong%20connection/affinity%20with%20each%20other%2C%20while%20the%0Ahomophily%20in%20abnormal%20nodes%20is%20significantly%20weaker%20than%20normal%20nodes.%20However%2C%0Athis%20anomaly-discriminative%20property%20is%20ignored%20by%20existing%20GAD%20methods%20that%0Aare%20typically%20built%20using%20a%20conventional%20anomaly%20detection%20objective%2C%20such%20as%0Adata%20reconstruction.%20In%20this%20work%2C%20we%20explore%20this%20property%20to%20introduce%20a%0Anovel%20unsupervised%20anomaly%20scoring%20measure%20for%20GAD%2C%20local%20node%20affinity%2C%20that%0Aassigns%20a%20larger%20anomaly%20score%20to%20nodes%20that%20are%20less%20affiliated%20with%20their%0Aneighbors%2C%20with%20the%20affinity%20defined%20as%20similarity%20on%20node%0Aattributes/representations.%20We%20further%20propose%20Truncated%20Affinity%20Maximization%0A%28TAM%29%20that%20learns%20tailored%20node%20representations%20for%20our%20anomaly%20measure%20by%0Amaximizing%20the%20local%20affinity%20of%20nodes%20to%20their%20neighbors.%20Optimizing%20on%20the%0Aoriginal%20graph%20structure%20can%20be%20biased%20by%20nonhomophily%20edges%20%28i.e.%2C%20edges%0Aconnecting%20normal%20and%20abnormal%20nodes%29.%20Thus%2C%20TAM%20is%20instead%20optimized%20on%0Atruncated%20graphs%20where%20non-homophily%20edges%20are%20removed%20iteratively%20to%20mitigate%0Athis%20bias.%20The%20learned%20representations%20result%20in%20significantly%20stronger%20local%0Aaffinity%20for%20normal%20nodes%20than%20abnormal%20nodes.%20Extensive%20empirical%20results%20on%0A10%20real-world%20GAD%20datasets%20show%20that%20TAM%20substantially%20outperforms%20seven%0Acompeting%20models%2C%20achieving%20over%2010%25%20increase%20in%20AUROC/AUPRC%20compared%20to%20the%0Abest%20contenders%20on%20challenging%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mala-lab/TAM-master/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00006v5&entry.124074799=Read"},
{"title": "Terrain Point Cloud Inpainting via Signal Decomposition", "author": "Yizhou Xie and Xiangning Xie and Yuran Wang and Yanci Zhang and Zejun Lv", "abstract": "  The rapid development of 3D acquisition technology has made it possible to\nobtain point clouds of real-world terrains. However, due to limitations in\nsensor acquisition technology or specific requirements, point clouds often\ncontain defects such as holes with missing data. Inpainting algorithms are\nwidely used to patch these holes. However, existing traditional inpainting\nalgorithms rely on precise hole boundaries, which limits their ability to\nhandle cases where the boundaries are not well-defined. On the other hand,\nlearning-based completion methods often prioritize reconstructing the entire\npoint cloud instead of solely focusing on hole filling. Based on the fact that\nreal-world terrain exhibits both global smoothness and rich local detail, we\npropose a novel representation for terrain point clouds. This representation\ncan help to repair the holes without clear boundaries. Specifically, it\ndecomposes terrains into low-frequency and high-frequency components, which are\nrepresented by B-spline surfaces and relative height maps respectively. In this\nway, the terrain point cloud inpainting problem is transformed into a B-spline\nsurface fitting and 2D image inpainting problem. By solving the two problems,\nthe highly complex and irregular holes on the terrain point clouds can be\nwell-filled, which not only satisfies the global terrain undulation but also\nexhibits rich geometric details. The experimental results also demonstrate the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2404.03572v1", "date": "2024-04-04", "relevancy": 2.467, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5193}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4843}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4766}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Terrain%20Point%20Cloud%20Inpainting%20via%20Signal%20Decomposition&body=Title%3A%20Terrain%20Point%20Cloud%20Inpainting%20via%20Signal%20Decomposition%0AAuthor%3A%20Yizhou%20Xie%20and%20Xiangning%20Xie%20and%20Yuran%20Wang%20and%20Yanci%20Zhang%20and%20Zejun%20Lv%0AAbstract%3A%20%20%20The%20rapid%20development%20of%203D%20acquisition%20technology%20has%20made%20it%20possible%20to%0Aobtain%20point%20clouds%20of%20real-world%20terrains.%20However%2C%20due%20to%20limitations%20in%0Asensor%20acquisition%20technology%20or%20specific%20requirements%2C%20point%20clouds%20often%0Acontain%20defects%20such%20as%20holes%20with%20missing%20data.%20Inpainting%20algorithms%20are%0Awidely%20used%20to%20patch%20these%20holes.%20However%2C%20existing%20traditional%20inpainting%0Aalgorithms%20rely%20on%20precise%20hole%20boundaries%2C%20which%20limits%20their%20ability%20to%0Ahandle%20cases%20where%20the%20boundaries%20are%20not%20well-defined.%20On%20the%20other%20hand%2C%0Alearning-based%20completion%20methods%20often%20prioritize%20reconstructing%20the%20entire%0Apoint%20cloud%20instead%20of%20solely%20focusing%20on%20hole%20filling.%20Based%20on%20the%20fact%20that%0Areal-world%20terrain%20exhibits%20both%20global%20smoothness%20and%20rich%20local%20detail%2C%20we%0Apropose%20a%20novel%20representation%20for%20terrain%20point%20clouds.%20This%20representation%0Acan%20help%20to%20repair%20the%20holes%20without%20clear%20boundaries.%20Specifically%2C%20it%0Adecomposes%20terrains%20into%20low-frequency%20and%20high-frequency%20components%2C%20which%20are%0Arepresented%20by%20B-spline%20surfaces%20and%20relative%20height%20maps%20respectively.%20In%20this%0Away%2C%20the%20terrain%20point%20cloud%20inpainting%20problem%20is%20transformed%20into%20a%20B-spline%0Asurface%20fitting%20and%202D%20image%20inpainting%20problem.%20By%20solving%20the%20two%20problems%2C%0Athe%20highly%20complex%20and%20irregular%20holes%20on%20the%20terrain%20point%20clouds%20can%20be%0Awell-filled%2C%20which%20not%20only%20satisfies%20the%20global%20terrain%20undulation%20but%20also%0Aexhibits%20rich%20geometric%20details.%20The%20experimental%20results%20also%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03572v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Terrain%20Point%20Cloud%20Inpainting%20via%20Signal%20Decomposition&entry.906535625=Yizhou%20Xie%20and%20Xiangning%20Xie%20and%20Yuran%20Wang%20and%20Yanci%20Zhang%20and%20Zejun%20Lv&entry.1292438233=%20%20The%20rapid%20development%20of%203D%20acquisition%20technology%20has%20made%20it%20possible%20to%0Aobtain%20point%20clouds%20of%20real-world%20terrains.%20However%2C%20due%20to%20limitations%20in%0Asensor%20acquisition%20technology%20or%20specific%20requirements%2C%20point%20clouds%20often%0Acontain%20defects%20such%20as%20holes%20with%20missing%20data.%20Inpainting%20algorithms%20are%0Awidely%20used%20to%20patch%20these%20holes.%20However%2C%20existing%20traditional%20inpainting%0Aalgorithms%20rely%20on%20precise%20hole%20boundaries%2C%20which%20limits%20their%20ability%20to%0Ahandle%20cases%20where%20the%20boundaries%20are%20not%20well-defined.%20On%20the%20other%20hand%2C%0Alearning-based%20completion%20methods%20often%20prioritize%20reconstructing%20the%20entire%0Apoint%20cloud%20instead%20of%20solely%20focusing%20on%20hole%20filling.%20Based%20on%20the%20fact%20that%0Areal-world%20terrain%20exhibits%20both%20global%20smoothness%20and%20rich%20local%20detail%2C%20we%0Apropose%20a%20novel%20representation%20for%20terrain%20point%20clouds.%20This%20representation%0Acan%20help%20to%20repair%20the%20holes%20without%20clear%20boundaries.%20Specifically%2C%20it%0Adecomposes%20terrains%20into%20low-frequency%20and%20high-frequency%20components%2C%20which%20are%0Arepresented%20by%20B-spline%20surfaces%20and%20relative%20height%20maps%20respectively.%20In%20this%0Away%2C%20the%20terrain%20point%20cloud%20inpainting%20problem%20is%20transformed%20into%20a%20B-spline%0Asurface%20fitting%20and%202D%20image%20inpainting%20problem.%20By%20solving%20the%20two%20problems%2C%0Athe%20highly%20complex%20and%20irregular%20holes%20on%20the%20terrain%20point%20clouds%20can%20be%0Awell-filled%2C%20which%20not%20only%20satisfies%20the%20global%20terrain%20undulation%20but%20also%0Aexhibits%20rich%20geometric%20details.%20The%20experimental%20results%20also%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03572v1&entry.124074799=Read"},
{"title": "SemGrasp: Semantic Grasp Generation via Language Aligned Discretization", "author": "Kailin Li and Jingbo Wang and Lixin Yang and Cewu Lu and Bo Dai", "abstract": "  Generating natural human grasps necessitates consideration of not just object\ngeometry but also semantic information. Solely depending on object shape for\ngrasp generation confines the applications of prior methods in downstream\ntasks. This paper presents a novel semantic-based grasp generation method,\ntermed SemGrasp, which generates a static human grasp pose by incorporating\nsemantic information into the grasp representation. We introduce a discrete\nrepresentation that aligns the grasp space with semantic space, enabling the\ngeneration of grasp postures in accordance with language instructions. A\nMultimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating\nobject, grasp, and language within a unified semantic space. To facilitate the\ntraining of SemGrasp, we have compiled a large-scale, grasp-text-aligned\ndataset named CapGrasp, featuring about 260k detailed captions and 50k diverse\ngrasps. Experimental findings demonstrate that SemGrasp efficiently generates\nnatural human grasps in alignment with linguistic intentions. Our code, models,\nand dataset are available publicly at: https://kailinli.github.io/SemGrasp.\n", "link": "http://arxiv.org/abs/2404.03590v1", "date": "2024-04-04", "relevancy": 2.4185, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6724}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SemGrasp%3A%20Semantic%20Grasp%20Generation%20via%20Language%20Aligned%20Discretization&body=Title%3A%20SemGrasp%3A%20Semantic%20Grasp%20Generation%20via%20Language%20Aligned%20Discretization%0AAuthor%3A%20Kailin%20Li%20and%20Jingbo%20Wang%20and%20Lixin%20Yang%20and%20Cewu%20Lu%20and%20Bo%20Dai%0AAbstract%3A%20%20%20Generating%20natural%20human%20grasps%20necessitates%20consideration%20of%20not%20just%20object%0Ageometry%20but%20also%20semantic%20information.%20Solely%20depending%20on%20object%20shape%20for%0Agrasp%20generation%20confines%20the%20applications%20of%20prior%20methods%20in%20downstream%0Atasks.%20This%20paper%20presents%20a%20novel%20semantic-based%20grasp%20generation%20method%2C%0Atermed%20SemGrasp%2C%20which%20generates%20a%20static%20human%20grasp%20pose%20by%20incorporating%0Asemantic%20information%20into%20the%20grasp%20representation.%20We%20introduce%20a%20discrete%0Arepresentation%20that%20aligns%20the%20grasp%20space%20with%20semantic%20space%2C%20enabling%20the%0Ageneration%20of%20grasp%20postures%20in%20accordance%20with%20language%20instructions.%20A%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29%20is%20subsequently%20fine-tuned%2C%20integrating%0Aobject%2C%20grasp%2C%20and%20language%20within%20a%20unified%20semantic%20space.%20To%20facilitate%20the%0Atraining%20of%20SemGrasp%2C%20we%20have%20compiled%20a%20large-scale%2C%20grasp-text-aligned%0Adataset%20named%20CapGrasp%2C%20featuring%20about%20260k%20detailed%20captions%20and%2050k%20diverse%0Agrasps.%20Experimental%20findings%20demonstrate%20that%20SemGrasp%20efficiently%20generates%0Anatural%20human%20grasps%20in%20alignment%20with%20linguistic%20intentions.%20Our%20code%2C%20models%2C%0Aand%20dataset%20are%20available%20publicly%20at%3A%20https%3A//kailinli.github.io/SemGrasp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03590v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemGrasp%3A%20Semantic%20Grasp%20Generation%20via%20Language%20Aligned%20Discretization&entry.906535625=Kailin%20Li%20and%20Jingbo%20Wang%20and%20Lixin%20Yang%20and%20Cewu%20Lu%20and%20Bo%20Dai&entry.1292438233=%20%20Generating%20natural%20human%20grasps%20necessitates%20consideration%20of%20not%20just%20object%0Ageometry%20but%20also%20semantic%20information.%20Solely%20depending%20on%20object%20shape%20for%0Agrasp%20generation%20confines%20the%20applications%20of%20prior%20methods%20in%20downstream%0Atasks.%20This%20paper%20presents%20a%20novel%20semantic-based%20grasp%20generation%20method%2C%0Atermed%20SemGrasp%2C%20which%20generates%20a%20static%20human%20grasp%20pose%20by%20incorporating%0Asemantic%20information%20into%20the%20grasp%20representation.%20We%20introduce%20a%20discrete%0Arepresentation%20that%20aligns%20the%20grasp%20space%20with%20semantic%20space%2C%20enabling%20the%0Ageneration%20of%20grasp%20postures%20in%20accordance%20with%20language%20instructions.%20A%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29%20is%20subsequently%20fine-tuned%2C%20integrating%0Aobject%2C%20grasp%2C%20and%20language%20within%20a%20unified%20semantic%20space.%20To%20facilitate%20the%0Atraining%20of%20SemGrasp%2C%20we%20have%20compiled%20a%20large-scale%2C%20grasp-text-aligned%0Adataset%20named%20CapGrasp%2C%20featuring%20about%20260k%20detailed%20captions%20and%2050k%20diverse%0Agrasps.%20Experimental%20findings%20demonstrate%20that%20SemGrasp%20efficiently%20generates%0Anatural%20human%20grasps%20in%20alignment%20with%20linguistic%20intentions.%20Our%20code%2C%20models%2C%0Aand%20dataset%20are%20available%20publicly%20at%3A%20https%3A//kailinli.github.io/SemGrasp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03590v1&entry.124074799=Read"},
{"title": "Unified Spatio-Temporal Tri-Perspective View Representation for 3D\n  Semantic Occupancy Prediction", "author": "Sathira Silva and Savindu Bhashitha Wannigama and Gihan Jayatilaka and Muhammad Haris Khan and Roshan Ragel", "abstract": "  Holistic understanding and reasoning in 3D scenes play a vital role in the\nsuccess of autonomous driving systems. The evolution of 3D semantic occupancy\nprediction as a pretraining task for autonomous driving and robotic downstream\ntasks capture finer 3D details compared to methods like 3D detection. Existing\napproaches predominantly focus on spatial cues such as tri-perspective view\nembeddings (TPV), often overlooking temporal cues. This study introduces a\nspatiotemporal transformer architecture S2TPVFormer for temporally coherent 3D\nsemantic occupancy prediction. We enrich the prior process by including\ntemporal cues using a novel temporal cross-view hybrid attention mechanism\n(TCVHA) and generate spatiotemporal TPV embeddings (i.e. S2TPV embeddings).\nExperimental evaluations on the nuScenes dataset demonstrate a substantial 4.1%\nimprovement in mean Intersection over Union (mIoU) for 3D Semantic Occupancy\ncompared to TPVFormer, confirming the effectiveness of the proposed S2TPVFormer\nin enhancing 3D scene perception.\n", "link": "http://arxiv.org/abs/2401.13785v2", "date": "2024-04-04", "relevancy": 2.384, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6279}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5919}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5657}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unified%20Spatio-Temporal%20Tri-Perspective%20View%20Representation%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction&body=Title%3A%20Unified%20Spatio-Temporal%20Tri-Perspective%20View%20Representation%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction%0AAuthor%3A%20Sathira%20Silva%20and%20Savindu%20Bhashitha%20Wannigama%20and%20Gihan%20Jayatilaka%20and%20Muhammad%20Haris%20Khan%20and%20Roshan%20Ragel%0AAbstract%3A%20%20%20Holistic%20understanding%20and%20reasoning%20in%203D%20scenes%20play%20a%20vital%20role%20in%20the%0Asuccess%20of%20autonomous%20driving%20systems.%20The%20evolution%20of%203D%20semantic%20occupancy%0Aprediction%20as%20a%20pretraining%20task%20for%20autonomous%20driving%20and%20robotic%20downstream%0Atasks%20capture%20finer%203D%20details%20compared%20to%20methods%20like%203D%20detection.%20Existing%0Aapproaches%20predominantly%20focus%20on%20spatial%20cues%20such%20as%20tri-perspective%20view%0Aembeddings%20%28TPV%29%2C%20often%20overlooking%20temporal%20cues.%20This%20study%20introduces%20a%0Aspatiotemporal%20transformer%20architecture%20S2TPVFormer%20for%20temporally%20coherent%203D%0Asemantic%20occupancy%20prediction.%20We%20enrich%20the%20prior%20process%20by%20including%0Atemporal%20cues%20using%20a%20novel%20temporal%20cross-view%20hybrid%20attention%20mechanism%0A%28TCVHA%29%20and%20generate%20spatiotemporal%20TPV%20embeddings%20%28i.e.%20S2TPV%20embeddings%29.%0AExperimental%20evaluations%20on%20the%20nuScenes%20dataset%20demonstrate%20a%20substantial%204.1%25%0Aimprovement%20in%20mean%20Intersection%20over%20Union%20%28mIoU%29%20for%203D%20Semantic%20Occupancy%0Acompared%20to%20TPVFormer%2C%20confirming%20the%20effectiveness%20of%20the%20proposed%20S2TPVFormer%0Ain%20enhancing%203D%20scene%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13785v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Spatio-Temporal%20Tri-Perspective%20View%20Representation%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction&entry.906535625=Sathira%20Silva%20and%20Savindu%20Bhashitha%20Wannigama%20and%20Gihan%20Jayatilaka%20and%20Muhammad%20Haris%20Khan%20and%20Roshan%20Ragel&entry.1292438233=%20%20Holistic%20understanding%20and%20reasoning%20in%203D%20scenes%20play%20a%20vital%20role%20in%20the%0Asuccess%20of%20autonomous%20driving%20systems.%20The%20evolution%20of%203D%20semantic%20occupancy%0Aprediction%20as%20a%20pretraining%20task%20for%20autonomous%20driving%20and%20robotic%20downstream%0Atasks%20capture%20finer%203D%20details%20compared%20to%20methods%20like%203D%20detection.%20Existing%0Aapproaches%20predominantly%20focus%20on%20spatial%20cues%20such%20as%20tri-perspective%20view%0Aembeddings%20%28TPV%29%2C%20often%20overlooking%20temporal%20cues.%20This%20study%20introduces%20a%0Aspatiotemporal%20transformer%20architecture%20S2TPVFormer%20for%20temporally%20coherent%203D%0Asemantic%20occupancy%20prediction.%20We%20enrich%20the%20prior%20process%20by%20including%0Atemporal%20cues%20using%20a%20novel%20temporal%20cross-view%20hybrid%20attention%20mechanism%0A%28TCVHA%29%20and%20generate%20spatiotemporal%20TPV%20embeddings%20%28i.e.%20S2TPV%20embeddings%29.%0AExperimental%20evaluations%20on%20the%20nuScenes%20dataset%20demonstrate%20a%20substantial%204.1%25%0Aimprovement%20in%20mean%20Intersection%20over%20Union%20%28mIoU%29%20for%203D%20Semantic%20Occupancy%0Acompared%20to%20TPVFormer%2C%20confirming%20the%20effectiveness%20of%20the%20proposed%20S2TPVFormer%0Ain%20enhancing%203D%20scene%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13785v2&entry.124074799=Read"},
{"title": "LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace", "author": "Bin Gao and Yan Yang and Ya-xiang Yuan", "abstract": "  Bilevel optimization, with broad applications in machine learning, has an\nintricate hierarchical structure. Gradient-based methods have emerged as a\ncommon approach to large-scale bilevel problems. However, the computation of\nthe hyper-gradient, which involves a Hessian inverse vector product, confines\nthe efficiency and is regarded as a bottleneck. To circumvent the inverse, we\nconstruct a sequence of low-dimensional approximate Krylov subspaces with the\naid of the Lanczos process. As a result, the constructed subspace is able to\ndynamically and incrementally approximate the Hessian inverse vector product\nwith less effort and thus leads to a favorable estimate of the hyper-gradient.\nMoreover, we propose a~provable subspace-based framework for bilevel problems\nwhere one central step is to solve a small-size tridiagonal linear system. To\nthe best of our knowledge, this is the first time that subspace techniques are\nincorporated into bilevel optimization. This successful trial not only enjoys\n$\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency\nin a synthetic problem and two deep learning tasks.\n", "link": "http://arxiv.org/abs/2404.03331v1", "date": "2024-04-04", "relevancy": 2.3626, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4832}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4701}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4643}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LancBiO%3A%20dynamic%20Lanczos-aided%20bilevel%20optimization%20via%20Krylov%20subspace&body=Title%3A%20LancBiO%3A%20dynamic%20Lanczos-aided%20bilevel%20optimization%20via%20Krylov%20subspace%0AAuthor%3A%20Bin%20Gao%20and%20Yan%20Yang%20and%20Ya-xiang%20Yuan%0AAbstract%3A%20%20%20Bilevel%20optimization%2C%20with%20broad%20applications%20in%20machine%20learning%2C%20has%20an%0Aintricate%20hierarchical%20structure.%20Gradient-based%20methods%20have%20emerged%20as%20a%0Acommon%20approach%20to%20large-scale%20bilevel%20problems.%20However%2C%20the%20computation%20of%0Athe%20hyper-gradient%2C%20which%20involves%20a%20Hessian%20inverse%20vector%20product%2C%20confines%0Athe%20efficiency%20and%20is%20regarded%20as%20a%20bottleneck.%20To%20circumvent%20the%20inverse%2C%20we%0Aconstruct%20a%20sequence%20of%20low-dimensional%20approximate%20Krylov%20subspaces%20with%20the%0Aaid%20of%20the%20Lanczos%20process.%20As%20a%20result%2C%20the%20constructed%20subspace%20is%20able%20to%0Adynamically%20and%20incrementally%20approximate%20the%20Hessian%20inverse%20vector%20product%0Awith%20less%20effort%20and%20thus%20leads%20to%20a%20favorable%20estimate%20of%20the%20hyper-gradient.%0AMoreover%2C%20we%20propose%20a~provable%20subspace-based%20framework%20for%20bilevel%20problems%0Awhere%20one%20central%20step%20is%20to%20solve%20a%20small-size%20tridiagonal%20linear%20system.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%20that%20subspace%20techniques%20are%0Aincorporated%20into%20bilevel%20optimization.%20This%20successful%20trial%20not%20only%20enjoys%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-1%7D%29%24%20convergence%20rate%20but%20also%20demonstrates%20efficiency%0Ain%20a%20synthetic%20problem%20and%20two%20deep%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03331v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LancBiO%3A%20dynamic%20Lanczos-aided%20bilevel%20optimization%20via%20Krylov%20subspace&entry.906535625=Bin%20Gao%20and%20Yan%20Yang%20and%20Ya-xiang%20Yuan&entry.1292438233=%20%20Bilevel%20optimization%2C%20with%20broad%20applications%20in%20machine%20learning%2C%20has%20an%0Aintricate%20hierarchical%20structure.%20Gradient-based%20methods%20have%20emerged%20as%20a%0Acommon%20approach%20to%20large-scale%20bilevel%20problems.%20However%2C%20the%20computation%20of%0Athe%20hyper-gradient%2C%20which%20involves%20a%20Hessian%20inverse%20vector%20product%2C%20confines%0Athe%20efficiency%20and%20is%20regarded%20as%20a%20bottleneck.%20To%20circumvent%20the%20inverse%2C%20we%0Aconstruct%20a%20sequence%20of%20low-dimensional%20approximate%20Krylov%20subspaces%20with%20the%0Aaid%20of%20the%20Lanczos%20process.%20As%20a%20result%2C%20the%20constructed%20subspace%20is%20able%20to%0Adynamically%20and%20incrementally%20approximate%20the%20Hessian%20inverse%20vector%20product%0Awith%20less%20effort%20and%20thus%20leads%20to%20a%20favorable%20estimate%20of%20the%20hyper-gradient.%0AMoreover%2C%20we%20propose%20a~provable%20subspace-based%20framework%20for%20bilevel%20problems%0Awhere%20one%20central%20step%20is%20to%20solve%20a%20small-size%20tridiagonal%20linear%20system.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%20that%20subspace%20techniques%20are%0Aincorporated%20into%20bilevel%20optimization.%20This%20successful%20trial%20not%20only%20enjoys%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-1%7D%29%24%20convergence%20rate%20but%20also%20demonstrates%20efficiency%0Ain%20a%20synthetic%20problem%20and%20two%20deep%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03331v1&entry.124074799=Read"},
{"title": "Approximate Gradient Coding for Privacy-Flexible Federated Learning with\n  Non-IID Data", "author": "Okko Makkonen and Sampo Niemel\u00e4 and Camilla Hollanti and Serge Kas Hanna", "abstract": "  This work focuses on the challenges of non-IID data and stragglers/dropouts\nin federated learning. We introduce and explore a privacy-flexible paradigm\nthat models parts of the clients' local data as non-private, offering a more\nversatile and business-oriented perspective on privacy. Within this framework,\nwe propose a data-driven strategy for mitigating the effects of label\nheterogeneity and client straggling on federated learning. Our solution\ncombines both offline data sharing and approximate gradient coding techniques.\nThrough numerical simulations using the MNIST dataset, we demonstrate that our\napproach enables achieving a deliberate trade-off between privacy and utility,\nleading to improved model convergence and accuracy while using an adaptable\nportion of non-private data.\n", "link": "http://arxiv.org/abs/2404.03524v1", "date": "2024-04-04", "relevancy": 2.3581, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4748}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4704}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4696}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Approximate%20Gradient%20Coding%20for%20Privacy-Flexible%20Federated%20Learning%20with%0A%20%20Non-IID%20Data&body=Title%3A%20Approximate%20Gradient%20Coding%20for%20Privacy-Flexible%20Federated%20Learning%20with%0A%20%20Non-IID%20Data%0AAuthor%3A%20Okko%20Makkonen%20and%20Sampo%20Niemel%C3%A4%20and%20Camilla%20Hollanti%20and%20Serge%20Kas%20Hanna%0AAbstract%3A%20%20%20This%20work%20focuses%20on%20the%20challenges%20of%20non-IID%20data%20and%20stragglers/dropouts%0Ain%20federated%20learning.%20We%20introduce%20and%20explore%20a%20privacy-flexible%20paradigm%0Athat%20models%20parts%20of%20the%20clients%27%20local%20data%20as%20non-private%2C%20offering%20a%20more%0Aversatile%20and%20business-oriented%20perspective%20on%20privacy.%20Within%20this%20framework%2C%0Awe%20propose%20a%20data-driven%20strategy%20for%20mitigating%20the%20effects%20of%20label%0Aheterogeneity%20and%20client%20straggling%20on%20federated%20learning.%20Our%20solution%0Acombines%20both%20offline%20data%20sharing%20and%20approximate%20gradient%20coding%20techniques.%0AThrough%20numerical%20simulations%20using%20the%20MNIST%20dataset%2C%20we%20demonstrate%20that%20our%0Aapproach%20enables%20achieving%20a%20deliberate%20trade-off%20between%20privacy%20and%20utility%2C%0Aleading%20to%20improved%20model%20convergence%20and%20accuracy%20while%20using%20an%20adaptable%0Aportion%20of%20non-private%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03524v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximate%20Gradient%20Coding%20for%20Privacy-Flexible%20Federated%20Learning%20with%0A%20%20Non-IID%20Data&entry.906535625=Okko%20Makkonen%20and%20Sampo%20Niemel%C3%A4%20and%20Camilla%20Hollanti%20and%20Serge%20Kas%20Hanna&entry.1292438233=%20%20This%20work%20focuses%20on%20the%20challenges%20of%20non-IID%20data%20and%20stragglers/dropouts%0Ain%20federated%20learning.%20We%20introduce%20and%20explore%20a%20privacy-flexible%20paradigm%0Athat%20models%20parts%20of%20the%20clients%27%20local%20data%20as%20non-private%2C%20offering%20a%20more%0Aversatile%20and%20business-oriented%20perspective%20on%20privacy.%20Within%20this%20framework%2C%0Awe%20propose%20a%20data-driven%20strategy%20for%20mitigating%20the%20effects%20of%20label%0Aheterogeneity%20and%20client%20straggling%20on%20federated%20learning.%20Our%20solution%0Acombines%20both%20offline%20data%20sharing%20and%20approximate%20gradient%20coding%20techniques.%0AThrough%20numerical%20simulations%20using%20the%20MNIST%20dataset%2C%20we%20demonstrate%20that%20our%0Aapproach%20enables%20achieving%20a%20deliberate%20trade-off%20between%20privacy%20and%20utility%2C%0Aleading%20to%20improved%20model%20convergence%20and%20accuracy%20while%20using%20an%20adaptable%0Aportion%20of%20non-private%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03524v1&entry.124074799=Read"},
{"title": "A replica analysis of Self-Training of Linear Classifier", "author": "Takashi Takahashi", "abstract": "  Self-training (ST) is a simple and standard approach in semi-supervised\nlearning that has been applied to many machine learning problems. Despite its\nwidespread acceptance and practical effectiveness, it is still not well\nunderstood why and how ST improves performance by fitting the model to\npotentially erroneous pseudo-labels. To investigate the properties of ST, in\nthis study, we derive and analyze a sharp characterization of the behavior of\niterative ST when training a linear classifier by minimizing the\nridge-regularized convex loss for binary Gaussian mixtures, in the asymptotic\nlimit where input dimension and data size diverge proportionally. The\nderivation is based on the replica method of statistical mechanics. The result\nindicates that, when the total number of iterations is large, ST may find a\nclassification plane with the optimal direction regardless of the label\nimbalance by accumulating small parameter updates over long iterations. It is\nargued that this is because the small update of ST can accumulate information\nof the data in an almost noiseless way. However, when a label imbalance is\npresent in true labels, the performance of the ST is significantly lower than\nthat of supervised learning with true labels, because the ratio between the\nnorm of the weight and the magnitude of the bias can become significantly\nlarge. To overcome the problems in label imbalanced cases, several heuristics\nare introduced. By numerically analyzing the asymptotic formula, it is\ndemonstrated that with the proposed heuristics, ST can find a classifier whose\nperformance is nearly compatible with supervised learning using true labels\neven in the presence of significant label imbalance.\n", "link": "http://arxiv.org/abs/2205.07739v2", "date": "2024-04-04", "relevancy": 2.3564, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4789}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4746}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4603}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20replica%20analysis%20of%20Self-Training%20of%20Linear%20Classifier&body=Title%3A%20A%20replica%20analysis%20of%20Self-Training%20of%20Linear%20Classifier%0AAuthor%3A%20Takashi%20Takahashi%0AAbstract%3A%20%20%20Self-training%20%28ST%29%20is%20a%20simple%20and%20standard%20approach%20in%20semi-supervised%0Alearning%20that%20has%20been%20applied%20to%20many%20machine%20learning%20problems.%20Despite%20its%0Awidespread%20acceptance%20and%20practical%20effectiveness%2C%20it%20is%20still%20not%20well%0Aunderstood%20why%20and%20how%20ST%20improves%20performance%20by%20fitting%20the%20model%20to%0Apotentially%20erroneous%20pseudo-labels.%20To%20investigate%20the%20properties%20of%20ST%2C%20in%0Athis%20study%2C%20we%20derive%20and%20analyze%20a%20sharp%20characterization%20of%20the%20behavior%20of%0Aiterative%20ST%20when%20training%20a%20linear%20classifier%20by%20minimizing%20the%0Aridge-regularized%20convex%20loss%20for%20binary%20Gaussian%20mixtures%2C%20in%20the%20asymptotic%0Alimit%20where%20input%20dimension%20and%20data%20size%20diverge%20proportionally.%20The%0Aderivation%20is%20based%20on%20the%20replica%20method%20of%20statistical%20mechanics.%20The%20result%0Aindicates%20that%2C%20when%20the%20total%20number%20of%20iterations%20is%20large%2C%20ST%20may%20find%20a%0Aclassification%20plane%20with%20the%20optimal%20direction%20regardless%20of%20the%20label%0Aimbalance%20by%20accumulating%20small%20parameter%20updates%20over%20long%20iterations.%20It%20is%0Aargued%20that%20this%20is%20because%20the%20small%20update%20of%20ST%20can%20accumulate%20information%0Aof%20the%20data%20in%20an%20almost%20noiseless%20way.%20However%2C%20when%20a%20label%20imbalance%20is%0Apresent%20in%20true%20labels%2C%20the%20performance%20of%20the%20ST%20is%20significantly%20lower%20than%0Athat%20of%20supervised%20learning%20with%20true%20labels%2C%20because%20the%20ratio%20between%20the%0Anorm%20of%20the%20weight%20and%20the%20magnitude%20of%20the%20bias%20can%20become%20significantly%0Alarge.%20To%20overcome%20the%20problems%20in%20label%20imbalanced%20cases%2C%20several%20heuristics%0Aare%20introduced.%20By%20numerically%20analyzing%20the%20asymptotic%20formula%2C%20it%20is%0Ademonstrated%20that%20with%20the%20proposed%20heuristics%2C%20ST%20can%20find%20a%20classifier%20whose%0Aperformance%20is%20nearly%20compatible%20with%20supervised%20learning%20using%20true%20labels%0Aeven%20in%20the%20presence%20of%20significant%20label%20imbalance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.07739v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20replica%20analysis%20of%20Self-Training%20of%20Linear%20Classifier&entry.906535625=Takashi%20Takahashi&entry.1292438233=%20%20Self-training%20%28ST%29%20is%20a%20simple%20and%20standard%20approach%20in%20semi-supervised%0Alearning%20that%20has%20been%20applied%20to%20many%20machine%20learning%20problems.%20Despite%20its%0Awidespread%20acceptance%20and%20practical%20effectiveness%2C%20it%20is%20still%20not%20well%0Aunderstood%20why%20and%20how%20ST%20improves%20performance%20by%20fitting%20the%20model%20to%0Apotentially%20erroneous%20pseudo-labels.%20To%20investigate%20the%20properties%20of%20ST%2C%20in%0Athis%20study%2C%20we%20derive%20and%20analyze%20a%20sharp%20characterization%20of%20the%20behavior%20of%0Aiterative%20ST%20when%20training%20a%20linear%20classifier%20by%20minimizing%20the%0Aridge-regularized%20convex%20loss%20for%20binary%20Gaussian%20mixtures%2C%20in%20the%20asymptotic%0Alimit%20where%20input%20dimension%20and%20data%20size%20diverge%20proportionally.%20The%0Aderivation%20is%20based%20on%20the%20replica%20method%20of%20statistical%20mechanics.%20The%20result%0Aindicates%20that%2C%20when%20the%20total%20number%20of%20iterations%20is%20large%2C%20ST%20may%20find%20a%0Aclassification%20plane%20with%20the%20optimal%20direction%20regardless%20of%20the%20label%0Aimbalance%20by%20accumulating%20small%20parameter%20updates%20over%20long%20iterations.%20It%20is%0Aargued%20that%20this%20is%20because%20the%20small%20update%20of%20ST%20can%20accumulate%20information%0Aof%20the%20data%20in%20an%20almost%20noiseless%20way.%20However%2C%20when%20a%20label%20imbalance%20is%0Apresent%20in%20true%20labels%2C%20the%20performance%20of%20the%20ST%20is%20significantly%20lower%20than%0Athat%20of%20supervised%20learning%20with%20true%20labels%2C%20because%20the%20ratio%20between%20the%0Anorm%20of%20the%20weight%20and%20the%20magnitude%20of%20the%20bias%20can%20become%20significantly%0Alarge.%20To%20overcome%20the%20problems%20in%20label%20imbalanced%20cases%2C%20several%20heuristics%0Aare%20introduced.%20By%20numerically%20analyzing%20the%20asymptotic%20formula%2C%20it%20is%0Ademonstrated%20that%20with%20the%20proposed%20heuristics%2C%20ST%20can%20find%20a%20classifier%20whose%0Aperformance%20is%20nearly%20compatible%20with%20supervised%20learning%20using%20true%20labels%0Aeven%20in%20the%20presence%20of%20significant%20label%20imbalance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.07739v2&entry.124074799=Read"},
{"title": "COMO: Compact Mapping and Odometry", "author": "Eric Dexheimer and Andrew J. Davison", "abstract": "  We present COMO, a real-time monocular mapping and odometry system that\nencodes dense geometry via a compact set of 3D anchor points. Decoding anchor\npoint projections into dense geometry via per-keyframe depth covariance\nfunctions guarantees that depth maps are joined together at visible anchor\npoints. The representation enables joint optimization of camera poses and dense\ngeometry, intrinsic 3D consistency, and efficient second-order inference. To\nmaintain a compact yet expressive map, we introduce a frontend that leverages\nthe covariance function for tracking and initializing potentially visually\nindistinct 3D points across frames. Altogether, we introduce a real-time system\ncapable of estimating accurate poses and consistent geometry.\n", "link": "http://arxiv.org/abs/2404.03531v1", "date": "2024-04-04", "relevancy": 2.3513, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6151}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.571}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5617}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20COMO%3A%20Compact%20Mapping%20and%20Odometry&body=Title%3A%20COMO%3A%20Compact%20Mapping%20and%20Odometry%0AAuthor%3A%20Eric%20Dexheimer%20and%20Andrew%20J.%20Davison%0AAbstract%3A%20%20%20We%20present%20COMO%2C%20a%20real-time%20monocular%20mapping%20and%20odometry%20system%20that%0Aencodes%20dense%20geometry%20via%20a%20compact%20set%20of%203D%20anchor%20points.%20Decoding%20anchor%0Apoint%20projections%20into%20dense%20geometry%20via%20per-keyframe%20depth%20covariance%0Afunctions%20guarantees%20that%20depth%20maps%20are%20joined%20together%20at%20visible%20anchor%0Apoints.%20The%20representation%20enables%20joint%20optimization%20of%20camera%20poses%20and%20dense%0Ageometry%2C%20intrinsic%203D%20consistency%2C%20and%20efficient%20second-order%20inference.%20To%0Amaintain%20a%20compact%20yet%20expressive%20map%2C%20we%20introduce%20a%20frontend%20that%20leverages%0Athe%20covariance%20function%20for%20tracking%20and%20initializing%20potentially%20visually%0Aindistinct%203D%20points%20across%20frames.%20Altogether%2C%20we%20introduce%20a%20real-time%20system%0Acapable%20of%20estimating%20accurate%20poses%20and%20consistent%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03531v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMO%3A%20Compact%20Mapping%20and%20Odometry&entry.906535625=Eric%20Dexheimer%20and%20Andrew%20J.%20Davison&entry.1292438233=%20%20We%20present%20COMO%2C%20a%20real-time%20monocular%20mapping%20and%20odometry%20system%20that%0Aencodes%20dense%20geometry%20via%20a%20compact%20set%20of%203D%20anchor%20points.%20Decoding%20anchor%0Apoint%20projections%20into%20dense%20geometry%20via%20per-keyframe%20depth%20covariance%0Afunctions%20guarantees%20that%20depth%20maps%20are%20joined%20together%20at%20visible%20anchor%0Apoints.%20The%20representation%20enables%20joint%20optimization%20of%20camera%20poses%20and%20dense%0Ageometry%2C%20intrinsic%203D%20consistency%2C%20and%20efficient%20second-order%20inference.%20To%0Amaintain%20a%20compact%20yet%20expressive%20map%2C%20we%20introduce%20a%20frontend%20that%20leverages%0Athe%20covariance%20function%20for%20tracking%20and%20initializing%20potentially%20visually%0Aindistinct%203D%20points%20across%20frames.%20Altogether%2C%20we%20introduce%20a%20real-time%20system%0Acapable%20of%20estimating%20accurate%20poses%20and%20consistent%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03531v1&entry.124074799=Read"},
{"title": "The More You See in 2D, the More You Perceive in 3D", "author": "Xinyang Han and Zelin Gao and Angjoo Kanazawa and Shubham Goel and Yossi Gandelsman", "abstract": "  Humans can infer 3D structure from 2D images of an object based on past\nexperience and improve their 3D understanding as they see more images. Inspired\nby this behavior, we introduce SAP3D, a system for 3D reconstruction and novel\nview synthesis from an arbitrary number of unposed images. Given a few unposed\nimages of an object, we adapt a pre-trained view-conditioned diffusion model\ntogether with the camera poses of the images via test-time fine-tuning. The\nadapted diffusion model and the obtained camera poses are then utilized as\ninstance-specific priors for 3D reconstruction and novel view synthesis. We\nshow that as the number of input images increases, the performance of our\napproach improves, bridging the gap between optimization-based prior-less 3D\nreconstruction methods and single-image-to-3D diffusion-based methods. We\ndemonstrate our system on real images as well as standard synthetic benchmarks.\nOur ablation studies confirm that this adaption behavior is key for more\naccurate 3D understanding.\n", "link": "http://arxiv.org/abs/2404.03652v1", "date": "2024-04-04", "relevancy": 2.3301, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6096}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5997}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5486}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20More%20You%20See%20in%202D%2C%20the%20More%20You%20Perceive%20in%203D&body=Title%3A%20The%20More%20You%20See%20in%202D%2C%20the%20More%20You%20Perceive%20in%203D%0AAuthor%3A%20Xinyang%20Han%20and%20Zelin%20Gao%20and%20Angjoo%20Kanazawa%20and%20Shubham%20Goel%20and%20Yossi%20Gandelsman%0AAbstract%3A%20%20%20Humans%20can%20infer%203D%20structure%20from%202D%20images%20of%20an%20object%20based%20on%20past%0Aexperience%20and%20improve%20their%203D%20understanding%20as%20they%20see%20more%20images.%20Inspired%0Aby%20this%20behavior%2C%20we%20introduce%20SAP3D%2C%20a%20system%20for%203D%20reconstruction%20and%20novel%0Aview%20synthesis%20from%20an%20arbitrary%20number%20of%20unposed%20images.%20Given%20a%20few%20unposed%0Aimages%20of%20an%20object%2C%20we%20adapt%20a%20pre-trained%20view-conditioned%20diffusion%20model%0Atogether%20with%20the%20camera%20poses%20of%20the%20images%20via%20test-time%20fine-tuning.%20The%0Aadapted%20diffusion%20model%20and%20the%20obtained%20camera%20poses%20are%20then%20utilized%20as%0Ainstance-specific%20priors%20for%203D%20reconstruction%20and%20novel%20view%20synthesis.%20We%0Ashow%20that%20as%20the%20number%20of%20input%20images%20increases%2C%20the%20performance%20of%20our%0Aapproach%20improves%2C%20bridging%20the%20gap%20between%20optimization-based%20prior-less%203D%0Areconstruction%20methods%20and%20single-image-to-3D%20diffusion-based%20methods.%20We%0Ademonstrate%20our%20system%20on%20real%20images%20as%20well%20as%20standard%20synthetic%20benchmarks.%0AOur%20ablation%20studies%20confirm%20that%20this%20adaption%20behavior%20is%20key%20for%20more%0Aaccurate%203D%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03652v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20More%20You%20See%20in%202D%2C%20the%20More%20You%20Perceive%20in%203D&entry.906535625=Xinyang%20Han%20and%20Zelin%20Gao%20and%20Angjoo%20Kanazawa%20and%20Shubham%20Goel%20and%20Yossi%20Gandelsman&entry.1292438233=%20%20Humans%20can%20infer%203D%20structure%20from%202D%20images%20of%20an%20object%20based%20on%20past%0Aexperience%20and%20improve%20their%203D%20understanding%20as%20they%20see%20more%20images.%20Inspired%0Aby%20this%20behavior%2C%20we%20introduce%20SAP3D%2C%20a%20system%20for%203D%20reconstruction%20and%20novel%0Aview%20synthesis%20from%20an%20arbitrary%20number%20of%20unposed%20images.%20Given%20a%20few%20unposed%0Aimages%20of%20an%20object%2C%20we%20adapt%20a%20pre-trained%20view-conditioned%20diffusion%20model%0Atogether%20with%20the%20camera%20poses%20of%20the%20images%20via%20test-time%20fine-tuning.%20The%0Aadapted%20diffusion%20model%20and%20the%20obtained%20camera%20poses%20are%20then%20utilized%20as%0Ainstance-specific%20priors%20for%203D%20reconstruction%20and%20novel%20view%20synthesis.%20We%0Ashow%20that%20as%20the%20number%20of%20input%20images%20increases%2C%20the%20performance%20of%20our%0Aapproach%20improves%2C%20bridging%20the%20gap%20between%20optimization-based%20prior-less%203D%0Areconstruction%20methods%20and%20single-image-to-3D%20diffusion-based%20methods.%20We%0Ademonstrate%20our%20system%20on%20real%20images%20as%20well%20as%20standard%20synthetic%20benchmarks.%0AOur%20ablation%20studies%20confirm%20that%20this%20adaption%20behavior%20is%20key%20for%20more%0Aaccurate%203D%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03652v1&entry.124074799=Read"},
{"title": "Is CLIP the main roadblock for fine-grained open-world perception?", "author": "Lorenzo Bianchi and Fabio Carrara and Nicola Messina and Fabrizio Falchi", "abstract": "  Modern applications increasingly demand flexible computer vision models that\nadapt to novel concepts not encountered during training. This necessity is\npivotal in emerging domains like extended reality, robotics, and autonomous\ndriving, which require the ability to respond to open-world stimuli. A key\ningredient is the ability to identify objects based on free-form textual\nqueries defined at inference time - a task known as open-vocabulary object\ndetection. Multimodal backbones like CLIP are the main enabling technology for\ncurrent open-world perception solutions. Despite performing well on generic\nqueries, recent studies highlighted limitations on the fine-grained recognition\ncapabilities in open-vocabulary settings - i.e., for distinguishing subtle\nobject features like color, shape, and material. In this paper, we perform a\ndetailed examination of these open-vocabulary object recognition limitations to\nfind the root cause. We evaluate the performance of CLIP, the most commonly\nused vision-language backbone, against a fine-grained object-matching\nbenchmark, revealing interesting analogies between the limitations of\nopen-vocabulary object detectors and their backbones. Experiments suggest that\nthe lack of fine-grained understanding is caused by the poor separability of\nobject characteristics in the CLIP latent space. Therefore, we try to\nunderstand whether fine-grained knowledge is present in CLIP embeddings but not\nexploited at inference time due, for example, to the unsuitability of the\ncosine similarity matching function, which may discard important object\ncharacteristics. Our preliminary experiments show that simple CLIP latent-space\nre-projections help separate fine-grained concepts, paving the way towards the\ndevelopment of backbones inherently able to process fine-grained details. The\ncode for reproducing these experiments is available at\nhttps://github.com/lorebianchi98/FG-CLIP.\n", "link": "http://arxiv.org/abs/2404.03539v1", "date": "2024-04-04", "relevancy": 2.3138, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5918}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5739}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5565}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Is%20CLIP%20the%20main%20roadblock%20for%20fine-grained%20open-world%20perception%3F&body=Title%3A%20Is%20CLIP%20the%20main%20roadblock%20for%20fine-grained%20open-world%20perception%3F%0AAuthor%3A%20Lorenzo%20Bianchi%20and%20Fabio%20Carrara%20and%20Nicola%20Messina%20and%20Fabrizio%20Falchi%0AAbstract%3A%20%20%20Modern%20applications%20increasingly%20demand%20flexible%20computer%20vision%20models%20that%0Aadapt%20to%20novel%20concepts%20not%20encountered%20during%20training.%20This%20necessity%20is%0Apivotal%20in%20emerging%20domains%20like%20extended%20reality%2C%20robotics%2C%20and%20autonomous%0Adriving%2C%20which%20require%20the%20ability%20to%20respond%20to%20open-world%20stimuli.%20A%20key%0Aingredient%20is%20the%20ability%20to%20identify%20objects%20based%20on%20free-form%20textual%0Aqueries%20defined%20at%20inference%20time%20-%20a%20task%20known%20as%20open-vocabulary%20object%0Adetection.%20Multimodal%20backbones%20like%20CLIP%20are%20the%20main%20enabling%20technology%20for%0Acurrent%20open-world%20perception%20solutions.%20Despite%20performing%20well%20on%20generic%0Aqueries%2C%20recent%20studies%20highlighted%20limitations%20on%20the%20fine-grained%20recognition%0Acapabilities%20in%20open-vocabulary%20settings%20-%20i.e.%2C%20for%20distinguishing%20subtle%0Aobject%20features%20like%20color%2C%20shape%2C%20and%20material.%20In%20this%20paper%2C%20we%20perform%20a%0Adetailed%20examination%20of%20these%20open-vocabulary%20object%20recognition%20limitations%20to%0Afind%20the%20root%20cause.%20We%20evaluate%20the%20performance%20of%20CLIP%2C%20the%20most%20commonly%0Aused%20vision-language%20backbone%2C%20against%20a%20fine-grained%20object-matching%0Abenchmark%2C%20revealing%20interesting%20analogies%20between%20the%20limitations%20of%0Aopen-vocabulary%20object%20detectors%20and%20their%20backbones.%20Experiments%20suggest%20that%0Athe%20lack%20of%20fine-grained%20understanding%20is%20caused%20by%20the%20poor%20separability%20of%0Aobject%20characteristics%20in%20the%20CLIP%20latent%20space.%20Therefore%2C%20we%20try%20to%0Aunderstand%20whether%20fine-grained%20knowledge%20is%20present%20in%20CLIP%20embeddings%20but%20not%0Aexploited%20at%20inference%20time%20due%2C%20for%20example%2C%20to%20the%20unsuitability%20of%20the%0Acosine%20similarity%20matching%20function%2C%20which%20may%20discard%20important%20object%0Acharacteristics.%20Our%20preliminary%20experiments%20show%20that%20simple%20CLIP%20latent-space%0Are-projections%20help%20separate%20fine-grained%20concepts%2C%20paving%20the%20way%20towards%20the%0Adevelopment%20of%20backbones%20inherently%20able%20to%20process%20fine-grained%20details.%20The%0Acode%20for%20reproducing%20these%20experiments%20is%20available%20at%0Ahttps%3A//github.com/lorebianchi98/FG-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03539v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20CLIP%20the%20main%20roadblock%20for%20fine-grained%20open-world%20perception%3F&entry.906535625=Lorenzo%20Bianchi%20and%20Fabio%20Carrara%20and%20Nicola%20Messina%20and%20Fabrizio%20Falchi&entry.1292438233=%20%20Modern%20applications%20increasingly%20demand%20flexible%20computer%20vision%20models%20that%0Aadapt%20to%20novel%20concepts%20not%20encountered%20during%20training.%20This%20necessity%20is%0Apivotal%20in%20emerging%20domains%20like%20extended%20reality%2C%20robotics%2C%20and%20autonomous%0Adriving%2C%20which%20require%20the%20ability%20to%20respond%20to%20open-world%20stimuli.%20A%20key%0Aingredient%20is%20the%20ability%20to%20identify%20objects%20based%20on%20free-form%20textual%0Aqueries%20defined%20at%20inference%20time%20-%20a%20task%20known%20as%20open-vocabulary%20object%0Adetection.%20Multimodal%20backbones%20like%20CLIP%20are%20the%20main%20enabling%20technology%20for%0Acurrent%20open-world%20perception%20solutions.%20Despite%20performing%20well%20on%20generic%0Aqueries%2C%20recent%20studies%20highlighted%20limitations%20on%20the%20fine-grained%20recognition%0Acapabilities%20in%20open-vocabulary%20settings%20-%20i.e.%2C%20for%20distinguishing%20subtle%0Aobject%20features%20like%20color%2C%20shape%2C%20and%20material.%20In%20this%20paper%2C%20we%20perform%20a%0Adetailed%20examination%20of%20these%20open-vocabulary%20object%20recognition%20limitations%20to%0Afind%20the%20root%20cause.%20We%20evaluate%20the%20performance%20of%20CLIP%2C%20the%20most%20commonly%0Aused%20vision-language%20backbone%2C%20against%20a%20fine-grained%20object-matching%0Abenchmark%2C%20revealing%20interesting%20analogies%20between%20the%20limitations%20of%0Aopen-vocabulary%20object%20detectors%20and%20their%20backbones.%20Experiments%20suggest%20that%0Athe%20lack%20of%20fine-grained%20understanding%20is%20caused%20by%20the%20poor%20separability%20of%0Aobject%20characteristics%20in%20the%20CLIP%20latent%20space.%20Therefore%2C%20we%20try%20to%0Aunderstand%20whether%20fine-grained%20knowledge%20is%20present%20in%20CLIP%20embeddings%20but%20not%0Aexploited%20at%20inference%20time%20due%2C%20for%20example%2C%20to%20the%20unsuitability%20of%20the%0Acosine%20similarity%20matching%20function%2C%20which%20may%20discard%20important%20object%0Acharacteristics.%20Our%20preliminary%20experiments%20show%20that%20simple%20CLIP%20latent-space%0Are-projections%20help%20separate%20fine-grained%20concepts%2C%20paving%20the%20way%20towards%20the%0Adevelopment%20of%20backbones%20inherently%20able%20to%20process%20fine-grained%20details.%20The%0Acode%20for%20reproducing%20these%20experiments%20is%20available%20at%0Ahttps%3A//github.com/lorebianchi98/FG-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03539v1&entry.124074799=Read"},
{"title": "A Comprehensive Survey on Self-Supervised Learning for Recommendation", "author": "Xubin Ren and Wei Wei and Lianghao Xia and Chao Huang", "abstract": "  Recommender systems play a crucial role in tackling the challenge of\ninformation overload by delivering personalized recommendations based on\nindividual user preferences. Deep learning techniques, such as RNNs, GNNs, and\nTransformer architectures, have significantly propelled the advancement of\nrecommender systems by enhancing their comprehension of user behaviors and\npreferences. However, supervised learning methods encounter challenges in\nreal-life scenarios due to data sparsity, resulting in limitations in their\nability to learn representations effectively. To address this, self-supervised\nlearning (SSL) techniques have emerged as a solution, leveraging inherent data\nstructures to generate supervision signals without relying solely on labeled\ndata. By leveraging unlabeled data and extracting meaningful representations,\nrecommender systems utilizing SSL can make accurate predictions and\nrecommendations even when confronted with data sparsity. In this paper, we\nprovide a comprehensive review of self-supervised learning frameworks designed\nfor recommender systems, encompassing a thorough analysis of over 170 papers.\nWe conduct an exploration of nine distinct scenarios, enabling a comprehensive\nunderstanding of SSL-enhanced recommenders in different contexts. For each\ndomain, we elaborate on different self-supervised learning paradigms, namely\ncontrastive learning, generative learning, and adversarial learning, so as to\npresent technical details of how SSL enhances recommender systems in various\ncontexts. We consistently maintain the related open-source materials at\nhttps://github.com/HKUDS/Awesome-SSLRec-Papers.\n", "link": "http://arxiv.org/abs/2404.03354v1", "date": "2024-04-04", "relevancy": 2.3045, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5167}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4402}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4258}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20on%20Self-Supervised%20Learning%20for%20Recommendation&body=Title%3A%20A%20Comprehensive%20Survey%20on%20Self-Supervised%20Learning%20for%20Recommendation%0AAuthor%3A%20Xubin%20Ren%20and%20Wei%20Wei%20and%20Lianghao%20Xia%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Recommender%20systems%20play%20a%20crucial%20role%20in%20tackling%20the%20challenge%20of%0Ainformation%20overload%20by%20delivering%20personalized%20recommendations%20based%20on%0Aindividual%20user%20preferences.%20Deep%20learning%20techniques%2C%20such%20as%20RNNs%2C%20GNNs%2C%20and%0ATransformer%20architectures%2C%20have%20significantly%20propelled%20the%20advancement%20of%0Arecommender%20systems%20by%20enhancing%20their%20comprehension%20of%20user%20behaviors%20and%0Apreferences.%20However%2C%20supervised%20learning%20methods%20encounter%20challenges%20in%0Areal-life%20scenarios%20due%20to%20data%20sparsity%2C%20resulting%20in%20limitations%20in%20their%0Aability%20to%20learn%20representations%20effectively.%20To%20address%20this%2C%20self-supervised%0Alearning%20%28SSL%29%20techniques%20have%20emerged%20as%20a%20solution%2C%20leveraging%20inherent%20data%0Astructures%20to%20generate%20supervision%20signals%20without%20relying%20solely%20on%20labeled%0Adata.%20By%20leveraging%20unlabeled%20data%20and%20extracting%20meaningful%20representations%2C%0Arecommender%20systems%20utilizing%20SSL%20can%20make%20accurate%20predictions%20and%0Arecommendations%20even%20when%20confronted%20with%20data%20sparsity.%20In%20this%20paper%2C%20we%0Aprovide%20a%20comprehensive%20review%20of%20self-supervised%20learning%20frameworks%20designed%0Afor%20recommender%20systems%2C%20encompassing%20a%20thorough%20analysis%20of%20over%20170%20papers.%0AWe%20conduct%20an%20exploration%20of%20nine%20distinct%20scenarios%2C%20enabling%20a%20comprehensive%0Aunderstanding%20of%20SSL-enhanced%20recommenders%20in%20different%20contexts.%20For%20each%0Adomain%2C%20we%20elaborate%20on%20different%20self-supervised%20learning%20paradigms%2C%20namely%0Acontrastive%20learning%2C%20generative%20learning%2C%20and%20adversarial%20learning%2C%20so%20as%20to%0Apresent%20technical%20details%20of%20how%20SSL%20enhances%20recommender%20systems%20in%20various%0Acontexts.%20We%20consistently%20maintain%20the%20related%20open-source%20materials%20at%0Ahttps%3A//github.com/HKUDS/Awesome-SSLRec-Papers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03354v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20on%20Self-Supervised%20Learning%20for%20Recommendation&entry.906535625=Xubin%20Ren%20and%20Wei%20Wei%20and%20Lianghao%20Xia%20and%20Chao%20Huang&entry.1292438233=%20%20Recommender%20systems%20play%20a%20crucial%20role%20in%20tackling%20the%20challenge%20of%0Ainformation%20overload%20by%20delivering%20personalized%20recommendations%20based%20on%0Aindividual%20user%20preferences.%20Deep%20learning%20techniques%2C%20such%20as%20RNNs%2C%20GNNs%2C%20and%0ATransformer%20architectures%2C%20have%20significantly%20propelled%20the%20advancement%20of%0Arecommender%20systems%20by%20enhancing%20their%20comprehension%20of%20user%20behaviors%20and%0Apreferences.%20However%2C%20supervised%20learning%20methods%20encounter%20challenges%20in%0Areal-life%20scenarios%20due%20to%20data%20sparsity%2C%20resulting%20in%20limitations%20in%20their%0Aability%20to%20learn%20representations%20effectively.%20To%20address%20this%2C%20self-supervised%0Alearning%20%28SSL%29%20techniques%20have%20emerged%20as%20a%20solution%2C%20leveraging%20inherent%20data%0Astructures%20to%20generate%20supervision%20signals%20without%20relying%20solely%20on%20labeled%0Adata.%20By%20leveraging%20unlabeled%20data%20and%20extracting%20meaningful%20representations%2C%0Arecommender%20systems%20utilizing%20SSL%20can%20make%20accurate%20predictions%20and%0Arecommendations%20even%20when%20confronted%20with%20data%20sparsity.%20In%20this%20paper%2C%20we%0Aprovide%20a%20comprehensive%20review%20of%20self-supervised%20learning%20frameworks%20designed%0Afor%20recommender%20systems%2C%20encompassing%20a%20thorough%20analysis%20of%20over%20170%20papers.%0AWe%20conduct%20an%20exploration%20of%20nine%20distinct%20scenarios%2C%20enabling%20a%20comprehensive%0Aunderstanding%20of%20SSL-enhanced%20recommenders%20in%20different%20contexts.%20For%20each%0Adomain%2C%20we%20elaborate%20on%20different%20self-supervised%20learning%20paradigms%2C%20namely%0Acontrastive%20learning%2C%20generative%20learning%2C%20and%20adversarial%20learning%2C%20so%20as%20to%0Apresent%20technical%20details%20of%20how%20SSL%20enhances%20recommender%20systems%20in%20various%0Acontexts.%20We%20consistently%20maintain%20the%20related%20open-source%20materials%20at%0Ahttps%3A//github.com/HKUDS/Awesome-SSLRec-Papers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03354v1&entry.124074799=Read"},
{"title": "MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with\n  Interleaved Visual-Textual Tokens", "author": "Kirolos Ataallah and Xiaoqian Shen and Eslam Abdelrahman and Essam Sleiman and Deyao Zhu and Jian Ding and Mohamed Elhoseiny", "abstract": "  This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM)\ndesigned specifically for video understanding. The model is capable of\nprocessing both temporal visual and textual data, making it adept at\nunderstanding the complexities of videos. Building upon the success of\nMiniGPT-v2, which excelled in translating visual features into the LLM space\nfor single images and achieved impressive results on various image-text\nbenchmarks, this paper extends the model's capabilities to process a sequence\nof frames, enabling it to comprehend videos. MiniGPT4-video does not only\nconsider visual content but also incorporates textual conversations, allowing\nthe model to effectively answer queries involving both visual and text\ncomponents. The proposed model outperforms existing state-of-the-art methods,\nregistering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF,\nand TVQA benchmarks respectively. Our models and code have been made publicly\navailable here https://vision-cair.github.io/MiniGPT4-video/\n", "link": "http://arxiv.org/abs/2404.03413v1", "date": "2024-04-04", "relevancy": 2.2996, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5641}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MiniGPT4-Video%3A%20Advancing%20Multimodal%20LLMs%20for%20Video%20Understanding%20with%0A%20%20Interleaved%20Visual-Textual%20Tokens&body=Title%3A%20MiniGPT4-Video%3A%20Advancing%20Multimodal%20LLMs%20for%20Video%20Understanding%20with%0A%20%20Interleaved%20Visual-Textual%20Tokens%0AAuthor%3A%20Kirolos%20Ataallah%20and%20Xiaoqian%20Shen%20and%20Eslam%20Abdelrahman%20and%20Essam%20Sleiman%20and%20Deyao%20Zhu%20and%20Jian%20Ding%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20This%20paper%20introduces%20MiniGPT4-Video%2C%20a%20multimodal%20Large%20Language%20Model%20%28LLM%29%0Adesigned%20specifically%20for%20video%20understanding.%20The%20model%20is%20capable%20of%0Aprocessing%20both%20temporal%20visual%20and%20textual%20data%2C%20making%20it%20adept%20at%0Aunderstanding%20the%20complexities%20of%20videos.%20Building%20upon%20the%20success%20of%0AMiniGPT-v2%2C%20which%20excelled%20in%20translating%20visual%20features%20into%20the%20LLM%20space%0Afor%20single%20images%20and%20achieved%20impressive%20results%20on%20various%20image-text%0Abenchmarks%2C%20this%20paper%20extends%20the%20model%27s%20capabilities%20to%20process%20a%20sequence%0Aof%20frames%2C%20enabling%20it%20to%20comprehend%20videos.%20MiniGPT4-video%20does%20not%20only%0Aconsider%20visual%20content%20but%20also%20incorporates%20textual%20conversations%2C%20allowing%0Athe%20model%20to%20effectively%20answer%20queries%20involving%20both%20visual%20and%20text%0Acomponents.%20The%20proposed%20model%20outperforms%20existing%20state-of-the-art%20methods%2C%0Aregistering%20gains%20of%204.22%25%2C%201.13%25%2C%2020.82%25%2C%20and%2013.1%25%20on%20the%20MSVD%2C%20MSRVTT%2C%20TGIF%2C%0Aand%20TVQA%20benchmarks%20respectively.%20Our%20models%20and%20code%20have%20been%20made%20publicly%0Aavailable%20here%20https%3A//vision-cair.github.io/MiniGPT4-video/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03413v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniGPT4-Video%3A%20Advancing%20Multimodal%20LLMs%20for%20Video%20Understanding%20with%0A%20%20Interleaved%20Visual-Textual%20Tokens&entry.906535625=Kirolos%20Ataallah%20and%20Xiaoqian%20Shen%20and%20Eslam%20Abdelrahman%20and%20Essam%20Sleiman%20and%20Deyao%20Zhu%20and%20Jian%20Ding%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20This%20paper%20introduces%20MiniGPT4-Video%2C%20a%20multimodal%20Large%20Language%20Model%20%28LLM%29%0Adesigned%20specifically%20for%20video%20understanding.%20The%20model%20is%20capable%20of%0Aprocessing%20both%20temporal%20visual%20and%20textual%20data%2C%20making%20it%20adept%20at%0Aunderstanding%20the%20complexities%20of%20videos.%20Building%20upon%20the%20success%20of%0AMiniGPT-v2%2C%20which%20excelled%20in%20translating%20visual%20features%20into%20the%20LLM%20space%0Afor%20single%20images%20and%20achieved%20impressive%20results%20on%20various%20image-text%0Abenchmarks%2C%20this%20paper%20extends%20the%20model%27s%20capabilities%20to%20process%20a%20sequence%0Aof%20frames%2C%20enabling%20it%20to%20comprehend%20videos.%20MiniGPT4-video%20does%20not%20only%0Aconsider%20visual%20content%20but%20also%20incorporates%20textual%20conversations%2C%20allowing%0Athe%20model%20to%20effectively%20answer%20queries%20involving%20both%20visual%20and%20text%0Acomponents.%20The%20proposed%20model%20outperforms%20existing%20state-of-the-art%20methods%2C%0Aregistering%20gains%20of%204.22%25%2C%201.13%25%2C%2020.82%25%2C%20and%2013.1%25%20on%20the%20MSVD%2C%20MSRVTT%2C%20TGIF%2C%0Aand%20TVQA%20benchmarks%20respectively.%20Our%20models%20and%20code%20have%20been%20made%20publicly%0Aavailable%20here%20https%3A//vision-cair.github.io/MiniGPT4-video/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03413v1&entry.124074799=Read"},
{"title": "MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation", "author": "Hanzhe Hu and Zhizhuo Zhou and Varun Jampani and Shubham Tulsiani", "abstract": "  We present MVD-Fusion: a method for single-view 3D inference via generative\nmodeling of multi-view-consistent RGB-D images. While recent methods pursuing\n3D inference advocate learning novel-view generative models, these generations\nare not 3D-consistent and require a distillation process to generate a 3D\noutput. We instead cast the task of 3D inference as directly generating\nmutually-consistent multiple views and build on the insight that additionally\ninferring depth can provide a mechanism for enforcing this consistency.\nSpecifically, we train a denoising diffusion model to generate multi-view RGB-D\nimages given a single RGB input image and leverage the (intermediate noisy)\ndepth estimates to obtain reprojection-based conditioning to maintain\nmulti-view consistency. We train our model using large-scale synthetic dataset\nObajverse as well as the real-world CO3D dataset comprising of generic camera\nviewpoints. We demonstrate that our approach can yield more accurate synthesis\ncompared to recent state-of-the-art, including distillation-based 3D inference\nand prior multi-view generation methods. We also evaluate the geometry induced\nby our multi-view depth prediction and find that it yields a more accurate\nrepresentation than other direct 3D inference approaches.\n", "link": "http://arxiv.org/abs/2404.03656v1", "date": "2024-04-04", "relevancy": 2.2911, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5806}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5702}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5594}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MVD-Fusion%3A%20Single-view%203D%20via%20Depth-consistent%20Multi-view%20Generation&body=Title%3A%20MVD-Fusion%3A%20Single-view%203D%20via%20Depth-consistent%20Multi-view%20Generation%0AAuthor%3A%20Hanzhe%20Hu%20and%20Zhizhuo%20Zhou%20and%20Varun%20Jampani%20and%20Shubham%20Tulsiani%0AAbstract%3A%20%20%20We%20present%20MVD-Fusion%3A%20a%20method%20for%20single-view%203D%20inference%20via%20generative%0Amodeling%20of%20multi-view-consistent%20RGB-D%20images.%20While%20recent%20methods%20pursuing%0A3D%20inference%20advocate%20learning%20novel-view%20generative%20models%2C%20these%20generations%0Aare%20not%203D-consistent%20and%20require%20a%20distillation%20process%20to%20generate%20a%203D%0Aoutput.%20We%20instead%20cast%20the%20task%20of%203D%20inference%20as%20directly%20generating%0Amutually-consistent%20multiple%20views%20and%20build%20on%20the%20insight%20that%20additionally%0Ainferring%20depth%20can%20provide%20a%20mechanism%20for%20enforcing%20this%20consistency.%0ASpecifically%2C%20we%20train%20a%20denoising%20diffusion%20model%20to%20generate%20multi-view%20RGB-D%0Aimages%20given%20a%20single%20RGB%20input%20image%20and%20leverage%20the%20%28intermediate%20noisy%29%0Adepth%20estimates%20to%20obtain%20reprojection-based%20conditioning%20to%20maintain%0Amulti-view%20consistency.%20We%20train%20our%20model%20using%20large-scale%20synthetic%20dataset%0AObajverse%20as%20well%20as%20the%20real-world%20CO3D%20dataset%20comprising%20of%20generic%20camera%0Aviewpoints.%20We%20demonstrate%20that%20our%20approach%20can%20yield%20more%20accurate%20synthesis%0Acompared%20to%20recent%20state-of-the-art%2C%20including%20distillation-based%203D%20inference%0Aand%20prior%20multi-view%20generation%20methods.%20We%20also%20evaluate%20the%20geometry%20induced%0Aby%20our%20multi-view%20depth%20prediction%20and%20find%20that%20it%20yields%20a%20more%20accurate%0Arepresentation%20than%20other%20direct%203D%20inference%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03656v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVD-Fusion%3A%20Single-view%203D%20via%20Depth-consistent%20Multi-view%20Generation&entry.906535625=Hanzhe%20Hu%20and%20Zhizhuo%20Zhou%20and%20Varun%20Jampani%20and%20Shubham%20Tulsiani&entry.1292438233=%20%20We%20present%20MVD-Fusion%3A%20a%20method%20for%20single-view%203D%20inference%20via%20generative%0Amodeling%20of%20multi-view-consistent%20RGB-D%20images.%20While%20recent%20methods%20pursuing%0A3D%20inference%20advocate%20learning%20novel-view%20generative%20models%2C%20these%20generations%0Aare%20not%203D-consistent%20and%20require%20a%20distillation%20process%20to%20generate%20a%203D%0Aoutput.%20We%20instead%20cast%20the%20task%20of%203D%20inference%20as%20directly%20generating%0Amutually-consistent%20multiple%20views%20and%20build%20on%20the%20insight%20that%20additionally%0Ainferring%20depth%20can%20provide%20a%20mechanism%20for%20enforcing%20this%20consistency.%0ASpecifically%2C%20we%20train%20a%20denoising%20diffusion%20model%20to%20generate%20multi-view%20RGB-D%0Aimages%20given%20a%20single%20RGB%20input%20image%20and%20leverage%20the%20%28intermediate%20noisy%29%0Adepth%20estimates%20to%20obtain%20reprojection-based%20conditioning%20to%20maintain%0Amulti-view%20consistency.%20We%20train%20our%20model%20using%20large-scale%20synthetic%20dataset%0AObajverse%20as%20well%20as%20the%20real-world%20CO3D%20dataset%20comprising%20of%20generic%20camera%0Aviewpoints.%20We%20demonstrate%20that%20our%20approach%20can%20yield%20more%20accurate%20synthesis%0Acompared%20to%20recent%20state-of-the-art%2C%20including%20distillation-based%203D%20inference%0Aand%20prior%20multi-view%20generation%20methods.%20We%20also%20evaluate%20the%20geometry%20induced%0Aby%20our%20multi-view%20depth%20prediction%20and%20find%20that%20it%20yields%20a%20more%20accurate%0Arepresentation%20than%20other%20direct%203D%20inference%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03656v1&entry.124074799=Read"},
{"title": "RaFE: Generative Radiance Fields Restoration", "author": "Zhongkai Wu and Ziyu Wan and Jing Zhang and Jing Liao and Dong Xu", "abstract": "  NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel\nview synthesis and 3D reconstruction, but its performance is sensitive to input\nimage quality, which struggles to achieve high-fidelity rendering when provided\nwith low-quality sparse input viewpoints. Previous methods for NeRF restoration\nare tailored for specific degradation type, ignoring the generality of\nrestoration. To overcome this limitation, we propose a generic radiance fields\nrestoration pipeline, named RaFE, which applies to various types of\ndegradations, such as low resolution, blurriness, noise, compression artifacts,\nor their combinations. Our approach leverages the success of off-the-shelf 2D\nrestoration methods to recover the multi-view images individually. Instead of\nreconstructing a blurred NeRF by averaging inconsistencies, we introduce a\nnovel approach using Generative Adversarial Networks (GANs) for NeRF generation\nto better accommodate the geometric and appearance inconsistencies present in\nthe multi-view images. Specifically, we adopt a two-level tri-plane\narchitecture, where the coarse level remains fixed to represent the low-quality\nNeRF, and a fine-level residual tri-plane to be added to the coarse level is\nmodeled as a distribution with GAN to capture potential variations in\nrestoration. We validate RaFE on both synthetic and real cases for various\nrestoration tasks, demonstrating superior performance in both quantitative and\nqualitative evaluations, surpassing other 3D restoration methods specific to\nsingle task. Please see our project website\nhttps://zkaiwu.github.io/RaFE-Project/.\n", "link": "http://arxiv.org/abs/2404.03654v1", "date": "2024-04-04", "relevancy": 2.2511, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6138}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5554}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5147}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RaFE%3A%20Generative%20Radiance%20Fields%20Restoration&body=Title%3A%20RaFE%3A%20Generative%20Radiance%20Fields%20Restoration%0AAuthor%3A%20Zhongkai%20Wu%20and%20Ziyu%20Wan%20and%20Jing%20Zhang%20and%20Jing%20Liao%20and%20Dong%20Xu%0AAbstract%3A%20%20%20NeRF%20%28Neural%20Radiance%20Fields%29%20has%20demonstrated%20tremendous%20potential%20in%20novel%0Aview%20synthesis%20and%203D%20reconstruction%2C%20but%20its%20performance%20is%20sensitive%20to%20input%0Aimage%20quality%2C%20which%20struggles%20to%20achieve%20high-fidelity%20rendering%20when%20provided%0Awith%20low-quality%20sparse%20input%20viewpoints.%20Previous%20methods%20for%20NeRF%20restoration%0Aare%20tailored%20for%20specific%20degradation%20type%2C%20ignoring%20the%20generality%20of%0Arestoration.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20generic%20radiance%20fields%0Arestoration%20pipeline%2C%20named%20RaFE%2C%20which%20applies%20to%20various%20types%20of%0Adegradations%2C%20such%20as%20low%20resolution%2C%20blurriness%2C%20noise%2C%20compression%20artifacts%2C%0Aor%20their%20combinations.%20Our%20approach%20leverages%20the%20success%20of%20off-the-shelf%202D%0Arestoration%20methods%20to%20recover%20the%20multi-view%20images%20individually.%20Instead%20of%0Areconstructing%20a%20blurred%20NeRF%20by%20averaging%20inconsistencies%2C%20we%20introduce%20a%0Anovel%20approach%20using%20Generative%20Adversarial%20Networks%20%28GANs%29%20for%20NeRF%20generation%0Ato%20better%20accommodate%20the%20geometric%20and%20appearance%20inconsistencies%20present%20in%0Athe%20multi-view%20images.%20Specifically%2C%20we%20adopt%20a%20two-level%20tri-plane%0Aarchitecture%2C%20where%20the%20coarse%20level%20remains%20fixed%20to%20represent%20the%20low-quality%0ANeRF%2C%20and%20a%20fine-level%20residual%20tri-plane%20to%20be%20added%20to%20the%20coarse%20level%20is%0Amodeled%20as%20a%20distribution%20with%20GAN%20to%20capture%20potential%20variations%20in%0Arestoration.%20We%20validate%20RaFE%20on%20both%20synthetic%20and%20real%20cases%20for%20various%0Arestoration%20tasks%2C%20demonstrating%20superior%20performance%20in%20both%20quantitative%20and%0Aqualitative%20evaluations%2C%20surpassing%20other%203D%20restoration%20methods%20specific%20to%0Asingle%20task.%20Please%20see%20our%20project%20website%0Ahttps%3A//zkaiwu.github.io/RaFE-Project/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03654v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaFE%3A%20Generative%20Radiance%20Fields%20Restoration&entry.906535625=Zhongkai%20Wu%20and%20Ziyu%20Wan%20and%20Jing%20Zhang%20and%20Jing%20Liao%20and%20Dong%20Xu&entry.1292438233=%20%20NeRF%20%28Neural%20Radiance%20Fields%29%20has%20demonstrated%20tremendous%20potential%20in%20novel%0Aview%20synthesis%20and%203D%20reconstruction%2C%20but%20its%20performance%20is%20sensitive%20to%20input%0Aimage%20quality%2C%20which%20struggles%20to%20achieve%20high-fidelity%20rendering%20when%20provided%0Awith%20low-quality%20sparse%20input%20viewpoints.%20Previous%20methods%20for%20NeRF%20restoration%0Aare%20tailored%20for%20specific%20degradation%20type%2C%20ignoring%20the%20generality%20of%0Arestoration.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20generic%20radiance%20fields%0Arestoration%20pipeline%2C%20named%20RaFE%2C%20which%20applies%20to%20various%20types%20of%0Adegradations%2C%20such%20as%20low%20resolution%2C%20blurriness%2C%20noise%2C%20compression%20artifacts%2C%0Aor%20their%20combinations.%20Our%20approach%20leverages%20the%20success%20of%20off-the-shelf%202D%0Arestoration%20methods%20to%20recover%20the%20multi-view%20images%20individually.%20Instead%20of%0Areconstructing%20a%20blurred%20NeRF%20by%20averaging%20inconsistencies%2C%20we%20introduce%20a%0Anovel%20approach%20using%20Generative%20Adversarial%20Networks%20%28GANs%29%20for%20NeRF%20generation%0Ato%20better%20accommodate%20the%20geometric%20and%20appearance%20inconsistencies%20present%20in%0Athe%20multi-view%20images.%20Specifically%2C%20we%20adopt%20a%20two-level%20tri-plane%0Aarchitecture%2C%20where%20the%20coarse%20level%20remains%20fixed%20to%20represent%20the%20low-quality%0ANeRF%2C%20and%20a%20fine-level%20residual%20tri-plane%20to%20be%20added%20to%20the%20coarse%20level%20is%0Amodeled%20as%20a%20distribution%20with%20GAN%20to%20capture%20potential%20variations%20in%0Arestoration.%20We%20validate%20RaFE%20on%20both%20synthetic%20and%20real%20cases%20for%20various%0Arestoration%20tasks%2C%20demonstrating%20superior%20performance%20in%20both%20quantitative%20and%0Aqualitative%20evaluations%2C%20surpassing%20other%203D%20restoration%20methods%20specific%20to%0Asingle%20task.%20Please%20see%20our%20project%20website%0Ahttps%3A//zkaiwu.github.io/RaFE-Project/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03654v1&entry.124074799=Read"},
{"title": "Know Your Neighbors: Improving Single-View Reconstruction via Spatial\n  Vision-Language Reasoning", "author": "Rui Li and Tobias Fischer and Mattia Segu and Marc Pollefeys and Luc Van Gool and Federico Tombari", "abstract": "  Recovering the 3D scene geometry from a single view is a fundamental yet\nill-posed problem in computer vision. While classical depth estimation methods\ninfer only a 2.5D scene representation limited to the image plane, recent\napproaches based on radiance fields reconstruct a full 3D representation.\nHowever, these methods still struggle with occluded regions since inferring\ngeometry without visual observation requires (i) semantic knowledge of the\nsurroundings, and (ii) reasoning about spatial context. We propose KYN, a novel\nmethod for single-view scene reconstruction that reasons about semantic and\nspatial context to predict each point's density. We introduce a vision-language\nmodulation module to enrich point features with fine-grained semantic\ninformation. We aggregate point representations across the scene through a\nlanguage-guided spatial attention mechanism to yield per-point density\npredictions aware of the 3D semantic context. We show that KYN improves 3D\nshape recovery compared to predicting density for each 3D point in isolation.\nWe achieve state-of-the-art results in scene and object reconstruction on\nKITTI-360, and show improved zero-shot generalization compared to prior work.\nProject page: https://ruili3.github.io/kyn.\n", "link": "http://arxiv.org/abs/2404.03658v1", "date": "2024-04-04", "relevancy": 2.2477, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5656}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5639}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5574}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Know%20Your%20Neighbors%3A%20Improving%20Single-View%20Reconstruction%20via%20Spatial%0A%20%20Vision-Language%20Reasoning&body=Title%3A%20Know%20Your%20Neighbors%3A%20Improving%20Single-View%20Reconstruction%20via%20Spatial%0A%20%20Vision-Language%20Reasoning%0AAuthor%3A%20Rui%20Li%20and%20Tobias%20Fischer%20and%20Mattia%20Segu%20and%20Marc%20Pollefeys%20and%20Luc%20Van%20Gool%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Recovering%20the%203D%20scene%20geometry%20from%20a%20single%20view%20is%20a%20fundamental%20yet%0Aill-posed%20problem%20in%20computer%20vision.%20While%20classical%20depth%20estimation%20methods%0Ainfer%20only%20a%202.5D%20scene%20representation%20limited%20to%20the%20image%20plane%2C%20recent%0Aapproaches%20based%20on%20radiance%20fields%20reconstruct%20a%20full%203D%20representation.%0AHowever%2C%20these%20methods%20still%20struggle%20with%20occluded%20regions%20since%20inferring%0Ageometry%20without%20visual%20observation%20requires%20%28i%29%20semantic%20knowledge%20of%20the%0Asurroundings%2C%20and%20%28ii%29%20reasoning%20about%20spatial%20context.%20We%20propose%20KYN%2C%20a%20novel%0Amethod%20for%20single-view%20scene%20reconstruction%20that%20reasons%20about%20semantic%20and%0Aspatial%20context%20to%20predict%20each%20point%27s%20density.%20We%20introduce%20a%20vision-language%0Amodulation%20module%20to%20enrich%20point%20features%20with%20fine-grained%20semantic%0Ainformation.%20We%20aggregate%20point%20representations%20across%20the%20scene%20through%20a%0Alanguage-guided%20spatial%20attention%20mechanism%20to%20yield%20per-point%20density%0Apredictions%20aware%20of%20the%203D%20semantic%20context.%20We%20show%20that%20KYN%20improves%203D%0Ashape%20recovery%20compared%20to%20predicting%20density%20for%20each%203D%20point%20in%20isolation.%0AWe%20achieve%20state-of-the-art%20results%20in%20scene%20and%20object%20reconstruction%20on%0AKITTI-360%2C%20and%20show%20improved%20zero-shot%20generalization%20compared%20to%20prior%20work.%0AProject%20page%3A%20https%3A//ruili3.github.io/kyn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03658v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Know%20Your%20Neighbors%3A%20Improving%20Single-View%20Reconstruction%20via%20Spatial%0A%20%20Vision-Language%20Reasoning&entry.906535625=Rui%20Li%20and%20Tobias%20Fischer%20and%20Mattia%20Segu%20and%20Marc%20Pollefeys%20and%20Luc%20Van%20Gool%20and%20Federico%20Tombari&entry.1292438233=%20%20Recovering%20the%203D%20scene%20geometry%20from%20a%20single%20view%20is%20a%20fundamental%20yet%0Aill-posed%20problem%20in%20computer%20vision.%20While%20classical%20depth%20estimation%20methods%0Ainfer%20only%20a%202.5D%20scene%20representation%20limited%20to%20the%20image%20plane%2C%20recent%0Aapproaches%20based%20on%20radiance%20fields%20reconstruct%20a%20full%203D%20representation.%0AHowever%2C%20these%20methods%20still%20struggle%20with%20occluded%20regions%20since%20inferring%0Ageometry%20without%20visual%20observation%20requires%20%28i%29%20semantic%20knowledge%20of%20the%0Asurroundings%2C%20and%20%28ii%29%20reasoning%20about%20spatial%20context.%20We%20propose%20KYN%2C%20a%20novel%0Amethod%20for%20single-view%20scene%20reconstruction%20that%20reasons%20about%20semantic%20and%0Aspatial%20context%20to%20predict%20each%20point%27s%20density.%20We%20introduce%20a%20vision-language%0Amodulation%20module%20to%20enrich%20point%20features%20with%20fine-grained%20semantic%0Ainformation.%20We%20aggregate%20point%20representations%20across%20the%20scene%20through%20a%0Alanguage-guided%20spatial%20attention%20mechanism%20to%20yield%20per-point%20density%0Apredictions%20aware%20of%20the%203D%20semantic%20context.%20We%20show%20that%20KYN%20improves%203D%0Ashape%20recovery%20compared%20to%20predicting%20density%20for%20each%203D%20point%20in%20isolation.%0AWe%20achieve%20state-of-the-art%20results%20in%20scene%20and%20object%20reconstruction%20on%0AKITTI-360%2C%20and%20show%20improved%20zero-shot%20generalization%20compared%20to%20prior%20work.%0AProject%20page%3A%20https%3A//ruili3.github.io/kyn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03658v1&entry.124074799=Read"},
{"title": "Generalization Bounds for Message Passing Networks on Mixture of\n  Graphons", "author": "Sohir Maskey and Gitta Kutyniok and Ron Levie", "abstract": "  We study the generalization capabilities of Message Passing Neural Networks\n(MPNNs), a prevalent class of Graph Neural Networks (GNN). We derive\ngeneralization bounds specifically for MPNNs with normalized sum aggregation\nand mean aggregation. Our analysis is based on a data generation model\nincorporating a finite set of template graphons. Each graph within this\nframework is generated by sampling from one of the graphons with a certain\ndegree of perturbation. In particular, we extend previous MPNN generalization\nresults to a more realistic setting, which includes the following\nmodifications: 1) we analyze simple random graphs with Bernoulli-distributed\nedges instead of weighted graphs; 2) we sample both graphs and graph signals\nfrom perturbed graphons instead of clean graphons; and 3) we analyze sparse\ngraphs instead of dense graphs. In this more realistic and challenging\nscenario, we provide a generalization bound that decreases as the average\nnumber of nodes in the graphs increases. Our results imply that MPNNs with\nhigher complexity than the size of the training set can still generalize\neffectively, as long as the graphs are sufficiently large.\n", "link": "http://arxiv.org/abs/2404.03473v1", "date": "2024-04-04", "relevancy": 2.2159, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.464}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4399}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4257}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalization%20Bounds%20for%20Message%20Passing%20Networks%20on%20Mixture%20of%0A%20%20Graphons&body=Title%3A%20Generalization%20Bounds%20for%20Message%20Passing%20Networks%20on%20Mixture%20of%0A%20%20Graphons%0AAuthor%3A%20Sohir%20Maskey%20and%20Gitta%20Kutyniok%20and%20Ron%20Levie%0AAbstract%3A%20%20%20We%20study%20the%20generalization%20capabilities%20of%20Message%20Passing%20Neural%20Networks%0A%28MPNNs%29%2C%20a%20prevalent%20class%20of%20Graph%20Neural%20Networks%20%28GNN%29.%20We%20derive%0Ageneralization%20bounds%20specifically%20for%20MPNNs%20with%20normalized%20sum%20aggregation%0Aand%20mean%20aggregation.%20Our%20analysis%20is%20based%20on%20a%20data%20generation%20model%0Aincorporating%20a%20finite%20set%20of%20template%20graphons.%20Each%20graph%20within%20this%0Aframework%20is%20generated%20by%20sampling%20from%20one%20of%20the%20graphons%20with%20a%20certain%0Adegree%20of%20perturbation.%20In%20particular%2C%20we%20extend%20previous%20MPNN%20generalization%0Aresults%20to%20a%20more%20realistic%20setting%2C%20which%20includes%20the%20following%0Amodifications%3A%201%29%20we%20analyze%20simple%20random%20graphs%20with%20Bernoulli-distributed%0Aedges%20instead%20of%20weighted%20graphs%3B%202%29%20we%20sample%20both%20graphs%20and%20graph%20signals%0Afrom%20perturbed%20graphons%20instead%20of%20clean%20graphons%3B%20and%203%29%20we%20analyze%20sparse%0Agraphs%20instead%20of%20dense%20graphs.%20In%20this%20more%20realistic%20and%20challenging%0Ascenario%2C%20we%20provide%20a%20generalization%20bound%20that%20decreases%20as%20the%20average%0Anumber%20of%20nodes%20in%20the%20graphs%20increases.%20Our%20results%20imply%20that%20MPNNs%20with%0Ahigher%20complexity%20than%20the%20size%20of%20the%20training%20set%20can%20still%20generalize%0Aeffectively%2C%20as%20long%20as%20the%20graphs%20are%20sufficiently%20large.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03473v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Bounds%20for%20Message%20Passing%20Networks%20on%20Mixture%20of%0A%20%20Graphons&entry.906535625=Sohir%20Maskey%20and%20Gitta%20Kutyniok%20and%20Ron%20Levie&entry.1292438233=%20%20We%20study%20the%20generalization%20capabilities%20of%20Message%20Passing%20Neural%20Networks%0A%28MPNNs%29%2C%20a%20prevalent%20class%20of%20Graph%20Neural%20Networks%20%28GNN%29.%20We%20derive%0Ageneralization%20bounds%20specifically%20for%20MPNNs%20with%20normalized%20sum%20aggregation%0Aand%20mean%20aggregation.%20Our%20analysis%20is%20based%20on%20a%20data%20generation%20model%0Aincorporating%20a%20finite%20set%20of%20template%20graphons.%20Each%20graph%20within%20this%0Aframework%20is%20generated%20by%20sampling%20from%20one%20of%20the%20graphons%20with%20a%20certain%0Adegree%20of%20perturbation.%20In%20particular%2C%20we%20extend%20previous%20MPNN%20generalization%0Aresults%20to%20a%20more%20realistic%20setting%2C%20which%20includes%20the%20following%0Amodifications%3A%201%29%20we%20analyze%20simple%20random%20graphs%20with%20Bernoulli-distributed%0Aedges%20instead%20of%20weighted%20graphs%3B%202%29%20we%20sample%20both%20graphs%20and%20graph%20signals%0Afrom%20perturbed%20graphons%20instead%20of%20clean%20graphons%3B%20and%203%29%20we%20analyze%20sparse%0Agraphs%20instead%20of%20dense%20graphs.%20In%20this%20more%20realistic%20and%20challenging%0Ascenario%2C%20we%20provide%20a%20generalization%20bound%20that%20decreases%20as%20the%20average%0Anumber%20of%20nodes%20in%20the%20graphs%20increases.%20Our%20results%20imply%20that%20MPNNs%20with%0Ahigher%20complexity%20than%20the%20size%20of%20the%20training%20set%20can%20still%20generalize%0Aeffectively%2C%20as%20long%20as%20the%20graphs%20are%20sufficiently%20large.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03473v1&entry.124074799=Read"},
{"title": "DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image\n  Enhancement", "author": "Shangquan Sun and Wenqi Ren and Jingyang Peng and Fenglong Song and Xiaochun Cao", "abstract": "  Many existing methods for low-light image enhancement (LLIE) based on Retinex\ntheory ignore important factors that affect the validity of this theory in\ndigital imaging, such as noise, quantization error, non-linearity, and dynamic\nrange overflow. In this paper, we propose a new expression called\nDigital-Imaging Retinex theory (DI-Retinex) through theoretical and\nexperimental analysis of Retinex theory in digital imaging. Our new expression\nincludes an offset term in the enhancement model, which allows for pixel-wise\nbrightness contrast adjustment with a non-linear mapping function. In addition,\nto solve the lowlight enhancement problem in an unsupervised manner, we propose\nan image-adaptive masked reverse degradation loss in Gamma space. We also\ndesign a variance suppression loss for regulating the additional offset term.\nExtensive experiments show that our proposed method outperforms all existing\nunsupervised methods in terms of visual quality, model size, and speed. Our\nalgorithm can also assist downstream face detectors in low-light, as it shows\nthe most performance gain after the low-light enhancement compared to other\nmethods.\n", "link": "http://arxiv.org/abs/2404.03327v1", "date": "2024-04-04", "relevancy": 2.2155, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5719}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5581}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5425}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DI-Retinex%3A%20Digital-Imaging%20Retinex%20Theory%20for%20Low-Light%20Image%0A%20%20Enhancement&body=Title%3A%20DI-Retinex%3A%20Digital-Imaging%20Retinex%20Theory%20for%20Low-Light%20Image%0A%20%20Enhancement%0AAuthor%3A%20Shangquan%20Sun%20and%20Wenqi%20Ren%20and%20Jingyang%20Peng%20and%20Fenglong%20Song%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20Many%20existing%20methods%20for%20low-light%20image%20enhancement%20%28LLIE%29%20based%20on%20Retinex%0Atheory%20ignore%20important%20factors%20that%20affect%20the%20validity%20of%20this%20theory%20in%0Adigital%20imaging%2C%20such%20as%20noise%2C%20quantization%20error%2C%20non-linearity%2C%20and%20dynamic%0Arange%20overflow.%20In%20this%20paper%2C%20we%20propose%20a%20new%20expression%20called%0ADigital-Imaging%20Retinex%20theory%20%28DI-Retinex%29%20through%20theoretical%20and%0Aexperimental%20analysis%20of%20Retinex%20theory%20in%20digital%20imaging.%20Our%20new%20expression%0Aincludes%20an%20offset%20term%20in%20the%20enhancement%20model%2C%20which%20allows%20for%20pixel-wise%0Abrightness%20contrast%20adjustment%20with%20a%20non-linear%20mapping%20function.%20In%20addition%2C%0Ato%20solve%20the%20lowlight%20enhancement%20problem%20in%20an%20unsupervised%20manner%2C%20we%20propose%0Aan%20image-adaptive%20masked%20reverse%20degradation%20loss%20in%20Gamma%20space.%20We%20also%0Adesign%20a%20variance%20suppression%20loss%20for%20regulating%20the%20additional%20offset%20term.%0AExtensive%20experiments%20show%20that%20our%20proposed%20method%20outperforms%20all%20existing%0Aunsupervised%20methods%20in%20terms%20of%20visual%20quality%2C%20model%20size%2C%20and%20speed.%20Our%0Aalgorithm%20can%20also%20assist%20downstream%20face%20detectors%20in%20low-light%2C%20as%20it%20shows%0Athe%20most%20performance%20gain%20after%20the%20low-light%20enhancement%20compared%20to%20other%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03327v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DI-Retinex%3A%20Digital-Imaging%20Retinex%20Theory%20for%20Low-Light%20Image%0A%20%20Enhancement&entry.906535625=Shangquan%20Sun%20and%20Wenqi%20Ren%20and%20Jingyang%20Peng%20and%20Fenglong%20Song%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Many%20existing%20methods%20for%20low-light%20image%20enhancement%20%28LLIE%29%20based%20on%20Retinex%0Atheory%20ignore%20important%20factors%20that%20affect%20the%20validity%20of%20this%20theory%20in%0Adigital%20imaging%2C%20such%20as%20noise%2C%20quantization%20error%2C%20non-linearity%2C%20and%20dynamic%0Arange%20overflow.%20In%20this%20paper%2C%20we%20propose%20a%20new%20expression%20called%0ADigital-Imaging%20Retinex%20theory%20%28DI-Retinex%29%20through%20theoretical%20and%0Aexperimental%20analysis%20of%20Retinex%20theory%20in%20digital%20imaging.%20Our%20new%20expression%0Aincludes%20an%20offset%20term%20in%20the%20enhancement%20model%2C%20which%20allows%20for%20pixel-wise%0Abrightness%20contrast%20adjustment%20with%20a%20non-linear%20mapping%20function.%20In%20addition%2C%0Ato%20solve%20the%20lowlight%20enhancement%20problem%20in%20an%20unsupervised%20manner%2C%20we%20propose%0Aan%20image-adaptive%20masked%20reverse%20degradation%20loss%20in%20Gamma%20space.%20We%20also%0Adesign%20a%20variance%20suppression%20loss%20for%20regulating%20the%20additional%20offset%20term.%0AExtensive%20experiments%20show%20that%20our%20proposed%20method%20outperforms%20all%20existing%0Aunsupervised%20methods%20in%20terms%20of%20visual%20quality%2C%20model%20size%2C%20and%20speed.%20Our%0Aalgorithm%20can%20also%20assist%20downstream%20face%20detectors%20in%20low-light%2C%20as%20it%20shows%0Athe%20most%20performance%20gain%20after%20the%20low-light%20enhancement%20compared%20to%20other%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03327v1&entry.124074799=Read"},
{"title": "MorpheuS: Neural Dynamic 360\u00b0 Surface Reconstruction from Monocular\n  RGB-D Video", "author": "Hengyi Wang and Jingwen Wang and Lourdes Agapito", "abstract": "  Neural rendering has demonstrated remarkable success in dynamic scene\nreconstruction. Thanks to the expressiveness of neural representations, prior\nworks can accurately capture the motion and achieve high-fidelity\nreconstruction of the target object. Despite this, real-world video scenarios\noften feature large unobserved regions where neural representations struggle to\nachieve realistic completion. To tackle this challenge, we introduce MorpheuS,\na framework for dynamic 360{\\deg} surface reconstruction from a casually\ncaptured RGB-D video. Our approach models the target scene as a canonical field\nthat encodes its geometry and appearance, in conjunction with a deformation\nfield that warps points from the current frame to the canonical space. We\nleverage a view-dependent diffusion prior and distill knowledge from it to\nachieve realistic completion of unobserved regions. Experimental results on\nvarious real-world and synthetic datasets show that our method can achieve\nhigh-fidelity 360{\\deg} surface reconstruction of a deformable object from a\nmonocular RGB-D video.\n", "link": "http://arxiv.org/abs/2312.00778v2", "date": "2024-04-04", "relevancy": 2.2153, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5669}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5537}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5487}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MorpheuS%3A%20Neural%20Dynamic%20360%C2%B0%20Surface%20Reconstruction%20from%20Monocular%0A%20%20RGB-D%20Video&body=Title%3A%20MorpheuS%3A%20Neural%20Dynamic%20360%C2%B0%20Surface%20Reconstruction%20from%20Monocular%0A%20%20RGB-D%20Video%0AAuthor%3A%20Hengyi%20Wang%20and%20Jingwen%20Wang%20and%20Lourdes%20Agapito%0AAbstract%3A%20%20%20Neural%20rendering%20has%20demonstrated%20remarkable%20success%20in%20dynamic%20scene%0Areconstruction.%20Thanks%20to%20the%20expressiveness%20of%20neural%20representations%2C%20prior%0Aworks%20can%20accurately%20capture%20the%20motion%20and%20achieve%20high-fidelity%0Areconstruction%20of%20the%20target%20object.%20Despite%20this%2C%20real-world%20video%20scenarios%0Aoften%20feature%20large%20unobserved%20regions%20where%20neural%20representations%20struggle%20to%0Aachieve%20realistic%20completion.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20MorpheuS%2C%0Aa%20framework%20for%20dynamic%20360%7B%5Cdeg%7D%20surface%20reconstruction%20from%20a%20casually%0Acaptured%20RGB-D%20video.%20Our%20approach%20models%20the%20target%20scene%20as%20a%20canonical%20field%0Athat%20encodes%20its%20geometry%20and%20appearance%2C%20in%20conjunction%20with%20a%20deformation%0Afield%20that%20warps%20points%20from%20the%20current%20frame%20to%20the%20canonical%20space.%20We%0Aleverage%20a%20view-dependent%20diffusion%20prior%20and%20distill%20knowledge%20from%20it%20to%0Aachieve%20realistic%20completion%20of%20unobserved%20regions.%20Experimental%20results%20on%0Avarious%20real-world%20and%20synthetic%20datasets%20show%20that%20our%20method%20can%20achieve%0Ahigh-fidelity%20360%7B%5Cdeg%7D%20surface%20reconstruction%20of%20a%20deformable%20object%20from%20a%0Amonocular%20RGB-D%20video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00778v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MorpheuS%3A%20Neural%20Dynamic%20360%C2%B0%20Surface%20Reconstruction%20from%20Monocular%0A%20%20RGB-D%20Video&entry.906535625=Hengyi%20Wang%20and%20Jingwen%20Wang%20and%20Lourdes%20Agapito&entry.1292438233=%20%20Neural%20rendering%20has%20demonstrated%20remarkable%20success%20in%20dynamic%20scene%0Areconstruction.%20Thanks%20to%20the%20expressiveness%20of%20neural%20representations%2C%20prior%0Aworks%20can%20accurately%20capture%20the%20motion%20and%20achieve%20high-fidelity%0Areconstruction%20of%20the%20target%20object.%20Despite%20this%2C%20real-world%20video%20scenarios%0Aoften%20feature%20large%20unobserved%20regions%20where%20neural%20representations%20struggle%20to%0Aachieve%20realistic%20completion.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20MorpheuS%2C%0Aa%20framework%20for%20dynamic%20360%7B%5Cdeg%7D%20surface%20reconstruction%20from%20a%20casually%0Acaptured%20RGB-D%20video.%20Our%20approach%20models%20the%20target%20scene%20as%20a%20canonical%20field%0Athat%20encodes%20its%20geometry%20and%20appearance%2C%20in%20conjunction%20with%20a%20deformation%0Afield%20that%20warps%20points%20from%20the%20current%20frame%20to%20the%20canonical%20space.%20We%0Aleverage%20a%20view-dependent%20diffusion%20prior%20and%20distill%20knowledge%20from%20it%20to%0Aachieve%20realistic%20completion%20of%20unobserved%20regions.%20Experimental%20results%20on%0Avarious%20real-world%20and%20synthetic%20datasets%20show%20that%20our%20method%20can%20achieve%0Ahigh-fidelity%20360%7B%5Cdeg%7D%20surface%20reconstruction%20of%20a%20deformable%20object%20from%20a%0Amonocular%20RGB-D%20video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00778v2&entry.124074799=Read"},
{"title": "LongVLM: Efficient Long Video Understanding via Large Language Models", "author": "Yuetian Weng and Mingfei Han and Haoyu He and Xiaojun Chang and Bohan Zhuang", "abstract": "  Empowered by Large Language Models (LLMs), recent advancements in VideoLLMs\nhave driven progress in various video understanding tasks. These models encode\nvideo representations through pooling or query aggregation over a vast number\nof visual tokens, making computational and memory costs affordable. Despite\nsuccessfully providing an overall comprehension of video content, existing\nVideoLLMs still face challenges in achieving detailed understanding in videos\ndue to overlooking local information in long-term videos. To tackle this\nchallenge, we introduce LongVLM, a straightforward yet powerful VideoLLM for\nlong video understanding, building upon the observation that long videos often\nconsist of sequential key events, complex actions, and camera movements. Our\napproach proposes to decompose long videos into multiple short-term segments\nand encode local features for each local segment via a hierarchical token\nmerging module. These features are concatenated in temporal order to maintain\nthe storyline across sequential short-term segments. Additionally, we propose\nto integrate global semantics into each local feature to enhance context\nunderstanding. In this way, we encode video representations that incorporate\nboth local and global information, enabling the LLM to generate comprehensive\nresponses for long-term videos. Experimental results on the VideoChatGPT\nbenchmark and zero-shot video question-answering datasets demonstrate the\nsuperior capabilities of our model over the previous state-of-the-art methods.\nQualitative examples demonstrate that our model produces more precise responses\nfor long videos understanding. Code is available at\n\\url{https://github.com/ziplab/LongVLM}.\n", "link": "http://arxiv.org/abs/2404.03384v1", "date": "2024-04-04", "relevancy": 2.2068, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5598}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5489}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5384}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LongVLM%3A%20Efficient%20Long%20Video%20Understanding%20via%20Large%20Language%20Models&body=Title%3A%20LongVLM%3A%20Efficient%20Long%20Video%20Understanding%20via%20Large%20Language%20Models%0AAuthor%3A%20Yuetian%20Weng%20and%20Mingfei%20Han%20and%20Haoyu%20He%20and%20Xiaojun%20Chang%20and%20Bohan%20Zhuang%0AAbstract%3A%20%20%20Empowered%20by%20Large%20Language%20Models%20%28LLMs%29%2C%20recent%20advancements%20in%20VideoLLMs%0Ahave%20driven%20progress%20in%20various%20video%20understanding%20tasks.%20These%20models%20encode%0Avideo%20representations%20through%20pooling%20or%20query%20aggregation%20over%20a%20vast%20number%0Aof%20visual%20tokens%2C%20making%20computational%20and%20memory%20costs%20affordable.%20Despite%0Asuccessfully%20providing%20an%20overall%20comprehension%20of%20video%20content%2C%20existing%0AVideoLLMs%20still%20face%20challenges%20in%20achieving%20detailed%20understanding%20in%20videos%0Adue%20to%20overlooking%20local%20information%20in%20long-term%20videos.%20To%20tackle%20this%0Achallenge%2C%20we%20introduce%20LongVLM%2C%20a%20straightforward%20yet%20powerful%20VideoLLM%20for%0Along%20video%20understanding%2C%20building%20upon%20the%20observation%20that%20long%20videos%20often%0Aconsist%20of%20sequential%20key%20events%2C%20complex%20actions%2C%20and%20camera%20movements.%20Our%0Aapproach%20proposes%20to%20decompose%20long%20videos%20into%20multiple%20short-term%20segments%0Aand%20encode%20local%20features%20for%20each%20local%20segment%20via%20a%20hierarchical%20token%0Amerging%20module.%20These%20features%20are%20concatenated%20in%20temporal%20order%20to%20maintain%0Athe%20storyline%20across%20sequential%20short-term%20segments.%20Additionally%2C%20we%20propose%0Ato%20integrate%20global%20semantics%20into%20each%20local%20feature%20to%20enhance%20context%0Aunderstanding.%20In%20this%20way%2C%20we%20encode%20video%20representations%20that%20incorporate%0Aboth%20local%20and%20global%20information%2C%20enabling%20the%20LLM%20to%20generate%20comprehensive%0Aresponses%20for%20long-term%20videos.%20Experimental%20results%20on%20the%20VideoChatGPT%0Abenchmark%20and%20zero-shot%20video%20question-answering%20datasets%20demonstrate%20the%0Asuperior%20capabilities%20of%20our%20model%20over%20the%20previous%20state-of-the-art%20methods.%0AQualitative%20examples%20demonstrate%20that%20our%20model%20produces%20more%20precise%20responses%0Afor%20long%20videos%20understanding.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ziplab/LongVLM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03384v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVLM%3A%20Efficient%20Long%20Video%20Understanding%20via%20Large%20Language%20Models&entry.906535625=Yuetian%20Weng%20and%20Mingfei%20Han%20and%20Haoyu%20He%20and%20Xiaojun%20Chang%20and%20Bohan%20Zhuang&entry.1292438233=%20%20Empowered%20by%20Large%20Language%20Models%20%28LLMs%29%2C%20recent%20advancements%20in%20VideoLLMs%0Ahave%20driven%20progress%20in%20various%20video%20understanding%20tasks.%20These%20models%20encode%0Avideo%20representations%20through%20pooling%20or%20query%20aggregation%20over%20a%20vast%20number%0Aof%20visual%20tokens%2C%20making%20computational%20and%20memory%20costs%20affordable.%20Despite%0Asuccessfully%20providing%20an%20overall%20comprehension%20of%20video%20content%2C%20existing%0AVideoLLMs%20still%20face%20challenges%20in%20achieving%20detailed%20understanding%20in%20videos%0Adue%20to%20overlooking%20local%20information%20in%20long-term%20videos.%20To%20tackle%20this%0Achallenge%2C%20we%20introduce%20LongVLM%2C%20a%20straightforward%20yet%20powerful%20VideoLLM%20for%0Along%20video%20understanding%2C%20building%20upon%20the%20observation%20that%20long%20videos%20often%0Aconsist%20of%20sequential%20key%20events%2C%20complex%20actions%2C%20and%20camera%20movements.%20Our%0Aapproach%20proposes%20to%20decompose%20long%20videos%20into%20multiple%20short-term%20segments%0Aand%20encode%20local%20features%20for%20each%20local%20segment%20via%20a%20hierarchical%20token%0Amerging%20module.%20These%20features%20are%20concatenated%20in%20temporal%20order%20to%20maintain%0Athe%20storyline%20across%20sequential%20short-term%20segments.%20Additionally%2C%20we%20propose%0Ato%20integrate%20global%20semantics%20into%20each%20local%20feature%20to%20enhance%20context%0Aunderstanding.%20In%20this%20way%2C%20we%20encode%20video%20representations%20that%20incorporate%0Aboth%20local%20and%20global%20information%2C%20enabling%20the%20LLM%20to%20generate%20comprehensive%0Aresponses%20for%20long-term%20videos.%20Experimental%20results%20on%20the%20VideoChatGPT%0Abenchmark%20and%20zero-shot%20video%20question-answering%20datasets%20demonstrate%20the%0Asuperior%20capabilities%20of%20our%20model%20over%20the%20previous%20state-of-the-art%20methods.%0AQualitative%20examples%20demonstrate%20that%20our%20model%20produces%20more%20precise%20responses%0Afor%20long%20videos%20understanding.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ziplab/LongVLM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03384v1&entry.124074799=Read"},
{"title": "OW-VISCap: Open-World Video Instance Segmentation and Captioning", "author": "Anwesa Choudhuri and Girish Chowdhary and Alexander G. Schwing", "abstract": "  Open-world video instance segmentation is an important video understanding\ntask. Yet most methods either operate in a closed-world setting, require an\nadditional user-input, or use classic region-based proposals to identify never\nbefore seen objects. Further, these methods only assign a one-word label to\ndetected objects, and don't generate rich object-centric descriptions. They\nalso often suffer from highly overlapping predictions. To address these issues,\nwe propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),\nan approach to jointly segment, track, and caption previously seen or unseen\nobjects in a video. For this, we introduce open-world object queries to\ndiscover never before seen objects without additional user-input. We generate\nrich and descriptive object-centric captions for each detected object via a\nmasked attention augmented LLM input. We introduce an inter-query contrastive\nloss to ensure that the object queries differ from one another. Our generalized\napproach matches or surpasses state-of-the-art on three tasks: open-world video\ninstance segmentation on the BURST dataset, dense video object captioning on\nthe VidSTG dataset, and closed-world video instance segmentation on the OVIS\ndataset.\n", "link": "http://arxiv.org/abs/2404.03657v1", "date": "2024-04-04", "relevancy": 2.2036, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5628}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5359}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OW-VISCap%3A%20Open-World%20Video%20Instance%20Segmentation%20and%20Captioning&body=Title%3A%20OW-VISCap%3A%20Open-World%20Video%20Instance%20Segmentation%20and%20Captioning%0AAuthor%3A%20Anwesa%20Choudhuri%20and%20Girish%20Chowdhary%20and%20Alexander%20G.%20Schwing%0AAbstract%3A%20%20%20Open-world%20video%20instance%20segmentation%20is%20an%20important%20video%20understanding%0Atask.%20Yet%20most%20methods%20either%20operate%20in%20a%20closed-world%20setting%2C%20require%20an%0Aadditional%20user-input%2C%20or%20use%20classic%20region-based%20proposals%20to%20identify%20never%0Abefore%20seen%20objects.%20Further%2C%20these%20methods%20only%20assign%20a%20one-word%20label%20to%0Adetected%20objects%2C%20and%20don%27t%20generate%20rich%20object-centric%20descriptions.%20They%0Aalso%20often%20suffer%20from%20highly%20overlapping%20predictions.%20To%20address%20these%20issues%2C%0Awe%20propose%20Open-World%20Video%20Instance%20Segmentation%20and%20Captioning%20%28OW-VISCap%29%2C%0Aan%20approach%20to%20jointly%20segment%2C%20track%2C%20and%20caption%20previously%20seen%20or%20unseen%0Aobjects%20in%20a%20video.%20For%20this%2C%20we%20introduce%20open-world%20object%20queries%20to%0Adiscover%20never%20before%20seen%20objects%20without%20additional%20user-input.%20We%20generate%0Arich%20and%20descriptive%20object-centric%20captions%20for%20each%20detected%20object%20via%20a%0Amasked%20attention%20augmented%20LLM%20input.%20We%20introduce%20an%20inter-query%20contrastive%0Aloss%20to%20ensure%20that%20the%20object%20queries%20differ%20from%20one%20another.%20Our%20generalized%0Aapproach%20matches%20or%20surpasses%20state-of-the-art%20on%20three%20tasks%3A%20open-world%20video%0Ainstance%20segmentation%20on%20the%20BURST%20dataset%2C%20dense%20video%20object%20captioning%20on%0Athe%20VidSTG%20dataset%2C%20and%20closed-world%20video%20instance%20segmentation%20on%20the%20OVIS%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03657v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OW-VISCap%3A%20Open-World%20Video%20Instance%20Segmentation%20and%20Captioning&entry.906535625=Anwesa%20Choudhuri%20and%20Girish%20Chowdhary%20and%20Alexander%20G.%20Schwing&entry.1292438233=%20%20Open-world%20video%20instance%20segmentation%20is%20an%20important%20video%20understanding%0Atask.%20Yet%20most%20methods%20either%20operate%20in%20a%20closed-world%20setting%2C%20require%20an%0Aadditional%20user-input%2C%20or%20use%20classic%20region-based%20proposals%20to%20identify%20never%0Abefore%20seen%20objects.%20Further%2C%20these%20methods%20only%20assign%20a%20one-word%20label%20to%0Adetected%20objects%2C%20and%20don%27t%20generate%20rich%20object-centric%20descriptions.%20They%0Aalso%20often%20suffer%20from%20highly%20overlapping%20predictions.%20To%20address%20these%20issues%2C%0Awe%20propose%20Open-World%20Video%20Instance%20Segmentation%20and%20Captioning%20%28OW-VISCap%29%2C%0Aan%20approach%20to%20jointly%20segment%2C%20track%2C%20and%20caption%20previously%20seen%20or%20unseen%0Aobjects%20in%20a%20video.%20For%20this%2C%20we%20introduce%20open-world%20object%20queries%20to%0Adiscover%20never%20before%20seen%20objects%20without%20additional%20user-input.%20We%20generate%0Arich%20and%20descriptive%20object-centric%20captions%20for%20each%20detected%20object%20via%20a%0Amasked%20attention%20augmented%20LLM%20input.%20We%20introduce%20an%20inter-query%20contrastive%0Aloss%20to%20ensure%20that%20the%20object%20queries%20differ%20from%20one%20another.%20Our%20generalized%0Aapproach%20matches%20or%20surpasses%20state-of-the-art%20on%20three%20tasks%3A%20open-world%20video%0Ainstance%20segmentation%20on%20the%20BURST%20dataset%2C%20dense%20video%20object%20captioning%20on%0Athe%20VidSTG%20dataset%2C%20and%20closed-world%20video%20instance%20segmentation%20on%20the%20OVIS%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03657v1&entry.124074799=Read"},
{"title": "Learning Subject-Aware Cropping by Outpainting Professional Photos", "author": "James Hong and Lu Yuan and Micha\u00ebl Gharbi and Matthew Fisher and Kayvon Fatahalian", "abstract": "  How to frame (or crop) a photo often depends on the image subject and its\ncontext; e.g., a human portrait. Recent works have defined the subject-aware\nimage cropping task as a nuanced and practical version of image cropping. We\npropose a weakly-supervised approach (GenCrop) to learn what makes a\nhigh-quality, subject-aware crop from professional stock images. Unlike\nsupervised prior work, GenCrop requires no new manual annotations beyond the\nexisting stock image collection. The key challenge in learning from this data,\nhowever, is that the images are already cropped and we do not know what regions\nwere removed. Our insight is to combine a library of stock images with a\nmodern, pre-trained text-to-image diffusion model. The stock image collection\nprovides diversity and its images serve as pseudo-labels for a good crop, while\nthe text-image diffusion model is used to out-paint (i.e., outward inpainting)\nrealistic uncropped images. Using this procedure, we are able to automatically\ngenerate a large dataset of cropped-uncropped training pairs to train a\ncropping model. Despite being weakly-supervised, GenCrop is competitive with\nstate-of-the-art supervised methods and significantly better than comparable\nweakly-supervised baselines on quantitative and qualitative evaluation metrics.\n", "link": "http://arxiv.org/abs/2312.12080v2", "date": "2024-04-04", "relevancy": 2.1867, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5812}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5461}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5124}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Subject-Aware%20Cropping%20by%20Outpainting%20Professional%20Photos&body=Title%3A%20Learning%20Subject-Aware%20Cropping%20by%20Outpainting%20Professional%20Photos%0AAuthor%3A%20James%20Hong%20and%20Lu%20Yuan%20and%20Micha%C3%ABl%20Gharbi%20and%20Matthew%20Fisher%20and%20Kayvon%20Fatahalian%0AAbstract%3A%20%20%20How%20to%20frame%20%28or%20crop%29%20a%20photo%20often%20depends%20on%20the%20image%20subject%20and%20its%0Acontext%3B%20e.g.%2C%20a%20human%20portrait.%20Recent%20works%20have%20defined%20the%20subject-aware%0Aimage%20cropping%20task%20as%20a%20nuanced%20and%20practical%20version%20of%20image%20cropping.%20We%0Apropose%20a%20weakly-supervised%20approach%20%28GenCrop%29%20to%20learn%20what%20makes%20a%0Ahigh-quality%2C%20subject-aware%20crop%20from%20professional%20stock%20images.%20Unlike%0Asupervised%20prior%20work%2C%20GenCrop%20requires%20no%20new%20manual%20annotations%20beyond%20the%0Aexisting%20stock%20image%20collection.%20The%20key%20challenge%20in%20learning%20from%20this%20data%2C%0Ahowever%2C%20is%20that%20the%20images%20are%20already%20cropped%20and%20we%20do%20not%20know%20what%20regions%0Awere%20removed.%20Our%20insight%20is%20to%20combine%20a%20library%20of%20stock%20images%20with%20a%0Amodern%2C%20pre-trained%20text-to-image%20diffusion%20model.%20The%20stock%20image%20collection%0Aprovides%20diversity%20and%20its%20images%20serve%20as%20pseudo-labels%20for%20a%20good%20crop%2C%20while%0Athe%20text-image%20diffusion%20model%20is%20used%20to%20out-paint%20%28i.e.%2C%20outward%20inpainting%29%0Arealistic%20uncropped%20images.%20Using%20this%20procedure%2C%20we%20are%20able%20to%20automatically%0Agenerate%20a%20large%20dataset%20of%20cropped-uncropped%20training%20pairs%20to%20train%20a%0Acropping%20model.%20Despite%20being%20weakly-supervised%2C%20GenCrop%20is%20competitive%20with%0Astate-of-the-art%20supervised%20methods%20and%20significantly%20better%20than%20comparable%0Aweakly-supervised%20baselines%20on%20quantitative%20and%20qualitative%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12080v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Subject-Aware%20Cropping%20by%20Outpainting%20Professional%20Photos&entry.906535625=James%20Hong%20and%20Lu%20Yuan%20and%20Micha%C3%ABl%20Gharbi%20and%20Matthew%20Fisher%20and%20Kayvon%20Fatahalian&entry.1292438233=%20%20How%20to%20frame%20%28or%20crop%29%20a%20photo%20often%20depends%20on%20the%20image%20subject%20and%20its%0Acontext%3B%20e.g.%2C%20a%20human%20portrait.%20Recent%20works%20have%20defined%20the%20subject-aware%0Aimage%20cropping%20task%20as%20a%20nuanced%20and%20practical%20version%20of%20image%20cropping.%20We%0Apropose%20a%20weakly-supervised%20approach%20%28GenCrop%29%20to%20learn%20what%20makes%20a%0Ahigh-quality%2C%20subject-aware%20crop%20from%20professional%20stock%20images.%20Unlike%0Asupervised%20prior%20work%2C%20GenCrop%20requires%20no%20new%20manual%20annotations%20beyond%20the%0Aexisting%20stock%20image%20collection.%20The%20key%20challenge%20in%20learning%20from%20this%20data%2C%0Ahowever%2C%20is%20that%20the%20images%20are%20already%20cropped%20and%20we%20do%20not%20know%20what%20regions%0Awere%20removed.%20Our%20insight%20is%20to%20combine%20a%20library%20of%20stock%20images%20with%20a%0Amodern%2C%20pre-trained%20text-to-image%20diffusion%20model.%20The%20stock%20image%20collection%0Aprovides%20diversity%20and%20its%20images%20serve%20as%20pseudo-labels%20for%20a%20good%20crop%2C%20while%0Athe%20text-image%20diffusion%20model%20is%20used%20to%20out-paint%20%28i.e.%2C%20outward%20inpainting%29%0Arealistic%20uncropped%20images.%20Using%20this%20procedure%2C%20we%20are%20able%20to%20automatically%0Agenerate%20a%20large%20dataset%20of%20cropped-uncropped%20training%20pairs%20to%20train%20a%0Acropping%20model.%20Despite%20being%20weakly-supervised%2C%20GenCrop%20is%20competitive%20with%0Astate-of-the-art%20supervised%20methods%20and%20significantly%20better%20than%20comparable%0Aweakly-supervised%20baselines%20on%20quantitative%20and%20qualitative%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12080v2&entry.124074799=Read"},
{"title": "Integrating Hyperparameter Search into GramML", "author": "Hern\u00e1n Ceferino V\u00e1zquez and Jorge Sanchez and Rafael Carrascosa", "abstract": "  Automated Machine Learning (AutoML) has become increasingly popular in recent\nyears due to its ability to reduce the amount of time and expertise required to\ndesign and develop machine learning systems. This is very important for the\npractice of machine learning, as it allows building strong baselines quickly,\nimproving the efficiency of the data scientists, and reducing the time to\nproduction. However, despite the advantages of AutoML, it faces several\nchallenges, such as defining the solutions space and exploring it efficiently.\nRecently, some approaches have been shown to be able to do it using tree-based\nsearch algorithms and context-free grammars. In particular, GramML presents a\nmodel-free reinforcement learning approach that leverages pipeline\nconfiguration grammars and operates using Monte Carlo tree search. However, one\nof the limitations of GramML is that it uses default hyperparameters, limiting\nthe search problem to finding optimal pipeline structures for the available\ndata preprocessors and models. In this work, we propose an extension to GramML\nthat supports larger search spaces including hyperparameter search. We\nevaluated the approach using an OpenML benchmark and found significant\nimprovements compared to other state-of-the-art techniques.\n", "link": "http://arxiv.org/abs/2404.03419v1", "date": "2024-04-04", "relevancy": 2.1764, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4411}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4389}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4258}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Integrating%20Hyperparameter%20Search%20into%20GramML&body=Title%3A%20Integrating%20Hyperparameter%20Search%20into%20GramML%0AAuthor%3A%20Hern%C3%A1n%20Ceferino%20V%C3%A1zquez%20and%20Jorge%20Sanchez%20and%20Rafael%20Carrascosa%0AAbstract%3A%20%20%20Automated%20Machine%20Learning%20%28AutoML%29%20has%20become%20increasingly%20popular%20in%20recent%0Ayears%20due%20to%20its%20ability%20to%20reduce%20the%20amount%20of%20time%20and%20expertise%20required%20to%0Adesign%20and%20develop%20machine%20learning%20systems.%20This%20is%20very%20important%20for%20the%0Apractice%20of%20machine%20learning%2C%20as%20it%20allows%20building%20strong%20baselines%20quickly%2C%0Aimproving%20the%20efficiency%20of%20the%20data%20scientists%2C%20and%20reducing%20the%20time%20to%0Aproduction.%20However%2C%20despite%20the%20advantages%20of%20AutoML%2C%20it%20faces%20several%0Achallenges%2C%20such%20as%20defining%20the%20solutions%20space%20and%20exploring%20it%20efficiently.%0ARecently%2C%20some%20approaches%20have%20been%20shown%20to%20be%20able%20to%20do%20it%20using%20tree-based%0Asearch%20algorithms%20and%20context-free%20grammars.%20In%20particular%2C%20GramML%20presents%20a%0Amodel-free%20reinforcement%20learning%20approach%20that%20leverages%20pipeline%0Aconfiguration%20grammars%20and%20operates%20using%20Monte%20Carlo%20tree%20search.%20However%2C%20one%0Aof%20the%20limitations%20of%20GramML%20is%20that%20it%20uses%20default%20hyperparameters%2C%20limiting%0Athe%20search%20problem%20to%20finding%20optimal%20pipeline%20structures%20for%20the%20available%0Adata%20preprocessors%20and%20models.%20In%20this%20work%2C%20we%20propose%20an%20extension%20to%20GramML%0Athat%20supports%20larger%20search%20spaces%20including%20hyperparameter%20search.%20We%0Aevaluated%20the%20approach%20using%20an%20OpenML%20benchmark%20and%20found%20significant%0Aimprovements%20compared%20to%20other%20state-of-the-art%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03419v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Hyperparameter%20Search%20into%20GramML&entry.906535625=Hern%C3%A1n%20Ceferino%20V%C3%A1zquez%20and%20Jorge%20Sanchez%20and%20Rafael%20Carrascosa&entry.1292438233=%20%20Automated%20Machine%20Learning%20%28AutoML%29%20has%20become%20increasingly%20popular%20in%20recent%0Ayears%20due%20to%20its%20ability%20to%20reduce%20the%20amount%20of%20time%20and%20expertise%20required%20to%0Adesign%20and%20develop%20machine%20learning%20systems.%20This%20is%20very%20important%20for%20the%0Apractice%20of%20machine%20learning%2C%20as%20it%20allows%20building%20strong%20baselines%20quickly%2C%0Aimproving%20the%20efficiency%20of%20the%20data%20scientists%2C%20and%20reducing%20the%20time%20to%0Aproduction.%20However%2C%20despite%20the%20advantages%20of%20AutoML%2C%20it%20faces%20several%0Achallenges%2C%20such%20as%20defining%20the%20solutions%20space%20and%20exploring%20it%20efficiently.%0ARecently%2C%20some%20approaches%20have%20been%20shown%20to%20be%20able%20to%20do%20it%20using%20tree-based%0Asearch%20algorithms%20and%20context-free%20grammars.%20In%20particular%2C%20GramML%20presents%20a%0Amodel-free%20reinforcement%20learning%20approach%20that%20leverages%20pipeline%0Aconfiguration%20grammars%20and%20operates%20using%20Monte%20Carlo%20tree%20search.%20However%2C%20one%0Aof%20the%20limitations%20of%20GramML%20is%20that%20it%20uses%20default%20hyperparameters%2C%20limiting%0Athe%20search%20problem%20to%20finding%20optimal%20pipeline%20structures%20for%20the%20available%0Adata%20preprocessors%20and%20models.%20In%20this%20work%2C%20we%20propose%20an%20extension%20to%20GramML%0Athat%20supports%20larger%20search%20spaces%20including%20hyperparameter%20search.%20We%0Aevaluated%20the%20approach%20using%20an%20OpenML%20benchmark%20and%20found%20significant%0Aimprovements%20compared%20to%20other%20state-of-the-art%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03419v1&entry.124074799=Read"},
{"title": "Towards Robust Continual Learning with Bayesian Adaptive Moment\n  Regularization", "author": "Jack Foster and Alexandra Brintrup", "abstract": "  The pursuit of long-term autonomy mandates that robotic agents must\ncontinuously adapt to their changing environments and learn to solve new tasks.\nContinual learning seeks to overcome the challenge of catastrophic forgetting,\nwhere learning to solve new tasks causes a model to forget previously learnt\ninformation. Prior-based continual learning methods are appealing for robotic\napplications as they are space efficient and typically do not increase in\ncomputational complexity as the number of tasks grows. Despite these desirable\nproperties, prior-based approaches typically fail on important benchmarks and\nconsequently are limited in their potential applications compared to their\nmemory-based counterparts. We introduce Bayesian adaptive moment regularization\n(BAdam), a novel prior-based method that better constrains parameter growth,\nleading to lower catastrophic forgetting. Our method boasts a range of\ndesirable properties for robotic applications such as being lightweight and\ntask label-free, converging quickly, and offering calibrated uncertainty that\nis important for safe real-world deployment. Results show that BAdam achieves\nstate-of-the-art performance for prior-based methods on challenging\nsingle-headed class-incremental experiments such as Split MNIST and Split\nFashionMNIST, and does so without relying on task labels or discrete task\nboundaries.\n", "link": "http://arxiv.org/abs/2309.08546v2", "date": "2024-04-04", "relevancy": 2.1675, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6231}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5272}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5241}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Continual%20Learning%20with%20Bayesian%20Adaptive%20Moment%0A%20%20Regularization&body=Title%3A%20Towards%20Robust%20Continual%20Learning%20with%20Bayesian%20Adaptive%20Moment%0A%20%20Regularization%0AAuthor%3A%20Jack%20Foster%20and%20Alexandra%20Brintrup%0AAbstract%3A%20%20%20The%20pursuit%20of%20long-term%20autonomy%20mandates%20that%20robotic%20agents%20must%0Acontinuously%20adapt%20to%20their%20changing%20environments%20and%20learn%20to%20solve%20new%20tasks.%0AContinual%20learning%20seeks%20to%20overcome%20the%20challenge%20of%20catastrophic%20forgetting%2C%0Awhere%20learning%20to%20solve%20new%20tasks%20causes%20a%20model%20to%20forget%20previously%20learnt%0Ainformation.%20Prior-based%20continual%20learning%20methods%20are%20appealing%20for%20robotic%0Aapplications%20as%20they%20are%20space%20efficient%20and%20typically%20do%20not%20increase%20in%0Acomputational%20complexity%20as%20the%20number%20of%20tasks%20grows.%20Despite%20these%20desirable%0Aproperties%2C%20prior-based%20approaches%20typically%20fail%20on%20important%20benchmarks%20and%0Aconsequently%20are%20limited%20in%20their%20potential%20applications%20compared%20to%20their%0Amemory-based%20counterparts.%20We%20introduce%20Bayesian%20adaptive%20moment%20regularization%0A%28BAdam%29%2C%20a%20novel%20prior-based%20method%20that%20better%20constrains%20parameter%20growth%2C%0Aleading%20to%20lower%20catastrophic%20forgetting.%20Our%20method%20boasts%20a%20range%20of%0Adesirable%20properties%20for%20robotic%20applications%20such%20as%20being%20lightweight%20and%0Atask%20label-free%2C%20converging%20quickly%2C%20and%20offering%20calibrated%20uncertainty%20that%0Ais%20important%20for%20safe%20real-world%20deployment.%20Results%20show%20that%20BAdam%20achieves%0Astate-of-the-art%20performance%20for%20prior-based%20methods%20on%20challenging%0Asingle-headed%20class-incremental%20experiments%20such%20as%20Split%20MNIST%20and%20Split%0AFashionMNIST%2C%20and%20does%20so%20without%20relying%20on%20task%20labels%20or%20discrete%20task%0Aboundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08546v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Continual%20Learning%20with%20Bayesian%20Adaptive%20Moment%0A%20%20Regularization&entry.906535625=Jack%20Foster%20and%20Alexandra%20Brintrup&entry.1292438233=%20%20The%20pursuit%20of%20long-term%20autonomy%20mandates%20that%20robotic%20agents%20must%0Acontinuously%20adapt%20to%20their%20changing%20environments%20and%20learn%20to%20solve%20new%20tasks.%0AContinual%20learning%20seeks%20to%20overcome%20the%20challenge%20of%20catastrophic%20forgetting%2C%0Awhere%20learning%20to%20solve%20new%20tasks%20causes%20a%20model%20to%20forget%20previously%20learnt%0Ainformation.%20Prior-based%20continual%20learning%20methods%20are%20appealing%20for%20robotic%0Aapplications%20as%20they%20are%20space%20efficient%20and%20typically%20do%20not%20increase%20in%0Acomputational%20complexity%20as%20the%20number%20of%20tasks%20grows.%20Despite%20these%20desirable%0Aproperties%2C%20prior-based%20approaches%20typically%20fail%20on%20important%20benchmarks%20and%0Aconsequently%20are%20limited%20in%20their%20potential%20applications%20compared%20to%20their%0Amemory-based%20counterparts.%20We%20introduce%20Bayesian%20adaptive%20moment%20regularization%0A%28BAdam%29%2C%20a%20novel%20prior-based%20method%20that%20better%20constrains%20parameter%20growth%2C%0Aleading%20to%20lower%20catastrophic%20forgetting.%20Our%20method%20boasts%20a%20range%20of%0Adesirable%20properties%20for%20robotic%20applications%20such%20as%20being%20lightweight%20and%0Atask%20label-free%2C%20converging%20quickly%2C%20and%20offering%20calibrated%20uncertainty%20that%0Ais%20important%20for%20safe%20real-world%20deployment.%20Results%20show%20that%20BAdam%20achieves%0Astate-of-the-art%20performance%20for%20prior-based%20methods%20on%20challenging%0Asingle-headed%20class-incremental%20experiments%20such%20as%20Split%20MNIST%20and%20Split%0AFashionMNIST%2C%20and%20does%20so%20without%20relying%20on%20task%20labels%20or%20discrete%20task%0Aboundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08546v2&entry.124074799=Read"},
{"title": "If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face\n  Recognition through Synthetic Faces", "author": "Andrea Atzori and Fadi Boutros and Naser Damer and Gianni Fenu and Mirko Marras", "abstract": "  Recent advances in deep face recognition have spurred a growing demand for\nlarge, diverse, and manually annotated face datasets. Acquiring authentic,\nhigh-quality data for face recognition has proven to be a challenge, primarily\ndue to privacy concerns. Large face datasets are primarily sourced from\nweb-based images, lacking explicit user consent. In this paper, we examine\nwhether and how synthetic face data can be used to train effective face\nrecognition models with reduced reliance on authentic images, thereby\nmitigating data collection concerns. First, we explored the performance gap\namong recent state-of-the-art face recognition models, trained with synthetic\ndata only and authentic (scarce) data only. Then, we deepened our analysis by\ntraining a state-of-the-art backbone with various combinations of synthetic and\nauthentic data, gaining insights into optimizing the limited use of the latter\nfor verification accuracy. Finally, we assessed the effectiveness of data\naugmentation approaches on synthetic and authentic data, with the same goal in\nmind. Our results highlighted the effectiveness of FR trained on combined\ndatasets, particularly when combined with appropriate augmentation techniques.\n", "link": "http://arxiv.org/abs/2404.03537v1", "date": "2024-04-04", "relevancy": 2.1628, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5465}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5269}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20If%20It%27s%20Not%20Enough%2C%20Make%20It%20So%3A%20Reducing%20Authentic%20Data%20Demand%20in%20Face%0A%20%20Recognition%20through%20Synthetic%20Faces&body=Title%3A%20If%20It%27s%20Not%20Enough%2C%20Make%20It%20So%3A%20Reducing%20Authentic%20Data%20Demand%20in%20Face%0A%20%20Recognition%20through%20Synthetic%20Faces%0AAuthor%3A%20Andrea%20Atzori%20and%20Fadi%20Boutros%20and%20Naser%20Damer%20and%20Gianni%20Fenu%20and%20Mirko%20Marras%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20face%20recognition%20have%20spurred%20a%20growing%20demand%20for%0Alarge%2C%20diverse%2C%20and%20manually%20annotated%20face%20datasets.%20Acquiring%20authentic%2C%0Ahigh-quality%20data%20for%20face%20recognition%20has%20proven%20to%20be%20a%20challenge%2C%20primarily%0Adue%20to%20privacy%20concerns.%20Large%20face%20datasets%20are%20primarily%20sourced%20from%0Aweb-based%20images%2C%20lacking%20explicit%20user%20consent.%20In%20this%20paper%2C%20we%20examine%0Awhether%20and%20how%20synthetic%20face%20data%20can%20be%20used%20to%20train%20effective%20face%0Arecognition%20models%20with%20reduced%20reliance%20on%20authentic%20images%2C%20thereby%0Amitigating%20data%20collection%20concerns.%20First%2C%20we%20explored%20the%20performance%20gap%0Aamong%20recent%20state-of-the-art%20face%20recognition%20models%2C%20trained%20with%20synthetic%0Adata%20only%20and%20authentic%20%28scarce%29%20data%20only.%20Then%2C%20we%20deepened%20our%20analysis%20by%0Atraining%20a%20state-of-the-art%20backbone%20with%20various%20combinations%20of%20synthetic%20and%0Aauthentic%20data%2C%20gaining%20insights%20into%20optimizing%20the%20limited%20use%20of%20the%20latter%0Afor%20verification%20accuracy.%20Finally%2C%20we%20assessed%20the%20effectiveness%20of%20data%0Aaugmentation%20approaches%20on%20synthetic%20and%20authentic%20data%2C%20with%20the%20same%20goal%20in%0Amind.%20Our%20results%20highlighted%20the%20effectiveness%20of%20FR%20trained%20on%20combined%0Adatasets%2C%20particularly%20when%20combined%20with%20appropriate%20augmentation%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03537v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=If%20It%27s%20Not%20Enough%2C%20Make%20It%20So%3A%20Reducing%20Authentic%20Data%20Demand%20in%20Face%0A%20%20Recognition%20through%20Synthetic%20Faces&entry.906535625=Andrea%20Atzori%20and%20Fadi%20Boutros%20and%20Naser%20Damer%20and%20Gianni%20Fenu%20and%20Mirko%20Marras&entry.1292438233=%20%20Recent%20advances%20in%20deep%20face%20recognition%20have%20spurred%20a%20growing%20demand%20for%0Alarge%2C%20diverse%2C%20and%20manually%20annotated%20face%20datasets.%20Acquiring%20authentic%2C%0Ahigh-quality%20data%20for%20face%20recognition%20has%20proven%20to%20be%20a%20challenge%2C%20primarily%0Adue%20to%20privacy%20concerns.%20Large%20face%20datasets%20are%20primarily%20sourced%20from%0Aweb-based%20images%2C%20lacking%20explicit%20user%20consent.%20In%20this%20paper%2C%20we%20examine%0Awhether%20and%20how%20synthetic%20face%20data%20can%20be%20used%20to%20train%20effective%20face%0Arecognition%20models%20with%20reduced%20reliance%20on%20authentic%20images%2C%20thereby%0Amitigating%20data%20collection%20concerns.%20First%2C%20we%20explored%20the%20performance%20gap%0Aamong%20recent%20state-of-the-art%20face%20recognition%20models%2C%20trained%20with%20synthetic%0Adata%20only%20and%20authentic%20%28scarce%29%20data%20only.%20Then%2C%20we%20deepened%20our%20analysis%20by%0Atraining%20a%20state-of-the-art%20backbone%20with%20various%20combinations%20of%20synthetic%20and%0Aauthentic%20data%2C%20gaining%20insights%20into%20optimizing%20the%20limited%20use%20of%20the%20latter%0Afor%20verification%20accuracy.%20Finally%2C%20we%20assessed%20the%20effectiveness%20of%20data%0Aaugmentation%20approaches%20on%20synthetic%20and%20authentic%20data%2C%20with%20the%20same%20goal%20in%0Amind.%20Our%20results%20highlighted%20the%20effectiveness%20of%20FR%20trained%20on%20combined%0Adatasets%2C%20particularly%20when%20combined%20with%20appropriate%20augmentation%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03537v1&entry.124074799=Read"},
{"title": "Towards Automated Movie Trailer Generation", "author": "Dawit Mureja Argaw and Mattia Soldan and Alejandro Pardo and Chen Zhao and Fabian Caba Heilbron and Joon Son Chung and Bernard Ghanem", "abstract": "  Movie trailers are an essential tool for promoting films and attracting\naudiences. However, the process of creating trailers can be time-consuming and\nexpensive. To streamline this process, we propose an automatic trailer\ngeneration framework that generates plausible trailers from a full movie by\nautomating shot selection and composition. Our approach draws inspiration from\nmachine translation techniques and models the movies and trailers as sequences\nof shots, thus formulating the trailer generation problem as a\nsequence-to-sequence task. We introduce Trailer Generation Transformer (TGT), a\ndeep-learning framework utilizing an encoder-decoder architecture. TGT movie\nencoder is tasked with contextualizing each movie shot representation via\nself-attention, while the autoregressive trailer decoder predicts the feature\nrepresentation of the next trailer shot, accounting for the relevance of shots'\ntemporal order in trailers. Our TGT significantly outperforms previous methods\non a comprehensive suite of metrics.\n", "link": "http://arxiv.org/abs/2404.03477v1", "date": "2024-04-04", "relevancy": 2.1551, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.56}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5422}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5269}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Automated%20Movie%20Trailer%20Generation&body=Title%3A%20Towards%20Automated%20Movie%20Trailer%20Generation%0AAuthor%3A%20Dawit%20Mureja%20Argaw%20and%20Mattia%20Soldan%20and%20Alejandro%20Pardo%20and%20Chen%20Zhao%20and%20Fabian%20Caba%20Heilbron%20and%20Joon%20Son%20Chung%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20Movie%20trailers%20are%20an%20essential%20tool%20for%20promoting%20films%20and%20attracting%0Aaudiences.%20However%2C%20the%20process%20of%20creating%20trailers%20can%20be%20time-consuming%20and%0Aexpensive.%20To%20streamline%20this%20process%2C%20we%20propose%20an%20automatic%20trailer%0Ageneration%20framework%20that%20generates%20plausible%20trailers%20from%20a%20full%20movie%20by%0Aautomating%20shot%20selection%20and%20composition.%20Our%20approach%20draws%20inspiration%20from%0Amachine%20translation%20techniques%20and%20models%20the%20movies%20and%20trailers%20as%20sequences%0Aof%20shots%2C%20thus%20formulating%20the%20trailer%20generation%20problem%20as%20a%0Asequence-to-sequence%20task.%20We%20introduce%20Trailer%20Generation%20Transformer%20%28TGT%29%2C%20a%0Adeep-learning%20framework%20utilizing%20an%20encoder-decoder%20architecture.%20TGT%20movie%0Aencoder%20is%20tasked%20with%20contextualizing%20each%20movie%20shot%20representation%20via%0Aself-attention%2C%20while%20the%20autoregressive%20trailer%20decoder%20predicts%20the%20feature%0Arepresentation%20of%20the%20next%20trailer%20shot%2C%20accounting%20for%20the%20relevance%20of%20shots%27%0Atemporal%20order%20in%20trailers.%20Our%20TGT%20significantly%20outperforms%20previous%20methods%0Aon%20a%20comprehensive%20suite%20of%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03477v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Automated%20Movie%20Trailer%20Generation&entry.906535625=Dawit%20Mureja%20Argaw%20and%20Mattia%20Soldan%20and%20Alejandro%20Pardo%20and%20Chen%20Zhao%20and%20Fabian%20Caba%20Heilbron%20and%20Joon%20Son%20Chung%20and%20Bernard%20Ghanem&entry.1292438233=%20%20Movie%20trailers%20are%20an%20essential%20tool%20for%20promoting%20films%20and%20attracting%0Aaudiences.%20However%2C%20the%20process%20of%20creating%20trailers%20can%20be%20time-consuming%20and%0Aexpensive.%20To%20streamline%20this%20process%2C%20we%20propose%20an%20automatic%20trailer%0Ageneration%20framework%20that%20generates%20plausible%20trailers%20from%20a%20full%20movie%20by%0Aautomating%20shot%20selection%20and%20composition.%20Our%20approach%20draws%20inspiration%20from%0Amachine%20translation%20techniques%20and%20models%20the%20movies%20and%20trailers%20as%20sequences%0Aof%20shots%2C%20thus%20formulating%20the%20trailer%20generation%20problem%20as%20a%0Asequence-to-sequence%20task.%20We%20introduce%20Trailer%20Generation%20Transformer%20%28TGT%29%2C%20a%0Adeep-learning%20framework%20utilizing%20an%20encoder-decoder%20architecture.%20TGT%20movie%0Aencoder%20is%20tasked%20with%20contextualizing%20each%20movie%20shot%20representation%20via%0Aself-attention%2C%20while%20the%20autoregressive%20trailer%20decoder%20predicts%20the%20feature%0Arepresentation%20of%20the%20next%20trailer%20shot%2C%20accounting%20for%20the%20relevance%20of%20shots%27%0Atemporal%20order%20in%20trailers.%20Our%20TGT%20significantly%20outperforms%20previous%20methods%0Aon%20a%20comprehensive%20suite%20of%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03477v1&entry.124074799=Read"},
{"title": "3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting", "author": "Zhiyin Qian and Shaofei Wang and Marko Mihajlovic and Andreas Geiger and Siyu Tang", "abstract": "  We introduce an approach that creates animatable human avatars from monocular\nvideos using 3D Gaussian Splatting (3DGS). Existing methods based on neural\nradiance fields (NeRFs) achieve high-quality novel-view/novel-pose image\nsynthesis but often require days of training, and are extremely slow at\ninference time. Recently, the community has explored fast grid structures for\nefficient training of clothed avatars. Albeit being extremely fast at training,\nthese methods can barely achieve an interactive rendering frame rate with\naround 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a\nnon-rigid deformation network to reconstruct animatable clothed human avatars\nthat can be trained within 30 minutes and rendered at real-time frame rates\n(50+ FPS). Given the explicit nature of our representation, we further\nintroduce as-isometric-as-possible regularizations on both the Gaussian mean\nvectors and the covariance matrices, enhancing the generalization of our model\non highly articulated unseen poses. Experimental results show that our method\nachieves comparable and even better performance compared to state-of-the-art\napproaches on animatable avatar creation from a monocular input, while being\n400x and 250x faster in training and inference, respectively.\n", "link": "http://arxiv.org/abs/2312.09228v3", "date": "2024-04-04", "relevancy": 2.1542, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5417}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5364}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.536}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203DGS-Avatar%3A%20Animatable%20Avatars%20via%20Deformable%203D%20Gaussian%20Splatting&body=Title%3A%203DGS-Avatar%3A%20Animatable%20Avatars%20via%20Deformable%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zhiyin%20Qian%20and%20Shaofei%20Wang%20and%20Marko%20Mihajlovic%20and%20Andreas%20Geiger%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20We%20introduce%20an%20approach%20that%20creates%20animatable%20human%20avatars%20from%20monocular%0Avideos%20using%203D%20Gaussian%20Splatting%20%283DGS%29.%20Existing%20methods%20based%20on%20neural%0Aradiance%20fields%20%28NeRFs%29%20achieve%20high-quality%20novel-view/novel-pose%20image%0Asynthesis%20but%20often%20require%20days%20of%20training%2C%20and%20are%20extremely%20slow%20at%0Ainference%20time.%20Recently%2C%20the%20community%20has%20explored%20fast%20grid%20structures%20for%0Aefficient%20training%20of%20clothed%20avatars.%20Albeit%20being%20extremely%20fast%20at%20training%2C%0Athese%20methods%20can%20barely%20achieve%20an%20interactive%20rendering%20frame%20rate%20with%0Aaround%2015%20FPS.%20In%20this%20paper%2C%20we%20use%203D%20Gaussian%20Splatting%20and%20learn%20a%0Anon-rigid%20deformation%20network%20to%20reconstruct%20animatable%20clothed%20human%20avatars%0Athat%20can%20be%20trained%20within%2030%20minutes%20and%20rendered%20at%20real-time%20frame%20rates%0A%2850%2B%20FPS%29.%20Given%20the%20explicit%20nature%20of%20our%20representation%2C%20we%20further%0Aintroduce%20as-isometric-as-possible%20regularizations%20on%20both%20the%20Gaussian%20mean%0Avectors%20and%20the%20covariance%20matrices%2C%20enhancing%20the%20generalization%20of%20our%20model%0Aon%20highly%20articulated%20unseen%20poses.%20Experimental%20results%20show%20that%20our%20method%0Aachieves%20comparable%20and%20even%20better%20performance%20compared%20to%20state-of-the-art%0Aapproaches%20on%20animatable%20avatar%20creation%20from%20a%20monocular%20input%2C%20while%20being%0A400x%20and%20250x%20faster%20in%20training%20and%20inference%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09228v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGS-Avatar%3A%20Animatable%20Avatars%20via%20Deformable%203D%20Gaussian%20Splatting&entry.906535625=Zhiyin%20Qian%20and%20Shaofei%20Wang%20and%20Marko%20Mihajlovic%20and%20Andreas%20Geiger%20and%20Siyu%20Tang&entry.1292438233=%20%20We%20introduce%20an%20approach%20that%20creates%20animatable%20human%20avatars%20from%20monocular%0Avideos%20using%203D%20Gaussian%20Splatting%20%283DGS%29.%20Existing%20methods%20based%20on%20neural%0Aradiance%20fields%20%28NeRFs%29%20achieve%20high-quality%20novel-view/novel-pose%20image%0Asynthesis%20but%20often%20require%20days%20of%20training%2C%20and%20are%20extremely%20slow%20at%0Ainference%20time.%20Recently%2C%20the%20community%20has%20explored%20fast%20grid%20structures%20for%0Aefficient%20training%20of%20clothed%20avatars.%20Albeit%20being%20extremely%20fast%20at%20training%2C%0Athese%20methods%20can%20barely%20achieve%20an%20interactive%20rendering%20frame%20rate%20with%0Aaround%2015%20FPS.%20In%20this%20paper%2C%20we%20use%203D%20Gaussian%20Splatting%20and%20learn%20a%0Anon-rigid%20deformation%20network%20to%20reconstruct%20animatable%20clothed%20human%20avatars%0Athat%20can%20be%20trained%20within%2030%20minutes%20and%20rendered%20at%20real-time%20frame%20rates%0A%2850%2B%20FPS%29.%20Given%20the%20explicit%20nature%20of%20our%20representation%2C%20we%20further%0Aintroduce%20as-isometric-as-possible%20regularizations%20on%20both%20the%20Gaussian%20mean%0Avectors%20and%20the%20covariance%20matrices%2C%20enhancing%20the%20generalization%20of%20our%20model%0Aon%20highly%20articulated%20unseen%20poses.%20Experimental%20results%20show%20that%20our%20method%0Aachieves%20comparable%20and%20even%20better%20performance%20compared%20to%20state-of-the-art%0Aapproaches%20on%20animatable%20avatar%20creation%20from%20a%20monocular%20input%2C%20while%20being%0A400x%20and%20250x%20faster%20in%20training%20and%20inference%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09228v3&entry.124074799=Read"},
{"title": "GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint\n  Registration", "author": "Ilir Tahiraj and Felix Fent and Philipp Hafemann and Egon Ye and Markus Lienkamp", "abstract": "  State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic\nregistration methods such as Iterative Closest Point (ICP) and its variants.\nThese methods suffer from biased results due to their pair-wise registration\nprocedure as well as their sensitivity to initialization and parameterization.\nThis often leads to misalignments in the calibration process. Probabilistic\nregistration methods compensate for these drawbacks by specifically modeling\nthe probabilistic nature of the observations. This paper presents GMMCalib, an\nautomatic target-based extrinsic calibration approach for multi-LiDAR systems.\nUsing an implementation of a Gaussian Mixture Model (GMM)-based registration\nmethod that allows joint registration of multiple point clouds, this\ndata-driven approach is compared to ICP algorithms. We perform simulation\nexperiments using the digital twin of the EDGAR research vehicle and validate\nthe results in a real-world environment. We also address the local minima\nproblem of local registration methods for extrinsic sensor calibration and use\na distance-based metric to evaluate the calibration results. Our results show\nthat an increase in robustness against sensor miscalibrations can be achieved\nby using GMM-based registration algorithms. The code is open source and\navailable on GitHub.\n", "link": "http://arxiv.org/abs/2404.03427v1", "date": "2024-04-04", "relevancy": 2.1399, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5383}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5323}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GMMCalib%3A%20Extrinsic%20Calibration%20of%20LiDAR%20Sensors%20using%20GMM-based%20Joint%0A%20%20Registration&body=Title%3A%20GMMCalib%3A%20Extrinsic%20Calibration%20of%20LiDAR%20Sensors%20using%20GMM-based%20Joint%0A%20%20Registration%0AAuthor%3A%20Ilir%20Tahiraj%20and%20Felix%20Fent%20and%20Philipp%20Hafemann%20and%20Egon%20Ye%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20State-of-the-art%20LiDAR%20calibration%20frameworks%20mainly%20use%20non-probabilistic%0Aregistration%20methods%20such%20as%20Iterative%20Closest%20Point%20%28ICP%29%20and%20its%20variants.%0AThese%20methods%20suffer%20from%20biased%20results%20due%20to%20their%20pair-wise%20registration%0Aprocedure%20as%20well%20as%20their%20sensitivity%20to%20initialization%20and%20parameterization.%0AThis%20often%20leads%20to%20misalignments%20in%20the%20calibration%20process.%20Probabilistic%0Aregistration%20methods%20compensate%20for%20these%20drawbacks%20by%20specifically%20modeling%0Athe%20probabilistic%20nature%20of%20the%20observations.%20This%20paper%20presents%20GMMCalib%2C%20an%0Aautomatic%20target-based%20extrinsic%20calibration%20approach%20for%20multi-LiDAR%20systems.%0AUsing%20an%20implementation%20of%20a%20Gaussian%20Mixture%20Model%20%28GMM%29-based%20registration%0Amethod%20that%20allows%20joint%20registration%20of%20multiple%20point%20clouds%2C%20this%0Adata-driven%20approach%20is%20compared%20to%20ICP%20algorithms.%20We%20perform%20simulation%0Aexperiments%20using%20the%20digital%20twin%20of%20the%20EDGAR%20research%20vehicle%20and%20validate%0Athe%20results%20in%20a%20real-world%20environment.%20We%20also%20address%20the%20local%20minima%0Aproblem%20of%20local%20registration%20methods%20for%20extrinsic%20sensor%20calibration%20and%20use%0Aa%20distance-based%20metric%20to%20evaluate%20the%20calibration%20results.%20Our%20results%20show%0Athat%20an%20increase%20in%20robustness%20against%20sensor%20miscalibrations%20can%20be%20achieved%0Aby%20using%20GMM-based%20registration%20algorithms.%20The%20code%20is%20open%20source%20and%0Aavailable%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03427v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMMCalib%3A%20Extrinsic%20Calibration%20of%20LiDAR%20Sensors%20using%20GMM-based%20Joint%0A%20%20Registration&entry.906535625=Ilir%20Tahiraj%20and%20Felix%20Fent%20and%20Philipp%20Hafemann%20and%20Egon%20Ye%20and%20Markus%20Lienkamp&entry.1292438233=%20%20State-of-the-art%20LiDAR%20calibration%20frameworks%20mainly%20use%20non-probabilistic%0Aregistration%20methods%20such%20as%20Iterative%20Closest%20Point%20%28ICP%29%20and%20its%20variants.%0AThese%20methods%20suffer%20from%20biased%20results%20due%20to%20their%20pair-wise%20registration%0Aprocedure%20as%20well%20as%20their%20sensitivity%20to%20initialization%20and%20parameterization.%0AThis%20often%20leads%20to%20misalignments%20in%20the%20calibration%20process.%20Probabilistic%0Aregistration%20methods%20compensate%20for%20these%20drawbacks%20by%20specifically%20modeling%0Athe%20probabilistic%20nature%20of%20the%20observations.%20This%20paper%20presents%20GMMCalib%2C%20an%0Aautomatic%20target-based%20extrinsic%20calibration%20approach%20for%20multi-LiDAR%20systems.%0AUsing%20an%20implementation%20of%20a%20Gaussian%20Mixture%20Model%20%28GMM%29-based%20registration%0Amethod%20that%20allows%20joint%20registration%20of%20multiple%20point%20clouds%2C%20this%0Adata-driven%20approach%20is%20compared%20to%20ICP%20algorithms.%20We%20perform%20simulation%0Aexperiments%20using%20the%20digital%20twin%20of%20the%20EDGAR%20research%20vehicle%20and%20validate%0Athe%20results%20in%20a%20real-world%20environment.%20We%20also%20address%20the%20local%20minima%0Aproblem%20of%20local%20registration%20methods%20for%20extrinsic%20sensor%20calibration%20and%20use%0Aa%20distance-based%20metric%20to%20evaluate%20the%20calibration%20results.%20Our%20results%20show%0Athat%20an%20increase%20in%20robustness%20against%20sensor%20miscalibrations%20can%20be%20achieved%0Aby%20using%20GMM-based%20registration%20algorithms.%20The%20code%20is%20open%20source%20and%0Aavailable%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03427v1&entry.124074799=Read"},
{"title": "Bootstrapping SparseFormers from Vision Foundation Models", "author": "Ziteng Gao and Zhan Tong and Kevin Qinghong Lin and Joya Chen and Mike Zheng Shou", "abstract": "  The recently proposed SparseFormer architecture provides an alternative\napproach to visual understanding by utilizing a significantly lower number of\nvisual tokens via adjusting RoIs, greatly reducing computational costs while\nstill achieving promising performance. However, training SparseFormers from\nscratch is still expensive, and scaling up the number of parameters can be\nchallenging. In this paper, we propose to bootstrap SparseFormers from\nViT-based vision foundation models in a simple and efficient way. Since the\nmajority of SparseFormer blocks are the standard transformer ones, we can\ninherit weights from large-scale pre-trained vision transformers and freeze\nthem as much as possible. Therefore, we only need to train the\nSparseFormer-specific lightweight focusing transformer to adjust token RoIs and\nfine-tune a few early pre-trained blocks to align the final token\nrepresentation. In such a way, we can bootstrap SparseFormer architectures from\nvarious large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or\nCLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and\nwithout labels or captions within just a few hours. As a result, the\nbootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9%\naccuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from\nCLIPs also demonstrates notable zero-shot performance with highly reduced\ncomputational cost without seeing any caption during the bootstrapping\nprocedure. In addition, CLIP-bootstrapped SparseFormers, which align the output\nspace with language without seeing a word, can serve as efficient vision\nencoders in multimodal large language models. Code and models are available at\nhttps://github.com/showlab/sparseformer\n", "link": "http://arxiv.org/abs/2312.01987v2", "date": "2024-04-04", "relevancy": 2.1359, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5473}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5382}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5245}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20SparseFormers%20from%20Vision%20Foundation%20Models&body=Title%3A%20Bootstrapping%20SparseFormers%20from%20Vision%20Foundation%20Models%0AAuthor%3A%20Ziteng%20Gao%20and%20Zhan%20Tong%20and%20Kevin%20Qinghong%20Lin%20and%20Joya%20Chen%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20The%20recently%20proposed%20SparseFormer%20architecture%20provides%20an%20alternative%0Aapproach%20to%20visual%20understanding%20by%20utilizing%20a%20significantly%20lower%20number%20of%0Avisual%20tokens%20via%20adjusting%20RoIs%2C%20greatly%20reducing%20computational%20costs%20while%0Astill%20achieving%20promising%20performance.%20However%2C%20training%20SparseFormers%20from%0Ascratch%20is%20still%20expensive%2C%20and%20scaling%20up%20the%20number%20of%20parameters%20can%20be%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20to%20bootstrap%20SparseFormers%20from%0AViT-based%20vision%20foundation%20models%20in%20a%20simple%20and%20efficient%20way.%20Since%20the%0Amajority%20of%20SparseFormer%20blocks%20are%20the%20standard%20transformer%20ones%2C%20we%20can%0Ainherit%20weights%20from%20large-scale%20pre-trained%20vision%20transformers%20and%20freeze%0Athem%20as%20much%20as%20possible.%20Therefore%2C%20we%20only%20need%20to%20train%20the%0ASparseFormer-specific%20lightweight%20focusing%20transformer%20to%20adjust%20token%20RoIs%20and%0Afine-tune%20a%20few%20early%20pre-trained%20blocks%20to%20align%20the%20final%20token%0Arepresentation.%20In%20such%20a%20way%2C%20we%20can%20bootstrap%20SparseFormer%20architectures%20from%0Avarious%20large-scale%20pre-trained%20models%20%28e.g.%2C%20IN-21K%20pre-trained%20AugRegs%20or%0ACLIPs%29%20using%20a%20rather%20smaller%20amount%20of%20training%20samples%20%28e.g.%2C%20IN-1K%29%20and%0Awithout%20labels%20or%20captions%20within%20just%20a%20few%20hours.%20As%20a%20result%2C%20the%0Abootstrapped%20unimodal%20SparseFormer%20%28from%20AugReg-ViT-L/16-384%29%20can%20reach%2084.9%25%0Aaccuracy%20on%20IN-1K%20with%20only%2049%20tokens%2C%20and%20the%20multimodal%20SparseFormer%20from%0ACLIPs%20also%20demonstrates%20notable%20zero-shot%20performance%20with%20highly%20reduced%0Acomputational%20cost%20without%20seeing%20any%20caption%20during%20the%20bootstrapping%0Aprocedure.%20In%20addition%2C%20CLIP-bootstrapped%20SparseFormers%2C%20which%20align%20the%20output%0Aspace%20with%20language%20without%20seeing%20a%20word%2C%20can%20serve%20as%20efficient%20vision%0Aencoders%20in%20multimodal%20large%20language%20models.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/showlab/sparseformer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01987v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20SparseFormers%20from%20Vision%20Foundation%20Models&entry.906535625=Ziteng%20Gao%20and%20Zhan%20Tong%20and%20Kevin%20Qinghong%20Lin%20and%20Joya%20Chen%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20The%20recently%20proposed%20SparseFormer%20architecture%20provides%20an%20alternative%0Aapproach%20to%20visual%20understanding%20by%20utilizing%20a%20significantly%20lower%20number%20of%0Avisual%20tokens%20via%20adjusting%20RoIs%2C%20greatly%20reducing%20computational%20costs%20while%0Astill%20achieving%20promising%20performance.%20However%2C%20training%20SparseFormers%20from%0Ascratch%20is%20still%20expensive%2C%20and%20scaling%20up%20the%20number%20of%20parameters%20can%20be%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20to%20bootstrap%20SparseFormers%20from%0AViT-based%20vision%20foundation%20models%20in%20a%20simple%20and%20efficient%20way.%20Since%20the%0Amajority%20of%20SparseFormer%20blocks%20are%20the%20standard%20transformer%20ones%2C%20we%20can%0Ainherit%20weights%20from%20large-scale%20pre-trained%20vision%20transformers%20and%20freeze%0Athem%20as%20much%20as%20possible.%20Therefore%2C%20we%20only%20need%20to%20train%20the%0ASparseFormer-specific%20lightweight%20focusing%20transformer%20to%20adjust%20token%20RoIs%20and%0Afine-tune%20a%20few%20early%20pre-trained%20blocks%20to%20align%20the%20final%20token%0Arepresentation.%20In%20such%20a%20way%2C%20we%20can%20bootstrap%20SparseFormer%20architectures%20from%0Avarious%20large-scale%20pre-trained%20models%20%28e.g.%2C%20IN-21K%20pre-trained%20AugRegs%20or%0ACLIPs%29%20using%20a%20rather%20smaller%20amount%20of%20training%20samples%20%28e.g.%2C%20IN-1K%29%20and%0Awithout%20labels%20or%20captions%20within%20just%20a%20few%20hours.%20As%20a%20result%2C%20the%0Abootstrapped%20unimodal%20SparseFormer%20%28from%20AugReg-ViT-L/16-384%29%20can%20reach%2084.9%25%0Aaccuracy%20on%20IN-1K%20with%20only%2049%20tokens%2C%20and%20the%20multimodal%20SparseFormer%20from%0ACLIPs%20also%20demonstrates%20notable%20zero-shot%20performance%20with%20highly%20reduced%0Acomputational%20cost%20without%20seeing%20any%20caption%20during%20the%20bootstrapping%0Aprocedure.%20In%20addition%2C%20CLIP-bootstrapped%20SparseFormers%2C%20which%20align%20the%20output%0Aspace%20with%20language%20without%20seeing%20a%20word%2C%20can%20serve%20as%20efficient%20vision%0Aencoders%20in%20multimodal%20large%20language%20models.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/showlab/sparseformer%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01987v2&entry.124074799=Read"},
{"title": "DIDA: Denoised Imitation Learning based on Domain Adaptation", "author": "Kaichen Huang and Hai-Hang Sun and Shenghua Wan and Minghao Shao and Shuai Feng and Le Gan and De-Chuan Zhan", "abstract": "  Imitating skills from low-quality datasets, such as sub-optimal\ndemonstrations and observations with distractors, is common in real-world\napplications. In this work, we focus on the problem of Learning from Noisy\nDemonstrations (LND), where the imitator is required to learn from data with\nnoise that often occurs during the processes of data collection or\ntransmission. Previous IL methods improve the robustness of learned policies by\ninjecting an adversarially learned Gaussian noise into pure expert data or\nutilizing additional ranking information, but they may fail in the LND setting.\nTo alleviate the above problems, we propose Denoised Imitation learning based\non Domain Adaptation (DIDA), which designs two discriminators to distinguish\nthe noise level and expertise level of data, facilitating a feature encoder to\nlearn task-related but domain-agnostic representations. Experiment results on\nMuJoCo demonstrate that DIDA can successfully handle challenging imitation\ntasks from demonstrations with various types of noise, outperforming most\nbaseline methods.\n", "link": "http://arxiv.org/abs/2404.03382v1", "date": "2024-04-04", "relevancy": 2.1346, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5602}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5333}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DIDA%3A%20Denoised%20Imitation%20Learning%20based%20on%20Domain%20Adaptation&body=Title%3A%20DIDA%3A%20Denoised%20Imitation%20Learning%20based%20on%20Domain%20Adaptation%0AAuthor%3A%20Kaichen%20Huang%20and%20Hai-Hang%20Sun%20and%20Shenghua%20Wan%20and%20Minghao%20Shao%20and%20Shuai%20Feng%20and%20Le%20Gan%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20Imitating%20skills%20from%20low-quality%20datasets%2C%20such%20as%20sub-optimal%0Ademonstrations%20and%20observations%20with%20distractors%2C%20is%20common%20in%20real-world%0Aapplications.%20In%20this%20work%2C%20we%20focus%20on%20the%20problem%20of%20Learning%20from%20Noisy%0ADemonstrations%20%28LND%29%2C%20where%20the%20imitator%20is%20required%20to%20learn%20from%20data%20with%0Anoise%20that%20often%20occurs%20during%20the%20processes%20of%20data%20collection%20or%0Atransmission.%20Previous%20IL%20methods%20improve%20the%20robustness%20of%20learned%20policies%20by%0Ainjecting%20an%20adversarially%20learned%20Gaussian%20noise%20into%20pure%20expert%20data%20or%0Autilizing%20additional%20ranking%20information%2C%20but%20they%20may%20fail%20in%20the%20LND%20setting.%0ATo%20alleviate%20the%20above%20problems%2C%20we%20propose%20Denoised%20Imitation%20learning%20based%0Aon%20Domain%20Adaptation%20%28DIDA%29%2C%20which%20designs%20two%20discriminators%20to%20distinguish%0Athe%20noise%20level%20and%20expertise%20level%20of%20data%2C%20facilitating%20a%20feature%20encoder%20to%0Alearn%20task-related%20but%20domain-agnostic%20representations.%20Experiment%20results%20on%0AMuJoCo%20demonstrate%20that%20DIDA%20can%20successfully%20handle%20challenging%20imitation%0Atasks%20from%20demonstrations%20with%20various%20types%20of%20noise%2C%20outperforming%20most%0Abaseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03382v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIDA%3A%20Denoised%20Imitation%20Learning%20based%20on%20Domain%20Adaptation&entry.906535625=Kaichen%20Huang%20and%20Hai-Hang%20Sun%20and%20Shenghua%20Wan%20and%20Minghao%20Shao%20and%20Shuai%20Feng%20and%20Le%20Gan%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20Imitating%20skills%20from%20low-quality%20datasets%2C%20such%20as%20sub-optimal%0Ademonstrations%20and%20observations%20with%20distractors%2C%20is%20common%20in%20real-world%0Aapplications.%20In%20this%20work%2C%20we%20focus%20on%20the%20problem%20of%20Learning%20from%20Noisy%0ADemonstrations%20%28LND%29%2C%20where%20the%20imitator%20is%20required%20to%20learn%20from%20data%20with%0Anoise%20that%20often%20occurs%20during%20the%20processes%20of%20data%20collection%20or%0Atransmission.%20Previous%20IL%20methods%20improve%20the%20robustness%20of%20learned%20policies%20by%0Ainjecting%20an%20adversarially%20learned%20Gaussian%20noise%20into%20pure%20expert%20data%20or%0Autilizing%20additional%20ranking%20information%2C%20but%20they%20may%20fail%20in%20the%20LND%20setting.%0ATo%20alleviate%20the%20above%20problems%2C%20we%20propose%20Denoised%20Imitation%20learning%20based%0Aon%20Domain%20Adaptation%20%28DIDA%29%2C%20which%20designs%20two%20discriminators%20to%20distinguish%0Athe%20noise%20level%20and%20expertise%20level%20of%20data%2C%20facilitating%20a%20feature%20encoder%20to%0Alearn%20task-related%20but%20domain-agnostic%20representations.%20Experiment%20results%20on%0AMuJoCo%20demonstrate%20that%20DIDA%20can%20successfully%20handle%20challenging%20imitation%0Atasks%20from%20demonstrations%20with%20various%20types%20of%20noise%2C%20outperforming%20most%0Abaseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03382v1&entry.124074799=Read"},
{"title": "Factored Task and Motion Planning with Combined Optimization, Sampling\n  and Learning", "author": "Joaquim Ortiz-Haro", "abstract": "  In this thesis, we aim to improve the performance of TAMP algorithms from\nthree complementary perspectives. First, we investigate the integration of\ndiscrete task planning with continuous trajectory optimization. Our main\ncontribution is a conflict-based solver that automatically discovers why a task\nplan might fail when considering the constraints of the physical world. This\ninformation is then fed back into the task planner, resulting in an efficient,\nbidirectional, and intuitive interface between task and motion, capable of\nsolving TAMP problems with multiple objects, robots, and tight physical\nconstraints. In the second part, we first illustrate that, given the wide range\nof tasks and environments within TAMP, neither sampling nor optimization is\nsuperior in all settings. To combine the strengths of both approaches, we have\ndesigned meta-solvers for TAMP, adaptive solvers that automatically select\nwhich algorithms and computations to use and how to best decompose each problem\nto find a solution faster. In the third part, we combine deep learning\narchitectures with model-based reasoning to accelerate computations within our\nTAMP solver. Specifically, we target infeasibility detection and nonlinear\noptimization, focusing on generalization, accuracy, compute time, and data\nefficiency. At the core of our contributions is a refined, factored\nrepresentation of the trajectory optimization problems inside TAMP. This\nstructure not only facilitates more efficient planning, encoding of geometric\ninfeasibility, and meta-reasoning but also provides better generalization in\nneural architectures.\n", "link": "http://arxiv.org/abs/2404.03567v1", "date": "2024-04-04", "relevancy": 2.1313, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5413}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5339}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5239}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Factored%20Task%20and%20Motion%20Planning%20with%20Combined%20Optimization%2C%20Sampling%0A%20%20and%20Learning&body=Title%3A%20Factored%20Task%20and%20Motion%20Planning%20with%20Combined%20Optimization%2C%20Sampling%0A%20%20and%20Learning%0AAuthor%3A%20Joaquim%20Ortiz-Haro%0AAbstract%3A%20%20%20In%20this%20thesis%2C%20we%20aim%20to%20improve%20the%20performance%20of%20TAMP%20algorithms%20from%0Athree%20complementary%20perspectives.%20First%2C%20we%20investigate%20the%20integration%20of%0Adiscrete%20task%20planning%20with%20continuous%20trajectory%20optimization.%20Our%20main%0Acontribution%20is%20a%20conflict-based%20solver%20that%20automatically%20discovers%20why%20a%20task%0Aplan%20might%20fail%20when%20considering%20the%20constraints%20of%20the%20physical%20world.%20This%0Ainformation%20is%20then%20fed%20back%20into%20the%20task%20planner%2C%20resulting%20in%20an%20efficient%2C%0Abidirectional%2C%20and%20intuitive%20interface%20between%20task%20and%20motion%2C%20capable%20of%0Asolving%20TAMP%20problems%20with%20multiple%20objects%2C%20robots%2C%20and%20tight%20physical%0Aconstraints.%20In%20the%20second%20part%2C%20we%20first%20illustrate%20that%2C%20given%20the%20wide%20range%0Aof%20tasks%20and%20environments%20within%20TAMP%2C%20neither%20sampling%20nor%20optimization%20is%0Asuperior%20in%20all%20settings.%20To%20combine%20the%20strengths%20of%20both%20approaches%2C%20we%20have%0Adesigned%20meta-solvers%20for%20TAMP%2C%20adaptive%20solvers%20that%20automatically%20select%0Awhich%20algorithms%20and%20computations%20to%20use%20and%20how%20to%20best%20decompose%20each%20problem%0Ato%20find%20a%20solution%20faster.%20In%20the%20third%20part%2C%20we%20combine%20deep%20learning%0Aarchitectures%20with%20model-based%20reasoning%20to%20accelerate%20computations%20within%20our%0ATAMP%20solver.%20Specifically%2C%20we%20target%20infeasibility%20detection%20and%20nonlinear%0Aoptimization%2C%20focusing%20on%20generalization%2C%20accuracy%2C%20compute%20time%2C%20and%20data%0Aefficiency.%20At%20the%20core%20of%20our%20contributions%20is%20a%20refined%2C%20factored%0Arepresentation%20of%20the%20trajectory%20optimization%20problems%20inside%20TAMP.%20This%0Astructure%20not%20only%20facilitates%20more%20efficient%20planning%2C%20encoding%20of%20geometric%0Ainfeasibility%2C%20and%20meta-reasoning%20but%20also%20provides%20better%20generalization%20in%0Aneural%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03567v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factored%20Task%20and%20Motion%20Planning%20with%20Combined%20Optimization%2C%20Sampling%0A%20%20and%20Learning&entry.906535625=Joaquim%20Ortiz-Haro&entry.1292438233=%20%20In%20this%20thesis%2C%20we%20aim%20to%20improve%20the%20performance%20of%20TAMP%20algorithms%20from%0Athree%20complementary%20perspectives.%20First%2C%20we%20investigate%20the%20integration%20of%0Adiscrete%20task%20planning%20with%20continuous%20trajectory%20optimization.%20Our%20main%0Acontribution%20is%20a%20conflict-based%20solver%20that%20automatically%20discovers%20why%20a%20task%0Aplan%20might%20fail%20when%20considering%20the%20constraints%20of%20the%20physical%20world.%20This%0Ainformation%20is%20then%20fed%20back%20into%20the%20task%20planner%2C%20resulting%20in%20an%20efficient%2C%0Abidirectional%2C%20and%20intuitive%20interface%20between%20task%20and%20motion%2C%20capable%20of%0Asolving%20TAMP%20problems%20with%20multiple%20objects%2C%20robots%2C%20and%20tight%20physical%0Aconstraints.%20In%20the%20second%20part%2C%20we%20first%20illustrate%20that%2C%20given%20the%20wide%20range%0Aof%20tasks%20and%20environments%20within%20TAMP%2C%20neither%20sampling%20nor%20optimization%20is%0Asuperior%20in%20all%20settings.%20To%20combine%20the%20strengths%20of%20both%20approaches%2C%20we%20have%0Adesigned%20meta-solvers%20for%20TAMP%2C%20adaptive%20solvers%20that%20automatically%20select%0Awhich%20algorithms%20and%20computations%20to%20use%20and%20how%20to%20best%20decompose%20each%20problem%0Ato%20find%20a%20solution%20faster.%20In%20the%20third%20part%2C%20we%20combine%20deep%20learning%0Aarchitectures%20with%20model-based%20reasoning%20to%20accelerate%20computations%20within%20our%0ATAMP%20solver.%20Specifically%2C%20we%20target%20infeasibility%20detection%20and%20nonlinear%0Aoptimization%2C%20focusing%20on%20generalization%2C%20accuracy%2C%20compute%20time%2C%20and%20data%0Aefficiency.%20At%20the%20core%20of%20our%20contributions%20is%20a%20refined%2C%20factored%0Arepresentation%20of%20the%20trajectory%20optimization%20problems%20inside%20TAMP.%20This%0Astructure%20not%20only%20facilitates%20more%20efficient%20planning%2C%20encoding%20of%20geometric%0Ainfeasibility%2C%20and%20meta-reasoning%20but%20also%20provides%20better%20generalization%20in%0Aneural%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03567v1&entry.124074799=Read"},
{"title": "AI and the Problem of Knowledge Collapse", "author": "Andrew J. Peterson", "abstract": "  While artificial intelligence has the potential to process vast amounts of\ndata, generate new insights, and unlock greater productivity, its widespread\nadoption may entail unforeseen consequences. We identify conditions under which\nAI, by reducing the cost of access to certain modes of knowledge, can\nparadoxically harm public understanding. While large language models are\ntrained on vast amounts of diverse data, they naturally generate output towards\nthe 'center' of the distribution. This is generally useful, but widespread\nreliance on recursive AI systems could lead to a process we define as\n\"knowledge collapse\", and argue this could harm innovation and the richness of\nhuman understanding and culture. However, unlike AI models that cannot choose\nwhat data they are trained on, humans may strategically seek out diverse forms\nof knowledge if they perceive them to be worthwhile. To investigate this, we\nprovide a simple model in which a community of learners or innovators choose to\nuse traditional methods or to rely on a discounted AI-assisted process and\nidentify conditions under which knowledge collapse occurs. In our default\nmodel, a 20% discount on AI-generated content generates public beliefs 2.3\ntimes further from the truth than when there is no discount. Finally, based on\nthe results, we consider further research directions to counteract such\noutcomes.\n", "link": "http://arxiv.org/abs/2404.03502v1", "date": "2024-04-04", "relevancy": 2.108, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4435}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4143}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.407}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AI%20and%20the%20Problem%20of%20Knowledge%20Collapse&body=Title%3A%20AI%20and%20the%20Problem%20of%20Knowledge%20Collapse%0AAuthor%3A%20Andrew%20J.%20Peterson%0AAbstract%3A%20%20%20While%20artificial%20intelligence%20has%20the%20potential%20to%20process%20vast%20amounts%20of%0Adata%2C%20generate%20new%20insights%2C%20and%20unlock%20greater%20productivity%2C%20its%20widespread%0Aadoption%20may%20entail%20unforeseen%20consequences.%20We%20identify%20conditions%20under%20which%0AAI%2C%20by%20reducing%20the%20cost%20of%20access%20to%20certain%20modes%20of%20knowledge%2C%20can%0Aparadoxically%20harm%20public%20understanding.%20While%20large%20language%20models%20are%0Atrained%20on%20vast%20amounts%20of%20diverse%20data%2C%20they%20naturally%20generate%20output%20towards%0Athe%20%27center%27%20of%20the%20distribution.%20This%20is%20generally%20useful%2C%20but%20widespread%0Areliance%20on%20recursive%20AI%20systems%20could%20lead%20to%20a%20process%20we%20define%20as%0A%22knowledge%20collapse%22%2C%20and%20argue%20this%20could%20harm%20innovation%20and%20the%20richness%20of%0Ahuman%20understanding%20and%20culture.%20However%2C%20unlike%20AI%20models%20that%20cannot%20choose%0Awhat%20data%20they%20are%20trained%20on%2C%20humans%20may%20strategically%20seek%20out%20diverse%20forms%0Aof%20knowledge%20if%20they%20perceive%20them%20to%20be%20worthwhile.%20To%20investigate%20this%2C%20we%0Aprovide%20a%20simple%20model%20in%20which%20a%20community%20of%20learners%20or%20innovators%20choose%20to%0Ause%20traditional%20methods%20or%20to%20rely%20on%20a%20discounted%20AI-assisted%20process%20and%0Aidentify%20conditions%20under%20which%20knowledge%20collapse%20occurs.%20In%20our%20default%0Amodel%2C%20a%2020%25%20discount%20on%20AI-generated%20content%20generates%20public%20beliefs%202.3%0Atimes%20further%20from%20the%20truth%20than%20when%20there%20is%20no%20discount.%20Finally%2C%20based%20on%0Athe%20results%2C%20we%20consider%20further%20research%20directions%20to%20counteract%20such%0Aoutcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03502v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20and%20the%20Problem%20of%20Knowledge%20Collapse&entry.906535625=Andrew%20J.%20Peterson&entry.1292438233=%20%20While%20artificial%20intelligence%20has%20the%20potential%20to%20process%20vast%20amounts%20of%0Adata%2C%20generate%20new%20insights%2C%20and%20unlock%20greater%20productivity%2C%20its%20widespread%0Aadoption%20may%20entail%20unforeseen%20consequences.%20We%20identify%20conditions%20under%20which%0AAI%2C%20by%20reducing%20the%20cost%20of%20access%20to%20certain%20modes%20of%20knowledge%2C%20can%0Aparadoxically%20harm%20public%20understanding.%20While%20large%20language%20models%20are%0Atrained%20on%20vast%20amounts%20of%20diverse%20data%2C%20they%20naturally%20generate%20output%20towards%0Athe%20%27center%27%20of%20the%20distribution.%20This%20is%20generally%20useful%2C%20but%20widespread%0Areliance%20on%20recursive%20AI%20systems%20could%20lead%20to%20a%20process%20we%20define%20as%0A%22knowledge%20collapse%22%2C%20and%20argue%20this%20could%20harm%20innovation%20and%20the%20richness%20of%0Ahuman%20understanding%20and%20culture.%20However%2C%20unlike%20AI%20models%20that%20cannot%20choose%0Awhat%20data%20they%20are%20trained%20on%2C%20humans%20may%20strategically%20seek%20out%20diverse%20forms%0Aof%20knowledge%20if%20they%20perceive%20them%20to%20be%20worthwhile.%20To%20investigate%20this%2C%20we%0Aprovide%20a%20simple%20model%20in%20which%20a%20community%20of%20learners%20or%20innovators%20choose%20to%0Ause%20traditional%20methods%20or%20to%20rely%20on%20a%20discounted%20AI-assisted%20process%20and%0Aidentify%20conditions%20under%20which%20knowledge%20collapse%20occurs.%20In%20our%20default%0Amodel%2C%20a%2020%25%20discount%20on%20AI-generated%20content%20generates%20public%20beliefs%202.3%0Atimes%20further%20from%20the%20truth%20than%20when%20there%20is%20no%20discount.%20Finally%2C%20based%20on%0Athe%20results%2C%20we%20consider%20further%20research%20directions%20to%20counteract%20such%0Aoutcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03502v1&entry.124074799=Read"},
{"title": "DQ-DETR: DETR with Dynamic Query for Tiny Object Detection", "author": "Yi-Xin Huang and Hou-I Liu and Hong-Han Shuai and Wen-Huang Cheng", "abstract": "  Despite previous DETR-like methods having performed successfully in generic\nobject detection, tiny object detection is still a challenging task for them\nsince the positional information of object queries is not customized for\ndetecting tiny objects, whose scale is extraordinarily smaller than general\nobjects. Also, DETR-like methods using a fixed number of queries make them\nunsuitable for aerial datasets, which only contain tiny objects, and the\nnumbers of instances are imbalanced between different images. Thus, we present\na simple yet effective model, named DQ-DETR, which consists of three different\ncomponents: categorical counting module, counting-guided feature enhancement,\nand dynamic query selection to solve the above-mentioned problems. DQ-DETR uses\nthe prediction and density maps from the categorical counting module to\ndynamically adjust the number of object queries and improve the positional\ninformation of queries. Our model DQ-DETR outperforms previous CNN-based and\nDETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2\ndataset, which mostly consists of tiny objects.\n", "link": "http://arxiv.org/abs/2404.03507v1", "date": "2024-04-04", "relevancy": 2.1068, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5372}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5231}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DQ-DETR%3A%20DETR%20with%20Dynamic%20Query%20for%20Tiny%20Object%20Detection&body=Title%3A%20DQ-DETR%3A%20DETR%20with%20Dynamic%20Query%20for%20Tiny%20Object%20Detection%0AAuthor%3A%20Yi-Xin%20Huang%20and%20Hou-I%20Liu%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng%0AAbstract%3A%20%20%20Despite%20previous%20DETR-like%20methods%20having%20performed%20successfully%20in%20generic%0Aobject%20detection%2C%20tiny%20object%20detection%20is%20still%20a%20challenging%20task%20for%20them%0Asince%20the%20positional%20information%20of%20object%20queries%20is%20not%20customized%20for%0Adetecting%20tiny%20objects%2C%20whose%20scale%20is%20extraordinarily%20smaller%20than%20general%0Aobjects.%20Also%2C%20DETR-like%20methods%20using%20a%20fixed%20number%20of%20queries%20make%20them%0Aunsuitable%20for%20aerial%20datasets%2C%20which%20only%20contain%20tiny%20objects%2C%20and%20the%0Anumbers%20of%20instances%20are%20imbalanced%20between%20different%20images.%20Thus%2C%20we%20present%0Aa%20simple%20yet%20effective%20model%2C%20named%20DQ-DETR%2C%20which%20consists%20of%20three%20different%0Acomponents%3A%20categorical%20counting%20module%2C%20counting-guided%20feature%20enhancement%2C%0Aand%20dynamic%20query%20selection%20to%20solve%20the%20above-mentioned%20problems.%20DQ-DETR%20uses%0Athe%20prediction%20and%20density%20maps%20from%20the%20categorical%20counting%20module%20to%0Adynamically%20adjust%20the%20number%20of%20object%20queries%20and%20improve%20the%20positional%0Ainformation%20of%20queries.%20Our%20model%20DQ-DETR%20outperforms%20previous%20CNN-based%20and%0ADETR-like%20methods%2C%20achieving%20state-of-the-art%20mAP%2030.2%25%20on%20the%20AI-TOD-V2%0Adataset%2C%20which%20mostly%20consists%20of%20tiny%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03507v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DQ-DETR%3A%20DETR%20with%20Dynamic%20Query%20for%20Tiny%20Object%20Detection&entry.906535625=Yi-Xin%20Huang%20and%20Hou-I%20Liu%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng&entry.1292438233=%20%20Despite%20previous%20DETR-like%20methods%20having%20performed%20successfully%20in%20generic%0Aobject%20detection%2C%20tiny%20object%20detection%20is%20still%20a%20challenging%20task%20for%20them%0Asince%20the%20positional%20information%20of%20object%20queries%20is%20not%20customized%20for%0Adetecting%20tiny%20objects%2C%20whose%20scale%20is%20extraordinarily%20smaller%20than%20general%0Aobjects.%20Also%2C%20DETR-like%20methods%20using%20a%20fixed%20number%20of%20queries%20make%20them%0Aunsuitable%20for%20aerial%20datasets%2C%20which%20only%20contain%20tiny%20objects%2C%20and%20the%0Anumbers%20of%20instances%20are%20imbalanced%20between%20different%20images.%20Thus%2C%20we%20present%0Aa%20simple%20yet%20effective%20model%2C%20named%20DQ-DETR%2C%20which%20consists%20of%20three%20different%0Acomponents%3A%20categorical%20counting%20module%2C%20counting-guided%20feature%20enhancement%2C%0Aand%20dynamic%20query%20selection%20to%20solve%20the%20above-mentioned%20problems.%20DQ-DETR%20uses%0Athe%20prediction%20and%20density%20maps%20from%20the%20categorical%20counting%20module%20to%0Adynamically%20adjust%20the%20number%20of%20object%20queries%20and%20improve%20the%20positional%0Ainformation%20of%20queries.%20Our%20model%20DQ-DETR%20outperforms%20previous%20CNN-based%20and%0ADETR-like%20methods%2C%20achieving%20state-of-the-art%20mAP%2030.2%25%20on%20the%20AI-TOD-V2%0Adataset%2C%20which%20mostly%20consists%20of%20tiny%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03507v1&entry.124074799=Read"},
{"title": "Towards Fine-grained Large Object Segmentation 1st Place Solution to 3D\n  AI Challenge 2020 -- Instance Segmentation Track", "author": "Zehui Chen and Qiaofei Li and Feng Zhao", "abstract": "  This technical report introduces our solutions of Team 'FineGrainedSeg' for\nInstance Segmentation track in 3D AI Challenge 2020. In order to handle\nextremely large objects in 3D-FUTURE, we adopt PointRend as our basic\nframework, which outputs more fine-grained masks compared to HTC and SOLOv2.\nOur final submission is an ensemble of 5 PointRend models, which achieves the\n1st place on both validation and test leaderboards. The code is available at\nhttps://github.com/zehuichen123/3DFuture_ins_seg.\n", "link": "http://arxiv.org/abs/2009.04650v2", "date": "2024-04-04", "relevancy": 2.0974, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5538}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5133}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4993}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Fine-grained%20Large%20Object%20Segmentation%201st%20Place%20Solution%20to%203D%0A%20%20AI%20Challenge%202020%20--%20Instance%20Segmentation%20Track&body=Title%3A%20Towards%20Fine-grained%20Large%20Object%20Segmentation%201st%20Place%20Solution%20to%203D%0A%20%20AI%20Challenge%202020%20--%20Instance%20Segmentation%20Track%0AAuthor%3A%20Zehui%20Chen%20and%20Qiaofei%20Li%20and%20Feng%20Zhao%0AAbstract%3A%20%20%20This%20technical%20report%20introduces%20our%20solutions%20of%20Team%20%27FineGrainedSeg%27%20for%0AInstance%20Segmentation%20track%20in%203D%20AI%20Challenge%202020.%20In%20order%20to%20handle%0Aextremely%20large%20objects%20in%203D-FUTURE%2C%20we%20adopt%20PointRend%20as%20our%20basic%0Aframework%2C%20which%20outputs%20more%20fine-grained%20masks%20compared%20to%20HTC%20and%20SOLOv2.%0AOur%20final%20submission%20is%20an%20ensemble%20of%205%20PointRend%20models%2C%20which%20achieves%20the%0A1st%20place%20on%20both%20validation%20and%20test%20leaderboards.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zehuichen123/3DFuture_ins_seg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2009.04650v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fine-grained%20Large%20Object%20Segmentation%201st%20Place%20Solution%20to%203D%0A%20%20AI%20Challenge%202020%20--%20Instance%20Segmentation%20Track&entry.906535625=Zehui%20Chen%20and%20Qiaofei%20Li%20and%20Feng%20Zhao&entry.1292438233=%20%20This%20technical%20report%20introduces%20our%20solutions%20of%20Team%20%27FineGrainedSeg%27%20for%0AInstance%20Segmentation%20track%20in%203D%20AI%20Challenge%202020.%20In%20order%20to%20handle%0Aextremely%20large%20objects%20in%203D-FUTURE%2C%20we%20adopt%20PointRend%20as%20our%20basic%0Aframework%2C%20which%20outputs%20more%20fine-grained%20masks%20compared%20to%20HTC%20and%20SOLOv2.%0AOur%20final%20submission%20is%20an%20ensemble%20of%205%20PointRend%20models%2C%20which%20achieves%20the%0A1st%20place%20on%20both%20validation%20and%20test%20leaderboards.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zehuichen123/3DFuture_ins_seg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2009.04650v2&entry.124074799=Read"},
{"title": "SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for\n  Imbalanced Clustering", "author": "Chuyu Zhang and Hui Ren and Xuming He", "abstract": "  Deep clustering, which learns representation and semantic clustering without\nlabels information, poses a great challenge for deep learning-based approaches.\nDespite significant progress in recent years, most existing methods focus on\nuniformly distributed datasets, significantly limiting the practical\napplicability of their methods. In this paper, we propose a more practical\nproblem setting named deep imbalanced clustering, where the underlying classes\nexhibit an imbalance distribution. To address this challenge, we introduce a\nnovel optimal transport-based pseudo-label learning framework. Our framework\nformulates pseudo-label generation as a Semantic-regularized Progressive\nPartial Optimal Transport (SP$^2$OT) problem, which progressively transports\neach sample to imbalanced clusters under several prior distribution and\nsemantic relation constraints, thus generating high-quality and imbalance-aware\npseudo-labels. To solve SP$^2$OT, we develop a Majorization-Minimization-based\noptimization algorithm. To be more precise, we employ the strategy of\nmajorization to reformulate the SP$^2$OT problem into a Progressive Partial\nOptimal Transport problem, which can be transformed into an unbalanced optimal\ntransport problem with augmented constraints and can be solved efficiently by a\nfast matrix scaling algorithm. Experiments on various datasets, including a\nhuman-curated long-tailed CIFAR100, challenging ImageNet-R, and large-scale\nsubsets of fine-grained iNaturalist2018 datasets, demonstrate the superiority\nof our method.\n", "link": "http://arxiv.org/abs/2404.03446v1", "date": "2024-04-04", "relevancy": 2.0972, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5278}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5199}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SP%24%5E2%24OT%3A%20Semantic-Regularized%20Progressive%20Partial%20Optimal%20Transport%20for%0A%20%20Imbalanced%20Clustering&body=Title%3A%20SP%24%5E2%24OT%3A%20Semantic-Regularized%20Progressive%20Partial%20Optimal%20Transport%20for%0A%20%20Imbalanced%20Clustering%0AAuthor%3A%20Chuyu%20Zhang%20and%20Hui%20Ren%20and%20Xuming%20He%0AAbstract%3A%20%20%20Deep%20clustering%2C%20which%20learns%20representation%20and%20semantic%20clustering%20without%0Alabels%20information%2C%20poses%20a%20great%20challenge%20for%20deep%20learning-based%20approaches.%0ADespite%20significant%20progress%20in%20recent%20years%2C%20most%20existing%20methods%20focus%20on%0Auniformly%20distributed%20datasets%2C%20significantly%20limiting%20the%20practical%0Aapplicability%20of%20their%20methods.%20In%20this%20paper%2C%20we%20propose%20a%20more%20practical%0Aproblem%20setting%20named%20deep%20imbalanced%20clustering%2C%20where%20the%20underlying%20classes%0Aexhibit%20an%20imbalance%20distribution.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Anovel%20optimal%20transport-based%20pseudo-label%20learning%20framework.%20Our%20framework%0Aformulates%20pseudo-label%20generation%20as%20a%20Semantic-regularized%20Progressive%0APartial%20Optimal%20Transport%20%28SP%24%5E2%24OT%29%20problem%2C%20which%20progressively%20transports%0Aeach%20sample%20to%20imbalanced%20clusters%20under%20several%20prior%20distribution%20and%0Asemantic%20relation%20constraints%2C%20thus%20generating%20high-quality%20and%20imbalance-aware%0Apseudo-labels.%20To%20solve%20SP%24%5E2%24OT%2C%20we%20develop%20a%20Majorization-Minimization-based%0Aoptimization%20algorithm.%20To%20be%20more%20precise%2C%20we%20employ%20the%20strategy%20of%0Amajorization%20to%20reformulate%20the%20SP%24%5E2%24OT%20problem%20into%20a%20Progressive%20Partial%0AOptimal%20Transport%20problem%2C%20which%20can%20be%20transformed%20into%20an%20unbalanced%20optimal%0Atransport%20problem%20with%20augmented%20constraints%20and%20can%20be%20solved%20efficiently%20by%20a%0Afast%20matrix%20scaling%20algorithm.%20Experiments%20on%20various%20datasets%2C%20including%20a%0Ahuman-curated%20long-tailed%20CIFAR100%2C%20challenging%20ImageNet-R%2C%20and%20large-scale%0Asubsets%20of%20fine-grained%20iNaturalist2018%20datasets%2C%20demonstrate%20the%20superiority%0Aof%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03446v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SP%24%5E2%24OT%3A%20Semantic-Regularized%20Progressive%20Partial%20Optimal%20Transport%20for%0A%20%20Imbalanced%20Clustering&entry.906535625=Chuyu%20Zhang%20and%20Hui%20Ren%20and%20Xuming%20He&entry.1292438233=%20%20Deep%20clustering%2C%20which%20learns%20representation%20and%20semantic%20clustering%20without%0Alabels%20information%2C%20poses%20a%20great%20challenge%20for%20deep%20learning-based%20approaches.%0ADespite%20significant%20progress%20in%20recent%20years%2C%20most%20existing%20methods%20focus%20on%0Auniformly%20distributed%20datasets%2C%20significantly%20limiting%20the%20practical%0Aapplicability%20of%20their%20methods.%20In%20this%20paper%2C%20we%20propose%20a%20more%20practical%0Aproblem%20setting%20named%20deep%20imbalanced%20clustering%2C%20where%20the%20underlying%20classes%0Aexhibit%20an%20imbalance%20distribution.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Anovel%20optimal%20transport-based%20pseudo-label%20learning%20framework.%20Our%20framework%0Aformulates%20pseudo-label%20generation%20as%20a%20Semantic-regularized%20Progressive%0APartial%20Optimal%20Transport%20%28SP%24%5E2%24OT%29%20problem%2C%20which%20progressively%20transports%0Aeach%20sample%20to%20imbalanced%20clusters%20under%20several%20prior%20distribution%20and%0Asemantic%20relation%20constraints%2C%20thus%20generating%20high-quality%20and%20imbalance-aware%0Apseudo-labels.%20To%20solve%20SP%24%5E2%24OT%2C%20we%20develop%20a%20Majorization-Minimization-based%0Aoptimization%20algorithm.%20To%20be%20more%20precise%2C%20we%20employ%20the%20strategy%20of%0Amajorization%20to%20reformulate%20the%20SP%24%5E2%24OT%20problem%20into%20a%20Progressive%20Partial%0AOptimal%20Transport%20problem%2C%20which%20can%20be%20transformed%20into%20an%20unbalanced%20optimal%0Atransport%20problem%20with%20augmented%20constraints%20and%20can%20be%20solved%20efficiently%20by%20a%0Afast%20matrix%20scaling%20algorithm.%20Experiments%20on%20various%20datasets%2C%20including%20a%0Ahuman-curated%20long-tailed%20CIFAR100%2C%20challenging%20ImageNet-R%2C%20and%20large-scale%0Asubsets%20of%20fine-grained%20iNaturalist2018%20datasets%2C%20demonstrate%20the%20superiority%0Aof%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03446v1&entry.124074799=Read"},
{"title": "AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position\n  and Scale", "author": "Adam Pardyl and Micha\u0142 Wronka and Maciej Wo\u0142czyk and Kamil Adamczewski and Tomasz Trzci\u0144ski and Bartosz Zieli\u0144ski", "abstract": "  Active Visual Exploration (AVE) is a task that involves dynamically selecting\nobservations (glimpses), which is critical to facilitate comprehension and\nnavigation within an environment. While modern AVE methods have demonstrated\nimpressive performance, they are constrained to fixed-scale glimpses from rigid\ngrids. In contrast, existing mobile platforms equipped with optical zoom\ncapabilities can capture glimpses of arbitrary positions and scales. To address\nthis gap between software and hardware capabilities, we introduce AdaGlimpse.\nIt uses Soft Actor-Critic, a reinforcement learning algorithm tailored for\nexploration tasks, to select glimpses of arbitrary position and scale. This\napproach enables our model to rapidly establish a general awareness of the\nenvironment before zooming in for detailed analysis. Experimental results\ndemonstrate that AdaGlimpse surpasses previous methods across various visual\ntasks while maintaining greater applicability in realistic AVE scenarios.\n", "link": "http://arxiv.org/abs/2404.03482v1", "date": "2024-04-04", "relevancy": 2.0967, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5966}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5208}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4986}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AdaGlimpse%3A%20Active%20Visual%20Exploration%20with%20Arbitrary%20Glimpse%20Position%0A%20%20and%20Scale&body=Title%3A%20AdaGlimpse%3A%20Active%20Visual%20Exploration%20with%20Arbitrary%20Glimpse%20Position%0A%20%20and%20Scale%0AAuthor%3A%20Adam%20Pardyl%20and%20Micha%C5%82%20Wronka%20and%20Maciej%20Wo%C5%82czyk%20and%20Kamil%20Adamczewski%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bartosz%20Zieli%C5%84ski%0AAbstract%3A%20%20%20Active%20Visual%20Exploration%20%28AVE%29%20is%20a%20task%20that%20involves%20dynamically%20selecting%0Aobservations%20%28glimpses%29%2C%20which%20is%20critical%20to%20facilitate%20comprehension%20and%0Anavigation%20within%20an%20environment.%20While%20modern%20AVE%20methods%20have%20demonstrated%0Aimpressive%20performance%2C%20they%20are%20constrained%20to%20fixed-scale%20glimpses%20from%20rigid%0Agrids.%20In%20contrast%2C%20existing%20mobile%20platforms%20equipped%20with%20optical%20zoom%0Acapabilities%20can%20capture%20glimpses%20of%20arbitrary%20positions%20and%20scales.%20To%20address%0Athis%20gap%20between%20software%20and%20hardware%20capabilities%2C%20we%20introduce%20AdaGlimpse.%0AIt%20uses%20Soft%20Actor-Critic%2C%20a%20reinforcement%20learning%20algorithm%20tailored%20for%0Aexploration%20tasks%2C%20to%20select%20glimpses%20of%20arbitrary%20position%20and%20scale.%20This%0Aapproach%20enables%20our%20model%20to%20rapidly%20establish%20a%20general%20awareness%20of%20the%0Aenvironment%20before%20zooming%20in%20for%20detailed%20analysis.%20Experimental%20results%0Ademonstrate%20that%20AdaGlimpse%20surpasses%20previous%20methods%20across%20various%20visual%0Atasks%20while%20maintaining%20greater%20applicability%20in%20realistic%20AVE%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03482v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaGlimpse%3A%20Active%20Visual%20Exploration%20with%20Arbitrary%20Glimpse%20Position%0A%20%20and%20Scale&entry.906535625=Adam%20Pardyl%20and%20Micha%C5%82%20Wronka%20and%20Maciej%20Wo%C5%82czyk%20and%20Kamil%20Adamczewski%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bartosz%20Zieli%C5%84ski&entry.1292438233=%20%20Active%20Visual%20Exploration%20%28AVE%29%20is%20a%20task%20that%20involves%20dynamically%20selecting%0Aobservations%20%28glimpses%29%2C%20which%20is%20critical%20to%20facilitate%20comprehension%20and%0Anavigation%20within%20an%20environment.%20While%20modern%20AVE%20methods%20have%20demonstrated%0Aimpressive%20performance%2C%20they%20are%20constrained%20to%20fixed-scale%20glimpses%20from%20rigid%0Agrids.%20In%20contrast%2C%20existing%20mobile%20platforms%20equipped%20with%20optical%20zoom%0Acapabilities%20can%20capture%20glimpses%20of%20arbitrary%20positions%20and%20scales.%20To%20address%0Athis%20gap%20between%20software%20and%20hardware%20capabilities%2C%20we%20introduce%20AdaGlimpse.%0AIt%20uses%20Soft%20Actor-Critic%2C%20a%20reinforcement%20learning%20algorithm%20tailored%20for%0Aexploration%20tasks%2C%20to%20select%20glimpses%20of%20arbitrary%20position%20and%20scale.%20This%0Aapproach%20enables%20our%20model%20to%20rapidly%20establish%20a%20general%20awareness%20of%20the%0Aenvironment%20before%20zooming%20in%20for%20detailed%20analysis.%20Experimental%20results%0Ademonstrate%20that%20AdaGlimpse%20surpasses%20previous%20methods%20across%20various%20visual%0Atasks%20while%20maintaining%20greater%20applicability%20in%20realistic%20AVE%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03482v1&entry.124074799=Read"},
{"title": "ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State\n  Space Model", "author": "Hongruixuan Chen and Jian Song and Chengxi Han and Junshi Xia and Naoto Yokoya", "abstract": "  Convolutional neural networks (CNN) and Transformers have made impressive\nprogress in the field of remote sensing change detection (CD). However, both\narchitectures have their inherent shortcomings. Recently, the Mamba\narchitecture, based on spatial state models, has shown remarkable performance\nin a series of natural language processing tasks, which can effectively\ncompensate for the shortcomings of the above two architectures. In this paper,\nwe explore for the first time the potential of the Mamba architecture for\nremote sensing change detection tasks. We tailor the corresponding frameworks,\ncalled MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD),\nsemantic change detection (SCD), and building damage assessment (BDA),\nrespectively. All three frameworks adopt the cutting-edge visual Mamba\narchitecture as the encoder, which allows full learning of global spatial\ncontextual information from the input images. For the change decoder, which is\navailable in all three architectures, we propose three spatio-temporal\nrelationship modeling mechanisms, which can be naturally combined with the\nMamba architecture and fully utilize its attribute to achieve spatio-temporal\ninteraction of multi-temporal features and obtain accurate change information.\nOn five benchmark datasets, our proposed frameworks outperform current CNN- and\nTransformer-based approaches without using any complex strategies or tricks,\nfully demonstrating the potential of the Mamba architecture. Specifically, we\nobtained 83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU,\nLEVIR-CD+, and WHU-CD; on the SCD dataset SECOND, we obtained 24.04% SeK; and\non the xBD dataset, we obtained 81.41% overall F1 score. The source code will\nbe available in https://github.com/ChenHongruixuan/MambaCD\n", "link": "http://arxiv.org/abs/2404.03425v1", "date": "2024-04-04", "relevancy": 2.0962, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model&body=Title%3A%20ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model%0AAuthor%3A%20Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Chengxi%20Han%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNN%29%20and%20Transformers%20have%20made%20impressive%0Aprogress%20in%20the%20field%20of%20remote%20sensing%20change%20detection%20%28CD%29.%20However%2C%20both%0Aarchitectures%20have%20their%20inherent%20shortcomings.%20Recently%2C%20the%20Mamba%0Aarchitecture%2C%20based%20on%20spatial%20state%20models%2C%20has%20shown%20remarkable%20performance%0Ain%20a%20series%20of%20natural%20language%20processing%20tasks%2C%20which%20can%20effectively%0Acompensate%20for%20the%20shortcomings%20of%20the%20above%20two%20architectures.%20In%20this%20paper%2C%0Awe%20explore%20for%20the%20first%20time%20the%20potential%20of%20the%20Mamba%20architecture%20for%0Aremote%20sensing%20change%20detection%20tasks.%20We%20tailor%20the%20corresponding%20frameworks%2C%0Acalled%20MambaBCD%2C%20MambaSCD%2C%20and%20MambaBDA%2C%20for%20binary%20change%20detection%20%28BCD%29%2C%0Asemantic%20change%20detection%20%28SCD%29%2C%20and%20building%20damage%20assessment%20%28BDA%29%2C%0Arespectively.%20All%20three%20frameworks%20adopt%20the%20cutting-edge%20visual%20Mamba%0Aarchitecture%20as%20the%20encoder%2C%20which%20allows%20full%20learning%20of%20global%20spatial%0Acontextual%20information%20from%20the%20input%20images.%20For%20the%20change%20decoder%2C%20which%20is%0Aavailable%20in%20all%20three%20architectures%2C%20we%20propose%20three%20spatio-temporal%0Arelationship%20modeling%20mechanisms%2C%20which%20can%20be%20naturally%20combined%20with%20the%0AMamba%20architecture%20and%20fully%20utilize%20its%20attribute%20to%20achieve%20spatio-temporal%0Ainteraction%20of%20multi-temporal%20features%20and%20obtain%20accurate%20change%20information.%0AOn%20five%20benchmark%20datasets%2C%20our%20proposed%20frameworks%20outperform%20current%20CNN-%20and%0ATransformer-based%20approaches%20without%20using%20any%20complex%20strategies%20or%20tricks%2C%0Afully%20demonstrating%20the%20potential%20of%20the%20Mamba%20architecture.%20Specifically%2C%20we%0Aobtained%2083.11%25%2C%2088.39%25%20and%2094.19%25%20F1%20scores%20on%20the%20three%20BCD%20datasets%20SYSU%2C%0ALEVIR-CD%2B%2C%20and%20WHU-CD%3B%20on%20the%20SCD%20dataset%20SECOND%2C%20we%20obtained%2024.04%25%20SeK%3B%20and%0Aon%20the%20xBD%20dataset%2C%20we%20obtained%2081.41%25%20overall%20F1%20score.%20The%20source%20code%20will%0Abe%20available%20in%20https%3A//github.com/ChenHongruixuan/MambaCD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03425v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChangeMamba%3A%20Remote%20Sensing%20Change%20Detection%20with%20Spatio-Temporal%20State%0A%20%20Space%20Model&entry.906535625=Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Chengxi%20Han%20and%20Junshi%20Xia%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNN%29%20and%20Transformers%20have%20made%20impressive%0Aprogress%20in%20the%20field%20of%20remote%20sensing%20change%20detection%20%28CD%29.%20However%2C%20both%0Aarchitectures%20have%20their%20inherent%20shortcomings.%20Recently%2C%20the%20Mamba%0Aarchitecture%2C%20based%20on%20spatial%20state%20models%2C%20has%20shown%20remarkable%20performance%0Ain%20a%20series%20of%20natural%20language%20processing%20tasks%2C%20which%20can%20effectively%0Acompensate%20for%20the%20shortcomings%20of%20the%20above%20two%20architectures.%20In%20this%20paper%2C%0Awe%20explore%20for%20the%20first%20time%20the%20potential%20of%20the%20Mamba%20architecture%20for%0Aremote%20sensing%20change%20detection%20tasks.%20We%20tailor%20the%20corresponding%20frameworks%2C%0Acalled%20MambaBCD%2C%20MambaSCD%2C%20and%20MambaBDA%2C%20for%20binary%20change%20detection%20%28BCD%29%2C%0Asemantic%20change%20detection%20%28SCD%29%2C%20and%20building%20damage%20assessment%20%28BDA%29%2C%0Arespectively.%20All%20three%20frameworks%20adopt%20the%20cutting-edge%20visual%20Mamba%0Aarchitecture%20as%20the%20encoder%2C%20which%20allows%20full%20learning%20of%20global%20spatial%0Acontextual%20information%20from%20the%20input%20images.%20For%20the%20change%20decoder%2C%20which%20is%0Aavailable%20in%20all%20three%20architectures%2C%20we%20propose%20three%20spatio-temporal%0Arelationship%20modeling%20mechanisms%2C%20which%20can%20be%20naturally%20combined%20with%20the%0AMamba%20architecture%20and%20fully%20utilize%20its%20attribute%20to%20achieve%20spatio-temporal%0Ainteraction%20of%20multi-temporal%20features%20and%20obtain%20accurate%20change%20information.%0AOn%20five%20benchmark%20datasets%2C%20our%20proposed%20frameworks%20outperform%20current%20CNN-%20and%0ATransformer-based%20approaches%20without%20using%20any%20complex%20strategies%20or%20tricks%2C%0Afully%20demonstrating%20the%20potential%20of%20the%20Mamba%20architecture.%20Specifically%2C%20we%0Aobtained%2083.11%25%2C%2088.39%25%20and%2094.19%25%20F1%20scores%20on%20the%20three%20BCD%20datasets%20SYSU%2C%0ALEVIR-CD%2B%2C%20and%20WHU-CD%3B%20on%20the%20SCD%20dataset%20SECOND%2C%20we%20obtained%2024.04%25%20SeK%3B%20and%0Aon%20the%20xBD%20dataset%2C%20we%20obtained%2081.41%25%20overall%20F1%20score.%20The%20source%20code%20will%0Abe%20available%20in%20https%3A//github.com/ChenHongruixuan/MambaCD%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03425v1&entry.124074799=Read"},
{"title": "Two Tricks to Improve Unsupervised Segmentation Learning", "author": "Alp Eren Sari and Francesco Locatello and Paolo Favar", "abstract": "  We present two practical improvement techniques for unsupervised segmentation\nlearning. These techniques address limitations in the resolution and accuracy\nof predicted segmentation maps of recent state-of-the-art methods. Firstly, we\nleverage image post-processing techniques such as guided filtering to refine\nthe output masks, improving accuracy while avoiding substantial computational\ncosts. Secondly, we introduce a multi-scale consistency criterion, based on a\nteacher-student training scheme. This criterion matches segmentation masks\npredicted from regions of the input image extracted at different resolutions to\neach other. Experimental results on several benchmarks used in unsupervised\nsegmentation learning demonstrate the effectiveness of our proposed techniques.\n", "link": "http://arxiv.org/abs/2404.03392v1", "date": "2024-04-04", "relevancy": 2.0922, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5364}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5172}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5043}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Two%20Tricks%20to%20Improve%20Unsupervised%20Segmentation%20Learning&body=Title%3A%20Two%20Tricks%20to%20Improve%20Unsupervised%20Segmentation%20Learning%0AAuthor%3A%20Alp%20Eren%20Sari%20and%20Francesco%20Locatello%20and%20Paolo%20Favar%0AAbstract%3A%20%20%20We%20present%20two%20practical%20improvement%20techniques%20for%20unsupervised%20segmentation%0Alearning.%20These%20techniques%20address%20limitations%20in%20the%20resolution%20and%20accuracy%0Aof%20predicted%20segmentation%20maps%20of%20recent%20state-of-the-art%20methods.%20Firstly%2C%20we%0Aleverage%20image%20post-processing%20techniques%20such%20as%20guided%20filtering%20to%20refine%0Athe%20output%20masks%2C%20improving%20accuracy%20while%20avoiding%20substantial%20computational%0Acosts.%20Secondly%2C%20we%20introduce%20a%20multi-scale%20consistency%20criterion%2C%20based%20on%20a%0Ateacher-student%20training%20scheme.%20This%20criterion%20matches%20segmentation%20masks%0Apredicted%20from%20regions%20of%20the%20input%20image%20extracted%20at%20different%20resolutions%20to%0Aeach%20other.%20Experimental%20results%20on%20several%20benchmarks%20used%20in%20unsupervised%0Asegmentation%20learning%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03392v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Tricks%20to%20Improve%20Unsupervised%20Segmentation%20Learning&entry.906535625=Alp%20Eren%20Sari%20and%20Francesco%20Locatello%20and%20Paolo%20Favar&entry.1292438233=%20%20We%20present%20two%20practical%20improvement%20techniques%20for%20unsupervised%20segmentation%0Alearning.%20These%20techniques%20address%20limitations%20in%20the%20resolution%20and%20accuracy%0Aof%20predicted%20segmentation%20maps%20of%20recent%20state-of-the-art%20methods.%20Firstly%2C%20we%0Aleverage%20image%20post-processing%20techniques%20such%20as%20guided%20filtering%20to%20refine%0Athe%20output%20masks%2C%20improving%20accuracy%20while%20avoiding%20substantial%20computational%0Acosts.%20Secondly%2C%20we%20introduce%20a%20multi-scale%20consistency%20criterion%2C%20based%20on%20a%0Ateacher-student%20training%20scheme.%20This%20criterion%20matches%20segmentation%20masks%0Apredicted%20from%20regions%20of%20the%20input%20image%20extracted%20at%20different%20resolutions%20to%0Aeach%20other.%20Experimental%20results%20on%20several%20benchmarks%20used%20in%20unsupervised%0Asegmentation%20learning%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03392v1&entry.124074799=Read"},
{"title": "Generalizable 3D Scene Reconstruction via Divide and Conquer from a\n  Single View", "author": "Andreea Dogaru and Mert \u00d6zer and Bernhard Egger", "abstract": "  Single-view 3D reconstruction is currently approached from two dominant\nperspectives: reconstruction of scenes with limited diversity using 3D data\nsupervision or reconstruction of diverse singular objects using large image\npriors. However, real-world scenarios are far more complex and exceed the\ncapabilities of these methods. We therefore propose a hybrid method following a\ndivide-and-conquer strategy. We first process the scene holistically,\nextracting depth and semantic information, and then leverage a single-shot\nobject-level method for the detailed reconstruction of individual components.\nBy following a compositional processing approach, the overall framework\nachieves full reconstruction of complex 3D scenes from a single image. We\npurposely design our pipeline to be highly modular by carefully integrating\nspecific procedures for each processing step, without requiring an end-to-end\ntraining of the whole system. This enables the pipeline to naturally improve as\nfuture methods can replace the individual modules. We demonstrate the\nreconstruction performance of our approach on both synthetic and real-world\nscenes, comparing favorable against prior works. Project page:\nhttps://andreeadogaru.github.io/Gen3DSR.\n", "link": "http://arxiv.org/abs/2404.03421v1", "date": "2024-04-04", "relevancy": 2.0762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5343}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5226}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5024}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalizable%203D%20Scene%20Reconstruction%20via%20Divide%20and%20Conquer%20from%20a%0A%20%20Single%20View&body=Title%3A%20Generalizable%203D%20Scene%20Reconstruction%20via%20Divide%20and%20Conquer%20from%20a%0A%20%20Single%20View%0AAuthor%3A%20Andreea%20Dogaru%20and%20Mert%20%C3%96zer%20and%20Bernhard%20Egger%0AAbstract%3A%20%20%20Single-view%203D%20reconstruction%20is%20currently%20approached%20from%20two%20dominant%0Aperspectives%3A%20reconstruction%20of%20scenes%20with%20limited%20diversity%20using%203D%20data%0Asupervision%20or%20reconstruction%20of%20diverse%20singular%20objects%20using%20large%20image%0Apriors.%20However%2C%20real-world%20scenarios%20are%20far%20more%20complex%20and%20exceed%20the%0Acapabilities%20of%20these%20methods.%20We%20therefore%20propose%20a%20hybrid%20method%20following%20a%0Adivide-and-conquer%20strategy.%20We%20first%20process%20the%20scene%20holistically%2C%0Aextracting%20depth%20and%20semantic%20information%2C%20and%20then%20leverage%20a%20single-shot%0Aobject-level%20method%20for%20the%20detailed%20reconstruction%20of%20individual%20components.%0ABy%20following%20a%20compositional%20processing%20approach%2C%20the%20overall%20framework%0Aachieves%20full%20reconstruction%20of%20complex%203D%20scenes%20from%20a%20single%20image.%20We%0Apurposely%20design%20our%20pipeline%20to%20be%20highly%20modular%20by%20carefully%20integrating%0Aspecific%20procedures%20for%20each%20processing%20step%2C%20without%20requiring%20an%20end-to-end%0Atraining%20of%20the%20whole%20system.%20This%20enables%20the%20pipeline%20to%20naturally%20improve%20as%0Afuture%20methods%20can%20replace%20the%20individual%20modules.%20We%20demonstrate%20the%0Areconstruction%20performance%20of%20our%20approach%20on%20both%20synthetic%20and%20real-world%0Ascenes%2C%20comparing%20favorable%20against%20prior%20works.%20Project%20page%3A%0Ahttps%3A//andreeadogaru.github.io/Gen3DSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03421v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%203D%20Scene%20Reconstruction%20via%20Divide%20and%20Conquer%20from%20a%0A%20%20Single%20View&entry.906535625=Andreea%20Dogaru%20and%20Mert%20%C3%96zer%20and%20Bernhard%20Egger&entry.1292438233=%20%20Single-view%203D%20reconstruction%20is%20currently%20approached%20from%20two%20dominant%0Aperspectives%3A%20reconstruction%20of%20scenes%20with%20limited%20diversity%20using%203D%20data%0Asupervision%20or%20reconstruction%20of%20diverse%20singular%20objects%20using%20large%20image%0Apriors.%20However%2C%20real-world%20scenarios%20are%20far%20more%20complex%20and%20exceed%20the%0Acapabilities%20of%20these%20methods.%20We%20therefore%20propose%20a%20hybrid%20method%20following%20a%0Adivide-and-conquer%20strategy.%20We%20first%20process%20the%20scene%20holistically%2C%0Aextracting%20depth%20and%20semantic%20information%2C%20and%20then%20leverage%20a%20single-shot%0Aobject-level%20method%20for%20the%20detailed%20reconstruction%20of%20individual%20components.%0ABy%20following%20a%20compositional%20processing%20approach%2C%20the%20overall%20framework%0Aachieves%20full%20reconstruction%20of%20complex%203D%20scenes%20from%20a%20single%20image.%20We%0Apurposely%20design%20our%20pipeline%20to%20be%20highly%20modular%20by%20carefully%20integrating%0Aspecific%20procedures%20for%20each%20processing%20step%2C%20without%20requiring%20an%20end-to-end%0Atraining%20of%20the%20whole%20system.%20This%20enables%20the%20pipeline%20to%20naturally%20improve%20as%0Afuture%20methods%20can%20replace%20the%20individual%20modules.%20We%20demonstrate%20the%0Areconstruction%20performance%20of%20our%20approach%20on%20both%20synthetic%20and%20real-world%0Ascenes%2C%20comparing%20favorable%20against%20prior%20works.%20Project%20page%3A%0Ahttps%3A//andreeadogaru.github.io/Gen3DSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03421v1&entry.124074799=Read"},
{"title": "Benchmarking ChatGPT on Algorithmic Reasoning", "author": "Sean McLeish and Avi Schwarzschild and Tom Goldstein", "abstract": "  We evaluate ChatGPT's ability to solve algorithm problems from the CLRS\nbenchmark suite that is designed for GNNs. The benchmark requires the use of a\nspecified classical algorithm to solve a given problem. We find that ChatGPT\noutperforms specialist GNN models, using Python to successfully solve these\nproblems. This raises new points in the discussion about learning algorithms\nwith neural networks.\n", "link": "http://arxiv.org/abs/2404.03441v1", "date": "2024-04-04", "relevancy": 2.074, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4201}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4169}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4074}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20ChatGPT%20on%20Algorithmic%20Reasoning&body=Title%3A%20Benchmarking%20ChatGPT%20on%20Algorithmic%20Reasoning%0AAuthor%3A%20Sean%20McLeish%20and%20Avi%20Schwarzschild%20and%20Tom%20Goldstein%0AAbstract%3A%20%20%20We%20evaluate%20ChatGPT%27s%20ability%20to%20solve%20algorithm%20problems%20from%20the%20CLRS%0Abenchmark%20suite%20that%20is%20designed%20for%20GNNs.%20The%20benchmark%20requires%20the%20use%20of%20a%0Aspecified%20classical%20algorithm%20to%20solve%20a%20given%20problem.%20We%20find%20that%20ChatGPT%0Aoutperforms%20specialist%20GNN%20models%2C%20using%20Python%20to%20successfully%20solve%20these%0Aproblems.%20This%20raises%20new%20points%20in%20the%20discussion%20about%20learning%20algorithms%0Awith%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03441v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20ChatGPT%20on%20Algorithmic%20Reasoning&entry.906535625=Sean%20McLeish%20and%20Avi%20Schwarzschild%20and%20Tom%20Goldstein&entry.1292438233=%20%20We%20evaluate%20ChatGPT%27s%20ability%20to%20solve%20algorithm%20problems%20from%20the%20CLRS%0Abenchmark%20suite%20that%20is%20designed%20for%20GNNs.%20The%20benchmark%20requires%20the%20use%20of%20a%0Aspecified%20classical%20algorithm%20to%20solve%20a%20given%20problem.%20We%20find%20that%20ChatGPT%0Aoutperforms%20specialist%20GNN%20models%2C%20using%20Python%20to%20successfully%20solve%20these%0Aproblems.%20This%20raises%20new%20points%20in%20the%20discussion%20about%20learning%20algorithms%0Awith%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03441v1&entry.124074799=Read"},
{"title": "How does Multi-Task Training Affect Transformer In-Context Capabilities?\n  Investigations with Function Classes", "author": "Harmon Bhasin and Timothy Ossowski and Yiqiao Zhong and Junjie Hu", "abstract": "  Large language models (LLM) have recently shown the extraordinary ability to\nperform unseen tasks based on few-shot examples provided as text, also known as\nin-context learning (ICL). While recent works have attempted to understand the\nmechanisms driving ICL, few have explored training strategies that incentivize\nthese models to generalize to multiple tasks. Multi-task learning (MTL) for\ngeneralist models is a promising direction that offers transfer learning\npotential, enabling large parameterized models to be trained from simpler,\nrelated tasks. In this work, we investigate the combination of MTL with ICL to\nbuild models that efficiently learn tasks while being robust to\nout-of-distribution examples. We propose several effective curriculum learning\nstrategies that allow ICL models to achieve higher data efficiency and more\nstable convergence. Our experiments reveal that ICL models can effectively\nlearn difficult tasks by training on progressively harder tasks while mixing in\nprior tasks, denoted as mixed curriculum in this work. Our code and models are\navailable at https://github.com/harmonbhasin/curriculum_learning_icl .\n", "link": "http://arxiv.org/abs/2404.03558v1", "date": "2024-04-04", "relevancy": 2.0637, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5765}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4772}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4708}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20does%20Multi-Task%20Training%20Affect%20Transformer%20In-Context%20Capabilities%3F%0A%20%20Investigations%20with%20Function%20Classes&body=Title%3A%20How%20does%20Multi-Task%20Training%20Affect%20Transformer%20In-Context%20Capabilities%3F%0A%20%20Investigations%20with%20Function%20Classes%0AAuthor%3A%20Harmon%20Bhasin%20and%20Timothy%20Ossowski%20and%20Yiqiao%20Zhong%20and%20Junjie%20Hu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLM%29%20have%20recently%20shown%20the%20extraordinary%20ability%20to%0Aperform%20unseen%20tasks%20based%20on%20few-shot%20examples%20provided%20as%20text%2C%20also%20known%20as%0Ain-context%20learning%20%28ICL%29.%20While%20recent%20works%20have%20attempted%20to%20understand%20the%0Amechanisms%20driving%20ICL%2C%20few%20have%20explored%20training%20strategies%20that%20incentivize%0Athese%20models%20to%20generalize%20to%20multiple%20tasks.%20Multi-task%20learning%20%28MTL%29%20for%0Ageneralist%20models%20is%20a%20promising%20direction%20that%20offers%20transfer%20learning%0Apotential%2C%20enabling%20large%20parameterized%20models%20to%20be%20trained%20from%20simpler%2C%0Arelated%20tasks.%20In%20this%20work%2C%20we%20investigate%20the%20combination%20of%20MTL%20with%20ICL%20to%0Abuild%20models%20that%20efficiently%20learn%20tasks%20while%20being%20robust%20to%0Aout-of-distribution%20examples.%20We%20propose%20several%20effective%20curriculum%20learning%0Astrategies%20that%20allow%20ICL%20models%20to%20achieve%20higher%20data%20efficiency%20and%20more%0Astable%20convergence.%20Our%20experiments%20reveal%20that%20ICL%20models%20can%20effectively%0Alearn%20difficult%20tasks%20by%20training%20on%20progressively%20harder%20tasks%20while%20mixing%20in%0Aprior%20tasks%2C%20denoted%20as%20mixed%20curriculum%20in%20this%20work.%20Our%20code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/harmonbhasin/curriculum_learning_icl%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03558v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20does%20Multi-Task%20Training%20Affect%20Transformer%20In-Context%20Capabilities%3F%0A%20%20Investigations%20with%20Function%20Classes&entry.906535625=Harmon%20Bhasin%20and%20Timothy%20Ossowski%20and%20Yiqiao%20Zhong%20and%20Junjie%20Hu&entry.1292438233=%20%20Large%20language%20models%20%28LLM%29%20have%20recently%20shown%20the%20extraordinary%20ability%20to%0Aperform%20unseen%20tasks%20based%20on%20few-shot%20examples%20provided%20as%20text%2C%20also%20known%20as%0Ain-context%20learning%20%28ICL%29.%20While%20recent%20works%20have%20attempted%20to%20understand%20the%0Amechanisms%20driving%20ICL%2C%20few%20have%20explored%20training%20strategies%20that%20incentivize%0Athese%20models%20to%20generalize%20to%20multiple%20tasks.%20Multi-task%20learning%20%28MTL%29%20for%0Ageneralist%20models%20is%20a%20promising%20direction%20that%20offers%20transfer%20learning%0Apotential%2C%20enabling%20large%20parameterized%20models%20to%20be%20trained%20from%20simpler%2C%0Arelated%20tasks.%20In%20this%20work%2C%20we%20investigate%20the%20combination%20of%20MTL%20with%20ICL%20to%0Abuild%20models%20that%20efficiently%20learn%20tasks%20while%20being%20robust%20to%0Aout-of-distribution%20examples.%20We%20propose%20several%20effective%20curriculum%20learning%0Astrategies%20that%20allow%20ICL%20models%20to%20achieve%20higher%20data%20efficiency%20and%20more%0Astable%20convergence.%20Our%20experiments%20reveal%20that%20ICL%20models%20can%20effectively%0Alearn%20difficult%20tasks%20by%20training%20on%20progressively%20harder%20tasks%20while%20mixing%20in%0Aprior%20tasks%2C%20denoted%20as%20mixed%20curriculum%20in%20this%20work.%20Our%20code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/harmonbhasin/curriculum_learning_icl%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03558v1&entry.124074799=Read"},
{"title": "Provably Robust and Plausible Counterfactual Explanations for Neural\n  Networks via Robust Optimisation", "author": "Junqi Jiang and Jianglin Lan and Francesco Leofante and Antonio Rago and Francesca Toni", "abstract": "  Counterfactual Explanations (CEs) have received increasing interest as a\nmajor methodology for explaining neural network classifiers. Usually, CEs for\nan input-output pair are defined as data points with minimum distance to the\ninput that are classified with a different label than the output. To tackle the\nestablished problem that CEs are easily invalidated when model parameters are\nupdated (e.g. retrained), studies have proposed ways to certify the robustness\nof CEs under model parameter changes bounded by a norm ball. However, existing\nmethods targeting this form of robustness are not sound or complete, and they\nmay generate implausible CEs, i.e., outliers wrt the training dataset. In fact,\nno existing method simultaneously optimises for closeness and plausibility\nwhile preserving robustness guarantees. In this work, we propose Provably\nRObust and PLAusible Counterfactual Explanations (PROPLACE), a method\nleveraging on robust optimisation techniques to address the aforementioned\nlimitations in the literature. We formulate an iterative algorithm to compute\nprovably robust CEs and prove its convergence, soundness and completeness.\nThrough a comparative experiment involving six baselines, five of which target\nrobustness, we show that PROPLACE achieves state-of-the-art performances\nagainst metrics on three evaluation aspects.\n", "link": "http://arxiv.org/abs/2309.12545v2", "date": "2024-04-04", "relevancy": 2.0618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5051}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Provably%20Robust%20and%20Plausible%20Counterfactual%20Explanations%20for%20Neural%0A%20%20Networks%20via%20Robust%20Optimisation&body=Title%3A%20Provably%20Robust%20and%20Plausible%20Counterfactual%20Explanations%20for%20Neural%0A%20%20Networks%20via%20Robust%20Optimisation%0AAuthor%3A%20Junqi%20Jiang%20and%20Jianglin%20Lan%20and%20Francesco%20Leofante%20and%20Antonio%20Rago%20and%20Francesca%20Toni%0AAbstract%3A%20%20%20Counterfactual%20Explanations%20%28CEs%29%20have%20received%20increasing%20interest%20as%20a%0Amajor%20methodology%20for%20explaining%20neural%20network%20classifiers.%20Usually%2C%20CEs%20for%0Aan%20input-output%20pair%20are%20defined%20as%20data%20points%20with%20minimum%20distance%20to%20the%0Ainput%20that%20are%20classified%20with%20a%20different%20label%20than%20the%20output.%20To%20tackle%20the%0Aestablished%20problem%20that%20CEs%20are%20easily%20invalidated%20when%20model%20parameters%20are%0Aupdated%20%28e.g.%20retrained%29%2C%20studies%20have%20proposed%20ways%20to%20certify%20the%20robustness%0Aof%20CEs%20under%20model%20parameter%20changes%20bounded%20by%20a%20norm%20ball.%20However%2C%20existing%0Amethods%20targeting%20this%20form%20of%20robustness%20are%20not%20sound%20or%20complete%2C%20and%20they%0Amay%20generate%20implausible%20CEs%2C%20i.e.%2C%20outliers%20wrt%20the%20training%20dataset.%20In%20fact%2C%0Ano%20existing%20method%20simultaneously%20optimises%20for%20closeness%20and%20plausibility%0Awhile%20preserving%20robustness%20guarantees.%20In%20this%20work%2C%20we%20propose%20Provably%0ARObust%20and%20PLAusible%20Counterfactual%20Explanations%20%28PROPLACE%29%2C%20a%20method%0Aleveraging%20on%20robust%20optimisation%20techniques%20to%20address%20the%20aforementioned%0Alimitations%20in%20the%20literature.%20We%20formulate%20an%20iterative%20algorithm%20to%20compute%0Aprovably%20robust%20CEs%20and%20prove%20its%20convergence%2C%20soundness%20and%20completeness.%0AThrough%20a%20comparative%20experiment%20involving%20six%20baselines%2C%20five%20of%20which%20target%0Arobustness%2C%20we%20show%20that%20PROPLACE%20achieves%20state-of-the-art%20performances%0Aagainst%20metrics%20on%20three%20evaluation%20aspects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12545v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20Robust%20and%20Plausible%20Counterfactual%20Explanations%20for%20Neural%0A%20%20Networks%20via%20Robust%20Optimisation&entry.906535625=Junqi%20Jiang%20and%20Jianglin%20Lan%20and%20Francesco%20Leofante%20and%20Antonio%20Rago%20and%20Francesca%20Toni&entry.1292438233=%20%20Counterfactual%20Explanations%20%28CEs%29%20have%20received%20increasing%20interest%20as%20a%0Amajor%20methodology%20for%20explaining%20neural%20network%20classifiers.%20Usually%2C%20CEs%20for%0Aan%20input-output%20pair%20are%20defined%20as%20data%20points%20with%20minimum%20distance%20to%20the%0Ainput%20that%20are%20classified%20with%20a%20different%20label%20than%20the%20output.%20To%20tackle%20the%0Aestablished%20problem%20that%20CEs%20are%20easily%20invalidated%20when%20model%20parameters%20are%0Aupdated%20%28e.g.%20retrained%29%2C%20studies%20have%20proposed%20ways%20to%20certify%20the%20robustness%0Aof%20CEs%20under%20model%20parameter%20changes%20bounded%20by%20a%20norm%20ball.%20However%2C%20existing%0Amethods%20targeting%20this%20form%20of%20robustness%20are%20not%20sound%20or%20complete%2C%20and%20they%0Amay%20generate%20implausible%20CEs%2C%20i.e.%2C%20outliers%20wrt%20the%20training%20dataset.%20In%20fact%2C%0Ano%20existing%20method%20simultaneously%20optimises%20for%20closeness%20and%20plausibility%0Awhile%20preserving%20robustness%20guarantees.%20In%20this%20work%2C%20we%20propose%20Provably%0ARObust%20and%20PLAusible%20Counterfactual%20Explanations%20%28PROPLACE%29%2C%20a%20method%0Aleveraging%20on%20robust%20optimisation%20techniques%20to%20address%20the%20aforementioned%0Alimitations%20in%20the%20literature.%20We%20formulate%20an%20iterative%20algorithm%20to%20compute%0Aprovably%20robust%20CEs%20and%20prove%20its%20convergence%2C%20soundness%20and%20completeness.%0AThrough%20a%20comparative%20experiment%20involving%20six%20baselines%2C%20five%20of%20which%20target%0Arobustness%2C%20we%20show%20that%20PROPLACE%20achieves%20state-of-the-art%20performances%0Aagainst%20metrics%20on%20three%20evaluation%20aspects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12545v2&entry.124074799=Read"},
{"title": "TinyVQA: Compact Multimodal Deep Neural Network for Visual Question\n  Answering on Resource-Constrained Devices", "author": "Hasib-Al Rashid and Argho Sarkar and Aryya Gangopadhyay and Maryam Rahnemoonfar and Tinoosh Mohsenin", "abstract": "  Traditional machine learning models often require powerful hardware, making\nthem unsuitable for deployment on resource-limited devices. Tiny Machine\nLearning (tinyML) has emerged as a promising approach for running machine\nlearning models on these devices, but integrating multiple data modalities into\ntinyML models still remains a challenge due to increased complexity, latency,\nand power consumption. This paper proposes TinyVQA, a novel multimodal deep\nneural network for visual question answering tasks that can be deployed on\nresource-constrained tinyML hardware. TinyVQA leverages a supervised\nattention-based model to learn how to answer questions about images using both\nvision and language modalities. Distilled knowledge from the supervised\nattention-based VQA model trains the memory aware compact TinyVQA model and low\nbit-width quantization technique is employed to further compress the model for\ndeployment on tinyML devices. The TinyVQA model was evaluated on the FloodNet\ndataset, which is used for post-disaster damage assessment. The compact model\nachieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA for\nreal-world applications. Additionally, the model was deployed on a Crazyflie\n2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA model\nachieved low latencies of 56 ms and consumes 693 mW power while deployed on the\ntiny drone, showcasing its suitability for resource-constrained embedded\nsystems.\n", "link": "http://arxiv.org/abs/2404.03574v1", "date": "2024-04-04", "relevancy": 2.0587, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5449}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5128}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TinyVQA%3A%20Compact%20Multimodal%20Deep%20Neural%20Network%20for%20Visual%20Question%0A%20%20Answering%20on%20Resource-Constrained%20Devices&body=Title%3A%20TinyVQA%3A%20Compact%20Multimodal%20Deep%20Neural%20Network%20for%20Visual%20Question%0A%20%20Answering%20on%20Resource-Constrained%20Devices%0AAuthor%3A%20Hasib-Al%20Rashid%20and%20Argho%20Sarkar%20and%20Aryya%20Gangopadhyay%20and%20Maryam%20Rahnemoonfar%20and%20Tinoosh%20Mohsenin%0AAbstract%3A%20%20%20Traditional%20machine%20learning%20models%20often%20require%20powerful%20hardware%2C%20making%0Athem%20unsuitable%20for%20deployment%20on%20resource-limited%20devices.%20Tiny%20Machine%0ALearning%20%28tinyML%29%20has%20emerged%20as%20a%20promising%20approach%20for%20running%20machine%0Alearning%20models%20on%20these%20devices%2C%20but%20integrating%20multiple%20data%20modalities%20into%0AtinyML%20models%20still%20remains%20a%20challenge%20due%20to%20increased%20complexity%2C%20latency%2C%0Aand%20power%20consumption.%20This%20paper%20proposes%20TinyVQA%2C%20a%20novel%20multimodal%20deep%0Aneural%20network%20for%20visual%20question%20answering%20tasks%20that%20can%20be%20deployed%20on%0Aresource-constrained%20tinyML%20hardware.%20TinyVQA%20leverages%20a%20supervised%0Aattention-based%20model%20to%20learn%20how%20to%20answer%20questions%20about%20images%20using%20both%0Avision%20and%20language%20modalities.%20Distilled%20knowledge%20from%20the%20supervised%0Aattention-based%20VQA%20model%20trains%20the%20memory%20aware%20compact%20TinyVQA%20model%20and%20low%0Abit-width%20quantization%20technique%20is%20employed%20to%20further%20compress%20the%20model%20for%0Adeployment%20on%20tinyML%20devices.%20The%20TinyVQA%20model%20was%20evaluated%20on%20the%20FloodNet%0Adataset%2C%20which%20is%20used%20for%20post-disaster%20damage%20assessment.%20The%20compact%20model%0Aachieved%20an%20accuracy%20of%2079.5%25%2C%20demonstrating%20the%20effectiveness%20of%20TinyVQA%20for%0Areal-world%20applications.%20Additionally%2C%20the%20model%20was%20deployed%20on%20a%20Crazyflie%0A2.0%20drone%2C%20equipped%20with%20an%20AI%20deck%20and%20GAP8%20microprocessor.%20The%20TinyVQA%20model%0Aachieved%20low%20latencies%20of%2056%20ms%20and%20consumes%20693%20mW%20power%20while%20deployed%20on%20the%0Atiny%20drone%2C%20showcasing%20its%20suitability%20for%20resource-constrained%20embedded%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03574v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinyVQA%3A%20Compact%20Multimodal%20Deep%20Neural%20Network%20for%20Visual%20Question%0A%20%20Answering%20on%20Resource-Constrained%20Devices&entry.906535625=Hasib-Al%20Rashid%20and%20Argho%20Sarkar%20and%20Aryya%20Gangopadhyay%20and%20Maryam%20Rahnemoonfar%20and%20Tinoosh%20Mohsenin&entry.1292438233=%20%20Traditional%20machine%20learning%20models%20often%20require%20powerful%20hardware%2C%20making%0Athem%20unsuitable%20for%20deployment%20on%20resource-limited%20devices.%20Tiny%20Machine%0ALearning%20%28tinyML%29%20has%20emerged%20as%20a%20promising%20approach%20for%20running%20machine%0Alearning%20models%20on%20these%20devices%2C%20but%20integrating%20multiple%20data%20modalities%20into%0AtinyML%20models%20still%20remains%20a%20challenge%20due%20to%20increased%20complexity%2C%20latency%2C%0Aand%20power%20consumption.%20This%20paper%20proposes%20TinyVQA%2C%20a%20novel%20multimodal%20deep%0Aneural%20network%20for%20visual%20question%20answering%20tasks%20that%20can%20be%20deployed%20on%0Aresource-constrained%20tinyML%20hardware.%20TinyVQA%20leverages%20a%20supervised%0Aattention-based%20model%20to%20learn%20how%20to%20answer%20questions%20about%20images%20using%20both%0Avision%20and%20language%20modalities.%20Distilled%20knowledge%20from%20the%20supervised%0Aattention-based%20VQA%20model%20trains%20the%20memory%20aware%20compact%20TinyVQA%20model%20and%20low%0Abit-width%20quantization%20technique%20is%20employed%20to%20further%20compress%20the%20model%20for%0Adeployment%20on%20tinyML%20devices.%20The%20TinyVQA%20model%20was%20evaluated%20on%20the%20FloodNet%0Adataset%2C%20which%20is%20used%20for%20post-disaster%20damage%20assessment.%20The%20compact%20model%0Aachieved%20an%20accuracy%20of%2079.5%25%2C%20demonstrating%20the%20effectiveness%20of%20TinyVQA%20for%0Areal-world%20applications.%20Additionally%2C%20the%20model%20was%20deployed%20on%20a%20Crazyflie%0A2.0%20drone%2C%20equipped%20with%20an%20AI%20deck%20and%20GAP8%20microprocessor.%20The%20TinyVQA%20model%0Aachieved%20low%20latencies%20of%2056%20ms%20and%20consumes%20693%20mW%20power%20while%20deployed%20on%20the%0Atiny%20drone%2C%20showcasing%20its%20suitability%20for%20resource-constrained%20embedded%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03574v1&entry.124074799=Read"},
{"title": "Laser Learning Environment: A new environment for coordination-critical\n  multi-agent tasks", "author": "Yannick Molinghen and Rapha\u00ebl Avalos and Mark Van Achter and Ann Now\u00e9 and Tom Lenaerts", "abstract": "  We introduce the Laser Learning Environment (LLE), a collaborative\nmulti-agent reinforcement learning environment in which coordination is\ncentral. In LLE, agents depend on each other to make progress\n(interdependence), must jointly take specific sequences of actions to succeed\n(perfect coordination), and accomplishing those joint actions does not yield\nany intermediate reward (zero-incentive dynamics). The challenge of such\nproblems lies in the difficulty of escaping state space bottlenecks caused by\ninterdependence steps since escaping those bottlenecks is not rewarded. We test\nmultiple state-of-the-art value-based MARL algorithms against LLE and show that\nthey consistently fail at the collaborative task because of their inability to\nescape state space bottlenecks, even though they successfully achieve perfect\ncoordination. We show that Q-learning extensions such as prioritized experience\nreplay and n-steps return hinder exploration in environments with\nzero-incentive dynamics, and find that intrinsic curiosity with random network\ndistillation is not sufficient to escape those bottlenecks. We demonstrate the\nneed for novel methods to solve this problem and the relevance of LLE as\ncooperative MARL benchmark.\n", "link": "http://arxiv.org/abs/2404.03596v1", "date": "2024-04-04", "relevancy": 2.0527, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5415}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5344}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Laser%20Learning%20Environment%3A%20A%20new%20environment%20for%20coordination-critical%0A%20%20multi-agent%20tasks&body=Title%3A%20Laser%20Learning%20Environment%3A%20A%20new%20environment%20for%20coordination-critical%0A%20%20multi-agent%20tasks%0AAuthor%3A%20Yannick%20Molinghen%20and%20Rapha%C3%ABl%20Avalos%20and%20Mark%20Van%20Achter%20and%20Ann%20Now%C3%A9%20and%20Tom%20Lenaerts%0AAbstract%3A%20%20%20We%20introduce%20the%20Laser%20Learning%20Environment%20%28LLE%29%2C%20a%20collaborative%0Amulti-agent%20reinforcement%20learning%20environment%20in%20which%20coordination%20is%0Acentral.%20In%20LLE%2C%20agents%20depend%20on%20each%20other%20to%20make%20progress%0A%28interdependence%29%2C%20must%20jointly%20take%20specific%20sequences%20of%20actions%20to%20succeed%0A%28perfect%20coordination%29%2C%20and%20accomplishing%20those%20joint%20actions%20does%20not%20yield%0Aany%20intermediate%20reward%20%28zero-incentive%20dynamics%29.%20The%20challenge%20of%20such%0Aproblems%20lies%20in%20the%20difficulty%20of%20escaping%20state%20space%20bottlenecks%20caused%20by%0Ainterdependence%20steps%20since%20escaping%20those%20bottlenecks%20is%20not%20rewarded.%20We%20test%0Amultiple%20state-of-the-art%20value-based%20MARL%20algorithms%20against%20LLE%20and%20show%20that%0Athey%20consistently%20fail%20at%20the%20collaborative%20task%20because%20of%20their%20inability%20to%0Aescape%20state%20space%20bottlenecks%2C%20even%20though%20they%20successfully%20achieve%20perfect%0Acoordination.%20We%20show%20that%20Q-learning%20extensions%20such%20as%20prioritized%20experience%0Areplay%20and%20n-steps%20return%20hinder%20exploration%20in%20environments%20with%0Azero-incentive%20dynamics%2C%20and%20find%20that%20intrinsic%20curiosity%20with%20random%20network%0Adistillation%20is%20not%20sufficient%20to%20escape%20those%20bottlenecks.%20We%20demonstrate%20the%0Aneed%20for%20novel%20methods%20to%20solve%20this%20problem%20and%20the%20relevance%20of%20LLE%20as%0Acooperative%20MARL%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03596v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laser%20Learning%20Environment%3A%20A%20new%20environment%20for%20coordination-critical%0A%20%20multi-agent%20tasks&entry.906535625=Yannick%20Molinghen%20and%20Rapha%C3%ABl%20Avalos%20and%20Mark%20Van%20Achter%20and%20Ann%20Now%C3%A9%20and%20Tom%20Lenaerts&entry.1292438233=%20%20We%20introduce%20the%20Laser%20Learning%20Environment%20%28LLE%29%2C%20a%20collaborative%0Amulti-agent%20reinforcement%20learning%20environment%20in%20which%20coordination%20is%0Acentral.%20In%20LLE%2C%20agents%20depend%20on%20each%20other%20to%20make%20progress%0A%28interdependence%29%2C%20must%20jointly%20take%20specific%20sequences%20of%20actions%20to%20succeed%0A%28perfect%20coordination%29%2C%20and%20accomplishing%20those%20joint%20actions%20does%20not%20yield%0Aany%20intermediate%20reward%20%28zero-incentive%20dynamics%29.%20The%20challenge%20of%20such%0Aproblems%20lies%20in%20the%20difficulty%20of%20escaping%20state%20space%20bottlenecks%20caused%20by%0Ainterdependence%20steps%20since%20escaping%20those%20bottlenecks%20is%20not%20rewarded.%20We%20test%0Amultiple%20state-of-the-art%20value-based%20MARL%20algorithms%20against%20LLE%20and%20show%20that%0Athey%20consistently%20fail%20at%20the%20collaborative%20task%20because%20of%20their%20inability%20to%0Aescape%20state%20space%20bottlenecks%2C%20even%20though%20they%20successfully%20achieve%20perfect%0Acoordination.%20We%20show%20that%20Q-learning%20extensions%20such%20as%20prioritized%20experience%0Areplay%20and%20n-steps%20return%20hinder%20exploration%20in%20environments%20with%0Azero-incentive%20dynamics%2C%20and%20find%20that%20intrinsic%20curiosity%20with%20random%20network%0Adistillation%20is%20not%20sufficient%20to%20escape%20those%20bottlenecks.%20We%20demonstrate%20the%0Aneed%20for%20novel%20methods%20to%20solve%20this%20problem%20and%20the%20relevance%20of%20LLE%20as%0Acooperative%20MARL%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03596v1&entry.124074799=Read"},
{"title": "Unveiling LLMs: The Evolution of Latent Representations in a Temporal\n  Knowledge Graph", "author": "Marco Bronzini and Carlo Nicolini and Bruno Lepri and Jacopo Staiano and Andrea Passerini", "abstract": "  Large Language Models (LLMs) demonstrate an impressive capacity to recall a\nvast range of common factual knowledge information. However, unravelling the\nunderlying reasoning of LLMs and explaining their internal mechanisms of\nexploiting this factual knowledge remain active areas of investigation. Our\nwork analyzes the factual knowledge encoded in the latent representation of\nLLMs when prompted to assess the truthfulness of factual claims. We propose an\nend-to-end framework that jointly decodes the factual knowledge embedded in the\nlatent space of LLMs from a vector space to a set of ground predicates and\nrepresents its evolution across the layers using a temporal knowledge graph.\nOur framework relies on the technique of activation patching which intervenes\nin the inference computation of a model by dynamically altering its latent\nrepresentations. Consequently, we neither rely on external models nor training\nprocesses. We showcase our framework with local and global interpretability\nanalyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The\nlocal interpretability analysis exposes different latent errors from\nrepresentation to multi-hop reasoning errors. On the other hand, the global\nanalysis uncovered patterns in the underlying evolution of the model's factual\nknowledge (e.g., store-and-seek factual information). By enabling graph-based\nanalyses of the latent representations, this work represents a step towards the\nmechanistic interpretability of LLMs.\n", "link": "http://arxiv.org/abs/2404.03623v1", "date": "2024-04-04", "relevancy": 2.0467, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5083}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unveiling%20LLMs%3A%20The%20Evolution%20of%20Latent%20Representations%20in%20a%20Temporal%0A%20%20Knowledge%20Graph&body=Title%3A%20Unveiling%20LLMs%3A%20The%20Evolution%20of%20Latent%20Representations%20in%20a%20Temporal%0A%20%20Knowledge%20Graph%0AAuthor%3A%20Marco%20Bronzini%20and%20Carlo%20Nicolini%20and%20Bruno%20Lepri%20and%20Jacopo%20Staiano%20and%20Andrea%20Passerini%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20an%20impressive%20capacity%20to%20recall%20a%0Avast%20range%20of%20common%20factual%20knowledge%20information.%20However%2C%20unravelling%20the%0Aunderlying%20reasoning%20of%20LLMs%20and%20explaining%20their%20internal%20mechanisms%20of%0Aexploiting%20this%20factual%20knowledge%20remain%20active%20areas%20of%20investigation.%20Our%0Awork%20analyzes%20the%20factual%20knowledge%20encoded%20in%20the%20latent%20representation%20of%0ALLMs%20when%20prompted%20to%20assess%20the%20truthfulness%20of%20factual%20claims.%20We%20propose%20an%0Aend-to-end%20framework%20that%20jointly%20decodes%20the%20factual%20knowledge%20embedded%20in%20the%0Alatent%20space%20of%20LLMs%20from%20a%20vector%20space%20to%20a%20set%20of%20ground%20predicates%20and%0Arepresents%20its%20evolution%20across%20the%20layers%20using%20a%20temporal%20knowledge%20graph.%0AOur%20framework%20relies%20on%20the%20technique%20of%20activation%20patching%20which%20intervenes%0Ain%20the%20inference%20computation%20of%20a%20model%20by%20dynamically%20altering%20its%20latent%0Arepresentations.%20Consequently%2C%20we%20neither%20rely%20on%20external%20models%20nor%20training%0Aprocesses.%20We%20showcase%20our%20framework%20with%20local%20and%20global%20interpretability%0Aanalyses%20using%20two%20claim%20verification%20datasets%3A%20FEVER%20and%20CLIMATE-FEVER.%20The%0Alocal%20interpretability%20analysis%20exposes%20different%20latent%20errors%20from%0Arepresentation%20to%20multi-hop%20reasoning%20errors.%20On%20the%20other%20hand%2C%20the%20global%0Aanalysis%20uncovered%20patterns%20in%20the%20underlying%20evolution%20of%20the%20model%27s%20factual%0Aknowledge%20%28e.g.%2C%20store-and-seek%20factual%20information%29.%20By%20enabling%20graph-based%0Aanalyses%20of%20the%20latent%20representations%2C%20this%20work%20represents%20a%20step%20towards%20the%0Amechanistic%20interpretability%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03623v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20LLMs%3A%20The%20Evolution%20of%20Latent%20Representations%20in%20a%20Temporal%0A%20%20Knowledge%20Graph&entry.906535625=Marco%20Bronzini%20and%20Carlo%20Nicolini%20and%20Bruno%20Lepri%20and%20Jacopo%20Staiano%20and%20Andrea%20Passerini&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20an%20impressive%20capacity%20to%20recall%20a%0Avast%20range%20of%20common%20factual%20knowledge%20information.%20However%2C%20unravelling%20the%0Aunderlying%20reasoning%20of%20LLMs%20and%20explaining%20their%20internal%20mechanisms%20of%0Aexploiting%20this%20factual%20knowledge%20remain%20active%20areas%20of%20investigation.%20Our%0Awork%20analyzes%20the%20factual%20knowledge%20encoded%20in%20the%20latent%20representation%20of%0ALLMs%20when%20prompted%20to%20assess%20the%20truthfulness%20of%20factual%20claims.%20We%20propose%20an%0Aend-to-end%20framework%20that%20jointly%20decodes%20the%20factual%20knowledge%20embedded%20in%20the%0Alatent%20space%20of%20LLMs%20from%20a%20vector%20space%20to%20a%20set%20of%20ground%20predicates%20and%0Arepresents%20its%20evolution%20across%20the%20layers%20using%20a%20temporal%20knowledge%20graph.%0AOur%20framework%20relies%20on%20the%20technique%20of%20activation%20patching%20which%20intervenes%0Ain%20the%20inference%20computation%20of%20a%20model%20by%20dynamically%20altering%20its%20latent%0Arepresentations.%20Consequently%2C%20we%20neither%20rely%20on%20external%20models%20nor%20training%0Aprocesses.%20We%20showcase%20our%20framework%20with%20local%20and%20global%20interpretability%0Aanalyses%20using%20two%20claim%20verification%20datasets%3A%20FEVER%20and%20CLIMATE-FEVER.%20The%0Alocal%20interpretability%20analysis%20exposes%20different%20latent%20errors%20from%0Arepresentation%20to%20multi-hop%20reasoning%20errors.%20On%20the%20other%20hand%2C%20the%20global%0Aanalysis%20uncovered%20patterns%20in%20the%20underlying%20evolution%20of%20the%20model%27s%20factual%0Aknowledge%20%28e.g.%2C%20store-and-seek%20factual%20information%29.%20By%20enabling%20graph-based%0Aanalyses%20of%20the%20latent%20representations%2C%20this%20work%20represents%20a%20step%20towards%20the%0Amechanistic%20interpretability%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03623v1&entry.124074799=Read"},
{"title": "EulerFormer: Sequential User Behavior Modeling with Complex Vector\n  Attention", "author": "Zhen Tian and Wayne Xin Zhao and Changwang Zhang and Xin Zhao and Zhongrui Ma and Ji-Rong Wen", "abstract": "  To capture user preference, transformer models have been widely applied to\nmodel sequential user behavior data. The core of transformer architecture lies\nin the self-attention mechanism, which computes the pairwise attention scores\nin a sequence. Due to the permutation-equivariant nature, positional encoding\nis used to enhance the attention between token representations. In this\nsetting, the pairwise attention scores can be derived by both semantic\ndifference and positional difference. However, prior studies often model the\ntwo kinds of difference measurements in different ways, which potentially\nlimits the expressive capacity of sequence modeling. To address this issue,\nthis paper proposes a novel transformer variant with complex vector attention,\nnamed EulerFormer, which provides a unified theoretical framework to formulate\nboth semantic difference and positional difference. The EulerFormer involves\ntwo key technical improvements. First, it employs a new transformation function\nfor efficiently transforming the sequence tokens into polar-form complex\nvectors using Euler's formula, enabling the unified modeling of both semantic\nand positional information in a complex rotation form.Secondly, it develops a\ndifferential rotation mechanism, where the semantic rotation angles can be\ncontrolled by an adaptation function, enabling the adaptive integration of the\nsemantic and positional information according to the semantic\ncontexts.Furthermore, a phase contrastive learning task is proposed to improve\nthe isotropy of contextual representations in EulerFormer. Our theoretical\nframework possesses a high degree of completeness and generality. It is more\nrobust to semantic variations and possesses moresuperior theoretical properties\nin principle. Extensive experiments conducted on four public datasets\ndemonstrate the effectiveness and efficiency of our approach.\n", "link": "http://arxiv.org/abs/2403.17729v2", "date": "2024-04-04", "relevancy": 2.0451, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4919}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EulerFormer%3A%20Sequential%20User%20Behavior%20Modeling%20with%20Complex%20Vector%0A%20%20Attention&body=Title%3A%20EulerFormer%3A%20Sequential%20User%20Behavior%20Modeling%20with%20Complex%20Vector%0A%20%20Attention%0AAuthor%3A%20Zhen%20Tian%20and%20Wayne%20Xin%20Zhao%20and%20Changwang%20Zhang%20and%20Xin%20Zhao%20and%20Zhongrui%20Ma%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20To%20capture%20user%20preference%2C%20transformer%20models%20have%20been%20widely%20applied%20to%0Amodel%20sequential%20user%20behavior%20data.%20The%20core%20of%20transformer%20architecture%20lies%0Ain%20the%20self-attention%20mechanism%2C%20which%20computes%20the%20pairwise%20attention%20scores%0Ain%20a%20sequence.%20Due%20to%20the%20permutation-equivariant%20nature%2C%20positional%20encoding%0Ais%20used%20to%20enhance%20the%20attention%20between%20token%20representations.%20In%20this%0Asetting%2C%20the%20pairwise%20attention%20scores%20can%20be%20derived%20by%20both%20semantic%0Adifference%20and%20positional%20difference.%20However%2C%20prior%20studies%20often%20model%20the%0Atwo%20kinds%20of%20difference%20measurements%20in%20different%20ways%2C%20which%20potentially%0Alimits%20the%20expressive%20capacity%20of%20sequence%20modeling.%20To%20address%20this%20issue%2C%0Athis%20paper%20proposes%20a%20novel%20transformer%20variant%20with%20complex%20vector%20attention%2C%0Anamed%20EulerFormer%2C%20which%20provides%20a%20unified%20theoretical%20framework%20to%20formulate%0Aboth%20semantic%20difference%20and%20positional%20difference.%20The%20EulerFormer%20involves%0Atwo%20key%20technical%20improvements.%20First%2C%20it%20employs%20a%20new%20transformation%20function%0Afor%20efficiently%20transforming%20the%20sequence%20tokens%20into%20polar-form%20complex%0Avectors%20using%20Euler%27s%20formula%2C%20enabling%20the%20unified%20modeling%20of%20both%20semantic%0Aand%20positional%20information%20in%20a%20complex%20rotation%20form.Secondly%2C%20it%20develops%20a%0Adifferential%20rotation%20mechanism%2C%20where%20the%20semantic%20rotation%20angles%20can%20be%0Acontrolled%20by%20an%20adaptation%20function%2C%20enabling%20the%20adaptive%20integration%20of%20the%0Asemantic%20and%20positional%20information%20according%20to%20the%20semantic%0Acontexts.Furthermore%2C%20a%20phase%20contrastive%20learning%20task%20is%20proposed%20to%20improve%0Athe%20isotropy%20of%20contextual%20representations%20in%20EulerFormer.%20Our%20theoretical%0Aframework%20possesses%20a%20high%20degree%20of%20completeness%20and%20generality.%20It%20is%20more%0Arobust%20to%20semantic%20variations%20and%20possesses%20moresuperior%20theoretical%20properties%0Ain%20principle.%20Extensive%20experiments%20conducted%20on%20four%20public%20datasets%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17729v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EulerFormer%3A%20Sequential%20User%20Behavior%20Modeling%20with%20Complex%20Vector%0A%20%20Attention&entry.906535625=Zhen%20Tian%20and%20Wayne%20Xin%20Zhao%20and%20Changwang%20Zhang%20and%20Xin%20Zhao%20and%20Zhongrui%20Ma%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20To%20capture%20user%20preference%2C%20transformer%20models%20have%20been%20widely%20applied%20to%0Amodel%20sequential%20user%20behavior%20data.%20The%20core%20of%20transformer%20architecture%20lies%0Ain%20the%20self-attention%20mechanism%2C%20which%20computes%20the%20pairwise%20attention%20scores%0Ain%20a%20sequence.%20Due%20to%20the%20permutation-equivariant%20nature%2C%20positional%20encoding%0Ais%20used%20to%20enhance%20the%20attention%20between%20token%20representations.%20In%20this%0Asetting%2C%20the%20pairwise%20attention%20scores%20can%20be%20derived%20by%20both%20semantic%0Adifference%20and%20positional%20difference.%20However%2C%20prior%20studies%20often%20model%20the%0Atwo%20kinds%20of%20difference%20measurements%20in%20different%20ways%2C%20which%20potentially%0Alimits%20the%20expressive%20capacity%20of%20sequence%20modeling.%20To%20address%20this%20issue%2C%0Athis%20paper%20proposes%20a%20novel%20transformer%20variant%20with%20complex%20vector%20attention%2C%0Anamed%20EulerFormer%2C%20which%20provides%20a%20unified%20theoretical%20framework%20to%20formulate%0Aboth%20semantic%20difference%20and%20positional%20difference.%20The%20EulerFormer%20involves%0Atwo%20key%20technical%20improvements.%20First%2C%20it%20employs%20a%20new%20transformation%20function%0Afor%20efficiently%20transforming%20the%20sequence%20tokens%20into%20polar-form%20complex%0Avectors%20using%20Euler%27s%20formula%2C%20enabling%20the%20unified%20modeling%20of%20both%20semantic%0Aand%20positional%20information%20in%20a%20complex%20rotation%20form.Secondly%2C%20it%20develops%20a%0Adifferential%20rotation%20mechanism%2C%20where%20the%20semantic%20rotation%20angles%20can%20be%0Acontrolled%20by%20an%20adaptation%20function%2C%20enabling%20the%20adaptive%20integration%20of%20the%0Asemantic%20and%20positional%20information%20according%20to%20the%20semantic%0Acontexts.Furthermore%2C%20a%20phase%20contrastive%20learning%20task%20is%20proposed%20to%20improve%0Athe%20isotropy%20of%20contextual%20representations%20in%20EulerFormer.%20Our%20theoretical%0Aframework%20possesses%20a%20high%20degree%20of%20completeness%20and%20generality.%20It%20is%20more%0Arobust%20to%20semantic%20variations%20and%20possesses%20moresuperior%20theoretical%20properties%0Ain%20principle.%20Extensive%20experiments%20conducted%20on%20four%20public%20datasets%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17729v2&entry.124074799=Read"},
{"title": "Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive\n  Model-Aware Approach", "author": "Chengkai Huang and Rui Wang and Kaige Xie and Tong Yu and Lina Yao", "abstract": "  Retrieval-augmented large language models (LLMs) have been remarkably\ncompetent in various NLP tasks. Despite their great success, the knowledge\nprovided by the retrieval process is not always useful for improving the model\nprediction, since in some samples LLMs may already be quite knowledgeable and\nthus be able to answer the question correctly without retrieval. Aiming to save\nthe cost of retrieval, previous work has proposed to determine when to do/skip\nthe retrieval in a data-aware manner by analyzing the LLMs' pretraining data.\nHowever, these data-aware methods pose privacy risks and memory limitations,\nespecially when requiring access to sensitive or extensive pretraining data.\nMoreover, these methods offer limited adaptability under fine-tuning or\ncontinual learning settings. We hypothesize that token embeddings are able to\ncapture the model's intrinsic knowledge, which offers a safer and more\nstraightforward way to judge the need for retrieval without the privacy risks\nassociated with accessing pre-training data. Moreover, it alleviates the need\nto retain all the data utilized during model pre-training, necessitating only\nthe upkeep of the token embeddings. Extensive experiments and in-depth analyses\ndemonstrate the superiority of our model-aware approach.\n", "link": "http://arxiv.org/abs/2404.03514v1", "date": "2024-04-04", "relevancy": 2.0394, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5204}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.489}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learn%20When%20%28not%29%20to%20Trust%20Language%20Models%3A%20A%20Privacy-Centric%20Adaptive%0A%20%20Model-Aware%20Approach&body=Title%3A%20Learn%20When%20%28not%29%20to%20Trust%20Language%20Models%3A%20A%20Privacy-Centric%20Adaptive%0A%20%20Model-Aware%20Approach%0AAuthor%3A%20Chengkai%20Huang%20and%20Rui%20Wang%20and%20Kaige%20Xie%20and%20Tong%20Yu%20and%20Lina%20Yao%0AAbstract%3A%20%20%20Retrieval-augmented%20large%20language%20models%20%28LLMs%29%20have%20been%20remarkably%0Acompetent%20in%20various%20NLP%20tasks.%20Despite%20their%20great%20success%2C%20the%20knowledge%0Aprovided%20by%20the%20retrieval%20process%20is%20not%20always%20useful%20for%20improving%20the%20model%0Aprediction%2C%20since%20in%20some%20samples%20LLMs%20may%20already%20be%20quite%20knowledgeable%20and%0Athus%20be%20able%20to%20answer%20the%20question%20correctly%20without%20retrieval.%20Aiming%20to%20save%0Athe%20cost%20of%20retrieval%2C%20previous%20work%20has%20proposed%20to%20determine%20when%20to%20do/skip%0Athe%20retrieval%20in%20a%20data-aware%20manner%20by%20analyzing%20the%20LLMs%27%20pretraining%20data.%0AHowever%2C%20these%20data-aware%20methods%20pose%20privacy%20risks%20and%20memory%20limitations%2C%0Aespecially%20when%20requiring%20access%20to%20sensitive%20or%20extensive%20pretraining%20data.%0AMoreover%2C%20these%20methods%20offer%20limited%20adaptability%20under%20fine-tuning%20or%0Acontinual%20learning%20settings.%20We%20hypothesize%20that%20token%20embeddings%20are%20able%20to%0Acapture%20the%20model%27s%20intrinsic%20knowledge%2C%20which%20offers%20a%20safer%20and%20more%0Astraightforward%20way%20to%20judge%20the%20need%20for%20retrieval%20without%20the%20privacy%20risks%0Aassociated%20with%20accessing%20pre-training%20data.%20Moreover%2C%20it%20alleviates%20the%20need%0Ato%20retain%20all%20the%20data%20utilized%20during%20model%20pre-training%2C%20necessitating%20only%0Athe%20upkeep%20of%20the%20token%20embeddings.%20Extensive%20experiments%20and%20in-depth%20analyses%0Ademonstrate%20the%20superiority%20of%20our%20model-aware%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03514v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20When%20%28not%29%20to%20Trust%20Language%20Models%3A%20A%20Privacy-Centric%20Adaptive%0A%20%20Model-Aware%20Approach&entry.906535625=Chengkai%20Huang%20and%20Rui%20Wang%20and%20Kaige%20Xie%20and%20Tong%20Yu%20and%20Lina%20Yao&entry.1292438233=%20%20Retrieval-augmented%20large%20language%20models%20%28LLMs%29%20have%20been%20remarkably%0Acompetent%20in%20various%20NLP%20tasks.%20Despite%20their%20great%20success%2C%20the%20knowledge%0Aprovided%20by%20the%20retrieval%20process%20is%20not%20always%20useful%20for%20improving%20the%20model%0Aprediction%2C%20since%20in%20some%20samples%20LLMs%20may%20already%20be%20quite%20knowledgeable%20and%0Athus%20be%20able%20to%20answer%20the%20question%20correctly%20without%20retrieval.%20Aiming%20to%20save%0Athe%20cost%20of%20retrieval%2C%20previous%20work%20has%20proposed%20to%20determine%20when%20to%20do/skip%0Athe%20retrieval%20in%20a%20data-aware%20manner%20by%20analyzing%20the%20LLMs%27%20pretraining%20data.%0AHowever%2C%20these%20data-aware%20methods%20pose%20privacy%20risks%20and%20memory%20limitations%2C%0Aespecially%20when%20requiring%20access%20to%20sensitive%20or%20extensive%20pretraining%20data.%0AMoreover%2C%20these%20methods%20offer%20limited%20adaptability%20under%20fine-tuning%20or%0Acontinual%20learning%20settings.%20We%20hypothesize%20that%20token%20embeddings%20are%20able%20to%0Acapture%20the%20model%27s%20intrinsic%20knowledge%2C%20which%20offers%20a%20safer%20and%20more%0Astraightforward%20way%20to%20judge%20the%20need%20for%20retrieval%20without%20the%20privacy%20risks%0Aassociated%20with%20accessing%20pre-training%20data.%20Moreover%2C%20it%20alleviates%20the%20need%0Ato%20retain%20all%20the%20data%20utilized%20during%20model%20pre-training%2C%20necessitating%20only%0Athe%20upkeep%20of%20the%20token%20embeddings.%20Extensive%20experiments%20and%20in-depth%20analyses%0Ademonstrate%20the%20superiority%20of%20our%20model-aware%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03514v1&entry.124074799=Read"},
{"title": "Meta Invariance Defense Towards Generalizable Robustness to Unknown\n  Adversarial Attacks", "author": "Lei Zhang and Yuhang Zhou and Yi Yang and Xinbo Gao", "abstract": "  Despite providing high-performance solutions for computer vision tasks, the\ndeep neural network (DNN) model has been proved to be extremely vulnerable to\nadversarial attacks. Current defense mainly focuses on the known attacks, but\nthe adversarial robustness to the unknown attacks is seriously overlooked.\nBesides, commonly used adaptive learning and fine-tuning technique is\nunsuitable for adversarial defense since it is essentially a zero-shot problem\nwhen deployed. Thus, to tackle this challenge, we propose an attack-agnostic\ndefense method named Meta Invariance Defense (MID). Specifically, various\ncombinations of adversarial attacks are randomly sampled from a manually\nconstructed Attacker Pool to constitute different defense tasks against unknown\nattacks, in which a student encoder is supervised by multi-consistency\ndistillation to learn the attack-invariant features via a meta principle. The\nproposed MID has two merits: 1) Full distillation from pixel-, feature- and\nprediction-level between benign and adversarial samples facilitates the\ndiscovery of attack-invariance. 2) The model simultaneously achieves robustness\nto the imperceptible adversarial perturbations in high-level image\nclassification and attack-suppression in low-level robust image regeneration.\nTheoretical and empirical studies on numerous benchmarks such as ImageNet\nverify the generalizable robustness and superiority of MID under various\nattacks.\n", "link": "http://arxiv.org/abs/2404.03340v1", "date": "2024-04-04", "relevancy": 2.0276, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5325}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.503}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5006}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Meta%20Invariance%20Defense%20Towards%20Generalizable%20Robustness%20to%20Unknown%0A%20%20Adversarial%20Attacks&body=Title%3A%20Meta%20Invariance%20Defense%20Towards%20Generalizable%20Robustness%20to%20Unknown%0A%20%20Adversarial%20Attacks%0AAuthor%3A%20Lei%20Zhang%20and%20Yuhang%20Zhou%20and%20Yi%20Yang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Despite%20providing%20high-performance%20solutions%20for%20computer%20vision%20tasks%2C%20the%0Adeep%20neural%20network%20%28DNN%29%20model%20has%20been%20proved%20to%20be%20extremely%20vulnerable%20to%0Aadversarial%20attacks.%20Current%20defense%20mainly%20focuses%20on%20the%20known%20attacks%2C%20but%0Athe%20adversarial%20robustness%20to%20the%20unknown%20attacks%20is%20seriously%20overlooked.%0ABesides%2C%20commonly%20used%20adaptive%20learning%20and%20fine-tuning%20technique%20is%0Aunsuitable%20for%20adversarial%20defense%20since%20it%20is%20essentially%20a%20zero-shot%20problem%0Awhen%20deployed.%20Thus%2C%20to%20tackle%20this%20challenge%2C%20we%20propose%20an%20attack-agnostic%0Adefense%20method%20named%20Meta%20Invariance%20Defense%20%28MID%29.%20Specifically%2C%20various%0Acombinations%20of%20adversarial%20attacks%20are%20randomly%20sampled%20from%20a%20manually%0Aconstructed%20Attacker%20Pool%20to%20constitute%20different%20defense%20tasks%20against%20unknown%0Aattacks%2C%20in%20which%20a%20student%20encoder%20is%20supervised%20by%20multi-consistency%0Adistillation%20to%20learn%20the%20attack-invariant%20features%20via%20a%20meta%20principle.%20The%0Aproposed%20MID%20has%20two%20merits%3A%201%29%20Full%20distillation%20from%20pixel-%2C%20feature-%20and%0Aprediction-level%20between%20benign%20and%20adversarial%20samples%20facilitates%20the%0Adiscovery%20of%20attack-invariance.%202%29%20The%20model%20simultaneously%20achieves%20robustness%0Ato%20the%20imperceptible%20adversarial%20perturbations%20in%20high-level%20image%0Aclassification%20and%20attack-suppression%20in%20low-level%20robust%20image%20regeneration.%0ATheoretical%20and%20empirical%20studies%20on%20numerous%20benchmarks%20such%20as%20ImageNet%0Averify%20the%20generalizable%20robustness%20and%20superiority%20of%20MID%20under%20various%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03340v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta%20Invariance%20Defense%20Towards%20Generalizable%20Robustness%20to%20Unknown%0A%20%20Adversarial%20Attacks&entry.906535625=Lei%20Zhang%20and%20Yuhang%20Zhou%20and%20Yi%20Yang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Despite%20providing%20high-performance%20solutions%20for%20computer%20vision%20tasks%2C%20the%0Adeep%20neural%20network%20%28DNN%29%20model%20has%20been%20proved%20to%20be%20extremely%20vulnerable%20to%0Aadversarial%20attacks.%20Current%20defense%20mainly%20focuses%20on%20the%20known%20attacks%2C%20but%0Athe%20adversarial%20robustness%20to%20the%20unknown%20attacks%20is%20seriously%20overlooked.%0ABesides%2C%20commonly%20used%20adaptive%20learning%20and%20fine-tuning%20technique%20is%0Aunsuitable%20for%20adversarial%20defense%20since%20it%20is%20essentially%20a%20zero-shot%20problem%0Awhen%20deployed.%20Thus%2C%20to%20tackle%20this%20challenge%2C%20we%20propose%20an%20attack-agnostic%0Adefense%20method%20named%20Meta%20Invariance%20Defense%20%28MID%29.%20Specifically%2C%20various%0Acombinations%20of%20adversarial%20attacks%20are%20randomly%20sampled%20from%20a%20manually%0Aconstructed%20Attacker%20Pool%20to%20constitute%20different%20defense%20tasks%20against%20unknown%0Aattacks%2C%20in%20which%20a%20student%20encoder%20is%20supervised%20by%20multi-consistency%0Adistillation%20to%20learn%20the%20attack-invariant%20features%20via%20a%20meta%20principle.%20The%0Aproposed%20MID%20has%20two%20merits%3A%201%29%20Full%20distillation%20from%20pixel-%2C%20feature-%20and%0Aprediction-level%20between%20benign%20and%20adversarial%20samples%20facilitates%20the%0Adiscovery%20of%20attack-invariance.%202%29%20The%20model%20simultaneously%20achieves%20robustness%0Ato%20the%20imperceptible%20adversarial%20perturbations%20in%20high-level%20image%0Aclassification%20and%20attack-suppression%20in%20low-level%20robust%20image%20regeneration.%0ATheoretical%20and%20empirical%20studies%20on%20numerous%20benchmarks%20such%20as%20ImageNet%0Averify%20the%20generalizable%20robustness%20and%20superiority%20of%20MID%20under%20various%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03340v1&entry.124074799=Read"},
{"title": "On the Theoretical Expressive Power and the Design Space of Higher-Order\n  Graph Transformers", "author": "Cai Zhou and Rose Yu and Yusu Wang", "abstract": "  Graph transformers have recently received significant attention in graph\nlearning, partly due to their ability to capture more global interaction via\nself-attention. Nevertheless, while higher-order graph neural networks have\nbeen reasonably well studied, the exploration of extending graph transformers\nto higher-order variants is just starting. Both theoretical understanding and\nempirical results are limited. In this paper, we provide a systematic study of\nthe theoretical expressive power of order-$k$ graph transformers and sparse\nvariants. We first show that, an order-$k$ graph transformer without additional\nstructural information is less expressive than the $k$-Weisfeiler Lehman\n($k$-WL) test despite its high computational cost. We then explore strategies\nto both sparsify and enhance the higher-order graph transformers, aiming to\nimprove both their efficiency and expressiveness. Indeed, sparsification based\non neighborhood information can enhance the expressive power, as it provides\nadditional information about input graph structures. In particular, we show\nthat a natural neighborhood-based sparse order-$k$ transformer model is not\nonly computationally efficient, but also expressive -- as expressive as $k$-WL\ntest. We further study several other sparse graph attention models that are\ncomputationally efficient and provide their expressiveness analysis. Finally,\nwe provide experimental results to show the effectiveness of the different\nsparsification strategies.\n", "link": "http://arxiv.org/abs/2404.03380v1", "date": "2024-04-04", "relevancy": 2.0255, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5631}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5054}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4847}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Theoretical%20Expressive%20Power%20and%20the%20Design%20Space%20of%20Higher-Order%0A%20%20Graph%20Transformers&body=Title%3A%20On%20the%20Theoretical%20Expressive%20Power%20and%20the%20Design%20Space%20of%20Higher-Order%0A%20%20Graph%20Transformers%0AAuthor%3A%20Cai%20Zhou%20and%20Rose%20Yu%20and%20Yusu%20Wang%0AAbstract%3A%20%20%20Graph%20transformers%20have%20recently%20received%20significant%20attention%20in%20graph%0Alearning%2C%20partly%20due%20to%20their%20ability%20to%20capture%20more%20global%20interaction%20via%0Aself-attention.%20Nevertheless%2C%20while%20higher-order%20graph%20neural%20networks%20have%0Abeen%20reasonably%20well%20studied%2C%20the%20exploration%20of%20extending%20graph%20transformers%0Ato%20higher-order%20variants%20is%20just%20starting.%20Both%20theoretical%20understanding%20and%0Aempirical%20results%20are%20limited.%20In%20this%20paper%2C%20we%20provide%20a%20systematic%20study%20of%0Athe%20theoretical%20expressive%20power%20of%20order-%24k%24%20graph%20transformers%20and%20sparse%0Avariants.%20We%20first%20show%20that%2C%20an%20order-%24k%24%20graph%20transformer%20without%20additional%0Astructural%20information%20is%20less%20expressive%20than%20the%20%24k%24-Weisfeiler%20Lehman%0A%28%24k%24-WL%29%20test%20despite%20its%20high%20computational%20cost.%20We%20then%20explore%20strategies%0Ato%20both%20sparsify%20and%20enhance%20the%20higher-order%20graph%20transformers%2C%20aiming%20to%0Aimprove%20both%20their%20efficiency%20and%20expressiveness.%20Indeed%2C%20sparsification%20based%0Aon%20neighborhood%20information%20can%20enhance%20the%20expressive%20power%2C%20as%20it%20provides%0Aadditional%20information%20about%20input%20graph%20structures.%20In%20particular%2C%20we%20show%0Athat%20a%20natural%20neighborhood-based%20sparse%20order-%24k%24%20transformer%20model%20is%20not%0Aonly%20computationally%20efficient%2C%20but%20also%20expressive%20--%20as%20expressive%20as%20%24k%24-WL%0Atest.%20We%20further%20study%20several%20other%20sparse%20graph%20attention%20models%20that%20are%0Acomputationally%20efficient%20and%20provide%20their%20expressiveness%20analysis.%20Finally%2C%0Awe%20provide%20experimental%20results%20to%20show%20the%20effectiveness%20of%20the%20different%0Asparsification%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03380v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Theoretical%20Expressive%20Power%20and%20the%20Design%20Space%20of%20Higher-Order%0A%20%20Graph%20Transformers&entry.906535625=Cai%20Zhou%20and%20Rose%20Yu%20and%20Yusu%20Wang&entry.1292438233=%20%20Graph%20transformers%20have%20recently%20received%20significant%20attention%20in%20graph%0Alearning%2C%20partly%20due%20to%20their%20ability%20to%20capture%20more%20global%20interaction%20via%0Aself-attention.%20Nevertheless%2C%20while%20higher-order%20graph%20neural%20networks%20have%0Abeen%20reasonably%20well%20studied%2C%20the%20exploration%20of%20extending%20graph%20transformers%0Ato%20higher-order%20variants%20is%20just%20starting.%20Both%20theoretical%20understanding%20and%0Aempirical%20results%20are%20limited.%20In%20this%20paper%2C%20we%20provide%20a%20systematic%20study%20of%0Athe%20theoretical%20expressive%20power%20of%20order-%24k%24%20graph%20transformers%20and%20sparse%0Avariants.%20We%20first%20show%20that%2C%20an%20order-%24k%24%20graph%20transformer%20without%20additional%0Astructural%20information%20is%20less%20expressive%20than%20the%20%24k%24-Weisfeiler%20Lehman%0A%28%24k%24-WL%29%20test%20despite%20its%20high%20computational%20cost.%20We%20then%20explore%20strategies%0Ato%20both%20sparsify%20and%20enhance%20the%20higher-order%20graph%20transformers%2C%20aiming%20to%0Aimprove%20both%20their%20efficiency%20and%20expressiveness.%20Indeed%2C%20sparsification%20based%0Aon%20neighborhood%20information%20can%20enhance%20the%20expressive%20power%2C%20as%20it%20provides%0Aadditional%20information%20about%20input%20graph%20structures.%20In%20particular%2C%20we%20show%0Athat%20a%20natural%20neighborhood-based%20sparse%20order-%24k%24%20transformer%20model%20is%20not%0Aonly%20computationally%20efficient%2C%20but%20also%20expressive%20--%20as%20expressive%20as%20%24k%24-WL%0Atest.%20We%20further%20study%20several%20other%20sparse%20graph%20attention%20models%20that%20are%0Acomputationally%20efficient%20and%20provide%20their%20expressiveness%20analysis.%20Finally%2C%0Awe%20provide%20experimental%20results%20to%20show%20the%20effectiveness%20of%20the%20different%0Asparsification%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03380v1&entry.124074799=Read"},
{"title": "On the Efficiency of Convolutional Neural Networks", "author": "Andrew Lavin", "abstract": "  Since the breakthrough performance of AlexNet in 2012, convolutional neural\nnetworks (convnets) have grown into extremely powerful vision models. Deep\nlearning researchers have used convnets to produce accurate results that were\nunachievable a decade ago. Yet computer scientists make computational\nefficiency their primary objective. Accuracy with exorbitant cost is not\nacceptable; an algorithm must also minimize its computational requirements.\nConfronted with the daunting computation that convnets use, deep learning\nresearchers also became interested in efficiency. Researchers applied\ntremendous effort to find the convnet architectures that have the greatest\nefficiency. However, skepticism grew among researchers and engineers alike\nabout the relevance of arithmetic complexity. Contrary to the prevailing view\nthat latency and arithmetic complexity are irreconcilable, a simple formula\nrelates both through computational efficiency. This insight enabled us to\nco-optimize the separate factors that determine latency. We observed that the\ndegenerate conv2d layers that produce the best accuracy-complexity trade-off\nalso have low operational intensity. Therefore, kernels that implement these\nlayers use significant memory resources. We solved this optimization problem\nwith block-fusion kernels that implement all layers of a residual block,\nthereby creating temporal locality, avoiding communication, and reducing\nworkspace size. Our ConvFirst model with block-fusion kernels ran approximately\nfour times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal\naccuracy on the ImageNet-1K classification task. Our unified approach to\nconvnet efficiency envisions a new era of models and kernels that achieve\ngreater accuracy at lower cost.\n", "link": "http://arxiv.org/abs/2404.03617v1", "date": "2024-04-04", "relevancy": 2.0049, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5111}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5004}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4981}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Efficiency%20of%20Convolutional%20Neural%20Networks&body=Title%3A%20On%20the%20Efficiency%20of%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Andrew%20Lavin%0AAbstract%3A%20%20%20Since%20the%20breakthrough%20performance%20of%20AlexNet%20in%202012%2C%20convolutional%20neural%0Anetworks%20%28convnets%29%20have%20grown%20into%20extremely%20powerful%20vision%20models.%20Deep%0Alearning%20researchers%20have%20used%20convnets%20to%20produce%20accurate%20results%20that%20were%0Aunachievable%20a%20decade%20ago.%20Yet%20computer%20scientists%20make%20computational%0Aefficiency%20their%20primary%20objective.%20Accuracy%20with%20exorbitant%20cost%20is%20not%0Aacceptable%3B%20an%20algorithm%20must%20also%20minimize%20its%20computational%20requirements.%0AConfronted%20with%20the%20daunting%20computation%20that%20convnets%20use%2C%20deep%20learning%0Aresearchers%20also%20became%20interested%20in%20efficiency.%20Researchers%20applied%0Atremendous%20effort%20to%20find%20the%20convnet%20architectures%20that%20have%20the%20greatest%0Aefficiency.%20However%2C%20skepticism%20grew%20among%20researchers%20and%20engineers%20alike%0Aabout%20the%20relevance%20of%20arithmetic%20complexity.%20Contrary%20to%20the%20prevailing%20view%0Athat%20latency%20and%20arithmetic%20complexity%20are%20irreconcilable%2C%20a%20simple%20formula%0Arelates%20both%20through%20computational%20efficiency.%20This%20insight%20enabled%20us%20to%0Aco-optimize%20the%20separate%20factors%20that%20determine%20latency.%20We%20observed%20that%20the%0Adegenerate%20conv2d%20layers%20that%20produce%20the%20best%20accuracy-complexity%20trade-off%0Aalso%20have%20low%20operational%20intensity.%20Therefore%2C%20kernels%20that%20implement%20these%0Alayers%20use%20significant%20memory%20resources.%20We%20solved%20this%20optimization%20problem%0Awith%20block-fusion%20kernels%20that%20implement%20all%20layers%20of%20a%20residual%20block%2C%0Athereby%20creating%20temporal%20locality%2C%20avoiding%20communication%2C%20and%20reducing%0Aworkspace%20size.%20Our%20ConvFirst%20model%20with%20block-fusion%20kernels%20ran%20approximately%0Afour%20times%20as%20fast%20as%20the%20ConvNeXt%20baseline%20with%20PyTorch%20Inductor%2C%20at%20equal%0Aaccuracy%20on%20the%20ImageNet-1K%20classification%20task.%20Our%20unified%20approach%20to%0Aconvnet%20efficiency%20envisions%20a%20new%20era%20of%20models%20and%20kernels%20that%20achieve%0Agreater%20accuracy%20at%20lower%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03617v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Efficiency%20of%20Convolutional%20Neural%20Networks&entry.906535625=Andrew%20Lavin&entry.1292438233=%20%20Since%20the%20breakthrough%20performance%20of%20AlexNet%20in%202012%2C%20convolutional%20neural%0Anetworks%20%28convnets%29%20have%20grown%20into%20extremely%20powerful%20vision%20models.%20Deep%0Alearning%20researchers%20have%20used%20convnets%20to%20produce%20accurate%20results%20that%20were%0Aunachievable%20a%20decade%20ago.%20Yet%20computer%20scientists%20make%20computational%0Aefficiency%20their%20primary%20objective.%20Accuracy%20with%20exorbitant%20cost%20is%20not%0Aacceptable%3B%20an%20algorithm%20must%20also%20minimize%20its%20computational%20requirements.%0AConfronted%20with%20the%20daunting%20computation%20that%20convnets%20use%2C%20deep%20learning%0Aresearchers%20also%20became%20interested%20in%20efficiency.%20Researchers%20applied%0Atremendous%20effort%20to%20find%20the%20convnet%20architectures%20that%20have%20the%20greatest%0Aefficiency.%20However%2C%20skepticism%20grew%20among%20researchers%20and%20engineers%20alike%0Aabout%20the%20relevance%20of%20arithmetic%20complexity.%20Contrary%20to%20the%20prevailing%20view%0Athat%20latency%20and%20arithmetic%20complexity%20are%20irreconcilable%2C%20a%20simple%20formula%0Arelates%20both%20through%20computational%20efficiency.%20This%20insight%20enabled%20us%20to%0Aco-optimize%20the%20separate%20factors%20that%20determine%20latency.%20We%20observed%20that%20the%0Adegenerate%20conv2d%20layers%20that%20produce%20the%20best%20accuracy-complexity%20trade-off%0Aalso%20have%20low%20operational%20intensity.%20Therefore%2C%20kernels%20that%20implement%20these%0Alayers%20use%20significant%20memory%20resources.%20We%20solved%20this%20optimization%20problem%0Awith%20block-fusion%20kernels%20that%20implement%20all%20layers%20of%20a%20residual%20block%2C%0Athereby%20creating%20temporal%20locality%2C%20avoiding%20communication%2C%20and%20reducing%0Aworkspace%20size.%20Our%20ConvFirst%20model%20with%20block-fusion%20kernels%20ran%20approximately%0Afour%20times%20as%20fast%20as%20the%20ConvNeXt%20baseline%20with%20PyTorch%20Inductor%2C%20at%20equal%0Aaccuracy%20on%20the%20ImageNet-1K%20classification%20task.%20Our%20unified%20approach%20to%0Aconvnet%20efficiency%20envisions%20a%20new%20era%20of%20models%20and%20kernels%20that%20achieve%0Agreater%20accuracy%20at%20lower%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03617v1&entry.124074799=Read"},
{"title": "Chemist-X: Large Language Model-empowered Agent for Reaction Condition\n  Recommendation in Chemical Synthesis", "author": "Kexin Chen and Junyou Li and Kunyi Wang and Yuyang Du and Jiahui Yu and Jiamin Lu and Lanqing Li and Jiezhong Qiu and Jianzhang Pan and Yi Huang and Qun Fang and Pheng Ann Heng and Guangyong Chen", "abstract": "  Recent AI research plots a promising future of automatic chemical reactions\nwithin the chemistry society. This study proposes Chemist-X, a transformative\nAI agent that automates the reaction condition recommendation (RCR) task in\nchemical synthesis with retrieval-augmented generation (RAG) technology. To\nemulate expert chemists' strategies when solving RCR tasks, Chemist-X utilizes\nadvanced RAG schemes to interrogate online molecular databases and distill\ncritical data from the latest literature database. Further, the agent leverages\nstate-of-the-art computer-aided design (CAD) tools with a large language model\n(LLM) supervised programming interface. With the ability to utilize updated\nchemical knowledge and CAD tools, our agent significantly outperforms\nconventional synthesis AIs confined to the fixed knowledge within its training\ndata. Chemist-X considerably reduces chemists' workload and allows them to\nfocus on more fundamental and creative problems, thereby bringing closer\ncomputational techniques and chemical research and making a remarkable leap\ntoward harnessing AI's full capabilities in scientific discovery.\n", "link": "http://arxiv.org/abs/2311.10776v5", "date": "2024-04-04", "relevancy": 1.9851, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5851}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4798}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4772}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Chemist-X%3A%20Large%20Language%20Model-empowered%20Agent%20for%20Reaction%20Condition%0A%20%20Recommendation%20in%20Chemical%20Synthesis&body=Title%3A%20Chemist-X%3A%20Large%20Language%20Model-empowered%20Agent%20for%20Reaction%20Condition%0A%20%20Recommendation%20in%20Chemical%20Synthesis%0AAuthor%3A%20Kexin%20Chen%20and%20Junyou%20Li%20and%20Kunyi%20Wang%20and%20Yuyang%20Du%20and%20Jiahui%20Yu%20and%20Jiamin%20Lu%20and%20Lanqing%20Li%20and%20Jiezhong%20Qiu%20and%20Jianzhang%20Pan%20and%20Yi%20Huang%20and%20Qun%20Fang%20and%20Pheng%20Ann%20Heng%20and%20Guangyong%20Chen%0AAbstract%3A%20%20%20Recent%20AI%20research%20plots%20a%20promising%20future%20of%20automatic%20chemical%20reactions%0Awithin%20the%20chemistry%20society.%20This%20study%20proposes%20Chemist-X%2C%20a%20transformative%0AAI%20agent%20that%20automates%20the%20reaction%20condition%20recommendation%20%28RCR%29%20task%20in%0Achemical%20synthesis%20with%20retrieval-augmented%20generation%20%28RAG%29%20technology.%20To%0Aemulate%20expert%20chemists%27%20strategies%20when%20solving%20RCR%20tasks%2C%20Chemist-X%20utilizes%0Aadvanced%20RAG%20schemes%20to%20interrogate%20online%20molecular%20databases%20and%20distill%0Acritical%20data%20from%20the%20latest%20literature%20database.%20Further%2C%20the%20agent%20leverages%0Astate-of-the-art%20computer-aided%20design%20%28CAD%29%20tools%20with%20a%20large%20language%20model%0A%28LLM%29%20supervised%20programming%20interface.%20With%20the%20ability%20to%20utilize%20updated%0Achemical%20knowledge%20and%20CAD%20tools%2C%20our%20agent%20significantly%20outperforms%0Aconventional%20synthesis%20AIs%20confined%20to%20the%20fixed%20knowledge%20within%20its%20training%0Adata.%20Chemist-X%20considerably%20reduces%20chemists%27%20workload%20and%20allows%20them%20to%0Afocus%20on%20more%20fundamental%20and%20creative%20problems%2C%20thereby%20bringing%20closer%0Acomputational%20techniques%20and%20chemical%20research%20and%20making%20a%20remarkable%20leap%0Atoward%20harnessing%20AI%27s%20full%20capabilities%20in%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10776v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chemist-X%3A%20Large%20Language%20Model-empowered%20Agent%20for%20Reaction%20Condition%0A%20%20Recommendation%20in%20Chemical%20Synthesis&entry.906535625=Kexin%20Chen%20and%20Junyou%20Li%20and%20Kunyi%20Wang%20and%20Yuyang%20Du%20and%20Jiahui%20Yu%20and%20Jiamin%20Lu%20and%20Lanqing%20Li%20and%20Jiezhong%20Qiu%20and%20Jianzhang%20Pan%20and%20Yi%20Huang%20and%20Qun%20Fang%20and%20Pheng%20Ann%20Heng%20and%20Guangyong%20Chen&entry.1292438233=%20%20Recent%20AI%20research%20plots%20a%20promising%20future%20of%20automatic%20chemical%20reactions%0Awithin%20the%20chemistry%20society.%20This%20study%20proposes%20Chemist-X%2C%20a%20transformative%0AAI%20agent%20that%20automates%20the%20reaction%20condition%20recommendation%20%28RCR%29%20task%20in%0Achemical%20synthesis%20with%20retrieval-augmented%20generation%20%28RAG%29%20technology.%20To%0Aemulate%20expert%20chemists%27%20strategies%20when%20solving%20RCR%20tasks%2C%20Chemist-X%20utilizes%0Aadvanced%20RAG%20schemes%20to%20interrogate%20online%20molecular%20databases%20and%20distill%0Acritical%20data%20from%20the%20latest%20literature%20database.%20Further%2C%20the%20agent%20leverages%0Astate-of-the-art%20computer-aided%20design%20%28CAD%29%20tools%20with%20a%20large%20language%20model%0A%28LLM%29%20supervised%20programming%20interface.%20With%20the%20ability%20to%20utilize%20updated%0Achemical%20knowledge%20and%20CAD%20tools%2C%20our%20agent%20significantly%20outperforms%0Aconventional%20synthesis%20AIs%20confined%20to%20the%20fixed%20knowledge%20within%20its%20training%0Adata.%20Chemist-X%20considerably%20reduces%20chemists%27%20workload%20and%20allows%20them%20to%0Afocus%20on%20more%20fundamental%20and%20creative%20problems%2C%20thereby%20bringing%20closer%0Acomputational%20techniques%20and%20chemical%20research%20and%20making%20a%20remarkable%20leap%0Atoward%20harnessing%20AI%27s%20full%20capabilities%20in%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10776v5&entry.124074799=Read"},
{"title": "NMF-Based Analysis of Mobile Eye-Tracking Data", "author": "Daniel Kl\u00f6tzl and Tim Krake and Frank Heyen and Michael Becher and Maurice Koch and Daniel Weiskopf and Kuno Kurzhals", "abstract": "  The depiction of scanpaths from mobile eye-tracking recordings by thumbnails\nfrom the stimulus allows the application of visual computing to detect areas of\ninterest in an unsupervised way. We suggest using nonnegative matrix\nfactorization (NMF) to identify such areas in stimuli. For a user-defined\ninteger k, NMF produces an explainable decomposition into k components, each\nconsisting of a spatial representation associated with a temporal indicator. In\nthe context of multiple eye-tracking recordings, this leads to k spatial\nrepresentations, where the temporal indicator highlights the appearance within\nrecordings. The choice of k provides an opportunity to control the refinement\nof the decomposition, i.e., the number of areas to detect. We combine our\nNMF-based approach with visualization techniques to enable an exploratory\nanalysis of multiple recordings. Finally, we demonstrate the usefulness of our\napproach with mobile eye-tracking data of an art gallery.\n", "link": "http://arxiv.org/abs/2404.03417v1", "date": "2024-04-04", "relevancy": 1.9724, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5096}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4841}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4803}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NMF-Based%20Analysis%20of%20Mobile%20Eye-Tracking%20Data&body=Title%3A%20NMF-Based%20Analysis%20of%20Mobile%20Eye-Tracking%20Data%0AAuthor%3A%20Daniel%20Kl%C3%B6tzl%20and%20Tim%20Krake%20and%20Frank%20Heyen%20and%20Michael%20Becher%20and%20Maurice%20Koch%20and%20Daniel%20Weiskopf%20and%20Kuno%20Kurzhals%0AAbstract%3A%20%20%20The%20depiction%20of%20scanpaths%20from%20mobile%20eye-tracking%20recordings%20by%20thumbnails%0Afrom%20the%20stimulus%20allows%20the%20application%20of%20visual%20computing%20to%20detect%20areas%20of%0Ainterest%20in%20an%20unsupervised%20way.%20We%20suggest%20using%20nonnegative%20matrix%0Afactorization%20%28NMF%29%20to%20identify%20such%20areas%20in%20stimuli.%20For%20a%20user-defined%0Ainteger%20k%2C%20NMF%20produces%20an%20explainable%20decomposition%20into%20k%20components%2C%20each%0Aconsisting%20of%20a%20spatial%20representation%20associated%20with%20a%20temporal%20indicator.%20In%0Athe%20context%20of%20multiple%20eye-tracking%20recordings%2C%20this%20leads%20to%20k%20spatial%0Arepresentations%2C%20where%20the%20temporal%20indicator%20highlights%20the%20appearance%20within%0Arecordings.%20The%20choice%20of%20k%20provides%20an%20opportunity%20to%20control%20the%20refinement%0Aof%20the%20decomposition%2C%20i.e.%2C%20the%20number%20of%20areas%20to%20detect.%20We%20combine%20our%0ANMF-based%20approach%20with%20visualization%20techniques%20to%20enable%20an%20exploratory%0Aanalysis%20of%20multiple%20recordings.%20Finally%2C%20we%20demonstrate%20the%20usefulness%20of%20our%0Aapproach%20with%20mobile%20eye-tracking%20data%20of%20an%20art%20gallery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03417v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NMF-Based%20Analysis%20of%20Mobile%20Eye-Tracking%20Data&entry.906535625=Daniel%20Kl%C3%B6tzl%20and%20Tim%20Krake%20and%20Frank%20Heyen%20and%20Michael%20Becher%20and%20Maurice%20Koch%20and%20Daniel%20Weiskopf%20and%20Kuno%20Kurzhals&entry.1292438233=%20%20The%20depiction%20of%20scanpaths%20from%20mobile%20eye-tracking%20recordings%20by%20thumbnails%0Afrom%20the%20stimulus%20allows%20the%20application%20of%20visual%20computing%20to%20detect%20areas%20of%0Ainterest%20in%20an%20unsupervised%20way.%20We%20suggest%20using%20nonnegative%20matrix%0Afactorization%20%28NMF%29%20to%20identify%20such%20areas%20in%20stimuli.%20For%20a%20user-defined%0Ainteger%20k%2C%20NMF%20produces%20an%20explainable%20decomposition%20into%20k%20components%2C%20each%0Aconsisting%20of%20a%20spatial%20representation%20associated%20with%20a%20temporal%20indicator.%20In%0Athe%20context%20of%20multiple%20eye-tracking%20recordings%2C%20this%20leads%20to%20k%20spatial%0Arepresentations%2C%20where%20the%20temporal%20indicator%20highlights%20the%20appearance%20within%0Arecordings.%20The%20choice%20of%20k%20provides%20an%20opportunity%20to%20control%20the%20refinement%0Aof%20the%20decomposition%2C%20i.e.%2C%20the%20number%20of%20areas%20to%20detect.%20We%20combine%20our%0ANMF-based%20approach%20with%20visualization%20techniques%20to%20enable%20an%20exploratory%0Aanalysis%20of%20multiple%20recordings.%20Finally%2C%20we%20demonstrate%20the%20usefulness%20of%20our%0Aapproach%20with%20mobile%20eye-tracking%20data%20of%20an%20art%20gallery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03417v1&entry.124074799=Read"},
{"title": "Hessian Aware Low-Rank Weight Perturbation for Continual Learning", "author": "Jiaqi Li and Rui Wang and Yuanhao Lai and Changjian Shui and Sabyasachi Sahoo and Charles X. Ling and Shichun Yang and Boyu Wang and Christian Gagn\u00e9 and Fan Zhou", "abstract": "  Continual learning aims to learn a series of tasks sequentially without\nforgetting the knowledge acquired from the previous ones. In this work, we\npropose the Hessian Aware Low-Rank Perturbation algorithm for continual\nlearning. By modeling the parameter transitions along the sequential tasks with\nthe weight matrix transformation, we propose to apply the low-rank\napproximation on the task-adaptive parameters in each layer of the neural\nnetworks. Specifically, we theoretically demonstrate the quantitative\nrelationship between the Hessian and the proposed low-rank approximation. The\napproximation ranks are then globally determined according to the marginal\nincrement of the empirical loss estimated by the layer-specific gradient and\nlow-rank approximation error. Furthermore, we control the model capacity by\npruning less important parameters to diminish the parameter growth. We conduct\nextensive experiments on various benchmarks, including a dataset with\nlarge-scale tasks, and compare our method against some recent state-of-the-art\nmethods to demonstrate the effectiveness and scalability of our proposed\nmethod. Empirical results show that our method performs better on different\nbenchmarks, especially in achieving task order robustness and handling the\nforgetting issue. The source code is at https://github.com/lijiaqi/HALRP.\n", "link": "http://arxiv.org/abs/2311.15161v2", "date": "2024-04-04", "relevancy": 1.972, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4911}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4876}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hessian%20Aware%20Low-Rank%20Weight%20Perturbation%20for%20Continual%20Learning&body=Title%3A%20Hessian%20Aware%20Low-Rank%20Weight%20Perturbation%20for%20Continual%20Learning%0AAuthor%3A%20Jiaqi%20Li%20and%20Rui%20Wang%20and%20Yuanhao%20Lai%20and%20Changjian%20Shui%20and%20Sabyasachi%20Sahoo%20and%20Charles%20X.%20Ling%20and%20Shichun%20Yang%20and%20Boyu%20Wang%20and%20Christian%20Gagn%C3%A9%20and%20Fan%20Zhou%0AAbstract%3A%20%20%20Continual%20learning%20aims%20to%20learn%20a%20series%20of%20tasks%20sequentially%20without%0Aforgetting%20the%20knowledge%20acquired%20from%20the%20previous%20ones.%20In%20this%20work%2C%20we%0Apropose%20the%20Hessian%20Aware%20Low-Rank%20Perturbation%20algorithm%20for%20continual%0Alearning.%20By%20modeling%20the%20parameter%20transitions%20along%20the%20sequential%20tasks%20with%0Athe%20weight%20matrix%20transformation%2C%20we%20propose%20to%20apply%20the%20low-rank%0Aapproximation%20on%20the%20task-adaptive%20parameters%20in%20each%20layer%20of%20the%20neural%0Anetworks.%20Specifically%2C%20we%20theoretically%20demonstrate%20the%20quantitative%0Arelationship%20between%20the%20Hessian%20and%20the%20proposed%20low-rank%20approximation.%20The%0Aapproximation%20ranks%20are%20then%20globally%20determined%20according%20to%20the%20marginal%0Aincrement%20of%20the%20empirical%20loss%20estimated%20by%20the%20layer-specific%20gradient%20and%0Alow-rank%20approximation%20error.%20Furthermore%2C%20we%20control%20the%20model%20capacity%20by%0Apruning%20less%20important%20parameters%20to%20diminish%20the%20parameter%20growth.%20We%20conduct%0Aextensive%20experiments%20on%20various%20benchmarks%2C%20including%20a%20dataset%20with%0Alarge-scale%20tasks%2C%20and%20compare%20our%20method%20against%20some%20recent%20state-of-the-art%0Amethods%20to%20demonstrate%20the%20effectiveness%20and%20scalability%20of%20our%20proposed%0Amethod.%20Empirical%20results%20show%20that%20our%20method%20performs%20better%20on%20different%0Abenchmarks%2C%20especially%20in%20achieving%20task%20order%20robustness%20and%20handling%20the%0Aforgetting%20issue.%20The%20source%20code%20is%20at%20https%3A//github.com/lijiaqi/HALRP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15161v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hessian%20Aware%20Low-Rank%20Weight%20Perturbation%20for%20Continual%20Learning&entry.906535625=Jiaqi%20Li%20and%20Rui%20Wang%20and%20Yuanhao%20Lai%20and%20Changjian%20Shui%20and%20Sabyasachi%20Sahoo%20and%20Charles%20X.%20Ling%20and%20Shichun%20Yang%20and%20Boyu%20Wang%20and%20Christian%20Gagn%C3%A9%20and%20Fan%20Zhou&entry.1292438233=%20%20Continual%20learning%20aims%20to%20learn%20a%20series%20of%20tasks%20sequentially%20without%0Aforgetting%20the%20knowledge%20acquired%20from%20the%20previous%20ones.%20In%20this%20work%2C%20we%0Apropose%20the%20Hessian%20Aware%20Low-Rank%20Perturbation%20algorithm%20for%20continual%0Alearning.%20By%20modeling%20the%20parameter%20transitions%20along%20the%20sequential%20tasks%20with%0Athe%20weight%20matrix%20transformation%2C%20we%20propose%20to%20apply%20the%20low-rank%0Aapproximation%20on%20the%20task-adaptive%20parameters%20in%20each%20layer%20of%20the%20neural%0Anetworks.%20Specifically%2C%20we%20theoretically%20demonstrate%20the%20quantitative%0Arelationship%20between%20the%20Hessian%20and%20the%20proposed%20low-rank%20approximation.%20The%0Aapproximation%20ranks%20are%20then%20globally%20determined%20according%20to%20the%20marginal%0Aincrement%20of%20the%20empirical%20loss%20estimated%20by%20the%20layer-specific%20gradient%20and%0Alow-rank%20approximation%20error.%20Furthermore%2C%20we%20control%20the%20model%20capacity%20by%0Apruning%20less%20important%20parameters%20to%20diminish%20the%20parameter%20growth.%20We%20conduct%0Aextensive%20experiments%20on%20various%20benchmarks%2C%20including%20a%20dataset%20with%0Alarge-scale%20tasks%2C%20and%20compare%20our%20method%20against%20some%20recent%20state-of-the-art%0Amethods%20to%20demonstrate%20the%20effectiveness%20and%20scalability%20of%20our%20proposed%0Amethod.%20Empirical%20results%20show%20that%20our%20method%20performs%20better%20on%20different%0Abenchmarks%2C%20especially%20in%20achieving%20task%20order%20robustness%20and%20handling%20the%0Aforgetting%20issue.%20The%20source%20code%20is%20at%20https%3A//github.com/lijiaqi/HALRP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15161v2&entry.124074799=Read"},
{"title": "Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian\n  Splatting", "author": "Jeongmin Bae and Seoha Kim and Youngsik Yun and Hahyun Lee and Gun Bang and Youngjung Uh", "abstract": "  As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view\nsynthesis, it is a natural extension to deform a canonical 3DGS to multiple\nframes. However, previous works fail to accurately reconstruct dynamic scenes,\nespecially 1) static parts moving along nearby dynamic parts, and 2) some\ndynamic areas are blurry. We attribute the failure to the wrong design of the\ndeformation field, which is built as a coordinate-based function. This approach\nis problematic because 3DGS is a mixture of multiple fields centered at the\nGaussians, not just a single coordinate-based framework. To resolve this\nproblem, we define the deformation as a function of per-Gaussian embeddings and\ntemporal embeddings. Moreover, we decompose deformations as coarse and fine\ndeformations to model slow and fast movements, respectively. Also, we introduce\nan efficient training strategy for faster convergence and higher quality.\nProject page: https://jeongminb.github.io/e-d3dgs/\n", "link": "http://arxiv.org/abs/2404.03613v1", "date": "2024-04-04", "relevancy": 1.9673, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5052}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4935}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4542}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Jeongmin%20Bae%20and%20Seoha%20Kim%20and%20Youngsik%20Yun%20and%20Hahyun%20Lee%20and%20Gun%20Bang%20and%20Youngjung%20Uh%0AAbstract%3A%20%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20provides%20fast%20and%20high-quality%20novel%20view%0Asynthesis%2C%20it%20is%20a%20natural%20extension%20to%20deform%20a%20canonical%203DGS%20to%20multiple%0Aframes.%20However%2C%20previous%20works%20fail%20to%20accurately%20reconstruct%20dynamic%20scenes%2C%0Aespecially%201%29%20static%20parts%20moving%20along%20nearby%20dynamic%20parts%2C%20and%202%29%20some%0Adynamic%20areas%20are%20blurry.%20We%20attribute%20the%20failure%20to%20the%20wrong%20design%20of%20the%0Adeformation%20field%2C%20which%20is%20built%20as%20a%20coordinate-based%20function.%20This%20approach%0Ais%20problematic%20because%203DGS%20is%20a%20mixture%20of%20multiple%20fields%20centered%20at%20the%0AGaussians%2C%20not%20just%20a%20single%20coordinate-based%20framework.%20To%20resolve%20this%0Aproblem%2C%20we%20define%20the%20deformation%20as%20a%20function%20of%20per-Gaussian%20embeddings%20and%0Atemporal%20embeddings.%20Moreover%2C%20we%20decompose%20deformations%20as%20coarse%20and%20fine%0Adeformations%20to%20model%20slow%20and%20fast%20movements%2C%20respectively.%20Also%2C%20we%20introduce%0Aan%20efficient%20training%20strategy%20for%20faster%20convergence%20and%20higher%20quality.%0AProject%20page%3A%20https%3A//jeongminb.github.io/e-d3dgs/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03613v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Jeongmin%20Bae%20and%20Seoha%20Kim%20and%20Youngsik%20Yun%20and%20Hahyun%20Lee%20and%20Gun%20Bang%20and%20Youngjung%20Uh&entry.1292438233=%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20provides%20fast%20and%20high-quality%20novel%20view%0Asynthesis%2C%20it%20is%20a%20natural%20extension%20to%20deform%20a%20canonical%203DGS%20to%20multiple%0Aframes.%20However%2C%20previous%20works%20fail%20to%20accurately%20reconstruct%20dynamic%20scenes%2C%0Aespecially%201%29%20static%20parts%20moving%20along%20nearby%20dynamic%20parts%2C%20and%202%29%20some%0Adynamic%20areas%20are%20blurry.%20We%20attribute%20the%20failure%20to%20the%20wrong%20design%20of%20the%0Adeformation%20field%2C%20which%20is%20built%20as%20a%20coordinate-based%20function.%20This%20approach%0Ais%20problematic%20because%203DGS%20is%20a%20mixture%20of%20multiple%20fields%20centered%20at%20the%0AGaussians%2C%20not%20just%20a%20single%20coordinate-based%20framework.%20To%20resolve%20this%0Aproblem%2C%20we%20define%20the%20deformation%20as%20a%20function%20of%20per-Gaussian%20embeddings%20and%0Atemporal%20embeddings.%20Moreover%2C%20we%20decompose%20deformations%20as%20coarse%20and%20fine%0Adeformations%20to%20model%20slow%20and%20fast%20movements%2C%20respectively.%20Also%2C%20we%20introduce%0Aan%20efficient%20training%20strategy%20for%20faster%20convergence%20and%20higher%20quality.%0AProject%20page%3A%20https%3A//jeongminb.github.io/e-d3dgs/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03613v1&entry.124074799=Read"},
{"title": "Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting", "author": "Busra Asan and Abdullah Akg\u00fcl and Alper Unal and Melih Kandemir and Gozde Unal", "abstract": "  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n", "link": "http://arxiv.org/abs/2403.16612v2", "date": "2024-04-04", "relevancy": 1.954, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4907}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4779}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Calibrating%20Bayesian%20UNet%2B%2B%20for%20Sub-Seasonal%20Forecasting&body=Title%3A%20Calibrating%20Bayesian%20UNet%2B%2B%20for%20Sub-Seasonal%20Forecasting%0AAuthor%3A%20Busra%20Asan%20and%20Abdullah%20Akg%C3%BCl%20and%20Alper%20Unal%20and%20Melih%20Kandemir%20and%20Gozde%20Unal%0AAbstract%3A%20%20%20Seasonal%20forecasting%20is%20a%20crucial%20task%20when%20it%20comes%20to%20detecting%20the%20extreme%0Aheat%20and%20colds%20that%20occur%20due%20to%20climate%20change.%20Confidence%20in%20the%20predictions%0Ashould%20be%20reliable%20since%20a%20small%20increase%20in%20the%20temperatures%20in%20a%20year%20has%20a%0Abig%20impact%20on%20the%20world.%20Calibration%20of%20the%20neural%20networks%20provides%20a%20way%20to%0Aensure%20our%20confidence%20in%20the%20predictions.%20However%2C%20calibrating%20regression%0Amodels%20is%20an%20under-researched%20topic%2C%20especially%20in%20forecasters.%20We%20calibrate%20a%0AUNet%2B%2B%20based%20architecture%2C%20which%20was%20shown%20to%20outperform%20physics-based%20models%0Ain%20temperature%20anomalies.%20We%20show%20that%20with%20a%20slight%20trade-off%20between%0Aprediction%20error%20and%20calibration%20error%2C%20it%20is%20possible%20to%20get%20more%20reliable%20and%0Asharper%20forecasts.%20We%20believe%20that%20calibration%20should%20be%20an%20important%20part%20of%0Asafety-critical%20machine%20learning%20applications%20such%20as%20weather%20forecasters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16612v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrating%20Bayesian%20UNet%2B%2B%20for%20Sub-Seasonal%20Forecasting&entry.906535625=Busra%20Asan%20and%20Abdullah%20Akg%C3%BCl%20and%20Alper%20Unal%20and%20Melih%20Kandemir%20and%20Gozde%20Unal&entry.1292438233=%20%20Seasonal%20forecasting%20is%20a%20crucial%20task%20when%20it%20comes%20to%20detecting%20the%20extreme%0Aheat%20and%20colds%20that%20occur%20due%20to%20climate%20change.%20Confidence%20in%20the%20predictions%0Ashould%20be%20reliable%20since%20a%20small%20increase%20in%20the%20temperatures%20in%20a%20year%20has%20a%0Abig%20impact%20on%20the%20world.%20Calibration%20of%20the%20neural%20networks%20provides%20a%20way%20to%0Aensure%20our%20confidence%20in%20the%20predictions.%20However%2C%20calibrating%20regression%0Amodels%20is%20an%20under-researched%20topic%2C%20especially%20in%20forecasters.%20We%20calibrate%20a%0AUNet%2B%2B%20based%20architecture%2C%20which%20was%20shown%20to%20outperform%20physics-based%20models%0Ain%20temperature%20anomalies.%20We%20show%20that%20with%20a%20slight%20trade-off%20between%0Aprediction%20error%20and%20calibration%20error%2C%20it%20is%20possible%20to%20get%20more%20reliable%20and%0Asharper%20forecasts.%20We%20believe%20that%20calibration%20should%20be%20an%20important%20part%20of%0Asafety-critical%20machine%20learning%20applications%20such%20as%20weather%20forecasters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16612v2&entry.124074799=Read"},
{"title": "Set-Type Belief Propagation with Applications to Poisson Multi-Bernoulli\n  SLAM", "author": "Hyowon Kim and Angel F. Garc\u00eda-Fern\u00e1ndez and Yu Ge and Yuxuan Xia and Lennart Svensson and Henk Wymeersch", "abstract": "  Belief propagation (BP) is a useful probabilistic inference algorithm for\nefficiently computing approximate marginal probability densities of random\nvariables. However, in its standard form, BP is only applicable to the\nvector-type random variables with a fixed and known number of vector elements,\nwhile certain applications rely on RFSs with an unknown number of vector\nelements. In this paper, we develop BP rules for factor graphs defined on\nsequences of RFSs where each RFS has an unknown number of elements, with the\nintention of deriving novel inference methods for RFSs. Furthermore, we show\nthat vector-type BP is a special case of set-type BP, where each RFS follows\nthe Bernoulli process. To demonstrate the validity of developed set-type BP, we\napply it to the PMB filter for SLAM, which naturally leads to new set-type\nBP-mapping, SLAM, multi-target tracking, and simultaneous localization and\ntracking filters. Finally, we explore the relationships between the vector-type\nBP and the proposed set-type BP PMB-SLAM implementations and show a performance\ngain of the proposed set-type BP PMB-SLAM filter in comparison with the\nvector-type BP-SLAM filter.\n", "link": "http://arxiv.org/abs/2305.04797v3", "date": "2024-04-04", "relevancy": 1.9473, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5873}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5059}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4275}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Set-Type%20Belief%20Propagation%20with%20Applications%20to%20Poisson%20Multi-Bernoulli%0A%20%20SLAM&body=Title%3A%20Set-Type%20Belief%20Propagation%20with%20Applications%20to%20Poisson%20Multi-Bernoulli%0A%20%20SLAM%0AAuthor%3A%20Hyowon%20Kim%20and%20Angel%20F.%20Garc%C3%ADa-Fern%C3%A1ndez%20and%20Yu%20Ge%20and%20Yuxuan%20Xia%20and%20Lennart%20Svensson%20and%20Henk%20Wymeersch%0AAbstract%3A%20%20%20Belief%20propagation%20%28BP%29%20is%20a%20useful%20probabilistic%20inference%20algorithm%20for%0Aefficiently%20computing%20approximate%20marginal%20probability%20densities%20of%20random%0Avariables.%20However%2C%20in%20its%20standard%20form%2C%20BP%20is%20only%20applicable%20to%20the%0Avector-type%20random%20variables%20with%20a%20fixed%20and%20known%20number%20of%20vector%20elements%2C%0Awhile%20certain%20applications%20rely%20on%20RFSs%20with%20an%20unknown%20number%20of%20vector%0Aelements.%20In%20this%20paper%2C%20we%20develop%20BP%20rules%20for%20factor%20graphs%20defined%20on%0Asequences%20of%20RFSs%20where%20each%20RFS%20has%20an%20unknown%20number%20of%20elements%2C%20with%20the%0Aintention%20of%20deriving%20novel%20inference%20methods%20for%20RFSs.%20Furthermore%2C%20we%20show%0Athat%20vector-type%20BP%20is%20a%20special%20case%20of%20set-type%20BP%2C%20where%20each%20RFS%20follows%0Athe%20Bernoulli%20process.%20To%20demonstrate%20the%20validity%20of%20developed%20set-type%20BP%2C%20we%0Aapply%20it%20to%20the%20PMB%20filter%20for%20SLAM%2C%20which%20naturally%20leads%20to%20new%20set-type%0ABP-mapping%2C%20SLAM%2C%20multi-target%20tracking%2C%20and%20simultaneous%20localization%20and%0Atracking%20filters.%20Finally%2C%20we%20explore%20the%20relationships%20between%20the%20vector-type%0ABP%20and%20the%20proposed%20set-type%20BP%20PMB-SLAM%20implementations%20and%20show%20a%20performance%0Again%20of%20the%20proposed%20set-type%20BP%20PMB-SLAM%20filter%20in%20comparison%20with%20the%0Avector-type%20BP-SLAM%20filter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.04797v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Set-Type%20Belief%20Propagation%20with%20Applications%20to%20Poisson%20Multi-Bernoulli%0A%20%20SLAM&entry.906535625=Hyowon%20Kim%20and%20Angel%20F.%20Garc%C3%ADa-Fern%C3%A1ndez%20and%20Yu%20Ge%20and%20Yuxuan%20Xia%20and%20Lennart%20Svensson%20and%20Henk%20Wymeersch&entry.1292438233=%20%20Belief%20propagation%20%28BP%29%20is%20a%20useful%20probabilistic%20inference%20algorithm%20for%0Aefficiently%20computing%20approximate%20marginal%20probability%20densities%20of%20random%0Avariables.%20However%2C%20in%20its%20standard%20form%2C%20BP%20is%20only%20applicable%20to%20the%0Avector-type%20random%20variables%20with%20a%20fixed%20and%20known%20number%20of%20vector%20elements%2C%0Awhile%20certain%20applications%20rely%20on%20RFSs%20with%20an%20unknown%20number%20of%20vector%0Aelements.%20In%20this%20paper%2C%20we%20develop%20BP%20rules%20for%20factor%20graphs%20defined%20on%0Asequences%20of%20RFSs%20where%20each%20RFS%20has%20an%20unknown%20number%20of%20elements%2C%20with%20the%0Aintention%20of%20deriving%20novel%20inference%20methods%20for%20RFSs.%20Furthermore%2C%20we%20show%0Athat%20vector-type%20BP%20is%20a%20special%20case%20of%20set-type%20BP%2C%20where%20each%20RFS%20follows%0Athe%20Bernoulli%20process.%20To%20demonstrate%20the%20validity%20of%20developed%20set-type%20BP%2C%20we%0Aapply%20it%20to%20the%20PMB%20filter%20for%20SLAM%2C%20which%20naturally%20leads%20to%20new%20set-type%0ABP-mapping%2C%20SLAM%2C%20multi-target%20tracking%2C%20and%20simultaneous%20localization%20and%0Atracking%20filters.%20Finally%2C%20we%20explore%20the%20relationships%20between%20the%20vector-type%0ABP%20and%20the%20proposed%20set-type%20BP%20PMB-SLAM%20implementations%20and%20show%20a%20performance%0Again%20of%20the%20proposed%20set-type%20BP%20PMB-SLAM%20filter%20in%20comparison%20with%20the%0Avector-type%20BP-SLAM%20filter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.04797v3&entry.124074799=Read"},
{"title": "Parametric-Task MAP-Elites", "author": "Timoth\u00e9e Anne and Jean-Baptiste Mouret", "abstract": "  Optimizing a set of functions simultaneously by leveraging their similarity\nis called multi-task optimization. Current black-box multi-task algorithms only\nsolve a finite set of tasks, even when the tasks originate from a continuous\nspace. In this paper, we introduce Parametric-Task MAP-Elites (PT-ME), a new\nblack-box algorithm for continuous multi-task optimization problems. This\nalgorithm (1) solves a new task at each iteration, effectively covering the\ncontinuous space, and (2) exploits a new variation operator based on local\nlinear regression. The resulting dataset of solutions makes it possible to\ncreate a function that maps any task parameter to its optimal solution. We show\nthat PT-ME outperforms all baselines, including the deep reinforcement learning\nalgorithm PPO on two parametric-task toy problems and a robotic problem in\nsimulation.\n", "link": "http://arxiv.org/abs/2402.01275v2", "date": "2024-04-04", "relevancy": 1.9468, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4937}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4679}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Parametric-Task%20MAP-Elites&body=Title%3A%20Parametric-Task%20MAP-Elites%0AAuthor%3A%20Timoth%C3%A9e%20Anne%20and%20Jean-Baptiste%20Mouret%0AAbstract%3A%20%20%20Optimizing%20a%20set%20of%20functions%20simultaneously%20by%20leveraging%20their%20similarity%0Ais%20called%20multi-task%20optimization.%20Current%20black-box%20multi-task%20algorithms%20only%0Asolve%20a%20finite%20set%20of%20tasks%2C%20even%20when%20the%20tasks%20originate%20from%20a%20continuous%0Aspace.%20In%20this%20paper%2C%20we%20introduce%20Parametric-Task%20MAP-Elites%20%28PT-ME%29%2C%20a%20new%0Ablack-box%20algorithm%20for%20continuous%20multi-task%20optimization%20problems.%20This%0Aalgorithm%20%281%29%20solves%20a%20new%20task%20at%20each%20iteration%2C%20effectively%20covering%20the%0Acontinuous%20space%2C%20and%20%282%29%20exploits%20a%20new%20variation%20operator%20based%20on%20local%0Alinear%20regression.%20The%20resulting%20dataset%20of%20solutions%20makes%20it%20possible%20to%0Acreate%20a%20function%20that%20maps%20any%20task%20parameter%20to%20its%20optimal%20solution.%20We%20show%0Athat%20PT-ME%20outperforms%20all%20baselines%2C%20including%20the%20deep%20reinforcement%20learning%0Aalgorithm%20PPO%20on%20two%20parametric-task%20toy%20problems%20and%20a%20robotic%20problem%20in%0Asimulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01275v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parametric-Task%20MAP-Elites&entry.906535625=Timoth%C3%A9e%20Anne%20and%20Jean-Baptiste%20Mouret&entry.1292438233=%20%20Optimizing%20a%20set%20of%20functions%20simultaneously%20by%20leveraging%20their%20similarity%0Ais%20called%20multi-task%20optimization.%20Current%20black-box%20multi-task%20algorithms%20only%0Asolve%20a%20finite%20set%20of%20tasks%2C%20even%20when%20the%20tasks%20originate%20from%20a%20continuous%0Aspace.%20In%20this%20paper%2C%20we%20introduce%20Parametric-Task%20MAP-Elites%20%28PT-ME%29%2C%20a%20new%0Ablack-box%20algorithm%20for%20continuous%20multi-task%20optimization%20problems.%20This%0Aalgorithm%20%281%29%20solves%20a%20new%20task%20at%20each%20iteration%2C%20effectively%20covering%20the%0Acontinuous%20space%2C%20and%20%282%29%20exploits%20a%20new%20variation%20operator%20based%20on%20local%0Alinear%20regression.%20The%20resulting%20dataset%20of%20solutions%20makes%20it%20possible%20to%0Acreate%20a%20function%20that%20maps%20any%20task%20parameter%20to%20its%20optimal%20solution.%20We%20show%0Athat%20PT-ME%20outperforms%20all%20baselines%2C%20including%20the%20deep%20reinforcement%20learning%0Aalgorithm%20PPO%20on%20two%20parametric-task%20toy%20problems%20and%20a%20robotic%20problem%20in%0Asimulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01275v2&entry.124074799=Read"},
{"title": "Alzheimer's disease detection in PSG signals", "author": "Lorena Gallego-Vi\u00f1ar\u00e1s and Juan Miguel Mira-Tom\u00e1s and Anna Michela-Gaeta and Gerard Pinol-Ripoll and Ferr\u00e1n Barb\u00e9 and Pablo M. Olmos and Arrate Mu\u00f1oz-Barrutia", "abstract": "  Alzheimer's disease (AD) and sleep disorders exhibit a close association,\nwhere disruptions in sleep patterns often precede the onset of Mild Cognitive\nImpairment (MCI) and early-stage AD. This study delves into the potential of\nutilizing sleep-related electroencephalography (EEG) signals acquired through\npolysomnography (PSG) for the early detection of AD. Our primary focus is on\nexploring semi-supervised Deep Learning techniques for the classification of\nEEG signals due to the clinical scenario characterized by the limited data\navailability. The methodology entails testing and comparing the performance of\nsemi-supervised SMATE and TapNet models, benchmarked against the supervised XCM\nmodel, and unsupervised Hidden Markov Models (HMMs). The study highlights the\nsignificance of spatial and temporal analysis capabilities, conducting\nindependent analyses of each sleep stage. Results demonstrate the effectiveness\nof SMATE in leveraging limited labeled data, achieving stable metrics across\nall sleep stages, and reaching 90% accuracy in its supervised form. Comparative\nanalyses reveal SMATE's superior performance over TapNet and HMM, while XCM\nexcels in supervised scenarios with an accuracy range of 92 - 94%. These\nfindings underscore the potential of semi-supervised models in early AD\ndetection, particularly in overcoming the challenges associated with the\nscarcity of labeled data. Ablation tests affirm the critical role of\nspatio-temporal feature extraction in semi-supervised predictive performance,\nand t-SNE visualizations validate the model's proficiency in distinguishing AD\npatterns. Overall, this research contributes to the advancement of AD detection\nthrough innovative Deep Learning approaches, highlighting the crucial role of\nsemi-supervised learning in addressing data limitations.\n", "link": "http://arxiv.org/abs/2404.03549v1", "date": "2024-04-04", "relevancy": 1.9462, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5431}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4442}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Alzheimer%27s%20disease%20detection%20in%20PSG%20signals&body=Title%3A%20Alzheimer%27s%20disease%20detection%20in%20PSG%20signals%0AAuthor%3A%20Lorena%20Gallego-Vi%C3%B1ar%C3%A1s%20and%20Juan%20Miguel%20Mira-Tom%C3%A1s%20and%20Anna%20Michela-Gaeta%20and%20Gerard%20Pinol-Ripoll%20and%20Ferr%C3%A1n%20Barb%C3%A9%20and%20Pablo%20M.%20Olmos%20and%20Arrate%20Mu%C3%B1oz-Barrutia%0AAbstract%3A%20%20%20Alzheimer%27s%20disease%20%28AD%29%20and%20sleep%20disorders%20exhibit%20a%20close%20association%2C%0Awhere%20disruptions%20in%20sleep%20patterns%20often%20precede%20the%20onset%20of%20Mild%20Cognitive%0AImpairment%20%28MCI%29%20and%20early-stage%20AD.%20This%20study%20delves%20into%20the%20potential%20of%0Autilizing%20sleep-related%20electroencephalography%20%28EEG%29%20signals%20acquired%20through%0Apolysomnography%20%28PSG%29%20for%20the%20early%20detection%20of%20AD.%20Our%20primary%20focus%20is%20on%0Aexploring%20semi-supervised%20Deep%20Learning%20techniques%20for%20the%20classification%20of%0AEEG%20signals%20due%20to%20the%20clinical%20scenario%20characterized%20by%20the%20limited%20data%0Aavailability.%20The%20methodology%20entails%20testing%20and%20comparing%20the%20performance%20of%0Asemi-supervised%20SMATE%20and%20TapNet%20models%2C%20benchmarked%20against%20the%20supervised%20XCM%0Amodel%2C%20and%20unsupervised%20Hidden%20Markov%20Models%20%28HMMs%29.%20The%20study%20highlights%20the%0Asignificance%20of%20spatial%20and%20temporal%20analysis%20capabilities%2C%20conducting%0Aindependent%20analyses%20of%20each%20sleep%20stage.%20Results%20demonstrate%20the%20effectiveness%0Aof%20SMATE%20in%20leveraging%20limited%20labeled%20data%2C%20achieving%20stable%20metrics%20across%0Aall%20sleep%20stages%2C%20and%20reaching%2090%25%20accuracy%20in%20its%20supervised%20form.%20Comparative%0Aanalyses%20reveal%20SMATE%27s%20superior%20performance%20over%20TapNet%20and%20HMM%2C%20while%20XCM%0Aexcels%20in%20supervised%20scenarios%20with%20an%20accuracy%20range%20of%2092%20-%2094%25.%20These%0Afindings%20underscore%20the%20potential%20of%20semi-supervised%20models%20in%20early%20AD%0Adetection%2C%20particularly%20in%20overcoming%20the%20challenges%20associated%20with%20the%0Ascarcity%20of%20labeled%20data.%20Ablation%20tests%20affirm%20the%20critical%20role%20of%0Aspatio-temporal%20feature%20extraction%20in%20semi-supervised%20predictive%20performance%2C%0Aand%20t-SNE%20visualizations%20validate%20the%20model%27s%20proficiency%20in%20distinguishing%20AD%0Apatterns.%20Overall%2C%20this%20research%20contributes%20to%20the%20advancement%20of%20AD%20detection%0Athrough%20innovative%20Deep%20Learning%20approaches%2C%20highlighting%20the%20crucial%20role%20of%0Asemi-supervised%20learning%20in%20addressing%20data%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03549v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alzheimer%27s%20disease%20detection%20in%20PSG%20signals&entry.906535625=Lorena%20Gallego-Vi%C3%B1ar%C3%A1s%20and%20Juan%20Miguel%20Mira-Tom%C3%A1s%20and%20Anna%20Michela-Gaeta%20and%20Gerard%20Pinol-Ripoll%20and%20Ferr%C3%A1n%20Barb%C3%A9%20and%20Pablo%20M.%20Olmos%20and%20Arrate%20Mu%C3%B1oz-Barrutia&entry.1292438233=%20%20Alzheimer%27s%20disease%20%28AD%29%20and%20sleep%20disorders%20exhibit%20a%20close%20association%2C%0Awhere%20disruptions%20in%20sleep%20patterns%20often%20precede%20the%20onset%20of%20Mild%20Cognitive%0AImpairment%20%28MCI%29%20and%20early-stage%20AD.%20This%20study%20delves%20into%20the%20potential%20of%0Autilizing%20sleep-related%20electroencephalography%20%28EEG%29%20signals%20acquired%20through%0Apolysomnography%20%28PSG%29%20for%20the%20early%20detection%20of%20AD.%20Our%20primary%20focus%20is%20on%0Aexploring%20semi-supervised%20Deep%20Learning%20techniques%20for%20the%20classification%20of%0AEEG%20signals%20due%20to%20the%20clinical%20scenario%20characterized%20by%20the%20limited%20data%0Aavailability.%20The%20methodology%20entails%20testing%20and%20comparing%20the%20performance%20of%0Asemi-supervised%20SMATE%20and%20TapNet%20models%2C%20benchmarked%20against%20the%20supervised%20XCM%0Amodel%2C%20and%20unsupervised%20Hidden%20Markov%20Models%20%28HMMs%29.%20The%20study%20highlights%20the%0Asignificance%20of%20spatial%20and%20temporal%20analysis%20capabilities%2C%20conducting%0Aindependent%20analyses%20of%20each%20sleep%20stage.%20Results%20demonstrate%20the%20effectiveness%0Aof%20SMATE%20in%20leveraging%20limited%20labeled%20data%2C%20achieving%20stable%20metrics%20across%0Aall%20sleep%20stages%2C%20and%20reaching%2090%25%20accuracy%20in%20its%20supervised%20form.%20Comparative%0Aanalyses%20reveal%20SMATE%27s%20superior%20performance%20over%20TapNet%20and%20HMM%2C%20while%20XCM%0Aexcels%20in%20supervised%20scenarios%20with%20an%20accuracy%20range%20of%2092%20-%2094%25.%20These%0Afindings%20underscore%20the%20potential%20of%20semi-supervised%20models%20in%20early%20AD%0Adetection%2C%20particularly%20in%20overcoming%20the%20challenges%20associated%20with%20the%0Ascarcity%20of%20labeled%20data.%20Ablation%20tests%20affirm%20the%20critical%20role%20of%0Aspatio-temporal%20feature%20extraction%20in%20semi-supervised%20predictive%20performance%2C%0Aand%20t-SNE%20visualizations%20validate%20the%20model%27s%20proficiency%20in%20distinguishing%20AD%0Apatterns.%20Overall%2C%20this%20research%20contributes%20to%20the%20advancement%20of%20AD%20detection%0Athrough%20innovative%20Deep%20Learning%20approaches%2C%20highlighting%20the%20crucial%20role%20of%0Asemi-supervised%20learning%20in%20addressing%20data%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03549v1&entry.124074799=Read"},
{"title": "As Good As A Coin Toss: Human detection of AI-generated images, videos,\n  audio, and audiovisual stimuli", "author": "Di Cooke and Abigail Edwards and Sophia Barkoff and Kathryn Kelly", "abstract": "  As synthetic media becomes progressively more realistic and barriers to using\nit continue to lower, the technology has been increasingly utilized for\nmalicious purposes, from financial fraud to nonconsensual pornography. Today,\nthe principal defense against being misled by synthetic media relies on the\nability of the human observer to visually and auditorily discern between real\nand fake. However, it remains unclear just how vulnerable people actually are\nto deceptive synthetic media in the course of their day to day lives. We\nconducted a perceptual study with 1276 participants to assess how accurate\npeople were at distinguishing synthetic images, audio only, video only, and\naudiovisual stimuli from authentic. To reflect the circumstances under which\npeople would likely encounter synthetic media in the wild, testing conditions\nand stimuli emulated a typical online platform, while all synthetic media used\nin the survey was sourced from publicly accessible generative AI technology.\n  We find that overall, participants struggled to meaningfully discern between\nsynthetic and authentic content. We also find that detection performance\nworsens when the stimuli contains synthetic content as compared to authentic\ncontent, images featuring human faces as compared to non face objects, a single\nmodality as compared to multimodal stimuli, mixed authenticity as compared to\nbeing fully synthetic for audiovisual stimuli, and features foreign languages\nas compared to languages the observer is fluent in. Finally, we also find that\nprior knowledge of synthetic media does not meaningfully impact their detection\nperformance. Collectively, these results indicate that people are highly\nsusceptible to being tricked by synthetic media in their daily lives and that\nhuman perceptual detection capabilities can no longer be relied upon as an\neffective counterdefense.\n", "link": "http://arxiv.org/abs/2403.16760v3", "date": "2024-04-04", "relevancy": 1.9459, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4934}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4924}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4544}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli&body=Title%3A%20As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli%0AAuthor%3A%20Di%20Cooke%20and%20Abigail%20Edwards%20and%20Sophia%20Barkoff%20and%20Kathryn%20Kelly%0AAbstract%3A%20%20%20As%20synthetic%20media%20becomes%20progressively%20more%20realistic%20and%20barriers%20to%20using%0Ait%20continue%20to%20lower%2C%20the%20technology%20has%20been%20increasingly%20utilized%20for%0Amalicious%20purposes%2C%20from%20financial%20fraud%20to%20nonconsensual%20pornography.%20Today%2C%0Athe%20principal%20defense%20against%20being%20misled%20by%20synthetic%20media%20relies%20on%20the%0Aability%20of%20the%20human%20observer%20to%20visually%20and%20auditorily%20discern%20between%20real%0Aand%20fake.%20However%2C%20it%20remains%20unclear%20just%20how%20vulnerable%20people%20actually%20are%0Ato%20deceptive%20synthetic%20media%20in%20the%20course%20of%20their%20day%20to%20day%20lives.%20We%0Aconducted%20a%20perceptual%20study%20with%201276%20participants%20to%20assess%20how%20accurate%0Apeople%20were%20at%20distinguishing%20synthetic%20images%2C%20audio%20only%2C%20video%20only%2C%20and%0Aaudiovisual%20stimuli%20from%20authentic.%20To%20reflect%20the%20circumstances%20under%20which%0Apeople%20would%20likely%20encounter%20synthetic%20media%20in%20the%20wild%2C%20testing%20conditions%0Aand%20stimuli%20emulated%20a%20typical%20online%20platform%2C%20while%20all%20synthetic%20media%20used%0Ain%20the%20survey%20was%20sourced%20from%20publicly%20accessible%20generative%20AI%20technology.%0A%20%20We%20find%20that%20overall%2C%20participants%20struggled%20to%20meaningfully%20discern%20between%0Asynthetic%20and%20authentic%20content.%20We%20also%20find%20that%20detection%20performance%0Aworsens%20when%20the%20stimuli%20contains%20synthetic%20content%20as%20compared%20to%20authentic%0Acontent%2C%20images%20featuring%20human%20faces%20as%20compared%20to%20non%20face%20objects%2C%20a%20single%0Amodality%20as%20compared%20to%20multimodal%20stimuli%2C%20mixed%20authenticity%20as%20compared%20to%0Abeing%20fully%20synthetic%20for%20audiovisual%20stimuli%2C%20and%20features%20foreign%20languages%0Aas%20compared%20to%20languages%20the%20observer%20is%20fluent%20in.%20Finally%2C%20we%20also%20find%20that%0Aprior%20knowledge%20of%20synthetic%20media%20does%20not%20meaningfully%20impact%20their%20detection%0Aperformance.%20Collectively%2C%20these%20results%20indicate%20that%20people%20are%20highly%0Asusceptible%20to%20being%20tricked%20by%20synthetic%20media%20in%20their%20daily%20lives%20and%20that%0Ahuman%20perceptual%20detection%20capabilities%20can%20no%20longer%20be%20relied%20upon%20as%20an%0Aeffective%20counterdefense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16760v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli&entry.906535625=Di%20Cooke%20and%20Abigail%20Edwards%20and%20Sophia%20Barkoff%20and%20Kathryn%20Kelly&entry.1292438233=%20%20As%20synthetic%20media%20becomes%20progressively%20more%20realistic%20and%20barriers%20to%20using%0Ait%20continue%20to%20lower%2C%20the%20technology%20has%20been%20increasingly%20utilized%20for%0Amalicious%20purposes%2C%20from%20financial%20fraud%20to%20nonconsensual%20pornography.%20Today%2C%0Athe%20principal%20defense%20against%20being%20misled%20by%20synthetic%20media%20relies%20on%20the%0Aability%20of%20the%20human%20observer%20to%20visually%20and%20auditorily%20discern%20between%20real%0Aand%20fake.%20However%2C%20it%20remains%20unclear%20just%20how%20vulnerable%20people%20actually%20are%0Ato%20deceptive%20synthetic%20media%20in%20the%20course%20of%20their%20day%20to%20day%20lives.%20We%0Aconducted%20a%20perceptual%20study%20with%201276%20participants%20to%20assess%20how%20accurate%0Apeople%20were%20at%20distinguishing%20synthetic%20images%2C%20audio%20only%2C%20video%20only%2C%20and%0Aaudiovisual%20stimuli%20from%20authentic.%20To%20reflect%20the%20circumstances%20under%20which%0Apeople%20would%20likely%20encounter%20synthetic%20media%20in%20the%20wild%2C%20testing%20conditions%0Aand%20stimuli%20emulated%20a%20typical%20online%20platform%2C%20while%20all%20synthetic%20media%20used%0Ain%20the%20survey%20was%20sourced%20from%20publicly%20accessible%20generative%20AI%20technology.%0A%20%20We%20find%20that%20overall%2C%20participants%20struggled%20to%20meaningfully%20discern%20between%0Asynthetic%20and%20authentic%20content.%20We%20also%20find%20that%20detection%20performance%0Aworsens%20when%20the%20stimuli%20contains%20synthetic%20content%20as%20compared%20to%20authentic%0Acontent%2C%20images%20featuring%20human%20faces%20as%20compared%20to%20non%20face%20objects%2C%20a%20single%0Amodality%20as%20compared%20to%20multimodal%20stimuli%2C%20mixed%20authenticity%20as%20compared%20to%0Abeing%20fully%20synthetic%20for%20audiovisual%20stimuli%2C%20and%20features%20foreign%20languages%0Aas%20compared%20to%20languages%20the%20observer%20is%20fluent%20in.%20Finally%2C%20we%20also%20find%20that%0Aprior%20knowledge%20of%20synthetic%20media%20does%20not%20meaningfully%20impact%20their%20detection%0Aperformance.%20Collectively%2C%20these%20results%20indicate%20that%20people%20are%20highly%0Asusceptible%20to%20being%20tricked%20by%20synthetic%20media%20in%20their%20daily%20lives%20and%20that%0Ahuman%20perceptual%20detection%20capabilities%20can%20no%20longer%20be%20relied%20upon%20as%20an%0Aeffective%20counterdefense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16760v3&entry.124074799=Read"},
{"title": "Performance of computer vision algorithms for fine-grained\n  classification using crowdsourced insect images", "author": "Rita Pucci and Vincent J. Kalkman and Dan Stowell", "abstract": "  With fine-grained classification, we identify unique characteristics to\ndistinguish among classes of the same super-class. We are focusing on species\nrecognition in Insecta, as they are critical for biodiversity monitoring and at\nthe base of many ecosystems. With citizen science campaigns, billions of images\nare collected in the wild. Once these are labelled, experts can use them to\ncreate distribution maps. However, the labelling process is time-consuming,\nwhich is where computer vision comes in. The field of computer vision offers a\nwide range of algorithms, each with its strengths and weaknesses; how do we\nidentify the algorithm that is in line with our application? To answer this\nquestion, we provide a full and detailed evaluation of nine algorithms among\ndeep convolutional networks (CNN), vision transformers (ViT), and\nlocality-based vision transformers (LBVT) on 4 different aspects:\nclassification performance, embedding quality, computational cost, and gradient\nactivity. We offer insights that we haven't yet had in this domain proving to\nwhich extent these algorithms solve the fine-grained tasks in Insecta. We found\nthat the ViT performs the best on inference speed and computational cost while\nthe LBVT outperforms the others on performance and embedding quality; the CNN\nprovide a trade-off among the metrics.\n", "link": "http://arxiv.org/abs/2404.03474v1", "date": "2024-04-04", "relevancy": 1.9302, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4794}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.475}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Performance%20of%20computer%20vision%20algorithms%20for%20fine-grained%0A%20%20classification%20using%20crowdsourced%20insect%20images&body=Title%3A%20Performance%20of%20computer%20vision%20algorithms%20for%20fine-grained%0A%20%20classification%20using%20crowdsourced%20insect%20images%0AAuthor%3A%20Rita%20Pucci%20and%20Vincent%20J.%20Kalkman%20and%20Dan%20Stowell%0AAbstract%3A%20%20%20With%20fine-grained%20classification%2C%20we%20identify%20unique%20characteristics%20to%0Adistinguish%20among%20classes%20of%20the%20same%20super-class.%20We%20are%20focusing%20on%20species%0Arecognition%20in%20Insecta%2C%20as%20they%20are%20critical%20for%20biodiversity%20monitoring%20and%20at%0Athe%20base%20of%20many%20ecosystems.%20With%20citizen%20science%20campaigns%2C%20billions%20of%20images%0Aare%20collected%20in%20the%20wild.%20Once%20these%20are%20labelled%2C%20experts%20can%20use%20them%20to%0Acreate%20distribution%20maps.%20However%2C%20the%20labelling%20process%20is%20time-consuming%2C%0Awhich%20is%20where%20computer%20vision%20comes%20in.%20The%20field%20of%20computer%20vision%20offers%20a%0Awide%20range%20of%20algorithms%2C%20each%20with%20its%20strengths%20and%20weaknesses%3B%20how%20do%20we%0Aidentify%20the%20algorithm%20that%20is%20in%20line%20with%20our%20application%3F%20To%20answer%20this%0Aquestion%2C%20we%20provide%20a%20full%20and%20detailed%20evaluation%20of%20nine%20algorithms%20among%0Adeep%20convolutional%20networks%20%28CNN%29%2C%20vision%20transformers%20%28ViT%29%2C%20and%0Alocality-based%20vision%20transformers%20%28LBVT%29%20on%204%20different%20aspects%3A%0Aclassification%20performance%2C%20embedding%20quality%2C%20computational%20cost%2C%20and%20gradient%0Aactivity.%20We%20offer%20insights%20that%20we%20haven%27t%20yet%20had%20in%20this%20domain%20proving%20to%0Awhich%20extent%20these%20algorithms%20solve%20the%20fine-grained%20tasks%20in%20Insecta.%20We%20found%0Athat%20the%20ViT%20performs%20the%20best%20on%20inference%20speed%20and%20computational%20cost%20while%0Athe%20LBVT%20outperforms%20the%20others%20on%20performance%20and%20embedding%20quality%3B%20the%20CNN%0Aprovide%20a%20trade-off%20among%20the%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03474v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20of%20computer%20vision%20algorithms%20for%20fine-grained%0A%20%20classification%20using%20crowdsourced%20insect%20images&entry.906535625=Rita%20Pucci%20and%20Vincent%20J.%20Kalkman%20and%20Dan%20Stowell&entry.1292438233=%20%20With%20fine-grained%20classification%2C%20we%20identify%20unique%20characteristics%20to%0Adistinguish%20among%20classes%20of%20the%20same%20super-class.%20We%20are%20focusing%20on%20species%0Arecognition%20in%20Insecta%2C%20as%20they%20are%20critical%20for%20biodiversity%20monitoring%20and%20at%0Athe%20base%20of%20many%20ecosystems.%20With%20citizen%20science%20campaigns%2C%20billions%20of%20images%0Aare%20collected%20in%20the%20wild.%20Once%20these%20are%20labelled%2C%20experts%20can%20use%20them%20to%0Acreate%20distribution%20maps.%20However%2C%20the%20labelling%20process%20is%20time-consuming%2C%0Awhich%20is%20where%20computer%20vision%20comes%20in.%20The%20field%20of%20computer%20vision%20offers%20a%0Awide%20range%20of%20algorithms%2C%20each%20with%20its%20strengths%20and%20weaknesses%3B%20how%20do%20we%0Aidentify%20the%20algorithm%20that%20is%20in%20line%20with%20our%20application%3F%20To%20answer%20this%0Aquestion%2C%20we%20provide%20a%20full%20and%20detailed%20evaluation%20of%20nine%20algorithms%20among%0Adeep%20convolutional%20networks%20%28CNN%29%2C%20vision%20transformers%20%28ViT%29%2C%20and%0Alocality-based%20vision%20transformers%20%28LBVT%29%20on%204%20different%20aspects%3A%0Aclassification%20performance%2C%20embedding%20quality%2C%20computational%20cost%2C%20and%20gradient%0Aactivity.%20We%20offer%20insights%20that%20we%20haven%27t%20yet%20had%20in%20this%20domain%20proving%20to%0Awhich%20extent%20these%20algorithms%20solve%20the%20fine-grained%20tasks%20in%20Insecta.%20We%20found%0Athat%20the%20ViT%20performs%20the%20best%20on%20inference%20speed%20and%20computational%20cost%20while%0Athe%20LBVT%20outperforms%20the%20others%20on%20performance%20and%20embedding%20quality%3B%20the%20CNN%0Aprovide%20a%20trade-off%20among%20the%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03474v1&entry.124074799=Read"},
{"title": "From Language Modeling to Instruction Following: Understanding the\n  Behavior Shift in LLMs after Instruction Tuning", "author": "Xuansheng Wu and Wenlin Yao and Jianshu Chen and Xiaoman Pan and Xiaoyang Wang and Ninghao Liu and Dong Yu", "abstract": "  Large Language Models (LLMs) have achieved remarkable success, where\ninstruction tuning is the critical step in aligning LLMs with user intentions.\nIn this work, we investigate how the instruction tuning adjusts pre-trained\nmodels with a focus on intrinsic changes. Specifically, we first develop\nseveral local and global explanation methods, including a gradient-based method\nfor input-output attribution, and techniques for interpreting patterns and\nconcepts in self-attention and feed-forward layers. The impact of instruction\ntuning is then studied by comparing the explanations derived from the\npre-trained and instruction-tuned models. This approach provides an internal\nperspective of the model shifts on a human-comprehensible level. Our findings\nreveal three significant impacts of instruction tuning: 1) It empowers LLMs to\nrecognize the instruction parts of user prompts, and promotes the response\ngeneration constantly conditioned on the instructions. 2) It encourages the\nself-attention heads to capture more word-word relationships about instruction\nverbs. 3) It encourages the feed-forward networks to rotate their pre-trained\nknowledge toward user-oriented tasks. These insights contribute to a more\ncomprehensive understanding of instruction tuning and lay the groundwork for\nfuture work that aims at explaining and optimizing LLMs for various\napplications. Our code and data are publicly available at\nhttps://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.\n", "link": "http://arxiv.org/abs/2310.00492v3", "date": "2024-04-04", "relevancy": 1.9082, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.466}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4571}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20Language%20Modeling%20to%20Instruction%20Following%3A%20Understanding%20the%0A%20%20Behavior%20Shift%20in%20LLMs%20after%20Instruction%20Tuning&body=Title%3A%20From%20Language%20Modeling%20to%20Instruction%20Following%3A%20Understanding%20the%0A%20%20Behavior%20Shift%20in%20LLMs%20after%20Instruction%20Tuning%0AAuthor%3A%20Xuansheng%20Wu%20and%20Wenlin%20Yao%20and%20Jianshu%20Chen%20and%20Xiaoman%20Pan%20and%20Xiaoyang%20Wang%20and%20Ninghao%20Liu%20and%20Dong%20Yu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%2C%20where%0Ainstruction%20tuning%20is%20the%20critical%20step%20in%20aligning%20LLMs%20with%20user%20intentions.%0AIn%20this%20work%2C%20we%20investigate%20how%20the%20instruction%20tuning%20adjusts%20pre-trained%0Amodels%20with%20a%20focus%20on%20intrinsic%20changes.%20Specifically%2C%20we%20first%20develop%0Aseveral%20local%20and%20global%20explanation%20methods%2C%20including%20a%20gradient-based%20method%0Afor%20input-output%20attribution%2C%20and%20techniques%20for%20interpreting%20patterns%20and%0Aconcepts%20in%20self-attention%20and%20feed-forward%20layers.%20The%20impact%20of%20instruction%0Atuning%20is%20then%20studied%20by%20comparing%20the%20explanations%20derived%20from%20the%0Apre-trained%20and%20instruction-tuned%20models.%20This%20approach%20provides%20an%20internal%0Aperspective%20of%20the%20model%20shifts%20on%20a%20human-comprehensible%20level.%20Our%20findings%0Areveal%20three%20significant%20impacts%20of%20instruction%20tuning%3A%201%29%20It%20empowers%20LLMs%20to%0Arecognize%20the%20instruction%20parts%20of%20user%20prompts%2C%20and%20promotes%20the%20response%0Ageneration%20constantly%20conditioned%20on%20the%20instructions.%202%29%20It%20encourages%20the%0Aself-attention%20heads%20to%20capture%20more%20word-word%20relationships%20about%20instruction%0Averbs.%203%29%20It%20encourages%20the%20feed-forward%20networks%20to%20rotate%20their%20pre-trained%0Aknowledge%20toward%20user-oriented%20tasks.%20These%20insights%20contribute%20to%20a%20more%0Acomprehensive%20understanding%20of%20instruction%20tuning%20and%20lay%20the%20groundwork%20for%0Afuture%20work%20that%20aims%20at%20explaining%20and%20optimizing%20LLMs%20for%20various%0Aapplications.%20Our%20code%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00492v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Language%20Modeling%20to%20Instruction%20Following%3A%20Understanding%20the%0A%20%20Behavior%20Shift%20in%20LLMs%20after%20Instruction%20Tuning&entry.906535625=Xuansheng%20Wu%20and%20Wenlin%20Yao%20and%20Jianshu%20Chen%20and%20Xiaoman%20Pan%20and%20Xiaoyang%20Wang%20and%20Ninghao%20Liu%20and%20Dong%20Yu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%2C%20where%0Ainstruction%20tuning%20is%20the%20critical%20step%20in%20aligning%20LLMs%20with%20user%20intentions.%0AIn%20this%20work%2C%20we%20investigate%20how%20the%20instruction%20tuning%20adjusts%20pre-trained%0Amodels%20with%20a%20focus%20on%20intrinsic%20changes.%20Specifically%2C%20we%20first%20develop%0Aseveral%20local%20and%20global%20explanation%20methods%2C%20including%20a%20gradient-based%20method%0Afor%20input-output%20attribution%2C%20and%20techniques%20for%20interpreting%20patterns%20and%0Aconcepts%20in%20self-attention%20and%20feed-forward%20layers.%20The%20impact%20of%20instruction%0Atuning%20is%20then%20studied%20by%20comparing%20the%20explanations%20derived%20from%20the%0Apre-trained%20and%20instruction-tuned%20models.%20This%20approach%20provides%20an%20internal%0Aperspective%20of%20the%20model%20shifts%20on%20a%20human-comprehensible%20level.%20Our%20findings%0Areveal%20three%20significant%20impacts%20of%20instruction%20tuning%3A%201%29%20It%20empowers%20LLMs%20to%0Arecognize%20the%20instruction%20parts%20of%20user%20prompts%2C%20and%20promotes%20the%20response%0Ageneration%20constantly%20conditioned%20on%20the%20instructions.%202%29%20It%20encourages%20the%0Aself-attention%20heads%20to%20capture%20more%20word-word%20relationships%20about%20instruction%0Averbs.%203%29%20It%20encourages%20the%20feed-forward%20networks%20to%20rotate%20their%20pre-trained%0Aknowledge%20toward%20user-oriented%20tasks.%20These%20insights%20contribute%20to%20a%20more%0Acomprehensive%20understanding%20of%20instruction%20tuning%20and%20lay%20the%20groundwork%20for%0Afuture%20work%20that%20aims%20at%20explaining%20and%20optimizing%20LLMs%20for%20various%0Aapplications.%20Our%20code%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00492v3&entry.124074799=Read"},
{"title": "Better-than-KL PAC-Bayes Bounds", "author": "Ilja Kuzborskij and Kwang-Sung Jun and Yulian Wu and Kyoungseok Jang and Francesco Orabona", "abstract": "  Let $f(\\theta, X_1),$ $ \\dots,$ $ f(\\theta, X_n)$ be a sequence of random\nelements, where $f$ is a fixed scalar function, $X_1, \\dots, X_n$ are\nindependent random variables (data), and $\\theta$ is a random parameter\ndistributed according to some data-dependent posterior distribution $P_n$. In\nthis paper, we consider the problem of proving concentration inequalities to\nestimate the mean of the sequence. An example of such a problem is the\nestimation of the generalization error of some predictor trained by a\nstochastic algorithm, such as a neural network where $f$ is a loss function.\nClassically, this problem is approached through a PAC-Bayes analysis where, in\naddition to the posterior, we choose a prior distribution which captures our\nbelief about the inductive bias of the learning problem. Then, the key quantity\nin PAC-Bayes concentration bounds is a divergence that captures the complexity\nof the learning problem where the de facto standard choice is the KL\ndivergence. However, the tightness of this choice has rarely been questioned.\n  In this paper, we challenge the tightness of the KL-divergence-based bounds\nby showing that it is possible to achieve a strictly tighter bound. In\nparticular, we demonstrate new high-probability PAC-Bayes bounds with a novel\nand better-than-KL divergence that is inspired by Zhang et al. (2022). Our\nproof is inspired by recent advances in regret analysis of gambling algorithms,\nand its use to derive concentration inequalities. Our result is\nfirst-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are\nnot known to be strictly better than KL. Thus, we believe our work marks the\nfirst step towards identifying optimal rates of PAC-Bayes bounds.\n", "link": "http://arxiv.org/abs/2402.09201v2", "date": "2024-04-04", "relevancy": 1.907, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4874}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4818}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4674}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Better-than-KL%20PAC-Bayes%20Bounds&body=Title%3A%20Better-than-KL%20PAC-Bayes%20Bounds%0AAuthor%3A%20Ilja%20Kuzborskij%20and%20Kwang-Sung%20Jun%20and%20Yulian%20Wu%20and%20Kyoungseok%20Jang%20and%20Francesco%20Orabona%0AAbstract%3A%20%20%20Let%20%24f%28%5Ctheta%2C%20X_1%29%2C%24%20%24%20%5Cdots%2C%24%20%24%20f%28%5Ctheta%2C%20X_n%29%24%20be%20a%20sequence%20of%20random%0Aelements%2C%20where%20%24f%24%20is%20a%20fixed%20scalar%20function%2C%20%24X_1%2C%20%5Cdots%2C%20X_n%24%20are%0Aindependent%20random%20variables%20%28data%29%2C%20and%20%24%5Ctheta%24%20is%20a%20random%20parameter%0Adistributed%20according%20to%20some%20data-dependent%20posterior%20distribution%20%24P_n%24.%20In%0Athis%20paper%2C%20we%20consider%20the%20problem%20of%20proving%20concentration%20inequalities%20to%0Aestimate%20the%20mean%20of%20the%20sequence.%20An%20example%20of%20such%20a%20problem%20is%20the%0Aestimation%20of%20the%20generalization%20error%20of%20some%20predictor%20trained%20by%20a%0Astochastic%20algorithm%2C%20such%20as%20a%20neural%20network%20where%20%24f%24%20is%20a%20loss%20function.%0AClassically%2C%20this%20problem%20is%20approached%20through%20a%20PAC-Bayes%20analysis%20where%2C%20in%0Aaddition%20to%20the%20posterior%2C%20we%20choose%20a%20prior%20distribution%20which%20captures%20our%0Abelief%20about%20the%20inductive%20bias%20of%20the%20learning%20problem.%20Then%2C%20the%20key%20quantity%0Ain%20PAC-Bayes%20concentration%20bounds%20is%20a%20divergence%20that%20captures%20the%20complexity%0Aof%20the%20learning%20problem%20where%20the%20de%20facto%20standard%20choice%20is%20the%20KL%0Adivergence.%20However%2C%20the%20tightness%20of%20this%20choice%20has%20rarely%20been%20questioned.%0A%20%20In%20this%20paper%2C%20we%20challenge%20the%20tightness%20of%20the%20KL-divergence-based%20bounds%0Aby%20showing%20that%20it%20is%20possible%20to%20achieve%20a%20strictly%20tighter%20bound.%20In%0Aparticular%2C%20we%20demonstrate%20new%20high-probability%20PAC-Bayes%20bounds%20with%20a%20novel%0Aand%20better-than-KL%20divergence%20that%20is%20inspired%20by%20Zhang%20et%20al.%20%282022%29.%20Our%0Aproof%20is%20inspired%20by%20recent%20advances%20in%20regret%20analysis%20of%20gambling%20algorithms%2C%0Aand%20its%20use%20to%20derive%20concentration%20inequalities.%20Our%20result%20is%0Afirst-of-its-kind%20in%20that%20existing%20PAC-Bayes%20bounds%20with%20non-KL%20divergences%20are%0Anot%20known%20to%20be%20strictly%20better%20than%20KL.%20Thus%2C%20we%20believe%20our%20work%20marks%20the%0Afirst%20step%20towards%20identifying%20optimal%20rates%20of%20PAC-Bayes%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09201v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better-than-KL%20PAC-Bayes%20Bounds&entry.906535625=Ilja%20Kuzborskij%20and%20Kwang-Sung%20Jun%20and%20Yulian%20Wu%20and%20Kyoungseok%20Jang%20and%20Francesco%20Orabona&entry.1292438233=%20%20Let%20%24f%28%5Ctheta%2C%20X_1%29%2C%24%20%24%20%5Cdots%2C%24%20%24%20f%28%5Ctheta%2C%20X_n%29%24%20be%20a%20sequence%20of%20random%0Aelements%2C%20where%20%24f%24%20is%20a%20fixed%20scalar%20function%2C%20%24X_1%2C%20%5Cdots%2C%20X_n%24%20are%0Aindependent%20random%20variables%20%28data%29%2C%20and%20%24%5Ctheta%24%20is%20a%20random%20parameter%0Adistributed%20according%20to%20some%20data-dependent%20posterior%20distribution%20%24P_n%24.%20In%0Athis%20paper%2C%20we%20consider%20the%20problem%20of%20proving%20concentration%20inequalities%20to%0Aestimate%20the%20mean%20of%20the%20sequence.%20An%20example%20of%20such%20a%20problem%20is%20the%0Aestimation%20of%20the%20generalization%20error%20of%20some%20predictor%20trained%20by%20a%0Astochastic%20algorithm%2C%20such%20as%20a%20neural%20network%20where%20%24f%24%20is%20a%20loss%20function.%0AClassically%2C%20this%20problem%20is%20approached%20through%20a%20PAC-Bayes%20analysis%20where%2C%20in%0Aaddition%20to%20the%20posterior%2C%20we%20choose%20a%20prior%20distribution%20which%20captures%20our%0Abelief%20about%20the%20inductive%20bias%20of%20the%20learning%20problem.%20Then%2C%20the%20key%20quantity%0Ain%20PAC-Bayes%20concentration%20bounds%20is%20a%20divergence%20that%20captures%20the%20complexity%0Aof%20the%20learning%20problem%20where%20the%20de%20facto%20standard%20choice%20is%20the%20KL%0Adivergence.%20However%2C%20the%20tightness%20of%20this%20choice%20has%20rarely%20been%20questioned.%0A%20%20In%20this%20paper%2C%20we%20challenge%20the%20tightness%20of%20the%20KL-divergence-based%20bounds%0Aby%20showing%20that%20it%20is%20possible%20to%20achieve%20a%20strictly%20tighter%20bound.%20In%0Aparticular%2C%20we%20demonstrate%20new%20high-probability%20PAC-Bayes%20bounds%20with%20a%20novel%0Aand%20better-than-KL%20divergence%20that%20is%20inspired%20by%20Zhang%20et%20al.%20%282022%29.%20Our%0Aproof%20is%20inspired%20by%20recent%20advances%20in%20regret%20analysis%20of%20gambling%20algorithms%2C%0Aand%20its%20use%20to%20derive%20concentration%20inequalities.%20Our%20result%20is%0Afirst-of-its-kind%20in%20that%20existing%20PAC-Bayes%20bounds%20with%20non-KL%20divergences%20are%0Anot%20known%20to%20be%20strictly%20better%20than%20KL.%20Thus%2C%20we%20believe%20our%20work%20marks%20the%0Afirst%20step%20towards%20identifying%20optimal%20rates%20of%20PAC-Bayes%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09201v2&entry.124074799=Read"},
{"title": "Can Small Language Models Help Large Language Models Reason Better?:\n  LM-Guided Chain-of-Thought", "author": "Jooyoung Lee and Fan Yang and Thanh Tran and Qian Hu and Emre Barut and Kai-Wei Chang and Chengwei Su", "abstract": "  We introduce a novel framework, LM-Guided CoT, that leverages a lightweight\n(i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM\nin reasoning tasks. Specifically, the lightweight LM first generates a\nrationale for each input instance. The Frozen large LM is then prompted to\npredict a task output based on the rationale generated by the lightweight LM.\nOur approach is resource-efficient in the sense that it only requires training\nthe lightweight LM. We optimize the model through 1) knowledge distillation and\n2) reinforcement learning from rationale-oriented and task-oriented reward\nsignals. We assess our method with multi-hop extractive question answering (QA)\nbenchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our\napproach outperforms all baselines regarding answer prediction accuracy. We\nalso find that reinforcement learning helps the model to produce higher-quality\nrationales with improved QA performance.\n", "link": "http://arxiv.org/abs/2404.03414v1", "date": "2024-04-04", "relevancy": 1.9051, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.48}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4798}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4712}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Can%20Small%20Language%20Models%20Help%20Large%20Language%20Models%20Reason%20Better%3F%3A%0A%20%20LM-Guided%20Chain-of-Thought&body=Title%3A%20Can%20Small%20Language%20Models%20Help%20Large%20Language%20Models%20Reason%20Better%3F%3A%0A%20%20LM-Guided%20Chain-of-Thought%0AAuthor%3A%20Jooyoung%20Lee%20and%20Fan%20Yang%20and%20Thanh%20Tran%20and%20Qian%20Hu%20and%20Emre%20Barut%20and%20Kai-Wei%20Chang%20and%20Chengwei%20Su%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20framework%2C%20LM-Guided%20CoT%2C%20that%20leverages%20a%20lightweight%0A%28i.e.%2C%20%3C1B%29%20language%20model%20%28LM%29%20for%20guiding%20a%20black-box%20large%20%28i.e.%2C%20%3E10B%29%20LM%0Ain%20reasoning%20tasks.%20Specifically%2C%20the%20lightweight%20LM%20first%20generates%20a%0Arationale%20for%20each%20input%20instance.%20The%20Frozen%20large%20LM%20is%20then%20prompted%20to%0Apredict%20a%20task%20output%20based%20on%20the%20rationale%20generated%20by%20the%20lightweight%20LM.%0AOur%20approach%20is%20resource-efficient%20in%20the%20sense%20that%20it%20only%20requires%20training%0Athe%20lightweight%20LM.%20We%20optimize%20the%20model%20through%201%29%20knowledge%20distillation%20and%0A2%29%20reinforcement%20learning%20from%20rationale-oriented%20and%20task-oriented%20reward%0Asignals.%20We%20assess%20our%20method%20with%20multi-hop%20extractive%20question%20answering%20%28QA%29%0Abenchmarks%2C%20HotpotQA%2C%20and%202WikiMultiHopQA.%20Experimental%20results%20show%20that%20our%0Aapproach%20outperforms%20all%20baselines%20regarding%20answer%20prediction%20accuracy.%20We%0Aalso%20find%20that%20reinforcement%20learning%20helps%20the%20model%20to%20produce%20higher-quality%0Arationales%20with%20improved%20QA%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03414v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Small%20Language%20Models%20Help%20Large%20Language%20Models%20Reason%20Better%3F%3A%0A%20%20LM-Guided%20Chain-of-Thought&entry.906535625=Jooyoung%20Lee%20and%20Fan%20Yang%20and%20Thanh%20Tran%20and%20Qian%20Hu%20and%20Emre%20Barut%20and%20Kai-Wei%20Chang%20and%20Chengwei%20Su&entry.1292438233=%20%20We%20introduce%20a%20novel%20framework%2C%20LM-Guided%20CoT%2C%20that%20leverages%20a%20lightweight%0A%28i.e.%2C%20%3C1B%29%20language%20model%20%28LM%29%20for%20guiding%20a%20black-box%20large%20%28i.e.%2C%20%3E10B%29%20LM%0Ain%20reasoning%20tasks.%20Specifically%2C%20the%20lightweight%20LM%20first%20generates%20a%0Arationale%20for%20each%20input%20instance.%20The%20Frozen%20large%20LM%20is%20then%20prompted%20to%0Apredict%20a%20task%20output%20based%20on%20the%20rationale%20generated%20by%20the%20lightweight%20LM.%0AOur%20approach%20is%20resource-efficient%20in%20the%20sense%20that%20it%20only%20requires%20training%0Athe%20lightweight%20LM.%20We%20optimize%20the%20model%20through%201%29%20knowledge%20distillation%20and%0A2%29%20reinforcement%20learning%20from%20rationale-oriented%20and%20task-oriented%20reward%0Asignals.%20We%20assess%20our%20method%20with%20multi-hop%20extractive%20question%20answering%20%28QA%29%0Abenchmarks%2C%20HotpotQA%2C%20and%202WikiMultiHopQA.%20Experimental%20results%20show%20that%20our%0Aapproach%20outperforms%20all%20baselines%20regarding%20answer%20prediction%20accuracy.%20We%0Aalso%20find%20that%20reinforcement%20learning%20helps%20the%20model%20to%20produce%20higher-quality%0Arationales%20with%20improved%20QA%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03414v1&entry.124074799=Read"},
{"title": "InsectMamba: Insect Pest Classification with State Space Model", "author": "Qianning Wang and Chenglin Wang and Zhixin Lai and Yucheng Zhou", "abstract": "  The classification of insect pests is a critical task in agricultural\ntechnology, vital for ensuring food security and environmental sustainability.\nHowever, the complexity of pest identification, due to factors like high\ncamouflage and species diversity, poses significant obstacles. Existing methods\nstruggle with the fine-grained feature extraction needed to distinguish between\nclosely related pest species. Although recent advancements have utilized\nmodified network structures and combined deep learning approaches to improve\naccuracy, challenges persist due to the similarity between pests and their\nsurroundings. To address this problem, we introduce InsectMamba, a novel\napproach that integrates State Space Models (SSMs), Convolutional Neural\nNetworks (CNNs), Multi-Head Self-Attention mechanism (MSA), and Multilayer\nPerceptrons (MLPs) within Mix-SSM blocks. This integration facilitates the\nextraction of comprehensive visual features by leveraging the strengths of each\nencoding strategy. A selective module is also proposed to adaptively aggregate\nthese features, enhancing the model's ability to discern pest characteristics.\nInsectMamba was evaluated against strong competitors across five insect pest\nclassification datasets. The results demonstrate its superior performance and\nverify the significance of each model component by an ablation study.\n", "link": "http://arxiv.org/abs/2404.03611v1", "date": "2024-04-04", "relevancy": 1.8926, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4715}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4651}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InsectMamba%3A%20Insect%20Pest%20Classification%20with%20State%20Space%20Model&body=Title%3A%20InsectMamba%3A%20Insect%20Pest%20Classification%20with%20State%20Space%20Model%0AAuthor%3A%20Qianning%20Wang%20and%20Chenglin%20Wang%20and%20Zhixin%20Lai%20and%20Yucheng%20Zhou%0AAbstract%3A%20%20%20The%20classification%20of%20insect%20pests%20is%20a%20critical%20task%20in%20agricultural%0Atechnology%2C%20vital%20for%20ensuring%20food%20security%20and%20environmental%20sustainability.%0AHowever%2C%20the%20complexity%20of%20pest%20identification%2C%20due%20to%20factors%20like%20high%0Acamouflage%20and%20species%20diversity%2C%20poses%20significant%20obstacles.%20Existing%20methods%0Astruggle%20with%20the%20fine-grained%20feature%20extraction%20needed%20to%20distinguish%20between%0Aclosely%20related%20pest%20species.%20Although%20recent%20advancements%20have%20utilized%0Amodified%20network%20structures%20and%20combined%20deep%20learning%20approaches%20to%20improve%0Aaccuracy%2C%20challenges%20persist%20due%20to%20the%20similarity%20between%20pests%20and%20their%0Asurroundings.%20To%20address%20this%20problem%2C%20we%20introduce%20InsectMamba%2C%20a%20novel%0Aapproach%20that%20integrates%20State%20Space%20Models%20%28SSMs%29%2C%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%2C%20Multi-Head%20Self-Attention%20mechanism%20%28MSA%29%2C%20and%20Multilayer%0APerceptrons%20%28MLPs%29%20within%20Mix-SSM%20blocks.%20This%20integration%20facilitates%20the%0Aextraction%20of%20comprehensive%20visual%20features%20by%20leveraging%20the%20strengths%20of%20each%0Aencoding%20strategy.%20A%20selective%20module%20is%20also%20proposed%20to%20adaptively%20aggregate%0Athese%20features%2C%20enhancing%20the%20model%27s%20ability%20to%20discern%20pest%20characteristics.%0AInsectMamba%20was%20evaluated%20against%20strong%20competitors%20across%20five%20insect%20pest%0Aclassification%20datasets.%20The%20results%20demonstrate%20its%20superior%20performance%20and%0Averify%20the%20significance%20of%20each%20model%20component%20by%20an%20ablation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03611v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsectMamba%3A%20Insect%20Pest%20Classification%20with%20State%20Space%20Model&entry.906535625=Qianning%20Wang%20and%20Chenglin%20Wang%20and%20Zhixin%20Lai%20and%20Yucheng%20Zhou&entry.1292438233=%20%20The%20classification%20of%20insect%20pests%20is%20a%20critical%20task%20in%20agricultural%0Atechnology%2C%20vital%20for%20ensuring%20food%20security%20and%20environmental%20sustainability.%0AHowever%2C%20the%20complexity%20of%20pest%20identification%2C%20due%20to%20factors%20like%20high%0Acamouflage%20and%20species%20diversity%2C%20poses%20significant%20obstacles.%20Existing%20methods%0Astruggle%20with%20the%20fine-grained%20feature%20extraction%20needed%20to%20distinguish%20between%0Aclosely%20related%20pest%20species.%20Although%20recent%20advancements%20have%20utilized%0Amodified%20network%20structures%20and%20combined%20deep%20learning%20approaches%20to%20improve%0Aaccuracy%2C%20challenges%20persist%20due%20to%20the%20similarity%20between%20pests%20and%20their%0Asurroundings.%20To%20address%20this%20problem%2C%20we%20introduce%20InsectMamba%2C%20a%20novel%0Aapproach%20that%20integrates%20State%20Space%20Models%20%28SSMs%29%2C%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%2C%20Multi-Head%20Self-Attention%20mechanism%20%28MSA%29%2C%20and%20Multilayer%0APerceptrons%20%28MLPs%29%20within%20Mix-SSM%20blocks.%20This%20integration%20facilitates%20the%0Aextraction%20of%20comprehensive%20visual%20features%20by%20leveraging%20the%20strengths%20of%20each%0Aencoding%20strategy.%20A%20selective%20module%20is%20also%20proposed%20to%20adaptively%20aggregate%0Athese%20features%2C%20enhancing%20the%20model%27s%20ability%20to%20discern%20pest%20characteristics.%0AInsectMamba%20was%20evaluated%20against%20strong%20competitors%20across%20five%20insect%20pest%0Aclassification%20datasets.%20The%20results%20demonstrate%20its%20superior%20performance%20and%0Averify%20the%20significance%20of%20each%20model%20component%20by%20an%20ablation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03611v1&entry.124074799=Read"},
{"title": "RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language\n  Models", "author": "Meiling Tao and Xuechen Liang and Tianyu Shi and Lei Yu and Yiting Xie", "abstract": "  This study presents RoleCraft-GLM, an innovative framework aimed at enhancing\npersonalized role-playing with Large Language Models (LLMs). RoleCraft-GLM\naddresses the key issue of lacking personalized interactions in conversational\nAI, and offers a solution with detailed and emotionally nuanced character\nportrayals. We contribute a unique conversational dataset that shifts from\nconventional celebrity-centric characters to diverse, non-celebrity personas,\nthus enhancing the realism and complexity of language modeling interactions.\nAdditionally, our approach includes meticulous character development, ensuring\ndialogues are both realistic and emotionally resonant. The effectiveness of\nRoleCraft-GLM is validated through various case studies, highlighting its\nversatility and skill in different scenarios. Our framework excels in\ngenerating dialogues that accurately reflect characters' personality traits and\nemotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks\na significant leap in personalized AI interactions, and paves the way for more\nauthentic and immersive AI-assisted role-playing experiences by enabling more\nnuanced and emotionally rich dialogues\n", "link": "http://arxiv.org/abs/2401.09432v2", "date": "2024-04-04", "relevancy": 1.8919, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4706}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4607}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RoleCraft-GLM%3A%20Advancing%20Personalized%20Role-Playing%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20RoleCraft-GLM%3A%20Advancing%20Personalized%20Role-Playing%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Meiling%20Tao%20and%20Xuechen%20Liang%20and%20Tianyu%20Shi%20and%20Lei%20Yu%20and%20Yiting%20Xie%0AAbstract%3A%20%20%20This%20study%20presents%20RoleCraft-GLM%2C%20an%20innovative%20framework%20aimed%20at%20enhancing%0Apersonalized%20role-playing%20with%20Large%20Language%20Models%20%28LLMs%29.%20RoleCraft-GLM%0Aaddresses%20the%20key%20issue%20of%20lacking%20personalized%20interactions%20in%20conversational%0AAI%2C%20and%20offers%20a%20solution%20with%20detailed%20and%20emotionally%20nuanced%20character%0Aportrayals.%20We%20contribute%20a%20unique%20conversational%20dataset%20that%20shifts%20from%0Aconventional%20celebrity-centric%20characters%20to%20diverse%2C%20non-celebrity%20personas%2C%0Athus%20enhancing%20the%20realism%20and%20complexity%20of%20language%20modeling%20interactions.%0AAdditionally%2C%20our%20approach%20includes%20meticulous%20character%20development%2C%20ensuring%0Adialogues%20are%20both%20realistic%20and%20emotionally%20resonant.%20The%20effectiveness%20of%0ARoleCraft-GLM%20is%20validated%20through%20various%20case%20studies%2C%20highlighting%20its%0Aversatility%20and%20skill%20in%20different%20scenarios.%20Our%20framework%20excels%20in%0Agenerating%20dialogues%20that%20accurately%20reflect%20characters%27%20personality%20traits%20and%0Aemotions%2C%20thereby%20boosting%20user%20engagement.%20In%20conclusion%2C%20RoleCraft-GLM%20marks%0Aa%20significant%20leap%20in%20personalized%20AI%20interactions%2C%20and%20paves%20the%20way%20for%20more%0Aauthentic%20and%20immersive%20AI-assisted%20role-playing%20experiences%20by%20enabling%20more%0Anuanced%20and%20emotionally%20rich%20dialogues%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09432v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoleCraft-GLM%3A%20Advancing%20Personalized%20Role-Playing%20in%20Large%20Language%0A%20%20Models&entry.906535625=Meiling%20Tao%20and%20Xuechen%20Liang%20and%20Tianyu%20Shi%20and%20Lei%20Yu%20and%20Yiting%20Xie&entry.1292438233=%20%20This%20study%20presents%20RoleCraft-GLM%2C%20an%20innovative%20framework%20aimed%20at%20enhancing%0Apersonalized%20role-playing%20with%20Large%20Language%20Models%20%28LLMs%29.%20RoleCraft-GLM%0Aaddresses%20the%20key%20issue%20of%20lacking%20personalized%20interactions%20in%20conversational%0AAI%2C%20and%20offers%20a%20solution%20with%20detailed%20and%20emotionally%20nuanced%20character%0Aportrayals.%20We%20contribute%20a%20unique%20conversational%20dataset%20that%20shifts%20from%0Aconventional%20celebrity-centric%20characters%20to%20diverse%2C%20non-celebrity%20personas%2C%0Athus%20enhancing%20the%20realism%20and%20complexity%20of%20language%20modeling%20interactions.%0AAdditionally%2C%20our%20approach%20includes%20meticulous%20character%20development%2C%20ensuring%0Adialogues%20are%20both%20realistic%20and%20emotionally%20resonant.%20The%20effectiveness%20of%0ARoleCraft-GLM%20is%20validated%20through%20various%20case%20studies%2C%20highlighting%20its%0Aversatility%20and%20skill%20in%20different%20scenarios.%20Our%20framework%20excels%20in%0Agenerating%20dialogues%20that%20accurately%20reflect%20characters%27%20personality%20traits%20and%0Aemotions%2C%20thereby%20boosting%20user%20engagement.%20In%20conclusion%2C%20RoleCraft-GLM%20marks%0Aa%20significant%20leap%20in%20personalized%20AI%20interactions%2C%20and%20paves%20the%20way%20for%20more%0Aauthentic%20and%20immersive%20AI-assisted%20role-playing%20experiences%20by%20enabling%20more%0Anuanced%20and%20emotionally%20rich%20dialogues%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09432v2&entry.124074799=Read"},
{"title": "Incorporating Recklessness to Collaborative Filtering based Recommender\n  Systems", "author": "Diego P\u00e9rez-L\u00f3pez and Fernando Ortega and \u00c1ngel Gonz\u00e1lez-Prieto and Jorge Due\u00f1as-Ler\u00edn", "abstract": "  Recommender systems are intrinsically tied to a reliability/coverage dilemma:\nThe more reliable we desire the forecasts, the more conservative the decision\nwill be and thus, the fewer items will be recommended. This leads to a\nsignificant drop in the novelty of these systems, since instead of recommending\nuncertain unusual items, they focus on predicting items with guaranteed\nsuccess. In this paper, we propose the inclusion of a new term in the learning\nprocess of matrix factorization-based recommender systems, called recklessness,\nthat takes into account the variance of the output probability distribution of\nthe predicted ratings. In this way, gauging this recklessness measure we can\nforce more spiky output distribution, enabling the control of the risk level\ndesired when making decisions about the reliability of a prediction.\nExperimental results demonstrate that recklessness not only allows for risk\nregulation but also improves the quantity and quality of predictions provided\nby the recommender system.\n", "link": "http://arxiv.org/abs/2308.02058v2", "date": "2024-04-04", "relevancy": 1.8709, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4573}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Recklessness%20to%20Collaborative%20Filtering%20based%20Recommender%0A%20%20Systems&body=Title%3A%20Incorporating%20Recklessness%20to%20Collaborative%20Filtering%20based%20Recommender%0A%20%20Systems%0AAuthor%3A%20Diego%20P%C3%A9rez-L%C3%B3pez%20and%20Fernando%20Ortega%20and%20%C3%81ngel%20Gonz%C3%A1lez-Prieto%20and%20Jorge%20Due%C3%B1as-Ler%C3%ADn%0AAbstract%3A%20%20%20Recommender%20systems%20are%20intrinsically%20tied%20to%20a%20reliability/coverage%20dilemma%3A%0AThe%20more%20reliable%20we%20desire%20the%20forecasts%2C%20the%20more%20conservative%20the%20decision%0Awill%20be%20and%20thus%2C%20the%20fewer%20items%20will%20be%20recommended.%20This%20leads%20to%20a%0Asignificant%20drop%20in%20the%20novelty%20of%20these%20systems%2C%20since%20instead%20of%20recommending%0Auncertain%20unusual%20items%2C%20they%20focus%20on%20predicting%20items%20with%20guaranteed%0Asuccess.%20In%20this%20paper%2C%20we%20propose%20the%20inclusion%20of%20a%20new%20term%20in%20the%20learning%0Aprocess%20of%20matrix%20factorization-based%20recommender%20systems%2C%20called%20recklessness%2C%0Athat%20takes%20into%20account%20the%20variance%20of%20the%20output%20probability%20distribution%20of%0Athe%20predicted%20ratings.%20In%20this%20way%2C%20gauging%20this%20recklessness%20measure%20we%20can%0Aforce%20more%20spiky%20output%20distribution%2C%20enabling%20the%20control%20of%20the%20risk%20level%0Adesired%20when%20making%20decisions%20about%20the%20reliability%20of%20a%20prediction.%0AExperimental%20results%20demonstrate%20that%20recklessness%20not%20only%20allows%20for%20risk%0Aregulation%20but%20also%20improves%20the%20quantity%20and%20quality%20of%20predictions%20provided%0Aby%20the%20recommender%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.02058v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Recklessness%20to%20Collaborative%20Filtering%20based%20Recommender%0A%20%20Systems&entry.906535625=Diego%20P%C3%A9rez-L%C3%B3pez%20and%20Fernando%20Ortega%20and%20%C3%81ngel%20Gonz%C3%A1lez-Prieto%20and%20Jorge%20Due%C3%B1as-Ler%C3%ADn&entry.1292438233=%20%20Recommender%20systems%20are%20intrinsically%20tied%20to%20a%20reliability/coverage%20dilemma%3A%0AThe%20more%20reliable%20we%20desire%20the%20forecasts%2C%20the%20more%20conservative%20the%20decision%0Awill%20be%20and%20thus%2C%20the%20fewer%20items%20will%20be%20recommended.%20This%20leads%20to%20a%0Asignificant%20drop%20in%20the%20novelty%20of%20these%20systems%2C%20since%20instead%20of%20recommending%0Auncertain%20unusual%20items%2C%20they%20focus%20on%20predicting%20items%20with%20guaranteed%0Asuccess.%20In%20this%20paper%2C%20we%20propose%20the%20inclusion%20of%20a%20new%20term%20in%20the%20learning%0Aprocess%20of%20matrix%20factorization-based%20recommender%20systems%2C%20called%20recklessness%2C%0Athat%20takes%20into%20account%20the%20variance%20of%20the%20output%20probability%20distribution%20of%0Athe%20predicted%20ratings.%20In%20this%20way%2C%20gauging%20this%20recklessness%20measure%20we%20can%0Aforce%20more%20spiky%20output%20distribution%2C%20enabling%20the%20control%20of%20the%20risk%20level%0Adesired%20when%20making%20decisions%20about%20the%20reliability%20of%20a%20prediction.%0AExperimental%20results%20demonstrate%20that%20recklessness%20not%20only%20allows%20for%20risk%0Aregulation%20but%20also%20improves%20the%20quantity%20and%20quality%20of%20predictions%20provided%0Aby%20the%20recommender%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.02058v2&entry.124074799=Read"},
{"title": "Deep Learning in Cardiology", "author": "Paschalis Bizopoulos and Dimitrios Koutsouris", "abstract": "  The medical field is creating large amount of data that physicians are unable\nto decipher and use efficiently. Moreover, rule-based expert systems are\ninefficient in solving complicated medical tasks or for creating insights using\nbig data. Deep learning has emerged as a more accurate and effective technology\nin a wide range of medical problems such as diagnosis, prediction and\nintervention. Deep learning is a representation learning method that consists\nof layers that transform the data non-linearly, thus, revealing hierarchical\nrelationships and structures. In this review we survey deep learning\napplication papers that use structured data, signal and imaging modalities from\ncardiology. We discuss the advantages and limitations of applying deep learning\nin cardiology that also apply in medicine in general, while proposing certain\ndirections as the most viable for clinical use.\n", "link": "http://arxiv.org/abs/1902.11122v5", "date": "2024-04-04", "relevancy": 1.8546, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4755}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4615}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.461}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20in%20Cardiology&body=Title%3A%20Deep%20Learning%20in%20Cardiology%0AAuthor%3A%20Paschalis%20Bizopoulos%20and%20Dimitrios%20Koutsouris%0AAbstract%3A%20%20%20The%20medical%20field%20is%20creating%20large%20amount%20of%20data%20that%20physicians%20are%20unable%0Ato%20decipher%20and%20use%20efficiently.%20Moreover%2C%20rule-based%20expert%20systems%20are%0Ainefficient%20in%20solving%20complicated%20medical%20tasks%20or%20for%20creating%20insights%20using%0Abig%20data.%20Deep%20learning%20has%20emerged%20as%20a%20more%20accurate%20and%20effective%20technology%0Ain%20a%20wide%20range%20of%20medical%20problems%20such%20as%20diagnosis%2C%20prediction%20and%0Aintervention.%20Deep%20learning%20is%20a%20representation%20learning%20method%20that%20consists%0Aof%20layers%20that%20transform%20the%20data%20non-linearly%2C%20thus%2C%20revealing%20hierarchical%0Arelationships%20and%20structures.%20In%20this%20review%20we%20survey%20deep%20learning%0Aapplication%20papers%20that%20use%20structured%20data%2C%20signal%20and%20imaging%20modalities%20from%0Acardiology.%20We%20discuss%20the%20advantages%20and%20limitations%20of%20applying%20deep%20learning%0Ain%20cardiology%20that%20also%20apply%20in%20medicine%20in%20general%2C%20while%20proposing%20certain%0Adirections%20as%20the%20most%20viable%20for%20clinical%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1902.11122v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20in%20Cardiology&entry.906535625=Paschalis%20Bizopoulos%20and%20Dimitrios%20Koutsouris&entry.1292438233=%20%20The%20medical%20field%20is%20creating%20large%20amount%20of%20data%20that%20physicians%20are%20unable%0Ato%20decipher%20and%20use%20efficiently.%20Moreover%2C%20rule-based%20expert%20systems%20are%0Ainefficient%20in%20solving%20complicated%20medical%20tasks%20or%20for%20creating%20insights%20using%0Abig%20data.%20Deep%20learning%20has%20emerged%20as%20a%20more%20accurate%20and%20effective%20technology%0Ain%20a%20wide%20range%20of%20medical%20problems%20such%20as%20diagnosis%2C%20prediction%20and%0Aintervention.%20Deep%20learning%20is%20a%20representation%20learning%20method%20that%20consists%0Aof%20layers%20that%20transform%20the%20data%20non-linearly%2C%20thus%2C%20revealing%20hierarchical%0Arelationships%20and%20structures.%20In%20this%20review%20we%20survey%20deep%20learning%0Aapplication%20papers%20that%20use%20structured%20data%2C%20signal%20and%20imaging%20modalities%20from%0Acardiology.%20We%20discuss%20the%20advantages%20and%20limitations%20of%20applying%20deep%20learning%0Ain%20cardiology%20that%20also%20apply%20in%20medicine%20in%20general%2C%20while%20proposing%20certain%0Adirections%20as%20the%20most%20viable%20for%20clinical%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/1902.11122v5&entry.124074799=Read"},
{"title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering", "author": "Azmine Toushik Wasi and Taki Hasan Rafi and Raima Islam and Dong-Kyu Chae", "abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n", "link": "http://arxiv.org/abs/2404.03528v1", "date": "2024-04-04", "relevancy": 1.849, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4747}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4517}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BanglaAutoKG%3A%20Automatic%20Bangla%20Knowledge%20Graph%20Construction%20with%0A%20%20Semantic%20Neural%20Graph%20Filtering&body=Title%3A%20BanglaAutoKG%3A%20Automatic%20Bangla%20Knowledge%20Graph%20Construction%20with%0A%20%20Semantic%20Neural%20Graph%20Filtering%0AAuthor%3A%20Azmine%20Toushik%20Wasi%20and%20Taki%20Hasan%20Rafi%20and%20Raima%20Islam%20and%20Dong-Kyu%20Chae%0AAbstract%3A%20%20%20Knowledge%20Graphs%20%28KGs%29%20have%20proven%20essential%20in%20information%20processing%20and%0Areasoning%20applications%20because%20they%20link%20related%20entities%20and%20give%20context-rich%0Ainformation%2C%20supporting%20efficient%20information%20retrieval%20and%20knowledge%0Adiscovery%3B%20presenting%20information%20flow%20in%20a%20very%20effective%20manner.%20Despite%0Abeing%20widely%20used%20globally%2C%20Bangla%20is%20relatively%20underrepresented%20in%20KGs%20due%20to%0Aa%20lack%20of%20comprehensive%20datasets%2C%20encoders%2C%20NER%20%28named%20entity%20recognition%29%0Amodels%2C%20POS%20%28part-of-speech%29%20taggers%2C%20and%20lemmatizers%2C%20hindering%20efficient%0Ainformation%20processing%20and%20reasoning%20applications%20in%20the%20language.%20Addressing%0Athe%20KG%20scarcity%20in%20Bengali%2C%20we%20propose%20BanglaAutoKG%2C%20a%20pioneering%20framework%0Athat%20is%20able%20to%20automatically%20construct%20Bengali%20KGs%20from%20any%20Bangla%20text.%20We%0Autilize%20multilingual%20LLMs%20to%20understand%20various%20languages%20and%20correlate%0Aentities%20and%20relations%20universally.%20By%20employing%20a%20translation%20dictionary%20to%0Aidentify%20English%20equivalents%20and%20extracting%20word%20features%20from%20pre-trained%20BERT%0Amodels%2C%20we%20construct%20the%20foundational%20KG.%20To%20reduce%20noise%20and%20align%20word%0Aembeddings%20with%20our%20goal%2C%20we%20employ%20graph-based%20polynomial%20filters.%20Lastly%2C%20we%0Aimplement%20a%20GNN-based%20semantic%20filter%2C%20which%20elevates%20contextual%20understanding%0Aand%20trims%20unnecessary%20edges%2C%20culminating%20in%20the%20formation%20of%20the%20definitive%20KG.%0AEmpirical%20findings%20and%20case%20studies%20demonstrate%20the%20universal%20effectiveness%20of%0Aour%20model%2C%20capable%20of%20autonomously%20constructing%20semantically%20enriched%20KGs%20from%0Aany%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03528v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BanglaAutoKG%3A%20Automatic%20Bangla%20Knowledge%20Graph%20Construction%20with%0A%20%20Semantic%20Neural%20Graph%20Filtering&entry.906535625=Azmine%20Toushik%20Wasi%20and%20Taki%20Hasan%20Rafi%20and%20Raima%20Islam%20and%20Dong-Kyu%20Chae&entry.1292438233=%20%20Knowledge%20Graphs%20%28KGs%29%20have%20proven%20essential%20in%20information%20processing%20and%0Areasoning%20applications%20because%20they%20link%20related%20entities%20and%20give%20context-rich%0Ainformation%2C%20supporting%20efficient%20information%20retrieval%20and%20knowledge%0Adiscovery%3B%20presenting%20information%20flow%20in%20a%20very%20effective%20manner.%20Despite%0Abeing%20widely%20used%20globally%2C%20Bangla%20is%20relatively%20underrepresented%20in%20KGs%20due%20to%0Aa%20lack%20of%20comprehensive%20datasets%2C%20encoders%2C%20NER%20%28named%20entity%20recognition%29%0Amodels%2C%20POS%20%28part-of-speech%29%20taggers%2C%20and%20lemmatizers%2C%20hindering%20efficient%0Ainformation%20processing%20and%20reasoning%20applications%20in%20the%20language.%20Addressing%0Athe%20KG%20scarcity%20in%20Bengali%2C%20we%20propose%20BanglaAutoKG%2C%20a%20pioneering%20framework%0Athat%20is%20able%20to%20automatically%20construct%20Bengali%20KGs%20from%20any%20Bangla%20text.%20We%0Autilize%20multilingual%20LLMs%20to%20understand%20various%20languages%20and%20correlate%0Aentities%20and%20relations%20universally.%20By%20employing%20a%20translation%20dictionary%20to%0Aidentify%20English%20equivalents%20and%20extracting%20word%20features%20from%20pre-trained%20BERT%0Amodels%2C%20we%20construct%20the%20foundational%20KG.%20To%20reduce%20noise%20and%20align%20word%0Aembeddings%20with%20our%20goal%2C%20we%20employ%20graph-based%20polynomial%20filters.%20Lastly%2C%20we%0Aimplement%20a%20GNN-based%20semantic%20filter%2C%20which%20elevates%20contextual%20understanding%0Aand%20trims%20unnecessary%20edges%2C%20culminating%20in%20the%20formation%20of%20the%20definitive%20KG.%0AEmpirical%20findings%20and%20case%20studies%20demonstrate%20the%20universal%20effectiveness%20of%0Aour%20model%2C%20capable%20of%20autonomously%20constructing%20semantically%20enriched%20KGs%20from%0Aany%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03528v1&entry.124074799=Read"},
{"title": "Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch\n  Processes", "author": "Christian W. Frey", "abstract": "  Industrial production processes, especially in the pharmaceutical industry,\nare complex systems that require continuous monitoring to ensure efficiency,\nproduct quality, and safety. This paper presents a hybrid unsupervised learning\nstrategy (HULS) for monitoring complex industrial processes. Addressing the\nlimitations of traditional Self-Organizing Maps (SOMs), especially in scenarios\nwith unbalanced data sets and highly correlated process variables, HULS\ncombines existing unsupervised learning techniques to address these challenges.\nTo evaluate the performance of the HULS concept, comparative experiments are\nperformed based on a laboratory batch\n", "link": "http://arxiv.org/abs/2403.13032v2", "date": "2024-04-04", "relevancy": 1.8212, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4735}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4425}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4422}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Unsupervised%20Learning%20Strategy%20for%20Monitoring%20Industrial%20Batch%0A%20%20Processes&body=Title%3A%20Hybrid%20Unsupervised%20Learning%20Strategy%20for%20Monitoring%20Industrial%20Batch%0A%20%20Processes%0AAuthor%3A%20Christian%20W.%20Frey%0AAbstract%3A%20%20%20Industrial%20production%20processes%2C%20especially%20in%20the%20pharmaceutical%20industry%2C%0Aare%20complex%20systems%20that%20require%20continuous%20monitoring%20to%20ensure%20efficiency%2C%0Aproduct%20quality%2C%20and%20safety.%20This%20paper%20presents%20a%20hybrid%20unsupervised%20learning%0Astrategy%20%28HULS%29%20for%20monitoring%20complex%20industrial%20processes.%20Addressing%20the%0Alimitations%20of%20traditional%20Self-Organizing%20Maps%20%28SOMs%29%2C%20especially%20in%20scenarios%0Awith%20unbalanced%20data%20sets%20and%20highly%20correlated%20process%20variables%2C%20HULS%0Acombines%20existing%20unsupervised%20learning%20techniques%20to%20address%20these%20challenges.%0ATo%20evaluate%20the%20performance%20of%20the%20HULS%20concept%2C%20comparative%20experiments%20are%0Aperformed%20based%20on%20a%20laboratory%20batch%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13032v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Unsupervised%20Learning%20Strategy%20for%20Monitoring%20Industrial%20Batch%0A%20%20Processes&entry.906535625=Christian%20W.%20Frey&entry.1292438233=%20%20Industrial%20production%20processes%2C%20especially%20in%20the%20pharmaceutical%20industry%2C%0Aare%20complex%20systems%20that%20require%20continuous%20monitoring%20to%20ensure%20efficiency%2C%0Aproduct%20quality%2C%20and%20safety.%20This%20paper%20presents%20a%20hybrid%20unsupervised%20learning%0Astrategy%20%28HULS%29%20for%20monitoring%20complex%20industrial%20processes.%20Addressing%20the%0Alimitations%20of%20traditional%20Self-Organizing%20Maps%20%28SOMs%29%2C%20especially%20in%20scenarios%0Awith%20unbalanced%20data%20sets%20and%20highly%20correlated%20process%20variables%2C%20HULS%0Acombines%20existing%20unsupervised%20learning%20techniques%20to%20address%20these%20challenges.%0ATo%20evaluate%20the%20performance%20of%20the%20HULS%20concept%2C%20comparative%20experiments%20are%0Aperformed%20based%20on%20a%20laboratory%20batch%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13032v2&entry.124074799=Read"},
{"title": "PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects\n  and Environments", "author": "Kairui Ding and Boyuan Chen and Ruihai Wu and Yuyang Li and Zongzheng Zhang and Huan-ang Gao and Siqi Li and Yixin Zhu and Guyue Zhou and Hao Dong and Hao Zhao", "abstract": "  Robotic manipulation of ungraspable objects with two-finger grippers presents\nsignificant challenges due to the paucity of graspable features, while\ntraditional pre-grasping techniques, which rely on repositioning objects and\nleveraging external aids like table edges, lack the adaptability across object\ncategories and scenes. Addressing this, we introduce PreAfford, a novel\npre-grasping planning framework that utilizes a point-level affordance\nrepresentation and a relay training approach to enhance adaptability across a\nbroad range of environments and object types, including those previously\nunseen. Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly\nimproves grasping success rates by 69% and validates its practicality through\nreal-world experiments. This work offers a robust and adaptable solution for\nmanipulating ungraspable objects.\n", "link": "http://arxiv.org/abs/2404.03634v1", "date": "2024-04-04", "relevancy": 1.8205, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6234}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6143}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5579}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PreAfford%3A%20Universal%20Affordance-Based%20Pre-Grasping%20for%20Diverse%20Objects%0A%20%20and%20Environments&body=Title%3A%20PreAfford%3A%20Universal%20Affordance-Based%20Pre-Grasping%20for%20Diverse%20Objects%0A%20%20and%20Environments%0AAuthor%3A%20Kairui%20Ding%20and%20Boyuan%20Chen%20and%20Ruihai%20Wu%20and%20Yuyang%20Li%20and%20Zongzheng%20Zhang%20and%20Huan-ang%20Gao%20and%20Siqi%20Li%20and%20Yixin%20Zhu%20and%20Guyue%20Zhou%20and%20Hao%20Dong%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Robotic%20manipulation%20of%20ungraspable%20objects%20with%20two-finger%20grippers%20presents%0Asignificant%20challenges%20due%20to%20the%20paucity%20of%20graspable%20features%2C%20while%0Atraditional%20pre-grasping%20techniques%2C%20which%20rely%20on%20repositioning%20objects%20and%0Aleveraging%20external%20aids%20like%20table%20edges%2C%20lack%20the%20adaptability%20across%20object%0Acategories%20and%20scenes.%20Addressing%20this%2C%20we%20introduce%20PreAfford%2C%20a%20novel%0Apre-grasping%20planning%20framework%20that%20utilizes%20a%20point-level%20affordance%0Arepresentation%20and%20a%20relay%20training%20approach%20to%20enhance%20adaptability%20across%20a%0Abroad%20range%20of%20environments%20and%20object%20types%2C%20including%20those%20previously%0Aunseen.%20Demonstrated%20on%20the%20ShapeNet-v2%20dataset%2C%20PreAfford%20significantly%0Aimproves%20grasping%20success%20rates%20by%2069%25%20and%20validates%20its%20practicality%20through%0Areal-world%20experiments.%20This%20work%20offers%20a%20robust%20and%20adaptable%20solution%20for%0Amanipulating%20ungraspable%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03634v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PreAfford%3A%20Universal%20Affordance-Based%20Pre-Grasping%20for%20Diverse%20Objects%0A%20%20and%20Environments&entry.906535625=Kairui%20Ding%20and%20Boyuan%20Chen%20and%20Ruihai%20Wu%20and%20Yuyang%20Li%20and%20Zongzheng%20Zhang%20and%20Huan-ang%20Gao%20and%20Siqi%20Li%20and%20Yixin%20Zhu%20and%20Guyue%20Zhou%20and%20Hao%20Dong%20and%20Hao%20Zhao&entry.1292438233=%20%20Robotic%20manipulation%20of%20ungraspable%20objects%20with%20two-finger%20grippers%20presents%0Asignificant%20challenges%20due%20to%20the%20paucity%20of%20graspable%20features%2C%20while%0Atraditional%20pre-grasping%20techniques%2C%20which%20rely%20on%20repositioning%20objects%20and%0Aleveraging%20external%20aids%20like%20table%20edges%2C%20lack%20the%20adaptability%20across%20object%0Acategories%20and%20scenes.%20Addressing%20this%2C%20we%20introduce%20PreAfford%2C%20a%20novel%0Apre-grasping%20planning%20framework%20that%20utilizes%20a%20point-level%20affordance%0Arepresentation%20and%20a%20relay%20training%20approach%20to%20enhance%20adaptability%20across%20a%0Abroad%20range%20of%20environments%20and%20object%20types%2C%20including%20those%20previously%0Aunseen.%20Demonstrated%20on%20the%20ShapeNet-v2%20dataset%2C%20PreAfford%20significantly%0Aimproves%20grasping%20success%20rates%20by%2069%25%20and%20validates%20its%20practicality%20through%0Areal-world%20experiments.%20This%20work%20offers%20a%20robust%20and%20adaptable%20solution%20for%0Amanipulating%20ungraspable%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03634v1&entry.124074799=Read"},
{"title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion\n  Tokens", "author": "Jiacheng Liu and Sewon Min and Luke Zettlemoyer and Yejin Choi and Hannaneh Hajishirzi", "abstract": "  Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers.\n", "link": "http://arxiv.org/abs/2401.17377v3", "date": "2024-04-04", "relevancy": 1.8136, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.51}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4526}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4316}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Infini-gram%3A%20Scaling%20Unbounded%20n-gram%20Language%20Models%20to%20a%20Trillion%0A%20%20Tokens&body=Title%3A%20Infini-gram%3A%20Scaling%20Unbounded%20n-gram%20Language%20Models%20to%20a%20Trillion%0A%20%20Tokens%0AAuthor%3A%20Jiacheng%20Liu%20and%20Sewon%20Min%20and%20Luke%20Zettlemoyer%20and%20Yejin%20Choi%20and%20Hannaneh%20Hajishirzi%0AAbstract%3A%20%20%20Are%20%24n%24-gram%20language%20models%20still%20relevant%20in%20this%20era%20of%20neural%20large%0Alanguage%20models%20%28LLMs%29%3F%20Our%20answer%20is%20yes%2C%20and%20we%20showcase%20their%20values%20in%20both%0Atext%20analysis%20and%20improving%20neural%20LLMs.%20This%20was%20done%20by%20modernizing%20%24n%24-gram%0ALMs%20in%20two%20aspects.%20First%2C%20we%20train%20them%20at%20the%20same%20data%20scale%20as%20neural%20LLMs%0A--%205%20trillion%20tokens.%20This%20is%20the%20largest%20%24n%24-gram%20LM%20ever%20built.%20Second%2C%0Aexisting%20%24n%24-gram%20LMs%20use%20small%20%24n%24%20which%20hinders%20their%20performance%3B%20we%20instead%0Aallow%20%24n%24%20to%20be%20arbitrarily%20large%2C%20by%20introducing%20a%20new%20%24%5Cinfty%24-gram%20LM%20with%0Abackoff.%20Instead%20of%20pre-computing%20%24n%24-gram%20count%20tables%20%28which%20would%20be%20very%0Aexpensive%29%2C%20we%20develop%20an%20engine%20named%20infini-gram%20--%20powered%20by%20suffix%20arrays%0A--%20that%20can%20compute%20%24%5Cinfty%24-gram%20%28as%20well%20as%20%24n%24-gram%20with%20arbitrary%20%24n%24%29%0Aprobabilities%20with%20millisecond-level%20latency.%20The%20%24%5Cinfty%24-gram%20framework%20and%0Ainfini-gram%20engine%20enable%20us%20to%20conduct%20many%20novel%20and%20interesting%20analyses%20of%0Ahuman-written%20and%20machine-generated%20text%3A%20we%20find%20that%20the%20%24%5Cinfty%24-gram%20LM%20has%0Afairly%20high%20accuracy%20for%20next-token%20prediction%20%2847%25%29%2C%20and%20can%20complement%20neural%0ALLMs%20to%20greatly%20reduce%20their%20perplexity.%20When%20analyzing%20machine-generated%20text%2C%0Awe%20also%20observe%20irregularities%20in%20the%20machine--%24%5Cinfty%24-gram%20agreement%20level%0Awith%20respect%20to%20the%20suffix%20length%2C%20which%20indicates%20deficiencies%20in%20neural%20LLM%0Apretraining%20and%20the%20positional%20embeddings%20of%20Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17377v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infini-gram%3A%20Scaling%20Unbounded%20n-gram%20Language%20Models%20to%20a%20Trillion%0A%20%20Tokens&entry.906535625=Jiacheng%20Liu%20and%20Sewon%20Min%20and%20Luke%20Zettlemoyer%20and%20Yejin%20Choi%20and%20Hannaneh%20Hajishirzi&entry.1292438233=%20%20Are%20%24n%24-gram%20language%20models%20still%20relevant%20in%20this%20era%20of%20neural%20large%0Alanguage%20models%20%28LLMs%29%3F%20Our%20answer%20is%20yes%2C%20and%20we%20showcase%20their%20values%20in%20both%0Atext%20analysis%20and%20improving%20neural%20LLMs.%20This%20was%20done%20by%20modernizing%20%24n%24-gram%0ALMs%20in%20two%20aspects.%20First%2C%20we%20train%20them%20at%20the%20same%20data%20scale%20as%20neural%20LLMs%0A--%205%20trillion%20tokens.%20This%20is%20the%20largest%20%24n%24-gram%20LM%20ever%20built.%20Second%2C%0Aexisting%20%24n%24-gram%20LMs%20use%20small%20%24n%24%20which%20hinders%20their%20performance%3B%20we%20instead%0Aallow%20%24n%24%20to%20be%20arbitrarily%20large%2C%20by%20introducing%20a%20new%20%24%5Cinfty%24-gram%20LM%20with%0Abackoff.%20Instead%20of%20pre-computing%20%24n%24-gram%20count%20tables%20%28which%20would%20be%20very%0Aexpensive%29%2C%20we%20develop%20an%20engine%20named%20infini-gram%20--%20powered%20by%20suffix%20arrays%0A--%20that%20can%20compute%20%24%5Cinfty%24-gram%20%28as%20well%20as%20%24n%24-gram%20with%20arbitrary%20%24n%24%29%0Aprobabilities%20with%20millisecond-level%20latency.%20The%20%24%5Cinfty%24-gram%20framework%20and%0Ainfini-gram%20engine%20enable%20us%20to%20conduct%20many%20novel%20and%20interesting%20analyses%20of%0Ahuman-written%20and%20machine-generated%20text%3A%20we%20find%20that%20the%20%24%5Cinfty%24-gram%20LM%20has%0Afairly%20high%20accuracy%20for%20next-token%20prediction%20%2847%25%29%2C%20and%20can%20complement%20neural%0ALLMs%20to%20greatly%20reduce%20their%20perplexity.%20When%20analyzing%20machine-generated%20text%2C%0Awe%20also%20observe%20irregularities%20in%20the%20machine--%24%5Cinfty%24-gram%20agreement%20level%0Awith%20respect%20to%20the%20suffix%20length%2C%20which%20indicates%20deficiencies%20in%20neural%20LLM%0Apretraining%20and%20the%20positional%20embeddings%20of%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17377v3&entry.124074799=Read"},
{"title": "About Test-time training for outlier detection", "author": "Simon Kl\u00fcttermann and Emmanuel M\u00fcller", "abstract": "  In this paper, we introduce DOUST, our method applying test-time training for\noutlier detection, significantly improving the detection performance. After\nthoroughly evaluating our algorithm on common benchmark datasets, we discuss a\ncommon problem and show that it disappears with a large enough test set. Thus,\nwe conclude that under reasonable conditions, our algorithm can reach almost\nsupervised performance even when no labeled outliers are given.\n", "link": "http://arxiv.org/abs/2404.03495v1", "date": "2024-04-04", "relevancy": 1.8108, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4757}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4502}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.446}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20About%20Test-time%20training%20for%20outlier%20detection&body=Title%3A%20About%20Test-time%20training%20for%20outlier%20detection%0AAuthor%3A%20Simon%20Kl%C3%BCttermann%20and%20Emmanuel%20M%C3%BCller%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20DOUST%2C%20our%20method%20applying%20test-time%20training%20for%0Aoutlier%20detection%2C%20significantly%20improving%20the%20detection%20performance.%20After%0Athoroughly%20evaluating%20our%20algorithm%20on%20common%20benchmark%20datasets%2C%20we%20discuss%20a%0Acommon%20problem%20and%20show%20that%20it%20disappears%20with%20a%20large%20enough%20test%20set.%20Thus%2C%0Awe%20conclude%20that%20under%20reasonable%20conditions%2C%20our%20algorithm%20can%20reach%20almost%0Asupervised%20performance%20even%20when%20no%20labeled%20outliers%20are%20given.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03495v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=About%20Test-time%20training%20for%20outlier%20detection&entry.906535625=Simon%20Kl%C3%BCttermann%20and%20Emmanuel%20M%C3%BCller&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20DOUST%2C%20our%20method%20applying%20test-time%20training%20for%0Aoutlier%20detection%2C%20significantly%20improving%20the%20detection%20performance.%20After%0Athoroughly%20evaluating%20our%20algorithm%20on%20common%20benchmark%20datasets%2C%20we%20discuss%20a%0Acommon%20problem%20and%20show%20that%20it%20disappears%20with%20a%20large%20enough%20test%20set.%20Thus%2C%0Awe%20conclude%20that%20under%20reasonable%20conditions%2C%20our%20algorithm%20can%20reach%20almost%0Asupervised%20performance%20even%20when%20no%20labeled%20outliers%20are%20given.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03495v1&entry.124074799=Read"},
{"title": "Capabilities of Large Language Models in Control Engineering: A\n  Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra", "author": "Darioush Kevian and Usman Syed and Xingang Guo and Aaron Havens and Geir Dullerud and Peter Seiler and Lianhui Qin and Bin Hu", "abstract": "  In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solving\nundergraduate-level control problems. Controls provides an interesting case\nstudy for LLM reasoning due to its combination of mathematical theory and\nengineering design. We introduce ControlBench, a benchmark dataset tailored to\nreflect the breadth, depth, and complexity of classical control design. We use\nthis dataset to study and evaluate the problem-solving abilities of these LLMs\nin the context of control engineering. We present evaluations conducted by a\npanel of human experts, providing insights into the accuracy, reasoning, and\nexplanatory prowess of LLMs in control engineering. Our analysis reveals the\nstrengths and limitations of each LLM in the context of classical control, and\nour results imply that Claude 3 Opus has become the state-of-the-art LLM for\nsolving undergraduate control problems. Our study serves as an initial step\ntowards the broader goal of employing artificial general intelligence in\ncontrol engineering.\n", "link": "http://arxiv.org/abs/2404.03647v1", "date": "2024-04-04", "relevancy": 1.8037, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4488}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Capabilities%20of%20Large%20Language%20Models%20in%20Control%20Engineering%3A%20A%0A%20%20Benchmark%20Study%20on%20GPT-4%2C%20Claude%203%20Opus%2C%20and%20Gemini%201.0%20Ultra&body=Title%3A%20Capabilities%20of%20Large%20Language%20Models%20in%20Control%20Engineering%3A%20A%0A%20%20Benchmark%20Study%20on%20GPT-4%2C%20Claude%203%20Opus%2C%20and%20Gemini%201.0%20Ultra%0AAuthor%3A%20Darioush%20Kevian%20and%20Usman%20Syed%20and%20Xingang%20Guo%20and%20Aaron%20Havens%20and%20Geir%20Dullerud%20and%20Peter%20Seiler%20and%20Lianhui%20Qin%20and%20Bin%20Hu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explore%20the%20capabilities%20of%20state-of-the-art%20large%20language%0Amodels%20%28LLMs%29%20such%20as%20GPT-4%2C%20Claude%203%20Opus%2C%20and%20Gemini%201.0%20Ultra%20in%20solving%0Aundergraduate-level%20control%20problems.%20Controls%20provides%20an%20interesting%20case%0Astudy%20for%20LLM%20reasoning%20due%20to%20its%20combination%20of%20mathematical%20theory%20and%0Aengineering%20design.%20We%20introduce%20ControlBench%2C%20a%20benchmark%20dataset%20tailored%20to%0Areflect%20the%20breadth%2C%20depth%2C%20and%20complexity%20of%20classical%20control%20design.%20We%20use%0Athis%20dataset%20to%20study%20and%20evaluate%20the%20problem-solving%20abilities%20of%20these%20LLMs%0Ain%20the%20context%20of%20control%20engineering.%20We%20present%20evaluations%20conducted%20by%20a%0Apanel%20of%20human%20experts%2C%20providing%20insights%20into%20the%20accuracy%2C%20reasoning%2C%20and%0Aexplanatory%20prowess%20of%20LLMs%20in%20control%20engineering.%20Our%20analysis%20reveals%20the%0Astrengths%20and%20limitations%20of%20each%20LLM%20in%20the%20context%20of%20classical%20control%2C%20and%0Aour%20results%20imply%20that%20Claude%203%20Opus%20has%20become%20the%20state-of-the-art%20LLM%20for%0Asolving%20undergraduate%20control%20problems.%20Our%20study%20serves%20as%20an%20initial%20step%0Atowards%20the%20broader%20goal%20of%20employing%20artificial%20general%20intelligence%20in%0Acontrol%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03647v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capabilities%20of%20Large%20Language%20Models%20in%20Control%20Engineering%3A%20A%0A%20%20Benchmark%20Study%20on%20GPT-4%2C%20Claude%203%20Opus%2C%20and%20Gemini%201.0%20Ultra&entry.906535625=Darioush%20Kevian%20and%20Usman%20Syed%20and%20Xingang%20Guo%20and%20Aaron%20Havens%20and%20Geir%20Dullerud%20and%20Peter%20Seiler%20and%20Lianhui%20Qin%20and%20Bin%20Hu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explore%20the%20capabilities%20of%20state-of-the-art%20large%20language%0Amodels%20%28LLMs%29%20such%20as%20GPT-4%2C%20Claude%203%20Opus%2C%20and%20Gemini%201.0%20Ultra%20in%20solving%0Aundergraduate-level%20control%20problems.%20Controls%20provides%20an%20interesting%20case%0Astudy%20for%20LLM%20reasoning%20due%20to%20its%20combination%20of%20mathematical%20theory%20and%0Aengineering%20design.%20We%20introduce%20ControlBench%2C%20a%20benchmark%20dataset%20tailored%20to%0Areflect%20the%20breadth%2C%20depth%2C%20and%20complexity%20of%20classical%20control%20design.%20We%20use%0Athis%20dataset%20to%20study%20and%20evaluate%20the%20problem-solving%20abilities%20of%20these%20LLMs%0Ain%20the%20context%20of%20control%20engineering.%20We%20present%20evaluations%20conducted%20by%20a%0Apanel%20of%20human%20experts%2C%20providing%20insights%20into%20the%20accuracy%2C%20reasoning%2C%20and%0Aexplanatory%20prowess%20of%20LLMs%20in%20control%20engineering.%20Our%20analysis%20reveals%20the%0Astrengths%20and%20limitations%20of%20each%20LLM%20in%20the%20context%20of%20classical%20control%2C%20and%0Aour%20results%20imply%20that%20Claude%203%20Opus%20has%20become%20the%20state-of-the-art%20LLM%20for%0Asolving%20undergraduate%20control%20problems.%20Our%20study%20serves%20as%20an%20initial%20step%0Atowards%20the%20broader%20goal%20of%20employing%20artificial%20general%20intelligence%20in%0Acontrol%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03647v1&entry.124074799=Read"},
{"title": "You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF\n  Robotic Grasping of Novel Objects", "author": "Lei Zhou and Haozhe Wang and Zhengshen Zhang and Zhiyang Liu and Francis EH Tay and adn Marcelo H. Ang. Jr", "abstract": "  In the realm of robotic grasping, achieving accurate and reliable\ninteractions with the environment is a pivotal challenge. Traditional methods\nof grasp planning methods utilizing partial point clouds derived from depth\nimage often suffer from reduced scene understanding due to occlusion,\nultimately impeding their grasping accuracy. Furthermore, scene reconstruction\nmethods have primarily relied upon static techniques, which are susceptible to\nenvironment change during manipulation process limits their efficacy in\nreal-time grasping tasks. To address these limitations, this paper introduces a\nnovel two-stage pipeline for dynamic scene reconstruction. In the first stage,\nour approach takes scene scanning as input to register each target object with\nmesh reconstruction and novel object pose tracking. In the second stage, pose\ntracking is still performed to provide object poses in real-time, enabling our\napproach to transform the reconstructed object point clouds back into the\nscene. Unlike conventional methodologies, which rely on static scene snapshots,\nour method continuously captures the evolving scene geometry, resulting in a\ncomprehensive and up-to-date point cloud representation. By circumventing the\nconstraints posed by occlusion, our method enhances the overall grasp planning\nprocess and empowers state-of-the-art 6-DoF robotic grasping algorithms to\nexhibit markedly improved accuracy.\n", "link": "http://arxiv.org/abs/2404.03462v1", "date": "2024-04-04", "relevancy": 1.7937, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.647}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5889}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5819}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20You%20Only%20Scan%20Once%3A%20A%20Dynamic%20Scene%20Reconstruction%20Pipeline%20for%206-DoF%0A%20%20Robotic%20Grasping%20of%20Novel%20Objects&body=Title%3A%20You%20Only%20Scan%20Once%3A%20A%20Dynamic%20Scene%20Reconstruction%20Pipeline%20for%206-DoF%0A%20%20Robotic%20Grasping%20of%20Novel%20Objects%0AAuthor%3A%20Lei%20Zhou%20and%20Haozhe%20Wang%20and%20Zhengshen%20Zhang%20and%20Zhiyang%20Liu%20and%20Francis%20EH%20Tay%20and%20adn%20Marcelo%20H.%20Ang.%20Jr%0AAbstract%3A%20%20%20In%20the%20realm%20of%20robotic%20grasping%2C%20achieving%20accurate%20and%20reliable%0Ainteractions%20with%20the%20environment%20is%20a%20pivotal%20challenge.%20Traditional%20methods%0Aof%20grasp%20planning%20methods%20utilizing%20partial%20point%20clouds%20derived%20from%20depth%0Aimage%20often%20suffer%20from%20reduced%20scene%20understanding%20due%20to%20occlusion%2C%0Aultimately%20impeding%20their%20grasping%20accuracy.%20Furthermore%2C%20scene%20reconstruction%0Amethods%20have%20primarily%20relied%20upon%20static%20techniques%2C%20which%20are%20susceptible%20to%0Aenvironment%20change%20during%20manipulation%20process%20limits%20their%20efficacy%20in%0Areal-time%20grasping%20tasks.%20To%20address%20these%20limitations%2C%20this%20paper%20introduces%20a%0Anovel%20two-stage%20pipeline%20for%20dynamic%20scene%20reconstruction.%20In%20the%20first%20stage%2C%0Aour%20approach%20takes%20scene%20scanning%20as%20input%20to%20register%20each%20target%20object%20with%0Amesh%20reconstruction%20and%20novel%20object%20pose%20tracking.%20In%20the%20second%20stage%2C%20pose%0Atracking%20is%20still%20performed%20to%20provide%20object%20poses%20in%20real-time%2C%20enabling%20our%0Aapproach%20to%20transform%20the%20reconstructed%20object%20point%20clouds%20back%20into%20the%0Ascene.%20Unlike%20conventional%20methodologies%2C%20which%20rely%20on%20static%20scene%20snapshots%2C%0Aour%20method%20continuously%20captures%20the%20evolving%20scene%20geometry%2C%20resulting%20in%20a%0Acomprehensive%20and%20up-to-date%20point%20cloud%20representation.%20By%20circumventing%20the%0Aconstraints%20posed%20by%20occlusion%2C%20our%20method%20enhances%20the%20overall%20grasp%20planning%0Aprocess%20and%20empowers%20state-of-the-art%206-DoF%20robotic%20grasping%20algorithms%20to%0Aexhibit%20markedly%20improved%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03462v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Only%20Scan%20Once%3A%20A%20Dynamic%20Scene%20Reconstruction%20Pipeline%20for%206-DoF%0A%20%20Robotic%20Grasping%20of%20Novel%20Objects&entry.906535625=Lei%20Zhou%20and%20Haozhe%20Wang%20and%20Zhengshen%20Zhang%20and%20Zhiyang%20Liu%20and%20Francis%20EH%20Tay%20and%20adn%20Marcelo%20H.%20Ang.%20Jr&entry.1292438233=%20%20In%20the%20realm%20of%20robotic%20grasping%2C%20achieving%20accurate%20and%20reliable%0Ainteractions%20with%20the%20environment%20is%20a%20pivotal%20challenge.%20Traditional%20methods%0Aof%20grasp%20planning%20methods%20utilizing%20partial%20point%20clouds%20derived%20from%20depth%0Aimage%20often%20suffer%20from%20reduced%20scene%20understanding%20due%20to%20occlusion%2C%0Aultimately%20impeding%20their%20grasping%20accuracy.%20Furthermore%2C%20scene%20reconstruction%0Amethods%20have%20primarily%20relied%20upon%20static%20techniques%2C%20which%20are%20susceptible%20to%0Aenvironment%20change%20during%20manipulation%20process%20limits%20their%20efficacy%20in%0Areal-time%20grasping%20tasks.%20To%20address%20these%20limitations%2C%20this%20paper%20introduces%20a%0Anovel%20two-stage%20pipeline%20for%20dynamic%20scene%20reconstruction.%20In%20the%20first%20stage%2C%0Aour%20approach%20takes%20scene%20scanning%20as%20input%20to%20register%20each%20target%20object%20with%0Amesh%20reconstruction%20and%20novel%20object%20pose%20tracking.%20In%20the%20second%20stage%2C%20pose%0Atracking%20is%20still%20performed%20to%20provide%20object%20poses%20in%20real-time%2C%20enabling%20our%0Aapproach%20to%20transform%20the%20reconstructed%20object%20point%20clouds%20back%20into%20the%0Ascene.%20Unlike%20conventional%20methodologies%2C%20which%20rely%20on%20static%20scene%20snapshots%2C%0Aour%20method%20continuously%20captures%20the%20evolving%20scene%20geometry%2C%20resulting%20in%20a%0Acomprehensive%20and%20up-to-date%20point%20cloud%20representation.%20By%20circumventing%20the%0Aconstraints%20posed%20by%20occlusion%2C%20our%20method%20enhances%20the%20overall%20grasp%20planning%0Aprocess%20and%20empowers%20state-of-the-art%206-DoF%20robotic%20grasping%20algorithms%20to%0Aexhibit%20markedly%20improved%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03462v1&entry.124074799=Read"},
{"title": "Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity", "author": "Jake Varley and Sumeet Singh and Deepali Jain and Krzysztof Choromanski and Andy Zeng and Somnath Basu Roy Chowdhury and Avinava Dubey and Vikas Sindhwani", "abstract": "  We present an embodied AI system which receives open-ended natural language\ninstructions from a human, and controls two arms to collaboratively accomplish\npotentially long-horizon tasks over a large workspace. Our system is modular:\nit deploys state of the art Large Language Models for task\nplanning,Vision-Language models for semantic perception, and Point Cloud\ntransformers for grasping. With semantic and physical safety in mind, these\nmodules are interfaced with a real-time trajectory optimizer and a compliant\ntracking controller to enable human-robot proximity. We demonstrate performance\nfor the following tasks: bi-arm sorting, bottle opening, and trash disposal\ntasks. These are done zero-shot where the models used have not been trained\nwith any real world data from this bi-arm robot, scenes or workspace.Composing\nboth learning- and non-learning-based components in a modular fashion with\ninterpretable inputs and outputs allows the user to easily debug points of\nfailures and fragilities. One may also in-place swap modules to improve the\nrobustness of the overall platform, for instance with imitation-learned\npolicies.\n", "link": "http://arxiv.org/abs/2404.03570v1", "date": "2024-04-04", "relevancy": 1.7912, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6068}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5973}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5931}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Embodied%20AI%20with%20Two%20Arms%3A%20Zero-shot%20Learning%2C%20Safety%20and%20Modularity&body=Title%3A%20Embodied%20AI%20with%20Two%20Arms%3A%20Zero-shot%20Learning%2C%20Safety%20and%20Modularity%0AAuthor%3A%20Jake%20Varley%20and%20Sumeet%20Singh%20and%20Deepali%20Jain%20and%20Krzysztof%20Choromanski%20and%20Andy%20Zeng%20and%20Somnath%20Basu%20Roy%20Chowdhury%20and%20Avinava%20Dubey%20and%20Vikas%20Sindhwani%0AAbstract%3A%20%20%20We%20present%20an%20embodied%20AI%20system%20which%20receives%20open-ended%20natural%20language%0Ainstructions%20from%20a%20human%2C%20and%20controls%20two%20arms%20to%20collaboratively%20accomplish%0Apotentially%20long-horizon%20tasks%20over%20a%20large%20workspace.%20Our%20system%20is%20modular%3A%0Ait%20deploys%20state%20of%20the%20art%20Large%20Language%20Models%20for%20task%0Aplanning%2CVision-Language%20models%20for%20semantic%20perception%2C%20and%20Point%20Cloud%0Atransformers%20for%20grasping.%20With%20semantic%20and%20physical%20safety%20in%20mind%2C%20these%0Amodules%20are%20interfaced%20with%20a%20real-time%20trajectory%20optimizer%20and%20a%20compliant%0Atracking%20controller%20to%20enable%20human-robot%20proximity.%20We%20demonstrate%20performance%0Afor%20the%20following%20tasks%3A%20bi-arm%20sorting%2C%20bottle%20opening%2C%20and%20trash%20disposal%0Atasks.%20These%20are%20done%20zero-shot%20where%20the%20models%20used%20have%20not%20been%20trained%0Awith%20any%20real%20world%20data%20from%20this%20bi-arm%20robot%2C%20scenes%20or%20workspace.Composing%0Aboth%20learning-%20and%20non-learning-based%20components%20in%20a%20modular%20fashion%20with%0Ainterpretable%20inputs%20and%20outputs%20allows%20the%20user%20to%20easily%20debug%20points%20of%0Afailures%20and%20fragilities.%20One%20may%20also%20in-place%20swap%20modules%20to%20improve%20the%0Arobustness%20of%20the%20overall%20platform%2C%20for%20instance%20with%20imitation-learned%0Apolicies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03570v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20AI%20with%20Two%20Arms%3A%20Zero-shot%20Learning%2C%20Safety%20and%20Modularity&entry.906535625=Jake%20Varley%20and%20Sumeet%20Singh%20and%20Deepali%20Jain%20and%20Krzysztof%20Choromanski%20and%20Andy%20Zeng%20and%20Somnath%20Basu%20Roy%20Chowdhury%20and%20Avinava%20Dubey%20and%20Vikas%20Sindhwani&entry.1292438233=%20%20We%20present%20an%20embodied%20AI%20system%20which%20receives%20open-ended%20natural%20language%0Ainstructions%20from%20a%20human%2C%20and%20controls%20two%20arms%20to%20collaboratively%20accomplish%0Apotentially%20long-horizon%20tasks%20over%20a%20large%20workspace.%20Our%20system%20is%20modular%3A%0Ait%20deploys%20state%20of%20the%20art%20Large%20Language%20Models%20for%20task%0Aplanning%2CVision-Language%20models%20for%20semantic%20perception%2C%20and%20Point%20Cloud%0Atransformers%20for%20grasping.%20With%20semantic%20and%20physical%20safety%20in%20mind%2C%20these%0Amodules%20are%20interfaced%20with%20a%20real-time%20trajectory%20optimizer%20and%20a%20compliant%0Atracking%20controller%20to%20enable%20human-robot%20proximity.%20We%20demonstrate%20performance%0Afor%20the%20following%20tasks%3A%20bi-arm%20sorting%2C%20bottle%20opening%2C%20and%20trash%20disposal%0Atasks.%20These%20are%20done%20zero-shot%20where%20the%20models%20used%20have%20not%20been%20trained%0Awith%20any%20real%20world%20data%20from%20this%20bi-arm%20robot%2C%20scenes%20or%20workspace.Composing%0Aboth%20learning-%20and%20non-learning-based%20components%20in%20a%20modular%20fashion%20with%0Ainterpretable%20inputs%20and%20outputs%20allows%20the%20user%20to%20easily%20debug%20points%20of%0Afailures%20and%20fragilities.%20One%20may%20also%20in-place%20swap%20modules%20to%20improve%20the%0Arobustness%20of%20the%20overall%20platform%2C%20for%20instance%20with%20imitation-learned%0Apolicies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03570v1&entry.124074799=Read"},
{"title": "WorDepth: Variational Language Prior for Monocular Depth Estimation", "author": "Ziyao Zeng and Daniel Wang and Fengyu Yang and Hyoungseob Park and Yangchao Wu and Stefano Soatto and Byung-Woo Hong and Dong Lao and Alex Wong", "abstract": "  Three-dimensional (3D) reconstruction from a single image is an ill-posed\nproblem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text\ndescription(s) is similarly ill-posed, i.e. spatial arrangements of objects\ndescribed. We investigate the question of whether two inherently ambiguous\nmodalities can be used in conjunction to produce metric-scaled reconstructions.\nTo test this, we focus on monocular depth estimation, the problem of predicting\na dense depth map from a single image, but with an additional text caption\ndescribing the scene. To this end, we begin by encoding the text caption as a\nmean and standard deviation; using a variational framework, we learn the\ndistribution of the plausible metric reconstructions of 3D scenes corresponding\nto the text captions as a prior. To \"select\" a specific reconstruction or depth\nmap, we encode the given image through a conditional sampler that samples from\nthe latent space of the variational text encoder, which is then decoded to the\noutput depth map. Our approach is trained alternatingly between the text and\nimage branches: in one optimization step, we predict the mean and standard\ndeviation from the text description and sample from a standard Gaussian, and in\nthe other, we sample using a (image) conditional sampler. Once trained, we\ndirectly predict depth from the encoded text using the conditional sampler. We\ndemonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where\nwe show that language can consistently improve performance in both.\n", "link": "http://arxiv.org/abs/2404.03635v1", "date": "2024-04-04", "relevancy": 1.7833, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5916}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5856}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WorDepth%3A%20Variational%20Language%20Prior%20for%20Monocular%20Depth%20Estimation&body=Title%3A%20WorDepth%3A%20Variational%20Language%20Prior%20for%20Monocular%20Depth%20Estimation%0AAuthor%3A%20Ziyao%20Zeng%20and%20Daniel%20Wang%20and%20Fengyu%20Yang%20and%20Hyoungseob%20Park%20and%20Yangchao%20Wu%20and%20Stefano%20Soatto%20and%20Byung-Woo%20Hong%20and%20Dong%20Lao%20and%20Alex%20Wong%0AAbstract%3A%20%20%20Three-dimensional%20%283D%29%20reconstruction%20from%20a%20single%20image%20is%20an%20ill-posed%0Aproblem%20with%20inherent%20ambiguities%2C%20i.e.%20scale.%20Predicting%20a%203D%20scene%20from%20text%0Adescription%28s%29%20is%20similarly%20ill-posed%2C%20i.e.%20spatial%20arrangements%20of%20objects%0Adescribed.%20We%20investigate%20the%20question%20of%20whether%20two%20inherently%20ambiguous%0Amodalities%20can%20be%20used%20in%20conjunction%20to%20produce%20metric-scaled%20reconstructions.%0ATo%20test%20this%2C%20we%20focus%20on%20monocular%20depth%20estimation%2C%20the%20problem%20of%20predicting%0Aa%20dense%20depth%20map%20from%20a%20single%20image%2C%20but%20with%20an%20additional%20text%20caption%0Adescribing%20the%20scene.%20To%20this%20end%2C%20we%20begin%20by%20encoding%20the%20text%20caption%20as%20a%0Amean%20and%20standard%20deviation%3B%20using%20a%20variational%20framework%2C%20we%20learn%20the%0Adistribution%20of%20the%20plausible%20metric%20reconstructions%20of%203D%20scenes%20corresponding%0Ato%20the%20text%20captions%20as%20a%20prior.%20To%20%22select%22%20a%20specific%20reconstruction%20or%20depth%0Amap%2C%20we%20encode%20the%20given%20image%20through%20a%20conditional%20sampler%20that%20samples%20from%0Athe%20latent%20space%20of%20the%20variational%20text%20encoder%2C%20which%20is%20then%20decoded%20to%20the%0Aoutput%20depth%20map.%20Our%20approach%20is%20trained%20alternatingly%20between%20the%20text%20and%0Aimage%20branches%3A%20in%20one%20optimization%20step%2C%20we%20predict%20the%20mean%20and%20standard%0Adeviation%20from%20the%20text%20description%20and%20sample%20from%20a%20standard%20Gaussian%2C%20and%20in%0Athe%20other%2C%20we%20sample%20using%20a%20%28image%29%20conditional%20sampler.%20Once%20trained%2C%20we%0Adirectly%20predict%20depth%20from%20the%20encoded%20text%20using%20the%20conditional%20sampler.%20We%0Ademonstrate%20our%20approach%20on%20indoor%20%28NYUv2%29%20and%20outdoor%20%28KITTI%29%20scenarios%2C%20where%0Awe%20show%20that%20language%20can%20consistently%20improve%20performance%20in%20both.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03635v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorDepth%3A%20Variational%20Language%20Prior%20for%20Monocular%20Depth%20Estimation&entry.906535625=Ziyao%20Zeng%20and%20Daniel%20Wang%20and%20Fengyu%20Yang%20and%20Hyoungseob%20Park%20and%20Yangchao%20Wu%20and%20Stefano%20Soatto%20and%20Byung-Woo%20Hong%20and%20Dong%20Lao%20and%20Alex%20Wong&entry.1292438233=%20%20Three-dimensional%20%283D%29%20reconstruction%20from%20a%20single%20image%20is%20an%20ill-posed%0Aproblem%20with%20inherent%20ambiguities%2C%20i.e.%20scale.%20Predicting%20a%203D%20scene%20from%20text%0Adescription%28s%29%20is%20similarly%20ill-posed%2C%20i.e.%20spatial%20arrangements%20of%20objects%0Adescribed.%20We%20investigate%20the%20question%20of%20whether%20two%20inherently%20ambiguous%0Amodalities%20can%20be%20used%20in%20conjunction%20to%20produce%20metric-scaled%20reconstructions.%0ATo%20test%20this%2C%20we%20focus%20on%20monocular%20depth%20estimation%2C%20the%20problem%20of%20predicting%0Aa%20dense%20depth%20map%20from%20a%20single%20image%2C%20but%20with%20an%20additional%20text%20caption%0Adescribing%20the%20scene.%20To%20this%20end%2C%20we%20begin%20by%20encoding%20the%20text%20caption%20as%20a%0Amean%20and%20standard%20deviation%3B%20using%20a%20variational%20framework%2C%20we%20learn%20the%0Adistribution%20of%20the%20plausible%20metric%20reconstructions%20of%203D%20scenes%20corresponding%0Ato%20the%20text%20captions%20as%20a%20prior.%20To%20%22select%22%20a%20specific%20reconstruction%20or%20depth%0Amap%2C%20we%20encode%20the%20given%20image%20through%20a%20conditional%20sampler%20that%20samples%20from%0Athe%20latent%20space%20of%20the%20variational%20text%20encoder%2C%20which%20is%20then%20decoded%20to%20the%0Aoutput%20depth%20map.%20Our%20approach%20is%20trained%20alternatingly%20between%20the%20text%20and%0Aimage%20branches%3A%20in%20one%20optimization%20step%2C%20we%20predict%20the%20mean%20and%20standard%0Adeviation%20from%20the%20text%20description%20and%20sample%20from%20a%20standard%20Gaussian%2C%20and%20in%0Athe%20other%2C%20we%20sample%20using%20a%20%28image%29%20conditional%20sampler.%20Once%20trained%2C%20we%0Adirectly%20predict%20depth%20from%20the%20encoded%20text%20using%20the%20conditional%20sampler.%20We%0Ademonstrate%20our%20approach%20on%20indoor%20%28NYUv2%29%20and%20outdoor%20%28KITTI%29%20scenarios%2C%20where%0Awe%20show%20that%20language%20can%20consistently%20improve%20performance%20in%20both.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03635v1&entry.124074799=Read"},
{"title": "CMB: A Comprehensive Medical Benchmark in Chinese", "author": "Xidong Wang and Guiming Hardy Chen and Dingjie Song and Zhiyi Zhang and Zhihong Chen and Qingying Xiao and Feng Jiang and Jianquan Li and Xiang Wan and Benyou Wang and Haizhou Li", "abstract": "  Large Language Models (LLMs) provide a possibility to make a great\nbreakthrough in medicine. The establishment of a standardized medical benchmark\nbecomes a fundamental cornerstone to measure progression. However, medical\nenvironments in different regions have their local characteristics, e.g., the\nubiquity and significance of traditional Chinese medicine within China.\nTherefore, merely translating English-based medical evaluation may result in\n\\textit{contextual incongruities} to a local region. To solve the issue, we\npropose a localized medical benchmark called CMB, a Comprehensive Medical\nBenchmark in Chinese, designed and rooted entirely within the native Chinese\nlinguistic and cultural framework. While traditional Chinese medicine is\nintegral to this evaluation, it does not constitute its entirety. Using this\nbenchmark, we have evaluated several prominent large-scale LLMs, including\nChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical\ndomain. We hope this benchmark provide first-hand experience in existing LLMs\nfor medicine and also facilitate the widespread adoption and enhancement of\nmedical LLMs within China. Our data and code are publicly available at\nhttps://github.com/FreedomIntelligence/CMB.\n", "link": "http://arxiv.org/abs/2308.08833v2", "date": "2024-04-04", "relevancy": 1.749, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4598}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4358}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4297}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CMB%3A%20A%20Comprehensive%20Medical%20Benchmark%20in%20Chinese&body=Title%3A%20CMB%3A%20A%20Comprehensive%20Medical%20Benchmark%20in%20Chinese%0AAuthor%3A%20Xidong%20Wang%20and%20Guiming%20Hardy%20Chen%20and%20Dingjie%20Song%20and%20Zhiyi%20Zhang%20and%20Zhihong%20Chen%20and%20Qingying%20Xiao%20and%20Feng%20Jiang%20and%20Jianquan%20Li%20and%20Xiang%20Wan%20and%20Benyou%20Wang%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20provide%20a%20possibility%20to%20make%20a%20great%0Abreakthrough%20in%20medicine.%20The%20establishment%20of%20a%20standardized%20medical%20benchmark%0Abecomes%20a%20fundamental%20cornerstone%20to%20measure%20progression.%20However%2C%20medical%0Aenvironments%20in%20different%20regions%20have%20their%20local%20characteristics%2C%20e.g.%2C%20the%0Aubiquity%20and%20significance%20of%20traditional%20Chinese%20medicine%20within%20China.%0ATherefore%2C%20merely%20translating%20English-based%20medical%20evaluation%20may%20result%20in%0A%5Ctextit%7Bcontextual%20incongruities%7D%20to%20a%20local%20region.%20To%20solve%20the%20issue%2C%20we%0Apropose%20a%20localized%20medical%20benchmark%20called%20CMB%2C%20a%20Comprehensive%20Medical%0ABenchmark%20in%20Chinese%2C%20designed%20and%20rooted%20entirely%20within%20the%20native%20Chinese%0Alinguistic%20and%20cultural%20framework.%20While%20traditional%20Chinese%20medicine%20is%0Aintegral%20to%20this%20evaluation%2C%20it%20does%20not%20constitute%20its%20entirety.%20Using%20this%0Abenchmark%2C%20we%20have%20evaluated%20several%20prominent%20large-scale%20LLMs%2C%20including%0AChatGPT%2C%20GPT-4%2C%20dedicated%20Chinese%20LLMs%2C%20and%20LLMs%20specialized%20in%20the%20medical%0Adomain.%20We%20hope%20this%20benchmark%20provide%20first-hand%20experience%20in%20existing%20LLMs%0Afor%20medicine%20and%20also%20facilitate%20the%20widespread%20adoption%20and%20enhancement%20of%0Amedical%20LLMs%20within%20China.%20Our%20data%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//github.com/FreedomIntelligence/CMB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08833v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMB%3A%20A%20Comprehensive%20Medical%20Benchmark%20in%20Chinese&entry.906535625=Xidong%20Wang%20and%20Guiming%20Hardy%20Chen%20and%20Dingjie%20Song%20and%20Zhiyi%20Zhang%20and%20Zhihong%20Chen%20and%20Qingying%20Xiao%20and%20Feng%20Jiang%20and%20Jianquan%20Li%20and%20Xiang%20Wan%20and%20Benyou%20Wang%20and%20Haizhou%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20provide%20a%20possibility%20to%20make%20a%20great%0Abreakthrough%20in%20medicine.%20The%20establishment%20of%20a%20standardized%20medical%20benchmark%0Abecomes%20a%20fundamental%20cornerstone%20to%20measure%20progression.%20However%2C%20medical%0Aenvironments%20in%20different%20regions%20have%20their%20local%20characteristics%2C%20e.g.%2C%20the%0Aubiquity%20and%20significance%20of%20traditional%20Chinese%20medicine%20within%20China.%0ATherefore%2C%20merely%20translating%20English-based%20medical%20evaluation%20may%20result%20in%0A%5Ctextit%7Bcontextual%20incongruities%7D%20to%20a%20local%20region.%20To%20solve%20the%20issue%2C%20we%0Apropose%20a%20localized%20medical%20benchmark%20called%20CMB%2C%20a%20Comprehensive%20Medical%0ABenchmark%20in%20Chinese%2C%20designed%20and%20rooted%20entirely%20within%20the%20native%20Chinese%0Alinguistic%20and%20cultural%20framework.%20While%20traditional%20Chinese%20medicine%20is%0Aintegral%20to%20this%20evaluation%2C%20it%20does%20not%20constitute%20its%20entirety.%20Using%20this%0Abenchmark%2C%20we%20have%20evaluated%20several%20prominent%20large-scale%20LLMs%2C%20including%0AChatGPT%2C%20GPT-4%2C%20dedicated%20Chinese%20LLMs%2C%20and%20LLMs%20specialized%20in%20the%20medical%0Adomain.%20We%20hope%20this%20benchmark%20provide%20first-hand%20experience%20in%20existing%20LLMs%0Afor%20medicine%20and%20also%20facilitate%20the%20widespread%20adoption%20and%20enhancement%20of%0Amedical%20LLMs%20within%20China.%20Our%20data%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//github.com/FreedomIntelligence/CMB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08833v2&entry.124074799=Read"},
{"title": "Using construction waste hauling trucks' GPS data to classify\n  earthwork-related locations: A Chengdu case study", "author": "Lei Yu and Ke Han", "abstract": "  Earthwork-related locations (ERLs), such as construction sites, earth dumping\nground, and concrete mixing stations, are major sources of urban dust pollution\n(particulate matters). The effective management of ERLs is crucial and requires\ntimely and efficient tracking of these locations throughout the city. This work\naims to identify and classify urban ERLs using GPS trajectory data of over\n16,000 construction waste hauling trucks (CWHTs), as well as 58 urban features\nencompassing geographic, land cover, POI and transport dimensions. We compare\nseveral machine learning models and examine the impact of various\nspatial-temporal features on classification performance using real-world data\nin Chengdu, China. The results demonstrate that 77.8% classification accuracy\ncan be achieved with a limited number of features. This classification\nframework was implemented in the Alpha MAPS system in Chengdu, which has\nsuccessfully identified 724 construction cites/earth dumping ground, 48\nconcrete mixing stations, and 80 truck parking locations in the city during\nDecember 2023, which has enabled local authority to effectively manage urban\ndust pollution at low personnel costs.\n", "link": "http://arxiv.org/abs/2402.14698v3", "date": "2024-04-04", "relevancy": 1.7273, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4512}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4231}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4053}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20construction%20waste%20hauling%20trucks%27%20GPS%20data%20to%20classify%0A%20%20earthwork-related%20locations%3A%20A%20Chengdu%20case%20study&body=Title%3A%20Using%20construction%20waste%20hauling%20trucks%27%20GPS%20data%20to%20classify%0A%20%20earthwork-related%20locations%3A%20A%20Chengdu%20case%20study%0AAuthor%3A%20Lei%20Yu%20and%20Ke%20Han%0AAbstract%3A%20%20%20Earthwork-related%20locations%20%28ERLs%29%2C%20such%20as%20construction%20sites%2C%20earth%20dumping%0Aground%2C%20and%20concrete%20mixing%20stations%2C%20are%20major%20sources%20of%20urban%20dust%20pollution%0A%28particulate%20matters%29.%20The%20effective%20management%20of%20ERLs%20is%20crucial%20and%20requires%0Atimely%20and%20efficient%20tracking%20of%20these%20locations%20throughout%20the%20city.%20This%20work%0Aaims%20to%20identify%20and%20classify%20urban%20ERLs%20using%20GPS%20trajectory%20data%20of%20over%0A16%2C000%20construction%20waste%20hauling%20trucks%20%28CWHTs%29%2C%20as%20well%20as%2058%20urban%20features%0Aencompassing%20geographic%2C%20land%20cover%2C%20POI%20and%20transport%20dimensions.%20We%20compare%0Aseveral%20machine%20learning%20models%20and%20examine%20the%20impact%20of%20various%0Aspatial-temporal%20features%20on%20classification%20performance%20using%20real-world%20data%0Ain%20Chengdu%2C%20China.%20The%20results%20demonstrate%20that%2077.8%25%20classification%20accuracy%0Acan%20be%20achieved%20with%20a%20limited%20number%20of%20features.%20This%20classification%0Aframework%20was%20implemented%20in%20the%20Alpha%20MAPS%20system%20in%20Chengdu%2C%20which%20has%0Asuccessfully%20identified%20724%20construction%20cites/earth%20dumping%20ground%2C%2048%0Aconcrete%20mixing%20stations%2C%20and%2080%20truck%20parking%20locations%20in%20the%20city%20during%0ADecember%202023%2C%20which%20has%20enabled%20local%20authority%20to%20effectively%20manage%20urban%0Adust%20pollution%20at%20low%20personnel%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14698v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20construction%20waste%20hauling%20trucks%27%20GPS%20data%20to%20classify%0A%20%20earthwork-related%20locations%3A%20A%20Chengdu%20case%20study&entry.906535625=Lei%20Yu%20and%20Ke%20Han&entry.1292438233=%20%20Earthwork-related%20locations%20%28ERLs%29%2C%20such%20as%20construction%20sites%2C%20earth%20dumping%0Aground%2C%20and%20concrete%20mixing%20stations%2C%20are%20major%20sources%20of%20urban%20dust%20pollution%0A%28particulate%20matters%29.%20The%20effective%20management%20of%20ERLs%20is%20crucial%20and%20requires%0Atimely%20and%20efficient%20tracking%20of%20these%20locations%20throughout%20the%20city.%20This%20work%0Aaims%20to%20identify%20and%20classify%20urban%20ERLs%20using%20GPS%20trajectory%20data%20of%20over%0A16%2C000%20construction%20waste%20hauling%20trucks%20%28CWHTs%29%2C%20as%20well%20as%2058%20urban%20features%0Aencompassing%20geographic%2C%20land%20cover%2C%20POI%20and%20transport%20dimensions.%20We%20compare%0Aseveral%20machine%20learning%20models%20and%20examine%20the%20impact%20of%20various%0Aspatial-temporal%20features%20on%20classification%20performance%20using%20real-world%20data%0Ain%20Chengdu%2C%20China.%20The%20results%20demonstrate%20that%2077.8%25%20classification%20accuracy%0Acan%20be%20achieved%20with%20a%20limited%20number%20of%20features.%20This%20classification%0Aframework%20was%20implemented%20in%20the%20Alpha%20MAPS%20system%20in%20Chengdu%2C%20which%20has%0Asuccessfully%20identified%20724%20construction%20cites/earth%20dumping%20ground%2C%2048%0Aconcrete%20mixing%20stations%2C%20and%2080%20truck%20parking%20locations%20in%20the%20city%20during%0ADecember%202023%2C%20which%20has%20enabled%20local%20authority%20to%20effectively%20manage%20urban%0Adust%20pollution%20at%20low%20personnel%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14698v3&entry.124074799=Read"},
{"title": "Permissible Knowledge Pooling", "author": "Huimin Dong", "abstract": "  Information pooling has been extensively formalised across various logical\nframeworks in distributed systems, characterized by diverse information-sharing\npatterns. These approaches generally adopt an intersection perspective,\naggregating all possible information, regardless of whether it is known or\nunknown to the agents. In contrast, this work adopts a unique stance,\nemphasising that sharing knowledge means distributing what is known, rather\nthan what remains uncertain. This paper introduces a dynamic logic for\nknowledge pooling or sharing and further discusses a potential framework for\npermissible knowledge pooling.\n", "link": "http://arxiv.org/abs/2404.03418v1", "date": "2024-04-04", "relevancy": 1.7114, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4148}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4147}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Permissible%20Knowledge%20Pooling&body=Title%3A%20Permissible%20Knowledge%20Pooling%0AAuthor%3A%20Huimin%20Dong%0AAbstract%3A%20%20%20Information%20pooling%20has%20been%20extensively%20formalised%20across%20various%20logical%0Aframeworks%20in%20distributed%20systems%2C%20characterized%20by%20diverse%20information-sharing%0Apatterns.%20These%20approaches%20generally%20adopt%20an%20intersection%20perspective%2C%0Aaggregating%20all%20possible%20information%2C%20regardless%20of%20whether%20it%20is%20known%20or%0Aunknown%20to%20the%20agents.%20In%20contrast%2C%20this%20work%20adopts%20a%20unique%20stance%2C%0Aemphasising%20that%20sharing%20knowledge%20means%20distributing%20what%20is%20known%2C%20rather%0Athan%20what%20remains%20uncertain.%20This%20paper%20introduces%20a%20dynamic%20logic%20for%0Aknowledge%20pooling%20or%20sharing%20and%20further%20discusses%20a%20potential%20framework%20for%0Apermissible%20knowledge%20pooling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03418v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Permissible%20Knowledge%20Pooling&entry.906535625=Huimin%20Dong&entry.1292438233=%20%20Information%20pooling%20has%20been%20extensively%20formalised%20across%20various%20logical%0Aframeworks%20in%20distributed%20systems%2C%20characterized%20by%20diverse%20information-sharing%0Apatterns.%20These%20approaches%20generally%20adopt%20an%20intersection%20perspective%2C%0Aaggregating%20all%20possible%20information%2C%20regardless%20of%20whether%20it%20is%20known%20or%0Aunknown%20to%20the%20agents.%20In%20contrast%2C%20this%20work%20adopts%20a%20unique%20stance%2C%0Aemphasising%20that%20sharing%20knowledge%20means%20distributing%20what%20is%20known%2C%20rather%0Athan%20what%20remains%20uncertain.%20This%20paper%20introduces%20a%20dynamic%20logic%20for%0Aknowledge%20pooling%20or%20sharing%20and%20further%20discusses%20a%20potential%20framework%20for%0Apermissible%20knowledge%20pooling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03418v1&entry.124074799=Read"},
{"title": "Distributionally Robust Reinforcement Learning with Interactive Data\n  Collection: Fundamental Hardness and Near-Optimal Algorithm", "author": "Miao Lu and Han Zhong and Tong Zhang and Jose Blanchet", "abstract": "  The sim-to-real gap, which represents the disparity between training and\ntesting environments, poses a significant challenge in reinforcement learning\n(RL). A promising approach to addressing this challenge is distributionally\nrobust RL, often framed as a robust Markov decision process (RMDP). In this\nframework, the objective is to find a robust policy that achieves good\nperformance under the worst-case scenario among all environments within a\npre-specified uncertainty set centered around the training environment. Unlike\nprevious work, which relies on a generative model or a pre-collected offline\ndataset enjoying good coverage of the deployment environment, we tackle robust\nRL via interactive data collection, where the learner interacts with the\ntraining environment only and refines the policy through trial and error. In\nthis robust RL paradigm, two main challenges emerge: managing distributional\nrobustness while striking a balance between exploration and exploitation during\ndata collection. Initially, we establish that sample-efficient learning without\nadditional assumptions is unattainable owing to the curse of support shift;\ni.e., the potential disjointedness of the distributional supports between the\ntraining and testing environments. To circumvent such a hardness result, we\nintroduce the vanishing minimal value assumption to RMDPs with a\ntotal-variation (TV) distance robust set, postulating that the minimal value of\nthe optimal robust value function is zero. We prove that such an assumption\neffectively eliminates the support shift issue for RMDPs with a TV distance\nrobust set, and present an algorithm with a provable sample complexity\nguarantee. Our work makes the initial step to uncovering the inherent\ndifficulty of robust RL via interactive data collection and sufficient\nconditions for designing a sample-efficient algorithm accompanied by sharp\nsample complexity analysis.\n", "link": "http://arxiv.org/abs/2404.03578v1", "date": "2024-04-04", "relevancy": 1.0192, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5257}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4864}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Distributionally%20Robust%20Reinforcement%20Learning%20with%20Interactive%20Data%0A%20%20Collection%3A%20Fundamental%20Hardness%20and%20Near-Optimal%20Algorithm&body=Title%3A%20Distributionally%20Robust%20Reinforcement%20Learning%20with%20Interactive%20Data%0A%20%20Collection%3A%20Fundamental%20Hardness%20and%20Near-Optimal%20Algorithm%0AAuthor%3A%20Miao%20Lu%20and%20Han%20Zhong%20and%20Tong%20Zhang%20and%20Jose%20Blanchet%0AAbstract%3A%20%20%20The%20sim-to-real%20gap%2C%20which%20represents%20the%20disparity%20between%20training%20and%0Atesting%20environments%2C%20poses%20a%20significant%20challenge%20in%20reinforcement%20learning%0A%28RL%29.%20A%20promising%20approach%20to%20addressing%20this%20challenge%20is%20distributionally%0Arobust%20RL%2C%20often%20framed%20as%20a%20robust%20Markov%20decision%20process%20%28RMDP%29.%20In%20this%0Aframework%2C%20the%20objective%20is%20to%20find%20a%20robust%20policy%20that%20achieves%20good%0Aperformance%20under%20the%20worst-case%20scenario%20among%20all%20environments%20within%20a%0Apre-specified%20uncertainty%20set%20centered%20around%20the%20training%20environment.%20Unlike%0Aprevious%20work%2C%20which%20relies%20on%20a%20generative%20model%20or%20a%20pre-collected%20offline%0Adataset%20enjoying%20good%20coverage%20of%20the%20deployment%20environment%2C%20we%20tackle%20robust%0ARL%20via%20interactive%20data%20collection%2C%20where%20the%20learner%20interacts%20with%20the%0Atraining%20environment%20only%20and%20refines%20the%20policy%20through%20trial%20and%20error.%20In%0Athis%20robust%20RL%20paradigm%2C%20two%20main%20challenges%20emerge%3A%20managing%20distributional%0Arobustness%20while%20striking%20a%20balance%20between%20exploration%20and%20exploitation%20during%0Adata%20collection.%20Initially%2C%20we%20establish%20that%20sample-efficient%20learning%20without%0Aadditional%20assumptions%20is%20unattainable%20owing%20to%20the%20curse%20of%20support%20shift%3B%0Ai.e.%2C%20the%20potential%20disjointedness%20of%20the%20distributional%20supports%20between%20the%0Atraining%20and%20testing%20environments.%20To%20circumvent%20such%20a%20hardness%20result%2C%20we%0Aintroduce%20the%20vanishing%20minimal%20value%20assumption%20to%20RMDPs%20with%20a%0Atotal-variation%20%28TV%29%20distance%20robust%20set%2C%20postulating%20that%20the%20minimal%20value%20of%0Athe%20optimal%20robust%20value%20function%20is%20zero.%20We%20prove%20that%20such%20an%20assumption%0Aeffectively%20eliminates%20the%20support%20shift%20issue%20for%20RMDPs%20with%20a%20TV%20distance%0Arobust%20set%2C%20and%20present%20an%20algorithm%20with%20a%20provable%20sample%20complexity%0Aguarantee.%20Our%20work%20makes%20the%20initial%20step%20to%20uncovering%20the%20inherent%0Adifficulty%20of%20robust%20RL%20via%20interactive%20data%20collection%20and%20sufficient%0Aconditions%20for%20designing%20a%20sample-efficient%20algorithm%20accompanied%20by%20sharp%0Asample%20complexity%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03578v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributionally%20Robust%20Reinforcement%20Learning%20with%20Interactive%20Data%0A%20%20Collection%3A%20Fundamental%20Hardness%20and%20Near-Optimal%20Algorithm&entry.906535625=Miao%20Lu%20and%20Han%20Zhong%20and%20Tong%20Zhang%20and%20Jose%20Blanchet&entry.1292438233=%20%20The%20sim-to-real%20gap%2C%20which%20represents%20the%20disparity%20between%20training%20and%0Atesting%20environments%2C%20poses%20a%20significant%20challenge%20in%20reinforcement%20learning%0A%28RL%29.%20A%20promising%20approach%20to%20addressing%20this%20challenge%20is%20distributionally%0Arobust%20RL%2C%20often%20framed%20as%20a%20robust%20Markov%20decision%20process%20%28RMDP%29.%20In%20this%0Aframework%2C%20the%20objective%20is%20to%20find%20a%20robust%20policy%20that%20achieves%20good%0Aperformance%20under%20the%20worst-case%20scenario%20among%20all%20environments%20within%20a%0Apre-specified%20uncertainty%20set%20centered%20around%20the%20training%20environment.%20Unlike%0Aprevious%20work%2C%20which%20relies%20on%20a%20generative%20model%20or%20a%20pre-collected%20offline%0Adataset%20enjoying%20good%20coverage%20of%20the%20deployment%20environment%2C%20we%20tackle%20robust%0ARL%20via%20interactive%20data%20collection%2C%20where%20the%20learner%20interacts%20with%20the%0Atraining%20environment%20only%20and%20refines%20the%20policy%20through%20trial%20and%20error.%20In%0Athis%20robust%20RL%20paradigm%2C%20two%20main%20challenges%20emerge%3A%20managing%20distributional%0Arobustness%20while%20striking%20a%20balance%20between%20exploration%20and%20exploitation%20during%0Adata%20collection.%20Initially%2C%20we%20establish%20that%20sample-efficient%20learning%20without%0Aadditional%20assumptions%20is%20unattainable%20owing%20to%20the%20curse%20of%20support%20shift%3B%0Ai.e.%2C%20the%20potential%20disjointedness%20of%20the%20distributional%20supports%20between%20the%0Atraining%20and%20testing%20environments.%20To%20circumvent%20such%20a%20hardness%20result%2C%20we%0Aintroduce%20the%20vanishing%20minimal%20value%20assumption%20to%20RMDPs%20with%20a%0Atotal-variation%20%28TV%29%20distance%20robust%20set%2C%20postulating%20that%20the%20minimal%20value%20of%0Athe%20optimal%20robust%20value%20function%20is%20zero.%20We%20prove%20that%20such%20an%20assumption%0Aeffectively%20eliminates%20the%20support%20shift%20issue%20for%20RMDPs%20with%20a%20TV%20distance%0Arobust%20set%2C%20and%20present%20an%20algorithm%20with%20a%20provable%20sample%20complexity%0Aguarantee.%20Our%20work%20makes%20the%20initial%20step%20to%20uncovering%20the%20inherent%0Adifficulty%20of%20robust%20RL%20via%20interactive%20data%20collection%20and%20sufficient%0Aconditions%20for%20designing%20a%20sample-efficient%20algorithm%20accompanied%20by%20sharp%0Asample%20complexity%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03578v1&entry.124074799=Read"},
{"title": "ROBUST: 221 Bugs in the Robot Operating System", "author": "Christopher S. Timperley and Gijs van der Hoorn and Andr\u00e9 Santos and Harshavardhan Deshpande and Andrzej W\u0105sowski", "abstract": "  As robotic systems such as autonomous cars and delivery drones assume greater\nroles and responsibilities within society, the likelihood and impact of\ncatastrophic software failure within those systems is increased.To aid\nresearchers in the development of new methods to measure and assure the safety\nand quality of robotics software, we systematically curated a dataset of 221\nbugs across 7 popular and diverse software systems implemented via the Robot\nOperating System (ROS). We produce historically accurate recreations of each of\nthe 221 defective software versions in the form of Docker images, and use a\ngrounded theory approach to examine and categorize their corresponding faults,\nfailures, and fixes. Finally, we reflect on the implications of our findings\nand outline future research directions for the community.\n", "link": "http://arxiv.org/abs/2404.03629v1", "date": "2024-04-04", "relevancy": 1.3988, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5078}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4875}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4411}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ROBUST%3A%20221%20Bugs%20in%20the%20Robot%20Operating%20System&body=Title%3A%20ROBUST%3A%20221%20Bugs%20in%20the%20Robot%20Operating%20System%0AAuthor%3A%20Christopher%20S.%20Timperley%20and%20Gijs%20van%20der%20Hoorn%20and%20Andr%C3%A9%20Santos%20and%20Harshavardhan%20Deshpande%20and%20Andrzej%20W%C4%85sowski%0AAbstract%3A%20%20%20As%20robotic%20systems%20such%20as%20autonomous%20cars%20and%20delivery%20drones%20assume%20greater%0Aroles%20and%20responsibilities%20within%20society%2C%20the%20likelihood%20and%20impact%20of%0Acatastrophic%20software%20failure%20within%20those%20systems%20is%20increased.To%20aid%0Aresearchers%20in%20the%20development%20of%20new%20methods%20to%20measure%20and%20assure%20the%20safety%0Aand%20quality%20of%20robotics%20software%2C%20we%20systematically%20curated%20a%20dataset%20of%20221%0Abugs%20across%207%20popular%20and%20diverse%20software%20systems%20implemented%20via%20the%20Robot%0AOperating%20System%20%28ROS%29.%20We%20produce%20historically%20accurate%20recreations%20of%20each%20of%0Athe%20221%20defective%20software%20versions%20in%20the%20form%20of%20Docker%20images%2C%20and%20use%20a%0Agrounded%20theory%20approach%20to%20examine%20and%20categorize%20their%20corresponding%20faults%2C%0Afailures%2C%20and%20fixes.%20Finally%2C%20we%20reflect%20on%20the%20implications%20of%20our%20findings%0Aand%20outline%20future%20research%20directions%20for%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03629v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROBUST%3A%20221%20Bugs%20in%20the%20Robot%20Operating%20System&entry.906535625=Christopher%20S.%20Timperley%20and%20Gijs%20van%20der%20Hoorn%20and%20Andr%C3%A9%20Santos%20and%20Harshavardhan%20Deshpande%20and%20Andrzej%20W%C4%85sowski&entry.1292438233=%20%20As%20robotic%20systems%20such%20as%20autonomous%20cars%20and%20delivery%20drones%20assume%20greater%0Aroles%20and%20responsibilities%20within%20society%2C%20the%20likelihood%20and%20impact%20of%0Acatastrophic%20software%20failure%20within%20those%20systems%20is%20increased.To%20aid%0Aresearchers%20in%20the%20development%20of%20new%20methods%20to%20measure%20and%20assure%20the%20safety%0Aand%20quality%20of%20robotics%20software%2C%20we%20systematically%20curated%20a%20dataset%20of%20221%0Abugs%20across%207%20popular%20and%20diverse%20software%20systems%20implemented%20via%20the%20Robot%0AOperating%20System%20%28ROS%29.%20We%20produce%20historically%20accurate%20recreations%20of%20each%20of%0Athe%20221%20defective%20software%20versions%20in%20the%20form%20of%20Docker%20images%2C%20and%20use%20a%0Agrounded%20theory%20approach%20to%20examine%20and%20categorize%20their%20corresponding%20faults%2C%0Afailures%2C%20and%20fixes.%20Finally%2C%20we%20reflect%20on%20the%20implications%20of%20our%20findings%0Aand%20outline%20future%20research%20directions%20for%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03629v1&entry.124074799=Read"},
{"title": "Sailor: Open Language Models for South-East Asia", "author": "Longxu Dou and Qian Liu and Guangtao Zeng and Jia Guo and Jiahui Zhou and Wei Lu and Min Lin", "abstract": "  We present Sailor, a family of open language models ranging from 0.5B to 7B\nparameters, tailored for South-East Asian (SEA) languages. These models are\ncontinually pre-trained from Qwen1.5, a great language model for multilingual\nuse cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily\ncovering the languages of English, Chinese, Vietnamese, Thai, Indonesian,\nMalay, and Lao. The training leverages several techniques, including BPE\ndropout for improving the model robustness, aggressive data cleaning and\ndeduplication, and small proxy models to optimize data mixture. Experimental\nresults on four typical tasks indicate that Sailor models demonstrate strong\nperformance across different benchmarks, including commonsense reasoning,\nquestion answering, reading comprehension and examination. Embracing the\nopen-source spirit, we share our insights through this report to spark a wider\ninterest in developing large language models for multilingual use cases.\n", "link": "http://arxiv.org/abs/2404.03608v1", "date": "2024-04-04", "relevancy": 1.6385, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4337}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3893}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sailor%3A%20Open%20Language%20Models%20for%20South-East%20Asia&body=Title%3A%20Sailor%3A%20Open%20Language%20Models%20for%20South-East%20Asia%0AAuthor%3A%20Longxu%20Dou%20and%20Qian%20Liu%20and%20Guangtao%20Zeng%20and%20Jia%20Guo%20and%20Jiahui%20Zhou%20and%20Wei%20Lu%20and%20Min%20Lin%0AAbstract%3A%20%20%20We%20present%20Sailor%2C%20a%20family%20of%20open%20language%20models%20ranging%20from%200.5B%20to%207B%0Aparameters%2C%20tailored%20for%20South-East%20Asian%20%28SEA%29%20languages.%20These%20models%20are%0Acontinually%20pre-trained%20from%20Qwen1.5%2C%20a%20great%20language%20model%20for%20multilingual%0Ause%20cases.%20From%20Qwen1.5%2C%20Sailor%20models%20accept%20200B%20to%20400B%20tokens%2C%20primarily%0Acovering%20the%20languages%20of%20English%2C%20Chinese%2C%20Vietnamese%2C%20Thai%2C%20Indonesian%2C%0AMalay%2C%20and%20Lao.%20The%20training%20leverages%20several%20techniques%2C%20including%20BPE%0Adropout%20for%20improving%20the%20model%20robustness%2C%20aggressive%20data%20cleaning%20and%0Adeduplication%2C%20and%20small%20proxy%20models%20to%20optimize%20data%20mixture.%20Experimental%0Aresults%20on%20four%20typical%20tasks%20indicate%20that%20Sailor%20models%20demonstrate%20strong%0Aperformance%20across%20different%20benchmarks%2C%20including%20commonsense%20reasoning%2C%0Aquestion%20answering%2C%20reading%20comprehension%20and%20examination.%20Embracing%20the%0Aopen-source%20spirit%2C%20we%20share%20our%20insights%20through%20this%20report%20to%20spark%20a%20wider%0Ainterest%20in%20developing%20large%20language%20models%20for%20multilingual%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03608v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sailor%3A%20Open%20Language%20Models%20for%20South-East%20Asia&entry.906535625=Longxu%20Dou%20and%20Qian%20Liu%20and%20Guangtao%20Zeng%20and%20Jia%20Guo%20and%20Jiahui%20Zhou%20and%20Wei%20Lu%20and%20Min%20Lin&entry.1292438233=%20%20We%20present%20Sailor%2C%20a%20family%20of%20open%20language%20models%20ranging%20from%200.5B%20to%207B%0Aparameters%2C%20tailored%20for%20South-East%20Asian%20%28SEA%29%20languages.%20These%20models%20are%0Acontinually%20pre-trained%20from%20Qwen1.5%2C%20a%20great%20language%20model%20for%20multilingual%0Ause%20cases.%20From%20Qwen1.5%2C%20Sailor%20models%20accept%20200B%20to%20400B%20tokens%2C%20primarily%0Acovering%20the%20languages%20of%20English%2C%20Chinese%2C%20Vietnamese%2C%20Thai%2C%20Indonesian%2C%0AMalay%2C%20and%20Lao.%20The%20training%20leverages%20several%20techniques%2C%20including%20BPE%0Adropout%20for%20improving%20the%20model%20robustness%2C%20aggressive%20data%20cleaning%20and%0Adeduplication%2C%20and%20small%20proxy%20models%20to%20optimize%20data%20mixture.%20Experimental%0Aresults%20on%20four%20typical%20tasks%20indicate%20that%20Sailor%20models%20demonstrate%20strong%0Aperformance%20across%20different%20benchmarks%2C%20including%20commonsense%20reasoning%2C%0Aquestion%20answering%2C%20reading%20comprehension%20and%20examination.%20Embracing%20the%0Aopen-source%20spirit%2C%20we%20share%20our%20insights%20through%20this%20report%20to%20spark%20a%20wider%0Ainterest%20in%20developing%20large%20language%20models%20for%20multilingual%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03608v1&entry.124074799=Read"},
{"title": "Trust in AI: Progress, Challenges, and Future Directions", "author": "Saleh Afroogh and Ali Akbari and Evan Malone and Mohammadali Kargar and Hananeh Alambeigi", "abstract": "  The increasing use of artificial intelligence (AI) systems in our daily life\nthrough various applications, services, and products explains the significance\nof trust/distrust in AI from a user perspective. AI-driven systems (as opposed\nto other technologies) have ubiquitously diffused in our life not only as some\nbeneficial tools to be used by human agents but also are going to be\nsubstitutive agents on our behalf, or manipulative minds that would influence\nhuman thought, decision, and agency. Trust/distrust in AI plays the role of a\nregulator and could significantly control the level of this diffusion, as trust\ncan increase, and distrust may reduce the rate of adoption of AI. Recently,\nvarieties of studies have paid attention to the variant dimension of\ntrust/distrust in AI, and its relevant considerations. In this systematic\nliterature review, after conceptualization of trust in the current AI\nliterature review, we will investigate trust in different types of\nhuman-Machine interaction, and its impact on technology acceptance in different\ndomains. In addition to that, we propose a taxonomy of technical (i.e., safety,\naccuracy, robustness) and non-technical axiological (i.e., ethical, legal, and\nmixed) trustworthiness metrics, and some trustworthy measurements. Moreover, we\nexamine some major trust-breakers in AI (e.g., autonomy and dignity threat),\nand trust makers; and propose some future directions and probable solutions for\nthe transition to a trustworthy AI.\n", "link": "http://arxiv.org/abs/2403.14680v3", "date": "2024-04-04", "relevancy": 1.266, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4506}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4214}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4108}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Trust%20in%20AI%3A%20Progress%2C%20Challenges%2C%20and%20Future%20Directions&body=Title%3A%20Trust%20in%20AI%3A%20Progress%2C%20Challenges%2C%20and%20Future%20Directions%0AAuthor%3A%20Saleh%20Afroogh%20and%20Ali%20Akbari%20and%20Evan%20Malone%20and%20Mohammadali%20Kargar%20and%20Hananeh%20Alambeigi%0AAbstract%3A%20%20%20The%20increasing%20use%20of%20artificial%20intelligence%20%28AI%29%20systems%20in%20our%20daily%20life%0Athrough%20various%20applications%2C%20services%2C%20and%20products%20explains%20the%20significance%0Aof%20trust/distrust%20in%20AI%20from%20a%20user%20perspective.%20AI-driven%20systems%20%28as%20opposed%0Ato%20other%20technologies%29%20have%20ubiquitously%20diffused%20in%20our%20life%20not%20only%20as%20some%0Abeneficial%20tools%20to%20be%20used%20by%20human%20agents%20but%20also%20are%20going%20to%20be%0Asubstitutive%20agents%20on%20our%20behalf%2C%20or%20manipulative%20minds%20that%20would%20influence%0Ahuman%20thought%2C%20decision%2C%20and%20agency.%20Trust/distrust%20in%20AI%20plays%20the%20role%20of%20a%0Aregulator%20and%20could%20significantly%20control%20the%20level%20of%20this%20diffusion%2C%20as%20trust%0Acan%20increase%2C%20and%20distrust%20may%20reduce%20the%20rate%20of%20adoption%20of%20AI.%20Recently%2C%0Avarieties%20of%20studies%20have%20paid%20attention%20to%20the%20variant%20dimension%20of%0Atrust/distrust%20in%20AI%2C%20and%20its%20relevant%20considerations.%20In%20this%20systematic%0Aliterature%20review%2C%20after%20conceptualization%20of%20trust%20in%20the%20current%20AI%0Aliterature%20review%2C%20we%20will%20investigate%20trust%20in%20different%20types%20of%0Ahuman-Machine%20interaction%2C%20and%20its%20impact%20on%20technology%20acceptance%20in%20different%0Adomains.%20In%20addition%20to%20that%2C%20we%20propose%20a%20taxonomy%20of%20technical%20%28i.e.%2C%20safety%2C%0Aaccuracy%2C%20robustness%29%20and%20non-technical%20axiological%20%28i.e.%2C%20ethical%2C%20legal%2C%20and%0Amixed%29%20trustworthiness%20metrics%2C%20and%20some%20trustworthy%20measurements.%20Moreover%2C%20we%0Aexamine%20some%20major%20trust-breakers%20in%20AI%20%28e.g.%2C%20autonomy%20and%20dignity%20threat%29%2C%0Aand%20trust%20makers%3B%20and%20propose%20some%20future%20directions%20and%20probable%20solutions%20for%0Athe%20transition%20to%20a%20trustworthy%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14680v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%20in%20AI%3A%20Progress%2C%20Challenges%2C%20and%20Future%20Directions&entry.906535625=Saleh%20Afroogh%20and%20Ali%20Akbari%20and%20Evan%20Malone%20and%20Mohammadali%20Kargar%20and%20Hananeh%20Alambeigi&entry.1292438233=%20%20The%20increasing%20use%20of%20artificial%20intelligence%20%28AI%29%20systems%20in%20our%20daily%20life%0Athrough%20various%20applications%2C%20services%2C%20and%20products%20explains%20the%20significance%0Aof%20trust/distrust%20in%20AI%20from%20a%20user%20perspective.%20AI-driven%20systems%20%28as%20opposed%0Ato%20other%20technologies%29%20have%20ubiquitously%20diffused%20in%20our%20life%20not%20only%20as%20some%0Abeneficial%20tools%20to%20be%20used%20by%20human%20agents%20but%20also%20are%20going%20to%20be%0Asubstitutive%20agents%20on%20our%20behalf%2C%20or%20manipulative%20minds%20that%20would%20influence%0Ahuman%20thought%2C%20decision%2C%20and%20agency.%20Trust/distrust%20in%20AI%20plays%20the%20role%20of%20a%0Aregulator%20and%20could%20significantly%20control%20the%20level%20of%20this%20diffusion%2C%20as%20trust%0Acan%20increase%2C%20and%20distrust%20may%20reduce%20the%20rate%20of%20adoption%20of%20AI.%20Recently%2C%0Avarieties%20of%20studies%20have%20paid%20attention%20to%20the%20variant%20dimension%20of%0Atrust/distrust%20in%20AI%2C%20and%20its%20relevant%20considerations.%20In%20this%20systematic%0Aliterature%20review%2C%20after%20conceptualization%20of%20trust%20in%20the%20current%20AI%0Aliterature%20review%2C%20we%20will%20investigate%20trust%20in%20different%20types%20of%0Ahuman-Machine%20interaction%2C%20and%20its%20impact%20on%20technology%20acceptance%20in%20different%0Adomains.%20In%20addition%20to%20that%2C%20we%20propose%20a%20taxonomy%20of%20technical%20%28i.e.%2C%20safety%2C%0Aaccuracy%2C%20robustness%29%20and%20non-technical%20axiological%20%28i.e.%2C%20ethical%2C%20legal%2C%20and%0Amixed%29%20trustworthiness%20metrics%2C%20and%20some%20trustworthy%20measurements.%20Moreover%2C%20we%0Aexamine%20some%20major%20trust-breakers%20in%20AI%20%28e.g.%2C%20autonomy%20and%20dignity%20threat%29%2C%0Aand%20trust%20makers%3B%20and%20propose%20some%20future%20directions%20and%20probable%20solutions%20for%0Athe%20transition%20to%20a%20trustworthy%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14680v3&entry.124074799=Read"},
{"title": "Analyzing Musical Characteristics of National Anthems in Relation to\n  Global Indices", "author": "S M Rakib Hasan and Aakar Dhakal and Ms. Ayesha Siddiqua and Mohammad Mominur Rahman and Md Maidul Islam and Mohammed Arfat Raihan Chowdhury and S M Masfequier Rahman Swapno and SM Nuruzzaman Nobel", "abstract": "  Music plays a huge part in shaping peoples' psychology and behavioral\npatterns. This paper investigates the connection between national anthems and\ndifferent global indices with computational music analysis and statistical\ncorrelation analysis. We analyze national anthem musical data to determine\nwhether certain musical characteristics are associated with peace, happiness,\nsuicide rate, crime rate, etc. To achieve this, we collect national anthems\nfrom 169 countries and use computational music analysis techniques to extract\npitch, tempo, beat, and other pertinent audio features. We then compare these\nmusical characteristics with data on different global indices to ascertain\nwhether a significant correlation exists. Our findings indicate that there may\nbe a correlation between the musical characteristics of national anthems and\nthe indices we investigated. The implications of our findings for music\npsychology and policymakers interested in promoting social well-being are\ndiscussed. This paper emphasizes the potential of musical data analysis in\nsocial research and offers a novel perspective on the relationship between\nmusic and social indices. The source code and data are made open-access for\nreproducibility and future research endeavors. It can be accessed at\nhttp://bit.ly/na_code.\n", "link": "http://arxiv.org/abs/2404.03606v1", "date": "2024-04-04", "relevancy": 1.2696, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3372}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3084}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.2904}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Musical%20Characteristics%20of%20National%20Anthems%20in%20Relation%20to%0A%20%20Global%20Indices&body=Title%3A%20Analyzing%20Musical%20Characteristics%20of%20National%20Anthems%20in%20Relation%20to%0A%20%20Global%20Indices%0AAuthor%3A%20S%20M%20Rakib%20Hasan%20and%20Aakar%20Dhakal%20and%20Ms.%20Ayesha%20Siddiqua%20and%20Mohammad%20Mominur%20Rahman%20and%20Md%20Maidul%20Islam%20and%20Mohammed%20Arfat%20Raihan%20Chowdhury%20and%20S%20M%20Masfequier%20Rahman%20Swapno%20and%20SM%20Nuruzzaman%20Nobel%0AAbstract%3A%20%20%20Music%20plays%20a%20huge%20part%20in%20shaping%20peoples%27%20psychology%20and%20behavioral%0Apatterns.%20This%20paper%20investigates%20the%20connection%20between%20national%20anthems%20and%0Adifferent%20global%20indices%20with%20computational%20music%20analysis%20and%20statistical%0Acorrelation%20analysis.%20We%20analyze%20national%20anthem%20musical%20data%20to%20determine%0Awhether%20certain%20musical%20characteristics%20are%20associated%20with%20peace%2C%20happiness%2C%0Asuicide%20rate%2C%20crime%20rate%2C%20etc.%20To%20achieve%20this%2C%20we%20collect%20national%20anthems%0Afrom%20169%20countries%20and%20use%20computational%20music%20analysis%20techniques%20to%20extract%0Apitch%2C%20tempo%2C%20beat%2C%20and%20other%20pertinent%20audio%20features.%20We%20then%20compare%20these%0Amusical%20characteristics%20with%20data%20on%20different%20global%20indices%20to%20ascertain%0Awhether%20a%20significant%20correlation%20exists.%20Our%20findings%20indicate%20that%20there%20may%0Abe%20a%20correlation%20between%20the%20musical%20characteristics%20of%20national%20anthems%20and%0Athe%20indices%20we%20investigated.%20The%20implications%20of%20our%20findings%20for%20music%0Apsychology%20and%20policymakers%20interested%20in%20promoting%20social%20well-being%20are%0Adiscussed.%20This%20paper%20emphasizes%20the%20potential%20of%20musical%20data%20analysis%20in%0Asocial%20research%20and%20offers%20a%20novel%20perspective%20on%20the%20relationship%20between%0Amusic%20and%20social%20indices.%20The%20source%20code%20and%20data%20are%20made%20open-access%20for%0Areproducibility%20and%20future%20research%20endeavors.%20It%20can%20be%20accessed%20at%0Ahttp%3A//bit.ly/na_code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03606v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Musical%20Characteristics%20of%20National%20Anthems%20in%20Relation%20to%0A%20%20Global%20Indices&entry.906535625=S%20M%20Rakib%20Hasan%20and%20Aakar%20Dhakal%20and%20Ms.%20Ayesha%20Siddiqua%20and%20Mohammad%20Mominur%20Rahman%20and%20Md%20Maidul%20Islam%20and%20Mohammed%20Arfat%20Raihan%20Chowdhury%20and%20S%20M%20Masfequier%20Rahman%20Swapno%20and%20SM%20Nuruzzaman%20Nobel&entry.1292438233=%20%20Music%20plays%20a%20huge%20part%20in%20shaping%20peoples%27%20psychology%20and%20behavioral%0Apatterns.%20This%20paper%20investigates%20the%20connection%20between%20national%20anthems%20and%0Adifferent%20global%20indices%20with%20computational%20music%20analysis%20and%20statistical%0Acorrelation%20analysis.%20We%20analyze%20national%20anthem%20musical%20data%20to%20determine%0Awhether%20certain%20musical%20characteristics%20are%20associated%20with%20peace%2C%20happiness%2C%0Asuicide%20rate%2C%20crime%20rate%2C%20etc.%20To%20achieve%20this%2C%20we%20collect%20national%20anthems%0Afrom%20169%20countries%20and%20use%20computational%20music%20analysis%20techniques%20to%20extract%0Apitch%2C%20tempo%2C%20beat%2C%20and%20other%20pertinent%20audio%20features.%20We%20then%20compare%20these%0Amusical%20characteristics%20with%20data%20on%20different%20global%20indices%20to%20ascertain%0Awhether%20a%20significant%20correlation%20exists.%20Our%20findings%20indicate%20that%20there%20may%0Abe%20a%20correlation%20between%20the%20musical%20characteristics%20of%20national%20anthems%20and%0Athe%20indices%20we%20investigated.%20The%20implications%20of%20our%20findings%20for%20music%0Apsychology%20and%20policymakers%20interested%20in%20promoting%20social%20well-being%20are%0Adiscussed.%20This%20paper%20emphasizes%20the%20potential%20of%20musical%20data%20analysis%20in%0Asocial%20research%20and%20offers%20a%20novel%20perspective%20on%20the%20relationship%20between%0Amusic%20and%20social%20indices.%20The%20source%20code%20and%20data%20are%20made%20open-access%20for%0Areproducibility%20and%20future%20research%20endeavors.%20It%20can%20be%20accessed%20at%0Ahttp%3A//bit.ly/na_code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03606v1&entry.124074799=Read"},
{"title": "CodeEditorBench: Evaluating Code Editing Capability of Large Language\n  Models", "author": "Jiawei Guo and Ziming Li and Xueling Liu and Kaijing Ma and Tianyu Zheng and Zhouliang Yu and Ding Pan and Yizhi LI and Ruibo Liu and Yue Wang and Shuyue Guo and Xingwei Qu and Xiang Yue and Ge Zhang and Wenhu Chen and Jie Fu", "abstract": "  Large Language Models (LLMs) for code are rapidly evolving, with code editing\nemerging as a critical capability. We introduce CodeEditorBench, an evaluation\nframework designed to rigorously assess the performance of LLMs in code editing\ntasks, including debugging, translating, polishing, and requirement switching.\nUnlike existing benchmarks focusing solely on code generation, CodeEditorBench\nemphasizes real-world scenarios and practical aspects of software development.\nWe curate diverse coding challenges and scenarios from five sources, covering\nvarious programming languages, complexity levels, and editing tasks. Evaluation\nof 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and\nGPT-4), outperform open-source models in CodeEditorBench, highlighting\ndifferences in model performance based on problem types and prompt\nsensitivities. CodeEditorBench aims to catalyze advancements in LLMs by\nproviding a robust platform for assessing code editing capabilities. We will\nrelease all prompts and datasets to enable the community to expand the dataset\nand benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to\nthe advancement of LLMs in code editing and provide a valuable resource for\nresearchers and practitioners.\n", "link": "http://arxiv.org/abs/2404.03543v1", "date": "2024-04-04", "relevancy": 0.878, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4419}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4403}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4348}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CodeEditorBench%3A%20Evaluating%20Code%20Editing%20Capability%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20CodeEditorBench%3A%20Evaluating%20Code%20Editing%20Capability%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Jiawei%20Guo%20and%20Ziming%20Li%20and%20Xueling%20Liu%20and%20Kaijing%20Ma%20and%20Tianyu%20Zheng%20and%20Zhouliang%20Yu%20and%20Ding%20Pan%20and%20Yizhi%20LI%20and%20Ruibo%20Liu%20and%20Yue%20Wang%20and%20Shuyue%20Guo%20and%20Xingwei%20Qu%20and%20Xiang%20Yue%20and%20Ge%20Zhang%20and%20Wenhu%20Chen%20and%20Jie%20Fu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20for%20code%20are%20rapidly%20evolving%2C%20with%20code%20editing%0Aemerging%20as%20a%20critical%20capability.%20We%20introduce%20CodeEditorBench%2C%20an%20evaluation%0Aframework%20designed%20to%20rigorously%20assess%20the%20performance%20of%20LLMs%20in%20code%20editing%0Atasks%2C%20including%20debugging%2C%20translating%2C%20polishing%2C%20and%20requirement%20switching.%0AUnlike%20existing%20benchmarks%20focusing%20solely%20on%20code%20generation%2C%20CodeEditorBench%0Aemphasizes%20real-world%20scenarios%20and%20practical%20aspects%20of%20software%20development.%0AWe%20curate%20diverse%20coding%20challenges%20and%20scenarios%20from%20five%20sources%2C%20covering%0Avarious%20programming%20languages%2C%20complexity%20levels%2C%20and%20editing%20tasks.%20Evaluation%0Aof%2019%20LLMs%20reveals%20that%20closed-source%20models%20%28particularly%20Gemini-Ultra%20and%0AGPT-4%29%2C%20outperform%20open-source%20models%20in%20CodeEditorBench%2C%20highlighting%0Adifferences%20in%20model%20performance%20based%20on%20problem%20types%20and%20prompt%0Asensitivities.%20CodeEditorBench%20aims%20to%20catalyze%20advancements%20in%20LLMs%20by%0Aproviding%20a%20robust%20platform%20for%20assessing%20code%20editing%20capabilities.%20We%20will%0Arelease%20all%20prompts%20and%20datasets%20to%20enable%20the%20community%20to%20expand%20the%20dataset%0Aand%20benchmark%20emerging%20LLMs.%20By%20introducing%20CodeEditorBench%2C%20we%20contribute%20to%0Athe%20advancement%20of%20LLMs%20in%20code%20editing%20and%20provide%20a%20valuable%20resource%20for%0Aresearchers%20and%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03543v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodeEditorBench%3A%20Evaluating%20Code%20Editing%20Capability%20of%20Large%20Language%0A%20%20Models&entry.906535625=Jiawei%20Guo%20and%20Ziming%20Li%20and%20Xueling%20Liu%20and%20Kaijing%20Ma%20and%20Tianyu%20Zheng%20and%20Zhouliang%20Yu%20and%20Ding%20Pan%20and%20Yizhi%20LI%20and%20Ruibo%20Liu%20and%20Yue%20Wang%20and%20Shuyue%20Guo%20and%20Xingwei%20Qu%20and%20Xiang%20Yue%20and%20Ge%20Zhang%20and%20Wenhu%20Chen%20and%20Jie%20Fu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20for%20code%20are%20rapidly%20evolving%2C%20with%20code%20editing%0Aemerging%20as%20a%20critical%20capability.%20We%20introduce%20CodeEditorBench%2C%20an%20evaluation%0Aframework%20designed%20to%20rigorously%20assess%20the%20performance%20of%20LLMs%20in%20code%20editing%0Atasks%2C%20including%20debugging%2C%20translating%2C%20polishing%2C%20and%20requirement%20switching.%0AUnlike%20existing%20benchmarks%20focusing%20solely%20on%20code%20generation%2C%20CodeEditorBench%0Aemphasizes%20real-world%20scenarios%20and%20practical%20aspects%20of%20software%20development.%0AWe%20curate%20diverse%20coding%20challenges%20and%20scenarios%20from%20five%20sources%2C%20covering%0Avarious%20programming%20languages%2C%20complexity%20levels%2C%20and%20editing%20tasks.%20Evaluation%0Aof%2019%20LLMs%20reveals%20that%20closed-source%20models%20%28particularly%20Gemini-Ultra%20and%0AGPT-4%29%2C%20outperform%20open-source%20models%20in%20CodeEditorBench%2C%20highlighting%0Adifferences%20in%20model%20performance%20based%20on%20problem%20types%20and%20prompt%0Asensitivities.%20CodeEditorBench%20aims%20to%20catalyze%20advancements%20in%20LLMs%20by%0Aproviding%20a%20robust%20platform%20for%20assessing%20code%20editing%20capabilities.%20We%20will%0Arelease%20all%20prompts%20and%20datasets%20to%20enable%20the%20community%20to%20expand%20the%20dataset%0Aand%20benchmark%20emerging%20LLMs.%20By%20introducing%20CodeEditorBench%2C%20we%20contribute%20to%0Athe%20advancement%20of%20LLMs%20in%20code%20editing%20and%20provide%20a%20valuable%20resource%20for%0Aresearchers%20and%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03543v1&entry.124074799=Read"},
{"title": "PointInfinity: Resolution-Invariant Point Diffusion Models", "author": "Zixuan Huang and Justin Johnson and Shoubhik Debnath and James M. Rehg and Chao-Yuan Wu", "abstract": "  We present PointInfinity, an efficient family of point cloud diffusion\nmodels. Our core idea is to use a transformer-based architecture with a\nfixed-size, resolution-invariant latent representation. This enables efficient\ntraining with low-resolution point clouds, while allowing high-resolution point\nclouds to be generated during inference. More importantly, we show that scaling\nthe test-time resolution beyond the training resolution improves the fidelity\nof generated point clouds and surfaces. We analyze this phenomenon and draw a\nlink to classifier-free guidance commonly used in diffusion models,\ndemonstrating that both allow trading off fidelity and variability during\ninference. Experiments on CO3D show that PointInfinity can efficiently generate\nhigh-resolution point clouds (up to 131k points, 31 times more than Point-E)\nwith state-of-the-art quality.\n", "link": "http://arxiv.org/abs/2404.03566v1", "date": "2024-04-04", "relevancy": 1.6895, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5909}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.585}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5433}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PointInfinity%3A%20Resolution-Invariant%20Point%20Diffusion%20Models&body=Title%3A%20PointInfinity%3A%20Resolution-Invariant%20Point%20Diffusion%20Models%0AAuthor%3A%20Zixuan%20Huang%20and%20Justin%20Johnson%20and%20Shoubhik%20Debnath%20and%20James%20M.%20Rehg%20and%20Chao-Yuan%20Wu%0AAbstract%3A%20%20%20We%20present%20PointInfinity%2C%20an%20efficient%20family%20of%20point%20cloud%20diffusion%0Amodels.%20Our%20core%20idea%20is%20to%20use%20a%20transformer-based%20architecture%20with%20a%0Afixed-size%2C%20resolution-invariant%20latent%20representation.%20This%20enables%20efficient%0Atraining%20with%20low-resolution%20point%20clouds%2C%20while%20allowing%20high-resolution%20point%0Aclouds%20to%20be%20generated%20during%20inference.%20More%20importantly%2C%20we%20show%20that%20scaling%0Athe%20test-time%20resolution%20beyond%20the%20training%20resolution%20improves%20the%20fidelity%0Aof%20generated%20point%20clouds%20and%20surfaces.%20We%20analyze%20this%20phenomenon%20and%20draw%20a%0Alink%20to%20classifier-free%20guidance%20commonly%20used%20in%20diffusion%20models%2C%0Ademonstrating%20that%20both%20allow%20trading%20off%20fidelity%20and%20variability%20during%0Ainference.%20Experiments%20on%20CO3D%20show%20that%20PointInfinity%20can%20efficiently%20generate%0Ahigh-resolution%20point%20clouds%20%28up%20to%20131k%20points%2C%2031%20times%20more%20than%20Point-E%29%0Awith%20state-of-the-art%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03566v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointInfinity%3A%20Resolution-Invariant%20Point%20Diffusion%20Models&entry.906535625=Zixuan%20Huang%20and%20Justin%20Johnson%20and%20Shoubhik%20Debnath%20and%20James%20M.%20Rehg%20and%20Chao-Yuan%20Wu&entry.1292438233=%20%20We%20present%20PointInfinity%2C%20an%20efficient%20family%20of%20point%20cloud%20diffusion%0Amodels.%20Our%20core%20idea%20is%20to%20use%20a%20transformer-based%20architecture%20with%20a%0Afixed-size%2C%20resolution-invariant%20latent%20representation.%20This%20enables%20efficient%0Atraining%20with%20low-resolution%20point%20clouds%2C%20while%20allowing%20high-resolution%20point%0Aclouds%20to%20be%20generated%20during%20inference.%20More%20importantly%2C%20we%20show%20that%20scaling%0Athe%20test-time%20resolution%20beyond%20the%20training%20resolution%20improves%20the%20fidelity%0Aof%20generated%20point%20clouds%20and%20surfaces.%20We%20analyze%20this%20phenomenon%20and%20draw%20a%0Alink%20to%20classifier-free%20guidance%20commonly%20used%20in%20diffusion%20models%2C%0Ademonstrating%20that%20both%20allow%20trading%20off%20fidelity%20and%20variability%20during%0Ainference.%20Experiments%20on%20CO3D%20show%20that%20PointInfinity%20can%20efficiently%20generate%0Ahigh-resolution%20point%20clouds%20%28up%20to%20131k%20points%2C%2031%20times%20more%20than%20Point-E%29%0Awith%20state-of-the-art%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03566v1&entry.124074799=Read"},
{"title": "Mitigating the Impact of Outlier Channels for Language Model\n  Quantization with Activation Regularization", "author": "Aniruddha Nrusimha and Mayank Mishra and Naigang Wang and Dan Alistarh and Rameswar Panda and Yoon Kim", "abstract": "  We consider the problem of accurate quantization for language models, where\nboth the weights and activations are uniformly quantized to 4 bits per\nparameter, the lowest bitwidth format natively supported by GPU hardware. In\nthis context, the key challenge is activation quantization: it is known that\nlanguage models contain outlier channels whose values on average are orders of\nmagnitude higher than than other channels, which prevents accurate low-bitwidth\nquantization with known techniques. We systematically study this phenomena and\nfind that these outlier channels emerge early in training, and that they occur\nmore frequently in layers with residual streams. We then propose a simple\nstrategy which regularizes a layer's inputs via quantization-aware training\n(QAT) and its outputs via activation kurtosis regularization. We show that\nregularizing both the inputs and outputs is crucial for preventing a model's\n\"migrating\" the difficulty in input quantization to the weights, which makes\npost-training quantization (PTQ) of weights more difficult. When combined with\nweight PTQ, we show that our approach can obtain a W4A4 model that performs\ncompetitively to the standard-precision W16A16 baseline.\n", "link": "http://arxiv.org/abs/2404.03605v1", "date": "2024-04-04", "relevancy": 1.4351, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4939}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.475}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4713}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mitigating%20the%20Impact%20of%20Outlier%20Channels%20for%20Language%20Model%0A%20%20Quantization%20with%20Activation%20Regularization&body=Title%3A%20Mitigating%20the%20Impact%20of%20Outlier%20Channels%20for%20Language%20Model%0A%20%20Quantization%20with%20Activation%20Regularization%0AAuthor%3A%20Aniruddha%20Nrusimha%20and%20Mayank%20Mishra%20and%20Naigang%20Wang%20and%20Dan%20Alistarh%20and%20Rameswar%20Panda%20and%20Yoon%20Kim%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20accurate%20quantization%20for%20language%20models%2C%20where%0Aboth%20the%20weights%20and%20activations%20are%20uniformly%20quantized%20to%204%20bits%20per%0Aparameter%2C%20the%20lowest%20bitwidth%20format%20natively%20supported%20by%20GPU%20hardware.%20In%0Athis%20context%2C%20the%20key%20challenge%20is%20activation%20quantization%3A%20it%20is%20known%20that%0Alanguage%20models%20contain%20outlier%20channels%20whose%20values%20on%20average%20are%20orders%20of%0Amagnitude%20higher%20than%20than%20other%20channels%2C%20which%20prevents%20accurate%20low-bitwidth%0Aquantization%20with%20known%20techniques.%20We%20systematically%20study%20this%20phenomena%20and%0Afind%20that%20these%20outlier%20channels%20emerge%20early%20in%20training%2C%20and%20that%20they%20occur%0Amore%20frequently%20in%20layers%20with%20residual%20streams.%20We%20then%20propose%20a%20simple%0Astrategy%20which%20regularizes%20a%20layer%27s%20inputs%20via%20quantization-aware%20training%0A%28QAT%29%20and%20its%20outputs%20via%20activation%20kurtosis%20regularization.%20We%20show%20that%0Aregularizing%20both%20the%20inputs%20and%20outputs%20is%20crucial%20for%20preventing%20a%20model%27s%0A%22migrating%22%20the%20difficulty%20in%20input%20quantization%20to%20the%20weights%2C%20which%20makes%0Apost-training%20quantization%20%28PTQ%29%20of%20weights%20more%20difficult.%20When%20combined%20with%0Aweight%20PTQ%2C%20we%20show%20that%20our%20approach%20can%20obtain%20a%20W4A4%20model%20that%20performs%0Acompetitively%20to%20the%20standard-precision%20W16A16%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03605v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20the%20Impact%20of%20Outlier%20Channels%20for%20Language%20Model%0A%20%20Quantization%20with%20Activation%20Regularization&entry.906535625=Aniruddha%20Nrusimha%20and%20Mayank%20Mishra%20and%20Naigang%20Wang%20and%20Dan%20Alistarh%20and%20Rameswar%20Panda%20and%20Yoon%20Kim&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20accurate%20quantization%20for%20language%20models%2C%20where%0Aboth%20the%20weights%20and%20activations%20are%20uniformly%20quantized%20to%204%20bits%20per%0Aparameter%2C%20the%20lowest%20bitwidth%20format%20natively%20supported%20by%20GPU%20hardware.%20In%0Athis%20context%2C%20the%20key%20challenge%20is%20activation%20quantization%3A%20it%20is%20known%20that%0Alanguage%20models%20contain%20outlier%20channels%20whose%20values%20on%20average%20are%20orders%20of%0Amagnitude%20higher%20than%20than%20other%20channels%2C%20which%20prevents%20accurate%20low-bitwidth%0Aquantization%20with%20known%20techniques.%20We%20systematically%20study%20this%20phenomena%20and%0Afind%20that%20these%20outlier%20channels%20emerge%20early%20in%20training%2C%20and%20that%20they%20occur%0Amore%20frequently%20in%20layers%20with%20residual%20streams.%20We%20then%20propose%20a%20simple%0Astrategy%20which%20regularizes%20a%20layer%27s%20inputs%20via%20quantization-aware%20training%0A%28QAT%29%20and%20its%20outputs%20via%20activation%20kurtosis%20regularization.%20We%20show%20that%0Aregularizing%20both%20the%20inputs%20and%20outputs%20is%20crucial%20for%20preventing%20a%20model%27s%0A%22migrating%22%20the%20difficulty%20in%20input%20quantization%20to%20the%20weights%2C%20which%20makes%0Apost-training%20quantization%20%28PTQ%29%20of%20weights%20more%20difficult.%20When%20combined%20with%0Aweight%20PTQ%2C%20we%20show%20that%20our%20approach%20can%20obtain%20a%20W4A4%20model%20that%20performs%0Acompetitively%20to%20the%20standard-precision%20W16A16%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03605v1&entry.124074799=Read"},
{"title": "Expectable Motion Unit: Avoiding Hazards From Human Involuntary Motions\n  in Human-Robot Interaction", "author": "Robin Jeanne Kirschner and Henning Mayer and Lisa Burr and Nico Mansfeld and Saeed Abdolshah and Sami Haddadin", "abstract": "  In robotics, many control and planning schemes have been developed to ensure\nhuman physical safety in human- robot interaction. The human psychological\nstate and the ex- pectation towards the robot, however, are typically\nneglected. Even if the robot behaviour is regarded as biomechanically safe,\nhumans may still react with a rapid involuntary motion (IM) caused by a startle\nor surprise. Such sudden, uncontrolled motions can jeopardize safety and should\nbe prevented by any means. In this letter, we propose the Expectable Motion\nUnit (EMU), which ensures that a certain probability of IM occurrence is not\nexceeded in a typical HRI setting. Based on a model of IM occurrence generated\nthrough an experiment with 29 participants, we establish the mapping between\nrobot velocity, robot-human distance, and the relative frequency of IM\noccurrence. This mapping is processed towards a real-time capable robot motion\ngenerator that limits the robot velocity during task execution if necessary.\nThe EMU is combined in a holistic safety framework that integrates both the\nphysical and psychological safety knowledge. A validation experiment showed\nthat the EMU successfully avoids human IM in five out of six cases.\n", "link": "http://arxiv.org/abs/2109.07201v2", "date": "2024-04-04", "relevancy": 1.6273, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5621}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5456}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5148}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Expectable%20Motion%20Unit%3A%20Avoiding%20Hazards%20From%20Human%20Involuntary%20Motions%0A%20%20in%20Human-Robot%20Interaction&body=Title%3A%20Expectable%20Motion%20Unit%3A%20Avoiding%20Hazards%20From%20Human%20Involuntary%20Motions%0A%20%20in%20Human-Robot%20Interaction%0AAuthor%3A%20Robin%20Jeanne%20Kirschner%20and%20Henning%20Mayer%20and%20Lisa%20Burr%20and%20Nico%20Mansfeld%20and%20Saeed%20Abdolshah%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20In%20robotics%2C%20many%20control%20and%20planning%20schemes%20have%20been%20developed%20to%20ensure%0Ahuman%20physical%20safety%20in%20human-%20robot%20interaction.%20The%20human%20psychological%0Astate%20and%20the%20ex-%20pectation%20towards%20the%20robot%2C%20however%2C%20are%20typically%0Aneglected.%20Even%20if%20the%20robot%20behaviour%20is%20regarded%20as%20biomechanically%20safe%2C%0Ahumans%20may%20still%20react%20with%20a%20rapid%20involuntary%20motion%20%28IM%29%20caused%20by%20a%20startle%0Aor%20surprise.%20Such%20sudden%2C%20uncontrolled%20motions%20can%20jeopardize%20safety%20and%20should%0Abe%20prevented%20by%20any%20means.%20In%20this%20letter%2C%20we%20propose%20the%20Expectable%20Motion%0AUnit%20%28EMU%29%2C%20which%20ensures%20that%20a%20certain%20probability%20of%20IM%20occurrence%20is%20not%0Aexceeded%20in%20a%20typical%20HRI%20setting.%20Based%20on%20a%20model%20of%20IM%20occurrence%20generated%0Athrough%20an%20experiment%20with%2029%20participants%2C%20we%20establish%20the%20mapping%20between%0Arobot%20velocity%2C%20robot-human%20distance%2C%20and%20the%20relative%20frequency%20of%20IM%0Aoccurrence.%20This%20mapping%20is%20processed%20towards%20a%20real-time%20capable%20robot%20motion%0Agenerator%20that%20limits%20the%20robot%20velocity%20during%20task%20execution%20if%20necessary.%0AThe%20EMU%20is%20combined%20in%20a%20holistic%20safety%20framework%20that%20integrates%20both%20the%0Aphysical%20and%20psychological%20safety%20knowledge.%20A%20validation%20experiment%20showed%0Athat%20the%20EMU%20successfully%20avoids%20human%20IM%20in%20five%20out%20of%20six%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2109.07201v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expectable%20Motion%20Unit%3A%20Avoiding%20Hazards%20From%20Human%20Involuntary%20Motions%0A%20%20in%20Human-Robot%20Interaction&entry.906535625=Robin%20Jeanne%20Kirschner%20and%20Henning%20Mayer%20and%20Lisa%20Burr%20and%20Nico%20Mansfeld%20and%20Saeed%20Abdolshah%20and%20Sami%20Haddadin&entry.1292438233=%20%20In%20robotics%2C%20many%20control%20and%20planning%20schemes%20have%20been%20developed%20to%20ensure%0Ahuman%20physical%20safety%20in%20human-%20robot%20interaction.%20The%20human%20psychological%0Astate%20and%20the%20ex-%20pectation%20towards%20the%20robot%2C%20however%2C%20are%20typically%0Aneglected.%20Even%20if%20the%20robot%20behaviour%20is%20regarded%20as%20biomechanically%20safe%2C%0Ahumans%20may%20still%20react%20with%20a%20rapid%20involuntary%20motion%20%28IM%29%20caused%20by%20a%20startle%0Aor%20surprise.%20Such%20sudden%2C%20uncontrolled%20motions%20can%20jeopardize%20safety%20and%20should%0Abe%20prevented%20by%20any%20means.%20In%20this%20letter%2C%20we%20propose%20the%20Expectable%20Motion%0AUnit%20%28EMU%29%2C%20which%20ensures%20that%20a%20certain%20probability%20of%20IM%20occurrence%20is%20not%0Aexceeded%20in%20a%20typical%20HRI%20setting.%20Based%20on%20a%20model%20of%20IM%20occurrence%20generated%0Athrough%20an%20experiment%20with%2029%20participants%2C%20we%20establish%20the%20mapping%20between%0Arobot%20velocity%2C%20robot-human%20distance%2C%20and%20the%20relative%20frequency%20of%20IM%0Aoccurrence.%20This%20mapping%20is%20processed%20towards%20a%20real-time%20capable%20robot%20motion%0Agenerator%20that%20limits%20the%20robot%20velocity%20during%20task%20execution%20if%20necessary.%0AThe%20EMU%20is%20combined%20in%20a%20holistic%20safety%20framework%20that%20integrates%20both%20the%0Aphysical%20and%20psychological%20safety%20knowledge.%20A%20validation%20experiment%20showed%0Athat%20the%20EMU%20successfully%20avoids%20human%20IM%20in%20five%20out%20of%20six%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.07201v2&entry.124074799=Read"},
{"title": "DiffBody: Human Body Restoration by Imagining with Generative Diffusion\n  Prior", "author": "Yiming Zhang and Zhe Wang and Xinjie Li and Yunchen Yuan and Chengsong Zhang and Xiao Sun and Zhihang Zhong and Jian Wang", "abstract": "  Human body restoration plays a vital role in various applications related to\nthe human body. Despite recent advances in general image restoration using\ngenerative models, their performance in human body restoration remains\nmediocre, often resulting in foreground and background blending, over-smoothing\nsurface textures, missing accessories, and distorted limbs. Addressing these\nchallenges, we propose a novel approach by constructing a human body-aware\ndiffusion model that leverages domain-specific knowledge to enhance\nperformance. Specifically, we employ a pretrained body attention module to\nguide the diffusion model's focus on the foreground, addressing issues caused\nby blending between the subject and background. We also demonstrate the value\nof revisiting the language modality of the diffusion model in restoration tasks\nby seamlessly incorporating text prompt to improve the quality of surface\ntexture and additional clothing and accessories details. Additionally, we\nintroduce a diffusion sampler tailored for fine-grained human body parts,\nutilizing local semantic information to rectify limb distortions. Lastly, we\ncollect a comprehensive dataset for benchmarking and advancing the field of\nhuman body restoration. Extensive experimental validation showcases the\nsuperiority of our approach, both quantitatively and qualitatively, over\nexisting methods.\n", "link": "http://arxiv.org/abs/2404.03642v1", "date": "2024-04-04", "relevancy": 1.2269, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6553}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6042}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5808}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiffBody%3A%20Human%20Body%20Restoration%20by%20Imagining%20with%20Generative%20Diffusion%0A%20%20Prior&body=Title%3A%20DiffBody%3A%20Human%20Body%20Restoration%20by%20Imagining%20with%20Generative%20Diffusion%0A%20%20Prior%0AAuthor%3A%20Yiming%20Zhang%20and%20Zhe%20Wang%20and%20Xinjie%20Li%20and%20Yunchen%20Yuan%20and%20Chengsong%20Zhang%20and%20Xiao%20Sun%20and%20Zhihang%20Zhong%20and%20Jian%20Wang%0AAbstract%3A%20%20%20Human%20body%20restoration%20plays%20a%20vital%20role%20in%20various%20applications%20related%20to%0Athe%20human%20body.%20Despite%20recent%20advances%20in%20general%20image%20restoration%20using%0Agenerative%20models%2C%20their%20performance%20in%20human%20body%20restoration%20remains%0Amediocre%2C%20often%20resulting%20in%20foreground%20and%20background%20blending%2C%20over-smoothing%0Asurface%20textures%2C%20missing%20accessories%2C%20and%20distorted%20limbs.%20Addressing%20these%0Achallenges%2C%20we%20propose%20a%20novel%20approach%20by%20constructing%20a%20human%20body-aware%0Adiffusion%20model%20that%20leverages%20domain-specific%20knowledge%20to%20enhance%0Aperformance.%20Specifically%2C%20we%20employ%20a%20pretrained%20body%20attention%20module%20to%0Aguide%20the%20diffusion%20model%27s%20focus%20on%20the%20foreground%2C%20addressing%20issues%20caused%0Aby%20blending%20between%20the%20subject%20and%20background.%20We%20also%20demonstrate%20the%20value%0Aof%20revisiting%20the%20language%20modality%20of%20the%20diffusion%20model%20in%20restoration%20tasks%0Aby%20seamlessly%20incorporating%20text%20prompt%20to%20improve%20the%20quality%20of%20surface%0Atexture%20and%20additional%20clothing%20and%20accessories%20details.%20Additionally%2C%20we%0Aintroduce%20a%20diffusion%20sampler%20tailored%20for%20fine-grained%20human%20body%20parts%2C%0Autilizing%20local%20semantic%20information%20to%20rectify%20limb%20distortions.%20Lastly%2C%20we%0Acollect%20a%20comprehensive%20dataset%20for%20benchmarking%20and%20advancing%20the%20field%20of%0Ahuman%20body%20restoration.%20Extensive%20experimental%20validation%20showcases%20the%0Asuperiority%20of%20our%20approach%2C%20both%20quantitatively%20and%20qualitatively%2C%20over%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03642v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffBody%3A%20Human%20Body%20Restoration%20by%20Imagining%20with%20Generative%20Diffusion%0A%20%20Prior&entry.906535625=Yiming%20Zhang%20and%20Zhe%20Wang%20and%20Xinjie%20Li%20and%20Yunchen%20Yuan%20and%20Chengsong%20Zhang%20and%20Xiao%20Sun%20and%20Zhihang%20Zhong%20and%20Jian%20Wang&entry.1292438233=%20%20Human%20body%20restoration%20plays%20a%20vital%20role%20in%20various%20applications%20related%20to%0Athe%20human%20body.%20Despite%20recent%20advances%20in%20general%20image%20restoration%20using%0Agenerative%20models%2C%20their%20performance%20in%20human%20body%20restoration%20remains%0Amediocre%2C%20often%20resulting%20in%20foreground%20and%20background%20blending%2C%20over-smoothing%0Asurface%20textures%2C%20missing%20accessories%2C%20and%20distorted%20limbs.%20Addressing%20these%0Achallenges%2C%20we%20propose%20a%20novel%20approach%20by%20constructing%20a%20human%20body-aware%0Adiffusion%20model%20that%20leverages%20domain-specific%20knowledge%20to%20enhance%0Aperformance.%20Specifically%2C%20we%20employ%20a%20pretrained%20body%20attention%20module%20to%0Aguide%20the%20diffusion%20model%27s%20focus%20on%20the%20foreground%2C%20addressing%20issues%20caused%0Aby%20blending%20between%20the%20subject%20and%20background.%20We%20also%20demonstrate%20the%20value%0Aof%20revisiting%20the%20language%20modality%20of%20the%20diffusion%20model%20in%20restoration%20tasks%0Aby%20seamlessly%20incorporating%20text%20prompt%20to%20improve%20the%20quality%20of%20surface%0Atexture%20and%20additional%20clothing%20and%20accessories%20details.%20Additionally%2C%20we%0Aintroduce%20a%20diffusion%20sampler%20tailored%20for%20fine-grained%20human%20body%20parts%2C%0Autilizing%20local%20semantic%20information%20to%20rectify%20limb%20distortions.%20Lastly%2C%20we%0Acollect%20a%20comprehensive%20dataset%20for%20benchmarking%20and%20advancing%20the%20field%20of%0Ahuman%20body%20restoration.%20Extensive%20experimental%20validation%20showcases%20the%0Asuperiority%20of%20our%20approach%2C%20both%20quantitatively%20and%20qualitatively%2C%20over%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03642v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


