<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240722.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Learning Where to Look: Self-supervised Viewpoint Selection for Active\n  Localization using Geometrical Information", "author": "Luca Di Giammarino and Boyang Sun and Giorgio Grisetti and Marc Pollefeys and Hermann Blum and Daniel Barath", "abstract": "  Accurate localization in diverse environments is a fundamental challenge in\ncomputer vision and robotics. The task involves determining a sensor's precise\nposition and orientation, typically a camera, within a given space. Traditional\nlocalization methods often rely on passive sensing, which may struggle in\nscenarios with limited features or dynamic environments. In response, this\npaper explores the domain of active localization, emphasizing the importance of\nviewpoint selection to enhance localization accuracy. Our contributions involve\nusing a data-driven approach with a simple architecture designed for real-time\noperation, a self-supervised data training method, and the capability to\nconsistently integrate our map into a planning framework tailored for\nreal-world robotics applications. Our results demonstrate that our method\nperforms better than the existing one, targeting similar problems and\ngeneralizing on synthetic and real data. We also release an open-source\nimplementation to benefit the community.\n", "link": "http://arxiv.org/abs/2407.15593v1", "date": "2024-07-22", "relevancy": 3.1117, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.641}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6351}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Where%20to%20Look%3A%20Self-supervised%20Viewpoint%20Selection%20for%20Active%0A%20%20Localization%20using%20Geometrical%20Information&body=Title%3A%20Learning%20Where%20to%20Look%3A%20Self-supervised%20Viewpoint%20Selection%20for%20Active%0A%20%20Localization%20using%20Geometrical%20Information%0AAuthor%3A%20Luca%20Di%20Giammarino%20and%20Boyang%20Sun%20and%20Giorgio%20Grisetti%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Daniel%20Barath%0AAbstract%3A%20%20%20Accurate%20localization%20in%20diverse%20environments%20is%20a%20fundamental%20challenge%20in%0Acomputer%20vision%20and%20robotics.%20The%20task%20involves%20determining%20a%20sensor%27s%20precise%0Aposition%20and%20orientation%2C%20typically%20a%20camera%2C%20within%20a%20given%20space.%20Traditional%0Alocalization%20methods%20often%20rely%20on%20passive%20sensing%2C%20which%20may%20struggle%20in%0Ascenarios%20with%20limited%20features%20or%20dynamic%20environments.%20In%20response%2C%20this%0Apaper%20explores%20the%20domain%20of%20active%20localization%2C%20emphasizing%20the%20importance%20of%0Aviewpoint%20selection%20to%20enhance%20localization%20accuracy.%20Our%20contributions%20involve%0Ausing%20a%20data-driven%20approach%20with%20a%20simple%20architecture%20designed%20for%20real-time%0Aoperation%2C%20a%20self-supervised%20data%20training%20method%2C%20and%20the%20capability%20to%0Aconsistently%20integrate%20our%20map%20into%20a%20planning%20framework%20tailored%20for%0Areal-world%20robotics%20applications.%20Our%20results%20demonstrate%20that%20our%20method%0Aperforms%20better%20than%20the%20existing%20one%2C%20targeting%20similar%20problems%20and%0Ageneralizing%20on%20synthetic%20and%20real%20data.%20We%20also%20release%20an%20open-source%0Aimplementation%20to%20benefit%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Where%2520to%2520Look%253A%2520Self-supervised%2520Viewpoint%2520Selection%2520for%2520Active%250A%2520%2520Localization%2520using%2520Geometrical%2520Information%26entry.906535625%3DLuca%2520Di%2520Giammarino%2520and%2520Boyang%2520Sun%2520and%2520Giorgio%2520Grisetti%2520and%2520Marc%2520Pollefeys%2520and%2520Hermann%2520Blum%2520and%2520Daniel%2520Barath%26entry.1292438233%3D%2520%2520Accurate%2520localization%2520in%2520diverse%2520environments%2520is%2520a%2520fundamental%2520challenge%2520in%250Acomputer%2520vision%2520and%2520robotics.%2520The%2520task%2520involves%2520determining%2520a%2520sensor%2527s%2520precise%250Aposition%2520and%2520orientation%252C%2520typically%2520a%2520camera%252C%2520within%2520a%2520given%2520space.%2520Traditional%250Alocalization%2520methods%2520often%2520rely%2520on%2520passive%2520sensing%252C%2520which%2520may%2520struggle%2520in%250Ascenarios%2520with%2520limited%2520features%2520or%2520dynamic%2520environments.%2520In%2520response%252C%2520this%250Apaper%2520explores%2520the%2520domain%2520of%2520active%2520localization%252C%2520emphasizing%2520the%2520importance%2520of%250Aviewpoint%2520selection%2520to%2520enhance%2520localization%2520accuracy.%2520Our%2520contributions%2520involve%250Ausing%2520a%2520data-driven%2520approach%2520with%2520a%2520simple%2520architecture%2520designed%2520for%2520real-time%250Aoperation%252C%2520a%2520self-supervised%2520data%2520training%2520method%252C%2520and%2520the%2520capability%2520to%250Aconsistently%2520integrate%2520our%2520map%2520into%2520a%2520planning%2520framework%2520tailored%2520for%250Areal-world%2520robotics%2520applications.%2520Our%2520results%2520demonstrate%2520that%2520our%2520method%250Aperforms%2520better%2520than%2520the%2520existing%2520one%252C%2520targeting%2520similar%2520problems%2520and%250Ageneralizing%2520on%2520synthetic%2520and%2520real%2520data.%2520We%2520also%2520release%2520an%2520open-source%250Aimplementation%2520to%2520benefit%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Where%20to%20Look%3A%20Self-supervised%20Viewpoint%20Selection%20for%20Active%0A%20%20Localization%20using%20Geometrical%20Information&entry.906535625=Luca%20Di%20Giammarino%20and%20Boyang%20Sun%20and%20Giorgio%20Grisetti%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Daniel%20Barath&entry.1292438233=%20%20Accurate%20localization%20in%20diverse%20environments%20is%20a%20fundamental%20challenge%20in%0Acomputer%20vision%20and%20robotics.%20The%20task%20involves%20determining%20a%20sensor%27s%20precise%0Aposition%20and%20orientation%2C%20typically%20a%20camera%2C%20within%20a%20given%20space.%20Traditional%0Alocalization%20methods%20often%20rely%20on%20passive%20sensing%2C%20which%20may%20struggle%20in%0Ascenarios%20with%20limited%20features%20or%20dynamic%20environments.%20In%20response%2C%20this%0Apaper%20explores%20the%20domain%20of%20active%20localization%2C%20emphasizing%20the%20importance%20of%0Aviewpoint%20selection%20to%20enhance%20localization%20accuracy.%20Our%20contributions%20involve%0Ausing%20a%20data-driven%20approach%20with%20a%20simple%20architecture%20designed%20for%20real-time%0Aoperation%2C%20a%20self-supervised%20data%20training%20method%2C%20and%20the%20capability%20to%0Aconsistently%20integrate%20our%20map%20into%20a%20planning%20framework%20tailored%20for%0Areal-world%20robotics%20applications.%20Our%20results%20demonstrate%20that%20our%20method%0Aperforms%20better%20than%20the%20existing%20one%2C%20targeting%20similar%20problems%20and%0Ageneralizing%20on%20synthetic%20and%20real%20data.%20We%20also%20release%20an%20open-source%0Aimplementation%20to%20benefit%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15593v1&entry.124074799=Read"},
{"title": "DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing", "author": "Minghao Chen and Iro Laina and Andrea Vedaldi", "abstract": "  We consider the problem of editing 3D objects and scenes based on open-ended\nlanguage instructions. A common approach to this problem is to use a 2D image\ngenerator or editor to guide the 3D editing process, obviating the need for 3D\ndata. However, this process is often inefficient due to the need for iterative\nupdates of costly 3D representations, such as neural radiance fields, either\nthrough individual view edits or score distillation sampling. A major\ndisadvantage of this approach is the slow convergence caused by aggregating\ninconsistent information across views, as the guidance from 2D models is not\nmulti-view consistent. We thus introduce the Direct Gaussian Editor (DGE), a\nmethod that addresses these issues in two stages. First, we modify a given\nhigh-quality image editor like InstructPix2Pix to be multi-view consistent. To\ndo so, we propose a training-free approach that integrates cues from the 3D\ngeometry of the underlying scene. Second, given a multi-view consistent edited\nsequence of images, we directly and efficiently optimize the 3D representation,\nwhich is based on 3D Gaussian Splatting. Because it avoids incremental and\niterative edits, DGE is significantly more accurate and efficient than existing\napproaches and offers additional benefits, such as enabling selective editing\nof parts of the scene.\n", "link": "http://arxiv.org/abs/2404.18929v2", "date": "2024-07-22", "relevancy": 3.0906, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6494}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6034}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGE%3A%20Direct%20Gaussian%203D%20Editing%20by%20Consistent%20Multi-view%20Editing&body=Title%3A%20DGE%3A%20Direct%20Gaussian%203D%20Editing%20by%20Consistent%20Multi-view%20Editing%0AAuthor%3A%20Minghao%20Chen%20and%20Iro%20Laina%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20editing%203D%20objects%20and%20scenes%20based%20on%20open-ended%0Alanguage%20instructions.%20A%20common%20approach%20to%20this%20problem%20is%20to%20use%20a%202D%20image%0Agenerator%20or%20editor%20to%20guide%20the%203D%20editing%20process%2C%20obviating%20the%20need%20for%203D%0Adata.%20However%2C%20this%20process%20is%20often%20inefficient%20due%20to%20the%20need%20for%20iterative%0Aupdates%20of%20costly%203D%20representations%2C%20such%20as%20neural%20radiance%20fields%2C%20either%0Athrough%20individual%20view%20edits%20or%20score%20distillation%20sampling.%20A%20major%0Adisadvantage%20of%20this%20approach%20is%20the%20slow%20convergence%20caused%20by%20aggregating%0Ainconsistent%20information%20across%20views%2C%20as%20the%20guidance%20from%202D%20models%20is%20not%0Amulti-view%20consistent.%20We%20thus%20introduce%20the%20Direct%20Gaussian%20Editor%20%28DGE%29%2C%20a%0Amethod%20that%20addresses%20these%20issues%20in%20two%20stages.%20First%2C%20we%20modify%20a%20given%0Ahigh-quality%20image%20editor%20like%20InstructPix2Pix%20to%20be%20multi-view%20consistent.%20To%0Ado%20so%2C%20we%20propose%20a%20training-free%20approach%20that%20integrates%20cues%20from%20the%203D%0Ageometry%20of%20the%20underlying%20scene.%20Second%2C%20given%20a%20multi-view%20consistent%20edited%0Asequence%20of%20images%2C%20we%20directly%20and%20efficiently%20optimize%20the%203D%20representation%2C%0Awhich%20is%20based%20on%203D%20Gaussian%20Splatting.%20Because%20it%20avoids%20incremental%20and%0Aiterative%20edits%2C%20DGE%20is%20significantly%20more%20accurate%20and%20efficient%20than%20existing%0Aapproaches%20and%20offers%20additional%20benefits%2C%20such%20as%20enabling%20selective%20editing%0Aof%20parts%20of%20the%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGE%253A%2520Direct%2520Gaussian%25203D%2520Editing%2520by%2520Consistent%2520Multi-view%2520Editing%26entry.906535625%3DMinghao%2520Chen%2520and%2520Iro%2520Laina%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520editing%25203D%2520objects%2520and%2520scenes%2520based%2520on%2520open-ended%250Alanguage%2520instructions.%2520A%2520common%2520approach%2520to%2520this%2520problem%2520is%2520to%2520use%2520a%25202D%2520image%250Agenerator%2520or%2520editor%2520to%2520guide%2520the%25203D%2520editing%2520process%252C%2520obviating%2520the%2520need%2520for%25203D%250Adata.%2520However%252C%2520this%2520process%2520is%2520often%2520inefficient%2520due%2520to%2520the%2520need%2520for%2520iterative%250Aupdates%2520of%2520costly%25203D%2520representations%252C%2520such%2520as%2520neural%2520radiance%2520fields%252C%2520either%250Athrough%2520individual%2520view%2520edits%2520or%2520score%2520distillation%2520sampling.%2520A%2520major%250Adisadvantage%2520of%2520this%2520approach%2520is%2520the%2520slow%2520convergence%2520caused%2520by%2520aggregating%250Ainconsistent%2520information%2520across%2520views%252C%2520as%2520the%2520guidance%2520from%25202D%2520models%2520is%2520not%250Amulti-view%2520consistent.%2520We%2520thus%2520introduce%2520the%2520Direct%2520Gaussian%2520Editor%2520%2528DGE%2529%252C%2520a%250Amethod%2520that%2520addresses%2520these%2520issues%2520in%2520two%2520stages.%2520First%252C%2520we%2520modify%2520a%2520given%250Ahigh-quality%2520image%2520editor%2520like%2520InstructPix2Pix%2520to%2520be%2520multi-view%2520consistent.%2520To%250Ado%2520so%252C%2520we%2520propose%2520a%2520training-free%2520approach%2520that%2520integrates%2520cues%2520from%2520the%25203D%250Ageometry%2520of%2520the%2520underlying%2520scene.%2520Second%252C%2520given%2520a%2520multi-view%2520consistent%2520edited%250Asequence%2520of%2520images%252C%2520we%2520directly%2520and%2520efficiently%2520optimize%2520the%25203D%2520representation%252C%250Awhich%2520is%2520based%2520on%25203D%2520Gaussian%2520Splatting.%2520Because%2520it%2520avoids%2520incremental%2520and%250Aiterative%2520edits%252C%2520DGE%2520is%2520significantly%2520more%2520accurate%2520and%2520efficient%2520than%2520existing%250Aapproaches%2520and%2520offers%2520additional%2520benefits%252C%2520such%2520as%2520enabling%2520selective%2520editing%250Aof%2520parts%2520of%2520the%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGE%3A%20Direct%20Gaussian%203D%20Editing%20by%20Consistent%20Multi-view%20Editing&entry.906535625=Minghao%20Chen%20and%20Iro%20Laina%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20editing%203D%20objects%20and%20scenes%20based%20on%20open-ended%0Alanguage%20instructions.%20A%20common%20approach%20to%20this%20problem%20is%20to%20use%20a%202D%20image%0Agenerator%20or%20editor%20to%20guide%20the%203D%20editing%20process%2C%20obviating%20the%20need%20for%203D%0Adata.%20However%2C%20this%20process%20is%20often%20inefficient%20due%20to%20the%20need%20for%20iterative%0Aupdates%20of%20costly%203D%20representations%2C%20such%20as%20neural%20radiance%20fields%2C%20either%0Athrough%20individual%20view%20edits%20or%20score%20distillation%20sampling.%20A%20major%0Adisadvantage%20of%20this%20approach%20is%20the%20slow%20convergence%20caused%20by%20aggregating%0Ainconsistent%20information%20across%20views%2C%20as%20the%20guidance%20from%202D%20models%20is%20not%0Amulti-view%20consistent.%20We%20thus%20introduce%20the%20Direct%20Gaussian%20Editor%20%28DGE%29%2C%20a%0Amethod%20that%20addresses%20these%20issues%20in%20two%20stages.%20First%2C%20we%20modify%20a%20given%0Ahigh-quality%20image%20editor%20like%20InstructPix2Pix%20to%20be%20multi-view%20consistent.%20To%0Ado%20so%2C%20we%20propose%20a%20training-free%20approach%20that%20integrates%20cues%20from%20the%203D%0Ageometry%20of%20the%20underlying%20scene.%20Second%2C%20given%20a%20multi-view%20consistent%20edited%0Asequence%20of%20images%2C%20we%20directly%20and%20efficiently%20optimize%20the%203D%20representation%2C%0Awhich%20is%20based%20on%203D%20Gaussian%20Splatting.%20Because%20it%20avoids%20incremental%20and%0Aiterative%20edits%2C%20DGE%20is%20significantly%20more%20accurate%20and%20efficient%20than%20existing%0Aapproaches%20and%20offers%20additional%20benefits%2C%20such%20as%20enabling%20selective%20editing%0Aof%20parts%20of%20the%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18929v2&entry.124074799=Read"},
{"title": "Context-Aware Indoor Point Cloud Object Generation through User\n  Instructions", "author": "Yiyang Luo and Ke Lin and Chao Gu", "abstract": "  Indoor scene modification has emerged as a prominent area within computer\nvision, particularly for its applications in Augmented Reality (AR) and Virtual\nReality (VR). Traditional methods often rely on pre-existing object databases\nand predetermined object positions, limiting their flexibility and adaptability\nto new scenarios. In response to this challenge, we present a novel end-to-end\nmulti-modal deep neural network capable of generating point cloud objects\nseamlessly integrated with their surroundings, driven by textual instructions.\nOur model revolutionizes scene modification by enabling the creation of new\nenvironments with previously unseen object layouts, eliminating the need for\npre-stored CAD models. Leveraging Point-E as our generative model, we introduce\ninnovative techniques such as quantized position prediction and Top-K\nestimation to address the issue of false negatives resulting from ambiguous\nlanguage descriptions. Furthermore, we conduct comprehensive evaluations to\nshowcase the diversity of generated objects, the efficacy of textual\ninstructions, and the quantitative metrics, affirming the realism and\nversatility of our model in generating indoor objects. To provide a holistic\nassessment, we incorporate visual grounding as an additional metric, ensuring\nthe quality and coherence of the scenes produced by our model. Through these\nadvancements, our approach not only advances the state-of-the-art in indoor\nscene modification but also lays the foundation for future innovations in\nimmersive computing and digital environment creation.\n", "link": "http://arxiv.org/abs/2311.16501v2", "date": "2024-07-22", "relevancy": 3.0728, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6161}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6138}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Indoor%20Point%20Cloud%20Object%20Generation%20through%20User%0A%20%20Instructions&body=Title%3A%20Context-Aware%20Indoor%20Point%20Cloud%20Object%20Generation%20through%20User%0A%20%20Instructions%0AAuthor%3A%20Yiyang%20Luo%20and%20Ke%20Lin%20and%20Chao%20Gu%0AAbstract%3A%20%20%20Indoor%20scene%20modification%20has%20emerged%20as%20a%20prominent%20area%20within%20computer%0Avision%2C%20particularly%20for%20its%20applications%20in%20Augmented%20Reality%20%28AR%29%20and%20Virtual%0AReality%20%28VR%29.%20Traditional%20methods%20often%20rely%20on%20pre-existing%20object%20databases%0Aand%20predetermined%20object%20positions%2C%20limiting%20their%20flexibility%20and%20adaptability%0Ato%20new%20scenarios.%20In%20response%20to%20this%20challenge%2C%20we%20present%20a%20novel%20end-to-end%0Amulti-modal%20deep%20neural%20network%20capable%20of%20generating%20point%20cloud%20objects%0Aseamlessly%20integrated%20with%20their%20surroundings%2C%20driven%20by%20textual%20instructions.%0AOur%20model%20revolutionizes%20scene%20modification%20by%20enabling%20the%20creation%20of%20new%0Aenvironments%20with%20previously%20unseen%20object%20layouts%2C%20eliminating%20the%20need%20for%0Apre-stored%20CAD%20models.%20Leveraging%20Point-E%20as%20our%20generative%20model%2C%20we%20introduce%0Ainnovative%20techniques%20such%20as%20quantized%20position%20prediction%20and%20Top-K%0Aestimation%20to%20address%20the%20issue%20of%20false%20negatives%20resulting%20from%20ambiguous%0Alanguage%20descriptions.%20Furthermore%2C%20we%20conduct%20comprehensive%20evaluations%20to%0Ashowcase%20the%20diversity%20of%20generated%20objects%2C%20the%20efficacy%20of%20textual%0Ainstructions%2C%20and%20the%20quantitative%20metrics%2C%20affirming%20the%20realism%20and%0Aversatility%20of%20our%20model%20in%20generating%20indoor%20objects.%20To%20provide%20a%20holistic%0Aassessment%2C%20we%20incorporate%20visual%20grounding%20as%20an%20additional%20metric%2C%20ensuring%0Athe%20quality%20and%20coherence%20of%20the%20scenes%20produced%20by%20our%20model.%20Through%20these%0Aadvancements%2C%20our%20approach%20not%20only%20advances%20the%20state-of-the-art%20in%20indoor%0Ascene%20modification%20but%20also%20lays%20the%20foundation%20for%20future%20innovations%20in%0Aimmersive%20computing%20and%20digital%20environment%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Indoor%2520Point%2520Cloud%2520Object%2520Generation%2520through%2520User%250A%2520%2520Instructions%26entry.906535625%3DYiyang%2520Luo%2520and%2520Ke%2520Lin%2520and%2520Chao%2520Gu%26entry.1292438233%3D%2520%2520Indoor%2520scene%2520modification%2520has%2520emerged%2520as%2520a%2520prominent%2520area%2520within%2520computer%250Avision%252C%2520particularly%2520for%2520its%2520applications%2520in%2520Augmented%2520Reality%2520%2528AR%2529%2520and%2520Virtual%250AReality%2520%2528VR%2529.%2520Traditional%2520methods%2520often%2520rely%2520on%2520pre-existing%2520object%2520databases%250Aand%2520predetermined%2520object%2520positions%252C%2520limiting%2520their%2520flexibility%2520and%2520adaptability%250Ato%2520new%2520scenarios.%2520In%2520response%2520to%2520this%2520challenge%252C%2520we%2520present%2520a%2520novel%2520end-to-end%250Amulti-modal%2520deep%2520neural%2520network%2520capable%2520of%2520generating%2520point%2520cloud%2520objects%250Aseamlessly%2520integrated%2520with%2520their%2520surroundings%252C%2520driven%2520by%2520textual%2520instructions.%250AOur%2520model%2520revolutionizes%2520scene%2520modification%2520by%2520enabling%2520the%2520creation%2520of%2520new%250Aenvironments%2520with%2520previously%2520unseen%2520object%2520layouts%252C%2520eliminating%2520the%2520need%2520for%250Apre-stored%2520CAD%2520models.%2520Leveraging%2520Point-E%2520as%2520our%2520generative%2520model%252C%2520we%2520introduce%250Ainnovative%2520techniques%2520such%2520as%2520quantized%2520position%2520prediction%2520and%2520Top-K%250Aestimation%2520to%2520address%2520the%2520issue%2520of%2520false%2520negatives%2520resulting%2520from%2520ambiguous%250Alanguage%2520descriptions.%2520Furthermore%252C%2520we%2520conduct%2520comprehensive%2520evaluations%2520to%250Ashowcase%2520the%2520diversity%2520of%2520generated%2520objects%252C%2520the%2520efficacy%2520of%2520textual%250Ainstructions%252C%2520and%2520the%2520quantitative%2520metrics%252C%2520affirming%2520the%2520realism%2520and%250Aversatility%2520of%2520our%2520model%2520in%2520generating%2520indoor%2520objects.%2520To%2520provide%2520a%2520holistic%250Aassessment%252C%2520we%2520incorporate%2520visual%2520grounding%2520as%2520an%2520additional%2520metric%252C%2520ensuring%250Athe%2520quality%2520and%2520coherence%2520of%2520the%2520scenes%2520produced%2520by%2520our%2520model.%2520Through%2520these%250Aadvancements%252C%2520our%2520approach%2520not%2520only%2520advances%2520the%2520state-of-the-art%2520in%2520indoor%250Ascene%2520modification%2520but%2520also%2520lays%2520the%2520foundation%2520for%2520future%2520innovations%2520in%250Aimmersive%2520computing%2520and%2520digital%2520environment%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Indoor%20Point%20Cloud%20Object%20Generation%20through%20User%0A%20%20Instructions&entry.906535625=Yiyang%20Luo%20and%20Ke%20Lin%20and%20Chao%20Gu&entry.1292438233=%20%20Indoor%20scene%20modification%20has%20emerged%20as%20a%20prominent%20area%20within%20computer%0Avision%2C%20particularly%20for%20its%20applications%20in%20Augmented%20Reality%20%28AR%29%20and%20Virtual%0AReality%20%28VR%29.%20Traditional%20methods%20often%20rely%20on%20pre-existing%20object%20databases%0Aand%20predetermined%20object%20positions%2C%20limiting%20their%20flexibility%20and%20adaptability%0Ato%20new%20scenarios.%20In%20response%20to%20this%20challenge%2C%20we%20present%20a%20novel%20end-to-end%0Amulti-modal%20deep%20neural%20network%20capable%20of%20generating%20point%20cloud%20objects%0Aseamlessly%20integrated%20with%20their%20surroundings%2C%20driven%20by%20textual%20instructions.%0AOur%20model%20revolutionizes%20scene%20modification%20by%20enabling%20the%20creation%20of%20new%0Aenvironments%20with%20previously%20unseen%20object%20layouts%2C%20eliminating%20the%20need%20for%0Apre-stored%20CAD%20models.%20Leveraging%20Point-E%20as%20our%20generative%20model%2C%20we%20introduce%0Ainnovative%20techniques%20such%20as%20quantized%20position%20prediction%20and%20Top-K%0Aestimation%20to%20address%20the%20issue%20of%20false%20negatives%20resulting%20from%20ambiguous%0Alanguage%20descriptions.%20Furthermore%2C%20we%20conduct%20comprehensive%20evaluations%20to%0Ashowcase%20the%20diversity%20of%20generated%20objects%2C%20the%20efficacy%20of%20textual%0Ainstructions%2C%20and%20the%20quantitative%20metrics%2C%20affirming%20the%20realism%20and%0Aversatility%20of%20our%20model%20in%20generating%20indoor%20objects.%20To%20provide%20a%20holistic%0Aassessment%2C%20we%20incorporate%20visual%20grounding%20as%20an%20additional%20metric%2C%20ensuring%0Athe%20quality%20and%20coherence%20of%20the%20scenes%20produced%20by%20our%20model.%20Through%20these%0Aadvancements%2C%20our%20approach%20not%20only%20advances%20the%20state-of-the-art%20in%20indoor%0Ascene%20modification%20but%20also%20lays%20the%20foundation%20for%20future%20innovations%20in%0Aimmersive%20computing%20and%20digital%20environment%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16501v2&entry.124074799=Read"},
{"title": "SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization\n  in Unseen Environments", "author": "Niklas Gard and Anna Hilsmann and Peter Eisert", "abstract": "  In this paper, we present SPVLoc, a global indoor localization method that\naccurately determines the six-dimensional (6D) camera pose of a query image and\nrequires minimal scene-specific prior knowledge and no scene-specific training.\nOur approach employs a novel matching procedure to localize the perspective\ncamera's viewport, given as an RGB image, within a set of panoramic semantic\nlayout representations of the indoor environment. The panoramas are rendered\nfrom an untextured 3D reference model, which only comprises approximate\nstructural information about room shapes, along with door and window\nannotations. We demonstrate that a straightforward convolutional network\nstructure can successfully achieve image-to-panorama and ultimately\nimage-to-model matching. Through a viewport classification score, we rank\nreference panoramas and select the best match for the query image. Then, a 6D\nrelative pose is estimated between the chosen panorama and query image. Our\nexperiments demonstrate that this approach not only efficiently bridges the\ndomain gap but also generalizes well to previously unseen scenes that are not\npart of the training data. Moreover, it achieves superior localization accuracy\ncompared to the state of the art methods and also estimates more degrees of\nfreedom of the camera pose. Our source code is publicly available at\nhttps://fraunhoferhhi.github.io/spvloc .\n", "link": "http://arxiv.org/abs/2404.10527v2", "date": "2024-07-22", "relevancy": 3.0571, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6491}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.622}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPVLoc%3A%20Semantic%20Panoramic%20Viewport%20Matching%20for%206D%20Camera%20Localization%0A%20%20in%20Unseen%20Environments&body=Title%3A%20SPVLoc%3A%20Semantic%20Panoramic%20Viewport%20Matching%20for%206D%20Camera%20Localization%0A%20%20in%20Unseen%20Environments%0AAuthor%3A%20Niklas%20Gard%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20SPVLoc%2C%20a%20global%20indoor%20localization%20method%20that%0Aaccurately%20determines%20the%20six-dimensional%20%286D%29%20camera%20pose%20of%20a%20query%20image%20and%0Arequires%20minimal%20scene-specific%20prior%20knowledge%20and%20no%20scene-specific%20training.%0AOur%20approach%20employs%20a%20novel%20matching%20procedure%20to%20localize%20the%20perspective%0Acamera%27s%20viewport%2C%20given%20as%20an%20RGB%20image%2C%20within%20a%20set%20of%20panoramic%20semantic%0Alayout%20representations%20of%20the%20indoor%20environment.%20The%20panoramas%20are%20rendered%0Afrom%20an%20untextured%203D%20reference%20model%2C%20which%20only%20comprises%20approximate%0Astructural%20information%20about%20room%20shapes%2C%20along%20with%20door%20and%20window%0Aannotations.%20We%20demonstrate%20that%20a%20straightforward%20convolutional%20network%0Astructure%20can%20successfully%20achieve%20image-to-panorama%20and%20ultimately%0Aimage-to-model%20matching.%20Through%20a%20viewport%20classification%20score%2C%20we%20rank%0Areference%20panoramas%20and%20select%20the%20best%20match%20for%20the%20query%20image.%20Then%2C%20a%206D%0Arelative%20pose%20is%20estimated%20between%20the%20chosen%20panorama%20and%20query%20image.%20Our%0Aexperiments%20demonstrate%20that%20this%20approach%20not%20only%20efficiently%20bridges%20the%0Adomain%20gap%20but%20also%20generalizes%20well%20to%20previously%20unseen%20scenes%20that%20are%20not%0Apart%20of%20the%20training%20data.%20Moreover%2C%20it%20achieves%20superior%20localization%20accuracy%0Acompared%20to%20the%20state%20of%20the%20art%20methods%20and%20also%20estimates%20more%20degrees%20of%0Afreedom%20of%20the%20camera%20pose.%20Our%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//fraunhoferhhi.github.io/spvloc%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPVLoc%253A%2520Semantic%2520Panoramic%2520Viewport%2520Matching%2520for%25206D%2520Camera%2520Localization%250A%2520%2520in%2520Unseen%2520Environments%26entry.906535625%3DNiklas%2520Gard%2520and%2520Anna%2520Hilsmann%2520and%2520Peter%2520Eisert%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520SPVLoc%252C%2520a%2520global%2520indoor%2520localization%2520method%2520that%250Aaccurately%2520determines%2520the%2520six-dimensional%2520%25286D%2529%2520camera%2520pose%2520of%2520a%2520query%2520image%2520and%250Arequires%2520minimal%2520scene-specific%2520prior%2520knowledge%2520and%2520no%2520scene-specific%2520training.%250AOur%2520approach%2520employs%2520a%2520novel%2520matching%2520procedure%2520to%2520localize%2520the%2520perspective%250Acamera%2527s%2520viewport%252C%2520given%2520as%2520an%2520RGB%2520image%252C%2520within%2520a%2520set%2520of%2520panoramic%2520semantic%250Alayout%2520representations%2520of%2520the%2520indoor%2520environment.%2520The%2520panoramas%2520are%2520rendered%250Afrom%2520an%2520untextured%25203D%2520reference%2520model%252C%2520which%2520only%2520comprises%2520approximate%250Astructural%2520information%2520about%2520room%2520shapes%252C%2520along%2520with%2520door%2520and%2520window%250Aannotations.%2520We%2520demonstrate%2520that%2520a%2520straightforward%2520convolutional%2520network%250Astructure%2520can%2520successfully%2520achieve%2520image-to-panorama%2520and%2520ultimately%250Aimage-to-model%2520matching.%2520Through%2520a%2520viewport%2520classification%2520score%252C%2520we%2520rank%250Areference%2520panoramas%2520and%2520select%2520the%2520best%2520match%2520for%2520the%2520query%2520image.%2520Then%252C%2520a%25206D%250Arelative%2520pose%2520is%2520estimated%2520between%2520the%2520chosen%2520panorama%2520and%2520query%2520image.%2520Our%250Aexperiments%2520demonstrate%2520that%2520this%2520approach%2520not%2520only%2520efficiently%2520bridges%2520the%250Adomain%2520gap%2520but%2520also%2520generalizes%2520well%2520to%2520previously%2520unseen%2520scenes%2520that%2520are%2520not%250Apart%2520of%2520the%2520training%2520data.%2520Moreover%252C%2520it%2520achieves%2520superior%2520localization%2520accuracy%250Acompared%2520to%2520the%2520state%2520of%2520the%2520art%2520methods%2520and%2520also%2520estimates%2520more%2520degrees%2520of%250Afreedom%2520of%2520the%2520camera%2520pose.%2520Our%2520source%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//fraunhoferhhi.github.io/spvloc%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPVLoc%3A%20Semantic%20Panoramic%20Viewport%20Matching%20for%206D%20Camera%20Localization%0A%20%20in%20Unseen%20Environments&entry.906535625=Niklas%20Gard%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20SPVLoc%2C%20a%20global%20indoor%20localization%20method%20that%0Aaccurately%20determines%20the%20six-dimensional%20%286D%29%20camera%20pose%20of%20a%20query%20image%20and%0Arequires%20minimal%20scene-specific%20prior%20knowledge%20and%20no%20scene-specific%20training.%0AOur%20approach%20employs%20a%20novel%20matching%20procedure%20to%20localize%20the%20perspective%0Acamera%27s%20viewport%2C%20given%20as%20an%20RGB%20image%2C%20within%20a%20set%20of%20panoramic%20semantic%0Alayout%20representations%20of%20the%20indoor%20environment.%20The%20panoramas%20are%20rendered%0Afrom%20an%20untextured%203D%20reference%20model%2C%20which%20only%20comprises%20approximate%0Astructural%20information%20about%20room%20shapes%2C%20along%20with%20door%20and%20window%0Aannotations.%20We%20demonstrate%20that%20a%20straightforward%20convolutional%20network%0Astructure%20can%20successfully%20achieve%20image-to-panorama%20and%20ultimately%0Aimage-to-model%20matching.%20Through%20a%20viewport%20classification%20score%2C%20we%20rank%0Areference%20panoramas%20and%20select%20the%20best%20match%20for%20the%20query%20image.%20Then%2C%20a%206D%0Arelative%20pose%20is%20estimated%20between%20the%20chosen%20panorama%20and%20query%20image.%20Our%0Aexperiments%20demonstrate%20that%20this%20approach%20not%20only%20efficiently%20bridges%20the%0Adomain%20gap%20but%20also%20generalizes%20well%20to%20previously%20unseen%20scenes%20that%20are%20not%0Apart%20of%20the%20training%20data.%20Moreover%2C%20it%20achieves%20superior%20localization%20accuracy%0Acompared%20to%20the%20state%20of%20the%20art%20methods%20and%20also%20estimates%20more%20degrees%20of%0Afreedom%20of%20the%20camera%20pose.%20Our%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//fraunhoferhhi.github.io/spvloc%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10527v2&entry.124074799=Read"},
{"title": "HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global\n  Positioning", "author": "Eugene Valassakis and Guillermo Garcia-Hernando", "abstract": "  Predicting camera-space hand meshes from single RGB images is crucial for\nenabling realistic hand interactions in 3D virtual and augmented worlds.\nPrevious work typically divided the task into two stages: given a cropped image\nof the hand, predict meshes in relative coordinates, followed by lifting these\npredictions into camera space in a separate and independent stage, often\nresulting in the loss of valuable contextual and scale information. To prevent\nthe loss of these cues, we propose unifying these two stages into an end-to-end\nsolution that addresses the 2D-3D correspondence problem. This solution enables\nback-propagation from camera space outputs to the rest of the network through a\nnew differentiable global positioning module. We also introduce an image\nrectification step that harmonizes both the training dataset and the input\nimage as if they were acquired with the same camera, helping to alleviate the\ninherent scale-depth ambiguity of the problem. We validate the effectiveness of\nour framework in evaluations against several baselines and state-of-the-art\napproaches across three public benchmarks.\n", "link": "http://arxiv.org/abs/2407.15844v1", "date": "2024-07-22", "relevancy": 3.0346, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6169}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6074}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HandDGP%3A%20Camera-Space%20Hand%20Mesh%20Prediction%20with%20Differentiable%20Global%0A%20%20Positioning&body=Title%3A%20HandDGP%3A%20Camera-Space%20Hand%20Mesh%20Prediction%20with%20Differentiable%20Global%0A%20%20Positioning%0AAuthor%3A%20Eugene%20Valassakis%20and%20Guillermo%20Garcia-Hernando%0AAbstract%3A%20%20%20Predicting%20camera-space%20hand%20meshes%20from%20single%20RGB%20images%20is%20crucial%20for%0Aenabling%20realistic%20hand%20interactions%20in%203D%20virtual%20and%20augmented%20worlds.%0APrevious%20work%20typically%20divided%20the%20task%20into%20two%20stages%3A%20given%20a%20cropped%20image%0Aof%20the%20hand%2C%20predict%20meshes%20in%20relative%20coordinates%2C%20followed%20by%20lifting%20these%0Apredictions%20into%20camera%20space%20in%20a%20separate%20and%20independent%20stage%2C%20often%0Aresulting%20in%20the%20loss%20of%20valuable%20contextual%20and%20scale%20information.%20To%20prevent%0Athe%20loss%20of%20these%20cues%2C%20we%20propose%20unifying%20these%20two%20stages%20into%20an%20end-to-end%0Asolution%20that%20addresses%20the%202D-3D%20correspondence%20problem.%20This%20solution%20enables%0Aback-propagation%20from%20camera%20space%20outputs%20to%20the%20rest%20of%20the%20network%20through%20a%0Anew%20differentiable%20global%20positioning%20module.%20We%20also%20introduce%20an%20image%0Arectification%20step%20that%20harmonizes%20both%20the%20training%20dataset%20and%20the%20input%0Aimage%20as%20if%20they%20were%20acquired%20with%20the%20same%20camera%2C%20helping%20to%20alleviate%20the%0Ainherent%20scale-depth%20ambiguity%20of%20the%20problem.%20We%20validate%20the%20effectiveness%20of%0Aour%20framework%20in%20evaluations%20against%20several%20baselines%20and%20state-of-the-art%0Aapproaches%20across%20three%20public%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandDGP%253A%2520Camera-Space%2520Hand%2520Mesh%2520Prediction%2520with%2520Differentiable%2520Global%250A%2520%2520Positioning%26entry.906535625%3DEugene%2520Valassakis%2520and%2520Guillermo%2520Garcia-Hernando%26entry.1292438233%3D%2520%2520Predicting%2520camera-space%2520hand%2520meshes%2520from%2520single%2520RGB%2520images%2520is%2520crucial%2520for%250Aenabling%2520realistic%2520hand%2520interactions%2520in%25203D%2520virtual%2520and%2520augmented%2520worlds.%250APrevious%2520work%2520typically%2520divided%2520the%2520task%2520into%2520two%2520stages%253A%2520given%2520a%2520cropped%2520image%250Aof%2520the%2520hand%252C%2520predict%2520meshes%2520in%2520relative%2520coordinates%252C%2520followed%2520by%2520lifting%2520these%250Apredictions%2520into%2520camera%2520space%2520in%2520a%2520separate%2520and%2520independent%2520stage%252C%2520often%250Aresulting%2520in%2520the%2520loss%2520of%2520valuable%2520contextual%2520and%2520scale%2520information.%2520To%2520prevent%250Athe%2520loss%2520of%2520these%2520cues%252C%2520we%2520propose%2520unifying%2520these%2520two%2520stages%2520into%2520an%2520end-to-end%250Asolution%2520that%2520addresses%2520the%25202D-3D%2520correspondence%2520problem.%2520This%2520solution%2520enables%250Aback-propagation%2520from%2520camera%2520space%2520outputs%2520to%2520the%2520rest%2520of%2520the%2520network%2520through%2520a%250Anew%2520differentiable%2520global%2520positioning%2520module.%2520We%2520also%2520introduce%2520an%2520image%250Arectification%2520step%2520that%2520harmonizes%2520both%2520the%2520training%2520dataset%2520and%2520the%2520input%250Aimage%2520as%2520if%2520they%2520were%2520acquired%2520with%2520the%2520same%2520camera%252C%2520helping%2520to%2520alleviate%2520the%250Ainherent%2520scale-depth%2520ambiguity%2520of%2520the%2520problem.%2520We%2520validate%2520the%2520effectiveness%2520of%250Aour%2520framework%2520in%2520evaluations%2520against%2520several%2520baselines%2520and%2520state-of-the-art%250Aapproaches%2520across%2520three%2520public%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HandDGP%3A%20Camera-Space%20Hand%20Mesh%20Prediction%20with%20Differentiable%20Global%0A%20%20Positioning&entry.906535625=Eugene%20Valassakis%20and%20Guillermo%20Garcia-Hernando&entry.1292438233=%20%20Predicting%20camera-space%20hand%20meshes%20from%20single%20RGB%20images%20is%20crucial%20for%0Aenabling%20realistic%20hand%20interactions%20in%203D%20virtual%20and%20augmented%20worlds.%0APrevious%20work%20typically%20divided%20the%20task%20into%20two%20stages%3A%20given%20a%20cropped%20image%0Aof%20the%20hand%2C%20predict%20meshes%20in%20relative%20coordinates%2C%20followed%20by%20lifting%20these%0Apredictions%20into%20camera%20space%20in%20a%20separate%20and%20independent%20stage%2C%20often%0Aresulting%20in%20the%20loss%20of%20valuable%20contextual%20and%20scale%20information.%20To%20prevent%0Athe%20loss%20of%20these%20cues%2C%20we%20propose%20unifying%20these%20two%20stages%20into%20an%20end-to-end%0Asolution%20that%20addresses%20the%202D-3D%20correspondence%20problem.%20This%20solution%20enables%0Aback-propagation%20from%20camera%20space%20outputs%20to%20the%20rest%20of%20the%20network%20through%20a%0Anew%20differentiable%20global%20positioning%20module.%20We%20also%20introduce%20an%20image%0Arectification%20step%20that%20harmonizes%20both%20the%20training%20dataset%20and%20the%20input%0Aimage%20as%20if%20they%20were%20acquired%20with%20the%20same%20camera%2C%20helping%20to%20alleviate%20the%0Ainherent%20scale-depth%20ambiguity%20of%20the%20problem.%20We%20validate%20the%20effectiveness%20of%0Aour%20framework%20in%20evaluations%20against%20several%20baselines%20and%20state-of-the-art%0Aapproaches%20across%20three%20public%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15844v1&entry.124074799=Read"},
{"title": "RADA: Robust and Accurate Feature Learning with Domain Adaptation", "author": "Jingtai He and Gehao Zhang and Tingting Liu and Songlin Du", "abstract": "  Recent advancements in keypoint detection and descriptor extraction have\nshown impressive performance in local feature learning tasks. However, existing\nmethods generally exhibit suboptimal performance under extreme conditions such\nas significant appearance changes and domain shifts. In this study, we\nintroduce a multi-level feature aggregation network that incorporates two\npivotal components to facilitate the learning of robust and accurate features\nwith domain adaptation. First, we employ domain adaptation supervision to align\nhigh-level feature distributions across different domains to achieve invariant\ndomain representations. Second, we propose a Transformer-based booster that\nenhances descriptor robustness by integrating visual and geometric information\nthrough wave position encoding concepts, effectively handling complex\nconditions. To ensure the accuracy and robustness of features, we adopt a\nhierarchical architecture to capture comprehensive information and apply\nmeticulous targeted supervision to keypoint detection, descriptor extraction,\nand their coupled processing. Extensive experiments demonstrate that our\nmethod, RADA, achieves excellent results in image matching, camera pose\nestimation, and visual localization tasks.\n", "link": "http://arxiv.org/abs/2407.15791v1", "date": "2024-07-22", "relevancy": 3.0313, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6368}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6009}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RADA%3A%20Robust%20and%20Accurate%20Feature%20Learning%20with%20Domain%20Adaptation&body=Title%3A%20RADA%3A%20Robust%20and%20Accurate%20Feature%20Learning%20with%20Domain%20Adaptation%0AAuthor%3A%20Jingtai%20He%20and%20Gehao%20Zhang%20and%20Tingting%20Liu%20and%20Songlin%20Du%0AAbstract%3A%20%20%20Recent%20advancements%20in%20keypoint%20detection%20and%20descriptor%20extraction%20have%0Ashown%20impressive%20performance%20in%20local%20feature%20learning%20tasks.%20However%2C%20existing%0Amethods%20generally%20exhibit%20suboptimal%20performance%20under%20extreme%20conditions%20such%0Aas%20significant%20appearance%20changes%20and%20domain%20shifts.%20In%20this%20study%2C%20we%0Aintroduce%20a%20multi-level%20feature%20aggregation%20network%20that%20incorporates%20two%0Apivotal%20components%20to%20facilitate%20the%20learning%20of%20robust%20and%20accurate%20features%0Awith%20domain%20adaptation.%20First%2C%20we%20employ%20domain%20adaptation%20supervision%20to%20align%0Ahigh-level%20feature%20distributions%20across%20different%20domains%20to%20achieve%20invariant%0Adomain%20representations.%20Second%2C%20we%20propose%20a%20Transformer-based%20booster%20that%0Aenhances%20descriptor%20robustness%20by%20integrating%20visual%20and%20geometric%20information%0Athrough%20wave%20position%20encoding%20concepts%2C%20effectively%20handling%20complex%0Aconditions.%20To%20ensure%20the%20accuracy%20and%20robustness%20of%20features%2C%20we%20adopt%20a%0Ahierarchical%20architecture%20to%20capture%20comprehensive%20information%20and%20apply%0Ameticulous%20targeted%20supervision%20to%20keypoint%20detection%2C%20descriptor%20extraction%2C%0Aand%20their%20coupled%20processing.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%2C%20RADA%2C%20achieves%20excellent%20results%20in%20image%20matching%2C%20camera%20pose%0Aestimation%2C%20and%20visual%20localization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRADA%253A%2520Robust%2520and%2520Accurate%2520Feature%2520Learning%2520with%2520Domain%2520Adaptation%26entry.906535625%3DJingtai%2520He%2520and%2520Gehao%2520Zhang%2520and%2520Tingting%2520Liu%2520and%2520Songlin%2520Du%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520keypoint%2520detection%2520and%2520descriptor%2520extraction%2520have%250Ashown%2520impressive%2520performance%2520in%2520local%2520feature%2520learning%2520tasks.%2520However%252C%2520existing%250Amethods%2520generally%2520exhibit%2520suboptimal%2520performance%2520under%2520extreme%2520conditions%2520such%250Aas%2520significant%2520appearance%2520changes%2520and%2520domain%2520shifts.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520a%2520multi-level%2520feature%2520aggregation%2520network%2520that%2520incorporates%2520two%250Apivotal%2520components%2520to%2520facilitate%2520the%2520learning%2520of%2520robust%2520and%2520accurate%2520features%250Awith%2520domain%2520adaptation.%2520First%252C%2520we%2520employ%2520domain%2520adaptation%2520supervision%2520to%2520align%250Ahigh-level%2520feature%2520distributions%2520across%2520different%2520domains%2520to%2520achieve%2520invariant%250Adomain%2520representations.%2520Second%252C%2520we%2520propose%2520a%2520Transformer-based%2520booster%2520that%250Aenhances%2520descriptor%2520robustness%2520by%2520integrating%2520visual%2520and%2520geometric%2520information%250Athrough%2520wave%2520position%2520encoding%2520concepts%252C%2520effectively%2520handling%2520complex%250Aconditions.%2520To%2520ensure%2520the%2520accuracy%2520and%2520robustness%2520of%2520features%252C%2520we%2520adopt%2520a%250Ahierarchical%2520architecture%2520to%2520capture%2520comprehensive%2520information%2520and%2520apply%250Ameticulous%2520targeted%2520supervision%2520to%2520keypoint%2520detection%252C%2520descriptor%2520extraction%252C%250Aand%2520their%2520coupled%2520processing.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%252C%2520RADA%252C%2520achieves%2520excellent%2520results%2520in%2520image%2520matching%252C%2520camera%2520pose%250Aestimation%252C%2520and%2520visual%2520localization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RADA%3A%20Robust%20and%20Accurate%20Feature%20Learning%20with%20Domain%20Adaptation&entry.906535625=Jingtai%20He%20and%20Gehao%20Zhang%20and%20Tingting%20Liu%20and%20Songlin%20Du&entry.1292438233=%20%20Recent%20advancements%20in%20keypoint%20detection%20and%20descriptor%20extraction%20have%0Ashown%20impressive%20performance%20in%20local%20feature%20learning%20tasks.%20However%2C%20existing%0Amethods%20generally%20exhibit%20suboptimal%20performance%20under%20extreme%20conditions%20such%0Aas%20significant%20appearance%20changes%20and%20domain%20shifts.%20In%20this%20study%2C%20we%0Aintroduce%20a%20multi-level%20feature%20aggregation%20network%20that%20incorporates%20two%0Apivotal%20components%20to%20facilitate%20the%20learning%20of%20robust%20and%20accurate%20features%0Awith%20domain%20adaptation.%20First%2C%20we%20employ%20domain%20adaptation%20supervision%20to%20align%0Ahigh-level%20feature%20distributions%20across%20different%20domains%20to%20achieve%20invariant%0Adomain%20representations.%20Second%2C%20we%20propose%20a%20Transformer-based%20booster%20that%0Aenhances%20descriptor%20robustness%20by%20integrating%20visual%20and%20geometric%20information%0Athrough%20wave%20position%20encoding%20concepts%2C%20effectively%20handling%20complex%0Aconditions.%20To%20ensure%20the%20accuracy%20and%20robustness%20of%20features%2C%20we%20adopt%20a%0Ahierarchical%20architecture%20to%20capture%20comprehensive%20information%20and%20apply%0Ameticulous%20targeted%20supervision%20to%20keypoint%20detection%2C%20descriptor%20extraction%2C%0Aand%20their%20coupled%20processing.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%2C%20RADA%2C%20achieves%20excellent%20results%20in%20image%20matching%2C%20camera%20pose%0Aestimation%2C%20and%20visual%20localization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15791v1&entry.124074799=Read"},
{"title": "Towards Latent Masked Image Modeling for Self-Supervised Visual\n  Representation Learning", "author": "Yibing Wei and Abhinav Gupta and Pedro Morgado", "abstract": "  Masked Image Modeling (MIM) has emerged as a promising method for deriving\nvisual representations from unlabeled image data by predicting missing pixels\nfrom masked portions of images. It excels in region-aware learning and provides\nstrong initializations for various tasks, but struggles to capture high-level\nsemantics without further supervised fine-tuning, likely due to the low-level\nnature of its pixel reconstruction objective. A promising yet unrealized\nframework is learning representations through masked reconstruction in latent\nspace, combining the locality of MIM with the high-level targets. However, this\napproach poses significant training challenges as the reconstruction targets\nare learned in conjunction with the model, potentially leading to trivial or\nsuboptimal solutions.Our study is among the first to thoroughly analyze and\naddress the challenges of such framework, which we refer to as Latent MIM.\nThrough a series of carefully designed experiments and extensive analysis, we\nidentify the source of these challenges, including representation collapsing\nfor joint online/target optimization, learning objectives, the high region\ncorrelation in latent space and decoding conditioning. By sequentially\naddressing these issues, we demonstrate that Latent MIM can indeed learn\nhigh-level representations while retaining the benefits of MIM models.\n", "link": "http://arxiv.org/abs/2407.15837v1", "date": "2024-07-22", "relevancy": 2.9842, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5857}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Latent%20Masked%20Image%20Modeling%20for%20Self-Supervised%20Visual%0A%20%20Representation%20Learning&body=Title%3A%20Towards%20Latent%20Masked%20Image%20Modeling%20for%20Self-Supervised%20Visual%0A%20%20Representation%20Learning%0AAuthor%3A%20Yibing%20Wei%20and%20Abhinav%20Gupta%20and%20Pedro%20Morgado%0AAbstract%3A%20%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20emerged%20as%20a%20promising%20method%20for%20deriving%0Avisual%20representations%20from%20unlabeled%20image%20data%20by%20predicting%20missing%20pixels%0Afrom%20masked%20portions%20of%20images.%20It%20excels%20in%20region-aware%20learning%20and%20provides%0Astrong%20initializations%20for%20various%20tasks%2C%20but%20struggles%20to%20capture%20high-level%0Asemantics%20without%20further%20supervised%20fine-tuning%2C%20likely%20due%20to%20the%20low-level%0Anature%20of%20its%20pixel%20reconstruction%20objective.%20A%20promising%20yet%20unrealized%0Aframework%20is%20learning%20representations%20through%20masked%20reconstruction%20in%20latent%0Aspace%2C%20combining%20the%20locality%20of%20MIM%20with%20the%20high-level%20targets.%20However%2C%20this%0Aapproach%20poses%20significant%20training%20challenges%20as%20the%20reconstruction%20targets%0Aare%20learned%20in%20conjunction%20with%20the%20model%2C%20potentially%20leading%20to%20trivial%20or%0Asuboptimal%20solutions.Our%20study%20is%20among%20the%20first%20to%20thoroughly%20analyze%20and%0Aaddress%20the%20challenges%20of%20such%20framework%2C%20which%20we%20refer%20to%20as%20Latent%20MIM.%0AThrough%20a%20series%20of%20carefully%20designed%20experiments%20and%20extensive%20analysis%2C%20we%0Aidentify%20the%20source%20of%20these%20challenges%2C%20including%20representation%20collapsing%0Afor%20joint%20online/target%20optimization%2C%20learning%20objectives%2C%20the%20high%20region%0Acorrelation%20in%20latent%20space%20and%20decoding%20conditioning.%20By%20sequentially%0Aaddressing%20these%20issues%2C%20we%20demonstrate%20that%20Latent%20MIM%20can%20indeed%20learn%0Ahigh-level%20representations%20while%20retaining%20the%20benefits%20of%20MIM%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Latent%2520Masked%2520Image%2520Modeling%2520for%2520Self-Supervised%2520Visual%250A%2520%2520Representation%2520Learning%26entry.906535625%3DYibing%2520Wei%2520and%2520Abhinav%2520Gupta%2520and%2520Pedro%2520Morgado%26entry.1292438233%3D%2520%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%2520has%2520emerged%2520as%2520a%2520promising%2520method%2520for%2520deriving%250Avisual%2520representations%2520from%2520unlabeled%2520image%2520data%2520by%2520predicting%2520missing%2520pixels%250Afrom%2520masked%2520portions%2520of%2520images.%2520It%2520excels%2520in%2520region-aware%2520learning%2520and%2520provides%250Astrong%2520initializations%2520for%2520various%2520tasks%252C%2520but%2520struggles%2520to%2520capture%2520high-level%250Asemantics%2520without%2520further%2520supervised%2520fine-tuning%252C%2520likely%2520due%2520to%2520the%2520low-level%250Anature%2520of%2520its%2520pixel%2520reconstruction%2520objective.%2520A%2520promising%2520yet%2520unrealized%250Aframework%2520is%2520learning%2520representations%2520through%2520masked%2520reconstruction%2520in%2520latent%250Aspace%252C%2520combining%2520the%2520locality%2520of%2520MIM%2520with%2520the%2520high-level%2520targets.%2520However%252C%2520this%250Aapproach%2520poses%2520significant%2520training%2520challenges%2520as%2520the%2520reconstruction%2520targets%250Aare%2520learned%2520in%2520conjunction%2520with%2520the%2520model%252C%2520potentially%2520leading%2520to%2520trivial%2520or%250Asuboptimal%2520solutions.Our%2520study%2520is%2520among%2520the%2520first%2520to%2520thoroughly%2520analyze%2520and%250Aaddress%2520the%2520challenges%2520of%2520such%2520framework%252C%2520which%2520we%2520refer%2520to%2520as%2520Latent%2520MIM.%250AThrough%2520a%2520series%2520of%2520carefully%2520designed%2520experiments%2520and%2520extensive%2520analysis%252C%2520we%250Aidentify%2520the%2520source%2520of%2520these%2520challenges%252C%2520including%2520representation%2520collapsing%250Afor%2520joint%2520online/target%2520optimization%252C%2520learning%2520objectives%252C%2520the%2520high%2520region%250Acorrelation%2520in%2520latent%2520space%2520and%2520decoding%2520conditioning.%2520By%2520sequentially%250Aaddressing%2520these%2520issues%252C%2520we%2520demonstrate%2520that%2520Latent%2520MIM%2520can%2520indeed%2520learn%250Ahigh-level%2520representations%2520while%2520retaining%2520the%2520benefits%2520of%2520MIM%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Latent%20Masked%20Image%20Modeling%20for%20Self-Supervised%20Visual%0A%20%20Representation%20Learning&entry.906535625=Yibing%20Wei%20and%20Abhinav%20Gupta%20and%20Pedro%20Morgado&entry.1292438233=%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20emerged%20as%20a%20promising%20method%20for%20deriving%0Avisual%20representations%20from%20unlabeled%20image%20data%20by%20predicting%20missing%20pixels%0Afrom%20masked%20portions%20of%20images.%20It%20excels%20in%20region-aware%20learning%20and%20provides%0Astrong%20initializations%20for%20various%20tasks%2C%20but%20struggles%20to%20capture%20high-level%0Asemantics%20without%20further%20supervised%20fine-tuning%2C%20likely%20due%20to%20the%20low-level%0Anature%20of%20its%20pixel%20reconstruction%20objective.%20A%20promising%20yet%20unrealized%0Aframework%20is%20learning%20representations%20through%20masked%20reconstruction%20in%20latent%0Aspace%2C%20combining%20the%20locality%20of%20MIM%20with%20the%20high-level%20targets.%20However%2C%20this%0Aapproach%20poses%20significant%20training%20challenges%20as%20the%20reconstruction%20targets%0Aare%20learned%20in%20conjunction%20with%20the%20model%2C%20potentially%20leading%20to%20trivial%20or%0Asuboptimal%20solutions.Our%20study%20is%20among%20the%20first%20to%20thoroughly%20analyze%20and%0Aaddress%20the%20challenges%20of%20such%20framework%2C%20which%20we%20refer%20to%20as%20Latent%20MIM.%0AThrough%20a%20series%20of%20carefully%20designed%20experiments%20and%20extensive%20analysis%2C%20we%0Aidentify%20the%20source%20of%20these%20challenges%2C%20including%20representation%20collapsing%0Afor%20joint%20online/target%20optimization%2C%20learning%20objectives%2C%20the%20high%20region%0Acorrelation%20in%20latent%20space%20and%20decoding%20conditioning.%20By%20sequentially%0Aaddressing%20these%20issues%2C%20we%20demonstrate%20that%20Latent%20MIM%20can%20indeed%20learn%0Ahigh-level%20representations%20while%20retaining%20the%20benefits%20of%20MIM%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15837v1&entry.124074799=Read"},
{"title": "Visual-Semantic Decomposition and Partial Alignment for Document-based\n  Zero-Shot Learning", "author": "Xiangyan Qu and Jing Yu and Keke Gai and Jiamin Zhuang and Yuanmin Tang and Gang Xiong and Gaopeng Gou and Qi Wu", "abstract": "  Recent work shows that documents from encyclopedias serve as helpful\nauxiliary information for zero-shot learning. Existing methods align the entire\nsemantics of a document with corresponding images to transfer knowledge.\nHowever, they disregard that semantic information is not equivalent between\nthem, resulting in a suboptimal alignment. In this work, we propose a novel\nnetwork to extract multi-view semantic concepts from documents and images and\nalign the matching rather than entire concepts. Specifically, we propose a\nsemantic decomposition module to generate multi-view semantic embeddings from\nvisual and textual sides, providing the basic concepts for partial alignment.\nTo alleviate the issue of information redundancy among embeddings, we propose\nthe local-to-semantic variance loss to capture distinct local details and\nmultiple semantic diversity loss to enforce orthogonality among embeddings.\nSubsequently, two losses are introduced to partially align visual-semantic\nembedding pairs according to their semantic relevance at the view and\nword-to-patch levels. Consequently, we consistently outperform state-of-the-art\nmethods under two document sources in three standard benchmarks for\ndocument-based zero-shot learning. Qualitatively, we show that our model learns\nthe interpretable partial association.\n", "link": "http://arxiv.org/abs/2407.15613v1", "date": "2024-07-22", "relevancy": 2.8383, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5705}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5668}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-Semantic%20Decomposition%20and%20Partial%20Alignment%20for%20Document-based%0A%20%20Zero-Shot%20Learning&body=Title%3A%20Visual-Semantic%20Decomposition%20and%20Partial%20Alignment%20for%20Document-based%0A%20%20Zero-Shot%20Learning%0AAuthor%3A%20Xiangyan%20Qu%20and%20Jing%20Yu%20and%20Keke%20Gai%20and%20Jiamin%20Zhuang%20and%20Yuanmin%20Tang%20and%20Gang%20Xiong%20and%20Gaopeng%20Gou%20and%20Qi%20Wu%0AAbstract%3A%20%20%20Recent%20work%20shows%20that%20documents%20from%20encyclopedias%20serve%20as%20helpful%0Aauxiliary%20information%20for%20zero-shot%20learning.%20Existing%20methods%20align%20the%20entire%0Asemantics%20of%20a%20document%20with%20corresponding%20images%20to%20transfer%20knowledge.%0AHowever%2C%20they%20disregard%20that%20semantic%20information%20is%20not%20equivalent%20between%0Athem%2C%20resulting%20in%20a%20suboptimal%20alignment.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Anetwork%20to%20extract%20multi-view%20semantic%20concepts%20from%20documents%20and%20images%20and%0Aalign%20the%20matching%20rather%20than%20entire%20concepts.%20Specifically%2C%20we%20propose%20a%0Asemantic%20decomposition%20module%20to%20generate%20multi-view%20semantic%20embeddings%20from%0Avisual%20and%20textual%20sides%2C%20providing%20the%20basic%20concepts%20for%20partial%20alignment.%0ATo%20alleviate%20the%20issue%20of%20information%20redundancy%20among%20embeddings%2C%20we%20propose%0Athe%20local-to-semantic%20variance%20loss%20to%20capture%20distinct%20local%20details%20and%0Amultiple%20semantic%20diversity%20loss%20to%20enforce%20orthogonality%20among%20embeddings.%0ASubsequently%2C%20two%20losses%20are%20introduced%20to%20partially%20align%20visual-semantic%0Aembedding%20pairs%20according%20to%20their%20semantic%20relevance%20at%20the%20view%20and%0Aword-to-patch%20levels.%20Consequently%2C%20we%20consistently%20outperform%20state-of-the-art%0Amethods%20under%20two%20document%20sources%20in%20three%20standard%20benchmarks%20for%0Adocument-based%20zero-shot%20learning.%20Qualitatively%2C%20we%20show%20that%20our%20model%20learns%0Athe%20interpretable%20partial%20association.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-Semantic%2520Decomposition%2520and%2520Partial%2520Alignment%2520for%2520Document-based%250A%2520%2520Zero-Shot%2520Learning%26entry.906535625%3DXiangyan%2520Qu%2520and%2520Jing%2520Yu%2520and%2520Keke%2520Gai%2520and%2520Jiamin%2520Zhuang%2520and%2520Yuanmin%2520Tang%2520and%2520Gang%2520Xiong%2520and%2520Gaopeng%2520Gou%2520and%2520Qi%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520work%2520shows%2520that%2520documents%2520from%2520encyclopedias%2520serve%2520as%2520helpful%250Aauxiliary%2520information%2520for%2520zero-shot%2520learning.%2520Existing%2520methods%2520align%2520the%2520entire%250Asemantics%2520of%2520a%2520document%2520with%2520corresponding%2520images%2520to%2520transfer%2520knowledge.%250AHowever%252C%2520they%2520disregard%2520that%2520semantic%2520information%2520is%2520not%2520equivalent%2520between%250Athem%252C%2520resulting%2520in%2520a%2520suboptimal%2520alignment.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Anetwork%2520to%2520extract%2520multi-view%2520semantic%2520concepts%2520from%2520documents%2520and%2520images%2520and%250Aalign%2520the%2520matching%2520rather%2520than%2520entire%2520concepts.%2520Specifically%252C%2520we%2520propose%2520a%250Asemantic%2520decomposition%2520module%2520to%2520generate%2520multi-view%2520semantic%2520embeddings%2520from%250Avisual%2520and%2520textual%2520sides%252C%2520providing%2520the%2520basic%2520concepts%2520for%2520partial%2520alignment.%250ATo%2520alleviate%2520the%2520issue%2520of%2520information%2520redundancy%2520among%2520embeddings%252C%2520we%2520propose%250Athe%2520local-to-semantic%2520variance%2520loss%2520to%2520capture%2520distinct%2520local%2520details%2520and%250Amultiple%2520semantic%2520diversity%2520loss%2520to%2520enforce%2520orthogonality%2520among%2520embeddings.%250ASubsequently%252C%2520two%2520losses%2520are%2520introduced%2520to%2520partially%2520align%2520visual-semantic%250Aembedding%2520pairs%2520according%2520to%2520their%2520semantic%2520relevance%2520at%2520the%2520view%2520and%250Aword-to-patch%2520levels.%2520Consequently%252C%2520we%2520consistently%2520outperform%2520state-of-the-art%250Amethods%2520under%2520two%2520document%2520sources%2520in%2520three%2520standard%2520benchmarks%2520for%250Adocument-based%2520zero-shot%2520learning.%2520Qualitatively%252C%2520we%2520show%2520that%2520our%2520model%2520learns%250Athe%2520interpretable%2520partial%2520association.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-Semantic%20Decomposition%20and%20Partial%20Alignment%20for%20Document-based%0A%20%20Zero-Shot%20Learning&entry.906535625=Xiangyan%20Qu%20and%20Jing%20Yu%20and%20Keke%20Gai%20and%20Jiamin%20Zhuang%20and%20Yuanmin%20Tang%20and%20Gang%20Xiong%20and%20Gaopeng%20Gou%20and%20Qi%20Wu&entry.1292438233=%20%20Recent%20work%20shows%20that%20documents%20from%20encyclopedias%20serve%20as%20helpful%0Aauxiliary%20information%20for%20zero-shot%20learning.%20Existing%20methods%20align%20the%20entire%0Asemantics%20of%20a%20document%20with%20corresponding%20images%20to%20transfer%20knowledge.%0AHowever%2C%20they%20disregard%20that%20semantic%20information%20is%20not%20equivalent%20between%0Athem%2C%20resulting%20in%20a%20suboptimal%20alignment.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Anetwork%20to%20extract%20multi-view%20semantic%20concepts%20from%20documents%20and%20images%20and%0Aalign%20the%20matching%20rather%20than%20entire%20concepts.%20Specifically%2C%20we%20propose%20a%0Asemantic%20decomposition%20module%20to%20generate%20multi-view%20semantic%20embeddings%20from%0Avisual%20and%20textual%20sides%2C%20providing%20the%20basic%20concepts%20for%20partial%20alignment.%0ATo%20alleviate%20the%20issue%20of%20information%20redundancy%20among%20embeddings%2C%20we%20propose%0Athe%20local-to-semantic%20variance%20loss%20to%20capture%20distinct%20local%20details%20and%0Amultiple%20semantic%20diversity%20loss%20to%20enforce%20orthogonality%20among%20embeddings.%0ASubsequently%2C%20two%20losses%20are%20introduced%20to%20partially%20align%20visual-semantic%0Aembedding%20pairs%20according%20to%20their%20semantic%20relevance%20at%20the%20view%20and%0Aword-to-patch%20levels.%20Consequently%2C%20we%20consistently%20outperform%20state-of-the-art%0Amethods%20under%20two%20document%20sources%20in%20three%20standard%20benchmarks%20for%0Adocument-based%20zero-shot%20learning.%20Qualitatively%2C%20we%20show%20that%20our%20model%20learns%0Athe%20interpretable%20partial%20association.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15613v1&entry.124074799=Read"},
{"title": "Snail-Radar: A large-scale diverse dataset for the evaluation of\n  4D-radar-based SLAM systems", "author": "Jianzhu Huai and Binliang Wang and Yuan Zhuang and Yiwen Chen and Qipeng Li and Yulong Han and Charles Toth", "abstract": "  4D radars are increasingly favored for odometry and mapping of autonomous\nsystems due to their robustness in harsh weather and dynamic environments.\nExisting datasets, however, often cover limited areas and are typically\ncaptured using a single platform. To address this gap, we present a diverse\nlarge-scale dataset specifically designed for 4D radar-based localization and\nmapping. This dataset was gathered using three different platforms: a handheld\ndevice, an e-bike, and an SUV, under a variety of environmental conditions,\nincluding clear days, nighttime, and heavy rain. The data collection occurred\nfrom September 2023 to February 2024, encompassing diverse settings such as\nroads in a vegetated campus and tunnels on highways. Each route was traversed\nmultiple times to facilitate place recognition evaluations. The sensor suite\nincluded a 3D lidar, 4D radars, stereo cameras, consumer-grade IMUs, and a\nGNSS/INS system. Sensor data packets were synchronized to GNSS time using a\ntwo-step process: a convex hull algorithm was applied to smooth host time\njitter, and then odometry and correlation algorithms were used to correct\nconstant time offsets. Extrinsic calibration between sensors was achieved\nthrough manual measurements and subsequent nonlinear optimization. The\nreference motion for the platforms was generated by registering lidar scans to\na terrestrial laser scanner (TLS) point cloud map using a lidar inertial\nodometry (LIO) method in localization mode. Additionally, a data reversion\ntechnique was introduced to enable backward LIO processing. We believe this\ndataset will boost research in radar-based point cloud registration, odometry,\nmapping, and place recognition.\n", "link": "http://arxiv.org/abs/2407.11705v2", "date": "2024-07-22", "relevancy": 2.8291, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6099}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5846}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snail-Radar%3A%20A%20large-scale%20diverse%20dataset%20for%20the%20evaluation%20of%0A%20%204D-radar-based%20SLAM%20systems&body=Title%3A%20Snail-Radar%3A%20A%20large-scale%20diverse%20dataset%20for%20the%20evaluation%20of%0A%20%204D-radar-based%20SLAM%20systems%0AAuthor%3A%20Jianzhu%20Huai%20and%20Binliang%20Wang%20and%20Yuan%20Zhuang%20and%20Yiwen%20Chen%20and%20Qipeng%20Li%20and%20Yulong%20Han%20and%20Charles%20Toth%0AAbstract%3A%20%20%204D%20radars%20are%20increasingly%20favored%20for%20odometry%20and%20mapping%20of%20autonomous%0Asystems%20due%20to%20their%20robustness%20in%20harsh%20weather%20and%20dynamic%20environments.%0AExisting%20datasets%2C%20however%2C%20often%20cover%20limited%20areas%20and%20are%20typically%0Acaptured%20using%20a%20single%20platform.%20To%20address%20this%20gap%2C%20we%20present%20a%20diverse%0Alarge-scale%20dataset%20specifically%20designed%20for%204D%20radar-based%20localization%20and%0Amapping.%20This%20dataset%20was%20gathered%20using%20three%20different%20platforms%3A%20a%20handheld%0Adevice%2C%20an%20e-bike%2C%20and%20an%20SUV%2C%20under%20a%20variety%20of%20environmental%20conditions%2C%0Aincluding%20clear%20days%2C%20nighttime%2C%20and%20heavy%20rain.%20The%20data%20collection%20occurred%0Afrom%20September%202023%20to%20February%202024%2C%20encompassing%20diverse%20settings%20such%20as%0Aroads%20in%20a%20vegetated%20campus%20and%20tunnels%20on%20highways.%20Each%20route%20was%20traversed%0Amultiple%20times%20to%20facilitate%20place%20recognition%20evaluations.%20The%20sensor%20suite%0Aincluded%20a%203D%20lidar%2C%204D%20radars%2C%20stereo%20cameras%2C%20consumer-grade%20IMUs%2C%20and%20a%0AGNSS/INS%20system.%20Sensor%20data%20packets%20were%20synchronized%20to%20GNSS%20time%20using%20a%0Atwo-step%20process%3A%20a%20convex%20hull%20algorithm%20was%20applied%20to%20smooth%20host%20time%0Ajitter%2C%20and%20then%20odometry%20and%20correlation%20algorithms%20were%20used%20to%20correct%0Aconstant%20time%20offsets.%20Extrinsic%20calibration%20between%20sensors%20was%20achieved%0Athrough%20manual%20measurements%20and%20subsequent%20nonlinear%20optimization.%20The%0Areference%20motion%20for%20the%20platforms%20was%20generated%20by%20registering%20lidar%20scans%20to%0Aa%20terrestrial%20laser%20scanner%20%28TLS%29%20point%20cloud%20map%20using%20a%20lidar%20inertial%0Aodometry%20%28LIO%29%20method%20in%20localization%20mode.%20Additionally%2C%20a%20data%20reversion%0Atechnique%20was%20introduced%20to%20enable%20backward%20LIO%20processing.%20We%20believe%20this%0Adataset%20will%20boost%20research%20in%20radar-based%20point%20cloud%20registration%2C%20odometry%2C%0Amapping%2C%20and%20place%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnail-Radar%253A%2520A%2520large-scale%2520diverse%2520dataset%2520for%2520the%2520evaluation%2520of%250A%2520%25204D-radar-based%2520SLAM%2520systems%26entry.906535625%3DJianzhu%2520Huai%2520and%2520Binliang%2520Wang%2520and%2520Yuan%2520Zhuang%2520and%2520Yiwen%2520Chen%2520and%2520Qipeng%2520Li%2520and%2520Yulong%2520Han%2520and%2520Charles%2520Toth%26entry.1292438233%3D%2520%25204D%2520radars%2520are%2520increasingly%2520favored%2520for%2520odometry%2520and%2520mapping%2520of%2520autonomous%250Asystems%2520due%2520to%2520their%2520robustness%2520in%2520harsh%2520weather%2520and%2520dynamic%2520environments.%250AExisting%2520datasets%252C%2520however%252C%2520often%2520cover%2520limited%2520areas%2520and%2520are%2520typically%250Acaptured%2520using%2520a%2520single%2520platform.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520a%2520diverse%250Alarge-scale%2520dataset%2520specifically%2520designed%2520for%25204D%2520radar-based%2520localization%2520and%250Amapping.%2520This%2520dataset%2520was%2520gathered%2520using%2520three%2520different%2520platforms%253A%2520a%2520handheld%250Adevice%252C%2520an%2520e-bike%252C%2520and%2520an%2520SUV%252C%2520under%2520a%2520variety%2520of%2520environmental%2520conditions%252C%250Aincluding%2520clear%2520days%252C%2520nighttime%252C%2520and%2520heavy%2520rain.%2520The%2520data%2520collection%2520occurred%250Afrom%2520September%25202023%2520to%2520February%25202024%252C%2520encompassing%2520diverse%2520settings%2520such%2520as%250Aroads%2520in%2520a%2520vegetated%2520campus%2520and%2520tunnels%2520on%2520highways.%2520Each%2520route%2520was%2520traversed%250Amultiple%2520times%2520to%2520facilitate%2520place%2520recognition%2520evaluations.%2520The%2520sensor%2520suite%250Aincluded%2520a%25203D%2520lidar%252C%25204D%2520radars%252C%2520stereo%2520cameras%252C%2520consumer-grade%2520IMUs%252C%2520and%2520a%250AGNSS/INS%2520system.%2520Sensor%2520data%2520packets%2520were%2520synchronized%2520to%2520GNSS%2520time%2520using%2520a%250Atwo-step%2520process%253A%2520a%2520convex%2520hull%2520algorithm%2520was%2520applied%2520to%2520smooth%2520host%2520time%250Ajitter%252C%2520and%2520then%2520odometry%2520and%2520correlation%2520algorithms%2520were%2520used%2520to%2520correct%250Aconstant%2520time%2520offsets.%2520Extrinsic%2520calibration%2520between%2520sensors%2520was%2520achieved%250Athrough%2520manual%2520measurements%2520and%2520subsequent%2520nonlinear%2520optimization.%2520The%250Areference%2520motion%2520for%2520the%2520platforms%2520was%2520generated%2520by%2520registering%2520lidar%2520scans%2520to%250Aa%2520terrestrial%2520laser%2520scanner%2520%2528TLS%2529%2520point%2520cloud%2520map%2520using%2520a%2520lidar%2520inertial%250Aodometry%2520%2528LIO%2529%2520method%2520in%2520localization%2520mode.%2520Additionally%252C%2520a%2520data%2520reversion%250Atechnique%2520was%2520introduced%2520to%2520enable%2520backward%2520LIO%2520processing.%2520We%2520believe%2520this%250Adataset%2520will%2520boost%2520research%2520in%2520radar-based%2520point%2520cloud%2520registration%252C%2520odometry%252C%250Amapping%252C%2520and%2520place%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snail-Radar%3A%20A%20large-scale%20diverse%20dataset%20for%20the%20evaluation%20of%0A%20%204D-radar-based%20SLAM%20systems&entry.906535625=Jianzhu%20Huai%20and%20Binliang%20Wang%20and%20Yuan%20Zhuang%20and%20Yiwen%20Chen%20and%20Qipeng%20Li%20and%20Yulong%20Han%20and%20Charles%20Toth&entry.1292438233=%20%204D%20radars%20are%20increasingly%20favored%20for%20odometry%20and%20mapping%20of%20autonomous%0Asystems%20due%20to%20their%20robustness%20in%20harsh%20weather%20and%20dynamic%20environments.%0AExisting%20datasets%2C%20however%2C%20often%20cover%20limited%20areas%20and%20are%20typically%0Acaptured%20using%20a%20single%20platform.%20To%20address%20this%20gap%2C%20we%20present%20a%20diverse%0Alarge-scale%20dataset%20specifically%20designed%20for%204D%20radar-based%20localization%20and%0Amapping.%20This%20dataset%20was%20gathered%20using%20three%20different%20platforms%3A%20a%20handheld%0Adevice%2C%20an%20e-bike%2C%20and%20an%20SUV%2C%20under%20a%20variety%20of%20environmental%20conditions%2C%0Aincluding%20clear%20days%2C%20nighttime%2C%20and%20heavy%20rain.%20The%20data%20collection%20occurred%0Afrom%20September%202023%20to%20February%202024%2C%20encompassing%20diverse%20settings%20such%20as%0Aroads%20in%20a%20vegetated%20campus%20and%20tunnels%20on%20highways.%20Each%20route%20was%20traversed%0Amultiple%20times%20to%20facilitate%20place%20recognition%20evaluations.%20The%20sensor%20suite%0Aincluded%20a%203D%20lidar%2C%204D%20radars%2C%20stereo%20cameras%2C%20consumer-grade%20IMUs%2C%20and%20a%0AGNSS/INS%20system.%20Sensor%20data%20packets%20were%20synchronized%20to%20GNSS%20time%20using%20a%0Atwo-step%20process%3A%20a%20convex%20hull%20algorithm%20was%20applied%20to%20smooth%20host%20time%0Ajitter%2C%20and%20then%20odometry%20and%20correlation%20algorithms%20were%20used%20to%20correct%0Aconstant%20time%20offsets.%20Extrinsic%20calibration%20between%20sensors%20was%20achieved%0Athrough%20manual%20measurements%20and%20subsequent%20nonlinear%20optimization.%20The%0Areference%20motion%20for%20the%20platforms%20was%20generated%20by%20registering%20lidar%20scans%20to%0Aa%20terrestrial%20laser%20scanner%20%28TLS%29%20point%20cloud%20map%20using%20a%20lidar%20inertial%0Aodometry%20%28LIO%29%20method%20in%20localization%20mode.%20Additionally%2C%20a%20data%20reversion%0Atechnique%20was%20introduced%20to%20enable%20backward%20LIO%20processing.%20We%20believe%20this%0Adataset%20will%20boost%20research%20in%20radar-based%20point%20cloud%20registration%2C%20odometry%2C%0Amapping%2C%20and%20place%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11705v2&entry.124074799=Read"},
{"title": "NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo", "author": "Fotios Logothetis and Ignas Budvytis and Roberto Cipolla", "abstract": "  In this work we present a novel multi-view photometric stereo (MVPS) method.\nLike many works in 3D reconstruction we are leveraging neural shape\nrepresentations and learnt renderers. However, our work differs from the\nstate-of-the-art multi-view PS methods such as PS-NeRF or Supernormal in that\nwe explicitly leverage per-pixel intensity renderings rather than relying\nmainly on estimated normals.\n  We model point light attenuation and explicitly raytrace cast shadows in\norder to best approximate the incoming radiance for each point. The estimated\nincoming radiance is used as input to a fully neural material renderer that\nuses minimal prior assumptions and it is jointly optimised with the surface.\nEstimated normals and segmentation maps are also incorporated in order to\nmaximise the surface accuracy.\n  Our method is among the first (along with Supernormal) to outperform the\nclassical MVPS approach proposed by the DiLiGenT-MV benchmark and achieves\naverage 0.2mm Chamfer distance for objects imaged at approx 1.5m distance away\nwith approximate 400x400 resolution. Moreover, our method shows high robustness\nto the sparse MVPS setup (6 views, 6 lights) greatly outperforming the SOTA\ncompetitor (0.38mm vs 0.61mm), illustrating the importance of neural rendering\nin multi-view photometric stereo.\n", "link": "http://arxiv.org/abs/2405.12057v2", "date": "2024-07-22", "relevancy": 2.8264, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5706}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5626}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NPLMV-PS%3A%20Neural%20Point-Light%20Multi-View%20Photometric%20Stereo&body=Title%3A%20NPLMV-PS%3A%20Neural%20Point-Light%20Multi-View%20Photometric%20Stereo%0AAuthor%3A%20Fotios%20Logothetis%20and%20Ignas%20Budvytis%20and%20Roberto%20Cipolla%0AAbstract%3A%20%20%20In%20this%20work%20we%20present%20a%20novel%20multi-view%20photometric%20stereo%20%28MVPS%29%20method.%0ALike%20many%20works%20in%203D%20reconstruction%20we%20are%20leveraging%20neural%20shape%0Arepresentations%20and%20learnt%20renderers.%20However%2C%20our%20work%20differs%20from%20the%0Astate-of-the-art%20multi-view%20PS%20methods%20such%20as%20PS-NeRF%20or%20Supernormal%20in%20that%0Awe%20explicitly%20leverage%20per-pixel%20intensity%20renderings%20rather%20than%20relying%0Amainly%20on%20estimated%20normals.%0A%20%20We%20model%20point%20light%20attenuation%20and%20explicitly%20raytrace%20cast%20shadows%20in%0Aorder%20to%20best%20approximate%20the%20incoming%20radiance%20for%20each%20point.%20The%20estimated%0Aincoming%20radiance%20is%20used%20as%20input%20to%20a%20fully%20neural%20material%20renderer%20that%0Auses%20minimal%20prior%20assumptions%20and%20it%20is%20jointly%20optimised%20with%20the%20surface.%0AEstimated%20normals%20and%20segmentation%20maps%20are%20also%20incorporated%20in%20order%20to%0Amaximise%20the%20surface%20accuracy.%0A%20%20Our%20method%20is%20among%20the%20first%20%28along%20with%20Supernormal%29%20to%20outperform%20the%0Aclassical%20MVPS%20approach%20proposed%20by%20the%20DiLiGenT-MV%20benchmark%20and%20achieves%0Aaverage%200.2mm%20Chamfer%20distance%20for%20objects%20imaged%20at%20approx%201.5m%20distance%20away%0Awith%20approximate%20400x400%20resolution.%20Moreover%2C%20our%20method%20shows%20high%20robustness%0Ato%20the%20sparse%20MVPS%20setup%20%286%20views%2C%206%20lights%29%20greatly%20outperforming%20the%20SOTA%0Acompetitor%20%280.38mm%20vs%200.61mm%29%2C%20illustrating%20the%20importance%20of%20neural%20rendering%0Ain%20multi-view%20photometric%20stereo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12057v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNPLMV-PS%253A%2520Neural%2520Point-Light%2520Multi-View%2520Photometric%2520Stereo%26entry.906535625%3DFotios%2520Logothetis%2520and%2520Ignas%2520Budvytis%2520and%2520Roberto%2520Cipolla%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520present%2520a%2520novel%2520multi-view%2520photometric%2520stereo%2520%2528MVPS%2529%2520method.%250ALike%2520many%2520works%2520in%25203D%2520reconstruction%2520we%2520are%2520leveraging%2520neural%2520shape%250Arepresentations%2520and%2520learnt%2520renderers.%2520However%252C%2520our%2520work%2520differs%2520from%2520the%250Astate-of-the-art%2520multi-view%2520PS%2520methods%2520such%2520as%2520PS-NeRF%2520or%2520Supernormal%2520in%2520that%250Awe%2520explicitly%2520leverage%2520per-pixel%2520intensity%2520renderings%2520rather%2520than%2520relying%250Amainly%2520on%2520estimated%2520normals.%250A%2520%2520We%2520model%2520point%2520light%2520attenuation%2520and%2520explicitly%2520raytrace%2520cast%2520shadows%2520in%250Aorder%2520to%2520best%2520approximate%2520the%2520incoming%2520radiance%2520for%2520each%2520point.%2520The%2520estimated%250Aincoming%2520radiance%2520is%2520used%2520as%2520input%2520to%2520a%2520fully%2520neural%2520material%2520renderer%2520that%250Auses%2520minimal%2520prior%2520assumptions%2520and%2520it%2520is%2520jointly%2520optimised%2520with%2520the%2520surface.%250AEstimated%2520normals%2520and%2520segmentation%2520maps%2520are%2520also%2520incorporated%2520in%2520order%2520to%250Amaximise%2520the%2520surface%2520accuracy.%250A%2520%2520Our%2520method%2520is%2520among%2520the%2520first%2520%2528along%2520with%2520Supernormal%2529%2520to%2520outperform%2520the%250Aclassical%2520MVPS%2520approach%2520proposed%2520by%2520the%2520DiLiGenT-MV%2520benchmark%2520and%2520achieves%250Aaverage%25200.2mm%2520Chamfer%2520distance%2520for%2520objects%2520imaged%2520at%2520approx%25201.5m%2520distance%2520away%250Awith%2520approximate%2520400x400%2520resolution.%2520Moreover%252C%2520our%2520method%2520shows%2520high%2520robustness%250Ato%2520the%2520sparse%2520MVPS%2520setup%2520%25286%2520views%252C%25206%2520lights%2529%2520greatly%2520outperforming%2520the%2520SOTA%250Acompetitor%2520%25280.38mm%2520vs%25200.61mm%2529%252C%2520illustrating%2520the%2520importance%2520of%2520neural%2520rendering%250Ain%2520multi-view%2520photometric%2520stereo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12057v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NPLMV-PS%3A%20Neural%20Point-Light%20Multi-View%20Photometric%20Stereo&entry.906535625=Fotios%20Logothetis%20and%20Ignas%20Budvytis%20and%20Roberto%20Cipolla&entry.1292438233=%20%20In%20this%20work%20we%20present%20a%20novel%20multi-view%20photometric%20stereo%20%28MVPS%29%20method.%0ALike%20many%20works%20in%203D%20reconstruction%20we%20are%20leveraging%20neural%20shape%0Arepresentations%20and%20learnt%20renderers.%20However%2C%20our%20work%20differs%20from%20the%0Astate-of-the-art%20multi-view%20PS%20methods%20such%20as%20PS-NeRF%20or%20Supernormal%20in%20that%0Awe%20explicitly%20leverage%20per-pixel%20intensity%20renderings%20rather%20than%20relying%0Amainly%20on%20estimated%20normals.%0A%20%20We%20model%20point%20light%20attenuation%20and%20explicitly%20raytrace%20cast%20shadows%20in%0Aorder%20to%20best%20approximate%20the%20incoming%20radiance%20for%20each%20point.%20The%20estimated%0Aincoming%20radiance%20is%20used%20as%20input%20to%20a%20fully%20neural%20material%20renderer%20that%0Auses%20minimal%20prior%20assumptions%20and%20it%20is%20jointly%20optimised%20with%20the%20surface.%0AEstimated%20normals%20and%20segmentation%20maps%20are%20also%20incorporated%20in%20order%20to%0Amaximise%20the%20surface%20accuracy.%0A%20%20Our%20method%20is%20among%20the%20first%20%28along%20with%20Supernormal%29%20to%20outperform%20the%0Aclassical%20MVPS%20approach%20proposed%20by%20the%20DiLiGenT-MV%20benchmark%20and%20achieves%0Aaverage%200.2mm%20Chamfer%20distance%20for%20objects%20imaged%20at%20approx%201.5m%20distance%20away%0Awith%20approximate%20400x400%20resolution.%20Moreover%2C%20our%20method%20shows%20high%20robustness%0Ato%20the%20sparse%20MVPS%20setup%20%286%20views%2C%206%20lights%29%20greatly%20outperforming%20the%20SOTA%0Acompetitor%20%280.38mm%20vs%200.61mm%29%2C%20illustrating%20the%20importance%20of%20neural%20rendering%0Ain%20multi-view%20photometric%20stereo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12057v2&entry.124074799=Read"},
{"title": "Decomposition of Neural Discrete Representations for Large-Scale 3D\n  Mapping", "author": "Minseong Park and Suhan Woo and Euntai Kim", "abstract": "  Learning efficient representations of local features is a key challenge in\nfeature volume-based 3D neural mapping, especially in large-scale environments.\nIn this paper, we introduce Decomposition-based Neural Mapping (DNMap), a\nstorage-efficient large-scale 3D mapping method that employs a discrete\nrepresentation based on a decomposition strategy. This decomposition strategy\naims to efficiently capture repetitive and representative patterns of shapes by\ndecomposing each discrete embedding into component vectors that are shared\nacross the embedding space. Our DNMap optimizes a set of component vectors,\nrather than entire discrete embeddings, and learns composition rather than\nindexing the discrete embeddings. Furthermore, to complement the mapping\nquality, we additionally learn low-resolution continuous embeddings that\nrequire tiny storage space. By combining these representations with a shallow\nneural network and an efficient octree-based feature volume, our DNMap\nsuccessfully approximates signed distance functions and compresses the feature\nvolume while preserving mapping quality. Our source code is available at\nhttps://github.com/minseong-p/dnmap.\n", "link": "http://arxiv.org/abs/2407.15554v1", "date": "2024-07-22", "relevancy": 2.8086, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6014}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5546}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposition%20of%20Neural%20Discrete%20Representations%20for%20Large-Scale%203D%0A%20%20Mapping&body=Title%3A%20Decomposition%20of%20Neural%20Discrete%20Representations%20for%20Large-Scale%203D%0A%20%20Mapping%0AAuthor%3A%20Minseong%20Park%20and%20Suhan%20Woo%20and%20Euntai%20Kim%0AAbstract%3A%20%20%20Learning%20efficient%20representations%20of%20local%20features%20is%20a%20key%20challenge%20in%0Afeature%20volume-based%203D%20neural%20mapping%2C%20especially%20in%20large-scale%20environments.%0AIn%20this%20paper%2C%20we%20introduce%20Decomposition-based%20Neural%20Mapping%20%28DNMap%29%2C%20a%0Astorage-efficient%20large-scale%203D%20mapping%20method%20that%20employs%20a%20discrete%0Arepresentation%20based%20on%20a%20decomposition%20strategy.%20This%20decomposition%20strategy%0Aaims%20to%20efficiently%20capture%20repetitive%20and%20representative%20patterns%20of%20shapes%20by%0Adecomposing%20each%20discrete%20embedding%20into%20component%20vectors%20that%20are%20shared%0Aacross%20the%20embedding%20space.%20Our%20DNMap%20optimizes%20a%20set%20of%20component%20vectors%2C%0Arather%20than%20entire%20discrete%20embeddings%2C%20and%20learns%20composition%20rather%20than%0Aindexing%20the%20discrete%20embeddings.%20Furthermore%2C%20to%20complement%20the%20mapping%0Aquality%2C%20we%20additionally%20learn%20low-resolution%20continuous%20embeddings%20that%0Arequire%20tiny%20storage%20space.%20By%20combining%20these%20representations%20with%20a%20shallow%0Aneural%20network%20and%20an%20efficient%20octree-based%20feature%20volume%2C%20our%20DNMap%0Asuccessfully%20approximates%20signed%20distance%20functions%20and%20compresses%20the%20feature%0Avolume%20while%20preserving%20mapping%20quality.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/minseong-p/dnmap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposition%2520of%2520Neural%2520Discrete%2520Representations%2520for%2520Large-Scale%25203D%250A%2520%2520Mapping%26entry.906535625%3DMinseong%2520Park%2520and%2520Suhan%2520Woo%2520and%2520Euntai%2520Kim%26entry.1292438233%3D%2520%2520Learning%2520efficient%2520representations%2520of%2520local%2520features%2520is%2520a%2520key%2520challenge%2520in%250Afeature%2520volume-based%25203D%2520neural%2520mapping%252C%2520especially%2520in%2520large-scale%2520environments.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520Decomposition-based%2520Neural%2520Mapping%2520%2528DNMap%2529%252C%2520a%250Astorage-efficient%2520large-scale%25203D%2520mapping%2520method%2520that%2520employs%2520a%2520discrete%250Arepresentation%2520based%2520on%2520a%2520decomposition%2520strategy.%2520This%2520decomposition%2520strategy%250Aaims%2520to%2520efficiently%2520capture%2520repetitive%2520and%2520representative%2520patterns%2520of%2520shapes%2520by%250Adecomposing%2520each%2520discrete%2520embedding%2520into%2520component%2520vectors%2520that%2520are%2520shared%250Aacross%2520the%2520embedding%2520space.%2520Our%2520DNMap%2520optimizes%2520a%2520set%2520of%2520component%2520vectors%252C%250Arather%2520than%2520entire%2520discrete%2520embeddings%252C%2520and%2520learns%2520composition%2520rather%2520than%250Aindexing%2520the%2520discrete%2520embeddings.%2520Furthermore%252C%2520to%2520complement%2520the%2520mapping%250Aquality%252C%2520we%2520additionally%2520learn%2520low-resolution%2520continuous%2520embeddings%2520that%250Arequire%2520tiny%2520storage%2520space.%2520By%2520combining%2520these%2520representations%2520with%2520a%2520shallow%250Aneural%2520network%2520and%2520an%2520efficient%2520octree-based%2520feature%2520volume%252C%2520our%2520DNMap%250Asuccessfully%2520approximates%2520signed%2520distance%2520functions%2520and%2520compresses%2520the%2520feature%250Avolume%2520while%2520preserving%2520mapping%2520quality.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/minseong-p/dnmap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposition%20of%20Neural%20Discrete%20Representations%20for%20Large-Scale%203D%0A%20%20Mapping&entry.906535625=Minseong%20Park%20and%20Suhan%20Woo%20and%20Euntai%20Kim&entry.1292438233=%20%20Learning%20efficient%20representations%20of%20local%20features%20is%20a%20key%20challenge%20in%0Afeature%20volume-based%203D%20neural%20mapping%2C%20especially%20in%20large-scale%20environments.%0AIn%20this%20paper%2C%20we%20introduce%20Decomposition-based%20Neural%20Mapping%20%28DNMap%29%2C%20a%0Astorage-efficient%20large-scale%203D%20mapping%20method%20that%20employs%20a%20discrete%0Arepresentation%20based%20on%20a%20decomposition%20strategy.%20This%20decomposition%20strategy%0Aaims%20to%20efficiently%20capture%20repetitive%20and%20representative%20patterns%20of%20shapes%20by%0Adecomposing%20each%20discrete%20embedding%20into%20component%20vectors%20that%20are%20shared%0Aacross%20the%20embedding%20space.%20Our%20DNMap%20optimizes%20a%20set%20of%20component%20vectors%2C%0Arather%20than%20entire%20discrete%20embeddings%2C%20and%20learns%20composition%20rather%20than%0Aindexing%20the%20discrete%20embeddings.%20Furthermore%2C%20to%20complement%20the%20mapping%0Aquality%2C%20we%20additionally%20learn%20low-resolution%20continuous%20embeddings%20that%0Arequire%20tiny%20storage%20space.%20By%20combining%20these%20representations%20with%20a%20shallow%0Aneural%20network%20and%20an%20efficient%20octree-based%20feature%20volume%2C%20our%20DNMap%0Asuccessfully%20approximates%20signed%20distance%20functions%20and%20compresses%20the%20feature%0Avolume%20while%20preserving%20mapping%20quality.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/minseong-p/dnmap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15554v1&entry.124074799=Read"},
{"title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning", "author": "Emanuele Frascaroli and Aniello Panariello and Pietro Buzzega and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n", "link": "http://arxiv.org/abs/2407.15793v1", "date": "2024-07-22", "relevancy": 2.7956, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5741}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5663}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning&body=Title%3A%20CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning%0AAuthor%3A%20Emanuele%20Frascaroli%20and%20Aniello%20Panariello%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20Transformers%20and%20Vision-Language%20Models%20%28VLMs%29%20such%20as%0ACLIP%2C%20large%20pre-trained%20models%20have%20become%20a%20common%20strategy%20to%20enhance%0Aperformance%20in%20Continual%20Learning%20scenarios.%20This%20led%20to%20the%20development%20of%0Anumerous%20prompting%20strategies%20to%20effectively%20fine-tune%20transformer-based%20models%0Awithout%20succumbing%20to%20catastrophic%20forgetting.%20However%2C%20these%20methods%20struggle%0Ato%20specialize%20the%20model%20on%20domains%20significantly%20deviating%20from%20the%0Apre-training%20and%20preserving%20its%20zero-shot%20capabilities.%20In%20this%20work%2C%20we%0Apropose%20Continual%20Generative%20training%20for%20Incremental%20prompt-Learning%2C%20a%20novel%0Aapproach%20to%20mitigate%20forgetting%20while%20adapting%20a%20VLM%2C%20which%20exploits%20generative%0Areplay%20to%20align%20prompts%20to%20tasks.%20We%20also%20introduce%20a%20new%20metric%20to%20evaluate%0Azero-shot%20capabilities%20within%20CL%20benchmarks.%20Through%20extensive%20experiments%20on%0Adifferent%20domains%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20framework%20in%0Aadapting%20to%20new%20tasks%20while%20improving%20zero-shot%20capabilities.%20Further%20analysis%0Areveals%20that%20our%20approach%20can%20bridge%20the%20gap%20with%20joint%20prompt%20tuning.%20The%0Acodebase%20is%20available%20at%20https%3A//github.com/aimagelab/mammoth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2520with%2520Generative%2520Latent%2520Replay%253A%2520a%2520Strong%2520Baseline%2520for%2520Incremental%250A%2520%2520Learning%26entry.906535625%3DEmanuele%2520Frascaroli%2520and%2520Aniello%2520Panariello%2520and%2520Pietro%2520Buzzega%2520and%2520Lorenzo%2520Bonicelli%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520Transformers%2520and%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520such%2520as%250ACLIP%252C%2520large%2520pre-trained%2520models%2520have%2520become%2520a%2520common%2520strategy%2520to%2520enhance%250Aperformance%2520in%2520Continual%2520Learning%2520scenarios.%2520This%2520led%2520to%2520the%2520development%2520of%250Anumerous%2520prompting%2520strategies%2520to%2520effectively%2520fine-tune%2520transformer-based%2520models%250Awithout%2520succumbing%2520to%2520catastrophic%2520forgetting.%2520However%252C%2520these%2520methods%2520struggle%250Ato%2520specialize%2520the%2520model%2520on%2520domains%2520significantly%2520deviating%2520from%2520the%250Apre-training%2520and%2520preserving%2520its%2520zero-shot%2520capabilities.%2520In%2520this%2520work%252C%2520we%250Apropose%2520Continual%2520Generative%2520training%2520for%2520Incremental%2520prompt-Learning%252C%2520a%2520novel%250Aapproach%2520to%2520mitigate%2520forgetting%2520while%2520adapting%2520a%2520VLM%252C%2520which%2520exploits%2520generative%250Areplay%2520to%2520align%2520prompts%2520to%2520tasks.%2520We%2520also%2520introduce%2520a%2520new%2520metric%2520to%2520evaluate%250Azero-shot%2520capabilities%2520within%2520CL%2520benchmarks.%2520Through%2520extensive%2520experiments%2520on%250Adifferent%2520domains%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework%2520in%250Aadapting%2520to%2520new%2520tasks%2520while%2520improving%2520zero-shot%2520capabilities.%2520Further%2520analysis%250Areveals%2520that%2520our%2520approach%2520can%2520bridge%2520the%2520gap%2520with%2520joint%2520prompt%2520tuning.%2520The%250Acodebase%2520is%2520available%2520at%2520https%253A//github.com/aimagelab/mammoth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning&entry.906535625=Emanuele%20Frascaroli%20and%20Aniello%20Panariello%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20With%20the%20emergence%20of%20Transformers%20and%20Vision-Language%20Models%20%28VLMs%29%20such%20as%0ACLIP%2C%20large%20pre-trained%20models%20have%20become%20a%20common%20strategy%20to%20enhance%0Aperformance%20in%20Continual%20Learning%20scenarios.%20This%20led%20to%20the%20development%20of%0Anumerous%20prompting%20strategies%20to%20effectively%20fine-tune%20transformer-based%20models%0Awithout%20succumbing%20to%20catastrophic%20forgetting.%20However%2C%20these%20methods%20struggle%0Ato%20specialize%20the%20model%20on%20domains%20significantly%20deviating%20from%20the%0Apre-training%20and%20preserving%20its%20zero-shot%20capabilities.%20In%20this%20work%2C%20we%0Apropose%20Continual%20Generative%20training%20for%20Incremental%20prompt-Learning%2C%20a%20novel%0Aapproach%20to%20mitigate%20forgetting%20while%20adapting%20a%20VLM%2C%20which%20exploits%20generative%0Areplay%20to%20align%20prompts%20to%20tasks.%20We%20also%20introduce%20a%20new%20metric%20to%20evaluate%0Azero-shot%20capabilities%20within%20CL%20benchmarks.%20Through%20extensive%20experiments%20on%0Adifferent%20domains%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20framework%20in%0Aadapting%20to%20new%20tasks%20while%20improving%20zero-shot%20capabilities.%20Further%20analysis%0Areveals%20that%20our%20approach%20can%20bridge%20the%20gap%20with%20joint%20prompt%20tuning.%20The%0Acodebase%20is%20available%20at%20https%3A//github.com/aimagelab/mammoth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15793v1&entry.124074799=Read"},
{"title": "A Survey on Video Prediction: From Deterministic to Generative\n  Approaches", "author": "Ruibo Ming and Zhewei Huang and Zhuoxuan Ju and Jianming Hu and Lihui Peng and Shuchang Zhou", "abstract": "  Video prediction, a fundamental task in computer vision, aims to enable\nmodels to generate sequences of future frames based on existing video content.\nThis task has garnered widespread application across various domains. In this\npaper, we comprehensively survey both historical and contemporary works in this\nfield, encompassing the most widely used datasets and algorithms. Our survey\nscrutinizes the challenges and evolving landscape of video prediction within\nthe realm of computer vision. We propose a novel taxonomy centered on the\nstochastic nature of video prediction algorithms. This taxonomy accentuates the\ngradual transition from deterministic to generative prediction methodologies,\nunderlining significant advancements and shifts in approach.\n", "link": "http://arxiv.org/abs/2401.14718v3", "date": "2024-07-22", "relevancy": 2.7945, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5734}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5541}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Video%20Prediction%3A%20From%20Deterministic%20to%20Generative%0A%20%20Approaches&body=Title%3A%20A%20Survey%20on%20Video%20Prediction%3A%20From%20Deterministic%20to%20Generative%0A%20%20Approaches%0AAuthor%3A%20Ruibo%20Ming%20and%20Zhewei%20Huang%20and%20Zhuoxuan%20Ju%20and%20Jianming%20Hu%20and%20Lihui%20Peng%20and%20Shuchang%20Zhou%0AAbstract%3A%20%20%20Video%20prediction%2C%20a%20fundamental%20task%20in%20computer%20vision%2C%20aims%20to%20enable%0Amodels%20to%20generate%20sequences%20of%20future%20frames%20based%20on%20existing%20video%20content.%0AThis%20task%20has%20garnered%20widespread%20application%20across%20various%20domains.%20In%20this%0Apaper%2C%20we%20comprehensively%20survey%20both%20historical%20and%20contemporary%20works%20in%20this%0Afield%2C%20encompassing%20the%20most%20widely%20used%20datasets%20and%20algorithms.%20Our%20survey%0Ascrutinizes%20the%20challenges%20and%20evolving%20landscape%20of%20video%20prediction%20within%0Athe%20realm%20of%20computer%20vision.%20We%20propose%20a%20novel%20taxonomy%20centered%20on%20the%0Astochastic%20nature%20of%20video%20prediction%20algorithms.%20This%20taxonomy%20accentuates%20the%0Agradual%20transition%20from%20deterministic%20to%20generative%20prediction%20methodologies%2C%0Aunderlining%20significant%20advancements%20and%20shifts%20in%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14718v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Video%2520Prediction%253A%2520From%2520Deterministic%2520to%2520Generative%250A%2520%2520Approaches%26entry.906535625%3DRuibo%2520Ming%2520and%2520Zhewei%2520Huang%2520and%2520Zhuoxuan%2520Ju%2520and%2520Jianming%2520Hu%2520and%2520Lihui%2520Peng%2520and%2520Shuchang%2520Zhou%26entry.1292438233%3D%2520%2520Video%2520prediction%252C%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%2520aims%2520to%2520enable%250Amodels%2520to%2520generate%2520sequences%2520of%2520future%2520frames%2520based%2520on%2520existing%2520video%2520content.%250AThis%2520task%2520has%2520garnered%2520widespread%2520application%2520across%2520various%2520domains.%2520In%2520this%250Apaper%252C%2520we%2520comprehensively%2520survey%2520both%2520historical%2520and%2520contemporary%2520works%2520in%2520this%250Afield%252C%2520encompassing%2520the%2520most%2520widely%2520used%2520datasets%2520and%2520algorithms.%2520Our%2520survey%250Ascrutinizes%2520the%2520challenges%2520and%2520evolving%2520landscape%2520of%2520video%2520prediction%2520within%250Athe%2520realm%2520of%2520computer%2520vision.%2520We%2520propose%2520a%2520novel%2520taxonomy%2520centered%2520on%2520the%250Astochastic%2520nature%2520of%2520video%2520prediction%2520algorithms.%2520This%2520taxonomy%2520accentuates%2520the%250Agradual%2520transition%2520from%2520deterministic%2520to%2520generative%2520prediction%2520methodologies%252C%250Aunderlining%2520significant%2520advancements%2520and%2520shifts%2520in%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14718v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Video%20Prediction%3A%20From%20Deterministic%20to%20Generative%0A%20%20Approaches&entry.906535625=Ruibo%20Ming%20and%20Zhewei%20Huang%20and%20Zhuoxuan%20Ju%20and%20Jianming%20Hu%20and%20Lihui%20Peng%20and%20Shuchang%20Zhou&entry.1292438233=%20%20Video%20prediction%2C%20a%20fundamental%20task%20in%20computer%20vision%2C%20aims%20to%20enable%0Amodels%20to%20generate%20sequences%20of%20future%20frames%20based%20on%20existing%20video%20content.%0AThis%20task%20has%20garnered%20widespread%20application%20across%20various%20domains.%20In%20this%0Apaper%2C%20we%20comprehensively%20survey%20both%20historical%20and%20contemporary%20works%20in%20this%0Afield%2C%20encompassing%20the%20most%20widely%20used%20datasets%20and%20algorithms.%20Our%20survey%0Ascrutinizes%20the%20challenges%20and%20evolving%20landscape%20of%20video%20prediction%20within%0Athe%20realm%20of%20computer%20vision.%20We%20propose%20a%20novel%20taxonomy%20centered%20on%20the%0Astochastic%20nature%20of%20video%20prediction%20algorithms.%20This%20taxonomy%20accentuates%20the%0Agradual%20transition%20from%20deterministic%20to%20generative%20prediction%20methodologies%2C%0Aunderlining%20significant%20advancements%20and%20shifts%20in%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14718v3&entry.124074799=Read"},
{"title": "SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked\n  AutoEncoder", "author": "Jaeseong Lee and Junha Hyung and Sohyun Jeong and Jaegul Choo", "abstract": "  Face swapping has gained significant attention for its varied applications.\nMost previous face swapping approaches have relied on the seesaw game training\nscheme, also known as the target-oriented approach. However, this often leads\nto instability in model training and results in undesired samples with blended\nidentities due to the target identity leakage problem. Source-oriented methods\nachieve more stable training with self-reconstruction objective but often fail\nto accurately reflect target image's skin color and illumination. This paper\nintroduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a\nnovel self-supervised approach that combines the strengths of both\ntarget-oriented and source-oriented approaches. Our training scheme addresses\nthe limitations of traditional training methods by circumventing the\nconventional seesaw game and introducing clear ground truth through its\nself-reconstruction training regime. Our model effectively mitigates identity\nleakage and reflects target albedo and illumination through learned\ndisentangled identity and non-identity features. Additionally, we closely\ntackle the shape misalignment and volume discrepancy problems with new\ntechniques, including perforation confusion and random mesh scaling. SAMAE\nestablishes a new state-of-the-art, surpassing other baseline methods,\npreserving both identity and non-identity attributes without sacrificing on\neither aspect.\n", "link": "http://arxiv.org/abs/2402.07370v2", "date": "2024-07-22", "relevancy": 2.7914, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5682}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5591}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfSwapper%3A%20Self-Supervised%20Face%20Swapping%20via%20Shape%20Agnostic%20Masked%0A%20%20AutoEncoder&body=Title%3A%20SelfSwapper%3A%20Self-Supervised%20Face%20Swapping%20via%20Shape%20Agnostic%20Masked%0A%20%20AutoEncoder%0AAuthor%3A%20Jaeseong%20Lee%20and%20Junha%20Hyung%20and%20Sohyun%20Jeong%20and%20Jaegul%20Choo%0AAbstract%3A%20%20%20Face%20swapping%20has%20gained%20significant%20attention%20for%20its%20varied%20applications.%0AMost%20previous%20face%20swapping%20approaches%20have%20relied%20on%20the%20seesaw%20game%20training%0Ascheme%2C%20also%20known%20as%20the%20target-oriented%20approach.%20However%2C%20this%20often%20leads%0Ato%20instability%20in%20model%20training%20and%20results%20in%20undesired%20samples%20with%20blended%0Aidentities%20due%20to%20the%20target%20identity%20leakage%20problem.%20Source-oriented%20methods%0Aachieve%20more%20stable%20training%20with%20self-reconstruction%20objective%20but%20often%20fail%0Ato%20accurately%20reflect%20target%20image%27s%20skin%20color%20and%20illumination.%20This%20paper%0Aintroduces%20the%20Shape%20Agnostic%20Masked%20AutoEncoder%20%28SAMAE%29%20training%20scheme%2C%20a%0Anovel%20self-supervised%20approach%20that%20combines%20the%20strengths%20of%20both%0Atarget-oriented%20and%20source-oriented%20approaches.%20Our%20training%20scheme%20addresses%0Athe%20limitations%20of%20traditional%20training%20methods%20by%20circumventing%20the%0Aconventional%20seesaw%20game%20and%20introducing%20clear%20ground%20truth%20through%20its%0Aself-reconstruction%20training%20regime.%20Our%20model%20effectively%20mitigates%20identity%0Aleakage%20and%20reflects%20target%20albedo%20and%20illumination%20through%20learned%0Adisentangled%20identity%20and%20non-identity%20features.%20Additionally%2C%20we%20closely%0Atackle%20the%20shape%20misalignment%20and%20volume%20discrepancy%20problems%20with%20new%0Atechniques%2C%20including%20perforation%20confusion%20and%20random%20mesh%20scaling.%20SAMAE%0Aestablishes%20a%20new%20state-of-the-art%2C%20surpassing%20other%20baseline%20methods%2C%0Apreserving%20both%20identity%20and%20non-identity%20attributes%20without%20sacrificing%20on%0Aeither%20aspect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfSwapper%253A%2520Self-Supervised%2520Face%2520Swapping%2520via%2520Shape%2520Agnostic%2520Masked%250A%2520%2520AutoEncoder%26entry.906535625%3DJaeseong%2520Lee%2520and%2520Junha%2520Hyung%2520and%2520Sohyun%2520Jeong%2520and%2520Jaegul%2520Choo%26entry.1292438233%3D%2520%2520Face%2520swapping%2520has%2520gained%2520significant%2520attention%2520for%2520its%2520varied%2520applications.%250AMost%2520previous%2520face%2520swapping%2520approaches%2520have%2520relied%2520on%2520the%2520seesaw%2520game%2520training%250Ascheme%252C%2520also%2520known%2520as%2520the%2520target-oriented%2520approach.%2520However%252C%2520this%2520often%2520leads%250Ato%2520instability%2520in%2520model%2520training%2520and%2520results%2520in%2520undesired%2520samples%2520with%2520blended%250Aidentities%2520due%2520to%2520the%2520target%2520identity%2520leakage%2520problem.%2520Source-oriented%2520methods%250Aachieve%2520more%2520stable%2520training%2520with%2520self-reconstruction%2520objective%2520but%2520often%2520fail%250Ato%2520accurately%2520reflect%2520target%2520image%2527s%2520skin%2520color%2520and%2520illumination.%2520This%2520paper%250Aintroduces%2520the%2520Shape%2520Agnostic%2520Masked%2520AutoEncoder%2520%2528SAMAE%2529%2520training%2520scheme%252C%2520a%250Anovel%2520self-supervised%2520approach%2520that%2520combines%2520the%2520strengths%2520of%2520both%250Atarget-oriented%2520and%2520source-oriented%2520approaches.%2520Our%2520training%2520scheme%2520addresses%250Athe%2520limitations%2520of%2520traditional%2520training%2520methods%2520by%2520circumventing%2520the%250Aconventional%2520seesaw%2520game%2520and%2520introducing%2520clear%2520ground%2520truth%2520through%2520its%250Aself-reconstruction%2520training%2520regime.%2520Our%2520model%2520effectively%2520mitigates%2520identity%250Aleakage%2520and%2520reflects%2520target%2520albedo%2520and%2520illumination%2520through%2520learned%250Adisentangled%2520identity%2520and%2520non-identity%2520features.%2520Additionally%252C%2520we%2520closely%250Atackle%2520the%2520shape%2520misalignment%2520and%2520volume%2520discrepancy%2520problems%2520with%2520new%250Atechniques%252C%2520including%2520perforation%2520confusion%2520and%2520random%2520mesh%2520scaling.%2520SAMAE%250Aestablishes%2520a%2520new%2520state-of-the-art%252C%2520surpassing%2520other%2520baseline%2520methods%252C%250Apreserving%2520both%2520identity%2520and%2520non-identity%2520attributes%2520without%2520sacrificing%2520on%250Aeither%2520aspect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfSwapper%3A%20Self-Supervised%20Face%20Swapping%20via%20Shape%20Agnostic%20Masked%0A%20%20AutoEncoder&entry.906535625=Jaeseong%20Lee%20and%20Junha%20Hyung%20and%20Sohyun%20Jeong%20and%20Jaegul%20Choo&entry.1292438233=%20%20Face%20swapping%20has%20gained%20significant%20attention%20for%20its%20varied%20applications.%0AMost%20previous%20face%20swapping%20approaches%20have%20relied%20on%20the%20seesaw%20game%20training%0Ascheme%2C%20also%20known%20as%20the%20target-oriented%20approach.%20However%2C%20this%20often%20leads%0Ato%20instability%20in%20model%20training%20and%20results%20in%20undesired%20samples%20with%20blended%0Aidentities%20due%20to%20the%20target%20identity%20leakage%20problem.%20Source-oriented%20methods%0Aachieve%20more%20stable%20training%20with%20self-reconstruction%20objective%20but%20often%20fail%0Ato%20accurately%20reflect%20target%20image%27s%20skin%20color%20and%20illumination.%20This%20paper%0Aintroduces%20the%20Shape%20Agnostic%20Masked%20AutoEncoder%20%28SAMAE%29%20training%20scheme%2C%20a%0Anovel%20self-supervised%20approach%20that%20combines%20the%20strengths%20of%20both%0Atarget-oriented%20and%20source-oriented%20approaches.%20Our%20training%20scheme%20addresses%0Athe%20limitations%20of%20traditional%20training%20methods%20by%20circumventing%20the%0Aconventional%20seesaw%20game%20and%20introducing%20clear%20ground%20truth%20through%20its%0Aself-reconstruction%20training%20regime.%20Our%20model%20effectively%20mitigates%20identity%0Aleakage%20and%20reflects%20target%20albedo%20and%20illumination%20through%20learned%0Adisentangled%20identity%20and%20non-identity%20features.%20Additionally%2C%20we%20closely%0Atackle%20the%20shape%20misalignment%20and%20volume%20discrepancy%20problems%20with%20new%0Atechniques%2C%20including%20perforation%20confusion%20and%20random%20mesh%20scaling.%20SAMAE%0Aestablishes%20a%20new%20state-of-the-art%2C%20surpassing%20other%20baseline%20methods%2C%0Apreserving%20both%20identity%20and%20non-identity%20attributes%20without%20sacrificing%20on%0Aeither%20aspect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07370v2&entry.124074799=Read"},
{"title": "SAM2CLIP2SAM: Vision Language Model for Segmentation of 3D CT Scans for\n  Covid-19 Detection", "author": "Dimitrios Kollias and Anastasios Arsenos and James Wingate and Stefanos Kollias", "abstract": "  This paper presents a new approach for effective segmentation of images that\ncan be integrated into any model and methodology; the paradigm that we choose\nis classification of medical images (3-D chest CT scans) for Covid-19\ndetection. Our approach includes a combination of vision-language models that\nsegment the CT scans, which are then fed to a deep neural architecture, named\nRACNet, for Covid-19 detection. In particular, a novel framework, named\nSAM2CLIP2SAM, is introduced for segmentation that leverages the strengths of\nboth Segment Anything Model (SAM) and Contrastive Language-Image Pre-Training\n(CLIP) to accurately segment the right and left lungs in CT scans, subsequently\nfeeding these segmented outputs into RACNet for classification of COVID-19 and\nnon-COVID-19 cases. At first, SAM produces multiple part-based segmentation\nmasks for each slice in the CT scan; then CLIP selects only the masks that are\nassociated with the regions of interest (ROIs), i.e., the right and left lungs;\nfinally SAM is given these ROIs as prompts and generates the final segmentation\nmask for the lungs. Experiments are presented across two Covid-19 annotated\ndatabases which illustrate the improved performance obtained when our method\nhas been used for segmentation of the CT scans.\n", "link": "http://arxiv.org/abs/2407.15728v1", "date": "2024-07-22", "relevancy": 2.7898, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5932}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5403}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM2CLIP2SAM%3A%20Vision%20Language%20Model%20for%20Segmentation%20of%203D%20CT%20Scans%20for%0A%20%20Covid-19%20Detection&body=Title%3A%20SAM2CLIP2SAM%3A%20Vision%20Language%20Model%20for%20Segmentation%20of%203D%20CT%20Scans%20for%0A%20%20Covid-19%20Detection%0AAuthor%3A%20Dimitrios%20Kollias%20and%20Anastasios%20Arsenos%20and%20James%20Wingate%20and%20Stefanos%20Kollias%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20approach%20for%20effective%20segmentation%20of%20images%20that%0Acan%20be%20integrated%20into%20any%20model%20and%20methodology%3B%20the%20paradigm%20that%20we%20choose%0Ais%20classification%20of%20medical%20images%20%283-D%20chest%20CT%20scans%29%20for%20Covid-19%0Adetection.%20Our%20approach%20includes%20a%20combination%20of%20vision-language%20models%20that%0Asegment%20the%20CT%20scans%2C%20which%20are%20then%20fed%20to%20a%20deep%20neural%20architecture%2C%20named%0ARACNet%2C%20for%20Covid-19%20detection.%20In%20particular%2C%20a%20novel%20framework%2C%20named%0ASAM2CLIP2SAM%2C%20is%20introduced%20for%20segmentation%20that%20leverages%20the%20strengths%20of%0Aboth%20Segment%20Anything%20Model%20%28SAM%29%20and%20Contrastive%20Language-Image%20Pre-Training%0A%28CLIP%29%20to%20accurately%20segment%20the%20right%20and%20left%20lungs%20in%20CT%20scans%2C%20subsequently%0Afeeding%20these%20segmented%20outputs%20into%20RACNet%20for%20classification%20of%20COVID-19%20and%0Anon-COVID-19%20cases.%20At%20first%2C%20SAM%20produces%20multiple%20part-based%20segmentation%0Amasks%20for%20each%20slice%20in%20the%20CT%20scan%3B%20then%20CLIP%20selects%20only%20the%20masks%20that%20are%0Aassociated%20with%20the%20regions%20of%20interest%20%28ROIs%29%2C%20i.e.%2C%20the%20right%20and%20left%20lungs%3B%0Afinally%20SAM%20is%20given%20these%20ROIs%20as%20prompts%20and%20generates%20the%20final%20segmentation%0Amask%20for%20the%20lungs.%20Experiments%20are%20presented%20across%20two%20Covid-19%20annotated%0Adatabases%20which%20illustrate%20the%20improved%20performance%20obtained%20when%20our%20method%0Ahas%20been%20used%20for%20segmentation%20of%20the%20CT%20scans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM2CLIP2SAM%253A%2520Vision%2520Language%2520Model%2520for%2520Segmentation%2520of%25203D%2520CT%2520Scans%2520for%250A%2520%2520Covid-19%2520Detection%26entry.906535625%3DDimitrios%2520Kollias%2520and%2520Anastasios%2520Arsenos%2520and%2520James%2520Wingate%2520and%2520Stefanos%2520Kollias%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520approach%2520for%2520effective%2520segmentation%2520of%2520images%2520that%250Acan%2520be%2520integrated%2520into%2520any%2520model%2520and%2520methodology%253B%2520the%2520paradigm%2520that%2520we%2520choose%250Ais%2520classification%2520of%2520medical%2520images%2520%25283-D%2520chest%2520CT%2520scans%2529%2520for%2520Covid-19%250Adetection.%2520Our%2520approach%2520includes%2520a%2520combination%2520of%2520vision-language%2520models%2520that%250Asegment%2520the%2520CT%2520scans%252C%2520which%2520are%2520then%2520fed%2520to%2520a%2520deep%2520neural%2520architecture%252C%2520named%250ARACNet%252C%2520for%2520Covid-19%2520detection.%2520In%2520particular%252C%2520a%2520novel%2520framework%252C%2520named%250ASAM2CLIP2SAM%252C%2520is%2520introduced%2520for%2520segmentation%2520that%2520leverages%2520the%2520strengths%2520of%250Aboth%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520and%2520Contrastive%2520Language-Image%2520Pre-Training%250A%2528CLIP%2529%2520to%2520accurately%2520segment%2520the%2520right%2520and%2520left%2520lungs%2520in%2520CT%2520scans%252C%2520subsequently%250Afeeding%2520these%2520segmented%2520outputs%2520into%2520RACNet%2520for%2520classification%2520of%2520COVID-19%2520and%250Anon-COVID-19%2520cases.%2520At%2520first%252C%2520SAM%2520produces%2520multiple%2520part-based%2520segmentation%250Amasks%2520for%2520each%2520slice%2520in%2520the%2520CT%2520scan%253B%2520then%2520CLIP%2520selects%2520only%2520the%2520masks%2520that%2520are%250Aassociated%2520with%2520the%2520regions%2520of%2520interest%2520%2528ROIs%2529%252C%2520i.e.%252C%2520the%2520right%2520and%2520left%2520lungs%253B%250Afinally%2520SAM%2520is%2520given%2520these%2520ROIs%2520as%2520prompts%2520and%2520generates%2520the%2520final%2520segmentation%250Amask%2520for%2520the%2520lungs.%2520Experiments%2520are%2520presented%2520across%2520two%2520Covid-19%2520annotated%250Adatabases%2520which%2520illustrate%2520the%2520improved%2520performance%2520obtained%2520when%2520our%2520method%250Ahas%2520been%2520used%2520for%2520segmentation%2520of%2520the%2520CT%2520scans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM2CLIP2SAM%3A%20Vision%20Language%20Model%20for%20Segmentation%20of%203D%20CT%20Scans%20for%0A%20%20Covid-19%20Detection&entry.906535625=Dimitrios%20Kollias%20and%20Anastasios%20Arsenos%20and%20James%20Wingate%20and%20Stefanos%20Kollias&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20approach%20for%20effective%20segmentation%20of%20images%20that%0Acan%20be%20integrated%20into%20any%20model%20and%20methodology%3B%20the%20paradigm%20that%20we%20choose%0Ais%20classification%20of%20medical%20images%20%283-D%20chest%20CT%20scans%29%20for%20Covid-19%0Adetection.%20Our%20approach%20includes%20a%20combination%20of%20vision-language%20models%20that%0Asegment%20the%20CT%20scans%2C%20which%20are%20then%20fed%20to%20a%20deep%20neural%20architecture%2C%20named%0ARACNet%2C%20for%20Covid-19%20detection.%20In%20particular%2C%20a%20novel%20framework%2C%20named%0ASAM2CLIP2SAM%2C%20is%20introduced%20for%20segmentation%20that%20leverages%20the%20strengths%20of%0Aboth%20Segment%20Anything%20Model%20%28SAM%29%20and%20Contrastive%20Language-Image%20Pre-Training%0A%28CLIP%29%20to%20accurately%20segment%20the%20right%20and%20left%20lungs%20in%20CT%20scans%2C%20subsequently%0Afeeding%20these%20segmented%20outputs%20into%20RACNet%20for%20classification%20of%20COVID-19%20and%0Anon-COVID-19%20cases.%20At%20first%2C%20SAM%20produces%20multiple%20part-based%20segmentation%0Amasks%20for%20each%20slice%20in%20the%20CT%20scan%3B%20then%20CLIP%20selects%20only%20the%20masks%20that%20are%0Aassociated%20with%20the%20regions%20of%20interest%20%28ROIs%29%2C%20i.e.%2C%20the%20right%20and%20left%20lungs%3B%0Afinally%20SAM%20is%20given%20these%20ROIs%20as%20prompts%20and%20generates%20the%20final%20segmentation%0Amask%20for%20the%20lungs.%20Experiments%20are%20presented%20across%20two%20Covid-19%20annotated%0Adatabases%20which%20illustrate%20the%20improved%20performance%20obtained%20when%20our%20method%0Ahas%20been%20used%20for%20segmentation%20of%20the%20CT%20scans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15728v1&entry.124074799=Read"},
{"title": "DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface\n  Reconstruction", "author": "Yuxin Yao and Siyu Ren and Junhui Hou and Zhi Deng and Juyong Zhang and Wenping Wang", "abstract": "  This paper explores the problem of reconstructing temporally consistent\nsurfaces from a 3D point cloud sequence without correspondence. To address this\nchallenging task, we propose DynoSurf, an unsupervised learning framework\nintegrating a template surface representation with a learnable deformation\nfield. Specifically, we design a coarse-to-fine strategy for learning the\ntemplate surface based on the deformable tetrahedron representation.\nFurthermore, we propose a learnable deformation representation based on the\nlearnable control points and blending weights, which can deform the template\nsurface non-rigidly while maintaining the consistency of the local shape.\nExperimental results demonstrate the significant superiority of DynoSurf over\ncurrent state-of-the-art approaches, showcasing its potential as a powerful\ntool for dynamic mesh reconstruction. The code is publicly available at\nhttps://github.com/yaoyx689/DynoSurf.\n", "link": "http://arxiv.org/abs/2403.11586v2", "date": "2024-07-22", "relevancy": 2.7715, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6231}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.52}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynoSurf%3A%20Neural%20Deformation-based%20Temporally%20Consistent%20Dynamic%20Surface%0A%20%20Reconstruction&body=Title%3A%20DynoSurf%3A%20Neural%20Deformation-based%20Temporally%20Consistent%20Dynamic%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Yuxin%20Yao%20and%20Siyu%20Ren%20and%20Junhui%20Hou%20and%20Zhi%20Deng%20and%20Juyong%20Zhang%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20problem%20of%20reconstructing%20temporally%20consistent%0Asurfaces%20from%20a%203D%20point%20cloud%20sequence%20without%20correspondence.%20To%20address%20this%0Achallenging%20task%2C%20we%20propose%20DynoSurf%2C%20an%20unsupervised%20learning%20framework%0Aintegrating%20a%20template%20surface%20representation%20with%20a%20learnable%20deformation%0Afield.%20Specifically%2C%20we%20design%20a%20coarse-to-fine%20strategy%20for%20learning%20the%0Atemplate%20surface%20based%20on%20the%20deformable%20tetrahedron%20representation.%0AFurthermore%2C%20we%20propose%20a%20learnable%20deformation%20representation%20based%20on%20the%0Alearnable%20control%20points%20and%20blending%20weights%2C%20which%20can%20deform%20the%20template%0Asurface%20non-rigidly%20while%20maintaining%20the%20consistency%20of%20the%20local%20shape.%0AExperimental%20results%20demonstrate%20the%20significant%20superiority%20of%20DynoSurf%20over%0Acurrent%20state-of-the-art%20approaches%2C%20showcasing%20its%20potential%20as%20a%20powerful%0Atool%20for%20dynamic%20mesh%20reconstruction.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yaoyx689/DynoSurf.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynoSurf%253A%2520Neural%2520Deformation-based%2520Temporally%2520Consistent%2520Dynamic%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DYuxin%2520Yao%2520and%2520Siyu%2520Ren%2520and%2520Junhui%2520Hou%2520and%2520Zhi%2520Deng%2520and%2520Juyong%2520Zhang%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520problem%2520of%2520reconstructing%2520temporally%2520consistent%250Asurfaces%2520from%2520a%25203D%2520point%2520cloud%2520sequence%2520without%2520correspondence.%2520To%2520address%2520this%250Achallenging%2520task%252C%2520we%2520propose%2520DynoSurf%252C%2520an%2520unsupervised%2520learning%2520framework%250Aintegrating%2520a%2520template%2520surface%2520representation%2520with%2520a%2520learnable%2520deformation%250Afield.%2520Specifically%252C%2520we%2520design%2520a%2520coarse-to-fine%2520strategy%2520for%2520learning%2520the%250Atemplate%2520surface%2520based%2520on%2520the%2520deformable%2520tetrahedron%2520representation.%250AFurthermore%252C%2520we%2520propose%2520a%2520learnable%2520deformation%2520representation%2520based%2520on%2520the%250Alearnable%2520control%2520points%2520and%2520blending%2520weights%252C%2520which%2520can%2520deform%2520the%2520template%250Asurface%2520non-rigidly%2520while%2520maintaining%2520the%2520consistency%2520of%2520the%2520local%2520shape.%250AExperimental%2520results%2520demonstrate%2520the%2520significant%2520superiority%2520of%2520DynoSurf%2520over%250Acurrent%2520state-of-the-art%2520approaches%252C%2520showcasing%2520its%2520potential%2520as%2520a%2520powerful%250Atool%2520for%2520dynamic%2520mesh%2520reconstruction.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yaoyx689/DynoSurf.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynoSurf%3A%20Neural%20Deformation-based%20Temporally%20Consistent%20Dynamic%20Surface%0A%20%20Reconstruction&entry.906535625=Yuxin%20Yao%20and%20Siyu%20Ren%20and%20Junhui%20Hou%20and%20Zhi%20Deng%20and%20Juyong%20Zhang%20and%20Wenping%20Wang&entry.1292438233=%20%20This%20paper%20explores%20the%20problem%20of%20reconstructing%20temporally%20consistent%0Asurfaces%20from%20a%203D%20point%20cloud%20sequence%20without%20correspondence.%20To%20address%20this%0Achallenging%20task%2C%20we%20propose%20DynoSurf%2C%20an%20unsupervised%20learning%20framework%0Aintegrating%20a%20template%20surface%20representation%20with%20a%20learnable%20deformation%0Afield.%20Specifically%2C%20we%20design%20a%20coarse-to-fine%20strategy%20for%20learning%20the%0Atemplate%20surface%20based%20on%20the%20deformable%20tetrahedron%20representation.%0AFurthermore%2C%20we%20propose%20a%20learnable%20deformation%20representation%20based%20on%20the%0Alearnable%20control%20points%20and%20blending%20weights%2C%20which%20can%20deform%20the%20template%0Asurface%20non-rigidly%20while%20maintaining%20the%20consistency%20of%20the%20local%20shape.%0AExperimental%20results%20demonstrate%20the%20significant%20superiority%20of%20DynoSurf%20over%0Acurrent%20state-of-the-art%20approaches%2C%20showcasing%20its%20potential%20as%20a%20powerful%0Atool%20for%20dynamic%20mesh%20reconstruction.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yaoyx689/DynoSurf.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11586v2&entry.124074799=Read"},
{"title": "Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language\n  Encoders", "author": "Laura Niss and Kevin Vogt-Lowell and Theodoros Tsiligkaridis", "abstract": "  Despite the proliferation of large vision-language foundation models,\nestimation of the learning and forgetting outcomes following fine-tuning of\nthese models remains largely unexplored. Inspired by work highlighting the\nsignificance of the modality gap in contrastive dual-encoders, we propose the\nInter-Intra Modal Measure (IIMM). Combining terms quantifying the similarity\nbetween image embeddings and the similarity between incorrect image and label\nembedding pairs, the IIMM functions as a strong predictor of performance\nchanges with fine-tuning. Our extensive empirical analysis across four\nstate-of-the-art vision-language models (CLIP, SigLIP, CoCa, EVA-02-CLIP) and\nfive fine-tuning techniques (full fine-tuning, BitFit, attention-weight tuning,\nLoRA, CLIP-Adapter) demonstrates a strong, statistically significant linear\nrelationship: fine-tuning on tasks with higher IIMM scores produces greater\nin-domain performance gains but also induces more severe out-of-domain\nperformance degradation, with some parameter-efficient fine-tuning (PEFT)\nmethods showing extreme forgetting. We compare our measure against transfer\nscores from state-of-the-art model selection methods and show that the IIMM is\nsignificantly more predictive of accuracy gains. With only a single forward\npass of the target data, practitioners can leverage this key insight to\nheuristically evaluate the degree to which a model can be expected to improve\nfollowing fine-tuning. Given additional knowledge about the model's performance\non a few diverse tasks, this heuristic further evolves into a strong predictor\nof expected performance changes when training for new tasks.\n", "link": "http://arxiv.org/abs/2407.15731v1", "date": "2024-07-22", "relevancy": 2.762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5769}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5415}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Embeddings%20Inform%20Learning%20and%20Forgetting%20with%20Vision-Language%0A%20%20Encoders&body=Title%3A%20Zero-Shot%20Embeddings%20Inform%20Learning%20and%20Forgetting%20with%20Vision-Language%0A%20%20Encoders%0AAuthor%3A%20Laura%20Niss%20and%20Kevin%20Vogt-Lowell%20and%20Theodoros%20Tsiligkaridis%0AAbstract%3A%20%20%20Despite%20the%20proliferation%20of%20large%20vision-language%20foundation%20models%2C%0Aestimation%20of%20the%20learning%20and%20forgetting%20outcomes%20following%20fine-tuning%20of%0Athese%20models%20remains%20largely%20unexplored.%20Inspired%20by%20work%20highlighting%20the%0Asignificance%20of%20the%20modality%20gap%20in%20contrastive%20dual-encoders%2C%20we%20propose%20the%0AInter-Intra%20Modal%20Measure%20%28IIMM%29.%20Combining%20terms%20quantifying%20the%20similarity%0Abetween%20image%20embeddings%20and%20the%20similarity%20between%20incorrect%20image%20and%20label%0Aembedding%20pairs%2C%20the%20IIMM%20functions%20as%20a%20strong%20predictor%20of%20performance%0Achanges%20with%20fine-tuning.%20Our%20extensive%20empirical%20analysis%20across%20four%0Astate-of-the-art%20vision-language%20models%20%28CLIP%2C%20SigLIP%2C%20CoCa%2C%20EVA-02-CLIP%29%20and%0Afive%20fine-tuning%20techniques%20%28full%20fine-tuning%2C%20BitFit%2C%20attention-weight%20tuning%2C%0ALoRA%2C%20CLIP-Adapter%29%20demonstrates%20a%20strong%2C%20statistically%20significant%20linear%0Arelationship%3A%20fine-tuning%20on%20tasks%20with%20higher%20IIMM%20scores%20produces%20greater%0Ain-domain%20performance%20gains%20but%20also%20induces%20more%20severe%20out-of-domain%0Aperformance%20degradation%2C%20with%20some%20parameter-efficient%20fine-tuning%20%28PEFT%29%0Amethods%20showing%20extreme%20forgetting.%20We%20compare%20our%20measure%20against%20transfer%0Ascores%20from%20state-of-the-art%20model%20selection%20methods%20and%20show%20that%20the%20IIMM%20is%0Asignificantly%20more%20predictive%20of%20accuracy%20gains.%20With%20only%20a%20single%20forward%0Apass%20of%20the%20target%20data%2C%20practitioners%20can%20leverage%20this%20key%20insight%20to%0Aheuristically%20evaluate%20the%20degree%20to%20which%20a%20model%20can%20be%20expected%20to%20improve%0Afollowing%20fine-tuning.%20Given%20additional%20knowledge%20about%20the%20model%27s%20performance%0Aon%20a%20few%20diverse%20tasks%2C%20this%20heuristic%20further%20evolves%20into%20a%20strong%20predictor%0Aof%20expected%20performance%20changes%20when%20training%20for%20new%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Embeddings%2520Inform%2520Learning%2520and%2520Forgetting%2520with%2520Vision-Language%250A%2520%2520Encoders%26entry.906535625%3DLaura%2520Niss%2520and%2520Kevin%2520Vogt-Lowell%2520and%2520Theodoros%2520Tsiligkaridis%26entry.1292438233%3D%2520%2520Despite%2520the%2520proliferation%2520of%2520large%2520vision-language%2520foundation%2520models%252C%250Aestimation%2520of%2520the%2520learning%2520and%2520forgetting%2520outcomes%2520following%2520fine-tuning%2520of%250Athese%2520models%2520remains%2520largely%2520unexplored.%2520Inspired%2520by%2520work%2520highlighting%2520the%250Asignificance%2520of%2520the%2520modality%2520gap%2520in%2520contrastive%2520dual-encoders%252C%2520we%2520propose%2520the%250AInter-Intra%2520Modal%2520Measure%2520%2528IIMM%2529.%2520Combining%2520terms%2520quantifying%2520the%2520similarity%250Abetween%2520image%2520embeddings%2520and%2520the%2520similarity%2520between%2520incorrect%2520image%2520and%2520label%250Aembedding%2520pairs%252C%2520the%2520IIMM%2520functions%2520as%2520a%2520strong%2520predictor%2520of%2520performance%250Achanges%2520with%2520fine-tuning.%2520Our%2520extensive%2520empirical%2520analysis%2520across%2520four%250Astate-of-the-art%2520vision-language%2520models%2520%2528CLIP%252C%2520SigLIP%252C%2520CoCa%252C%2520EVA-02-CLIP%2529%2520and%250Afive%2520fine-tuning%2520techniques%2520%2528full%2520fine-tuning%252C%2520BitFit%252C%2520attention-weight%2520tuning%252C%250ALoRA%252C%2520CLIP-Adapter%2529%2520demonstrates%2520a%2520strong%252C%2520statistically%2520significant%2520linear%250Arelationship%253A%2520fine-tuning%2520on%2520tasks%2520with%2520higher%2520IIMM%2520scores%2520produces%2520greater%250Ain-domain%2520performance%2520gains%2520but%2520also%2520induces%2520more%2520severe%2520out-of-domain%250Aperformance%2520degradation%252C%2520with%2520some%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%250Amethods%2520showing%2520extreme%2520forgetting.%2520We%2520compare%2520our%2520measure%2520against%2520transfer%250Ascores%2520from%2520state-of-the-art%2520model%2520selection%2520methods%2520and%2520show%2520that%2520the%2520IIMM%2520is%250Asignificantly%2520more%2520predictive%2520of%2520accuracy%2520gains.%2520With%2520only%2520a%2520single%2520forward%250Apass%2520of%2520the%2520target%2520data%252C%2520practitioners%2520can%2520leverage%2520this%2520key%2520insight%2520to%250Aheuristically%2520evaluate%2520the%2520degree%2520to%2520which%2520a%2520model%2520can%2520be%2520expected%2520to%2520improve%250Afollowing%2520fine-tuning.%2520Given%2520additional%2520knowledge%2520about%2520the%2520model%2527s%2520performance%250Aon%2520a%2520few%2520diverse%2520tasks%252C%2520this%2520heuristic%2520further%2520evolves%2520into%2520a%2520strong%2520predictor%250Aof%2520expected%2520performance%2520changes%2520when%2520training%2520for%2520new%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Embeddings%20Inform%20Learning%20and%20Forgetting%20with%20Vision-Language%0A%20%20Encoders&entry.906535625=Laura%20Niss%20and%20Kevin%20Vogt-Lowell%20and%20Theodoros%20Tsiligkaridis&entry.1292438233=%20%20Despite%20the%20proliferation%20of%20large%20vision-language%20foundation%20models%2C%0Aestimation%20of%20the%20learning%20and%20forgetting%20outcomes%20following%20fine-tuning%20of%0Athese%20models%20remains%20largely%20unexplored.%20Inspired%20by%20work%20highlighting%20the%0Asignificance%20of%20the%20modality%20gap%20in%20contrastive%20dual-encoders%2C%20we%20propose%20the%0AInter-Intra%20Modal%20Measure%20%28IIMM%29.%20Combining%20terms%20quantifying%20the%20similarity%0Abetween%20image%20embeddings%20and%20the%20similarity%20between%20incorrect%20image%20and%20label%0Aembedding%20pairs%2C%20the%20IIMM%20functions%20as%20a%20strong%20predictor%20of%20performance%0Achanges%20with%20fine-tuning.%20Our%20extensive%20empirical%20analysis%20across%20four%0Astate-of-the-art%20vision-language%20models%20%28CLIP%2C%20SigLIP%2C%20CoCa%2C%20EVA-02-CLIP%29%20and%0Afive%20fine-tuning%20techniques%20%28full%20fine-tuning%2C%20BitFit%2C%20attention-weight%20tuning%2C%0ALoRA%2C%20CLIP-Adapter%29%20demonstrates%20a%20strong%2C%20statistically%20significant%20linear%0Arelationship%3A%20fine-tuning%20on%20tasks%20with%20higher%20IIMM%20scores%20produces%20greater%0Ain-domain%20performance%20gains%20but%20also%20induces%20more%20severe%20out-of-domain%0Aperformance%20degradation%2C%20with%20some%20parameter-efficient%20fine-tuning%20%28PEFT%29%0Amethods%20showing%20extreme%20forgetting.%20We%20compare%20our%20measure%20against%20transfer%0Ascores%20from%20state-of-the-art%20model%20selection%20methods%20and%20show%20that%20the%20IIMM%20is%0Asignificantly%20more%20predictive%20of%20accuracy%20gains.%20With%20only%20a%20single%20forward%0Apass%20of%20the%20target%20data%2C%20practitioners%20can%20leverage%20this%20key%20insight%20to%0Aheuristically%20evaluate%20the%20degree%20to%20which%20a%20model%20can%20be%20expected%20to%20improve%0Afollowing%20fine-tuning.%20Given%20additional%20knowledge%20about%20the%20model%27s%20performance%0Aon%20a%20few%20diverse%20tasks%2C%20this%20heuristic%20further%20evolves%20into%20a%20strong%20predictor%0Aof%20expected%20performance%20changes%20when%20training%20for%20new%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15731v1&entry.124074799=Read"},
{"title": "Dark Side Augmentation: Generating Diverse Night Examples for Metric\n  Learning", "author": "Albert Mohwald and Tomas Jenicek and Ond\u0159ej Chum", "abstract": "  Image retrieval methods based on CNN descriptors rely on metric learning from\na large number of diverse examples of positive and negative image pairs.\nDomains, such as night-time images, with limited availability and variability\nof training data suffer from poor retrieval performance even with methods\nperforming well on standard benchmarks. We propose to train a GAN-based\nsynthetic-image generator, translating available day-time image examples into\nnight images. Such a generator is used in metric learning as a form of\naugmentation, supplying training data to the scarce domain. Various types of\ngenerators are evaluated and analyzed. We contribute with a novel light-weight\nGAN architecture that enforces the consistency between the original and\ntranslated image through edge consistency. The proposed architecture also\nallows a simultaneous training of an edge detector that operates on both night\nand day images. To further increase the variability in the training examples\nand to maximize the generalization of the trained model, we propose a novel\nmethod of diverse anchor mining.\n  The proposed method improves over the state-of-the-art results on a standard\nTokyo 24/7 day-night retrieval benchmark while preserving the performance on\nOxford and Paris datasets. This is achieved without the need of training image\npairs of matching day and night images. The source code is available at\nhttps://github.com/mohwald/gandtr .\n", "link": "http://arxiv.org/abs/2309.16351v2", "date": "2024-07-22", "relevancy": 2.7572, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5628}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5519}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dark%20Side%20Augmentation%3A%20Generating%20Diverse%20Night%20Examples%20for%20Metric%0A%20%20Learning&body=Title%3A%20Dark%20Side%20Augmentation%3A%20Generating%20Diverse%20Night%20Examples%20for%20Metric%0A%20%20Learning%0AAuthor%3A%20Albert%20Mohwald%20and%20Tomas%20Jenicek%20and%20Ond%C5%99ej%20Chum%0AAbstract%3A%20%20%20Image%20retrieval%20methods%20based%20on%20CNN%20descriptors%20rely%20on%20metric%20learning%20from%0Aa%20large%20number%20of%20diverse%20examples%20of%20positive%20and%20negative%20image%20pairs.%0ADomains%2C%20such%20as%20night-time%20images%2C%20with%20limited%20availability%20and%20variability%0Aof%20training%20data%20suffer%20from%20poor%20retrieval%20performance%20even%20with%20methods%0Aperforming%20well%20on%20standard%20benchmarks.%20We%20propose%20to%20train%20a%20GAN-based%0Asynthetic-image%20generator%2C%20translating%20available%20day-time%20image%20examples%20into%0Anight%20images.%20Such%20a%20generator%20is%20used%20in%20metric%20learning%20as%20a%20form%20of%0Aaugmentation%2C%20supplying%20training%20data%20to%20the%20scarce%20domain.%20Various%20types%20of%0Agenerators%20are%20evaluated%20and%20analyzed.%20We%20contribute%20with%20a%20novel%20light-weight%0AGAN%20architecture%20that%20enforces%20the%20consistency%20between%20the%20original%20and%0Atranslated%20image%20through%20edge%20consistency.%20The%20proposed%20architecture%20also%0Aallows%20a%20simultaneous%20training%20of%20an%20edge%20detector%20that%20operates%20on%20both%20night%0Aand%20day%20images.%20To%20further%20increase%20the%20variability%20in%20the%20training%20examples%0Aand%20to%20maximize%20the%20generalization%20of%20the%20trained%20model%2C%20we%20propose%20a%20novel%0Amethod%20of%20diverse%20anchor%20mining.%0A%20%20The%20proposed%20method%20improves%20over%20the%20state-of-the-art%20results%20on%20a%20standard%0ATokyo%2024/7%20day-night%20retrieval%20benchmark%20while%20preserving%20the%20performance%20on%0AOxford%20and%20Paris%20datasets.%20This%20is%20achieved%20without%20the%20need%20of%20training%20image%0Apairs%20of%20matching%20day%20and%20night%20images.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/mohwald/gandtr%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDark%2520Side%2520Augmentation%253A%2520Generating%2520Diverse%2520Night%2520Examples%2520for%2520Metric%250A%2520%2520Learning%26entry.906535625%3DAlbert%2520Mohwald%2520and%2520Tomas%2520Jenicek%2520and%2520Ond%25C5%2599ej%2520Chum%26entry.1292438233%3D%2520%2520Image%2520retrieval%2520methods%2520based%2520on%2520CNN%2520descriptors%2520rely%2520on%2520metric%2520learning%2520from%250Aa%2520large%2520number%2520of%2520diverse%2520examples%2520of%2520positive%2520and%2520negative%2520image%2520pairs.%250ADomains%252C%2520such%2520as%2520night-time%2520images%252C%2520with%2520limited%2520availability%2520and%2520variability%250Aof%2520training%2520data%2520suffer%2520from%2520poor%2520retrieval%2520performance%2520even%2520with%2520methods%250Aperforming%2520well%2520on%2520standard%2520benchmarks.%2520We%2520propose%2520to%2520train%2520a%2520GAN-based%250Asynthetic-image%2520generator%252C%2520translating%2520available%2520day-time%2520image%2520examples%2520into%250Anight%2520images.%2520Such%2520a%2520generator%2520is%2520used%2520in%2520metric%2520learning%2520as%2520a%2520form%2520of%250Aaugmentation%252C%2520supplying%2520training%2520data%2520to%2520the%2520scarce%2520domain.%2520Various%2520types%2520of%250Agenerators%2520are%2520evaluated%2520and%2520analyzed.%2520We%2520contribute%2520with%2520a%2520novel%2520light-weight%250AGAN%2520architecture%2520that%2520enforces%2520the%2520consistency%2520between%2520the%2520original%2520and%250Atranslated%2520image%2520through%2520edge%2520consistency.%2520The%2520proposed%2520architecture%2520also%250Aallows%2520a%2520simultaneous%2520training%2520of%2520an%2520edge%2520detector%2520that%2520operates%2520on%2520both%2520night%250Aand%2520day%2520images.%2520To%2520further%2520increase%2520the%2520variability%2520in%2520the%2520training%2520examples%250Aand%2520to%2520maximize%2520the%2520generalization%2520of%2520the%2520trained%2520model%252C%2520we%2520propose%2520a%2520novel%250Amethod%2520of%2520diverse%2520anchor%2520mining.%250A%2520%2520The%2520proposed%2520method%2520improves%2520over%2520the%2520state-of-the-art%2520results%2520on%2520a%2520standard%250ATokyo%252024/7%2520day-night%2520retrieval%2520benchmark%2520while%2520preserving%2520the%2520performance%2520on%250AOxford%2520and%2520Paris%2520datasets.%2520This%2520is%2520achieved%2520without%2520the%2520need%2520of%2520training%2520image%250Apairs%2520of%2520matching%2520day%2520and%2520night%2520images.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mohwald/gandtr%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dark%20Side%20Augmentation%3A%20Generating%20Diverse%20Night%20Examples%20for%20Metric%0A%20%20Learning&entry.906535625=Albert%20Mohwald%20and%20Tomas%20Jenicek%20and%20Ond%C5%99ej%20Chum&entry.1292438233=%20%20Image%20retrieval%20methods%20based%20on%20CNN%20descriptors%20rely%20on%20metric%20learning%20from%0Aa%20large%20number%20of%20diverse%20examples%20of%20positive%20and%20negative%20image%20pairs.%0ADomains%2C%20such%20as%20night-time%20images%2C%20with%20limited%20availability%20and%20variability%0Aof%20training%20data%20suffer%20from%20poor%20retrieval%20performance%20even%20with%20methods%0Aperforming%20well%20on%20standard%20benchmarks.%20We%20propose%20to%20train%20a%20GAN-based%0Asynthetic-image%20generator%2C%20translating%20available%20day-time%20image%20examples%20into%0Anight%20images.%20Such%20a%20generator%20is%20used%20in%20metric%20learning%20as%20a%20form%20of%0Aaugmentation%2C%20supplying%20training%20data%20to%20the%20scarce%20domain.%20Various%20types%20of%0Agenerators%20are%20evaluated%20and%20analyzed.%20We%20contribute%20with%20a%20novel%20light-weight%0AGAN%20architecture%20that%20enforces%20the%20consistency%20between%20the%20original%20and%0Atranslated%20image%20through%20edge%20consistency.%20The%20proposed%20architecture%20also%0Aallows%20a%20simultaneous%20training%20of%20an%20edge%20detector%20that%20operates%20on%20both%20night%0Aand%20day%20images.%20To%20further%20increase%20the%20variability%20in%20the%20training%20examples%0Aand%20to%20maximize%20the%20generalization%20of%20the%20trained%20model%2C%20we%20propose%20a%20novel%0Amethod%20of%20diverse%20anchor%20mining.%0A%20%20The%20proposed%20method%20improves%20over%20the%20state-of-the-art%20results%20on%20a%20standard%0ATokyo%2024/7%20day-night%20retrieval%20benchmark%20while%20preserving%20the%20performance%20on%0AOxford%20and%20Paris%20datasets.%20This%20is%20achieved%20without%20the%20need%20of%20training%20image%0Apairs%20of%20matching%20day%20and%20night%20images.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/mohwald/gandtr%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16351v2&entry.124074799=Read"},
{"title": "Differentiable Convex Polyhedra Optimization from Multi-view Images", "author": "Daxuan Ren and Haiyi Mei and Hezi Shi and Jianmin Zheng and Jianfei Cai and Lei Yang", "abstract": "  This paper presents a novel approach for the differentiable rendering of\nconvex polyhedra, addressing the limitations of recent methods that rely on\nimplicit field supervision. Our technique introduces a strategy that combines\nnon-differentiable computation of hyperplane intersection through duality\ntransform with differentiable optimization for vertex positioning with\nthree-plane intersection, enabling gradient-based optimization without the need\nfor 3D implicit fields. This allows for efficient shape representation across a\nrange of applications, from shape parsing to compact mesh reconstruction. This\nwork not only overcomes the challenges of previous approaches but also sets a\nnew standard for representing shapes with convex polyhedra.\n", "link": "http://arxiv.org/abs/2407.15686v1", "date": "2024-07-22", "relevancy": 2.7357, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5502}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5502}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Convex%20Polyhedra%20Optimization%20from%20Multi-view%20Images&body=Title%3A%20Differentiable%20Convex%20Polyhedra%20Optimization%20from%20Multi-view%20Images%0AAuthor%3A%20Daxuan%20Ren%20and%20Haiyi%20Mei%20and%20Hezi%20Shi%20and%20Jianmin%20Zheng%20and%20Jianfei%20Cai%20and%20Lei%20Yang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20the%20differentiable%20rendering%20of%0Aconvex%20polyhedra%2C%20addressing%20the%20limitations%20of%20recent%20methods%20that%20rely%20on%0Aimplicit%20field%20supervision.%20Our%20technique%20introduces%20a%20strategy%20that%20combines%0Anon-differentiable%20computation%20of%20hyperplane%20intersection%20through%20duality%0Atransform%20with%20differentiable%20optimization%20for%20vertex%20positioning%20with%0Athree-plane%20intersection%2C%20enabling%20gradient-based%20optimization%20without%20the%20need%0Afor%203D%20implicit%20fields.%20This%20allows%20for%20efficient%20shape%20representation%20across%20a%0Arange%20of%20applications%2C%20from%20shape%20parsing%20to%20compact%20mesh%20reconstruction.%20This%0Awork%20not%20only%20overcomes%20the%20challenges%20of%20previous%20approaches%20but%20also%20sets%20a%0Anew%20standard%20for%20representing%20shapes%20with%20convex%20polyhedra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Convex%2520Polyhedra%2520Optimization%2520from%2520Multi-view%2520Images%26entry.906535625%3DDaxuan%2520Ren%2520and%2520Haiyi%2520Mei%2520and%2520Hezi%2520Shi%2520and%2520Jianmin%2520Zheng%2520and%2520Jianfei%2520Cai%2520and%2520Lei%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520the%2520differentiable%2520rendering%2520of%250Aconvex%2520polyhedra%252C%2520addressing%2520the%2520limitations%2520of%2520recent%2520methods%2520that%2520rely%2520on%250Aimplicit%2520field%2520supervision.%2520Our%2520technique%2520introduces%2520a%2520strategy%2520that%2520combines%250Anon-differentiable%2520computation%2520of%2520hyperplane%2520intersection%2520through%2520duality%250Atransform%2520with%2520differentiable%2520optimization%2520for%2520vertex%2520positioning%2520with%250Athree-plane%2520intersection%252C%2520enabling%2520gradient-based%2520optimization%2520without%2520the%2520need%250Afor%25203D%2520implicit%2520fields.%2520This%2520allows%2520for%2520efficient%2520shape%2520representation%2520across%2520a%250Arange%2520of%2520applications%252C%2520from%2520shape%2520parsing%2520to%2520compact%2520mesh%2520reconstruction.%2520This%250Awork%2520not%2520only%2520overcomes%2520the%2520challenges%2520of%2520previous%2520approaches%2520but%2520also%2520sets%2520a%250Anew%2520standard%2520for%2520representing%2520shapes%2520with%2520convex%2520polyhedra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Convex%20Polyhedra%20Optimization%20from%20Multi-view%20Images&entry.906535625=Daxuan%20Ren%20and%20Haiyi%20Mei%20and%20Hezi%20Shi%20and%20Jianmin%20Zheng%20and%20Jianfei%20Cai%20and%20Lei%20Yang&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20the%20differentiable%20rendering%20of%0Aconvex%20polyhedra%2C%20addressing%20the%20limitations%20of%20recent%20methods%20that%20rely%20on%0Aimplicit%20field%20supervision.%20Our%20technique%20introduces%20a%20strategy%20that%20combines%0Anon-differentiable%20computation%20of%20hyperplane%20intersection%20through%20duality%0Atransform%20with%20differentiable%20optimization%20for%20vertex%20positioning%20with%0Athree-plane%20intersection%2C%20enabling%20gradient-based%20optimization%20without%20the%20need%0Afor%203D%20implicit%20fields.%20This%20allows%20for%20efficient%20shape%20representation%20across%20a%0Arange%20of%20applications%2C%20from%20shape%20parsing%20to%20compact%20mesh%20reconstruction.%20This%0Awork%20not%20only%20overcomes%20the%20challenges%20of%20previous%20approaches%20but%20also%20sets%20a%0Anew%20standard%20for%20representing%20shapes%20with%20convex%20polyhedra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15686v1&entry.124074799=Read"},
{"title": "Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning", "author": "Shiming Chen and Wenjin Hou and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Zero-shot learning (ZSL) recognizes the unseen classes by conducting\nvisual-semantic interactions to transfer semantic knowledge from seen classes\nto unseen ones, supported by semantic information (e.g., attributes). However,\nexisting ZSL methods simply extract visual features using a pre-trained network\nbackbone (i.e., CNN or ViT), which fail to learn matched visual-semantic\ncorrespondences for representing semantic-related visual features as lacking of\nthe guidance of semantic information, resulting in undesirable visual-semantic\ninteractions. To tackle this issue, we propose a progressive semantic-guided\nvision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly\nconsiders two properties in the whole network: i) discover the semantic-related\nvisual representations explicitly, and ii) discard the semantic-unrelated\nvisual information. Specifically, we first introduce semantic-embedded token\nlearning to improve the visual-semantic correspondences via semantic\nenhancement and discover the semantic-related visual tokens explicitly with\nsemantic-guided token attention. Then, we fuse low semantic-visual\ncorrespondence visual tokens to discard the semantic-unrelated visual\ninformation for visual enhancement. These two operations are integrated into\nvarious encoders to progressively learn semantic-related visual representations\nfor accurate visual-semantic interactions in ZSL. The extensive experiments\nshow that our ZSLViT achieves significant performance gains on three popular\nbenchmark datasets, i.e., CUB, SUN, and AWA2. Codes are available at:\nhttps://github.com/shiming-chen/ZSLViT .\n", "link": "http://arxiv.org/abs/2404.07713v2", "date": "2024-07-22", "relevancy": 2.6975, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5215}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Semantic-Guided%20Vision%20Transformer%20for%20Zero-Shot%20Learning&body=Title%3A%20Progressive%20Semantic-Guided%20Vision%20Transformer%20for%20Zero-Shot%20Learning%0AAuthor%3A%20Shiming%20Chen%20and%20Wenjin%20Hou%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Zero-shot%20learning%20%28ZSL%29%20recognizes%20the%20unseen%20classes%20by%20conducting%0Avisual-semantic%20interactions%20to%20transfer%20semantic%20knowledge%20from%20seen%20classes%0Ato%20unseen%20ones%2C%20supported%20by%20semantic%20information%20%28e.g.%2C%20attributes%29.%20However%2C%0Aexisting%20ZSL%20methods%20simply%20extract%20visual%20features%20using%20a%20pre-trained%20network%0Abackbone%20%28i.e.%2C%20CNN%20or%20ViT%29%2C%20which%20fail%20to%20learn%20matched%20visual-semantic%0Acorrespondences%20for%20representing%20semantic-related%20visual%20features%20as%20lacking%20of%0Athe%20guidance%20of%20semantic%20information%2C%20resulting%20in%20undesirable%20visual-semantic%0Ainteractions.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20progressive%20semantic-guided%0Avision%20transformer%20for%20zero-shot%20learning%20%28dubbed%20ZSLViT%29.%20ZSLViT%20mainly%0Aconsiders%20two%20properties%20in%20the%20whole%20network%3A%20i%29%20discover%20the%20semantic-related%0Avisual%20representations%20explicitly%2C%20and%20ii%29%20discard%20the%20semantic-unrelated%0Avisual%20information.%20Specifically%2C%20we%20first%20introduce%20semantic-embedded%20token%0Alearning%20to%20improve%20the%20visual-semantic%20correspondences%20via%20semantic%0Aenhancement%20and%20discover%20the%20semantic-related%20visual%20tokens%20explicitly%20with%0Asemantic-guided%20token%20attention.%20Then%2C%20we%20fuse%20low%20semantic-visual%0Acorrespondence%20visual%20tokens%20to%20discard%20the%20semantic-unrelated%20visual%0Ainformation%20for%20visual%20enhancement.%20These%20two%20operations%20are%20integrated%20into%0Avarious%20encoders%20to%20progressively%20learn%20semantic-related%20visual%20representations%0Afor%20accurate%20visual-semantic%20interactions%20in%20ZSL.%20The%20extensive%20experiments%0Ashow%20that%20our%20ZSLViT%20achieves%20significant%20performance%20gains%20on%20three%20popular%0Abenchmark%20datasets%2C%20i.e.%2C%20CUB%2C%20SUN%2C%20and%20AWA2.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/shiming-chen/ZSLViT%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Semantic-Guided%2520Vision%2520Transformer%2520for%2520Zero-Shot%2520Learning%26entry.906535625%3DShiming%2520Chen%2520and%2520Wenjin%2520Hou%2520and%2520Salman%2520Khan%2520and%2520Fahad%2520Shahbaz%2520Khan%26entry.1292438233%3D%2520%2520Zero-shot%2520learning%2520%2528ZSL%2529%2520recognizes%2520the%2520unseen%2520classes%2520by%2520conducting%250Avisual-semantic%2520interactions%2520to%2520transfer%2520semantic%2520knowledge%2520from%2520seen%2520classes%250Ato%2520unseen%2520ones%252C%2520supported%2520by%2520semantic%2520information%2520%2528e.g.%252C%2520attributes%2529.%2520However%252C%250Aexisting%2520ZSL%2520methods%2520simply%2520extract%2520visual%2520features%2520using%2520a%2520pre-trained%2520network%250Abackbone%2520%2528i.e.%252C%2520CNN%2520or%2520ViT%2529%252C%2520which%2520fail%2520to%2520learn%2520matched%2520visual-semantic%250Acorrespondences%2520for%2520representing%2520semantic-related%2520visual%2520features%2520as%2520lacking%2520of%250Athe%2520guidance%2520of%2520semantic%2520information%252C%2520resulting%2520in%2520undesirable%2520visual-semantic%250Ainteractions.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520progressive%2520semantic-guided%250Avision%2520transformer%2520for%2520zero-shot%2520learning%2520%2528dubbed%2520ZSLViT%2529.%2520ZSLViT%2520mainly%250Aconsiders%2520two%2520properties%2520in%2520the%2520whole%2520network%253A%2520i%2529%2520discover%2520the%2520semantic-related%250Avisual%2520representations%2520explicitly%252C%2520and%2520ii%2529%2520discard%2520the%2520semantic-unrelated%250Avisual%2520information.%2520Specifically%252C%2520we%2520first%2520introduce%2520semantic-embedded%2520token%250Alearning%2520to%2520improve%2520the%2520visual-semantic%2520correspondences%2520via%2520semantic%250Aenhancement%2520and%2520discover%2520the%2520semantic-related%2520visual%2520tokens%2520explicitly%2520with%250Asemantic-guided%2520token%2520attention.%2520Then%252C%2520we%2520fuse%2520low%2520semantic-visual%250Acorrespondence%2520visual%2520tokens%2520to%2520discard%2520the%2520semantic-unrelated%2520visual%250Ainformation%2520for%2520visual%2520enhancement.%2520These%2520two%2520operations%2520are%2520integrated%2520into%250Avarious%2520encoders%2520to%2520progressively%2520learn%2520semantic-related%2520visual%2520representations%250Afor%2520accurate%2520visual-semantic%2520interactions%2520in%2520ZSL.%2520The%2520extensive%2520experiments%250Ashow%2520that%2520our%2520ZSLViT%2520achieves%2520significant%2520performance%2520gains%2520on%2520three%2520popular%250Abenchmark%2520datasets%252C%2520i.e.%252C%2520CUB%252C%2520SUN%252C%2520and%2520AWA2.%2520Codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/shiming-chen/ZSLViT%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Semantic-Guided%20Vision%20Transformer%20for%20Zero-Shot%20Learning&entry.906535625=Shiming%20Chen%20and%20Wenjin%20Hou%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Zero-shot%20learning%20%28ZSL%29%20recognizes%20the%20unseen%20classes%20by%20conducting%0Avisual-semantic%20interactions%20to%20transfer%20semantic%20knowledge%20from%20seen%20classes%0Ato%20unseen%20ones%2C%20supported%20by%20semantic%20information%20%28e.g.%2C%20attributes%29.%20However%2C%0Aexisting%20ZSL%20methods%20simply%20extract%20visual%20features%20using%20a%20pre-trained%20network%0Abackbone%20%28i.e.%2C%20CNN%20or%20ViT%29%2C%20which%20fail%20to%20learn%20matched%20visual-semantic%0Acorrespondences%20for%20representing%20semantic-related%20visual%20features%20as%20lacking%20of%0Athe%20guidance%20of%20semantic%20information%2C%20resulting%20in%20undesirable%20visual-semantic%0Ainteractions.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20progressive%20semantic-guided%0Avision%20transformer%20for%20zero-shot%20learning%20%28dubbed%20ZSLViT%29.%20ZSLViT%20mainly%0Aconsiders%20two%20properties%20in%20the%20whole%20network%3A%20i%29%20discover%20the%20semantic-related%0Avisual%20representations%20explicitly%2C%20and%20ii%29%20discard%20the%20semantic-unrelated%0Avisual%20information.%20Specifically%2C%20we%20first%20introduce%20semantic-embedded%20token%0Alearning%20to%20improve%20the%20visual-semantic%20correspondences%20via%20semantic%0Aenhancement%20and%20discover%20the%20semantic-related%20visual%20tokens%20explicitly%20with%0Asemantic-guided%20token%20attention.%20Then%2C%20we%20fuse%20low%20semantic-visual%0Acorrespondence%20visual%20tokens%20to%20discard%20the%20semantic-unrelated%20visual%0Ainformation%20for%20visual%20enhancement.%20These%20two%20operations%20are%20integrated%20into%0Avarious%20encoders%20to%20progressively%20learn%20semantic-related%20visual%20representations%0Afor%20accurate%20visual-semantic%20interactions%20in%20ZSL.%20The%20extensive%20experiments%0Ashow%20that%20our%20ZSLViT%20achieves%20significant%20performance%20gains%20on%20three%20popular%0Abenchmark%20datasets%2C%20i.e.%2C%20CUB%2C%20SUN%2C%20and%20AWA2.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/shiming-chen/ZSLViT%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07713v2&entry.124074799=Read"},
{"title": "TokenPacker: Efficient Visual Projector for Multimodal LLM", "author": "Wentong Li and Yuqian Yuan and Jian Liu and Dongqi Tang and Song Wang and Jianke Zhu and Lei Zhang", "abstract": "  The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.\n", "link": "http://arxiv.org/abs/2407.02392v2", "date": "2024-07-22", "relevancy": 2.6902, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5691}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5497}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM&body=Title%3A%20TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM%0AAuthor%3A%20Wentong%20Li%20and%20Yuqian%20Yuan%20and%20Jian%20Liu%20and%20Dongqi%20Tang%20and%20Song%20Wang%20and%20Jianke%20Zhu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20The%20visual%20projector%20serves%20as%20an%20essential%20bridge%20between%20the%20visual%20encoder%0Aand%20the%20Large%20Language%20Model%20%28LLM%29%20in%20a%20Multimodal%20LLM%20%28MLLM%29.%20Typically%2C%20MLLMs%0Aadopt%20a%20simple%20MLP%20to%20preserve%20all%20visual%20contexts%20via%20one-to-one%0Atransformation.%20However%2C%20the%20visual%20tokens%20are%20redundant%20and%20can%20be%0Aconsiderably%20increased%20when%20dealing%20with%20high-resolution%20images%2C%20impairing%20the%0Aefficiency%20of%20MLLMs%20significantly.%20Some%20recent%20works%20have%20introduced%20resampler%0Aor%20abstractor%20to%20reduce%20the%20number%20of%20resulting%20visual%20tokens.%20Unfortunately%2C%0Athey%20fail%20to%20capture%20finer%20details%20and%20undermine%20the%20visual%20reasoning%0Acapabilities%20of%20MLLMs.%20In%20this%20work%2C%20we%20propose%20a%20novel%20visual%20projector%2C%20which%0Aadopts%20a%20coarse-to-fine%20scheme%20to%20inject%20the%20enriched%20characteristics%20to%0Agenerate%20the%20condensed%20visual%20tokens.%20In%20specific%2C%20we%20first%20interpolate%20the%0Avisual%20features%20as%20a%20low-resolution%20point%20query%2C%20providing%20the%20overall%20visual%0Arepresentation%20as%20the%20foundation.%20Then%2C%20we%20introduce%20a%20region-to-point%0Ainjection%20module%20that%20utilizes%20high-resolution%2C%20multi-level%20region-based%20cues%0Aas%20fine-grained%20reference%20keys%20and%20values%2C%20allowing%20them%20to%20be%20fully%20absorbed%0Awithin%20the%20corresponding%20local%20context%20region.%20This%20step%20effectively%20updates%0Athe%20coarse%20point%20query%2C%20transforming%20it%20into%20an%20enriched%20one%20for%20the%20subsequent%0ALLM%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20compresses%0Athe%20visual%20tokens%20by%2075%25~89%25%2C%20while%20achieves%20comparable%20or%20even%20better%0Aperformance%20across%20diverse%20benchmarks%20with%20significantly%20higher%20efficiency.%20The%0Asource%20codes%20can%20be%20found%20at%20https%3A//github.com/CircleRadon/TokenPacker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02392v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenPacker%253A%2520Efficient%2520Visual%2520Projector%2520for%2520Multimodal%2520LLM%26entry.906535625%3DWentong%2520Li%2520and%2520Yuqian%2520Yuan%2520and%2520Jian%2520Liu%2520and%2520Dongqi%2520Tang%2520and%2520Song%2520Wang%2520and%2520Jianke%2520Zhu%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520visual%2520projector%2520serves%2520as%2520an%2520essential%2520bridge%2520between%2520the%2520visual%2520encoder%250Aand%2520the%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520in%2520a%2520Multimodal%2520LLM%2520%2528MLLM%2529.%2520Typically%252C%2520MLLMs%250Aadopt%2520a%2520simple%2520MLP%2520to%2520preserve%2520all%2520visual%2520contexts%2520via%2520one-to-one%250Atransformation.%2520However%252C%2520the%2520visual%2520tokens%2520are%2520redundant%2520and%2520can%2520be%250Aconsiderably%2520increased%2520when%2520dealing%2520with%2520high-resolution%2520images%252C%2520impairing%2520the%250Aefficiency%2520of%2520MLLMs%2520significantly.%2520Some%2520recent%2520works%2520have%2520introduced%2520resampler%250Aor%2520abstractor%2520to%2520reduce%2520the%2520number%2520of%2520resulting%2520visual%2520tokens.%2520Unfortunately%252C%250Athey%2520fail%2520to%2520capture%2520finer%2520details%2520and%2520undermine%2520the%2520visual%2520reasoning%250Acapabilities%2520of%2520MLLMs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520visual%2520projector%252C%2520which%250Aadopts%2520a%2520coarse-to-fine%2520scheme%2520to%2520inject%2520the%2520enriched%2520characteristics%2520to%250Agenerate%2520the%2520condensed%2520visual%2520tokens.%2520In%2520specific%252C%2520we%2520first%2520interpolate%2520the%250Avisual%2520features%2520as%2520a%2520low-resolution%2520point%2520query%252C%2520providing%2520the%2520overall%2520visual%250Arepresentation%2520as%2520the%2520foundation.%2520Then%252C%2520we%2520introduce%2520a%2520region-to-point%250Ainjection%2520module%2520that%2520utilizes%2520high-resolution%252C%2520multi-level%2520region-based%2520cues%250Aas%2520fine-grained%2520reference%2520keys%2520and%2520values%252C%2520allowing%2520them%2520to%2520be%2520fully%2520absorbed%250Awithin%2520the%2520corresponding%2520local%2520context%2520region.%2520This%2520step%2520effectively%2520updates%250Athe%2520coarse%2520point%2520query%252C%2520transforming%2520it%2520into%2520an%2520enriched%2520one%2520for%2520the%2520subsequent%250ALLM%2520reasoning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520compresses%250Athe%2520visual%2520tokens%2520by%252075%2525~89%2525%252C%2520while%2520achieves%2520comparable%2520or%2520even%2520better%250Aperformance%2520across%2520diverse%2520benchmarks%2520with%2520significantly%2520higher%2520efficiency.%2520The%250Asource%2520codes%2520can%2520be%2520found%2520at%2520https%253A//github.com/CircleRadon/TokenPacker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02392v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM&entry.906535625=Wentong%20Li%20and%20Yuqian%20Yuan%20and%20Jian%20Liu%20and%20Dongqi%20Tang%20and%20Song%20Wang%20and%20Jianke%20Zhu%20and%20Lei%20Zhang&entry.1292438233=%20%20The%20visual%20projector%20serves%20as%20an%20essential%20bridge%20between%20the%20visual%20encoder%0Aand%20the%20Large%20Language%20Model%20%28LLM%29%20in%20a%20Multimodal%20LLM%20%28MLLM%29.%20Typically%2C%20MLLMs%0Aadopt%20a%20simple%20MLP%20to%20preserve%20all%20visual%20contexts%20via%20one-to-one%0Atransformation.%20However%2C%20the%20visual%20tokens%20are%20redundant%20and%20can%20be%0Aconsiderably%20increased%20when%20dealing%20with%20high-resolution%20images%2C%20impairing%20the%0Aefficiency%20of%20MLLMs%20significantly.%20Some%20recent%20works%20have%20introduced%20resampler%0Aor%20abstractor%20to%20reduce%20the%20number%20of%20resulting%20visual%20tokens.%20Unfortunately%2C%0Athey%20fail%20to%20capture%20finer%20details%20and%20undermine%20the%20visual%20reasoning%0Acapabilities%20of%20MLLMs.%20In%20this%20work%2C%20we%20propose%20a%20novel%20visual%20projector%2C%20which%0Aadopts%20a%20coarse-to-fine%20scheme%20to%20inject%20the%20enriched%20characteristics%20to%0Agenerate%20the%20condensed%20visual%20tokens.%20In%20specific%2C%20we%20first%20interpolate%20the%0Avisual%20features%20as%20a%20low-resolution%20point%20query%2C%20providing%20the%20overall%20visual%0Arepresentation%20as%20the%20foundation.%20Then%2C%20we%20introduce%20a%20region-to-point%0Ainjection%20module%20that%20utilizes%20high-resolution%2C%20multi-level%20region-based%20cues%0Aas%20fine-grained%20reference%20keys%20and%20values%2C%20allowing%20them%20to%20be%20fully%20absorbed%0Awithin%20the%20corresponding%20local%20context%20region.%20This%20step%20effectively%20updates%0Athe%20coarse%20point%20query%2C%20transforming%20it%20into%20an%20enriched%20one%20for%20the%20subsequent%0ALLM%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20compresses%0Athe%20visual%20tokens%20by%2075%25~89%25%2C%20while%20achieves%20comparable%20or%20even%20better%0Aperformance%20across%20diverse%20benchmarks%20with%20significantly%20higher%20efficiency.%20The%0Asource%20codes%20can%20be%20found%20at%20https%3A//github.com/CircleRadon/TokenPacker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02392v2&entry.124074799=Read"},
{"title": "Towards Open-World Object-based Anomaly Detection via Self-Supervised\n  Outlier Synthesis", "author": "Brian K. S. Isaac-Medina and Yona Falinie A. Gaus and Neelanjan Bhowmik and Toby P. Breckon", "abstract": "  Object detection is a pivotal task in computer vision that has received\nsignificant attention in previous years. Nonetheless, the capability of a\ndetector to localise objects out of the training distribution remains\nunexplored. Whilst recent approaches in object-level out-of-distribution (OoD)\ndetection heavily rely on class labels, such approaches contradict truly\nopen-world scenarios where the class distribution is often unknown. In this\ncontext, anomaly detection focuses on detecting unseen instances rather than\nclassifying detections as OoD. This work aims to bridge this gap by leveraging\nan open-world object detector and an OoD detector via virtual outlier\nsynthesis. This is achieved by using the detector backbone features to first\nlearn object pseudo-classes via self-supervision. These pseudo-classes serve as\nthe basis for class-conditional virtual outlier sampling of anomalous features\nthat are classified by an OoD head. Our approach empowers our overall object\ndetector architecture to learn anomaly-aware feature representations without\nrelying on class labels, hence enabling truly open-world object anomaly\ndetection. Empirical validation of our approach demonstrates its effectiveness\nacross diverse datasets encompassing various imaging modalities (visible,\ninfrared, and X-ray). Moreover, our method establishes state-of-the-art\nperformance on object-level anomaly detection, achieving an average recall\nscore improvement of over 5.4% for natural images and 23.5% for a security\nX-ray dataset compared to the current approaches. In addition, our method\ndetects anomalies in datasets where current approaches fail. Code available at\nhttps://github.com/KostadinovShalon/oln-ssos.\n", "link": "http://arxiv.org/abs/2407.15763v1", "date": "2024-07-22", "relevancy": 2.6704, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5417}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Open-World%20Object-based%20Anomaly%20Detection%20via%20Self-Supervised%0A%20%20Outlier%20Synthesis&body=Title%3A%20Towards%20Open-World%20Object-based%20Anomaly%20Detection%20via%20Self-Supervised%0A%20%20Outlier%20Synthesis%0AAuthor%3A%20Brian%20K.%20S.%20Isaac-Medina%20and%20Yona%20Falinie%20A.%20Gaus%20and%20Neelanjan%20Bhowmik%20and%20Toby%20P.%20Breckon%0AAbstract%3A%20%20%20Object%20detection%20is%20a%20pivotal%20task%20in%20computer%20vision%20that%20has%20received%0Asignificant%20attention%20in%20previous%20years.%20Nonetheless%2C%20the%20capability%20of%20a%0Adetector%20to%20localise%20objects%20out%20of%20the%20training%20distribution%20remains%0Aunexplored.%20Whilst%20recent%20approaches%20in%20object-level%20out-of-distribution%20%28OoD%29%0Adetection%20heavily%20rely%20on%20class%20labels%2C%20such%20approaches%20contradict%20truly%0Aopen-world%20scenarios%20where%20the%20class%20distribution%20is%20often%20unknown.%20In%20this%0Acontext%2C%20anomaly%20detection%20focuses%20on%20detecting%20unseen%20instances%20rather%20than%0Aclassifying%20detections%20as%20OoD.%20This%20work%20aims%20to%20bridge%20this%20gap%20by%20leveraging%0Aan%20open-world%20object%20detector%20and%20an%20OoD%20detector%20via%20virtual%20outlier%0Asynthesis.%20This%20is%20achieved%20by%20using%20the%20detector%20backbone%20features%20to%20first%0Alearn%20object%20pseudo-classes%20via%20self-supervision.%20These%20pseudo-classes%20serve%20as%0Athe%20basis%20for%20class-conditional%20virtual%20outlier%20sampling%20of%20anomalous%20features%0Athat%20are%20classified%20by%20an%20OoD%20head.%20Our%20approach%20empowers%20our%20overall%20object%0Adetector%20architecture%20to%20learn%20anomaly-aware%20feature%20representations%20without%0Arelying%20on%20class%20labels%2C%20hence%20enabling%20truly%20open-world%20object%20anomaly%0Adetection.%20Empirical%20validation%20of%20our%20approach%20demonstrates%20its%20effectiveness%0Aacross%20diverse%20datasets%20encompassing%20various%20imaging%20modalities%20%28visible%2C%0Ainfrared%2C%20and%20X-ray%29.%20Moreover%2C%20our%20method%20establishes%20state-of-the-art%0Aperformance%20on%20object-level%20anomaly%20detection%2C%20achieving%20an%20average%20recall%0Ascore%20improvement%20of%20over%205.4%25%20for%20natural%20images%20and%2023.5%25%20for%20a%20security%0AX-ray%20dataset%20compared%20to%20the%20current%20approaches.%20In%20addition%2C%20our%20method%0Adetects%20anomalies%20in%20datasets%20where%20current%20approaches%20fail.%20Code%20available%20at%0Ahttps%3A//github.com/KostadinovShalon/oln-ssos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Open-World%2520Object-based%2520Anomaly%2520Detection%2520via%2520Self-Supervised%250A%2520%2520Outlier%2520Synthesis%26entry.906535625%3DBrian%2520K.%2520S.%2520Isaac-Medina%2520and%2520Yona%2520Falinie%2520A.%2520Gaus%2520and%2520Neelanjan%2520Bhowmik%2520and%2520Toby%2520P.%2520Breckon%26entry.1292438233%3D%2520%2520Object%2520detection%2520is%2520a%2520pivotal%2520task%2520in%2520computer%2520vision%2520that%2520has%2520received%250Asignificant%2520attention%2520in%2520previous%2520years.%2520Nonetheless%252C%2520the%2520capability%2520of%2520a%250Adetector%2520to%2520localise%2520objects%2520out%2520of%2520the%2520training%2520distribution%2520remains%250Aunexplored.%2520Whilst%2520recent%2520approaches%2520in%2520object-level%2520out-of-distribution%2520%2528OoD%2529%250Adetection%2520heavily%2520rely%2520on%2520class%2520labels%252C%2520such%2520approaches%2520contradict%2520truly%250Aopen-world%2520scenarios%2520where%2520the%2520class%2520distribution%2520is%2520often%2520unknown.%2520In%2520this%250Acontext%252C%2520anomaly%2520detection%2520focuses%2520on%2520detecting%2520unseen%2520instances%2520rather%2520than%250Aclassifying%2520detections%2520as%2520OoD.%2520This%2520work%2520aims%2520to%2520bridge%2520this%2520gap%2520by%2520leveraging%250Aan%2520open-world%2520object%2520detector%2520and%2520an%2520OoD%2520detector%2520via%2520virtual%2520outlier%250Asynthesis.%2520This%2520is%2520achieved%2520by%2520using%2520the%2520detector%2520backbone%2520features%2520to%2520first%250Alearn%2520object%2520pseudo-classes%2520via%2520self-supervision.%2520These%2520pseudo-classes%2520serve%2520as%250Athe%2520basis%2520for%2520class-conditional%2520virtual%2520outlier%2520sampling%2520of%2520anomalous%2520features%250Athat%2520are%2520classified%2520by%2520an%2520OoD%2520head.%2520Our%2520approach%2520empowers%2520our%2520overall%2520object%250Adetector%2520architecture%2520to%2520learn%2520anomaly-aware%2520feature%2520representations%2520without%250Arelying%2520on%2520class%2520labels%252C%2520hence%2520enabling%2520truly%2520open-world%2520object%2520anomaly%250Adetection.%2520Empirical%2520validation%2520of%2520our%2520approach%2520demonstrates%2520its%2520effectiveness%250Aacross%2520diverse%2520datasets%2520encompassing%2520various%2520imaging%2520modalities%2520%2528visible%252C%250Ainfrared%252C%2520and%2520X-ray%2529.%2520Moreover%252C%2520our%2520method%2520establishes%2520state-of-the-art%250Aperformance%2520on%2520object-level%2520anomaly%2520detection%252C%2520achieving%2520an%2520average%2520recall%250Ascore%2520improvement%2520of%2520over%25205.4%2525%2520for%2520natural%2520images%2520and%252023.5%2525%2520for%2520a%2520security%250AX-ray%2520dataset%2520compared%2520to%2520the%2520current%2520approaches.%2520In%2520addition%252C%2520our%2520method%250Adetects%2520anomalies%2520in%2520datasets%2520where%2520current%2520approaches%2520fail.%2520Code%2520available%2520at%250Ahttps%253A//github.com/KostadinovShalon/oln-ssos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Open-World%20Object-based%20Anomaly%20Detection%20via%20Self-Supervised%0A%20%20Outlier%20Synthesis&entry.906535625=Brian%20K.%20S.%20Isaac-Medina%20and%20Yona%20Falinie%20A.%20Gaus%20and%20Neelanjan%20Bhowmik%20and%20Toby%20P.%20Breckon&entry.1292438233=%20%20Object%20detection%20is%20a%20pivotal%20task%20in%20computer%20vision%20that%20has%20received%0Asignificant%20attention%20in%20previous%20years.%20Nonetheless%2C%20the%20capability%20of%20a%0Adetector%20to%20localise%20objects%20out%20of%20the%20training%20distribution%20remains%0Aunexplored.%20Whilst%20recent%20approaches%20in%20object-level%20out-of-distribution%20%28OoD%29%0Adetection%20heavily%20rely%20on%20class%20labels%2C%20such%20approaches%20contradict%20truly%0Aopen-world%20scenarios%20where%20the%20class%20distribution%20is%20often%20unknown.%20In%20this%0Acontext%2C%20anomaly%20detection%20focuses%20on%20detecting%20unseen%20instances%20rather%20than%0Aclassifying%20detections%20as%20OoD.%20This%20work%20aims%20to%20bridge%20this%20gap%20by%20leveraging%0Aan%20open-world%20object%20detector%20and%20an%20OoD%20detector%20via%20virtual%20outlier%0Asynthesis.%20This%20is%20achieved%20by%20using%20the%20detector%20backbone%20features%20to%20first%0Alearn%20object%20pseudo-classes%20via%20self-supervision.%20These%20pseudo-classes%20serve%20as%0Athe%20basis%20for%20class-conditional%20virtual%20outlier%20sampling%20of%20anomalous%20features%0Athat%20are%20classified%20by%20an%20OoD%20head.%20Our%20approach%20empowers%20our%20overall%20object%0Adetector%20architecture%20to%20learn%20anomaly-aware%20feature%20representations%20without%0Arelying%20on%20class%20labels%2C%20hence%20enabling%20truly%20open-world%20object%20anomaly%0Adetection.%20Empirical%20validation%20of%20our%20approach%20demonstrates%20its%20effectiveness%0Aacross%20diverse%20datasets%20encompassing%20various%20imaging%20modalities%20%28visible%2C%0Ainfrared%2C%20and%20X-ray%29.%20Moreover%2C%20our%20method%20establishes%20state-of-the-art%0Aperformance%20on%20object-level%20anomaly%20detection%2C%20achieving%20an%20average%20recall%0Ascore%20improvement%20of%20over%205.4%25%20for%20natural%20images%20and%2023.5%25%20for%20a%20security%0AX-ray%20dataset%20compared%20to%20the%20current%20approaches.%20In%20addition%2C%20our%20method%0Adetects%20anomalies%20in%20datasets%20where%20current%20approaches%20fail.%20Code%20available%20at%0Ahttps%3A//github.com/KostadinovShalon/oln-ssos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15763v1&entry.124074799=Read"},
{"title": "360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation", "author": "Wenxuan Lu and Mengshun Hu and Yansheng Qiu and Liang Liao and Zheng Wang", "abstract": "  With the development of VR-related techniques, viewers can enjoy a realistic\nand immersive experience through a head-mounted display, while omnidirectional\nvideo with a low frame rate can lead to user dizziness. However, the prevailing\nplane frame interpolation methodologies are unsuitable for Omnidirectional\nVideo Interpolation, chiefly due to the lack of models tailored to such videos\nwith strong distortion, compounded by the scarcity of valuable datasets for\nOmnidirectional Video Frame Interpolation. In this paper, we introduce the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. We especially\npropose a pyramid distortion-sensitive feature extractor that uses the unique\ncharacteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto facilitate the synthesis of intermediate frames further. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we presented four\ndifferent distortion conditions scenes in the proposed 360VFI dataset to\nevaluate the challenge triggered by distortion during interpolation. Besides,\nexperimental results demonstrate that Omnidirectional Video Interpolation can\nbe effectively improved by modeling for omnidirectional distortion.\n", "link": "http://arxiv.org/abs/2407.14066v2", "date": "2024-07-22", "relevancy": 2.6688, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5566}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5278}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20360VFI%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Video%20Frame%0A%20%20Interpolation&body=Title%3A%20360VFI%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Video%20Frame%0A%20%20Interpolation%0AAuthor%3A%20Wenxuan%20Lu%20and%20Mengshun%20Hu%20and%20Yansheng%20Qiu%20and%20Liang%20Liao%20and%20Zheng%20Wang%0AAbstract%3A%20%20%20With%20the%20development%20of%20VR-related%20techniques%2C%20viewers%20can%20enjoy%20a%20realistic%0Aand%20immersive%20experience%20through%20a%20head-mounted%20display%2C%20while%20omnidirectional%0Avideo%20with%20a%20low%20frame%20rate%20can%20lead%20to%20user%20dizziness.%20However%2C%20the%20prevailing%0Aplane%20frame%20interpolation%20methodologies%20are%20unsuitable%20for%20Omnidirectional%0AVideo%20Interpolation%2C%20chiefly%20due%20to%20the%20lack%20of%20models%20tailored%20to%20such%20videos%0Awith%20strong%20distortion%2C%20compounded%20by%20the%20scarcity%20of%20valuable%20datasets%20for%0AOmnidirectional%20Video%20Frame%20Interpolation.%20In%20this%20paper%2C%20we%20introduce%20the%0Abenchmark%20dataset%2C%20360VFI%2C%20for%20Omnidirectional%20Video%20Frame%20Interpolation.%20We%0Apresent%20a%20practical%20implementation%20that%20introduces%20a%20distortion%20prior%20from%0Aomnidirectional%20video%20into%20the%20network%20to%20modulate%20distortions.%20We%20especially%0Apropose%20a%20pyramid%20distortion-sensitive%20feature%20extractor%20that%20uses%20the%20unique%0Acharacteristics%20of%20equirectangular%20projection%20%28ERP%29%20format%20as%20prior%0Ainformation.%20Moreover%2C%20we%20devise%20a%20decoder%20that%20uses%20an%20affine%20transformation%0Ato%20facilitate%20the%20synthesis%20of%20intermediate%20frames%20further.%20360VFI%20is%20the%20first%0Adataset%20and%20benchmark%20that%20explores%20the%20challenge%20of%20Omnidirectional%20Video%0AFrame%20Interpolation.%20Through%20our%20benchmark%20analysis%2C%20we%20presented%20four%0Adifferent%20distortion%20conditions%20scenes%20in%20the%20proposed%20360VFI%20dataset%20to%0Aevaluate%20the%20challenge%20triggered%20by%20distortion%20during%20interpolation.%20Besides%2C%0Aexperimental%20results%20demonstrate%20that%20Omnidirectional%20Video%20Interpolation%20can%0Abe%20effectively%20improved%20by%20modeling%20for%20omnidirectional%20distortion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D360VFI%253A%2520A%2520Dataset%2520and%2520Benchmark%2520for%2520Omnidirectional%2520Video%2520Frame%250A%2520%2520Interpolation%26entry.906535625%3DWenxuan%2520Lu%2520and%2520Mengshun%2520Hu%2520and%2520Yansheng%2520Qiu%2520and%2520Liang%2520Liao%2520and%2520Zheng%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520VR-related%2520techniques%252C%2520viewers%2520can%2520enjoy%2520a%2520realistic%250Aand%2520immersive%2520experience%2520through%2520a%2520head-mounted%2520display%252C%2520while%2520omnidirectional%250Avideo%2520with%2520a%2520low%2520frame%2520rate%2520can%2520lead%2520to%2520user%2520dizziness.%2520However%252C%2520the%2520prevailing%250Aplane%2520frame%2520interpolation%2520methodologies%2520are%2520unsuitable%2520for%2520Omnidirectional%250AVideo%2520Interpolation%252C%2520chiefly%2520due%2520to%2520the%2520lack%2520of%2520models%2520tailored%2520to%2520such%2520videos%250Awith%2520strong%2520distortion%252C%2520compounded%2520by%2520the%2520scarcity%2520of%2520valuable%2520datasets%2520for%250AOmnidirectional%2520Video%2520Frame%2520Interpolation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%250Abenchmark%2520dataset%252C%2520360VFI%252C%2520for%2520Omnidirectional%2520Video%2520Frame%2520Interpolation.%2520We%250Apresent%2520a%2520practical%2520implementation%2520that%2520introduces%2520a%2520distortion%2520prior%2520from%250Aomnidirectional%2520video%2520into%2520the%2520network%2520to%2520modulate%2520distortions.%2520We%2520especially%250Apropose%2520a%2520pyramid%2520distortion-sensitive%2520feature%2520extractor%2520that%2520uses%2520the%2520unique%250Acharacteristics%2520of%2520equirectangular%2520projection%2520%2528ERP%2529%2520format%2520as%2520prior%250Ainformation.%2520Moreover%252C%2520we%2520devise%2520a%2520decoder%2520that%2520uses%2520an%2520affine%2520transformation%250Ato%2520facilitate%2520the%2520synthesis%2520of%2520intermediate%2520frames%2520further.%2520360VFI%2520is%2520the%2520first%250Adataset%2520and%2520benchmark%2520that%2520explores%2520the%2520challenge%2520of%2520Omnidirectional%2520Video%250AFrame%2520Interpolation.%2520Through%2520our%2520benchmark%2520analysis%252C%2520we%2520presented%2520four%250Adifferent%2520distortion%2520conditions%2520scenes%2520in%2520the%2520proposed%2520360VFI%2520dataset%2520to%250Aevaluate%2520the%2520challenge%2520triggered%2520by%2520distortion%2520during%2520interpolation.%2520Besides%252C%250Aexperimental%2520results%2520demonstrate%2520that%2520Omnidirectional%2520Video%2520Interpolation%2520can%250Abe%2520effectively%2520improved%2520by%2520modeling%2520for%2520omnidirectional%2520distortion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360VFI%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Video%20Frame%0A%20%20Interpolation&entry.906535625=Wenxuan%20Lu%20and%20Mengshun%20Hu%20and%20Yansheng%20Qiu%20and%20Liang%20Liao%20and%20Zheng%20Wang&entry.1292438233=%20%20With%20the%20development%20of%20VR-related%20techniques%2C%20viewers%20can%20enjoy%20a%20realistic%0Aand%20immersive%20experience%20through%20a%20head-mounted%20display%2C%20while%20omnidirectional%0Avideo%20with%20a%20low%20frame%20rate%20can%20lead%20to%20user%20dizziness.%20However%2C%20the%20prevailing%0Aplane%20frame%20interpolation%20methodologies%20are%20unsuitable%20for%20Omnidirectional%0AVideo%20Interpolation%2C%20chiefly%20due%20to%20the%20lack%20of%20models%20tailored%20to%20such%20videos%0Awith%20strong%20distortion%2C%20compounded%20by%20the%20scarcity%20of%20valuable%20datasets%20for%0AOmnidirectional%20Video%20Frame%20Interpolation.%20In%20this%20paper%2C%20we%20introduce%20the%0Abenchmark%20dataset%2C%20360VFI%2C%20for%20Omnidirectional%20Video%20Frame%20Interpolation.%20We%0Apresent%20a%20practical%20implementation%20that%20introduces%20a%20distortion%20prior%20from%0Aomnidirectional%20video%20into%20the%20network%20to%20modulate%20distortions.%20We%20especially%0Apropose%20a%20pyramid%20distortion-sensitive%20feature%20extractor%20that%20uses%20the%20unique%0Acharacteristics%20of%20equirectangular%20projection%20%28ERP%29%20format%20as%20prior%0Ainformation.%20Moreover%2C%20we%20devise%20a%20decoder%20that%20uses%20an%20affine%20transformation%0Ato%20facilitate%20the%20synthesis%20of%20intermediate%20frames%20further.%20360VFI%20is%20the%20first%0Adataset%20and%20benchmark%20that%20explores%20the%20challenge%20of%20Omnidirectional%20Video%0AFrame%20Interpolation.%20Through%20our%20benchmark%20analysis%2C%20we%20presented%20four%0Adifferent%20distortion%20conditions%20scenes%20in%20the%20proposed%20360VFI%20dataset%20to%0Aevaluate%20the%20challenge%20triggered%20by%20distortion%20during%20interpolation.%20Besides%2C%0Aexperimental%20results%20demonstrate%20that%20Omnidirectional%20Video%20Interpolation%20can%0Abe%20effectively%20improved%20by%20modeling%20for%20omnidirectional%20distortion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14066v2&entry.124074799=Read"},
{"title": "AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot\n  Anomaly Detection", "author": "Yunkang Cao and Jiangning Zhang and Luca Frittoli and Yuqi Cheng and Weiming Shen and Giacomo Boracchi", "abstract": "  Zero-shot anomaly detection (ZSAD) targets the identification of anomalies\nwithin images from arbitrary novel categories. This study introduces AdaCLIP\nfor the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP.\nAdaCLIP incorporates learnable prompts into CLIP and optimizes them through\ntraining on auxiliary annotated anomaly detection data. Two types of learnable\nprompts are proposed: static and dynamic. Static prompts are shared across all\nimages, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic\nprompts are generated for each test image, providing CLIP with dynamic\nadaptation capabilities. The combination of static and dynamic prompts is\nreferred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive\nexperiments conducted across 14 real-world anomaly detection datasets from\nindustrial and medical domains indicate that AdaCLIP outperforms other ZSAD\nmethods and can generalize better to different categories and even domains.\nFinally, our analysis highlights the importance of diverse auxiliary data and\noptimized prompts for enhanced generalization capacity. Code is available at\nhttps://github.com/caoyunkang/AdaCLIP.\n", "link": "http://arxiv.org/abs/2407.15795v1", "date": "2024-07-22", "relevancy": 2.6586, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5855}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.507}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaCLIP%3A%20Adapting%20CLIP%20with%20Hybrid%20Learnable%20Prompts%20for%20Zero-Shot%0A%20%20Anomaly%20Detection&body=Title%3A%20AdaCLIP%3A%20Adapting%20CLIP%20with%20Hybrid%20Learnable%20Prompts%20for%20Zero-Shot%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Yunkang%20Cao%20and%20Jiangning%20Zhang%20and%20Luca%20Frittoli%20and%20Yuqi%20Cheng%20and%20Weiming%20Shen%20and%20Giacomo%20Boracchi%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20targets%20the%20identification%20of%20anomalies%0Awithin%20images%20from%20arbitrary%20novel%20categories.%20This%20study%20introduces%20AdaCLIP%0Afor%20the%20ZSAD%20task%2C%20leveraging%20a%20pre-trained%20vision-language%20model%20%28VLM%29%2C%20CLIP.%0AAdaCLIP%20incorporates%20learnable%20prompts%20into%20CLIP%20and%20optimizes%20them%20through%0Atraining%20on%20auxiliary%20annotated%20anomaly%20detection%20data.%20Two%20types%20of%20learnable%0Aprompts%20are%20proposed%3A%20static%20and%20dynamic.%20Static%20prompts%20are%20shared%20across%20all%0Aimages%2C%20serving%20to%20preliminarily%20adapt%20CLIP%20for%20ZSAD.%20In%20contrast%2C%20dynamic%0Aprompts%20are%20generated%20for%20each%20test%20image%2C%20providing%20CLIP%20with%20dynamic%0Aadaptation%20capabilities.%20The%20combination%20of%20static%20and%20dynamic%20prompts%20is%0Areferred%20to%20as%20hybrid%20prompts%2C%20and%20yields%20enhanced%20ZSAD%20performance.%20Extensive%0Aexperiments%20conducted%20across%2014%20real-world%20anomaly%20detection%20datasets%20from%0Aindustrial%20and%20medical%20domains%20indicate%20that%20AdaCLIP%20outperforms%20other%20ZSAD%0Amethods%20and%20can%20generalize%20better%20to%20different%20categories%20and%20even%20domains.%0AFinally%2C%20our%20analysis%20highlights%20the%20importance%20of%20diverse%20auxiliary%20data%20and%0Aoptimized%20prompts%20for%20enhanced%20generalization%20capacity.%20Code%20is%20available%20at%0Ahttps%3A//github.com/caoyunkang/AdaCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaCLIP%253A%2520Adapting%2520CLIP%2520with%2520Hybrid%2520Learnable%2520Prompts%2520for%2520Zero-Shot%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DYunkang%2520Cao%2520and%2520Jiangning%2520Zhang%2520and%2520Luca%2520Frittoli%2520and%2520Yuqi%2520Cheng%2520and%2520Weiming%2520Shen%2520and%2520Giacomo%2520Boracchi%26entry.1292438233%3D%2520%2520Zero-shot%2520anomaly%2520detection%2520%2528ZSAD%2529%2520targets%2520the%2520identification%2520of%2520anomalies%250Awithin%2520images%2520from%2520arbitrary%2520novel%2520categories.%2520This%2520study%2520introduces%2520AdaCLIP%250Afor%2520the%2520ZSAD%2520task%252C%2520leveraging%2520a%2520pre-trained%2520vision-language%2520model%2520%2528VLM%2529%252C%2520CLIP.%250AAdaCLIP%2520incorporates%2520learnable%2520prompts%2520into%2520CLIP%2520and%2520optimizes%2520them%2520through%250Atraining%2520on%2520auxiliary%2520annotated%2520anomaly%2520detection%2520data.%2520Two%2520types%2520of%2520learnable%250Aprompts%2520are%2520proposed%253A%2520static%2520and%2520dynamic.%2520Static%2520prompts%2520are%2520shared%2520across%2520all%250Aimages%252C%2520serving%2520to%2520preliminarily%2520adapt%2520CLIP%2520for%2520ZSAD.%2520In%2520contrast%252C%2520dynamic%250Aprompts%2520are%2520generated%2520for%2520each%2520test%2520image%252C%2520providing%2520CLIP%2520with%2520dynamic%250Aadaptation%2520capabilities.%2520The%2520combination%2520of%2520static%2520and%2520dynamic%2520prompts%2520is%250Areferred%2520to%2520as%2520hybrid%2520prompts%252C%2520and%2520yields%2520enhanced%2520ZSAD%2520performance.%2520Extensive%250Aexperiments%2520conducted%2520across%252014%2520real-world%2520anomaly%2520detection%2520datasets%2520from%250Aindustrial%2520and%2520medical%2520domains%2520indicate%2520that%2520AdaCLIP%2520outperforms%2520other%2520ZSAD%250Amethods%2520and%2520can%2520generalize%2520better%2520to%2520different%2520categories%2520and%2520even%2520domains.%250AFinally%252C%2520our%2520analysis%2520highlights%2520the%2520importance%2520of%2520diverse%2520auxiliary%2520data%2520and%250Aoptimized%2520prompts%2520for%2520enhanced%2520generalization%2520capacity.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/caoyunkang/AdaCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaCLIP%3A%20Adapting%20CLIP%20with%20Hybrid%20Learnable%20Prompts%20for%20Zero-Shot%0A%20%20Anomaly%20Detection&entry.906535625=Yunkang%20Cao%20and%20Jiangning%20Zhang%20and%20Luca%20Frittoli%20and%20Yuqi%20Cheng%20and%20Weiming%20Shen%20and%20Giacomo%20Boracchi&entry.1292438233=%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20targets%20the%20identification%20of%20anomalies%0Awithin%20images%20from%20arbitrary%20novel%20categories.%20This%20study%20introduces%20AdaCLIP%0Afor%20the%20ZSAD%20task%2C%20leveraging%20a%20pre-trained%20vision-language%20model%20%28VLM%29%2C%20CLIP.%0AAdaCLIP%20incorporates%20learnable%20prompts%20into%20CLIP%20and%20optimizes%20them%20through%0Atraining%20on%20auxiliary%20annotated%20anomaly%20detection%20data.%20Two%20types%20of%20learnable%0Aprompts%20are%20proposed%3A%20static%20and%20dynamic.%20Static%20prompts%20are%20shared%20across%20all%0Aimages%2C%20serving%20to%20preliminarily%20adapt%20CLIP%20for%20ZSAD.%20In%20contrast%2C%20dynamic%0Aprompts%20are%20generated%20for%20each%20test%20image%2C%20providing%20CLIP%20with%20dynamic%0Aadaptation%20capabilities.%20The%20combination%20of%20static%20and%20dynamic%20prompts%20is%0Areferred%20to%20as%20hybrid%20prompts%2C%20and%20yields%20enhanced%20ZSAD%20performance.%20Extensive%0Aexperiments%20conducted%20across%2014%20real-world%20anomaly%20detection%20datasets%20from%0Aindustrial%20and%20medical%20domains%20indicate%20that%20AdaCLIP%20outperforms%20other%20ZSAD%0Amethods%20and%20can%20generalize%20better%20to%20different%20categories%20and%20even%20domains.%0AFinally%2C%20our%20analysis%20highlights%20the%20importance%20of%20diverse%20auxiliary%20data%20and%0Aoptimized%20prompts%20for%20enhanced%20generalization%20capacity.%20Code%20is%20available%20at%0Ahttps%3A//github.com/caoyunkang/AdaCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15795v1&entry.124074799=Read"},
{"title": "Sample-Efficient Linear Representation Learning from Non-IID\n  Non-Isotropic Data", "author": "Thomas T. C. K. Zhang and Leonardo F. Toso and James Anderson and Nikolai Matni", "abstract": "  A powerful concept behind much of the recent progress in machine learning is\nthe extraction of common features across data from heterogeneous sources or\ntasks. Intuitively, using all of one's data to learn a common representation\nfunction benefits both computational effort and statistical generalization by\nleaving a smaller number of parameters to fine-tune on a given task. Toward\ntheoretically grounding these merits, we propose a general setting of\nrecovering linear operators $M$ from noisy vector measurements $y = Mx + w$,\nwhere the covariates $x$ may be both non-i.i.d. and non-isotropic. We\ndemonstrate that existing isotropy-agnostic representation learning approaches\nincur biases on the representation update, which causes the scaling of the\nnoise terms to lose favorable dependence on the number of source tasks. This in\nturn can cause the sample complexity of representation learning to be\nbottlenecked by the single-task data size. We introduce an adaptation,\n$\\texttt{De-bias & Feature-Whiten}$ ($\\texttt{DFW}$), of the popular\nalternating minimization-descent scheme proposed independently in Collins et\nal., (2021) and Nayer and Vaswani (2022), and establish linear convergence to\nthe optimal representation with noise level scaling down with the\n$\\textit{total}$ source data size. This leads to generalization bounds on the\nsame order as an oracle empirical risk minimizer. We verify the vital\nimportance of $\\texttt{DFW}$ on various numerical simulations. In particular,\nwe show that vanilla alternating-minimization descent fails catastrophically\neven for iid, but mildly non-isotropic data. Our analysis unifies and\ngeneralizes prior work, and provides a flexible framework for a wider range of\napplications, such as in controls and dynamical systems.\n", "link": "http://arxiv.org/abs/2308.04428v2", "date": "2024-07-22", "relevancy": 2.6228, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5617}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5086}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-Efficient%20Linear%20Representation%20Learning%20from%20Non-IID%0A%20%20Non-Isotropic%20Data&body=Title%3A%20Sample-Efficient%20Linear%20Representation%20Learning%20from%20Non-IID%0A%20%20Non-Isotropic%20Data%0AAuthor%3A%20Thomas%20T.%20C.%20K.%20Zhang%20and%20Leonardo%20F.%20Toso%20and%20James%20Anderson%20and%20Nikolai%20Matni%0AAbstract%3A%20%20%20A%20powerful%20concept%20behind%20much%20of%20the%20recent%20progress%20in%20machine%20learning%20is%0Athe%20extraction%20of%20common%20features%20across%20data%20from%20heterogeneous%20sources%20or%0Atasks.%20Intuitively%2C%20using%20all%20of%20one%27s%20data%20to%20learn%20a%20common%20representation%0Afunction%20benefits%20both%20computational%20effort%20and%20statistical%20generalization%20by%0Aleaving%20a%20smaller%20number%20of%20parameters%20to%20fine-tune%20on%20a%20given%20task.%20Toward%0Atheoretically%20grounding%20these%20merits%2C%20we%20propose%20a%20general%20setting%20of%0Arecovering%20linear%20operators%20%24M%24%20from%20noisy%20vector%20measurements%20%24y%20%3D%20Mx%20%2B%20w%24%2C%0Awhere%20the%20covariates%20%24x%24%20may%20be%20both%20non-i.i.d.%20and%20non-isotropic.%20We%0Ademonstrate%20that%20existing%20isotropy-agnostic%20representation%20learning%20approaches%0Aincur%20biases%20on%20the%20representation%20update%2C%20which%20causes%20the%20scaling%20of%20the%0Anoise%20terms%20to%20lose%20favorable%20dependence%20on%20the%20number%20of%20source%20tasks.%20This%20in%0Aturn%20can%20cause%20the%20sample%20complexity%20of%20representation%20learning%20to%20be%0Abottlenecked%20by%20the%20single-task%20data%20size.%20We%20introduce%20an%20adaptation%2C%0A%24%5Ctexttt%7BDe-bias%20%26%20Feature-Whiten%7D%24%20%28%24%5Ctexttt%7BDFW%7D%24%29%2C%20of%20the%20popular%0Aalternating%20minimization-descent%20scheme%20proposed%20independently%20in%20Collins%20et%0Aal.%2C%20%282021%29%20and%20Nayer%20and%20Vaswani%20%282022%29%2C%20and%20establish%20linear%20convergence%20to%0Athe%20optimal%20representation%20with%20noise%20level%20scaling%20down%20with%20the%0A%24%5Ctextit%7Btotal%7D%24%20source%20data%20size.%20This%20leads%20to%20generalization%20bounds%20on%20the%0Asame%20order%20as%20an%20oracle%20empirical%20risk%20minimizer.%20We%20verify%20the%20vital%0Aimportance%20of%20%24%5Ctexttt%7BDFW%7D%24%20on%20various%20numerical%20simulations.%20In%20particular%2C%0Awe%20show%20that%20vanilla%20alternating-minimization%20descent%20fails%20catastrophically%0Aeven%20for%20iid%2C%20but%20mildly%20non-isotropic%20data.%20Our%20analysis%20unifies%20and%0Ageneralizes%20prior%20work%2C%20and%20provides%20a%20flexible%20framework%20for%20a%20wider%20range%20of%0Aapplications%2C%20such%20as%20in%20controls%20and%20dynamical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04428v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-Efficient%2520Linear%2520Representation%2520Learning%2520from%2520Non-IID%250A%2520%2520Non-Isotropic%2520Data%26entry.906535625%3DThomas%2520T.%2520C.%2520K.%2520Zhang%2520and%2520Leonardo%2520F.%2520Toso%2520and%2520James%2520Anderson%2520and%2520Nikolai%2520Matni%26entry.1292438233%3D%2520%2520A%2520powerful%2520concept%2520behind%2520much%2520of%2520the%2520recent%2520progress%2520in%2520machine%2520learning%2520is%250Athe%2520extraction%2520of%2520common%2520features%2520across%2520data%2520from%2520heterogeneous%2520sources%2520or%250Atasks.%2520Intuitively%252C%2520using%2520all%2520of%2520one%2527s%2520data%2520to%2520learn%2520a%2520common%2520representation%250Afunction%2520benefits%2520both%2520computational%2520effort%2520and%2520statistical%2520generalization%2520by%250Aleaving%2520a%2520smaller%2520number%2520of%2520parameters%2520to%2520fine-tune%2520on%2520a%2520given%2520task.%2520Toward%250Atheoretically%2520grounding%2520these%2520merits%252C%2520we%2520propose%2520a%2520general%2520setting%2520of%250Arecovering%2520linear%2520operators%2520%2524M%2524%2520from%2520noisy%2520vector%2520measurements%2520%2524y%2520%253D%2520Mx%2520%252B%2520w%2524%252C%250Awhere%2520the%2520covariates%2520%2524x%2524%2520may%2520be%2520both%2520non-i.i.d.%2520and%2520non-isotropic.%2520We%250Ademonstrate%2520that%2520existing%2520isotropy-agnostic%2520representation%2520learning%2520approaches%250Aincur%2520biases%2520on%2520the%2520representation%2520update%252C%2520which%2520causes%2520the%2520scaling%2520of%2520the%250Anoise%2520terms%2520to%2520lose%2520favorable%2520dependence%2520on%2520the%2520number%2520of%2520source%2520tasks.%2520This%2520in%250Aturn%2520can%2520cause%2520the%2520sample%2520complexity%2520of%2520representation%2520learning%2520to%2520be%250Abottlenecked%2520by%2520the%2520single-task%2520data%2520size.%2520We%2520introduce%2520an%2520adaptation%252C%250A%2524%255Ctexttt%257BDe-bias%2520%2526%2520Feature-Whiten%257D%2524%2520%2528%2524%255Ctexttt%257BDFW%257D%2524%2529%252C%2520of%2520the%2520popular%250Aalternating%2520minimization-descent%2520scheme%2520proposed%2520independently%2520in%2520Collins%2520et%250Aal.%252C%2520%25282021%2529%2520and%2520Nayer%2520and%2520Vaswani%2520%25282022%2529%252C%2520and%2520establish%2520linear%2520convergence%2520to%250Athe%2520optimal%2520representation%2520with%2520noise%2520level%2520scaling%2520down%2520with%2520the%250A%2524%255Ctextit%257Btotal%257D%2524%2520source%2520data%2520size.%2520This%2520leads%2520to%2520generalization%2520bounds%2520on%2520the%250Asame%2520order%2520as%2520an%2520oracle%2520empirical%2520risk%2520minimizer.%2520We%2520verify%2520the%2520vital%250Aimportance%2520of%2520%2524%255Ctexttt%257BDFW%257D%2524%2520on%2520various%2520numerical%2520simulations.%2520In%2520particular%252C%250Awe%2520show%2520that%2520vanilla%2520alternating-minimization%2520descent%2520fails%2520catastrophically%250Aeven%2520for%2520iid%252C%2520but%2520mildly%2520non-isotropic%2520data.%2520Our%2520analysis%2520unifies%2520and%250Ageneralizes%2520prior%2520work%252C%2520and%2520provides%2520a%2520flexible%2520framework%2520for%2520a%2520wider%2520range%2520of%250Aapplications%252C%2520such%2520as%2520in%2520controls%2520and%2520dynamical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04428v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-Efficient%20Linear%20Representation%20Learning%20from%20Non-IID%0A%20%20Non-Isotropic%20Data&entry.906535625=Thomas%20T.%20C.%20K.%20Zhang%20and%20Leonardo%20F.%20Toso%20and%20James%20Anderson%20and%20Nikolai%20Matni&entry.1292438233=%20%20A%20powerful%20concept%20behind%20much%20of%20the%20recent%20progress%20in%20machine%20learning%20is%0Athe%20extraction%20of%20common%20features%20across%20data%20from%20heterogeneous%20sources%20or%0Atasks.%20Intuitively%2C%20using%20all%20of%20one%27s%20data%20to%20learn%20a%20common%20representation%0Afunction%20benefits%20both%20computational%20effort%20and%20statistical%20generalization%20by%0Aleaving%20a%20smaller%20number%20of%20parameters%20to%20fine-tune%20on%20a%20given%20task.%20Toward%0Atheoretically%20grounding%20these%20merits%2C%20we%20propose%20a%20general%20setting%20of%0Arecovering%20linear%20operators%20%24M%24%20from%20noisy%20vector%20measurements%20%24y%20%3D%20Mx%20%2B%20w%24%2C%0Awhere%20the%20covariates%20%24x%24%20may%20be%20both%20non-i.i.d.%20and%20non-isotropic.%20We%0Ademonstrate%20that%20existing%20isotropy-agnostic%20representation%20learning%20approaches%0Aincur%20biases%20on%20the%20representation%20update%2C%20which%20causes%20the%20scaling%20of%20the%0Anoise%20terms%20to%20lose%20favorable%20dependence%20on%20the%20number%20of%20source%20tasks.%20This%20in%0Aturn%20can%20cause%20the%20sample%20complexity%20of%20representation%20learning%20to%20be%0Abottlenecked%20by%20the%20single-task%20data%20size.%20We%20introduce%20an%20adaptation%2C%0A%24%5Ctexttt%7BDe-bias%20%26%20Feature-Whiten%7D%24%20%28%24%5Ctexttt%7BDFW%7D%24%29%2C%20of%20the%20popular%0Aalternating%20minimization-descent%20scheme%20proposed%20independently%20in%20Collins%20et%0Aal.%2C%20%282021%29%20and%20Nayer%20and%20Vaswani%20%282022%29%2C%20and%20establish%20linear%20convergence%20to%0Athe%20optimal%20representation%20with%20noise%20level%20scaling%20down%20with%20the%0A%24%5Ctextit%7Btotal%7D%24%20source%20data%20size.%20This%20leads%20to%20generalization%20bounds%20on%20the%0Asame%20order%20as%20an%20oracle%20empirical%20risk%20minimizer.%20We%20verify%20the%20vital%0Aimportance%20of%20%24%5Ctexttt%7BDFW%7D%24%20on%20various%20numerical%20simulations.%20In%20particular%2C%0Awe%20show%20that%20vanilla%20alternating-minimization%20descent%20fails%20catastrophically%0Aeven%20for%20iid%2C%20but%20mildly%20non-isotropic%20data.%20Our%20analysis%20unifies%20and%0Ageneralizes%20prior%20work%2C%20and%20provides%20a%20flexible%20framework%20for%20a%20wider%20range%20of%0Aapplications%2C%20such%20as%20in%20controls%20and%20dynamical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04428v2&entry.124074799=Read"},
{"title": "AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description", "author": "Junyu Xie and Tengda Han and Max Bain and Arsha Nagrani and G\u00fcl Varol and Weidi Xie and Andrew Zisserman", "abstract": "  Our objective is to generate Audio Descriptions (ADs) for both movies and TV\nseries in a training-free manner. We use the power of off-the-shelf\nVisual-Language Models (VLMs) and Large Language Models (LLMs), and develop\nvisual and text prompting strategies for this task. Our contributions are\nthree-fold: (i) We demonstrate that a VLM can successfully name and refer to\ncharacters if directly prompted with character information through visual\nindications without requiring any fine-tuning; (ii) A two-stage process is\ndeveloped to generate ADs, with the first stage asking the VLM to\ncomprehensively describe the video, followed by a second stage utilising a LLM\nto summarise dense textual information into one succinct AD sentence; (iii) A\nnew dataset for TV audio description is formulated. Our approach, named\nAutoAD-Zero, demonstrates outstanding performance (even competitive with some\nmodels fine-tuned on ground truth ADs) in AD generation for both movies and TV\nseries, achieving state-of-the-art CRITIC scores.\n", "link": "http://arxiv.org/abs/2407.15850v1", "date": "2024-07-22", "relevancy": 2.6213, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5379}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoAD-Zero%3A%20A%20Training-Free%20Framework%20for%20Zero-Shot%20Audio%20Description&body=Title%3A%20AutoAD-Zero%3A%20A%20Training-Free%20Framework%20for%20Zero-Shot%20Audio%20Description%0AAuthor%3A%20Junyu%20Xie%20and%20Tengda%20Han%20and%20Max%20Bain%20and%20Arsha%20Nagrani%20and%20G%C3%BCl%20Varol%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Our%20objective%20is%20to%20generate%20Audio%20Descriptions%20%28ADs%29%20for%20both%20movies%20and%20TV%0Aseries%20in%20a%20training-free%20manner.%20We%20use%20the%20power%20of%20off-the-shelf%0AVisual-Language%20Models%20%28VLMs%29%20and%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20develop%0Avisual%20and%20text%20prompting%20strategies%20for%20this%20task.%20Our%20contributions%20are%0Athree-fold%3A%20%28i%29%20We%20demonstrate%20that%20a%20VLM%20can%20successfully%20name%20and%20refer%20to%0Acharacters%20if%20directly%20prompted%20with%20character%20information%20through%20visual%0Aindications%20without%20requiring%20any%20fine-tuning%3B%20%28ii%29%20A%20two-stage%20process%20is%0Adeveloped%20to%20generate%20ADs%2C%20with%20the%20first%20stage%20asking%20the%20VLM%20to%0Acomprehensively%20describe%20the%20video%2C%20followed%20by%20a%20second%20stage%20utilising%20a%20LLM%0Ato%20summarise%20dense%20textual%20information%20into%20one%20succinct%20AD%20sentence%3B%20%28iii%29%20A%0Anew%20dataset%20for%20TV%20audio%20description%20is%20formulated.%20Our%20approach%2C%20named%0AAutoAD-Zero%2C%20demonstrates%20outstanding%20performance%20%28even%20competitive%20with%20some%0Amodels%20fine-tuned%20on%20ground%20truth%20ADs%29%20in%20AD%20generation%20for%20both%20movies%20and%20TV%0Aseries%2C%20achieving%20state-of-the-art%20CRITIC%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoAD-Zero%253A%2520A%2520Training-Free%2520Framework%2520for%2520Zero-Shot%2520Audio%2520Description%26entry.906535625%3DJunyu%2520Xie%2520and%2520Tengda%2520Han%2520and%2520Max%2520Bain%2520and%2520Arsha%2520Nagrani%2520and%2520G%25C3%25BCl%2520Varol%2520and%2520Weidi%2520Xie%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Our%2520objective%2520is%2520to%2520generate%2520Audio%2520Descriptions%2520%2528ADs%2529%2520for%2520both%2520movies%2520and%2520TV%250Aseries%2520in%2520a%2520training-free%2520manner.%2520We%2520use%2520the%2520power%2520of%2520off-the-shelf%250AVisual-Language%2520Models%2520%2528VLMs%2529%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520and%2520develop%250Avisual%2520and%2520text%2520prompting%2520strategies%2520for%2520this%2520task.%2520Our%2520contributions%2520are%250Athree-fold%253A%2520%2528i%2529%2520We%2520demonstrate%2520that%2520a%2520VLM%2520can%2520successfully%2520name%2520and%2520refer%2520to%250Acharacters%2520if%2520directly%2520prompted%2520with%2520character%2520information%2520through%2520visual%250Aindications%2520without%2520requiring%2520any%2520fine-tuning%253B%2520%2528ii%2529%2520A%2520two-stage%2520process%2520is%250Adeveloped%2520to%2520generate%2520ADs%252C%2520with%2520the%2520first%2520stage%2520asking%2520the%2520VLM%2520to%250Acomprehensively%2520describe%2520the%2520video%252C%2520followed%2520by%2520a%2520second%2520stage%2520utilising%2520a%2520LLM%250Ato%2520summarise%2520dense%2520textual%2520information%2520into%2520one%2520succinct%2520AD%2520sentence%253B%2520%2528iii%2529%2520A%250Anew%2520dataset%2520for%2520TV%2520audio%2520description%2520is%2520formulated.%2520Our%2520approach%252C%2520named%250AAutoAD-Zero%252C%2520demonstrates%2520outstanding%2520performance%2520%2528even%2520competitive%2520with%2520some%250Amodels%2520fine-tuned%2520on%2520ground%2520truth%2520ADs%2529%2520in%2520AD%2520generation%2520for%2520both%2520movies%2520and%2520TV%250Aseries%252C%2520achieving%2520state-of-the-art%2520CRITIC%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoAD-Zero%3A%20A%20Training-Free%20Framework%20for%20Zero-Shot%20Audio%20Description&entry.906535625=Junyu%20Xie%20and%20Tengda%20Han%20and%20Max%20Bain%20and%20Arsha%20Nagrani%20and%20G%C3%BCl%20Varol%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Our%20objective%20is%20to%20generate%20Audio%20Descriptions%20%28ADs%29%20for%20both%20movies%20and%20TV%0Aseries%20in%20a%20training-free%20manner.%20We%20use%20the%20power%20of%20off-the-shelf%0AVisual-Language%20Models%20%28VLMs%29%20and%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20develop%0Avisual%20and%20text%20prompting%20strategies%20for%20this%20task.%20Our%20contributions%20are%0Athree-fold%3A%20%28i%29%20We%20demonstrate%20that%20a%20VLM%20can%20successfully%20name%20and%20refer%20to%0Acharacters%20if%20directly%20prompted%20with%20character%20information%20through%20visual%0Aindications%20without%20requiring%20any%20fine-tuning%3B%20%28ii%29%20A%20two-stage%20process%20is%0Adeveloped%20to%20generate%20ADs%2C%20with%20the%20first%20stage%20asking%20the%20VLM%20to%0Acomprehensively%20describe%20the%20video%2C%20followed%20by%20a%20second%20stage%20utilising%20a%20LLM%0Ato%20summarise%20dense%20textual%20information%20into%20one%20succinct%20AD%20sentence%3B%20%28iii%29%20A%0Anew%20dataset%20for%20TV%20audio%20description%20is%20formulated.%20Our%20approach%2C%20named%0AAutoAD-Zero%2C%20demonstrates%20outstanding%20performance%20%28even%20competitive%20with%20some%0Amodels%20fine-tuned%20on%20ground%20truth%20ADs%29%20in%20AD%20generation%20for%20both%20movies%20and%20TV%0Aseries%2C%20achieving%20state-of-the-art%20CRITIC%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15850v1&entry.124074799=Read"},
{"title": "SEGIC: Unleashing the Emergent Correspondence for In-Context\n  Segmentation", "author": "Lingchen Meng and Shiyi Lan and Hengduo Li and Jose M. Alvarez and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  In-context segmentation aims at segmenting novel images using a few labeled\nexample images, termed as \"in-context examples\", exploring content similarities\nbetween examples and the target. The resulting models can be generalized\nseamlessly to novel segmentation tasks, significantly reducing the labeling and\ntraining costs compared with conventional pipelines. However, in-context\nsegmentation is more challenging than classic ones requiring the model to learn\nsegmentation rules conditioned on a few samples. Unlike previous work with\nad-hoc or non-end-to-end designs, we propose SEGIC, an end-to-end\nsegment-in-context framework built upon a single vision foundation model (VFM).\nIn particular, SEGIC leverages the emergent correspondence within VFM to\ncapture dense relationships between target images and in-context samples. As\nsuch, information from in-context samples is then extracted into three types of\ninstructions, i.e. geometric, visual, and meta instructions, serving as\nexplicit conditions for the final mask prediction. SEGIC is a straightforward\nyet effective approach that yields state-of-the-art performance on one-shot\nsegmentation benchmarks. Notably, SEGIC can be easily generalized to diverse\ntasks, including video object segmentation and open-vocabulary segmentation.\nCode will be available at https://github.com/MengLcool/SEGIC.\n", "link": "http://arxiv.org/abs/2311.14671v3", "date": "2024-07-22", "relevancy": 2.6073, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5443}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5104}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEGIC%3A%20Unleashing%20the%20Emergent%20Correspondence%20for%20In-Context%0A%20%20Segmentation&body=Title%3A%20SEGIC%3A%20Unleashing%20the%20Emergent%20Correspondence%20for%20In-Context%0A%20%20Segmentation%0AAuthor%3A%20Lingchen%20Meng%20and%20Shiyi%20Lan%20and%20Hengduo%20Li%20and%20Jose%20M.%20Alvarez%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20In-context%20segmentation%20aims%20at%20segmenting%20novel%20images%20using%20a%20few%20labeled%0Aexample%20images%2C%20termed%20as%20%22in-context%20examples%22%2C%20exploring%20content%20similarities%0Abetween%20examples%20and%20the%20target.%20The%20resulting%20models%20can%20be%20generalized%0Aseamlessly%20to%20novel%20segmentation%20tasks%2C%20significantly%20reducing%20the%20labeling%20and%0Atraining%20costs%20compared%20with%20conventional%20pipelines.%20However%2C%20in-context%0Asegmentation%20is%20more%20challenging%20than%20classic%20ones%20requiring%20the%20model%20to%20learn%0Asegmentation%20rules%20conditioned%20on%20a%20few%20samples.%20Unlike%20previous%20work%20with%0Aad-hoc%20or%20non-end-to-end%20designs%2C%20we%20propose%20SEGIC%2C%20an%20end-to-end%0Asegment-in-context%20framework%20built%20upon%20a%20single%20vision%20foundation%20model%20%28VFM%29.%0AIn%20particular%2C%20SEGIC%20leverages%20the%20emergent%20correspondence%20within%20VFM%20to%0Acapture%20dense%20relationships%20between%20target%20images%20and%20in-context%20samples.%20As%0Asuch%2C%20information%20from%20in-context%20samples%20is%20then%20extracted%20into%20three%20types%20of%0Ainstructions%2C%20i.e.%20geometric%2C%20visual%2C%20and%20meta%20instructions%2C%20serving%20as%0Aexplicit%20conditions%20for%20the%20final%20mask%20prediction.%20SEGIC%20is%20a%20straightforward%0Ayet%20effective%20approach%20that%20yields%20state-of-the-art%20performance%20on%20one-shot%0Asegmentation%20benchmarks.%20Notably%2C%20SEGIC%20can%20be%20easily%20generalized%20to%20diverse%0Atasks%2C%20including%20video%20object%20segmentation%20and%20open-vocabulary%20segmentation.%0ACode%20will%20be%20available%20at%20https%3A//github.com/MengLcool/SEGIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14671v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEGIC%253A%2520Unleashing%2520the%2520Emergent%2520Correspondence%2520for%2520In-Context%250A%2520%2520Segmentation%26entry.906535625%3DLingchen%2520Meng%2520and%2520Shiyi%2520Lan%2520and%2520Hengduo%2520Li%2520and%2520Jose%2520M.%2520Alvarez%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520In-context%2520segmentation%2520aims%2520at%2520segmenting%2520novel%2520images%2520using%2520a%2520few%2520labeled%250Aexample%2520images%252C%2520termed%2520as%2520%2522in-context%2520examples%2522%252C%2520exploring%2520content%2520similarities%250Abetween%2520examples%2520and%2520the%2520target.%2520The%2520resulting%2520models%2520can%2520be%2520generalized%250Aseamlessly%2520to%2520novel%2520segmentation%2520tasks%252C%2520significantly%2520reducing%2520the%2520labeling%2520and%250Atraining%2520costs%2520compared%2520with%2520conventional%2520pipelines.%2520However%252C%2520in-context%250Asegmentation%2520is%2520more%2520challenging%2520than%2520classic%2520ones%2520requiring%2520the%2520model%2520to%2520learn%250Asegmentation%2520rules%2520conditioned%2520on%2520a%2520few%2520samples.%2520Unlike%2520previous%2520work%2520with%250Aad-hoc%2520or%2520non-end-to-end%2520designs%252C%2520we%2520propose%2520SEGIC%252C%2520an%2520end-to-end%250Asegment-in-context%2520framework%2520built%2520upon%2520a%2520single%2520vision%2520foundation%2520model%2520%2528VFM%2529.%250AIn%2520particular%252C%2520SEGIC%2520leverages%2520the%2520emergent%2520correspondence%2520within%2520VFM%2520to%250Acapture%2520dense%2520relationships%2520between%2520target%2520images%2520and%2520in-context%2520samples.%2520As%250Asuch%252C%2520information%2520from%2520in-context%2520samples%2520is%2520then%2520extracted%2520into%2520three%2520types%2520of%250Ainstructions%252C%2520i.e.%2520geometric%252C%2520visual%252C%2520and%2520meta%2520instructions%252C%2520serving%2520as%250Aexplicit%2520conditions%2520for%2520the%2520final%2520mask%2520prediction.%2520SEGIC%2520is%2520a%2520straightforward%250Ayet%2520effective%2520approach%2520that%2520yields%2520state-of-the-art%2520performance%2520on%2520one-shot%250Asegmentation%2520benchmarks.%2520Notably%252C%2520SEGIC%2520can%2520be%2520easily%2520generalized%2520to%2520diverse%250Atasks%252C%2520including%2520video%2520object%2520segmentation%2520and%2520open-vocabulary%2520segmentation.%250ACode%2520will%2520be%2520available%2520at%2520https%253A//github.com/MengLcool/SEGIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14671v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEGIC%3A%20Unleashing%20the%20Emergent%20Correspondence%20for%20In-Context%0A%20%20Segmentation&entry.906535625=Lingchen%20Meng%20and%20Shiyi%20Lan%20and%20Hengduo%20Li%20and%20Jose%20M.%20Alvarez%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20In-context%20segmentation%20aims%20at%20segmenting%20novel%20images%20using%20a%20few%20labeled%0Aexample%20images%2C%20termed%20as%20%22in-context%20examples%22%2C%20exploring%20content%20similarities%0Abetween%20examples%20and%20the%20target.%20The%20resulting%20models%20can%20be%20generalized%0Aseamlessly%20to%20novel%20segmentation%20tasks%2C%20significantly%20reducing%20the%20labeling%20and%0Atraining%20costs%20compared%20with%20conventional%20pipelines.%20However%2C%20in-context%0Asegmentation%20is%20more%20challenging%20than%20classic%20ones%20requiring%20the%20model%20to%20learn%0Asegmentation%20rules%20conditioned%20on%20a%20few%20samples.%20Unlike%20previous%20work%20with%0Aad-hoc%20or%20non-end-to-end%20designs%2C%20we%20propose%20SEGIC%2C%20an%20end-to-end%0Asegment-in-context%20framework%20built%20upon%20a%20single%20vision%20foundation%20model%20%28VFM%29.%0AIn%20particular%2C%20SEGIC%20leverages%20the%20emergent%20correspondence%20within%20VFM%20to%0Acapture%20dense%20relationships%20between%20target%20images%20and%20in-context%20samples.%20As%0Asuch%2C%20information%20from%20in-context%20samples%20is%20then%20extracted%20into%20three%20types%20of%0Ainstructions%2C%20i.e.%20geometric%2C%20visual%2C%20and%20meta%20instructions%2C%20serving%20as%0Aexplicit%20conditions%20for%20the%20final%20mask%20prediction.%20SEGIC%20is%20a%20straightforward%0Ayet%20effective%20approach%20that%20yields%20state-of-the-art%20performance%20on%20one-shot%0Asegmentation%20benchmarks.%20Notably%2C%20SEGIC%20can%20be%20easily%20generalized%20to%20diverse%0Atasks%2C%20including%20video%20object%20segmentation%20and%20open-vocabulary%20segmentation.%0ACode%20will%20be%20available%20at%20https%3A//github.com/MengLcool/SEGIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14671v3&entry.124074799=Read"},
{"title": "STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay", "author": "Yongcan Yu and Lijun Sheng and Ran He and Jian Liang", "abstract": "  Test-time adaptation (TTA) aims to address the distribution shift between the\ntraining and test data with only unlabeled data at test time. Existing TTA\nmethods often focus on improving recognition performance specifically for test\ndata associated with classes in the training set. However, during the\nopen-world inference process, there are inevitably test data instances from\nunknown classes, commonly referred to as outliers. This paper pays attention to\nthe problem that conducts both sample recognition and outlier rejection during\ninference while outliers exist. To address this problem, we propose a new\napproach called STAble Memory rePlay (STAMP), which performs optimization over\na stable memory bank instead of the risky mini-batch. In particular, the memory\nbank is dynamically updated by selecting low-entropy and label-consistent\nsamples in a class-balanced manner. In addition, we develop a self-weighted\nentropy minimization strategy that assigns higher weight to low-entropy\nsamples. Extensive results demonstrate that STAMP outperforms existing TTA\nmethods in terms of both recognition and outlier detection performance. The\ncode is released at https://github.com/yuyongcan/STAMP.\n", "link": "http://arxiv.org/abs/2407.15773v1", "date": "2024-07-22", "relevancy": 2.6037, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5013}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAMP%3A%20Outlier-Aware%20Test-Time%20Adaptation%20with%20Stable%20Memory%20Replay&body=Title%3A%20STAMP%3A%20Outlier-Aware%20Test-Time%20Adaptation%20with%20Stable%20Memory%20Replay%0AAuthor%3A%20Yongcan%20Yu%20and%20Lijun%20Sheng%20and%20Ran%20He%20and%20Jian%20Liang%0AAbstract%3A%20%20%20Test-time%20adaptation%20%28TTA%29%20aims%20to%20address%20the%20distribution%20shift%20between%20the%0Atraining%20and%20test%20data%20with%20only%20unlabeled%20data%20at%20test%20time.%20Existing%20TTA%0Amethods%20often%20focus%20on%20improving%20recognition%20performance%20specifically%20for%20test%0Adata%20associated%20with%20classes%20in%20the%20training%20set.%20However%2C%20during%20the%0Aopen-world%20inference%20process%2C%20there%20are%20inevitably%20test%20data%20instances%20from%0Aunknown%20classes%2C%20commonly%20referred%20to%20as%20outliers.%20This%20paper%20pays%20attention%20to%0Athe%20problem%20that%20conducts%20both%20sample%20recognition%20and%20outlier%20rejection%20during%0Ainference%20while%20outliers%20exist.%20To%20address%20this%20problem%2C%20we%20propose%20a%20new%0Aapproach%20called%20STAble%20Memory%20rePlay%20%28STAMP%29%2C%20which%20performs%20optimization%20over%0Aa%20stable%20memory%20bank%20instead%20of%20the%20risky%20mini-batch.%20In%20particular%2C%20the%20memory%0Abank%20is%20dynamically%20updated%20by%20selecting%20low-entropy%20and%20label-consistent%0Asamples%20in%20a%20class-balanced%20manner.%20In%20addition%2C%20we%20develop%20a%20self-weighted%0Aentropy%20minimization%20strategy%20that%20assigns%20higher%20weight%20to%20low-entropy%0Asamples.%20Extensive%20results%20demonstrate%20that%20STAMP%20outperforms%20existing%20TTA%0Amethods%20in%20terms%20of%20both%20recognition%20and%20outlier%20detection%20performance.%20The%0Acode%20is%20released%20at%20https%3A//github.com/yuyongcan/STAMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAMP%253A%2520Outlier-Aware%2520Test-Time%2520Adaptation%2520with%2520Stable%2520Memory%2520Replay%26entry.906535625%3DYongcan%2520Yu%2520and%2520Lijun%2520Sheng%2520and%2520Ran%2520He%2520and%2520Jian%2520Liang%26entry.1292438233%3D%2520%2520Test-time%2520adaptation%2520%2528TTA%2529%2520aims%2520to%2520address%2520the%2520distribution%2520shift%2520between%2520the%250Atraining%2520and%2520test%2520data%2520with%2520only%2520unlabeled%2520data%2520at%2520test%2520time.%2520Existing%2520TTA%250Amethods%2520often%2520focus%2520on%2520improving%2520recognition%2520performance%2520specifically%2520for%2520test%250Adata%2520associated%2520with%2520classes%2520in%2520the%2520training%2520set.%2520However%252C%2520during%2520the%250Aopen-world%2520inference%2520process%252C%2520there%2520are%2520inevitably%2520test%2520data%2520instances%2520from%250Aunknown%2520classes%252C%2520commonly%2520referred%2520to%2520as%2520outliers.%2520This%2520paper%2520pays%2520attention%2520to%250Athe%2520problem%2520that%2520conducts%2520both%2520sample%2520recognition%2520and%2520outlier%2520rejection%2520during%250Ainference%2520while%2520outliers%2520exist.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520new%250Aapproach%2520called%2520STAble%2520Memory%2520rePlay%2520%2528STAMP%2529%252C%2520which%2520performs%2520optimization%2520over%250Aa%2520stable%2520memory%2520bank%2520instead%2520of%2520the%2520risky%2520mini-batch.%2520In%2520particular%252C%2520the%2520memory%250Abank%2520is%2520dynamically%2520updated%2520by%2520selecting%2520low-entropy%2520and%2520label-consistent%250Asamples%2520in%2520a%2520class-balanced%2520manner.%2520In%2520addition%252C%2520we%2520develop%2520a%2520self-weighted%250Aentropy%2520minimization%2520strategy%2520that%2520assigns%2520higher%2520weight%2520to%2520low-entropy%250Asamples.%2520Extensive%2520results%2520demonstrate%2520that%2520STAMP%2520outperforms%2520existing%2520TTA%250Amethods%2520in%2520terms%2520of%2520both%2520recognition%2520and%2520outlier%2520detection%2520performance.%2520The%250Acode%2520is%2520released%2520at%2520https%253A//github.com/yuyongcan/STAMP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAMP%3A%20Outlier-Aware%20Test-Time%20Adaptation%20with%20Stable%20Memory%20Replay&entry.906535625=Yongcan%20Yu%20and%20Lijun%20Sheng%20and%20Ran%20He%20and%20Jian%20Liang&entry.1292438233=%20%20Test-time%20adaptation%20%28TTA%29%20aims%20to%20address%20the%20distribution%20shift%20between%20the%0Atraining%20and%20test%20data%20with%20only%20unlabeled%20data%20at%20test%20time.%20Existing%20TTA%0Amethods%20often%20focus%20on%20improving%20recognition%20performance%20specifically%20for%20test%0Adata%20associated%20with%20classes%20in%20the%20training%20set.%20However%2C%20during%20the%0Aopen-world%20inference%20process%2C%20there%20are%20inevitably%20test%20data%20instances%20from%0Aunknown%20classes%2C%20commonly%20referred%20to%20as%20outliers.%20This%20paper%20pays%20attention%20to%0Athe%20problem%20that%20conducts%20both%20sample%20recognition%20and%20outlier%20rejection%20during%0Ainference%20while%20outliers%20exist.%20To%20address%20this%20problem%2C%20we%20propose%20a%20new%0Aapproach%20called%20STAble%20Memory%20rePlay%20%28STAMP%29%2C%20which%20performs%20optimization%20over%0Aa%20stable%20memory%20bank%20instead%20of%20the%20risky%20mini-batch.%20In%20particular%2C%20the%20memory%0Abank%20is%20dynamically%20updated%20by%20selecting%20low-entropy%20and%20label-consistent%0Asamples%20in%20a%20class-balanced%20manner.%20In%20addition%2C%20we%20develop%20a%20self-weighted%0Aentropy%20minimization%20strategy%20that%20assigns%20higher%20weight%20to%20low-entropy%0Asamples.%20Extensive%20results%20demonstrate%20that%20STAMP%20outperforms%20existing%20TTA%0Amethods%20in%20terms%20of%20both%20recognition%20and%20outlier%20detection%20performance.%20The%0Acode%20is%20released%20at%20https%3A//github.com/yuyongcan/STAMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15773v1&entry.124074799=Read"},
{"title": "Multicell-Fold: geometric learning in folding multicellular life", "author": "Haiqian Yang and Anh Q. Nguyen and Dapeng Bi and Markus J. Buehler and Ming Guo", "abstract": "  During developmental processes such as embryogenesis, how a group of cells\nfold into specific structures, is a central question in biology that defines\nhow living organisms form. Establishing tissue-level morphology critically\nrelies on how every single cell decides to position itself relative to its\nneighboring cells. Despite its importance, it remains a major challenge to\nunderstand and predict the behavior of every cell within the living tissue over\ntime during such intricate processes. To tackle this question, we propose a\ngeometric deep learning model that can predict multicellular folding and\nembryogenesis, accurately capturing the highly convoluted spatial interactions\namong cells. We demonstrate that multicellular data can be represented with\nboth granular and foam-like physical pictures through a unified graph data\nstructure, considering both cellular interactions and cell junction networks.\nWe successfully use our model to achieve two important tasks, interpretable 4-D\nmorphological sequence alignment, and predicting local cell rearrangements\nbefore they occur at single-cell resolution. Furthermore, using an activation\nmap and ablation studies, we demonstrate that cell geometries and cell junction\nnetworks together regulate local cell rearrangement which is critical for\nembryo morphogenesis. This approach provides a novel paradigm to study\nmorphogenesis, highlighting a unified data structure and harnessing the power\nof geometric deep learning to accurately model the mechanisms and behaviors of\ncells during development. It offers a pathway toward creating a unified dynamic\nmorphological atlas for a variety of developmental processes such as\nembryogenesis.\n", "link": "http://arxiv.org/abs/2407.07055v2", "date": "2024-07-22", "relevancy": 2.4969, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multicell-Fold%3A%20geometric%20learning%20in%20folding%20multicellular%20life&body=Title%3A%20Multicell-Fold%3A%20geometric%20learning%20in%20folding%20multicellular%20life%0AAuthor%3A%20Haiqian%20Yang%20and%20Anh%20Q.%20Nguyen%20and%20Dapeng%20Bi%20and%20Markus%20J.%20Buehler%20and%20Ming%20Guo%0AAbstract%3A%20%20%20During%20developmental%20processes%20such%20as%20embryogenesis%2C%20how%20a%20group%20of%20cells%0Afold%20into%20specific%20structures%2C%20is%20a%20central%20question%20in%20biology%20that%20defines%0Ahow%20living%20organisms%20form.%20Establishing%20tissue-level%20morphology%20critically%0Arelies%20on%20how%20every%20single%20cell%20decides%20to%20position%20itself%20relative%20to%20its%0Aneighboring%20cells.%20Despite%20its%20importance%2C%20it%20remains%20a%20major%20challenge%20to%0Aunderstand%20and%20predict%20the%20behavior%20of%20every%20cell%20within%20the%20living%20tissue%20over%0Atime%20during%20such%20intricate%20processes.%20To%20tackle%20this%20question%2C%20we%20propose%20a%0Ageometric%20deep%20learning%20model%20that%20can%20predict%20multicellular%20folding%20and%0Aembryogenesis%2C%20accurately%20capturing%20the%20highly%20convoluted%20spatial%20interactions%0Aamong%20cells.%20We%20demonstrate%20that%20multicellular%20data%20can%20be%20represented%20with%0Aboth%20granular%20and%20foam-like%20physical%20pictures%20through%20a%20unified%20graph%20data%0Astructure%2C%20considering%20both%20cellular%20interactions%20and%20cell%20junction%20networks.%0AWe%20successfully%20use%20our%20model%20to%20achieve%20two%20important%20tasks%2C%20interpretable%204-D%0Amorphological%20sequence%20alignment%2C%20and%20predicting%20local%20cell%20rearrangements%0Abefore%20they%20occur%20at%20single-cell%20resolution.%20Furthermore%2C%20using%20an%20activation%0Amap%20and%20ablation%20studies%2C%20we%20demonstrate%20that%20cell%20geometries%20and%20cell%20junction%0Anetworks%20together%20regulate%20local%20cell%20rearrangement%20which%20is%20critical%20for%0Aembryo%20morphogenesis.%20This%20approach%20provides%20a%20novel%20paradigm%20to%20study%0Amorphogenesis%2C%20highlighting%20a%20unified%20data%20structure%20and%20harnessing%20the%20power%0Aof%20geometric%20deep%20learning%20to%20accurately%20model%20the%20mechanisms%20and%20behaviors%20of%0Acells%20during%20development.%20It%20offers%20a%20pathway%20toward%20creating%20a%20unified%20dynamic%0Amorphological%20atlas%20for%20a%20variety%20of%20developmental%20processes%20such%20as%0Aembryogenesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulticell-Fold%253A%2520geometric%2520learning%2520in%2520folding%2520multicellular%2520life%26entry.906535625%3DHaiqian%2520Yang%2520and%2520Anh%2520Q.%2520Nguyen%2520and%2520Dapeng%2520Bi%2520and%2520Markus%2520J.%2520Buehler%2520and%2520Ming%2520Guo%26entry.1292438233%3D%2520%2520During%2520developmental%2520processes%2520such%2520as%2520embryogenesis%252C%2520how%2520a%2520group%2520of%2520cells%250Afold%2520into%2520specific%2520structures%252C%2520is%2520a%2520central%2520question%2520in%2520biology%2520that%2520defines%250Ahow%2520living%2520organisms%2520form.%2520Establishing%2520tissue-level%2520morphology%2520critically%250Arelies%2520on%2520how%2520every%2520single%2520cell%2520decides%2520to%2520position%2520itself%2520relative%2520to%2520its%250Aneighboring%2520cells.%2520Despite%2520its%2520importance%252C%2520it%2520remains%2520a%2520major%2520challenge%2520to%250Aunderstand%2520and%2520predict%2520the%2520behavior%2520of%2520every%2520cell%2520within%2520the%2520living%2520tissue%2520over%250Atime%2520during%2520such%2520intricate%2520processes.%2520To%2520tackle%2520this%2520question%252C%2520we%2520propose%2520a%250Ageometric%2520deep%2520learning%2520model%2520that%2520can%2520predict%2520multicellular%2520folding%2520and%250Aembryogenesis%252C%2520accurately%2520capturing%2520the%2520highly%2520convoluted%2520spatial%2520interactions%250Aamong%2520cells.%2520We%2520demonstrate%2520that%2520multicellular%2520data%2520can%2520be%2520represented%2520with%250Aboth%2520granular%2520and%2520foam-like%2520physical%2520pictures%2520through%2520a%2520unified%2520graph%2520data%250Astructure%252C%2520considering%2520both%2520cellular%2520interactions%2520and%2520cell%2520junction%2520networks.%250AWe%2520successfully%2520use%2520our%2520model%2520to%2520achieve%2520two%2520important%2520tasks%252C%2520interpretable%25204-D%250Amorphological%2520sequence%2520alignment%252C%2520and%2520predicting%2520local%2520cell%2520rearrangements%250Abefore%2520they%2520occur%2520at%2520single-cell%2520resolution.%2520Furthermore%252C%2520using%2520an%2520activation%250Amap%2520and%2520ablation%2520studies%252C%2520we%2520demonstrate%2520that%2520cell%2520geometries%2520and%2520cell%2520junction%250Anetworks%2520together%2520regulate%2520local%2520cell%2520rearrangement%2520which%2520is%2520critical%2520for%250Aembryo%2520morphogenesis.%2520This%2520approach%2520provides%2520a%2520novel%2520paradigm%2520to%2520study%250Amorphogenesis%252C%2520highlighting%2520a%2520unified%2520data%2520structure%2520and%2520harnessing%2520the%2520power%250Aof%2520geometric%2520deep%2520learning%2520to%2520accurately%2520model%2520the%2520mechanisms%2520and%2520behaviors%2520of%250Acells%2520during%2520development.%2520It%2520offers%2520a%2520pathway%2520toward%2520creating%2520a%2520unified%2520dynamic%250Amorphological%2520atlas%2520for%2520a%2520variety%2520of%2520developmental%2520processes%2520such%2520as%250Aembryogenesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multicell-Fold%3A%20geometric%20learning%20in%20folding%20multicellular%20life&entry.906535625=Haiqian%20Yang%20and%20Anh%20Q.%20Nguyen%20and%20Dapeng%20Bi%20and%20Markus%20J.%20Buehler%20and%20Ming%20Guo&entry.1292438233=%20%20During%20developmental%20processes%20such%20as%20embryogenesis%2C%20how%20a%20group%20of%20cells%0Afold%20into%20specific%20structures%2C%20is%20a%20central%20question%20in%20biology%20that%20defines%0Ahow%20living%20organisms%20form.%20Establishing%20tissue-level%20morphology%20critically%0Arelies%20on%20how%20every%20single%20cell%20decides%20to%20position%20itself%20relative%20to%20its%0Aneighboring%20cells.%20Despite%20its%20importance%2C%20it%20remains%20a%20major%20challenge%20to%0Aunderstand%20and%20predict%20the%20behavior%20of%20every%20cell%20within%20the%20living%20tissue%20over%0Atime%20during%20such%20intricate%20processes.%20To%20tackle%20this%20question%2C%20we%20propose%20a%0Ageometric%20deep%20learning%20model%20that%20can%20predict%20multicellular%20folding%20and%0Aembryogenesis%2C%20accurately%20capturing%20the%20highly%20convoluted%20spatial%20interactions%0Aamong%20cells.%20We%20demonstrate%20that%20multicellular%20data%20can%20be%20represented%20with%0Aboth%20granular%20and%20foam-like%20physical%20pictures%20through%20a%20unified%20graph%20data%0Astructure%2C%20considering%20both%20cellular%20interactions%20and%20cell%20junction%20networks.%0AWe%20successfully%20use%20our%20model%20to%20achieve%20two%20important%20tasks%2C%20interpretable%204-D%0Amorphological%20sequence%20alignment%2C%20and%20predicting%20local%20cell%20rearrangements%0Abefore%20they%20occur%20at%20single-cell%20resolution.%20Furthermore%2C%20using%20an%20activation%0Amap%20and%20ablation%20studies%2C%20we%20demonstrate%20that%20cell%20geometries%20and%20cell%20junction%0Anetworks%20together%20regulate%20local%20cell%20rearrangement%20which%20is%20critical%20for%0Aembryo%20morphogenesis.%20This%20approach%20provides%20a%20novel%20paradigm%20to%20study%0Amorphogenesis%2C%20highlighting%20a%20unified%20data%20structure%20and%20harnessing%20the%20power%0Aof%20geometric%20deep%20learning%20to%20accurately%20model%20the%20mechanisms%20and%20behaviors%20of%0Acells%20during%20development.%20It%20offers%20a%20pathway%20toward%20creating%20a%20unified%20dynamic%0Amorphological%20atlas%20for%20a%20variety%20of%20developmental%20processes%20such%20as%0Aembryogenesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07055v2&entry.124074799=Read"},
{"title": "Local Occupancy-Enhanced Object Grasping with Multiple Triplanar\n  Projection", "author": "Kangqi Ma and Hao Dong and Yadong Mu", "abstract": "  This paper addresses the challenge of robotic grasping of general objects.\nSimilar to prior research, the task reads a single-view 3D observation (i.e.,\npoint clouds) captured by a depth camera as input. Crucially, the success of\nobject grasping highly demands a comprehensive understanding of the shape of\nobjects within the scene. However, single-view observations often suffer from\nocclusions (including both self and inter-object occlusions), which lead to\ngaps in the point clouds, especially in complex cluttered scenes. This renders\nincomplete perception of the object shape and frequently causes failures or\ninaccurate pose estimation during object grasping. In this paper, we tackle\nthis issue with an effective albeit simple solution, namely completing\ngrasping-related scene regions through local occupancy prediction. Following\nprior practice, the proposed model first runs by proposing a number of most\nlikely grasp points in the scene. Around each grasp point, a module is designed\nto infer any voxel in its neighborhood to be either void or occupied by some\nobject. Importantly, the occupancy map is inferred by fusing both local and\nglobal cues. We implement a multi-group tri-plane scheme for efficiently\naggregating long-distance contextual information. The model further estimates\n6-DoF grasp poses utilizing the local occupancy-enhanced object shape\ninformation and returns the top-ranked grasp proposal. Comprehensive\nexperiments on both the large-scale GraspNet-1Billion benchmark and real\nrobotic arm demonstrate that the proposed method can effectively complete the\nunobserved parts in cluttered and occluded scenes. Benefiting from the\noccupancy-enhanced feature, our model clearly outstrips other competing methods\nunder various performance metrics such as grasping average precision.\n", "link": "http://arxiv.org/abs/2407.15771v1", "date": "2024-07-22", "relevancy": 2.4646, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6216}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6188}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Occupancy-Enhanced%20Object%20Grasping%20with%20Multiple%20Triplanar%0A%20%20Projection&body=Title%3A%20Local%20Occupancy-Enhanced%20Object%20Grasping%20with%20Multiple%20Triplanar%0A%20%20Projection%0AAuthor%3A%20Kangqi%20Ma%20and%20Hao%20Dong%20and%20Yadong%20Mu%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20robotic%20grasping%20of%20general%20objects.%0ASimilar%20to%20prior%20research%2C%20the%20task%20reads%20a%20single-view%203D%20observation%20%28i.e.%2C%0Apoint%20clouds%29%20captured%20by%20a%20depth%20camera%20as%20input.%20Crucially%2C%20the%20success%20of%0Aobject%20grasping%20highly%20demands%20a%20comprehensive%20understanding%20of%20the%20shape%20of%0Aobjects%20within%20the%20scene.%20However%2C%20single-view%20observations%20often%20suffer%20from%0Aocclusions%20%28including%20both%20self%20and%20inter-object%20occlusions%29%2C%20which%20lead%20to%0Agaps%20in%20the%20point%20clouds%2C%20especially%20in%20complex%20cluttered%20scenes.%20This%20renders%0Aincomplete%20perception%20of%20the%20object%20shape%20and%20frequently%20causes%20failures%20or%0Ainaccurate%20pose%20estimation%20during%20object%20grasping.%20In%20this%20paper%2C%20we%20tackle%0Athis%20issue%20with%20an%20effective%20albeit%20simple%20solution%2C%20namely%20completing%0Agrasping-related%20scene%20regions%20through%20local%20occupancy%20prediction.%20Following%0Aprior%20practice%2C%20the%20proposed%20model%20first%20runs%20by%20proposing%20a%20number%20of%20most%0Alikely%20grasp%20points%20in%20the%20scene.%20Around%20each%20grasp%20point%2C%20a%20module%20is%20designed%0Ato%20infer%20any%20voxel%20in%20its%20neighborhood%20to%20be%20either%20void%20or%20occupied%20by%20some%0Aobject.%20Importantly%2C%20the%20occupancy%20map%20is%20inferred%20by%20fusing%20both%20local%20and%0Aglobal%20cues.%20We%20implement%20a%20multi-group%20tri-plane%20scheme%20for%20efficiently%0Aaggregating%20long-distance%20contextual%20information.%20The%20model%20further%20estimates%0A6-DoF%20grasp%20poses%20utilizing%20the%20local%20occupancy-enhanced%20object%20shape%0Ainformation%20and%20returns%20the%20top-ranked%20grasp%20proposal.%20Comprehensive%0Aexperiments%20on%20both%20the%20large-scale%20GraspNet-1Billion%20benchmark%20and%20real%0Arobotic%20arm%20demonstrate%20that%20the%20proposed%20method%20can%20effectively%20complete%20the%0Aunobserved%20parts%20in%20cluttered%20and%20occluded%20scenes.%20Benefiting%20from%20the%0Aoccupancy-enhanced%20feature%2C%20our%20model%20clearly%20outstrips%20other%20competing%20methods%0Aunder%20various%20performance%20metrics%20such%20as%20grasping%20average%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Occupancy-Enhanced%2520Object%2520Grasping%2520with%2520Multiple%2520Triplanar%250A%2520%2520Projection%26entry.906535625%3DKangqi%2520Ma%2520and%2520Hao%2520Dong%2520and%2520Yadong%2520Mu%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520robotic%2520grasping%2520of%2520general%2520objects.%250ASimilar%2520to%2520prior%2520research%252C%2520the%2520task%2520reads%2520a%2520single-view%25203D%2520observation%2520%2528i.e.%252C%250Apoint%2520clouds%2529%2520captured%2520by%2520a%2520depth%2520camera%2520as%2520input.%2520Crucially%252C%2520the%2520success%2520of%250Aobject%2520grasping%2520highly%2520demands%2520a%2520comprehensive%2520understanding%2520of%2520the%2520shape%2520of%250Aobjects%2520within%2520the%2520scene.%2520However%252C%2520single-view%2520observations%2520often%2520suffer%2520from%250Aocclusions%2520%2528including%2520both%2520self%2520and%2520inter-object%2520occlusions%2529%252C%2520which%2520lead%2520to%250Agaps%2520in%2520the%2520point%2520clouds%252C%2520especially%2520in%2520complex%2520cluttered%2520scenes.%2520This%2520renders%250Aincomplete%2520perception%2520of%2520the%2520object%2520shape%2520and%2520frequently%2520causes%2520failures%2520or%250Ainaccurate%2520pose%2520estimation%2520during%2520object%2520grasping.%2520In%2520this%2520paper%252C%2520we%2520tackle%250Athis%2520issue%2520with%2520an%2520effective%2520albeit%2520simple%2520solution%252C%2520namely%2520completing%250Agrasping-related%2520scene%2520regions%2520through%2520local%2520occupancy%2520prediction.%2520Following%250Aprior%2520practice%252C%2520the%2520proposed%2520model%2520first%2520runs%2520by%2520proposing%2520a%2520number%2520of%2520most%250Alikely%2520grasp%2520points%2520in%2520the%2520scene.%2520Around%2520each%2520grasp%2520point%252C%2520a%2520module%2520is%2520designed%250Ato%2520infer%2520any%2520voxel%2520in%2520its%2520neighborhood%2520to%2520be%2520either%2520void%2520or%2520occupied%2520by%2520some%250Aobject.%2520Importantly%252C%2520the%2520occupancy%2520map%2520is%2520inferred%2520by%2520fusing%2520both%2520local%2520and%250Aglobal%2520cues.%2520We%2520implement%2520a%2520multi-group%2520tri-plane%2520scheme%2520for%2520efficiently%250Aaggregating%2520long-distance%2520contextual%2520information.%2520The%2520model%2520further%2520estimates%250A6-DoF%2520grasp%2520poses%2520utilizing%2520the%2520local%2520occupancy-enhanced%2520object%2520shape%250Ainformation%2520and%2520returns%2520the%2520top-ranked%2520grasp%2520proposal.%2520Comprehensive%250Aexperiments%2520on%2520both%2520the%2520large-scale%2520GraspNet-1Billion%2520benchmark%2520and%2520real%250Arobotic%2520arm%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%2520effectively%2520complete%2520the%250Aunobserved%2520parts%2520in%2520cluttered%2520and%2520occluded%2520scenes.%2520Benefiting%2520from%2520the%250Aoccupancy-enhanced%2520feature%252C%2520our%2520model%2520clearly%2520outstrips%2520other%2520competing%2520methods%250Aunder%2520various%2520performance%2520metrics%2520such%2520as%2520grasping%2520average%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Occupancy-Enhanced%20Object%20Grasping%20with%20Multiple%20Triplanar%0A%20%20Projection&entry.906535625=Kangqi%20Ma%20and%20Hao%20Dong%20and%20Yadong%20Mu&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20robotic%20grasping%20of%20general%20objects.%0ASimilar%20to%20prior%20research%2C%20the%20task%20reads%20a%20single-view%203D%20observation%20%28i.e.%2C%0Apoint%20clouds%29%20captured%20by%20a%20depth%20camera%20as%20input.%20Crucially%2C%20the%20success%20of%0Aobject%20grasping%20highly%20demands%20a%20comprehensive%20understanding%20of%20the%20shape%20of%0Aobjects%20within%20the%20scene.%20However%2C%20single-view%20observations%20often%20suffer%20from%0Aocclusions%20%28including%20both%20self%20and%20inter-object%20occlusions%29%2C%20which%20lead%20to%0Agaps%20in%20the%20point%20clouds%2C%20especially%20in%20complex%20cluttered%20scenes.%20This%20renders%0Aincomplete%20perception%20of%20the%20object%20shape%20and%20frequently%20causes%20failures%20or%0Ainaccurate%20pose%20estimation%20during%20object%20grasping.%20In%20this%20paper%2C%20we%20tackle%0Athis%20issue%20with%20an%20effective%20albeit%20simple%20solution%2C%20namely%20completing%0Agrasping-related%20scene%20regions%20through%20local%20occupancy%20prediction.%20Following%0Aprior%20practice%2C%20the%20proposed%20model%20first%20runs%20by%20proposing%20a%20number%20of%20most%0Alikely%20grasp%20points%20in%20the%20scene.%20Around%20each%20grasp%20point%2C%20a%20module%20is%20designed%0Ato%20infer%20any%20voxel%20in%20its%20neighborhood%20to%20be%20either%20void%20or%20occupied%20by%20some%0Aobject.%20Importantly%2C%20the%20occupancy%20map%20is%20inferred%20by%20fusing%20both%20local%20and%0Aglobal%20cues.%20We%20implement%20a%20multi-group%20tri-plane%20scheme%20for%20efficiently%0Aaggregating%20long-distance%20contextual%20information.%20The%20model%20further%20estimates%0A6-DoF%20grasp%20poses%20utilizing%20the%20local%20occupancy-enhanced%20object%20shape%0Ainformation%20and%20returns%20the%20top-ranked%20grasp%20proposal.%20Comprehensive%0Aexperiments%20on%20both%20the%20large-scale%20GraspNet-1Billion%20benchmark%20and%20real%0Arobotic%20arm%20demonstrate%20that%20the%20proposed%20method%20can%20effectively%20complete%20the%0Aunobserved%20parts%20in%20cluttered%20and%20occluded%20scenes.%20Benefiting%20from%20the%0Aoccupancy-enhanced%20feature%2C%20our%20model%20clearly%20outstrips%20other%20competing%20methods%0Aunder%20various%20performance%20metrics%20such%20as%20grasping%20average%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15771v1&entry.124074799=Read"},
{"title": "Cinemo: Consistent and Controllable Image Animation with Motion\n  Diffusion Models", "author": "Xin Ma and Yaohui Wang and Gengyu Jia and Xinyuan Chen and Yuan-Fang Li and Cunjian Chen and Yu Qiao", "abstract": "  Diffusion models have achieved great progress in image animation due to\npowerful generative capabilities. However, maintaining spatio-temporal\nconsistency with detailed information from the input static image over time\n(e.g., style, background, and object of the input static image) and ensuring\nsmoothness in animated video narratives guided by textual prompts still remains\nchallenging. In this paper, we introduce Cinemo, a novel image animation\napproach towards achieving better motion controllability, as well as stronger\ntemporal consistency and smoothness. In general, we propose three effective\nstrategies at the training and inference stages of Cinemo to accomplish our\ngoal. At the training stage, Cinemo focuses on learning the distribution of\nmotion residuals, rather than directly predicting subsequent via a motion\ndiffusion model. Additionally, a structural similarity index-based strategy is\nproposed to enable Cinemo to have better controllability of motion intensity.\nAt the inference stage, a noise refinement technique based on discrete cosine\ntransformation is introduced to mitigate sudden motion changes. Such three\nstrategies enable Cinemo to produce highly consistent, smooth, and\nmotion-controllable results. Compared to previous methods, Cinemo offers\nsimpler and more precise user controllability. Extensive experiments against\nseveral state-of-the-art methods, including both commercial tools and research\napproaches, across multiple metrics, demonstrate the effectiveness and\nsuperiority of our proposed approach.\n", "link": "http://arxiv.org/abs/2407.15642v1", "date": "2024-07-22", "relevancy": 2.4627, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6428}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6147}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cinemo%3A%20Consistent%20and%20Controllable%20Image%20Animation%20with%20Motion%0A%20%20Diffusion%20Models&body=Title%3A%20Cinemo%3A%20Consistent%20and%20Controllable%20Image%20Animation%20with%20Motion%0A%20%20Diffusion%20Models%0AAuthor%3A%20Xin%20Ma%20and%20Yaohui%20Wang%20and%20Gengyu%20Jia%20and%20Xinyuan%20Chen%20and%20Yuan-Fang%20Li%20and%20Cunjian%20Chen%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20great%20progress%20in%20image%20animation%20due%20to%0Apowerful%20generative%20capabilities.%20However%2C%20maintaining%20spatio-temporal%0Aconsistency%20with%20detailed%20information%20from%20the%20input%20static%20image%20over%20time%0A%28e.g.%2C%20style%2C%20background%2C%20and%20object%20of%20the%20input%20static%20image%29%20and%20ensuring%0Asmoothness%20in%20animated%20video%20narratives%20guided%20by%20textual%20prompts%20still%20remains%0Achallenging.%20In%20this%20paper%2C%20we%20introduce%20Cinemo%2C%20a%20novel%20image%20animation%0Aapproach%20towards%20achieving%20better%20motion%20controllability%2C%20as%20well%20as%20stronger%0Atemporal%20consistency%20and%20smoothness.%20In%20general%2C%20we%20propose%20three%20effective%0Astrategies%20at%20the%20training%20and%20inference%20stages%20of%20Cinemo%20to%20accomplish%20our%0Agoal.%20At%20the%20training%20stage%2C%20Cinemo%20focuses%20on%20learning%20the%20distribution%20of%0Amotion%20residuals%2C%20rather%20than%20directly%20predicting%20subsequent%20via%20a%20motion%0Adiffusion%20model.%20Additionally%2C%20a%20structural%20similarity%20index-based%20strategy%20is%0Aproposed%20to%20enable%20Cinemo%20to%20have%20better%20controllability%20of%20motion%20intensity.%0AAt%20the%20inference%20stage%2C%20a%20noise%20refinement%20technique%20based%20on%20discrete%20cosine%0Atransformation%20is%20introduced%20to%20mitigate%20sudden%20motion%20changes.%20Such%20three%0Astrategies%20enable%20Cinemo%20to%20produce%20highly%20consistent%2C%20smooth%2C%20and%0Amotion-controllable%20results.%20Compared%20to%20previous%20methods%2C%20Cinemo%20offers%0Asimpler%20and%20more%20precise%20user%20controllability.%20Extensive%20experiments%20against%0Aseveral%20state-of-the-art%20methods%2C%20including%20both%20commercial%20tools%20and%20research%0Aapproaches%2C%20across%20multiple%20metrics%2C%20demonstrate%20the%20effectiveness%20and%0Asuperiority%20of%20our%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCinemo%253A%2520Consistent%2520and%2520Controllable%2520Image%2520Animation%2520with%2520Motion%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DXin%2520Ma%2520and%2520Yaohui%2520Wang%2520and%2520Gengyu%2520Jia%2520and%2520Xinyuan%2520Chen%2520and%2520Yuan-Fang%2520Li%2520and%2520Cunjian%2520Chen%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520achieved%2520great%2520progress%2520in%2520image%2520animation%2520due%2520to%250Apowerful%2520generative%2520capabilities.%2520However%252C%2520maintaining%2520spatio-temporal%250Aconsistency%2520with%2520detailed%2520information%2520from%2520the%2520input%2520static%2520image%2520over%2520time%250A%2528e.g.%252C%2520style%252C%2520background%252C%2520and%2520object%2520of%2520the%2520input%2520static%2520image%2529%2520and%2520ensuring%250Asmoothness%2520in%2520animated%2520video%2520narratives%2520guided%2520by%2520textual%2520prompts%2520still%2520remains%250Achallenging.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Cinemo%252C%2520a%2520novel%2520image%2520animation%250Aapproach%2520towards%2520achieving%2520better%2520motion%2520controllability%252C%2520as%2520well%2520as%2520stronger%250Atemporal%2520consistency%2520and%2520smoothness.%2520In%2520general%252C%2520we%2520propose%2520three%2520effective%250Astrategies%2520at%2520the%2520training%2520and%2520inference%2520stages%2520of%2520Cinemo%2520to%2520accomplish%2520our%250Agoal.%2520At%2520the%2520training%2520stage%252C%2520Cinemo%2520focuses%2520on%2520learning%2520the%2520distribution%2520of%250Amotion%2520residuals%252C%2520rather%2520than%2520directly%2520predicting%2520subsequent%2520via%2520a%2520motion%250Adiffusion%2520model.%2520Additionally%252C%2520a%2520structural%2520similarity%2520index-based%2520strategy%2520is%250Aproposed%2520to%2520enable%2520Cinemo%2520to%2520have%2520better%2520controllability%2520of%2520motion%2520intensity.%250AAt%2520the%2520inference%2520stage%252C%2520a%2520noise%2520refinement%2520technique%2520based%2520on%2520discrete%2520cosine%250Atransformation%2520is%2520introduced%2520to%2520mitigate%2520sudden%2520motion%2520changes.%2520Such%2520three%250Astrategies%2520enable%2520Cinemo%2520to%2520produce%2520highly%2520consistent%252C%2520smooth%252C%2520and%250Amotion-controllable%2520results.%2520Compared%2520to%2520previous%2520methods%252C%2520Cinemo%2520offers%250Asimpler%2520and%2520more%2520precise%2520user%2520controllability.%2520Extensive%2520experiments%2520against%250Aseveral%2520state-of-the-art%2520methods%252C%2520including%2520both%2520commercial%2520tools%2520and%2520research%250Aapproaches%252C%2520across%2520multiple%2520metrics%252C%2520demonstrate%2520the%2520effectiveness%2520and%250Asuperiority%2520of%2520our%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cinemo%3A%20Consistent%20and%20Controllable%20Image%20Animation%20with%20Motion%0A%20%20Diffusion%20Models&entry.906535625=Xin%20Ma%20and%20Yaohui%20Wang%20and%20Gengyu%20Jia%20and%20Xinyuan%20Chen%20and%20Yuan-Fang%20Li%20and%20Cunjian%20Chen%20and%20Yu%20Qiao&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20great%20progress%20in%20image%20animation%20due%20to%0Apowerful%20generative%20capabilities.%20However%2C%20maintaining%20spatio-temporal%0Aconsistency%20with%20detailed%20information%20from%20the%20input%20static%20image%20over%20time%0A%28e.g.%2C%20style%2C%20background%2C%20and%20object%20of%20the%20input%20static%20image%29%20and%20ensuring%0Asmoothness%20in%20animated%20video%20narratives%20guided%20by%20textual%20prompts%20still%20remains%0Achallenging.%20In%20this%20paper%2C%20we%20introduce%20Cinemo%2C%20a%20novel%20image%20animation%0Aapproach%20towards%20achieving%20better%20motion%20controllability%2C%20as%20well%20as%20stronger%0Atemporal%20consistency%20and%20smoothness.%20In%20general%2C%20we%20propose%20three%20effective%0Astrategies%20at%20the%20training%20and%20inference%20stages%20of%20Cinemo%20to%20accomplish%20our%0Agoal.%20At%20the%20training%20stage%2C%20Cinemo%20focuses%20on%20learning%20the%20distribution%20of%0Amotion%20residuals%2C%20rather%20than%20directly%20predicting%20subsequent%20via%20a%20motion%0Adiffusion%20model.%20Additionally%2C%20a%20structural%20similarity%20index-based%20strategy%20is%0Aproposed%20to%20enable%20Cinemo%20to%20have%20better%20controllability%20of%20motion%20intensity.%0AAt%20the%20inference%20stage%2C%20a%20noise%20refinement%20technique%20based%20on%20discrete%20cosine%0Atransformation%20is%20introduced%20to%20mitigate%20sudden%20motion%20changes.%20Such%20three%0Astrategies%20enable%20Cinemo%20to%20produce%20highly%20consistent%2C%20smooth%2C%20and%0Amotion-controllable%20results.%20Compared%20to%20previous%20methods%2C%20Cinemo%20offers%0Asimpler%20and%20more%20precise%20user%20controllability.%20Extensive%20experiments%20against%0Aseveral%20state-of-the-art%20methods%2C%20including%20both%20commercial%20tools%20and%20research%0Aapproaches%2C%20across%20multiple%20metrics%2C%20demonstrate%20the%20effectiveness%20and%0Asuperiority%20of%20our%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15642v1&entry.124074799=Read"},
{"title": "TOM: A Development Platform For Wearable Intelligent Assistants", "author": "Nuwan Janaka and Shengdong Zhao and David Hsu and Sherisse Tan Jing Wen and Koh Chun Keat", "abstract": "  Advanced digital assistants can significantly enhance task performance,\nreduce user burden, and provide personalized guidance to improve users'\nabilities. However, the development of such intelligent digital assistants\npresents a formidable challenge. To address this, we introduce TOM, a\nconceptual architecture and software platform (https://github.com/TOM-Platform)\ndesigned to support the development of intelligent wearable assistants that are\ncontextually aware of both the user and the environment. This system was\ndeveloped collaboratively with AR/MR researchers, HCI researchers, AI/Robotic\nresearchers, and software developers, and it continues to evolve to meet the\ndiverse requirements of these stakeholders. TOM facilitates the creation of\nintelligent assistive AR applications for daily activities and supports the\nrecording and analysis of user interactions, integration of new devices, and\nthe provision of assistance for various activities. Additionally, we showcase\nseveral proof-of-concept assistive services and discuss the challenges involved\nin developing such services.\n", "link": "http://arxiv.org/abs/2407.15523v1", "date": "2024-07-22", "relevancy": 2.4533, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4943}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.493}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TOM%3A%20A%20Development%20Platform%20For%20Wearable%20Intelligent%20Assistants&body=Title%3A%20TOM%3A%20A%20Development%20Platform%20For%20Wearable%20Intelligent%20Assistants%0AAuthor%3A%20Nuwan%20Janaka%20and%20Shengdong%20Zhao%20and%20David%20Hsu%20and%20Sherisse%20Tan%20Jing%20Wen%20and%20Koh%20Chun%20Keat%0AAbstract%3A%20%20%20Advanced%20digital%20assistants%20can%20significantly%20enhance%20task%20performance%2C%0Areduce%20user%20burden%2C%20and%20provide%20personalized%20guidance%20to%20improve%20users%27%0Aabilities.%20However%2C%20the%20development%20of%20such%20intelligent%20digital%20assistants%0Apresents%20a%20formidable%20challenge.%20To%20address%20this%2C%20we%20introduce%20TOM%2C%20a%0Aconceptual%20architecture%20and%20software%20platform%20%28https%3A//github.com/TOM-Platform%29%0Adesigned%20to%20support%20the%20development%20of%20intelligent%20wearable%20assistants%20that%20are%0Acontextually%20aware%20of%20both%20the%20user%20and%20the%20environment.%20This%20system%20was%0Adeveloped%20collaboratively%20with%20AR/MR%20researchers%2C%20HCI%20researchers%2C%20AI/Robotic%0Aresearchers%2C%20and%20software%20developers%2C%20and%20it%20continues%20to%20evolve%20to%20meet%20the%0Adiverse%20requirements%20of%20these%20stakeholders.%20TOM%20facilitates%20the%20creation%20of%0Aintelligent%20assistive%20AR%20applications%20for%20daily%20activities%20and%20supports%20the%0Arecording%20and%20analysis%20of%20user%20interactions%2C%20integration%20of%20new%20devices%2C%20and%0Athe%20provision%20of%20assistance%20for%20various%20activities.%20Additionally%2C%20we%20showcase%0Aseveral%20proof-of-concept%20assistive%20services%20and%20discuss%20the%20challenges%20involved%0Ain%20developing%20such%20services.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTOM%253A%2520A%2520Development%2520Platform%2520For%2520Wearable%2520Intelligent%2520Assistants%26entry.906535625%3DNuwan%2520Janaka%2520and%2520Shengdong%2520Zhao%2520and%2520David%2520Hsu%2520and%2520Sherisse%2520Tan%2520Jing%2520Wen%2520and%2520Koh%2520Chun%2520Keat%26entry.1292438233%3D%2520%2520Advanced%2520digital%2520assistants%2520can%2520significantly%2520enhance%2520task%2520performance%252C%250Areduce%2520user%2520burden%252C%2520and%2520provide%2520personalized%2520guidance%2520to%2520improve%2520users%2527%250Aabilities.%2520However%252C%2520the%2520development%2520of%2520such%2520intelligent%2520digital%2520assistants%250Apresents%2520a%2520formidable%2520challenge.%2520To%2520address%2520this%252C%2520we%2520introduce%2520TOM%252C%2520a%250Aconceptual%2520architecture%2520and%2520software%2520platform%2520%2528https%253A//github.com/TOM-Platform%2529%250Adesigned%2520to%2520support%2520the%2520development%2520of%2520intelligent%2520wearable%2520assistants%2520that%2520are%250Acontextually%2520aware%2520of%2520both%2520the%2520user%2520and%2520the%2520environment.%2520This%2520system%2520was%250Adeveloped%2520collaboratively%2520with%2520AR/MR%2520researchers%252C%2520HCI%2520researchers%252C%2520AI/Robotic%250Aresearchers%252C%2520and%2520software%2520developers%252C%2520and%2520it%2520continues%2520to%2520evolve%2520to%2520meet%2520the%250Adiverse%2520requirements%2520of%2520these%2520stakeholders.%2520TOM%2520facilitates%2520the%2520creation%2520of%250Aintelligent%2520assistive%2520AR%2520applications%2520for%2520daily%2520activities%2520and%2520supports%2520the%250Arecording%2520and%2520analysis%2520of%2520user%2520interactions%252C%2520integration%2520of%2520new%2520devices%252C%2520and%250Athe%2520provision%2520of%2520assistance%2520for%2520various%2520activities.%2520Additionally%252C%2520we%2520showcase%250Aseveral%2520proof-of-concept%2520assistive%2520services%2520and%2520discuss%2520the%2520challenges%2520involved%250Ain%2520developing%2520such%2520services.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TOM%3A%20A%20Development%20Platform%20For%20Wearable%20Intelligent%20Assistants&entry.906535625=Nuwan%20Janaka%20and%20Shengdong%20Zhao%20and%20David%20Hsu%20and%20Sherisse%20Tan%20Jing%20Wen%20and%20Koh%20Chun%20Keat&entry.1292438233=%20%20Advanced%20digital%20assistants%20can%20significantly%20enhance%20task%20performance%2C%0Areduce%20user%20burden%2C%20and%20provide%20personalized%20guidance%20to%20improve%20users%27%0Aabilities.%20However%2C%20the%20development%20of%20such%20intelligent%20digital%20assistants%0Apresents%20a%20formidable%20challenge.%20To%20address%20this%2C%20we%20introduce%20TOM%2C%20a%0Aconceptual%20architecture%20and%20software%20platform%20%28https%3A//github.com/TOM-Platform%29%0Adesigned%20to%20support%20the%20development%20of%20intelligent%20wearable%20assistants%20that%20are%0Acontextually%20aware%20of%20both%20the%20user%20and%20the%20environment.%20This%20system%20was%0Adeveloped%20collaboratively%20with%20AR/MR%20researchers%2C%20HCI%20researchers%2C%20AI/Robotic%0Aresearchers%2C%20and%20software%20developers%2C%20and%20it%20continues%20to%20evolve%20to%20meet%20the%0Adiverse%20requirements%20of%20these%20stakeholders.%20TOM%20facilitates%20the%20creation%20of%0Aintelligent%20assistive%20AR%20applications%20for%20daily%20activities%20and%20supports%20the%0Arecording%20and%20analysis%20of%20user%20interactions%2C%20integration%20of%20new%20devices%2C%20and%0Athe%20provision%20of%20assistance%20for%20various%20activities.%20Additionally%2C%20we%20showcase%0Aseveral%20proof-of-concept%20assistive%20services%20and%20discuss%20the%20challenges%20involved%0Ain%20developing%20such%20services.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15523v1&entry.124074799=Read"},
{"title": "The Rlign Algorithm for Enhanced Electrocardiogram Analysis through\n  R-Peak Alignment for Explainable Classification and Clustering", "author": "Lucas Plagwitz and Lucas Bickmann and Michael Fujarski and Alexander Brenner and Warnes Gobalakrishnan and Lars Eckardt and Antonius B\u00fcscher and Julian Varghese", "abstract": "  Electrocardiogram (ECG) recordings have long been vital in diagnosing\ndifferent cardiac conditions. Recently, research in the field of automatic ECG\nprocessing using machine learning methods has gained importance, mainly by\nutilizing deep learning methods on raw ECG signals. A major advantage of models\nlike convolutional neural networks (CNNs) is their ability to effectively\nprocess biomedical imaging or signal data. However, this strength is tempered\nby challenges related to their lack of explainability, the need for a large\namount of training data, and the complexities involved in adapting them for\nunsupervised clustering tasks. In addressing these tasks, we aim to reintroduce\nshallow learning techniques, including support vector machines and principal\ncomponents analysis, into ECG signal processing by leveraging their\nsemi-structured, cyclic form. To this end, we developed and evaluated a\ntransformation that effectively restructures ECG signals into a fully\nstructured format, facilitating their subsequent analysis using shallow\nlearning algorithms. In this study, we present this adaptive transformative\napproach that aligns R-peaks across all signals in a dataset and resamples the\nsegments between R-peaks, both with and without heart rate dependencies. We\nillustrate the substantial benefit of this transformation for traditional\nanalysis techniques in the areas of classification, clustering, and\nexplainability, outperforming commercial software for median beat\ntransformation and CNN approaches. Our approach demonstrates a significant\nadvantage for shallow machine learning methods over CNNs, especially when\ndealing with limited training data. Additionally, we release a fully tested and\npublicly accessible code framework, providing a robust alignment pipeline to\nsupport future research, available at https://github.com/ imi-ms/rlign.\n", "link": "http://arxiv.org/abs/2407.15555v1", "date": "2024-07-22", "relevancy": 2.4413, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4891}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4888}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Rlign%20Algorithm%20for%20Enhanced%20Electrocardiogram%20Analysis%20through%0A%20%20R-Peak%20Alignment%20for%20Explainable%20Classification%20and%20Clustering&body=Title%3A%20The%20Rlign%20Algorithm%20for%20Enhanced%20Electrocardiogram%20Analysis%20through%0A%20%20R-Peak%20Alignment%20for%20Explainable%20Classification%20and%20Clustering%0AAuthor%3A%20Lucas%20Plagwitz%20and%20Lucas%20Bickmann%20and%20Michael%20Fujarski%20and%20Alexander%20Brenner%20and%20Warnes%20Gobalakrishnan%20and%20Lars%20Eckardt%20and%20Antonius%20B%C3%BCscher%20and%20Julian%20Varghese%0AAbstract%3A%20%20%20Electrocardiogram%20%28ECG%29%20recordings%20have%20long%20been%20vital%20in%20diagnosing%0Adifferent%20cardiac%20conditions.%20Recently%2C%20research%20in%20the%20field%20of%20automatic%20ECG%0Aprocessing%20using%20machine%20learning%20methods%20has%20gained%20importance%2C%20mainly%20by%0Autilizing%20deep%20learning%20methods%20on%20raw%20ECG%20signals.%20A%20major%20advantage%20of%20models%0Alike%20convolutional%20neural%20networks%20%28CNNs%29%20is%20their%20ability%20to%20effectively%0Aprocess%20biomedical%20imaging%20or%20signal%20data.%20However%2C%20this%20strength%20is%20tempered%0Aby%20challenges%20related%20to%20their%20lack%20of%20explainability%2C%20the%20need%20for%20a%20large%0Aamount%20of%20training%20data%2C%20and%20the%20complexities%20involved%20in%20adapting%20them%20for%0Aunsupervised%20clustering%20tasks.%20In%20addressing%20these%20tasks%2C%20we%20aim%20to%20reintroduce%0Ashallow%20learning%20techniques%2C%20including%20support%20vector%20machines%20and%20principal%0Acomponents%20analysis%2C%20into%20ECG%20signal%20processing%20by%20leveraging%20their%0Asemi-structured%2C%20cyclic%20form.%20To%20this%20end%2C%20we%20developed%20and%20evaluated%20a%0Atransformation%20that%20effectively%20restructures%20ECG%20signals%20into%20a%20fully%0Astructured%20format%2C%20facilitating%20their%20subsequent%20analysis%20using%20shallow%0Alearning%20algorithms.%20In%20this%20study%2C%20we%20present%20this%20adaptive%20transformative%0Aapproach%20that%20aligns%20R-peaks%20across%20all%20signals%20in%20a%20dataset%20and%20resamples%20the%0Asegments%20between%20R-peaks%2C%20both%20with%20and%20without%20heart%20rate%20dependencies.%20We%0Aillustrate%20the%20substantial%20benefit%20of%20this%20transformation%20for%20traditional%0Aanalysis%20techniques%20in%20the%20areas%20of%20classification%2C%20clustering%2C%20and%0Aexplainability%2C%20outperforming%20commercial%20software%20for%20median%20beat%0Atransformation%20and%20CNN%20approaches.%20Our%20approach%20demonstrates%20a%20significant%0Aadvantage%20for%20shallow%20machine%20learning%20methods%20over%20CNNs%2C%20especially%20when%0Adealing%20with%20limited%20training%20data.%20Additionally%2C%20we%20release%20a%20fully%20tested%20and%0Apublicly%20accessible%20code%20framework%2C%20providing%20a%20robust%20alignment%20pipeline%20to%0Asupport%20future%20research%2C%20available%20at%20https%3A//github.com/%20imi-ms/rlign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Rlign%2520Algorithm%2520for%2520Enhanced%2520Electrocardiogram%2520Analysis%2520through%250A%2520%2520R-Peak%2520Alignment%2520for%2520Explainable%2520Classification%2520and%2520Clustering%26entry.906535625%3DLucas%2520Plagwitz%2520and%2520Lucas%2520Bickmann%2520and%2520Michael%2520Fujarski%2520and%2520Alexander%2520Brenner%2520and%2520Warnes%2520Gobalakrishnan%2520and%2520Lars%2520Eckardt%2520and%2520Antonius%2520B%25C3%25BCscher%2520and%2520Julian%2520Varghese%26entry.1292438233%3D%2520%2520Electrocardiogram%2520%2528ECG%2529%2520recordings%2520have%2520long%2520been%2520vital%2520in%2520diagnosing%250Adifferent%2520cardiac%2520conditions.%2520Recently%252C%2520research%2520in%2520the%2520field%2520of%2520automatic%2520ECG%250Aprocessing%2520using%2520machine%2520learning%2520methods%2520has%2520gained%2520importance%252C%2520mainly%2520by%250Autilizing%2520deep%2520learning%2520methods%2520on%2520raw%2520ECG%2520signals.%2520A%2520major%2520advantage%2520of%2520models%250Alike%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520is%2520their%2520ability%2520to%2520effectively%250Aprocess%2520biomedical%2520imaging%2520or%2520signal%2520data.%2520However%252C%2520this%2520strength%2520is%2520tempered%250Aby%2520challenges%2520related%2520to%2520their%2520lack%2520of%2520explainability%252C%2520the%2520need%2520for%2520a%2520large%250Aamount%2520of%2520training%2520data%252C%2520and%2520the%2520complexities%2520involved%2520in%2520adapting%2520them%2520for%250Aunsupervised%2520clustering%2520tasks.%2520In%2520addressing%2520these%2520tasks%252C%2520we%2520aim%2520to%2520reintroduce%250Ashallow%2520learning%2520techniques%252C%2520including%2520support%2520vector%2520machines%2520and%2520principal%250Acomponents%2520analysis%252C%2520into%2520ECG%2520signal%2520processing%2520by%2520leveraging%2520their%250Asemi-structured%252C%2520cyclic%2520form.%2520To%2520this%2520end%252C%2520we%2520developed%2520and%2520evaluated%2520a%250Atransformation%2520that%2520effectively%2520restructures%2520ECG%2520signals%2520into%2520a%2520fully%250Astructured%2520format%252C%2520facilitating%2520their%2520subsequent%2520analysis%2520using%2520shallow%250Alearning%2520algorithms.%2520In%2520this%2520study%252C%2520we%2520present%2520this%2520adaptive%2520transformative%250Aapproach%2520that%2520aligns%2520R-peaks%2520across%2520all%2520signals%2520in%2520a%2520dataset%2520and%2520resamples%2520the%250Asegments%2520between%2520R-peaks%252C%2520both%2520with%2520and%2520without%2520heart%2520rate%2520dependencies.%2520We%250Aillustrate%2520the%2520substantial%2520benefit%2520of%2520this%2520transformation%2520for%2520traditional%250Aanalysis%2520techniques%2520in%2520the%2520areas%2520of%2520classification%252C%2520clustering%252C%2520and%250Aexplainability%252C%2520outperforming%2520commercial%2520software%2520for%2520median%2520beat%250Atransformation%2520and%2520CNN%2520approaches.%2520Our%2520approach%2520demonstrates%2520a%2520significant%250Aadvantage%2520for%2520shallow%2520machine%2520learning%2520methods%2520over%2520CNNs%252C%2520especially%2520when%250Adealing%2520with%2520limited%2520training%2520data.%2520Additionally%252C%2520we%2520release%2520a%2520fully%2520tested%2520and%250Apublicly%2520accessible%2520code%2520framework%252C%2520providing%2520a%2520robust%2520alignment%2520pipeline%2520to%250Asupport%2520future%2520research%252C%2520available%2520at%2520https%253A//github.com/%2520imi-ms/rlign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Rlign%20Algorithm%20for%20Enhanced%20Electrocardiogram%20Analysis%20through%0A%20%20R-Peak%20Alignment%20for%20Explainable%20Classification%20and%20Clustering&entry.906535625=Lucas%20Plagwitz%20and%20Lucas%20Bickmann%20and%20Michael%20Fujarski%20and%20Alexander%20Brenner%20and%20Warnes%20Gobalakrishnan%20and%20Lars%20Eckardt%20and%20Antonius%20B%C3%BCscher%20and%20Julian%20Varghese&entry.1292438233=%20%20Electrocardiogram%20%28ECG%29%20recordings%20have%20long%20been%20vital%20in%20diagnosing%0Adifferent%20cardiac%20conditions.%20Recently%2C%20research%20in%20the%20field%20of%20automatic%20ECG%0Aprocessing%20using%20machine%20learning%20methods%20has%20gained%20importance%2C%20mainly%20by%0Autilizing%20deep%20learning%20methods%20on%20raw%20ECG%20signals.%20A%20major%20advantage%20of%20models%0Alike%20convolutional%20neural%20networks%20%28CNNs%29%20is%20their%20ability%20to%20effectively%0Aprocess%20biomedical%20imaging%20or%20signal%20data.%20However%2C%20this%20strength%20is%20tempered%0Aby%20challenges%20related%20to%20their%20lack%20of%20explainability%2C%20the%20need%20for%20a%20large%0Aamount%20of%20training%20data%2C%20and%20the%20complexities%20involved%20in%20adapting%20them%20for%0Aunsupervised%20clustering%20tasks.%20In%20addressing%20these%20tasks%2C%20we%20aim%20to%20reintroduce%0Ashallow%20learning%20techniques%2C%20including%20support%20vector%20machines%20and%20principal%0Acomponents%20analysis%2C%20into%20ECG%20signal%20processing%20by%20leveraging%20their%0Asemi-structured%2C%20cyclic%20form.%20To%20this%20end%2C%20we%20developed%20and%20evaluated%20a%0Atransformation%20that%20effectively%20restructures%20ECG%20signals%20into%20a%20fully%0Astructured%20format%2C%20facilitating%20their%20subsequent%20analysis%20using%20shallow%0Alearning%20algorithms.%20In%20this%20study%2C%20we%20present%20this%20adaptive%20transformative%0Aapproach%20that%20aligns%20R-peaks%20across%20all%20signals%20in%20a%20dataset%20and%20resamples%20the%0Asegments%20between%20R-peaks%2C%20both%20with%20and%20without%20heart%20rate%20dependencies.%20We%0Aillustrate%20the%20substantial%20benefit%20of%20this%20transformation%20for%20traditional%0Aanalysis%20techniques%20in%20the%20areas%20of%20classification%2C%20clustering%2C%20and%0Aexplainability%2C%20outperforming%20commercial%20software%20for%20median%20beat%0Atransformation%20and%20CNN%20approaches.%20Our%20approach%20demonstrates%20a%20significant%0Aadvantage%20for%20shallow%20machine%20learning%20methods%20over%20CNNs%2C%20especially%20when%0Adealing%20with%20limited%20training%20data.%20Additionally%2C%20we%20release%20a%20fully%20tested%20and%0Apublicly%20accessible%20code%20framework%2C%20providing%20a%20robust%20alignment%20pipeline%20to%0Asupport%20future%20research%2C%20available%20at%20https%3A//github.com/%20imi-ms/rlign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15555v1&entry.124074799=Read"},
{"title": "Graph Condensation: A Survey", "author": "Xinyi Gao and Junliang Yu and Tong Chen and Guanhua Ye and Wentao Zhang and Hongzhi Yin", "abstract": "  The rapid growth of graph data poses significant challenges in storage,\ntransmission, and particularly the training of graph neural networks (GNNs). To\naddress these challenges, graph condensation (GC) has emerged as an innovative\nsolution. GC focuses on synthesizing a compact yet highly representative graph,\nenabling GNNs trained on it to achieve performance comparable to those trained\non the original large graph. The notable efficacy of GC and its broad prospects\nhave garnered significant attention and spurred extensive research. This survey\npaper provides an up-to-date and systematic overview of GC, organizing existing\nresearch into five categories aligned with critical GC evaluation criteria:\neffectiveness, generalization, efficiency, fairness, and robustness. To\nfacilitate an in-depth and comprehensive understanding of GC, this paper\nexamines various methods under each category and thoroughly discusses two\nessential components within GC: optimization strategies and condensed graph\ngeneration. We also empirically compare and analyze representative GC methods\nwith diverse optimization strategies based on the five proposed GC evaluation\ncriteria. Finally, we explore the applications of GC in various fields, outline\nthe related open-source libraries, and highlight the present challenges and\nnovel insights, with the aim of promoting advancements in future research. The\nrelated resources can be found at\nhttps://github.com/XYGaoG/Graph-Condensation-Papers.\n", "link": "http://arxiv.org/abs/2401.11720v2", "date": "2024-07-22", "relevancy": 2.3944, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4795}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4788}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Condensation%3A%20A%20Survey&body=Title%3A%20Graph%20Condensation%3A%20A%20Survey%0AAuthor%3A%20Xinyi%20Gao%20and%20Junliang%20Yu%20and%20Tong%20Chen%20and%20Guanhua%20Ye%20and%20Wentao%20Zhang%20and%20Hongzhi%20Yin%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20graph%20data%20poses%20significant%20challenges%20in%20storage%2C%0Atransmission%2C%20and%20particularly%20the%20training%20of%20graph%20neural%20networks%20%28GNNs%29.%20To%0Aaddress%20these%20challenges%2C%20graph%20condensation%20%28GC%29%20has%20emerged%20as%20an%20innovative%0Asolution.%20GC%20focuses%20on%20synthesizing%20a%20compact%20yet%20highly%20representative%20graph%2C%0Aenabling%20GNNs%20trained%20on%20it%20to%20achieve%20performance%20comparable%20to%20those%20trained%0Aon%20the%20original%20large%20graph.%20The%20notable%20efficacy%20of%20GC%20and%20its%20broad%20prospects%0Ahave%20garnered%20significant%20attention%20and%20spurred%20extensive%20research.%20This%20survey%0Apaper%20provides%20an%20up-to-date%20and%20systematic%20overview%20of%20GC%2C%20organizing%20existing%0Aresearch%20into%20five%20categories%20aligned%20with%20critical%20GC%20evaluation%20criteria%3A%0Aeffectiveness%2C%20generalization%2C%20efficiency%2C%20fairness%2C%20and%20robustness.%20To%0Afacilitate%20an%20in-depth%20and%20comprehensive%20understanding%20of%20GC%2C%20this%20paper%0Aexamines%20various%20methods%20under%20each%20category%20and%20thoroughly%20discusses%20two%0Aessential%20components%20within%20GC%3A%20optimization%20strategies%20and%20condensed%20graph%0Ageneration.%20We%20also%20empirically%20compare%20and%20analyze%20representative%20GC%20methods%0Awith%20diverse%20optimization%20strategies%20based%20on%20the%20five%20proposed%20GC%20evaluation%0Acriteria.%20Finally%2C%20we%20explore%20the%20applications%20of%20GC%20in%20various%20fields%2C%20outline%0Athe%20related%20open-source%20libraries%2C%20and%20highlight%20the%20present%20challenges%20and%0Anovel%20insights%2C%20with%20the%20aim%20of%20promoting%20advancements%20in%20future%20research.%20The%0Arelated%20resources%20can%20be%20found%20at%0Ahttps%3A//github.com/XYGaoG/Graph-Condensation-Papers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11720v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Condensation%253A%2520A%2520Survey%26entry.906535625%3DXinyi%2520Gao%2520and%2520Junliang%2520Yu%2520and%2520Tong%2520Chen%2520and%2520Guanhua%2520Ye%2520and%2520Wentao%2520Zhang%2520and%2520Hongzhi%2520Yin%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520graph%2520data%2520poses%2520significant%2520challenges%2520in%2520storage%252C%250Atransmission%252C%2520and%2520particularly%2520the%2520training%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520To%250Aaddress%2520these%2520challenges%252C%2520graph%2520condensation%2520%2528GC%2529%2520has%2520emerged%2520as%2520an%2520innovative%250Asolution.%2520GC%2520focuses%2520on%2520synthesizing%2520a%2520compact%2520yet%2520highly%2520representative%2520graph%252C%250Aenabling%2520GNNs%2520trained%2520on%2520it%2520to%2520achieve%2520performance%2520comparable%2520to%2520those%2520trained%250Aon%2520the%2520original%2520large%2520graph.%2520The%2520notable%2520efficacy%2520of%2520GC%2520and%2520its%2520broad%2520prospects%250Ahave%2520garnered%2520significant%2520attention%2520and%2520spurred%2520extensive%2520research.%2520This%2520survey%250Apaper%2520provides%2520an%2520up-to-date%2520and%2520systematic%2520overview%2520of%2520GC%252C%2520organizing%2520existing%250Aresearch%2520into%2520five%2520categories%2520aligned%2520with%2520critical%2520GC%2520evaluation%2520criteria%253A%250Aeffectiveness%252C%2520generalization%252C%2520efficiency%252C%2520fairness%252C%2520and%2520robustness.%2520To%250Afacilitate%2520an%2520in-depth%2520and%2520comprehensive%2520understanding%2520of%2520GC%252C%2520this%2520paper%250Aexamines%2520various%2520methods%2520under%2520each%2520category%2520and%2520thoroughly%2520discusses%2520two%250Aessential%2520components%2520within%2520GC%253A%2520optimization%2520strategies%2520and%2520condensed%2520graph%250Ageneration.%2520We%2520also%2520empirically%2520compare%2520and%2520analyze%2520representative%2520GC%2520methods%250Awith%2520diverse%2520optimization%2520strategies%2520based%2520on%2520the%2520five%2520proposed%2520GC%2520evaluation%250Acriteria.%2520Finally%252C%2520we%2520explore%2520the%2520applications%2520of%2520GC%2520in%2520various%2520fields%252C%2520outline%250Athe%2520related%2520open-source%2520libraries%252C%2520and%2520highlight%2520the%2520present%2520challenges%2520and%250Anovel%2520insights%252C%2520with%2520the%2520aim%2520of%2520promoting%2520advancements%2520in%2520future%2520research.%2520The%250Arelated%2520resources%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/XYGaoG/Graph-Condensation-Papers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11720v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Condensation%3A%20A%20Survey&entry.906535625=Xinyi%20Gao%20and%20Junliang%20Yu%20and%20Tong%20Chen%20and%20Guanhua%20Ye%20and%20Wentao%20Zhang%20and%20Hongzhi%20Yin&entry.1292438233=%20%20The%20rapid%20growth%20of%20graph%20data%20poses%20significant%20challenges%20in%20storage%2C%0Atransmission%2C%20and%20particularly%20the%20training%20of%20graph%20neural%20networks%20%28GNNs%29.%20To%0Aaddress%20these%20challenges%2C%20graph%20condensation%20%28GC%29%20has%20emerged%20as%20an%20innovative%0Asolution.%20GC%20focuses%20on%20synthesizing%20a%20compact%20yet%20highly%20representative%20graph%2C%0Aenabling%20GNNs%20trained%20on%20it%20to%20achieve%20performance%20comparable%20to%20those%20trained%0Aon%20the%20original%20large%20graph.%20The%20notable%20efficacy%20of%20GC%20and%20its%20broad%20prospects%0Ahave%20garnered%20significant%20attention%20and%20spurred%20extensive%20research.%20This%20survey%0Apaper%20provides%20an%20up-to-date%20and%20systematic%20overview%20of%20GC%2C%20organizing%20existing%0Aresearch%20into%20five%20categories%20aligned%20with%20critical%20GC%20evaluation%20criteria%3A%0Aeffectiveness%2C%20generalization%2C%20efficiency%2C%20fairness%2C%20and%20robustness.%20To%0Afacilitate%20an%20in-depth%20and%20comprehensive%20understanding%20of%20GC%2C%20this%20paper%0Aexamines%20various%20methods%20under%20each%20category%20and%20thoroughly%20discusses%20two%0Aessential%20components%20within%20GC%3A%20optimization%20strategies%20and%20condensed%20graph%0Ageneration.%20We%20also%20empirically%20compare%20and%20analyze%20representative%20GC%20methods%0Awith%20diverse%20optimization%20strategies%20based%20on%20the%20five%20proposed%20GC%20evaluation%0Acriteria.%20Finally%2C%20we%20explore%20the%20applications%20of%20GC%20in%20various%20fields%2C%20outline%0Athe%20related%20open-source%20libraries%2C%20and%20highlight%20the%20present%20challenges%20and%0Anovel%20insights%2C%20with%20the%20aim%20of%20promoting%20advancements%20in%20future%20research.%20The%0Arelated%20resources%20can%20be%20found%20at%0Ahttps%3A//github.com/XYGaoG/Graph-Condensation-Papers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11720v2&entry.124074799=Read"},
{"title": "Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using\n  Highly Noisy Data", "author": "Yike Zhang and Dingjie Su and Eduardo Davalos and Jack H. Noble", "abstract": "  Cochlear Implant (CI) procedures involve inserting an array of electrodes\ninto the cochlea located inside the inner ear. Mastoidectomy is a surgical\nprocedure that uses a high-speed drill to remove part of the mastoid region of\nthe temporal bone, providing safe access to the cochlea through the middle and\ninner ear. We aim to develop an intraoperative navigation system that registers\nplans created using 3D preoperative Computerized Tomography (CT) volumes with\nthe 2D surgical microscope view. Herein, we propose a method to synthesize the\nmastoidectomy volume using only the preoperative CT scan, where the mastoid is\nintact. We introduce an unsupervised learning framework designed to synthesize\nmastoidectomy. For model training purposes, this method uses postoperative CT\nscans to avoid manual data cleaning or labeling, even when the region removed\nduring mastoidectomy is visible but affected by metal artifacts, low\nsignal-to-noise ratio, or electrode wiring. Our approach estimates\nmastoidectomy regions with a mean dice score of 70.0%. This approach represents\na major step forward for CI intraoperative navigation by predicting realistic\nmastoidectomy-removed regions in preoperative planning that can be used to\nregister the pre-surgery plan to intraoperative microscopy.\n", "link": "http://arxiv.org/abs/2407.15787v1", "date": "2024-07-22", "relevancy": 2.3843, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4772}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4772}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Mastoidectomy%20for%20Cochlear%20CT%20Mesh%20Reconstruction%20Using%0A%20%20Highly%20Noisy%20Data&body=Title%3A%20Unsupervised%20Mastoidectomy%20for%20Cochlear%20CT%20Mesh%20Reconstruction%20Using%0A%20%20Highly%20Noisy%20Data%0AAuthor%3A%20Yike%20Zhang%20and%20Dingjie%20Su%20and%20Eduardo%20Davalos%20and%20Jack%20H.%20Noble%0AAbstract%3A%20%20%20Cochlear%20Implant%20%28CI%29%20procedures%20involve%20inserting%20an%20array%20of%20electrodes%0Ainto%20the%20cochlea%20located%20inside%20the%20inner%20ear.%20Mastoidectomy%20is%20a%20surgical%0Aprocedure%20that%20uses%20a%20high-speed%20drill%20to%20remove%20part%20of%20the%20mastoid%20region%20of%0Athe%20temporal%20bone%2C%20providing%20safe%20access%20to%20the%20cochlea%20through%20the%20middle%20and%0Ainner%20ear.%20We%20aim%20to%20develop%20an%20intraoperative%20navigation%20system%20that%20registers%0Aplans%20created%20using%203D%20preoperative%20Computerized%20Tomography%20%28CT%29%20volumes%20with%0Athe%202D%20surgical%20microscope%20view.%20Herein%2C%20we%20propose%20a%20method%20to%20synthesize%20the%0Amastoidectomy%20volume%20using%20only%20the%20preoperative%20CT%20scan%2C%20where%20the%20mastoid%20is%0Aintact.%20We%20introduce%20an%20unsupervised%20learning%20framework%20designed%20to%20synthesize%0Amastoidectomy.%20For%20model%20training%20purposes%2C%20this%20method%20uses%20postoperative%20CT%0Ascans%20to%20avoid%20manual%20data%20cleaning%20or%20labeling%2C%20even%20when%20the%20region%20removed%0Aduring%20mastoidectomy%20is%20visible%20but%20affected%20by%20metal%20artifacts%2C%20low%0Asignal-to-noise%20ratio%2C%20or%20electrode%20wiring.%20Our%20approach%20estimates%0Amastoidectomy%20regions%20with%20a%20mean%20dice%20score%20of%2070.0%25.%20This%20approach%20represents%0Aa%20major%20step%20forward%20for%20CI%20intraoperative%20navigation%20by%20predicting%20realistic%0Amastoidectomy-removed%20regions%20in%20preoperative%20planning%20that%20can%20be%20used%20to%0Aregister%20the%20pre-surgery%20plan%20to%20intraoperative%20microscopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Mastoidectomy%2520for%2520Cochlear%2520CT%2520Mesh%2520Reconstruction%2520Using%250A%2520%2520Highly%2520Noisy%2520Data%26entry.906535625%3DYike%2520Zhang%2520and%2520Dingjie%2520Su%2520and%2520Eduardo%2520Davalos%2520and%2520Jack%2520H.%2520Noble%26entry.1292438233%3D%2520%2520Cochlear%2520Implant%2520%2528CI%2529%2520procedures%2520involve%2520inserting%2520an%2520array%2520of%2520electrodes%250Ainto%2520the%2520cochlea%2520located%2520inside%2520the%2520inner%2520ear.%2520Mastoidectomy%2520is%2520a%2520surgical%250Aprocedure%2520that%2520uses%2520a%2520high-speed%2520drill%2520to%2520remove%2520part%2520of%2520the%2520mastoid%2520region%2520of%250Athe%2520temporal%2520bone%252C%2520providing%2520safe%2520access%2520to%2520the%2520cochlea%2520through%2520the%2520middle%2520and%250Ainner%2520ear.%2520We%2520aim%2520to%2520develop%2520an%2520intraoperative%2520navigation%2520system%2520that%2520registers%250Aplans%2520created%2520using%25203D%2520preoperative%2520Computerized%2520Tomography%2520%2528CT%2529%2520volumes%2520with%250Athe%25202D%2520surgical%2520microscope%2520view.%2520Herein%252C%2520we%2520propose%2520a%2520method%2520to%2520synthesize%2520the%250Amastoidectomy%2520volume%2520using%2520only%2520the%2520preoperative%2520CT%2520scan%252C%2520where%2520the%2520mastoid%2520is%250Aintact.%2520We%2520introduce%2520an%2520unsupervised%2520learning%2520framework%2520designed%2520to%2520synthesize%250Amastoidectomy.%2520For%2520model%2520training%2520purposes%252C%2520this%2520method%2520uses%2520postoperative%2520CT%250Ascans%2520to%2520avoid%2520manual%2520data%2520cleaning%2520or%2520labeling%252C%2520even%2520when%2520the%2520region%2520removed%250Aduring%2520mastoidectomy%2520is%2520visible%2520but%2520affected%2520by%2520metal%2520artifacts%252C%2520low%250Asignal-to-noise%2520ratio%252C%2520or%2520electrode%2520wiring.%2520Our%2520approach%2520estimates%250Amastoidectomy%2520regions%2520with%2520a%2520mean%2520dice%2520score%2520of%252070.0%2525.%2520This%2520approach%2520represents%250Aa%2520major%2520step%2520forward%2520for%2520CI%2520intraoperative%2520navigation%2520by%2520predicting%2520realistic%250Amastoidectomy-removed%2520regions%2520in%2520preoperative%2520planning%2520that%2520can%2520be%2520used%2520to%250Aregister%2520the%2520pre-surgery%2520plan%2520to%2520intraoperative%2520microscopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Mastoidectomy%20for%20Cochlear%20CT%20Mesh%20Reconstruction%20Using%0A%20%20Highly%20Noisy%20Data&entry.906535625=Yike%20Zhang%20and%20Dingjie%20Su%20and%20Eduardo%20Davalos%20and%20Jack%20H.%20Noble&entry.1292438233=%20%20Cochlear%20Implant%20%28CI%29%20procedures%20involve%20inserting%20an%20array%20of%20electrodes%0Ainto%20the%20cochlea%20located%20inside%20the%20inner%20ear.%20Mastoidectomy%20is%20a%20surgical%0Aprocedure%20that%20uses%20a%20high-speed%20drill%20to%20remove%20part%20of%20the%20mastoid%20region%20of%0Athe%20temporal%20bone%2C%20providing%20safe%20access%20to%20the%20cochlea%20through%20the%20middle%20and%0Ainner%20ear.%20We%20aim%20to%20develop%20an%20intraoperative%20navigation%20system%20that%20registers%0Aplans%20created%20using%203D%20preoperative%20Computerized%20Tomography%20%28CT%29%20volumes%20with%0Athe%202D%20surgical%20microscope%20view.%20Herein%2C%20we%20propose%20a%20method%20to%20synthesize%20the%0Amastoidectomy%20volume%20using%20only%20the%20preoperative%20CT%20scan%2C%20where%20the%20mastoid%20is%0Aintact.%20We%20introduce%20an%20unsupervised%20learning%20framework%20designed%20to%20synthesize%0Amastoidectomy.%20For%20model%20training%20purposes%2C%20this%20method%20uses%20postoperative%20CT%0Ascans%20to%20avoid%20manual%20data%20cleaning%20or%20labeling%2C%20even%20when%20the%20region%20removed%0Aduring%20mastoidectomy%20is%20visible%20but%20affected%20by%20metal%20artifacts%2C%20low%0Asignal-to-noise%20ratio%2C%20or%20electrode%20wiring.%20Our%20approach%20estimates%0Amastoidectomy%20regions%20with%20a%20mean%20dice%20score%20of%2070.0%25.%20This%20approach%20represents%0Aa%20major%20step%20forward%20for%20CI%20intraoperative%20navigation%20by%20predicting%20realistic%0Amastoidectomy-removed%20regions%20in%20preoperative%20planning%20that%20can%20be%20used%20to%0Aregister%20the%20pre-surgery%20plan%20to%20intraoperative%20microscopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15787v1&entry.124074799=Read"},
{"title": "Large-scale Time-Varying Portfolio Optimisation using Graph Attention\n  Networks", "author": "Kamesh Korangi and Christophe Mues and Cristi\u00e1n Bravo", "abstract": "  Apart from assessing individual asset performance, investors in financial\nmarkets also need to consider how a set of firms performs collectively as a\nportfolio. Whereas traditional Markowitz-based mean-variance portfolios are\nwidespread, network-based optimisation techniques have built upon these\ndevelopments. However, most studies do not contain firms at risk of default and\nremove any firms that drop off indices over a certain time. This is the first\nstudy to incorporate risky firms and use all the firms in portfolio\noptimisation. We propose and empirically test a novel method that leverages\nGraph Attention networks (GATs), a subclass of Graph Neural Networks (GNNs).\nGNNs, as deep learning-based models, can exploit network data to uncover\nnonlinear relationships. Their ability to handle high-dimensional features and\naccommodate customised layers for specific purposes makes them particularly\nappealing for large-scale problems such as mid- and small-cap portfolio\noptimization. This study utilises 30 years of data on mid-cap firms, creating\ngraphs of firms using distance correlation and the Triangulated Maximally\nFiltered Graph approach. These graphs are the inputs to a GAT model that we\ntrain using custom layers which impose weight and allocation constraints and a\nloss function derived from the Sharpe ratio, thus directly maximising portfolio\nrisk-adjusted returns. This new model is benchmarked against a network\ncharacteristic-based portfolio, a mean variance-based portfolio, and an\nequal-weighted portfolio. The results show that the portfolio produced by the\nGAT-based model outperforms all benchmarks and is consistently superior to\nother strategies over a long period while also being informative of market\ndynamics.\n", "link": "http://arxiv.org/abs/2407.15532v1", "date": "2024-07-22", "relevancy": 2.3581, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5058}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4637}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20Time-Varying%20Portfolio%20Optimisation%20using%20Graph%20Attention%0A%20%20Networks&body=Title%3A%20Large-scale%20Time-Varying%20Portfolio%20Optimisation%20using%20Graph%20Attention%0A%20%20Networks%0AAuthor%3A%20Kamesh%20Korangi%20and%20Christophe%20Mues%20and%20Cristi%C3%A1n%20Bravo%0AAbstract%3A%20%20%20Apart%20from%20assessing%20individual%20asset%20performance%2C%20investors%20in%20financial%0Amarkets%20also%20need%20to%20consider%20how%20a%20set%20of%20firms%20performs%20collectively%20as%20a%0Aportfolio.%20Whereas%20traditional%20Markowitz-based%20mean-variance%20portfolios%20are%0Awidespread%2C%20network-based%20optimisation%20techniques%20have%20built%20upon%20these%0Adevelopments.%20However%2C%20most%20studies%20do%20not%20contain%20firms%20at%20risk%20of%20default%20and%0Aremove%20any%20firms%20that%20drop%20off%20indices%20over%20a%20certain%20time.%20This%20is%20the%20first%0Astudy%20to%20incorporate%20risky%20firms%20and%20use%20all%20the%20firms%20in%20portfolio%0Aoptimisation.%20We%20propose%20and%20empirically%20test%20a%20novel%20method%20that%20leverages%0AGraph%20Attention%20networks%20%28GATs%29%2C%20a%20subclass%20of%20Graph%20Neural%20Networks%20%28GNNs%29.%0AGNNs%2C%20as%20deep%20learning-based%20models%2C%20can%20exploit%20network%20data%20to%20uncover%0Anonlinear%20relationships.%20Their%20ability%20to%20handle%20high-dimensional%20features%20and%0Aaccommodate%20customised%20layers%20for%20specific%20purposes%20makes%20them%20particularly%0Aappealing%20for%20large-scale%20problems%20such%20as%20mid-%20and%20small-cap%20portfolio%0Aoptimization.%20This%20study%20utilises%2030%20years%20of%20data%20on%20mid-cap%20firms%2C%20creating%0Agraphs%20of%20firms%20using%20distance%20correlation%20and%20the%20Triangulated%20Maximally%0AFiltered%20Graph%20approach.%20These%20graphs%20are%20the%20inputs%20to%20a%20GAT%20model%20that%20we%0Atrain%20using%20custom%20layers%20which%20impose%20weight%20and%20allocation%20constraints%20and%20a%0Aloss%20function%20derived%20from%20the%20Sharpe%20ratio%2C%20thus%20directly%20maximising%20portfolio%0Arisk-adjusted%20returns.%20This%20new%20model%20is%20benchmarked%20against%20a%20network%0Acharacteristic-based%20portfolio%2C%20a%20mean%20variance-based%20portfolio%2C%20and%20an%0Aequal-weighted%20portfolio.%20The%20results%20show%20that%20the%20portfolio%20produced%20by%20the%0AGAT-based%20model%20outperforms%20all%20benchmarks%20and%20is%20consistently%20superior%20to%0Aother%20strategies%20over%20a%20long%20period%20while%20also%20being%20informative%20of%20market%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520Time-Varying%2520Portfolio%2520Optimisation%2520using%2520Graph%2520Attention%250A%2520%2520Networks%26entry.906535625%3DKamesh%2520Korangi%2520and%2520Christophe%2520Mues%2520and%2520Cristi%25C3%25A1n%2520Bravo%26entry.1292438233%3D%2520%2520Apart%2520from%2520assessing%2520individual%2520asset%2520performance%252C%2520investors%2520in%2520financial%250Amarkets%2520also%2520need%2520to%2520consider%2520how%2520a%2520set%2520of%2520firms%2520performs%2520collectively%2520as%2520a%250Aportfolio.%2520Whereas%2520traditional%2520Markowitz-based%2520mean-variance%2520portfolios%2520are%250Awidespread%252C%2520network-based%2520optimisation%2520techniques%2520have%2520built%2520upon%2520these%250Adevelopments.%2520However%252C%2520most%2520studies%2520do%2520not%2520contain%2520firms%2520at%2520risk%2520of%2520default%2520and%250Aremove%2520any%2520firms%2520that%2520drop%2520off%2520indices%2520over%2520a%2520certain%2520time.%2520This%2520is%2520the%2520first%250Astudy%2520to%2520incorporate%2520risky%2520firms%2520and%2520use%2520all%2520the%2520firms%2520in%2520portfolio%250Aoptimisation.%2520We%2520propose%2520and%2520empirically%2520test%2520a%2520novel%2520method%2520that%2520leverages%250AGraph%2520Attention%2520networks%2520%2528GATs%2529%252C%2520a%2520subclass%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%250AGNNs%252C%2520as%2520deep%2520learning-based%2520models%252C%2520can%2520exploit%2520network%2520data%2520to%2520uncover%250Anonlinear%2520relationships.%2520Their%2520ability%2520to%2520handle%2520high-dimensional%2520features%2520and%250Aaccommodate%2520customised%2520layers%2520for%2520specific%2520purposes%2520makes%2520them%2520particularly%250Aappealing%2520for%2520large-scale%2520problems%2520such%2520as%2520mid-%2520and%2520small-cap%2520portfolio%250Aoptimization.%2520This%2520study%2520utilises%252030%2520years%2520of%2520data%2520on%2520mid-cap%2520firms%252C%2520creating%250Agraphs%2520of%2520firms%2520using%2520distance%2520correlation%2520and%2520the%2520Triangulated%2520Maximally%250AFiltered%2520Graph%2520approach.%2520These%2520graphs%2520are%2520the%2520inputs%2520to%2520a%2520GAT%2520model%2520that%2520we%250Atrain%2520using%2520custom%2520layers%2520which%2520impose%2520weight%2520and%2520allocation%2520constraints%2520and%2520a%250Aloss%2520function%2520derived%2520from%2520the%2520Sharpe%2520ratio%252C%2520thus%2520directly%2520maximising%2520portfolio%250Arisk-adjusted%2520returns.%2520This%2520new%2520model%2520is%2520benchmarked%2520against%2520a%2520network%250Acharacteristic-based%2520portfolio%252C%2520a%2520mean%2520variance-based%2520portfolio%252C%2520and%2520an%250Aequal-weighted%2520portfolio.%2520The%2520results%2520show%2520that%2520the%2520portfolio%2520produced%2520by%2520the%250AGAT-based%2520model%2520outperforms%2520all%2520benchmarks%2520and%2520is%2520consistently%2520superior%2520to%250Aother%2520strategies%2520over%2520a%2520long%2520period%2520while%2520also%2520being%2520informative%2520of%2520market%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20Time-Varying%20Portfolio%20Optimisation%20using%20Graph%20Attention%0A%20%20Networks&entry.906535625=Kamesh%20Korangi%20and%20Christophe%20Mues%20and%20Cristi%C3%A1n%20Bravo&entry.1292438233=%20%20Apart%20from%20assessing%20individual%20asset%20performance%2C%20investors%20in%20financial%0Amarkets%20also%20need%20to%20consider%20how%20a%20set%20of%20firms%20performs%20collectively%20as%20a%0Aportfolio.%20Whereas%20traditional%20Markowitz-based%20mean-variance%20portfolios%20are%0Awidespread%2C%20network-based%20optimisation%20techniques%20have%20built%20upon%20these%0Adevelopments.%20However%2C%20most%20studies%20do%20not%20contain%20firms%20at%20risk%20of%20default%20and%0Aremove%20any%20firms%20that%20drop%20off%20indices%20over%20a%20certain%20time.%20This%20is%20the%20first%0Astudy%20to%20incorporate%20risky%20firms%20and%20use%20all%20the%20firms%20in%20portfolio%0Aoptimisation.%20We%20propose%20and%20empirically%20test%20a%20novel%20method%20that%20leverages%0AGraph%20Attention%20networks%20%28GATs%29%2C%20a%20subclass%20of%20Graph%20Neural%20Networks%20%28GNNs%29.%0AGNNs%2C%20as%20deep%20learning-based%20models%2C%20can%20exploit%20network%20data%20to%20uncover%0Anonlinear%20relationships.%20Their%20ability%20to%20handle%20high-dimensional%20features%20and%0Aaccommodate%20customised%20layers%20for%20specific%20purposes%20makes%20them%20particularly%0Aappealing%20for%20large-scale%20problems%20such%20as%20mid-%20and%20small-cap%20portfolio%0Aoptimization.%20This%20study%20utilises%2030%20years%20of%20data%20on%20mid-cap%20firms%2C%20creating%0Agraphs%20of%20firms%20using%20distance%20correlation%20and%20the%20Triangulated%20Maximally%0AFiltered%20Graph%20approach.%20These%20graphs%20are%20the%20inputs%20to%20a%20GAT%20model%20that%20we%0Atrain%20using%20custom%20layers%20which%20impose%20weight%20and%20allocation%20constraints%20and%20a%0Aloss%20function%20derived%20from%20the%20Sharpe%20ratio%2C%20thus%20directly%20maximising%20portfolio%0Arisk-adjusted%20returns.%20This%20new%20model%20is%20benchmarked%20against%20a%20network%0Acharacteristic-based%20portfolio%2C%20a%20mean%20variance-based%20portfolio%2C%20and%20an%0Aequal-weighted%20portfolio.%20The%20results%20show%20that%20the%20portfolio%20produced%20by%20the%0AGAT-based%20model%20outperforms%20all%20benchmarks%20and%20is%20consistently%20superior%20to%0Aother%20strategies%20over%20a%20long%20period%20while%20also%20being%20informative%20of%20market%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15532v1&entry.124074799=Read"},
{"title": "Fast and Effective Weight Update for Pruned Large Language Models", "author": "Vladim\u00edr Bo\u017ea", "abstract": "  Pruning large language models (LLMs) is a challenging task due to their\nenormous size. The primary difficulty is fine-tuning the model after pruning,\nwhich is needed to recover the lost performance caused by dropping weights.\nRecent approaches have either ignored fine-tuning entirely, focusing on\nefficient pruning criteria, or attempted layer-wise weight updates, preserving\nthe behavior of each layer. However, even layer-wise weight updates can be\ncostly for LLMs, and previous works have resorted to various approximations.\n  In our paper, we propose a fast and effective weight update algorithm for\npruned layers based on the Alternating Direction Method of Multipliers (ADMM).\nWe further extend it with a simple gradual pruning mask selection and achieve\nstate-of-the-art pruning performance across a wide range of LLMs. Code is\navailable at https://github.com/fmfi-compbio/admm-pruning.\n", "link": "http://arxiv.org/abs/2401.02938v2", "date": "2024-07-22", "relevancy": 2.3555, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4851}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Effective%20Weight%20Update%20for%20Pruned%20Large%20Language%20Models&body=Title%3A%20Fast%20and%20Effective%20Weight%20Update%20for%20Pruned%20Large%20Language%20Models%0AAuthor%3A%20Vladim%C3%ADr%20Bo%C5%BEa%0AAbstract%3A%20%20%20Pruning%20large%20language%20models%20%28LLMs%29%20is%20a%20challenging%20task%20due%20to%20their%0Aenormous%20size.%20The%20primary%20difficulty%20is%20fine-tuning%20the%20model%20after%20pruning%2C%0Awhich%20is%20needed%20to%20recover%20the%20lost%20performance%20caused%20by%20dropping%20weights.%0ARecent%20approaches%20have%20either%20ignored%20fine-tuning%20entirely%2C%20focusing%20on%0Aefficient%20pruning%20criteria%2C%20or%20attempted%20layer-wise%20weight%20updates%2C%20preserving%0Athe%20behavior%20of%20each%20layer.%20However%2C%20even%20layer-wise%20weight%20updates%20can%20be%0Acostly%20for%20LLMs%2C%20and%20previous%20works%20have%20resorted%20to%20various%20approximations.%0A%20%20In%20our%20paper%2C%20we%20propose%20a%20fast%20and%20effective%20weight%20update%20algorithm%20for%0Apruned%20layers%20based%20on%20the%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29.%0AWe%20further%20extend%20it%20with%20a%20simple%20gradual%20pruning%20mask%20selection%20and%20achieve%0Astate-of-the-art%20pruning%20performance%20across%20a%20wide%20range%20of%20LLMs.%20Code%20is%0Aavailable%20at%20https%3A//github.com/fmfi-compbio/admm-pruning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02938v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Effective%2520Weight%2520Update%2520for%2520Pruned%2520Large%2520Language%2520Models%26entry.906535625%3DVladim%25C3%25ADr%2520Bo%25C5%25BEa%26entry.1292438233%3D%2520%2520Pruning%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520a%2520challenging%2520task%2520due%2520to%2520their%250Aenormous%2520size.%2520The%2520primary%2520difficulty%2520is%2520fine-tuning%2520the%2520model%2520after%2520pruning%252C%250Awhich%2520is%2520needed%2520to%2520recover%2520the%2520lost%2520performance%2520caused%2520by%2520dropping%2520weights.%250ARecent%2520approaches%2520have%2520either%2520ignored%2520fine-tuning%2520entirely%252C%2520focusing%2520on%250Aefficient%2520pruning%2520criteria%252C%2520or%2520attempted%2520layer-wise%2520weight%2520updates%252C%2520preserving%250Athe%2520behavior%2520of%2520each%2520layer.%2520However%252C%2520even%2520layer-wise%2520weight%2520updates%2520can%2520be%250Acostly%2520for%2520LLMs%252C%2520and%2520previous%2520works%2520have%2520resorted%2520to%2520various%2520approximations.%250A%2520%2520In%2520our%2520paper%252C%2520we%2520propose%2520a%2520fast%2520and%2520effective%2520weight%2520update%2520algorithm%2520for%250Apruned%2520layers%2520based%2520on%2520the%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%2520%2528ADMM%2529.%250AWe%2520further%2520extend%2520it%2520with%2520a%2520simple%2520gradual%2520pruning%2520mask%2520selection%2520and%2520achieve%250Astate-of-the-art%2520pruning%2520performance%2520across%2520a%2520wide%2520range%2520of%2520LLMs.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/fmfi-compbio/admm-pruning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02938v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Effective%20Weight%20Update%20for%20Pruned%20Large%20Language%20Models&entry.906535625=Vladim%C3%ADr%20Bo%C5%BEa&entry.1292438233=%20%20Pruning%20large%20language%20models%20%28LLMs%29%20is%20a%20challenging%20task%20due%20to%20their%0Aenormous%20size.%20The%20primary%20difficulty%20is%20fine-tuning%20the%20model%20after%20pruning%2C%0Awhich%20is%20needed%20to%20recover%20the%20lost%20performance%20caused%20by%20dropping%20weights.%0ARecent%20approaches%20have%20either%20ignored%20fine-tuning%20entirely%2C%20focusing%20on%0Aefficient%20pruning%20criteria%2C%20or%20attempted%20layer-wise%20weight%20updates%2C%20preserving%0Athe%20behavior%20of%20each%20layer.%20However%2C%20even%20layer-wise%20weight%20updates%20can%20be%0Acostly%20for%20LLMs%2C%20and%20previous%20works%20have%20resorted%20to%20various%20approximations.%0A%20%20In%20our%20paper%2C%20we%20propose%20a%20fast%20and%20effective%20weight%20update%20algorithm%20for%0Apruned%20layers%20based%20on%20the%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29.%0AWe%20further%20extend%20it%20with%20a%20simple%20gradual%20pruning%20mask%20selection%20and%20achieve%0Astate-of-the-art%20pruning%20performance%20across%20a%20wide%20range%20of%20LLMs.%20Code%20is%0Aavailable%20at%20https%3A//github.com/fmfi-compbio/admm-pruning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02938v2&entry.124074799=Read"},
{"title": "Robotic Gas Source Localization with Probabilistic Mapping and Online\n  Dispersion Simulation", "author": "Pepe Ojeda and Javier Monroy and Javier Gonzalez-Jimenez", "abstract": "  Gas source localization (GSL) with an autonomous robot is a problem with many\nprospective applications, from finding pipe leaks to emergency-response\nscenarios. In this work, we present a new method to perform GSL in realistic\nindoor environments, featuring obstacles and turbulent flow. Given the highly\ncomplex relationship between the source position and the measurements available\nto the robot (the single-point gas concentration, and the wind vector) we\npropose an observation model that derives from contrasting the online,\nreal-time simulation of the gas dispersion from any candidate source\nlocalization against a gas concentration map built from sensor readings. To\naccount for a convenient and grounded integration of both into a probabilistic\nestimation framework, we introduce the concept of probabilistic gas-hit maps,\nwhich provide a higher level of abstraction to model the time-dependent nature\nof gas dispersion. Results from both simulated and real experiments show the\ncapabilities of our current proposal to deal with source localization in\ncomplex indoor environments.\n", "link": "http://arxiv.org/abs/2304.08879v3", "date": "2024-07-22", "relevancy": 2.354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6067}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5949}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20Gas%20Source%20Localization%20with%20Probabilistic%20Mapping%20and%20Online%0A%20%20Dispersion%20Simulation&body=Title%3A%20Robotic%20Gas%20Source%20Localization%20with%20Probabilistic%20Mapping%20and%20Online%0A%20%20Dispersion%20Simulation%0AAuthor%3A%20Pepe%20Ojeda%20and%20Javier%20Monroy%20and%20Javier%20Gonzalez-Jimenez%0AAbstract%3A%20%20%20Gas%20source%20localization%20%28GSL%29%20with%20an%20autonomous%20robot%20is%20a%20problem%20with%20many%0Aprospective%20applications%2C%20from%20finding%20pipe%20leaks%20to%20emergency-response%0Ascenarios.%20In%20this%20work%2C%20we%20present%20a%20new%20method%20to%20perform%20GSL%20in%20realistic%0Aindoor%20environments%2C%20featuring%20obstacles%20and%20turbulent%20flow.%20Given%20the%20highly%0Acomplex%20relationship%20between%20the%20source%20position%20and%20the%20measurements%20available%0Ato%20the%20robot%20%28the%20single-point%20gas%20concentration%2C%20and%20the%20wind%20vector%29%20we%0Apropose%20an%20observation%20model%20that%20derives%20from%20contrasting%20the%20online%2C%0Areal-time%20simulation%20of%20the%20gas%20dispersion%20from%20any%20candidate%20source%0Alocalization%20against%20a%20gas%20concentration%20map%20built%20from%20sensor%20readings.%20To%0Aaccount%20for%20a%20convenient%20and%20grounded%20integration%20of%20both%20into%20a%20probabilistic%0Aestimation%20framework%2C%20we%20introduce%20the%20concept%20of%20probabilistic%20gas-hit%20maps%2C%0Awhich%20provide%20a%20higher%20level%20of%20abstraction%20to%20model%20the%20time-dependent%20nature%0Aof%20gas%20dispersion.%20Results%20from%20both%20simulated%20and%20real%20experiments%20show%20the%0Acapabilities%20of%20our%20current%20proposal%20to%20deal%20with%20source%20localization%20in%0Acomplex%20indoor%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.08879v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520Gas%2520Source%2520Localization%2520with%2520Probabilistic%2520Mapping%2520and%2520Online%250A%2520%2520Dispersion%2520Simulation%26entry.906535625%3DPepe%2520Ojeda%2520and%2520Javier%2520Monroy%2520and%2520Javier%2520Gonzalez-Jimenez%26entry.1292438233%3D%2520%2520Gas%2520source%2520localization%2520%2528GSL%2529%2520with%2520an%2520autonomous%2520robot%2520is%2520a%2520problem%2520with%2520many%250Aprospective%2520applications%252C%2520from%2520finding%2520pipe%2520leaks%2520to%2520emergency-response%250Ascenarios.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%2520method%2520to%2520perform%2520GSL%2520in%2520realistic%250Aindoor%2520environments%252C%2520featuring%2520obstacles%2520and%2520turbulent%2520flow.%2520Given%2520the%2520highly%250Acomplex%2520relationship%2520between%2520the%2520source%2520position%2520and%2520the%2520measurements%2520available%250Ato%2520the%2520robot%2520%2528the%2520single-point%2520gas%2520concentration%252C%2520and%2520the%2520wind%2520vector%2529%2520we%250Apropose%2520an%2520observation%2520model%2520that%2520derives%2520from%2520contrasting%2520the%2520online%252C%250Areal-time%2520simulation%2520of%2520the%2520gas%2520dispersion%2520from%2520any%2520candidate%2520source%250Alocalization%2520against%2520a%2520gas%2520concentration%2520map%2520built%2520from%2520sensor%2520readings.%2520To%250Aaccount%2520for%2520a%2520convenient%2520and%2520grounded%2520integration%2520of%2520both%2520into%2520a%2520probabilistic%250Aestimation%2520framework%252C%2520we%2520introduce%2520the%2520concept%2520of%2520probabilistic%2520gas-hit%2520maps%252C%250Awhich%2520provide%2520a%2520higher%2520level%2520of%2520abstraction%2520to%2520model%2520the%2520time-dependent%2520nature%250Aof%2520gas%2520dispersion.%2520Results%2520from%2520both%2520simulated%2520and%2520real%2520experiments%2520show%2520the%250Acapabilities%2520of%2520our%2520current%2520proposal%2520to%2520deal%2520with%2520source%2520localization%2520in%250Acomplex%2520indoor%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.08879v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20Gas%20Source%20Localization%20with%20Probabilistic%20Mapping%20and%20Online%0A%20%20Dispersion%20Simulation&entry.906535625=Pepe%20Ojeda%20and%20Javier%20Monroy%20and%20Javier%20Gonzalez-Jimenez&entry.1292438233=%20%20Gas%20source%20localization%20%28GSL%29%20with%20an%20autonomous%20robot%20is%20a%20problem%20with%20many%0Aprospective%20applications%2C%20from%20finding%20pipe%20leaks%20to%20emergency-response%0Ascenarios.%20In%20this%20work%2C%20we%20present%20a%20new%20method%20to%20perform%20GSL%20in%20realistic%0Aindoor%20environments%2C%20featuring%20obstacles%20and%20turbulent%20flow.%20Given%20the%20highly%0Acomplex%20relationship%20between%20the%20source%20position%20and%20the%20measurements%20available%0Ato%20the%20robot%20%28the%20single-point%20gas%20concentration%2C%20and%20the%20wind%20vector%29%20we%0Apropose%20an%20observation%20model%20that%20derives%20from%20contrasting%20the%20online%2C%0Areal-time%20simulation%20of%20the%20gas%20dispersion%20from%20any%20candidate%20source%0Alocalization%20against%20a%20gas%20concentration%20map%20built%20from%20sensor%20readings.%20To%0Aaccount%20for%20a%20convenient%20and%20grounded%20integration%20of%20both%20into%20a%20probabilistic%0Aestimation%20framework%2C%20we%20introduce%20the%20concept%20of%20probabilistic%20gas-hit%20maps%2C%0Awhich%20provide%20a%20higher%20level%20of%20abstraction%20to%20model%20the%20time-dependent%20nature%0Aof%20gas%20dispersion.%20Results%20from%20both%20simulated%20and%20real%20experiments%20show%20the%0Acapabilities%20of%20our%20current%20proposal%20to%20deal%20with%20source%20localization%20in%0Acomplex%20indoor%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.08879v3&entry.124074799=Read"},
{"title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts", "author": "Mikayel Samvelyan and Sharath Chandra Raparthy and Andrei Lupu and Eric Hambro and Aram H. Markosyan and Manish Bhatt and Yuning Mao and Minqi Jiang and Jack Parker-Holder and Jakob Foerster and Tim Rockt\u00e4schel and Roberta Raileanu", "abstract": "  As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem, and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nfine-tuning models with synthetic data generated by the Rainbow Teaming method\nsignificantly enhances their safety without sacrificing general performance or\nhelpfulness. We additionally explore the versatility of Rainbow Teaming by\napplying it to question answering and cybersecurity, showcasing its potential\nto drive robust open-ended self-improvement in a wide range of applications.\n", "link": "http://arxiv.org/abs/2402.16822v2", "date": "2024-07-22", "relevancy": 2.3405, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4688}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rainbow%20Teaming%3A%20Open-Ended%20Generation%20of%20Diverse%20Adversarial%20Prompts&body=Title%3A%20Rainbow%20Teaming%3A%20Open-Ended%20Generation%20of%20Diverse%20Adversarial%20Prompts%0AAuthor%3A%20Mikayel%20Samvelyan%20and%20Sharath%20Chandra%20Raparthy%20and%20Andrei%20Lupu%20and%20Eric%20Hambro%20and%20Aram%20H.%20Markosyan%20and%20Manish%20Bhatt%20and%20Yuning%20Mao%20and%20Minqi%20Jiang%20and%20Jack%20Parker-Holder%20and%20Jakob%20Foerster%20and%20Tim%20Rockt%C3%A4schel%20and%20Roberta%20Raileanu%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20become%20increasingly%20prevalent%20across%20many%0Areal-world%20applications%2C%20understanding%20and%20enhancing%20their%20robustness%20to%0Aadversarial%20attacks%20is%20of%20paramount%20importance.%20Existing%20methods%20for%0Aidentifying%20adversarial%20prompts%20tend%20to%20focus%20on%20specific%20domains%2C%20lack%0Adiversity%2C%20or%20require%20extensive%20human%20annotations.%20To%20address%20these%0Alimitations%2C%20we%20present%20Rainbow%20Teaming%2C%20a%20novel%20black-box%20approach%20for%0Aproducing%20a%20diverse%20collection%20of%20adversarial%20prompts.%20Rainbow%20Teaming%20casts%0Aadversarial%20prompt%20generation%20as%20a%20quality-diversity%20problem%2C%20and%20uses%0Aopen-ended%20search%20to%20generate%20prompts%20that%20are%20both%20effective%20and%20diverse.%0AFocusing%20on%20the%20safety%20domain%2C%20we%20use%20Rainbow%20Teaming%20to%20target%20various%0Astate-of-the-art%20LLMs%2C%20including%20the%20Llama%202%20and%20Llama%203%20models.%20Our%20approach%0Areveals%20hundreds%20of%20effective%20adversarial%20prompts%2C%20with%20an%20attack%20success%20rate%0Aexceeding%2090%25%20across%20all%20tested%20models.%20Furthermore%2C%20we%20demonstrate%20that%0Afine-tuning%20models%20with%20synthetic%20data%20generated%20by%20the%20Rainbow%20Teaming%20method%0Asignificantly%20enhances%20their%20safety%20without%20sacrificing%20general%20performance%20or%0Ahelpfulness.%20We%20additionally%20explore%20the%20versatility%20of%20Rainbow%20Teaming%20by%0Aapplying%20it%20to%20question%20answering%20and%20cybersecurity%2C%20showcasing%20its%20potential%0Ato%20drive%20robust%20open-ended%20self-improvement%20in%20a%20wide%20range%20of%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRainbow%2520Teaming%253A%2520Open-Ended%2520Generation%2520of%2520Diverse%2520Adversarial%2520Prompts%26entry.906535625%3DMikayel%2520Samvelyan%2520and%2520Sharath%2520Chandra%2520Raparthy%2520and%2520Andrei%2520Lupu%2520and%2520Eric%2520Hambro%2520and%2520Aram%2520H.%2520Markosyan%2520and%2520Manish%2520Bhatt%2520and%2520Yuning%2520Mao%2520and%2520Minqi%2520Jiang%2520and%2520Jack%2520Parker-Holder%2520and%2520Jakob%2520Foerster%2520and%2520Tim%2520Rockt%25C3%25A4schel%2520and%2520Roberta%2520Raileanu%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%2520increasingly%2520prevalent%2520across%2520many%250Areal-world%2520applications%252C%2520understanding%2520and%2520enhancing%2520their%2520robustness%2520to%250Aadversarial%2520attacks%2520is%2520of%2520paramount%2520importance.%2520Existing%2520methods%2520for%250Aidentifying%2520adversarial%2520prompts%2520tend%2520to%2520focus%2520on%2520specific%2520domains%252C%2520lack%250Adiversity%252C%2520or%2520require%2520extensive%2520human%2520annotations.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520present%2520Rainbow%2520Teaming%252C%2520a%2520novel%2520black-box%2520approach%2520for%250Aproducing%2520a%2520diverse%2520collection%2520of%2520adversarial%2520prompts.%2520Rainbow%2520Teaming%2520casts%250Aadversarial%2520prompt%2520generation%2520as%2520a%2520quality-diversity%2520problem%252C%2520and%2520uses%250Aopen-ended%2520search%2520to%2520generate%2520prompts%2520that%2520are%2520both%2520effective%2520and%2520diverse.%250AFocusing%2520on%2520the%2520safety%2520domain%252C%2520we%2520use%2520Rainbow%2520Teaming%2520to%2520target%2520various%250Astate-of-the-art%2520LLMs%252C%2520including%2520the%2520Llama%25202%2520and%2520Llama%25203%2520models.%2520Our%2520approach%250Areveals%2520hundreds%2520of%2520effective%2520adversarial%2520prompts%252C%2520with%2520an%2520attack%2520success%2520rate%250Aexceeding%252090%2525%2520across%2520all%2520tested%2520models.%2520Furthermore%252C%2520we%2520demonstrate%2520that%250Afine-tuning%2520models%2520with%2520synthetic%2520data%2520generated%2520by%2520the%2520Rainbow%2520Teaming%2520method%250Asignificantly%2520enhances%2520their%2520safety%2520without%2520sacrificing%2520general%2520performance%2520or%250Ahelpfulness.%2520We%2520additionally%2520explore%2520the%2520versatility%2520of%2520Rainbow%2520Teaming%2520by%250Aapplying%2520it%2520to%2520question%2520answering%2520and%2520cybersecurity%252C%2520showcasing%2520its%2520potential%250Ato%2520drive%2520robust%2520open-ended%2520self-improvement%2520in%2520a%2520wide%2520range%2520of%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rainbow%20Teaming%3A%20Open-Ended%20Generation%20of%20Diverse%20Adversarial%20Prompts&entry.906535625=Mikayel%20Samvelyan%20and%20Sharath%20Chandra%20Raparthy%20and%20Andrei%20Lupu%20and%20Eric%20Hambro%20and%20Aram%20H.%20Markosyan%20and%20Manish%20Bhatt%20and%20Yuning%20Mao%20and%20Minqi%20Jiang%20and%20Jack%20Parker-Holder%20and%20Jakob%20Foerster%20and%20Tim%20Rockt%C3%A4schel%20and%20Roberta%20Raileanu&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20become%20increasingly%20prevalent%20across%20many%0Areal-world%20applications%2C%20understanding%20and%20enhancing%20their%20robustness%20to%0Aadversarial%20attacks%20is%20of%20paramount%20importance.%20Existing%20methods%20for%0Aidentifying%20adversarial%20prompts%20tend%20to%20focus%20on%20specific%20domains%2C%20lack%0Adiversity%2C%20or%20require%20extensive%20human%20annotations.%20To%20address%20these%0Alimitations%2C%20we%20present%20Rainbow%20Teaming%2C%20a%20novel%20black-box%20approach%20for%0Aproducing%20a%20diverse%20collection%20of%20adversarial%20prompts.%20Rainbow%20Teaming%20casts%0Aadversarial%20prompt%20generation%20as%20a%20quality-diversity%20problem%2C%20and%20uses%0Aopen-ended%20search%20to%20generate%20prompts%20that%20are%20both%20effective%20and%20diverse.%0AFocusing%20on%20the%20safety%20domain%2C%20we%20use%20Rainbow%20Teaming%20to%20target%20various%0Astate-of-the-art%20LLMs%2C%20including%20the%20Llama%202%20and%20Llama%203%20models.%20Our%20approach%0Areveals%20hundreds%20of%20effective%20adversarial%20prompts%2C%20with%20an%20attack%20success%20rate%0Aexceeding%2090%25%20across%20all%20tested%20models.%20Furthermore%2C%20we%20demonstrate%20that%0Afine-tuning%20models%20with%20synthetic%20data%20generated%20by%20the%20Rainbow%20Teaming%20method%0Asignificantly%20enhances%20their%20safety%20without%20sacrificing%20general%20performance%20or%0Ahelpfulness.%20We%20additionally%20explore%20the%20versatility%20of%20Rainbow%20Teaming%20by%0Aapplying%20it%20to%20question%20answering%20and%20cybersecurity%2C%20showcasing%20its%20potential%0Ato%20drive%20robust%20open-ended%20self-improvement%20in%20a%20wide%20range%20of%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16822v2&entry.124074799=Read"},
{"title": "Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition", "author": "Jinfu Liu and Chen Chen and Mengyuan Liu", "abstract": "  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n", "link": "http://arxiv.org/abs/2407.15706v1", "date": "2024-07-22", "relevancy": 2.34, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6207}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5714}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition&body=Title%3A%20Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition%0AAuthor%3A%20Jinfu%20Liu%20and%20Chen%20Chen%20and%20Mengyuan%20Liu%0AAbstract%3A%20%20%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20due%20to%0Athe%20utilization%20of%20concise%20and%20resilient%20skeletons.%20Nevertheless%2C%20the%20absence%0Aof%20detailed%20body%20information%20in%20skeletons%20restricts%20performance%2C%20while%20other%0Amultimodal%20methods%20require%20substantial%20inference%20resources%20and%20are%20inefficient%0Awhen%20using%20multimodal%20data%20during%20both%20training%20and%20inference%20stages.%20To%0Aaddress%20this%20and%20fully%20harness%20the%20complementary%20multimodal%20features%2C%20we%0Apropose%20a%20novel%20multi-modality%20co-learning%20%28MMCL%29%20framework%20by%20leveraging%20the%0Amultimodal%20large%20language%20models%20%28LLMs%29%20as%20auxiliary%20networks%20for%20efficient%0Askeleton-based%20action%20recognition%2C%20which%20engages%20in%20multi-modality%20co-learning%0Aduring%20the%20training%20stage%20and%20keeps%20efficiency%20by%20employing%20only%20concise%0Askeletons%20in%20inference.%20Our%20MMCL%20framework%20primarily%20consists%20of%20two%20modules.%0AFirst%2C%20the%20Feature%20Alignment%20Module%20%28FAM%29%20extracts%20rich%20RGB%20features%20from%20video%0Aframes%20and%20aligns%20them%20with%20global%20skeleton%20features%20via%20contrastive%20learning.%0ASecond%2C%20the%20Feature%20Refinement%20Module%20%28FRM%29%20uses%20RGB%20images%20with%20temporal%0Ainformation%20and%20text%20instruction%20to%20generate%20instructive%20features%20based%20on%20the%0Apowerful%20generalization%20of%20multimodal%20LLMs.%20These%20instructive%20text%20features%0Awill%20further%20refine%20the%20classification%20scores%20and%20the%20refined%20scores%20will%0Aenhance%20the%20model%27s%20robustness%20and%20generalization%20in%20a%20manner%20similar%20to%20soft%0Alabels.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20Northwestern-UCLA%0Abenchmarks%20consistently%20verify%20the%20effectiveness%20of%20our%20MMCL%2C%20which%20outperforms%0Athe%20existing%20skeleton-based%20action%20recognition%20methods.%20Meanwhile%2C%20experiments%0Aon%20UTD-MHAD%20and%20SYSU-Action%20datasets%20demonstrate%20the%20commendable%20generalization%0Aof%20our%20MMCL%20in%20zero-shot%20and%20domain-adaptive%20action%20recognition.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/liujf69/MMCL-Action.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modality%2520Co-Learning%2520for%2520Efficient%2520Skeleton-based%2520Action%250A%2520%2520Recognition%26entry.906535625%3DJinfu%2520Liu%2520and%2520Chen%2520Chen%2520and%2520Mengyuan%2520Liu%26entry.1292438233%3D%2520%2520Skeleton-based%2520action%2520recognition%2520has%2520garnered%2520significant%2520attention%2520due%2520to%250Athe%2520utilization%2520of%2520concise%2520and%2520resilient%2520skeletons.%2520Nevertheless%252C%2520the%2520absence%250Aof%2520detailed%2520body%2520information%2520in%2520skeletons%2520restricts%2520performance%252C%2520while%2520other%250Amultimodal%2520methods%2520require%2520substantial%2520inference%2520resources%2520and%2520are%2520inefficient%250Awhen%2520using%2520multimodal%2520data%2520during%2520both%2520training%2520and%2520inference%2520stages.%2520To%250Aaddress%2520this%2520and%2520fully%2520harness%2520the%2520complementary%2520multimodal%2520features%252C%2520we%250Apropose%2520a%2520novel%2520multi-modality%2520co-learning%2520%2528MMCL%2529%2520framework%2520by%2520leveraging%2520the%250Amultimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520auxiliary%2520networks%2520for%2520efficient%250Askeleton-based%2520action%2520recognition%252C%2520which%2520engages%2520in%2520multi-modality%2520co-learning%250Aduring%2520the%2520training%2520stage%2520and%2520keeps%2520efficiency%2520by%2520employing%2520only%2520concise%250Askeletons%2520in%2520inference.%2520Our%2520MMCL%2520framework%2520primarily%2520consists%2520of%2520two%2520modules.%250AFirst%252C%2520the%2520Feature%2520Alignment%2520Module%2520%2528FAM%2529%2520extracts%2520rich%2520RGB%2520features%2520from%2520video%250Aframes%2520and%2520aligns%2520them%2520with%2520global%2520skeleton%2520features%2520via%2520contrastive%2520learning.%250ASecond%252C%2520the%2520Feature%2520Refinement%2520Module%2520%2528FRM%2529%2520uses%2520RGB%2520images%2520with%2520temporal%250Ainformation%2520and%2520text%2520instruction%2520to%2520generate%2520instructive%2520features%2520based%2520on%2520the%250Apowerful%2520generalization%2520of%2520multimodal%2520LLMs.%2520These%2520instructive%2520text%2520features%250Awill%2520further%2520refine%2520the%2520classification%2520scores%2520and%2520the%2520refined%2520scores%2520will%250Aenhance%2520the%2520model%2527s%2520robustness%2520and%2520generalization%2520in%2520a%2520manner%2520similar%2520to%2520soft%250Alabels.%2520Extensive%2520experiments%2520on%2520NTU%2520RGB%252BD%252C%2520NTU%2520RGB%252BD%2520120%2520and%2520Northwestern-UCLA%250Abenchmarks%2520consistently%2520verify%2520the%2520effectiveness%2520of%2520our%2520MMCL%252C%2520which%2520outperforms%250Athe%2520existing%2520skeleton-based%2520action%2520recognition%2520methods.%2520Meanwhile%252C%2520experiments%250Aon%2520UTD-MHAD%2520and%2520SYSU-Action%2520datasets%2520demonstrate%2520the%2520commendable%2520generalization%250Aof%2520our%2520MMCL%2520in%2520zero-shot%2520and%2520domain-adaptive%2520action%2520recognition.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/liujf69/MMCL-Action.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition&entry.906535625=Jinfu%20Liu%20and%20Chen%20Chen%20and%20Mengyuan%20Liu&entry.1292438233=%20%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20due%20to%0Athe%20utilization%20of%20concise%20and%20resilient%20skeletons.%20Nevertheless%2C%20the%20absence%0Aof%20detailed%20body%20information%20in%20skeletons%20restricts%20performance%2C%20while%20other%0Amultimodal%20methods%20require%20substantial%20inference%20resources%20and%20are%20inefficient%0Awhen%20using%20multimodal%20data%20during%20both%20training%20and%20inference%20stages.%20To%0Aaddress%20this%20and%20fully%20harness%20the%20complementary%20multimodal%20features%2C%20we%0Apropose%20a%20novel%20multi-modality%20co-learning%20%28MMCL%29%20framework%20by%20leveraging%20the%0Amultimodal%20large%20language%20models%20%28LLMs%29%20as%20auxiliary%20networks%20for%20efficient%0Askeleton-based%20action%20recognition%2C%20which%20engages%20in%20multi-modality%20co-learning%0Aduring%20the%20training%20stage%20and%20keeps%20efficiency%20by%20employing%20only%20concise%0Askeletons%20in%20inference.%20Our%20MMCL%20framework%20primarily%20consists%20of%20two%20modules.%0AFirst%2C%20the%20Feature%20Alignment%20Module%20%28FAM%29%20extracts%20rich%20RGB%20features%20from%20video%0Aframes%20and%20aligns%20them%20with%20global%20skeleton%20features%20via%20contrastive%20learning.%0ASecond%2C%20the%20Feature%20Refinement%20Module%20%28FRM%29%20uses%20RGB%20images%20with%20temporal%0Ainformation%20and%20text%20instruction%20to%20generate%20instructive%20features%20based%20on%20the%0Apowerful%20generalization%20of%20multimodal%20LLMs.%20These%20instructive%20text%20features%0Awill%20further%20refine%20the%20classification%20scores%20and%20the%20refined%20scores%20will%0Aenhance%20the%20model%27s%20robustness%20and%20generalization%20in%20a%20manner%20similar%20to%20soft%0Alabels.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20Northwestern-UCLA%0Abenchmarks%20consistently%20verify%20the%20effectiveness%20of%20our%20MMCL%2C%20which%20outperforms%0Athe%20existing%20skeleton-based%20action%20recognition%20methods.%20Meanwhile%2C%20experiments%0Aon%20UTD-MHAD%20and%20SYSU-Action%20datasets%20demonstrate%20the%20commendable%20generalization%0Aof%20our%20MMCL%20in%20zero-shot%20and%20domain-adaptive%20action%20recognition.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/liujf69/MMCL-Action.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15706v1&entry.124074799=Read"},
{"title": "Reinforcement Learning Meets Visual Odometry", "author": "Nico Messikommer and Giovanni Cioffi and Mathias Gehrig and Davide Scaramuzza", "abstract": "  Visual Odometry (VO) is essential to downstream mobile robotics and\naugmented/virtual reality tasks. Despite recent advances, existing VO methods\nstill rely on heuristic design choices that require several weeks of\nhyperparameter tuning by human experts, hindering generalizability and\nrobustness. We address these challenges by reframing VO as a sequential\ndecision-making task and applying Reinforcement Learning (RL) to adapt the VO\nprocess dynamically. Our approach introduces a neural network, operating as an\nagent within the VO pipeline, to make decisions such as keyframe and grid-size\nselection based on real-time conditions. Our method minimizes reliance on\nheuristic choices using a reward function based on pose error, runtime, and\nother metrics to guide the system. Our RL framework treats the VO system and\nthe image sequence as an environment, with the agent receiving observations\nfrom keypoints, map statistics, and prior poses. Experimental results using\nclassical VO methods and public benchmarks demonstrate improvements in accuracy\nand robustness, validating the generalizability of our RL-enhanced VO approach\nto different scenarios. We believe this paradigm shift advances VO technology\nby eliminating the need for time-intensive parameter tuning of heuristics.\n", "link": "http://arxiv.org/abs/2407.15626v1", "date": "2024-07-22", "relevancy": 2.3093, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.608}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20Meets%20Visual%20Odometry&body=Title%3A%20Reinforcement%20Learning%20Meets%20Visual%20Odometry%0AAuthor%3A%20Nico%20Messikommer%20and%20Giovanni%20Cioffi%20and%20Mathias%20Gehrig%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Visual%20Odometry%20%28VO%29%20is%20essential%20to%20downstream%20mobile%20robotics%20and%0Aaugmented/virtual%20reality%20tasks.%20Despite%20recent%20advances%2C%20existing%20VO%20methods%0Astill%20rely%20on%20heuristic%20design%20choices%20that%20require%20several%20weeks%20of%0Ahyperparameter%20tuning%20by%20human%20experts%2C%20hindering%20generalizability%20and%0Arobustness.%20We%20address%20these%20challenges%20by%20reframing%20VO%20as%20a%20sequential%0Adecision-making%20task%20and%20applying%20Reinforcement%20Learning%20%28RL%29%20to%20adapt%20the%20VO%0Aprocess%20dynamically.%20Our%20approach%20introduces%20a%20neural%20network%2C%20operating%20as%20an%0Aagent%20within%20the%20VO%20pipeline%2C%20to%20make%20decisions%20such%20as%20keyframe%20and%20grid-size%0Aselection%20based%20on%20real-time%20conditions.%20Our%20method%20minimizes%20reliance%20on%0Aheuristic%20choices%20using%20a%20reward%20function%20based%20on%20pose%20error%2C%20runtime%2C%20and%0Aother%20metrics%20to%20guide%20the%20system.%20Our%20RL%20framework%20treats%20the%20VO%20system%20and%0Athe%20image%20sequence%20as%20an%20environment%2C%20with%20the%20agent%20receiving%20observations%0Afrom%20keypoints%2C%20map%20statistics%2C%20and%20prior%20poses.%20Experimental%20results%20using%0Aclassical%20VO%20methods%20and%20public%20benchmarks%20demonstrate%20improvements%20in%20accuracy%0Aand%20robustness%2C%20validating%20the%20generalizability%20of%20our%20RL-enhanced%20VO%20approach%0Ato%20different%20scenarios.%20We%20believe%20this%20paradigm%20shift%20advances%20VO%20technology%0Aby%20eliminating%20the%20need%20for%20time-intensive%20parameter%20tuning%20of%20heuristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520Meets%2520Visual%2520Odometry%26entry.906535625%3DNico%2520Messikommer%2520and%2520Giovanni%2520Cioffi%2520and%2520Mathias%2520Gehrig%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Visual%2520Odometry%2520%2528VO%2529%2520is%2520essential%2520to%2520downstream%2520mobile%2520robotics%2520and%250Aaugmented/virtual%2520reality%2520tasks.%2520Despite%2520recent%2520advances%252C%2520existing%2520VO%2520methods%250Astill%2520rely%2520on%2520heuristic%2520design%2520choices%2520that%2520require%2520several%2520weeks%2520of%250Ahyperparameter%2520tuning%2520by%2520human%2520experts%252C%2520hindering%2520generalizability%2520and%250Arobustness.%2520We%2520address%2520these%2520challenges%2520by%2520reframing%2520VO%2520as%2520a%2520sequential%250Adecision-making%2520task%2520and%2520applying%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520adapt%2520the%2520VO%250Aprocess%2520dynamically.%2520Our%2520approach%2520introduces%2520a%2520neural%2520network%252C%2520operating%2520as%2520an%250Aagent%2520within%2520the%2520VO%2520pipeline%252C%2520to%2520make%2520decisions%2520such%2520as%2520keyframe%2520and%2520grid-size%250Aselection%2520based%2520on%2520real-time%2520conditions.%2520Our%2520method%2520minimizes%2520reliance%2520on%250Aheuristic%2520choices%2520using%2520a%2520reward%2520function%2520based%2520on%2520pose%2520error%252C%2520runtime%252C%2520and%250Aother%2520metrics%2520to%2520guide%2520the%2520system.%2520Our%2520RL%2520framework%2520treats%2520the%2520VO%2520system%2520and%250Athe%2520image%2520sequence%2520as%2520an%2520environment%252C%2520with%2520the%2520agent%2520receiving%2520observations%250Afrom%2520keypoints%252C%2520map%2520statistics%252C%2520and%2520prior%2520poses.%2520Experimental%2520results%2520using%250Aclassical%2520VO%2520methods%2520and%2520public%2520benchmarks%2520demonstrate%2520improvements%2520in%2520accuracy%250Aand%2520robustness%252C%2520validating%2520the%2520generalizability%2520of%2520our%2520RL-enhanced%2520VO%2520approach%250Ato%2520different%2520scenarios.%2520We%2520believe%2520this%2520paradigm%2520shift%2520advances%2520VO%2520technology%250Aby%2520eliminating%2520the%2520need%2520for%2520time-intensive%2520parameter%2520tuning%2520of%2520heuristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20Meets%20Visual%20Odometry&entry.906535625=Nico%20Messikommer%20and%20Giovanni%20Cioffi%20and%20Mathias%20Gehrig%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Visual%20Odometry%20%28VO%29%20is%20essential%20to%20downstream%20mobile%20robotics%20and%0Aaugmented/virtual%20reality%20tasks.%20Despite%20recent%20advances%2C%20existing%20VO%20methods%0Astill%20rely%20on%20heuristic%20design%20choices%20that%20require%20several%20weeks%20of%0Ahyperparameter%20tuning%20by%20human%20experts%2C%20hindering%20generalizability%20and%0Arobustness.%20We%20address%20these%20challenges%20by%20reframing%20VO%20as%20a%20sequential%0Adecision-making%20task%20and%20applying%20Reinforcement%20Learning%20%28RL%29%20to%20adapt%20the%20VO%0Aprocess%20dynamically.%20Our%20approach%20introduces%20a%20neural%20network%2C%20operating%20as%20an%0Aagent%20within%20the%20VO%20pipeline%2C%20to%20make%20decisions%20such%20as%20keyframe%20and%20grid-size%0Aselection%20based%20on%20real-time%20conditions.%20Our%20method%20minimizes%20reliance%20on%0Aheuristic%20choices%20using%20a%20reward%20function%20based%20on%20pose%20error%2C%20runtime%2C%20and%0Aother%20metrics%20to%20guide%20the%20system.%20Our%20RL%20framework%20treats%20the%20VO%20system%20and%0Athe%20image%20sequence%20as%20an%20environment%2C%20with%20the%20agent%20receiving%20observations%0Afrom%20keypoints%2C%20map%20statistics%2C%20and%20prior%20poses.%20Experimental%20results%20using%0Aclassical%20VO%20methods%20and%20public%20benchmarks%20demonstrate%20improvements%20in%20accuracy%0Aand%20robustness%2C%20validating%20the%20generalizability%20of%20our%20RL-enhanced%20VO%20approach%0Ato%20different%20scenarios.%20We%20believe%20this%20paradigm%20shift%20advances%20VO%20technology%0Aby%20eliminating%20the%20need%20for%20time-intensive%20parameter%20tuning%20of%20heuristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15626v1&entry.124074799=Read"},
{"title": "LLMmap: Fingerprinting For Large Language Models", "author": "Dario Pasquini and Evgenios M. Kornaropoulos and Giuseppe Ateniese", "abstract": "  We introduce LLMmap, a first-generation fingerprinting attack targeted at\nLLM-integrated applications. LLMmap employs an active fingerprinting approach,\nsending carefully crafted queries to the application and analyzing the\nresponses to identify the specific LLM model in use. With as few as 8\ninteractions, LLMmap can accurately identify LLMs with over 95% accuracy. More\nimportantly, LLMmap is designed to be robust across different application\nlayers, allowing it to identify LLMs operating under various system prompts,\nstochastic sampling hyperparameters, and even complex generation frameworks\nsuch as RAG or Chain-of-Thought.\n", "link": "http://arxiv.org/abs/2407.15847v1", "date": "2024-07-22", "relevancy": 2.2897, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4744}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4522}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMmap%3A%20Fingerprinting%20For%20Large%20Language%20Models&body=Title%3A%20LLMmap%3A%20Fingerprinting%20For%20Large%20Language%20Models%0AAuthor%3A%20Dario%20Pasquini%20and%20Evgenios%20M.%20Kornaropoulos%20and%20Giuseppe%20Ateniese%0AAbstract%3A%20%20%20We%20introduce%20LLMmap%2C%20a%20first-generation%20fingerprinting%20attack%20targeted%20at%0ALLM-integrated%20applications.%20LLMmap%20employs%20an%20active%20fingerprinting%20approach%2C%0Asending%20carefully%20crafted%20queries%20to%20the%20application%20and%20analyzing%20the%0Aresponses%20to%20identify%20the%20specific%20LLM%20model%20in%20use.%20With%20as%20few%20as%208%0Ainteractions%2C%20LLMmap%20can%20accurately%20identify%20LLMs%20with%20over%2095%25%20accuracy.%20More%0Aimportantly%2C%20LLMmap%20is%20designed%20to%20be%20robust%20across%20different%20application%0Alayers%2C%20allowing%20it%20to%20identify%20LLMs%20operating%20under%20various%20system%20prompts%2C%0Astochastic%20sampling%20hyperparameters%2C%20and%20even%20complex%20generation%20frameworks%0Asuch%20as%20RAG%20or%20Chain-of-Thought.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMmap%253A%2520Fingerprinting%2520For%2520Large%2520Language%2520Models%26entry.906535625%3DDario%2520Pasquini%2520and%2520Evgenios%2520M.%2520Kornaropoulos%2520and%2520Giuseppe%2520Ateniese%26entry.1292438233%3D%2520%2520We%2520introduce%2520LLMmap%252C%2520a%2520first-generation%2520fingerprinting%2520attack%2520targeted%2520at%250ALLM-integrated%2520applications.%2520LLMmap%2520employs%2520an%2520active%2520fingerprinting%2520approach%252C%250Asending%2520carefully%2520crafted%2520queries%2520to%2520the%2520application%2520and%2520analyzing%2520the%250Aresponses%2520to%2520identify%2520the%2520specific%2520LLM%2520model%2520in%2520use.%2520With%2520as%2520few%2520as%25208%250Ainteractions%252C%2520LLMmap%2520can%2520accurately%2520identify%2520LLMs%2520with%2520over%252095%2525%2520accuracy.%2520More%250Aimportantly%252C%2520LLMmap%2520is%2520designed%2520to%2520be%2520robust%2520across%2520different%2520application%250Alayers%252C%2520allowing%2520it%2520to%2520identify%2520LLMs%2520operating%2520under%2520various%2520system%2520prompts%252C%250Astochastic%2520sampling%2520hyperparameters%252C%2520and%2520even%2520complex%2520generation%2520frameworks%250Asuch%2520as%2520RAG%2520or%2520Chain-of-Thought.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMmap%3A%20Fingerprinting%20For%20Large%20Language%20Models&entry.906535625=Dario%20Pasquini%20and%20Evgenios%20M.%20Kornaropoulos%20and%20Giuseppe%20Ateniese&entry.1292438233=%20%20We%20introduce%20LLMmap%2C%20a%20first-generation%20fingerprinting%20attack%20targeted%20at%0ALLM-integrated%20applications.%20LLMmap%20employs%20an%20active%20fingerprinting%20approach%2C%0Asending%20carefully%20crafted%20queries%20to%20the%20application%20and%20analyzing%20the%0Aresponses%20to%20identify%20the%20specific%20LLM%20model%20in%20use.%20With%20as%20few%20as%208%0Ainteractions%2C%20LLMmap%20can%20accurately%20identify%20LLMs%20with%20over%2095%25%20accuracy.%20More%0Aimportantly%2C%20LLMmap%20is%20designed%20to%20be%20robust%20across%20different%20application%0Alayers%2C%20allowing%20it%20to%20identify%20LLMs%20operating%20under%20various%20system%20prompts%2C%0Astochastic%20sampling%20hyperparameters%2C%20and%20even%20complex%20generation%20frameworks%0Asuch%20as%20RAG%20or%20Chain-of-Thought.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15847v1&entry.124074799=Read"},
{"title": "Disentangling spatio-temporal knowledge for weakly supervised object\n  detection and segmentation in surgical video", "author": "Guiqiu Liao and Matjaz Jogan and Sai Koushik and Eric Eaton and Daniel A. Hashimoto", "abstract": "  Weakly supervised video object segmentation (WSVOS) enables the\nidentification of segmentation maps without requiring an extensive training\ndataset of object masks, relying instead on coarse video labels indicating\nobject presence. Current state-of-the-art methods either require multiple\nindependent stages of processing that employ motion cues or, in the case of\nend-to-end trainable networks, lack in segmentation accuracy, in part due to\nthe difficulty of learning segmentation maps from videos with transient object\npresence. This limits the application of WSVOS for semantic annotation of\nsurgical videos where multiple surgical tools frequently move in and out of the\nfield of view, a problem that is more difficult than typically encountered in\nWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks\n(VDST-Net), a framework to disentangle spatiotemporal information using\nsemi-decoupled knowledge distillation to predict high-quality class activation\nmaps (CAMs). A teacher network designed to resolve temporal conflicts when\nspecifics about object location and timing in the video are not provided works\nwith a student network that integrates information over time by leveraging\ntemporal dependencies. We demonstrate the efficacy of our framework on a public\nreference dataset and on a more challenging surgical video dataset where\nobjects are, on average, present in less than 60\\% of annotated frames. Our\nmethod outperforms state-of-the-art techniques and generates superior\nsegmentation masks under video-level weak supervision.\n", "link": "http://arxiv.org/abs/2407.15794v1", "date": "2024-07-22", "relevancy": 2.2852, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6026}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video&body=Title%3A%20Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video%0AAuthor%3A%20Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Sai%20Koushik%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto%0AAbstract%3A%20%20%20Weakly%20supervised%20video%20object%20segmentation%20%28WSVOS%29%20enables%20the%0Aidentification%20of%20segmentation%20maps%20without%20requiring%20an%20extensive%20training%0Adataset%20of%20object%20masks%2C%20relying%20instead%20on%20coarse%20video%20labels%20indicating%0Aobject%20presence.%20Current%20state-of-the-art%20methods%20either%20require%20multiple%0Aindependent%20stages%20of%20processing%20that%20employ%20motion%20cues%20or%2C%20in%20the%20case%20of%0Aend-to-end%20trainable%20networks%2C%20lack%20in%20segmentation%20accuracy%2C%20in%20part%20due%20to%0Athe%20difficulty%20of%20learning%20segmentation%20maps%20from%20videos%20with%20transient%20object%0Apresence.%20This%20limits%20the%20application%20of%20WSVOS%20for%20semantic%20annotation%20of%0Asurgical%20videos%20where%20multiple%20surgical%20tools%20frequently%20move%20in%20and%20out%20of%20the%0Afield%20of%20view%2C%20a%20problem%20that%20is%20more%20difficult%20than%20typically%20encountered%20in%0AWSVOS.%20This%20paper%20introduces%20Video%20Spatio-Temporal%20Disentanglement%20Networks%0A%28VDST-Net%29%2C%20a%20framework%20to%20disentangle%20spatiotemporal%20information%20using%0Asemi-decoupled%20knowledge%20distillation%20to%20predict%20high-quality%20class%20activation%0Amaps%20%28CAMs%29.%20A%20teacher%20network%20designed%20to%20resolve%20temporal%20conflicts%20when%0Aspecifics%20about%20object%20location%20and%20timing%20in%20the%20video%20are%20not%20provided%20works%0Awith%20a%20student%20network%20that%20integrates%20information%20over%20time%20by%20leveraging%0Atemporal%20dependencies.%20We%20demonstrate%20the%20efficacy%20of%20our%20framework%20on%20a%20public%0Areference%20dataset%20and%20on%20a%20more%20challenging%20surgical%20video%20dataset%20where%0Aobjects%20are%2C%20on%20average%2C%20present%20in%20less%20than%2060%5C%25%20of%20annotated%20frames.%20Our%0Amethod%20outperforms%20state-of-the-art%20techniques%20and%20generates%20superior%0Asegmentation%20masks%20under%20video-level%20weak%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520spatio-temporal%2520knowledge%2520for%2520weakly%2520supervised%2520object%250A%2520%2520detection%2520and%2520segmentation%2520in%2520surgical%2520video%26entry.906535625%3DGuiqiu%2520Liao%2520and%2520Matjaz%2520Jogan%2520and%2520Sai%2520Koushik%2520and%2520Eric%2520Eaton%2520and%2520Daniel%2520A.%2520Hashimoto%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520video%2520object%2520segmentation%2520%2528WSVOS%2529%2520enables%2520the%250Aidentification%2520of%2520segmentation%2520maps%2520without%2520requiring%2520an%2520extensive%2520training%250Adataset%2520of%2520object%2520masks%252C%2520relying%2520instead%2520on%2520coarse%2520video%2520labels%2520indicating%250Aobject%2520presence.%2520Current%2520state-of-the-art%2520methods%2520either%2520require%2520multiple%250Aindependent%2520stages%2520of%2520processing%2520that%2520employ%2520motion%2520cues%2520or%252C%2520in%2520the%2520case%2520of%250Aend-to-end%2520trainable%2520networks%252C%2520lack%2520in%2520segmentation%2520accuracy%252C%2520in%2520part%2520due%2520to%250Athe%2520difficulty%2520of%2520learning%2520segmentation%2520maps%2520from%2520videos%2520with%2520transient%2520object%250Apresence.%2520This%2520limits%2520the%2520application%2520of%2520WSVOS%2520for%2520semantic%2520annotation%2520of%250Asurgical%2520videos%2520where%2520multiple%2520surgical%2520tools%2520frequently%2520move%2520in%2520and%2520out%2520of%2520the%250Afield%2520of%2520view%252C%2520a%2520problem%2520that%2520is%2520more%2520difficult%2520than%2520typically%2520encountered%2520in%250AWSVOS.%2520This%2520paper%2520introduces%2520Video%2520Spatio-Temporal%2520Disentanglement%2520Networks%250A%2528VDST-Net%2529%252C%2520a%2520framework%2520to%2520disentangle%2520spatiotemporal%2520information%2520using%250Asemi-decoupled%2520knowledge%2520distillation%2520to%2520predict%2520high-quality%2520class%2520activation%250Amaps%2520%2528CAMs%2529.%2520A%2520teacher%2520network%2520designed%2520to%2520resolve%2520temporal%2520conflicts%2520when%250Aspecifics%2520about%2520object%2520location%2520and%2520timing%2520in%2520the%2520video%2520are%2520not%2520provided%2520works%250Awith%2520a%2520student%2520network%2520that%2520integrates%2520information%2520over%2520time%2520by%2520leveraging%250Atemporal%2520dependencies.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520framework%2520on%2520a%2520public%250Areference%2520dataset%2520and%2520on%2520a%2520more%2520challenging%2520surgical%2520video%2520dataset%2520where%250Aobjects%2520are%252C%2520on%2520average%252C%2520present%2520in%2520less%2520than%252060%255C%2525%2520of%2520annotated%2520frames.%2520Our%250Amethod%2520outperforms%2520state-of-the-art%2520techniques%2520and%2520generates%2520superior%250Asegmentation%2520masks%2520under%2520video-level%2520weak%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video&entry.906535625=Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Sai%20Koushik%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto&entry.1292438233=%20%20Weakly%20supervised%20video%20object%20segmentation%20%28WSVOS%29%20enables%20the%0Aidentification%20of%20segmentation%20maps%20without%20requiring%20an%20extensive%20training%0Adataset%20of%20object%20masks%2C%20relying%20instead%20on%20coarse%20video%20labels%20indicating%0Aobject%20presence.%20Current%20state-of-the-art%20methods%20either%20require%20multiple%0Aindependent%20stages%20of%20processing%20that%20employ%20motion%20cues%20or%2C%20in%20the%20case%20of%0Aend-to-end%20trainable%20networks%2C%20lack%20in%20segmentation%20accuracy%2C%20in%20part%20due%20to%0Athe%20difficulty%20of%20learning%20segmentation%20maps%20from%20videos%20with%20transient%20object%0Apresence.%20This%20limits%20the%20application%20of%20WSVOS%20for%20semantic%20annotation%20of%0Asurgical%20videos%20where%20multiple%20surgical%20tools%20frequently%20move%20in%20and%20out%20of%20the%0Afield%20of%20view%2C%20a%20problem%20that%20is%20more%20difficult%20than%20typically%20encountered%20in%0AWSVOS.%20This%20paper%20introduces%20Video%20Spatio-Temporal%20Disentanglement%20Networks%0A%28VDST-Net%29%2C%20a%20framework%20to%20disentangle%20spatiotemporal%20information%20using%0Asemi-decoupled%20knowledge%20distillation%20to%20predict%20high-quality%20class%20activation%0Amaps%20%28CAMs%29.%20A%20teacher%20network%20designed%20to%20resolve%20temporal%20conflicts%20when%0Aspecifics%20about%20object%20location%20and%20timing%20in%20the%20video%20are%20not%20provided%20works%0Awith%20a%20student%20network%20that%20integrates%20information%20over%20time%20by%20leveraging%0Atemporal%20dependencies.%20We%20demonstrate%20the%20efficacy%20of%20our%20framework%20on%20a%20public%0Areference%20dataset%20and%20on%20a%20more%20challenging%20surgical%20video%20dataset%20where%0Aobjects%20are%2C%20on%20average%2C%20present%20in%20less%20than%2060%5C%25%20of%20annotated%20frames.%20Our%0Amethod%20outperforms%20state-of-the-art%20techniques%20and%20generates%20superior%0Asegmentation%20masks%20under%20video-level%20weak%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15794v1&entry.124074799=Read"},
{"title": "BotArtist: Generic approach for bot detection in Twitter via\n  semi-automatic machine learning pipeline", "author": "Alexander Shevtsov and Despoina Antonakaki and Ioannis Lamprou and Polyvios Pratikakis and Sotiris Ioannidis", "abstract": "  Twitter, as one of the most popular social networks, provides a platform for\ncommunication and online discourse. Unfortunately, it has also become a target\nfor bots and fake accounts, resulting in the spread of false information and\nmanipulation. This paper introduces a semi-automatic machine learning pipeline\n(SAMLP) designed to address the challenges correlated with machine learning\nmodel development. Through this pipeline, we develop a comprehensive bot\ndetection model named BotArtist, based on user profile features. SAMLP\nleverages nine distinct publicly available datasets to train the BotArtist\nmodel. To assess BotArtist's performance against current state-of-the-art\nsolutions, we select 35 existing Twitter bot detection methods, each utilizing\na diverse range of features. Our comparative evaluation of BotArtist and these\nexisting methods, conducted across nine public datasets under standardized\nconditions, reveals that the proposed model outperforms existing solutions by\nalmost 10%, in terms of F1-score, achieving an average score of 83.19 and 68.5\nover specific and general approaches respectively. As a result of this\nresearch, we provide a dataset of the extracted features combined with\nBotArtist predictions over the 10.929.533 Twitter user profiles, collected via\nTwitter API during the 2022 Russo-Ukrainian War, over a 16-month period. This\ndataset was created in collaboration with [Shevtsov et al., 2022a] where the\noriginal authors share anonymized tweets on the discussion of the\nRusso-Ukrainian war with a total amount of 127.275.386 tweets. The combination\nof the existing text dataset and the provided labeled bot and human profiles\nwill allow for the future development of a more advanced bot detection large\nlanguage model in the post-Twitter API era.\n", "link": "http://arxiv.org/abs/2306.00037v4", "date": "2024-07-22", "relevancy": 2.2733, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4716}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BotArtist%3A%20Generic%20approach%20for%20bot%20detection%20in%20Twitter%20via%0A%20%20semi-automatic%20machine%20learning%20pipeline&body=Title%3A%20BotArtist%3A%20Generic%20approach%20for%20bot%20detection%20in%20Twitter%20via%0A%20%20semi-automatic%20machine%20learning%20pipeline%0AAuthor%3A%20Alexander%20Shevtsov%20and%20Despoina%20Antonakaki%20and%20Ioannis%20Lamprou%20and%20Polyvios%20Pratikakis%20and%20Sotiris%20Ioannidis%0AAbstract%3A%20%20%20Twitter%2C%20as%20one%20of%20the%20most%20popular%20social%20networks%2C%20provides%20a%20platform%20for%0Acommunication%20and%20online%20discourse.%20Unfortunately%2C%20it%20has%20also%20become%20a%20target%0Afor%20bots%20and%20fake%20accounts%2C%20resulting%20in%20the%20spread%20of%20false%20information%20and%0Amanipulation.%20This%20paper%20introduces%20a%20semi-automatic%20machine%20learning%20pipeline%0A%28SAMLP%29%20designed%20to%20address%20the%20challenges%20correlated%20with%20machine%20learning%0Amodel%20development.%20Through%20this%20pipeline%2C%20we%20develop%20a%20comprehensive%20bot%0Adetection%20model%20named%20BotArtist%2C%20based%20on%20user%20profile%20features.%20SAMLP%0Aleverages%20nine%20distinct%20publicly%20available%20datasets%20to%20train%20the%20BotArtist%0Amodel.%20To%20assess%20BotArtist%27s%20performance%20against%20current%20state-of-the-art%0Asolutions%2C%20we%20select%2035%20existing%20Twitter%20bot%20detection%20methods%2C%20each%20utilizing%0Aa%20diverse%20range%20of%20features.%20Our%20comparative%20evaluation%20of%20BotArtist%20and%20these%0Aexisting%20methods%2C%20conducted%20across%20nine%20public%20datasets%20under%20standardized%0Aconditions%2C%20reveals%20that%20the%20proposed%20model%20outperforms%20existing%20solutions%20by%0Aalmost%2010%25%2C%20in%20terms%20of%20F1-score%2C%20achieving%20an%20average%20score%20of%2083.19%20and%2068.5%0Aover%20specific%20and%20general%20approaches%20respectively.%20As%20a%20result%20of%20this%0Aresearch%2C%20we%20provide%20a%20dataset%20of%20the%20extracted%20features%20combined%20with%0ABotArtist%20predictions%20over%20the%2010.929.533%20Twitter%20user%20profiles%2C%20collected%20via%0ATwitter%20API%20during%20the%202022%20Russo-Ukrainian%20War%2C%20over%20a%2016-month%20period.%20This%0Adataset%20was%20created%20in%20collaboration%20with%20%5BShevtsov%20et%20al.%2C%202022a%5D%20where%20the%0Aoriginal%20authors%20share%20anonymized%20tweets%20on%20the%20discussion%20of%20the%0ARusso-Ukrainian%20war%20with%20a%20total%20amount%20of%20127.275.386%20tweets.%20The%20combination%0Aof%20the%20existing%20text%20dataset%20and%20the%20provided%20labeled%20bot%20and%20human%20profiles%0Awill%20allow%20for%20the%20future%20development%20of%20a%20more%20advanced%20bot%20detection%20large%0Alanguage%20model%20in%20the%20post-Twitter%20API%20era.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00037v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBotArtist%253A%2520Generic%2520approach%2520for%2520bot%2520detection%2520in%2520Twitter%2520via%250A%2520%2520semi-automatic%2520machine%2520learning%2520pipeline%26entry.906535625%3DAlexander%2520Shevtsov%2520and%2520Despoina%2520Antonakaki%2520and%2520Ioannis%2520Lamprou%2520and%2520Polyvios%2520Pratikakis%2520and%2520Sotiris%2520Ioannidis%26entry.1292438233%3D%2520%2520Twitter%252C%2520as%2520one%2520of%2520the%2520most%2520popular%2520social%2520networks%252C%2520provides%2520a%2520platform%2520for%250Acommunication%2520and%2520online%2520discourse.%2520Unfortunately%252C%2520it%2520has%2520also%2520become%2520a%2520target%250Afor%2520bots%2520and%2520fake%2520accounts%252C%2520resulting%2520in%2520the%2520spread%2520of%2520false%2520information%2520and%250Amanipulation.%2520This%2520paper%2520introduces%2520a%2520semi-automatic%2520machine%2520learning%2520pipeline%250A%2528SAMLP%2529%2520designed%2520to%2520address%2520the%2520challenges%2520correlated%2520with%2520machine%2520learning%250Amodel%2520development.%2520Through%2520this%2520pipeline%252C%2520we%2520develop%2520a%2520comprehensive%2520bot%250Adetection%2520model%2520named%2520BotArtist%252C%2520based%2520on%2520user%2520profile%2520features.%2520SAMLP%250Aleverages%2520nine%2520distinct%2520publicly%2520available%2520datasets%2520to%2520train%2520the%2520BotArtist%250Amodel.%2520To%2520assess%2520BotArtist%2527s%2520performance%2520against%2520current%2520state-of-the-art%250Asolutions%252C%2520we%2520select%252035%2520existing%2520Twitter%2520bot%2520detection%2520methods%252C%2520each%2520utilizing%250Aa%2520diverse%2520range%2520of%2520features.%2520Our%2520comparative%2520evaluation%2520of%2520BotArtist%2520and%2520these%250Aexisting%2520methods%252C%2520conducted%2520across%2520nine%2520public%2520datasets%2520under%2520standardized%250Aconditions%252C%2520reveals%2520that%2520the%2520proposed%2520model%2520outperforms%2520existing%2520solutions%2520by%250Aalmost%252010%2525%252C%2520in%2520terms%2520of%2520F1-score%252C%2520achieving%2520an%2520average%2520score%2520of%252083.19%2520and%252068.5%250Aover%2520specific%2520and%2520general%2520approaches%2520respectively.%2520As%2520a%2520result%2520of%2520this%250Aresearch%252C%2520we%2520provide%2520a%2520dataset%2520of%2520the%2520extracted%2520features%2520combined%2520with%250ABotArtist%2520predictions%2520over%2520the%252010.929.533%2520Twitter%2520user%2520profiles%252C%2520collected%2520via%250ATwitter%2520API%2520during%2520the%25202022%2520Russo-Ukrainian%2520War%252C%2520over%2520a%252016-month%2520period.%2520This%250Adataset%2520was%2520created%2520in%2520collaboration%2520with%2520%255BShevtsov%2520et%2520al.%252C%25202022a%255D%2520where%2520the%250Aoriginal%2520authors%2520share%2520anonymized%2520tweets%2520on%2520the%2520discussion%2520of%2520the%250ARusso-Ukrainian%2520war%2520with%2520a%2520total%2520amount%2520of%2520127.275.386%2520tweets.%2520The%2520combination%250Aof%2520the%2520existing%2520text%2520dataset%2520and%2520the%2520provided%2520labeled%2520bot%2520and%2520human%2520profiles%250Awill%2520allow%2520for%2520the%2520future%2520development%2520of%2520a%2520more%2520advanced%2520bot%2520detection%2520large%250Alanguage%2520model%2520in%2520the%2520post-Twitter%2520API%2520era.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00037v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BotArtist%3A%20Generic%20approach%20for%20bot%20detection%20in%20Twitter%20via%0A%20%20semi-automatic%20machine%20learning%20pipeline&entry.906535625=Alexander%20Shevtsov%20and%20Despoina%20Antonakaki%20and%20Ioannis%20Lamprou%20and%20Polyvios%20Pratikakis%20and%20Sotiris%20Ioannidis&entry.1292438233=%20%20Twitter%2C%20as%20one%20of%20the%20most%20popular%20social%20networks%2C%20provides%20a%20platform%20for%0Acommunication%20and%20online%20discourse.%20Unfortunately%2C%20it%20has%20also%20become%20a%20target%0Afor%20bots%20and%20fake%20accounts%2C%20resulting%20in%20the%20spread%20of%20false%20information%20and%0Amanipulation.%20This%20paper%20introduces%20a%20semi-automatic%20machine%20learning%20pipeline%0A%28SAMLP%29%20designed%20to%20address%20the%20challenges%20correlated%20with%20machine%20learning%0Amodel%20development.%20Through%20this%20pipeline%2C%20we%20develop%20a%20comprehensive%20bot%0Adetection%20model%20named%20BotArtist%2C%20based%20on%20user%20profile%20features.%20SAMLP%0Aleverages%20nine%20distinct%20publicly%20available%20datasets%20to%20train%20the%20BotArtist%0Amodel.%20To%20assess%20BotArtist%27s%20performance%20against%20current%20state-of-the-art%0Asolutions%2C%20we%20select%2035%20existing%20Twitter%20bot%20detection%20methods%2C%20each%20utilizing%0Aa%20diverse%20range%20of%20features.%20Our%20comparative%20evaluation%20of%20BotArtist%20and%20these%0Aexisting%20methods%2C%20conducted%20across%20nine%20public%20datasets%20under%20standardized%0Aconditions%2C%20reveals%20that%20the%20proposed%20model%20outperforms%20existing%20solutions%20by%0Aalmost%2010%25%2C%20in%20terms%20of%20F1-score%2C%20achieving%20an%20average%20score%20of%2083.19%20and%2068.5%0Aover%20specific%20and%20general%20approaches%20respectively.%20As%20a%20result%20of%20this%0Aresearch%2C%20we%20provide%20a%20dataset%20of%20the%20extracted%20features%20combined%20with%0ABotArtist%20predictions%20over%20the%2010.929.533%20Twitter%20user%20profiles%2C%20collected%20via%0ATwitter%20API%20during%20the%202022%20Russo-Ukrainian%20War%2C%20over%20a%2016-month%20period.%20This%0Adataset%20was%20created%20in%20collaboration%20with%20%5BShevtsov%20et%20al.%2C%202022a%5D%20where%20the%0Aoriginal%20authors%20share%20anonymized%20tweets%20on%20the%20discussion%20of%20the%0ARusso-Ukrainian%20war%20with%20a%20total%20amount%20of%20127.275.386%20tweets.%20The%20combination%0Aof%20the%20existing%20text%20dataset%20and%20the%20provided%20labeled%20bot%20and%20human%20profiles%0Awill%20allow%20for%20the%20future%20development%20of%20a%20more%20advanced%20bot%20detection%20large%0Alanguage%20model%20in%20the%20post-Twitter%20API%20era.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00037v4&entry.124074799=Read"},
{"title": "Probing Fine-Grained Action Understanding and Cross-View Generalization\n  of Foundation Models", "author": "Thinesh Thiyakesan Ponbagavathi and Kunyu Peng and Alina Roitberg", "abstract": "  Foundation models (FMs) are large neural networks trained on broad datasets,\nexcelling in downstream tasks with minimal fine-tuning. Human activity\nrecognition in video has advanced with FMs, driven by competition among\ndifferent architectures. However, high accuracies on standard benchmarks can\ndraw an artificially rosy picture, as they often overlook real-world factors\nlike changing camera perspectives. Popular benchmarks, mostly from YouTube or\nmovies, offer diverse views but only coarse actions, which are insufficient for\nuse-cases needing fine-grained, domain-specific actions. Domain-specific\ndatasets (e.g., for industrial assembly) typically use data from limited static\nperspectives. This paper empirically evaluates how perspective changes affect\ndifferent FMs in fine-grained human activity recognition. We compare multiple\nbackbone architectures and design choices, including image- and video- based\nmodels, and various strategies for temporal information fusion, including\ncommonly used score averaging and more novel attention-based temporal\naggregation mechanisms. This is the first systematic study of different\nfoundation models and specific design choices for human activity recognition\nfrom unknown views, conducted with the goal to provide guidance for backbone-\nand temporal- fusion scheme selection. Code and models will be made publicly\navailable to the community.\n", "link": "http://arxiv.org/abs/2407.15605v1", "date": "2024-07-22", "relevancy": 2.2525, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20Fine-Grained%20Action%20Understanding%20and%20Cross-View%20Generalization%0A%20%20of%20Foundation%20Models&body=Title%3A%20Probing%20Fine-Grained%20Action%20Understanding%20and%20Cross-View%20Generalization%0A%20%20of%20Foundation%20Models%0AAuthor%3A%20Thinesh%20Thiyakesan%20Ponbagavathi%20and%20Kunyu%20Peng%20and%20Alina%20Roitberg%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20large%20neural%20networks%20trained%20on%20broad%20datasets%2C%0Aexcelling%20in%20downstream%20tasks%20with%20minimal%20fine-tuning.%20Human%20activity%0Arecognition%20in%20video%20has%20advanced%20with%20FMs%2C%20driven%20by%20competition%20among%0Adifferent%20architectures.%20However%2C%20high%20accuracies%20on%20standard%20benchmarks%20can%0Adraw%20an%20artificially%20rosy%20picture%2C%20as%20they%20often%20overlook%20real-world%20factors%0Alike%20changing%20camera%20perspectives.%20Popular%20benchmarks%2C%20mostly%20from%20YouTube%20or%0Amovies%2C%20offer%20diverse%20views%20but%20only%20coarse%20actions%2C%20which%20are%20insufficient%20for%0Ause-cases%20needing%20fine-grained%2C%20domain-specific%20actions.%20Domain-specific%0Adatasets%20%28e.g.%2C%20for%20industrial%20assembly%29%20typically%20use%20data%20from%20limited%20static%0Aperspectives.%20This%20paper%20empirically%20evaluates%20how%20perspective%20changes%20affect%0Adifferent%20FMs%20in%20fine-grained%20human%20activity%20recognition.%20We%20compare%20multiple%0Abackbone%20architectures%20and%20design%20choices%2C%20including%20image-%20and%20video-%20based%0Amodels%2C%20and%20various%20strategies%20for%20temporal%20information%20fusion%2C%20including%0Acommonly%20used%20score%20averaging%20and%20more%20novel%20attention-based%20temporal%0Aaggregation%20mechanisms.%20This%20is%20the%20first%20systematic%20study%20of%20different%0Afoundation%20models%20and%20specific%20design%20choices%20for%20human%20activity%20recognition%0Afrom%20unknown%20views%2C%20conducted%20with%20the%20goal%20to%20provide%20guidance%20for%20backbone-%0Aand%20temporal-%20fusion%20scheme%20selection.%20Code%20and%20models%20will%20be%20made%20publicly%0Aavailable%20to%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520Fine-Grained%2520Action%2520Understanding%2520and%2520Cross-View%2520Generalization%250A%2520%2520of%2520Foundation%2520Models%26entry.906535625%3DThinesh%2520Thiyakesan%2520Ponbagavathi%2520and%2520Kunyu%2520Peng%2520and%2520Alina%2520Roitberg%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520large%2520neural%2520networks%2520trained%2520on%2520broad%2520datasets%252C%250Aexcelling%2520in%2520downstream%2520tasks%2520with%2520minimal%2520fine-tuning.%2520Human%2520activity%250Arecognition%2520in%2520video%2520has%2520advanced%2520with%2520FMs%252C%2520driven%2520by%2520competition%2520among%250Adifferent%2520architectures.%2520However%252C%2520high%2520accuracies%2520on%2520standard%2520benchmarks%2520can%250Adraw%2520an%2520artificially%2520rosy%2520picture%252C%2520as%2520they%2520often%2520overlook%2520real-world%2520factors%250Alike%2520changing%2520camera%2520perspectives.%2520Popular%2520benchmarks%252C%2520mostly%2520from%2520YouTube%2520or%250Amovies%252C%2520offer%2520diverse%2520views%2520but%2520only%2520coarse%2520actions%252C%2520which%2520are%2520insufficient%2520for%250Ause-cases%2520needing%2520fine-grained%252C%2520domain-specific%2520actions.%2520Domain-specific%250Adatasets%2520%2528e.g.%252C%2520for%2520industrial%2520assembly%2529%2520typically%2520use%2520data%2520from%2520limited%2520static%250Aperspectives.%2520This%2520paper%2520empirically%2520evaluates%2520how%2520perspective%2520changes%2520affect%250Adifferent%2520FMs%2520in%2520fine-grained%2520human%2520activity%2520recognition.%2520We%2520compare%2520multiple%250Abackbone%2520architectures%2520and%2520design%2520choices%252C%2520including%2520image-%2520and%2520video-%2520based%250Amodels%252C%2520and%2520various%2520strategies%2520for%2520temporal%2520information%2520fusion%252C%2520including%250Acommonly%2520used%2520score%2520averaging%2520and%2520more%2520novel%2520attention-based%2520temporal%250Aaggregation%2520mechanisms.%2520This%2520is%2520the%2520first%2520systematic%2520study%2520of%2520different%250Afoundation%2520models%2520and%2520specific%2520design%2520choices%2520for%2520human%2520activity%2520recognition%250Afrom%2520unknown%2520views%252C%2520conducted%2520with%2520the%2520goal%2520to%2520provide%2520guidance%2520for%2520backbone-%250Aand%2520temporal-%2520fusion%2520scheme%2520selection.%2520Code%2520and%2520models%2520will%2520be%2520made%2520publicly%250Aavailable%2520to%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20Fine-Grained%20Action%20Understanding%20and%20Cross-View%20Generalization%0A%20%20of%20Foundation%20Models&entry.906535625=Thinesh%20Thiyakesan%20Ponbagavathi%20and%20Kunyu%20Peng%20and%20Alina%20Roitberg&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20large%20neural%20networks%20trained%20on%20broad%20datasets%2C%0Aexcelling%20in%20downstream%20tasks%20with%20minimal%20fine-tuning.%20Human%20activity%0Arecognition%20in%20video%20has%20advanced%20with%20FMs%2C%20driven%20by%20competition%20among%0Adifferent%20architectures.%20However%2C%20high%20accuracies%20on%20standard%20benchmarks%20can%0Adraw%20an%20artificially%20rosy%20picture%2C%20as%20they%20often%20overlook%20real-world%20factors%0Alike%20changing%20camera%20perspectives.%20Popular%20benchmarks%2C%20mostly%20from%20YouTube%20or%0Amovies%2C%20offer%20diverse%20views%20but%20only%20coarse%20actions%2C%20which%20are%20insufficient%20for%0Ause-cases%20needing%20fine-grained%2C%20domain-specific%20actions.%20Domain-specific%0Adatasets%20%28e.g.%2C%20for%20industrial%20assembly%29%20typically%20use%20data%20from%20limited%20static%0Aperspectives.%20This%20paper%20empirically%20evaluates%20how%20perspective%20changes%20affect%0Adifferent%20FMs%20in%20fine-grained%20human%20activity%20recognition.%20We%20compare%20multiple%0Abackbone%20architectures%20and%20design%20choices%2C%20including%20image-%20and%20video-%20based%0Amodels%2C%20and%20various%20strategies%20for%20temporal%20information%20fusion%2C%20including%0Acommonly%20used%20score%20averaging%20and%20more%20novel%20attention-based%20temporal%0Aaggregation%20mechanisms.%20This%20is%20the%20first%20systematic%20study%20of%20different%0Afoundation%20models%20and%20specific%20design%20choices%20for%20human%20activity%20recognition%0Afrom%20unknown%20views%2C%20conducted%20with%20the%20goal%20to%20provide%20guidance%20for%20backbone-%0Aand%20temporal-%20fusion%20scheme%20selection.%20Code%20and%20models%20will%20be%20made%20publicly%0Aavailable%20to%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15605v1&entry.124074799=Read"},
{"title": "Distance-based mutual congestion feature selection with genetic\n  algorithm for high-dimensional medical datasets", "author": "Hossein Nematzadeh and Joseph Mani and Zahra Nematzadeh and Ebrahim Akbari and Radziah Mohamad", "abstract": "  Feature selection poses a challenge in small-sample high-dimensional\ndatasets, where the number of features exceeds the number of observations, as\nseen in microarray, gene expression, and medical datasets. There isn't a\nuniversally optimal feature selection method applicable to any data\ndistribution, and as a result, the literature consistently endeavors to address\nthis issue. One recent approach in feature selection is termed frequency-based\nfeature selection. However, existing methods in this domain tend to overlook\nfeature values, focusing solely on the distribution in the response variable.\nIn response, this paper introduces the Distance-based Mutual Congestion (DMC)\nas a filter method that considers both the feature values and the distribution\nof observations in the response variable. DMC sorts the features of datasets,\nand the top 5% are retained and clustered by KMeans to mitigate\nmulticollinearity. This is achieved by randomly selecting one feature from each\ncluster. The selected features form the feature space, and the search space for\nthe Genetic Algorithm with Adaptive Rates (GAwAR) will be approximated using\nthis feature space. GAwAR approximates the combination of the top 10 features\nthat maximizes prediction accuracy within a wrapper scheme. To prevent\npremature convergence, GAwAR adaptively updates the crossover and mutation\nrates. The hybrid DMC-GAwAR is applicable to binary classification datasets,\nand experimental results demonstrate its superiority over some recent works.\nThe implementation and corresponding data are available at\nhttps://github.com/hnematzadeh/DMC-GAwAR\n", "link": "http://arxiv.org/abs/2407.15611v1", "date": "2024-07-22", "relevancy": 2.249, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4578}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4493}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distance-based%20mutual%20congestion%20feature%20selection%20with%20genetic%0A%20%20algorithm%20for%20high-dimensional%20medical%20datasets&body=Title%3A%20Distance-based%20mutual%20congestion%20feature%20selection%20with%20genetic%0A%20%20algorithm%20for%20high-dimensional%20medical%20datasets%0AAuthor%3A%20Hossein%20Nematzadeh%20and%20Joseph%20Mani%20and%20Zahra%20Nematzadeh%20and%20Ebrahim%20Akbari%20and%20Radziah%20Mohamad%0AAbstract%3A%20%20%20Feature%20selection%20poses%20a%20challenge%20in%20small-sample%20high-dimensional%0Adatasets%2C%20where%20the%20number%20of%20features%20exceeds%20the%20number%20of%20observations%2C%20as%0Aseen%20in%20microarray%2C%20gene%20expression%2C%20and%20medical%20datasets.%20There%20isn%27t%20a%0Auniversally%20optimal%20feature%20selection%20method%20applicable%20to%20any%20data%0Adistribution%2C%20and%20as%20a%20result%2C%20the%20literature%20consistently%20endeavors%20to%20address%0Athis%20issue.%20One%20recent%20approach%20in%20feature%20selection%20is%20termed%20frequency-based%0Afeature%20selection.%20However%2C%20existing%20methods%20in%20this%20domain%20tend%20to%20overlook%0Afeature%20values%2C%20focusing%20solely%20on%20the%20distribution%20in%20the%20response%20variable.%0AIn%20response%2C%20this%20paper%20introduces%20the%20Distance-based%20Mutual%20Congestion%20%28DMC%29%0Aas%20a%20filter%20method%20that%20considers%20both%20the%20feature%20values%20and%20the%20distribution%0Aof%20observations%20in%20the%20response%20variable.%20DMC%20sorts%20the%20features%20of%20datasets%2C%0Aand%20the%20top%205%25%20are%20retained%20and%20clustered%20by%20KMeans%20to%20mitigate%0Amulticollinearity.%20This%20is%20achieved%20by%20randomly%20selecting%20one%20feature%20from%20each%0Acluster.%20The%20selected%20features%20form%20the%20feature%20space%2C%20and%20the%20search%20space%20for%0Athe%20Genetic%20Algorithm%20with%20Adaptive%20Rates%20%28GAwAR%29%20will%20be%20approximated%20using%0Athis%20feature%20space.%20GAwAR%20approximates%20the%20combination%20of%20the%20top%2010%20features%0Athat%20maximizes%20prediction%20accuracy%20within%20a%20wrapper%20scheme.%20To%20prevent%0Apremature%20convergence%2C%20GAwAR%20adaptively%20updates%20the%20crossover%20and%20mutation%0Arates.%20The%20hybrid%20DMC-GAwAR%20is%20applicable%20to%20binary%20classification%20datasets%2C%0Aand%20experimental%20results%20demonstrate%20its%20superiority%20over%20some%20recent%20works.%0AThe%20implementation%20and%20corresponding%20data%20are%20available%20at%0Ahttps%3A//github.com/hnematzadeh/DMC-GAwAR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistance-based%2520mutual%2520congestion%2520feature%2520selection%2520with%2520genetic%250A%2520%2520algorithm%2520for%2520high-dimensional%2520medical%2520datasets%26entry.906535625%3DHossein%2520Nematzadeh%2520and%2520Joseph%2520Mani%2520and%2520Zahra%2520Nematzadeh%2520and%2520Ebrahim%2520Akbari%2520and%2520Radziah%2520Mohamad%26entry.1292438233%3D%2520%2520Feature%2520selection%2520poses%2520a%2520challenge%2520in%2520small-sample%2520high-dimensional%250Adatasets%252C%2520where%2520the%2520number%2520of%2520features%2520exceeds%2520the%2520number%2520of%2520observations%252C%2520as%250Aseen%2520in%2520microarray%252C%2520gene%2520expression%252C%2520and%2520medical%2520datasets.%2520There%2520isn%2527t%2520a%250Auniversally%2520optimal%2520feature%2520selection%2520method%2520applicable%2520to%2520any%2520data%250Adistribution%252C%2520and%2520as%2520a%2520result%252C%2520the%2520literature%2520consistently%2520endeavors%2520to%2520address%250Athis%2520issue.%2520One%2520recent%2520approach%2520in%2520feature%2520selection%2520is%2520termed%2520frequency-based%250Afeature%2520selection.%2520However%252C%2520existing%2520methods%2520in%2520this%2520domain%2520tend%2520to%2520overlook%250Afeature%2520values%252C%2520focusing%2520solely%2520on%2520the%2520distribution%2520in%2520the%2520response%2520variable.%250AIn%2520response%252C%2520this%2520paper%2520introduces%2520the%2520Distance-based%2520Mutual%2520Congestion%2520%2528DMC%2529%250Aas%2520a%2520filter%2520method%2520that%2520considers%2520both%2520the%2520feature%2520values%2520and%2520the%2520distribution%250Aof%2520observations%2520in%2520the%2520response%2520variable.%2520DMC%2520sorts%2520the%2520features%2520of%2520datasets%252C%250Aand%2520the%2520top%25205%2525%2520are%2520retained%2520and%2520clustered%2520by%2520KMeans%2520to%2520mitigate%250Amulticollinearity.%2520This%2520is%2520achieved%2520by%2520randomly%2520selecting%2520one%2520feature%2520from%2520each%250Acluster.%2520The%2520selected%2520features%2520form%2520the%2520feature%2520space%252C%2520and%2520the%2520search%2520space%2520for%250Athe%2520Genetic%2520Algorithm%2520with%2520Adaptive%2520Rates%2520%2528GAwAR%2529%2520will%2520be%2520approximated%2520using%250Athis%2520feature%2520space.%2520GAwAR%2520approximates%2520the%2520combination%2520of%2520the%2520top%252010%2520features%250Athat%2520maximizes%2520prediction%2520accuracy%2520within%2520a%2520wrapper%2520scheme.%2520To%2520prevent%250Apremature%2520convergence%252C%2520GAwAR%2520adaptively%2520updates%2520the%2520crossover%2520and%2520mutation%250Arates.%2520The%2520hybrid%2520DMC-GAwAR%2520is%2520applicable%2520to%2520binary%2520classification%2520datasets%252C%250Aand%2520experimental%2520results%2520demonstrate%2520its%2520superiority%2520over%2520some%2520recent%2520works.%250AThe%2520implementation%2520and%2520corresponding%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/hnematzadeh/DMC-GAwAR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distance-based%20mutual%20congestion%20feature%20selection%20with%20genetic%0A%20%20algorithm%20for%20high-dimensional%20medical%20datasets&entry.906535625=Hossein%20Nematzadeh%20and%20Joseph%20Mani%20and%20Zahra%20Nematzadeh%20and%20Ebrahim%20Akbari%20and%20Radziah%20Mohamad&entry.1292438233=%20%20Feature%20selection%20poses%20a%20challenge%20in%20small-sample%20high-dimensional%0Adatasets%2C%20where%20the%20number%20of%20features%20exceeds%20the%20number%20of%20observations%2C%20as%0Aseen%20in%20microarray%2C%20gene%20expression%2C%20and%20medical%20datasets.%20There%20isn%27t%20a%0Auniversally%20optimal%20feature%20selection%20method%20applicable%20to%20any%20data%0Adistribution%2C%20and%20as%20a%20result%2C%20the%20literature%20consistently%20endeavors%20to%20address%0Athis%20issue.%20One%20recent%20approach%20in%20feature%20selection%20is%20termed%20frequency-based%0Afeature%20selection.%20However%2C%20existing%20methods%20in%20this%20domain%20tend%20to%20overlook%0Afeature%20values%2C%20focusing%20solely%20on%20the%20distribution%20in%20the%20response%20variable.%0AIn%20response%2C%20this%20paper%20introduces%20the%20Distance-based%20Mutual%20Congestion%20%28DMC%29%0Aas%20a%20filter%20method%20that%20considers%20both%20the%20feature%20values%20and%20the%20distribution%0Aof%20observations%20in%20the%20response%20variable.%20DMC%20sorts%20the%20features%20of%20datasets%2C%0Aand%20the%20top%205%25%20are%20retained%20and%20clustered%20by%20KMeans%20to%20mitigate%0Amulticollinearity.%20This%20is%20achieved%20by%20randomly%20selecting%20one%20feature%20from%20each%0Acluster.%20The%20selected%20features%20form%20the%20feature%20space%2C%20and%20the%20search%20space%20for%0Athe%20Genetic%20Algorithm%20with%20Adaptive%20Rates%20%28GAwAR%29%20will%20be%20approximated%20using%0Athis%20feature%20space.%20GAwAR%20approximates%20the%20combination%20of%20the%20top%2010%20features%0Athat%20maximizes%20prediction%20accuracy%20within%20a%20wrapper%20scheme.%20To%20prevent%0Apremature%20convergence%2C%20GAwAR%20adaptively%20updates%20the%20crossover%20and%20mutation%0Arates.%20The%20hybrid%20DMC-GAwAR%20is%20applicable%20to%20binary%20classification%20datasets%2C%0Aand%20experimental%20results%20demonstrate%20its%20superiority%20over%20some%20recent%20works.%0AThe%20implementation%20and%20corresponding%20data%20are%20available%20at%0Ahttps%3A//github.com/hnematzadeh/DMC-GAwAR%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15611v1&entry.124074799=Read"},
{"title": "A Life-long Learning Intrusion Detection System for 6G-Enabled IoV", "author": "Abdelaziz Amara korba and Souad Sebaa and Malik Mabrouki and Yacine Ghamri-Doudane and Karima Benatchba", "abstract": "  The introduction of 6G technology into the Internet of Vehicles (IoV)\npromises to revolutionize connectivity with ultra-high data rates and seamless\nnetwork coverage. However, this technological leap also brings significant\nchallenges, particularly for the dynamic and diverse IoV landscape, which must\nmeet the rigorous reliability and security requirements of 6G networks.\nFurthermore, integrating 6G will likely increase the IoV's susceptibility to a\nspectrum of emerging cyber threats. Therefore, it is crucial for security\nmechanisms to dynamically adapt and learn new attack patterns, keeping pace\nwith the rapid evolution and diversification of these threats - a capability\ncurrently lacking in existing systems. This paper presents a novel intrusion\ndetection system leveraging the paradigm of life-long (or continual) learning.\nOur methodology combines class-incremental learning with federated learning, an\napproach ideally suited to the distributed nature of the IoV. This strategy\neffectively harnesses the collective intelligence of Connected and Automated\nVehicles (CAVs) and edge computing capabilities to train the detection system.\nTo the best of our knowledge, this study is the first to synergize\nclass-incremental learning with federated learning specifically for cyber\nattack detection. Through comprehensive experiments on a recent network traffic\ndataset, our system has exhibited a robust adaptability in learning new cyber\nattack patterns, while effectively retaining knowledge of previously\nencountered ones. Additionally, it has proven to maintain high accuracy and a\nlow false positive rate.\n", "link": "http://arxiv.org/abs/2407.15700v1", "date": "2024-07-22", "relevancy": 2.2483, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4538}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4526}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Life-long%20Learning%20Intrusion%20Detection%20System%20for%206G-Enabled%20IoV&body=Title%3A%20A%20Life-long%20Learning%20Intrusion%20Detection%20System%20for%206G-Enabled%20IoV%0AAuthor%3A%20Abdelaziz%20Amara%20korba%20and%20Souad%20Sebaa%20and%20Malik%20Mabrouki%20and%20Yacine%20Ghamri-Doudane%20and%20Karima%20Benatchba%0AAbstract%3A%20%20%20The%20introduction%20of%206G%20technology%20into%20the%20Internet%20of%20Vehicles%20%28IoV%29%0Apromises%20to%20revolutionize%20connectivity%20with%20ultra-high%20data%20rates%20and%20seamless%0Anetwork%20coverage.%20However%2C%20this%20technological%20leap%20also%20brings%20significant%0Achallenges%2C%20particularly%20for%20the%20dynamic%20and%20diverse%20IoV%20landscape%2C%20which%20must%0Ameet%20the%20rigorous%20reliability%20and%20security%20requirements%20of%206G%20networks.%0AFurthermore%2C%20integrating%206G%20will%20likely%20increase%20the%20IoV%27s%20susceptibility%20to%20a%0Aspectrum%20of%20emerging%20cyber%20threats.%20Therefore%2C%20it%20is%20crucial%20for%20security%0Amechanisms%20to%20dynamically%20adapt%20and%20learn%20new%20attack%20patterns%2C%20keeping%20pace%0Awith%20the%20rapid%20evolution%20and%20diversification%20of%20these%20threats%20-%20a%20capability%0Acurrently%20lacking%20in%20existing%20systems.%20This%20paper%20presents%20a%20novel%20intrusion%0Adetection%20system%20leveraging%20the%20paradigm%20of%20life-long%20%28or%20continual%29%20learning.%0AOur%20methodology%20combines%20class-incremental%20learning%20with%20federated%20learning%2C%20an%0Aapproach%20ideally%20suited%20to%20the%20distributed%20nature%20of%20the%20IoV.%20This%20strategy%0Aeffectively%20harnesses%20the%20collective%20intelligence%20of%20Connected%20and%20Automated%0AVehicles%20%28CAVs%29%20and%20edge%20computing%20capabilities%20to%20train%20the%20detection%20system.%0ATo%20the%20best%20of%20our%20knowledge%2C%20this%20study%20is%20the%20first%20to%20synergize%0Aclass-incremental%20learning%20with%20federated%20learning%20specifically%20for%20cyber%0Aattack%20detection.%20Through%20comprehensive%20experiments%20on%20a%20recent%20network%20traffic%0Adataset%2C%20our%20system%20has%20exhibited%20a%20robust%20adaptability%20in%20learning%20new%20cyber%0Aattack%20patterns%2C%20while%20effectively%20retaining%20knowledge%20of%20previously%0Aencountered%20ones.%20Additionally%2C%20it%20has%20proven%20to%20maintain%20high%20accuracy%20and%20a%0Alow%20false%20positive%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Life-long%2520Learning%2520Intrusion%2520Detection%2520System%2520for%25206G-Enabled%2520IoV%26entry.906535625%3DAbdelaziz%2520Amara%2520korba%2520and%2520Souad%2520Sebaa%2520and%2520Malik%2520Mabrouki%2520and%2520Yacine%2520Ghamri-Doudane%2520and%2520Karima%2520Benatchba%26entry.1292438233%3D%2520%2520The%2520introduction%2520of%25206G%2520technology%2520into%2520the%2520Internet%2520of%2520Vehicles%2520%2528IoV%2529%250Apromises%2520to%2520revolutionize%2520connectivity%2520with%2520ultra-high%2520data%2520rates%2520and%2520seamless%250Anetwork%2520coverage.%2520However%252C%2520this%2520technological%2520leap%2520also%2520brings%2520significant%250Achallenges%252C%2520particularly%2520for%2520the%2520dynamic%2520and%2520diverse%2520IoV%2520landscape%252C%2520which%2520must%250Ameet%2520the%2520rigorous%2520reliability%2520and%2520security%2520requirements%2520of%25206G%2520networks.%250AFurthermore%252C%2520integrating%25206G%2520will%2520likely%2520increase%2520the%2520IoV%2527s%2520susceptibility%2520to%2520a%250Aspectrum%2520of%2520emerging%2520cyber%2520threats.%2520Therefore%252C%2520it%2520is%2520crucial%2520for%2520security%250Amechanisms%2520to%2520dynamically%2520adapt%2520and%2520learn%2520new%2520attack%2520patterns%252C%2520keeping%2520pace%250Awith%2520the%2520rapid%2520evolution%2520and%2520diversification%2520of%2520these%2520threats%2520-%2520a%2520capability%250Acurrently%2520lacking%2520in%2520existing%2520systems.%2520This%2520paper%2520presents%2520a%2520novel%2520intrusion%250Adetection%2520system%2520leveraging%2520the%2520paradigm%2520of%2520life-long%2520%2528or%2520continual%2529%2520learning.%250AOur%2520methodology%2520combines%2520class-incremental%2520learning%2520with%2520federated%2520learning%252C%2520an%250Aapproach%2520ideally%2520suited%2520to%2520the%2520distributed%2520nature%2520of%2520the%2520IoV.%2520This%2520strategy%250Aeffectively%2520harnesses%2520the%2520collective%2520intelligence%2520of%2520Connected%2520and%2520Automated%250AVehicles%2520%2528CAVs%2529%2520and%2520edge%2520computing%2520capabilities%2520to%2520train%2520the%2520detection%2520system.%250ATo%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520study%2520is%2520the%2520first%2520to%2520synergize%250Aclass-incremental%2520learning%2520with%2520federated%2520learning%2520specifically%2520for%2520cyber%250Aattack%2520detection.%2520Through%2520comprehensive%2520experiments%2520on%2520a%2520recent%2520network%2520traffic%250Adataset%252C%2520our%2520system%2520has%2520exhibited%2520a%2520robust%2520adaptability%2520in%2520learning%2520new%2520cyber%250Aattack%2520patterns%252C%2520while%2520effectively%2520retaining%2520knowledge%2520of%2520previously%250Aencountered%2520ones.%2520Additionally%252C%2520it%2520has%2520proven%2520to%2520maintain%2520high%2520accuracy%2520and%2520a%250Alow%2520false%2520positive%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Life-long%20Learning%20Intrusion%20Detection%20System%20for%206G-Enabled%20IoV&entry.906535625=Abdelaziz%20Amara%20korba%20and%20Souad%20Sebaa%20and%20Malik%20Mabrouki%20and%20Yacine%20Ghamri-Doudane%20and%20Karima%20Benatchba&entry.1292438233=%20%20The%20introduction%20of%206G%20technology%20into%20the%20Internet%20of%20Vehicles%20%28IoV%29%0Apromises%20to%20revolutionize%20connectivity%20with%20ultra-high%20data%20rates%20and%20seamless%0Anetwork%20coverage.%20However%2C%20this%20technological%20leap%20also%20brings%20significant%0Achallenges%2C%20particularly%20for%20the%20dynamic%20and%20diverse%20IoV%20landscape%2C%20which%20must%0Ameet%20the%20rigorous%20reliability%20and%20security%20requirements%20of%206G%20networks.%0AFurthermore%2C%20integrating%206G%20will%20likely%20increase%20the%20IoV%27s%20susceptibility%20to%20a%0Aspectrum%20of%20emerging%20cyber%20threats.%20Therefore%2C%20it%20is%20crucial%20for%20security%0Amechanisms%20to%20dynamically%20adapt%20and%20learn%20new%20attack%20patterns%2C%20keeping%20pace%0Awith%20the%20rapid%20evolution%20and%20diversification%20of%20these%20threats%20-%20a%20capability%0Acurrently%20lacking%20in%20existing%20systems.%20This%20paper%20presents%20a%20novel%20intrusion%0Adetection%20system%20leveraging%20the%20paradigm%20of%20life-long%20%28or%20continual%29%20learning.%0AOur%20methodology%20combines%20class-incremental%20learning%20with%20federated%20learning%2C%20an%0Aapproach%20ideally%20suited%20to%20the%20distributed%20nature%20of%20the%20IoV.%20This%20strategy%0Aeffectively%20harnesses%20the%20collective%20intelligence%20of%20Connected%20and%20Automated%0AVehicles%20%28CAVs%29%20and%20edge%20computing%20capabilities%20to%20train%20the%20detection%20system.%0ATo%20the%20best%20of%20our%20knowledge%2C%20this%20study%20is%20the%20first%20to%20synergize%0Aclass-incremental%20learning%20with%20federated%20learning%20specifically%20for%20cyber%0Aattack%20detection.%20Through%20comprehensive%20experiments%20on%20a%20recent%20network%20traffic%0Adataset%2C%20our%20system%20has%20exhibited%20a%20robust%20adaptability%20in%20learning%20new%20cyber%0Aattack%20patterns%2C%20while%20effectively%20retaining%20knowledge%20of%20previously%0Aencountered%20ones.%20Additionally%2C%20it%20has%20proven%20to%20maintain%20high%20accuracy%20and%20a%0Alow%20false%20positive%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15700v1&entry.124074799=Read"},
{"title": "MSSPlace: Multi-Sensor Place Recognition with Visual and Text Semantics", "author": "Alexander Melekhin and Dmitry Yudin and Ilia Petryashin and Vitaly Bezuglyj", "abstract": "  Place recognition is a challenging task in computer vision, crucial for\nenabling autonomous vehicles and robots to navigate previously visited\nenvironments. While significant progress has been made in learnable multimodal\nmethods that combine onboard camera images and LiDAR point clouds, the full\npotential of these methods remains largely unexplored in localization\napplications. In this paper, we study the impact of leveraging a multi-camera\nsetup and integrating diverse data sources for multimodal place recognition,\nincorporating explicit visual semantics and text descriptions. Our proposed\nmethod named MSSPlace utilizes images from multiple cameras, LiDAR point\nclouds, semantic segmentation masks, and text annotations to generate\ncomprehensive place descriptors. We employ a late fusion approach to integrate\nthese modalities, providing a unified representation. Through extensive\nexperiments on the Oxford RobotCar and NCLT datasets, we systematically analyze\nthe impact of each data source on the overall quality of place descriptors. Our\nexperiments demonstrate that combining data from multiple sensors significantly\nimproves place recognition model performance compared to single modality\napproaches and leads to state-of-the-art quality. We also show that separate\nusage of visual or textual semantics (which are more compact representations of\nsensory data) can achieve promising results in place recognition. The code for\nour method is publicly available: https://github.com/alexmelekhin/MSSPlace\n", "link": "http://arxiv.org/abs/2407.15663v1", "date": "2024-07-22", "relevancy": 2.2354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6116}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSSPlace%3A%20Multi-Sensor%20Place%20Recognition%20with%20Visual%20and%20Text%20Semantics&body=Title%3A%20MSSPlace%3A%20Multi-Sensor%20Place%20Recognition%20with%20Visual%20and%20Text%20Semantics%0AAuthor%3A%20Alexander%20Melekhin%20and%20Dmitry%20Yudin%20and%20Ilia%20Petryashin%20and%20Vitaly%20Bezuglyj%0AAbstract%3A%20%20%20Place%20recognition%20is%20a%20challenging%20task%20in%20computer%20vision%2C%20crucial%20for%0Aenabling%20autonomous%20vehicles%20and%20robots%20to%20navigate%20previously%20visited%0Aenvironments.%20While%20significant%20progress%20has%20been%20made%20in%20learnable%20multimodal%0Amethods%20that%20combine%20onboard%20camera%20images%20and%20LiDAR%20point%20clouds%2C%20the%20full%0Apotential%20of%20these%20methods%20remains%20largely%20unexplored%20in%20localization%0Aapplications.%20In%20this%20paper%2C%20we%20study%20the%20impact%20of%20leveraging%20a%20multi-camera%0Asetup%20and%20integrating%20diverse%20data%20sources%20for%20multimodal%20place%20recognition%2C%0Aincorporating%20explicit%20visual%20semantics%20and%20text%20descriptions.%20Our%20proposed%0Amethod%20named%20MSSPlace%20utilizes%20images%20from%20multiple%20cameras%2C%20LiDAR%20point%0Aclouds%2C%20semantic%20segmentation%20masks%2C%20and%20text%20annotations%20to%20generate%0Acomprehensive%20place%20descriptors.%20We%20employ%20a%20late%20fusion%20approach%20to%20integrate%0Athese%20modalities%2C%20providing%20a%20unified%20representation.%20Through%20extensive%0Aexperiments%20on%20the%20Oxford%20RobotCar%20and%20NCLT%20datasets%2C%20we%20systematically%20analyze%0Athe%20impact%20of%20each%20data%20source%20on%20the%20overall%20quality%20of%20place%20descriptors.%20Our%0Aexperiments%20demonstrate%20that%20combining%20data%20from%20multiple%20sensors%20significantly%0Aimproves%20place%20recognition%20model%20performance%20compared%20to%20single%20modality%0Aapproaches%20and%20leads%20to%20state-of-the-art%20quality.%20We%20also%20show%20that%20separate%0Ausage%20of%20visual%20or%20textual%20semantics%20%28which%20are%20more%20compact%20representations%20of%0Asensory%20data%29%20can%20achieve%20promising%20results%20in%20place%20recognition.%20The%20code%20for%0Aour%20method%20is%20publicly%20available%3A%20https%3A//github.com/alexmelekhin/MSSPlace%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSSPlace%253A%2520Multi-Sensor%2520Place%2520Recognition%2520with%2520Visual%2520and%2520Text%2520Semantics%26entry.906535625%3DAlexander%2520Melekhin%2520and%2520Dmitry%2520Yudin%2520and%2520Ilia%2520Petryashin%2520and%2520Vitaly%2520Bezuglyj%26entry.1292438233%3D%2520%2520Place%2520recognition%2520is%2520a%2520challenging%2520task%2520in%2520computer%2520vision%252C%2520crucial%2520for%250Aenabling%2520autonomous%2520vehicles%2520and%2520robots%2520to%2520navigate%2520previously%2520visited%250Aenvironments.%2520While%2520significant%2520progress%2520has%2520been%2520made%2520in%2520learnable%2520multimodal%250Amethods%2520that%2520combine%2520onboard%2520camera%2520images%2520and%2520LiDAR%2520point%2520clouds%252C%2520the%2520full%250Apotential%2520of%2520these%2520methods%2520remains%2520largely%2520unexplored%2520in%2520localization%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520impact%2520of%2520leveraging%2520a%2520multi-camera%250Asetup%2520and%2520integrating%2520diverse%2520data%2520sources%2520for%2520multimodal%2520place%2520recognition%252C%250Aincorporating%2520explicit%2520visual%2520semantics%2520and%2520text%2520descriptions.%2520Our%2520proposed%250Amethod%2520named%2520MSSPlace%2520utilizes%2520images%2520from%2520multiple%2520cameras%252C%2520LiDAR%2520point%250Aclouds%252C%2520semantic%2520segmentation%2520masks%252C%2520and%2520text%2520annotations%2520to%2520generate%250Acomprehensive%2520place%2520descriptors.%2520We%2520employ%2520a%2520late%2520fusion%2520approach%2520to%2520integrate%250Athese%2520modalities%252C%2520providing%2520a%2520unified%2520representation.%2520Through%2520extensive%250Aexperiments%2520on%2520the%2520Oxford%2520RobotCar%2520and%2520NCLT%2520datasets%252C%2520we%2520systematically%2520analyze%250Athe%2520impact%2520of%2520each%2520data%2520source%2520on%2520the%2520overall%2520quality%2520of%2520place%2520descriptors.%2520Our%250Aexperiments%2520demonstrate%2520that%2520combining%2520data%2520from%2520multiple%2520sensors%2520significantly%250Aimproves%2520place%2520recognition%2520model%2520performance%2520compared%2520to%2520single%2520modality%250Aapproaches%2520and%2520leads%2520to%2520state-of-the-art%2520quality.%2520We%2520also%2520show%2520that%2520separate%250Ausage%2520of%2520visual%2520or%2520textual%2520semantics%2520%2528which%2520are%2520more%2520compact%2520representations%2520of%250Asensory%2520data%2529%2520can%2520achieve%2520promising%2520results%2520in%2520place%2520recognition.%2520The%2520code%2520for%250Aour%2520method%2520is%2520publicly%2520available%253A%2520https%253A//github.com/alexmelekhin/MSSPlace%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSSPlace%3A%20Multi-Sensor%20Place%20Recognition%20with%20Visual%20and%20Text%20Semantics&entry.906535625=Alexander%20Melekhin%20and%20Dmitry%20Yudin%20and%20Ilia%20Petryashin%20and%20Vitaly%20Bezuglyj&entry.1292438233=%20%20Place%20recognition%20is%20a%20challenging%20task%20in%20computer%20vision%2C%20crucial%20for%0Aenabling%20autonomous%20vehicles%20and%20robots%20to%20navigate%20previously%20visited%0Aenvironments.%20While%20significant%20progress%20has%20been%20made%20in%20learnable%20multimodal%0Amethods%20that%20combine%20onboard%20camera%20images%20and%20LiDAR%20point%20clouds%2C%20the%20full%0Apotential%20of%20these%20methods%20remains%20largely%20unexplored%20in%20localization%0Aapplications.%20In%20this%20paper%2C%20we%20study%20the%20impact%20of%20leveraging%20a%20multi-camera%0Asetup%20and%20integrating%20diverse%20data%20sources%20for%20multimodal%20place%20recognition%2C%0Aincorporating%20explicit%20visual%20semantics%20and%20text%20descriptions.%20Our%20proposed%0Amethod%20named%20MSSPlace%20utilizes%20images%20from%20multiple%20cameras%2C%20LiDAR%20point%0Aclouds%2C%20semantic%20segmentation%20masks%2C%20and%20text%20annotations%20to%20generate%0Acomprehensive%20place%20descriptors.%20We%20employ%20a%20late%20fusion%20approach%20to%20integrate%0Athese%20modalities%2C%20providing%20a%20unified%20representation.%20Through%20extensive%0Aexperiments%20on%20the%20Oxford%20RobotCar%20and%20NCLT%20datasets%2C%20we%20systematically%20analyze%0Athe%20impact%20of%20each%20data%20source%20on%20the%20overall%20quality%20of%20place%20descriptors.%20Our%0Aexperiments%20demonstrate%20that%20combining%20data%20from%20multiple%20sensors%20significantly%0Aimproves%20place%20recognition%20model%20performance%20compared%20to%20single%20modality%0Aapproaches%20and%20leads%20to%20state-of-the-art%20quality.%20We%20also%20show%20that%20separate%0Ausage%20of%20visual%20or%20textual%20semantics%20%28which%20are%20more%20compact%20representations%20of%0Asensory%20data%29%20can%20achieve%20promising%20results%20in%20place%20recognition.%20The%20code%20for%0Aour%20method%20is%20publicly%20available%3A%20https%3A//github.com/alexmelekhin/MSSPlace%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15663v1&entry.124074799=Read"},
{"title": "Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight", "author": "Ziyuan Huang and Kaixiang Ji and Biao Gong and Zhiwu Qing and Qinglong Zhang and Kecheng Zheng and Jian Wang and Jingdong Chen and Ming Yang", "abstract": "  This paper introduces Chain-of-Sight, a vision-language bridge module that\naccelerates the pre-training of Multimodal Large Language Models (MLLMs). Our\napproach employs a sequence of visual resamplers that capture visual details at\nvarious spacial scales. This architecture not only leverages global and local\nvisual contexts effectively, but also facilitates the flexible extension of\nvisual tokens through a compound token scaling strategy, allowing up to a 16x\nincrease in the token count post pre-training. Consequently, Chain-of-Sight\nrequires significantly fewer visual tokens in the pre-training phase compared\nto the fine-tuning phase. This intentional reduction of visual tokens during\npre-training notably accelerates the pre-training process, cutting down the\nwall-clock training time by ~73%. Empirical results on a series of\nvision-language benchmarks reveal that the pre-train acceleration through\nChain-of-Sight is achieved without sacrificing performance, matching or\nsurpassing the standard pipeline of utilizing all visual tokens throughout the\nentire training process. Further scaling up the number of visual tokens for\npre-training leads to stronger performances, competitive to existing approaches\nin a series of benchmarks.\n", "link": "http://arxiv.org/abs/2407.15819v1", "date": "2024-07-22", "relevancy": 2.2319, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6239}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5112}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Pre-training%20of%20Multimodal%20LLMs%20via%20Chain-of-Sight&body=Title%3A%20Accelerating%20Pre-training%20of%20Multimodal%20LLMs%20via%20Chain-of-Sight%0AAuthor%3A%20Ziyuan%20Huang%20and%20Kaixiang%20Ji%20and%20Biao%20Gong%20and%20Zhiwu%20Qing%20and%20Qinglong%20Zhang%20and%20Kecheng%20Zheng%20and%20Jian%20Wang%20and%20Jingdong%20Chen%20and%20Ming%20Yang%0AAbstract%3A%20%20%20This%20paper%20introduces%20Chain-of-Sight%2C%20a%20vision-language%20bridge%20module%20that%0Aaccelerates%20the%20pre-training%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Our%0Aapproach%20employs%20a%20sequence%20of%20visual%20resamplers%20that%20capture%20visual%20details%20at%0Avarious%20spacial%20scales.%20This%20architecture%20not%20only%20leverages%20global%20and%20local%0Avisual%20contexts%20effectively%2C%20but%20also%20facilitates%20the%20flexible%20extension%20of%0Avisual%20tokens%20through%20a%20compound%20token%20scaling%20strategy%2C%20allowing%20up%20to%20a%2016x%0Aincrease%20in%20the%20token%20count%20post%20pre-training.%20Consequently%2C%20Chain-of-Sight%0Arequires%20significantly%20fewer%20visual%20tokens%20in%20the%20pre-training%20phase%20compared%0Ato%20the%20fine-tuning%20phase.%20This%20intentional%20reduction%20of%20visual%20tokens%20during%0Apre-training%20notably%20accelerates%20the%20pre-training%20process%2C%20cutting%20down%20the%0Awall-clock%20training%20time%20by%20~73%25.%20Empirical%20results%20on%20a%20series%20of%0Avision-language%20benchmarks%20reveal%20that%20the%20pre-train%20acceleration%20through%0AChain-of-Sight%20is%20achieved%20without%20sacrificing%20performance%2C%20matching%20or%0Asurpassing%20the%20standard%20pipeline%20of%20utilizing%20all%20visual%20tokens%20throughout%20the%0Aentire%20training%20process.%20Further%20scaling%20up%20the%20number%20of%20visual%20tokens%20for%0Apre-training%20leads%20to%20stronger%20performances%2C%20competitive%20to%20existing%20approaches%0Ain%20a%20series%20of%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Pre-training%2520of%2520Multimodal%2520LLMs%2520via%2520Chain-of-Sight%26entry.906535625%3DZiyuan%2520Huang%2520and%2520Kaixiang%2520Ji%2520and%2520Biao%2520Gong%2520and%2520Zhiwu%2520Qing%2520and%2520Qinglong%2520Zhang%2520and%2520Kecheng%2520Zheng%2520and%2520Jian%2520Wang%2520and%2520Jingdong%2520Chen%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Chain-of-Sight%252C%2520a%2520vision-language%2520bridge%2520module%2520that%250Aaccelerates%2520the%2520pre-training%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520Our%250Aapproach%2520employs%2520a%2520sequence%2520of%2520visual%2520resamplers%2520that%2520capture%2520visual%2520details%2520at%250Avarious%2520spacial%2520scales.%2520This%2520architecture%2520not%2520only%2520leverages%2520global%2520and%2520local%250Avisual%2520contexts%2520effectively%252C%2520but%2520also%2520facilitates%2520the%2520flexible%2520extension%2520of%250Avisual%2520tokens%2520through%2520a%2520compound%2520token%2520scaling%2520strategy%252C%2520allowing%2520up%2520to%2520a%252016x%250Aincrease%2520in%2520the%2520token%2520count%2520post%2520pre-training.%2520Consequently%252C%2520Chain-of-Sight%250Arequires%2520significantly%2520fewer%2520visual%2520tokens%2520in%2520the%2520pre-training%2520phase%2520compared%250Ato%2520the%2520fine-tuning%2520phase.%2520This%2520intentional%2520reduction%2520of%2520visual%2520tokens%2520during%250Apre-training%2520notably%2520accelerates%2520the%2520pre-training%2520process%252C%2520cutting%2520down%2520the%250Awall-clock%2520training%2520time%2520by%2520~73%2525.%2520Empirical%2520results%2520on%2520a%2520series%2520of%250Avision-language%2520benchmarks%2520reveal%2520that%2520the%2520pre-train%2520acceleration%2520through%250AChain-of-Sight%2520is%2520achieved%2520without%2520sacrificing%2520performance%252C%2520matching%2520or%250Asurpassing%2520the%2520standard%2520pipeline%2520of%2520utilizing%2520all%2520visual%2520tokens%2520throughout%2520the%250Aentire%2520training%2520process.%2520Further%2520scaling%2520up%2520the%2520number%2520of%2520visual%2520tokens%2520for%250Apre-training%2520leads%2520to%2520stronger%2520performances%252C%2520competitive%2520to%2520existing%2520approaches%250Ain%2520a%2520series%2520of%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Pre-training%20of%20Multimodal%20LLMs%20via%20Chain-of-Sight&entry.906535625=Ziyuan%20Huang%20and%20Kaixiang%20Ji%20and%20Biao%20Gong%20and%20Zhiwu%20Qing%20and%20Qinglong%20Zhang%20and%20Kecheng%20Zheng%20and%20Jian%20Wang%20and%20Jingdong%20Chen%20and%20Ming%20Yang&entry.1292438233=%20%20This%20paper%20introduces%20Chain-of-Sight%2C%20a%20vision-language%20bridge%20module%20that%0Aaccelerates%20the%20pre-training%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Our%0Aapproach%20employs%20a%20sequence%20of%20visual%20resamplers%20that%20capture%20visual%20details%20at%0Avarious%20spacial%20scales.%20This%20architecture%20not%20only%20leverages%20global%20and%20local%0Avisual%20contexts%20effectively%2C%20but%20also%20facilitates%20the%20flexible%20extension%20of%0Avisual%20tokens%20through%20a%20compound%20token%20scaling%20strategy%2C%20allowing%20up%20to%20a%2016x%0Aincrease%20in%20the%20token%20count%20post%20pre-training.%20Consequently%2C%20Chain-of-Sight%0Arequires%20significantly%20fewer%20visual%20tokens%20in%20the%20pre-training%20phase%20compared%0Ato%20the%20fine-tuning%20phase.%20This%20intentional%20reduction%20of%20visual%20tokens%20during%0Apre-training%20notably%20accelerates%20the%20pre-training%20process%2C%20cutting%20down%20the%0Awall-clock%20training%20time%20by%20~73%25.%20Empirical%20results%20on%20a%20series%20of%0Avision-language%20benchmarks%20reveal%20that%20the%20pre-train%20acceleration%20through%0AChain-of-Sight%20is%20achieved%20without%20sacrificing%20performance%2C%20matching%20or%0Asurpassing%20the%20standard%20pipeline%20of%20utilizing%20all%20visual%20tokens%20throughout%20the%0Aentire%20training%20process.%20Further%20scaling%20up%20the%20number%20of%20visual%20tokens%20for%0Apre-training%20leads%20to%20stronger%20performances%2C%20competitive%20to%20existing%20approaches%0Ain%20a%20series%20of%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15819v1&entry.124074799=Read"},
{"title": "TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly", "author": "Mengqi Guo and Chen Li and Yuyang Zhao and Gim Hee Lee", "abstract": "  Inferring step-wise actions to assemble 3D objects with primitive bricks from\nimages is a challenging task due to complex constraints and the vast number of\npossible combinations. Recent studies have demonstrated promising results on\nsequential LEGO brick assembly through the utilization of LEGO-Graph modeling\nto predict sequential actions. However, existing approaches are class-specific\nand require significant computational and 3D annotation resources. In this\nwork, we first propose a computationally efficient breadth-first search (BFS)\nLEGO-Tree structure to model the sequential assembly actions by considering\nconnections between consecutive layers. Based on the LEGO-Tree structure, we\nthen design a class-agnostic tree-transformer framework to predict the\nsequential assembly actions from the input multi-view images. A major challenge\nof the sequential brick assembly task is that the step-wise action labels are\ncostly and tedious to obtain in practice. We mitigate this problem by\nleveraging synthetic-to-real transfer learning. Specifically, our model is\nfirst pre-trained on synthetic data with full supervision from the available\naction labels. We then circumvent the requirement for action labels in the real\ndata by proposing an action-to-silhouette projection that replaces action\nlabels with input image silhouettes for self-supervision. Without any\nannotation on the real data, our model outperforms existing methods with 3D\nsupervision by 7.8% and 11.3% in mIoU on the MNIST and ModelNet Construction\ndatasets, respectively.\n", "link": "http://arxiv.org/abs/2407.15648v1", "date": "2024-07-22", "relevancy": 2.2263, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5644}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5604}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TreeSBA%3A%20Tree-Transformer%20for%20Self-Supervised%20Sequential%20Brick%20Assembly&body=Title%3A%20TreeSBA%3A%20Tree-Transformer%20for%20Self-Supervised%20Sequential%20Brick%20Assembly%0AAuthor%3A%20Mengqi%20Guo%20and%20Chen%20Li%20and%20Yuyang%20Zhao%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20Inferring%20step-wise%20actions%20to%20assemble%203D%20objects%20with%20primitive%20bricks%20from%0Aimages%20is%20a%20challenging%20task%20due%20to%20complex%20constraints%20and%20the%20vast%20number%20of%0Apossible%20combinations.%20Recent%20studies%20have%20demonstrated%20promising%20results%20on%0Asequential%20LEGO%20brick%20assembly%20through%20the%20utilization%20of%20LEGO-Graph%20modeling%0Ato%20predict%20sequential%20actions.%20However%2C%20existing%20approaches%20are%20class-specific%0Aand%20require%20significant%20computational%20and%203D%20annotation%20resources.%20In%20this%0Awork%2C%20we%20first%20propose%20a%20computationally%20efficient%20breadth-first%20search%20%28BFS%29%0ALEGO-Tree%20structure%20to%20model%20the%20sequential%20assembly%20actions%20by%20considering%0Aconnections%20between%20consecutive%20layers.%20Based%20on%20the%20LEGO-Tree%20structure%2C%20we%0Athen%20design%20a%20class-agnostic%20tree-transformer%20framework%20to%20predict%20the%0Asequential%20assembly%20actions%20from%20the%20input%20multi-view%20images.%20A%20major%20challenge%0Aof%20the%20sequential%20brick%20assembly%20task%20is%20that%20the%20step-wise%20action%20labels%20are%0Acostly%20and%20tedious%20to%20obtain%20in%20practice.%20We%20mitigate%20this%20problem%20by%0Aleveraging%20synthetic-to-real%20transfer%20learning.%20Specifically%2C%20our%20model%20is%0Afirst%20pre-trained%20on%20synthetic%20data%20with%20full%20supervision%20from%20the%20available%0Aaction%20labels.%20We%20then%20circumvent%20the%20requirement%20for%20action%20labels%20in%20the%20real%0Adata%20by%20proposing%20an%20action-to-silhouette%20projection%20that%20replaces%20action%0Alabels%20with%20input%20image%20silhouettes%20for%20self-supervision.%20Without%20any%0Aannotation%20on%20the%20real%20data%2C%20our%20model%20outperforms%20existing%20methods%20with%203D%0Asupervision%20by%207.8%25%20and%2011.3%25%20in%20mIoU%20on%20the%20MNIST%20and%20ModelNet%20Construction%0Adatasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreeSBA%253A%2520Tree-Transformer%2520for%2520Self-Supervised%2520Sequential%2520Brick%2520Assembly%26entry.906535625%3DMengqi%2520Guo%2520and%2520Chen%2520Li%2520and%2520Yuyang%2520Zhao%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520Inferring%2520step-wise%2520actions%2520to%2520assemble%25203D%2520objects%2520with%2520primitive%2520bricks%2520from%250Aimages%2520is%2520a%2520challenging%2520task%2520due%2520to%2520complex%2520constraints%2520and%2520the%2520vast%2520number%2520of%250Apossible%2520combinations.%2520Recent%2520studies%2520have%2520demonstrated%2520promising%2520results%2520on%250Asequential%2520LEGO%2520brick%2520assembly%2520through%2520the%2520utilization%2520of%2520LEGO-Graph%2520modeling%250Ato%2520predict%2520sequential%2520actions.%2520However%252C%2520existing%2520approaches%2520are%2520class-specific%250Aand%2520require%2520significant%2520computational%2520and%25203D%2520annotation%2520resources.%2520In%2520this%250Awork%252C%2520we%2520first%2520propose%2520a%2520computationally%2520efficient%2520breadth-first%2520search%2520%2528BFS%2529%250ALEGO-Tree%2520structure%2520to%2520model%2520the%2520sequential%2520assembly%2520actions%2520by%2520considering%250Aconnections%2520between%2520consecutive%2520layers.%2520Based%2520on%2520the%2520LEGO-Tree%2520structure%252C%2520we%250Athen%2520design%2520a%2520class-agnostic%2520tree-transformer%2520framework%2520to%2520predict%2520the%250Asequential%2520assembly%2520actions%2520from%2520the%2520input%2520multi-view%2520images.%2520A%2520major%2520challenge%250Aof%2520the%2520sequential%2520brick%2520assembly%2520task%2520is%2520that%2520the%2520step-wise%2520action%2520labels%2520are%250Acostly%2520and%2520tedious%2520to%2520obtain%2520in%2520practice.%2520We%2520mitigate%2520this%2520problem%2520by%250Aleveraging%2520synthetic-to-real%2520transfer%2520learning.%2520Specifically%252C%2520our%2520model%2520is%250Afirst%2520pre-trained%2520on%2520synthetic%2520data%2520with%2520full%2520supervision%2520from%2520the%2520available%250Aaction%2520labels.%2520We%2520then%2520circumvent%2520the%2520requirement%2520for%2520action%2520labels%2520in%2520the%2520real%250Adata%2520by%2520proposing%2520an%2520action-to-silhouette%2520projection%2520that%2520replaces%2520action%250Alabels%2520with%2520input%2520image%2520silhouettes%2520for%2520self-supervision.%2520Without%2520any%250Aannotation%2520on%2520the%2520real%2520data%252C%2520our%2520model%2520outperforms%2520existing%2520methods%2520with%25203D%250Asupervision%2520by%25207.8%2525%2520and%252011.3%2525%2520in%2520mIoU%2520on%2520the%2520MNIST%2520and%2520ModelNet%2520Construction%250Adatasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TreeSBA%3A%20Tree-Transformer%20for%20Self-Supervised%20Sequential%20Brick%20Assembly&entry.906535625=Mengqi%20Guo%20and%20Chen%20Li%20and%20Yuyang%20Zhao%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20Inferring%20step-wise%20actions%20to%20assemble%203D%20objects%20with%20primitive%20bricks%20from%0Aimages%20is%20a%20challenging%20task%20due%20to%20complex%20constraints%20and%20the%20vast%20number%20of%0Apossible%20combinations.%20Recent%20studies%20have%20demonstrated%20promising%20results%20on%0Asequential%20LEGO%20brick%20assembly%20through%20the%20utilization%20of%20LEGO-Graph%20modeling%0Ato%20predict%20sequential%20actions.%20However%2C%20existing%20approaches%20are%20class-specific%0Aand%20require%20significant%20computational%20and%203D%20annotation%20resources.%20In%20this%0Awork%2C%20we%20first%20propose%20a%20computationally%20efficient%20breadth-first%20search%20%28BFS%29%0ALEGO-Tree%20structure%20to%20model%20the%20sequential%20assembly%20actions%20by%20considering%0Aconnections%20between%20consecutive%20layers.%20Based%20on%20the%20LEGO-Tree%20structure%2C%20we%0Athen%20design%20a%20class-agnostic%20tree-transformer%20framework%20to%20predict%20the%0Asequential%20assembly%20actions%20from%20the%20input%20multi-view%20images.%20A%20major%20challenge%0Aof%20the%20sequential%20brick%20assembly%20task%20is%20that%20the%20step-wise%20action%20labels%20are%0Acostly%20and%20tedious%20to%20obtain%20in%20practice.%20We%20mitigate%20this%20problem%20by%0Aleveraging%20synthetic-to-real%20transfer%20learning.%20Specifically%2C%20our%20model%20is%0Afirst%20pre-trained%20on%20synthetic%20data%20with%20full%20supervision%20from%20the%20available%0Aaction%20labels.%20We%20then%20circumvent%20the%20requirement%20for%20action%20labels%20in%20the%20real%0Adata%20by%20proposing%20an%20action-to-silhouette%20projection%20that%20replaces%20action%0Alabels%20with%20input%20image%20silhouettes%20for%20self-supervision.%20Without%20any%0Aannotation%20on%20the%20real%20data%2C%20our%20model%20outperforms%20existing%20methods%20with%203D%0Asupervision%20by%207.8%25%20and%2011.3%25%20in%20mIoU%20on%20the%20MNIST%20and%20ModelNet%20Construction%0Adatasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15648v1&entry.124074799=Read"},
{"title": "CarFormer: Self-Driving with Learned Object-Centric Representations", "author": "Shadi Hamdan and Fatma G\u00fcney", "abstract": "  The choice of representation plays a key role in self-driving. Bird's eye\nview (BEV) representations have shown remarkable performance in recent years.\nIn this paper, we propose to learn object-centric representations in BEV to\ndistill a complex scene into more actionable information for self-driving. We\nfirst learn to place objects into slots with a slot attention model on BEV\nsequences. Based on these object-centric representations, we then train a\ntransformer to learn to drive as well as reason about the future of other\nvehicles. We found that object-centric slot representations outperform both\nscene-level and object-level approaches that use the exact attributes of\nobjects. Slot representations naturally incorporate information about objects\nfrom their spatial and temporal context such as position, heading, and speed\nwithout explicitly providing it. Our model with slots achieves an increased\ncompletion rate of the provided routes and, consequently, a higher driving\nscore, with a lower variance across multiple runs, affirming slots as a\nreliable alternative in object-centric approaches. Additionally, we validate\nour model's performance as a world model through forecasting experiments,\ndemonstrating its capability to predict future slot representations accurately.\nThe code and the pre-trained models can be found at\nhttps://kuis-ai.github.io/CarFormer/.\n", "link": "http://arxiv.org/abs/2407.15843v1", "date": "2024-07-22", "relevancy": 2.2223, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5635}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5592}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CarFormer%3A%20Self-Driving%20with%20Learned%20Object-Centric%20Representations&body=Title%3A%20CarFormer%3A%20Self-Driving%20with%20Learned%20Object-Centric%20Representations%0AAuthor%3A%20Shadi%20Hamdan%20and%20Fatma%20G%C3%BCney%0AAbstract%3A%20%20%20The%20choice%20of%20representation%20plays%20a%20key%20role%20in%20self-driving.%20Bird%27s%20eye%0Aview%20%28BEV%29%20representations%20have%20shown%20remarkable%20performance%20in%20recent%20years.%0AIn%20this%20paper%2C%20we%20propose%20to%20learn%20object-centric%20representations%20in%20BEV%20to%0Adistill%20a%20complex%20scene%20into%20more%20actionable%20information%20for%20self-driving.%20We%0Afirst%20learn%20to%20place%20objects%20into%20slots%20with%20a%20slot%20attention%20model%20on%20BEV%0Asequences.%20Based%20on%20these%20object-centric%20representations%2C%20we%20then%20train%20a%0Atransformer%20to%20learn%20to%20drive%20as%20well%20as%20reason%20about%20the%20future%20of%20other%0Avehicles.%20We%20found%20that%20object-centric%20slot%20representations%20outperform%20both%0Ascene-level%20and%20object-level%20approaches%20that%20use%20the%20exact%20attributes%20of%0Aobjects.%20Slot%20representations%20naturally%20incorporate%20information%20about%20objects%0Afrom%20their%20spatial%20and%20temporal%20context%20such%20as%20position%2C%20heading%2C%20and%20speed%0Awithout%20explicitly%20providing%20it.%20Our%20model%20with%20slots%20achieves%20an%20increased%0Acompletion%20rate%20of%20the%20provided%20routes%20and%2C%20consequently%2C%20a%20higher%20driving%0Ascore%2C%20with%20a%20lower%20variance%20across%20multiple%20runs%2C%20affirming%20slots%20as%20a%0Areliable%20alternative%20in%20object-centric%20approaches.%20Additionally%2C%20we%20validate%0Aour%20model%27s%20performance%20as%20a%20world%20model%20through%20forecasting%20experiments%2C%0Ademonstrating%20its%20capability%20to%20predict%20future%20slot%20representations%20accurately.%0AThe%20code%20and%20the%20pre-trained%20models%20can%20be%20found%20at%0Ahttps%3A//kuis-ai.github.io/CarFormer/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarFormer%253A%2520Self-Driving%2520with%2520Learned%2520Object-Centric%2520Representations%26entry.906535625%3DShadi%2520Hamdan%2520and%2520Fatma%2520G%25C3%25BCney%26entry.1292438233%3D%2520%2520The%2520choice%2520of%2520representation%2520plays%2520a%2520key%2520role%2520in%2520self-driving.%2520Bird%2527s%2520eye%250Aview%2520%2528BEV%2529%2520representations%2520have%2520shown%2520remarkable%2520performance%2520in%2520recent%2520years.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520to%2520learn%2520object-centric%2520representations%2520in%2520BEV%2520to%250Adistill%2520a%2520complex%2520scene%2520into%2520more%2520actionable%2520information%2520for%2520self-driving.%2520We%250Afirst%2520learn%2520to%2520place%2520objects%2520into%2520slots%2520with%2520a%2520slot%2520attention%2520model%2520on%2520BEV%250Asequences.%2520Based%2520on%2520these%2520object-centric%2520representations%252C%2520we%2520then%2520train%2520a%250Atransformer%2520to%2520learn%2520to%2520drive%2520as%2520well%2520as%2520reason%2520about%2520the%2520future%2520of%2520other%250Avehicles.%2520We%2520found%2520that%2520object-centric%2520slot%2520representations%2520outperform%2520both%250Ascene-level%2520and%2520object-level%2520approaches%2520that%2520use%2520the%2520exact%2520attributes%2520of%250Aobjects.%2520Slot%2520representations%2520naturally%2520incorporate%2520information%2520about%2520objects%250Afrom%2520their%2520spatial%2520and%2520temporal%2520context%2520such%2520as%2520position%252C%2520heading%252C%2520and%2520speed%250Awithout%2520explicitly%2520providing%2520it.%2520Our%2520model%2520with%2520slots%2520achieves%2520an%2520increased%250Acompletion%2520rate%2520of%2520the%2520provided%2520routes%2520and%252C%2520consequently%252C%2520a%2520higher%2520driving%250Ascore%252C%2520with%2520a%2520lower%2520variance%2520across%2520multiple%2520runs%252C%2520affirming%2520slots%2520as%2520a%250Areliable%2520alternative%2520in%2520object-centric%2520approaches.%2520Additionally%252C%2520we%2520validate%250Aour%2520model%2527s%2520performance%2520as%2520a%2520world%2520model%2520through%2520forecasting%2520experiments%252C%250Ademonstrating%2520its%2520capability%2520to%2520predict%2520future%2520slot%2520representations%2520accurately.%250AThe%2520code%2520and%2520the%2520pre-trained%2520models%2520can%2520be%2520found%2520at%250Ahttps%253A//kuis-ai.github.io/CarFormer/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CarFormer%3A%20Self-Driving%20with%20Learned%20Object-Centric%20Representations&entry.906535625=Shadi%20Hamdan%20and%20Fatma%20G%C3%BCney&entry.1292438233=%20%20The%20choice%20of%20representation%20plays%20a%20key%20role%20in%20self-driving.%20Bird%27s%20eye%0Aview%20%28BEV%29%20representations%20have%20shown%20remarkable%20performance%20in%20recent%20years.%0AIn%20this%20paper%2C%20we%20propose%20to%20learn%20object-centric%20representations%20in%20BEV%20to%0Adistill%20a%20complex%20scene%20into%20more%20actionable%20information%20for%20self-driving.%20We%0Afirst%20learn%20to%20place%20objects%20into%20slots%20with%20a%20slot%20attention%20model%20on%20BEV%0Asequences.%20Based%20on%20these%20object-centric%20representations%2C%20we%20then%20train%20a%0Atransformer%20to%20learn%20to%20drive%20as%20well%20as%20reason%20about%20the%20future%20of%20other%0Avehicles.%20We%20found%20that%20object-centric%20slot%20representations%20outperform%20both%0Ascene-level%20and%20object-level%20approaches%20that%20use%20the%20exact%20attributes%20of%0Aobjects.%20Slot%20representations%20naturally%20incorporate%20information%20about%20objects%0Afrom%20their%20spatial%20and%20temporal%20context%20such%20as%20position%2C%20heading%2C%20and%20speed%0Awithout%20explicitly%20providing%20it.%20Our%20model%20with%20slots%20achieves%20an%20increased%0Acompletion%20rate%20of%20the%20provided%20routes%20and%2C%20consequently%2C%20a%20higher%20driving%0Ascore%2C%20with%20a%20lower%20variance%20across%20multiple%20runs%2C%20affirming%20slots%20as%20a%0Areliable%20alternative%20in%20object-centric%20approaches.%20Additionally%2C%20we%20validate%0Aour%20model%27s%20performance%20as%20a%20world%20model%20through%20forecasting%20experiments%2C%0Ademonstrating%20its%20capability%20to%20predict%20future%20slot%20representations%20accurately.%0AThe%20code%20and%20the%20pre-trained%20models%20can%20be%20found%20at%0Ahttps%3A//kuis-ai.github.io/CarFormer/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15843v1&entry.124074799=Read"},
{"title": "BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis\n  in Large-scale Scenes", "author": "Chih-Hai Su and Chih-Yao Hu and Shr-Ruei Tsai and Jie-Ying Lee and Chin-Yang Lin and Yu-Lun Liu", "abstract": "  While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality,\ntheir protracted training duration remains a limitation. Generalizable and\nMVS-based NeRFs, although capable of mitigating training time, often incur\ntradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFs\nto enhance the rendering quality of MVS-based NeRFs in large-scale scenes. We\nfirst identify limitations in MVS-based NeRF methods, such as restricted\nviewport coverage and artifacts due to limited input views. Then, we address\nthese limitations by proposing a new method that selects and combines multiple\ncost volumes during volume rendering. Our method does not require training and\ncan adapt to any MVS-based NeRF methods in a feed-forward fashion to improve\nrendering quality. Furthermore, our approach is also end-to-end trainable,\nallowing fine-tuning on specific scenes. We demonstrate the effectiveness of\nour method through experiments on large-scale datasets, showing significant\nrendering quality improvements in large-scale scenes and unbounded outdoor\nscenarios. We release the source code of BoostMVSNeRFs at\nhttps://su-terry.github.io/BoostMVSNeRFs/.\n", "link": "http://arxiv.org/abs/2407.15848v1", "date": "2024-07-22", "relevancy": 2.1996, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5594}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5432}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoostMVSNeRFs%3A%20Boosting%20MVS-based%20NeRFs%20to%20Generalizable%20View%20Synthesis%0A%20%20in%20Large-scale%20Scenes&body=Title%3A%20BoostMVSNeRFs%3A%20Boosting%20MVS-based%20NeRFs%20to%20Generalizable%20View%20Synthesis%0A%20%20in%20Large-scale%20Scenes%0AAuthor%3A%20Chih-Hai%20Su%20and%20Chih-Yao%20Hu%20and%20Shr-Ruei%20Tsai%20and%20Jie-Ying%20Lee%20and%20Chin-Yang%20Lin%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20While%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20demonstrated%20exceptional%20quality%2C%0Atheir%20protracted%20training%20duration%20remains%20a%20limitation.%20Generalizable%20and%0AMVS-based%20NeRFs%2C%20although%20capable%20of%20mitigating%20training%20time%2C%20often%20incur%0Atradeoffs%20in%20quality.%20This%20paper%20presents%20a%20novel%20approach%20called%20BoostMVSNeRFs%0Ato%20enhance%20the%20rendering%20quality%20of%20MVS-based%20NeRFs%20in%20large-scale%20scenes.%20We%0Afirst%20identify%20limitations%20in%20MVS-based%20NeRF%20methods%2C%20such%20as%20restricted%0Aviewport%20coverage%20and%20artifacts%20due%20to%20limited%20input%20views.%20Then%2C%20we%20address%0Athese%20limitations%20by%20proposing%20a%20new%20method%20that%20selects%20and%20combines%20multiple%0Acost%20volumes%20during%20volume%20rendering.%20Our%20method%20does%20not%20require%20training%20and%0Acan%20adapt%20to%20any%20MVS-based%20NeRF%20methods%20in%20a%20feed-forward%20fashion%20to%20improve%0Arendering%20quality.%20Furthermore%2C%20our%20approach%20is%20also%20end-to-end%20trainable%2C%0Aallowing%20fine-tuning%20on%20specific%20scenes.%20We%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20through%20experiments%20on%20large-scale%20datasets%2C%20showing%20significant%0Arendering%20quality%20improvements%20in%20large-scale%20scenes%20and%20unbounded%20outdoor%0Ascenarios.%20We%20release%20the%20source%20code%20of%20BoostMVSNeRFs%20at%0Ahttps%3A//su-terry.github.io/BoostMVSNeRFs/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoostMVSNeRFs%253A%2520Boosting%2520MVS-based%2520NeRFs%2520to%2520Generalizable%2520View%2520Synthesis%250A%2520%2520in%2520Large-scale%2520Scenes%26entry.906535625%3DChih-Hai%2520Su%2520and%2520Chih-Yao%2520Hu%2520and%2520Shr-Ruei%2520Tsai%2520and%2520Jie-Ying%2520Lee%2520and%2520Chin-Yang%2520Lin%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520While%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520demonstrated%2520exceptional%2520quality%252C%250Atheir%2520protracted%2520training%2520duration%2520remains%2520a%2520limitation.%2520Generalizable%2520and%250AMVS-based%2520NeRFs%252C%2520although%2520capable%2520of%2520mitigating%2520training%2520time%252C%2520often%2520incur%250Atradeoffs%2520in%2520quality.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520called%2520BoostMVSNeRFs%250Ato%2520enhance%2520the%2520rendering%2520quality%2520of%2520MVS-based%2520NeRFs%2520in%2520large-scale%2520scenes.%2520We%250Afirst%2520identify%2520limitations%2520in%2520MVS-based%2520NeRF%2520methods%252C%2520such%2520as%2520restricted%250Aviewport%2520coverage%2520and%2520artifacts%2520due%2520to%2520limited%2520input%2520views.%2520Then%252C%2520we%2520address%250Athese%2520limitations%2520by%2520proposing%2520a%2520new%2520method%2520that%2520selects%2520and%2520combines%2520multiple%250Acost%2520volumes%2520during%2520volume%2520rendering.%2520Our%2520method%2520does%2520not%2520require%2520training%2520and%250Acan%2520adapt%2520to%2520any%2520MVS-based%2520NeRF%2520methods%2520in%2520a%2520feed-forward%2520fashion%2520to%2520improve%250Arendering%2520quality.%2520Furthermore%252C%2520our%2520approach%2520is%2520also%2520end-to-end%2520trainable%252C%250Aallowing%2520fine-tuning%2520on%2520specific%2520scenes.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520method%2520through%2520experiments%2520on%2520large-scale%2520datasets%252C%2520showing%2520significant%250Arendering%2520quality%2520improvements%2520in%2520large-scale%2520scenes%2520and%2520unbounded%2520outdoor%250Ascenarios.%2520We%2520release%2520the%2520source%2520code%2520of%2520BoostMVSNeRFs%2520at%250Ahttps%253A//su-terry.github.io/BoostMVSNeRFs/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoostMVSNeRFs%3A%20Boosting%20MVS-based%20NeRFs%20to%20Generalizable%20View%20Synthesis%0A%20%20in%20Large-scale%20Scenes&entry.906535625=Chih-Hai%20Su%20and%20Chih-Yao%20Hu%20and%20Shr-Ruei%20Tsai%20and%20Jie-Ying%20Lee%20and%20Chin-Yang%20Lin%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20While%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20demonstrated%20exceptional%20quality%2C%0Atheir%20protracted%20training%20duration%20remains%20a%20limitation.%20Generalizable%20and%0AMVS-based%20NeRFs%2C%20although%20capable%20of%20mitigating%20training%20time%2C%20often%20incur%0Atradeoffs%20in%20quality.%20This%20paper%20presents%20a%20novel%20approach%20called%20BoostMVSNeRFs%0Ato%20enhance%20the%20rendering%20quality%20of%20MVS-based%20NeRFs%20in%20large-scale%20scenes.%20We%0Afirst%20identify%20limitations%20in%20MVS-based%20NeRF%20methods%2C%20such%20as%20restricted%0Aviewport%20coverage%20and%20artifacts%20due%20to%20limited%20input%20views.%20Then%2C%20we%20address%0Athese%20limitations%20by%20proposing%20a%20new%20method%20that%20selects%20and%20combines%20multiple%0Acost%20volumes%20during%20volume%20rendering.%20Our%20method%20does%20not%20require%20training%20and%0Acan%20adapt%20to%20any%20MVS-based%20NeRF%20methods%20in%20a%20feed-forward%20fashion%20to%20improve%0Arendering%20quality.%20Furthermore%2C%20our%20approach%20is%20also%20end-to-end%20trainable%2C%0Aallowing%20fine-tuning%20on%20specific%20scenes.%20We%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20through%20experiments%20on%20large-scale%20datasets%2C%20showing%20significant%0Arendering%20quality%20improvements%20in%20large-scale%20scenes%20and%20unbounded%20outdoor%0Ascenarios.%20We%20release%20the%20source%20code%20of%20BoostMVSNeRFs%20at%0Ahttps%3A//su-terry.github.io/BoostMVSNeRFs/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15848v1&entry.124074799=Read"},
{"title": "REVEAL-IT: REinforcement learning with Visibility of Evolving Agent\n  poLicy for InTerpretability", "author": "Shuang Ao and Simon Khan and Haris Aziz and Flora D. Salim", "abstract": "  Understanding the agent's learning process, particularly the factors that\ncontribute to its success or failure post-training, is crucial for\ncomprehending the rationale behind the agent's decision-making process. Prior\nmethods clarify the learning process by creating a structural causal model\n(SCM) or visually representing the distribution of value functions.\nNevertheless, these approaches have constraints as they exclusively function in\n2D-environments or with uncomplicated transition dynamics. Understanding the\nagent's learning process in complicated environments or tasks is more\nchallenging. In this paper, we propose REVEAL-IT, a novel framework for\nexplaining the learning process of an agent in complex environments. Initially,\nwe visualize the policy structure and the agent's learning process for various\ntraining tasks. By visualizing these findings, we can understand how much a\nparticular training task or stage affects the agent's performance in test.\nThen, a GNN-based explainer learns to highlight the most important section of\nthe policy, providing a more clear and robust explanation of the agent's\nlearning process. The experiments demonstrate that explanations derived from\nthis framework can effectively help in the optimization of the training tasks,\nresulting in improved learning efficiency and final performance.\n", "link": "http://arxiv.org/abs/2406.14214v5", "date": "2024-07-22", "relevancy": 2.192, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.551}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5487}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability&body=Title%3A%20REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability%0AAuthor%3A%20Shuang%20Ao%20and%20Simon%20Khan%20and%20Haris%20Aziz%20and%20Flora%20D.%20Salim%0AAbstract%3A%20%20%20Understanding%20the%20agent%27s%20learning%20process%2C%20particularly%20the%20factors%20that%0Acontribute%20to%20its%20success%20or%20failure%20post-training%2C%20is%20crucial%20for%0Acomprehending%20the%20rationale%20behind%20the%20agent%27s%20decision-making%20process.%20Prior%0Amethods%20clarify%20the%20learning%20process%20by%20creating%20a%20structural%20causal%20model%0A%28SCM%29%20or%20visually%20representing%20the%20distribution%20of%20value%20functions.%0ANevertheless%2C%20these%20approaches%20have%20constraints%20as%20they%20exclusively%20function%20in%0A2D-environments%20or%20with%20uncomplicated%20transition%20dynamics.%20Understanding%20the%0Aagent%27s%20learning%20process%20in%20complicated%20environments%20or%20tasks%20is%20more%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20REVEAL-IT%2C%20a%20novel%20framework%20for%0Aexplaining%20the%20learning%20process%20of%20an%20agent%20in%20complex%20environments.%20Initially%2C%0Awe%20visualize%20the%20policy%20structure%20and%20the%20agent%27s%20learning%20process%20for%20various%0Atraining%20tasks.%20By%20visualizing%20these%20findings%2C%20we%20can%20understand%20how%20much%20a%0Aparticular%20training%20task%20or%20stage%20affects%20the%20agent%27s%20performance%20in%20test.%0AThen%2C%20a%20GNN-based%20explainer%20learns%20to%20highlight%20the%20most%20important%20section%20of%0Athe%20policy%2C%20providing%20a%20more%20clear%20and%20robust%20explanation%20of%20the%20agent%27s%0Alearning%20process.%20The%20experiments%20demonstrate%20that%20explanations%20derived%20from%0Athis%20framework%20can%20effectively%20help%20in%20the%20optimization%20of%20the%20training%20tasks%2C%0Aresulting%20in%20improved%20learning%20efficiency%20and%20final%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14214v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREVEAL-IT%253A%2520REinforcement%2520learning%2520with%2520Visibility%2520of%2520Evolving%2520Agent%250A%2520%2520poLicy%2520for%2520InTerpretability%26entry.906535625%3DShuang%2520Ao%2520and%2520Simon%2520Khan%2520and%2520Haris%2520Aziz%2520and%2520Flora%2520D.%2520Salim%26entry.1292438233%3D%2520%2520Understanding%2520the%2520agent%2527s%2520learning%2520process%252C%2520particularly%2520the%2520factors%2520that%250Acontribute%2520to%2520its%2520success%2520or%2520failure%2520post-training%252C%2520is%2520crucial%2520for%250Acomprehending%2520the%2520rationale%2520behind%2520the%2520agent%2527s%2520decision-making%2520process.%2520Prior%250Amethods%2520clarify%2520the%2520learning%2520process%2520by%2520creating%2520a%2520structural%2520causal%2520model%250A%2528SCM%2529%2520or%2520visually%2520representing%2520the%2520distribution%2520of%2520value%2520functions.%250ANevertheless%252C%2520these%2520approaches%2520have%2520constraints%2520as%2520they%2520exclusively%2520function%2520in%250A2D-environments%2520or%2520with%2520uncomplicated%2520transition%2520dynamics.%2520Understanding%2520the%250Aagent%2527s%2520learning%2520process%2520in%2520complicated%2520environments%2520or%2520tasks%2520is%2520more%250Achallenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520REVEAL-IT%252C%2520a%2520novel%2520framework%2520for%250Aexplaining%2520the%2520learning%2520process%2520of%2520an%2520agent%2520in%2520complex%2520environments.%2520Initially%252C%250Awe%2520visualize%2520the%2520policy%2520structure%2520and%2520the%2520agent%2527s%2520learning%2520process%2520for%2520various%250Atraining%2520tasks.%2520By%2520visualizing%2520these%2520findings%252C%2520we%2520can%2520understand%2520how%2520much%2520a%250Aparticular%2520training%2520task%2520or%2520stage%2520affects%2520the%2520agent%2527s%2520performance%2520in%2520test.%250AThen%252C%2520a%2520GNN-based%2520explainer%2520learns%2520to%2520highlight%2520the%2520most%2520important%2520section%2520of%250Athe%2520policy%252C%2520providing%2520a%2520more%2520clear%2520and%2520robust%2520explanation%2520of%2520the%2520agent%2527s%250Alearning%2520process.%2520The%2520experiments%2520demonstrate%2520that%2520explanations%2520derived%2520from%250Athis%2520framework%2520can%2520effectively%2520help%2520in%2520the%2520optimization%2520of%2520the%2520training%2520tasks%252C%250Aresulting%2520in%2520improved%2520learning%2520efficiency%2520and%2520final%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14214v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability&entry.906535625=Shuang%20Ao%20and%20Simon%20Khan%20and%20Haris%20Aziz%20and%20Flora%20D.%20Salim&entry.1292438233=%20%20Understanding%20the%20agent%27s%20learning%20process%2C%20particularly%20the%20factors%20that%0Acontribute%20to%20its%20success%20or%20failure%20post-training%2C%20is%20crucial%20for%0Acomprehending%20the%20rationale%20behind%20the%20agent%27s%20decision-making%20process.%20Prior%0Amethods%20clarify%20the%20learning%20process%20by%20creating%20a%20structural%20causal%20model%0A%28SCM%29%20or%20visually%20representing%20the%20distribution%20of%20value%20functions.%0ANevertheless%2C%20these%20approaches%20have%20constraints%20as%20they%20exclusively%20function%20in%0A2D-environments%20or%20with%20uncomplicated%20transition%20dynamics.%20Understanding%20the%0Aagent%27s%20learning%20process%20in%20complicated%20environments%20or%20tasks%20is%20more%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20REVEAL-IT%2C%20a%20novel%20framework%20for%0Aexplaining%20the%20learning%20process%20of%20an%20agent%20in%20complex%20environments.%20Initially%2C%0Awe%20visualize%20the%20policy%20structure%20and%20the%20agent%27s%20learning%20process%20for%20various%0Atraining%20tasks.%20By%20visualizing%20these%20findings%2C%20we%20can%20understand%20how%20much%20a%0Aparticular%20training%20task%20or%20stage%20affects%20the%20agent%27s%20performance%20in%20test.%0AThen%2C%20a%20GNN-based%20explainer%20learns%20to%20highlight%20the%20most%20important%20section%20of%0Athe%20policy%2C%20providing%20a%20more%20clear%20and%20robust%20explanation%20of%20the%20agent%27s%0Alearning%20process.%20The%20experiments%20demonstrate%20that%20explanations%20derived%20from%0Athis%20framework%20can%20effectively%20help%20in%20the%20optimization%20of%20the%20training%20tasks%2C%0Aresulting%20in%20improved%20learning%20efficiency%20and%20final%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14214v5&entry.124074799=Read"},
{"title": "TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON", "author": "John Chong Min Tan and Prince Saroj and Bharat Runwal and Hardik Maheshwari and Brian Lim Yi Sheng and Richard Cottrill and Alankrit Chona and Ambuj Kumar and Mehul Motani", "abstract": "  TaskGen is an open-sourced agentic framework which uses an Agent to solve an\narbitrary task by breaking them down into subtasks. Each subtask is mapped to\nan Equipped Function or another Agent to execute. In order to reduce verbosity\n(and hence token usage), TaskGen uses StrictJSON that ensures JSON output from\nthe Large Language Model (LLM), along with additional features such as type\nchecking and iterative error correction. Key to the philosophy of TaskGen is\nthe management of information/memory on a need-to-know basis. We empirically\nevaluate TaskGen on various environments such as 40x40 dynamic maze navigation\nwith changing obstacle locations (100% solve rate), TextWorld escape room\nsolving with dense rewards and detailed goals (96% solve rate), web browsing\n(69% of actions successful), solving the MATH dataset (71% solve rate over 100\nLevel-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset\n(F1 score of 47.03%)\n", "link": "http://arxiv.org/abs/2407.15734v1", "date": "2024-07-22", "relevancy": 2.184, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.595}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5643}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaskGen%3A%20A%20Task-Based%2C%20Memory-Infused%20Agentic%20Framework%20using%20StrictJSON&body=Title%3A%20TaskGen%3A%20A%20Task-Based%2C%20Memory-Infused%20Agentic%20Framework%20using%20StrictJSON%0AAuthor%3A%20John%20Chong%20Min%20Tan%20and%20Prince%20Saroj%20and%20Bharat%20Runwal%20and%20Hardik%20Maheshwari%20and%20Brian%20Lim%20Yi%20Sheng%20and%20Richard%20Cottrill%20and%20Alankrit%20Chona%20and%20Ambuj%20Kumar%20and%20Mehul%20Motani%0AAbstract%3A%20%20%20TaskGen%20is%20an%20open-sourced%20agentic%20framework%20which%20uses%20an%20Agent%20to%20solve%20an%0Aarbitrary%20task%20by%20breaking%20them%20down%20into%20subtasks.%20Each%20subtask%20is%20mapped%20to%0Aan%20Equipped%20Function%20or%20another%20Agent%20to%20execute.%20In%20order%20to%20reduce%20verbosity%0A%28and%20hence%20token%20usage%29%2C%20TaskGen%20uses%20StrictJSON%20that%20ensures%20JSON%20output%20from%0Athe%20Large%20Language%20Model%20%28LLM%29%2C%20along%20with%20additional%20features%20such%20as%20type%0Achecking%20and%20iterative%20error%20correction.%20Key%20to%20the%20philosophy%20of%20TaskGen%20is%0Athe%20management%20of%20information/memory%20on%20a%20need-to-know%20basis.%20We%20empirically%0Aevaluate%20TaskGen%20on%20various%20environments%20such%20as%2040x40%20dynamic%20maze%20navigation%0Awith%20changing%20obstacle%20locations%20%28100%25%20solve%20rate%29%2C%20TextWorld%20escape%20room%0Asolving%20with%20dense%20rewards%20and%20detailed%20goals%20%2896%25%20solve%20rate%29%2C%20web%20browsing%0A%2869%25%20of%20actions%20successful%29%2C%20solving%20the%20MATH%20dataset%20%2871%25%20solve%20rate%20over%20100%0ALevel-5%20problems%29%2C%20Retrieval%20Augmented%20Generation%20on%20NaturalQuestions%20dataset%0A%28F1%20score%20of%2047.03%25%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaskGen%253A%2520A%2520Task-Based%252C%2520Memory-Infused%2520Agentic%2520Framework%2520using%2520StrictJSON%26entry.906535625%3DJohn%2520Chong%2520Min%2520Tan%2520and%2520Prince%2520Saroj%2520and%2520Bharat%2520Runwal%2520and%2520Hardik%2520Maheshwari%2520and%2520Brian%2520Lim%2520Yi%2520Sheng%2520and%2520Richard%2520Cottrill%2520and%2520Alankrit%2520Chona%2520and%2520Ambuj%2520Kumar%2520and%2520Mehul%2520Motani%26entry.1292438233%3D%2520%2520TaskGen%2520is%2520an%2520open-sourced%2520agentic%2520framework%2520which%2520uses%2520an%2520Agent%2520to%2520solve%2520an%250Aarbitrary%2520task%2520by%2520breaking%2520them%2520down%2520into%2520subtasks.%2520Each%2520subtask%2520is%2520mapped%2520to%250Aan%2520Equipped%2520Function%2520or%2520another%2520Agent%2520to%2520execute.%2520In%2520order%2520to%2520reduce%2520verbosity%250A%2528and%2520hence%2520token%2520usage%2529%252C%2520TaskGen%2520uses%2520StrictJSON%2520that%2520ensures%2520JSON%2520output%2520from%250Athe%2520Large%2520Language%2520Model%2520%2528LLM%2529%252C%2520along%2520with%2520additional%2520features%2520such%2520as%2520type%250Achecking%2520and%2520iterative%2520error%2520correction.%2520Key%2520to%2520the%2520philosophy%2520of%2520TaskGen%2520is%250Athe%2520management%2520of%2520information/memory%2520on%2520a%2520need-to-know%2520basis.%2520We%2520empirically%250Aevaluate%2520TaskGen%2520on%2520various%2520environments%2520such%2520as%252040x40%2520dynamic%2520maze%2520navigation%250Awith%2520changing%2520obstacle%2520locations%2520%2528100%2525%2520solve%2520rate%2529%252C%2520TextWorld%2520escape%2520room%250Asolving%2520with%2520dense%2520rewards%2520and%2520detailed%2520goals%2520%252896%2525%2520solve%2520rate%2529%252C%2520web%2520browsing%250A%252869%2525%2520of%2520actions%2520successful%2529%252C%2520solving%2520the%2520MATH%2520dataset%2520%252871%2525%2520solve%2520rate%2520over%2520100%250ALevel-5%2520problems%2529%252C%2520Retrieval%2520Augmented%2520Generation%2520on%2520NaturalQuestions%2520dataset%250A%2528F1%2520score%2520of%252047.03%2525%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaskGen%3A%20A%20Task-Based%2C%20Memory-Infused%20Agentic%20Framework%20using%20StrictJSON&entry.906535625=John%20Chong%20Min%20Tan%20and%20Prince%20Saroj%20and%20Bharat%20Runwal%20and%20Hardik%20Maheshwari%20and%20Brian%20Lim%20Yi%20Sheng%20and%20Richard%20Cottrill%20and%20Alankrit%20Chona%20and%20Ambuj%20Kumar%20and%20Mehul%20Motani&entry.1292438233=%20%20TaskGen%20is%20an%20open-sourced%20agentic%20framework%20which%20uses%20an%20Agent%20to%20solve%20an%0Aarbitrary%20task%20by%20breaking%20them%20down%20into%20subtasks.%20Each%20subtask%20is%20mapped%20to%0Aan%20Equipped%20Function%20or%20another%20Agent%20to%20execute.%20In%20order%20to%20reduce%20verbosity%0A%28and%20hence%20token%20usage%29%2C%20TaskGen%20uses%20StrictJSON%20that%20ensures%20JSON%20output%20from%0Athe%20Large%20Language%20Model%20%28LLM%29%2C%20along%20with%20additional%20features%20such%20as%20type%0Achecking%20and%20iterative%20error%20correction.%20Key%20to%20the%20philosophy%20of%20TaskGen%20is%0Athe%20management%20of%20information/memory%20on%20a%20need-to-know%20basis.%20We%20empirically%0Aevaluate%20TaskGen%20on%20various%20environments%20such%20as%2040x40%20dynamic%20maze%20navigation%0Awith%20changing%20obstacle%20locations%20%28100%25%20solve%20rate%29%2C%20TextWorld%20escape%20room%0Asolving%20with%20dense%20rewards%20and%20detailed%20goals%20%2896%25%20solve%20rate%29%2C%20web%20browsing%0A%2869%25%20of%20actions%20successful%29%2C%20solving%20the%20MATH%20dataset%20%2871%25%20solve%20rate%20over%20100%0ALevel-5%20problems%29%2C%20Retrieval%20Augmented%20Generation%20on%20NaturalQuestions%20dataset%0A%28F1%20score%20of%2047.03%25%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15734v1&entry.124074799=Read"},
{"title": "HyperSurf: Quadruped Robot Leg Capable of Surface Recognition with GRU\n  and Real-to-Sim Transferring", "author": "Sergei Satsevich and Artem Bazhenov and Elizaveta Pestova and Yaroslav Savotin and Danil Belov and Liaisan Safarova and Artem Erhov and Batyr Khabibullin and Vyacheslav Kovalev and Aleksey Fedoseev and Dzmitry Tsetserukou", "abstract": "  This paper introduces a system of data collection acceleration and\nreal-to-sim transferring for surface recognition on a quadruped robot. The\nsystem features a mechanical single-leg setup capable of stepping on various\neasily interchangeable surfaces. Additionally, it incorporates a GRU-based\nSurface Recognition System, inspired by the system detailed in the Dog-Surf\npaper. This setup facilitates the expansion of dataset collection for model\ntraining, enabling data acquisition from hard-to-reach surfaces in laboratory\nconditions. Furthermore, it opens avenues for transferring surface properties\nfrom reality to simulation, thereby allowing the training of optimal gaits for\nlegged robots in simulation environments using a pre-prepared library of\ndigital twins of surfaces. Moreover, enhancements have been made to the\nGRU-based Surface Recognition System, allowing for the integration of data from\nboth the quadruped robot and the single-leg setup. The dataset and code have\nbeen made publicly available.\n", "link": "http://arxiv.org/abs/2407.15622v1", "date": "2024-07-22", "relevancy": 2.1837, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5655}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5532}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperSurf%3A%20Quadruped%20Robot%20Leg%20Capable%20of%20Surface%20Recognition%20with%20GRU%0A%20%20and%20Real-to-Sim%20Transferring&body=Title%3A%20HyperSurf%3A%20Quadruped%20Robot%20Leg%20Capable%20of%20Surface%20Recognition%20with%20GRU%0A%20%20and%20Real-to-Sim%20Transferring%0AAuthor%3A%20Sergei%20Satsevich%20and%20Artem%20Bazhenov%20and%20Elizaveta%20Pestova%20and%20Yaroslav%20Savotin%20and%20Danil%20Belov%20and%20Liaisan%20Safarova%20and%20Artem%20Erhov%20and%20Batyr%20Khabibullin%20and%20Vyacheslav%20Kovalev%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20system%20of%20data%20collection%20acceleration%20and%0Areal-to-sim%20transferring%20for%20surface%20recognition%20on%20a%20quadruped%20robot.%20The%0Asystem%20features%20a%20mechanical%20single-leg%20setup%20capable%20of%20stepping%20on%20various%0Aeasily%20interchangeable%20surfaces.%20Additionally%2C%20it%20incorporates%20a%20GRU-based%0ASurface%20Recognition%20System%2C%20inspired%20by%20the%20system%20detailed%20in%20the%20Dog-Surf%0Apaper.%20This%20setup%20facilitates%20the%20expansion%20of%20dataset%20collection%20for%20model%0Atraining%2C%20enabling%20data%20acquisition%20from%20hard-to-reach%20surfaces%20in%20laboratory%0Aconditions.%20Furthermore%2C%20it%20opens%20avenues%20for%20transferring%20surface%20properties%0Afrom%20reality%20to%20simulation%2C%20thereby%20allowing%20the%20training%20of%20optimal%20gaits%20for%0Alegged%20robots%20in%20simulation%20environments%20using%20a%20pre-prepared%20library%20of%0Adigital%20twins%20of%20surfaces.%20Moreover%2C%20enhancements%20have%20been%20made%20to%20the%0AGRU-based%20Surface%20Recognition%20System%2C%20allowing%20for%20the%20integration%20of%20data%20from%0Aboth%20the%20quadruped%20robot%20and%20the%20single-leg%20setup.%20The%20dataset%20and%20code%20have%0Abeen%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperSurf%253A%2520Quadruped%2520Robot%2520Leg%2520Capable%2520of%2520Surface%2520Recognition%2520with%2520GRU%250A%2520%2520and%2520Real-to-Sim%2520Transferring%26entry.906535625%3DSergei%2520Satsevich%2520and%2520Artem%2520Bazhenov%2520and%2520Elizaveta%2520Pestova%2520and%2520Yaroslav%2520Savotin%2520and%2520Danil%2520Belov%2520and%2520Liaisan%2520Safarova%2520and%2520Artem%2520Erhov%2520and%2520Batyr%2520Khabibullin%2520and%2520Vyacheslav%2520Kovalev%2520and%2520Aleksey%2520Fedoseev%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520system%2520of%2520data%2520collection%2520acceleration%2520and%250Areal-to-sim%2520transferring%2520for%2520surface%2520recognition%2520on%2520a%2520quadruped%2520robot.%2520The%250Asystem%2520features%2520a%2520mechanical%2520single-leg%2520setup%2520capable%2520of%2520stepping%2520on%2520various%250Aeasily%2520interchangeable%2520surfaces.%2520Additionally%252C%2520it%2520incorporates%2520a%2520GRU-based%250ASurface%2520Recognition%2520System%252C%2520inspired%2520by%2520the%2520system%2520detailed%2520in%2520the%2520Dog-Surf%250Apaper.%2520This%2520setup%2520facilitates%2520the%2520expansion%2520of%2520dataset%2520collection%2520for%2520model%250Atraining%252C%2520enabling%2520data%2520acquisition%2520from%2520hard-to-reach%2520surfaces%2520in%2520laboratory%250Aconditions.%2520Furthermore%252C%2520it%2520opens%2520avenues%2520for%2520transferring%2520surface%2520properties%250Afrom%2520reality%2520to%2520simulation%252C%2520thereby%2520allowing%2520the%2520training%2520of%2520optimal%2520gaits%2520for%250Alegged%2520robots%2520in%2520simulation%2520environments%2520using%2520a%2520pre-prepared%2520library%2520of%250Adigital%2520twins%2520of%2520surfaces.%2520Moreover%252C%2520enhancements%2520have%2520been%2520made%2520to%2520the%250AGRU-based%2520Surface%2520Recognition%2520System%252C%2520allowing%2520for%2520the%2520integration%2520of%2520data%2520from%250Aboth%2520the%2520quadruped%2520robot%2520and%2520the%2520single-leg%2520setup.%2520The%2520dataset%2520and%2520code%2520have%250Abeen%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperSurf%3A%20Quadruped%20Robot%20Leg%20Capable%20of%20Surface%20Recognition%20with%20GRU%0A%20%20and%20Real-to-Sim%20Transferring&entry.906535625=Sergei%20Satsevich%20and%20Artem%20Bazhenov%20and%20Elizaveta%20Pestova%20and%20Yaroslav%20Savotin%20and%20Danil%20Belov%20and%20Liaisan%20Safarova%20and%20Artem%20Erhov%20and%20Batyr%20Khabibullin%20and%20Vyacheslav%20Kovalev%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20This%20paper%20introduces%20a%20system%20of%20data%20collection%20acceleration%20and%0Areal-to-sim%20transferring%20for%20surface%20recognition%20on%20a%20quadruped%20robot.%20The%0Asystem%20features%20a%20mechanical%20single-leg%20setup%20capable%20of%20stepping%20on%20various%0Aeasily%20interchangeable%20surfaces.%20Additionally%2C%20it%20incorporates%20a%20GRU-based%0ASurface%20Recognition%20System%2C%20inspired%20by%20the%20system%20detailed%20in%20the%20Dog-Surf%0Apaper.%20This%20setup%20facilitates%20the%20expansion%20of%20dataset%20collection%20for%20model%0Atraining%2C%20enabling%20data%20acquisition%20from%20hard-to-reach%20surfaces%20in%20laboratory%0Aconditions.%20Furthermore%2C%20it%20opens%20avenues%20for%20transferring%20surface%20properties%0Afrom%20reality%20to%20simulation%2C%20thereby%20allowing%20the%20training%20of%20optimal%20gaits%20for%0Alegged%20robots%20in%20simulation%20environments%20using%20a%20pre-prepared%20library%20of%0Adigital%20twins%20of%20surfaces.%20Moreover%2C%20enhancements%20have%20been%20made%20to%20the%0AGRU-based%20Surface%20Recognition%20System%2C%20allowing%20for%20the%20integration%20of%20data%20from%0Aboth%20the%20quadruped%20robot%20and%20the%20single-leg%20setup.%20The%20dataset%20and%20code%20have%0Abeen%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15622v1&entry.124074799=Read"},
{"title": "SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams", "author": "Liangyan Jiang and Chuang Zhu and Yanxu Chen", "abstract": "  The spike camera, with its high temporal resolution, low latency, and high\ndynamic range, addresses high-speed imaging challenges like motion blur. It\ncaptures photons at each pixel independently, creating binary spike streams\nrich in temporal information but challenging for image reconstruction. Current\nalgorithms, both traditional and deep learning-based, still need to be improved\nin the utilization of the rich temporal detail and the restoration of the\ndetails of the reconstructed image. To overcome this, we introduce Swin\nSpikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike\nstreams. SwinSF is composed of Spike Feature Extraction, Spatial-Temporal\nFeature Extraction, and Final Reconstruction Module. It combines shifted window\nself-attention and proposed temporal spike attention, ensuring a comprehensive\nfeature extraction that encapsulates both spatial and temporal dynamics,\nleading to a more robust and accurate reconstruction of spike streams.\nFurthermore, we build a new synthesized dataset for spike image reconstruction\nwhich matches the resolution of the latest spike camera, ensuring its relevance\nand applicability to the latest developments in spike camera imaging.\nExperimental results demonstrate that the proposed network SwinSF sets a new\nbenchmark, achieving state-of-the-art performance across a series of datasets,\nincluding both real-world and synthesized data across various resolutions. Our\ncodes and proposed dataset will be available soon.\n", "link": "http://arxiv.org/abs/2407.15708v1", "date": "2024-07-22", "relevancy": 2.1796, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5791}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5214}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SwinSF%3A%20Image%20Reconstruction%20from%20Spatial-Temporal%20Spike%20Streams&body=Title%3A%20SwinSF%3A%20Image%20Reconstruction%20from%20Spatial-Temporal%20Spike%20Streams%0AAuthor%3A%20Liangyan%20Jiang%20and%20Chuang%20Zhu%20and%20Yanxu%20Chen%0AAbstract%3A%20%20%20The%20spike%20camera%2C%20with%20its%20high%20temporal%20resolution%2C%20low%20latency%2C%20and%20high%0Adynamic%20range%2C%20addresses%20high-speed%20imaging%20challenges%20like%20motion%20blur.%20It%0Acaptures%20photons%20at%20each%20pixel%20independently%2C%20creating%20binary%20spike%20streams%0Arich%20in%20temporal%20information%20but%20challenging%20for%20image%20reconstruction.%20Current%0Aalgorithms%2C%20both%20traditional%20and%20deep%20learning-based%2C%20still%20need%20to%20be%20improved%0Ain%20the%20utilization%20of%20the%20rich%20temporal%20detail%20and%20the%20restoration%20of%20the%0Adetails%20of%20the%20reconstructed%20image.%20To%20overcome%20this%2C%20we%20introduce%20Swin%0ASpikeformer%20%28SwinSF%29%2C%20a%20novel%20model%20for%20dynamic%20scene%20reconstruction%20from%20spike%0Astreams.%20SwinSF%20is%20composed%20of%20Spike%20Feature%20Extraction%2C%20Spatial-Temporal%0AFeature%20Extraction%2C%20and%20Final%20Reconstruction%20Module.%20It%20combines%20shifted%20window%0Aself-attention%20and%20proposed%20temporal%20spike%20attention%2C%20ensuring%20a%20comprehensive%0Afeature%20extraction%20that%20encapsulates%20both%20spatial%20and%20temporal%20dynamics%2C%0Aleading%20to%20a%20more%20robust%20and%20accurate%20reconstruction%20of%20spike%20streams.%0AFurthermore%2C%20we%20build%20a%20new%20synthesized%20dataset%20for%20spike%20image%20reconstruction%0Awhich%20matches%20the%20resolution%20of%20the%20latest%20spike%20camera%2C%20ensuring%20its%20relevance%0Aand%20applicability%20to%20the%20latest%20developments%20in%20spike%20camera%20imaging.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20network%20SwinSF%20sets%20a%20new%0Abenchmark%2C%20achieving%20state-of-the-art%20performance%20across%20a%20series%20of%20datasets%2C%0Aincluding%20both%20real-world%20and%20synthesized%20data%20across%20various%20resolutions.%20Our%0Acodes%20and%20proposed%20dataset%20will%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwinSF%253A%2520Image%2520Reconstruction%2520from%2520Spatial-Temporal%2520Spike%2520Streams%26entry.906535625%3DLiangyan%2520Jiang%2520and%2520Chuang%2520Zhu%2520and%2520Yanxu%2520Chen%26entry.1292438233%3D%2520%2520The%2520spike%2520camera%252C%2520with%2520its%2520high%2520temporal%2520resolution%252C%2520low%2520latency%252C%2520and%2520high%250Adynamic%2520range%252C%2520addresses%2520high-speed%2520imaging%2520challenges%2520like%2520motion%2520blur.%2520It%250Acaptures%2520photons%2520at%2520each%2520pixel%2520independently%252C%2520creating%2520binary%2520spike%2520streams%250Arich%2520in%2520temporal%2520information%2520but%2520challenging%2520for%2520image%2520reconstruction.%2520Current%250Aalgorithms%252C%2520both%2520traditional%2520and%2520deep%2520learning-based%252C%2520still%2520need%2520to%2520be%2520improved%250Ain%2520the%2520utilization%2520of%2520the%2520rich%2520temporal%2520detail%2520and%2520the%2520restoration%2520of%2520the%250Adetails%2520of%2520the%2520reconstructed%2520image.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520Swin%250ASpikeformer%2520%2528SwinSF%2529%252C%2520a%2520novel%2520model%2520for%2520dynamic%2520scene%2520reconstruction%2520from%2520spike%250Astreams.%2520SwinSF%2520is%2520composed%2520of%2520Spike%2520Feature%2520Extraction%252C%2520Spatial-Temporal%250AFeature%2520Extraction%252C%2520and%2520Final%2520Reconstruction%2520Module.%2520It%2520combines%2520shifted%2520window%250Aself-attention%2520and%2520proposed%2520temporal%2520spike%2520attention%252C%2520ensuring%2520a%2520comprehensive%250Afeature%2520extraction%2520that%2520encapsulates%2520both%2520spatial%2520and%2520temporal%2520dynamics%252C%250Aleading%2520to%2520a%2520more%2520robust%2520and%2520accurate%2520reconstruction%2520of%2520spike%2520streams.%250AFurthermore%252C%2520we%2520build%2520a%2520new%2520synthesized%2520dataset%2520for%2520spike%2520image%2520reconstruction%250Awhich%2520matches%2520the%2520resolution%2520of%2520the%2520latest%2520spike%2520camera%252C%2520ensuring%2520its%2520relevance%250Aand%2520applicability%2520to%2520the%2520latest%2520developments%2520in%2520spike%2520camera%2520imaging.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520network%2520SwinSF%2520sets%2520a%2520new%250Abenchmark%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520a%2520series%2520of%2520datasets%252C%250Aincluding%2520both%2520real-world%2520and%2520synthesized%2520data%2520across%2520various%2520resolutions.%2520Our%250Acodes%2520and%2520proposed%2520dataset%2520will%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SwinSF%3A%20Image%20Reconstruction%20from%20Spatial-Temporal%20Spike%20Streams&entry.906535625=Liangyan%20Jiang%20and%20Chuang%20Zhu%20and%20Yanxu%20Chen&entry.1292438233=%20%20The%20spike%20camera%2C%20with%20its%20high%20temporal%20resolution%2C%20low%20latency%2C%20and%20high%0Adynamic%20range%2C%20addresses%20high-speed%20imaging%20challenges%20like%20motion%20blur.%20It%0Acaptures%20photons%20at%20each%20pixel%20independently%2C%20creating%20binary%20spike%20streams%0Arich%20in%20temporal%20information%20but%20challenging%20for%20image%20reconstruction.%20Current%0Aalgorithms%2C%20both%20traditional%20and%20deep%20learning-based%2C%20still%20need%20to%20be%20improved%0Ain%20the%20utilization%20of%20the%20rich%20temporal%20detail%20and%20the%20restoration%20of%20the%0Adetails%20of%20the%20reconstructed%20image.%20To%20overcome%20this%2C%20we%20introduce%20Swin%0ASpikeformer%20%28SwinSF%29%2C%20a%20novel%20model%20for%20dynamic%20scene%20reconstruction%20from%20spike%0Astreams.%20SwinSF%20is%20composed%20of%20Spike%20Feature%20Extraction%2C%20Spatial-Temporal%0AFeature%20Extraction%2C%20and%20Final%20Reconstruction%20Module.%20It%20combines%20shifted%20window%0Aself-attention%20and%20proposed%20temporal%20spike%20attention%2C%20ensuring%20a%20comprehensive%0Afeature%20extraction%20that%20encapsulates%20both%20spatial%20and%20temporal%20dynamics%2C%0Aleading%20to%20a%20more%20robust%20and%20accurate%20reconstruction%20of%20spike%20streams.%0AFurthermore%2C%20we%20build%20a%20new%20synthesized%20dataset%20for%20spike%20image%20reconstruction%0Awhich%20matches%20the%20resolution%20of%20the%20latest%20spike%20camera%2C%20ensuring%20its%20relevance%0Aand%20applicability%20to%20the%20latest%20developments%20in%20spike%20camera%20imaging.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20network%20SwinSF%20sets%20a%20new%0Abenchmark%2C%20achieving%20state-of-the-art%20performance%20across%20a%20series%20of%20datasets%2C%0Aincluding%20both%20real-world%20and%20synthesized%20data%20across%20various%20resolutions.%20Our%0Acodes%20and%20proposed%20dataset%20will%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15708v1&entry.124074799=Read"},
{"title": "MILAN: Milli-Annotations for Lidar Semantic Segmentation", "author": "Nermin Samet and Gilles Puy and Oriane Sim\u00e9oni and Renaud Marlet", "abstract": "  Annotating lidar point clouds for autonomous driving is a notoriously\nexpensive and time-consuming task. In this work, we show that the quality of\nrecent self-supervised lidar scan representations allows a great reduction of\nthe annotation cost. Our method has two main steps. First, we show that\nself-supervised representations allow a simple and direct selection of highly\ninformative lidar scans to annotate: training a network on these selected scans\nleads to much better results than a random selection of scans and, more\ninterestingly, to results on par with selections made by SOTA active learning\nmethods. In a second step, we leverage the same self-supervised representations\nto cluster points in our selected scans. Asking the annotator to classify each\ncluster, with a single click per cluster, then permits us to close the gap with\nfully-annotated training sets, while only requiring one thousandth of the point\nlabels.\n", "link": "http://arxiv.org/abs/2407.15797v1", "date": "2024-07-22", "relevancy": 2.1794, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5587}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5351}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MILAN%3A%20Milli-Annotations%20for%20Lidar%20Semantic%20Segmentation&body=Title%3A%20MILAN%3A%20Milli-Annotations%20for%20Lidar%20Semantic%20Segmentation%0AAuthor%3A%20Nermin%20Samet%20and%20Gilles%20Puy%20and%20Oriane%20Sim%C3%A9oni%20and%20Renaud%20Marlet%0AAbstract%3A%20%20%20Annotating%20lidar%20point%20clouds%20for%20autonomous%20driving%20is%20a%20notoriously%0Aexpensive%20and%20time-consuming%20task.%20In%20this%20work%2C%20we%20show%20that%20the%20quality%20of%0Arecent%20self-supervised%20lidar%20scan%20representations%20allows%20a%20great%20reduction%20of%0Athe%20annotation%20cost.%20Our%20method%20has%20two%20main%20steps.%20First%2C%20we%20show%20that%0Aself-supervised%20representations%20allow%20a%20simple%20and%20direct%20selection%20of%20highly%0Ainformative%20lidar%20scans%20to%20annotate%3A%20training%20a%20network%20on%20these%20selected%20scans%0Aleads%20to%20much%20better%20results%20than%20a%20random%20selection%20of%20scans%20and%2C%20more%0Ainterestingly%2C%20to%20results%20on%20par%20with%20selections%20made%20by%20SOTA%20active%20learning%0Amethods.%20In%20a%20second%20step%2C%20we%20leverage%20the%20same%20self-supervised%20representations%0Ato%20cluster%20points%20in%20our%20selected%20scans.%20Asking%20the%20annotator%20to%20classify%20each%0Acluster%2C%20with%20a%20single%20click%20per%20cluster%2C%20then%20permits%20us%20to%20close%20the%20gap%20with%0Afully-annotated%20training%20sets%2C%20while%20only%20requiring%20one%20thousandth%20of%20the%20point%0Alabels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMILAN%253A%2520Milli-Annotations%2520for%2520Lidar%2520Semantic%2520Segmentation%26entry.906535625%3DNermin%2520Samet%2520and%2520Gilles%2520Puy%2520and%2520Oriane%2520Sim%25C3%25A9oni%2520and%2520Renaud%2520Marlet%26entry.1292438233%3D%2520%2520Annotating%2520lidar%2520point%2520clouds%2520for%2520autonomous%2520driving%2520is%2520a%2520notoriously%250Aexpensive%2520and%2520time-consuming%2520task.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520the%2520quality%2520of%250Arecent%2520self-supervised%2520lidar%2520scan%2520representations%2520allows%2520a%2520great%2520reduction%2520of%250Athe%2520annotation%2520cost.%2520Our%2520method%2520has%2520two%2520main%2520steps.%2520First%252C%2520we%2520show%2520that%250Aself-supervised%2520representations%2520allow%2520a%2520simple%2520and%2520direct%2520selection%2520of%2520highly%250Ainformative%2520lidar%2520scans%2520to%2520annotate%253A%2520training%2520a%2520network%2520on%2520these%2520selected%2520scans%250Aleads%2520to%2520much%2520better%2520results%2520than%2520a%2520random%2520selection%2520of%2520scans%2520and%252C%2520more%250Ainterestingly%252C%2520to%2520results%2520on%2520par%2520with%2520selections%2520made%2520by%2520SOTA%2520active%2520learning%250Amethods.%2520In%2520a%2520second%2520step%252C%2520we%2520leverage%2520the%2520same%2520self-supervised%2520representations%250Ato%2520cluster%2520points%2520in%2520our%2520selected%2520scans.%2520Asking%2520the%2520annotator%2520to%2520classify%2520each%250Acluster%252C%2520with%2520a%2520single%2520click%2520per%2520cluster%252C%2520then%2520permits%2520us%2520to%2520close%2520the%2520gap%2520with%250Afully-annotated%2520training%2520sets%252C%2520while%2520only%2520requiring%2520one%2520thousandth%2520of%2520the%2520point%250Alabels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MILAN%3A%20Milli-Annotations%20for%20Lidar%20Semantic%20Segmentation&entry.906535625=Nermin%20Samet%20and%20Gilles%20Puy%20and%20Oriane%20Sim%C3%A9oni%20and%20Renaud%20Marlet&entry.1292438233=%20%20Annotating%20lidar%20point%20clouds%20for%20autonomous%20driving%20is%20a%20notoriously%0Aexpensive%20and%20time-consuming%20task.%20In%20this%20work%2C%20we%20show%20that%20the%20quality%20of%0Arecent%20self-supervised%20lidar%20scan%20representations%20allows%20a%20great%20reduction%20of%0Athe%20annotation%20cost.%20Our%20method%20has%20two%20main%20steps.%20First%2C%20we%20show%20that%0Aself-supervised%20representations%20allow%20a%20simple%20and%20direct%20selection%20of%20highly%0Ainformative%20lidar%20scans%20to%20annotate%3A%20training%20a%20network%20on%20these%20selected%20scans%0Aleads%20to%20much%20better%20results%20than%20a%20random%20selection%20of%20scans%20and%2C%20more%0Ainterestingly%2C%20to%20results%20on%20par%20with%20selections%20made%20by%20SOTA%20active%20learning%0Amethods.%20In%20a%20second%20step%2C%20we%20leverage%20the%20same%20self-supervised%20representations%0Ato%20cluster%20points%20in%20our%20selected%20scans.%20Asking%20the%20annotator%20to%20classify%20each%0Acluster%2C%20with%20a%20single%20click%20per%20cluster%2C%20then%20permits%20us%20to%20close%20the%20gap%20with%0Afully-annotated%20training%20sets%2C%20while%20only%20requiring%20one%20thousandth%20of%20the%20point%0Alabels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15797v1&entry.124074799=Read"},
{"title": "DStruct2Design: Data and Benchmarks for Data Structure Driven Generative\n  Floor Plan Design", "author": "Zhi Hao Luo and Luis Lara and Ge Ya Luo and Florian Golemo and Christopher Beckham and Christopher Pal", "abstract": "  Text conditioned generative models for images have yielded impressive\nresults. Text conditioned floorplan generation as a special type of raster\nimage generation task also received particular attention. However there are\nmany use cases in floorpla generation where numerical properties of the\ngenerated result are more important than the aesthetics. For instance, one\nmight want to specify sizes for certain rooms in a floorplan and compare the\ngenerated floorplan with given specifications Current approaches, datasets and\ncommonly used evaluations do not support these kinds of constraints. As such,\nan attractive strategy is to generate an intermediate data structure that\ncontains numerical properties of a floorplan which can be used to generate the\nfinal floorplan image. To explore this setting we (1) construct a new dataset\nfor this data-structure to data-structure formulation of floorplan generation\nusing two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and\nprovide the tools to convert further procedurally generated ProcTHOR floorplan\ndata into our format. (2) We explore the task of floorplan generation given a\npartial or complete set of constraints and we design a series of metrics and\nbenchmarks to enable evaluating how well samples generated from models respect\nthe constraints. (3) We create multiple baselines by finetuning a large\nlanguage model (LLM), Llama3, and demonstrate the feasibility of using\nfloorplan data structure conditioned LLMs for the problem of floorplan\ngeneration respecting numerical constraints. We hope that our new datasets and\nbenchmarks will encourage further research on different ways to improve the\nperformance of LLMs and other generative modelling techniques for generating\ndesigns where quantitative constraints are only partially specified, but must\nbe respected.\n", "link": "http://arxiv.org/abs/2407.15723v1", "date": "2024-07-22", "relevancy": 2.1752, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5566}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5461}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DStruct2Design%3A%20Data%20and%20Benchmarks%20for%20Data%20Structure%20Driven%20Generative%0A%20%20Floor%20Plan%20Design&body=Title%3A%20DStruct2Design%3A%20Data%20and%20Benchmarks%20for%20Data%20Structure%20Driven%20Generative%0A%20%20Floor%20Plan%20Design%0AAuthor%3A%20Zhi%20Hao%20Luo%20and%20Luis%20Lara%20and%20Ge%20Ya%20Luo%20and%20Florian%20Golemo%20and%20Christopher%20Beckham%20and%20Christopher%20Pal%0AAbstract%3A%20%20%20Text%20conditioned%20generative%20models%20for%20images%20have%20yielded%20impressive%0Aresults.%20Text%20conditioned%20floorplan%20generation%20as%20a%20special%20type%20of%20raster%0Aimage%20generation%20task%20also%20received%20particular%20attention.%20However%20there%20are%0Amany%20use%20cases%20in%20floorpla%20generation%20where%20numerical%20properties%20of%20the%0Agenerated%20result%20are%20more%20important%20than%20the%20aesthetics.%20For%20instance%2C%20one%0Amight%20want%20to%20specify%20sizes%20for%20certain%20rooms%20in%20a%20floorplan%20and%20compare%20the%0Agenerated%20floorplan%20with%20given%20specifications%20Current%20approaches%2C%20datasets%20and%0Acommonly%20used%20evaluations%20do%20not%20support%20these%20kinds%20of%20constraints.%20As%20such%2C%0Aan%20attractive%20strategy%20is%20to%20generate%20an%20intermediate%20data%20structure%20that%0Acontains%20numerical%20properties%20of%20a%20floorplan%20which%20can%20be%20used%20to%20generate%20the%0Afinal%20floorplan%20image.%20To%20explore%20this%20setting%20we%20%281%29%20construct%20a%20new%20dataset%0Afor%20this%20data-structure%20to%20data-structure%20formulation%20of%20floorplan%20generation%0Ausing%20two%20popular%20image%20based%20floorplan%20datasets%20RPLAN%20and%20ProcTHOR-10k%2C%20and%0Aprovide%20the%20tools%20to%20convert%20further%20procedurally%20generated%20ProcTHOR%20floorplan%0Adata%20into%20our%20format.%20%282%29%20We%20explore%20the%20task%20of%20floorplan%20generation%20given%20a%0Apartial%20or%20complete%20set%20of%20constraints%20and%20we%20design%20a%20series%20of%20metrics%20and%0Abenchmarks%20to%20enable%20evaluating%20how%20well%20samples%20generated%20from%20models%20respect%0Athe%20constraints.%20%283%29%20We%20create%20multiple%20baselines%20by%20finetuning%20a%20large%0Alanguage%20model%20%28LLM%29%2C%20Llama3%2C%20and%20demonstrate%20the%20feasibility%20of%20using%0Afloorplan%20data%20structure%20conditioned%20LLMs%20for%20the%20problem%20of%20floorplan%0Ageneration%20respecting%20numerical%20constraints.%20We%20hope%20that%20our%20new%20datasets%20and%0Abenchmarks%20will%20encourage%20further%20research%20on%20different%20ways%20to%20improve%20the%0Aperformance%20of%20LLMs%20and%20other%20generative%20modelling%20techniques%20for%20generating%0Adesigns%20where%20quantitative%20constraints%20are%20only%20partially%20specified%2C%20but%20must%0Abe%20respected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDStruct2Design%253A%2520Data%2520and%2520Benchmarks%2520for%2520Data%2520Structure%2520Driven%2520Generative%250A%2520%2520Floor%2520Plan%2520Design%26entry.906535625%3DZhi%2520Hao%2520Luo%2520and%2520Luis%2520Lara%2520and%2520Ge%2520Ya%2520Luo%2520and%2520Florian%2520Golemo%2520and%2520Christopher%2520Beckham%2520and%2520Christopher%2520Pal%26entry.1292438233%3D%2520%2520Text%2520conditioned%2520generative%2520models%2520for%2520images%2520have%2520yielded%2520impressive%250Aresults.%2520Text%2520conditioned%2520floorplan%2520generation%2520as%2520a%2520special%2520type%2520of%2520raster%250Aimage%2520generation%2520task%2520also%2520received%2520particular%2520attention.%2520However%2520there%2520are%250Amany%2520use%2520cases%2520in%2520floorpla%2520generation%2520where%2520numerical%2520properties%2520of%2520the%250Agenerated%2520result%2520are%2520more%2520important%2520than%2520the%2520aesthetics.%2520For%2520instance%252C%2520one%250Amight%2520want%2520to%2520specify%2520sizes%2520for%2520certain%2520rooms%2520in%2520a%2520floorplan%2520and%2520compare%2520the%250Agenerated%2520floorplan%2520with%2520given%2520specifications%2520Current%2520approaches%252C%2520datasets%2520and%250Acommonly%2520used%2520evaluations%2520do%2520not%2520support%2520these%2520kinds%2520of%2520constraints.%2520As%2520such%252C%250Aan%2520attractive%2520strategy%2520is%2520to%2520generate%2520an%2520intermediate%2520data%2520structure%2520that%250Acontains%2520numerical%2520properties%2520of%2520a%2520floorplan%2520which%2520can%2520be%2520used%2520to%2520generate%2520the%250Afinal%2520floorplan%2520image.%2520To%2520explore%2520this%2520setting%2520we%2520%25281%2529%2520construct%2520a%2520new%2520dataset%250Afor%2520this%2520data-structure%2520to%2520data-structure%2520formulation%2520of%2520floorplan%2520generation%250Ausing%2520two%2520popular%2520image%2520based%2520floorplan%2520datasets%2520RPLAN%2520and%2520ProcTHOR-10k%252C%2520and%250Aprovide%2520the%2520tools%2520to%2520convert%2520further%2520procedurally%2520generated%2520ProcTHOR%2520floorplan%250Adata%2520into%2520our%2520format.%2520%25282%2529%2520We%2520explore%2520the%2520task%2520of%2520floorplan%2520generation%2520given%2520a%250Apartial%2520or%2520complete%2520set%2520of%2520constraints%2520and%2520we%2520design%2520a%2520series%2520of%2520metrics%2520and%250Abenchmarks%2520to%2520enable%2520evaluating%2520how%2520well%2520samples%2520generated%2520from%2520models%2520respect%250Athe%2520constraints.%2520%25283%2529%2520We%2520create%2520multiple%2520baselines%2520by%2520finetuning%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%252C%2520Llama3%252C%2520and%2520demonstrate%2520the%2520feasibility%2520of%2520using%250Afloorplan%2520data%2520structure%2520conditioned%2520LLMs%2520for%2520the%2520problem%2520of%2520floorplan%250Ageneration%2520respecting%2520numerical%2520constraints.%2520We%2520hope%2520that%2520our%2520new%2520datasets%2520and%250Abenchmarks%2520will%2520encourage%2520further%2520research%2520on%2520different%2520ways%2520to%2520improve%2520the%250Aperformance%2520of%2520LLMs%2520and%2520other%2520generative%2520modelling%2520techniques%2520for%2520generating%250Adesigns%2520where%2520quantitative%2520constraints%2520are%2520only%2520partially%2520specified%252C%2520but%2520must%250Abe%2520respected.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DStruct2Design%3A%20Data%20and%20Benchmarks%20for%20Data%20Structure%20Driven%20Generative%0A%20%20Floor%20Plan%20Design&entry.906535625=Zhi%20Hao%20Luo%20and%20Luis%20Lara%20and%20Ge%20Ya%20Luo%20and%20Florian%20Golemo%20and%20Christopher%20Beckham%20and%20Christopher%20Pal&entry.1292438233=%20%20Text%20conditioned%20generative%20models%20for%20images%20have%20yielded%20impressive%0Aresults.%20Text%20conditioned%20floorplan%20generation%20as%20a%20special%20type%20of%20raster%0Aimage%20generation%20task%20also%20received%20particular%20attention.%20However%20there%20are%0Amany%20use%20cases%20in%20floorpla%20generation%20where%20numerical%20properties%20of%20the%0Agenerated%20result%20are%20more%20important%20than%20the%20aesthetics.%20For%20instance%2C%20one%0Amight%20want%20to%20specify%20sizes%20for%20certain%20rooms%20in%20a%20floorplan%20and%20compare%20the%0Agenerated%20floorplan%20with%20given%20specifications%20Current%20approaches%2C%20datasets%20and%0Acommonly%20used%20evaluations%20do%20not%20support%20these%20kinds%20of%20constraints.%20As%20such%2C%0Aan%20attractive%20strategy%20is%20to%20generate%20an%20intermediate%20data%20structure%20that%0Acontains%20numerical%20properties%20of%20a%20floorplan%20which%20can%20be%20used%20to%20generate%20the%0Afinal%20floorplan%20image.%20To%20explore%20this%20setting%20we%20%281%29%20construct%20a%20new%20dataset%0Afor%20this%20data-structure%20to%20data-structure%20formulation%20of%20floorplan%20generation%0Ausing%20two%20popular%20image%20based%20floorplan%20datasets%20RPLAN%20and%20ProcTHOR-10k%2C%20and%0Aprovide%20the%20tools%20to%20convert%20further%20procedurally%20generated%20ProcTHOR%20floorplan%0Adata%20into%20our%20format.%20%282%29%20We%20explore%20the%20task%20of%20floorplan%20generation%20given%20a%0Apartial%20or%20complete%20set%20of%20constraints%20and%20we%20design%20a%20series%20of%20metrics%20and%0Abenchmarks%20to%20enable%20evaluating%20how%20well%20samples%20generated%20from%20models%20respect%0Athe%20constraints.%20%283%29%20We%20create%20multiple%20baselines%20by%20finetuning%20a%20large%0Alanguage%20model%20%28LLM%29%2C%20Llama3%2C%20and%20demonstrate%20the%20feasibility%20of%20using%0Afloorplan%20data%20structure%20conditioned%20LLMs%20for%20the%20problem%20of%20floorplan%0Ageneration%20respecting%20numerical%20constraints.%20We%20hope%20that%20our%20new%20datasets%20and%0Abenchmarks%20will%20encourage%20further%20research%20on%20different%20ways%20to%20improve%20the%0Aperformance%20of%20LLMs%20and%20other%20generative%20modelling%20techniques%20for%20generating%0Adesigns%20where%20quantitative%20constraints%20are%20only%20partially%20specified%2C%20but%20must%0Abe%20respected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15723v1&entry.124074799=Read"},
{"title": "PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans", "author": "Lisa Anita De Santi and J\u00f6rg Schl\u00f6tterer and Michael Scheschenja and Joel Wessendorf and Meike Nauta and Vincenzo Positano and Christin Seifert", "abstract": "  Information from neuroimaging examinations is increasingly used to support\ndiagnoses of dementia, e.g., Alzheimer's disease. While current clinical\npractice is mainly based on visual inspection and feature engineering, Deep\nLearning approaches can be used to automate the analysis and to discover new\nimage-biomarkers. Part-prototype neural networks (PP-NN) are an alternative to\nstandard blackbox models, and have shown promising results in general computer\nvision. PP-NN's base their reasoning on prototypical image regions that are\nlearned fully unsupervised, and combined with a simple-to-understand decision\nlayer. We present PIPNet3D, a PP-NN for volumetric images. We apply PIPNet3D to\nthe clinical diagnosis of Alzheimer's Disease from structural Magnetic\nResonance Imaging (sMRI). We assess the quality of prototypes under a\nsystematic evaluation framework, propose new functionally grounded metrics to\nevaluate brain prototypes and develop an evaluation scheme to assess their\ncoherency with domain experts. Our results show that PIPNet3D is an\ninterpretable, compact model for Alzheimer's diagnosis with its reasoning well\naligned to medical domain knowledge. Notably, PIPNet3D achieves the same\naccuracy as its blackbox counterpart; and removing the remaining clinically\nirrelevant prototypes from its decision process does not decrease predictive\nperformance.\n", "link": "http://arxiv.org/abs/2403.18328v3", "date": "2024-07-22", "relevancy": 2.1586, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5333}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIPNet3D%3A%20Interpretable%20Detection%20of%20Alzheimer%20in%20MRI%20Scans&body=Title%3A%20PIPNet3D%3A%20Interpretable%20Detection%20of%20Alzheimer%20in%20MRI%20Scans%0AAuthor%3A%20Lisa%20Anita%20De%20Santi%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Michael%20Scheschenja%20and%20Joel%20Wessendorf%20and%20Meike%20Nauta%20and%20Vincenzo%20Positano%20and%20Christin%20Seifert%0AAbstract%3A%20%20%20Information%20from%20neuroimaging%20examinations%20is%20increasingly%20used%20to%20support%0Adiagnoses%20of%20dementia%2C%20e.g.%2C%20Alzheimer%27s%20disease.%20While%20current%20clinical%0Apractice%20is%20mainly%20based%20on%20visual%20inspection%20and%20feature%20engineering%2C%20Deep%0ALearning%20approaches%20can%20be%20used%20to%20automate%20the%20analysis%20and%20to%20discover%20new%0Aimage-biomarkers.%20Part-prototype%20neural%20networks%20%28PP-NN%29%20are%20an%20alternative%20to%0Astandard%20blackbox%20models%2C%20and%20have%20shown%20promising%20results%20in%20general%20computer%0Avision.%20PP-NN%27s%20base%20their%20reasoning%20on%20prototypical%20image%20regions%20that%20are%0Alearned%20fully%20unsupervised%2C%20and%20combined%20with%20a%20simple-to-understand%20decision%0Alayer.%20We%20present%20PIPNet3D%2C%20a%20PP-NN%20for%20volumetric%20images.%20We%20apply%20PIPNet3D%20to%0Athe%20clinical%20diagnosis%20of%20Alzheimer%27s%20Disease%20from%20structural%20Magnetic%0AResonance%20Imaging%20%28sMRI%29.%20We%20assess%20the%20quality%20of%20prototypes%20under%20a%0Asystematic%20evaluation%20framework%2C%20propose%20new%20functionally%20grounded%20metrics%20to%0Aevaluate%20brain%20prototypes%20and%20develop%20an%20evaluation%20scheme%20to%20assess%20their%0Acoherency%20with%20domain%20experts.%20Our%20results%20show%20that%20PIPNet3D%20is%20an%0Ainterpretable%2C%20compact%20model%20for%20Alzheimer%27s%20diagnosis%20with%20its%20reasoning%20well%0Aaligned%20to%20medical%20domain%20knowledge.%20Notably%2C%20PIPNet3D%20achieves%20the%20same%0Aaccuracy%20as%20its%20blackbox%20counterpart%3B%20and%20removing%20the%20remaining%20clinically%0Airrelevant%20prototypes%20from%20its%20decision%20process%20does%20not%20decrease%20predictive%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18328v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIPNet3D%253A%2520Interpretable%2520Detection%2520of%2520Alzheimer%2520in%2520MRI%2520Scans%26entry.906535625%3DLisa%2520Anita%2520De%2520Santi%2520and%2520J%25C3%25B6rg%2520Schl%25C3%25B6tterer%2520and%2520Michael%2520Scheschenja%2520and%2520Joel%2520Wessendorf%2520and%2520Meike%2520Nauta%2520and%2520Vincenzo%2520Positano%2520and%2520Christin%2520Seifert%26entry.1292438233%3D%2520%2520Information%2520from%2520neuroimaging%2520examinations%2520is%2520increasingly%2520used%2520to%2520support%250Adiagnoses%2520of%2520dementia%252C%2520e.g.%252C%2520Alzheimer%2527s%2520disease.%2520While%2520current%2520clinical%250Apractice%2520is%2520mainly%2520based%2520on%2520visual%2520inspection%2520and%2520feature%2520engineering%252C%2520Deep%250ALearning%2520approaches%2520can%2520be%2520used%2520to%2520automate%2520the%2520analysis%2520and%2520to%2520discover%2520new%250Aimage-biomarkers.%2520Part-prototype%2520neural%2520networks%2520%2528PP-NN%2529%2520are%2520an%2520alternative%2520to%250Astandard%2520blackbox%2520models%252C%2520and%2520have%2520shown%2520promising%2520results%2520in%2520general%2520computer%250Avision.%2520PP-NN%2527s%2520base%2520their%2520reasoning%2520on%2520prototypical%2520image%2520regions%2520that%2520are%250Alearned%2520fully%2520unsupervised%252C%2520and%2520combined%2520with%2520a%2520simple-to-understand%2520decision%250Alayer.%2520We%2520present%2520PIPNet3D%252C%2520a%2520PP-NN%2520for%2520volumetric%2520images.%2520We%2520apply%2520PIPNet3D%2520to%250Athe%2520clinical%2520diagnosis%2520of%2520Alzheimer%2527s%2520Disease%2520from%2520structural%2520Magnetic%250AResonance%2520Imaging%2520%2528sMRI%2529.%2520We%2520assess%2520the%2520quality%2520of%2520prototypes%2520under%2520a%250Asystematic%2520evaluation%2520framework%252C%2520propose%2520new%2520functionally%2520grounded%2520metrics%2520to%250Aevaluate%2520brain%2520prototypes%2520and%2520develop%2520an%2520evaluation%2520scheme%2520to%2520assess%2520their%250Acoherency%2520with%2520domain%2520experts.%2520Our%2520results%2520show%2520that%2520PIPNet3D%2520is%2520an%250Ainterpretable%252C%2520compact%2520model%2520for%2520Alzheimer%2527s%2520diagnosis%2520with%2520its%2520reasoning%2520well%250Aaligned%2520to%2520medical%2520domain%2520knowledge.%2520Notably%252C%2520PIPNet3D%2520achieves%2520the%2520same%250Aaccuracy%2520as%2520its%2520blackbox%2520counterpart%253B%2520and%2520removing%2520the%2520remaining%2520clinically%250Airrelevant%2520prototypes%2520from%2520its%2520decision%2520process%2520does%2520not%2520decrease%2520predictive%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18328v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIPNet3D%3A%20Interpretable%20Detection%20of%20Alzheimer%20in%20MRI%20Scans&entry.906535625=Lisa%20Anita%20De%20Santi%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Michael%20Scheschenja%20and%20Joel%20Wessendorf%20and%20Meike%20Nauta%20and%20Vincenzo%20Positano%20and%20Christin%20Seifert&entry.1292438233=%20%20Information%20from%20neuroimaging%20examinations%20is%20increasingly%20used%20to%20support%0Adiagnoses%20of%20dementia%2C%20e.g.%2C%20Alzheimer%27s%20disease.%20While%20current%20clinical%0Apractice%20is%20mainly%20based%20on%20visual%20inspection%20and%20feature%20engineering%2C%20Deep%0ALearning%20approaches%20can%20be%20used%20to%20automate%20the%20analysis%20and%20to%20discover%20new%0Aimage-biomarkers.%20Part-prototype%20neural%20networks%20%28PP-NN%29%20are%20an%20alternative%20to%0Astandard%20blackbox%20models%2C%20and%20have%20shown%20promising%20results%20in%20general%20computer%0Avision.%20PP-NN%27s%20base%20their%20reasoning%20on%20prototypical%20image%20regions%20that%20are%0Alearned%20fully%20unsupervised%2C%20and%20combined%20with%20a%20simple-to-understand%20decision%0Alayer.%20We%20present%20PIPNet3D%2C%20a%20PP-NN%20for%20volumetric%20images.%20We%20apply%20PIPNet3D%20to%0Athe%20clinical%20diagnosis%20of%20Alzheimer%27s%20Disease%20from%20structural%20Magnetic%0AResonance%20Imaging%20%28sMRI%29.%20We%20assess%20the%20quality%20of%20prototypes%20under%20a%0Asystematic%20evaluation%20framework%2C%20propose%20new%20functionally%20grounded%20metrics%20to%0Aevaluate%20brain%20prototypes%20and%20develop%20an%20evaluation%20scheme%20to%20assess%20their%0Acoherency%20with%20domain%20experts.%20Our%20results%20show%20that%20PIPNet3D%20is%20an%0Ainterpretable%2C%20compact%20model%20for%20Alzheimer%27s%20diagnosis%20with%20its%20reasoning%20well%0Aaligned%20to%20medical%20domain%20knowledge.%20Notably%2C%20PIPNet3D%20achieves%20the%20same%0Aaccuracy%20as%20its%20blackbox%20counterpart%3B%20and%20removing%20the%20remaining%20clinically%0Airrelevant%20prototypes%20from%20its%20decision%20process%20does%20not%20decrease%20predictive%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18328v3&entry.124074799=Read"},
{"title": "Adaptive Extensions of Unbiased Risk Estimators for Unsupervised\n  Magnetic Resonance Image Denoising", "author": "Reeshad Khan and Dr. John Gauch and Dr. Ukash Nakarmi", "abstract": "  The application of Deep Neural Networks (DNNs) to image denoising has notably\nchallenged traditional denoising methods, particularly within complex noise\nscenarios prevalent in medical imaging. Despite the effectiveness of\ntraditional and some DNN-based methods, their reliance on high-quality,\nnoiseless ground truth images limits their practical utility. In response to\nthis, our work introduces and benchmarks innovative unsupervised learning\nstrategies, notably Stein's Unbiased Risk Estimator (SURE), its extension\n(eSURE), and our novel implementation, the Extended Poisson Unbiased Risk\nEstimator (ePURE), within medical imaging frameworks.\n  This paper presents a comprehensive evaluation of these methods on MRI data\nafflicted with Gaussian and Poisson noise types, a scenario typical in medical\nimaging but challenging for most denoising algorithms. Our main contribution\nlies in the effective adaptation and implementation of the SURE, eSURE, and\nparticularly the ePURE frameworks for medical images, showcasing their\nrobustness and efficacy in environments where traditional noiseless ground\ntruth cannot be obtained.\n", "link": "http://arxiv.org/abs/2407.15799v1", "date": "2024-07-22", "relevancy": 2.1555, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5927}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.532}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Extensions%20of%20Unbiased%20Risk%20Estimators%20for%20Unsupervised%0A%20%20Magnetic%20Resonance%20Image%20Denoising&body=Title%3A%20Adaptive%20Extensions%20of%20Unbiased%20Risk%20Estimators%20for%20Unsupervised%0A%20%20Magnetic%20Resonance%20Image%20Denoising%0AAuthor%3A%20Reeshad%20Khan%20and%20Dr.%20John%20Gauch%20and%20Dr.%20Ukash%20Nakarmi%0AAbstract%3A%20%20%20The%20application%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20to%20image%20denoising%20has%20notably%0Achallenged%20traditional%20denoising%20methods%2C%20particularly%20within%20complex%20noise%0Ascenarios%20prevalent%20in%20medical%20imaging.%20Despite%20the%20effectiveness%20of%0Atraditional%20and%20some%20DNN-based%20methods%2C%20their%20reliance%20on%20high-quality%2C%0Anoiseless%20ground%20truth%20images%20limits%20their%20practical%20utility.%20In%20response%20to%0Athis%2C%20our%20work%20introduces%20and%20benchmarks%20innovative%20unsupervised%20learning%0Astrategies%2C%20notably%20Stein%27s%20Unbiased%20Risk%20Estimator%20%28SURE%29%2C%20its%20extension%0A%28eSURE%29%2C%20and%20our%20novel%20implementation%2C%20the%20Extended%20Poisson%20Unbiased%20Risk%0AEstimator%20%28ePURE%29%2C%20within%20medical%20imaging%20frameworks.%0A%20%20This%20paper%20presents%20a%20comprehensive%20evaluation%20of%20these%20methods%20on%20MRI%20data%0Aafflicted%20with%20Gaussian%20and%20Poisson%20noise%20types%2C%20a%20scenario%20typical%20in%20medical%0Aimaging%20but%20challenging%20for%20most%20denoising%20algorithms.%20Our%20main%20contribution%0Alies%20in%20the%20effective%20adaptation%20and%20implementation%20of%20the%20SURE%2C%20eSURE%2C%20and%0Aparticularly%20the%20ePURE%20frameworks%20for%20medical%20images%2C%20showcasing%20their%0Arobustness%20and%20efficacy%20in%20environments%20where%20traditional%20noiseless%20ground%0Atruth%20cannot%20be%20obtained.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Extensions%2520of%2520Unbiased%2520Risk%2520Estimators%2520for%2520Unsupervised%250A%2520%2520Magnetic%2520Resonance%2520Image%2520Denoising%26entry.906535625%3DReeshad%2520Khan%2520and%2520Dr.%2520John%2520Gauch%2520and%2520Dr.%2520Ukash%2520Nakarmi%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520to%2520image%2520denoising%2520has%2520notably%250Achallenged%2520traditional%2520denoising%2520methods%252C%2520particularly%2520within%2520complex%2520noise%250Ascenarios%2520prevalent%2520in%2520medical%2520imaging.%2520Despite%2520the%2520effectiveness%2520of%250Atraditional%2520and%2520some%2520DNN-based%2520methods%252C%2520their%2520reliance%2520on%2520high-quality%252C%250Anoiseless%2520ground%2520truth%2520images%2520limits%2520their%2520practical%2520utility.%2520In%2520response%2520to%250Athis%252C%2520our%2520work%2520introduces%2520and%2520benchmarks%2520innovative%2520unsupervised%2520learning%250Astrategies%252C%2520notably%2520Stein%2527s%2520Unbiased%2520Risk%2520Estimator%2520%2528SURE%2529%252C%2520its%2520extension%250A%2528eSURE%2529%252C%2520and%2520our%2520novel%2520implementation%252C%2520the%2520Extended%2520Poisson%2520Unbiased%2520Risk%250AEstimator%2520%2528ePURE%2529%252C%2520within%2520medical%2520imaging%2520frameworks.%250A%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520evaluation%2520of%2520these%2520methods%2520on%2520MRI%2520data%250Aafflicted%2520with%2520Gaussian%2520and%2520Poisson%2520noise%2520types%252C%2520a%2520scenario%2520typical%2520in%2520medical%250Aimaging%2520but%2520challenging%2520for%2520most%2520denoising%2520algorithms.%2520Our%2520main%2520contribution%250Alies%2520in%2520the%2520effective%2520adaptation%2520and%2520implementation%2520of%2520the%2520SURE%252C%2520eSURE%252C%2520and%250Aparticularly%2520the%2520ePURE%2520frameworks%2520for%2520medical%2520images%252C%2520showcasing%2520their%250Arobustness%2520and%2520efficacy%2520in%2520environments%2520where%2520traditional%2520noiseless%2520ground%250Atruth%2520cannot%2520be%2520obtained.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Extensions%20of%20Unbiased%20Risk%20Estimators%20for%20Unsupervised%0A%20%20Magnetic%20Resonance%20Image%20Denoising&entry.906535625=Reeshad%20Khan%20and%20Dr.%20John%20Gauch%20and%20Dr.%20Ukash%20Nakarmi&entry.1292438233=%20%20The%20application%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20to%20image%20denoising%20has%20notably%0Achallenged%20traditional%20denoising%20methods%2C%20particularly%20within%20complex%20noise%0Ascenarios%20prevalent%20in%20medical%20imaging.%20Despite%20the%20effectiveness%20of%0Atraditional%20and%20some%20DNN-based%20methods%2C%20their%20reliance%20on%20high-quality%2C%0Anoiseless%20ground%20truth%20images%20limits%20their%20practical%20utility.%20In%20response%20to%0Athis%2C%20our%20work%20introduces%20and%20benchmarks%20innovative%20unsupervised%20learning%0Astrategies%2C%20notably%20Stein%27s%20Unbiased%20Risk%20Estimator%20%28SURE%29%2C%20its%20extension%0A%28eSURE%29%2C%20and%20our%20novel%20implementation%2C%20the%20Extended%20Poisson%20Unbiased%20Risk%0AEstimator%20%28ePURE%29%2C%20within%20medical%20imaging%20frameworks.%0A%20%20This%20paper%20presents%20a%20comprehensive%20evaluation%20of%20these%20methods%20on%20MRI%20data%0Aafflicted%20with%20Gaussian%20and%20Poisson%20noise%20types%2C%20a%20scenario%20typical%20in%20medical%0Aimaging%20but%20challenging%20for%20most%20denoising%20algorithms.%20Our%20main%20contribution%0Alies%20in%20the%20effective%20adaptation%20and%20implementation%20of%20the%20SURE%2C%20eSURE%2C%20and%0Aparticularly%20the%20ePURE%20frameworks%20for%20medical%20images%2C%20showcasing%20their%0Arobustness%20and%20efficacy%20in%20environments%20where%20traditional%20noiseless%20ground%0Atruth%20cannot%20be%20obtained.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15799v1&entry.124074799=Read"},
{"title": "Differentiable Product Quantization for Memory Efficient Camera\n  Relocalization", "author": "Zakaria Laskar and Iaroslav Melekhov and Assia Benbihi and Shuzhe Wang and Juho Kannala", "abstract": "  Camera relocalization relies on 3D models of the scene with a large memory\nfootprint that is incompatible with the memory budget of several applications.\nOne solution to reduce the scene memory size is map compression by removing\ncertain 3D points and descriptor quantization. This achieves high compression\nbut leads to performance drop due to information loss. To address the memory\nperformance trade-off, we train a light-weight scene-specific auto-encoder\nnetwork that performs descriptor quantization-dequantization in an end-to-end\ndifferentiable manner updating both product quantization centroids and network\nparameters through back-propagation. In addition to optimizing the network for\ndescriptor reconstruction, we encourage it to preserve the descriptor-matching\nperformance with margin-based metric loss functions. Results show that for a\nlocal descriptor memory of only 1MB, the synergistic combination of the\nproposed network and map compression achieves the best performance on the\nAachen Day-Night compared to existing compression methods.\n", "link": "http://arxiv.org/abs/2407.15540v1", "date": "2024-07-22", "relevancy": 2.1491, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.54}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5379}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Product%20Quantization%20for%20Memory%20Efficient%20Camera%0A%20%20Relocalization&body=Title%3A%20Differentiable%20Product%20Quantization%20for%20Memory%20Efficient%20Camera%0A%20%20Relocalization%0AAuthor%3A%20Zakaria%20Laskar%20and%20Iaroslav%20Melekhov%20and%20Assia%20Benbihi%20and%20Shuzhe%20Wang%20and%20Juho%20Kannala%0AAbstract%3A%20%20%20Camera%20relocalization%20relies%20on%203D%20models%20of%20the%20scene%20with%20a%20large%20memory%0Afootprint%20that%20is%20incompatible%20with%20the%20memory%20budget%20of%20several%20applications.%0AOne%20solution%20to%20reduce%20the%20scene%20memory%20size%20is%20map%20compression%20by%20removing%0Acertain%203D%20points%20and%20descriptor%20quantization.%20This%20achieves%20high%20compression%0Abut%20leads%20to%20performance%20drop%20due%20to%20information%20loss.%20To%20address%20the%20memory%0Aperformance%20trade-off%2C%20we%20train%20a%20light-weight%20scene-specific%20auto-encoder%0Anetwork%20that%20performs%20descriptor%20quantization-dequantization%20in%20an%20end-to-end%0Adifferentiable%20manner%20updating%20both%20product%20quantization%20centroids%20and%20network%0Aparameters%20through%20back-propagation.%20In%20addition%20to%20optimizing%20the%20network%20for%0Adescriptor%20reconstruction%2C%20we%20encourage%20it%20to%20preserve%20the%20descriptor-matching%0Aperformance%20with%20margin-based%20metric%20loss%20functions.%20Results%20show%20that%20for%20a%0Alocal%20descriptor%20memory%20of%20only%201MB%2C%20the%20synergistic%20combination%20of%20the%0Aproposed%20network%20and%20map%20compression%20achieves%20the%20best%20performance%20on%20the%0AAachen%20Day-Night%20compared%20to%20existing%20compression%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Product%2520Quantization%2520for%2520Memory%2520Efficient%2520Camera%250A%2520%2520Relocalization%26entry.906535625%3DZakaria%2520Laskar%2520and%2520Iaroslav%2520Melekhov%2520and%2520Assia%2520Benbihi%2520and%2520Shuzhe%2520Wang%2520and%2520Juho%2520Kannala%26entry.1292438233%3D%2520%2520Camera%2520relocalization%2520relies%2520on%25203D%2520models%2520of%2520the%2520scene%2520with%2520a%2520large%2520memory%250Afootprint%2520that%2520is%2520incompatible%2520with%2520the%2520memory%2520budget%2520of%2520several%2520applications.%250AOne%2520solution%2520to%2520reduce%2520the%2520scene%2520memory%2520size%2520is%2520map%2520compression%2520by%2520removing%250Acertain%25203D%2520points%2520and%2520descriptor%2520quantization.%2520This%2520achieves%2520high%2520compression%250Abut%2520leads%2520to%2520performance%2520drop%2520due%2520to%2520information%2520loss.%2520To%2520address%2520the%2520memory%250Aperformance%2520trade-off%252C%2520we%2520train%2520a%2520light-weight%2520scene-specific%2520auto-encoder%250Anetwork%2520that%2520performs%2520descriptor%2520quantization-dequantization%2520in%2520an%2520end-to-end%250Adifferentiable%2520manner%2520updating%2520both%2520product%2520quantization%2520centroids%2520and%2520network%250Aparameters%2520through%2520back-propagation.%2520In%2520addition%2520to%2520optimizing%2520the%2520network%2520for%250Adescriptor%2520reconstruction%252C%2520we%2520encourage%2520it%2520to%2520preserve%2520the%2520descriptor-matching%250Aperformance%2520with%2520margin-based%2520metric%2520loss%2520functions.%2520Results%2520show%2520that%2520for%2520a%250Alocal%2520descriptor%2520memory%2520of%2520only%25201MB%252C%2520the%2520synergistic%2520combination%2520of%2520the%250Aproposed%2520network%2520and%2520map%2520compression%2520achieves%2520the%2520best%2520performance%2520on%2520the%250AAachen%2520Day-Night%2520compared%2520to%2520existing%2520compression%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Product%20Quantization%20for%20Memory%20Efficient%20Camera%0A%20%20Relocalization&entry.906535625=Zakaria%20Laskar%20and%20Iaroslav%20Melekhov%20and%20Assia%20Benbihi%20and%20Shuzhe%20Wang%20and%20Juho%20Kannala&entry.1292438233=%20%20Camera%20relocalization%20relies%20on%203D%20models%20of%20the%20scene%20with%20a%20large%20memory%0Afootprint%20that%20is%20incompatible%20with%20the%20memory%20budget%20of%20several%20applications.%0AOne%20solution%20to%20reduce%20the%20scene%20memory%20size%20is%20map%20compression%20by%20removing%0Acertain%203D%20points%20and%20descriptor%20quantization.%20This%20achieves%20high%20compression%0Abut%20leads%20to%20performance%20drop%20due%20to%20information%20loss.%20To%20address%20the%20memory%0Aperformance%20trade-off%2C%20we%20train%20a%20light-weight%20scene-specific%20auto-encoder%0Anetwork%20that%20performs%20descriptor%20quantization-dequantization%20in%20an%20end-to-end%0Adifferentiable%20manner%20updating%20both%20product%20quantization%20centroids%20and%20network%0Aparameters%20through%20back-propagation.%20In%20addition%20to%20optimizing%20the%20network%20for%0Adescriptor%20reconstruction%2C%20we%20encourage%20it%20to%20preserve%20the%20descriptor-matching%0Aperformance%20with%20margin-based%20metric%20loss%20functions.%20Results%20show%20that%20for%20a%0Alocal%20descriptor%20memory%20of%20only%201MB%2C%20the%20synergistic%20combination%20of%20the%0Aproposed%20network%20and%20map%20compression%20achieves%20the%20best%20performance%20on%20the%0AAachen%20Day-Night%20compared%20to%20existing%20compression%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15540v1&entry.124074799=Read"},
{"title": "Reconstructing Training Data From Real World Models Trained with\n  Transfer Learning", "author": "Yakir Oz and Gilad Yehudai and Gal Vardi and Itai Antebi and Michal Irani and Niv Haim", "abstract": "  Current methods for reconstructing training data from trained classifiers are\nrestricted to very small models, limited training set sizes, and low-resolution\nimages. Such restrictions hinder their applicability to real-world scenarios.\nIn this paper, we present a novel approach enabling data reconstruction in\nrealistic settings for models trained on high-resolution images. Our method\nadapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --\nspecifically, targeting models trained via transfer learning over image\nembeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs\ndata reconstruction in the embedding space rather than in the image space,\nshowcasing its applicability beyond visual data. Moreover, we introduce a novel\nclustering-based method to identify good reconstructions from thousands of\ncandidates. This significantly improves on previous works that relied on\nknowledge of the training set to identify good reconstructed images. Our\nfindings shed light on a potential privacy risk for data leakage from models\ntrained using transfer learning.\n", "link": "http://arxiv.org/abs/2407.15845v1", "date": "2024-07-22", "relevancy": 2.145, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20Training%20Data%20From%20Real%20World%20Models%20Trained%20with%0A%20%20Transfer%20Learning&body=Title%3A%20Reconstructing%20Training%20Data%20From%20Real%20World%20Models%20Trained%20with%0A%20%20Transfer%20Learning%0AAuthor%3A%20Yakir%20Oz%20and%20Gilad%20Yehudai%20and%20Gal%20Vardi%20and%20Itai%20Antebi%20and%20Michal%20Irani%20and%20Niv%20Haim%0AAbstract%3A%20%20%20Current%20methods%20for%20reconstructing%20training%20data%20from%20trained%20classifiers%20are%0Arestricted%20to%20very%20small%20models%2C%20limited%20training%20set%20sizes%2C%20and%20low-resolution%0Aimages.%20Such%20restrictions%20hinder%20their%20applicability%20to%20real-world%20scenarios.%0AIn%20this%20paper%2C%20we%20present%20a%20novel%20approach%20enabling%20data%20reconstruction%20in%0Arealistic%20settings%20for%20models%20trained%20on%20high-resolution%20images.%20Our%20method%0Aadapts%20the%20reconstruction%20scheme%20of%20arXiv%3A2206.07758%20to%20real-world%20scenarios%20--%0Aspecifically%2C%20targeting%20models%20trained%20via%20transfer%20learning%20over%20image%0Aembeddings%20of%20large%20pre-trained%20models%20like%20DINO-ViT%20and%20CLIP.%20Our%20work%20employs%0Adata%20reconstruction%20in%20the%20embedding%20space%20rather%20than%20in%20the%20image%20space%2C%0Ashowcasing%20its%20applicability%20beyond%20visual%20data.%20Moreover%2C%20we%20introduce%20a%20novel%0Aclustering-based%20method%20to%20identify%20good%20reconstructions%20from%20thousands%20of%0Acandidates.%20This%20significantly%20improves%20on%20previous%20works%20that%20relied%20on%0Aknowledge%20of%20the%20training%20set%20to%20identify%20good%20reconstructed%20images.%20Our%0Afindings%20shed%20light%20on%20a%20potential%20privacy%20risk%20for%20data%20leakage%20from%20models%0Atrained%20using%20transfer%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520Training%2520Data%2520From%2520Real%2520World%2520Models%2520Trained%2520with%250A%2520%2520Transfer%2520Learning%26entry.906535625%3DYakir%2520Oz%2520and%2520Gilad%2520Yehudai%2520and%2520Gal%2520Vardi%2520and%2520Itai%2520Antebi%2520and%2520Michal%2520Irani%2520and%2520Niv%2520Haim%26entry.1292438233%3D%2520%2520Current%2520methods%2520for%2520reconstructing%2520training%2520data%2520from%2520trained%2520classifiers%2520are%250Arestricted%2520to%2520very%2520small%2520models%252C%2520limited%2520training%2520set%2520sizes%252C%2520and%2520low-resolution%250Aimages.%2520Such%2520restrictions%2520hinder%2520their%2520applicability%2520to%2520real-world%2520scenarios.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520enabling%2520data%2520reconstruction%2520in%250Arealistic%2520settings%2520for%2520models%2520trained%2520on%2520high-resolution%2520images.%2520Our%2520method%250Aadapts%2520the%2520reconstruction%2520scheme%2520of%2520arXiv%253A2206.07758%2520to%2520real-world%2520scenarios%2520--%250Aspecifically%252C%2520targeting%2520models%2520trained%2520via%2520transfer%2520learning%2520over%2520image%250Aembeddings%2520of%2520large%2520pre-trained%2520models%2520like%2520DINO-ViT%2520and%2520CLIP.%2520Our%2520work%2520employs%250Adata%2520reconstruction%2520in%2520the%2520embedding%2520space%2520rather%2520than%2520in%2520the%2520image%2520space%252C%250Ashowcasing%2520its%2520applicability%2520beyond%2520visual%2520data.%2520Moreover%252C%2520we%2520introduce%2520a%2520novel%250Aclustering-based%2520method%2520to%2520identify%2520good%2520reconstructions%2520from%2520thousands%2520of%250Acandidates.%2520This%2520significantly%2520improves%2520on%2520previous%2520works%2520that%2520relied%2520on%250Aknowledge%2520of%2520the%2520training%2520set%2520to%2520identify%2520good%2520reconstructed%2520images.%2520Our%250Afindings%2520shed%2520light%2520on%2520a%2520potential%2520privacy%2520risk%2520for%2520data%2520leakage%2520from%2520models%250Atrained%2520using%2520transfer%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20Training%20Data%20From%20Real%20World%20Models%20Trained%20with%0A%20%20Transfer%20Learning&entry.906535625=Yakir%20Oz%20and%20Gilad%20Yehudai%20and%20Gal%20Vardi%20and%20Itai%20Antebi%20and%20Michal%20Irani%20and%20Niv%20Haim&entry.1292438233=%20%20Current%20methods%20for%20reconstructing%20training%20data%20from%20trained%20classifiers%20are%0Arestricted%20to%20very%20small%20models%2C%20limited%20training%20set%20sizes%2C%20and%20low-resolution%0Aimages.%20Such%20restrictions%20hinder%20their%20applicability%20to%20real-world%20scenarios.%0AIn%20this%20paper%2C%20we%20present%20a%20novel%20approach%20enabling%20data%20reconstruction%20in%0Arealistic%20settings%20for%20models%20trained%20on%20high-resolution%20images.%20Our%20method%0Aadapts%20the%20reconstruction%20scheme%20of%20arXiv%3A2206.07758%20to%20real-world%20scenarios%20--%0Aspecifically%2C%20targeting%20models%20trained%20via%20transfer%20learning%20over%20image%0Aembeddings%20of%20large%20pre-trained%20models%20like%20DINO-ViT%20and%20CLIP.%20Our%20work%20employs%0Adata%20reconstruction%20in%20the%20embedding%20space%20rather%20than%20in%20the%20image%20space%2C%0Ashowcasing%20its%20applicability%20beyond%20visual%20data.%20Moreover%2C%20we%20introduce%20a%20novel%0Aclustering-based%20method%20to%20identify%20good%20reconstructions%20from%20thousands%20of%0Acandidates.%20This%20significantly%20improves%20on%20previous%20works%20that%20relied%20on%0Aknowledge%20of%20the%20training%20set%20to%20identify%20good%20reconstructed%20images.%20Our%0Afindings%20shed%20light%20on%20a%20potential%20privacy%20risk%20for%20data%20leakage%20from%20models%0Atrained%20using%20transfer%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15845v1&entry.124074799=Read"},
{"title": "Perceptions of Linguistic Uncertainty by Language Models and Humans", "author": "Catarina G Belem and Markelle Kelly and Mark Steyvers and Sameer Singh and Padhraic Smyth", "abstract": "  Uncertainty expressions such as ``probably'' or ``highly unlikely'' are\npervasive in human language. While prior work has established that there is\npopulation-level agreement in terms of how humans interpret these expressions,\nthere has been little inquiry into the abilities of language models to\ninterpret such expressions. In this paper, we investigate how language models\nmap linguistic expressions of uncertainty to numerical responses. Our approach\nassesses whether language models can employ theory of mind in this setting:\nunderstanding the uncertainty of another agent about a particular statement,\nindependently of the model's own certainty about that statement. We evaluate\nboth humans and 10 popular language models on a task created to assess these\nabilities. Unexpectedly, we find that 8 out of 10 models are able to map\nuncertainty expressions to probabilistic responses in a human-like manner.\nHowever, we observe systematically different behavior depending on whether a\nstatement is actually true or false. This sensitivity indicates that language\nmodels are substantially more susceptible to bias based on their prior\nknowledge (as compared to humans). These findings raise important questions and\nhave broad implications for human-AI alignment and AI-AI communication.\n", "link": "http://arxiv.org/abs/2407.15814v1", "date": "2024-07-22", "relevancy": 2.1447, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5524}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5387}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceptions%20of%20Linguistic%20Uncertainty%20by%20Language%20Models%20and%20Humans&body=Title%3A%20Perceptions%20of%20Linguistic%20Uncertainty%20by%20Language%20Models%20and%20Humans%0AAuthor%3A%20Catarina%20G%20Belem%20and%20Markelle%20Kelly%20and%20Mark%20Steyvers%20and%20Sameer%20Singh%20and%20Padhraic%20Smyth%0AAbstract%3A%20%20%20Uncertainty%20expressions%20such%20as%20%60%60probably%27%27%20or%20%60%60highly%20unlikely%27%27%20are%0Apervasive%20in%20human%20language.%20While%20prior%20work%20has%20established%20that%20there%20is%0Apopulation-level%20agreement%20in%20terms%20of%20how%20humans%20interpret%20these%20expressions%2C%0Athere%20has%20been%20little%20inquiry%20into%20the%20abilities%20of%20language%20models%20to%0Ainterpret%20such%20expressions.%20In%20this%20paper%2C%20we%20investigate%20how%20language%20models%0Amap%20linguistic%20expressions%20of%20uncertainty%20to%20numerical%20responses.%20Our%20approach%0Aassesses%20whether%20language%20models%20can%20employ%20theory%20of%20mind%20in%20this%20setting%3A%0Aunderstanding%20the%20uncertainty%20of%20another%20agent%20about%20a%20particular%20statement%2C%0Aindependently%20of%20the%20model%27s%20own%20certainty%20about%20that%20statement.%20We%20evaluate%0Aboth%20humans%20and%2010%20popular%20language%20models%20on%20a%20task%20created%20to%20assess%20these%0Aabilities.%20Unexpectedly%2C%20we%20find%20that%208%20out%20of%2010%20models%20are%20able%20to%20map%0Auncertainty%20expressions%20to%20probabilistic%20responses%20in%20a%20human-like%20manner.%0AHowever%2C%20we%20observe%20systematically%20different%20behavior%20depending%20on%20whether%20a%0Astatement%20is%20actually%20true%20or%20false.%20This%20sensitivity%20indicates%20that%20language%0Amodels%20are%20substantially%20more%20susceptible%20to%20bias%20based%20on%20their%20prior%0Aknowledge%20%28as%20compared%20to%20humans%29.%20These%20findings%20raise%20important%20questions%20and%0Ahave%20broad%20implications%20for%20human-AI%20alignment%20and%20AI-AI%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceptions%2520of%2520Linguistic%2520Uncertainty%2520by%2520Language%2520Models%2520and%2520Humans%26entry.906535625%3DCatarina%2520G%2520Belem%2520and%2520Markelle%2520Kelly%2520and%2520Mark%2520Steyvers%2520and%2520Sameer%2520Singh%2520and%2520Padhraic%2520Smyth%26entry.1292438233%3D%2520%2520Uncertainty%2520expressions%2520such%2520as%2520%2560%2560probably%2527%2527%2520or%2520%2560%2560highly%2520unlikely%2527%2527%2520are%250Apervasive%2520in%2520human%2520language.%2520While%2520prior%2520work%2520has%2520established%2520that%2520there%2520is%250Apopulation-level%2520agreement%2520in%2520terms%2520of%2520how%2520humans%2520interpret%2520these%2520expressions%252C%250Athere%2520has%2520been%2520little%2520inquiry%2520into%2520the%2520abilities%2520of%2520language%2520models%2520to%250Ainterpret%2520such%2520expressions.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520language%2520models%250Amap%2520linguistic%2520expressions%2520of%2520uncertainty%2520to%2520numerical%2520responses.%2520Our%2520approach%250Aassesses%2520whether%2520language%2520models%2520can%2520employ%2520theory%2520of%2520mind%2520in%2520this%2520setting%253A%250Aunderstanding%2520the%2520uncertainty%2520of%2520another%2520agent%2520about%2520a%2520particular%2520statement%252C%250Aindependently%2520of%2520the%2520model%2527s%2520own%2520certainty%2520about%2520that%2520statement.%2520We%2520evaluate%250Aboth%2520humans%2520and%252010%2520popular%2520language%2520models%2520on%2520a%2520task%2520created%2520to%2520assess%2520these%250Aabilities.%2520Unexpectedly%252C%2520we%2520find%2520that%25208%2520out%2520of%252010%2520models%2520are%2520able%2520to%2520map%250Auncertainty%2520expressions%2520to%2520probabilistic%2520responses%2520in%2520a%2520human-like%2520manner.%250AHowever%252C%2520we%2520observe%2520systematically%2520different%2520behavior%2520depending%2520on%2520whether%2520a%250Astatement%2520is%2520actually%2520true%2520or%2520false.%2520This%2520sensitivity%2520indicates%2520that%2520language%250Amodels%2520are%2520substantially%2520more%2520susceptible%2520to%2520bias%2520based%2520on%2520their%2520prior%250Aknowledge%2520%2528as%2520compared%2520to%2520humans%2529.%2520These%2520findings%2520raise%2520important%2520questions%2520and%250Ahave%2520broad%2520implications%2520for%2520human-AI%2520alignment%2520and%2520AI-AI%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceptions%20of%20Linguistic%20Uncertainty%20by%20Language%20Models%20and%20Humans&entry.906535625=Catarina%20G%20Belem%20and%20Markelle%20Kelly%20and%20Mark%20Steyvers%20and%20Sameer%20Singh%20and%20Padhraic%20Smyth&entry.1292438233=%20%20Uncertainty%20expressions%20such%20as%20%60%60probably%27%27%20or%20%60%60highly%20unlikely%27%27%20are%0Apervasive%20in%20human%20language.%20While%20prior%20work%20has%20established%20that%20there%20is%0Apopulation-level%20agreement%20in%20terms%20of%20how%20humans%20interpret%20these%20expressions%2C%0Athere%20has%20been%20little%20inquiry%20into%20the%20abilities%20of%20language%20models%20to%0Ainterpret%20such%20expressions.%20In%20this%20paper%2C%20we%20investigate%20how%20language%20models%0Amap%20linguistic%20expressions%20of%20uncertainty%20to%20numerical%20responses.%20Our%20approach%0Aassesses%20whether%20language%20models%20can%20employ%20theory%20of%20mind%20in%20this%20setting%3A%0Aunderstanding%20the%20uncertainty%20of%20another%20agent%20about%20a%20particular%20statement%2C%0Aindependently%20of%20the%20model%27s%20own%20certainty%20about%20that%20statement.%20We%20evaluate%0Aboth%20humans%20and%2010%20popular%20language%20models%20on%20a%20task%20created%20to%20assess%20these%0Aabilities.%20Unexpectedly%2C%20we%20find%20that%208%20out%20of%2010%20models%20are%20able%20to%20map%0Auncertainty%20expressions%20to%20probabilistic%20responses%20in%20a%20human-like%20manner.%0AHowever%2C%20we%20observe%20systematically%20different%20behavior%20depending%20on%20whether%20a%0Astatement%20is%20actually%20true%20or%20false.%20This%20sensitivity%20indicates%20that%20language%0Amodels%20are%20substantially%20more%20susceptible%20to%20bias%20based%20on%20their%20prior%0Aknowledge%20%28as%20compared%20to%20humans%29.%20These%20findings%20raise%20important%20questions%20and%0Ahave%20broad%20implications%20for%20human-AI%20alignment%20and%20AI-AI%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15814v1&entry.124074799=Read"},
{"title": "SS-SFR: Synthetic Scenes Spatial Frequency Response on Virtual KITTI and\n  Degraded Automotive Simulations for Object Detection", "author": "Daniel Jakab and Alexander Braun and Cathaoir Agnew and Reenu Mohandas and Brian Michael Deegan and Dara Molloy and Enda Ward and Tony Scanlan and Ciar\u00e1n Eising", "abstract": "  Automotive simulation can potentially compensate for a lack of training data\nin computer vision applications. However, there has been little to no image\nquality evaluation of automotive simulation and the impact of optical\ndegradations on simulation is little explored. In this work, we investigate\nVirtual KITTI and the impact of applying variations of Gaussian blur on image\nsharpness. Furthermore, we consider object detection, a common computer vision\napplication on three different state-of-the-art models, thus allowing us to\ncharacterize the relationship between object detection and sharpness. It was\nfound that while image sharpness (MTF50) degrades from an average of 0.245cy/px\nto approximately 0.119cy/px; object detection performance stays largely robust\nwithin 0.58\\%(Faster RCNN), 1.45\\%(YOLOF) and 1.93\\%(DETR) across all\nrespective held-out test sets.\n", "link": "http://arxiv.org/abs/2407.15646v1", "date": "2024-07-22", "relevancy": 2.1317, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5439}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5253}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SS-SFR%3A%20Synthetic%20Scenes%20Spatial%20Frequency%20Response%20on%20Virtual%20KITTI%20and%0A%20%20Degraded%20Automotive%20Simulations%20for%20Object%20Detection&body=Title%3A%20SS-SFR%3A%20Synthetic%20Scenes%20Spatial%20Frequency%20Response%20on%20Virtual%20KITTI%20and%0A%20%20Degraded%20Automotive%20Simulations%20for%20Object%20Detection%0AAuthor%3A%20Daniel%20Jakab%20and%20Alexander%20Braun%20and%20Cathaoir%20Agnew%20and%20Reenu%20Mohandas%20and%20Brian%20Michael%20Deegan%20and%20Dara%20Molloy%20and%20Enda%20Ward%20and%20Tony%20Scanlan%20and%20Ciar%C3%A1n%20Eising%0AAbstract%3A%20%20%20Automotive%20simulation%20can%20potentially%20compensate%20for%20a%20lack%20of%20training%20data%0Ain%20computer%20vision%20applications.%20However%2C%20there%20has%20been%20little%20to%20no%20image%0Aquality%20evaluation%20of%20automotive%20simulation%20and%20the%20impact%20of%20optical%0Adegradations%20on%20simulation%20is%20little%20explored.%20In%20this%20work%2C%20we%20investigate%0AVirtual%20KITTI%20and%20the%20impact%20of%20applying%20variations%20of%20Gaussian%20blur%20on%20image%0Asharpness.%20Furthermore%2C%20we%20consider%20object%20detection%2C%20a%20common%20computer%20vision%0Aapplication%20on%20three%20different%20state-of-the-art%20models%2C%20thus%20allowing%20us%20to%0Acharacterize%20the%20relationship%20between%20object%20detection%20and%20sharpness.%20It%20was%0Afound%20that%20while%20image%20sharpness%20%28MTF50%29%20degrades%20from%20an%20average%20of%200.245cy/px%0Ato%20approximately%200.119cy/px%3B%20object%20detection%20performance%20stays%20largely%20robust%0Awithin%200.58%5C%25%28Faster%20RCNN%29%2C%201.45%5C%25%28YOLOF%29%20and%201.93%5C%25%28DETR%29%20across%20all%0Arespective%20held-out%20test%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSS-SFR%253A%2520Synthetic%2520Scenes%2520Spatial%2520Frequency%2520Response%2520on%2520Virtual%2520KITTI%2520and%250A%2520%2520Degraded%2520Automotive%2520Simulations%2520for%2520Object%2520Detection%26entry.906535625%3DDaniel%2520Jakab%2520and%2520Alexander%2520Braun%2520and%2520Cathaoir%2520Agnew%2520and%2520Reenu%2520Mohandas%2520and%2520Brian%2520Michael%2520Deegan%2520and%2520Dara%2520Molloy%2520and%2520Enda%2520Ward%2520and%2520Tony%2520Scanlan%2520and%2520Ciar%25C3%25A1n%2520Eising%26entry.1292438233%3D%2520%2520Automotive%2520simulation%2520can%2520potentially%2520compensate%2520for%2520a%2520lack%2520of%2520training%2520data%250Ain%2520computer%2520vision%2520applications.%2520However%252C%2520there%2520has%2520been%2520little%2520to%2520no%2520image%250Aquality%2520evaluation%2520of%2520automotive%2520simulation%2520and%2520the%2520impact%2520of%2520optical%250Adegradations%2520on%2520simulation%2520is%2520little%2520explored.%2520In%2520this%2520work%252C%2520we%2520investigate%250AVirtual%2520KITTI%2520and%2520the%2520impact%2520of%2520applying%2520variations%2520of%2520Gaussian%2520blur%2520on%2520image%250Asharpness.%2520Furthermore%252C%2520we%2520consider%2520object%2520detection%252C%2520a%2520common%2520computer%2520vision%250Aapplication%2520on%2520three%2520different%2520state-of-the-art%2520models%252C%2520thus%2520allowing%2520us%2520to%250Acharacterize%2520the%2520relationship%2520between%2520object%2520detection%2520and%2520sharpness.%2520It%2520was%250Afound%2520that%2520while%2520image%2520sharpness%2520%2528MTF50%2529%2520degrades%2520from%2520an%2520average%2520of%25200.245cy/px%250Ato%2520approximately%25200.119cy/px%253B%2520object%2520detection%2520performance%2520stays%2520largely%2520robust%250Awithin%25200.58%255C%2525%2528Faster%2520RCNN%2529%252C%25201.45%255C%2525%2528YOLOF%2529%2520and%25201.93%255C%2525%2528DETR%2529%2520across%2520all%250Arespective%2520held-out%2520test%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SS-SFR%3A%20Synthetic%20Scenes%20Spatial%20Frequency%20Response%20on%20Virtual%20KITTI%20and%0A%20%20Degraded%20Automotive%20Simulations%20for%20Object%20Detection&entry.906535625=Daniel%20Jakab%20and%20Alexander%20Braun%20and%20Cathaoir%20Agnew%20and%20Reenu%20Mohandas%20and%20Brian%20Michael%20Deegan%20and%20Dara%20Molloy%20and%20Enda%20Ward%20and%20Tony%20Scanlan%20and%20Ciar%C3%A1n%20Eising&entry.1292438233=%20%20Automotive%20simulation%20can%20potentially%20compensate%20for%20a%20lack%20of%20training%20data%0Ain%20computer%20vision%20applications.%20However%2C%20there%20has%20been%20little%20to%20no%20image%0Aquality%20evaluation%20of%20automotive%20simulation%20and%20the%20impact%20of%20optical%0Adegradations%20on%20simulation%20is%20little%20explored.%20In%20this%20work%2C%20we%20investigate%0AVirtual%20KITTI%20and%20the%20impact%20of%20applying%20variations%20of%20Gaussian%20blur%20on%20image%0Asharpness.%20Furthermore%2C%20we%20consider%20object%20detection%2C%20a%20common%20computer%20vision%0Aapplication%20on%20three%20different%20state-of-the-art%20models%2C%20thus%20allowing%20us%20to%0Acharacterize%20the%20relationship%20between%20object%20detection%20and%20sharpness.%20It%20was%0Afound%20that%20while%20image%20sharpness%20%28MTF50%29%20degrades%20from%20an%20average%20of%200.245cy/px%0Ato%20approximately%200.119cy/px%3B%20object%20detection%20performance%20stays%20largely%20robust%0Awithin%200.58%5C%25%28Faster%20RCNN%29%2C%201.45%5C%25%28YOLOF%29%20and%201.93%5C%25%28DETR%29%20across%20all%0Arespective%20held-out%20test%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15646v1&entry.124074799=Read"},
{"title": "ACEGEN: Reinforcement learning of generative chemical agents for drug\n  discovery", "author": "Albert Bou and Morgan Thomas and Sebastian Dittert and Carles Navarro Ram\u00edrez and Maciej Majewski and Ye Wang and Shivam Patel and Gary Tresadern and Mazen Ahmad and Vincent Moens and Woody Sherman and Simone Sciabola and Gianni De Fabritiis", "abstract": "  In recent years, reinforcement learning (RL) has emerged as a valuable tool\nin drug design, offering the potential to propose and optimize molecules with\ndesired properties. However, striking a balance between capabilities,\nflexibility, reliability, and efficiency remains challenging due to the\ncomplexity of advanced RL algorithms and the significant reliance on\nspecialized code. In this work, we introduce ACEGEN, a comprehensive and\nstreamlined toolkit tailored for generative drug design, built using TorchRL, a\nmodern RL library that offers thoroughly tested reusable components. We\nvalidate ACEGEN by benchmarking against other published generative modeling\nalgorithms and show comparable or improved performance. We also show examples\nof ACEGEN applied in multiple drug discovery case studies. ACEGEN is accessible\nat \\url{https://github.com/acellera/acegen-open} and available for use under\nthe MIT license.\n", "link": "http://arxiv.org/abs/2405.04657v3", "date": "2024-07-22", "relevancy": 2.1229, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5615}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5115}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACEGEN%3A%20Reinforcement%20learning%20of%20generative%20chemical%20agents%20for%20drug%0A%20%20discovery&body=Title%3A%20ACEGEN%3A%20Reinforcement%20learning%20of%20generative%20chemical%20agents%20for%20drug%0A%20%20discovery%0AAuthor%3A%20Albert%20Bou%20and%20Morgan%20Thomas%20and%20Sebastian%20Dittert%20and%20Carles%20Navarro%20Ram%C3%ADrez%20and%20Maciej%20Majewski%20and%20Ye%20Wang%20and%20Shivam%20Patel%20and%20Gary%20Tresadern%20and%20Mazen%20Ahmad%20and%20Vincent%20Moens%20and%20Woody%20Sherman%20and%20Simone%20Sciabola%20and%20Gianni%20De%20Fabritiis%0AAbstract%3A%20%20%20In%20recent%20years%2C%20reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20valuable%20tool%0Ain%20drug%20design%2C%20offering%20the%20potential%20to%20propose%20and%20optimize%20molecules%20with%0Adesired%20properties.%20However%2C%20striking%20a%20balance%20between%20capabilities%2C%0Aflexibility%2C%20reliability%2C%20and%20efficiency%20remains%20challenging%20due%20to%20the%0Acomplexity%20of%20advanced%20RL%20algorithms%20and%20the%20significant%20reliance%20on%0Aspecialized%20code.%20In%20this%20work%2C%20we%20introduce%20ACEGEN%2C%20a%20comprehensive%20and%0Astreamlined%20toolkit%20tailored%20for%20generative%20drug%20design%2C%20built%20using%20TorchRL%2C%20a%0Amodern%20RL%20library%20that%20offers%20thoroughly%20tested%20reusable%20components.%20We%0Avalidate%20ACEGEN%20by%20benchmarking%20against%20other%20published%20generative%20modeling%0Aalgorithms%20and%20show%20comparable%20or%20improved%20performance.%20We%20also%20show%20examples%0Aof%20ACEGEN%20applied%20in%20multiple%20drug%20discovery%20case%20studies.%20ACEGEN%20is%20accessible%0Aat%20%5Curl%7Bhttps%3A//github.com/acellera/acegen-open%7D%20and%20available%20for%20use%20under%0Athe%20MIT%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04657v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACEGEN%253A%2520Reinforcement%2520learning%2520of%2520generative%2520chemical%2520agents%2520for%2520drug%250A%2520%2520discovery%26entry.906535625%3DAlbert%2520Bou%2520and%2520Morgan%2520Thomas%2520and%2520Sebastian%2520Dittert%2520and%2520Carles%2520Navarro%2520Ram%25C3%25ADrez%2520and%2520Maciej%2520Majewski%2520and%2520Ye%2520Wang%2520and%2520Shivam%2520Patel%2520and%2520Gary%2520Tresadern%2520and%2520Mazen%2520Ahmad%2520and%2520Vincent%2520Moens%2520and%2520Woody%2520Sherman%2520and%2520Simone%2520Sciabola%2520and%2520Gianni%2520De%2520Fabritiis%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520valuable%2520tool%250Ain%2520drug%2520design%252C%2520offering%2520the%2520potential%2520to%2520propose%2520and%2520optimize%2520molecules%2520with%250Adesired%2520properties.%2520However%252C%2520striking%2520a%2520balance%2520between%2520capabilities%252C%250Aflexibility%252C%2520reliability%252C%2520and%2520efficiency%2520remains%2520challenging%2520due%2520to%2520the%250Acomplexity%2520of%2520advanced%2520RL%2520algorithms%2520and%2520the%2520significant%2520reliance%2520on%250Aspecialized%2520code.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ACEGEN%252C%2520a%2520comprehensive%2520and%250Astreamlined%2520toolkit%2520tailored%2520for%2520generative%2520drug%2520design%252C%2520built%2520using%2520TorchRL%252C%2520a%250Amodern%2520RL%2520library%2520that%2520offers%2520thoroughly%2520tested%2520reusable%2520components.%2520We%250Avalidate%2520ACEGEN%2520by%2520benchmarking%2520against%2520other%2520published%2520generative%2520modeling%250Aalgorithms%2520and%2520show%2520comparable%2520or%2520improved%2520performance.%2520We%2520also%2520show%2520examples%250Aof%2520ACEGEN%2520applied%2520in%2520multiple%2520drug%2520discovery%2520case%2520studies.%2520ACEGEN%2520is%2520accessible%250Aat%2520%255Curl%257Bhttps%253A//github.com/acellera/acegen-open%257D%2520and%2520available%2520for%2520use%2520under%250Athe%2520MIT%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04657v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACEGEN%3A%20Reinforcement%20learning%20of%20generative%20chemical%20agents%20for%20drug%0A%20%20discovery&entry.906535625=Albert%20Bou%20and%20Morgan%20Thomas%20and%20Sebastian%20Dittert%20and%20Carles%20Navarro%20Ram%C3%ADrez%20and%20Maciej%20Majewski%20and%20Ye%20Wang%20and%20Shivam%20Patel%20and%20Gary%20Tresadern%20and%20Mazen%20Ahmad%20and%20Vincent%20Moens%20and%20Woody%20Sherman%20and%20Simone%20Sciabola%20and%20Gianni%20De%20Fabritiis&entry.1292438233=%20%20In%20recent%20years%2C%20reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20valuable%20tool%0Ain%20drug%20design%2C%20offering%20the%20potential%20to%20propose%20and%20optimize%20molecules%20with%0Adesired%20properties.%20However%2C%20striking%20a%20balance%20between%20capabilities%2C%0Aflexibility%2C%20reliability%2C%20and%20efficiency%20remains%20challenging%20due%20to%20the%0Acomplexity%20of%20advanced%20RL%20algorithms%20and%20the%20significant%20reliance%20on%0Aspecialized%20code.%20In%20this%20work%2C%20we%20introduce%20ACEGEN%2C%20a%20comprehensive%20and%0Astreamlined%20toolkit%20tailored%20for%20generative%20drug%20design%2C%20built%20using%20TorchRL%2C%20a%0Amodern%20RL%20library%20that%20offers%20thoroughly%20tested%20reusable%20components.%20We%0Avalidate%20ACEGEN%20by%20benchmarking%20against%20other%20published%20generative%20modeling%0Aalgorithms%20and%20show%20comparable%20or%20improved%20performance.%20We%20also%20show%20examples%0Aof%20ACEGEN%20applied%20in%20multiple%20drug%20discovery%20case%20studies.%20ACEGEN%20is%20accessible%0Aat%20%5Curl%7Bhttps%3A//github.com/acellera/acegen-open%7D%20and%20available%20for%20use%20under%0Athe%20MIT%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04657v3&entry.124074799=Read"},
{"title": "Towards a Universal Evaluation Model for Careful and Competent\n  Autonomous Driving", "author": "Kethan Reddy and Elias Nassif and Panagiotis Angeloudis and Mohammed Quddus and Washington Ochieng", "abstract": "  Virtual scenario-based testing methods to validate autonomous driving systems\nare predominantly centred around collision avoidance, and lack a comprehensive\napproach to evaluate optimal driving behaviour holistically. Furthermore,\ncurrent validation approaches do not align with authorisation and monitoring\nrequirements put forth by regulatory bodies. We address these validation gaps\nby outlining a universal evaluation framework that: incorporates the notion of\ncareful and competent driving, unifies behavioural competencies and evaluation\ncriteria, and is amenable at a scenario-specific and aggregate behaviour level.\nThis framework can be leveraged to evaluate optimal driving in scenario-based\ntesting, and for post-deployment monitoring to ensure continual compliance with\nregulation and safety standards.\n", "link": "http://arxiv.org/abs/2407.15596v1", "date": "2024-07-22", "relevancy": 2.1148, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5779}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5083}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Universal%20Evaluation%20Model%20for%20Careful%20and%20Competent%0A%20%20Autonomous%20Driving&body=Title%3A%20Towards%20a%20Universal%20Evaluation%20Model%20for%20Careful%20and%20Competent%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Kethan%20Reddy%20and%20Elias%20Nassif%20and%20Panagiotis%20Angeloudis%20and%20Mohammed%20Quddus%20and%20Washington%20Ochieng%0AAbstract%3A%20%20%20Virtual%20scenario-based%20testing%20methods%20to%20validate%20autonomous%20driving%20systems%0Aare%20predominantly%20centred%20around%20collision%20avoidance%2C%20and%20lack%20a%20comprehensive%0Aapproach%20to%20evaluate%20optimal%20driving%20behaviour%20holistically.%20Furthermore%2C%0Acurrent%20validation%20approaches%20do%20not%20align%20with%20authorisation%20and%20monitoring%0Arequirements%20put%20forth%20by%20regulatory%20bodies.%20We%20address%20these%20validation%20gaps%0Aby%20outlining%20a%20universal%20evaluation%20framework%20that%3A%20incorporates%20the%20notion%20of%0Acareful%20and%20competent%20driving%2C%20unifies%20behavioural%20competencies%20and%20evaluation%0Acriteria%2C%20and%20is%20amenable%20at%20a%20scenario-specific%20and%20aggregate%20behaviour%20level.%0AThis%20framework%20can%20be%20leveraged%20to%20evaluate%20optimal%20driving%20in%20scenario-based%0Atesting%2C%20and%20for%20post-deployment%20monitoring%20to%20ensure%20continual%20compliance%20with%0Aregulation%20and%20safety%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Universal%2520Evaluation%2520Model%2520for%2520Careful%2520and%2520Competent%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DKethan%2520Reddy%2520and%2520Elias%2520Nassif%2520and%2520Panagiotis%2520Angeloudis%2520and%2520Mohammed%2520Quddus%2520and%2520Washington%2520Ochieng%26entry.1292438233%3D%2520%2520Virtual%2520scenario-based%2520testing%2520methods%2520to%2520validate%2520autonomous%2520driving%2520systems%250Aare%2520predominantly%2520centred%2520around%2520collision%2520avoidance%252C%2520and%2520lack%2520a%2520comprehensive%250Aapproach%2520to%2520evaluate%2520optimal%2520driving%2520behaviour%2520holistically.%2520Furthermore%252C%250Acurrent%2520validation%2520approaches%2520do%2520not%2520align%2520with%2520authorisation%2520and%2520monitoring%250Arequirements%2520put%2520forth%2520by%2520regulatory%2520bodies.%2520We%2520address%2520these%2520validation%2520gaps%250Aby%2520outlining%2520a%2520universal%2520evaluation%2520framework%2520that%253A%2520incorporates%2520the%2520notion%2520of%250Acareful%2520and%2520competent%2520driving%252C%2520unifies%2520behavioural%2520competencies%2520and%2520evaluation%250Acriteria%252C%2520and%2520is%2520amenable%2520at%2520a%2520scenario-specific%2520and%2520aggregate%2520behaviour%2520level.%250AThis%2520framework%2520can%2520be%2520leveraged%2520to%2520evaluate%2520optimal%2520driving%2520in%2520scenario-based%250Atesting%252C%2520and%2520for%2520post-deployment%2520monitoring%2520to%2520ensure%2520continual%2520compliance%2520with%250Aregulation%2520and%2520safety%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Universal%20Evaluation%20Model%20for%20Careful%20and%20Competent%0A%20%20Autonomous%20Driving&entry.906535625=Kethan%20Reddy%20and%20Elias%20Nassif%20and%20Panagiotis%20Angeloudis%20and%20Mohammed%20Quddus%20and%20Washington%20Ochieng&entry.1292438233=%20%20Virtual%20scenario-based%20testing%20methods%20to%20validate%20autonomous%20driving%20systems%0Aare%20predominantly%20centred%20around%20collision%20avoidance%2C%20and%20lack%20a%20comprehensive%0Aapproach%20to%20evaluate%20optimal%20driving%20behaviour%20holistically.%20Furthermore%2C%0Acurrent%20validation%20approaches%20do%20not%20align%20with%20authorisation%20and%20monitoring%0Arequirements%20put%20forth%20by%20regulatory%20bodies.%20We%20address%20these%20validation%20gaps%0Aby%20outlining%20a%20universal%20evaluation%20framework%20that%3A%20incorporates%20the%20notion%20of%0Acareful%20and%20competent%20driving%2C%20unifies%20behavioural%20competencies%20and%20evaluation%0Acriteria%2C%20and%20is%20amenable%20at%20a%20scenario-specific%20and%20aggregate%20behaviour%20level.%0AThis%20framework%20can%20be%20leveraged%20to%20evaluate%20optimal%20driving%20in%20scenario-based%0Atesting%2C%20and%20for%20post-deployment%20monitoring%20to%20ensure%20continual%20compliance%20with%0Aregulation%20and%20safety%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15596v1&entry.124074799=Read"},
{"title": "Double Deep Learning-based Event Data Coding and Classification", "author": "Abdelrahman Seleem and Andr\u00e9 F. R. Guarda and Nuno M. M. Rodrigues and Fernando Pereira", "abstract": "  Event cameras have the ability to capture asynchronous per-pixel brightness\nchanges, called \"events\", offering advantages over traditional frame-based\ncameras for computer vision applications. Efficiently coding event data is\ncritical for transmission and storage, given the significant volume of events.\nThis paper proposes a novel double deep learning-based architecture for both\nevent data coding and classification, using a point cloud-based representation\nfor events. In this context, the conversions from events to point clouds and\nback to events are key steps in the proposed solution, and therefore its impact\nis evaluated in terms of compression and classification performance.\nExperimental results show that it is possible to achieve a classification\nperformance of compressed events which is similar to one of the original\nevents, even after applying a lossy point cloud codec, notably the recent\nlearning-based JPEG Pleno Point Cloud Coding standard, with a clear rate\nreduction. Experimental results also demonstrate that events coded using JPEG\nPCC achieve better classification performance than those coded using the\nconventional lossy MPEG Geometry-based Point Cloud Coding standard.\nFurthermore, the adoption of learning-based coding offers high potential for\nperforming computer vision tasks in the compressed domain, which allows\nskipping the decoding stage while mitigating the impact of coding artifacts.\n", "link": "http://arxiv.org/abs/2407.15531v1", "date": "2024-07-22", "relevancy": 2.1013, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5379}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Double%20Deep%20Learning-based%20Event%20Data%20Coding%20and%20Classification&body=Title%3A%20Double%20Deep%20Learning-based%20Event%20Data%20Coding%20and%20Classification%0AAuthor%3A%20Abdelrahman%20Seleem%20and%20Andr%C3%A9%20F.%20R.%20Guarda%20and%20Nuno%20M.%20M.%20Rodrigues%20and%20Fernando%20Pereira%0AAbstract%3A%20%20%20Event%20cameras%20have%20the%20ability%20to%20capture%20asynchronous%20per-pixel%20brightness%0Achanges%2C%20called%20%22events%22%2C%20offering%20advantages%20over%20traditional%20frame-based%0Acameras%20for%20computer%20vision%20applications.%20Efficiently%20coding%20event%20data%20is%0Acritical%20for%20transmission%20and%20storage%2C%20given%20the%20significant%20volume%20of%20events.%0AThis%20paper%20proposes%20a%20novel%20double%20deep%20learning-based%20architecture%20for%20both%0Aevent%20data%20coding%20and%20classification%2C%20using%20a%20point%20cloud-based%20representation%0Afor%20events.%20In%20this%20context%2C%20the%20conversions%20from%20events%20to%20point%20clouds%20and%0Aback%20to%20events%20are%20key%20steps%20in%20the%20proposed%20solution%2C%20and%20therefore%20its%20impact%0Ais%20evaluated%20in%20terms%20of%20compression%20and%20classification%20performance.%0AExperimental%20results%20show%20that%20it%20is%20possible%20to%20achieve%20a%20classification%0Aperformance%20of%20compressed%20events%20which%20is%20similar%20to%20one%20of%20the%20original%0Aevents%2C%20even%20after%20applying%20a%20lossy%20point%20cloud%20codec%2C%20notably%20the%20recent%0Alearning-based%20JPEG%20Pleno%20Point%20Cloud%20Coding%20standard%2C%20with%20a%20clear%20rate%0Areduction.%20Experimental%20results%20also%20demonstrate%20that%20events%20coded%20using%20JPEG%0APCC%20achieve%20better%20classification%20performance%20than%20those%20coded%20using%20the%0Aconventional%20lossy%20MPEG%20Geometry-based%20Point%20Cloud%20Coding%20standard.%0AFurthermore%2C%20the%20adoption%20of%20learning-based%20coding%20offers%20high%20potential%20for%0Aperforming%20computer%20vision%20tasks%20in%20the%20compressed%20domain%2C%20which%20allows%0Askipping%20the%20decoding%20stage%20while%20mitigating%20the%20impact%20of%20coding%20artifacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDouble%2520Deep%2520Learning-based%2520Event%2520Data%2520Coding%2520and%2520Classification%26entry.906535625%3DAbdelrahman%2520Seleem%2520and%2520Andr%25C3%25A9%2520F.%2520R.%2520Guarda%2520and%2520Nuno%2520M.%2520M.%2520Rodrigues%2520and%2520Fernando%2520Pereira%26entry.1292438233%3D%2520%2520Event%2520cameras%2520have%2520the%2520ability%2520to%2520capture%2520asynchronous%2520per-pixel%2520brightness%250Achanges%252C%2520called%2520%2522events%2522%252C%2520offering%2520advantages%2520over%2520traditional%2520frame-based%250Acameras%2520for%2520computer%2520vision%2520applications.%2520Efficiently%2520coding%2520event%2520data%2520is%250Acritical%2520for%2520transmission%2520and%2520storage%252C%2520given%2520the%2520significant%2520volume%2520of%2520events.%250AThis%2520paper%2520proposes%2520a%2520novel%2520double%2520deep%2520learning-based%2520architecture%2520for%2520both%250Aevent%2520data%2520coding%2520and%2520classification%252C%2520using%2520a%2520point%2520cloud-based%2520representation%250Afor%2520events.%2520In%2520this%2520context%252C%2520the%2520conversions%2520from%2520events%2520to%2520point%2520clouds%2520and%250Aback%2520to%2520events%2520are%2520key%2520steps%2520in%2520the%2520proposed%2520solution%252C%2520and%2520therefore%2520its%2520impact%250Ais%2520evaluated%2520in%2520terms%2520of%2520compression%2520and%2520classification%2520performance.%250AExperimental%2520results%2520show%2520that%2520it%2520is%2520possible%2520to%2520achieve%2520a%2520classification%250Aperformance%2520of%2520compressed%2520events%2520which%2520is%2520similar%2520to%2520one%2520of%2520the%2520original%250Aevents%252C%2520even%2520after%2520applying%2520a%2520lossy%2520point%2520cloud%2520codec%252C%2520notably%2520the%2520recent%250Alearning-based%2520JPEG%2520Pleno%2520Point%2520Cloud%2520Coding%2520standard%252C%2520with%2520a%2520clear%2520rate%250Areduction.%2520Experimental%2520results%2520also%2520demonstrate%2520that%2520events%2520coded%2520using%2520JPEG%250APCC%2520achieve%2520better%2520classification%2520performance%2520than%2520those%2520coded%2520using%2520the%250Aconventional%2520lossy%2520MPEG%2520Geometry-based%2520Point%2520Cloud%2520Coding%2520standard.%250AFurthermore%252C%2520the%2520adoption%2520of%2520learning-based%2520coding%2520offers%2520high%2520potential%2520for%250Aperforming%2520computer%2520vision%2520tasks%2520in%2520the%2520compressed%2520domain%252C%2520which%2520allows%250Askipping%2520the%2520decoding%2520stage%2520while%2520mitigating%2520the%2520impact%2520of%2520coding%2520artifacts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Double%20Deep%20Learning-based%20Event%20Data%20Coding%20and%20Classification&entry.906535625=Abdelrahman%20Seleem%20and%20Andr%C3%A9%20F.%20R.%20Guarda%20and%20Nuno%20M.%20M.%20Rodrigues%20and%20Fernando%20Pereira&entry.1292438233=%20%20Event%20cameras%20have%20the%20ability%20to%20capture%20asynchronous%20per-pixel%20brightness%0Achanges%2C%20called%20%22events%22%2C%20offering%20advantages%20over%20traditional%20frame-based%0Acameras%20for%20computer%20vision%20applications.%20Efficiently%20coding%20event%20data%20is%0Acritical%20for%20transmission%20and%20storage%2C%20given%20the%20significant%20volume%20of%20events.%0AThis%20paper%20proposes%20a%20novel%20double%20deep%20learning-based%20architecture%20for%20both%0Aevent%20data%20coding%20and%20classification%2C%20using%20a%20point%20cloud-based%20representation%0Afor%20events.%20In%20this%20context%2C%20the%20conversions%20from%20events%20to%20point%20clouds%20and%0Aback%20to%20events%20are%20key%20steps%20in%20the%20proposed%20solution%2C%20and%20therefore%20its%20impact%0Ais%20evaluated%20in%20terms%20of%20compression%20and%20classification%20performance.%0AExperimental%20results%20show%20that%20it%20is%20possible%20to%20achieve%20a%20classification%0Aperformance%20of%20compressed%20events%20which%20is%20similar%20to%20one%20of%20the%20original%0Aevents%2C%20even%20after%20applying%20a%20lossy%20point%20cloud%20codec%2C%20notably%20the%20recent%0Alearning-based%20JPEG%20Pleno%20Point%20Cloud%20Coding%20standard%2C%20with%20a%20clear%20rate%0Areduction.%20Experimental%20results%20also%20demonstrate%20that%20events%20coded%20using%20JPEG%0APCC%20achieve%20better%20classification%20performance%20than%20those%20coded%20using%20the%0Aconventional%20lossy%20MPEG%20Geometry-based%20Point%20Cloud%20Coding%20standard.%0AFurthermore%2C%20the%20adoption%20of%20learning-based%20coding%20offers%20high%20potential%20for%0Aperforming%20computer%20vision%20tasks%20in%20the%20compressed%20domain%2C%20which%20allows%0Askipping%20the%20decoding%20stage%20while%20mitigating%20the%20impact%20of%20coding%20artifacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15531v1&entry.124074799=Read"},
{"title": "Vision-Based Power Line Cables and Pylons Detection for Low Flying\n  Aircrafts", "author": "Jakub Gwizda\u0142a and Doruk Oner and Soumava Kumar Roy and Mian Akbar Shah and Ad Eberhard and Ivan Egorov and Philipp Kr\u00fcsi and Grigory Yakushev and Pascal Fua", "abstract": "  Power lines are dangerous for low-flying aircrafts, especially in\nlow-visibility conditions. Thus, a vision-based system able to analyze the\naircraft's surroundings and to provide the pilots with a \"second pair of eyes\"\ncan contribute to enhancing their safety. To this end, we have developed a deep\nlearning approach to jointly detect power line cables and pylons from images\ncaptured at distances of several hundred meters by aircraft-mounted cameras. In\ndoing so, we have combined a modern convolutional architecture with transfer\nlearning and a loss function adapted to curvilinear structure delineation. We\nuse a single network for both detection tasks and demonstrated its performance\non two benchmarking datasets. We have integrated it within an onboard system\nand run it in flight, and have demonstrated with our experiments that it\noutperforms the prior distant cable detection method on both datasets, while\nalso successfully detecting pylons, given their annotations are available for\nthe data.\n", "link": "http://arxiv.org/abs/2407.14352v2", "date": "2024-07-22", "relevancy": 2.0987, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5347}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Based%20Power%20Line%20Cables%20and%20Pylons%20Detection%20for%20Low%20Flying%0A%20%20Aircrafts&body=Title%3A%20Vision-Based%20Power%20Line%20Cables%20and%20Pylons%20Detection%20for%20Low%20Flying%0A%20%20Aircrafts%0AAuthor%3A%20Jakub%20Gwizda%C5%82a%20and%20Doruk%20Oner%20and%20Soumava%20Kumar%20Roy%20and%20Mian%20Akbar%20Shah%20and%20Ad%20Eberhard%20and%20Ivan%20Egorov%20and%20Philipp%20Kr%C3%BCsi%20and%20Grigory%20Yakushev%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Power%20lines%20are%20dangerous%20for%20low-flying%20aircrafts%2C%20especially%20in%0Alow-visibility%20conditions.%20Thus%2C%20a%20vision-based%20system%20able%20to%20analyze%20the%0Aaircraft%27s%20surroundings%20and%20to%20provide%20the%20pilots%20with%20a%20%22second%20pair%20of%20eyes%22%0Acan%20contribute%20to%20enhancing%20their%20safety.%20To%20this%20end%2C%20we%20have%20developed%20a%20deep%0Alearning%20approach%20to%20jointly%20detect%20power%20line%20cables%20and%20pylons%20from%20images%0Acaptured%20at%20distances%20of%20several%20hundred%20meters%20by%20aircraft-mounted%20cameras.%20In%0Adoing%20so%2C%20we%20have%20combined%20a%20modern%20convolutional%20architecture%20with%20transfer%0Alearning%20and%20a%20loss%20function%20adapted%20to%20curvilinear%20structure%20delineation.%20We%0Ause%20a%20single%20network%20for%20both%20detection%20tasks%20and%20demonstrated%20its%20performance%0Aon%20two%20benchmarking%20datasets.%20We%20have%20integrated%20it%20within%20an%20onboard%20system%0Aand%20run%20it%20in%20flight%2C%20and%20have%20demonstrated%20with%20our%20experiments%20that%20it%0Aoutperforms%20the%20prior%20distant%20cable%20detection%20method%20on%20both%20datasets%2C%20while%0Aalso%20successfully%20detecting%20pylons%2C%20given%20their%20annotations%20are%20available%20for%0Athe%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Based%2520Power%2520Line%2520Cables%2520and%2520Pylons%2520Detection%2520for%2520Low%2520Flying%250A%2520%2520Aircrafts%26entry.906535625%3DJakub%2520Gwizda%25C5%2582a%2520and%2520Doruk%2520Oner%2520and%2520Soumava%2520Kumar%2520Roy%2520and%2520Mian%2520Akbar%2520Shah%2520and%2520Ad%2520Eberhard%2520and%2520Ivan%2520Egorov%2520and%2520Philipp%2520Kr%25C3%25BCsi%2520and%2520Grigory%2520Yakushev%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520Power%2520lines%2520are%2520dangerous%2520for%2520low-flying%2520aircrafts%252C%2520especially%2520in%250Alow-visibility%2520conditions.%2520Thus%252C%2520a%2520vision-based%2520system%2520able%2520to%2520analyze%2520the%250Aaircraft%2527s%2520surroundings%2520and%2520to%2520provide%2520the%2520pilots%2520with%2520a%2520%2522second%2520pair%2520of%2520eyes%2522%250Acan%2520contribute%2520to%2520enhancing%2520their%2520safety.%2520To%2520this%2520end%252C%2520we%2520have%2520developed%2520a%2520deep%250Alearning%2520approach%2520to%2520jointly%2520detect%2520power%2520line%2520cables%2520and%2520pylons%2520from%2520images%250Acaptured%2520at%2520distances%2520of%2520several%2520hundred%2520meters%2520by%2520aircraft-mounted%2520cameras.%2520In%250Adoing%2520so%252C%2520we%2520have%2520combined%2520a%2520modern%2520convolutional%2520architecture%2520with%2520transfer%250Alearning%2520and%2520a%2520loss%2520function%2520adapted%2520to%2520curvilinear%2520structure%2520delineation.%2520We%250Ause%2520a%2520single%2520network%2520for%2520both%2520detection%2520tasks%2520and%2520demonstrated%2520its%2520performance%250Aon%2520two%2520benchmarking%2520datasets.%2520We%2520have%2520integrated%2520it%2520within%2520an%2520onboard%2520system%250Aand%2520run%2520it%2520in%2520flight%252C%2520and%2520have%2520demonstrated%2520with%2520our%2520experiments%2520that%2520it%250Aoutperforms%2520the%2520prior%2520distant%2520cable%2520detection%2520method%2520on%2520both%2520datasets%252C%2520while%250Aalso%2520successfully%2520detecting%2520pylons%252C%2520given%2520their%2520annotations%2520are%2520available%2520for%250Athe%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Based%20Power%20Line%20Cables%20and%20Pylons%20Detection%20for%20Low%20Flying%0A%20%20Aircrafts&entry.906535625=Jakub%20Gwizda%C5%82a%20and%20Doruk%20Oner%20and%20Soumava%20Kumar%20Roy%20and%20Mian%20Akbar%20Shah%20and%20Ad%20Eberhard%20and%20Ivan%20Egorov%20and%20Philipp%20Kr%C3%BCsi%20and%20Grigory%20Yakushev%20and%20Pascal%20Fua&entry.1292438233=%20%20Power%20lines%20are%20dangerous%20for%20low-flying%20aircrafts%2C%20especially%20in%0Alow-visibility%20conditions.%20Thus%2C%20a%20vision-based%20system%20able%20to%20analyze%20the%0Aaircraft%27s%20surroundings%20and%20to%20provide%20the%20pilots%20with%20a%20%22second%20pair%20of%20eyes%22%0Acan%20contribute%20to%20enhancing%20their%20safety.%20To%20this%20end%2C%20we%20have%20developed%20a%20deep%0Alearning%20approach%20to%20jointly%20detect%20power%20line%20cables%20and%20pylons%20from%20images%0Acaptured%20at%20distances%20of%20several%20hundred%20meters%20by%20aircraft-mounted%20cameras.%20In%0Adoing%20so%2C%20we%20have%20combined%20a%20modern%20convolutional%20architecture%20with%20transfer%0Alearning%20and%20a%20loss%20function%20adapted%20to%20curvilinear%20structure%20delineation.%20We%0Ause%20a%20single%20network%20for%20both%20detection%20tasks%20and%20demonstrated%20its%20performance%0Aon%20two%20benchmarking%20datasets.%20We%20have%20integrated%20it%20within%20an%20onboard%20system%0Aand%20run%20it%20in%20flight%2C%20and%20have%20demonstrated%20with%20our%20experiments%20that%20it%0Aoutperforms%20the%20prior%20distant%20cable%20detection%20method%20on%20both%20datasets%2C%20while%0Aalso%20successfully%20detecting%20pylons%2C%20given%20their%20annotations%20are%20available%20for%0Athe%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14352v2&entry.124074799=Read"},
{"title": "Learning Non-Vacuous Generalization Bounds from Optimization", "author": "Chengli Tan and Jiangshe Zhang and Junmin Liu", "abstract": "  One of the fundamental challenges in the deep learning community is to\ntheoretically understand how well a deep neural network generalizes to unseen\ndata. However, current approaches often yield generalization bounds that are\neither too loose to be informative of the true generalization error or only\nvalid to the compressed nets. In this study, we present a simple yet\nnon-vacuous generalization bound from the optimization perspective. We achieve\nthis goal by leveraging that the hypothesis set accessed by stochastic gradient\nalgorithms is essentially fractal-like and thus can derive a tighter bound over\nthe algorithm-dependent Rademacher complexity. The main argument rests on\nmodeling the discrete-time recursion process via a continuous-time stochastic\ndifferential equation driven by fractional Brownian motion. Numerical studies\ndemonstrate that our approach is able to yield plausible generalization\nguarantees for modern neural networks such as ResNet and Vision Transformer,\neven when they are trained on a large-scale dataset (e.g. ImageNet-1K).\n", "link": "http://arxiv.org/abs/2206.04359v2", "date": "2024-07-22", "relevancy": 2.089, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5271}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5194}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Non-Vacuous%20Generalization%20Bounds%20from%20Optimization&body=Title%3A%20Learning%20Non-Vacuous%20Generalization%20Bounds%20from%20Optimization%0AAuthor%3A%20Chengli%20Tan%20and%20Jiangshe%20Zhang%20and%20Junmin%20Liu%0AAbstract%3A%20%20%20One%20of%20the%20fundamental%20challenges%20in%20the%20deep%20learning%20community%20is%20to%0Atheoretically%20understand%20how%20well%20a%20deep%20neural%20network%20generalizes%20to%20unseen%0Adata.%20However%2C%20current%20approaches%20often%20yield%20generalization%20bounds%20that%20are%0Aeither%20too%20loose%20to%20be%20informative%20of%20the%20true%20generalization%20error%20or%20only%0Avalid%20to%20the%20compressed%20nets.%20In%20this%20study%2C%20we%20present%20a%20simple%20yet%0Anon-vacuous%20generalization%20bound%20from%20the%20optimization%20perspective.%20We%20achieve%0Athis%20goal%20by%20leveraging%20that%20the%20hypothesis%20set%20accessed%20by%20stochastic%20gradient%0Aalgorithms%20is%20essentially%20fractal-like%20and%20thus%20can%20derive%20a%20tighter%20bound%20over%0Athe%20algorithm-dependent%20Rademacher%20complexity.%20The%20main%20argument%20rests%20on%0Amodeling%20the%20discrete-time%20recursion%20process%20via%20a%20continuous-time%20stochastic%0Adifferential%20equation%20driven%20by%20fractional%20Brownian%20motion.%20Numerical%20studies%0Ademonstrate%20that%20our%20approach%20is%20able%20to%20yield%20plausible%20generalization%0Aguarantees%20for%20modern%20neural%20networks%20such%20as%20ResNet%20and%20Vision%20Transformer%2C%0Aeven%20when%20they%20are%20trained%20on%20a%20large-scale%20dataset%20%28e.g.%20ImageNet-1K%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.04359v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Non-Vacuous%2520Generalization%2520Bounds%2520from%2520Optimization%26entry.906535625%3DChengli%2520Tan%2520and%2520Jiangshe%2520Zhang%2520and%2520Junmin%2520Liu%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520fundamental%2520challenges%2520in%2520the%2520deep%2520learning%2520community%2520is%2520to%250Atheoretically%2520understand%2520how%2520well%2520a%2520deep%2520neural%2520network%2520generalizes%2520to%2520unseen%250Adata.%2520However%252C%2520current%2520approaches%2520often%2520yield%2520generalization%2520bounds%2520that%2520are%250Aeither%2520too%2520loose%2520to%2520be%2520informative%2520of%2520the%2520true%2520generalization%2520error%2520or%2520only%250Avalid%2520to%2520the%2520compressed%2520nets.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520simple%2520yet%250Anon-vacuous%2520generalization%2520bound%2520from%2520the%2520optimization%2520perspective.%2520We%2520achieve%250Athis%2520goal%2520by%2520leveraging%2520that%2520the%2520hypothesis%2520set%2520accessed%2520by%2520stochastic%2520gradient%250Aalgorithms%2520is%2520essentially%2520fractal-like%2520and%2520thus%2520can%2520derive%2520a%2520tighter%2520bound%2520over%250Athe%2520algorithm-dependent%2520Rademacher%2520complexity.%2520The%2520main%2520argument%2520rests%2520on%250Amodeling%2520the%2520discrete-time%2520recursion%2520process%2520via%2520a%2520continuous-time%2520stochastic%250Adifferential%2520equation%2520driven%2520by%2520fractional%2520Brownian%2520motion.%2520Numerical%2520studies%250Ademonstrate%2520that%2520our%2520approach%2520is%2520able%2520to%2520yield%2520plausible%2520generalization%250Aguarantees%2520for%2520modern%2520neural%2520networks%2520such%2520as%2520ResNet%2520and%2520Vision%2520Transformer%252C%250Aeven%2520when%2520they%2520are%2520trained%2520on%2520a%2520large-scale%2520dataset%2520%2528e.g.%2520ImageNet-1K%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.04359v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Non-Vacuous%20Generalization%20Bounds%20from%20Optimization&entry.906535625=Chengli%20Tan%20and%20Jiangshe%20Zhang%20and%20Junmin%20Liu&entry.1292438233=%20%20One%20of%20the%20fundamental%20challenges%20in%20the%20deep%20learning%20community%20is%20to%0Atheoretically%20understand%20how%20well%20a%20deep%20neural%20network%20generalizes%20to%20unseen%0Adata.%20However%2C%20current%20approaches%20often%20yield%20generalization%20bounds%20that%20are%0Aeither%20too%20loose%20to%20be%20informative%20of%20the%20true%20generalization%20error%20or%20only%0Avalid%20to%20the%20compressed%20nets.%20In%20this%20study%2C%20we%20present%20a%20simple%20yet%0Anon-vacuous%20generalization%20bound%20from%20the%20optimization%20perspective.%20We%20achieve%0Athis%20goal%20by%20leveraging%20that%20the%20hypothesis%20set%20accessed%20by%20stochastic%20gradient%0Aalgorithms%20is%20essentially%20fractal-like%20and%20thus%20can%20derive%20a%20tighter%20bound%20over%0Athe%20algorithm-dependent%20Rademacher%20complexity.%20The%20main%20argument%20rests%20on%0Amodeling%20the%20discrete-time%20recursion%20process%20via%20a%20continuous-time%20stochastic%0Adifferential%20equation%20driven%20by%20fractional%20Brownian%20motion.%20Numerical%20studies%0Ademonstrate%20that%20our%20approach%20is%20able%20to%20yield%20plausible%20generalization%0Aguarantees%20for%20modern%20neural%20networks%20such%20as%20ResNet%20and%20Vision%20Transformer%2C%0Aeven%20when%20they%20are%20trained%20on%20a%20large-scale%20dataset%20%28e.g.%20ImageNet-1K%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.04359v2&entry.124074799=Read"},
{"title": "Harmonizing Flows: Leveraging normalizing flows for unsupervised and\n  source-free MRI harmonization", "author": "Farzad Beizaee and Gregory A. Lodygensky and Chris L. Adamson and Deanne K. Thompso and Jeanie L. Y. Cheon and Alicia J. Spittl. Peter J. Anderso and Christian Desrosier and Jose Dolz", "abstract": "  Lack of standardization and various intrinsic parameters for magnetic\nresonance (MR) image acquisition results in heterogeneous images across\ndifferent sites and devices, which adversely affects the generalization of deep\nneural networks. To alleviate this issue, this work proposes a novel\nunsupervised harmonization framework that leverages normalizing flows to align\nMR images, thereby emulating the distribution of a source domain. The proposed\nstrategy comprises three key steps. Initially, a normalizing flow network is\ntrained to capture the distribution characteristics of the source domain. Then,\nwe train a shallow harmonizer network to reconstruct images from the source\ndomain via their augmented counterparts. Finally, during inference, the\nharmonizer network is updated to ensure that the output images conform to the\nlearned source domain distribution, as modeled by the normalizing flow network.\nOur approach, which is unsupervised, source-free, and task-agnostic is assessed\nin the context of both adults and neonatal cross-domain brain MRI segmentation,\nas well as neonatal brain age estimation, demonstrating its generalizability\nacross tasks and population demographics. The results underscore its superior\nperformance compared to existing methodologies. The code is available at\nhttps://github.com/farzad-bz/Harmonizing-Flows\n", "link": "http://arxiv.org/abs/2407.15717v1", "date": "2024-07-22", "relevancy": 2.0804, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5827}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5205}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonizing%20Flows%3A%20Leveraging%20normalizing%20flows%20for%20unsupervised%20and%0A%20%20source-free%20MRI%20harmonization&body=Title%3A%20Harmonizing%20Flows%3A%20Leveraging%20normalizing%20flows%20for%20unsupervised%20and%0A%20%20source-free%20MRI%20harmonization%0AAuthor%3A%20Farzad%20Beizaee%20and%20Gregory%20A.%20Lodygensky%20and%20Chris%20L.%20Adamson%20and%20Deanne%20K.%20Thompso%20and%20Jeanie%20L.%20Y.%20Cheon%20and%20Alicia%20J.%20Spittl.%20Peter%20J.%20Anderso%20and%20Christian%20Desrosier%20and%20Jose%20Dolz%0AAbstract%3A%20%20%20Lack%20of%20standardization%20and%20various%20intrinsic%20parameters%20for%20magnetic%0Aresonance%20%28MR%29%20image%20acquisition%20results%20in%20heterogeneous%20images%20across%0Adifferent%20sites%20and%20devices%2C%20which%20adversely%20affects%20the%20generalization%20of%20deep%0Aneural%20networks.%20To%20alleviate%20this%20issue%2C%20this%20work%20proposes%20a%20novel%0Aunsupervised%20harmonization%20framework%20that%20leverages%20normalizing%20flows%20to%20align%0AMR%20images%2C%20thereby%20emulating%20the%20distribution%20of%20a%20source%20domain.%20The%20proposed%0Astrategy%20comprises%20three%20key%20steps.%20Initially%2C%20a%20normalizing%20flow%20network%20is%0Atrained%20to%20capture%20the%20distribution%20characteristics%20of%20the%20source%20domain.%20Then%2C%0Awe%20train%20a%20shallow%20harmonizer%20network%20to%20reconstruct%20images%20from%20the%20source%0Adomain%20via%20their%20augmented%20counterparts.%20Finally%2C%20during%20inference%2C%20the%0Aharmonizer%20network%20is%20updated%20to%20ensure%20that%20the%20output%20images%20conform%20to%20the%0Alearned%20source%20domain%20distribution%2C%20as%20modeled%20by%20the%20normalizing%20flow%20network.%0AOur%20approach%2C%20which%20is%20unsupervised%2C%20source-free%2C%20and%20task-agnostic%20is%20assessed%0Ain%20the%20context%20of%20both%20adults%20and%20neonatal%20cross-domain%20brain%20MRI%20segmentation%2C%0Aas%20well%20as%20neonatal%20brain%20age%20estimation%2C%20demonstrating%20its%20generalizability%0Aacross%20tasks%20and%20population%20demographics.%20The%20results%20underscore%20its%20superior%0Aperformance%20compared%20to%20existing%20methodologies.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/farzad-bz/Harmonizing-Flows%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonizing%2520Flows%253A%2520Leveraging%2520normalizing%2520flows%2520for%2520unsupervised%2520and%250A%2520%2520source-free%2520MRI%2520harmonization%26entry.906535625%3DFarzad%2520Beizaee%2520and%2520Gregory%2520A.%2520Lodygensky%2520and%2520Chris%2520L.%2520Adamson%2520and%2520Deanne%2520K.%2520Thompso%2520and%2520Jeanie%2520L.%2520Y.%2520Cheon%2520and%2520Alicia%2520J.%2520Spittl.%2520Peter%2520J.%2520Anderso%2520and%2520Christian%2520Desrosier%2520and%2520Jose%2520Dolz%26entry.1292438233%3D%2520%2520Lack%2520of%2520standardization%2520and%2520various%2520intrinsic%2520parameters%2520for%2520magnetic%250Aresonance%2520%2528MR%2529%2520image%2520acquisition%2520results%2520in%2520heterogeneous%2520images%2520across%250Adifferent%2520sites%2520and%2520devices%252C%2520which%2520adversely%2520affects%2520the%2520generalization%2520of%2520deep%250Aneural%2520networks.%2520To%2520alleviate%2520this%2520issue%252C%2520this%2520work%2520proposes%2520a%2520novel%250Aunsupervised%2520harmonization%2520framework%2520that%2520leverages%2520normalizing%2520flows%2520to%2520align%250AMR%2520images%252C%2520thereby%2520emulating%2520the%2520distribution%2520of%2520a%2520source%2520domain.%2520The%2520proposed%250Astrategy%2520comprises%2520three%2520key%2520steps.%2520Initially%252C%2520a%2520normalizing%2520flow%2520network%2520is%250Atrained%2520to%2520capture%2520the%2520distribution%2520characteristics%2520of%2520the%2520source%2520domain.%2520Then%252C%250Awe%2520train%2520a%2520shallow%2520harmonizer%2520network%2520to%2520reconstruct%2520images%2520from%2520the%2520source%250Adomain%2520via%2520their%2520augmented%2520counterparts.%2520Finally%252C%2520during%2520inference%252C%2520the%250Aharmonizer%2520network%2520is%2520updated%2520to%2520ensure%2520that%2520the%2520output%2520images%2520conform%2520to%2520the%250Alearned%2520source%2520domain%2520distribution%252C%2520as%2520modeled%2520by%2520the%2520normalizing%2520flow%2520network.%250AOur%2520approach%252C%2520which%2520is%2520unsupervised%252C%2520source-free%252C%2520and%2520task-agnostic%2520is%2520assessed%250Ain%2520the%2520context%2520of%2520both%2520adults%2520and%2520neonatal%2520cross-domain%2520brain%2520MRI%2520segmentation%252C%250Aas%2520well%2520as%2520neonatal%2520brain%2520age%2520estimation%252C%2520demonstrating%2520its%2520generalizability%250Aacross%2520tasks%2520and%2520population%2520demographics.%2520The%2520results%2520underscore%2520its%2520superior%250Aperformance%2520compared%2520to%2520existing%2520methodologies.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/farzad-bz/Harmonizing-Flows%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonizing%20Flows%3A%20Leveraging%20normalizing%20flows%20for%20unsupervised%20and%0A%20%20source-free%20MRI%20harmonization&entry.906535625=Farzad%20Beizaee%20and%20Gregory%20A.%20Lodygensky%20and%20Chris%20L.%20Adamson%20and%20Deanne%20K.%20Thompso%20and%20Jeanie%20L.%20Y.%20Cheon%20and%20Alicia%20J.%20Spittl.%20Peter%20J.%20Anderso%20and%20Christian%20Desrosier%20and%20Jose%20Dolz&entry.1292438233=%20%20Lack%20of%20standardization%20and%20various%20intrinsic%20parameters%20for%20magnetic%0Aresonance%20%28MR%29%20image%20acquisition%20results%20in%20heterogeneous%20images%20across%0Adifferent%20sites%20and%20devices%2C%20which%20adversely%20affects%20the%20generalization%20of%20deep%0Aneural%20networks.%20To%20alleviate%20this%20issue%2C%20this%20work%20proposes%20a%20novel%0Aunsupervised%20harmonization%20framework%20that%20leverages%20normalizing%20flows%20to%20align%0AMR%20images%2C%20thereby%20emulating%20the%20distribution%20of%20a%20source%20domain.%20The%20proposed%0Astrategy%20comprises%20three%20key%20steps.%20Initially%2C%20a%20normalizing%20flow%20network%20is%0Atrained%20to%20capture%20the%20distribution%20characteristics%20of%20the%20source%20domain.%20Then%2C%0Awe%20train%20a%20shallow%20harmonizer%20network%20to%20reconstruct%20images%20from%20the%20source%0Adomain%20via%20their%20augmented%20counterparts.%20Finally%2C%20during%20inference%2C%20the%0Aharmonizer%20network%20is%20updated%20to%20ensure%20that%20the%20output%20images%20conform%20to%20the%0Alearned%20source%20domain%20distribution%2C%20as%20modeled%20by%20the%20normalizing%20flow%20network.%0AOur%20approach%2C%20which%20is%20unsupervised%2C%20source-free%2C%20and%20task-agnostic%20is%20assessed%0Ain%20the%20context%20of%20both%20adults%20and%20neonatal%20cross-domain%20brain%20MRI%20segmentation%2C%0Aas%20well%20as%20neonatal%20brain%20age%20estimation%2C%20demonstrating%20its%20generalizability%0Aacross%20tasks%20and%20population%20demographics.%20The%20results%20underscore%20its%20superior%0Aperformance%20compared%20to%20existing%20methodologies.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/farzad-bz/Harmonizing-Flows%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15717v1&entry.124074799=Read"},
{"title": "LDConv: Linear deformable convolution for improving convolutional neural\n  networks", "author": "Xin Zhang and Yingze Song and Tingting Song and Degang Yang and Yichen Ye and Jie Zhou and Liming Zhang", "abstract": "  Neural networks based on convolutional operations have achieved remarkable\nresults in the field of deep learning, but there are two inherent flaws in\nstandard convolutional operations. On the one hand, the convolution operation\nis confined to a local window, so it cannot capture information from other\nlocations, and its sampled shapes is fixed. On the other hand, the size of the\nconvolutional kernel are fixed to k $\\times$ k, which is a fixed square shape,\nand the number of parameters tends to grow squarely with size. Although\nDeformable Convolution (Deformable Conv) address the problem of fixed sampling\nof standard convolutions, the number of parameters also tends to grow in a\nsquared manner. In response to the above questions, the Linear Deformable\nConvolution (LDConv) is explored in this work, which gives the convolution\nkernel an arbitrary number of parameters and arbitrary sampled shapes to\nprovide richer options for the trade-off between network overhead and\nperformance. In LDConv, a novel coordinate generation algorithm is defined to\ngenerate different initial sampled positions for convolutional kernels of\narbitrary size. To adapt to changing targets, offsets are introduced to adjust\nthe shape of the samples at each position. LDConv corrects the growth trend of\nthe number of parameters for standard convolution and Deformable Conv to a\nlinear growth. Moreover, it completes the process of efficient feature\nextraction by irregular convolutional operations and brings more exploration\noptions for convolutional sampled shapes. Object detection experiments on\nrepresentative datasets COCO2017, VOC 7+12, and VisDrone-DET2021 fully\ndemonstrate the advantages of LDConv. LDConv is a plug-and-play convolutional\noperation that can replace the convolutional operation to improve network\nperformance. The code for the relevant tasks can be found at\nhttps://github.com/CV-ZhangXin/LDConv.\n", "link": "http://arxiv.org/abs/2311.11587v3", "date": "2024-07-22", "relevancy": 2.0618, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5274}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5134}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LDConv%3A%20Linear%20deformable%20convolution%20for%20improving%20convolutional%20neural%0A%20%20networks&body=Title%3A%20LDConv%3A%20Linear%20deformable%20convolution%20for%20improving%20convolutional%20neural%0A%20%20networks%0AAuthor%3A%20Xin%20Zhang%20and%20Yingze%20Song%20and%20Tingting%20Song%20and%20Degang%20Yang%20and%20Yichen%20Ye%20and%20Jie%20Zhou%20and%20Liming%20Zhang%0AAbstract%3A%20%20%20Neural%20networks%20based%20on%20convolutional%20operations%20have%20achieved%20remarkable%0Aresults%20in%20the%20field%20of%20deep%20learning%2C%20but%20there%20are%20two%20inherent%20flaws%20in%0Astandard%20convolutional%20operations.%20On%20the%20one%20hand%2C%20the%20convolution%20operation%0Ais%20confined%20to%20a%20local%20window%2C%20so%20it%20cannot%20capture%20information%20from%20other%0Alocations%2C%20and%20its%20sampled%20shapes%20is%20fixed.%20On%20the%20other%20hand%2C%20the%20size%20of%20the%0Aconvolutional%20kernel%20are%20fixed%20to%20k%20%24%5Ctimes%24%20k%2C%20which%20is%20a%20fixed%20square%20shape%2C%0Aand%20the%20number%20of%20parameters%20tends%20to%20grow%20squarely%20with%20size.%20Although%0ADeformable%20Convolution%20%28Deformable%20Conv%29%20address%20the%20problem%20of%20fixed%20sampling%0Aof%20standard%20convolutions%2C%20the%20number%20of%20parameters%20also%20tends%20to%20grow%20in%20a%0Asquared%20manner.%20In%20response%20to%20the%20above%20questions%2C%20the%20Linear%20Deformable%0AConvolution%20%28LDConv%29%20is%20explored%20in%20this%20work%2C%20which%20gives%20the%20convolution%0Akernel%20an%20arbitrary%20number%20of%20parameters%20and%20arbitrary%20sampled%20shapes%20to%0Aprovide%20richer%20options%20for%20the%20trade-off%20between%20network%20overhead%20and%0Aperformance.%20In%20LDConv%2C%20a%20novel%20coordinate%20generation%20algorithm%20is%20defined%20to%0Agenerate%20different%20initial%20sampled%20positions%20for%20convolutional%20kernels%20of%0Aarbitrary%20size.%20To%20adapt%20to%20changing%20targets%2C%20offsets%20are%20introduced%20to%20adjust%0Athe%20shape%20of%20the%20samples%20at%20each%20position.%20LDConv%20corrects%20the%20growth%20trend%20of%0Athe%20number%20of%20parameters%20for%20standard%20convolution%20and%20Deformable%20Conv%20to%20a%0Alinear%20growth.%20Moreover%2C%20it%20completes%20the%20process%20of%20efficient%20feature%0Aextraction%20by%20irregular%20convolutional%20operations%20and%20brings%20more%20exploration%0Aoptions%20for%20convolutional%20sampled%20shapes.%20Object%20detection%20experiments%20on%0Arepresentative%20datasets%20COCO2017%2C%20VOC%207%2B12%2C%20and%20VisDrone-DET2021%20fully%0Ademonstrate%20the%20advantages%20of%20LDConv.%20LDConv%20is%20a%20plug-and-play%20convolutional%0Aoperation%20that%20can%20replace%20the%20convolutional%20operation%20to%20improve%20network%0Aperformance.%20The%20code%20for%20the%20relevant%20tasks%20can%20be%20found%20at%0Ahttps%3A//github.com/CV-ZhangXin/LDConv.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11587v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLDConv%253A%2520Linear%2520deformable%2520convolution%2520for%2520improving%2520convolutional%2520neural%250A%2520%2520networks%26entry.906535625%3DXin%2520Zhang%2520and%2520Yingze%2520Song%2520and%2520Tingting%2520Song%2520and%2520Degang%2520Yang%2520and%2520Yichen%2520Ye%2520and%2520Jie%2520Zhou%2520and%2520Liming%2520Zhang%26entry.1292438233%3D%2520%2520Neural%2520networks%2520based%2520on%2520convolutional%2520operations%2520have%2520achieved%2520remarkable%250Aresults%2520in%2520the%2520field%2520of%2520deep%2520learning%252C%2520but%2520there%2520are%2520two%2520inherent%2520flaws%2520in%250Astandard%2520convolutional%2520operations.%2520On%2520the%2520one%2520hand%252C%2520the%2520convolution%2520operation%250Ais%2520confined%2520to%2520a%2520local%2520window%252C%2520so%2520it%2520cannot%2520capture%2520information%2520from%2520other%250Alocations%252C%2520and%2520its%2520sampled%2520shapes%2520is%2520fixed.%2520On%2520the%2520other%2520hand%252C%2520the%2520size%2520of%2520the%250Aconvolutional%2520kernel%2520are%2520fixed%2520to%2520k%2520%2524%255Ctimes%2524%2520k%252C%2520which%2520is%2520a%2520fixed%2520square%2520shape%252C%250Aand%2520the%2520number%2520of%2520parameters%2520tends%2520to%2520grow%2520squarely%2520with%2520size.%2520Although%250ADeformable%2520Convolution%2520%2528Deformable%2520Conv%2529%2520address%2520the%2520problem%2520of%2520fixed%2520sampling%250Aof%2520standard%2520convolutions%252C%2520the%2520number%2520of%2520parameters%2520also%2520tends%2520to%2520grow%2520in%2520a%250Asquared%2520manner.%2520In%2520response%2520to%2520the%2520above%2520questions%252C%2520the%2520Linear%2520Deformable%250AConvolution%2520%2528LDConv%2529%2520is%2520explored%2520in%2520this%2520work%252C%2520which%2520gives%2520the%2520convolution%250Akernel%2520an%2520arbitrary%2520number%2520of%2520parameters%2520and%2520arbitrary%2520sampled%2520shapes%2520to%250Aprovide%2520richer%2520options%2520for%2520the%2520trade-off%2520between%2520network%2520overhead%2520and%250Aperformance.%2520In%2520LDConv%252C%2520a%2520novel%2520coordinate%2520generation%2520algorithm%2520is%2520defined%2520to%250Agenerate%2520different%2520initial%2520sampled%2520positions%2520for%2520convolutional%2520kernels%2520of%250Aarbitrary%2520size.%2520To%2520adapt%2520to%2520changing%2520targets%252C%2520offsets%2520are%2520introduced%2520to%2520adjust%250Athe%2520shape%2520of%2520the%2520samples%2520at%2520each%2520position.%2520LDConv%2520corrects%2520the%2520growth%2520trend%2520of%250Athe%2520number%2520of%2520parameters%2520for%2520standard%2520convolution%2520and%2520Deformable%2520Conv%2520to%2520a%250Alinear%2520growth.%2520Moreover%252C%2520it%2520completes%2520the%2520process%2520of%2520efficient%2520feature%250Aextraction%2520by%2520irregular%2520convolutional%2520operations%2520and%2520brings%2520more%2520exploration%250Aoptions%2520for%2520convolutional%2520sampled%2520shapes.%2520Object%2520detection%2520experiments%2520on%250Arepresentative%2520datasets%2520COCO2017%252C%2520VOC%25207%252B12%252C%2520and%2520VisDrone-DET2021%2520fully%250Ademonstrate%2520the%2520advantages%2520of%2520LDConv.%2520LDConv%2520is%2520a%2520plug-and-play%2520convolutional%250Aoperation%2520that%2520can%2520replace%2520the%2520convolutional%2520operation%2520to%2520improve%2520network%250Aperformance.%2520The%2520code%2520for%2520the%2520relevant%2520tasks%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/CV-ZhangXin/LDConv.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11587v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LDConv%3A%20Linear%20deformable%20convolution%20for%20improving%20convolutional%20neural%0A%20%20networks&entry.906535625=Xin%20Zhang%20and%20Yingze%20Song%20and%20Tingting%20Song%20and%20Degang%20Yang%20and%20Yichen%20Ye%20and%20Jie%20Zhou%20and%20Liming%20Zhang&entry.1292438233=%20%20Neural%20networks%20based%20on%20convolutional%20operations%20have%20achieved%20remarkable%0Aresults%20in%20the%20field%20of%20deep%20learning%2C%20but%20there%20are%20two%20inherent%20flaws%20in%0Astandard%20convolutional%20operations.%20On%20the%20one%20hand%2C%20the%20convolution%20operation%0Ais%20confined%20to%20a%20local%20window%2C%20so%20it%20cannot%20capture%20information%20from%20other%0Alocations%2C%20and%20its%20sampled%20shapes%20is%20fixed.%20On%20the%20other%20hand%2C%20the%20size%20of%20the%0Aconvolutional%20kernel%20are%20fixed%20to%20k%20%24%5Ctimes%24%20k%2C%20which%20is%20a%20fixed%20square%20shape%2C%0Aand%20the%20number%20of%20parameters%20tends%20to%20grow%20squarely%20with%20size.%20Although%0ADeformable%20Convolution%20%28Deformable%20Conv%29%20address%20the%20problem%20of%20fixed%20sampling%0Aof%20standard%20convolutions%2C%20the%20number%20of%20parameters%20also%20tends%20to%20grow%20in%20a%0Asquared%20manner.%20In%20response%20to%20the%20above%20questions%2C%20the%20Linear%20Deformable%0AConvolution%20%28LDConv%29%20is%20explored%20in%20this%20work%2C%20which%20gives%20the%20convolution%0Akernel%20an%20arbitrary%20number%20of%20parameters%20and%20arbitrary%20sampled%20shapes%20to%0Aprovide%20richer%20options%20for%20the%20trade-off%20between%20network%20overhead%20and%0Aperformance.%20In%20LDConv%2C%20a%20novel%20coordinate%20generation%20algorithm%20is%20defined%20to%0Agenerate%20different%20initial%20sampled%20positions%20for%20convolutional%20kernels%20of%0Aarbitrary%20size.%20To%20adapt%20to%20changing%20targets%2C%20offsets%20are%20introduced%20to%20adjust%0Athe%20shape%20of%20the%20samples%20at%20each%20position.%20LDConv%20corrects%20the%20growth%20trend%20of%0Athe%20number%20of%20parameters%20for%20standard%20convolution%20and%20Deformable%20Conv%20to%20a%0Alinear%20growth.%20Moreover%2C%20it%20completes%20the%20process%20of%20efficient%20feature%0Aextraction%20by%20irregular%20convolutional%20operations%20and%20brings%20more%20exploration%0Aoptions%20for%20convolutional%20sampled%20shapes.%20Object%20detection%20experiments%20on%0Arepresentative%20datasets%20COCO2017%2C%20VOC%207%2B12%2C%20and%20VisDrone-DET2021%20fully%0Ademonstrate%20the%20advantages%20of%20LDConv.%20LDConv%20is%20a%20plug-and-play%20convolutional%0Aoperation%20that%20can%20replace%20the%20convolutional%20operation%20to%20improve%20network%0Aperformance.%20The%20code%20for%20the%20relevant%20tasks%20can%20be%20found%20at%0Ahttps%3A//github.com/CV-ZhangXin/LDConv.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11587v3&entry.124074799=Read"},
{"title": "Not All Pairs are Equal: Hierarchical Learning for\n  Average-Precision-Oriented Video Retrieval", "author": "Yang Liu and Qianqian Xu and Peisong Wen and Siran Dai and Qingming Huang", "abstract": "  The rapid growth of online video resources has significantly promoted the\ndevelopment of video retrieval methods. As a standard evaluation metric for\nvideo retrieval, Average Precision (AP) assesses the overall rankings of\nrelevant videos at the top list, making the predicted scores a reliable\nreference for users. However, recent video retrieval methods utilize pair-wise\nlosses that treat all sample pairs equally, leading to an evident gap between\nthe training objective and evaluation metric. To effectively bridge this gap,\nin this work, we aim to address two primary challenges: a) The current\nsimilarity measure and AP-based loss are suboptimal for video retrieval; b) The\nnoticeable noise from frame-to-frame matching introduces ambiguity in\nestimating the AP loss. In response to these challenges, we propose the\nHierarchical learning framework for Average-Precision-oriented Video Retrieval\n(HAP-VR). For the former challenge, we develop the TopK-Chamfer Similarity and\nQuadLinear-AP loss to measure and optimize video-level similarities in terms of\nAP. For the latter challenge, we suggest constraining the frame-level\nsimilarities to achieve an accurate AP loss estimation. Experimental results\npresent that HAP-VR outperforms existing methods on several benchmark datasets,\nproviding a feasible solution for video retrieval tasks and thus offering\npotential benefits for the multi-media application.\n", "link": "http://arxiv.org/abs/2407.15566v1", "date": "2024-07-22", "relevancy": 2.0587, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5247}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Pairs%20are%20Equal%3A%20Hierarchical%20Learning%20for%0A%20%20Average-Precision-Oriented%20Video%20Retrieval&body=Title%3A%20Not%20All%20Pairs%20are%20Equal%3A%20Hierarchical%20Learning%20for%0A%20%20Average-Precision-Oriented%20Video%20Retrieval%0AAuthor%3A%20Yang%20Liu%20and%20Qianqian%20Xu%20and%20Peisong%20Wen%20and%20Siran%20Dai%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20online%20video%20resources%20has%20significantly%20promoted%20the%0Adevelopment%20of%20video%20retrieval%20methods.%20As%20a%20standard%20evaluation%20metric%20for%0Avideo%20retrieval%2C%20Average%20Precision%20%28AP%29%20assesses%20the%20overall%20rankings%20of%0Arelevant%20videos%20at%20the%20top%20list%2C%20making%20the%20predicted%20scores%20a%20reliable%0Areference%20for%20users.%20However%2C%20recent%20video%20retrieval%20methods%20utilize%20pair-wise%0Alosses%20that%20treat%20all%20sample%20pairs%20equally%2C%20leading%20to%20an%20evident%20gap%20between%0Athe%20training%20objective%20and%20evaluation%20metric.%20To%20effectively%20bridge%20this%20gap%2C%0Ain%20this%20work%2C%20we%20aim%20to%20address%20two%20primary%20challenges%3A%20a%29%20The%20current%0Asimilarity%20measure%20and%20AP-based%20loss%20are%20suboptimal%20for%20video%20retrieval%3B%20b%29%20The%0Anoticeable%20noise%20from%20frame-to-frame%20matching%20introduces%20ambiguity%20in%0Aestimating%20the%20AP%20loss.%20In%20response%20to%20these%20challenges%2C%20we%20propose%20the%0AHierarchical%20learning%20framework%20for%20Average-Precision-oriented%20Video%20Retrieval%0A%28HAP-VR%29.%20For%20the%20former%20challenge%2C%20we%20develop%20the%20TopK-Chamfer%20Similarity%20and%0AQuadLinear-AP%20loss%20to%20measure%20and%20optimize%20video-level%20similarities%20in%20terms%20of%0AAP.%20For%20the%20latter%20challenge%2C%20we%20suggest%20constraining%20the%20frame-level%0Asimilarities%20to%20achieve%20an%20accurate%20AP%20loss%20estimation.%20Experimental%20results%0Apresent%20that%20HAP-VR%20outperforms%20existing%20methods%20on%20several%20benchmark%20datasets%2C%0Aproviding%20a%20feasible%20solution%20for%20video%20retrieval%20tasks%20and%20thus%20offering%0Apotential%20benefits%20for%20the%20multi-media%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Pairs%2520are%2520Equal%253A%2520Hierarchical%2520Learning%2520for%250A%2520%2520Average-Precision-Oriented%2520Video%2520Retrieval%26entry.906535625%3DYang%2520Liu%2520and%2520Qianqian%2520Xu%2520and%2520Peisong%2520Wen%2520and%2520Siran%2520Dai%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520online%2520video%2520resources%2520has%2520significantly%2520promoted%2520the%250Adevelopment%2520of%2520video%2520retrieval%2520methods.%2520As%2520a%2520standard%2520evaluation%2520metric%2520for%250Avideo%2520retrieval%252C%2520Average%2520Precision%2520%2528AP%2529%2520assesses%2520the%2520overall%2520rankings%2520of%250Arelevant%2520videos%2520at%2520the%2520top%2520list%252C%2520making%2520the%2520predicted%2520scores%2520a%2520reliable%250Areference%2520for%2520users.%2520However%252C%2520recent%2520video%2520retrieval%2520methods%2520utilize%2520pair-wise%250Alosses%2520that%2520treat%2520all%2520sample%2520pairs%2520equally%252C%2520leading%2520to%2520an%2520evident%2520gap%2520between%250Athe%2520training%2520objective%2520and%2520evaluation%2520metric.%2520To%2520effectively%2520bridge%2520this%2520gap%252C%250Ain%2520this%2520work%252C%2520we%2520aim%2520to%2520address%2520two%2520primary%2520challenges%253A%2520a%2529%2520The%2520current%250Asimilarity%2520measure%2520and%2520AP-based%2520loss%2520are%2520suboptimal%2520for%2520video%2520retrieval%253B%2520b%2529%2520The%250Anoticeable%2520noise%2520from%2520frame-to-frame%2520matching%2520introduces%2520ambiguity%2520in%250Aestimating%2520the%2520AP%2520loss.%2520In%2520response%2520to%2520these%2520challenges%252C%2520we%2520propose%2520the%250AHierarchical%2520learning%2520framework%2520for%2520Average-Precision-oriented%2520Video%2520Retrieval%250A%2528HAP-VR%2529.%2520For%2520the%2520former%2520challenge%252C%2520we%2520develop%2520the%2520TopK-Chamfer%2520Similarity%2520and%250AQuadLinear-AP%2520loss%2520to%2520measure%2520and%2520optimize%2520video-level%2520similarities%2520in%2520terms%2520of%250AAP.%2520For%2520the%2520latter%2520challenge%252C%2520we%2520suggest%2520constraining%2520the%2520frame-level%250Asimilarities%2520to%2520achieve%2520an%2520accurate%2520AP%2520loss%2520estimation.%2520Experimental%2520results%250Apresent%2520that%2520HAP-VR%2520outperforms%2520existing%2520methods%2520on%2520several%2520benchmark%2520datasets%252C%250Aproviding%2520a%2520feasible%2520solution%2520for%2520video%2520retrieval%2520tasks%2520and%2520thus%2520offering%250Apotential%2520benefits%2520for%2520the%2520multi-media%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Pairs%20are%20Equal%3A%20Hierarchical%20Learning%20for%0A%20%20Average-Precision-Oriented%20Video%20Retrieval&entry.906535625=Yang%20Liu%20and%20Qianqian%20Xu%20and%20Peisong%20Wen%20and%20Siran%20Dai%20and%20Qingming%20Huang&entry.1292438233=%20%20The%20rapid%20growth%20of%20online%20video%20resources%20has%20significantly%20promoted%20the%0Adevelopment%20of%20video%20retrieval%20methods.%20As%20a%20standard%20evaluation%20metric%20for%0Avideo%20retrieval%2C%20Average%20Precision%20%28AP%29%20assesses%20the%20overall%20rankings%20of%0Arelevant%20videos%20at%20the%20top%20list%2C%20making%20the%20predicted%20scores%20a%20reliable%0Areference%20for%20users.%20However%2C%20recent%20video%20retrieval%20methods%20utilize%20pair-wise%0Alosses%20that%20treat%20all%20sample%20pairs%20equally%2C%20leading%20to%20an%20evident%20gap%20between%0Athe%20training%20objective%20and%20evaluation%20metric.%20To%20effectively%20bridge%20this%20gap%2C%0Ain%20this%20work%2C%20we%20aim%20to%20address%20two%20primary%20challenges%3A%20a%29%20The%20current%0Asimilarity%20measure%20and%20AP-based%20loss%20are%20suboptimal%20for%20video%20retrieval%3B%20b%29%20The%0Anoticeable%20noise%20from%20frame-to-frame%20matching%20introduces%20ambiguity%20in%0Aestimating%20the%20AP%20loss.%20In%20response%20to%20these%20challenges%2C%20we%20propose%20the%0AHierarchical%20learning%20framework%20for%20Average-Precision-oriented%20Video%20Retrieval%0A%28HAP-VR%29.%20For%20the%20former%20challenge%2C%20we%20develop%20the%20TopK-Chamfer%20Similarity%20and%0AQuadLinear-AP%20loss%20to%20measure%20and%20optimize%20video-level%20similarities%20in%20terms%20of%0AAP.%20For%20the%20latter%20challenge%2C%20we%20suggest%20constraining%20the%20frame-level%0Asimilarities%20to%20achieve%20an%20accurate%20AP%20loss%20estimation.%20Experimental%20results%0Apresent%20that%20HAP-VR%20outperforms%20existing%20methods%20on%20several%20benchmark%20datasets%2C%0Aproviding%20a%20feasible%20solution%20for%20video%20retrieval%20tasks%20and%20thus%20offering%0Apotential%20benefits%20for%20the%20multi-media%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15566v1&entry.124074799=Read"},
{"title": "A spatiotemporal deep learning framework for prediction of crack\n  dynamics in heterogeneous solids: efficient mapping of concrete\n  microstructures to its fracture properties", "author": "Rasoul Najafi Koopas and Shahed Rezaei and Natalie Rauter and Richard Ostwald and Rolf Lammering", "abstract": "  A spatiotemporal deep learning framework is proposed that is capable of 2D\nfull-field prediction of fracture in concrete mesostructures. This framework\nnot only predicts fractures but also captures the entire history of the\nfracture process, from the crack initiation in the interfacial transition zone\nto the subsequent propagation of the cracks in the mortar matrix. In addition,\na convolutional neural network is developed which can predict the averaged\nstress-strain curve of the mesostructures. The UNet modeling framework, which\ncomprises an encoder-decoder section with skip connections, is used as the deep\nlearning surrogate model. Training and test data are generated from\nhigh-fidelity fracture simulations of randomly generated concrete\nmesostructures. These mesostructures include geometric variabilities such as\ndifferent aggregate particle geometrical features, spatial distribution, and\nthe total volume fraction of aggregates. The fracture simulations are carried\nout in Abaqus, utilizing the cohesive phase-field fracture modeling technique\nas the fracture modeling approach. In this work, to reduce the number of\ntraining datasets, the spatial distribution of three sets of material\nproperties for three-phase concrete mesostructures, along with the spatial\nphase-field damage index, are fed to the UNet to predict the corresponding\nstress and spatial damage index at the subsequent step. It is shown that after\nthe training process using this methodology, the UNet model is capable of\naccurately predicting damage on the unseen test dataset by using 470 datasets.\nMoreover, another novel aspect of this work is the conversion of irregular\nfinite element data into regular grids using a developed pipeline. This\napproach allows for the implementation of less complex UNet architecture and\nfacilitates the integration of phase-field fracture equations into surrogate\nmodels for future developments.\n", "link": "http://arxiv.org/abs/2407.15665v1", "date": "2024-07-22", "relevancy": 2.0515, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5277}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5235}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20spatiotemporal%20deep%20learning%20framework%20for%20prediction%20of%20crack%0A%20%20dynamics%20in%20heterogeneous%20solids%3A%20efficient%20mapping%20of%20concrete%0A%20%20microstructures%20to%20its%20fracture%20properties&body=Title%3A%20A%20spatiotemporal%20deep%20learning%20framework%20for%20prediction%20of%20crack%0A%20%20dynamics%20in%20heterogeneous%20solids%3A%20efficient%20mapping%20of%20concrete%0A%20%20microstructures%20to%20its%20fracture%20properties%0AAuthor%3A%20Rasoul%20Najafi%20Koopas%20and%20Shahed%20Rezaei%20and%20Natalie%20Rauter%20and%20Richard%20Ostwald%20and%20Rolf%20Lammering%0AAbstract%3A%20%20%20A%20spatiotemporal%20deep%20learning%20framework%20is%20proposed%20that%20is%20capable%20of%202D%0Afull-field%20prediction%20of%20fracture%20in%20concrete%20mesostructures.%20This%20framework%0Anot%20only%20predicts%20fractures%20but%20also%20captures%20the%20entire%20history%20of%20the%0Afracture%20process%2C%20from%20the%20crack%20initiation%20in%20the%20interfacial%20transition%20zone%0Ato%20the%20subsequent%20propagation%20of%20the%20cracks%20in%20the%20mortar%20matrix.%20In%20addition%2C%0Aa%20convolutional%20neural%20network%20is%20developed%20which%20can%20predict%20the%20averaged%0Astress-strain%20curve%20of%20the%20mesostructures.%20The%20UNet%20modeling%20framework%2C%20which%0Acomprises%20an%20encoder-decoder%20section%20with%20skip%20connections%2C%20is%20used%20as%20the%20deep%0Alearning%20surrogate%20model.%20Training%20and%20test%20data%20are%20generated%20from%0Ahigh-fidelity%20fracture%20simulations%20of%20randomly%20generated%20concrete%0Amesostructures.%20These%20mesostructures%20include%20geometric%20variabilities%20such%20as%0Adifferent%20aggregate%20particle%20geometrical%20features%2C%20spatial%20distribution%2C%20and%0Athe%20total%20volume%20fraction%20of%20aggregates.%20The%20fracture%20simulations%20are%20carried%0Aout%20in%20Abaqus%2C%20utilizing%20the%20cohesive%20phase-field%20fracture%20modeling%20technique%0Aas%20the%20fracture%20modeling%20approach.%20In%20this%20work%2C%20to%20reduce%20the%20number%20of%0Atraining%20datasets%2C%20the%20spatial%20distribution%20of%20three%20sets%20of%20material%0Aproperties%20for%20three-phase%20concrete%20mesostructures%2C%20along%20with%20the%20spatial%0Aphase-field%20damage%20index%2C%20are%20fed%20to%20the%20UNet%20to%20predict%20the%20corresponding%0Astress%20and%20spatial%20damage%20index%20at%20the%20subsequent%20step.%20It%20is%20shown%20that%20after%0Athe%20training%20process%20using%20this%20methodology%2C%20the%20UNet%20model%20is%20capable%20of%0Aaccurately%20predicting%20damage%20on%20the%20unseen%20test%20dataset%20by%20using%20470%20datasets.%0AMoreover%2C%20another%20novel%20aspect%20of%20this%20work%20is%20the%20conversion%20of%20irregular%0Afinite%20element%20data%20into%20regular%20grids%20using%20a%20developed%20pipeline.%20This%0Aapproach%20allows%20for%20the%20implementation%20of%20less%20complex%20UNet%20architecture%20and%0Afacilitates%20the%20integration%20of%20phase-field%20fracture%20equations%20into%20surrogate%0Amodels%20for%20future%20developments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520spatiotemporal%2520deep%2520learning%2520framework%2520for%2520prediction%2520of%2520crack%250A%2520%2520dynamics%2520in%2520heterogeneous%2520solids%253A%2520efficient%2520mapping%2520of%2520concrete%250A%2520%2520microstructures%2520to%2520its%2520fracture%2520properties%26entry.906535625%3DRasoul%2520Najafi%2520Koopas%2520and%2520Shahed%2520Rezaei%2520and%2520Natalie%2520Rauter%2520and%2520Richard%2520Ostwald%2520and%2520Rolf%2520Lammering%26entry.1292438233%3D%2520%2520A%2520spatiotemporal%2520deep%2520learning%2520framework%2520is%2520proposed%2520that%2520is%2520capable%2520of%25202D%250Afull-field%2520prediction%2520of%2520fracture%2520in%2520concrete%2520mesostructures.%2520This%2520framework%250Anot%2520only%2520predicts%2520fractures%2520but%2520also%2520captures%2520the%2520entire%2520history%2520of%2520the%250Afracture%2520process%252C%2520from%2520the%2520crack%2520initiation%2520in%2520the%2520interfacial%2520transition%2520zone%250Ato%2520the%2520subsequent%2520propagation%2520of%2520the%2520cracks%2520in%2520the%2520mortar%2520matrix.%2520In%2520addition%252C%250Aa%2520convolutional%2520neural%2520network%2520is%2520developed%2520which%2520can%2520predict%2520the%2520averaged%250Astress-strain%2520curve%2520of%2520the%2520mesostructures.%2520The%2520UNet%2520modeling%2520framework%252C%2520which%250Acomprises%2520an%2520encoder-decoder%2520section%2520with%2520skip%2520connections%252C%2520is%2520used%2520as%2520the%2520deep%250Alearning%2520surrogate%2520model.%2520Training%2520and%2520test%2520data%2520are%2520generated%2520from%250Ahigh-fidelity%2520fracture%2520simulations%2520of%2520randomly%2520generated%2520concrete%250Amesostructures.%2520These%2520mesostructures%2520include%2520geometric%2520variabilities%2520such%2520as%250Adifferent%2520aggregate%2520particle%2520geometrical%2520features%252C%2520spatial%2520distribution%252C%2520and%250Athe%2520total%2520volume%2520fraction%2520of%2520aggregates.%2520The%2520fracture%2520simulations%2520are%2520carried%250Aout%2520in%2520Abaqus%252C%2520utilizing%2520the%2520cohesive%2520phase-field%2520fracture%2520modeling%2520technique%250Aas%2520the%2520fracture%2520modeling%2520approach.%2520In%2520this%2520work%252C%2520to%2520reduce%2520the%2520number%2520of%250Atraining%2520datasets%252C%2520the%2520spatial%2520distribution%2520of%2520three%2520sets%2520of%2520material%250Aproperties%2520for%2520three-phase%2520concrete%2520mesostructures%252C%2520along%2520with%2520the%2520spatial%250Aphase-field%2520damage%2520index%252C%2520are%2520fed%2520to%2520the%2520UNet%2520to%2520predict%2520the%2520corresponding%250Astress%2520and%2520spatial%2520damage%2520index%2520at%2520the%2520subsequent%2520step.%2520It%2520is%2520shown%2520that%2520after%250Athe%2520training%2520process%2520using%2520this%2520methodology%252C%2520the%2520UNet%2520model%2520is%2520capable%2520of%250Aaccurately%2520predicting%2520damage%2520on%2520the%2520unseen%2520test%2520dataset%2520by%2520using%2520470%2520datasets.%250AMoreover%252C%2520another%2520novel%2520aspect%2520of%2520this%2520work%2520is%2520the%2520conversion%2520of%2520irregular%250Afinite%2520element%2520data%2520into%2520regular%2520grids%2520using%2520a%2520developed%2520pipeline.%2520This%250Aapproach%2520allows%2520for%2520the%2520implementation%2520of%2520less%2520complex%2520UNet%2520architecture%2520and%250Afacilitates%2520the%2520integration%2520of%2520phase-field%2520fracture%2520equations%2520into%2520surrogate%250Amodels%2520for%2520future%2520developments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20spatiotemporal%20deep%20learning%20framework%20for%20prediction%20of%20crack%0A%20%20dynamics%20in%20heterogeneous%20solids%3A%20efficient%20mapping%20of%20concrete%0A%20%20microstructures%20to%20its%20fracture%20properties&entry.906535625=Rasoul%20Najafi%20Koopas%20and%20Shahed%20Rezaei%20and%20Natalie%20Rauter%20and%20Richard%20Ostwald%20and%20Rolf%20Lammering&entry.1292438233=%20%20A%20spatiotemporal%20deep%20learning%20framework%20is%20proposed%20that%20is%20capable%20of%202D%0Afull-field%20prediction%20of%20fracture%20in%20concrete%20mesostructures.%20This%20framework%0Anot%20only%20predicts%20fractures%20but%20also%20captures%20the%20entire%20history%20of%20the%0Afracture%20process%2C%20from%20the%20crack%20initiation%20in%20the%20interfacial%20transition%20zone%0Ato%20the%20subsequent%20propagation%20of%20the%20cracks%20in%20the%20mortar%20matrix.%20In%20addition%2C%0Aa%20convolutional%20neural%20network%20is%20developed%20which%20can%20predict%20the%20averaged%0Astress-strain%20curve%20of%20the%20mesostructures.%20The%20UNet%20modeling%20framework%2C%20which%0Acomprises%20an%20encoder-decoder%20section%20with%20skip%20connections%2C%20is%20used%20as%20the%20deep%0Alearning%20surrogate%20model.%20Training%20and%20test%20data%20are%20generated%20from%0Ahigh-fidelity%20fracture%20simulations%20of%20randomly%20generated%20concrete%0Amesostructures.%20These%20mesostructures%20include%20geometric%20variabilities%20such%20as%0Adifferent%20aggregate%20particle%20geometrical%20features%2C%20spatial%20distribution%2C%20and%0Athe%20total%20volume%20fraction%20of%20aggregates.%20The%20fracture%20simulations%20are%20carried%0Aout%20in%20Abaqus%2C%20utilizing%20the%20cohesive%20phase-field%20fracture%20modeling%20technique%0Aas%20the%20fracture%20modeling%20approach.%20In%20this%20work%2C%20to%20reduce%20the%20number%20of%0Atraining%20datasets%2C%20the%20spatial%20distribution%20of%20three%20sets%20of%20material%0Aproperties%20for%20three-phase%20concrete%20mesostructures%2C%20along%20with%20the%20spatial%0Aphase-field%20damage%20index%2C%20are%20fed%20to%20the%20UNet%20to%20predict%20the%20corresponding%0Astress%20and%20spatial%20damage%20index%20at%20the%20subsequent%20step.%20It%20is%20shown%20that%20after%0Athe%20training%20process%20using%20this%20methodology%2C%20the%20UNet%20model%20is%20capable%20of%0Aaccurately%20predicting%20damage%20on%20the%20unseen%20test%20dataset%20by%20using%20470%20datasets.%0AMoreover%2C%20another%20novel%20aspect%20of%20this%20work%20is%20the%20conversion%20of%20irregular%0Afinite%20element%20data%20into%20regular%20grids%20using%20a%20developed%20pipeline.%20This%0Aapproach%20allows%20for%20the%20implementation%20of%20less%20complex%20UNet%20architecture%20and%0Afacilitates%20the%20integration%20of%20phase-field%20fracture%20equations%20into%20surrogate%0Amodels%20for%20future%20developments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15665v1&entry.124074799=Read"},
{"title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving", "author": "Jiale Xu and Rui Zhang and Cong Guo and Weiming Hu and Zihan Liu and Feiyang Wu and Yu Feng and Shixuan Sun and Changxu Shao and Yuhong Guo and Junping Zhao and Ke Zhang and Minyi Guo and Jingwen Leng", "abstract": "  Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.\n", "link": "http://arxiv.org/abs/2407.15309v1", "date": "2024-07-22", "relevancy": 2.0496, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5282}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vTensor%3A%20Flexible%20Virtual%20Tensor%20Management%20for%20Efficient%20LLM%20Serving&body=Title%3A%20vTensor%3A%20Flexible%20Virtual%20Tensor%20Management%20for%20Efficient%20LLM%20Serving%0AAuthor%3A%20Jiale%20Xu%20and%20Rui%20Zhang%20and%20Cong%20Guo%20and%20Weiming%20Hu%20and%20Zihan%20Liu%20and%20Feiyang%20Wu%20and%20Yu%20Feng%20and%20Shixuan%20Sun%20and%20Changxu%20Shao%20and%20Yuhong%20Guo%20and%20Junping%20Zhao%20and%20Ke%20Zhang%20and%20Minyi%20Guo%20and%20Jingwen%20Leng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20across%20various%20domains%2C%0Aprocessing%20millions%20of%20daily%20requests.%20This%20surge%20in%20demand%20poses%20significant%0Achallenges%20in%20optimizing%20throughput%20and%20latency%20while%20keeping%20costs%20manageable.%0AThe%20Key-Value%20%28KV%29%20cache%2C%20a%20standard%20method%20for%20retaining%20previous%0Acomputations%2C%20makes%20LLM%20inference%20highly%20bounded%20by%20memory.%20While%20batching%0Astrategies%20can%20enhance%20performance%2C%20they%20frequently%20lead%20to%20significant%20memory%0Afragmentation.%20Even%20though%20cutting-edge%20systems%20like%20vLLM%20mitigate%20KV%20cache%0Afragmentation%20using%20paged%20Attention%20mechanisms%2C%20they%20still%20suffer%20from%0Ainefficient%20memory%20and%20computational%20operations%20due%20to%20the%20tightly%20coupled%20page%0Amanagement%20and%20computation%20kernels.%0A%20%20This%20study%20introduces%20the%20vTensor%2C%20an%20innovative%20tensor%20structure%20for%20LLM%0Ainference%20based%20on%20GPU%20virtual%20memory%20management%20%28VMM%29.%20vTensor%20addresses%0Aexisting%20limitations%20by%20decoupling%20computation%20from%20memory%20defragmentation%20and%0Aoffering%20dynamic%20extensibility.%20Our%20framework%20employs%20a%20CPU-GPU%20heterogeneous%0Aapproach%2C%20ensuring%20efficient%2C%20fragmentation-free%20memory%20management%20while%0Aaccommodating%20various%20computation%20kernels%20across%20different%20LLM%20architectures.%0AExperimental%20results%20indicate%20that%20vTensor%20achieves%20an%20average%20speedup%20of%201.86x%0Aacross%20different%20models%2C%20with%20up%20to%202.42x%20in%20multi-turn%20chat%20scenarios.%0AAdditionally%2C%20vTensor%20provides%20average%20speedups%20of%202.12x%20and%203.15x%20in%20kernel%0Aevaluation%2C%20reaching%20up%20to%203.92x%20and%203.27x%20compared%20to%20SGLang%20Triton%0Aprefix-prefilling%20kernels%20and%20vLLM%20paged%20Attention%20kernel%2C%20respectively.%0AFurthermore%2C%20it%20frees%20approximately%2071.25%25%20%2857GB%29%20of%20memory%20on%20the%20NVIDIA%20A100%0AGPU%20compared%20to%20vLLM%2C%20enabling%20more%20memory-intensive%20workloads.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvTensor%253A%2520Flexible%2520Virtual%2520Tensor%2520Management%2520for%2520Efficient%2520LLM%2520Serving%26entry.906535625%3DJiale%2520Xu%2520and%2520Rui%2520Zhang%2520and%2520Cong%2520Guo%2520and%2520Weiming%2520Hu%2520and%2520Zihan%2520Liu%2520and%2520Feiyang%2520Wu%2520and%2520Yu%2520Feng%2520and%2520Shixuan%2520Sun%2520and%2520Changxu%2520Shao%2520and%2520Yuhong%2520Guo%2520and%2520Junping%2520Zhao%2520and%2520Ke%2520Zhang%2520and%2520Minyi%2520Guo%2520and%2520Jingwen%2520Leng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520widely%2520used%2520across%2520various%2520domains%252C%250Aprocessing%2520millions%2520of%2520daily%2520requests.%2520This%2520surge%2520in%2520demand%2520poses%2520significant%250Achallenges%2520in%2520optimizing%2520throughput%2520and%2520latency%2520while%2520keeping%2520costs%2520manageable.%250AThe%2520Key-Value%2520%2528KV%2529%2520cache%252C%2520a%2520standard%2520method%2520for%2520retaining%2520previous%250Acomputations%252C%2520makes%2520LLM%2520inference%2520highly%2520bounded%2520by%2520memory.%2520While%2520batching%250Astrategies%2520can%2520enhance%2520performance%252C%2520they%2520frequently%2520lead%2520to%2520significant%2520memory%250Afragmentation.%2520Even%2520though%2520cutting-edge%2520systems%2520like%2520vLLM%2520mitigate%2520KV%2520cache%250Afragmentation%2520using%2520paged%2520Attention%2520mechanisms%252C%2520they%2520still%2520suffer%2520from%250Ainefficient%2520memory%2520and%2520computational%2520operations%2520due%2520to%2520the%2520tightly%2520coupled%2520page%250Amanagement%2520and%2520computation%2520kernels.%250A%2520%2520This%2520study%2520introduces%2520the%2520vTensor%252C%2520an%2520innovative%2520tensor%2520structure%2520for%2520LLM%250Ainference%2520based%2520on%2520GPU%2520virtual%2520memory%2520management%2520%2528VMM%2529.%2520vTensor%2520addresses%250Aexisting%2520limitations%2520by%2520decoupling%2520computation%2520from%2520memory%2520defragmentation%2520and%250Aoffering%2520dynamic%2520extensibility.%2520Our%2520framework%2520employs%2520a%2520CPU-GPU%2520heterogeneous%250Aapproach%252C%2520ensuring%2520efficient%252C%2520fragmentation-free%2520memory%2520management%2520while%250Aaccommodating%2520various%2520computation%2520kernels%2520across%2520different%2520LLM%2520architectures.%250AExperimental%2520results%2520indicate%2520that%2520vTensor%2520achieves%2520an%2520average%2520speedup%2520of%25201.86x%250Aacross%2520different%2520models%252C%2520with%2520up%2520to%25202.42x%2520in%2520multi-turn%2520chat%2520scenarios.%250AAdditionally%252C%2520vTensor%2520provides%2520average%2520speedups%2520of%25202.12x%2520and%25203.15x%2520in%2520kernel%250Aevaluation%252C%2520reaching%2520up%2520to%25203.92x%2520and%25203.27x%2520compared%2520to%2520SGLang%2520Triton%250Aprefix-prefilling%2520kernels%2520and%2520vLLM%2520paged%2520Attention%2520kernel%252C%2520respectively.%250AFurthermore%252C%2520it%2520frees%2520approximately%252071.25%2525%2520%252857GB%2529%2520of%2520memory%2520on%2520the%2520NVIDIA%2520A100%250AGPU%2520compared%2520to%2520vLLM%252C%2520enabling%2520more%2520memory-intensive%2520workloads.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vTensor%3A%20Flexible%20Virtual%20Tensor%20Management%20for%20Efficient%20LLM%20Serving&entry.906535625=Jiale%20Xu%20and%20Rui%20Zhang%20and%20Cong%20Guo%20and%20Weiming%20Hu%20and%20Zihan%20Liu%20and%20Feiyang%20Wu%20and%20Yu%20Feng%20and%20Shixuan%20Sun%20and%20Changxu%20Shao%20and%20Yuhong%20Guo%20and%20Junping%20Zhao%20and%20Ke%20Zhang%20and%20Minyi%20Guo%20and%20Jingwen%20Leng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20across%20various%20domains%2C%0Aprocessing%20millions%20of%20daily%20requests.%20This%20surge%20in%20demand%20poses%20significant%0Achallenges%20in%20optimizing%20throughput%20and%20latency%20while%20keeping%20costs%20manageable.%0AThe%20Key-Value%20%28KV%29%20cache%2C%20a%20standard%20method%20for%20retaining%20previous%0Acomputations%2C%20makes%20LLM%20inference%20highly%20bounded%20by%20memory.%20While%20batching%0Astrategies%20can%20enhance%20performance%2C%20they%20frequently%20lead%20to%20significant%20memory%0Afragmentation.%20Even%20though%20cutting-edge%20systems%20like%20vLLM%20mitigate%20KV%20cache%0Afragmentation%20using%20paged%20Attention%20mechanisms%2C%20they%20still%20suffer%20from%0Ainefficient%20memory%20and%20computational%20operations%20due%20to%20the%20tightly%20coupled%20page%0Amanagement%20and%20computation%20kernels.%0A%20%20This%20study%20introduces%20the%20vTensor%2C%20an%20innovative%20tensor%20structure%20for%20LLM%0Ainference%20based%20on%20GPU%20virtual%20memory%20management%20%28VMM%29.%20vTensor%20addresses%0Aexisting%20limitations%20by%20decoupling%20computation%20from%20memory%20defragmentation%20and%0Aoffering%20dynamic%20extensibility.%20Our%20framework%20employs%20a%20CPU-GPU%20heterogeneous%0Aapproach%2C%20ensuring%20efficient%2C%20fragmentation-free%20memory%20management%20while%0Aaccommodating%20various%20computation%20kernels%20across%20different%20LLM%20architectures.%0AExperimental%20results%20indicate%20that%20vTensor%20achieves%20an%20average%20speedup%20of%201.86x%0Aacross%20different%20models%2C%20with%20up%20to%202.42x%20in%20multi-turn%20chat%20scenarios.%0AAdditionally%2C%20vTensor%20provides%20average%20speedups%20of%202.12x%20and%203.15x%20in%20kernel%0Aevaluation%2C%20reaching%20up%20to%203.92x%20and%203.27x%20compared%20to%20SGLang%20Triton%0Aprefix-prefilling%20kernels%20and%20vLLM%20paged%20Attention%20kernel%2C%20respectively.%0AFurthermore%2C%20it%20frees%20approximately%2071.25%25%20%2857GB%29%20of%20memory%20on%20the%20NVIDIA%20A100%0AGPU%20compared%20to%20vLLM%2C%20enabling%20more%20memory-intensive%20workloads.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15309v1&entry.124074799=Read"},
{"title": "Parallel Split Learning with Global Sampling", "author": "Mohammad Kohankhaki and Ahmad Ayad and Mahdi Barhoush and Anke Schmeink", "abstract": "  The expansion of IoT devices and the demands of Deep Learning have\nhighlighted significant challenges in Distributed Deep Learning (DDL) systems.\nParallel Split Learning (PSL) has emerged as a promising derivative of Split\nLearning that is well suited for distributed learning on resource-constrained\ndevices. However, PSL faces several obstacles, such as large effective batch\nsizes, non-IID data distributions, and the straggler effect. We view these\nissues as a sampling dilemma and propose to address them by orchestrating the\nmini-batch sampling process on the server side. We introduce the Uniform Global\nSampling (UGS) method to decouple the effective batch size from the number of\nclients and reduce mini-batch deviation in non-IID settings. To address the\nstraggler effect, we introduce the Latent Dirichlet Sampling (LDS) method,\nwhich generalizes UGS to balance the trade-off between batch deviation and\ntraining time. Our simulations reveal that our proposed methods enhance model\naccuracy by up to 34.1% in non-IID settings and reduce the training time in the\npresence of stragglers by up to 62%. In particular, LDS effectively mitigates\nthe straggler effect without compromising model accuracy or adding significant\ncomputational overhead compared to UGS. Our results demonstrate the potential\nof our methods as a promising solution for DDL in real applications.\n", "link": "http://arxiv.org/abs/2407.15738v1", "date": "2024-07-22", "relevancy": 2.0451, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5441}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.507}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20Split%20Learning%20with%20Global%20Sampling&body=Title%3A%20Parallel%20Split%20Learning%20with%20Global%20Sampling%0AAuthor%3A%20Mohammad%20Kohankhaki%20and%20Ahmad%20Ayad%20and%20Mahdi%20Barhoush%20and%20Anke%20Schmeink%0AAbstract%3A%20%20%20The%20expansion%20of%20IoT%20devices%20and%20the%20demands%20of%20Deep%20Learning%20have%0Ahighlighted%20significant%20challenges%20in%20Distributed%20Deep%20Learning%20%28DDL%29%20systems.%0AParallel%20Split%20Learning%20%28PSL%29%20has%20emerged%20as%20a%20promising%20derivative%20of%20Split%0ALearning%20that%20is%20well%20suited%20for%20distributed%20learning%20on%20resource-constrained%0Adevices.%20However%2C%20PSL%20faces%20several%20obstacles%2C%20such%20as%20large%20effective%20batch%0Asizes%2C%20non-IID%20data%20distributions%2C%20and%20the%20straggler%20effect.%20We%20view%20these%0Aissues%20as%20a%20sampling%20dilemma%20and%20propose%20to%20address%20them%20by%20orchestrating%20the%0Amini-batch%20sampling%20process%20on%20the%20server%20side.%20We%20introduce%20the%20Uniform%20Global%0ASampling%20%28UGS%29%20method%20to%20decouple%20the%20effective%20batch%20size%20from%20the%20number%20of%0Aclients%20and%20reduce%20mini-batch%20deviation%20in%20non-IID%20settings.%20To%20address%20the%0Astraggler%20effect%2C%20we%20introduce%20the%20Latent%20Dirichlet%20Sampling%20%28LDS%29%20method%2C%0Awhich%20generalizes%20UGS%20to%20balance%20the%20trade-off%20between%20batch%20deviation%20and%0Atraining%20time.%20Our%20simulations%20reveal%20that%20our%20proposed%20methods%20enhance%20model%0Aaccuracy%20by%20up%20to%2034.1%25%20in%20non-IID%20settings%20and%20reduce%20the%20training%20time%20in%20the%0Apresence%20of%20stragglers%20by%20up%20to%2062%25.%20In%20particular%2C%20LDS%20effectively%20mitigates%0Athe%20straggler%20effect%20without%20compromising%20model%20accuracy%20or%20adding%20significant%0Acomputational%20overhead%20compared%20to%20UGS.%20Our%20results%20demonstrate%20the%20potential%0Aof%20our%20methods%20as%20a%20promising%20solution%20for%20DDL%20in%20real%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520Split%2520Learning%2520with%2520Global%2520Sampling%26entry.906535625%3DMohammad%2520Kohankhaki%2520and%2520Ahmad%2520Ayad%2520and%2520Mahdi%2520Barhoush%2520and%2520Anke%2520Schmeink%26entry.1292438233%3D%2520%2520The%2520expansion%2520of%2520IoT%2520devices%2520and%2520the%2520demands%2520of%2520Deep%2520Learning%2520have%250Ahighlighted%2520significant%2520challenges%2520in%2520Distributed%2520Deep%2520Learning%2520%2528DDL%2529%2520systems.%250AParallel%2520Split%2520Learning%2520%2528PSL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520derivative%2520of%2520Split%250ALearning%2520that%2520is%2520well%2520suited%2520for%2520distributed%2520learning%2520on%2520resource-constrained%250Adevices.%2520However%252C%2520PSL%2520faces%2520several%2520obstacles%252C%2520such%2520as%2520large%2520effective%2520batch%250Asizes%252C%2520non-IID%2520data%2520distributions%252C%2520and%2520the%2520straggler%2520effect.%2520We%2520view%2520these%250Aissues%2520as%2520a%2520sampling%2520dilemma%2520and%2520propose%2520to%2520address%2520them%2520by%2520orchestrating%2520the%250Amini-batch%2520sampling%2520process%2520on%2520the%2520server%2520side.%2520We%2520introduce%2520the%2520Uniform%2520Global%250ASampling%2520%2528UGS%2529%2520method%2520to%2520decouple%2520the%2520effective%2520batch%2520size%2520from%2520the%2520number%2520of%250Aclients%2520and%2520reduce%2520mini-batch%2520deviation%2520in%2520non-IID%2520settings.%2520To%2520address%2520the%250Astraggler%2520effect%252C%2520we%2520introduce%2520the%2520Latent%2520Dirichlet%2520Sampling%2520%2528LDS%2529%2520method%252C%250Awhich%2520generalizes%2520UGS%2520to%2520balance%2520the%2520trade-off%2520between%2520batch%2520deviation%2520and%250Atraining%2520time.%2520Our%2520simulations%2520reveal%2520that%2520our%2520proposed%2520methods%2520enhance%2520model%250Aaccuracy%2520by%2520up%2520to%252034.1%2525%2520in%2520non-IID%2520settings%2520and%2520reduce%2520the%2520training%2520time%2520in%2520the%250Apresence%2520of%2520stragglers%2520by%2520up%2520to%252062%2525.%2520In%2520particular%252C%2520LDS%2520effectively%2520mitigates%250Athe%2520straggler%2520effect%2520without%2520compromising%2520model%2520accuracy%2520or%2520adding%2520significant%250Acomputational%2520overhead%2520compared%2520to%2520UGS.%2520Our%2520results%2520demonstrate%2520the%2520potential%250Aof%2520our%2520methods%2520as%2520a%2520promising%2520solution%2520for%2520DDL%2520in%2520real%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20Split%20Learning%20with%20Global%20Sampling&entry.906535625=Mohammad%20Kohankhaki%20and%20Ahmad%20Ayad%20and%20Mahdi%20Barhoush%20and%20Anke%20Schmeink&entry.1292438233=%20%20The%20expansion%20of%20IoT%20devices%20and%20the%20demands%20of%20Deep%20Learning%20have%0Ahighlighted%20significant%20challenges%20in%20Distributed%20Deep%20Learning%20%28DDL%29%20systems.%0AParallel%20Split%20Learning%20%28PSL%29%20has%20emerged%20as%20a%20promising%20derivative%20of%20Split%0ALearning%20that%20is%20well%20suited%20for%20distributed%20learning%20on%20resource-constrained%0Adevices.%20However%2C%20PSL%20faces%20several%20obstacles%2C%20such%20as%20large%20effective%20batch%0Asizes%2C%20non-IID%20data%20distributions%2C%20and%20the%20straggler%20effect.%20We%20view%20these%0Aissues%20as%20a%20sampling%20dilemma%20and%20propose%20to%20address%20them%20by%20orchestrating%20the%0Amini-batch%20sampling%20process%20on%20the%20server%20side.%20We%20introduce%20the%20Uniform%20Global%0ASampling%20%28UGS%29%20method%20to%20decouple%20the%20effective%20batch%20size%20from%20the%20number%20of%0Aclients%20and%20reduce%20mini-batch%20deviation%20in%20non-IID%20settings.%20To%20address%20the%0Astraggler%20effect%2C%20we%20introduce%20the%20Latent%20Dirichlet%20Sampling%20%28LDS%29%20method%2C%0Awhich%20generalizes%20UGS%20to%20balance%20the%20trade-off%20between%20batch%20deviation%20and%0Atraining%20time.%20Our%20simulations%20reveal%20that%20our%20proposed%20methods%20enhance%20model%0Aaccuracy%20by%20up%20to%2034.1%25%20in%20non-IID%20settings%20and%20reduce%20the%20training%20time%20in%20the%0Apresence%20of%20stragglers%20by%20up%20to%2062%25.%20In%20particular%2C%20LDS%20effectively%20mitigates%0Athe%20straggler%20effect%20without%20compromising%20model%20accuracy%20or%20adding%20significant%0Acomputational%20overhead%20compared%20to%20UGS.%20Our%20results%20demonstrate%20the%20potential%0Aof%20our%20methods%20as%20a%20promising%20solution%20for%20DDL%20in%20real%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15738v1&entry.124074799=Read"},
{"title": "Exploring the Effectiveness of Object-Centric Representations in Visual\n  Question Answering: Comparative Insights with Foundation Models", "author": "Amir Mohammad Karimi Mamaghan and Samuele Papa and Karl Henrik Johansson and Stefan Bauer and Andrea Dittadi", "abstract": "  Object-centric (OC) representations, which represent the state of a visual\nscene by modeling it as a composition of objects, have the potential to be used\nin various downstream tasks to achieve systematic compositional generalization\nand facilitate reasoning. However, these claims have not been thoroughly\nanalyzed yet. Recently, foundation models have demonstrated unparalleled\ncapabilities across diverse domains from language to computer vision, marking\nthem as a potential cornerstone of future research for a multitude of\ncomputational tasks. In this paper, we conduct an extensive empirical study on\nrepresentation learning for downstream Visual Question Answering (VQA), which\nrequires an accurate compositional understanding of the scene. We thoroughly\ninvestigate the benefits and trade-offs of OC models and alternative approaches\nincluding large pre-trained foundation models on both synthetic and real-world\ndata, and demonstrate a viable way to achieve the best of both worlds. The\nextensiveness of our study, encompassing over 800 downstream VQA models and 15\ndifferent types of upstream representations, also provides several additional\ninsights that we believe will be of interest to the community at large.\n", "link": "http://arxiv.org/abs/2407.15589v1", "date": "2024-07-22", "relevancy": 2.0446, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5142}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5124}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Effectiveness%20of%20Object-Centric%20Representations%20in%20Visual%0A%20%20Question%20Answering%3A%20Comparative%20Insights%20with%20Foundation%20Models&body=Title%3A%20Exploring%20the%20Effectiveness%20of%20Object-Centric%20Representations%20in%20Visual%0A%20%20Question%20Answering%3A%20Comparative%20Insights%20with%20Foundation%20Models%0AAuthor%3A%20Amir%20Mohammad%20Karimi%20Mamaghan%20and%20Samuele%20Papa%20and%20Karl%20Henrik%20Johansson%20and%20Stefan%20Bauer%20and%20Andrea%20Dittadi%0AAbstract%3A%20%20%20Object-centric%20%28OC%29%20representations%2C%20which%20represent%20the%20state%20of%20a%20visual%0Ascene%20by%20modeling%20it%20as%20a%20composition%20of%20objects%2C%20have%20the%20potential%20to%20be%20used%0Ain%20various%20downstream%20tasks%20to%20achieve%20systematic%20compositional%20generalization%0Aand%20facilitate%20reasoning.%20However%2C%20these%20claims%20have%20not%20been%20thoroughly%0Aanalyzed%20yet.%20Recently%2C%20foundation%20models%20have%20demonstrated%20unparalleled%0Acapabilities%20across%20diverse%20domains%20from%20language%20to%20computer%20vision%2C%20marking%0Athem%20as%20a%20potential%20cornerstone%20of%20future%20research%20for%20a%20multitude%20of%0Acomputational%20tasks.%20In%20this%20paper%2C%20we%20conduct%20an%20extensive%20empirical%20study%20on%0Arepresentation%20learning%20for%20downstream%20Visual%20Question%20Answering%20%28VQA%29%2C%20which%0Arequires%20an%20accurate%20compositional%20understanding%20of%20the%20scene.%20We%20thoroughly%0Ainvestigate%20the%20benefits%20and%20trade-offs%20of%20OC%20models%20and%20alternative%20approaches%0Aincluding%20large%20pre-trained%20foundation%20models%20on%20both%20synthetic%20and%20real-world%0Adata%2C%20and%20demonstrate%20a%20viable%20way%20to%20achieve%20the%20best%20of%20both%20worlds.%20The%0Aextensiveness%20of%20our%20study%2C%20encompassing%20over%20800%20downstream%20VQA%20models%20and%2015%0Adifferent%20types%20of%20upstream%20representations%2C%20also%20provides%20several%20additional%0Ainsights%20that%20we%20believe%20will%20be%20of%20interest%20to%20the%20community%20at%20large.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Effectiveness%2520of%2520Object-Centric%2520Representations%2520in%2520Visual%250A%2520%2520Question%2520Answering%253A%2520Comparative%2520Insights%2520with%2520Foundation%2520Models%26entry.906535625%3DAmir%2520Mohammad%2520Karimi%2520Mamaghan%2520and%2520Samuele%2520Papa%2520and%2520Karl%2520Henrik%2520Johansson%2520and%2520Stefan%2520Bauer%2520and%2520Andrea%2520Dittadi%26entry.1292438233%3D%2520%2520Object-centric%2520%2528OC%2529%2520representations%252C%2520which%2520represent%2520the%2520state%2520of%2520a%2520visual%250Ascene%2520by%2520modeling%2520it%2520as%2520a%2520composition%2520of%2520objects%252C%2520have%2520the%2520potential%2520to%2520be%2520used%250Ain%2520various%2520downstream%2520tasks%2520to%2520achieve%2520systematic%2520compositional%2520generalization%250Aand%2520facilitate%2520reasoning.%2520However%252C%2520these%2520claims%2520have%2520not%2520been%2520thoroughly%250Aanalyzed%2520yet.%2520Recently%252C%2520foundation%2520models%2520have%2520demonstrated%2520unparalleled%250Acapabilities%2520across%2520diverse%2520domains%2520from%2520language%2520to%2520computer%2520vision%252C%2520marking%250Athem%2520as%2520a%2520potential%2520cornerstone%2520of%2520future%2520research%2520for%2520a%2520multitude%2520of%250Acomputational%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520an%2520extensive%2520empirical%2520study%2520on%250Arepresentation%2520learning%2520for%2520downstream%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520which%250Arequires%2520an%2520accurate%2520compositional%2520understanding%2520of%2520the%2520scene.%2520We%2520thoroughly%250Ainvestigate%2520the%2520benefits%2520and%2520trade-offs%2520of%2520OC%2520models%2520and%2520alternative%2520approaches%250Aincluding%2520large%2520pre-trained%2520foundation%2520models%2520on%2520both%2520synthetic%2520and%2520real-world%250Adata%252C%2520and%2520demonstrate%2520a%2520viable%2520way%2520to%2520achieve%2520the%2520best%2520of%2520both%2520worlds.%2520The%250Aextensiveness%2520of%2520our%2520study%252C%2520encompassing%2520over%2520800%2520downstream%2520VQA%2520models%2520and%252015%250Adifferent%2520types%2520of%2520upstream%2520representations%252C%2520also%2520provides%2520several%2520additional%250Ainsights%2520that%2520we%2520believe%2520will%2520be%2520of%2520interest%2520to%2520the%2520community%2520at%2520large.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Effectiveness%20of%20Object-Centric%20Representations%20in%20Visual%0A%20%20Question%20Answering%3A%20Comparative%20Insights%20with%20Foundation%20Models&entry.906535625=Amir%20Mohammad%20Karimi%20Mamaghan%20and%20Samuele%20Papa%20and%20Karl%20Henrik%20Johansson%20and%20Stefan%20Bauer%20and%20Andrea%20Dittadi&entry.1292438233=%20%20Object-centric%20%28OC%29%20representations%2C%20which%20represent%20the%20state%20of%20a%20visual%0Ascene%20by%20modeling%20it%20as%20a%20composition%20of%20objects%2C%20have%20the%20potential%20to%20be%20used%0Ain%20various%20downstream%20tasks%20to%20achieve%20systematic%20compositional%20generalization%0Aand%20facilitate%20reasoning.%20However%2C%20these%20claims%20have%20not%20been%20thoroughly%0Aanalyzed%20yet.%20Recently%2C%20foundation%20models%20have%20demonstrated%20unparalleled%0Acapabilities%20across%20diverse%20domains%20from%20language%20to%20computer%20vision%2C%20marking%0Athem%20as%20a%20potential%20cornerstone%20of%20future%20research%20for%20a%20multitude%20of%0Acomputational%20tasks.%20In%20this%20paper%2C%20we%20conduct%20an%20extensive%20empirical%20study%20on%0Arepresentation%20learning%20for%20downstream%20Visual%20Question%20Answering%20%28VQA%29%2C%20which%0Arequires%20an%20accurate%20compositional%20understanding%20of%20the%20scene.%20We%20thoroughly%0Ainvestigate%20the%20benefits%20and%20trade-offs%20of%20OC%20models%20and%20alternative%20approaches%0Aincluding%20large%20pre-trained%20foundation%20models%20on%20both%20synthetic%20and%20real-world%0Adata%2C%20and%20demonstrate%20a%20viable%20way%20to%20achieve%20the%20best%20of%20both%20worlds.%20The%0Aextensiveness%20of%20our%20study%2C%20encompassing%20over%20800%20downstream%20VQA%20models%20and%2015%0Adifferent%20types%20of%20upstream%20representations%2C%20also%20provides%20several%20additional%0Ainsights%20that%20we%20believe%20will%20be%20of%20interest%20to%20the%20community%20at%20large.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15589v1&entry.124074799=Read"},
{"title": "SLVideo: A Sign Language Video Moment Retrieval Framework", "author": "Gon\u00e7alo Vinagre Martins and Afonso Quinaz and Carla Viegas and Sofia Cavaco and Jo\u00e3o Magalh\u00e3es", "abstract": "  Sign Language Recognition has been studied and developed throughout the years\nto help the deaf and hard-of-hearing people in their day-to-day lives. These\ntechnologies leverage manual sign recognition algorithms, however, most of them\nlack the recognition of facial expressions, which are also an essential part of\nSign Language as they allow the speaker to add expressiveness to their dialogue\nor even change the meaning of certain manual signs. SLVideo is a video moment\nretrieval software for Sign Language videos with a focus on both hands and\nfacial signs. The system extracts embedding representations for the hand and\nface signs from video frames to capture the language signs in full. This will\nthen allow the user to search for a specific sign language video segment with\ntext queries, or to search by similar sign language videos. To test this\nsystem, a collection of five hours of annotated Sign Language videos is used as\nthe dataset, and the initial results are promising in a zero-shot\nsetting.SLVideo is shown to not only address the problem of searching sign\nlanguage videos but also supports a Sign Language thesaurus with a search by\nsimilarity technique.\n  Project web page: https://novasearch.github.io/SLVideo/\n", "link": "http://arxiv.org/abs/2407.15668v1", "date": "2024-07-22", "relevancy": 2.0435, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.528}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.518}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLVideo%3A%20A%20Sign%20Language%20Video%20Moment%20Retrieval%20Framework&body=Title%3A%20SLVideo%3A%20A%20Sign%20Language%20Video%20Moment%20Retrieval%20Framework%0AAuthor%3A%20Gon%C3%A7alo%20Vinagre%20Martins%20and%20Afonso%20Quinaz%20and%20Carla%20Viegas%20and%20Sofia%20Cavaco%20and%20Jo%C3%A3o%20Magalh%C3%A3es%0AAbstract%3A%20%20%20Sign%20Language%20Recognition%20has%20been%20studied%20and%20developed%20throughout%20the%20years%0Ato%20help%20the%20deaf%20and%20hard-of-hearing%20people%20in%20their%20day-to-day%20lives.%20These%0Atechnologies%20leverage%20manual%20sign%20recognition%20algorithms%2C%20however%2C%20most%20of%20them%0Alack%20the%20recognition%20of%20facial%20expressions%2C%20which%20are%20also%20an%20essential%20part%20of%0ASign%20Language%20as%20they%20allow%20the%20speaker%20to%20add%20expressiveness%20to%20their%20dialogue%0Aor%20even%20change%20the%20meaning%20of%20certain%20manual%20signs.%20SLVideo%20is%20a%20video%20moment%0Aretrieval%20software%20for%20Sign%20Language%20videos%20with%20a%20focus%20on%20both%20hands%20and%0Afacial%20signs.%20The%20system%20extracts%20embedding%20representations%20for%20the%20hand%20and%0Aface%20signs%20from%20video%20frames%20to%20capture%20the%20language%20signs%20in%20full.%20This%20will%0Athen%20allow%20the%20user%20to%20search%20for%20a%20specific%20sign%20language%20video%20segment%20with%0Atext%20queries%2C%20or%20to%20search%20by%20similar%20sign%20language%20videos.%20To%20test%20this%0Asystem%2C%20a%20collection%20of%20five%20hours%20of%20annotated%20Sign%20Language%20videos%20is%20used%20as%0Athe%20dataset%2C%20and%20the%20initial%20results%20are%20promising%20in%20a%20zero-shot%0Asetting.SLVideo%20is%20shown%20to%20not%20only%20address%20the%20problem%20of%20searching%20sign%0Alanguage%20videos%20but%20also%20supports%20a%20Sign%20Language%20thesaurus%20with%20a%20search%20by%0Asimilarity%20technique.%0A%20%20Project%20web%20page%3A%20https%3A//novasearch.github.io/SLVideo/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLVideo%253A%2520A%2520Sign%2520Language%2520Video%2520Moment%2520Retrieval%2520Framework%26entry.906535625%3DGon%25C3%25A7alo%2520Vinagre%2520Martins%2520and%2520Afonso%2520Quinaz%2520and%2520Carla%2520Viegas%2520and%2520Sofia%2520Cavaco%2520and%2520Jo%25C3%25A3o%2520Magalh%25C3%25A3es%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Recognition%2520has%2520been%2520studied%2520and%2520developed%2520throughout%2520the%2520years%250Ato%2520help%2520the%2520deaf%2520and%2520hard-of-hearing%2520people%2520in%2520their%2520day-to-day%2520lives.%2520These%250Atechnologies%2520leverage%2520manual%2520sign%2520recognition%2520algorithms%252C%2520however%252C%2520most%2520of%2520them%250Alack%2520the%2520recognition%2520of%2520facial%2520expressions%252C%2520which%2520are%2520also%2520an%2520essential%2520part%2520of%250ASign%2520Language%2520as%2520they%2520allow%2520the%2520speaker%2520to%2520add%2520expressiveness%2520to%2520their%2520dialogue%250Aor%2520even%2520change%2520the%2520meaning%2520of%2520certain%2520manual%2520signs.%2520SLVideo%2520is%2520a%2520video%2520moment%250Aretrieval%2520software%2520for%2520Sign%2520Language%2520videos%2520with%2520a%2520focus%2520on%2520both%2520hands%2520and%250Afacial%2520signs.%2520The%2520system%2520extracts%2520embedding%2520representations%2520for%2520the%2520hand%2520and%250Aface%2520signs%2520from%2520video%2520frames%2520to%2520capture%2520the%2520language%2520signs%2520in%2520full.%2520This%2520will%250Athen%2520allow%2520the%2520user%2520to%2520search%2520for%2520a%2520specific%2520sign%2520language%2520video%2520segment%2520with%250Atext%2520queries%252C%2520or%2520to%2520search%2520by%2520similar%2520sign%2520language%2520videos.%2520To%2520test%2520this%250Asystem%252C%2520a%2520collection%2520of%2520five%2520hours%2520of%2520annotated%2520Sign%2520Language%2520videos%2520is%2520used%2520as%250Athe%2520dataset%252C%2520and%2520the%2520initial%2520results%2520are%2520promising%2520in%2520a%2520zero-shot%250Asetting.SLVideo%2520is%2520shown%2520to%2520not%2520only%2520address%2520the%2520problem%2520of%2520searching%2520sign%250Alanguage%2520videos%2520but%2520also%2520supports%2520a%2520Sign%2520Language%2520thesaurus%2520with%2520a%2520search%2520by%250Asimilarity%2520technique.%250A%2520%2520Project%2520web%2520page%253A%2520https%253A//novasearch.github.io/SLVideo/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLVideo%3A%20A%20Sign%20Language%20Video%20Moment%20Retrieval%20Framework&entry.906535625=Gon%C3%A7alo%20Vinagre%20Martins%20and%20Afonso%20Quinaz%20and%20Carla%20Viegas%20and%20Sofia%20Cavaco%20and%20Jo%C3%A3o%20Magalh%C3%A3es&entry.1292438233=%20%20Sign%20Language%20Recognition%20has%20been%20studied%20and%20developed%20throughout%20the%20years%0Ato%20help%20the%20deaf%20and%20hard-of-hearing%20people%20in%20their%20day-to-day%20lives.%20These%0Atechnologies%20leverage%20manual%20sign%20recognition%20algorithms%2C%20however%2C%20most%20of%20them%0Alack%20the%20recognition%20of%20facial%20expressions%2C%20which%20are%20also%20an%20essential%20part%20of%0ASign%20Language%20as%20they%20allow%20the%20speaker%20to%20add%20expressiveness%20to%20their%20dialogue%0Aor%20even%20change%20the%20meaning%20of%20certain%20manual%20signs.%20SLVideo%20is%20a%20video%20moment%0Aretrieval%20software%20for%20Sign%20Language%20videos%20with%20a%20focus%20on%20both%20hands%20and%0Afacial%20signs.%20The%20system%20extracts%20embedding%20representations%20for%20the%20hand%20and%0Aface%20signs%20from%20video%20frames%20to%20capture%20the%20language%20signs%20in%20full.%20This%20will%0Athen%20allow%20the%20user%20to%20search%20for%20a%20specific%20sign%20language%20video%20segment%20with%0Atext%20queries%2C%20or%20to%20search%20by%20similar%20sign%20language%20videos.%20To%20test%20this%0Asystem%2C%20a%20collection%20of%20five%20hours%20of%20annotated%20Sign%20Language%20videos%20is%20used%20as%0Athe%20dataset%2C%20and%20the%20initial%20results%20are%20promising%20in%20a%20zero-shot%0Asetting.SLVideo%20is%20shown%20to%20not%20only%20address%20the%20problem%20of%20searching%20sign%0Alanguage%20videos%20but%20also%20supports%20a%20Sign%20Language%20thesaurus%20with%20a%20search%20by%0Asimilarity%20technique.%0A%20%20Project%20web%20page%3A%20https%3A//novasearch.github.io/SLVideo/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15668v1&entry.124074799=Read"},
{"title": "Multiple importance sampling for stochastic gradient estimation", "author": "Corentin Sala\u00fcn and Xingchang Huang and Iliyan Georgiev and Niloy J. Mitra and Gurprit Singh", "abstract": "  We introduce a theoretical and practical framework for efficient importance\nsampling of mini-batch samples for gradient estimation from single and multiple\nprobability distributions. To handle noisy gradients, our framework dynamically\nevolves the importance distribution during training by utilizing a\nself-adaptive metric. Our framework combines multiple, diverse sampling\ndistributions, each tailored to specific parameter gradients. This approach\nfacilitates the importance sampling of vector-valued gradient estimation.\nRather than naively combining multiple distributions, our framework involves\noptimally weighting data contribution across multiple distributions. This\nadapted combination of multiple importance yields superior gradient estimates,\nleading to faster training convergence. We demonstrate the effectiveness of our\napproach through empirical evaluations across a range of optimization tasks\nlike classification and regression on both image and point cloud datasets.\n", "link": "http://arxiv.org/abs/2407.15525v1", "date": "2024-07-22", "relevancy": 2.0369, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5273}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.508}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple%20importance%20sampling%20for%20stochastic%20gradient%20estimation&body=Title%3A%20Multiple%20importance%20sampling%20for%20stochastic%20gradient%20estimation%0AAuthor%3A%20Corentin%20Sala%C3%BCn%20and%20Xingchang%20Huang%20and%20Iliyan%20Georgiev%20and%20Niloy%20J.%20Mitra%20and%20Gurprit%20Singh%0AAbstract%3A%20%20%20We%20introduce%20a%20theoretical%20and%20practical%20framework%20for%20efficient%20importance%0Asampling%20of%20mini-batch%20samples%20for%20gradient%20estimation%20from%20single%20and%20multiple%0Aprobability%20distributions.%20To%20handle%20noisy%20gradients%2C%20our%20framework%20dynamically%0Aevolves%20the%20importance%20distribution%20during%20training%20by%20utilizing%20a%0Aself-adaptive%20metric.%20Our%20framework%20combines%20multiple%2C%20diverse%20sampling%0Adistributions%2C%20each%20tailored%20to%20specific%20parameter%20gradients.%20This%20approach%0Afacilitates%20the%20importance%20sampling%20of%20vector-valued%20gradient%20estimation.%0ARather%20than%20naively%20combining%20multiple%20distributions%2C%20our%20framework%20involves%0Aoptimally%20weighting%20data%20contribution%20across%20multiple%20distributions.%20This%0Aadapted%20combination%20of%20multiple%20importance%20yields%20superior%20gradient%20estimates%2C%0Aleading%20to%20faster%20training%20convergence.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20through%20empirical%20evaluations%20across%20a%20range%20of%20optimization%20tasks%0Alike%20classification%20and%20regression%20on%20both%20image%20and%20point%20cloud%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple%2520importance%2520sampling%2520for%2520stochastic%2520gradient%2520estimation%26entry.906535625%3DCorentin%2520Sala%25C3%25BCn%2520and%2520Xingchang%2520Huang%2520and%2520Iliyan%2520Georgiev%2520and%2520Niloy%2520J.%2520Mitra%2520and%2520Gurprit%2520Singh%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520theoretical%2520and%2520practical%2520framework%2520for%2520efficient%2520importance%250Asampling%2520of%2520mini-batch%2520samples%2520for%2520gradient%2520estimation%2520from%2520single%2520and%2520multiple%250Aprobability%2520distributions.%2520To%2520handle%2520noisy%2520gradients%252C%2520our%2520framework%2520dynamically%250Aevolves%2520the%2520importance%2520distribution%2520during%2520training%2520by%2520utilizing%2520a%250Aself-adaptive%2520metric.%2520Our%2520framework%2520combines%2520multiple%252C%2520diverse%2520sampling%250Adistributions%252C%2520each%2520tailored%2520to%2520specific%2520parameter%2520gradients.%2520This%2520approach%250Afacilitates%2520the%2520importance%2520sampling%2520of%2520vector-valued%2520gradient%2520estimation.%250ARather%2520than%2520naively%2520combining%2520multiple%2520distributions%252C%2520our%2520framework%2520involves%250Aoptimally%2520weighting%2520data%2520contribution%2520across%2520multiple%2520distributions.%2520This%250Aadapted%2520combination%2520of%2520multiple%2520importance%2520yields%2520superior%2520gradient%2520estimates%252C%250Aleading%2520to%2520faster%2520training%2520convergence.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach%2520through%2520empirical%2520evaluations%2520across%2520a%2520range%2520of%2520optimization%2520tasks%250Alike%2520classification%2520and%2520regression%2520on%2520both%2520image%2520and%2520point%2520cloud%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple%20importance%20sampling%20for%20stochastic%20gradient%20estimation&entry.906535625=Corentin%20Sala%C3%BCn%20and%20Xingchang%20Huang%20and%20Iliyan%20Georgiev%20and%20Niloy%20J.%20Mitra%20and%20Gurprit%20Singh&entry.1292438233=%20%20We%20introduce%20a%20theoretical%20and%20practical%20framework%20for%20efficient%20importance%0Asampling%20of%20mini-batch%20samples%20for%20gradient%20estimation%20from%20single%20and%20multiple%0Aprobability%20distributions.%20To%20handle%20noisy%20gradients%2C%20our%20framework%20dynamically%0Aevolves%20the%20importance%20distribution%20during%20training%20by%20utilizing%20a%0Aself-adaptive%20metric.%20Our%20framework%20combines%20multiple%2C%20diverse%20sampling%0Adistributions%2C%20each%20tailored%20to%20specific%20parameter%20gradients.%20This%20approach%0Afacilitates%20the%20importance%20sampling%20of%20vector-valued%20gradient%20estimation.%0ARather%20than%20naively%20combining%20multiple%20distributions%2C%20our%20framework%20involves%0Aoptimally%20weighting%20data%20contribution%20across%20multiple%20distributions.%20This%0Aadapted%20combination%20of%20multiple%20importance%20yields%20superior%20gradient%20estimates%2C%0Aleading%20to%20faster%20training%20convergence.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20through%20empirical%20evaluations%20across%20a%20range%20of%20optimization%20tasks%0Alike%20classification%20and%20regression%20on%20both%20image%20and%20point%20cloud%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15525v1&entry.124074799=Read"},
{"title": "Norface: Improving Facial Expression Analysis by Identity Normalization", "author": "Hanwei Liu and Rudong An and Zhimeng Zhang and Bowen Ma and Wei Zhang and Yan Song and Yujing Hu and Wei Chen and Yu Ding", "abstract": "  Facial Expression Analysis remains a challenging task due to unexpected\ntask-irrelevant noise, such as identity, head pose, and background. To address\nthis issue, this paper proposes a novel framework, called Norface, that is\nunified for both Action Unit (AU) analysis and Facial Emotion Recognition (FER)\ntasks. Norface consists of a normalization network and a classification\nnetwork. First, the carefully designed normalization network struggles to\ndirectly remove the above task-irrelevant noise, by maintaining facial\nexpression consistency but normalizing all original images to a common identity\nwith consistent pose, and background. Then, these additional normalized images\nare fed into the classification network. Due to consistent identity and other\nfactors (e.g. head pose, background, etc.), the normalized images enable the\nclassification network to extract useful expression information more\neffectively. Additionally, the classification network incorporates a Mixture of\nExperts to refine the latent representation, including handling the input of\nfacial representations and the output of multiple (AU or emotion) labels.\nExtensive experiments validate the carefully designed framework with the\ninsight of identity normalization. The proposed method outperforms existing\nSOTA methods in multiple facial expression analysis tasks, including AU\ndetection, AU intensity estimation, and FER tasks, as well as their\ncross-dataset tasks. For the normalized datasets and code please visit\n{https://norface-fea.github.io/}.\n", "link": "http://arxiv.org/abs/2407.15617v1", "date": "2024-07-22", "relevancy": 2.0358, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5227}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5002}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Norface%3A%20Improving%20Facial%20Expression%20Analysis%20by%20Identity%20Normalization&body=Title%3A%20Norface%3A%20Improving%20Facial%20Expression%20Analysis%20by%20Identity%20Normalization%0AAuthor%3A%20Hanwei%20Liu%20and%20Rudong%20An%20and%20Zhimeng%20Zhang%20and%20Bowen%20Ma%20and%20Wei%20Zhang%20and%20Yan%20Song%20and%20Yujing%20Hu%20and%20Wei%20Chen%20and%20Yu%20Ding%0AAbstract%3A%20%20%20Facial%20Expression%20Analysis%20remains%20a%20challenging%20task%20due%20to%20unexpected%0Atask-irrelevant%20noise%2C%20such%20as%20identity%2C%20head%20pose%2C%20and%20background.%20To%20address%0Athis%20issue%2C%20this%20paper%20proposes%20a%20novel%20framework%2C%20called%20Norface%2C%20that%20is%0Aunified%20for%20both%20Action%20Unit%20%28AU%29%20analysis%20and%20Facial%20Emotion%20Recognition%20%28FER%29%0Atasks.%20Norface%20consists%20of%20a%20normalization%20network%20and%20a%20classification%0Anetwork.%20First%2C%20the%20carefully%20designed%20normalization%20network%20struggles%20to%0Adirectly%20remove%20the%20above%20task-irrelevant%20noise%2C%20by%20maintaining%20facial%0Aexpression%20consistency%20but%20normalizing%20all%20original%20images%20to%20a%20common%20identity%0Awith%20consistent%20pose%2C%20and%20background.%20Then%2C%20these%20additional%20normalized%20images%0Aare%20fed%20into%20the%20classification%20network.%20Due%20to%20consistent%20identity%20and%20other%0Afactors%20%28e.g.%20head%20pose%2C%20background%2C%20etc.%29%2C%20the%20normalized%20images%20enable%20the%0Aclassification%20network%20to%20extract%20useful%20expression%20information%20more%0Aeffectively.%20Additionally%2C%20the%20classification%20network%20incorporates%20a%20Mixture%20of%0AExperts%20to%20refine%20the%20latent%20representation%2C%20including%20handling%20the%20input%20of%0Afacial%20representations%20and%20the%20output%20of%20multiple%20%28AU%20or%20emotion%29%20labels.%0AExtensive%20experiments%20validate%20the%20carefully%20designed%20framework%20with%20the%0Ainsight%20of%20identity%20normalization.%20The%20proposed%20method%20outperforms%20existing%0ASOTA%20methods%20in%20multiple%20facial%20expression%20analysis%20tasks%2C%20including%20AU%0Adetection%2C%20AU%20intensity%20estimation%2C%20and%20FER%20tasks%2C%20as%20well%20as%20their%0Across-dataset%20tasks.%20For%20the%20normalized%20datasets%20and%20code%20please%20visit%0A%7Bhttps%3A//norface-fea.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNorface%253A%2520Improving%2520Facial%2520Expression%2520Analysis%2520by%2520Identity%2520Normalization%26entry.906535625%3DHanwei%2520Liu%2520and%2520Rudong%2520An%2520and%2520Zhimeng%2520Zhang%2520and%2520Bowen%2520Ma%2520and%2520Wei%2520Zhang%2520and%2520Yan%2520Song%2520and%2520Yujing%2520Hu%2520and%2520Wei%2520Chen%2520and%2520Yu%2520Ding%26entry.1292438233%3D%2520%2520Facial%2520Expression%2520Analysis%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520unexpected%250Atask-irrelevant%2520noise%252C%2520such%2520as%2520identity%252C%2520head%2520pose%252C%2520and%2520background.%2520To%2520address%250Athis%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520framework%252C%2520called%2520Norface%252C%2520that%2520is%250Aunified%2520for%2520both%2520Action%2520Unit%2520%2528AU%2529%2520analysis%2520and%2520Facial%2520Emotion%2520Recognition%2520%2528FER%2529%250Atasks.%2520Norface%2520consists%2520of%2520a%2520normalization%2520network%2520and%2520a%2520classification%250Anetwork.%2520First%252C%2520the%2520carefully%2520designed%2520normalization%2520network%2520struggles%2520to%250Adirectly%2520remove%2520the%2520above%2520task-irrelevant%2520noise%252C%2520by%2520maintaining%2520facial%250Aexpression%2520consistency%2520but%2520normalizing%2520all%2520original%2520images%2520to%2520a%2520common%2520identity%250Awith%2520consistent%2520pose%252C%2520and%2520background.%2520Then%252C%2520these%2520additional%2520normalized%2520images%250Aare%2520fed%2520into%2520the%2520classification%2520network.%2520Due%2520to%2520consistent%2520identity%2520and%2520other%250Afactors%2520%2528e.g.%2520head%2520pose%252C%2520background%252C%2520etc.%2529%252C%2520the%2520normalized%2520images%2520enable%2520the%250Aclassification%2520network%2520to%2520extract%2520useful%2520expression%2520information%2520more%250Aeffectively.%2520Additionally%252C%2520the%2520classification%2520network%2520incorporates%2520a%2520Mixture%2520of%250AExperts%2520to%2520refine%2520the%2520latent%2520representation%252C%2520including%2520handling%2520the%2520input%2520of%250Afacial%2520representations%2520and%2520the%2520output%2520of%2520multiple%2520%2528AU%2520or%2520emotion%2529%2520labels.%250AExtensive%2520experiments%2520validate%2520the%2520carefully%2520designed%2520framework%2520with%2520the%250Ainsight%2520of%2520identity%2520normalization.%2520The%2520proposed%2520method%2520outperforms%2520existing%250ASOTA%2520methods%2520in%2520multiple%2520facial%2520expression%2520analysis%2520tasks%252C%2520including%2520AU%250Adetection%252C%2520AU%2520intensity%2520estimation%252C%2520and%2520FER%2520tasks%252C%2520as%2520well%2520as%2520their%250Across-dataset%2520tasks.%2520For%2520the%2520normalized%2520datasets%2520and%2520code%2520please%2520visit%250A%257Bhttps%253A//norface-fea.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Norface%3A%20Improving%20Facial%20Expression%20Analysis%20by%20Identity%20Normalization&entry.906535625=Hanwei%20Liu%20and%20Rudong%20An%20and%20Zhimeng%20Zhang%20and%20Bowen%20Ma%20and%20Wei%20Zhang%20and%20Yan%20Song%20and%20Yujing%20Hu%20and%20Wei%20Chen%20and%20Yu%20Ding&entry.1292438233=%20%20Facial%20Expression%20Analysis%20remains%20a%20challenging%20task%20due%20to%20unexpected%0Atask-irrelevant%20noise%2C%20such%20as%20identity%2C%20head%20pose%2C%20and%20background.%20To%20address%0Athis%20issue%2C%20this%20paper%20proposes%20a%20novel%20framework%2C%20called%20Norface%2C%20that%20is%0Aunified%20for%20both%20Action%20Unit%20%28AU%29%20analysis%20and%20Facial%20Emotion%20Recognition%20%28FER%29%0Atasks.%20Norface%20consists%20of%20a%20normalization%20network%20and%20a%20classification%0Anetwork.%20First%2C%20the%20carefully%20designed%20normalization%20network%20struggles%20to%0Adirectly%20remove%20the%20above%20task-irrelevant%20noise%2C%20by%20maintaining%20facial%0Aexpression%20consistency%20but%20normalizing%20all%20original%20images%20to%20a%20common%20identity%0Awith%20consistent%20pose%2C%20and%20background.%20Then%2C%20these%20additional%20normalized%20images%0Aare%20fed%20into%20the%20classification%20network.%20Due%20to%20consistent%20identity%20and%20other%0Afactors%20%28e.g.%20head%20pose%2C%20background%2C%20etc.%29%2C%20the%20normalized%20images%20enable%20the%0Aclassification%20network%20to%20extract%20useful%20expression%20information%20more%0Aeffectively.%20Additionally%2C%20the%20classification%20network%20incorporates%20a%20Mixture%20of%0AExperts%20to%20refine%20the%20latent%20representation%2C%20including%20handling%20the%20input%20of%0Afacial%20representations%20and%20the%20output%20of%20multiple%20%28AU%20or%20emotion%29%20labels.%0AExtensive%20experiments%20validate%20the%20carefully%20designed%20framework%20with%20the%0Ainsight%20of%20identity%20normalization.%20The%20proposed%20method%20outperforms%20existing%0ASOTA%20methods%20in%20multiple%20facial%20expression%20analysis%20tasks%2C%20including%20AU%0Adetection%2C%20AU%20intensity%20estimation%2C%20and%20FER%20tasks%2C%20as%20well%20as%20their%0Across-dataset%20tasks.%20For%20the%20normalized%20datasets%20and%20code%20please%20visit%0A%7Bhttps%3A//norface-fea.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15617v1&entry.124074799=Read"},
{"title": "Explorative Imitation Learning: A Path Signature Approach for Continuous\n  Environments", "author": "Nathan Gavenski and Juarez Monteiro and Felipe Meneguzzi and Michael Luck and Odinaldo Rodrigues", "abstract": "  Some imitation learning methods combine behavioural cloning with\nself-supervision to infer actions from state pairs. However, most rely on a\nlarge number of expert trajectories to increase generalisation and human\nintervention to capture key aspects of the problem, such as domain constraints.\nIn this paper, we propose Continuous Imitation Learning from Observation\n(CILO), a new method augmenting imitation learning with two important features:\n(i) exploration, allowing for more diverse state transitions, requiring less\nexpert trajectories and resulting in fewer training iterations; and (ii) path\nsignatures, allowing for automatic encoding of constraints, through the\ncreation of non-parametric representations of agents and expert trajectories.\nWe compared CILO with a baseline and two leading imitation learning methods in\nfive environments. It had the best overall performance of all methods in all\nenvironments, outperforming the expert in two of them.\n", "link": "http://arxiv.org/abs/2407.04856v2", "date": "2024-07-22", "relevancy": 2.0357, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5586}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5064}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explorative%20Imitation%20Learning%3A%20A%20Path%20Signature%20Approach%20for%20Continuous%0A%20%20Environments&body=Title%3A%20Explorative%20Imitation%20Learning%3A%20A%20Path%20Signature%20Approach%20for%20Continuous%0A%20%20Environments%0AAuthor%3A%20Nathan%20Gavenski%20and%20Juarez%20Monteiro%20and%20Felipe%20Meneguzzi%20and%20Michael%20Luck%20and%20Odinaldo%20Rodrigues%0AAbstract%3A%20%20%20Some%20imitation%20learning%20methods%20combine%20behavioural%20cloning%20with%0Aself-supervision%20to%20infer%20actions%20from%20state%20pairs.%20However%2C%20most%20rely%20on%20a%0Alarge%20number%20of%20expert%20trajectories%20to%20increase%20generalisation%20and%20human%0Aintervention%20to%20capture%20key%20aspects%20of%20the%20problem%2C%20such%20as%20domain%20constraints.%0AIn%20this%20paper%2C%20we%20propose%20Continuous%20Imitation%20Learning%20from%20Observation%0A%28CILO%29%2C%20a%20new%20method%20augmenting%20imitation%20learning%20with%20two%20important%20features%3A%0A%28i%29%20exploration%2C%20allowing%20for%20more%20diverse%20state%20transitions%2C%20requiring%20less%0Aexpert%20trajectories%20and%20resulting%20in%20fewer%20training%20iterations%3B%20and%20%28ii%29%20path%0Asignatures%2C%20allowing%20for%20automatic%20encoding%20of%20constraints%2C%20through%20the%0Acreation%20of%20non-parametric%20representations%20of%20agents%20and%20expert%20trajectories.%0AWe%20compared%20CILO%20with%20a%20baseline%20and%20two%20leading%20imitation%20learning%20methods%20in%0Afive%20environments.%20It%20had%20the%20best%20overall%20performance%20of%20all%20methods%20in%20all%0Aenvironments%2C%20outperforming%20the%20expert%20in%20two%20of%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplorative%2520Imitation%2520Learning%253A%2520A%2520Path%2520Signature%2520Approach%2520for%2520Continuous%250A%2520%2520Environments%26entry.906535625%3DNathan%2520Gavenski%2520and%2520Juarez%2520Monteiro%2520and%2520Felipe%2520Meneguzzi%2520and%2520Michael%2520Luck%2520and%2520Odinaldo%2520Rodrigues%26entry.1292438233%3D%2520%2520Some%2520imitation%2520learning%2520methods%2520combine%2520behavioural%2520cloning%2520with%250Aself-supervision%2520to%2520infer%2520actions%2520from%2520state%2520pairs.%2520However%252C%2520most%2520rely%2520on%2520a%250Alarge%2520number%2520of%2520expert%2520trajectories%2520to%2520increase%2520generalisation%2520and%2520human%250Aintervention%2520to%2520capture%2520key%2520aspects%2520of%2520the%2520problem%252C%2520such%2520as%2520domain%2520constraints.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520Continuous%2520Imitation%2520Learning%2520from%2520Observation%250A%2528CILO%2529%252C%2520a%2520new%2520method%2520augmenting%2520imitation%2520learning%2520with%2520two%2520important%2520features%253A%250A%2528i%2529%2520exploration%252C%2520allowing%2520for%2520more%2520diverse%2520state%2520transitions%252C%2520requiring%2520less%250Aexpert%2520trajectories%2520and%2520resulting%2520in%2520fewer%2520training%2520iterations%253B%2520and%2520%2528ii%2529%2520path%250Asignatures%252C%2520allowing%2520for%2520automatic%2520encoding%2520of%2520constraints%252C%2520through%2520the%250Acreation%2520of%2520non-parametric%2520representations%2520of%2520agents%2520and%2520expert%2520trajectories.%250AWe%2520compared%2520CILO%2520with%2520a%2520baseline%2520and%2520two%2520leading%2520imitation%2520learning%2520methods%2520in%250Afive%2520environments.%2520It%2520had%2520the%2520best%2520overall%2520performance%2520of%2520all%2520methods%2520in%2520all%250Aenvironments%252C%2520outperforming%2520the%2520expert%2520in%2520two%2520of%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explorative%20Imitation%20Learning%3A%20A%20Path%20Signature%20Approach%20for%20Continuous%0A%20%20Environments&entry.906535625=Nathan%20Gavenski%20and%20Juarez%20Monteiro%20and%20Felipe%20Meneguzzi%20and%20Michael%20Luck%20and%20Odinaldo%20Rodrigues&entry.1292438233=%20%20Some%20imitation%20learning%20methods%20combine%20behavioural%20cloning%20with%0Aself-supervision%20to%20infer%20actions%20from%20state%20pairs.%20However%2C%20most%20rely%20on%20a%0Alarge%20number%20of%20expert%20trajectories%20to%20increase%20generalisation%20and%20human%0Aintervention%20to%20capture%20key%20aspects%20of%20the%20problem%2C%20such%20as%20domain%20constraints.%0AIn%20this%20paper%2C%20we%20propose%20Continuous%20Imitation%20Learning%20from%20Observation%0A%28CILO%29%2C%20a%20new%20method%20augmenting%20imitation%20learning%20with%20two%20important%20features%3A%0A%28i%29%20exploration%2C%20allowing%20for%20more%20diverse%20state%20transitions%2C%20requiring%20less%0Aexpert%20trajectories%20and%20resulting%20in%20fewer%20training%20iterations%3B%20and%20%28ii%29%20path%0Asignatures%2C%20allowing%20for%20automatic%20encoding%20of%20constraints%2C%20through%20the%0Acreation%20of%20non-parametric%20representations%20of%20agents%20and%20expert%20trajectories.%0AWe%20compared%20CILO%20with%20a%20baseline%20and%20two%20leading%20imitation%20learning%20methods%20in%0Afive%20environments.%20It%20had%20the%20best%20overall%20performance%20of%20all%20methods%20in%20all%0Aenvironments%2C%20outperforming%20the%20expert%20in%20two%20of%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04856v2&entry.124074799=Read"},
{"title": "Interrogating AI: Characterizing Emergent Playful Interactions with\n  ChatGPT", "author": "Mohammad Ronagh Nikghalb and Jinghui Cheng", "abstract": "  In an era of AI's growing capabilities and influences, recent advancements\nare constantly reshaping HCI and CSCW's view of AI. Playful interactions with\nAI systems naturally emerged as an important way for users to make sense of the\never-changing technology. However, these emergent and playful interactions are\nunderexamined. We target this gap by investigating playful interactions\nexhibited by users of an emerging AI technology, ChatGPT. Through a thematic\nanalysis of 372 user-generated posts on the ChatGPT subreddit, we found that\nmore than half of user discourse revolves around playful interactions. The\nanalysis further allowed us to construct a preliminary framework to describe\nthese interactions, categorizing them into six types: reflecting, jesting,\nimitating, challenging, tricking, and contriving; each included sub-categories.\nThis study contributes to the field of HCI and CSCW by illuminating the\nmultifaceted nature of playful interactions with AI, underlining their\nsignificance in helping users assess AI agency, shaping the human-AI\nrelationship, and offering rich implications to AI system design.\n", "link": "http://arxiv.org/abs/2401.08405v2", "date": "2024-07-22", "relevancy": 2.0301, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5345}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5152}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interrogating%20AI%3A%20Characterizing%20Emergent%20Playful%20Interactions%20with%0A%20%20ChatGPT&body=Title%3A%20Interrogating%20AI%3A%20Characterizing%20Emergent%20Playful%20Interactions%20with%0A%20%20ChatGPT%0AAuthor%3A%20Mohammad%20Ronagh%20Nikghalb%20and%20Jinghui%20Cheng%0AAbstract%3A%20%20%20In%20an%20era%20of%20AI%27s%20growing%20capabilities%20and%20influences%2C%20recent%20advancements%0Aare%20constantly%20reshaping%20HCI%20and%20CSCW%27s%20view%20of%20AI.%20Playful%20interactions%20with%0AAI%20systems%20naturally%20emerged%20as%20an%20important%20way%20for%20users%20to%20make%20sense%20of%20the%0Aever-changing%20technology.%20However%2C%20these%20emergent%20and%20playful%20interactions%20are%0Aunderexamined.%20We%20target%20this%20gap%20by%20investigating%20playful%20interactions%0Aexhibited%20by%20users%20of%20an%20emerging%20AI%20technology%2C%20ChatGPT.%20Through%20a%20thematic%0Aanalysis%20of%20372%20user-generated%20posts%20on%20the%20ChatGPT%20subreddit%2C%20we%20found%20that%0Amore%20than%20half%20of%20user%20discourse%20revolves%20around%20playful%20interactions.%20The%0Aanalysis%20further%20allowed%20us%20to%20construct%20a%20preliminary%20framework%20to%20describe%0Athese%20interactions%2C%20categorizing%20them%20into%20six%20types%3A%20reflecting%2C%20jesting%2C%0Aimitating%2C%20challenging%2C%20tricking%2C%20and%20contriving%3B%20each%20included%20sub-categories.%0AThis%20study%20contributes%20to%20the%20field%20of%20HCI%20and%20CSCW%20by%20illuminating%20the%0Amultifaceted%20nature%20of%20playful%20interactions%20with%20AI%2C%20underlining%20their%0Asignificance%20in%20helping%20users%20assess%20AI%20agency%2C%20shaping%20the%20human-AI%0Arelationship%2C%20and%20offering%20rich%20implications%20to%20AI%20system%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08405v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterrogating%2520AI%253A%2520Characterizing%2520Emergent%2520Playful%2520Interactions%2520with%250A%2520%2520ChatGPT%26entry.906535625%3DMohammad%2520Ronagh%2520Nikghalb%2520and%2520Jinghui%2520Cheng%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520of%2520AI%2527s%2520growing%2520capabilities%2520and%2520influences%252C%2520recent%2520advancements%250Aare%2520constantly%2520reshaping%2520HCI%2520and%2520CSCW%2527s%2520view%2520of%2520AI.%2520Playful%2520interactions%2520with%250AAI%2520systems%2520naturally%2520emerged%2520as%2520an%2520important%2520way%2520for%2520users%2520to%2520make%2520sense%2520of%2520the%250Aever-changing%2520technology.%2520However%252C%2520these%2520emergent%2520and%2520playful%2520interactions%2520are%250Aunderexamined.%2520We%2520target%2520this%2520gap%2520by%2520investigating%2520playful%2520interactions%250Aexhibited%2520by%2520users%2520of%2520an%2520emerging%2520AI%2520technology%252C%2520ChatGPT.%2520Through%2520a%2520thematic%250Aanalysis%2520of%2520372%2520user-generated%2520posts%2520on%2520the%2520ChatGPT%2520subreddit%252C%2520we%2520found%2520that%250Amore%2520than%2520half%2520of%2520user%2520discourse%2520revolves%2520around%2520playful%2520interactions.%2520The%250Aanalysis%2520further%2520allowed%2520us%2520to%2520construct%2520a%2520preliminary%2520framework%2520to%2520describe%250Athese%2520interactions%252C%2520categorizing%2520them%2520into%2520six%2520types%253A%2520reflecting%252C%2520jesting%252C%250Aimitating%252C%2520challenging%252C%2520tricking%252C%2520and%2520contriving%253B%2520each%2520included%2520sub-categories.%250AThis%2520study%2520contributes%2520to%2520the%2520field%2520of%2520HCI%2520and%2520CSCW%2520by%2520illuminating%2520the%250Amultifaceted%2520nature%2520of%2520playful%2520interactions%2520with%2520AI%252C%2520underlining%2520their%250Asignificance%2520in%2520helping%2520users%2520assess%2520AI%2520agency%252C%2520shaping%2520the%2520human-AI%250Arelationship%252C%2520and%2520offering%2520rich%2520implications%2520to%2520AI%2520system%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08405v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interrogating%20AI%3A%20Characterizing%20Emergent%20Playful%20Interactions%20with%0A%20%20ChatGPT&entry.906535625=Mohammad%20Ronagh%20Nikghalb%20and%20Jinghui%20Cheng&entry.1292438233=%20%20In%20an%20era%20of%20AI%27s%20growing%20capabilities%20and%20influences%2C%20recent%20advancements%0Aare%20constantly%20reshaping%20HCI%20and%20CSCW%27s%20view%20of%20AI.%20Playful%20interactions%20with%0AAI%20systems%20naturally%20emerged%20as%20an%20important%20way%20for%20users%20to%20make%20sense%20of%20the%0Aever-changing%20technology.%20However%2C%20these%20emergent%20and%20playful%20interactions%20are%0Aunderexamined.%20We%20target%20this%20gap%20by%20investigating%20playful%20interactions%0Aexhibited%20by%20users%20of%20an%20emerging%20AI%20technology%2C%20ChatGPT.%20Through%20a%20thematic%0Aanalysis%20of%20372%20user-generated%20posts%20on%20the%20ChatGPT%20subreddit%2C%20we%20found%20that%0Amore%20than%20half%20of%20user%20discourse%20revolves%20around%20playful%20interactions.%20The%0Aanalysis%20further%20allowed%20us%20to%20construct%20a%20preliminary%20framework%20to%20describe%0Athese%20interactions%2C%20categorizing%20them%20into%20six%20types%3A%20reflecting%2C%20jesting%2C%0Aimitating%2C%20challenging%2C%20tricking%2C%20and%20contriving%3B%20each%20included%20sub-categories.%0AThis%20study%20contributes%20to%20the%20field%20of%20HCI%20and%20CSCW%20by%20illuminating%20the%0Amultifaceted%20nature%20of%20playful%20interactions%20with%20AI%2C%20underlining%20their%0Asignificance%20in%20helping%20users%20assess%20AI%20agency%2C%20shaping%20the%20human-AI%0Arelationship%2C%20and%20offering%20rich%20implications%20to%20AI%20system%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08405v2&entry.124074799=Read"},
{"title": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers", "author": "Jonas Ngnaw\u00e9 and Sabyasachi Sahoo and Yann Pequignot and Fr\u00e9d\u00e9ric Precioso and Christian Gagn\u00e9", "abstract": "  Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate strong margin consistency\nwith a strong correlation between their input space margins and the logit\nmargins. Then, we show that we can effectively use the logit margin to\nconfidently detect brittle decisions with such models and accurately estimate\nrobust accuracy on an arbitrarily large test set by estimating the input\nmargins only on a small subset. Finally, we address cases where the model is\nnot sufficiently margin-consistent by learning a pseudo-margin from the feature\nrepresentation. Our findings highlight the potential of leveraging deep\nrepresentations to efficiently assess adversarial vulnerability in deployment\nscenarios.\n", "link": "http://arxiv.org/abs/2406.18451v2", "date": "2024-07-22", "relevancy": 2.0299, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Brittle%20Decisions%20for%20Free%3A%20Leveraging%20Margin%20Consistency%20in%0A%20%20Deep%20Robust%20Classifiers&body=Title%3A%20Detecting%20Brittle%20Decisions%20for%20Free%3A%20Leveraging%20Margin%20Consistency%20in%0A%20%20Deep%20Robust%20Classifiers%0AAuthor%3A%20Jonas%20Ngnaw%C3%A9%20and%20Sabyasachi%20Sahoo%20and%20Yann%20Pequignot%20and%20Fr%C3%A9d%C3%A9ric%20Precioso%20and%20Christian%20Gagn%C3%A9%0AAbstract%3A%20%20%20Despite%20extensive%20research%20on%20adversarial%20training%20strategies%20to%20improve%0Arobustness%2C%20the%20decisions%20of%20even%20the%20most%20robust%20deep%20learning%20models%20can%0Astill%20be%20quite%20sensitive%20to%20imperceptible%20perturbations%2C%20creating%20serious%20risks%0Awhen%20deploying%20them%20for%20high-stakes%20real-world%20applications.%20While%20detecting%0Asuch%20cases%20may%20be%20critical%2C%20evaluating%20a%20model%27s%20vulnerability%20at%20a%0Aper-instance%20level%20using%20adversarial%20attacks%20is%20computationally%20too%20intensive%0Aand%20unsuitable%20for%20real-time%20deployment%20scenarios.%20The%20input%20space%20margin%20is%0Athe%20exact%20score%20to%20detect%20non-robust%20samples%20and%20is%20intractable%20for%20deep%20neural%0Anetworks.%20This%20paper%20introduces%20the%20concept%20of%20margin%20consistency%20--%20a%20property%0Athat%20links%20the%20input%20space%20margins%20and%20the%20logit%20margins%20in%20robust%20models%20--%0Afor%20efficient%20detection%20of%20vulnerable%20samples.%20First%2C%20we%20establish%20that%20margin%0Aconsistency%20is%20a%20necessary%20and%20sufficient%20condition%20to%20use%20a%20model%27s%20logit%0Amargin%20as%20a%20score%20for%20identifying%20non-robust%20samples.%20Next%2C%20through%0Acomprehensive%20empirical%20analysis%20of%20various%20robustly%20trained%20models%20on%20CIFAR10%0Aand%20CIFAR100%20datasets%2C%20we%20show%20that%20they%20indicate%20strong%20margin%20consistency%0Awith%20a%20strong%20correlation%20between%20their%20input%20space%20margins%20and%20the%20logit%0Amargins.%20Then%2C%20we%20show%20that%20we%20can%20effectively%20use%20the%20logit%20margin%20to%0Aconfidently%20detect%20brittle%20decisions%20with%20such%20models%20and%20accurately%20estimate%0Arobust%20accuracy%20on%20an%20arbitrarily%20large%20test%20set%20by%20estimating%20the%20input%0Amargins%20only%20on%20a%20small%20subset.%20Finally%2C%20we%20address%20cases%20where%20the%20model%20is%0Anot%20sufficiently%20margin-consistent%20by%20learning%20a%20pseudo-margin%20from%20the%20feature%0Arepresentation.%20Our%20findings%20highlight%20the%20potential%20of%20leveraging%20deep%0Arepresentations%20to%20efficiently%20assess%20adversarial%20vulnerability%20in%20deployment%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Brittle%2520Decisions%2520for%2520Free%253A%2520Leveraging%2520Margin%2520Consistency%2520in%250A%2520%2520Deep%2520Robust%2520Classifiers%26entry.906535625%3DJonas%2520Ngnaw%25C3%25A9%2520and%2520Sabyasachi%2520Sahoo%2520and%2520Yann%2520Pequignot%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Precioso%2520and%2520Christian%2520Gagn%25C3%25A9%26entry.1292438233%3D%2520%2520Despite%2520extensive%2520research%2520on%2520adversarial%2520training%2520strategies%2520to%2520improve%250Arobustness%252C%2520the%2520decisions%2520of%2520even%2520the%2520most%2520robust%2520deep%2520learning%2520models%2520can%250Astill%2520be%2520quite%2520sensitive%2520to%2520imperceptible%2520perturbations%252C%2520creating%2520serious%2520risks%250Awhen%2520deploying%2520them%2520for%2520high-stakes%2520real-world%2520applications.%2520While%2520detecting%250Asuch%2520cases%2520may%2520be%2520critical%252C%2520evaluating%2520a%2520model%2527s%2520vulnerability%2520at%2520a%250Aper-instance%2520level%2520using%2520adversarial%2520attacks%2520is%2520computationally%2520too%2520intensive%250Aand%2520unsuitable%2520for%2520real-time%2520deployment%2520scenarios.%2520The%2520input%2520space%2520margin%2520is%250Athe%2520exact%2520score%2520to%2520detect%2520non-robust%2520samples%2520and%2520is%2520intractable%2520for%2520deep%2520neural%250Anetworks.%2520This%2520paper%2520introduces%2520the%2520concept%2520of%2520margin%2520consistency%2520--%2520a%2520property%250Athat%2520links%2520the%2520input%2520space%2520margins%2520and%2520the%2520logit%2520margins%2520in%2520robust%2520models%2520--%250Afor%2520efficient%2520detection%2520of%2520vulnerable%2520samples.%2520First%252C%2520we%2520establish%2520that%2520margin%250Aconsistency%2520is%2520a%2520necessary%2520and%2520sufficient%2520condition%2520to%2520use%2520a%2520model%2527s%2520logit%250Amargin%2520as%2520a%2520score%2520for%2520identifying%2520non-robust%2520samples.%2520Next%252C%2520through%250Acomprehensive%2520empirical%2520analysis%2520of%2520various%2520robustly%2520trained%2520models%2520on%2520CIFAR10%250Aand%2520CIFAR100%2520datasets%252C%2520we%2520show%2520that%2520they%2520indicate%2520strong%2520margin%2520consistency%250Awith%2520a%2520strong%2520correlation%2520between%2520their%2520input%2520space%2520margins%2520and%2520the%2520logit%250Amargins.%2520Then%252C%2520we%2520show%2520that%2520we%2520can%2520effectively%2520use%2520the%2520logit%2520margin%2520to%250Aconfidently%2520detect%2520brittle%2520decisions%2520with%2520such%2520models%2520and%2520accurately%2520estimate%250Arobust%2520accuracy%2520on%2520an%2520arbitrarily%2520large%2520test%2520set%2520by%2520estimating%2520the%2520input%250Amargins%2520only%2520on%2520a%2520small%2520subset.%2520Finally%252C%2520we%2520address%2520cases%2520where%2520the%2520model%2520is%250Anot%2520sufficiently%2520margin-consistent%2520by%2520learning%2520a%2520pseudo-margin%2520from%2520the%2520feature%250Arepresentation.%2520Our%2520findings%2520highlight%2520the%2520potential%2520of%2520leveraging%2520deep%250Arepresentations%2520to%2520efficiently%2520assess%2520adversarial%2520vulnerability%2520in%2520deployment%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Brittle%20Decisions%20for%20Free%3A%20Leveraging%20Margin%20Consistency%20in%0A%20%20Deep%20Robust%20Classifiers&entry.906535625=Jonas%20Ngnaw%C3%A9%20and%20Sabyasachi%20Sahoo%20and%20Yann%20Pequignot%20and%20Fr%C3%A9d%C3%A9ric%20Precioso%20and%20Christian%20Gagn%C3%A9&entry.1292438233=%20%20Despite%20extensive%20research%20on%20adversarial%20training%20strategies%20to%20improve%0Arobustness%2C%20the%20decisions%20of%20even%20the%20most%20robust%20deep%20learning%20models%20can%0Astill%20be%20quite%20sensitive%20to%20imperceptible%20perturbations%2C%20creating%20serious%20risks%0Awhen%20deploying%20them%20for%20high-stakes%20real-world%20applications.%20While%20detecting%0Asuch%20cases%20may%20be%20critical%2C%20evaluating%20a%20model%27s%20vulnerability%20at%20a%0Aper-instance%20level%20using%20adversarial%20attacks%20is%20computationally%20too%20intensive%0Aand%20unsuitable%20for%20real-time%20deployment%20scenarios.%20The%20input%20space%20margin%20is%0Athe%20exact%20score%20to%20detect%20non-robust%20samples%20and%20is%20intractable%20for%20deep%20neural%0Anetworks.%20This%20paper%20introduces%20the%20concept%20of%20margin%20consistency%20--%20a%20property%0Athat%20links%20the%20input%20space%20margins%20and%20the%20logit%20margins%20in%20robust%20models%20--%0Afor%20efficient%20detection%20of%20vulnerable%20samples.%20First%2C%20we%20establish%20that%20margin%0Aconsistency%20is%20a%20necessary%20and%20sufficient%20condition%20to%20use%20a%20model%27s%20logit%0Amargin%20as%20a%20score%20for%20identifying%20non-robust%20samples.%20Next%2C%20through%0Acomprehensive%20empirical%20analysis%20of%20various%20robustly%20trained%20models%20on%20CIFAR10%0Aand%20CIFAR100%20datasets%2C%20we%20show%20that%20they%20indicate%20strong%20margin%20consistency%0Awith%20a%20strong%20correlation%20between%20their%20input%20space%20margins%20and%20the%20logit%0Amargins.%20Then%2C%20we%20show%20that%20we%20can%20effectively%20use%20the%20logit%20margin%20to%0Aconfidently%20detect%20brittle%20decisions%20with%20such%20models%20and%20accurately%20estimate%0Arobust%20accuracy%20on%20an%20arbitrarily%20large%20test%20set%20by%20estimating%20the%20input%0Amargins%20only%20on%20a%20small%20subset.%20Finally%2C%20we%20address%20cases%20where%20the%20model%20is%0Anot%20sufficiently%20margin-consistent%20by%20learning%20a%20pseudo-margin%20from%20the%20feature%0Arepresentation.%20Our%20findings%20highlight%20the%20potential%20of%20leveraging%20deep%0Arepresentations%20to%20efficiently%20assess%20adversarial%20vulnerability%20in%20deployment%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18451v2&entry.124074799=Read"},
{"title": "Predictive Coding Networks and Inference Learning: Tutorial and Survey", "author": "Bj\u00f6rn van Zwol and Ro Jefferson and Egon L. van den Broek", "abstract": "  Recent years have witnessed a growing call for renewed emphasis on\nneuroscience-inspired approaches in artificial intelligence research, under the\nbanner of NeuroAI. A prime example of this is predictive coding networks\n(PCNs), based on the neuroscientific framework of predictive coding. This\nframework views the brain as a hierarchical Bayesian inference model that\nminimizes prediction errors through feedback connections. Unlike traditional\nneural networks trained with backpropagation (BP), PCNs utilize inference\nlearning (IL), a more biologically plausible algorithm that explains patterns\nof neural activity that BP cannot. Historically, IL has been more\ncomputationally intensive, but recent advancements have demonstrated that it\ncan achieve higher efficiency than BP with sufficient parallelization.\nFurthermore, PCNs can be mathematically considered a superset of traditional\nfeedforward neural networks (FNNs), significantly extending the range of\ntrainable architectures. As inherently probabilistic (graphical) latent\nvariable models, PCNs provide a versatile framework for both supervised\nlearning and unsupervised (generative) modeling that goes beyond traditional\nartificial neural networks. This work provides a comprehensive review and\ndetailed formal specification of PCNs, particularly situating them within the\ncontext of modern ML methods. Additionally, we introduce a Python library\n(PRECO) for practical implementation. This positions PC as a promising\nframework for future ML innovations.\n", "link": "http://arxiv.org/abs/2407.04117v2", "date": "2024-07-22", "relevancy": 2.0294, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5302}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5179}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictive%20Coding%20Networks%20and%20Inference%20Learning%3A%20Tutorial%20and%20Survey&body=Title%3A%20Predictive%20Coding%20Networks%20and%20Inference%20Learning%3A%20Tutorial%20and%20Survey%0AAuthor%3A%20Bj%C3%B6rn%20van%20Zwol%20and%20Ro%20Jefferson%20and%20Egon%20L.%20van%20den%20Broek%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20a%20growing%20call%20for%20renewed%20emphasis%20on%0Aneuroscience-inspired%20approaches%20in%20artificial%20intelligence%20research%2C%20under%20the%0Abanner%20of%20NeuroAI.%20A%20prime%20example%20of%20this%20is%20predictive%20coding%20networks%0A%28PCNs%29%2C%20based%20on%20the%20neuroscientific%20framework%20of%20predictive%20coding.%20This%0Aframework%20views%20the%20brain%20as%20a%20hierarchical%20Bayesian%20inference%20model%20that%0Aminimizes%20prediction%20errors%20through%20feedback%20connections.%20Unlike%20traditional%0Aneural%20networks%20trained%20with%20backpropagation%20%28BP%29%2C%20PCNs%20utilize%20inference%0Alearning%20%28IL%29%2C%20a%20more%20biologically%20plausible%20algorithm%20that%20explains%20patterns%0Aof%20neural%20activity%20that%20BP%20cannot.%20Historically%2C%20IL%20has%20been%20more%0Acomputationally%20intensive%2C%20but%20recent%20advancements%20have%20demonstrated%20that%20it%0Acan%20achieve%20higher%20efficiency%20than%20BP%20with%20sufficient%20parallelization.%0AFurthermore%2C%20PCNs%20can%20be%20mathematically%20considered%20a%20superset%20of%20traditional%0Afeedforward%20neural%20networks%20%28FNNs%29%2C%20significantly%20extending%20the%20range%20of%0Atrainable%20architectures.%20As%20inherently%20probabilistic%20%28graphical%29%20latent%0Avariable%20models%2C%20PCNs%20provide%20a%20versatile%20framework%20for%20both%20supervised%0Alearning%20and%20unsupervised%20%28generative%29%20modeling%20that%20goes%20beyond%20traditional%0Aartificial%20neural%20networks.%20This%20work%20provides%20a%20comprehensive%20review%20and%0Adetailed%20formal%20specification%20of%20PCNs%2C%20particularly%20situating%20them%20within%20the%0Acontext%20of%20modern%20ML%20methods.%20Additionally%2C%20we%20introduce%20a%20Python%20library%0A%28PRECO%29%20for%20practical%20implementation.%20This%20positions%20PC%20as%20a%20promising%0Aframework%20for%20future%20ML%20innovations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04117v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictive%2520Coding%2520Networks%2520and%2520Inference%2520Learning%253A%2520Tutorial%2520and%2520Survey%26entry.906535625%3DBj%25C3%25B6rn%2520van%2520Zwol%2520and%2520Ro%2520Jefferson%2520and%2520Egon%2520L.%2520van%2520den%2520Broek%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520a%2520growing%2520call%2520for%2520renewed%2520emphasis%2520on%250Aneuroscience-inspired%2520approaches%2520in%2520artificial%2520intelligence%2520research%252C%2520under%2520the%250Abanner%2520of%2520NeuroAI.%2520A%2520prime%2520example%2520of%2520this%2520is%2520predictive%2520coding%2520networks%250A%2528PCNs%2529%252C%2520based%2520on%2520the%2520neuroscientific%2520framework%2520of%2520predictive%2520coding.%2520This%250Aframework%2520views%2520the%2520brain%2520as%2520a%2520hierarchical%2520Bayesian%2520inference%2520model%2520that%250Aminimizes%2520prediction%2520errors%2520through%2520feedback%2520connections.%2520Unlike%2520traditional%250Aneural%2520networks%2520trained%2520with%2520backpropagation%2520%2528BP%2529%252C%2520PCNs%2520utilize%2520inference%250Alearning%2520%2528IL%2529%252C%2520a%2520more%2520biologically%2520plausible%2520algorithm%2520that%2520explains%2520patterns%250Aof%2520neural%2520activity%2520that%2520BP%2520cannot.%2520Historically%252C%2520IL%2520has%2520been%2520more%250Acomputationally%2520intensive%252C%2520but%2520recent%2520advancements%2520have%2520demonstrated%2520that%2520it%250Acan%2520achieve%2520higher%2520efficiency%2520than%2520BP%2520with%2520sufficient%2520parallelization.%250AFurthermore%252C%2520PCNs%2520can%2520be%2520mathematically%2520considered%2520a%2520superset%2520of%2520traditional%250Afeedforward%2520neural%2520networks%2520%2528FNNs%2529%252C%2520significantly%2520extending%2520the%2520range%2520of%250Atrainable%2520architectures.%2520As%2520inherently%2520probabilistic%2520%2528graphical%2529%2520latent%250Avariable%2520models%252C%2520PCNs%2520provide%2520a%2520versatile%2520framework%2520for%2520both%2520supervised%250Alearning%2520and%2520unsupervised%2520%2528generative%2529%2520modeling%2520that%2520goes%2520beyond%2520traditional%250Aartificial%2520neural%2520networks.%2520This%2520work%2520provides%2520a%2520comprehensive%2520review%2520and%250Adetailed%2520formal%2520specification%2520of%2520PCNs%252C%2520particularly%2520situating%2520them%2520within%2520the%250Acontext%2520of%2520modern%2520ML%2520methods.%2520Additionally%252C%2520we%2520introduce%2520a%2520Python%2520library%250A%2528PRECO%2529%2520for%2520practical%2520implementation.%2520This%2520positions%2520PC%2520as%2520a%2520promising%250Aframework%2520for%2520future%2520ML%2520innovations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04117v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Coding%20Networks%20and%20Inference%20Learning%3A%20Tutorial%20and%20Survey&entry.906535625=Bj%C3%B6rn%20van%20Zwol%20and%20Ro%20Jefferson%20and%20Egon%20L.%20van%20den%20Broek&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20a%20growing%20call%20for%20renewed%20emphasis%20on%0Aneuroscience-inspired%20approaches%20in%20artificial%20intelligence%20research%2C%20under%20the%0Abanner%20of%20NeuroAI.%20A%20prime%20example%20of%20this%20is%20predictive%20coding%20networks%0A%28PCNs%29%2C%20based%20on%20the%20neuroscientific%20framework%20of%20predictive%20coding.%20This%0Aframework%20views%20the%20brain%20as%20a%20hierarchical%20Bayesian%20inference%20model%20that%0Aminimizes%20prediction%20errors%20through%20feedback%20connections.%20Unlike%20traditional%0Aneural%20networks%20trained%20with%20backpropagation%20%28BP%29%2C%20PCNs%20utilize%20inference%0Alearning%20%28IL%29%2C%20a%20more%20biologically%20plausible%20algorithm%20that%20explains%20patterns%0Aof%20neural%20activity%20that%20BP%20cannot.%20Historically%2C%20IL%20has%20been%20more%0Acomputationally%20intensive%2C%20but%20recent%20advancements%20have%20demonstrated%20that%20it%0Acan%20achieve%20higher%20efficiency%20than%20BP%20with%20sufficient%20parallelization.%0AFurthermore%2C%20PCNs%20can%20be%20mathematically%20considered%20a%20superset%20of%20traditional%0Afeedforward%20neural%20networks%20%28FNNs%29%2C%20significantly%20extending%20the%20range%20of%0Atrainable%20architectures.%20As%20inherently%20probabilistic%20%28graphical%29%20latent%0Avariable%20models%2C%20PCNs%20provide%20a%20versatile%20framework%20for%20both%20supervised%0Alearning%20and%20unsupervised%20%28generative%29%20modeling%20that%20goes%20beyond%20traditional%0Aartificial%20neural%20networks.%20This%20work%20provides%20a%20comprehensive%20review%20and%0Adetailed%20formal%20specification%20of%20PCNs%2C%20particularly%20situating%20them%20within%20the%0Acontext%20of%20modern%20ML%20methods.%20Additionally%2C%20we%20introduce%20a%20Python%20library%0A%28PRECO%29%20for%20practical%20implementation.%20This%20positions%20PC%20as%20a%20promising%0Aframework%20for%20future%20ML%20innovations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04117v2&entry.124074799=Read"},
{"title": "Inverted Activations", "author": "Georgii Novikov and Ivan Oseledets", "abstract": "  The scaling of neural networks with increasing data and model sizes\nnecessitates more efficient deep learning algorithms. This paper addresses the\nmemory footprint challenge in neural network training by proposing a\nmodification to the handling of activation tensors in pointwise nonlinearity\nlayers. Traditionally, these layers save the entire input tensor for the\nbackward pass, leading to substantial memory use. Our method involves saving\nthe output tensor instead, reducing the memory required when the subsequent\nlayer also saves its input tensor. This approach is particularly beneficial for\ntransformer-based architectures like GPT, BERT, Mistral, and Llama. Application\nof our method involves taken an inverse function of nonlinearity. To the best\nof our knowledge, that can not be done analitically and instead we buid an\naccurate approximations using simpler functions. Experimental results confirm\nthat our method significantly reduces memory usage without affecting training\naccuracy. The implementation is available at\nhttps://github.com/PgLoLo/optiacts.\n", "link": "http://arxiv.org/abs/2407.15545v1", "date": "2024-07-22", "relevancy": 2.0294, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5224}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5101}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverted%20Activations&body=Title%3A%20Inverted%20Activations%0AAuthor%3A%20Georgii%20Novikov%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20The%20scaling%20of%20neural%20networks%20with%20increasing%20data%20and%20model%20sizes%0Anecessitates%20more%20efficient%20deep%20learning%20algorithms.%20This%20paper%20addresses%20the%0Amemory%20footprint%20challenge%20in%20neural%20network%20training%20by%20proposing%20a%0Amodification%20to%20the%20handling%20of%20activation%20tensors%20in%20pointwise%20nonlinearity%0Alayers.%20Traditionally%2C%20these%20layers%20save%20the%20entire%20input%20tensor%20for%20the%0Abackward%20pass%2C%20leading%20to%20substantial%20memory%20use.%20Our%20method%20involves%20saving%0Athe%20output%20tensor%20instead%2C%20reducing%20the%20memory%20required%20when%20the%20subsequent%0Alayer%20also%20saves%20its%20input%20tensor.%20This%20approach%20is%20particularly%20beneficial%20for%0Atransformer-based%20architectures%20like%20GPT%2C%20BERT%2C%20Mistral%2C%20and%20Llama.%20Application%0Aof%20our%20method%20involves%20taken%20an%20inverse%20function%20of%20nonlinearity.%20To%20the%20best%0Aof%20our%20knowledge%2C%20that%20can%20not%20be%20done%20analitically%20and%20instead%20we%20buid%20an%0Aaccurate%20approximations%20using%20simpler%20functions.%20Experimental%20results%20confirm%0Athat%20our%20method%20significantly%20reduces%20memory%20usage%20without%20affecting%20training%0Aaccuracy.%20The%20implementation%20is%20available%20at%0Ahttps%3A//github.com/PgLoLo/optiacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverted%2520Activations%26entry.906535625%3DGeorgii%2520Novikov%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3D%2520%2520The%2520scaling%2520of%2520neural%2520networks%2520with%2520increasing%2520data%2520and%2520model%2520sizes%250Anecessitates%2520more%2520efficient%2520deep%2520learning%2520algorithms.%2520This%2520paper%2520addresses%2520the%250Amemory%2520footprint%2520challenge%2520in%2520neural%2520network%2520training%2520by%2520proposing%2520a%250Amodification%2520to%2520the%2520handling%2520of%2520activation%2520tensors%2520in%2520pointwise%2520nonlinearity%250Alayers.%2520Traditionally%252C%2520these%2520layers%2520save%2520the%2520entire%2520input%2520tensor%2520for%2520the%250Abackward%2520pass%252C%2520leading%2520to%2520substantial%2520memory%2520use.%2520Our%2520method%2520involves%2520saving%250Athe%2520output%2520tensor%2520instead%252C%2520reducing%2520the%2520memory%2520required%2520when%2520the%2520subsequent%250Alayer%2520also%2520saves%2520its%2520input%2520tensor.%2520This%2520approach%2520is%2520particularly%2520beneficial%2520for%250Atransformer-based%2520architectures%2520like%2520GPT%252C%2520BERT%252C%2520Mistral%252C%2520and%2520Llama.%2520Application%250Aof%2520our%2520method%2520involves%2520taken%2520an%2520inverse%2520function%2520of%2520nonlinearity.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520that%2520can%2520not%2520be%2520done%2520analitically%2520and%2520instead%2520we%2520buid%2520an%250Aaccurate%2520approximations%2520using%2520simpler%2520functions.%2520Experimental%2520results%2520confirm%250Athat%2520our%2520method%2520significantly%2520reduces%2520memory%2520usage%2520without%2520affecting%2520training%250Aaccuracy.%2520The%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/PgLoLo/optiacts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverted%20Activations&entry.906535625=Georgii%20Novikov%20and%20Ivan%20Oseledets&entry.1292438233=%20%20The%20scaling%20of%20neural%20networks%20with%20increasing%20data%20and%20model%20sizes%0Anecessitates%20more%20efficient%20deep%20learning%20algorithms.%20This%20paper%20addresses%20the%0Amemory%20footprint%20challenge%20in%20neural%20network%20training%20by%20proposing%20a%0Amodification%20to%20the%20handling%20of%20activation%20tensors%20in%20pointwise%20nonlinearity%0Alayers.%20Traditionally%2C%20these%20layers%20save%20the%20entire%20input%20tensor%20for%20the%0Abackward%20pass%2C%20leading%20to%20substantial%20memory%20use.%20Our%20method%20involves%20saving%0Athe%20output%20tensor%20instead%2C%20reducing%20the%20memory%20required%20when%20the%20subsequent%0Alayer%20also%20saves%20its%20input%20tensor.%20This%20approach%20is%20particularly%20beneficial%20for%0Atransformer-based%20architectures%20like%20GPT%2C%20BERT%2C%20Mistral%2C%20and%20Llama.%20Application%0Aof%20our%20method%20involves%20taken%20an%20inverse%20function%20of%20nonlinearity.%20To%20the%20best%0Aof%20our%20knowledge%2C%20that%20can%20not%20be%20done%20analitically%20and%20instead%20we%20buid%20an%0Aaccurate%20approximations%20using%20simpler%20functions.%20Experimental%20results%20confirm%0Athat%20our%20method%20significantly%20reduces%20memory%20usage%20without%20affecting%20training%0Aaccuracy.%20The%20implementation%20is%20available%20at%0Ahttps%3A//github.com/PgLoLo/optiacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15545v1&entry.124074799=Read"},
{"title": "FSboard: Over 3 million characters of ASL fingerspelling collected via\n  smartphones", "author": "Manfred Georg and Garrett Tanzer and Saad Hassan and Maximus Shengelia and Esha Uboweja and Sam Sepah and Sean Forbes and Thad Starner", "abstract": "  Progress in machine understanding of sign languages has been slow and\nhampered by limited data. In this paper, we present FSboard, an American Sign\nLanguage fingerspelling dataset situated in a mobile text entry use case,\ncollected from 147 paid and consenting Deaf signers using Pixel 4A selfie\ncameras in a variety of environments. Fingerspelling recognition is an\nincomplete solution that is only one small part of sign language translation,\nbut it could provide some immediate benefit to Deaf/Hard of Hearing signers as\nmore broadly capable technology develops. At >3 million characters in length\nand >250 hours in duration, FSboard is the largest fingerspelling recognition\ndataset to date by a factor of >10x. As a simple baseline, we finetune 30 Hz\nMediaPipe Holistic landmark inputs into ByT5-Small and achieve 11.1% Character\nError Rate (CER) on a test set with unique phrases and signers. This quality\ndegrades gracefully when decreasing frame rate and excluding face/body\nlandmarks: plausible optimizations to help models run on device in real time.\n", "link": "http://arxiv.org/abs/2407.15806v1", "date": "2024-07-22", "relevancy": 2.0292, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4128}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4027}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FSboard%3A%20Over%203%20million%20characters%20of%20ASL%20fingerspelling%20collected%20via%0A%20%20smartphones&body=Title%3A%20FSboard%3A%20Over%203%20million%20characters%20of%20ASL%20fingerspelling%20collected%20via%0A%20%20smartphones%0AAuthor%3A%20Manfred%20Georg%20and%20Garrett%20Tanzer%20and%20Saad%20Hassan%20and%20Maximus%20Shengelia%20and%20Esha%20Uboweja%20and%20Sam%20Sepah%20and%20Sean%20Forbes%20and%20Thad%20Starner%0AAbstract%3A%20%20%20Progress%20in%20machine%20understanding%20of%20sign%20languages%20has%20been%20slow%20and%0Ahampered%20by%20limited%20data.%20In%20this%20paper%2C%20we%20present%20FSboard%2C%20an%20American%20Sign%0ALanguage%20fingerspelling%20dataset%20situated%20in%20a%20mobile%20text%20entry%20use%20case%2C%0Acollected%20from%20147%20paid%20and%20consenting%20Deaf%20signers%20using%20Pixel%204A%20selfie%0Acameras%20in%20a%20variety%20of%20environments.%20Fingerspelling%20recognition%20is%20an%0Aincomplete%20solution%20that%20is%20only%20one%20small%20part%20of%20sign%20language%20translation%2C%0Abut%20it%20could%20provide%20some%20immediate%20benefit%20to%20Deaf/Hard%20of%20Hearing%20signers%20as%0Amore%20broadly%20capable%20technology%20develops.%20At%20%3E3%20million%20characters%20in%20length%0Aand%20%3E250%20hours%20in%20duration%2C%20FSboard%20is%20the%20largest%20fingerspelling%20recognition%0Adataset%20to%20date%20by%20a%20factor%20of%20%3E10x.%20As%20a%20simple%20baseline%2C%20we%20finetune%2030%20Hz%0AMediaPipe%20Holistic%20landmark%20inputs%20into%20ByT5-Small%20and%20achieve%2011.1%25%20Character%0AError%20Rate%20%28CER%29%20on%20a%20test%20set%20with%20unique%20phrases%20and%20signers.%20This%20quality%0Adegrades%20gracefully%20when%20decreasing%20frame%20rate%20and%20excluding%20face/body%0Alandmarks%3A%20plausible%20optimizations%20to%20help%20models%20run%20on%20device%20in%20real%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFSboard%253A%2520Over%25203%2520million%2520characters%2520of%2520ASL%2520fingerspelling%2520collected%2520via%250A%2520%2520smartphones%26entry.906535625%3DManfred%2520Georg%2520and%2520Garrett%2520Tanzer%2520and%2520Saad%2520Hassan%2520and%2520Maximus%2520Shengelia%2520and%2520Esha%2520Uboweja%2520and%2520Sam%2520Sepah%2520and%2520Sean%2520Forbes%2520and%2520Thad%2520Starner%26entry.1292438233%3D%2520%2520Progress%2520in%2520machine%2520understanding%2520of%2520sign%2520languages%2520has%2520been%2520slow%2520and%250Ahampered%2520by%2520limited%2520data.%2520In%2520this%2520paper%252C%2520we%2520present%2520FSboard%252C%2520an%2520American%2520Sign%250ALanguage%2520fingerspelling%2520dataset%2520situated%2520in%2520a%2520mobile%2520text%2520entry%2520use%2520case%252C%250Acollected%2520from%2520147%2520paid%2520and%2520consenting%2520Deaf%2520signers%2520using%2520Pixel%25204A%2520selfie%250Acameras%2520in%2520a%2520variety%2520of%2520environments.%2520Fingerspelling%2520recognition%2520is%2520an%250Aincomplete%2520solution%2520that%2520is%2520only%2520one%2520small%2520part%2520of%2520sign%2520language%2520translation%252C%250Abut%2520it%2520could%2520provide%2520some%2520immediate%2520benefit%2520to%2520Deaf/Hard%2520of%2520Hearing%2520signers%2520as%250Amore%2520broadly%2520capable%2520technology%2520develops.%2520At%2520%253E3%2520million%2520characters%2520in%2520length%250Aand%2520%253E250%2520hours%2520in%2520duration%252C%2520FSboard%2520is%2520the%2520largest%2520fingerspelling%2520recognition%250Adataset%2520to%2520date%2520by%2520a%2520factor%2520of%2520%253E10x.%2520As%2520a%2520simple%2520baseline%252C%2520we%2520finetune%252030%2520Hz%250AMediaPipe%2520Holistic%2520landmark%2520inputs%2520into%2520ByT5-Small%2520and%2520achieve%252011.1%2525%2520Character%250AError%2520Rate%2520%2528CER%2529%2520on%2520a%2520test%2520set%2520with%2520unique%2520phrases%2520and%2520signers.%2520This%2520quality%250Adegrades%2520gracefully%2520when%2520decreasing%2520frame%2520rate%2520and%2520excluding%2520face/body%250Alandmarks%253A%2520plausible%2520optimizations%2520to%2520help%2520models%2520run%2520on%2520device%2520in%2520real%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FSboard%3A%20Over%203%20million%20characters%20of%20ASL%20fingerspelling%20collected%20via%0A%20%20smartphones&entry.906535625=Manfred%20Georg%20and%20Garrett%20Tanzer%20and%20Saad%20Hassan%20and%20Maximus%20Shengelia%20and%20Esha%20Uboweja%20and%20Sam%20Sepah%20and%20Sean%20Forbes%20and%20Thad%20Starner&entry.1292438233=%20%20Progress%20in%20machine%20understanding%20of%20sign%20languages%20has%20been%20slow%20and%0Ahampered%20by%20limited%20data.%20In%20this%20paper%2C%20we%20present%20FSboard%2C%20an%20American%20Sign%0ALanguage%20fingerspelling%20dataset%20situated%20in%20a%20mobile%20text%20entry%20use%20case%2C%0Acollected%20from%20147%20paid%20and%20consenting%20Deaf%20signers%20using%20Pixel%204A%20selfie%0Acameras%20in%20a%20variety%20of%20environments.%20Fingerspelling%20recognition%20is%20an%0Aincomplete%20solution%20that%20is%20only%20one%20small%20part%20of%20sign%20language%20translation%2C%0Abut%20it%20could%20provide%20some%20immediate%20benefit%20to%20Deaf/Hard%20of%20Hearing%20signers%20as%0Amore%20broadly%20capable%20technology%20develops.%20At%20%3E3%20million%20characters%20in%20length%0Aand%20%3E250%20hours%20in%20duration%2C%20FSboard%20is%20the%20largest%20fingerspelling%20recognition%0Adataset%20to%20date%20by%20a%20factor%20of%20%3E10x.%20As%20a%20simple%20baseline%2C%20we%20finetune%2030%20Hz%0AMediaPipe%20Holistic%20landmark%20inputs%20into%20ByT5-Small%20and%20achieve%2011.1%25%20Character%0AError%20Rate%20%28CER%29%20on%20a%20test%20set%20with%20unique%20phrases%20and%20signers.%20This%20quality%0Adegrades%20gracefully%20when%20decreasing%20frame%20rate%20and%20excluding%20face/body%0Alandmarks%3A%20plausible%20optimizations%20to%20help%20models%20run%20on%20device%20in%20real%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15806v1&entry.124074799=Read"},
{"title": "GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via\n  Generative Feature Extraction from MCI", "author": "Zhaojie Fang and Shenghao Zhu and Yifei Chen and Binfeng Zou and Fan Jia and Linwei Qiu and Chang Liu and Yiyu Huang and Xiang Feng and Feiwei Qin and Changmiao Wang and Yeru Wang and Jin Fan and Changbiao Chu and Wan-Zhen Wu and Hu Zhao", "abstract": "  Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder that\noften progresses from Mild Cognitive Impairment (MCI), leading to memory loss\nand significantly impacting patients' lives. Clinical trials indicate that\nearly targeted interventions for MCI patients can potentially slow or halt the\ndevelopment and progression of AD. Previous research has shown that accurate\nmedical classification requires the inclusion of extensive multimodal data,\nsuch as assessment scales and various neuroimaging techniques like Magnetic\nResonance Imaging (MRI) and Positron Emission Tomography (PET). However,\nconsistently tracking the diagnosis of the same individual over time and\nsimultaneously collecting multimodal data poses significant challenges. To\naddress this issue, we introduce GFE-Mamba, a classifier based on Generative\nFeature Extraction (GFE). This classifier effectively integrates data from\nassessment scales, MRI, and PET, enabling deeper multimodal fusion. It\nefficiently extracts both long and short sequence information and incorporates\nadditional information beyond the pixel space. This approach not only improves\nclassification accuracy but also enhances the interpretability and stability of\nthe model. We constructed datasets of over 3000 samples based on the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) for a two-step training\nprocess. Our experimental results demonstrate that the GFE-Mamba model is\neffective in predicting the conversion from MCI to AD and outperforms several\nstate-of-the-art methods. Our source code and ADNI dataset processing code are\navailable at https://github.com/Tinysqua/GFE-Mamba.\n", "link": "http://arxiv.org/abs/2407.15719v1", "date": "2024-07-22", "relevancy": 2.0292, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5224}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GFE-Mamba%3A%20Mamba-based%20AD%20Multi-modal%20Progression%20Assessment%20via%0A%20%20Generative%20Feature%20Extraction%20from%20MCI&body=Title%3A%20GFE-Mamba%3A%20Mamba-based%20AD%20Multi-modal%20Progression%20Assessment%20via%0A%20%20Generative%20Feature%20Extraction%20from%20MCI%0AAuthor%3A%20Zhaojie%20Fang%20and%20Shenghao%20Zhu%20and%20Yifei%20Chen%20and%20Binfeng%20Zou%20and%20Fan%20Jia%20and%20Linwei%20Qiu%20and%20Chang%20Liu%20and%20Yiyu%20Huang%20and%20Xiang%20Feng%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%20and%20Yeru%20Wang%20and%20Jin%20Fan%20and%20Changbiao%20Chu%20and%20Wan-Zhen%20Wu%20and%20Hu%20Zhao%0AAbstract%3A%20%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20an%20irreversible%20neurodegenerative%20disorder%20that%0Aoften%20progresses%20from%20Mild%20Cognitive%20Impairment%20%28MCI%29%2C%20leading%20to%20memory%20loss%0Aand%20significantly%20impacting%20patients%27%20lives.%20Clinical%20trials%20indicate%20that%0Aearly%20targeted%20interventions%20for%20MCI%20patients%20can%20potentially%20slow%20or%20halt%20the%0Adevelopment%20and%20progression%20of%20AD.%20Previous%20research%20has%20shown%20that%20accurate%0Amedical%20classification%20requires%20the%20inclusion%20of%20extensive%20multimodal%20data%2C%0Asuch%20as%20assessment%20scales%20and%20various%20neuroimaging%20techniques%20like%20Magnetic%0AResonance%20Imaging%20%28MRI%29%20and%20Positron%20Emission%20Tomography%20%28PET%29.%20However%2C%0Aconsistently%20tracking%20the%20diagnosis%20of%20the%20same%20individual%20over%20time%20and%0Asimultaneously%20collecting%20multimodal%20data%20poses%20significant%20challenges.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20GFE-Mamba%2C%20a%20classifier%20based%20on%20Generative%0AFeature%20Extraction%20%28GFE%29.%20This%20classifier%20effectively%20integrates%20data%20from%0Aassessment%20scales%2C%20MRI%2C%20and%20PET%2C%20enabling%20deeper%20multimodal%20fusion.%20It%0Aefficiently%20extracts%20both%20long%20and%20short%20sequence%20information%20and%20incorporates%0Aadditional%20information%20beyond%20the%20pixel%20space.%20This%20approach%20not%20only%20improves%0Aclassification%20accuracy%20but%20also%20enhances%20the%20interpretability%20and%20stability%20of%0Athe%20model.%20We%20constructed%20datasets%20of%20over%203000%20samples%20based%20on%20the%0AAlzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29%20for%20a%20two-step%20training%0Aprocess.%20Our%20experimental%20results%20demonstrate%20that%20the%20GFE-Mamba%20model%20is%0Aeffective%20in%20predicting%20the%20conversion%20from%20MCI%20to%20AD%20and%20outperforms%20several%0Astate-of-the-art%20methods.%20Our%20source%20code%20and%20ADNI%20dataset%20processing%20code%20are%0Aavailable%20at%20https%3A//github.com/Tinysqua/GFE-Mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGFE-Mamba%253A%2520Mamba-based%2520AD%2520Multi-modal%2520Progression%2520Assessment%2520via%250A%2520%2520Generative%2520Feature%2520Extraction%2520from%2520MCI%26entry.906535625%3DZhaojie%2520Fang%2520and%2520Shenghao%2520Zhu%2520and%2520Yifei%2520Chen%2520and%2520Binfeng%2520Zou%2520and%2520Fan%2520Jia%2520and%2520Linwei%2520Qiu%2520and%2520Chang%2520Liu%2520and%2520Yiyu%2520Huang%2520and%2520Xiang%2520Feng%2520and%2520Feiwei%2520Qin%2520and%2520Changmiao%2520Wang%2520and%2520Yeru%2520Wang%2520and%2520Jin%2520Fan%2520and%2520Changbiao%2520Chu%2520and%2520Wan-Zhen%2520Wu%2520and%2520Hu%2520Zhao%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520is%2520an%2520irreversible%2520neurodegenerative%2520disorder%2520that%250Aoften%2520progresses%2520from%2520Mild%2520Cognitive%2520Impairment%2520%2528MCI%2529%252C%2520leading%2520to%2520memory%2520loss%250Aand%2520significantly%2520impacting%2520patients%2527%2520lives.%2520Clinical%2520trials%2520indicate%2520that%250Aearly%2520targeted%2520interventions%2520for%2520MCI%2520patients%2520can%2520potentially%2520slow%2520or%2520halt%2520the%250Adevelopment%2520and%2520progression%2520of%2520AD.%2520Previous%2520research%2520has%2520shown%2520that%2520accurate%250Amedical%2520classification%2520requires%2520the%2520inclusion%2520of%2520extensive%2520multimodal%2520data%252C%250Asuch%2520as%2520assessment%2520scales%2520and%2520various%2520neuroimaging%2520techniques%2520like%2520Magnetic%250AResonance%2520Imaging%2520%2528MRI%2529%2520and%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529.%2520However%252C%250Aconsistently%2520tracking%2520the%2520diagnosis%2520of%2520the%2520same%2520individual%2520over%2520time%2520and%250Asimultaneously%2520collecting%2520multimodal%2520data%2520poses%2520significant%2520challenges.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520introduce%2520GFE-Mamba%252C%2520a%2520classifier%2520based%2520on%2520Generative%250AFeature%2520Extraction%2520%2528GFE%2529.%2520This%2520classifier%2520effectively%2520integrates%2520data%2520from%250Aassessment%2520scales%252C%2520MRI%252C%2520and%2520PET%252C%2520enabling%2520deeper%2520multimodal%2520fusion.%2520It%250Aefficiently%2520extracts%2520both%2520long%2520and%2520short%2520sequence%2520information%2520and%2520incorporates%250Aadditional%2520information%2520beyond%2520the%2520pixel%2520space.%2520This%2520approach%2520not%2520only%2520improves%250Aclassification%2520accuracy%2520but%2520also%2520enhances%2520the%2520interpretability%2520and%2520stability%2520of%250Athe%2520model.%2520We%2520constructed%2520datasets%2520of%2520over%25203000%2520samples%2520based%2520on%2520the%250AAlzheimer%2527s%2520Disease%2520Neuroimaging%2520Initiative%2520%2528ADNI%2529%2520for%2520a%2520two-step%2520training%250Aprocess.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520the%2520GFE-Mamba%2520model%2520is%250Aeffective%2520in%2520predicting%2520the%2520conversion%2520from%2520MCI%2520to%2520AD%2520and%2520outperforms%2520several%250Astate-of-the-art%2520methods.%2520Our%2520source%2520code%2520and%2520ADNI%2520dataset%2520processing%2520code%2520are%250Aavailable%2520at%2520https%253A//github.com/Tinysqua/GFE-Mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GFE-Mamba%3A%20Mamba-based%20AD%20Multi-modal%20Progression%20Assessment%20via%0A%20%20Generative%20Feature%20Extraction%20from%20MCI&entry.906535625=Zhaojie%20Fang%20and%20Shenghao%20Zhu%20and%20Yifei%20Chen%20and%20Binfeng%20Zou%20and%20Fan%20Jia%20and%20Linwei%20Qiu%20and%20Chang%20Liu%20and%20Yiyu%20Huang%20and%20Xiang%20Feng%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%20and%20Yeru%20Wang%20and%20Jin%20Fan%20and%20Changbiao%20Chu%20and%20Wan-Zhen%20Wu%20and%20Hu%20Zhao&entry.1292438233=%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20an%20irreversible%20neurodegenerative%20disorder%20that%0Aoften%20progresses%20from%20Mild%20Cognitive%20Impairment%20%28MCI%29%2C%20leading%20to%20memory%20loss%0Aand%20significantly%20impacting%20patients%27%20lives.%20Clinical%20trials%20indicate%20that%0Aearly%20targeted%20interventions%20for%20MCI%20patients%20can%20potentially%20slow%20or%20halt%20the%0Adevelopment%20and%20progression%20of%20AD.%20Previous%20research%20has%20shown%20that%20accurate%0Amedical%20classification%20requires%20the%20inclusion%20of%20extensive%20multimodal%20data%2C%0Asuch%20as%20assessment%20scales%20and%20various%20neuroimaging%20techniques%20like%20Magnetic%0AResonance%20Imaging%20%28MRI%29%20and%20Positron%20Emission%20Tomography%20%28PET%29.%20However%2C%0Aconsistently%20tracking%20the%20diagnosis%20of%20the%20same%20individual%20over%20time%20and%0Asimultaneously%20collecting%20multimodal%20data%20poses%20significant%20challenges.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20GFE-Mamba%2C%20a%20classifier%20based%20on%20Generative%0AFeature%20Extraction%20%28GFE%29.%20This%20classifier%20effectively%20integrates%20data%20from%0Aassessment%20scales%2C%20MRI%2C%20and%20PET%2C%20enabling%20deeper%20multimodal%20fusion.%20It%0Aefficiently%20extracts%20both%20long%20and%20short%20sequence%20information%20and%20incorporates%0Aadditional%20information%20beyond%20the%20pixel%20space.%20This%20approach%20not%20only%20improves%0Aclassification%20accuracy%20but%20also%20enhances%20the%20interpretability%20and%20stability%20of%0Athe%20model.%20We%20constructed%20datasets%20of%20over%203000%20samples%20based%20on%20the%0AAlzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29%20for%20a%20two-step%20training%0Aprocess.%20Our%20experimental%20results%20demonstrate%20that%20the%20GFE-Mamba%20model%20is%0Aeffective%20in%20predicting%20the%20conversion%20from%20MCI%20to%20AD%20and%20outperforms%20several%0Astate-of-the-art%20methods.%20Our%20source%20code%20and%20ADNI%20dataset%20processing%20code%20are%0Aavailable%20at%20https%3A//github.com/Tinysqua/GFE-Mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15719v1&entry.124074799=Read"},
{"title": "SoftCVI: contrastive variational inference with self-generated soft\n  labels", "author": "Daniel Ward and Mark Beaumont and Matteo Fasiolo", "abstract": "  Estimating a distribution given access to its unnormalized density is pivotal\nin Bayesian inference, where the posterior is generally known only up to an\nunknown normalizing constant. Variational inference and Markov chain Monte\nCarlo methods are the predominant tools for this task; however, both methods\nare often challenging to apply reliably, particularly when the posterior has\ncomplex geometry. Here, we introduce Soft Contrastive Variational Inference\n(SoftCVI), which allows a family of variational objectives to be derived\nthrough a contrastive estimation framework. These objectives have zero variance\ngradient when the variational approximation is exact, without the need for\nspecialized gradient estimators. The approach involves parameterizing a\nclassifier in terms of the variational distribution, which allows the inference\ntask to be reframed as a contrastive estimation problem, aiming to identify a\nsingle true posterior sample among a set of samples. Despite this framing, we\ndo not require positive or negative samples, but rather learn by sampling the\nvariational distribution and computing ground truth soft classification labels\nfrom the unnormalized posterior itself. We empirically investigate the\nperformance on a variety of Bayesian inference tasks, using both using both\nsimple (e.g. normal) and expressive (normalizing flow) variational\ndistributions. We find that SoftCVI objectives often outperform other commonly\nused variational objectives.\n", "link": "http://arxiv.org/abs/2407.15687v1", "date": "2024-07-22", "relevancy": 2.0256, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5126}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoftCVI%3A%20contrastive%20variational%20inference%20with%20self-generated%20soft%0A%20%20labels&body=Title%3A%20SoftCVI%3A%20contrastive%20variational%20inference%20with%20self-generated%20soft%0A%20%20labels%0AAuthor%3A%20Daniel%20Ward%20and%20Mark%20Beaumont%20and%20Matteo%20Fasiolo%0AAbstract%3A%20%20%20Estimating%20a%20distribution%20given%20access%20to%20its%20unnormalized%20density%20is%20pivotal%0Ain%20Bayesian%20inference%2C%20where%20the%20posterior%20is%20generally%20known%20only%20up%20to%20an%0Aunknown%20normalizing%20constant.%20Variational%20inference%20and%20Markov%20chain%20Monte%0ACarlo%20methods%20are%20the%20predominant%20tools%20for%20this%20task%3B%20however%2C%20both%20methods%0Aare%20often%20challenging%20to%20apply%20reliably%2C%20particularly%20when%20the%20posterior%20has%0Acomplex%20geometry.%20Here%2C%20we%20introduce%20Soft%20Contrastive%20Variational%20Inference%0A%28SoftCVI%29%2C%20which%20allows%20a%20family%20of%20variational%20objectives%20to%20be%20derived%0Athrough%20a%20contrastive%20estimation%20framework.%20These%20objectives%20have%20zero%20variance%0Agradient%20when%20the%20variational%20approximation%20is%20exact%2C%20without%20the%20need%20for%0Aspecialized%20gradient%20estimators.%20The%20approach%20involves%20parameterizing%20a%0Aclassifier%20in%20terms%20of%20the%20variational%20distribution%2C%20which%20allows%20the%20inference%0Atask%20to%20be%20reframed%20as%20a%20contrastive%20estimation%20problem%2C%20aiming%20to%20identify%20a%0Asingle%20true%20posterior%20sample%20among%20a%20set%20of%20samples.%20Despite%20this%20framing%2C%20we%0Ado%20not%20require%20positive%20or%20negative%20samples%2C%20but%20rather%20learn%20by%20sampling%20the%0Avariational%20distribution%20and%20computing%20ground%20truth%20soft%20classification%20labels%0Afrom%20the%20unnormalized%20posterior%20itself.%20We%20empirically%20investigate%20the%0Aperformance%20on%20a%20variety%20of%20Bayesian%20inference%20tasks%2C%20using%20both%20using%20both%0Asimple%20%28e.g.%20normal%29%20and%20expressive%20%28normalizing%20flow%29%20variational%0Adistributions.%20We%20find%20that%20SoftCVI%20objectives%20often%20outperform%20other%20commonly%0Aused%20variational%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftCVI%253A%2520contrastive%2520variational%2520inference%2520with%2520self-generated%2520soft%250A%2520%2520labels%26entry.906535625%3DDaniel%2520Ward%2520and%2520Mark%2520Beaumont%2520and%2520Matteo%2520Fasiolo%26entry.1292438233%3D%2520%2520Estimating%2520a%2520distribution%2520given%2520access%2520to%2520its%2520unnormalized%2520density%2520is%2520pivotal%250Ain%2520Bayesian%2520inference%252C%2520where%2520the%2520posterior%2520is%2520generally%2520known%2520only%2520up%2520to%2520an%250Aunknown%2520normalizing%2520constant.%2520Variational%2520inference%2520and%2520Markov%2520chain%2520Monte%250ACarlo%2520methods%2520are%2520the%2520predominant%2520tools%2520for%2520this%2520task%253B%2520however%252C%2520both%2520methods%250Aare%2520often%2520challenging%2520to%2520apply%2520reliably%252C%2520particularly%2520when%2520the%2520posterior%2520has%250Acomplex%2520geometry.%2520Here%252C%2520we%2520introduce%2520Soft%2520Contrastive%2520Variational%2520Inference%250A%2528SoftCVI%2529%252C%2520which%2520allows%2520a%2520family%2520of%2520variational%2520objectives%2520to%2520be%2520derived%250Athrough%2520a%2520contrastive%2520estimation%2520framework.%2520These%2520objectives%2520have%2520zero%2520variance%250Agradient%2520when%2520the%2520variational%2520approximation%2520is%2520exact%252C%2520without%2520the%2520need%2520for%250Aspecialized%2520gradient%2520estimators.%2520The%2520approach%2520involves%2520parameterizing%2520a%250Aclassifier%2520in%2520terms%2520of%2520the%2520variational%2520distribution%252C%2520which%2520allows%2520the%2520inference%250Atask%2520to%2520be%2520reframed%2520as%2520a%2520contrastive%2520estimation%2520problem%252C%2520aiming%2520to%2520identify%2520a%250Asingle%2520true%2520posterior%2520sample%2520among%2520a%2520set%2520of%2520samples.%2520Despite%2520this%2520framing%252C%2520we%250Ado%2520not%2520require%2520positive%2520or%2520negative%2520samples%252C%2520but%2520rather%2520learn%2520by%2520sampling%2520the%250Avariational%2520distribution%2520and%2520computing%2520ground%2520truth%2520soft%2520classification%2520labels%250Afrom%2520the%2520unnormalized%2520posterior%2520itself.%2520We%2520empirically%2520investigate%2520the%250Aperformance%2520on%2520a%2520variety%2520of%2520Bayesian%2520inference%2520tasks%252C%2520using%2520both%2520using%2520both%250Asimple%2520%2528e.g.%2520normal%2529%2520and%2520expressive%2520%2528normalizing%2520flow%2529%2520variational%250Adistributions.%2520We%2520find%2520that%2520SoftCVI%2520objectives%2520often%2520outperform%2520other%2520commonly%250Aused%2520variational%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoftCVI%3A%20contrastive%20variational%20inference%20with%20self-generated%20soft%0A%20%20labels&entry.906535625=Daniel%20Ward%20and%20Mark%20Beaumont%20and%20Matteo%20Fasiolo&entry.1292438233=%20%20Estimating%20a%20distribution%20given%20access%20to%20its%20unnormalized%20density%20is%20pivotal%0Ain%20Bayesian%20inference%2C%20where%20the%20posterior%20is%20generally%20known%20only%20up%20to%20an%0Aunknown%20normalizing%20constant.%20Variational%20inference%20and%20Markov%20chain%20Monte%0ACarlo%20methods%20are%20the%20predominant%20tools%20for%20this%20task%3B%20however%2C%20both%20methods%0Aare%20often%20challenging%20to%20apply%20reliably%2C%20particularly%20when%20the%20posterior%20has%0Acomplex%20geometry.%20Here%2C%20we%20introduce%20Soft%20Contrastive%20Variational%20Inference%0A%28SoftCVI%29%2C%20which%20allows%20a%20family%20of%20variational%20objectives%20to%20be%20derived%0Athrough%20a%20contrastive%20estimation%20framework.%20These%20objectives%20have%20zero%20variance%0Agradient%20when%20the%20variational%20approximation%20is%20exact%2C%20without%20the%20need%20for%0Aspecialized%20gradient%20estimators.%20The%20approach%20involves%20parameterizing%20a%0Aclassifier%20in%20terms%20of%20the%20variational%20distribution%2C%20which%20allows%20the%20inference%0Atask%20to%20be%20reframed%20as%20a%20contrastive%20estimation%20problem%2C%20aiming%20to%20identify%20a%0Asingle%20true%20posterior%20sample%20among%20a%20set%20of%20samples.%20Despite%20this%20framing%2C%20we%0Ado%20not%20require%20positive%20or%20negative%20samples%2C%20but%20rather%20learn%20by%20sampling%20the%0Avariational%20distribution%20and%20computing%20ground%20truth%20soft%20classification%20labels%0Afrom%20the%20unnormalized%20posterior%20itself.%20We%20empirically%20investigate%20the%0Aperformance%20on%20a%20variety%20of%20Bayesian%20inference%20tasks%2C%20using%20both%20using%20both%0Asimple%20%28e.g.%20normal%29%20and%20expressive%20%28normalizing%20flow%29%20variational%0Adistributions.%20We%20find%20that%20SoftCVI%20objectives%20often%20outperform%20other%20commonly%0Aused%20variational%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15687v1&entry.124074799=Read"},
{"title": "Link Polarity Prediction from Sparse and Noisy Labels via Multiscale\n  Social Balance", "author": "Marco Minici and Federico Cinus and Francesco Bonchi and Giuseppe Manco", "abstract": "  Signed Graph Neural Networks (SGNNs) have recently gained attention as an\neffective tool for several learning tasks on signed networks, i.e., graphs\nwhere edges have an associated polarity. One of these tasks is to predict the\npolarity of the links for which this information is missing, starting from the\nnetwork structure and the other available polarities. However, when the\navailable polarities are few and potentially noisy, such a task becomes\nchallenging.\n  In this work, we devise a semi-supervised learning framework that builds\naround the novel concept of \\emph{multiscale social balance} to improve the\nprediction of link polarities in settings characterized by limited data\nquantity and quality. Our model-agnostic approach can seamlessly integrate with\nany SGNN architecture, dynamically reweighting the importance of each data\nsample while making strategic use of the structural information from unlabeled\nedges combined with social balance theory.\n  Empirical validation demonstrates that our approach outperforms established\nbaseline models, effectively addressing the limitations imposed by noisy and\nsparse data. This result underlines the benefits of incorporating multiscale\nsocial balance into SGNNs, opening new avenues for robust and accurate\npredictions in signed network analysis.\n", "link": "http://arxiv.org/abs/2407.15643v1", "date": "2024-07-22", "relevancy": 2.0168, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5157}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5042}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Link%20Polarity%20Prediction%20from%20Sparse%20and%20Noisy%20Labels%20via%20Multiscale%0A%20%20Social%20Balance&body=Title%3A%20Link%20Polarity%20Prediction%20from%20Sparse%20and%20Noisy%20Labels%20via%20Multiscale%0A%20%20Social%20Balance%0AAuthor%3A%20Marco%20Minici%20and%20Federico%20Cinus%20and%20Francesco%20Bonchi%20and%20Giuseppe%20Manco%0AAbstract%3A%20%20%20Signed%20Graph%20Neural%20Networks%20%28SGNNs%29%20have%20recently%20gained%20attention%20as%20an%0Aeffective%20tool%20for%20several%20learning%20tasks%20on%20signed%20networks%2C%20i.e.%2C%20graphs%0Awhere%20edges%20have%20an%20associated%20polarity.%20One%20of%20these%20tasks%20is%20to%20predict%20the%0Apolarity%20of%20the%20links%20for%20which%20this%20information%20is%20missing%2C%20starting%20from%20the%0Anetwork%20structure%20and%20the%20other%20available%20polarities.%20However%2C%20when%20the%0Aavailable%20polarities%20are%20few%20and%20potentially%20noisy%2C%20such%20a%20task%20becomes%0Achallenging.%0A%20%20In%20this%20work%2C%20we%20devise%20a%20semi-supervised%20learning%20framework%20that%20builds%0Aaround%20the%20novel%20concept%20of%20%5Cemph%7Bmultiscale%20social%20balance%7D%20to%20improve%20the%0Aprediction%20of%20link%20polarities%20in%20settings%20characterized%20by%20limited%20data%0Aquantity%20and%20quality.%20Our%20model-agnostic%20approach%20can%20seamlessly%20integrate%20with%0Aany%20SGNN%20architecture%2C%20dynamically%20reweighting%20the%20importance%20of%20each%20data%0Asample%20while%20making%20strategic%20use%20of%20the%20structural%20information%20from%20unlabeled%0Aedges%20combined%20with%20social%20balance%20theory.%0A%20%20Empirical%20validation%20demonstrates%20that%20our%20approach%20outperforms%20established%0Abaseline%20models%2C%20effectively%20addressing%20the%20limitations%20imposed%20by%20noisy%20and%0Asparse%20data.%20This%20result%20underlines%20the%20benefits%20of%20incorporating%20multiscale%0Asocial%20balance%20into%20SGNNs%2C%20opening%20new%20avenues%20for%20robust%20and%20accurate%0Apredictions%20in%20signed%20network%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLink%2520Polarity%2520Prediction%2520from%2520Sparse%2520and%2520Noisy%2520Labels%2520via%2520Multiscale%250A%2520%2520Social%2520Balance%26entry.906535625%3DMarco%2520Minici%2520and%2520Federico%2520Cinus%2520and%2520Francesco%2520Bonchi%2520and%2520Giuseppe%2520Manco%26entry.1292438233%3D%2520%2520Signed%2520Graph%2520Neural%2520Networks%2520%2528SGNNs%2529%2520have%2520recently%2520gained%2520attention%2520as%2520an%250Aeffective%2520tool%2520for%2520several%2520learning%2520tasks%2520on%2520signed%2520networks%252C%2520i.e.%252C%2520graphs%250Awhere%2520edges%2520have%2520an%2520associated%2520polarity.%2520One%2520of%2520these%2520tasks%2520is%2520to%2520predict%2520the%250Apolarity%2520of%2520the%2520links%2520for%2520which%2520this%2520information%2520is%2520missing%252C%2520starting%2520from%2520the%250Anetwork%2520structure%2520and%2520the%2520other%2520available%2520polarities.%2520However%252C%2520when%2520the%250Aavailable%2520polarities%2520are%2520few%2520and%2520potentially%2520noisy%252C%2520such%2520a%2520task%2520becomes%250Achallenging.%250A%2520%2520In%2520this%2520work%252C%2520we%2520devise%2520a%2520semi-supervised%2520learning%2520framework%2520that%2520builds%250Aaround%2520the%2520novel%2520concept%2520of%2520%255Cemph%257Bmultiscale%2520social%2520balance%257D%2520to%2520improve%2520the%250Aprediction%2520of%2520link%2520polarities%2520in%2520settings%2520characterized%2520by%2520limited%2520data%250Aquantity%2520and%2520quality.%2520Our%2520model-agnostic%2520approach%2520can%2520seamlessly%2520integrate%2520with%250Aany%2520SGNN%2520architecture%252C%2520dynamically%2520reweighting%2520the%2520importance%2520of%2520each%2520data%250Asample%2520while%2520making%2520strategic%2520use%2520of%2520the%2520structural%2520information%2520from%2520unlabeled%250Aedges%2520combined%2520with%2520social%2520balance%2520theory.%250A%2520%2520Empirical%2520validation%2520demonstrates%2520that%2520our%2520approach%2520outperforms%2520established%250Abaseline%2520models%252C%2520effectively%2520addressing%2520the%2520limitations%2520imposed%2520by%2520noisy%2520and%250Asparse%2520data.%2520This%2520result%2520underlines%2520the%2520benefits%2520of%2520incorporating%2520multiscale%250Asocial%2520balance%2520into%2520SGNNs%252C%2520opening%2520new%2520avenues%2520for%2520robust%2520and%2520accurate%250Apredictions%2520in%2520signed%2520network%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Link%20Polarity%20Prediction%20from%20Sparse%20and%20Noisy%20Labels%20via%20Multiscale%0A%20%20Social%20Balance&entry.906535625=Marco%20Minici%20and%20Federico%20Cinus%20and%20Francesco%20Bonchi%20and%20Giuseppe%20Manco&entry.1292438233=%20%20Signed%20Graph%20Neural%20Networks%20%28SGNNs%29%20have%20recently%20gained%20attention%20as%20an%0Aeffective%20tool%20for%20several%20learning%20tasks%20on%20signed%20networks%2C%20i.e.%2C%20graphs%0Awhere%20edges%20have%20an%20associated%20polarity.%20One%20of%20these%20tasks%20is%20to%20predict%20the%0Apolarity%20of%20the%20links%20for%20which%20this%20information%20is%20missing%2C%20starting%20from%20the%0Anetwork%20structure%20and%20the%20other%20available%20polarities.%20However%2C%20when%20the%0Aavailable%20polarities%20are%20few%20and%20potentially%20noisy%2C%20such%20a%20task%20becomes%0Achallenging.%0A%20%20In%20this%20work%2C%20we%20devise%20a%20semi-supervised%20learning%20framework%20that%20builds%0Aaround%20the%20novel%20concept%20of%20%5Cemph%7Bmultiscale%20social%20balance%7D%20to%20improve%20the%0Aprediction%20of%20link%20polarities%20in%20settings%20characterized%20by%20limited%20data%0Aquantity%20and%20quality.%20Our%20model-agnostic%20approach%20can%20seamlessly%20integrate%20with%0Aany%20SGNN%20architecture%2C%20dynamically%20reweighting%20the%20importance%20of%20each%20data%0Asample%20while%20making%20strategic%20use%20of%20the%20structural%20information%20from%20unlabeled%0Aedges%20combined%20with%20social%20balance%20theory.%0A%20%20Empirical%20validation%20demonstrates%20that%20our%20approach%20outperforms%20established%0Abaseline%20models%2C%20effectively%20addressing%20the%20limitations%20imposed%20by%20noisy%20and%0Asparse%20data.%20This%20result%20underlines%20the%20benefits%20of%20incorporating%20multiscale%0Asocial%20balance%20into%20SGNNs%2C%20opening%20new%20avenues%20for%20robust%20and%20accurate%0Apredictions%20in%20signed%20network%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15643v1&entry.124074799=Read"},
{"title": "A Closer Look at GAN Priors: Exploiting Intermediate Features for\n  Enhanced Model Inversion Attacks", "author": "Yixiang Qiu and Hao Fang and Hongyao Yu and Bin Chen and MeiKang Qiu and Shu-Tao Xia", "abstract": "  Model Inversion (MI) attacks aim to reconstruct privacy-sensitive training\ndata from released models by utilizing output information, raising extensive\nconcerns about the security of Deep Neural Networks (DNNs). Recent advances in\ngenerative adversarial networks (GANs) have contributed significantly to the\nimproved performance of MI attacks due to their powerful ability to generate\nrealistic images with high fidelity and appropriate semantics. However,\nprevious MI attacks have solely disclosed private information in the latent\nspace of GAN priors, limiting their semantic extraction and transferability\nacross multiple target models and datasets. To address this challenge, we\npropose a novel method, Intermediate Features enhanced Generative Model\nInversion (IF-GMI), which disassembles the GAN structure and exploits features\nbetween intermediate blocks. This allows us to extend the optimization space\nfrom latent code to intermediate features with enhanced expressive\ncapabilities. To prevent GAN priors from generating unrealistic images, we\napply a L1 ball constraint to the optimization process. Experiments on multiple\nbenchmarks demonstrate that our method significantly outperforms previous\napproaches and achieves state-of-the-art results under various settings,\nespecially in the out-of-distribution (OOD) scenario. Our code is available at:\nhttps://github.com/final-solution/IF-GMI\n", "link": "http://arxiv.org/abs/2407.13863v2", "date": "2024-07-22", "relevancy": 2.0121, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5079}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5024}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Closer%20Look%20at%20GAN%20Priors%3A%20Exploiting%20Intermediate%20Features%20for%0A%20%20Enhanced%20Model%20Inversion%20Attacks&body=Title%3A%20A%20Closer%20Look%20at%20GAN%20Priors%3A%20Exploiting%20Intermediate%20Features%20for%0A%20%20Enhanced%20Model%20Inversion%20Attacks%0AAuthor%3A%20Yixiang%20Qiu%20and%20Hao%20Fang%20and%20Hongyao%20Yu%20and%20Bin%20Chen%20and%20MeiKang%20Qiu%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Model%20Inversion%20%28MI%29%20attacks%20aim%20to%20reconstruct%20privacy-sensitive%20training%0Adata%20from%20released%20models%20by%20utilizing%20output%20information%2C%20raising%20extensive%0Aconcerns%20about%20the%20security%20of%20Deep%20Neural%20Networks%20%28DNNs%29.%20Recent%20advances%20in%0Agenerative%20adversarial%20networks%20%28GANs%29%20have%20contributed%20significantly%20to%20the%0Aimproved%20performance%20of%20MI%20attacks%20due%20to%20their%20powerful%20ability%20to%20generate%0Arealistic%20images%20with%20high%20fidelity%20and%20appropriate%20semantics.%20However%2C%0Aprevious%20MI%20attacks%20have%20solely%20disclosed%20private%20information%20in%20the%20latent%0Aspace%20of%20GAN%20priors%2C%20limiting%20their%20semantic%20extraction%20and%20transferability%0Aacross%20multiple%20target%20models%20and%20datasets.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20novel%20method%2C%20Intermediate%20Features%20enhanced%20Generative%20Model%0AInversion%20%28IF-GMI%29%2C%20which%20disassembles%20the%20GAN%20structure%20and%20exploits%20features%0Abetween%20intermediate%20blocks.%20This%20allows%20us%20to%20extend%20the%20optimization%20space%0Afrom%20latent%20code%20to%20intermediate%20features%20with%20enhanced%20expressive%0Acapabilities.%20To%20prevent%20GAN%20priors%20from%20generating%20unrealistic%20images%2C%20we%0Aapply%20a%20L1%20ball%20constraint%20to%20the%20optimization%20process.%20Experiments%20on%20multiple%0Abenchmarks%20demonstrate%20that%20our%20method%20significantly%20outperforms%20previous%0Aapproaches%20and%20achieves%20state-of-the-art%20results%20under%20various%20settings%2C%0Aespecially%20in%20the%20out-of-distribution%20%28OOD%29%20scenario.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/final-solution/IF-GMI%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Closer%2520Look%2520at%2520GAN%2520Priors%253A%2520Exploiting%2520Intermediate%2520Features%2520for%250A%2520%2520Enhanced%2520Model%2520Inversion%2520Attacks%26entry.906535625%3DYixiang%2520Qiu%2520and%2520Hao%2520Fang%2520and%2520Hongyao%2520Yu%2520and%2520Bin%2520Chen%2520and%2520MeiKang%2520Qiu%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Model%2520Inversion%2520%2528MI%2529%2520attacks%2520aim%2520to%2520reconstruct%2520privacy-sensitive%2520training%250Adata%2520from%2520released%2520models%2520by%2520utilizing%2520output%2520information%252C%2520raising%2520extensive%250Aconcerns%2520about%2520the%2520security%2520of%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529.%2520Recent%2520advances%2520in%250Agenerative%2520adversarial%2520networks%2520%2528GANs%2529%2520have%2520contributed%2520significantly%2520to%2520the%250Aimproved%2520performance%2520of%2520MI%2520attacks%2520due%2520to%2520their%2520powerful%2520ability%2520to%2520generate%250Arealistic%2520images%2520with%2520high%2520fidelity%2520and%2520appropriate%2520semantics.%2520However%252C%250Aprevious%2520MI%2520attacks%2520have%2520solely%2520disclosed%2520private%2520information%2520in%2520the%2520latent%250Aspace%2520of%2520GAN%2520priors%252C%2520limiting%2520their%2520semantic%2520extraction%2520and%2520transferability%250Aacross%2520multiple%2520target%2520models%2520and%2520datasets.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520novel%2520method%252C%2520Intermediate%2520Features%2520enhanced%2520Generative%2520Model%250AInversion%2520%2528IF-GMI%2529%252C%2520which%2520disassembles%2520the%2520GAN%2520structure%2520and%2520exploits%2520features%250Abetween%2520intermediate%2520blocks.%2520This%2520allows%2520us%2520to%2520extend%2520the%2520optimization%2520space%250Afrom%2520latent%2520code%2520to%2520intermediate%2520features%2520with%2520enhanced%2520expressive%250Acapabilities.%2520To%2520prevent%2520GAN%2520priors%2520from%2520generating%2520unrealistic%2520images%252C%2520we%250Aapply%2520a%2520L1%2520ball%2520constraint%2520to%2520the%2520optimization%2520process.%2520Experiments%2520on%2520multiple%250Abenchmarks%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520previous%250Aapproaches%2520and%2520achieves%2520state-of-the-art%2520results%2520under%2520various%2520settings%252C%250Aespecially%2520in%2520the%2520out-of-distribution%2520%2528OOD%2529%2520scenario.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/final-solution/IF-GMI%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20GAN%20Priors%3A%20Exploiting%20Intermediate%20Features%20for%0A%20%20Enhanced%20Model%20Inversion%20Attacks&entry.906535625=Yixiang%20Qiu%20and%20Hao%20Fang%20and%20Hongyao%20Yu%20and%20Bin%20Chen%20and%20MeiKang%20Qiu%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Model%20Inversion%20%28MI%29%20attacks%20aim%20to%20reconstruct%20privacy-sensitive%20training%0Adata%20from%20released%20models%20by%20utilizing%20output%20information%2C%20raising%20extensive%0Aconcerns%20about%20the%20security%20of%20Deep%20Neural%20Networks%20%28DNNs%29.%20Recent%20advances%20in%0Agenerative%20adversarial%20networks%20%28GANs%29%20have%20contributed%20significantly%20to%20the%0Aimproved%20performance%20of%20MI%20attacks%20due%20to%20their%20powerful%20ability%20to%20generate%0Arealistic%20images%20with%20high%20fidelity%20and%20appropriate%20semantics.%20However%2C%0Aprevious%20MI%20attacks%20have%20solely%20disclosed%20private%20information%20in%20the%20latent%0Aspace%20of%20GAN%20priors%2C%20limiting%20their%20semantic%20extraction%20and%20transferability%0Aacross%20multiple%20target%20models%20and%20datasets.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20novel%20method%2C%20Intermediate%20Features%20enhanced%20Generative%20Model%0AInversion%20%28IF-GMI%29%2C%20which%20disassembles%20the%20GAN%20structure%20and%20exploits%20features%0Abetween%20intermediate%20blocks.%20This%20allows%20us%20to%20extend%20the%20optimization%20space%0Afrom%20latent%20code%20to%20intermediate%20features%20with%20enhanced%20expressive%0Acapabilities.%20To%20prevent%20GAN%20priors%20from%20generating%20unrealistic%20images%2C%20we%0Aapply%20a%20L1%20ball%20constraint%20to%20the%20optimization%20process.%20Experiments%20on%20multiple%0Abenchmarks%20demonstrate%20that%20our%20method%20significantly%20outperforms%20previous%0Aapproaches%20and%20achieves%20state-of-the-art%20results%20under%20various%20settings%2C%0Aespecially%20in%20the%20out-of-distribution%20%28OOD%29%20scenario.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/final-solution/IF-GMI%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13863v2&entry.124074799=Read"},
{"title": "Beyond Size and Class Balance: Alpha as a New Dataset Quality Metric for\n  Deep Learning", "author": "Josiah Couch and Ramy Arnaout and Rima Arnaout", "abstract": "  In deep learning, achieving high performance on image classification tasks\nrequires diverse training sets. However, dataset diversity is incompletely\nunderstood. The current best practice is to try to maximize dataset size and\nclass balance. Yet large, class-balanced datasets are not guaranteed to be\ndiverse: images can still be arbitrarily similar. We hypothesized that, for a\ngiven model architecture, better model performance can be achieved by\nmaximizing dataset diversity more directly. This could open a path for\nperformance improvement without additional computational resources or\narchitectural advances. To test this hypothesis, we introduce a comprehensive\nframework of diversity measures, developed in ecology, that generalizes\nfamiliar quantities like Shannon entropy by accounting for similarities and\ndifferences among images. (Dataset size and class balance emerge from this\nframework as special cases.) By analyzing thousands of subsets from seven\nmedical datasets representing ultrasound, X-ray, CT, and pathology images, we\nfound that the best correlates of performance were not size or class balance\nbut $A$ -- ``big alpha'' -- a set of generalized entropy measures interpreted\nas the effective number of image-class pairs in the dataset, after accounting\nfor similarities among images. One of these, $A_0$, explained 67\\% of the\nvariance in balanced accuracy across all subsets, vs. 54\\% for class balance\nand just 39\\% for size. The best pair was size and $A_1$ (79\\%), which\noutperformed size and class balance (74\\%). $A$ performed best for subsets from\nindividual datasets as well as across datasets, supporting the generality of\nthese results. We propose maximizing $A$ as a potential new way to improve the\nperformance of deep learning in medical imaging.\n", "link": "http://arxiv.org/abs/2407.15724v1", "date": "2024-07-22", "relevancy": 2.002, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5124}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4951}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Size%20and%20Class%20Balance%3A%20Alpha%20as%20a%20New%20Dataset%20Quality%20Metric%20for%0A%20%20Deep%20Learning&body=Title%3A%20Beyond%20Size%20and%20Class%20Balance%3A%20Alpha%20as%20a%20New%20Dataset%20Quality%20Metric%20for%0A%20%20Deep%20Learning%0AAuthor%3A%20Josiah%20Couch%20and%20Ramy%20Arnaout%20and%20Rima%20Arnaout%0AAbstract%3A%20%20%20In%20deep%20learning%2C%20achieving%20high%20performance%20on%20image%20classification%20tasks%0Arequires%20diverse%20training%20sets.%20However%2C%20dataset%20diversity%20is%20incompletely%0Aunderstood.%20The%20current%20best%20practice%20is%20to%20try%20to%20maximize%20dataset%20size%20and%0Aclass%20balance.%20Yet%20large%2C%20class-balanced%20datasets%20are%20not%20guaranteed%20to%20be%0Adiverse%3A%20images%20can%20still%20be%20arbitrarily%20similar.%20We%20hypothesized%20that%2C%20for%20a%0Agiven%20model%20architecture%2C%20better%20model%20performance%20can%20be%20achieved%20by%0Amaximizing%20dataset%20diversity%20more%20directly.%20This%20could%20open%20a%20path%20for%0Aperformance%20improvement%20without%20additional%20computational%20resources%20or%0Aarchitectural%20advances.%20To%20test%20this%20hypothesis%2C%20we%20introduce%20a%20comprehensive%0Aframework%20of%20diversity%20measures%2C%20developed%20in%20ecology%2C%20that%20generalizes%0Afamiliar%20quantities%20like%20Shannon%20entropy%20by%20accounting%20for%20similarities%20and%0Adifferences%20among%20images.%20%28Dataset%20size%20and%20class%20balance%20emerge%20from%20this%0Aframework%20as%20special%20cases.%29%20By%20analyzing%20thousands%20of%20subsets%20from%20seven%0Amedical%20datasets%20representing%20ultrasound%2C%20X-ray%2C%20CT%2C%20and%20pathology%20images%2C%20we%0Afound%20that%20the%20best%20correlates%20of%20performance%20were%20not%20size%20or%20class%20balance%0Abut%20%24A%24%20--%20%60%60big%20alpha%27%27%20--%20a%20set%20of%20generalized%20entropy%20measures%20interpreted%0Aas%20the%20effective%20number%20of%20image-class%20pairs%20in%20the%20dataset%2C%20after%20accounting%0Afor%20similarities%20among%20images.%20One%20of%20these%2C%20%24A_0%24%2C%20explained%2067%5C%25%20of%20the%0Avariance%20in%20balanced%20accuracy%20across%20all%20subsets%2C%20vs.%2054%5C%25%20for%20class%20balance%0Aand%20just%2039%5C%25%20for%20size.%20The%20best%20pair%20was%20size%20and%20%24A_1%24%20%2879%5C%25%29%2C%20which%0Aoutperformed%20size%20and%20class%20balance%20%2874%5C%25%29.%20%24A%24%20performed%20best%20for%20subsets%20from%0Aindividual%20datasets%20as%20well%20as%20across%20datasets%2C%20supporting%20the%20generality%20of%0Athese%20results.%20We%20propose%20maximizing%20%24A%24%20as%20a%20potential%20new%20way%20to%20improve%20the%0Aperformance%20of%20deep%20learning%20in%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Size%2520and%2520Class%2520Balance%253A%2520Alpha%2520as%2520a%2520New%2520Dataset%2520Quality%2520Metric%2520for%250A%2520%2520Deep%2520Learning%26entry.906535625%3DJosiah%2520Couch%2520and%2520Ramy%2520Arnaout%2520and%2520Rima%2520Arnaout%26entry.1292438233%3D%2520%2520In%2520deep%2520learning%252C%2520achieving%2520high%2520performance%2520on%2520image%2520classification%2520tasks%250Arequires%2520diverse%2520training%2520sets.%2520However%252C%2520dataset%2520diversity%2520is%2520incompletely%250Aunderstood.%2520The%2520current%2520best%2520practice%2520is%2520to%2520try%2520to%2520maximize%2520dataset%2520size%2520and%250Aclass%2520balance.%2520Yet%2520large%252C%2520class-balanced%2520datasets%2520are%2520not%2520guaranteed%2520to%2520be%250Adiverse%253A%2520images%2520can%2520still%2520be%2520arbitrarily%2520similar.%2520We%2520hypothesized%2520that%252C%2520for%2520a%250Agiven%2520model%2520architecture%252C%2520better%2520model%2520performance%2520can%2520be%2520achieved%2520by%250Amaximizing%2520dataset%2520diversity%2520more%2520directly.%2520This%2520could%2520open%2520a%2520path%2520for%250Aperformance%2520improvement%2520without%2520additional%2520computational%2520resources%2520or%250Aarchitectural%2520advances.%2520To%2520test%2520this%2520hypothesis%252C%2520we%2520introduce%2520a%2520comprehensive%250Aframework%2520of%2520diversity%2520measures%252C%2520developed%2520in%2520ecology%252C%2520that%2520generalizes%250Afamiliar%2520quantities%2520like%2520Shannon%2520entropy%2520by%2520accounting%2520for%2520similarities%2520and%250Adifferences%2520among%2520images.%2520%2528Dataset%2520size%2520and%2520class%2520balance%2520emerge%2520from%2520this%250Aframework%2520as%2520special%2520cases.%2529%2520By%2520analyzing%2520thousands%2520of%2520subsets%2520from%2520seven%250Amedical%2520datasets%2520representing%2520ultrasound%252C%2520X-ray%252C%2520CT%252C%2520and%2520pathology%2520images%252C%2520we%250Afound%2520that%2520the%2520best%2520correlates%2520of%2520performance%2520were%2520not%2520size%2520or%2520class%2520balance%250Abut%2520%2524A%2524%2520--%2520%2560%2560big%2520alpha%2527%2527%2520--%2520a%2520set%2520of%2520generalized%2520entropy%2520measures%2520interpreted%250Aas%2520the%2520effective%2520number%2520of%2520image-class%2520pairs%2520in%2520the%2520dataset%252C%2520after%2520accounting%250Afor%2520similarities%2520among%2520images.%2520One%2520of%2520these%252C%2520%2524A_0%2524%252C%2520explained%252067%255C%2525%2520of%2520the%250Avariance%2520in%2520balanced%2520accuracy%2520across%2520all%2520subsets%252C%2520vs.%252054%255C%2525%2520for%2520class%2520balance%250Aand%2520just%252039%255C%2525%2520for%2520size.%2520The%2520best%2520pair%2520was%2520size%2520and%2520%2524A_1%2524%2520%252879%255C%2525%2529%252C%2520which%250Aoutperformed%2520size%2520and%2520class%2520balance%2520%252874%255C%2525%2529.%2520%2524A%2524%2520performed%2520best%2520for%2520subsets%2520from%250Aindividual%2520datasets%2520as%2520well%2520as%2520across%2520datasets%252C%2520supporting%2520the%2520generality%2520of%250Athese%2520results.%2520We%2520propose%2520maximizing%2520%2524A%2524%2520as%2520a%2520potential%2520new%2520way%2520to%2520improve%2520the%250Aperformance%2520of%2520deep%2520learning%2520in%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Size%20and%20Class%20Balance%3A%20Alpha%20as%20a%20New%20Dataset%20Quality%20Metric%20for%0A%20%20Deep%20Learning&entry.906535625=Josiah%20Couch%20and%20Ramy%20Arnaout%20and%20Rima%20Arnaout&entry.1292438233=%20%20In%20deep%20learning%2C%20achieving%20high%20performance%20on%20image%20classification%20tasks%0Arequires%20diverse%20training%20sets.%20However%2C%20dataset%20diversity%20is%20incompletely%0Aunderstood.%20The%20current%20best%20practice%20is%20to%20try%20to%20maximize%20dataset%20size%20and%0Aclass%20balance.%20Yet%20large%2C%20class-balanced%20datasets%20are%20not%20guaranteed%20to%20be%0Adiverse%3A%20images%20can%20still%20be%20arbitrarily%20similar.%20We%20hypothesized%20that%2C%20for%20a%0Agiven%20model%20architecture%2C%20better%20model%20performance%20can%20be%20achieved%20by%0Amaximizing%20dataset%20diversity%20more%20directly.%20This%20could%20open%20a%20path%20for%0Aperformance%20improvement%20without%20additional%20computational%20resources%20or%0Aarchitectural%20advances.%20To%20test%20this%20hypothesis%2C%20we%20introduce%20a%20comprehensive%0Aframework%20of%20diversity%20measures%2C%20developed%20in%20ecology%2C%20that%20generalizes%0Afamiliar%20quantities%20like%20Shannon%20entropy%20by%20accounting%20for%20similarities%20and%0Adifferences%20among%20images.%20%28Dataset%20size%20and%20class%20balance%20emerge%20from%20this%0Aframework%20as%20special%20cases.%29%20By%20analyzing%20thousands%20of%20subsets%20from%20seven%0Amedical%20datasets%20representing%20ultrasound%2C%20X-ray%2C%20CT%2C%20and%20pathology%20images%2C%20we%0Afound%20that%20the%20best%20correlates%20of%20performance%20were%20not%20size%20or%20class%20balance%0Abut%20%24A%24%20--%20%60%60big%20alpha%27%27%20--%20a%20set%20of%20generalized%20entropy%20measures%20interpreted%0Aas%20the%20effective%20number%20of%20image-class%20pairs%20in%20the%20dataset%2C%20after%20accounting%0Afor%20similarities%20among%20images.%20One%20of%20these%2C%20%24A_0%24%2C%20explained%2067%5C%25%20of%20the%0Avariance%20in%20balanced%20accuracy%20across%20all%20subsets%2C%20vs.%2054%5C%25%20for%20class%20balance%0Aand%20just%2039%5C%25%20for%20size.%20The%20best%20pair%20was%20size%20and%20%24A_1%24%20%2879%5C%25%29%2C%20which%0Aoutperformed%20size%20and%20class%20balance%20%2874%5C%25%29.%20%24A%24%20performed%20best%20for%20subsets%20from%0Aindividual%20datasets%20as%20well%20as%20across%20datasets%2C%20supporting%20the%20generality%20of%0Athese%20results.%20We%20propose%20maximizing%20%24A%24%20as%20a%20potential%20new%20way%20to%20improve%20the%0Aperformance%20of%20deep%20learning%20in%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15724v1&entry.124074799=Read"},
{"title": "Exterior Penalty Policy Optimization with Penalty Metric Network under\n  Constraints", "author": "Shiqing Gao and Jiaxin Ding and Luoyi Fu and Xinbing Wang and Chenghu Zhou", "abstract": "  In Constrained Reinforcement Learning (CRL), agents explore the environment\nto learn the optimal policy while satisfying constraints. The penalty function\nmethod has recently been studied as an effective approach for handling\nconstraints, which imposes constraints penalties on the objective to transform\nthe constrained problem into an unconstrained one. However, it is challenging\nto choose appropriate penalties that balance policy performance and constraint\nsatisfaction efficiently. In this paper, we propose a theoretically guaranteed\npenalty function method, Exterior Penalty Policy Optimization (EPO), with\nadaptive penalties generated by a Penalty Metric Network (PMN). PMN responds\nappropriately to varying degrees of constraint violations, enabling efficient\nconstraint satisfaction and safe exploration. We theoretically prove that EPO\nconsistently improves constraint satisfaction with a convergence guarantee. We\npropose a new surrogate function and provide worst-case constraint violation\nand approximation error. In practice, we propose an effective smooth penalty\nfunction, which can be easily implemented with a first-order optimizer.\nExtensive experiments are conducted, showing that EPO outperforms the baselines\nin terms of policy performance and constraint satisfaction with a stable\ntraining process, particularly on complex tasks.\n", "link": "http://arxiv.org/abs/2407.15537v1", "date": "2024-07-22", "relevancy": 1.8059, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.463}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exterior%20Penalty%20Policy%20Optimization%20with%20Penalty%20Metric%20Network%20under%0A%20%20Constraints&body=Title%3A%20Exterior%20Penalty%20Policy%20Optimization%20with%20Penalty%20Metric%20Network%20under%0A%20%20Constraints%0AAuthor%3A%20Shiqing%20Gao%20and%20Jiaxin%20Ding%20and%20Luoyi%20Fu%20and%20Xinbing%20Wang%20and%20Chenghu%20Zhou%0AAbstract%3A%20%20%20In%20Constrained%20Reinforcement%20Learning%20%28CRL%29%2C%20agents%20explore%20the%20environment%0Ato%20learn%20the%20optimal%20policy%20while%20satisfying%20constraints.%20The%20penalty%20function%0Amethod%20has%20recently%20been%20studied%20as%20an%20effective%20approach%20for%20handling%0Aconstraints%2C%20which%20imposes%20constraints%20penalties%20on%20the%20objective%20to%20transform%0Athe%20constrained%20problem%20into%20an%20unconstrained%20one.%20However%2C%20it%20is%20challenging%0Ato%20choose%20appropriate%20penalties%20that%20balance%20policy%20performance%20and%20constraint%0Asatisfaction%20efficiently.%20In%20this%20paper%2C%20we%20propose%20a%20theoretically%20guaranteed%0Apenalty%20function%20method%2C%20Exterior%20Penalty%20Policy%20Optimization%20%28EPO%29%2C%20with%0Aadaptive%20penalties%20generated%20by%20a%20Penalty%20Metric%20Network%20%28PMN%29.%20PMN%20responds%0Aappropriately%20to%20varying%20degrees%20of%20constraint%20violations%2C%20enabling%20efficient%0Aconstraint%20satisfaction%20and%20safe%20exploration.%20We%20theoretically%20prove%20that%20EPO%0Aconsistently%20improves%20constraint%20satisfaction%20with%20a%20convergence%20guarantee.%20We%0Apropose%20a%20new%20surrogate%20function%20and%20provide%20worst-case%20constraint%20violation%0Aand%20approximation%20error.%20In%20practice%2C%20we%20propose%20an%20effective%20smooth%20penalty%0Afunction%2C%20which%20can%20be%20easily%20implemented%20with%20a%20first-order%20optimizer.%0AExtensive%20experiments%20are%20conducted%2C%20showing%20that%20EPO%20outperforms%20the%20baselines%0Ain%20terms%20of%20policy%20performance%20and%20constraint%20satisfaction%20with%20a%20stable%0Atraining%20process%2C%20particularly%20on%20complex%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExterior%2520Penalty%2520Policy%2520Optimization%2520with%2520Penalty%2520Metric%2520Network%2520under%250A%2520%2520Constraints%26entry.906535625%3DShiqing%2520Gao%2520and%2520Jiaxin%2520Ding%2520and%2520Luoyi%2520Fu%2520and%2520Xinbing%2520Wang%2520and%2520Chenghu%2520Zhou%26entry.1292438233%3D%2520%2520In%2520Constrained%2520Reinforcement%2520Learning%2520%2528CRL%2529%252C%2520agents%2520explore%2520the%2520environment%250Ato%2520learn%2520the%2520optimal%2520policy%2520while%2520satisfying%2520constraints.%2520The%2520penalty%2520function%250Amethod%2520has%2520recently%2520been%2520studied%2520as%2520an%2520effective%2520approach%2520for%2520handling%250Aconstraints%252C%2520which%2520imposes%2520constraints%2520penalties%2520on%2520the%2520objective%2520to%2520transform%250Athe%2520constrained%2520problem%2520into%2520an%2520unconstrained%2520one.%2520However%252C%2520it%2520is%2520challenging%250Ato%2520choose%2520appropriate%2520penalties%2520that%2520balance%2520policy%2520performance%2520and%2520constraint%250Asatisfaction%2520efficiently.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520theoretically%2520guaranteed%250Apenalty%2520function%2520method%252C%2520Exterior%2520Penalty%2520Policy%2520Optimization%2520%2528EPO%2529%252C%2520with%250Aadaptive%2520penalties%2520generated%2520by%2520a%2520Penalty%2520Metric%2520Network%2520%2528PMN%2529.%2520PMN%2520responds%250Aappropriately%2520to%2520varying%2520degrees%2520of%2520constraint%2520violations%252C%2520enabling%2520efficient%250Aconstraint%2520satisfaction%2520and%2520safe%2520exploration.%2520We%2520theoretically%2520prove%2520that%2520EPO%250Aconsistently%2520improves%2520constraint%2520satisfaction%2520with%2520a%2520convergence%2520guarantee.%2520We%250Apropose%2520a%2520new%2520surrogate%2520function%2520and%2520provide%2520worst-case%2520constraint%2520violation%250Aand%2520approximation%2520error.%2520In%2520practice%252C%2520we%2520propose%2520an%2520effective%2520smooth%2520penalty%250Afunction%252C%2520which%2520can%2520be%2520easily%2520implemented%2520with%2520a%2520first-order%2520optimizer.%250AExtensive%2520experiments%2520are%2520conducted%252C%2520showing%2520that%2520EPO%2520outperforms%2520the%2520baselines%250Ain%2520terms%2520of%2520policy%2520performance%2520and%2520constraint%2520satisfaction%2520with%2520a%2520stable%250Atraining%2520process%252C%2520particularly%2520on%2520complex%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exterior%20Penalty%20Policy%20Optimization%20with%20Penalty%20Metric%20Network%20under%0A%20%20Constraints&entry.906535625=Shiqing%20Gao%20and%20Jiaxin%20Ding%20and%20Luoyi%20Fu%20and%20Xinbing%20Wang%20and%20Chenghu%20Zhou&entry.1292438233=%20%20In%20Constrained%20Reinforcement%20Learning%20%28CRL%29%2C%20agents%20explore%20the%20environment%0Ato%20learn%20the%20optimal%20policy%20while%20satisfying%20constraints.%20The%20penalty%20function%0Amethod%20has%20recently%20been%20studied%20as%20an%20effective%20approach%20for%20handling%0Aconstraints%2C%20which%20imposes%20constraints%20penalties%20on%20the%20objective%20to%20transform%0Athe%20constrained%20problem%20into%20an%20unconstrained%20one.%20However%2C%20it%20is%20challenging%0Ato%20choose%20appropriate%20penalties%20that%20balance%20policy%20performance%20and%20constraint%0Asatisfaction%20efficiently.%20In%20this%20paper%2C%20we%20propose%20a%20theoretically%20guaranteed%0Apenalty%20function%20method%2C%20Exterior%20Penalty%20Policy%20Optimization%20%28EPO%29%2C%20with%0Aadaptive%20penalties%20generated%20by%20a%20Penalty%20Metric%20Network%20%28PMN%29.%20PMN%20responds%0Aappropriately%20to%20varying%20degrees%20of%20constraint%20violations%2C%20enabling%20efficient%0Aconstraint%20satisfaction%20and%20safe%20exploration.%20We%20theoretically%20prove%20that%20EPO%0Aconsistently%20improves%20constraint%20satisfaction%20with%20a%20convergence%20guarantee.%20We%0Apropose%20a%20new%20surrogate%20function%20and%20provide%20worst-case%20constraint%20violation%0Aand%20approximation%20error.%20In%20practice%2C%20we%20propose%20an%20effective%20smooth%20penalty%0Afunction%2C%20which%20can%20be%20easily%20implemented%20with%20a%20first-order%20optimizer.%0AExtensive%20experiments%20are%20conducted%2C%20showing%20that%20EPO%20outperforms%20the%20baselines%0Ain%20terms%20of%20policy%20performance%20and%20constraint%20satisfaction%20with%20a%20stable%0Atraining%20process%2C%20particularly%20on%20complex%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15537v1&entry.124074799=Read"},
{"title": "AI-Driven Fast and Early Detection of IoT Botnet Threats: A\n  Comprehensive Network Traffic Analysis Approach", "author": "Abdelaziz Amara korba and Aleddine Diaf and Yacine Ghamri-Doudane", "abstract": "  In the rapidly evolving landscape of cyber threats targeting the Internet of\nThings (IoT) ecosystem, and in light of the surge in botnet-driven Distributed\nDenial of Service (DDoS) and brute force attacks, this study focuses on the\nearly detection of IoT bots. It specifically addresses the detection of stealth\nbot communication that precedes and orchestrates attacks. This study proposes a\ncomprehensive methodology for analyzing IoT network traffic, including\nconsiderations for both unidirectional and bidirectional flow, as well as\npacket formats. It explores a wide spectrum of network features critical for\nrepresenting network traffic and characterizing benign IoT traffic patterns\neffectively. Moreover, it delves into the modeling of traffic using various\nsemi-supervised learning techniques. Through extensive experimentation with the\nIoT-23 dataset - a comprehensive collection featuring diverse botnet types and\ntraffic scenarios - we have demonstrated the feasibility of detecting botnet\ntraffic corresponding to different operations and types of bots, specifically\nfocusing on stealth command and control (C2) communications. The results\nobtained have demonstrated the feasibility of identifying C2 communication with\na 100% success rate through packet-based methods and 94% via flow based\napproaches, with a false positive rate of 1.53%.\n", "link": "http://arxiv.org/abs/2407.15688v1", "date": "2024-07-22", "relevancy": 1.5888, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4127}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3961}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Driven%20Fast%20and%20Early%20Detection%20of%20IoT%20Botnet%20Threats%3A%20A%0A%20%20Comprehensive%20Network%20Traffic%20Analysis%20Approach&body=Title%3A%20AI-Driven%20Fast%20and%20Early%20Detection%20of%20IoT%20Botnet%20Threats%3A%20A%0A%20%20Comprehensive%20Network%20Traffic%20Analysis%20Approach%0AAuthor%3A%20Abdelaziz%20Amara%20korba%20and%20Aleddine%20Diaf%20and%20Yacine%20Ghamri-Doudane%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20landscape%20of%20cyber%20threats%20targeting%20the%20Internet%20of%0AThings%20%28IoT%29%20ecosystem%2C%20and%20in%20light%20of%20the%20surge%20in%20botnet-driven%20Distributed%0ADenial%20of%20Service%20%28DDoS%29%20and%20brute%20force%20attacks%2C%20this%20study%20focuses%20on%20the%0Aearly%20detection%20of%20IoT%20bots.%20It%20specifically%20addresses%20the%20detection%20of%20stealth%0Abot%20communication%20that%20precedes%20and%20orchestrates%20attacks.%20This%20study%20proposes%20a%0Acomprehensive%20methodology%20for%20analyzing%20IoT%20network%20traffic%2C%20including%0Aconsiderations%20for%20both%20unidirectional%20and%20bidirectional%20flow%2C%20as%20well%20as%0Apacket%20formats.%20It%20explores%20a%20wide%20spectrum%20of%20network%20features%20critical%20for%0Arepresenting%20network%20traffic%20and%20characterizing%20benign%20IoT%20traffic%20patterns%0Aeffectively.%20Moreover%2C%20it%20delves%20into%20the%20modeling%20of%20traffic%20using%20various%0Asemi-supervised%20learning%20techniques.%20Through%20extensive%20experimentation%20with%20the%0AIoT-23%20dataset%20-%20a%20comprehensive%20collection%20featuring%20diverse%20botnet%20types%20and%0Atraffic%20scenarios%20-%20we%20have%20demonstrated%20the%20feasibility%20of%20detecting%20botnet%0Atraffic%20corresponding%20to%20different%20operations%20and%20types%20of%20bots%2C%20specifically%0Afocusing%20on%20stealth%20command%20and%20control%20%28C2%29%20communications.%20The%20results%0Aobtained%20have%20demonstrated%20the%20feasibility%20of%20identifying%20C2%20communication%20with%0Aa%20100%25%20success%20rate%20through%20packet-based%20methods%20and%2094%25%20via%20flow%20based%0Aapproaches%2C%20with%20a%20false%20positive%20rate%20of%201.53%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Driven%2520Fast%2520and%2520Early%2520Detection%2520of%2520IoT%2520Botnet%2520Threats%253A%2520A%250A%2520%2520Comprehensive%2520Network%2520Traffic%2520Analysis%2520Approach%26entry.906535625%3DAbdelaziz%2520Amara%2520korba%2520and%2520Aleddine%2520Diaf%2520and%2520Yacine%2520Ghamri-Doudane%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520cyber%2520threats%2520targeting%2520the%2520Internet%2520of%250AThings%2520%2528IoT%2529%2520ecosystem%252C%2520and%2520in%2520light%2520of%2520the%2520surge%2520in%2520botnet-driven%2520Distributed%250ADenial%2520of%2520Service%2520%2528DDoS%2529%2520and%2520brute%2520force%2520attacks%252C%2520this%2520study%2520focuses%2520on%2520the%250Aearly%2520detection%2520of%2520IoT%2520bots.%2520It%2520specifically%2520addresses%2520the%2520detection%2520of%2520stealth%250Abot%2520communication%2520that%2520precedes%2520and%2520orchestrates%2520attacks.%2520This%2520study%2520proposes%2520a%250Acomprehensive%2520methodology%2520for%2520analyzing%2520IoT%2520network%2520traffic%252C%2520including%250Aconsiderations%2520for%2520both%2520unidirectional%2520and%2520bidirectional%2520flow%252C%2520as%2520well%2520as%250Apacket%2520formats.%2520It%2520explores%2520a%2520wide%2520spectrum%2520of%2520network%2520features%2520critical%2520for%250Arepresenting%2520network%2520traffic%2520and%2520characterizing%2520benign%2520IoT%2520traffic%2520patterns%250Aeffectively.%2520Moreover%252C%2520it%2520delves%2520into%2520the%2520modeling%2520of%2520traffic%2520using%2520various%250Asemi-supervised%2520learning%2520techniques.%2520Through%2520extensive%2520experimentation%2520with%2520the%250AIoT-23%2520dataset%2520-%2520a%2520comprehensive%2520collection%2520featuring%2520diverse%2520botnet%2520types%2520and%250Atraffic%2520scenarios%2520-%2520we%2520have%2520demonstrated%2520the%2520feasibility%2520of%2520detecting%2520botnet%250Atraffic%2520corresponding%2520to%2520different%2520operations%2520and%2520types%2520of%2520bots%252C%2520specifically%250Afocusing%2520on%2520stealth%2520command%2520and%2520control%2520%2528C2%2529%2520communications.%2520The%2520results%250Aobtained%2520have%2520demonstrated%2520the%2520feasibility%2520of%2520identifying%2520C2%2520communication%2520with%250Aa%2520100%2525%2520success%2520rate%2520through%2520packet-based%2520methods%2520and%252094%2525%2520via%2520flow%2520based%250Aapproaches%252C%2520with%2520a%2520false%2520positive%2520rate%2520of%25201.53%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Driven%20Fast%20and%20Early%20Detection%20of%20IoT%20Botnet%20Threats%3A%20A%0A%20%20Comprehensive%20Network%20Traffic%20Analysis%20Approach&entry.906535625=Abdelaziz%20Amara%20korba%20and%20Aleddine%20Diaf%20and%20Yacine%20Ghamri-Doudane&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20cyber%20threats%20targeting%20the%20Internet%20of%0AThings%20%28IoT%29%20ecosystem%2C%20and%20in%20light%20of%20the%20surge%20in%20botnet-driven%20Distributed%0ADenial%20of%20Service%20%28DDoS%29%20and%20brute%20force%20attacks%2C%20this%20study%20focuses%20on%20the%0Aearly%20detection%20of%20IoT%20bots.%20It%20specifically%20addresses%20the%20detection%20of%20stealth%0Abot%20communication%20that%20precedes%20and%20orchestrates%20attacks.%20This%20study%20proposes%20a%0Acomprehensive%20methodology%20for%20analyzing%20IoT%20network%20traffic%2C%20including%0Aconsiderations%20for%20both%20unidirectional%20and%20bidirectional%20flow%2C%20as%20well%20as%0Apacket%20formats.%20It%20explores%20a%20wide%20spectrum%20of%20network%20features%20critical%20for%0Arepresenting%20network%20traffic%20and%20characterizing%20benign%20IoT%20traffic%20patterns%0Aeffectively.%20Moreover%2C%20it%20delves%20into%20the%20modeling%20of%20traffic%20using%20various%0Asemi-supervised%20learning%20techniques.%20Through%20extensive%20experimentation%20with%20the%0AIoT-23%20dataset%20-%20a%20comprehensive%20collection%20featuring%20diverse%20botnet%20types%20and%0Atraffic%20scenarios%20-%20we%20have%20demonstrated%20the%20feasibility%20of%20detecting%20botnet%0Atraffic%20corresponding%20to%20different%20operations%20and%20types%20of%20bots%2C%20specifically%0Afocusing%20on%20stealth%20command%20and%20control%20%28C2%29%20communications.%20The%20results%0Aobtained%20have%20demonstrated%20the%20feasibility%20of%20identifying%20C2%20communication%20with%0Aa%20100%25%20success%20rate%20through%20packet-based%20methods%20and%2094%25%20via%20flow%20based%0Aapproaches%2C%20with%20a%20false%20positive%20rate%20of%201.53%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15688v1&entry.124074799=Read"},
{"title": "Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model", "author": "Xunyu Zhu and Jian Li and Can Ma and Weiping Wang", "abstract": "  Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.\n", "link": "http://arxiv.org/abs/2407.10167v2", "date": "2024-07-22", "relevancy": 1.8816, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4774}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4727}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Key-Point-Driven%20Mathematical%20Reasoning%20Distillation%20of%20Large%20Language%0A%20%20Model&body=Title%3A%20Key-Point-Driven%20Mathematical%20Reasoning%20Distillation%20of%20Large%20Language%0A%20%20Model%0AAuthor%3A%20Xunyu%20Zhu%20and%20Jian%20Li%20and%20Can%20Ma%20and%20Weiping%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20exceptional%20proficiency%20in%0Amathematical%20reasoning%20tasks%20due%20to%20their%20extensive%20parameter%20counts%20and%0Atraining%20on%20vast%20datasets.%20Despite%20these%20capabilities%2C%20deploying%20LLMs%20is%0Ahindered%20by%20their%20computational%20demands.%20Distilling%20LLM%20mathematical%20reasoning%0Ainto%20Smaller%20Language%20Models%20%28SLMs%29%20has%20emerged%20as%20a%20solution%20to%20this%0Achallenge%2C%20although%20these%20smaller%20models%20often%20suffer%20from%20errors%20in%0Acalculation%20and%20semantic%20understanding.%20Prior%20work%20has%20proposed%0AProgram-of-Thought%20Distillation%20%28PoTD%29%20to%20avoid%20calculation%20error.%20To%20further%0Aaddress%20semantic%20understanding%20errors%2C%20we%20propose%20Key-Point-Driven%20Mathematical%0AReasoning%20Distillation%20%28KPDD%29.%20KPDD%20enhances%20the%20reasoning%20performance%20of%20SLMs%0Aby%20breaking%20down%20the%20problem-solving%20process%20into%20three%20stages%3A%20Core%20Question%0AExtraction%2C%20Problem-Solving%20Information%20Extraction%2C%20and%20Step-by-Step%20Solution.%0AThis%20method%20is%20further%20divided%20into%20KPDD-CoT%2C%20which%20generates%20Chain-of-Thought%0Arationales%2C%20and%20KPDD-PoT%2C%20which%20creates%20Program-of-Thought%20rationales.%20The%0Aexperiment%20results%20show%20that%20KPDD-CoT%20significantly%20improves%20reasoning%0Aabilities%2C%20while%20KPDD-PoT%20achieves%20state-of-the-art%20performance%20in%20mathematical%0Areasoning%20tasks.%20Our%20approach%20effectively%20mitigates%20misunderstanding%20errors%2C%0Aadvancing%20the%20deployment%20of%20efficient%20and%20capable%20SLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10167v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKey-Point-Driven%2520Mathematical%2520Reasoning%2520Distillation%2520of%2520Large%2520Language%250A%2520%2520Model%26entry.906535625%3DXunyu%2520Zhu%2520and%2520Jian%2520Li%2520and%2520Can%2520Ma%2520and%2520Weiping%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520exceptional%2520proficiency%2520in%250Amathematical%2520reasoning%2520tasks%2520due%2520to%2520their%2520extensive%2520parameter%2520counts%2520and%250Atraining%2520on%2520vast%2520datasets.%2520Despite%2520these%2520capabilities%252C%2520deploying%2520LLMs%2520is%250Ahindered%2520by%2520their%2520computational%2520demands.%2520Distilling%2520LLM%2520mathematical%2520reasoning%250Ainto%2520Smaller%2520Language%2520Models%2520%2528SLMs%2529%2520has%2520emerged%2520as%2520a%2520solution%2520to%2520this%250Achallenge%252C%2520although%2520these%2520smaller%2520models%2520often%2520suffer%2520from%2520errors%2520in%250Acalculation%2520and%2520semantic%2520understanding.%2520Prior%2520work%2520has%2520proposed%250AProgram-of-Thought%2520Distillation%2520%2528PoTD%2529%2520to%2520avoid%2520calculation%2520error.%2520To%2520further%250Aaddress%2520semantic%2520understanding%2520errors%252C%2520we%2520propose%2520Key-Point-Driven%2520Mathematical%250AReasoning%2520Distillation%2520%2528KPDD%2529.%2520KPDD%2520enhances%2520the%2520reasoning%2520performance%2520of%2520SLMs%250Aby%2520breaking%2520down%2520the%2520problem-solving%2520process%2520into%2520three%2520stages%253A%2520Core%2520Question%250AExtraction%252C%2520Problem-Solving%2520Information%2520Extraction%252C%2520and%2520Step-by-Step%2520Solution.%250AThis%2520method%2520is%2520further%2520divided%2520into%2520KPDD-CoT%252C%2520which%2520generates%2520Chain-of-Thought%250Arationales%252C%2520and%2520KPDD-PoT%252C%2520which%2520creates%2520Program-of-Thought%2520rationales.%2520The%250Aexperiment%2520results%2520show%2520that%2520KPDD-CoT%2520significantly%2520improves%2520reasoning%250Aabilities%252C%2520while%2520KPDD-PoT%2520achieves%2520state-of-the-art%2520performance%2520in%2520mathematical%250Areasoning%2520tasks.%2520Our%2520approach%2520effectively%2520mitigates%2520misunderstanding%2520errors%252C%250Aadvancing%2520the%2520deployment%2520of%2520efficient%2520and%2520capable%2520SLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10167v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Key-Point-Driven%20Mathematical%20Reasoning%20Distillation%20of%20Large%20Language%0A%20%20Model&entry.906535625=Xunyu%20Zhu%20and%20Jian%20Li%20and%20Can%20Ma%20and%20Weiping%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20exceptional%20proficiency%20in%0Amathematical%20reasoning%20tasks%20due%20to%20their%20extensive%20parameter%20counts%20and%0Atraining%20on%20vast%20datasets.%20Despite%20these%20capabilities%2C%20deploying%20LLMs%20is%0Ahindered%20by%20their%20computational%20demands.%20Distilling%20LLM%20mathematical%20reasoning%0Ainto%20Smaller%20Language%20Models%20%28SLMs%29%20has%20emerged%20as%20a%20solution%20to%20this%0Achallenge%2C%20although%20these%20smaller%20models%20often%20suffer%20from%20errors%20in%0Acalculation%20and%20semantic%20understanding.%20Prior%20work%20has%20proposed%0AProgram-of-Thought%20Distillation%20%28PoTD%29%20to%20avoid%20calculation%20error.%20To%20further%0Aaddress%20semantic%20understanding%20errors%2C%20we%20propose%20Key-Point-Driven%20Mathematical%0AReasoning%20Distillation%20%28KPDD%29.%20KPDD%20enhances%20the%20reasoning%20performance%20of%20SLMs%0Aby%20breaking%20down%20the%20problem-solving%20process%20into%20three%20stages%3A%20Core%20Question%0AExtraction%2C%20Problem-Solving%20Information%20Extraction%2C%20and%20Step-by-Step%20Solution.%0AThis%20method%20is%20further%20divided%20into%20KPDD-CoT%2C%20which%20generates%20Chain-of-Thought%0Arationales%2C%20and%20KPDD-PoT%2C%20which%20creates%20Program-of-Thought%20rationales.%20The%0Aexperiment%20results%20show%20that%20KPDD-CoT%20significantly%20improves%20reasoning%0Aabilities%2C%20while%20KPDD-PoT%20achieves%20state-of-the-art%20performance%20in%20mathematical%0Areasoning%20tasks.%20Our%20approach%20effectively%20mitigates%20misunderstanding%20errors%2C%0Aadvancing%20the%20deployment%20of%20efficient%20and%20capable%20SLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10167v2&entry.124074799=Read"},
{"title": "Do Large Language Models Have Compositional Ability? An Investigation\n  into Limitations and Scalability", "author": "Zhuoyan Xu and Zhenmei Shi and Yingyu Liang", "abstract": "  Large language models (LLMs) have emerged as powerful tools for many AI\nproblems and exhibit remarkable in-context learning (ICL) capabilities.\nCompositional ability, solving unseen complex tasks that combine two or more\nsimple tasks, is an essential reasoning ability for Artificial General\nIntelligence. Despite LLM's tremendous success, how they approach composite\ntasks, especially those not encountered during the pretraining phase, remains\nan open question and largely ununderstood. In this study, we delve into the ICL\ncapabilities of LLMs on composite tasks, with only simple tasks as in-context\nexamples. We develop a test suite of composite tasks that include linguistic\nand logical challenges and perform empirical studies across different LLM\nfamilies. We observe that models exhibit divergent behaviors: (1) For simpler\ncomposite tasks that apply distinct mapping mechanisms to different input\nsegments, the models demonstrate decent compositional ability, while scaling up\nthe model enhances this ability; (2) for more complex composite tasks that\ninvolving reasoning multiple steps, where each step represent one task, models\ntypically underperform, and scaling up generally provide no improvements. We\noffer theoretical analysis in a simplified setting, explaining that models\nexhibit compositional capability when the task handles different input parts\nseparately. We believe our work sheds new light on the capabilities of LLMs in\nsolving composite tasks regarding the nature of the tasks and model scale. Our\ndataset and code are available at\n{\\url{https://github.com/OliverXUZY/LLM_Compose}}.\n", "link": "http://arxiv.org/abs/2407.15720v1", "date": "2024-07-22", "relevancy": 1.9729, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5016}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4905}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Language%20Models%20Have%20Compositional%20Ability%3F%20An%20Investigation%0A%20%20into%20Limitations%20and%20Scalability&body=Title%3A%20Do%20Large%20Language%20Models%20Have%20Compositional%20Ability%3F%20An%20Investigation%0A%20%20into%20Limitations%20and%20Scalability%0AAuthor%3A%20Zhuoyan%20Xu%20and%20Zhenmei%20Shi%20and%20Yingyu%20Liang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20many%20AI%0Aproblems%20and%20exhibit%20remarkable%20in-context%20learning%20%28ICL%29%20capabilities.%0ACompositional%20ability%2C%20solving%20unseen%20complex%20tasks%20that%20combine%20two%20or%20more%0Asimple%20tasks%2C%20is%20an%20essential%20reasoning%20ability%20for%20Artificial%20General%0AIntelligence.%20Despite%20LLM%27s%20tremendous%20success%2C%20how%20they%20approach%20composite%0Atasks%2C%20especially%20those%20not%20encountered%20during%20the%20pretraining%20phase%2C%20remains%0Aan%20open%20question%20and%20largely%20ununderstood.%20In%20this%20study%2C%20we%20delve%20into%20the%20ICL%0Acapabilities%20of%20LLMs%20on%20composite%20tasks%2C%20with%20only%20simple%20tasks%20as%20in-context%0Aexamples.%20We%20develop%20a%20test%20suite%20of%20composite%20tasks%20that%20include%20linguistic%0Aand%20logical%20challenges%20and%20perform%20empirical%20studies%20across%20different%20LLM%0Afamilies.%20We%20observe%20that%20models%20exhibit%20divergent%20behaviors%3A%20%281%29%20For%20simpler%0Acomposite%20tasks%20that%20apply%20distinct%20mapping%20mechanisms%20to%20different%20input%0Asegments%2C%20the%20models%20demonstrate%20decent%20compositional%20ability%2C%20while%20scaling%20up%0Athe%20model%20enhances%20this%20ability%3B%20%282%29%20for%20more%20complex%20composite%20tasks%20that%0Ainvolving%20reasoning%20multiple%20steps%2C%20where%20each%20step%20represent%20one%20task%2C%20models%0Atypically%20underperform%2C%20and%20scaling%20up%20generally%20provide%20no%20improvements.%20We%0Aoffer%20theoretical%20analysis%20in%20a%20simplified%20setting%2C%20explaining%20that%20models%0Aexhibit%20compositional%20capability%20when%20the%20task%20handles%20different%20input%20parts%0Aseparately.%20We%20believe%20our%20work%20sheds%20new%20light%20on%20the%20capabilities%20of%20LLMs%20in%0Asolving%20composite%20tasks%20regarding%20the%20nature%20of%20the%20tasks%20and%20model%20scale.%20Our%0Adataset%20and%20code%20are%20available%20at%0A%7B%5Curl%7Bhttps%3A//github.com/OliverXUZY/LLM_Compose%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Language%2520Models%2520Have%2520Compositional%2520Ability%253F%2520An%2520Investigation%250A%2520%2520into%2520Limitations%2520and%2520Scalability%26entry.906535625%3DZhuoyan%2520Xu%2520and%2520Zhenmei%2520Shi%2520and%2520Yingyu%2520Liang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520many%2520AI%250Aproblems%2520and%2520exhibit%2520remarkable%2520in-context%2520learning%2520%2528ICL%2529%2520capabilities.%250ACompositional%2520ability%252C%2520solving%2520unseen%2520complex%2520tasks%2520that%2520combine%2520two%2520or%2520more%250Asimple%2520tasks%252C%2520is%2520an%2520essential%2520reasoning%2520ability%2520for%2520Artificial%2520General%250AIntelligence.%2520Despite%2520LLM%2527s%2520tremendous%2520success%252C%2520how%2520they%2520approach%2520composite%250Atasks%252C%2520especially%2520those%2520not%2520encountered%2520during%2520the%2520pretraining%2520phase%252C%2520remains%250Aan%2520open%2520question%2520and%2520largely%2520ununderstood.%2520In%2520this%2520study%252C%2520we%2520delve%2520into%2520the%2520ICL%250Acapabilities%2520of%2520LLMs%2520on%2520composite%2520tasks%252C%2520with%2520only%2520simple%2520tasks%2520as%2520in-context%250Aexamples.%2520We%2520develop%2520a%2520test%2520suite%2520of%2520composite%2520tasks%2520that%2520include%2520linguistic%250Aand%2520logical%2520challenges%2520and%2520perform%2520empirical%2520studies%2520across%2520different%2520LLM%250Afamilies.%2520We%2520observe%2520that%2520models%2520exhibit%2520divergent%2520behaviors%253A%2520%25281%2529%2520For%2520simpler%250Acomposite%2520tasks%2520that%2520apply%2520distinct%2520mapping%2520mechanisms%2520to%2520different%2520input%250Asegments%252C%2520the%2520models%2520demonstrate%2520decent%2520compositional%2520ability%252C%2520while%2520scaling%2520up%250Athe%2520model%2520enhances%2520this%2520ability%253B%2520%25282%2529%2520for%2520more%2520complex%2520composite%2520tasks%2520that%250Ainvolving%2520reasoning%2520multiple%2520steps%252C%2520where%2520each%2520step%2520represent%2520one%2520task%252C%2520models%250Atypically%2520underperform%252C%2520and%2520scaling%2520up%2520generally%2520provide%2520no%2520improvements.%2520We%250Aoffer%2520theoretical%2520analysis%2520in%2520a%2520simplified%2520setting%252C%2520explaining%2520that%2520models%250Aexhibit%2520compositional%2520capability%2520when%2520the%2520task%2520handles%2520different%2520input%2520parts%250Aseparately.%2520We%2520believe%2520our%2520work%2520sheds%2520new%2520light%2520on%2520the%2520capabilities%2520of%2520LLMs%2520in%250Asolving%2520composite%2520tasks%2520regarding%2520the%2520nature%2520of%2520the%2520tasks%2520and%2520model%2520scale.%2520Our%250Adataset%2520and%2520code%2520are%2520available%2520at%250A%257B%255Curl%257Bhttps%253A//github.com/OliverXUZY/LLM_Compose%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Language%20Models%20Have%20Compositional%20Ability%3F%20An%20Investigation%0A%20%20into%20Limitations%20and%20Scalability&entry.906535625=Zhuoyan%20Xu%20and%20Zhenmei%20Shi%20and%20Yingyu%20Liang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20many%20AI%0Aproblems%20and%20exhibit%20remarkable%20in-context%20learning%20%28ICL%29%20capabilities.%0ACompositional%20ability%2C%20solving%20unseen%20complex%20tasks%20that%20combine%20two%20or%20more%0Asimple%20tasks%2C%20is%20an%20essential%20reasoning%20ability%20for%20Artificial%20General%0AIntelligence.%20Despite%20LLM%27s%20tremendous%20success%2C%20how%20they%20approach%20composite%0Atasks%2C%20especially%20those%20not%20encountered%20during%20the%20pretraining%20phase%2C%20remains%0Aan%20open%20question%20and%20largely%20ununderstood.%20In%20this%20study%2C%20we%20delve%20into%20the%20ICL%0Acapabilities%20of%20LLMs%20on%20composite%20tasks%2C%20with%20only%20simple%20tasks%20as%20in-context%0Aexamples.%20We%20develop%20a%20test%20suite%20of%20composite%20tasks%20that%20include%20linguistic%0Aand%20logical%20challenges%20and%20perform%20empirical%20studies%20across%20different%20LLM%0Afamilies.%20We%20observe%20that%20models%20exhibit%20divergent%20behaviors%3A%20%281%29%20For%20simpler%0Acomposite%20tasks%20that%20apply%20distinct%20mapping%20mechanisms%20to%20different%20input%0Asegments%2C%20the%20models%20demonstrate%20decent%20compositional%20ability%2C%20while%20scaling%20up%0Athe%20model%20enhances%20this%20ability%3B%20%282%29%20for%20more%20complex%20composite%20tasks%20that%0Ainvolving%20reasoning%20multiple%20steps%2C%20where%20each%20step%20represent%20one%20task%2C%20models%0Atypically%20underperform%2C%20and%20scaling%20up%20generally%20provide%20no%20improvements.%20We%0Aoffer%20theoretical%20analysis%20in%20a%20simplified%20setting%2C%20explaining%20that%20models%0Aexhibit%20compositional%20capability%20when%20the%20task%20handles%20different%20input%20parts%0Aseparately.%20We%20believe%20our%20work%20sheds%20new%20light%20on%20the%20capabilities%20of%20LLMs%20in%0Asolving%20composite%20tasks%20regarding%20the%20nature%20of%20the%20tasks%20and%20model%20scale.%20Our%0Adataset%20and%20code%20are%20available%20at%0A%7B%5Curl%7Bhttps%3A//github.com/OliverXUZY/LLM_Compose%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15720v1&entry.124074799=Read"},
{"title": "Learning Multi-modal Representations by Watching Hundreds of Surgical\n  Video Lectures", "author": "Kun Yuan and Vinkle Srivastav and Tong Yu and Joel L. Lavanchy and Pietro Mascagni and Nassir Navab and Nicolas Padoy", "abstract": "  Recent advancements in surgical computer vision have been driven by\nvision-only models, which lack language semantics, relying on manually\nannotated videos to predict fixed object categories. This limits their\ngeneralizability to unseen surgical procedures and tasks. We propose leveraging\nsurgical video lectures from e-learning platforms to provide effective vision\nand language supervisory signals for multi-modal representation learning,\nbypassing manual annotations. We address surgery-specific linguistic challenges\nusing multiple automatic speech recognition systems for text transcriptions. We\nintroduce SurgVLP - Surgical Vision Language Pre-training - a novel method for\nmulti-modal representation learning. SurgVLP employs a new contrastive learning\nobjective, aligning video clip embeddings with corresponding multiple text\nembeddings in a joint latent space. We demonstrate the representational\ncapability of this space through several vision-and-language surgical tasks and\nvision-only tasks specific to surgery. Unlike current fully supervised\napproaches, SurgVLP adapts to different surgical procedures and tasks without\nspecific fine-tuning, achieving zero-shot adaptation to tasks such as surgical\ntool, phase, and triplet recognition without manual annotation. These results\nhighlight the transferability and versatility of the learned multi-modal\nrepresentations in surgical video analysis. The code is available at\nhttps://github.com/CAMMA-public/SurgVLP\n", "link": "http://arxiv.org/abs/2307.15220v3", "date": "2024-07-22", "relevancy": 1.6903, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5851}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5414}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Multi-modal%20Representations%20by%20Watching%20Hundreds%20of%20Surgical%0A%20%20Video%20Lectures&body=Title%3A%20Learning%20Multi-modal%20Representations%20by%20Watching%20Hundreds%20of%20Surgical%0A%20%20Video%20Lectures%0AAuthor%3A%20Kun%20Yuan%20and%20Vinkle%20Srivastav%20and%20Tong%20Yu%20and%20Joel%20L.%20Lavanchy%20and%20Pietro%20Mascagni%20and%20Nassir%20Navab%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Recent%20advancements%20in%20surgical%20computer%20vision%20have%20been%20driven%20by%0Avision-only%20models%2C%20which%20lack%20language%20semantics%2C%20relying%20on%20manually%0Aannotated%20videos%20to%20predict%20fixed%20object%20categories.%20This%20limits%20their%0Ageneralizability%20to%20unseen%20surgical%20procedures%20and%20tasks.%20We%20propose%20leveraging%0Asurgical%20video%20lectures%20from%20e-learning%20platforms%20to%20provide%20effective%20vision%0Aand%20language%20supervisory%20signals%20for%20multi-modal%20representation%20learning%2C%0Abypassing%20manual%20annotations.%20We%20address%20surgery-specific%20linguistic%20challenges%0Ausing%20multiple%20automatic%20speech%20recognition%20systems%20for%20text%20transcriptions.%20We%0Aintroduce%20SurgVLP%20-%20Surgical%20Vision%20Language%20Pre-training%20-%20a%20novel%20method%20for%0Amulti-modal%20representation%20learning.%20SurgVLP%20employs%20a%20new%20contrastive%20learning%0Aobjective%2C%20aligning%20video%20clip%20embeddings%20with%20corresponding%20multiple%20text%0Aembeddings%20in%20a%20joint%20latent%20space.%20We%20demonstrate%20the%20representational%0Acapability%20of%20this%20space%20through%20several%20vision-and-language%20surgical%20tasks%20and%0Avision-only%20tasks%20specific%20to%20surgery.%20Unlike%20current%20fully%20supervised%0Aapproaches%2C%20SurgVLP%20adapts%20to%20different%20surgical%20procedures%20and%20tasks%20without%0Aspecific%20fine-tuning%2C%20achieving%20zero-shot%20adaptation%20to%20tasks%20such%20as%20surgical%0Atool%2C%20phase%2C%20and%20triplet%20recognition%20without%20manual%20annotation.%20These%20results%0Ahighlight%20the%20transferability%20and%20versatility%20of%20the%20learned%20multi-modal%0Arepresentations%20in%20surgical%20video%20analysis.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/CAMMA-public/SurgVLP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.15220v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Multi-modal%2520Representations%2520by%2520Watching%2520Hundreds%2520of%2520Surgical%250A%2520%2520Video%2520Lectures%26entry.906535625%3DKun%2520Yuan%2520and%2520Vinkle%2520Srivastav%2520and%2520Tong%2520Yu%2520and%2520Joel%2520L.%2520Lavanchy%2520and%2520Pietro%2520Mascagni%2520and%2520Nassir%2520Navab%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520surgical%2520computer%2520vision%2520have%2520been%2520driven%2520by%250Avision-only%2520models%252C%2520which%2520lack%2520language%2520semantics%252C%2520relying%2520on%2520manually%250Aannotated%2520videos%2520to%2520predict%2520fixed%2520object%2520categories.%2520This%2520limits%2520their%250Ageneralizability%2520to%2520unseen%2520surgical%2520procedures%2520and%2520tasks.%2520We%2520propose%2520leveraging%250Asurgical%2520video%2520lectures%2520from%2520e-learning%2520platforms%2520to%2520provide%2520effective%2520vision%250Aand%2520language%2520supervisory%2520signals%2520for%2520multi-modal%2520representation%2520learning%252C%250Abypassing%2520manual%2520annotations.%2520We%2520address%2520surgery-specific%2520linguistic%2520challenges%250Ausing%2520multiple%2520automatic%2520speech%2520recognition%2520systems%2520for%2520text%2520transcriptions.%2520We%250Aintroduce%2520SurgVLP%2520-%2520Surgical%2520Vision%2520Language%2520Pre-training%2520-%2520a%2520novel%2520method%2520for%250Amulti-modal%2520representation%2520learning.%2520SurgVLP%2520employs%2520a%2520new%2520contrastive%2520learning%250Aobjective%252C%2520aligning%2520video%2520clip%2520embeddings%2520with%2520corresponding%2520multiple%2520text%250Aembeddings%2520in%2520a%2520joint%2520latent%2520space.%2520We%2520demonstrate%2520the%2520representational%250Acapability%2520of%2520this%2520space%2520through%2520several%2520vision-and-language%2520surgical%2520tasks%2520and%250Avision-only%2520tasks%2520specific%2520to%2520surgery.%2520Unlike%2520current%2520fully%2520supervised%250Aapproaches%252C%2520SurgVLP%2520adapts%2520to%2520different%2520surgical%2520procedures%2520and%2520tasks%2520without%250Aspecific%2520fine-tuning%252C%2520achieving%2520zero-shot%2520adaptation%2520to%2520tasks%2520such%2520as%2520surgical%250Atool%252C%2520phase%252C%2520and%2520triplet%2520recognition%2520without%2520manual%2520annotation.%2520These%2520results%250Ahighlight%2520the%2520transferability%2520and%2520versatility%2520of%2520the%2520learned%2520multi-modal%250Arepresentations%2520in%2520surgical%2520video%2520analysis.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/CAMMA-public/SurgVLP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.15220v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Multi-modal%20Representations%20by%20Watching%20Hundreds%20of%20Surgical%0A%20%20Video%20Lectures&entry.906535625=Kun%20Yuan%20and%20Vinkle%20Srivastav%20and%20Tong%20Yu%20and%20Joel%20L.%20Lavanchy%20and%20Pietro%20Mascagni%20and%20Nassir%20Navab%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Recent%20advancements%20in%20surgical%20computer%20vision%20have%20been%20driven%20by%0Avision-only%20models%2C%20which%20lack%20language%20semantics%2C%20relying%20on%20manually%0Aannotated%20videos%20to%20predict%20fixed%20object%20categories.%20This%20limits%20their%0Ageneralizability%20to%20unseen%20surgical%20procedures%20and%20tasks.%20We%20propose%20leveraging%0Asurgical%20video%20lectures%20from%20e-learning%20platforms%20to%20provide%20effective%20vision%0Aand%20language%20supervisory%20signals%20for%20multi-modal%20representation%20learning%2C%0Abypassing%20manual%20annotations.%20We%20address%20surgery-specific%20linguistic%20challenges%0Ausing%20multiple%20automatic%20speech%20recognition%20systems%20for%20text%20transcriptions.%20We%0Aintroduce%20SurgVLP%20-%20Surgical%20Vision%20Language%20Pre-training%20-%20a%20novel%20method%20for%0Amulti-modal%20representation%20learning.%20SurgVLP%20employs%20a%20new%20contrastive%20learning%0Aobjective%2C%20aligning%20video%20clip%20embeddings%20with%20corresponding%20multiple%20text%0Aembeddings%20in%20a%20joint%20latent%20space.%20We%20demonstrate%20the%20representational%0Acapability%20of%20this%20space%20through%20several%20vision-and-language%20surgical%20tasks%20and%0Avision-only%20tasks%20specific%20to%20surgery.%20Unlike%20current%20fully%20supervised%0Aapproaches%2C%20SurgVLP%20adapts%20to%20different%20surgical%20procedures%20and%20tasks%20without%0Aspecific%20fine-tuning%2C%20achieving%20zero-shot%20adaptation%20to%20tasks%20such%20as%20surgical%0Atool%2C%20phase%2C%20and%20triplet%20recognition%20without%20manual%20annotation.%20These%20results%0Ahighlight%20the%20transferability%20and%20versatility%20of%20the%20learned%20multi-modal%0Arepresentations%20in%20surgical%20video%20analysis.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/CAMMA-public/SurgVLP%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.15220v3&entry.124074799=Read"},
{"title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec\n  Language Models", "author": "Shahan Nercessian and Johannes Imort and Ninon Devis and Frederik Blang", "abstract": "  In this paper, we propose and investigate the use of neural audio codec\nlanguage models for the automatic generation of sample-based musical\ninstruments based on text or reference audio prompts. Our approach extends a\ngenerative audio framework to condition on pitch across an 88-key spectrum,\nvelocity, and a combined text/audio embedding. We identify maintaining timbral\nconsistency within the generated instruments as a major challenge. To tackle\nthis issue, we introduce three distinct conditioning schemes. We analyze our\nmethods through objective metrics and human listening tests, demonstrating that\nour approach can produce compelling musical instruments. Specifically, we\nintroduce a new objective metric to evaluate the timbral consistency of the\ngenerated instruments and adapt the average Contrastive Language-Audio\nPretraining (CLAP) score for the text-to-instrument case, noting that its naive\napplication is unsuitable for assessing this task. Our findings reveal a\ncomplex interplay between timbral consistency, the quality of generated\nsamples, and their correspondence to the input prompt.\n", "link": "http://arxiv.org/abs/2407.15641v1", "date": "2024-07-22", "relevancy": 1.9723, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4994}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4908}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Sample-Based%20Musical%20Instruments%20Using%20Neural%20Audio%20Codec%0A%20%20Language%20Models&body=Title%3A%20Generating%20Sample-Based%20Musical%20Instruments%20Using%20Neural%20Audio%20Codec%0A%20%20Language%20Models%0AAuthor%3A%20Shahan%20Nercessian%20and%20Johannes%20Imort%20and%20Ninon%20Devis%20and%20Frederik%20Blang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20and%20investigate%20the%20use%20of%20neural%20audio%20codec%0Alanguage%20models%20for%20the%20automatic%20generation%20of%20sample-based%20musical%0Ainstruments%20based%20on%20text%20or%20reference%20audio%20prompts.%20Our%20approach%20extends%20a%0Agenerative%20audio%20framework%20to%20condition%20on%20pitch%20across%20an%2088-key%20spectrum%2C%0Avelocity%2C%20and%20a%20combined%20text/audio%20embedding.%20We%20identify%20maintaining%20timbral%0Aconsistency%20within%20the%20generated%20instruments%20as%20a%20major%20challenge.%20To%20tackle%0Athis%20issue%2C%20we%20introduce%20three%20distinct%20conditioning%20schemes.%20We%20analyze%20our%0Amethods%20through%20objective%20metrics%20and%20human%20listening%20tests%2C%20demonstrating%20that%0Aour%20approach%20can%20produce%20compelling%20musical%20instruments.%20Specifically%2C%20we%0Aintroduce%20a%20new%20objective%20metric%20to%20evaluate%20the%20timbral%20consistency%20of%20the%0Agenerated%20instruments%20and%20adapt%20the%20average%20Contrastive%20Language-Audio%0APretraining%20%28CLAP%29%20score%20for%20the%20text-to-instrument%20case%2C%20noting%20that%20its%20naive%0Aapplication%20is%20unsuitable%20for%20assessing%20this%20task.%20Our%20findings%20reveal%20a%0Acomplex%20interplay%20between%20timbral%20consistency%2C%20the%20quality%20of%20generated%0Asamples%2C%20and%20their%20correspondence%20to%20the%20input%20prompt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Sample-Based%2520Musical%2520Instruments%2520Using%2520Neural%2520Audio%2520Codec%250A%2520%2520Language%2520Models%26entry.906535625%3DShahan%2520Nercessian%2520and%2520Johannes%2520Imort%2520and%2520Ninon%2520Devis%2520and%2520Frederik%2520Blang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520and%2520investigate%2520the%2520use%2520of%2520neural%2520audio%2520codec%250Alanguage%2520models%2520for%2520the%2520automatic%2520generation%2520of%2520sample-based%2520musical%250Ainstruments%2520based%2520on%2520text%2520or%2520reference%2520audio%2520prompts.%2520Our%2520approach%2520extends%2520a%250Agenerative%2520audio%2520framework%2520to%2520condition%2520on%2520pitch%2520across%2520an%252088-key%2520spectrum%252C%250Avelocity%252C%2520and%2520a%2520combined%2520text/audio%2520embedding.%2520We%2520identify%2520maintaining%2520timbral%250Aconsistency%2520within%2520the%2520generated%2520instruments%2520as%2520a%2520major%2520challenge.%2520To%2520tackle%250Athis%2520issue%252C%2520we%2520introduce%2520three%2520distinct%2520conditioning%2520schemes.%2520We%2520analyze%2520our%250Amethods%2520through%2520objective%2520metrics%2520and%2520human%2520listening%2520tests%252C%2520demonstrating%2520that%250Aour%2520approach%2520can%2520produce%2520compelling%2520musical%2520instruments.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520new%2520objective%2520metric%2520to%2520evaluate%2520the%2520timbral%2520consistency%2520of%2520the%250Agenerated%2520instruments%2520and%2520adapt%2520the%2520average%2520Contrastive%2520Language-Audio%250APretraining%2520%2528CLAP%2529%2520score%2520for%2520the%2520text-to-instrument%2520case%252C%2520noting%2520that%2520its%2520naive%250Aapplication%2520is%2520unsuitable%2520for%2520assessing%2520this%2520task.%2520Our%2520findings%2520reveal%2520a%250Acomplex%2520interplay%2520between%2520timbral%2520consistency%252C%2520the%2520quality%2520of%2520generated%250Asamples%252C%2520and%2520their%2520correspondence%2520to%2520the%2520input%2520prompt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Sample-Based%20Musical%20Instruments%20Using%20Neural%20Audio%20Codec%0A%20%20Language%20Models&entry.906535625=Shahan%20Nercessian%20and%20Johannes%20Imort%20and%20Ninon%20Devis%20and%20Frederik%20Blang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20and%20investigate%20the%20use%20of%20neural%20audio%20codec%0Alanguage%20models%20for%20the%20automatic%20generation%20of%20sample-based%20musical%0Ainstruments%20based%20on%20text%20or%20reference%20audio%20prompts.%20Our%20approach%20extends%20a%0Agenerative%20audio%20framework%20to%20condition%20on%20pitch%20across%20an%2088-key%20spectrum%2C%0Avelocity%2C%20and%20a%20combined%20text/audio%20embedding.%20We%20identify%20maintaining%20timbral%0Aconsistency%20within%20the%20generated%20instruments%20as%20a%20major%20challenge.%20To%20tackle%0Athis%20issue%2C%20we%20introduce%20three%20distinct%20conditioning%20schemes.%20We%20analyze%20our%0Amethods%20through%20objective%20metrics%20and%20human%20listening%20tests%2C%20demonstrating%20that%0Aour%20approach%20can%20produce%20compelling%20musical%20instruments.%20Specifically%2C%20we%0Aintroduce%20a%20new%20objective%20metric%20to%20evaluate%20the%20timbral%20consistency%20of%20the%0Agenerated%20instruments%20and%20adapt%20the%20average%20Contrastive%20Language-Audio%0APretraining%20%28CLAP%29%20score%20for%20the%20text-to-instrument%20case%2C%20noting%20that%20its%20naive%0Aapplication%20is%20unsuitable%20for%20assessing%20this%20task.%20Our%20findings%20reveal%20a%0Acomplex%20interplay%20between%20timbral%20consistency%2C%20the%20quality%20of%20generated%0Asamples%2C%20and%20their%20correspondence%20to%20the%20input%20prompt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15641v1&entry.124074799=Read"},
{"title": "Concept-Based Interpretable Reinforcement Learning with Limited to No\n  Human Labels", "author": "Zhuorui Ye and Stephanie Milani and Geoffrey J. Gordon and Fei Fang", "abstract": "  Recent advances in reinforcement learning (RL) have predominantly leveraged\nneural network-based policies for decision-making, yet these models often lack\ninterpretability, posing challenges for stakeholder comprehension and trust.\nConcept bottleneck models offer an interpretable alternative by integrating\nhuman-understandable concepts into neural networks. However, a significant\nlimitation in prior work is the assumption that human annotations for these\nconcepts are readily available during training, necessitating continuous\nreal-time input from human annotators. To overcome this limitation, we\nintroduce a novel training scheme that enables RL algorithms to efficiently\nlearn a concept-based policy by only querying humans to label a small set of\ndata, or in the extreme case, without any human labels. Our algorithm,\nLICORICE, involves three main contributions: interleaving concept learning and\nRL training, using a concept ensembles to actively select informative data\npoints for labeling, and decorrelating the concept data with a simple strategy.\nWe show how LICORICE reduces manual labeling efforts to to 500 or fewer concept\nlabels in three environments. Finally, we present an initial study to explore\nhow we can use powerful vision-language models to infer concepts from raw\nvisual inputs without explicit labels at minimal cost to performance.\n", "link": "http://arxiv.org/abs/2407.15786v1", "date": "2024-07-22", "relevancy": 1.5003, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4984}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept-Based%20Interpretable%20Reinforcement%20Learning%20with%20Limited%20to%20No%0A%20%20Human%20Labels&body=Title%3A%20Concept-Based%20Interpretable%20Reinforcement%20Learning%20with%20Limited%20to%20No%0A%20%20Human%20Labels%0AAuthor%3A%20Zhuorui%20Ye%20and%20Stephanie%20Milani%20and%20Geoffrey%20J.%20Gordon%20and%20Fei%20Fang%0AAbstract%3A%20%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20predominantly%20leveraged%0Aneural%20network-based%20policies%20for%20decision-making%2C%20yet%20these%20models%20often%20lack%0Ainterpretability%2C%20posing%20challenges%20for%20stakeholder%20comprehension%20and%20trust.%0AConcept%20bottleneck%20models%20offer%20an%20interpretable%20alternative%20by%20integrating%0Ahuman-understandable%20concepts%20into%20neural%20networks.%20However%2C%20a%20significant%0Alimitation%20in%20prior%20work%20is%20the%20assumption%20that%20human%20annotations%20for%20these%0Aconcepts%20are%20readily%20available%20during%20training%2C%20necessitating%20continuous%0Areal-time%20input%20from%20human%20annotators.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20a%20novel%20training%20scheme%20that%20enables%20RL%20algorithms%20to%20efficiently%0Alearn%20a%20concept-based%20policy%20by%20only%20querying%20humans%20to%20label%20a%20small%20set%20of%0Adata%2C%20or%20in%20the%20extreme%20case%2C%20without%20any%20human%20labels.%20Our%20algorithm%2C%0ALICORICE%2C%20involves%20three%20main%20contributions%3A%20interleaving%20concept%20learning%20and%0ARL%20training%2C%20using%20a%20concept%20ensembles%20to%20actively%20select%20informative%20data%0Apoints%20for%20labeling%2C%20and%20decorrelating%20the%20concept%20data%20with%20a%20simple%20strategy.%0AWe%20show%20how%20LICORICE%20reduces%20manual%20labeling%20efforts%20to%20to%20500%20or%20fewer%20concept%0Alabels%20in%20three%20environments.%20Finally%2C%20we%20present%20an%20initial%20study%20to%20explore%0Ahow%20we%20can%20use%20powerful%20vision-language%20models%20to%20infer%20concepts%20from%20raw%0Avisual%20inputs%20without%20explicit%20labels%20at%20minimal%20cost%20to%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept-Based%2520Interpretable%2520Reinforcement%2520Learning%2520with%2520Limited%2520to%2520No%250A%2520%2520Human%2520Labels%26entry.906535625%3DZhuorui%2520Ye%2520and%2520Stephanie%2520Milani%2520and%2520Geoffrey%2520J.%2520Gordon%2520and%2520Fei%2520Fang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520predominantly%2520leveraged%250Aneural%2520network-based%2520policies%2520for%2520decision-making%252C%2520yet%2520these%2520models%2520often%2520lack%250Ainterpretability%252C%2520posing%2520challenges%2520for%2520stakeholder%2520comprehension%2520and%2520trust.%250AConcept%2520bottleneck%2520models%2520offer%2520an%2520interpretable%2520alternative%2520by%2520integrating%250Ahuman-understandable%2520concepts%2520into%2520neural%2520networks.%2520However%252C%2520a%2520significant%250Alimitation%2520in%2520prior%2520work%2520is%2520the%2520assumption%2520that%2520human%2520annotations%2520for%2520these%250Aconcepts%2520are%2520readily%2520available%2520during%2520training%252C%2520necessitating%2520continuous%250Areal-time%2520input%2520from%2520human%2520annotators.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aintroduce%2520a%2520novel%2520training%2520scheme%2520that%2520enables%2520RL%2520algorithms%2520to%2520efficiently%250Alearn%2520a%2520concept-based%2520policy%2520by%2520only%2520querying%2520humans%2520to%2520label%2520a%2520small%2520set%2520of%250Adata%252C%2520or%2520in%2520the%2520extreme%2520case%252C%2520without%2520any%2520human%2520labels.%2520Our%2520algorithm%252C%250ALICORICE%252C%2520involves%2520three%2520main%2520contributions%253A%2520interleaving%2520concept%2520learning%2520and%250ARL%2520training%252C%2520using%2520a%2520concept%2520ensembles%2520to%2520actively%2520select%2520informative%2520data%250Apoints%2520for%2520labeling%252C%2520and%2520decorrelating%2520the%2520concept%2520data%2520with%2520a%2520simple%2520strategy.%250AWe%2520show%2520how%2520LICORICE%2520reduces%2520manual%2520labeling%2520efforts%2520to%2520to%2520500%2520or%2520fewer%2520concept%250Alabels%2520in%2520three%2520environments.%2520Finally%252C%2520we%2520present%2520an%2520initial%2520study%2520to%2520explore%250Ahow%2520we%2520can%2520use%2520powerful%2520vision-language%2520models%2520to%2520infer%2520concepts%2520from%2520raw%250Avisual%2520inputs%2520without%2520explicit%2520labels%2520at%2520minimal%2520cost%2520to%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept-Based%20Interpretable%20Reinforcement%20Learning%20with%20Limited%20to%20No%0A%20%20Human%20Labels&entry.906535625=Zhuorui%20Ye%20and%20Stephanie%20Milani%20and%20Geoffrey%20J.%20Gordon%20and%20Fei%20Fang&entry.1292438233=%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20predominantly%20leveraged%0Aneural%20network-based%20policies%20for%20decision-making%2C%20yet%20these%20models%20often%20lack%0Ainterpretability%2C%20posing%20challenges%20for%20stakeholder%20comprehension%20and%20trust.%0AConcept%20bottleneck%20models%20offer%20an%20interpretable%20alternative%20by%20integrating%0Ahuman-understandable%20concepts%20into%20neural%20networks.%20However%2C%20a%20significant%0Alimitation%20in%20prior%20work%20is%20the%20assumption%20that%20human%20annotations%20for%20these%0Aconcepts%20are%20readily%20available%20during%20training%2C%20necessitating%20continuous%0Areal-time%20input%20from%20human%20annotators.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20a%20novel%20training%20scheme%20that%20enables%20RL%20algorithms%20to%20efficiently%0Alearn%20a%20concept-based%20policy%20by%20only%20querying%20humans%20to%20label%20a%20small%20set%20of%0Adata%2C%20or%20in%20the%20extreme%20case%2C%20without%20any%20human%20labels.%20Our%20algorithm%2C%0ALICORICE%2C%20involves%20three%20main%20contributions%3A%20interleaving%20concept%20learning%20and%0ARL%20training%2C%20using%20a%20concept%20ensembles%20to%20actively%20select%20informative%20data%0Apoints%20for%20labeling%2C%20and%20decorrelating%20the%20concept%20data%20with%20a%20simple%20strategy.%0AWe%20show%20how%20LICORICE%20reduces%20manual%20labeling%20efforts%20to%20to%20500%20or%20fewer%20concept%0Alabels%20in%20three%20environments.%20Finally%2C%20we%20present%20an%20initial%20study%20to%20explore%0Ahow%20we%20can%20use%20powerful%20vision-language%20models%20to%20infer%20concepts%20from%20raw%0Avisual%20inputs%20without%20explicit%20labels%20at%20minimal%20cost%20to%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15786v1&entry.124074799=Read"},
{"title": "Estimating Probability Densities with Transformer and Denoising\n  Diffusion", "author": "Henry W. Leung and Jo Bovy and Joshua S. Speagle", "abstract": "  Transformers are often the go-to architecture to build foundation models that\ningest a large amount of training data. But these models do not estimate the\nprobability density distribution when trained on regression problems, yet\nobtaining full probabilistic outputs is crucial to many fields of science,\nwhere the probability distribution of the answer can be non-Gaussian and\nmultimodal. In this work, we demonstrate that training a probabilistic model\nusing a denoising diffusion head on top of the Transformer provides reasonable\nprobability density estimation even for high-dimensional inputs. The combined\nTransformer+Denoising Diffusion model allows conditioning the output\nprobability density on arbitrary combinations of inputs and it is thus a highly\nflexible density function emulator of all possible input/output combinations.\nWe illustrate our Transformer+Denoising Diffusion model by training it on a\nlarge dataset of astronomical observations and measured labels of stars within\nour Galaxy and we apply it to a variety of inference tasks to show that the\nmodel can infer labels accurately with reasonable distributions.\n", "link": "http://arxiv.org/abs/2407.15703v1", "date": "2024-07-22", "relevancy": 1.6669, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6267}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5397}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Probability%20Densities%20with%20Transformer%20and%20Denoising%0A%20%20Diffusion&body=Title%3A%20Estimating%20Probability%20Densities%20with%20Transformer%20and%20Denoising%0A%20%20Diffusion%0AAuthor%3A%20Henry%20W.%20Leung%20and%20Jo%20Bovy%20and%20Joshua%20S.%20Speagle%0AAbstract%3A%20%20%20Transformers%20are%20often%20the%20go-to%20architecture%20to%20build%20foundation%20models%20that%0Aingest%20a%20large%20amount%20of%20training%20data.%20But%20these%20models%20do%20not%20estimate%20the%0Aprobability%20density%20distribution%20when%20trained%20on%20regression%20problems%2C%20yet%0Aobtaining%20full%20probabilistic%20outputs%20is%20crucial%20to%20many%20fields%20of%20science%2C%0Awhere%20the%20probability%20distribution%20of%20the%20answer%20can%20be%20non-Gaussian%20and%0Amultimodal.%20In%20this%20work%2C%20we%20demonstrate%20that%20training%20a%20probabilistic%20model%0Ausing%20a%20denoising%20diffusion%20head%20on%20top%20of%20the%20Transformer%20provides%20reasonable%0Aprobability%20density%20estimation%20even%20for%20high-dimensional%20inputs.%20The%20combined%0ATransformer%2BDenoising%20Diffusion%20model%20allows%20conditioning%20the%20output%0Aprobability%20density%20on%20arbitrary%20combinations%20of%20inputs%20and%20it%20is%20thus%20a%20highly%0Aflexible%20density%20function%20emulator%20of%20all%20possible%20input/output%20combinations.%0AWe%20illustrate%20our%20Transformer%2BDenoising%20Diffusion%20model%20by%20training%20it%20on%20a%0Alarge%20dataset%20of%20astronomical%20observations%20and%20measured%20labels%20of%20stars%20within%0Aour%20Galaxy%20and%20we%20apply%20it%20to%20a%20variety%20of%20inference%20tasks%20to%20show%20that%20the%0Amodel%20can%20infer%20labels%20accurately%20with%20reasonable%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Probability%2520Densities%2520with%2520Transformer%2520and%2520Denoising%250A%2520%2520Diffusion%26entry.906535625%3DHenry%2520W.%2520Leung%2520and%2520Jo%2520Bovy%2520and%2520Joshua%2520S.%2520Speagle%26entry.1292438233%3D%2520%2520Transformers%2520are%2520often%2520the%2520go-to%2520architecture%2520to%2520build%2520foundation%2520models%2520that%250Aingest%2520a%2520large%2520amount%2520of%2520training%2520data.%2520But%2520these%2520models%2520do%2520not%2520estimate%2520the%250Aprobability%2520density%2520distribution%2520when%2520trained%2520on%2520regression%2520problems%252C%2520yet%250Aobtaining%2520full%2520probabilistic%2520outputs%2520is%2520crucial%2520to%2520many%2520fields%2520of%2520science%252C%250Awhere%2520the%2520probability%2520distribution%2520of%2520the%2520answer%2520can%2520be%2520non-Gaussian%2520and%250Amultimodal.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520training%2520a%2520probabilistic%2520model%250Ausing%2520a%2520denoising%2520diffusion%2520head%2520on%2520top%2520of%2520the%2520Transformer%2520provides%2520reasonable%250Aprobability%2520density%2520estimation%2520even%2520for%2520high-dimensional%2520inputs.%2520The%2520combined%250ATransformer%252BDenoising%2520Diffusion%2520model%2520allows%2520conditioning%2520the%2520output%250Aprobability%2520density%2520on%2520arbitrary%2520combinations%2520of%2520inputs%2520and%2520it%2520is%2520thus%2520a%2520highly%250Aflexible%2520density%2520function%2520emulator%2520of%2520all%2520possible%2520input/output%2520combinations.%250AWe%2520illustrate%2520our%2520Transformer%252BDenoising%2520Diffusion%2520model%2520by%2520training%2520it%2520on%2520a%250Alarge%2520dataset%2520of%2520astronomical%2520observations%2520and%2520measured%2520labels%2520of%2520stars%2520within%250Aour%2520Galaxy%2520and%2520we%2520apply%2520it%2520to%2520a%2520variety%2520of%2520inference%2520tasks%2520to%2520show%2520that%2520the%250Amodel%2520can%2520infer%2520labels%2520accurately%2520with%2520reasonable%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Probability%20Densities%20with%20Transformer%20and%20Denoising%0A%20%20Diffusion&entry.906535625=Henry%20W.%20Leung%20and%20Jo%20Bovy%20and%20Joshua%20S.%20Speagle&entry.1292438233=%20%20Transformers%20are%20often%20the%20go-to%20architecture%20to%20build%20foundation%20models%20that%0Aingest%20a%20large%20amount%20of%20training%20data.%20But%20these%20models%20do%20not%20estimate%20the%0Aprobability%20density%20distribution%20when%20trained%20on%20regression%20problems%2C%20yet%0Aobtaining%20full%20probabilistic%20outputs%20is%20crucial%20to%20many%20fields%20of%20science%2C%0Awhere%20the%20probability%20distribution%20of%20the%20answer%20can%20be%20non-Gaussian%20and%0Amultimodal.%20In%20this%20work%2C%20we%20demonstrate%20that%20training%20a%20probabilistic%20model%0Ausing%20a%20denoising%20diffusion%20head%20on%20top%20of%20the%20Transformer%20provides%20reasonable%0Aprobability%20density%20estimation%20even%20for%20high-dimensional%20inputs.%20The%20combined%0ATransformer%2BDenoising%20Diffusion%20model%20allows%20conditioning%20the%20output%0Aprobability%20density%20on%20arbitrary%20combinations%20of%20inputs%20and%20it%20is%20thus%20a%20highly%0Aflexible%20density%20function%20emulator%20of%20all%20possible%20input/output%20combinations.%0AWe%20illustrate%20our%20Transformer%2BDenoising%20Diffusion%20model%20by%20training%20it%20on%20a%0Alarge%20dataset%20of%20astronomical%20observations%20and%20measured%20labels%20of%20stars%20within%0Aour%20Galaxy%20and%20we%20apply%20it%20to%20a%20variety%20of%20inference%20tasks%20to%20show%20that%20the%0Amodel%20can%20infer%20labels%20accurately%20with%20reasonable%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15703v1&entry.124074799=Read"},
{"title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents", "author": "Anthony Costarelli and Mat Allen and Roman Hauksson and Grace Sodunke and Suhas Hariharan and Carlson Cheng and Wenjie Li and Joshua Clymer and Arjun Yadav", "abstract": "  Large language models have demonstrated remarkable few-shot performance on\nmany natural language understanding tasks. Despite several demonstrations of\nusing large language models in complex, strategic scenarios, there lacks a\ncomprehensive framework for evaluating agents' performance across various types\nof reasoning found in games. To address this gap, we introduce GameBench, a\ncross-domain benchmark for evaluating strategic reasoning abilities of LLM\nagents. We focus on 9 different game environments, where each covers at least\none axis of key reasoning skill identified in strategy games, and select games\nfor which strategy explanations are unlikely to form a significant portion of\nmodels' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base\nform along with two scaffolding frameworks designed to enhance strategic\nreasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning\n(RAP). Our results show that none of the tested models match human performance,\nand at worst GPT-4 performs worse than random action. CoT and RAP both improve\nscores but not comparable to human levels.\n", "link": "http://arxiv.org/abs/2406.06613v2", "date": "2024-07-22", "relevancy": 1.4575, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5133}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4955}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GameBench%3A%20Evaluating%20Strategic%20Reasoning%20Abilities%20of%20LLM%20Agents&body=Title%3A%20GameBench%3A%20Evaluating%20Strategic%20Reasoning%20Abilities%20of%20LLM%20Agents%0AAuthor%3A%20Anthony%20Costarelli%20and%20Mat%20Allen%20and%20Roman%20Hauksson%20and%20Grace%20Sodunke%20and%20Suhas%20Hariharan%20and%20Carlson%20Cheng%20and%20Wenjie%20Li%20and%20Joshua%20Clymer%20and%20Arjun%20Yadav%0AAbstract%3A%20%20%20Large%20language%20models%20have%20demonstrated%20remarkable%20few-shot%20performance%20on%0Amany%20natural%20language%20understanding%20tasks.%20Despite%20several%20demonstrations%20of%0Ausing%20large%20language%20models%20in%20complex%2C%20strategic%20scenarios%2C%20there%20lacks%20a%0Acomprehensive%20framework%20for%20evaluating%20agents%27%20performance%20across%20various%20types%0Aof%20reasoning%20found%20in%20games.%20To%20address%20this%20gap%2C%20we%20introduce%20GameBench%2C%20a%0Across-domain%20benchmark%20for%20evaluating%20strategic%20reasoning%20abilities%20of%20LLM%0Aagents.%20We%20focus%20on%209%20different%20game%20environments%2C%20where%20each%20covers%20at%20least%0Aone%20axis%20of%20key%20reasoning%20skill%20identified%20in%20strategy%20games%2C%20and%20select%20games%0Afor%20which%20strategy%20explanations%20are%20unlikely%20to%20form%20a%20significant%20portion%20of%0Amodels%27%20pretraining%20corpuses.%20Our%20evaluations%20use%20GPT-3%20and%20GPT-4%20in%20their%20base%0Aform%20along%20with%20two%20scaffolding%20frameworks%20designed%20to%20enhance%20strategic%0Areasoning%20ability%3A%20Chain-of-Thought%20%28CoT%29%20prompting%20and%20Reasoning%20Via%20Planning%0A%28RAP%29.%20Our%20results%20show%20that%20none%20of%20the%20tested%20models%20match%20human%20performance%2C%0Aand%20at%20worst%20GPT-4%20performs%20worse%20than%20random%20action.%20CoT%20and%20RAP%20both%20improve%0Ascores%20but%20not%20comparable%20to%20human%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGameBench%253A%2520Evaluating%2520Strategic%2520Reasoning%2520Abilities%2520of%2520LLM%2520Agents%26entry.906535625%3DAnthony%2520Costarelli%2520and%2520Mat%2520Allen%2520and%2520Roman%2520Hauksson%2520and%2520Grace%2520Sodunke%2520and%2520Suhas%2520Hariharan%2520and%2520Carlson%2520Cheng%2520and%2520Wenjie%2520Li%2520and%2520Joshua%2520Clymer%2520and%2520Arjun%2520Yadav%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520demonstrated%2520remarkable%2520few-shot%2520performance%2520on%250Amany%2520natural%2520language%2520understanding%2520tasks.%2520Despite%2520several%2520demonstrations%2520of%250Ausing%2520large%2520language%2520models%2520in%2520complex%252C%2520strategic%2520scenarios%252C%2520there%2520lacks%2520a%250Acomprehensive%2520framework%2520for%2520evaluating%2520agents%2527%2520performance%2520across%2520various%2520types%250Aof%2520reasoning%2520found%2520in%2520games.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520GameBench%252C%2520a%250Across-domain%2520benchmark%2520for%2520evaluating%2520strategic%2520reasoning%2520abilities%2520of%2520LLM%250Aagents.%2520We%2520focus%2520on%25209%2520different%2520game%2520environments%252C%2520where%2520each%2520covers%2520at%2520least%250Aone%2520axis%2520of%2520key%2520reasoning%2520skill%2520identified%2520in%2520strategy%2520games%252C%2520and%2520select%2520games%250Afor%2520which%2520strategy%2520explanations%2520are%2520unlikely%2520to%2520form%2520a%2520significant%2520portion%2520of%250Amodels%2527%2520pretraining%2520corpuses.%2520Our%2520evaluations%2520use%2520GPT-3%2520and%2520GPT-4%2520in%2520their%2520base%250Aform%2520along%2520with%2520two%2520scaffolding%2520frameworks%2520designed%2520to%2520enhance%2520strategic%250Areasoning%2520ability%253A%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520and%2520Reasoning%2520Via%2520Planning%250A%2528RAP%2529.%2520Our%2520results%2520show%2520that%2520none%2520of%2520the%2520tested%2520models%2520match%2520human%2520performance%252C%250Aand%2520at%2520worst%2520GPT-4%2520performs%2520worse%2520than%2520random%2520action.%2520CoT%2520and%2520RAP%2520both%2520improve%250Ascores%2520but%2520not%2520comparable%2520to%2520human%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GameBench%3A%20Evaluating%20Strategic%20Reasoning%20Abilities%20of%20LLM%20Agents&entry.906535625=Anthony%20Costarelli%20and%20Mat%20Allen%20and%20Roman%20Hauksson%20and%20Grace%20Sodunke%20and%20Suhas%20Hariharan%20and%20Carlson%20Cheng%20and%20Wenjie%20Li%20and%20Joshua%20Clymer%20and%20Arjun%20Yadav&entry.1292438233=%20%20Large%20language%20models%20have%20demonstrated%20remarkable%20few-shot%20performance%20on%0Amany%20natural%20language%20understanding%20tasks.%20Despite%20several%20demonstrations%20of%0Ausing%20large%20language%20models%20in%20complex%2C%20strategic%20scenarios%2C%20there%20lacks%20a%0Acomprehensive%20framework%20for%20evaluating%20agents%27%20performance%20across%20various%20types%0Aof%20reasoning%20found%20in%20games.%20To%20address%20this%20gap%2C%20we%20introduce%20GameBench%2C%20a%0Across-domain%20benchmark%20for%20evaluating%20strategic%20reasoning%20abilities%20of%20LLM%0Aagents.%20We%20focus%20on%209%20different%20game%20environments%2C%20where%20each%20covers%20at%20least%0Aone%20axis%20of%20key%20reasoning%20skill%20identified%20in%20strategy%20games%2C%20and%20select%20games%0Afor%20which%20strategy%20explanations%20are%20unlikely%20to%20form%20a%20significant%20portion%20of%0Amodels%27%20pretraining%20corpuses.%20Our%20evaluations%20use%20GPT-3%20and%20GPT-4%20in%20their%20base%0Aform%20along%20with%20two%20scaffolding%20frameworks%20designed%20to%20enhance%20strategic%0Areasoning%20ability%3A%20Chain-of-Thought%20%28CoT%29%20prompting%20and%20Reasoning%20Via%20Planning%0A%28RAP%29.%20Our%20results%20show%20that%20none%20of%20the%20tested%20models%20match%20human%20performance%2C%0Aand%20at%20worst%20GPT-4%20performs%20worse%20than%20random%20action.%20CoT%20and%20RAP%20both%20improve%0Ascores%20but%20not%20comparable%20to%20human%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06613v2&entry.124074799=Read"},
{"title": "A Simple and Optimal Policy Design with Safety against Heavy-Tailed Risk\n  for Stochastic Bandits", "author": "David Simchi-Levi and Zeyu Zheng and Feng Zhu", "abstract": "  We study the stochastic multi-armed bandit problem and design new policies\nthat enjoy both worst-case optimality for expected regret and light-tailed risk\nfor regret distribution. Specifically, our policy design (i) enjoys the\nworst-case optimality for the expected regret at order $O(\\sqrt{KT\\ln T})$ and\n(ii) has the worst-case tail probability of incurring a regret larger than any\n$x>0$ being upper bounded by $\\exp(-\\Omega(x/\\sqrt{KT}))$, a rate that we prove\nto be best achievable with respect to $T$ for all worst-case optimal policies.\nOur proposed policy achieves a delicate balance between doing more exploration\nat the beginning of the time horizon and doing more exploitation when\napproaching the end, compared to standard confidence-bound-based policies. We\nalso enhance the policy design to accommodate the \"any-time\" setting where $T$\nis unknown a priori, and prove equivalently desired policy performances as\ncompared to the \"fixed-time\" setting with known $T$. Numerical experiments are\nconducted to illustrate the theoretical findings. We find that from a\nmanagerial perspective, our new policy design yields better tail distributions\nand is preferable than celebrated policies especially when (i) there is a risk\nof under-estimating the volatility profile, or (ii) there is a challenge of\ntuning policy hyper-parameters. We conclude by extending our proposed policy\ndesign to the stochastic linear bandit setting that leads to both worst-case\noptimality in terms of expected regret and light-tailed risk on the regret\ndistribution.\n", "link": "http://arxiv.org/abs/2206.02969v6", "date": "2024-07-22", "relevancy": 1.7032, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4499}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4393}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20and%20Optimal%20Policy%20Design%20with%20Safety%20against%20Heavy-Tailed%20Risk%0A%20%20for%20Stochastic%20Bandits&body=Title%3A%20A%20Simple%20and%20Optimal%20Policy%20Design%20with%20Safety%20against%20Heavy-Tailed%20Risk%0A%20%20for%20Stochastic%20Bandits%0AAuthor%3A%20David%20Simchi-Levi%20and%20Zeyu%20Zheng%20and%20Feng%20Zhu%0AAbstract%3A%20%20%20We%20study%20the%20stochastic%20multi-armed%20bandit%20problem%20and%20design%20new%20policies%0Athat%20enjoy%20both%20worst-case%20optimality%20for%20expected%20regret%20and%20light-tailed%20risk%0Afor%20regret%20distribution.%20Specifically%2C%20our%20policy%20design%20%28i%29%20enjoys%20the%0Aworst-case%20optimality%20for%20the%20expected%20regret%20at%20order%20%24O%28%5Csqrt%7BKT%5Cln%20T%7D%29%24%20and%0A%28ii%29%20has%20the%20worst-case%20tail%20probability%20of%20incurring%20a%20regret%20larger%20than%20any%0A%24x%3E0%24%20being%20upper%20bounded%20by%20%24%5Cexp%28-%5COmega%28x/%5Csqrt%7BKT%7D%29%29%24%2C%20a%20rate%20that%20we%20prove%0Ato%20be%20best%20achievable%20with%20respect%20to%20%24T%24%20for%20all%20worst-case%20optimal%20policies.%0AOur%20proposed%20policy%20achieves%20a%20delicate%20balance%20between%20doing%20more%20exploration%0Aat%20the%20beginning%20of%20the%20time%20horizon%20and%20doing%20more%20exploitation%20when%0Aapproaching%20the%20end%2C%20compared%20to%20standard%20confidence-bound-based%20policies.%20We%0Aalso%20enhance%20the%20policy%20design%20to%20accommodate%20the%20%22any-time%22%20setting%20where%20%24T%24%0Ais%20unknown%20a%20priori%2C%20and%20prove%20equivalently%20desired%20policy%20performances%20as%0Acompared%20to%20the%20%22fixed-time%22%20setting%20with%20known%20%24T%24.%20Numerical%20experiments%20are%0Aconducted%20to%20illustrate%20the%20theoretical%20findings.%20We%20find%20that%20from%20a%0Amanagerial%20perspective%2C%20our%20new%20policy%20design%20yields%20better%20tail%20distributions%0Aand%20is%20preferable%20than%20celebrated%20policies%20especially%20when%20%28i%29%20there%20is%20a%20risk%0Aof%20under-estimating%20the%20volatility%20profile%2C%20or%20%28ii%29%20there%20is%20a%20challenge%20of%0Atuning%20policy%20hyper-parameters.%20We%20conclude%20by%20extending%20our%20proposed%20policy%0Adesign%20to%20the%20stochastic%20linear%20bandit%20setting%20that%20leads%20to%20both%20worst-case%0Aoptimality%20in%20terms%20of%20expected%20regret%20and%20light-tailed%20risk%20on%20the%20regret%0Adistribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.02969v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520and%2520Optimal%2520Policy%2520Design%2520with%2520Safety%2520against%2520Heavy-Tailed%2520Risk%250A%2520%2520for%2520Stochastic%2520Bandits%26entry.906535625%3DDavid%2520Simchi-Levi%2520and%2520Zeyu%2520Zheng%2520and%2520Feng%2520Zhu%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520stochastic%2520multi-armed%2520bandit%2520problem%2520and%2520design%2520new%2520policies%250Athat%2520enjoy%2520both%2520worst-case%2520optimality%2520for%2520expected%2520regret%2520and%2520light-tailed%2520risk%250Afor%2520regret%2520distribution.%2520Specifically%252C%2520our%2520policy%2520design%2520%2528i%2529%2520enjoys%2520the%250Aworst-case%2520optimality%2520for%2520the%2520expected%2520regret%2520at%2520order%2520%2524O%2528%255Csqrt%257BKT%255Cln%2520T%257D%2529%2524%2520and%250A%2528ii%2529%2520has%2520the%2520worst-case%2520tail%2520probability%2520of%2520incurring%2520a%2520regret%2520larger%2520than%2520any%250A%2524x%253E0%2524%2520being%2520upper%2520bounded%2520by%2520%2524%255Cexp%2528-%255COmega%2528x/%255Csqrt%257BKT%257D%2529%2529%2524%252C%2520a%2520rate%2520that%2520we%2520prove%250Ato%2520be%2520best%2520achievable%2520with%2520respect%2520to%2520%2524T%2524%2520for%2520all%2520worst-case%2520optimal%2520policies.%250AOur%2520proposed%2520policy%2520achieves%2520a%2520delicate%2520balance%2520between%2520doing%2520more%2520exploration%250Aat%2520the%2520beginning%2520of%2520the%2520time%2520horizon%2520and%2520doing%2520more%2520exploitation%2520when%250Aapproaching%2520the%2520end%252C%2520compared%2520to%2520standard%2520confidence-bound-based%2520policies.%2520We%250Aalso%2520enhance%2520the%2520policy%2520design%2520to%2520accommodate%2520the%2520%2522any-time%2522%2520setting%2520where%2520%2524T%2524%250Ais%2520unknown%2520a%2520priori%252C%2520and%2520prove%2520equivalently%2520desired%2520policy%2520performances%2520as%250Acompared%2520to%2520the%2520%2522fixed-time%2522%2520setting%2520with%2520known%2520%2524T%2524.%2520Numerical%2520experiments%2520are%250Aconducted%2520to%2520illustrate%2520the%2520theoretical%2520findings.%2520We%2520find%2520that%2520from%2520a%250Amanagerial%2520perspective%252C%2520our%2520new%2520policy%2520design%2520yields%2520better%2520tail%2520distributions%250Aand%2520is%2520preferable%2520than%2520celebrated%2520policies%2520especially%2520when%2520%2528i%2529%2520there%2520is%2520a%2520risk%250Aof%2520under-estimating%2520the%2520volatility%2520profile%252C%2520or%2520%2528ii%2529%2520there%2520is%2520a%2520challenge%2520of%250Atuning%2520policy%2520hyper-parameters.%2520We%2520conclude%2520by%2520extending%2520our%2520proposed%2520policy%250Adesign%2520to%2520the%2520stochastic%2520linear%2520bandit%2520setting%2520that%2520leads%2520to%2520both%2520worst-case%250Aoptimality%2520in%2520terms%2520of%2520expected%2520regret%2520and%2520light-tailed%2520risk%2520on%2520the%2520regret%250Adistribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.02969v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20and%20Optimal%20Policy%20Design%20with%20Safety%20against%20Heavy-Tailed%20Risk%0A%20%20for%20Stochastic%20Bandits&entry.906535625=David%20Simchi-Levi%20and%20Zeyu%20Zheng%20and%20Feng%20Zhu&entry.1292438233=%20%20We%20study%20the%20stochastic%20multi-armed%20bandit%20problem%20and%20design%20new%20policies%0Athat%20enjoy%20both%20worst-case%20optimality%20for%20expected%20regret%20and%20light-tailed%20risk%0Afor%20regret%20distribution.%20Specifically%2C%20our%20policy%20design%20%28i%29%20enjoys%20the%0Aworst-case%20optimality%20for%20the%20expected%20regret%20at%20order%20%24O%28%5Csqrt%7BKT%5Cln%20T%7D%29%24%20and%0A%28ii%29%20has%20the%20worst-case%20tail%20probability%20of%20incurring%20a%20regret%20larger%20than%20any%0A%24x%3E0%24%20being%20upper%20bounded%20by%20%24%5Cexp%28-%5COmega%28x/%5Csqrt%7BKT%7D%29%29%24%2C%20a%20rate%20that%20we%20prove%0Ato%20be%20best%20achievable%20with%20respect%20to%20%24T%24%20for%20all%20worst-case%20optimal%20policies.%0AOur%20proposed%20policy%20achieves%20a%20delicate%20balance%20between%20doing%20more%20exploration%0Aat%20the%20beginning%20of%20the%20time%20horizon%20and%20doing%20more%20exploitation%20when%0Aapproaching%20the%20end%2C%20compared%20to%20standard%20confidence-bound-based%20policies.%20We%0Aalso%20enhance%20the%20policy%20design%20to%20accommodate%20the%20%22any-time%22%20setting%20where%20%24T%24%0Ais%20unknown%20a%20priori%2C%20and%20prove%20equivalently%20desired%20policy%20performances%20as%0Acompared%20to%20the%20%22fixed-time%22%20setting%20with%20known%20%24T%24.%20Numerical%20experiments%20are%0Aconducted%20to%20illustrate%20the%20theoretical%20findings.%20We%20find%20that%20from%20a%0Amanagerial%20perspective%2C%20our%20new%20policy%20design%20yields%20better%20tail%20distributions%0Aand%20is%20preferable%20than%20celebrated%20policies%20especially%20when%20%28i%29%20there%20is%20a%20risk%0Aof%20under-estimating%20the%20volatility%20profile%2C%20or%20%28ii%29%20there%20is%20a%20challenge%20of%0Atuning%20policy%20hyper-parameters.%20We%20conclude%20by%20extending%20our%20proposed%20policy%0Adesign%20to%20the%20stochastic%20linear%20bandit%20setting%20that%20leads%20to%20both%20worst-case%0Aoptimality%20in%20terms%20of%20expected%20regret%20and%20light-tailed%20risk%20on%20the%20regret%0Adistribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.02969v6&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


