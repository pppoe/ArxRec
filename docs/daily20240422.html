<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240421.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Hybrid Generative and Discriminative PointNet on Unordered Point Sets", "author": "Yang Ye and Shihao Ji", "abstract": "  As point cloud provides a natural and flexible representation usable in\nmyriad applications (e.g., robotics and self-driving cars), the ability to\nsynthesize point clouds for analysis becomes crucial. Recently, Xie et al.\npropose a generative model for unordered point sets in the form of an\nenergy-based model (EBM). Despite the model achieving an impressive performance\nfor point cloud generation, one separate model needs to be trained for each\ncategory to capture the complex point set distributions. Besides, their method\nis unable to classify point clouds directly and requires additional fine-tuning\nfor classification. One interesting question is: Can we train a single network\nfor a hybrid generative and discriminative model of point clouds? A similar\nquestion has recently been answered in the affirmative for images, introducing\nthe framework of Joint Energy-based Model (JEM), which achieves high\nperformance in image classification and generation simultaneously. This paper\nproposes GDPNet, the first hybrid Generative and Discriminative PointNet that\nextends JEM for point cloud classification and generation. Our GDPNet retains\nstrong discriminative power of modern PointNet classifiers, while generating\npoint cloud samples rivaling state-of-the-art generative approaches.\n", "link": "http://arxiv.org/abs/2404.12925v1", "date": "2024-04-19", "relevancy": 2.8595, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5743}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5723}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5691}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Generative%20and%20Discriminative%20PointNet%20on%20Unordered%20Point%20Sets&body=Title%3A%20A%20Hybrid%20Generative%20and%20Discriminative%20PointNet%20on%20Unordered%20Point%20Sets%0AAuthor%3A%20Yang%20Ye%20and%20Shihao%20Ji%0AAbstract%3A%20%20%20As%20point%20cloud%20provides%20a%20natural%20and%20flexible%20representation%20usable%20in%0Amyriad%20applications%20%28e.g.%2C%20robotics%20and%20self-driving%20cars%29%2C%20the%20ability%20to%0Asynthesize%20point%20clouds%20for%20analysis%20becomes%20crucial.%20Recently%2C%20Xie%20et%20al.%0Apropose%20a%20generative%20model%20for%20unordered%20point%20sets%20in%20the%20form%20of%20an%0Aenergy-based%20model%20%28EBM%29.%20Despite%20the%20model%20achieving%20an%20impressive%20performance%0Afor%20point%20cloud%20generation%2C%20one%20separate%20model%20needs%20to%20be%20trained%20for%20each%0Acategory%20to%20capture%20the%20complex%20point%20set%20distributions.%20Besides%2C%20their%20method%0Ais%20unable%20to%20classify%20point%20clouds%20directly%20and%20requires%20additional%20fine-tuning%0Afor%20classification.%20One%20interesting%20question%20is%3A%20Can%20we%20train%20a%20single%20network%0Afor%20a%20hybrid%20generative%20and%20discriminative%20model%20of%20point%20clouds%3F%20A%20similar%0Aquestion%20has%20recently%20been%20answered%20in%20the%20affirmative%20for%20images%2C%20introducing%0Athe%20framework%20of%20Joint%20Energy-based%20Model%20%28JEM%29%2C%20which%20achieves%20high%0Aperformance%20in%20image%20classification%20and%20generation%20simultaneously.%20This%20paper%0Aproposes%20GDPNet%2C%20the%20first%20hybrid%20Generative%20and%20Discriminative%20PointNet%20that%0Aextends%20JEM%20for%20point%20cloud%20classification%20and%20generation.%20Our%20GDPNet%20retains%0Astrong%20discriminative%20power%20of%20modern%20PointNet%20classifiers%2C%20while%20generating%0Apoint%20cloud%20samples%20rivaling%20state-of-the-art%20generative%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12925v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Generative%20and%20Discriminative%20PointNet%20on%20Unordered%20Point%20Sets&entry.906535625=Yang%20Ye%20and%20Shihao%20Ji&entry.1292438233=%20%20As%20point%20cloud%20provides%20a%20natural%20and%20flexible%20representation%20usable%20in%0Amyriad%20applications%20%28e.g.%2C%20robotics%20and%20self-driving%20cars%29%2C%20the%20ability%20to%0Asynthesize%20point%20clouds%20for%20analysis%20becomes%20crucial.%20Recently%2C%20Xie%20et%20al.%0Apropose%20a%20generative%20model%20for%20unordered%20point%20sets%20in%20the%20form%20of%20an%0Aenergy-based%20model%20%28EBM%29.%20Despite%20the%20model%20achieving%20an%20impressive%20performance%0Afor%20point%20cloud%20generation%2C%20one%20separate%20model%20needs%20to%20be%20trained%20for%20each%0Acategory%20to%20capture%20the%20complex%20point%20set%20distributions.%20Besides%2C%20their%20method%0Ais%20unable%20to%20classify%20point%20clouds%20directly%20and%20requires%20additional%20fine-tuning%0Afor%20classification.%20One%20interesting%20question%20is%3A%20Can%20we%20train%20a%20single%20network%0Afor%20a%20hybrid%20generative%20and%20discriminative%20model%20of%20point%20clouds%3F%20A%20similar%0Aquestion%20has%20recently%20been%20answered%20in%20the%20affirmative%20for%20images%2C%20introducing%0Athe%20framework%20of%20Joint%20Energy-based%20Model%20%28JEM%29%2C%20which%20achieves%20high%0Aperformance%20in%20image%20classification%20and%20generation%20simultaneously.%20This%20paper%0Aproposes%20GDPNet%2C%20the%20first%20hybrid%20Generative%20and%20Discriminative%20PointNet%20that%0Aextends%20JEM%20for%20point%20cloud%20classification%20and%20generation.%20Our%20GDPNet%20retains%0Astrong%20discriminative%20power%20of%20modern%20PointNet%20classifiers%2C%20while%20generating%0Apoint%20cloud%20samples%20rivaling%20state-of-the-art%20generative%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12925v1&entry.124074799=Read"},
{"title": "Camera Agnostic Two-Head Network for Ego-Lane Inference", "author": "Chaehyeon Song and Sungho Yoon and Minhyeok Heo and Ayoung Kim and Sujung Kim", "abstract": "  Vision-based ego-lane inference using High-Definition (HD) maps is essential\nin autonomous driving and advanced driver assistance systems. The traditional\napproach necessitates well-calibrated cameras, which confines variation of\ncamera configuration, as the algorithm relies on intrinsic and extrinsic\ncalibration. In this paper, we propose a learning-based ego-lane inference by\ndirectly estimating the ego-lane index from a single image. To enhance robust\nperformance, our model incorporates the two-head structure inferring ego-lane\nin two perspectives simultaneously. Furthermore, we utilize an attention\nmechanism guided by vanishing point-and-line to adapt to changes in viewpoint\nwithout requiring accurate calibration. The high adaptability of our model was\nvalidated in diverse environments, devices, and camera mounting points and\norientations.\n", "link": "http://arxiv.org/abs/2404.12770v1", "date": "2024-04-19", "relevancy": 2.8329, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5721}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5682}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5594}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Camera%20Agnostic%20Two-Head%20Network%20for%20Ego-Lane%20Inference&body=Title%3A%20Camera%20Agnostic%20Two-Head%20Network%20for%20Ego-Lane%20Inference%0AAuthor%3A%20Chaehyeon%20Song%20and%20Sungho%20Yoon%20and%20Minhyeok%20Heo%20and%20Ayoung%20Kim%20and%20Sujung%20Kim%0AAbstract%3A%20%20%20Vision-based%20ego-lane%20inference%20using%20High-Definition%20%28HD%29%20maps%20is%20essential%0Ain%20autonomous%20driving%20and%20advanced%20driver%20assistance%20systems.%20The%20traditional%0Aapproach%20necessitates%20well-calibrated%20cameras%2C%20which%20confines%20variation%20of%0Acamera%20configuration%2C%20as%20the%20algorithm%20relies%20on%20intrinsic%20and%20extrinsic%0Acalibration.%20In%20this%20paper%2C%20we%20propose%20a%20learning-based%20ego-lane%20inference%20by%0Adirectly%20estimating%20the%20ego-lane%20index%20from%20a%20single%20image.%20To%20enhance%20robust%0Aperformance%2C%20our%20model%20incorporates%20the%20two-head%20structure%20inferring%20ego-lane%0Ain%20two%20perspectives%20simultaneously.%20Furthermore%2C%20we%20utilize%20an%20attention%0Amechanism%20guided%20by%20vanishing%20point-and-line%20to%20adapt%20to%20changes%20in%20viewpoint%0Awithout%20requiring%20accurate%20calibration.%20The%20high%20adaptability%20of%20our%20model%20was%0Avalidated%20in%20diverse%20environments%2C%20devices%2C%20and%20camera%20mounting%20points%20and%0Aorientations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12770v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Camera%20Agnostic%20Two-Head%20Network%20for%20Ego-Lane%20Inference&entry.906535625=Chaehyeon%20Song%20and%20Sungho%20Yoon%20and%20Minhyeok%20Heo%20and%20Ayoung%20Kim%20and%20Sujung%20Kim&entry.1292438233=%20%20Vision-based%20ego-lane%20inference%20using%20High-Definition%20%28HD%29%20maps%20is%20essential%0Ain%20autonomous%20driving%20and%20advanced%20driver%20assistance%20systems.%20The%20traditional%0Aapproach%20necessitates%20well-calibrated%20cameras%2C%20which%20confines%20variation%20of%0Acamera%20configuration%2C%20as%20the%20algorithm%20relies%20on%20intrinsic%20and%20extrinsic%0Acalibration.%20In%20this%20paper%2C%20we%20propose%20a%20learning-based%20ego-lane%20inference%20by%0Adirectly%20estimating%20the%20ego-lane%20index%20from%20a%20single%20image.%20To%20enhance%20robust%0Aperformance%2C%20our%20model%20incorporates%20the%20two-head%20structure%20inferring%20ego-lane%0Ain%20two%20perspectives%20simultaneously.%20Furthermore%2C%20we%20utilize%20an%20attention%0Amechanism%20guided%20by%20vanishing%20point-and-line%20to%20adapt%20to%20changes%20in%20viewpoint%0Awithout%20requiring%20accurate%20calibration.%20The%20high%20adaptability%20of%20our%20model%20was%0Avalidated%20in%20diverse%20environments%2C%20devices%2C%20and%20camera%20mounting%20points%20and%0Aorientations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12770v1&entry.124074799=Read"},
{"title": "Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation", "author": "Myrna C. Silva and Mahtab Dahaghin and Matteo Toso and Alessio Del Bue", "abstract": "  We introduce Contrastive Gaussian Clustering, a novel approach capable of\nprovide segmentation masks from any viewpoint and of enabling 3D segmentation\nof the scene. Recent works in novel-view synthesis have shown how to model the\nappearance of a scene via a cloud of 3D Gaussians, and how to generate accurate\nimages from a given viewpoint by projecting on it the Gaussians before $\\alpha$\nblending their color. Following this example, we train a model to include also\na segmentation feature vector for each Gaussian. These can then be used for 3D\nscene segmentation, by clustering Gaussians according to their feature vectors;\nand to generate 2D segmentation masks, by projecting the Gaussians on a plane\nand $\\alpha$ blending over their segmentation features. Using a combination of\ncontrastive learning and spatial regularization, our method can be trained on\ninconsistent 2D segmentation masks, and still learn to generate segmentation\nmasks consistent across all views. Moreover, the resulting model is extremely\naccurate, improving the IoU accuracy of the predicted masks by $+8\\%$ over the\nstate of the art. Code and trained models will be released soon.\n", "link": "http://arxiv.org/abs/2404.12784v1", "date": "2024-04-19", "relevancy": 2.7477, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5715}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5259}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Gaussian%20Clustering%3A%20Weakly%20Supervised%203D%20Scene%20Segmentation&body=Title%3A%20Contrastive%20Gaussian%20Clustering%3A%20Weakly%20Supervised%203D%20Scene%20Segmentation%0AAuthor%3A%20Myrna%20C.%20Silva%20and%20Mahtab%20Dahaghin%20and%20Matteo%20Toso%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20We%20introduce%20Contrastive%20Gaussian%20Clustering%2C%20a%20novel%20approach%20capable%20of%0Aprovide%20segmentation%20masks%20from%20any%20viewpoint%20and%20of%20enabling%203D%20segmentation%0Aof%20the%20scene.%20Recent%20works%20in%20novel-view%20synthesis%20have%20shown%20how%20to%20model%20the%0Aappearance%20of%20a%20scene%20via%20a%20cloud%20of%203D%20Gaussians%2C%20and%20how%20to%20generate%20accurate%0Aimages%20from%20a%20given%20viewpoint%20by%20projecting%20on%20it%20the%20Gaussians%20before%20%24%5Calpha%24%0Ablending%20their%20color.%20Following%20this%20example%2C%20we%20train%20a%20model%20to%20include%20also%0Aa%20segmentation%20feature%20vector%20for%20each%20Gaussian.%20These%20can%20then%20be%20used%20for%203D%0Ascene%20segmentation%2C%20by%20clustering%20Gaussians%20according%20to%20their%20feature%20vectors%3B%0Aand%20to%20generate%202D%20segmentation%20masks%2C%20by%20projecting%20the%20Gaussians%20on%20a%20plane%0Aand%20%24%5Calpha%24%20blending%20over%20their%20segmentation%20features.%20Using%20a%20combination%20of%0Acontrastive%20learning%20and%20spatial%20regularization%2C%20our%20method%20can%20be%20trained%20on%0Ainconsistent%202D%20segmentation%20masks%2C%20and%20still%20learn%20to%20generate%20segmentation%0Amasks%20consistent%20across%20all%20views.%20Moreover%2C%20the%20resulting%20model%20is%20extremely%0Aaccurate%2C%20improving%20the%20IoU%20accuracy%20of%20the%20predicted%20masks%20by%20%24%2B8%5C%25%24%20over%20the%0Astate%20of%20the%20art.%20Code%20and%20trained%20models%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12784v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Gaussian%20Clustering%3A%20Weakly%20Supervised%203D%20Scene%20Segmentation&entry.906535625=Myrna%20C.%20Silva%20and%20Mahtab%20Dahaghin%20and%20Matteo%20Toso%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20We%20introduce%20Contrastive%20Gaussian%20Clustering%2C%20a%20novel%20approach%20capable%20of%0Aprovide%20segmentation%20masks%20from%20any%20viewpoint%20and%20of%20enabling%203D%20segmentation%0Aof%20the%20scene.%20Recent%20works%20in%20novel-view%20synthesis%20have%20shown%20how%20to%20model%20the%0Aappearance%20of%20a%20scene%20via%20a%20cloud%20of%203D%20Gaussians%2C%20and%20how%20to%20generate%20accurate%0Aimages%20from%20a%20given%20viewpoint%20by%20projecting%20on%20it%20the%20Gaussians%20before%20%24%5Calpha%24%0Ablending%20their%20color.%20Following%20this%20example%2C%20we%20train%20a%20model%20to%20include%20also%0Aa%20segmentation%20feature%20vector%20for%20each%20Gaussian.%20These%20can%20then%20be%20used%20for%203D%0Ascene%20segmentation%2C%20by%20clustering%20Gaussians%20according%20to%20their%20feature%20vectors%3B%0Aand%20to%20generate%202D%20segmentation%20masks%2C%20by%20projecting%20the%20Gaussians%20on%20a%20plane%0Aand%20%24%5Calpha%24%20blending%20over%20their%20segmentation%20features.%20Using%20a%20combination%20of%0Acontrastive%20learning%20and%20spatial%20regularization%2C%20our%20method%20can%20be%20trained%20on%0Ainconsistent%202D%20segmentation%20masks%2C%20and%20still%20learn%20to%20generate%20segmentation%0Amasks%20consistent%20across%20all%20views.%20Moreover%2C%20the%20resulting%20model%20is%20extremely%0Aaccurate%2C%20improving%20the%20IoU%20accuracy%20of%20the%20predicted%20masks%20by%20%24%2B8%5C%25%24%20over%20the%0Astate%20of%20the%20art.%20Code%20and%20trained%20models%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12784v1&entry.124074799=Read"},
{"title": "Modeling Hierarchical Structural Distance for Unsupervised Domain\n  Adaptation", "author": "Yingxue Xu and Guihua Wen and Yang Hu and Pei Yang", "abstract": "  Unsupervised domain adaptation (UDA) aims to estimate a transferable model\nfor unlabeled target domains by exploiting labeled source data. Optimal\nTransport (OT) based methods have recently been proven to be a promising\nsolution for UDA with a solid theoretical foundation and competitive\nperformance. However, most of these methods solely focus on domain-level OT\nalignment by leveraging the geometry of domains for domain-invariant features\nbased on the global embeddings of images. However, global representations of\nimages may destroy image structure, leading to the loss of local details that\noffer category-discriminative information. This study proposes an end-to-end\nDeep Hierarchical Optimal Transport method (DeepHOT), which aims to learn both\ndomain-invariant and category-discriminative representations by mining\nhierarchical structural relations among domains. The main idea is to\nincorporate a domain-level OT and image-level OT into a unified OT framework,\nhierarchical optimal transport, to model the underlying geometry in both domain\nspace and image space. In DeepHOT framework, an image-level OT serves as the\nground distance metric for the domain-level OT, leading to the hierarchical\nstructural distance. Compared with the ground distance of the conventional\ndomain-level OT, the image-level OT captures structural associations among\nlocal regions of images that are beneficial to classification. In this way,\nDeepHOT, a unified OT framework, not only aligns domains by domain-level OT,\nbut also enhances the discriminative power through image-level OT. Moreover, to\novercome the limitation of high computational complexity, we propose a robust\nand efficient implementation of DeepHOT by approximating origin OT with sliced\nWasserstein distance in image-level OT and accomplishing the mini-batch\nunbalanced domain-level OT.\n", "link": "http://arxiv.org/abs/2211.11424v2", "date": "2024-04-19", "relevancy": 2.7469, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5377}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Modeling%20Hierarchical%20Structural%20Distance%20for%20Unsupervised%20Domain%0A%20%20Adaptation&body=Title%3A%20Modeling%20Hierarchical%20Structural%20Distance%20for%20Unsupervised%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Yingxue%20Xu%20and%20Guihua%20Wen%20and%20Yang%20Hu%20and%20Pei%20Yang%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20aims%20to%20estimate%20a%20transferable%20model%0Afor%20unlabeled%20target%20domains%20by%20exploiting%20labeled%20source%20data.%20Optimal%0ATransport%20%28OT%29%20based%20methods%20have%20recently%20been%20proven%20to%20be%20a%20promising%0Asolution%20for%20UDA%20with%20a%20solid%20theoretical%20foundation%20and%20competitive%0Aperformance.%20However%2C%20most%20of%20these%20methods%20solely%20focus%20on%20domain-level%20OT%0Aalignment%20by%20leveraging%20the%20geometry%20of%20domains%20for%20domain-invariant%20features%0Abased%20on%20the%20global%20embeddings%20of%20images.%20However%2C%20global%20representations%20of%0Aimages%20may%20destroy%20image%20structure%2C%20leading%20to%20the%20loss%20of%20local%20details%20that%0Aoffer%20category-discriminative%20information.%20This%20study%20proposes%20an%20end-to-end%0ADeep%20Hierarchical%20Optimal%20Transport%20method%20%28DeepHOT%29%2C%20which%20aims%20to%20learn%20both%0Adomain-invariant%20and%20category-discriminative%20representations%20by%20mining%0Ahierarchical%20structural%20relations%20among%20domains.%20The%20main%20idea%20is%20to%0Aincorporate%20a%20domain-level%20OT%20and%20image-level%20OT%20into%20a%20unified%20OT%20framework%2C%0Ahierarchical%20optimal%20transport%2C%20to%20model%20the%20underlying%20geometry%20in%20both%20domain%0Aspace%20and%20image%20space.%20In%20DeepHOT%20framework%2C%20an%20image-level%20OT%20serves%20as%20the%0Aground%20distance%20metric%20for%20the%20domain-level%20OT%2C%20leading%20to%20the%20hierarchical%0Astructural%20distance.%20Compared%20with%20the%20ground%20distance%20of%20the%20conventional%0Adomain-level%20OT%2C%20the%20image-level%20OT%20captures%20structural%20associations%20among%0Alocal%20regions%20of%20images%20that%20are%20beneficial%20to%20classification.%20In%20this%20way%2C%0ADeepHOT%2C%20a%20unified%20OT%20framework%2C%20not%20only%20aligns%20domains%20by%20domain-level%20OT%2C%0Abut%20also%20enhances%20the%20discriminative%20power%20through%20image-level%20OT.%20Moreover%2C%20to%0Aovercome%20the%20limitation%20of%20high%20computational%20complexity%2C%20we%20propose%20a%20robust%0Aand%20efficient%20implementation%20of%20DeepHOT%20by%20approximating%20origin%20OT%20with%20sliced%0AWasserstein%20distance%20in%20image-level%20OT%20and%20accomplishing%20the%20mini-batch%0Aunbalanced%20domain-level%20OT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.11424v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Hierarchical%20Structural%20Distance%20for%20Unsupervised%20Domain%0A%20%20Adaptation&entry.906535625=Yingxue%20Xu%20and%20Guihua%20Wen%20and%20Yang%20Hu%20and%20Pei%20Yang&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20aims%20to%20estimate%20a%20transferable%20model%0Afor%20unlabeled%20target%20domains%20by%20exploiting%20labeled%20source%20data.%20Optimal%0ATransport%20%28OT%29%20based%20methods%20have%20recently%20been%20proven%20to%20be%20a%20promising%0Asolution%20for%20UDA%20with%20a%20solid%20theoretical%20foundation%20and%20competitive%0Aperformance.%20However%2C%20most%20of%20these%20methods%20solely%20focus%20on%20domain-level%20OT%0Aalignment%20by%20leveraging%20the%20geometry%20of%20domains%20for%20domain-invariant%20features%0Abased%20on%20the%20global%20embeddings%20of%20images.%20However%2C%20global%20representations%20of%0Aimages%20may%20destroy%20image%20structure%2C%20leading%20to%20the%20loss%20of%20local%20details%20that%0Aoffer%20category-discriminative%20information.%20This%20study%20proposes%20an%20end-to-end%0ADeep%20Hierarchical%20Optimal%20Transport%20method%20%28DeepHOT%29%2C%20which%20aims%20to%20learn%20both%0Adomain-invariant%20and%20category-discriminative%20representations%20by%20mining%0Ahierarchical%20structural%20relations%20among%20domains.%20The%20main%20idea%20is%20to%0Aincorporate%20a%20domain-level%20OT%20and%20image-level%20OT%20into%20a%20unified%20OT%20framework%2C%0Ahierarchical%20optimal%20transport%2C%20to%20model%20the%20underlying%20geometry%20in%20both%20domain%0Aspace%20and%20image%20space.%20In%20DeepHOT%20framework%2C%20an%20image-level%20OT%20serves%20as%20the%0Aground%20distance%20metric%20for%20the%20domain-level%20OT%2C%20leading%20to%20the%20hierarchical%0Astructural%20distance.%20Compared%20with%20the%20ground%20distance%20of%20the%20conventional%0Adomain-level%20OT%2C%20the%20image-level%20OT%20captures%20structural%20associations%20among%0Alocal%20regions%20of%20images%20that%20are%20beneficial%20to%20classification.%20In%20this%20way%2C%0ADeepHOT%2C%20a%20unified%20OT%20framework%2C%20not%20only%20aligns%20domains%20by%20domain-level%20OT%2C%0Abut%20also%20enhances%20the%20discriminative%20power%20through%20image-level%20OT.%20Moreover%2C%20to%0Aovercome%20the%20limitation%20of%20high%20computational%20complexity%2C%20we%20propose%20a%20robust%0Aand%20efficient%20implementation%20of%20DeepHOT%20by%20approximating%20origin%20OT%20with%20sliced%0AWasserstein%20distance%20in%20image-level%20OT%20and%20accomplishing%20the%20mini-batch%0Aunbalanced%20domain-level%20OT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.11424v2&entry.124074799=Read"},
{"title": "Zero-Shot Stitching in Reinforcement Learning using Relative\n  Representations", "author": "Antonio Pio Ricciardi and Valentino Maiorca and Luca Moschella and Riccardo Marin and Emanuele Rodol\u00e0", "abstract": "  Visual Reinforcement Learning is a popular and powerful framework that takes\nfull advantage of the Deep Learning breakthrough. However, it is also known\nthat variations in the input (e.g., different colors of the panorama due to the\nseason of the year) or the task (e.g., changing the speed limit for a car to\nrespect) could require complete retraining of the agents. In this work, we\nleverage recent developments in unifying latent representations to demonstrate\nthat it is possible to combine the components of an agent, rather than retrain\nit from scratch. We build upon the recent relative representations framework\nand adapt it for Visual RL. This allows us to create completely new agents\ncapable of handling environment-task combinations never seen during training.\nOur work paves the road toward a more accessible and flexible use of\nreinforcement learning.\n", "link": "http://arxiv.org/abs/2404.12917v1", "date": "2024-04-19", "relevancy": 2.6956, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5519}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5255}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Stitching%20in%20Reinforcement%20Learning%20using%20Relative%0A%20%20Representations&body=Title%3A%20Zero-Shot%20Stitching%20in%20Reinforcement%20Learning%20using%20Relative%0A%20%20Representations%0AAuthor%3A%20Antonio%20Pio%20Ricciardi%20and%20Valentino%20Maiorca%20and%20Luca%20Moschella%20and%20Riccardo%20Marin%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Visual%20Reinforcement%20Learning%20is%20a%20popular%20and%20powerful%20framework%20that%20takes%0Afull%20advantage%20of%20the%20Deep%20Learning%20breakthrough.%20However%2C%20it%20is%20also%20known%0Athat%20variations%20in%20the%20input%20%28e.g.%2C%20different%20colors%20of%20the%20panorama%20due%20to%20the%0Aseason%20of%20the%20year%29%20or%20the%20task%20%28e.g.%2C%20changing%20the%20speed%20limit%20for%20a%20car%20to%0Arespect%29%20could%20require%20complete%20retraining%20of%20the%20agents.%20In%20this%20work%2C%20we%0Aleverage%20recent%20developments%20in%20unifying%20latent%20representations%20to%20demonstrate%0Athat%20it%20is%20possible%20to%20combine%20the%20components%20of%20an%20agent%2C%20rather%20than%20retrain%0Ait%20from%20scratch.%20We%20build%20upon%20the%20recent%20relative%20representations%20framework%0Aand%20adapt%20it%20for%20Visual%20RL.%20This%20allows%20us%20to%20create%20completely%20new%20agents%0Acapable%20of%20handling%20environment-task%20combinations%20never%20seen%20during%20training.%0AOur%20work%20paves%20the%20road%20toward%20a%20more%20accessible%20and%20flexible%20use%20of%0Areinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12917v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Stitching%20in%20Reinforcement%20Learning%20using%20Relative%0A%20%20Representations&entry.906535625=Antonio%20Pio%20Ricciardi%20and%20Valentino%20Maiorca%20and%20Luca%20Moschella%20and%20Riccardo%20Marin%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Visual%20Reinforcement%20Learning%20is%20a%20popular%20and%20powerful%20framework%20that%20takes%0Afull%20advantage%20of%20the%20Deep%20Learning%20breakthrough.%20However%2C%20it%20is%20also%20known%0Athat%20variations%20in%20the%20input%20%28e.g.%2C%20different%20colors%20of%20the%20panorama%20due%20to%20the%0Aseason%20of%20the%20year%29%20or%20the%20task%20%28e.g.%2C%20changing%20the%20speed%20limit%20for%20a%20car%20to%0Arespect%29%20could%20require%20complete%20retraining%20of%20the%20agents.%20In%20this%20work%2C%20we%0Aleverage%20recent%20developments%20in%20unifying%20latent%20representations%20to%20demonstrate%0Athat%20it%20is%20possible%20to%20combine%20the%20components%20of%20an%20agent%2C%20rather%20than%20retrain%0Ait%20from%20scratch.%20We%20build%20upon%20the%20recent%20relative%20representations%20framework%0Aand%20adapt%20it%20for%20Visual%20RL.%20This%20allows%20us%20to%20create%20completely%20new%20agents%0Acapable%20of%20handling%20environment-task%20combinations%20never%20seen%20during%20training.%0AOur%20work%20paves%20the%20road%20toward%20a%20more%20accessible%20and%20flexible%20use%20of%0Areinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12917v1&entry.124074799=Read"},
{"title": "Monocular 3D lane detection for Autonomous Driving: Recent Achievements,\n  Challenges, and Outlooks", "author": "Fulong Ma and Weiqing Qi and Guoyang Zhao and Linwei Zheng and Sheng Wang and Yuxuan Liu and Ming Liu", "abstract": "  3D lane detection is essential in autonomous driving as it extracts\nstructural and traffic information from the road in three-dimensional space,\naiding self-driving cars in logical, safe, and comfortable path planning and\nmotion control. Given the cost of sensors and the advantages of visual data in\ncolor information, 3D lane detection based on monocular vision is an important\nresearch direction in the realm of autonomous driving, increasingly gaining\nattention in both industry and academia. Regrettably, recent advancements in\nvisual perception seem inadequate for the development of fully reliable 3D lane\ndetection algorithms, which also hampers the progress of vision-based fully\nautonomous vehicles. We believe that there is still considerable room for\nimprovement in 3D lane detection algorithms for autonomous vehicles using\nvisual sensors, and significant enhancements are needed. This review looks back\nand analyzes the current state of achievements in the field of 3D lane\ndetection research. It covers all current monocular-based 3D lane detection\nprocesses, discusses the performance of these cutting-edge algorithms, analyzes\nthe time complexity of various algorithms, and highlights the main achievements\nand limitations of ongoing research efforts. The survey also includes a\ncomprehensive discussion of available 3D lane detection datasets and the\nchallenges that researchers face but have not yet resolved. Finally, our work\noutlines future research directions and invites researchers and practitioners\nto join this exciting field.\n", "link": "http://arxiv.org/abs/2404.06860v2", "date": "2024-04-19", "relevancy": 2.678, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5534}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5272}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5262}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Monocular%203D%20lane%20detection%20for%20Autonomous%20Driving%3A%20Recent%20Achievements%2C%0A%20%20Challenges%2C%20and%20Outlooks&body=Title%3A%20Monocular%203D%20lane%20detection%20for%20Autonomous%20Driving%3A%20Recent%20Achievements%2C%0A%20%20Challenges%2C%20and%20Outlooks%0AAuthor%3A%20Fulong%20Ma%20and%20Weiqing%20Qi%20and%20Guoyang%20Zhao%20and%20Linwei%20Zheng%20and%20Sheng%20Wang%20and%20Yuxuan%20Liu%20and%20Ming%20Liu%0AAbstract%3A%20%20%203D%20lane%20detection%20is%20essential%20in%20autonomous%20driving%20as%20it%20extracts%0Astructural%20and%20traffic%20information%20from%20the%20road%20in%20three-dimensional%20space%2C%0Aaiding%20self-driving%20cars%20in%20logical%2C%20safe%2C%20and%20comfortable%20path%20planning%20and%0Amotion%20control.%20Given%20the%20cost%20of%20sensors%20and%20the%20advantages%20of%20visual%20data%20in%0Acolor%20information%2C%203D%20lane%20detection%20based%20on%20monocular%20vision%20is%20an%20important%0Aresearch%20direction%20in%20the%20realm%20of%20autonomous%20driving%2C%20increasingly%20gaining%0Aattention%20in%20both%20industry%20and%20academia.%20Regrettably%2C%20recent%20advancements%20in%0Avisual%20perception%20seem%20inadequate%20for%20the%20development%20of%20fully%20reliable%203D%20lane%0Adetection%20algorithms%2C%20which%20also%20hampers%20the%20progress%20of%20vision-based%20fully%0Aautonomous%20vehicles.%20We%20believe%20that%20there%20is%20still%20considerable%20room%20for%0Aimprovement%20in%203D%20lane%20detection%20algorithms%20for%20autonomous%20vehicles%20using%0Avisual%20sensors%2C%20and%20significant%20enhancements%20are%20needed.%20This%20review%20looks%20back%0Aand%20analyzes%20the%20current%20state%20of%20achievements%20in%20the%20field%20of%203D%20lane%0Adetection%20research.%20It%20covers%20all%20current%20monocular-based%203D%20lane%20detection%0Aprocesses%2C%20discusses%20the%20performance%20of%20these%20cutting-edge%20algorithms%2C%20analyzes%0Athe%20time%20complexity%20of%20various%20algorithms%2C%20and%20highlights%20the%20main%20achievements%0Aand%20limitations%20of%20ongoing%20research%20efforts.%20The%20survey%20also%20includes%20a%0Acomprehensive%20discussion%20of%20available%203D%20lane%20detection%20datasets%20and%20the%0Achallenges%20that%20researchers%20face%20but%20have%20not%20yet%20resolved.%20Finally%2C%20our%20work%0Aoutlines%20future%20research%20directions%20and%20invites%20researchers%20and%20practitioners%0Ato%20join%20this%20exciting%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06860v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%203D%20lane%20detection%20for%20Autonomous%20Driving%3A%20Recent%20Achievements%2C%0A%20%20Challenges%2C%20and%20Outlooks&entry.906535625=Fulong%20Ma%20and%20Weiqing%20Qi%20and%20Guoyang%20Zhao%20and%20Linwei%20Zheng%20and%20Sheng%20Wang%20and%20Yuxuan%20Liu%20and%20Ming%20Liu&entry.1292438233=%20%203D%20lane%20detection%20is%20essential%20in%20autonomous%20driving%20as%20it%20extracts%0Astructural%20and%20traffic%20information%20from%20the%20road%20in%20three-dimensional%20space%2C%0Aaiding%20self-driving%20cars%20in%20logical%2C%20safe%2C%20and%20comfortable%20path%20planning%20and%0Amotion%20control.%20Given%20the%20cost%20of%20sensors%20and%20the%20advantages%20of%20visual%20data%20in%0Acolor%20information%2C%203D%20lane%20detection%20based%20on%20monocular%20vision%20is%20an%20important%0Aresearch%20direction%20in%20the%20realm%20of%20autonomous%20driving%2C%20increasingly%20gaining%0Aattention%20in%20both%20industry%20and%20academia.%20Regrettably%2C%20recent%20advancements%20in%0Avisual%20perception%20seem%20inadequate%20for%20the%20development%20of%20fully%20reliable%203D%20lane%0Adetection%20algorithms%2C%20which%20also%20hampers%20the%20progress%20of%20vision-based%20fully%0Aautonomous%20vehicles.%20We%20believe%20that%20there%20is%20still%20considerable%20room%20for%0Aimprovement%20in%203D%20lane%20detection%20algorithms%20for%20autonomous%20vehicles%20using%0Avisual%20sensors%2C%20and%20significant%20enhancements%20are%20needed.%20This%20review%20looks%20back%0Aand%20analyzes%20the%20current%20state%20of%20achievements%20in%20the%20field%20of%203D%20lane%0Adetection%20research.%20It%20covers%20all%20current%20monocular-based%203D%20lane%20detection%0Aprocesses%2C%20discusses%20the%20performance%20of%20these%20cutting-edge%20algorithms%2C%20analyzes%0Athe%20time%20complexity%20of%20various%20algorithms%2C%20and%20highlights%20the%20main%20achievements%0Aand%20limitations%20of%20ongoing%20research%20efforts.%20The%20survey%20also%20includes%20a%0Acomprehensive%20discussion%20of%20available%203D%20lane%20detection%20datasets%20and%20the%0Achallenges%20that%20researchers%20face%20but%20have%20not%20yet%20resolved.%20Finally%2C%20our%20work%0Aoutlines%20future%20research%20directions%20and%20invites%20researchers%20and%20practitioners%0Ato%20join%20this%20exciting%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06860v2&entry.124074799=Read"},
{"title": "FlyNeRF: NeRF-Based Aerial Mapping for High-Quality 3D Scene\n  Reconstruction", "author": "Maria Dronova and Vladislav Cheremnykh and Alexey Kotcov and Aleksey Fedoseev and Dzmitry Tsetserukou", "abstract": "  Current methods for 3D reconstruction and environmental mapping frequently\nface challenges in achieving high precision, highlighting the need for\npractical and effective solutions. In response to this issue, our study\nintroduces FlyNeRF, a system integrating Neural Radiance Fields (NeRF) with\ndrone-based data acquisition for high-quality 3D reconstruction. Utilizing\nunmanned aerial vehicle (UAV) for capturing images and corresponding spatial\ncoordinates, the obtained data is subsequently used for the initial NeRF-based\n3D reconstruction of the environment. Further evaluation of the reconstruction\nrender quality is accomplished by the image evaluation neural network developed\nwithin the scope of our system. According to the results of the image\nevaluation module, an autonomous algorithm determines the position for\nadditional image capture, thereby improving the reconstruction quality. The\nneural network introduced for render quality assessment demonstrates an\naccuracy of 97%. Furthermore, our adaptive methodology enhances the overall\nreconstruction quality, resulting in an average improvement of 2.5 dB in Peak\nSignal-to-Noise Ratio (PSNR) for the 10% quantile. The FlyNeRF demonstrates\npromising results, offering advancements in such fields as environmental\nmonitoring, surveillance, and digital twins, where high-fidelity 3D\nreconstructions are crucial.\n", "link": "http://arxiv.org/abs/2404.12970v1", "date": "2024-04-19", "relevancy": 2.6161, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5455}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5408}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4834}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FlyNeRF%3A%20NeRF-Based%20Aerial%20Mapping%20for%20High-Quality%203D%20Scene%0A%20%20Reconstruction&body=Title%3A%20FlyNeRF%3A%20NeRF-Based%20Aerial%20Mapping%20for%20High-Quality%203D%20Scene%0A%20%20Reconstruction%0AAuthor%3A%20Maria%20Dronova%20and%20Vladislav%20Cheremnykh%20and%20Alexey%20Kotcov%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20Current%20methods%20for%203D%20reconstruction%20and%20environmental%20mapping%20frequently%0Aface%20challenges%20in%20achieving%20high%20precision%2C%20highlighting%20the%20need%20for%0Apractical%20and%20effective%20solutions.%20In%20response%20to%20this%20issue%2C%20our%20study%0Aintroduces%20FlyNeRF%2C%20a%20system%20integrating%20Neural%20Radiance%20Fields%20%28NeRF%29%20with%0Adrone-based%20data%20acquisition%20for%20high-quality%203D%20reconstruction.%20Utilizing%0Aunmanned%20aerial%20vehicle%20%28UAV%29%20for%20capturing%20images%20and%20corresponding%20spatial%0Acoordinates%2C%20the%20obtained%20data%20is%20subsequently%20used%20for%20the%20initial%20NeRF-based%0A3D%20reconstruction%20of%20the%20environment.%20Further%20evaluation%20of%20the%20reconstruction%0Arender%20quality%20is%20accomplished%20by%20the%20image%20evaluation%20neural%20network%20developed%0Awithin%20the%20scope%20of%20our%20system.%20According%20to%20the%20results%20of%20the%20image%0Aevaluation%20module%2C%20an%20autonomous%20algorithm%20determines%20the%20position%20for%0Aadditional%20image%20capture%2C%20thereby%20improving%20the%20reconstruction%20quality.%20The%0Aneural%20network%20introduced%20for%20render%20quality%20assessment%20demonstrates%20an%0Aaccuracy%20of%2097%25.%20Furthermore%2C%20our%20adaptive%20methodology%20enhances%20the%20overall%0Areconstruction%20quality%2C%20resulting%20in%20an%20average%20improvement%20of%202.5%20dB%20in%20Peak%0ASignal-to-Noise%20Ratio%20%28PSNR%29%20for%20the%2010%25%20quantile.%20The%20FlyNeRF%20demonstrates%0Apromising%20results%2C%20offering%20advancements%20in%20such%20fields%20as%20environmental%0Amonitoring%2C%20surveillance%2C%20and%20digital%20twins%2C%20where%20high-fidelity%203D%0Areconstructions%20are%20crucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12970v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlyNeRF%3A%20NeRF-Based%20Aerial%20Mapping%20for%20High-Quality%203D%20Scene%0A%20%20Reconstruction&entry.906535625=Maria%20Dronova%20and%20Vladislav%20Cheremnykh%20and%20Alexey%20Kotcov%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20Current%20methods%20for%203D%20reconstruction%20and%20environmental%20mapping%20frequently%0Aface%20challenges%20in%20achieving%20high%20precision%2C%20highlighting%20the%20need%20for%0Apractical%20and%20effective%20solutions.%20In%20response%20to%20this%20issue%2C%20our%20study%0Aintroduces%20FlyNeRF%2C%20a%20system%20integrating%20Neural%20Radiance%20Fields%20%28NeRF%29%20with%0Adrone-based%20data%20acquisition%20for%20high-quality%203D%20reconstruction.%20Utilizing%0Aunmanned%20aerial%20vehicle%20%28UAV%29%20for%20capturing%20images%20and%20corresponding%20spatial%0Acoordinates%2C%20the%20obtained%20data%20is%20subsequently%20used%20for%20the%20initial%20NeRF-based%0A3D%20reconstruction%20of%20the%20environment.%20Further%20evaluation%20of%20the%20reconstruction%0Arender%20quality%20is%20accomplished%20by%20the%20image%20evaluation%20neural%20network%20developed%0Awithin%20the%20scope%20of%20our%20system.%20According%20to%20the%20results%20of%20the%20image%0Aevaluation%20module%2C%20an%20autonomous%20algorithm%20determines%20the%20position%20for%0Aadditional%20image%20capture%2C%20thereby%20improving%20the%20reconstruction%20quality.%20The%0Aneural%20network%20introduced%20for%20render%20quality%20assessment%20demonstrates%20an%0Aaccuracy%20of%2097%25.%20Furthermore%2C%20our%20adaptive%20methodology%20enhances%20the%20overall%0Areconstruction%20quality%2C%20resulting%20in%20an%20average%20improvement%20of%202.5%20dB%20in%20Peak%0ASignal-to-Noise%20Ratio%20%28PSNR%29%20for%20the%2010%25%20quantile.%20The%20FlyNeRF%20demonstrates%0Apromising%20results%2C%20offering%20advancements%20in%20such%20fields%20as%20environmental%0Amonitoring%2C%20surveillance%2C%20and%20digital%20twins%2C%20where%20high-fidelity%203D%0Areconstructions%20are%20crucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12970v1&entry.124074799=Read"},
{"title": "Continual Learning on a Diet: Learning from Sparsely Labeled Streams\n  Under Constrained Computation", "author": "Wenxuan Zhang and Youssef Mohamed and Bernard Ghanem and Philip H. S. Torr and Adel Bibi and Mohamed Elhoseiny", "abstract": "  We propose and study a realistic Continual Learning (CL) setting where\nlearning algorithms are granted a restricted computational budget per time step\nwhile training. We apply this setting to large-scale semi-supervised Continual\nLearning scenarios with sparse label rates. Previous proficient CL methods\nperform very poorly in this challenging setting. Overfitting to the sparse\nlabeled data and insufficient computational budget are the two main culprits\nfor such a poor performance. Our new setting encourages learning methods to\neffectively and efficiently utilize the unlabeled data during training. To that\nend, we propose a simple but highly effective baseline, DietCL, which utilizes\nboth unlabeled and labeled data jointly. DietCL meticulously allocates\ncomputational budget for both types of data. We validate our baseline, at\nscale, on several datasets, e.g., CLOC, ImageNet10K, and CGLM, under constraint\nbudget setups. DietCL outperforms, by a large margin, all existing supervised\nCL algorithms as well as more recent continual semi-supervised methods. Our\nextensive analysis and ablations demonstrate that DietCL is stable under a full\nspectrum of label sparsity, computational budget, and various other ablations.\n", "link": "http://arxiv.org/abs/2404.12766v1", "date": "2024-04-19", "relevancy": 2.5849, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.512}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20on%20a%20Diet%3A%20Learning%20from%20Sparsely%20Labeled%20Streams%0A%20%20Under%20Constrained%20Computation&body=Title%3A%20Continual%20Learning%20on%20a%20Diet%3A%20Learning%20from%20Sparsely%20Labeled%20Streams%0A%20%20Under%20Constrained%20Computation%0AAuthor%3A%20Wenxuan%20Zhang%20and%20Youssef%20Mohamed%20and%20Bernard%20Ghanem%20and%20Philip%20H.%20S.%20Torr%20and%20Adel%20Bibi%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20We%20propose%20and%20study%20a%20realistic%20Continual%20Learning%20%28CL%29%20setting%20where%0Alearning%20algorithms%20are%20granted%20a%20restricted%20computational%20budget%20per%20time%20step%0Awhile%20training.%20We%20apply%20this%20setting%20to%20large-scale%20semi-supervised%20Continual%0ALearning%20scenarios%20with%20sparse%20label%20rates.%20Previous%20proficient%20CL%20methods%0Aperform%20very%20poorly%20in%20this%20challenging%20setting.%20Overfitting%20to%20the%20sparse%0Alabeled%20data%20and%20insufficient%20computational%20budget%20are%20the%20two%20main%20culprits%0Afor%20such%20a%20poor%20performance.%20Our%20new%20setting%20encourages%20learning%20methods%20to%0Aeffectively%20and%20efficiently%20utilize%20the%20unlabeled%20data%20during%20training.%20To%20that%0Aend%2C%20we%20propose%20a%20simple%20but%20highly%20effective%20baseline%2C%20DietCL%2C%20which%20utilizes%0Aboth%20unlabeled%20and%20labeled%20data%20jointly.%20DietCL%20meticulously%20allocates%0Acomputational%20budget%20for%20both%20types%20of%20data.%20We%20validate%20our%20baseline%2C%20at%0Ascale%2C%20on%20several%20datasets%2C%20e.g.%2C%20CLOC%2C%20ImageNet10K%2C%20and%20CGLM%2C%20under%20constraint%0Abudget%20setups.%20DietCL%20outperforms%2C%20by%20a%20large%20margin%2C%20all%20existing%20supervised%0ACL%20algorithms%20as%20well%20as%20more%20recent%20continual%20semi-supervised%20methods.%20Our%0Aextensive%20analysis%20and%20ablations%20demonstrate%20that%20DietCL%20is%20stable%20under%20a%20full%0Aspectrum%20of%20label%20sparsity%2C%20computational%20budget%2C%20and%20various%20other%20ablations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12766v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20on%20a%20Diet%3A%20Learning%20from%20Sparsely%20Labeled%20Streams%0A%20%20Under%20Constrained%20Computation&entry.906535625=Wenxuan%20Zhang%20and%20Youssef%20Mohamed%20and%20Bernard%20Ghanem%20and%20Philip%20H.%20S.%20Torr%20and%20Adel%20Bibi%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20We%20propose%20and%20study%20a%20realistic%20Continual%20Learning%20%28CL%29%20setting%20where%0Alearning%20algorithms%20are%20granted%20a%20restricted%20computational%20budget%20per%20time%20step%0Awhile%20training.%20We%20apply%20this%20setting%20to%20large-scale%20semi-supervised%20Continual%0ALearning%20scenarios%20with%20sparse%20label%20rates.%20Previous%20proficient%20CL%20methods%0Aperform%20very%20poorly%20in%20this%20challenging%20setting.%20Overfitting%20to%20the%20sparse%0Alabeled%20data%20and%20insufficient%20computational%20budget%20are%20the%20two%20main%20culprits%0Afor%20such%20a%20poor%20performance.%20Our%20new%20setting%20encourages%20learning%20methods%20to%0Aeffectively%20and%20efficiently%20utilize%20the%20unlabeled%20data%20during%20training.%20To%20that%0Aend%2C%20we%20propose%20a%20simple%20but%20highly%20effective%20baseline%2C%20DietCL%2C%20which%20utilizes%0Aboth%20unlabeled%20and%20labeled%20data%20jointly.%20DietCL%20meticulously%20allocates%0Acomputational%20budget%20for%20both%20types%20of%20data.%20We%20validate%20our%20baseline%2C%20at%0Ascale%2C%20on%20several%20datasets%2C%20e.g.%2C%20CLOC%2C%20ImageNet10K%2C%20and%20CGLM%2C%20under%20constraint%0Abudget%20setups.%20DietCL%20outperforms%2C%20by%20a%20large%20margin%2C%20all%20existing%20supervised%0ACL%20algorithms%20as%20well%20as%20more%20recent%20continual%20semi-supervised%20methods.%20Our%0Aextensive%20analysis%20and%20ablations%20demonstrate%20that%20DietCL%20is%20stable%20under%20a%20full%0Aspectrum%20of%20label%20sparsity%2C%20computational%20budget%2C%20and%20various%20other%20ablations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12766v1&entry.124074799=Read"},
{"title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large\n  Language Models", "author": "Chuofan Ma and Yi Jiang and Jiannan Wu and Zehuan Yuan and Xiaojuan Qi", "abstract": "  We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded\nand fine-grained visual perception ability. Beyond holistic image\nunderstanding, Groma is adept at region-level tasks such as region captioning\nand visual grounding. Such capabilities are built upon a localized visual\ntokenization mechanism, where an image input is decomposed into regions of\ninterest and subsequently encoded into region tokens. By integrating region\ntokens into user instructions and model responses, we seamlessly enable Groma\nto understand user-specified region inputs and ground its textual output to\nimages. Besides, to enhance the grounded chat ability of Groma, we curate a\nvisually grounded instruction dataset by leveraging the powerful GPT-4V and\nvisual prompting techniques. Compared with MLLMs that rely on the language\nmodel or external module for localization, Groma consistently demonstrates\nsuperior performances in standard referring and grounding benchmarks,\nhighlighting the advantages of embedding localization into image tokenization.\nProject page: https://groma-mllm.github.io/.\n", "link": "http://arxiv.org/abs/2404.13013v1", "date": "2024-04-19", "relevancy": 2.5459, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5135}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4862}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Groma%3A%20Localized%20Visual%20Tokenization%20for%20Grounding%20Multimodal%20Large%0A%20%20Language%20Models&body=Title%3A%20Groma%3A%20Localized%20Visual%20Tokenization%20for%20Grounding%20Multimodal%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Chuofan%20Ma%20and%20Yi%20Jiang%20and%20Jiannan%20Wu%20and%20Zehuan%20Yuan%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20We%20introduce%20Groma%2C%20a%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20with%20grounded%0Aand%20fine-grained%20visual%20perception%20ability.%20Beyond%20holistic%20image%0Aunderstanding%2C%20Groma%20is%20adept%20at%20region-level%20tasks%20such%20as%20region%20captioning%0Aand%20visual%20grounding.%20Such%20capabilities%20are%20built%20upon%20a%20localized%20visual%0Atokenization%20mechanism%2C%20where%20an%20image%20input%20is%20decomposed%20into%20regions%20of%0Ainterest%20and%20subsequently%20encoded%20into%20region%20tokens.%20By%20integrating%20region%0Atokens%20into%20user%20instructions%20and%20model%20responses%2C%20we%20seamlessly%20enable%20Groma%0Ato%20understand%20user-specified%20region%20inputs%20and%20ground%20its%20textual%20output%20to%0Aimages.%20Besides%2C%20to%20enhance%20the%20grounded%20chat%20ability%20of%20Groma%2C%20we%20curate%20a%0Avisually%20grounded%20instruction%20dataset%20by%20leveraging%20the%20powerful%20GPT-4V%20and%0Avisual%20prompting%20techniques.%20Compared%20with%20MLLMs%20that%20rely%20on%20the%20language%0Amodel%20or%20external%20module%20for%20localization%2C%20Groma%20consistently%20demonstrates%0Asuperior%20performances%20in%20standard%20referring%20and%20grounding%20benchmarks%2C%0Ahighlighting%20the%20advantages%20of%20embedding%20localization%20into%20image%20tokenization.%0AProject%20page%3A%20https%3A//groma-mllm.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13013v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Groma%3A%20Localized%20Visual%20Tokenization%20for%20Grounding%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Chuofan%20Ma%20and%20Yi%20Jiang%20and%20Jiannan%20Wu%20and%20Zehuan%20Yuan%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20We%20introduce%20Groma%2C%20a%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20with%20grounded%0Aand%20fine-grained%20visual%20perception%20ability.%20Beyond%20holistic%20image%0Aunderstanding%2C%20Groma%20is%20adept%20at%20region-level%20tasks%20such%20as%20region%20captioning%0Aand%20visual%20grounding.%20Such%20capabilities%20are%20built%20upon%20a%20localized%20visual%0Atokenization%20mechanism%2C%20where%20an%20image%20input%20is%20decomposed%20into%20regions%20of%0Ainterest%20and%20subsequently%20encoded%20into%20region%20tokens.%20By%20integrating%20region%0Atokens%20into%20user%20instructions%20and%20model%20responses%2C%20we%20seamlessly%20enable%20Groma%0Ato%20understand%20user-specified%20region%20inputs%20and%20ground%20its%20textual%20output%20to%0Aimages.%20Besides%2C%20to%20enhance%20the%20grounded%20chat%20ability%20of%20Groma%2C%20we%20curate%20a%0Avisually%20grounded%20instruction%20dataset%20by%20leveraging%20the%20powerful%20GPT-4V%20and%0Avisual%20prompting%20techniques.%20Compared%20with%20MLLMs%20that%20rely%20on%20the%20language%0Amodel%20or%20external%20module%20for%20localization%2C%20Groma%20consistently%20demonstrates%0Asuperior%20performances%20in%20standard%20referring%20and%20grounding%20benchmarks%2C%0Ahighlighting%20the%20advantages%20of%20embedding%20localization%20into%20image%20tokenization.%0AProject%20page%3A%20https%3A//groma-mllm.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13013v1&entry.124074799=Read"},
{"title": "Unified Scene Representation and Reconstruction for 3D Large Language\n  Models", "author": "Tao Chu and Pan Zhang and Xiaoyi Dong and Yuhang Zang and Qiong Liu and Jiaqi Wang", "abstract": "  Enabling Large Language Models (LLMs) to interact with 3D environments is\nchallenging. Existing approaches extract point clouds either from ground truth\n(GT) geometry or 3D scenes reconstructed by auxiliary models. Text-image\naligned 2D features from CLIP are then lifted to point clouds, which serve as\ninputs for LLMs. However, this solution lacks the establishment of 3D\npoint-to-point connections, leading to a deficiency of spatial structure\ninformation. Concurrently, the absence of integration and unification between\nthe geometric and semantic representations of the scene culminates in a\ndiminished level of 3D scene understanding. In this paper, we demonstrate the\nimportance of having a unified scene representation and reconstruction\nframework, which is essential for LLMs in 3D scenes. Specifically, we introduce\nUni3DR^2 extracts 3D geometric and semantic aware representation features via\nthe frozen pre-trained 2D foundation models (e.g., CLIP and SAM) and a\nmulti-scale aggregate 3D decoder. Our learned 3D representations not only\ncontribute to the reconstruction process but also provide valuable knowledge\nfor LLMs. Experimental results validate that our Uni3DR^2 yields convincing\ngains over the baseline on the 3D reconstruction dataset ScanNet (increasing\nF-Score by +1.8\\%). When applied to LLMs, our Uni3DR^2-LLM exhibits superior\nperformance over the baseline on the 3D vision-language understanding dataset\nScanQA (increasing BLEU-1 by +4.0\\% and +4.2\\% on the val set and test set,\nrespectively). Furthermore, it outperforms the state-of-the-art method that\nuses additional GT point clouds on both ScanQA and 3DMV-VQA.\n", "link": "http://arxiv.org/abs/2404.13044v1", "date": "2024-04-19", "relevancy": 2.4959, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6466}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6258}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6006}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unified%20Scene%20Representation%20and%20Reconstruction%20for%203D%20Large%20Language%0A%20%20Models&body=Title%3A%20Unified%20Scene%20Representation%20and%20Reconstruction%20for%203D%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Tao%20Chu%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Qiong%20Liu%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Enabling%20Large%20Language%20Models%20%28LLMs%29%20to%20interact%20with%203D%20environments%20is%0Achallenging.%20Existing%20approaches%20extract%20point%20clouds%20either%20from%20ground%20truth%0A%28GT%29%20geometry%20or%203D%20scenes%20reconstructed%20by%20auxiliary%20models.%20Text-image%0Aaligned%202D%20features%20from%20CLIP%20are%20then%20lifted%20to%20point%20clouds%2C%20which%20serve%20as%0Ainputs%20for%20LLMs.%20However%2C%20this%20solution%20lacks%20the%20establishment%20of%203D%0Apoint-to-point%20connections%2C%20leading%20to%20a%20deficiency%20of%20spatial%20structure%0Ainformation.%20Concurrently%2C%20the%20absence%20of%20integration%20and%20unification%20between%0Athe%20geometric%20and%20semantic%20representations%20of%20the%20scene%20culminates%20in%20a%0Adiminished%20level%20of%203D%20scene%20understanding.%20In%20this%20paper%2C%20we%20demonstrate%20the%0Aimportance%20of%20having%20a%20unified%20scene%20representation%20and%20reconstruction%0Aframework%2C%20which%20is%20essential%20for%20LLMs%20in%203D%20scenes.%20Specifically%2C%20we%20introduce%0AUni3DR%5E2%20extracts%203D%20geometric%20and%20semantic%20aware%20representation%20features%20via%0Athe%20frozen%20pre-trained%202D%20foundation%20models%20%28e.g.%2C%20CLIP%20and%20SAM%29%20and%20a%0Amulti-scale%20aggregate%203D%20decoder.%20Our%20learned%203D%20representations%20not%20only%0Acontribute%20to%20the%20reconstruction%20process%20but%20also%20provide%20valuable%20knowledge%0Afor%20LLMs.%20Experimental%20results%20validate%20that%20our%20Uni3DR%5E2%20yields%20convincing%0Agains%20over%20the%20baseline%20on%20the%203D%20reconstruction%20dataset%20ScanNet%20%28increasing%0AF-Score%20by%20%2B1.8%5C%25%29.%20When%20applied%20to%20LLMs%2C%20our%20Uni3DR%5E2-LLM%20exhibits%20superior%0Aperformance%20over%20the%20baseline%20on%20the%203D%20vision-language%20understanding%20dataset%0AScanQA%20%28increasing%20BLEU-1%20by%20%2B4.0%5C%25%20and%20%2B4.2%5C%25%20on%20the%20val%20set%20and%20test%20set%2C%0Arespectively%29.%20Furthermore%2C%20it%20outperforms%20the%20state-of-the-art%20method%20that%0Auses%20additional%20GT%20point%20clouds%20on%20both%20ScanQA%20and%203DMV-VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13044v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Scene%20Representation%20and%20Reconstruction%20for%203D%20Large%20Language%0A%20%20Models&entry.906535625=Tao%20Chu%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Qiong%20Liu%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Enabling%20Large%20Language%20Models%20%28LLMs%29%20to%20interact%20with%203D%20environments%20is%0Achallenging.%20Existing%20approaches%20extract%20point%20clouds%20either%20from%20ground%20truth%0A%28GT%29%20geometry%20or%203D%20scenes%20reconstructed%20by%20auxiliary%20models.%20Text-image%0Aaligned%202D%20features%20from%20CLIP%20are%20then%20lifted%20to%20point%20clouds%2C%20which%20serve%20as%0Ainputs%20for%20LLMs.%20However%2C%20this%20solution%20lacks%20the%20establishment%20of%203D%0Apoint-to-point%20connections%2C%20leading%20to%20a%20deficiency%20of%20spatial%20structure%0Ainformation.%20Concurrently%2C%20the%20absence%20of%20integration%20and%20unification%20between%0Athe%20geometric%20and%20semantic%20representations%20of%20the%20scene%20culminates%20in%20a%0Adiminished%20level%20of%203D%20scene%20understanding.%20In%20this%20paper%2C%20we%20demonstrate%20the%0Aimportance%20of%20having%20a%20unified%20scene%20representation%20and%20reconstruction%0Aframework%2C%20which%20is%20essential%20for%20LLMs%20in%203D%20scenes.%20Specifically%2C%20we%20introduce%0AUni3DR%5E2%20extracts%203D%20geometric%20and%20semantic%20aware%20representation%20features%20via%0Athe%20frozen%20pre-trained%202D%20foundation%20models%20%28e.g.%2C%20CLIP%20and%20SAM%29%20and%20a%0Amulti-scale%20aggregate%203D%20decoder.%20Our%20learned%203D%20representations%20not%20only%0Acontribute%20to%20the%20reconstruction%20process%20but%20also%20provide%20valuable%20knowledge%0Afor%20LLMs.%20Experimental%20results%20validate%20that%20our%20Uni3DR%5E2%20yields%20convincing%0Agains%20over%20the%20baseline%20on%20the%203D%20reconstruction%20dataset%20ScanNet%20%28increasing%0AF-Score%20by%20%2B1.8%5C%25%29.%20When%20applied%20to%20LLMs%2C%20our%20Uni3DR%5E2-LLM%20exhibits%20superior%0Aperformance%20over%20the%20baseline%20on%20the%203D%20vision-language%20understanding%20dataset%0AScanQA%20%28increasing%20BLEU-1%20by%20%2B4.0%5C%25%20and%20%2B4.2%5C%25%20on%20the%20val%20set%20and%20test%20set%2C%0Arespectively%29.%20Furthermore%2C%20it%20outperforms%20the%20state-of-the-art%20method%20that%0Auses%20additional%20GT%20point%20clouds%20on%20both%20ScanQA%20and%203DMV-VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13044v1&entry.124074799=Read"},
{"title": "UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction\n  with Unsupervised SMILES Alignment", "author": "Kaipeng Zeng and Bo yang and Xin Zhao and Yu Zhang and Fan Nie and Xiaokang Yang and Yaohui Jin and Yanyan Xu", "abstract": "  Motivation: Retrosynthesis planning poses a formidable challenge in the\norganic chemical industry. Single-step retrosynthesis prediction, a crucial\nstep in the planning process, has witnessed a surge in interest in recent years\ndue to advancements in AI for science. Various deep learning-based methods have\nbeen proposed for this task in recent years, incorporating diverse levels of\nadditional chemical knowledge dependency.\n  Results: This paper introduces UAlign, a template-free graph-to-sequence\npipeline for retrosynthesis prediction. By combining graph neural networks and\nTransformers, our method can more effectively leverage the inherent graph\nstructure of molecules. Based on the fact that the majority of molecule\nstructures remain unchanged during a chemical reaction, we propose a simple yet\neffective SMILES alignment technique to facilitate the reuse of unchanged\nstructures for reactant generation. Extensive experiments show that our method\nsubstantially outperforms state-of-the-art template-free and\nsemi-template-based approaches. Importantly, our template-free method achieves\neffectiveness comparable to, or even surpasses, established powerful\ntemplate-based methods.\n  Scientific contribution: We present a novel graph-to-sequence template-free\nretrosynthesis prediction pipeline that overcomes the limitations of\nTransformer-based methods in molecular representation learning and insufficient\nutilization of chemical information. We propose an unsupervised learning\nmechanism for establishing product-atom correspondence with reactant SMILES\ntokens, achieving even better results than supervised SMILES alignment methods.\nExtensive experiments demonstrate that UAlign significantly outperforms\nstate-of-the-art template-free methods and rivals or surpasses template-based\napproaches, with up to 5\\% (top-5) and 5.4\\% (top-10) increased accuracy over\nthe strongest baseline.\n", "link": "http://arxiv.org/abs/2404.00044v2", "date": "2024-04-19", "relevancy": 2.3696, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4949}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4638}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4631}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UAlign%3A%20Pushing%20the%20Limit%20of%20Template-free%20Retrosynthesis%20Prediction%0A%20%20with%20Unsupervised%20SMILES%20Alignment&body=Title%3A%20UAlign%3A%20Pushing%20the%20Limit%20of%20Template-free%20Retrosynthesis%20Prediction%0A%20%20with%20Unsupervised%20SMILES%20Alignment%0AAuthor%3A%20Kaipeng%20Zeng%20and%20Bo%20yang%20and%20Xin%20Zhao%20and%20Yu%20Zhang%20and%20Fan%20Nie%20and%20Xiaokang%20Yang%20and%20Yaohui%20Jin%20and%20Yanyan%20Xu%0AAbstract%3A%20%20%20Motivation%3A%20Retrosynthesis%20planning%20poses%20a%20formidable%20challenge%20in%20the%0Aorganic%20chemical%20industry.%20Single-step%20retrosynthesis%20prediction%2C%20a%20crucial%0Astep%20in%20the%20planning%20process%2C%20has%20witnessed%20a%20surge%20in%20interest%20in%20recent%20years%0Adue%20to%20advancements%20in%20AI%20for%20science.%20Various%20deep%20learning-based%20methods%20have%0Abeen%20proposed%20for%20this%20task%20in%20recent%20years%2C%20incorporating%20diverse%20levels%20of%0Aadditional%20chemical%20knowledge%20dependency.%0A%20%20Results%3A%20This%20paper%20introduces%20UAlign%2C%20a%20template-free%20graph-to-sequence%0Apipeline%20for%20retrosynthesis%20prediction.%20By%20combining%20graph%20neural%20networks%20and%0ATransformers%2C%20our%20method%20can%20more%20effectively%20leverage%20the%20inherent%20graph%0Astructure%20of%20molecules.%20Based%20on%20the%20fact%20that%20the%20majority%20of%20molecule%0Astructures%20remain%20unchanged%20during%20a%20chemical%20reaction%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20SMILES%20alignment%20technique%20to%20facilitate%20the%20reuse%20of%20unchanged%0Astructures%20for%20reactant%20generation.%20Extensive%20experiments%20show%20that%20our%20method%0Asubstantially%20outperforms%20state-of-the-art%20template-free%20and%0Asemi-template-based%20approaches.%20Importantly%2C%20our%20template-free%20method%20achieves%0Aeffectiveness%20comparable%20to%2C%20or%20even%20surpasses%2C%20established%20powerful%0Atemplate-based%20methods.%0A%20%20Scientific%20contribution%3A%20We%20present%20a%20novel%20graph-to-sequence%20template-free%0Aretrosynthesis%20prediction%20pipeline%20that%20overcomes%20the%20limitations%20of%0ATransformer-based%20methods%20in%20molecular%20representation%20learning%20and%20insufficient%0Autilization%20of%20chemical%20information.%20We%20propose%20an%20unsupervised%20learning%0Amechanism%20for%20establishing%20product-atom%20correspondence%20with%20reactant%20SMILES%0Atokens%2C%20achieving%20even%20better%20results%20than%20supervised%20SMILES%20alignment%20methods.%0AExtensive%20experiments%20demonstrate%20that%20UAlign%20significantly%20outperforms%0Astate-of-the-art%20template-free%20methods%20and%20rivals%20or%20surpasses%20template-based%0Aapproaches%2C%20with%20up%20to%205%5C%25%20%28top-5%29%20and%205.4%5C%25%20%28top-10%29%20increased%20accuracy%20over%0Athe%20strongest%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00044v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAlign%3A%20Pushing%20the%20Limit%20of%20Template-free%20Retrosynthesis%20Prediction%0A%20%20with%20Unsupervised%20SMILES%20Alignment&entry.906535625=Kaipeng%20Zeng%20and%20Bo%20yang%20and%20Xin%20Zhao%20and%20Yu%20Zhang%20and%20Fan%20Nie%20and%20Xiaokang%20Yang%20and%20Yaohui%20Jin%20and%20Yanyan%20Xu&entry.1292438233=%20%20Motivation%3A%20Retrosynthesis%20planning%20poses%20a%20formidable%20challenge%20in%20the%0Aorganic%20chemical%20industry.%20Single-step%20retrosynthesis%20prediction%2C%20a%20crucial%0Astep%20in%20the%20planning%20process%2C%20has%20witnessed%20a%20surge%20in%20interest%20in%20recent%20years%0Adue%20to%20advancements%20in%20AI%20for%20science.%20Various%20deep%20learning-based%20methods%20have%0Abeen%20proposed%20for%20this%20task%20in%20recent%20years%2C%20incorporating%20diverse%20levels%20of%0Aadditional%20chemical%20knowledge%20dependency.%0A%20%20Results%3A%20This%20paper%20introduces%20UAlign%2C%20a%20template-free%20graph-to-sequence%0Apipeline%20for%20retrosynthesis%20prediction.%20By%20combining%20graph%20neural%20networks%20and%0ATransformers%2C%20our%20method%20can%20more%20effectively%20leverage%20the%20inherent%20graph%0Astructure%20of%20molecules.%20Based%20on%20the%20fact%20that%20the%20majority%20of%20molecule%0Astructures%20remain%20unchanged%20during%20a%20chemical%20reaction%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20SMILES%20alignment%20technique%20to%20facilitate%20the%20reuse%20of%20unchanged%0Astructures%20for%20reactant%20generation.%20Extensive%20experiments%20show%20that%20our%20method%0Asubstantially%20outperforms%20state-of-the-art%20template-free%20and%0Asemi-template-based%20approaches.%20Importantly%2C%20our%20template-free%20method%20achieves%0Aeffectiveness%20comparable%20to%2C%20or%20even%20surpasses%2C%20established%20powerful%0Atemplate-based%20methods.%0A%20%20Scientific%20contribution%3A%20We%20present%20a%20novel%20graph-to-sequence%20template-free%0Aretrosynthesis%20prediction%20pipeline%20that%20overcomes%20the%20limitations%20of%0ATransformer-based%20methods%20in%20molecular%20representation%20learning%20and%20insufficient%0Autilization%20of%20chemical%20information.%20We%20propose%20an%20unsupervised%20learning%0Amechanism%20for%20establishing%20product-atom%20correspondence%20with%20reactant%20SMILES%0Atokens%2C%20achieving%20even%20better%20results%20than%20supervised%20SMILES%20alignment%20methods.%0AExtensive%20experiments%20demonstrate%20that%20UAlign%20significantly%20outperforms%0Astate-of-the-art%20template-free%20methods%20and%20rivals%20or%20surpasses%20template-based%0Aapproaches%2C%20with%20up%20to%205%5C%25%20%28top-5%29%20and%205.4%5C%25%20%28top-10%29%20increased%20accuracy%20over%0Athe%20strongest%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00044v2&entry.124074799=Read"},
{"title": "A Point-Based Approach to Efficient LiDAR Multi-Task Perception", "author": "Christopher Lang and Alexander Braun and Lars Schillingmann and Abhinav Valada", "abstract": "  Multi-task networks can potentially improve performance and computational\nefficiency compared to single-task networks, facilitating online deployment.\nHowever, current multi-task architectures in point cloud perception combine\nmultiple task-specific point cloud representations, each requiring a separate\nfeature encoder and making the network structures bulky and slow. We propose\nPAttFormer, an efficient multi-task architecture for joint semantic\nsegmentation and object detection in point clouds that only relies on a\npoint-based representation. The network builds on transformer-based feature\nencoders using neighborhood attention and grid-pooling and a query-based\ndetection decoder using a novel 3D deformable-attention detection head design.\nUnlike other LiDAR-based multi-task architectures, our proposed PAttFormer does\nnot require separate feature encoders for multiple task-specific point cloud\nrepresentations, resulting in a network that is 3x smaller and 1.4x faster\nwhile achieving competitive performance on the nuScenes and KITTI benchmarks\nfor autonomous driving perception. Our extensive evaluations show substantial\ngains from multi-task learning, improving LiDAR semantic segmentation by +1.7%\nin mIou and 3D object detection by +1.7% in mAP on the nuScenes benchmark\ncompared to the single-task models.\n", "link": "http://arxiv.org/abs/2404.12798v1", "date": "2024-04-19", "relevancy": 2.3635, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5943}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5927}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5867}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Point-Based%20Approach%20to%20Efficient%20LiDAR%20Multi-Task%20Perception&body=Title%3A%20A%20Point-Based%20Approach%20to%20Efficient%20LiDAR%20Multi-Task%20Perception%0AAuthor%3A%20Christopher%20Lang%20and%20Alexander%20Braun%20and%20Lars%20Schillingmann%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Multi-task%20networks%20can%20potentially%20improve%20performance%20and%20computational%0Aefficiency%20compared%20to%20single-task%20networks%2C%20facilitating%20online%20deployment.%0AHowever%2C%20current%20multi-task%20architectures%20in%20point%20cloud%20perception%20combine%0Amultiple%20task-specific%20point%20cloud%20representations%2C%20each%20requiring%20a%20separate%0Afeature%20encoder%20and%20making%20the%20network%20structures%20bulky%20and%20slow.%20We%20propose%0APAttFormer%2C%20an%20efficient%20multi-task%20architecture%20for%20joint%20semantic%0Asegmentation%20and%20object%20detection%20in%20point%20clouds%20that%20only%20relies%20on%20a%0Apoint-based%20representation.%20The%20network%20builds%20on%20transformer-based%20feature%0Aencoders%20using%20neighborhood%20attention%20and%20grid-pooling%20and%20a%20query-based%0Adetection%20decoder%20using%20a%20novel%203D%20deformable-attention%20detection%20head%20design.%0AUnlike%20other%20LiDAR-based%20multi-task%20architectures%2C%20our%20proposed%20PAttFormer%20does%0Anot%20require%20separate%20feature%20encoders%20for%20multiple%20task-specific%20point%20cloud%0Arepresentations%2C%20resulting%20in%20a%20network%20that%20is%203x%20smaller%20and%201.4x%20faster%0Awhile%20achieving%20competitive%20performance%20on%20the%20nuScenes%20and%20KITTI%20benchmarks%0Afor%20autonomous%20driving%20perception.%20Our%20extensive%20evaluations%20show%20substantial%0Agains%20from%20multi-task%20learning%2C%20improving%20LiDAR%20semantic%20segmentation%20by%20%2B1.7%25%0Ain%20mIou%20and%203D%20object%20detection%20by%20%2B1.7%25%20in%20mAP%20on%20the%20nuScenes%20benchmark%0Acompared%20to%20the%20single-task%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12798v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Point-Based%20Approach%20to%20Efficient%20LiDAR%20Multi-Task%20Perception&entry.906535625=Christopher%20Lang%20and%20Alexander%20Braun%20and%20Lars%20Schillingmann%20and%20Abhinav%20Valada&entry.1292438233=%20%20Multi-task%20networks%20can%20potentially%20improve%20performance%20and%20computational%0Aefficiency%20compared%20to%20single-task%20networks%2C%20facilitating%20online%20deployment.%0AHowever%2C%20current%20multi-task%20architectures%20in%20point%20cloud%20perception%20combine%0Amultiple%20task-specific%20point%20cloud%20representations%2C%20each%20requiring%20a%20separate%0Afeature%20encoder%20and%20making%20the%20network%20structures%20bulky%20and%20slow.%20We%20propose%0APAttFormer%2C%20an%20efficient%20multi-task%20architecture%20for%20joint%20semantic%0Asegmentation%20and%20object%20detection%20in%20point%20clouds%20that%20only%20relies%20on%20a%0Apoint-based%20representation.%20The%20network%20builds%20on%20transformer-based%20feature%0Aencoders%20using%20neighborhood%20attention%20and%20grid-pooling%20and%20a%20query-based%0Adetection%20decoder%20using%20a%20novel%203D%20deformable-attention%20detection%20head%20design.%0AUnlike%20other%20LiDAR-based%20multi-task%20architectures%2C%20our%20proposed%20PAttFormer%20does%0Anot%20require%20separate%20feature%20encoders%20for%20multiple%20task-specific%20point%20cloud%0Arepresentations%2C%20resulting%20in%20a%20network%20that%20is%203x%20smaller%20and%201.4x%20faster%0Awhile%20achieving%20competitive%20performance%20on%20the%20nuScenes%20and%20KITTI%20benchmarks%0Afor%20autonomous%20driving%20perception.%20Our%20extensive%20evaluations%20show%20substantial%0Agains%20from%20multi-task%20learning%2C%20improving%20LiDAR%20semantic%20segmentation%20by%20%2B1.7%25%0Ain%20mIou%20and%203D%20object%20detection%20by%20%2B1.7%25%20in%20mAP%20on%20the%20nuScenes%20benchmark%0Acompared%20to%20the%20single-task%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12798v1&entry.124074799=Read"},
{"title": "FedGiA: An Efficient Hybrid Algorithm for Federated Learning", "author": "Shenglong Zhou and Geoffrey Ye Li", "abstract": "  Federated learning has shown its advances recently but is still facing many\nchallenges, such as how algorithms save communication resources and reduce\ncomputational costs, and whether they converge. To address these critical\nissues, we propose a hybrid federated learning algorithm (FedGiA) that combines\nthe gradient descent and the inexact alternating direction method of\nmultipliers. The proposed algorithm is more communication- and\ncomputation-efficient than several state-of-the-art algorithms theoretically\nand numerically. Moreover, it also converges globally under mild conditions.\n", "link": "http://arxiv.org/abs/2205.01438v6", "date": "2024-04-19", "relevancy": 2.2986, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4867}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.446}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FedGiA%3A%20An%20Efficient%20Hybrid%20Algorithm%20for%20Federated%20Learning&body=Title%3A%20FedGiA%3A%20An%20Efficient%20Hybrid%20Algorithm%20for%20Federated%20Learning%0AAuthor%3A%20Shenglong%20Zhou%20and%20Geoffrey%20Ye%20Li%0AAbstract%3A%20%20%20Federated%20learning%20has%20shown%20its%20advances%20recently%20but%20is%20still%20facing%20many%0Achallenges%2C%20such%20as%20how%20algorithms%20save%20communication%20resources%20and%20reduce%0Acomputational%20costs%2C%20and%20whether%20they%20converge.%20To%20address%20these%20critical%0Aissues%2C%20we%20propose%20a%20hybrid%20federated%20learning%20algorithm%20%28FedGiA%29%20that%20combines%0Athe%20gradient%20descent%20and%20the%20inexact%20alternating%20direction%20method%20of%0Amultipliers.%20The%20proposed%20algorithm%20is%20more%20communication-%20and%0Acomputation-efficient%20than%20several%20state-of-the-art%20algorithms%20theoretically%0Aand%20numerically.%20Moreover%2C%20it%20also%20converges%20globally%20under%20mild%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.01438v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedGiA%3A%20An%20Efficient%20Hybrid%20Algorithm%20for%20Federated%20Learning&entry.906535625=Shenglong%20Zhou%20and%20Geoffrey%20Ye%20Li&entry.1292438233=%20%20Federated%20learning%20has%20shown%20its%20advances%20recently%20but%20is%20still%20facing%20many%0Achallenges%2C%20such%20as%20how%20algorithms%20save%20communication%20resources%20and%20reduce%0Acomputational%20costs%2C%20and%20whether%20they%20converge.%20To%20address%20these%20critical%0Aissues%2C%20we%20propose%20a%20hybrid%20federated%20learning%20algorithm%20%28FedGiA%29%20that%20combines%0Athe%20gradient%20descent%20and%20the%20inexact%20alternating%20direction%20method%20of%0Amultipliers.%20The%20proposed%20algorithm%20is%20more%20communication-%20and%0Acomputation-efficient%20than%20several%20state-of-the-art%20algorithms%20theoretically%0Aand%20numerically.%20Moreover%2C%20it%20also%20converges%20globally%20under%20mild%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.01438v6&entry.124074799=Read"},
{"title": "The Positivity of the Neural Tangent Kernel", "author": "Lu\u00eds Carvalho and Jo\u00e3o L. Costa and Jos\u00e9 Mour\u00e3o and Gon\u00e7alo Oliveira", "abstract": "  The Neural Tangent Kernel (NTK) has emerged as a fundamental concept in the\nstudy of wide Neural Networks. In particular, it is known that the positivity\nof the NTK is directly related to the memorization capacity of sufficiently\nwide networks, i.e., to the possibility of reaching zero loss in training, via\ngradient descent. Here we will improve on previous works and obtain a sharp\nresult concerning the positivity of the NTK of feedforward networks of any\ndepth. More precisely, we will show that, for any non-polynomial activation\nfunction, the NTK is strictly positive definite. Our results are based on a\nnovel characterization of polynomial functions which is of independent\ninterest.\n", "link": "http://arxiv.org/abs/2404.12928v1", "date": "2024-04-19", "relevancy": 2.286, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4812}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4541}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4363}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Positivity%20of%20the%20Neural%20Tangent%20Kernel&body=Title%3A%20The%20Positivity%20of%20the%20Neural%20Tangent%20Kernel%0AAuthor%3A%20Lu%C3%ADs%20Carvalho%20and%20Jo%C3%A3o%20L.%20Costa%20and%20Jos%C3%A9%20Mour%C3%A3o%20and%20Gon%C3%A7alo%20Oliveira%0AAbstract%3A%20%20%20The%20Neural%20Tangent%20Kernel%20%28NTK%29%20has%20emerged%20as%20a%20fundamental%20concept%20in%20the%0Astudy%20of%20wide%20Neural%20Networks.%20In%20particular%2C%20it%20is%20known%20that%20the%20positivity%0Aof%20the%20NTK%20is%20directly%20related%20to%20the%20memorization%20capacity%20of%20sufficiently%0Awide%20networks%2C%20i.e.%2C%20to%20the%20possibility%20of%20reaching%20zero%20loss%20in%20training%2C%20via%0Agradient%20descent.%20Here%20we%20will%20improve%20on%20previous%20works%20and%20obtain%20a%20sharp%0Aresult%20concerning%20the%20positivity%20of%20the%20NTK%20of%20feedforward%20networks%20of%20any%0Adepth.%20More%20precisely%2C%20we%20will%20show%20that%2C%20for%20any%20non-polynomial%20activation%0Afunction%2C%20the%20NTK%20is%20strictly%20positive%20definite.%20Our%20results%20are%20based%20on%20a%0Anovel%20characterization%20of%20polynomial%20functions%20which%20is%20of%20independent%0Ainterest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12928v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Positivity%20of%20the%20Neural%20Tangent%20Kernel&entry.906535625=Lu%C3%ADs%20Carvalho%20and%20Jo%C3%A3o%20L.%20Costa%20and%20Jos%C3%A9%20Mour%C3%A3o%20and%20Gon%C3%A7alo%20Oliveira&entry.1292438233=%20%20The%20Neural%20Tangent%20Kernel%20%28NTK%29%20has%20emerged%20as%20a%20fundamental%20concept%20in%20the%0Astudy%20of%20wide%20Neural%20Networks.%20In%20particular%2C%20it%20is%20known%20that%20the%20positivity%0Aof%20the%20NTK%20is%20directly%20related%20to%20the%20memorization%20capacity%20of%20sufficiently%0Awide%20networks%2C%20i.e.%2C%20to%20the%20possibility%20of%20reaching%20zero%20loss%20in%20training%2C%20via%0Agradient%20descent.%20Here%20we%20will%20improve%20on%20previous%20works%20and%20obtain%20a%20sharp%0Aresult%20concerning%20the%20positivity%20of%20the%20NTK%20of%20feedforward%20networks%20of%20any%0Adepth.%20More%20precisely%2C%20we%20will%20show%20that%2C%20for%20any%20non-polynomial%20activation%0Afunction%2C%20the%20NTK%20is%20strictly%20positive%20definite.%20Our%20results%20are%20based%20on%20a%0Anovel%20characterization%20of%20polynomial%20functions%20which%20is%20of%20independent%0Ainterest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12928v1&entry.124074799=Read"},
{"title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context", "author": "Zhuofan Zong and Bingqi Ma and Dazhong Shen and Guanglu Song and Hao Shao and Dongzhi Jiang and Hongsheng Li and Yu Liu", "abstract": "  As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM) equipped with expert-routing low-rank\nadaptation (LoRA). In the fine-grained stage, we elaborately conduct the\nmixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse\ntask-specific knowledge from various experts. This coarse-to-fine paradigm\neffectively leverages representations from experts based on multimodal context\nand model expertise, further enhancing the generalization ability. We conduct\nextensive experiments to evaluate the effectiveness of the proposed approach.\nWithout any bells and whistles, MoVA can achieve significant performance gains\nover current state-of-the-art methods in a wide range of challenging multimodal\nbenchmarks. Codes and models will be available at\nhttps://github.com/TempleX98/MoVA.\n", "link": "http://arxiv.org/abs/2404.13046v1", "date": "2024-04-19", "relevancy": 2.2847, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6203}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5358}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MoVA%3A%20Adapting%20Mixture%20of%20Vision%20Experts%20to%20Multimodal%20Context&body=Title%3A%20MoVA%3A%20Adapting%20Mixture%20of%20Vision%20Experts%20to%20Multimodal%20Context%0AAuthor%3A%20Zhuofan%20Zong%20and%20Bingqi%20Ma%20and%20Dazhong%20Shen%20and%20Guanglu%20Song%20and%20Hao%20Shao%20and%20Dongzhi%20Jiang%20and%20Hongsheng%20Li%20and%20Yu%20Liu%0AAbstract%3A%20%20%20As%20the%20key%20component%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20the%20ability%0Aof%20the%20visual%20encoder%20greatly%20affects%20MLLM%27s%20understanding%20on%20diverse%20image%0Acontent.%20Although%20some%20large-scale%20pretrained%20vision%20encoders%20such%20as%20vision%0Aencoders%20in%20CLIP%20and%20DINOv2%20have%20brought%20promising%20performance%2C%20we%20found%20that%0Athere%20is%20still%20no%20single%20vision%20encoder%20that%20can%20dominate%20various%20image%20content%0Aunderstanding%2C%20e.g.%2C%20the%20CLIP%20vision%20encoder%20leads%20to%20outstanding%20results%20on%0Ageneral%20image%20understanding%20but%20poor%20performance%20on%20document%20or%20chart%20content.%0ATo%20alleviate%20the%20bias%20of%20CLIP%20vision%20encoder%2C%20we%20first%20delve%20into%20the%20inherent%0Abehavior%20of%20different%20pre-trained%20vision%20encoders%20and%20then%20propose%20the%20MoVA%2C%20a%0Apowerful%20and%20novel%20MLLM%2C%20adaptively%20routing%20and%20fusing%20task-specific%20vision%0Aexperts%20with%20a%20coarse-to-fine%20mechanism.%20In%20the%20coarse-grained%20stage%2C%20we%20design%0Aa%20context-aware%20expert%20routing%20strategy%20to%20dynamically%20select%20the%20most%20suitable%0Avision%20experts%20according%20to%20the%20user%20instruction%2C%20input%20image%2C%20and%20expertise%20of%0Avision%20experts.%20This%20benefits%20from%20the%20powerful%20model%20function%20understanding%0Aability%20of%20the%20large%20language%20model%20%28LLM%29%20equipped%20with%20expert-routing%20low-rank%0Aadaptation%20%28LoRA%29.%20In%20the%20fine-grained%20stage%2C%20we%20elaborately%20conduct%20the%0Amixture-of-vision-expert%20adapter%20%28MoV-Adapter%29%20to%20extract%20and%20fuse%0Atask-specific%20knowledge%20from%20various%20experts.%20This%20coarse-to-fine%20paradigm%0Aeffectively%20leverages%20representations%20from%20experts%20based%20on%20multimodal%20context%0Aand%20model%20expertise%2C%20further%20enhancing%20the%20generalization%20ability.%20We%20conduct%0Aextensive%20experiments%20to%20evaluate%20the%20effectiveness%20of%20the%20proposed%20approach.%0AWithout%20any%20bells%20and%20whistles%2C%20MoVA%20can%20achieve%20significant%20performance%20gains%0Aover%20current%20state-of-the-art%20methods%20in%20a%20wide%20range%20of%20challenging%20multimodal%0Abenchmarks.%20Codes%20and%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/TempleX98/MoVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13046v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoVA%3A%20Adapting%20Mixture%20of%20Vision%20Experts%20to%20Multimodal%20Context&entry.906535625=Zhuofan%20Zong%20and%20Bingqi%20Ma%20and%20Dazhong%20Shen%20and%20Guanglu%20Song%20and%20Hao%20Shao%20and%20Dongzhi%20Jiang%20and%20Hongsheng%20Li%20and%20Yu%20Liu&entry.1292438233=%20%20As%20the%20key%20component%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20the%20ability%0Aof%20the%20visual%20encoder%20greatly%20affects%20MLLM%27s%20understanding%20on%20diverse%20image%0Acontent.%20Although%20some%20large-scale%20pretrained%20vision%20encoders%20such%20as%20vision%0Aencoders%20in%20CLIP%20and%20DINOv2%20have%20brought%20promising%20performance%2C%20we%20found%20that%0Athere%20is%20still%20no%20single%20vision%20encoder%20that%20can%20dominate%20various%20image%20content%0Aunderstanding%2C%20e.g.%2C%20the%20CLIP%20vision%20encoder%20leads%20to%20outstanding%20results%20on%0Ageneral%20image%20understanding%20but%20poor%20performance%20on%20document%20or%20chart%20content.%0ATo%20alleviate%20the%20bias%20of%20CLIP%20vision%20encoder%2C%20we%20first%20delve%20into%20the%20inherent%0Abehavior%20of%20different%20pre-trained%20vision%20encoders%20and%20then%20propose%20the%20MoVA%2C%20a%0Apowerful%20and%20novel%20MLLM%2C%20adaptively%20routing%20and%20fusing%20task-specific%20vision%0Aexperts%20with%20a%20coarse-to-fine%20mechanism.%20In%20the%20coarse-grained%20stage%2C%20we%20design%0Aa%20context-aware%20expert%20routing%20strategy%20to%20dynamically%20select%20the%20most%20suitable%0Avision%20experts%20according%20to%20the%20user%20instruction%2C%20input%20image%2C%20and%20expertise%20of%0Avision%20experts.%20This%20benefits%20from%20the%20powerful%20model%20function%20understanding%0Aability%20of%20the%20large%20language%20model%20%28LLM%29%20equipped%20with%20expert-routing%20low-rank%0Aadaptation%20%28LoRA%29.%20In%20the%20fine-grained%20stage%2C%20we%20elaborately%20conduct%20the%0Amixture-of-vision-expert%20adapter%20%28MoV-Adapter%29%20to%20extract%20and%20fuse%0Atask-specific%20knowledge%20from%20various%20experts.%20This%20coarse-to-fine%20paradigm%0Aeffectively%20leverages%20representations%20from%20experts%20based%20on%20multimodal%20context%0Aand%20model%20expertise%2C%20further%20enhancing%20the%20generalization%20ability.%20We%20conduct%0Aextensive%20experiments%20to%20evaluate%20the%20effectiveness%20of%20the%20proposed%20approach.%0AWithout%20any%20bells%20and%20whistles%2C%20MoVA%20can%20achieve%20significant%20performance%20gains%0Aover%20current%20state-of-the-art%20methods%20in%20a%20wide%20range%20of%20challenging%20multimodal%0Abenchmarks.%20Codes%20and%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/TempleX98/MoVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13046v1&entry.124074799=Read"},
{"title": "Training-and-prompt-free General Painterly Harmonization Using\n  Image-wise Attention Sharing", "author": "Teng-Fang Hsiao and Bo-Kai Ruan and Hong-Han Shuai", "abstract": "  Painterly Image Harmonization aims at seamlessly blending disparate visual\nelements within a single coherent image. However, previous approaches often\nencounter significant limitations due to training data constraints, the need\nfor time-consuming fine-tuning, or reliance on additional prompts. To surmount\nthese hurdles, we design a Training-and-prompt-Free General Painterly\nHarmonization method using image-wise attention sharing (TF-GPH), which\nintegrates a novel \"share-attention module\". This module redefines the\ntraditional self-attention mechanism by allowing for comprehensive image-wise\nattention, facilitating the use of a state-of-the-art pretrained latent\ndiffusion model without the typical training data limitations. Additionally, we\nfurther introduce \"similarity reweighting\" mechanism enhances performance by\neffectively harnessing cross-image information, surpassing the capabilities of\nfine-tuning or prompt-based approaches. At last, we recognize the deficiencies\nin existing benchmarks and propose the \"General Painterly Harmonization\nBenchmark\", which employs range-based evaluation metrics to more accurately\nreflect real-world application. Extensive experiments demonstrate the superior\nefficacy of our method across various benchmarks. The code and web demo are\navailable at https://github.com/BlueDyee/TF-GPH.\n", "link": "http://arxiv.org/abs/2404.12900v1", "date": "2024-04-19", "relevancy": 2.284, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5984}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5546}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5502}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Training-and-prompt-free%20General%20Painterly%20Harmonization%20Using%0A%20%20Image-wise%20Attention%20Sharing&body=Title%3A%20Training-and-prompt-free%20General%20Painterly%20Harmonization%20Using%0A%20%20Image-wise%20Attention%20Sharing%0AAuthor%3A%20Teng-Fang%20Hsiao%20and%20Bo-Kai%20Ruan%20and%20Hong-Han%20Shuai%0AAbstract%3A%20%20%20Painterly%20Image%20Harmonization%20aims%20at%20seamlessly%20blending%20disparate%20visual%0Aelements%20within%20a%20single%20coherent%20image.%20However%2C%20previous%20approaches%20often%0Aencounter%20significant%20limitations%20due%20to%20training%20data%20constraints%2C%20the%20need%0Afor%20time-consuming%20fine-tuning%2C%20or%20reliance%20on%20additional%20prompts.%20To%20surmount%0Athese%20hurdles%2C%20we%20design%20a%20Training-and-prompt-Free%20General%20Painterly%0AHarmonization%20method%20using%20image-wise%20attention%20sharing%20%28TF-GPH%29%2C%20which%0Aintegrates%20a%20novel%20%22share-attention%20module%22.%20This%20module%20redefines%20the%0Atraditional%20self-attention%20mechanism%20by%20allowing%20for%20comprehensive%20image-wise%0Aattention%2C%20facilitating%20the%20use%20of%20a%20state-of-the-art%20pretrained%20latent%0Adiffusion%20model%20without%20the%20typical%20training%20data%20limitations.%20Additionally%2C%20we%0Afurther%20introduce%20%22similarity%20reweighting%22%20mechanism%20enhances%20performance%20by%0Aeffectively%20harnessing%20cross-image%20information%2C%20surpassing%20the%20capabilities%20of%0Afine-tuning%20or%20prompt-based%20approaches.%20At%20last%2C%20we%20recognize%20the%20deficiencies%0Ain%20existing%20benchmarks%20and%20propose%20the%20%22General%20Painterly%20Harmonization%0ABenchmark%22%2C%20which%20employs%20range-based%20evaluation%20metrics%20to%20more%20accurately%0Areflect%20real-world%20application.%20Extensive%20experiments%20demonstrate%20the%20superior%0Aefficacy%20of%20our%20method%20across%20various%20benchmarks.%20The%20code%20and%20web%20demo%20are%0Aavailable%20at%20https%3A//github.com/BlueDyee/TF-GPH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12900v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-and-prompt-free%20General%20Painterly%20Harmonization%20Using%0A%20%20Image-wise%20Attention%20Sharing&entry.906535625=Teng-Fang%20Hsiao%20and%20Bo-Kai%20Ruan%20and%20Hong-Han%20Shuai&entry.1292438233=%20%20Painterly%20Image%20Harmonization%20aims%20at%20seamlessly%20blending%20disparate%20visual%0Aelements%20within%20a%20single%20coherent%20image.%20However%2C%20previous%20approaches%20often%0Aencounter%20significant%20limitations%20due%20to%20training%20data%20constraints%2C%20the%20need%0Afor%20time-consuming%20fine-tuning%2C%20or%20reliance%20on%20additional%20prompts.%20To%20surmount%0Athese%20hurdles%2C%20we%20design%20a%20Training-and-prompt-Free%20General%20Painterly%0AHarmonization%20method%20using%20image-wise%20attention%20sharing%20%28TF-GPH%29%2C%20which%0Aintegrates%20a%20novel%20%22share-attention%20module%22.%20This%20module%20redefines%20the%0Atraditional%20self-attention%20mechanism%20by%20allowing%20for%20comprehensive%20image-wise%0Aattention%2C%20facilitating%20the%20use%20of%20a%20state-of-the-art%20pretrained%20latent%0Adiffusion%20model%20without%20the%20typical%20training%20data%20limitations.%20Additionally%2C%20we%0Afurther%20introduce%20%22similarity%20reweighting%22%20mechanism%20enhances%20performance%20by%0Aeffectively%20harnessing%20cross-image%20information%2C%20surpassing%20the%20capabilities%20of%0Afine-tuning%20or%20prompt-based%20approaches.%20At%20last%2C%20we%20recognize%20the%20deficiencies%0Ain%20existing%20benchmarks%20and%20propose%20the%20%22General%20Painterly%20Harmonization%0ABenchmark%22%2C%20which%20employs%20range-based%20evaluation%20metrics%20to%20more%20accurately%0Areflect%20real-world%20application.%20Extensive%20experiments%20demonstrate%20the%20superior%0Aefficacy%20of%20our%20method%20across%20various%20benchmarks.%20The%20code%20and%20web%20demo%20are%0Aavailable%20at%20https%3A//github.com/BlueDyee/TF-GPH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12900v1&entry.124074799=Read"},
{"title": "Language-Driven Active Learning for Diverse Open-Set 3D Object Detection", "author": "Ross Greer and Bj\u00f8rk Antoniussen and Andreas M\u00f8gelmose and Mohan Trivedi", "abstract": "  Object detection is crucial for ensuring safe autonomous driving. However,\ndata-driven approaches face challenges when encountering minority or novel\nobjects in the 3D driving scene. In this paper, we propose VisLED, a\nlanguage-driven active learning framework for diverse open-set 3D Object\nDetection. Our method leverages active learning techniques to query diverse and\ninformative data samples from an unlabeled pool, enhancing the model's ability\nto detect underrepresented or novel objects. Specifically, we introduce the\nVision-Language Embedding Diversity Querying (VisLED-Querying) algorithm, which\noperates in both open-world exploring and closed-world mining settings. In\nopen-world exploring, VisLED-Querying selects data points most novel relative\nto existing data, while in closed-world mining, it mines new instances of known\nclasses. We evaluate our approach on the nuScenes dataset and demonstrate its\neffectiveness compared to random sampling and entropy-querying methods. Our\nresults show that VisLED-Querying consistently outperforms random sampling and\noffers competitive performance compared to entropy-querying despite the\nlatter's model-optimality, highlighting the potential of VisLED for improving\nobject detection in autonomous driving scenarios.\n", "link": "http://arxiv.org/abs/2404.12856v1", "date": "2024-04-19", "relevancy": 2.2649, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6084}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5752}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5404}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Language-Driven%20Active%20Learning%20for%20Diverse%20Open-Set%203D%20Object%20Detection&body=Title%3A%20Language-Driven%20Active%20Learning%20for%20Diverse%20Open-Set%203D%20Object%20Detection%0AAuthor%3A%20Ross%20Greer%20and%20Bj%C3%B8rk%20Antoniussen%20and%20Andreas%20M%C3%B8gelmose%20and%20Mohan%20Trivedi%0AAbstract%3A%20%20%20Object%20detection%20is%20crucial%20for%20ensuring%20safe%20autonomous%20driving.%20However%2C%0Adata-driven%20approaches%20face%20challenges%20when%20encountering%20minority%20or%20novel%0Aobjects%20in%20the%203D%20driving%20scene.%20In%20this%20paper%2C%20we%20propose%20VisLED%2C%20a%0Alanguage-driven%20active%20learning%20framework%20for%20diverse%20open-set%203D%20Object%0ADetection.%20Our%20method%20leverages%20active%20learning%20techniques%20to%20query%20diverse%20and%0Ainformative%20data%20samples%20from%20an%20unlabeled%20pool%2C%20enhancing%20the%20model%27s%20ability%0Ato%20detect%20underrepresented%20or%20novel%20objects.%20Specifically%2C%20we%20introduce%20the%0AVision-Language%20Embedding%20Diversity%20Querying%20%28VisLED-Querying%29%20algorithm%2C%20which%0Aoperates%20in%20both%20open-world%20exploring%20and%20closed-world%20mining%20settings.%20In%0Aopen-world%20exploring%2C%20VisLED-Querying%20selects%20data%20points%20most%20novel%20relative%0Ato%20existing%20data%2C%20while%20in%20closed-world%20mining%2C%20it%20mines%20new%20instances%20of%20known%0Aclasses.%20We%20evaluate%20our%20approach%20on%20the%20nuScenes%20dataset%20and%20demonstrate%20its%0Aeffectiveness%20compared%20to%20random%20sampling%20and%20entropy-querying%20methods.%20Our%0Aresults%20show%20that%20VisLED-Querying%20consistently%20outperforms%20random%20sampling%20and%0Aoffers%20competitive%20performance%20compared%20to%20entropy-querying%20despite%20the%0Alatter%27s%20model-optimality%2C%20highlighting%20the%20potential%20of%20VisLED%20for%20improving%0Aobject%20detection%20in%20autonomous%20driving%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12856v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Driven%20Active%20Learning%20for%20Diverse%20Open-Set%203D%20Object%20Detection&entry.906535625=Ross%20Greer%20and%20Bj%C3%B8rk%20Antoniussen%20and%20Andreas%20M%C3%B8gelmose%20and%20Mohan%20Trivedi&entry.1292438233=%20%20Object%20detection%20is%20crucial%20for%20ensuring%20safe%20autonomous%20driving.%20However%2C%0Adata-driven%20approaches%20face%20challenges%20when%20encountering%20minority%20or%20novel%0Aobjects%20in%20the%203D%20driving%20scene.%20In%20this%20paper%2C%20we%20propose%20VisLED%2C%20a%0Alanguage-driven%20active%20learning%20framework%20for%20diverse%20open-set%203D%20Object%0ADetection.%20Our%20method%20leverages%20active%20learning%20techniques%20to%20query%20diverse%20and%0Ainformative%20data%20samples%20from%20an%20unlabeled%20pool%2C%20enhancing%20the%20model%27s%20ability%0Ato%20detect%20underrepresented%20or%20novel%20objects.%20Specifically%2C%20we%20introduce%20the%0AVision-Language%20Embedding%20Diversity%20Querying%20%28VisLED-Querying%29%20algorithm%2C%20which%0Aoperates%20in%20both%20open-world%20exploring%20and%20closed-world%20mining%20settings.%20In%0Aopen-world%20exploring%2C%20VisLED-Querying%20selects%20data%20points%20most%20novel%20relative%0Ato%20existing%20data%2C%20while%20in%20closed-world%20mining%2C%20it%20mines%20new%20instances%20of%20known%0Aclasses.%20We%20evaluate%20our%20approach%20on%20the%20nuScenes%20dataset%20and%20demonstrate%20its%0Aeffectiveness%20compared%20to%20random%20sampling%20and%20entropy-querying%20methods.%20Our%0Aresults%20show%20that%20VisLED-Querying%20consistently%20outperforms%20random%20sampling%20and%0Aoffers%20competitive%20performance%20compared%20to%20entropy-querying%20despite%20the%0Alatter%27s%20model-optimality%2C%20highlighting%20the%20potential%20of%20VisLED%20for%20improving%0Aobject%20detection%20in%20autonomous%20driving%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12856v1&entry.124074799=Read"},
{"title": "ECOR: Explainable CLIP for Object Recognition", "author": "Ali Rasekh and Sepehr Kazemi Ranjbar and Milad Heidari and Wolfgang Nejdl", "abstract": "  Large Vision Language Models (VLMs), such as CLIP, have significantly\ncontributed to various computer vision tasks, including object recognition and\nobject detection. Their open vocabulary feature enhances their value. However,\ntheir black-box nature and lack of explainability in predictions make them less\ntrustworthy in critical domains. Recently, some work has been done to force\nVLMs to provide reasonable rationales for object recognition, but this often\ncomes at the expense of classification accuracy. In this paper, we first\npropose a mathematical definition of explainability in the object recognition\ntask based on the joint probability distribution of categories and rationales,\nthen leverage this definition to fine-tune CLIP in an explainable manner.\nThrough evaluations of different datasets, our method demonstrates\nstate-of-the-art performance in explainable classification. Notably, it excels\nin zero-shot settings, showcasing its adaptability. This advancement improves\nexplainable object recognition, enhancing trust across diverse applications.\nThe code will be made available online upon publication.\n", "link": "http://arxiv.org/abs/2404.12839v1", "date": "2024-04-19", "relevancy": 2.2624, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6013}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5478}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.521}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ECOR%3A%20Explainable%20CLIP%20for%20Object%20Recognition&body=Title%3A%20ECOR%3A%20Explainable%20CLIP%20for%20Object%20Recognition%0AAuthor%3A%20Ali%20Rasekh%20and%20Sepehr%20Kazemi%20Ranjbar%20and%20Milad%20Heidari%20and%20Wolfgang%20Nejdl%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20significantly%0Acontributed%20to%20various%20computer%20vision%20tasks%2C%20including%20object%20recognition%20and%0Aobject%20detection.%20Their%20open%20vocabulary%20feature%20enhances%20their%20value.%20However%2C%0Atheir%20black-box%20nature%20and%20lack%20of%20explainability%20in%20predictions%20make%20them%20less%0Atrustworthy%20in%20critical%20domains.%20Recently%2C%20some%20work%20has%20been%20done%20to%20force%0AVLMs%20to%20provide%20reasonable%20rationales%20for%20object%20recognition%2C%20but%20this%20often%0Acomes%20at%20the%20expense%20of%20classification%20accuracy.%20In%20this%20paper%2C%20we%20first%0Apropose%20a%20mathematical%20definition%20of%20explainability%20in%20the%20object%20recognition%0Atask%20based%20on%20the%20joint%20probability%20distribution%20of%20categories%20and%20rationales%2C%0Athen%20leverage%20this%20definition%20to%20fine-tune%20CLIP%20in%20an%20explainable%20manner.%0AThrough%20evaluations%20of%20different%20datasets%2C%20our%20method%20demonstrates%0Astate-of-the-art%20performance%20in%20explainable%20classification.%20Notably%2C%20it%20excels%0Ain%20zero-shot%20settings%2C%20showcasing%20its%20adaptability.%20This%20advancement%20improves%0Aexplainable%20object%20recognition%2C%20enhancing%20trust%20across%20diverse%20applications.%0AThe%20code%20will%20be%20made%20available%20online%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12839v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECOR%3A%20Explainable%20CLIP%20for%20Object%20Recognition&entry.906535625=Ali%20Rasekh%20and%20Sepehr%20Kazemi%20Ranjbar%20and%20Milad%20Heidari%20and%20Wolfgang%20Nejdl&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20significantly%0Acontributed%20to%20various%20computer%20vision%20tasks%2C%20including%20object%20recognition%20and%0Aobject%20detection.%20Their%20open%20vocabulary%20feature%20enhances%20their%20value.%20However%2C%0Atheir%20black-box%20nature%20and%20lack%20of%20explainability%20in%20predictions%20make%20them%20less%0Atrustworthy%20in%20critical%20domains.%20Recently%2C%20some%20work%20has%20been%20done%20to%20force%0AVLMs%20to%20provide%20reasonable%20rationales%20for%20object%20recognition%2C%20but%20this%20often%0Acomes%20at%20the%20expense%20of%20classification%20accuracy.%20In%20this%20paper%2C%20we%20first%0Apropose%20a%20mathematical%20definition%20of%20explainability%20in%20the%20object%20recognition%0Atask%20based%20on%20the%20joint%20probability%20distribution%20of%20categories%20and%20rationales%2C%0Athen%20leverage%20this%20definition%20to%20fine-tune%20CLIP%20in%20an%20explainable%20manner.%0AThrough%20evaluations%20of%20different%20datasets%2C%20our%20method%20demonstrates%0Astate-of-the-art%20performance%20in%20explainable%20classification.%20Notably%2C%20it%20excels%0Ain%20zero-shot%20settings%2C%20showcasing%20its%20adaptability.%20This%20advancement%20improves%0Aexplainable%20object%20recognition%2C%20enhancing%20trust%20across%20diverse%20applications.%0AThe%20code%20will%20be%20made%20available%20online%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12839v1&entry.124074799=Read"},
{"title": "RANRAC: Robust Neural Scene Representations via Random Ray Consensus", "author": "Benno Buschmann and Andreea Dogaru and Elmar Eisemann and Michael Weinmann and Bernhard Egger", "abstract": "  Learning-based scene representations such as neural radiance fields or light\nfield networks, that rely on fitting a scene model to image observations,\ncommonly encounter challenges in the presence of inconsistencies within the\nimages caused by occlusions, inaccurately estimated camera parameters or\neffects like lens flare. To address this challenge, we introduce RANdom RAy\nConsensus (RANRAC), an efficient approach to eliminate the effect of\ninconsistent data, thereby taking inspiration from classical RANSAC based\noutlier detection for model fitting. In contrast to the down-weighting of the\neffect of outliers based on robust loss formulations, our approach reliably\ndetects and excludes inconsistent perspectives, resulting in clean images\nwithout floating artifacts. For this purpose, we formulate a fuzzy adaption of\nthe RANSAC paradigm, enabling its application to large scale models. We\ninterpret the minimal number of samples to determine the model parameters as a\ntunable hyperparameter, investigate the generation of hypotheses with\ndata-driven models, and analyze the validation of hypotheses in noisy\nenvironments. We demonstrate the compatibility and potential of our solution\nfor both photo-realistic robust multi-view reconstruction from real-world\nimages based on neural radiance fields and for single-shot reconstruction based\non light-field networks. In particular, the results indicate significant\nimprovements compared to state-of-the-art robust methods for novel-view\nsynthesis on both synthetic and captured scenes with various inconsistencies\nincluding occlusions, noisy camera pose estimates, and unfocused perspectives.\nThe results further indicate significant improvements for single-shot\nreconstruction from occluded images. Project Page:\nhttps://bennobuschmann.com/ranrac/\n", "link": "http://arxiv.org/abs/2312.09780v2", "date": "2024-04-19", "relevancy": 2.26, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5885}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5377}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RANRAC%3A%20Robust%20Neural%20Scene%20Representations%20via%20Random%20Ray%20Consensus&body=Title%3A%20RANRAC%3A%20Robust%20Neural%20Scene%20Representations%20via%20Random%20Ray%20Consensus%0AAuthor%3A%20Benno%20Buschmann%20and%20Andreea%20Dogaru%20and%20Elmar%20Eisemann%20and%20Michael%20Weinmann%20and%20Bernhard%20Egger%0AAbstract%3A%20%20%20Learning-based%20scene%20representations%20such%20as%20neural%20radiance%20fields%20or%20light%0Afield%20networks%2C%20that%20rely%20on%20fitting%20a%20scene%20model%20to%20image%20observations%2C%0Acommonly%20encounter%20challenges%20in%20the%20presence%20of%20inconsistencies%20within%20the%0Aimages%20caused%20by%20occlusions%2C%20inaccurately%20estimated%20camera%20parameters%20or%0Aeffects%20like%20lens%20flare.%20To%20address%20this%20challenge%2C%20we%20introduce%20RANdom%20RAy%0AConsensus%20%28RANRAC%29%2C%20an%20efficient%20approach%20to%20eliminate%20the%20effect%20of%0Ainconsistent%20data%2C%20thereby%20taking%20inspiration%20from%20classical%20RANSAC%20based%0Aoutlier%20detection%20for%20model%20fitting.%20In%20contrast%20to%20the%20down-weighting%20of%20the%0Aeffect%20of%20outliers%20based%20on%20robust%20loss%20formulations%2C%20our%20approach%20reliably%0Adetects%20and%20excludes%20inconsistent%20perspectives%2C%20resulting%20in%20clean%20images%0Awithout%20floating%20artifacts.%20For%20this%20purpose%2C%20we%20formulate%20a%20fuzzy%20adaption%20of%0Athe%20RANSAC%20paradigm%2C%20enabling%20its%20application%20to%20large%20scale%20models.%20We%0Ainterpret%20the%20minimal%20number%20of%20samples%20to%20determine%20the%20model%20parameters%20as%20a%0Atunable%20hyperparameter%2C%20investigate%20the%20generation%20of%20hypotheses%20with%0Adata-driven%20models%2C%20and%20analyze%20the%20validation%20of%20hypotheses%20in%20noisy%0Aenvironments.%20We%20demonstrate%20the%20compatibility%20and%20potential%20of%20our%20solution%0Afor%20both%20photo-realistic%20robust%20multi-view%20reconstruction%20from%20real-world%0Aimages%20based%20on%20neural%20radiance%20fields%20and%20for%20single-shot%20reconstruction%20based%0Aon%20light-field%20networks.%20In%20particular%2C%20the%20results%20indicate%20significant%0Aimprovements%20compared%20to%20state-of-the-art%20robust%20methods%20for%20novel-view%0Asynthesis%20on%20both%20synthetic%20and%20captured%20scenes%20with%20various%20inconsistencies%0Aincluding%20occlusions%2C%20noisy%20camera%20pose%20estimates%2C%20and%20unfocused%20perspectives.%0AThe%20results%20further%20indicate%20significant%20improvements%20for%20single-shot%0Areconstruction%20from%20occluded%20images.%20Project%20Page%3A%0Ahttps%3A//bennobuschmann.com/ranrac/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09780v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RANRAC%3A%20Robust%20Neural%20Scene%20Representations%20via%20Random%20Ray%20Consensus&entry.906535625=Benno%20Buschmann%20and%20Andreea%20Dogaru%20and%20Elmar%20Eisemann%20and%20Michael%20Weinmann%20and%20Bernhard%20Egger&entry.1292438233=%20%20Learning-based%20scene%20representations%20such%20as%20neural%20radiance%20fields%20or%20light%0Afield%20networks%2C%20that%20rely%20on%20fitting%20a%20scene%20model%20to%20image%20observations%2C%0Acommonly%20encounter%20challenges%20in%20the%20presence%20of%20inconsistencies%20within%20the%0Aimages%20caused%20by%20occlusions%2C%20inaccurately%20estimated%20camera%20parameters%20or%0Aeffects%20like%20lens%20flare.%20To%20address%20this%20challenge%2C%20we%20introduce%20RANdom%20RAy%0AConsensus%20%28RANRAC%29%2C%20an%20efficient%20approach%20to%20eliminate%20the%20effect%20of%0Ainconsistent%20data%2C%20thereby%20taking%20inspiration%20from%20classical%20RANSAC%20based%0Aoutlier%20detection%20for%20model%20fitting.%20In%20contrast%20to%20the%20down-weighting%20of%20the%0Aeffect%20of%20outliers%20based%20on%20robust%20loss%20formulations%2C%20our%20approach%20reliably%0Adetects%20and%20excludes%20inconsistent%20perspectives%2C%20resulting%20in%20clean%20images%0Awithout%20floating%20artifacts.%20For%20this%20purpose%2C%20we%20formulate%20a%20fuzzy%20adaption%20of%0Athe%20RANSAC%20paradigm%2C%20enabling%20its%20application%20to%20large%20scale%20models.%20We%0Ainterpret%20the%20minimal%20number%20of%20samples%20to%20determine%20the%20model%20parameters%20as%20a%0Atunable%20hyperparameter%2C%20investigate%20the%20generation%20of%20hypotheses%20with%0Adata-driven%20models%2C%20and%20analyze%20the%20validation%20of%20hypotheses%20in%20noisy%0Aenvironments.%20We%20demonstrate%20the%20compatibility%20and%20potential%20of%20our%20solution%0Afor%20both%20photo-realistic%20robust%20multi-view%20reconstruction%20from%20real-world%0Aimages%20based%20on%20neural%20radiance%20fields%20and%20for%20single-shot%20reconstruction%20based%0Aon%20light-field%20networks.%20In%20particular%2C%20the%20results%20indicate%20significant%0Aimprovements%20compared%20to%20state-of-the-art%20robust%20methods%20for%20novel-view%0Asynthesis%20on%20both%20synthetic%20and%20captured%20scenes%20with%20various%20inconsistencies%0Aincluding%20occlusions%2C%20noisy%20camera%20pose%20estimates%2C%20and%20unfocused%20perspectives.%0AThe%20results%20further%20indicate%20significant%20improvements%20for%20single-shot%0Areconstruction%20from%20occluded%20images.%20Project%20Page%3A%0Ahttps%3A//bennobuschmann.com/ranrac/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09780v2&entry.124074799=Read"},
{"title": "MultiModal-Learning for Predicting Molecular Properties: A Framework\n  Based on Image and Graph Structures", "author": "Zhuoyuan Wang and Jiacong Mi and Shan Lu and Jieyue He", "abstract": "  The quest for accurate prediction of drug molecule properties poses a\nfundamental challenge in the realm of Artificial Intelligence Drug Discovery\n(AIDD). An effective representation of drug molecules emerges as a pivotal\ncomponent in this pursuit. Contemporary leading-edge research predominantly\nresorts to self-supervised learning (SSL) techniques to extract meaningful\nstructural representations from large-scale, unlabeled molecular data,\nsubsequently fine-tuning these representations for an array of downstream\ntasks. However, an inherent shortcoming of these studies lies in their singular\nreliance on one modality of molecular information, such as molecule image or\nSMILES representations, thus neglecting the potential complementarity of\nvarious molecular modalities. In response to this limitation, we propose MolIG,\na novel MultiModaL molecular pre-training framework for predicting molecular\nproperties based on Image and Graph structures. MolIG model innovatively\nleverages the coherence and correlation between molecule graph and molecule\nimage to execute self-supervised tasks, effectively amalgamating the strengths\nof both molecular representation forms. This holistic approach allows for the\ncapture of pivotal molecular structural characteristics and high-level semantic\ninformation. Upon completion of pre-training, Graph Neural Network (GNN)\nEncoder is used for the prediction of downstream tasks. In comparison to\nadvanced baseline models, MolIG exhibits enhanced performance in downstream\ntasks pertaining to molecular property prediction within benchmark groups such\nas MoleculeNet Benchmark Group and ADMET Benchmark Group.\n", "link": "http://arxiv.org/abs/2311.16666v2", "date": "2024-04-19", "relevancy": 2.2347, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5879}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.566}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5396}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MultiModal-Learning%20for%20Predicting%20Molecular%20Properties%3A%20A%20Framework%0A%20%20Based%20on%20Image%20and%20Graph%20Structures&body=Title%3A%20MultiModal-Learning%20for%20Predicting%20Molecular%20Properties%3A%20A%20Framework%0A%20%20Based%20on%20Image%20and%20Graph%20Structures%0AAuthor%3A%20Zhuoyuan%20Wang%20and%20Jiacong%20Mi%20and%20Shan%20Lu%20and%20Jieyue%20He%0AAbstract%3A%20%20%20The%20quest%20for%20accurate%20prediction%20of%20drug%20molecule%20properties%20poses%20a%0Afundamental%20challenge%20in%20the%20realm%20of%20Artificial%20Intelligence%20Drug%20Discovery%0A%28AIDD%29.%20An%20effective%20representation%20of%20drug%20molecules%20emerges%20as%20a%20pivotal%0Acomponent%20in%20this%20pursuit.%20Contemporary%20leading-edge%20research%20predominantly%0Aresorts%20to%20self-supervised%20learning%20%28SSL%29%20techniques%20to%20extract%20meaningful%0Astructural%20representations%20from%20large-scale%2C%20unlabeled%20molecular%20data%2C%0Asubsequently%20fine-tuning%20these%20representations%20for%20an%20array%20of%20downstream%0Atasks.%20However%2C%20an%20inherent%20shortcoming%20of%20these%20studies%20lies%20in%20their%20singular%0Areliance%20on%20one%20modality%20of%20molecular%20information%2C%20such%20as%20molecule%20image%20or%0ASMILES%20representations%2C%20thus%20neglecting%20the%20potential%20complementarity%20of%0Avarious%20molecular%20modalities.%20In%20response%20to%20this%20limitation%2C%20we%20propose%20MolIG%2C%0Aa%20novel%20MultiModaL%20molecular%20pre-training%20framework%20for%20predicting%20molecular%0Aproperties%20based%20on%20Image%20and%20Graph%20structures.%20MolIG%20model%20innovatively%0Aleverages%20the%20coherence%20and%20correlation%20between%20molecule%20graph%20and%20molecule%0Aimage%20to%20execute%20self-supervised%20tasks%2C%20effectively%20amalgamating%20the%20strengths%0Aof%20both%20molecular%20representation%20forms.%20This%20holistic%20approach%20allows%20for%20the%0Acapture%20of%20pivotal%20molecular%20structural%20characteristics%20and%20high-level%20semantic%0Ainformation.%20Upon%20completion%20of%20pre-training%2C%20Graph%20Neural%20Network%20%28GNN%29%0AEncoder%20is%20used%20for%20the%20prediction%20of%20downstream%20tasks.%20In%20comparison%20to%0Aadvanced%20baseline%20models%2C%20MolIG%20exhibits%20enhanced%20performance%20in%20downstream%0Atasks%20pertaining%20to%20molecular%20property%20prediction%20within%20benchmark%20groups%20such%0Aas%20MoleculeNet%20Benchmark%20Group%20and%20ADMET%20Benchmark%20Group.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16666v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiModal-Learning%20for%20Predicting%20Molecular%20Properties%3A%20A%20Framework%0A%20%20Based%20on%20Image%20and%20Graph%20Structures&entry.906535625=Zhuoyuan%20Wang%20and%20Jiacong%20Mi%20and%20Shan%20Lu%20and%20Jieyue%20He&entry.1292438233=%20%20The%20quest%20for%20accurate%20prediction%20of%20drug%20molecule%20properties%20poses%20a%0Afundamental%20challenge%20in%20the%20realm%20of%20Artificial%20Intelligence%20Drug%20Discovery%0A%28AIDD%29.%20An%20effective%20representation%20of%20drug%20molecules%20emerges%20as%20a%20pivotal%0Acomponent%20in%20this%20pursuit.%20Contemporary%20leading-edge%20research%20predominantly%0Aresorts%20to%20self-supervised%20learning%20%28SSL%29%20techniques%20to%20extract%20meaningful%0Astructural%20representations%20from%20large-scale%2C%20unlabeled%20molecular%20data%2C%0Asubsequently%20fine-tuning%20these%20representations%20for%20an%20array%20of%20downstream%0Atasks.%20However%2C%20an%20inherent%20shortcoming%20of%20these%20studies%20lies%20in%20their%20singular%0Areliance%20on%20one%20modality%20of%20molecular%20information%2C%20such%20as%20molecule%20image%20or%0ASMILES%20representations%2C%20thus%20neglecting%20the%20potential%20complementarity%20of%0Avarious%20molecular%20modalities.%20In%20response%20to%20this%20limitation%2C%20we%20propose%20MolIG%2C%0Aa%20novel%20MultiModaL%20molecular%20pre-training%20framework%20for%20predicting%20molecular%0Aproperties%20based%20on%20Image%20and%20Graph%20structures.%20MolIG%20model%20innovatively%0Aleverages%20the%20coherence%20and%20correlation%20between%20molecule%20graph%20and%20molecule%0Aimage%20to%20execute%20self-supervised%20tasks%2C%20effectively%20amalgamating%20the%20strengths%0Aof%20both%20molecular%20representation%20forms.%20This%20holistic%20approach%20allows%20for%20the%0Acapture%20of%20pivotal%20molecular%20structural%20characteristics%20and%20high-level%20semantic%0Ainformation.%20Upon%20completion%20of%20pre-training%2C%20Graph%20Neural%20Network%20%28GNN%29%0AEncoder%20is%20used%20for%20the%20prediction%20of%20downstream%20tasks.%20In%20comparison%20to%0Aadvanced%20baseline%20models%2C%20MolIG%20exhibits%20enhanced%20performance%20in%20downstream%0Atasks%20pertaining%20to%20molecular%20property%20prediction%20within%20benchmark%20groups%20such%0Aas%20MoleculeNet%20Benchmark%20Group%20and%20ADMET%20Benchmark%20Group.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16666v2&entry.124074799=Read"},
{"title": "KoReA-SFL: Knowledge Replay-based Split Federated Learning Against\n  Catastrophic Forgetting", "author": "Zeke Xia and Ming Hu and Dengke Yan and Ruixuan Liu and Anran Li and Xiaofei Xie and Mingsong Chen", "abstract": "  Although Split Federated Learning (SFL) is good at enabling knowledge sharing\namong resource-constrained clients, it suffers from the problem of low training\naccuracy due to the neglect of data heterogeneity and catastrophic forgetting.\nTo address this issue, we propose a novel SFL approach named KoReA-SFL, which\nadopts a multi-model aggregation mechanism to alleviate gradient divergence\ncaused by heterogeneous data and a knowledge replay strategy to deal with\ncatastrophic forgetting. Specifically, in KoReA-SFL cloud servers (i.e., fed\nserver and main server) maintain multiple branch model portions rather than a\nglobal portion for local training and an aggregated master-model portion for\nknowledge sharing among branch portions. To avoid catastrophic forgetting, the\nmain server of KoReA-SFL selects multiple assistant devices for knowledge\nreplay according to the training data distribution of each server-side\nbranch-model portion. Experimental results obtained from non-IID and IID\nscenarios demonstrate that KoReA-SFL significantly outperforms conventional SFL\nmethods (by up to 23.25\\% test accuracy improvement).\n", "link": "http://arxiv.org/abs/2404.12846v1", "date": "2024-04-19", "relevancy": 2.2268, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4497}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4447}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4417}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20KoReA-SFL%3A%20Knowledge%20Replay-based%20Split%20Federated%20Learning%20Against%0A%20%20Catastrophic%20Forgetting&body=Title%3A%20KoReA-SFL%3A%20Knowledge%20Replay-based%20Split%20Federated%20Learning%20Against%0A%20%20Catastrophic%20Forgetting%0AAuthor%3A%20Zeke%20Xia%20and%20Ming%20Hu%20and%20Dengke%20Yan%20and%20Ruixuan%20Liu%20and%20Anran%20Li%20and%20Xiaofei%20Xie%20and%20Mingsong%20Chen%0AAbstract%3A%20%20%20Although%20Split%20Federated%20Learning%20%28SFL%29%20is%20good%20at%20enabling%20knowledge%20sharing%0Aamong%20resource-constrained%20clients%2C%20it%20suffers%20from%20the%20problem%20of%20low%20training%0Aaccuracy%20due%20to%20the%20neglect%20of%20data%20heterogeneity%20and%20catastrophic%20forgetting.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20SFL%20approach%20named%20KoReA-SFL%2C%20which%0Aadopts%20a%20multi-model%20aggregation%20mechanism%20to%20alleviate%20gradient%20divergence%0Acaused%20by%20heterogeneous%20data%20and%20a%20knowledge%20replay%20strategy%20to%20deal%20with%0Acatastrophic%20forgetting.%20Specifically%2C%20in%20KoReA-SFL%20cloud%20servers%20%28i.e.%2C%20fed%0Aserver%20and%20main%20server%29%20maintain%20multiple%20branch%20model%20portions%20rather%20than%20a%0Aglobal%20portion%20for%20local%20training%20and%20an%20aggregated%20master-model%20portion%20for%0Aknowledge%20sharing%20among%20branch%20portions.%20To%20avoid%20catastrophic%20forgetting%2C%20the%0Amain%20server%20of%20KoReA-SFL%20selects%20multiple%20assistant%20devices%20for%20knowledge%0Areplay%20according%20to%20the%20training%20data%20distribution%20of%20each%20server-side%0Abranch-model%20portion.%20Experimental%20results%20obtained%20from%20non-IID%20and%20IID%0Ascenarios%20demonstrate%20that%20KoReA-SFL%20significantly%20outperforms%20conventional%20SFL%0Amethods%20%28by%20up%20to%2023.25%5C%25%20test%20accuracy%20improvement%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12846v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KoReA-SFL%3A%20Knowledge%20Replay-based%20Split%20Federated%20Learning%20Against%0A%20%20Catastrophic%20Forgetting&entry.906535625=Zeke%20Xia%20and%20Ming%20Hu%20and%20Dengke%20Yan%20and%20Ruixuan%20Liu%20and%20Anran%20Li%20and%20Xiaofei%20Xie%20and%20Mingsong%20Chen&entry.1292438233=%20%20Although%20Split%20Federated%20Learning%20%28SFL%29%20is%20good%20at%20enabling%20knowledge%20sharing%0Aamong%20resource-constrained%20clients%2C%20it%20suffers%20from%20the%20problem%20of%20low%20training%0Aaccuracy%20due%20to%20the%20neglect%20of%20data%20heterogeneity%20and%20catastrophic%20forgetting.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20SFL%20approach%20named%20KoReA-SFL%2C%20which%0Aadopts%20a%20multi-model%20aggregation%20mechanism%20to%20alleviate%20gradient%20divergence%0Acaused%20by%20heterogeneous%20data%20and%20a%20knowledge%20replay%20strategy%20to%20deal%20with%0Acatastrophic%20forgetting.%20Specifically%2C%20in%20KoReA-SFL%20cloud%20servers%20%28i.e.%2C%20fed%0Aserver%20and%20main%20server%29%20maintain%20multiple%20branch%20model%20portions%20rather%20than%20a%0Aglobal%20portion%20for%20local%20training%20and%20an%20aggregated%20master-model%20portion%20for%0Aknowledge%20sharing%20among%20branch%20portions.%20To%20avoid%20catastrophic%20forgetting%2C%20the%0Amain%20server%20of%20KoReA-SFL%20selects%20multiple%20assistant%20devices%20for%20knowledge%0Areplay%20according%20to%20the%20training%20data%20distribution%20of%20each%20server-side%0Abranch-model%20portion.%20Experimental%20results%20obtained%20from%20non-IID%20and%20IID%0Ascenarios%20demonstrate%20that%20KoReA-SFL%20significantly%20outperforms%20conventional%20SFL%0Amethods%20%28by%20up%20to%2023.25%5C%25%20test%20accuracy%20improvement%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12846v1&entry.124074799=Read"},
{"title": "MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware\n  State Space Model", "author": "Kang Zeng and Hao Shi and Jiacheng Lin and Siyu Li and Jintao Cheng and Kaiwei Wang and Zhiyong Li and Kailun Yang", "abstract": "  LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment\nmoving objects in point clouds of the current scan using motion information\nfrom previous scans. Despite the promising results achieved by previous MOS\nmethods, several key issues, such as the weak coupling of temporal and spatial\ninformation, still need further study. In this paper, we propose a novel\nLiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model,\ntermed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue\nBootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial\ninformation in point clouds and alleviate the issue of overlooked temporal\nclues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to\nendow the model with the capacity to understand the temporal correlations of\nthe same object across different time steps. Specifically, MSSM emphasizes the\nmotion states of the same object at different time steps through two distinct\ntemporal modeling and correlation steps. We utilize an improved state space\nmodel to represent these motion differences, significantly modeling the motion\nstates. Finally, extensive experiments on the SemanticKITTI-MOS and KITTI-Road\nbenchmarks demonstrate that the proposed MambaMOS achieves state-of-the-art\nperformance. The source code of this work will be made publicly available at\nhttps://github.com/Terminal-K/MambaMOS.\n", "link": "http://arxiv.org/abs/2404.12794v1", "date": "2024-04-19", "relevancy": 2.226, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5869}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5504}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MambaMOS%3A%20LiDAR-based%203D%20Moving%20Object%20Segmentation%20with%20Motion-aware%0A%20%20State%20Space%20Model&body=Title%3A%20MambaMOS%3A%20LiDAR-based%203D%20Moving%20Object%20Segmentation%20with%20Motion-aware%0A%20%20State%20Space%20Model%0AAuthor%3A%20Kang%20Zeng%20and%20Hao%20Shi%20and%20Jiacheng%20Lin%20and%20Siyu%20Li%20and%20Jintao%20Cheng%20and%20Kaiwei%20Wang%20and%20Zhiyong%20Li%20and%20Kailun%20Yang%0AAbstract%3A%20%20%20LiDAR-based%20Moving%20Object%20Segmentation%20%28MOS%29%20aims%20to%20locate%20and%20segment%0Amoving%20objects%20in%20point%20clouds%20of%20the%20current%20scan%20using%20motion%20information%0Afrom%20previous%20scans.%20Despite%20the%20promising%20results%20achieved%20by%20previous%20MOS%0Amethods%2C%20several%20key%20issues%2C%20such%20as%20the%20weak%20coupling%20of%20temporal%20and%20spatial%0Ainformation%2C%20still%20need%20further%20study.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0ALiDAR-based%203D%20Moving%20Object%20Segmentation%20with%20Motion-aware%20State%20Space%20Model%2C%0Atermed%20MambaMOS.%20Firstly%2C%20we%20develop%20a%20novel%20embedding%20module%2C%20the%20Time%20Clue%0ABootstrapping%20Embedding%20%28TCBE%29%2C%20to%20enhance%20the%20coupling%20of%20temporal%20and%20spatial%0Ainformation%20in%20point%20clouds%20and%20alleviate%20the%20issue%20of%20overlooked%20temporal%0Aclues.%20Secondly%2C%20we%20introduce%20the%20Motion-aware%20State%20Space%20Model%20%28MSSM%29%20to%0Aendow%20the%20model%20with%20the%20capacity%20to%20understand%20the%20temporal%20correlations%20of%0Athe%20same%20object%20across%20different%20time%20steps.%20Specifically%2C%20MSSM%20emphasizes%20the%0Amotion%20states%20of%20the%20same%20object%20at%20different%20time%20steps%20through%20two%20distinct%0Atemporal%20modeling%20and%20correlation%20steps.%20We%20utilize%20an%20improved%20state%20space%0Amodel%20to%20represent%20these%20motion%20differences%2C%20significantly%20modeling%20the%20motion%0Astates.%20Finally%2C%20extensive%20experiments%20on%20the%20SemanticKITTI-MOS%20and%20KITTI-Road%0Abenchmarks%20demonstrate%20that%20the%20proposed%20MambaMOS%20achieves%20state-of-the-art%0Aperformance.%20The%20source%20code%20of%20this%20work%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/Terminal-K/MambaMOS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12794v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaMOS%3A%20LiDAR-based%203D%20Moving%20Object%20Segmentation%20with%20Motion-aware%0A%20%20State%20Space%20Model&entry.906535625=Kang%20Zeng%20and%20Hao%20Shi%20and%20Jiacheng%20Lin%20and%20Siyu%20Li%20and%20Jintao%20Cheng%20and%20Kaiwei%20Wang%20and%20Zhiyong%20Li%20and%20Kailun%20Yang&entry.1292438233=%20%20LiDAR-based%20Moving%20Object%20Segmentation%20%28MOS%29%20aims%20to%20locate%20and%20segment%0Amoving%20objects%20in%20point%20clouds%20of%20the%20current%20scan%20using%20motion%20information%0Afrom%20previous%20scans.%20Despite%20the%20promising%20results%20achieved%20by%20previous%20MOS%0Amethods%2C%20several%20key%20issues%2C%20such%20as%20the%20weak%20coupling%20of%20temporal%20and%20spatial%0Ainformation%2C%20still%20need%20further%20study.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0ALiDAR-based%203D%20Moving%20Object%20Segmentation%20with%20Motion-aware%20State%20Space%20Model%2C%0Atermed%20MambaMOS.%20Firstly%2C%20we%20develop%20a%20novel%20embedding%20module%2C%20the%20Time%20Clue%0ABootstrapping%20Embedding%20%28TCBE%29%2C%20to%20enhance%20the%20coupling%20of%20temporal%20and%20spatial%0Ainformation%20in%20point%20clouds%20and%20alleviate%20the%20issue%20of%20overlooked%20temporal%0Aclues.%20Secondly%2C%20we%20introduce%20the%20Motion-aware%20State%20Space%20Model%20%28MSSM%29%20to%0Aendow%20the%20model%20with%20the%20capacity%20to%20understand%20the%20temporal%20correlations%20of%0Athe%20same%20object%20across%20different%20time%20steps.%20Specifically%2C%20MSSM%20emphasizes%20the%0Amotion%20states%20of%20the%20same%20object%20at%20different%20time%20steps%20through%20two%20distinct%0Atemporal%20modeling%20and%20correlation%20steps.%20We%20utilize%20an%20improved%20state%20space%0Amodel%20to%20represent%20these%20motion%20differences%2C%20significantly%20modeling%20the%20motion%0Astates.%20Finally%2C%20extensive%20experiments%20on%20the%20SemanticKITTI-MOS%20and%20KITTI-Road%0Abenchmarks%20demonstrate%20that%20the%20proposed%20MambaMOS%20achieves%20state-of-the-art%0Aperformance.%20The%20source%20code%20of%20this%20work%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/Terminal-K/MambaMOS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12794v1&entry.124074799=Read"},
{"title": "Sentiment-oriented Transformer-based Variational Autoencoder Network for\n  Live Video Commenting", "author": "Fengyi Fu and Shancheng Fang and Weidong Chen and Zhendong Mao", "abstract": "  Automatic live video commenting is with increasing attention due to its\nsignificance in narration generation, topic explanation, etc. However, the\ndiverse sentiment consideration of the generated comments is missing from the\ncurrent methods. Sentimental factors are critical in interactive commenting,\nand lack of research so far. Thus, in this paper, we propose a\nSentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network\nwhich consists of a sentiment-oriented diversity encoder module and a batch\nattention module, to achieve diverse video commenting with multiple sentiments\nand multiple semantics. Specifically, our sentiment-oriented diversity encoder\nelegantly combines VAE and random mask mechanism to achieve semantic diversity\nunder sentiment guidance, which is then fused with cross-modal features to\ngenerate live video comments. Furthermore, a batch attention module is also\nproposed in this paper to alleviate the problem of missing sentimental samples,\ncaused by the data imbalance, which is common in live videos as the popularity\nof videos varies. Extensive experiments on Livebot and VideoIC datasets\ndemonstrate that the proposed So-TVAE outperforms the state-of-the-art methods\nin terms of the quality and diversity of generated comments. Related code is\navailable at https://github.com/fufy1024/So-TVAE.\n", "link": "http://arxiv.org/abs/2404.12782v1", "date": "2024-04-19", "relevancy": 2.2212, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5702}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5671}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5357}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sentiment-oriented%20Transformer-based%20Variational%20Autoencoder%20Network%20for%0A%20%20Live%20Video%20Commenting&body=Title%3A%20Sentiment-oriented%20Transformer-based%20Variational%20Autoencoder%20Network%20for%0A%20%20Live%20Video%20Commenting%0AAuthor%3A%20Fengyi%20Fu%20and%20Shancheng%20Fang%20and%20Weidong%20Chen%20and%20Zhendong%20Mao%0AAbstract%3A%20%20%20Automatic%20live%20video%20commenting%20is%20with%20increasing%20attention%20due%20to%20its%0Asignificance%20in%20narration%20generation%2C%20topic%20explanation%2C%20etc.%20However%2C%20the%0Adiverse%20sentiment%20consideration%20of%20the%20generated%20comments%20is%20missing%20from%20the%0Acurrent%20methods.%20Sentimental%20factors%20are%20critical%20in%20interactive%20commenting%2C%0Aand%20lack%20of%20research%20so%20far.%20Thus%2C%20in%20this%20paper%2C%20we%20propose%20a%0ASentiment-oriented%20Transformer-based%20Variational%20Autoencoder%20%28So-TVAE%29%20network%0Awhich%20consists%20of%20a%20sentiment-oriented%20diversity%20encoder%20module%20and%20a%20batch%0Aattention%20module%2C%20to%20achieve%20diverse%20video%20commenting%20with%20multiple%20sentiments%0Aand%20multiple%20semantics.%20Specifically%2C%20our%20sentiment-oriented%20diversity%20encoder%0Aelegantly%20combines%20VAE%20and%20random%20mask%20mechanism%20to%20achieve%20semantic%20diversity%0Aunder%20sentiment%20guidance%2C%20which%20is%20then%20fused%20with%20cross-modal%20features%20to%0Agenerate%20live%20video%20comments.%20Furthermore%2C%20a%20batch%20attention%20module%20is%20also%0Aproposed%20in%20this%20paper%20to%20alleviate%20the%20problem%20of%20missing%20sentimental%20samples%2C%0Acaused%20by%20the%20data%20imbalance%2C%20which%20is%20common%20in%20live%20videos%20as%20the%20popularity%0Aof%20videos%20varies.%20Extensive%20experiments%20on%20Livebot%20and%20VideoIC%20datasets%0Ademonstrate%20that%20the%20proposed%20So-TVAE%20outperforms%20the%20state-of-the-art%20methods%0Ain%20terms%20of%20the%20quality%20and%20diversity%20of%20generated%20comments.%20Related%20code%20is%0Aavailable%20at%20https%3A//github.com/fufy1024/So-TVAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12782v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sentiment-oriented%20Transformer-based%20Variational%20Autoencoder%20Network%20for%0A%20%20Live%20Video%20Commenting&entry.906535625=Fengyi%20Fu%20and%20Shancheng%20Fang%20and%20Weidong%20Chen%20and%20Zhendong%20Mao&entry.1292438233=%20%20Automatic%20live%20video%20commenting%20is%20with%20increasing%20attention%20due%20to%20its%0Asignificance%20in%20narration%20generation%2C%20topic%20explanation%2C%20etc.%20However%2C%20the%0Adiverse%20sentiment%20consideration%20of%20the%20generated%20comments%20is%20missing%20from%20the%0Acurrent%20methods.%20Sentimental%20factors%20are%20critical%20in%20interactive%20commenting%2C%0Aand%20lack%20of%20research%20so%20far.%20Thus%2C%20in%20this%20paper%2C%20we%20propose%20a%0ASentiment-oriented%20Transformer-based%20Variational%20Autoencoder%20%28So-TVAE%29%20network%0Awhich%20consists%20of%20a%20sentiment-oriented%20diversity%20encoder%20module%20and%20a%20batch%0Aattention%20module%2C%20to%20achieve%20diverse%20video%20commenting%20with%20multiple%20sentiments%0Aand%20multiple%20semantics.%20Specifically%2C%20our%20sentiment-oriented%20diversity%20encoder%0Aelegantly%20combines%20VAE%20and%20random%20mask%20mechanism%20to%20achieve%20semantic%20diversity%0Aunder%20sentiment%20guidance%2C%20which%20is%20then%20fused%20with%20cross-modal%20features%20to%0Agenerate%20live%20video%20comments.%20Furthermore%2C%20a%20batch%20attention%20module%20is%20also%0Aproposed%20in%20this%20paper%20to%20alleviate%20the%20problem%20of%20missing%20sentimental%20samples%2C%0Acaused%20by%20the%20data%20imbalance%2C%20which%20is%20common%20in%20live%20videos%20as%20the%20popularity%0Aof%20videos%20varies.%20Extensive%20experiments%20on%20Livebot%20and%20VideoIC%20datasets%0Ademonstrate%20that%20the%20proposed%20So-TVAE%20outperforms%20the%20state-of-the-art%20methods%0Ain%20terms%20of%20the%20quality%20and%20diversity%20of%20generated%20comments.%20Related%20code%20is%0Aavailable%20at%20https%3A//github.com/fufy1024/So-TVAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12782v1&entry.124074799=Read"},
{"title": "Learn2Talk: 3D Talking Face Learns from 2D Talking Face", "author": "Yixiang Zhuang and Baoping Cheng and Yao Cheng and Yuntao Jin and Renshuai Liu and Chengyang Li and Xuan Cheng and Jing Liao and Juncong Lin", "abstract": "  Speech-driven facial animation methods usually contain two main classes, 3D\nand 2D talking face, both of which attract considerable research attention in\nrecent years. However, to the best of our knowledge, the research on 3D talking\nface does not go deeper as 2D talking face, in the aspect of\nlip-synchronization (lip-sync) and speech perception. To mind the gap between\nthe two sub-fields, we propose a learning framework named Learn2Talk, which can\nconstruct a better 3D talking face network by exploiting two expertise points\nfrom the field of 2D talking face. Firstly, inspired by the audio-video sync\nnetwork, a 3D sync-lip expert model is devised for the pursuit of lip-sync\nbetween audio and 3D facial motion. Secondly, a teacher model selected from 2D\ntalking face methods is used to guide the training of the audio-to-3D motions\nregression network to yield more 3D vertex accuracy. Extensive experiments show\nthe advantages of the proposed framework in terms of lip-sync, vertex accuracy\nand speech perception, compared with state-of-the-arts. Finally, we show two\napplications of the proposed framework: audio-visual speech recognition and\nspeech-driven 3D Gaussian Splatting based avatar animation.\n", "link": "http://arxiv.org/abs/2404.12888v1", "date": "2024-04-19", "relevancy": 2.2005, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5728}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5278}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learn2Talk%3A%203D%20Talking%20Face%20Learns%20from%202D%20Talking%20Face&body=Title%3A%20Learn2Talk%3A%203D%20Talking%20Face%20Learns%20from%202D%20Talking%20Face%0AAuthor%3A%20Yixiang%20Zhuang%20and%20Baoping%20Cheng%20and%20Yao%20Cheng%20and%20Yuntao%20Jin%20and%20Renshuai%20Liu%20and%20Chengyang%20Li%20and%20Xuan%20Cheng%20and%20Jing%20Liao%20and%20Juncong%20Lin%0AAbstract%3A%20%20%20Speech-driven%20facial%20animation%20methods%20usually%20contain%20two%20main%20classes%2C%203D%0Aand%202D%20talking%20face%2C%20both%20of%20which%20attract%20considerable%20research%20attention%20in%0Arecent%20years.%20However%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20research%20on%203D%20talking%0Aface%20does%20not%20go%20deeper%20as%202D%20talking%20face%2C%20in%20the%20aspect%20of%0Alip-synchronization%20%28lip-sync%29%20and%20speech%20perception.%20To%20mind%20the%20gap%20between%0Athe%20two%20sub-fields%2C%20we%20propose%20a%20learning%20framework%20named%20Learn2Talk%2C%20which%20can%0Aconstruct%20a%20better%203D%20talking%20face%20network%20by%20exploiting%20two%20expertise%20points%0Afrom%20the%20field%20of%202D%20talking%20face.%20Firstly%2C%20inspired%20by%20the%20audio-video%20sync%0Anetwork%2C%20a%203D%20sync-lip%20expert%20model%20is%20devised%20for%20the%20pursuit%20of%20lip-sync%0Abetween%20audio%20and%203D%20facial%20motion.%20Secondly%2C%20a%20teacher%20model%20selected%20from%202D%0Atalking%20face%20methods%20is%20used%20to%20guide%20the%20training%20of%20the%20audio-to-3D%20motions%0Aregression%20network%20to%20yield%20more%203D%20vertex%20accuracy.%20Extensive%20experiments%20show%0Athe%20advantages%20of%20the%20proposed%20framework%20in%20terms%20of%20lip-sync%2C%20vertex%20accuracy%0Aand%20speech%20perception%2C%20compared%20with%20state-of-the-arts.%20Finally%2C%20we%20show%20two%0Aapplications%20of%20the%20proposed%20framework%3A%20audio-visual%20speech%20recognition%20and%0Aspeech-driven%203D%20Gaussian%20Splatting%20based%20avatar%20animation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12888v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn2Talk%3A%203D%20Talking%20Face%20Learns%20from%202D%20Talking%20Face&entry.906535625=Yixiang%20Zhuang%20and%20Baoping%20Cheng%20and%20Yao%20Cheng%20and%20Yuntao%20Jin%20and%20Renshuai%20Liu%20and%20Chengyang%20Li%20and%20Xuan%20Cheng%20and%20Jing%20Liao%20and%20Juncong%20Lin&entry.1292438233=%20%20Speech-driven%20facial%20animation%20methods%20usually%20contain%20two%20main%20classes%2C%203D%0Aand%202D%20talking%20face%2C%20both%20of%20which%20attract%20considerable%20research%20attention%20in%0Arecent%20years.%20However%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20research%20on%203D%20talking%0Aface%20does%20not%20go%20deeper%20as%202D%20talking%20face%2C%20in%20the%20aspect%20of%0Alip-synchronization%20%28lip-sync%29%20and%20speech%20perception.%20To%20mind%20the%20gap%20between%0Athe%20two%20sub-fields%2C%20we%20propose%20a%20learning%20framework%20named%20Learn2Talk%2C%20which%20can%0Aconstruct%20a%20better%203D%20talking%20face%20network%20by%20exploiting%20two%20expertise%20points%0Afrom%20the%20field%20of%202D%20talking%20face.%20Firstly%2C%20inspired%20by%20the%20audio-video%20sync%0Anetwork%2C%20a%203D%20sync-lip%20expert%20model%20is%20devised%20for%20the%20pursuit%20of%20lip-sync%0Abetween%20audio%20and%203D%20facial%20motion.%20Secondly%2C%20a%20teacher%20model%20selected%20from%202D%0Atalking%20face%20methods%20is%20used%20to%20guide%20the%20training%20of%20the%20audio-to-3D%20motions%0Aregression%20network%20to%20yield%20more%203D%20vertex%20accuracy.%20Extensive%20experiments%20show%0Athe%20advantages%20of%20the%20proposed%20framework%20in%20terms%20of%20lip-sync%2C%20vertex%20accuracy%0Aand%20speech%20perception%2C%20compared%20with%20state-of-the-arts.%20Finally%2C%20we%20show%20two%0Aapplications%20of%20the%20proposed%20framework%3A%20audio-visual%20speech%20recognition%20and%0Aspeech-driven%203D%20Gaussian%20Splatting%20based%20avatar%20animation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12888v1&entry.124074799=Read"},
{"title": "Foundation Model assisted Weakly Supervised LiDAR Semantic Segmentation", "author": "Yilong Chen and Zongyi Xu and xiaoshui Huang and Ruicheng Zhang and Xinqi Jiang and Xinbo Gao", "abstract": "  Current point cloud semantic segmentation has achieved great advances when\ngiven sufficient labels. However, the dense annotation of LiDAR point clouds\nremains prohibitively expensive and time-consuming, unable to keep up with the\ncontinuously growing volume of data. In this paper, we propose annotating\nimages with scattered points, followed by utilizing SAM (a Foundation model) to\ngenerate semantic segmentation labels for the images. Finally, by mapping the\nsegmentation labels of the images to the LiDAR space using the intrinsic and\nextrinsic parameters of the camera and LiDAR, we obtain labels for point cloud\nsemantic segmentation, and release Scatter-KITTI and Scatter-nuScenes, which\nare the first works to utilize image segmentation-based SAM for weakly\nsupervised point cloud semantic segmentation. Furthermore, to mitigate the\ninfluence of erroneous pseudo labels obtained from sparse annotations on point\ncloud features, we propose a multi-modal weakly supervised network for LiDAR\nsemantic segmentation, called MM-ScatterNet. This network combines features\nfrom both point cloud and image modalities, enhancing the representation\nlearning of point clouds by introducing consistency constraints between\nmulti-modal features and point cloud features. On the SemanticKITTI dataset, we\nachieve 66\\% of fully supervised performance using only 0.02% of annotated\ndata, and on the NuScenes dataset, we achieve 95% of fully supervised\nperformance using only 0.1% labeled points.\n", "link": "http://arxiv.org/abs/2404.12861v1", "date": "2024-04-19", "relevancy": 2.1864, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5496}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5482}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5438}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Foundation%20Model%20assisted%20Weakly%20Supervised%20LiDAR%20Semantic%20Segmentation&body=Title%3A%20Foundation%20Model%20assisted%20Weakly%20Supervised%20LiDAR%20Semantic%20Segmentation%0AAuthor%3A%20Yilong%20Chen%20and%20Zongyi%20Xu%20and%20xiaoshui%20Huang%20and%20Ruicheng%20Zhang%20and%20Xinqi%20Jiang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Current%20point%20cloud%20semantic%20segmentation%20has%20achieved%20great%20advances%20when%0Agiven%20sufficient%20labels.%20However%2C%20the%20dense%20annotation%20of%20LiDAR%20point%20clouds%0Aremains%20prohibitively%20expensive%20and%20time-consuming%2C%20unable%20to%20keep%20up%20with%20the%0Acontinuously%20growing%20volume%20of%20data.%20In%20this%20paper%2C%20we%20propose%20annotating%0Aimages%20with%20scattered%20points%2C%20followed%20by%20utilizing%20SAM%20%28a%20Foundation%20model%29%20to%0Agenerate%20semantic%20segmentation%20labels%20for%20the%20images.%20Finally%2C%20by%20mapping%20the%0Asegmentation%20labels%20of%20the%20images%20to%20the%20LiDAR%20space%20using%20the%20intrinsic%20and%0Aextrinsic%20parameters%20of%20the%20camera%20and%20LiDAR%2C%20we%20obtain%20labels%20for%20point%20cloud%0Asemantic%20segmentation%2C%20and%20release%20Scatter-KITTI%20and%20Scatter-nuScenes%2C%20which%0Aare%20the%20first%20works%20to%20utilize%20image%20segmentation-based%20SAM%20for%20weakly%0Asupervised%20point%20cloud%20semantic%20segmentation.%20Furthermore%2C%20to%20mitigate%20the%0Ainfluence%20of%20erroneous%20pseudo%20labels%20obtained%20from%20sparse%20annotations%20on%20point%0Acloud%20features%2C%20we%20propose%20a%20multi-modal%20weakly%20supervised%20network%20for%20LiDAR%0Asemantic%20segmentation%2C%20called%20MM-ScatterNet.%20This%20network%20combines%20features%0Afrom%20both%20point%20cloud%20and%20image%20modalities%2C%20enhancing%20the%20representation%0Alearning%20of%20point%20clouds%20by%20introducing%20consistency%20constraints%20between%0Amulti-modal%20features%20and%20point%20cloud%20features.%20On%20the%20SemanticKITTI%20dataset%2C%20we%0Aachieve%2066%5C%25%20of%20fully%20supervised%20performance%20using%20only%200.02%25%20of%20annotated%0Adata%2C%20and%20on%20the%20NuScenes%20dataset%2C%20we%20achieve%2095%25%20of%20fully%20supervised%0Aperformance%20using%20only%200.1%25%20labeled%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12861v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Model%20assisted%20Weakly%20Supervised%20LiDAR%20Semantic%20Segmentation&entry.906535625=Yilong%20Chen%20and%20Zongyi%20Xu%20and%20xiaoshui%20Huang%20and%20Ruicheng%20Zhang%20and%20Xinqi%20Jiang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Current%20point%20cloud%20semantic%20segmentation%20has%20achieved%20great%20advances%20when%0Agiven%20sufficient%20labels.%20However%2C%20the%20dense%20annotation%20of%20LiDAR%20point%20clouds%0Aremains%20prohibitively%20expensive%20and%20time-consuming%2C%20unable%20to%20keep%20up%20with%20the%0Acontinuously%20growing%20volume%20of%20data.%20In%20this%20paper%2C%20we%20propose%20annotating%0Aimages%20with%20scattered%20points%2C%20followed%20by%20utilizing%20SAM%20%28a%20Foundation%20model%29%20to%0Agenerate%20semantic%20segmentation%20labels%20for%20the%20images.%20Finally%2C%20by%20mapping%20the%0Asegmentation%20labels%20of%20the%20images%20to%20the%20LiDAR%20space%20using%20the%20intrinsic%20and%0Aextrinsic%20parameters%20of%20the%20camera%20and%20LiDAR%2C%20we%20obtain%20labels%20for%20point%20cloud%0Asemantic%20segmentation%2C%20and%20release%20Scatter-KITTI%20and%20Scatter-nuScenes%2C%20which%0Aare%20the%20first%20works%20to%20utilize%20image%20segmentation-based%20SAM%20for%20weakly%0Asupervised%20point%20cloud%20semantic%20segmentation.%20Furthermore%2C%20to%20mitigate%20the%0Ainfluence%20of%20erroneous%20pseudo%20labels%20obtained%20from%20sparse%20annotations%20on%20point%0Acloud%20features%2C%20we%20propose%20a%20multi-modal%20weakly%20supervised%20network%20for%20LiDAR%0Asemantic%20segmentation%2C%20called%20MM-ScatterNet.%20This%20network%20combines%20features%0Afrom%20both%20point%20cloud%20and%20image%20modalities%2C%20enhancing%20the%20representation%0Alearning%20of%20point%20clouds%20by%20introducing%20consistency%20constraints%20between%0Amulti-modal%20features%20and%20point%20cloud%20features.%20On%20the%20SemanticKITTI%20dataset%2C%20we%0Aachieve%2066%5C%25%20of%20fully%20supervised%20performance%20using%20only%200.02%25%20of%20annotated%0Adata%2C%20and%20on%20the%20NuScenes%20dataset%2C%20we%20achieve%2095%25%20of%20fully%20supervised%0Aperformance%20using%20only%200.1%25%20labeled%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12861v1&entry.124074799=Read"},
{"title": "An Analysis of Driver-Initiated Takeovers during Assisted Driving and\n  their Effect on Driver Satisfaction", "author": "Robin Schwager and Michael Grimm and Xin Liu and Lukas Ewecker and Tim Bruehl and Tin Stribor Sohn and Soeren Hohmann", "abstract": "  During the use of Advanced Driver Assistance Systems (ADAS), drivers can\nintervene in the active function and take back control due to various reasons.\nHowever, the specific reasons for driver-initiated takeovers in naturalistic\ndriving are still not well understood. In order to get more information on the\nreasons behind these takeovers, a test group study was conducted. There, 17\nparticipants used a predictive longitudinal driving function for their daily\ncommutes and annotated the reasons for their takeovers during active function\nuse. In this paper, the recorded takeovers are analyzed and the different\nreasons for them are highlighted. The results show that the reasons can be\ndivided into three main categories. The most common category consists of\ntakeovers which aim to adjust the behavior of the ADAS within its Operational\nDesign Domain (ODD) in order to better match the drivers' personal preferences.\nOther reasons include takeovers due to leaving the ADAS's ODD and corrections\nof incorrect sensing state information. Using the questionnaire results of the\ntest group study, it was found that the number and frequency of takeovers\nespecially within the ADAS's ODD have a significant negative impact on driver\nsatisfaction. Therefore, the driver satisfaction with the ADAS could be\nincreased by adapting its behavior to the drivers' wishes and thereby lowering\nthe number of takeovers within the ODD. The information contained in the\ntakeover behavior of the drivers could be used as feedback for the ADAS.\nFinally, it is shown that there are considerable differences in the takeover\nbehavior of different drivers, which shows a need for ADAS individualization.\n", "link": "http://arxiv.org/abs/2404.13027v1", "date": "2024-04-19", "relevancy": 2.1662, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4604}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4348}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4045}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Analysis%20of%20Driver-Initiated%20Takeovers%20during%20Assisted%20Driving%20and%0A%20%20their%20Effect%20on%20Driver%20Satisfaction&body=Title%3A%20An%20Analysis%20of%20Driver-Initiated%20Takeovers%20during%20Assisted%20Driving%20and%0A%20%20their%20Effect%20on%20Driver%20Satisfaction%0AAuthor%3A%20Robin%20Schwager%20and%20Michael%20Grimm%20and%20Xin%20Liu%20and%20Lukas%20Ewecker%20and%20Tim%20Bruehl%20and%20Tin%20Stribor%20Sohn%20and%20Soeren%20Hohmann%0AAbstract%3A%20%20%20During%20the%20use%20of%20Advanced%20Driver%20Assistance%20Systems%20%28ADAS%29%2C%20drivers%20can%0Aintervene%20in%20the%20active%20function%20and%20take%20back%20control%20due%20to%20various%20reasons.%0AHowever%2C%20the%20specific%20reasons%20for%20driver-initiated%20takeovers%20in%20naturalistic%0Adriving%20are%20still%20not%20well%20understood.%20In%20order%20to%20get%20more%20information%20on%20the%0Areasons%20behind%20these%20takeovers%2C%20a%20test%20group%20study%20was%20conducted.%20There%2C%2017%0Aparticipants%20used%20a%20predictive%20longitudinal%20driving%20function%20for%20their%20daily%0Acommutes%20and%20annotated%20the%20reasons%20for%20their%20takeovers%20during%20active%20function%0Ause.%20In%20this%20paper%2C%20the%20recorded%20takeovers%20are%20analyzed%20and%20the%20different%0Areasons%20for%20them%20are%20highlighted.%20The%20results%20show%20that%20the%20reasons%20can%20be%0Adivided%20into%20three%20main%20categories.%20The%20most%20common%20category%20consists%20of%0Atakeovers%20which%20aim%20to%20adjust%20the%20behavior%20of%20the%20ADAS%20within%20its%20Operational%0ADesign%20Domain%20%28ODD%29%20in%20order%20to%20better%20match%20the%20drivers%27%20personal%20preferences.%0AOther%20reasons%20include%20takeovers%20due%20to%20leaving%20the%20ADAS%27s%20ODD%20and%20corrections%0Aof%20incorrect%20sensing%20state%20information.%20Using%20the%20questionnaire%20results%20of%20the%0Atest%20group%20study%2C%20it%20was%20found%20that%20the%20number%20and%20frequency%20of%20takeovers%0Aespecially%20within%20the%20ADAS%27s%20ODD%20have%20a%20significant%20negative%20impact%20on%20driver%0Asatisfaction.%20Therefore%2C%20the%20driver%20satisfaction%20with%20the%20ADAS%20could%20be%0Aincreased%20by%20adapting%20its%20behavior%20to%20the%20drivers%27%20wishes%20and%20thereby%20lowering%0Athe%20number%20of%20takeovers%20within%20the%20ODD.%20The%20information%20contained%20in%20the%0Atakeover%20behavior%20of%20the%20drivers%20could%20be%20used%20as%20feedback%20for%20the%20ADAS.%0AFinally%2C%20it%20is%20shown%20that%20there%20are%20considerable%20differences%20in%20the%20takeover%0Abehavior%20of%20different%20drivers%2C%20which%20shows%20a%20need%20for%20ADAS%20individualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13027v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Analysis%20of%20Driver-Initiated%20Takeovers%20during%20Assisted%20Driving%20and%0A%20%20their%20Effect%20on%20Driver%20Satisfaction&entry.906535625=Robin%20Schwager%20and%20Michael%20Grimm%20and%20Xin%20Liu%20and%20Lukas%20Ewecker%20and%20Tim%20Bruehl%20and%20Tin%20Stribor%20Sohn%20and%20Soeren%20Hohmann&entry.1292438233=%20%20During%20the%20use%20of%20Advanced%20Driver%20Assistance%20Systems%20%28ADAS%29%2C%20drivers%20can%0Aintervene%20in%20the%20active%20function%20and%20take%20back%20control%20due%20to%20various%20reasons.%0AHowever%2C%20the%20specific%20reasons%20for%20driver-initiated%20takeovers%20in%20naturalistic%0Adriving%20are%20still%20not%20well%20understood.%20In%20order%20to%20get%20more%20information%20on%20the%0Areasons%20behind%20these%20takeovers%2C%20a%20test%20group%20study%20was%20conducted.%20There%2C%2017%0Aparticipants%20used%20a%20predictive%20longitudinal%20driving%20function%20for%20their%20daily%0Acommutes%20and%20annotated%20the%20reasons%20for%20their%20takeovers%20during%20active%20function%0Ause.%20In%20this%20paper%2C%20the%20recorded%20takeovers%20are%20analyzed%20and%20the%20different%0Areasons%20for%20them%20are%20highlighted.%20The%20results%20show%20that%20the%20reasons%20can%20be%0Adivided%20into%20three%20main%20categories.%20The%20most%20common%20category%20consists%20of%0Atakeovers%20which%20aim%20to%20adjust%20the%20behavior%20of%20the%20ADAS%20within%20its%20Operational%0ADesign%20Domain%20%28ODD%29%20in%20order%20to%20better%20match%20the%20drivers%27%20personal%20preferences.%0AOther%20reasons%20include%20takeovers%20due%20to%20leaving%20the%20ADAS%27s%20ODD%20and%20corrections%0Aof%20incorrect%20sensing%20state%20information.%20Using%20the%20questionnaire%20results%20of%20the%0Atest%20group%20study%2C%20it%20was%20found%20that%20the%20number%20and%20frequency%20of%20takeovers%0Aespecially%20within%20the%20ADAS%27s%20ODD%20have%20a%20significant%20negative%20impact%20on%20driver%0Asatisfaction.%20Therefore%2C%20the%20driver%20satisfaction%20with%20the%20ADAS%20could%20be%0Aincreased%20by%20adapting%20its%20behavior%20to%20the%20drivers%27%20wishes%20and%20thereby%20lowering%0Athe%20number%20of%20takeovers%20within%20the%20ODD.%20The%20information%20contained%20in%20the%0Atakeover%20behavior%20of%20the%20drivers%20could%20be%20used%20as%20feedback%20for%20the%20ADAS.%0AFinally%2C%20it%20is%20shown%20that%20there%20are%20considerable%20differences%20in%20the%20takeover%0Abehavior%20of%20different%20drivers%2C%20which%20shows%20a%20need%20for%20ADAS%20individualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13027v1&entry.124074799=Read"},
{"title": "A Large-scale Medical Visual Task Adaptation Benchmark", "author": "Shentong Mo and Xufang Luo and Yansen Wang and Dongsheng Li", "abstract": "  Visual task adaptation has been demonstrated to be effective in adapting\npre-trained Vision Transformers (ViTs) to general downstream visual tasks using\nspecialized learnable layers or tokens. However, there is yet a large-scale\nbenchmark to fully explore the effect of visual task adaptation on the\nrealistic and important medical domain, particularly across diverse medical\nvisual modalities, such as color images, X-ray, and CT. To close this gap, we\npresent Med-VTAB, a large-scale Medical Visual Task Adaptation Benchmark\nconsisting of 1.68 million medical images for diverse organs, modalities, and\nadaptation approaches. Based on Med-VTAB, we explore the scaling law of medical\nprompt tuning concerning tunable parameters and the generalizability of medical\nvisual adaptation using non-medical/medical pre-train weights. Besides, we\nstudy the impact of patient ID out-of-distribution on medical visual\nadaptation, which is a real and challenging scenario. Furthermore, results from\nMed-VTAB indicate that a single pre-trained model falls short in medical task\nadaptation. Therefore, we introduce GMoE-Adapter, a novel method that combines\nmedical and general pre-training weights through a gated mixture-of-experts\nadapter, achieving state-of-the-art results in medical visual task adaptation.\n", "link": "http://arxiv.org/abs/2404.12876v1", "date": "2024-04-19", "relevancy": 2.1496, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5176}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5134}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Large-scale%20Medical%20Visual%20Task%20Adaptation%20Benchmark&body=Title%3A%20A%20Large-scale%20Medical%20Visual%20Task%20Adaptation%20Benchmark%0AAuthor%3A%20Shentong%20Mo%20and%20Xufang%20Luo%20and%20Yansen%20Wang%20and%20Dongsheng%20Li%0AAbstract%3A%20%20%20Visual%20task%20adaptation%20has%20been%20demonstrated%20to%20be%20effective%20in%20adapting%0Apre-trained%20Vision%20Transformers%20%28ViTs%29%20to%20general%20downstream%20visual%20tasks%20using%0Aspecialized%20learnable%20layers%20or%20tokens.%20However%2C%20there%20is%20yet%20a%20large-scale%0Abenchmark%20to%20fully%20explore%20the%20effect%20of%20visual%20task%20adaptation%20on%20the%0Arealistic%20and%20important%20medical%20domain%2C%20particularly%20across%20diverse%20medical%0Avisual%20modalities%2C%20such%20as%20color%20images%2C%20X-ray%2C%20and%20CT.%20To%20close%20this%20gap%2C%20we%0Apresent%20Med-VTAB%2C%20a%20large-scale%20Medical%20Visual%20Task%20Adaptation%20Benchmark%0Aconsisting%20of%201.68%20million%20medical%20images%20for%20diverse%20organs%2C%20modalities%2C%20and%0Aadaptation%20approaches.%20Based%20on%20Med-VTAB%2C%20we%20explore%20the%20scaling%20law%20of%20medical%0Aprompt%20tuning%20concerning%20tunable%20parameters%20and%20the%20generalizability%20of%20medical%0Avisual%20adaptation%20using%20non-medical/medical%20pre-train%20weights.%20Besides%2C%20we%0Astudy%20the%20impact%20of%20patient%20ID%20out-of-distribution%20on%20medical%20visual%0Aadaptation%2C%20which%20is%20a%20real%20and%20challenging%20scenario.%20Furthermore%2C%20results%20from%0AMed-VTAB%20indicate%20that%20a%20single%20pre-trained%20model%20falls%20short%20in%20medical%20task%0Aadaptation.%20Therefore%2C%20we%20introduce%20GMoE-Adapter%2C%20a%20novel%20method%20that%20combines%0Amedical%20and%20general%20pre-training%20weights%20through%20a%20gated%20mixture-of-experts%0Aadapter%2C%20achieving%20state-of-the-art%20results%20in%20medical%20visual%20task%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12876v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-scale%20Medical%20Visual%20Task%20Adaptation%20Benchmark&entry.906535625=Shentong%20Mo%20and%20Xufang%20Luo%20and%20Yansen%20Wang%20and%20Dongsheng%20Li&entry.1292438233=%20%20Visual%20task%20adaptation%20has%20been%20demonstrated%20to%20be%20effective%20in%20adapting%0Apre-trained%20Vision%20Transformers%20%28ViTs%29%20to%20general%20downstream%20visual%20tasks%20using%0Aspecialized%20learnable%20layers%20or%20tokens.%20However%2C%20there%20is%20yet%20a%20large-scale%0Abenchmark%20to%20fully%20explore%20the%20effect%20of%20visual%20task%20adaptation%20on%20the%0Arealistic%20and%20important%20medical%20domain%2C%20particularly%20across%20diverse%20medical%0Avisual%20modalities%2C%20such%20as%20color%20images%2C%20X-ray%2C%20and%20CT.%20To%20close%20this%20gap%2C%20we%0Apresent%20Med-VTAB%2C%20a%20large-scale%20Medical%20Visual%20Task%20Adaptation%20Benchmark%0Aconsisting%20of%201.68%20million%20medical%20images%20for%20diverse%20organs%2C%20modalities%2C%20and%0Aadaptation%20approaches.%20Based%20on%20Med-VTAB%2C%20we%20explore%20the%20scaling%20law%20of%20medical%0Aprompt%20tuning%20concerning%20tunable%20parameters%20and%20the%20generalizability%20of%20medical%0Avisual%20adaptation%20using%20non-medical/medical%20pre-train%20weights.%20Besides%2C%20we%0Astudy%20the%20impact%20of%20patient%20ID%20out-of-distribution%20on%20medical%20visual%0Aadaptation%2C%20which%20is%20a%20real%20and%20challenging%20scenario.%20Furthermore%2C%20results%20from%0AMed-VTAB%20indicate%20that%20a%20single%20pre-trained%20model%20falls%20short%20in%20medical%20task%0Aadaptation.%20Therefore%2C%20we%20introduce%20GMoE-Adapter%2C%20a%20novel%20method%20that%20combines%0Amedical%20and%20general%20pre-training%20weights%20through%20a%20gated%20mixture-of-experts%0Aadapter%2C%20achieving%20state-of-the-art%20results%20in%20medical%20visual%20task%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12876v1&entry.124074799=Read"},
{"title": "Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned\n  Reinforcement Learning", "author": "Lisheng Wu and Ke Chen", "abstract": "  Exploration efficiency poses a significant challenge in goal-conditioned\nreinforcement learning (GCRL) tasks, particularly those with long horizons and\nsparse rewards. A primary limitation to exploration efficiency is the agent's\ninability to leverage environmental structural patterns. In this study, we\nintroduce a novel framework, GEASD, designed to capture these patterns through\nan adaptive skill distribution during the learning process. This distribution\noptimizes the local entropy of achieved goals within a contextual horizon,\nenhancing goal-spreading behaviors and facilitating deep exploration in states\ncontaining familiar structural patterns. Our experiments reveal marked\nimprovements in exploration efficiency using the adaptive skill distribution\ncompared to a uniform skill distribution. Additionally, the learned skill\ndistribution demonstrates robust generalization capabilities, achieving\nsubstantial exploration progress in unseen tasks containing similar local\nstructures.\n", "link": "http://arxiv.org/abs/2404.12999v1", "date": "2024-04-19", "relevancy": 2.1347, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6206}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5235}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5091}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Goal%20Exploration%20via%20Adaptive%20Skill%20Distribution%20for%20Goal-Conditioned%0A%20%20Reinforcement%20Learning&body=Title%3A%20Goal%20Exploration%20via%20Adaptive%20Skill%20Distribution%20for%20Goal-Conditioned%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Lisheng%20Wu%20and%20Ke%20Chen%0AAbstract%3A%20%20%20Exploration%20efficiency%20poses%20a%20significant%20challenge%20in%20goal-conditioned%0Areinforcement%20learning%20%28GCRL%29%20tasks%2C%20particularly%20those%20with%20long%20horizons%20and%0Asparse%20rewards.%20A%20primary%20limitation%20to%20exploration%20efficiency%20is%20the%20agent%27s%0Ainability%20to%20leverage%20environmental%20structural%20patterns.%20In%20this%20study%2C%20we%0Aintroduce%20a%20novel%20framework%2C%20GEASD%2C%20designed%20to%20capture%20these%20patterns%20through%0Aan%20adaptive%20skill%20distribution%20during%20the%20learning%20process.%20This%20distribution%0Aoptimizes%20the%20local%20entropy%20of%20achieved%20goals%20within%20a%20contextual%20horizon%2C%0Aenhancing%20goal-spreading%20behaviors%20and%20facilitating%20deep%20exploration%20in%20states%0Acontaining%20familiar%20structural%20patterns.%20Our%20experiments%20reveal%20marked%0Aimprovements%20in%20exploration%20efficiency%20using%20the%20adaptive%20skill%20distribution%0Acompared%20to%20a%20uniform%20skill%20distribution.%20Additionally%2C%20the%20learned%20skill%0Adistribution%20demonstrates%20robust%20generalization%20capabilities%2C%20achieving%0Asubstantial%20exploration%20progress%20in%20unseen%20tasks%20containing%20similar%20local%0Astructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12999v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal%20Exploration%20via%20Adaptive%20Skill%20Distribution%20for%20Goal-Conditioned%0A%20%20Reinforcement%20Learning&entry.906535625=Lisheng%20Wu%20and%20Ke%20Chen&entry.1292438233=%20%20Exploration%20efficiency%20poses%20a%20significant%20challenge%20in%20goal-conditioned%0Areinforcement%20learning%20%28GCRL%29%20tasks%2C%20particularly%20those%20with%20long%20horizons%20and%0Asparse%20rewards.%20A%20primary%20limitation%20to%20exploration%20efficiency%20is%20the%20agent%27s%0Ainability%20to%20leverage%20environmental%20structural%20patterns.%20In%20this%20study%2C%20we%0Aintroduce%20a%20novel%20framework%2C%20GEASD%2C%20designed%20to%20capture%20these%20patterns%20through%0Aan%20adaptive%20skill%20distribution%20during%20the%20learning%20process.%20This%20distribution%0Aoptimizes%20the%20local%20entropy%20of%20achieved%20goals%20within%20a%20contextual%20horizon%2C%0Aenhancing%20goal-spreading%20behaviors%20and%20facilitating%20deep%20exploration%20in%20states%0Acontaining%20familiar%20structural%20patterns.%20Our%20experiments%20reveal%20marked%0Aimprovements%20in%20exploration%20efficiency%20using%20the%20adaptive%20skill%20distribution%0Acompared%20to%20a%20uniform%20skill%20distribution.%20Additionally%2C%20the%20learned%20skill%0Adistribution%20demonstrates%20robust%20generalization%20capabilities%2C%20achieving%0Asubstantial%20exploration%20progress%20in%20unseen%20tasks%20containing%20similar%20local%0Astructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12999v1&entry.124074799=Read"},
{"title": "Data Alignment for Zero-Shot Concept Generation in Dermatology AI", "author": "Soham Gadgil and Mahtab Bigverdi", "abstract": "  AI in dermatology is evolving at a rapid pace but the major limitation to\ntraining trustworthy classifiers is the scarcity of data with ground-truth\nconcept level labels, which are meta-labels semantically meaningful to humans.\nFoundation models like CLIP providing zero-shot capabilities can help alleviate\nthis challenge by leveraging vast amounts of image-caption pairs available on\nthe internet. CLIP can be fine-tuned using domain specific image-caption pairs\nto improve classification performance. However, CLIP's pre-training data is not\nwell-aligned with the medical jargon that clinicians use to perform diagnoses.\nThe development of large language models (LLMs) in recent years has led to the\npossibility of leveraging the expressive nature of these models to generate\nrich text. Our goal is to use these models to generate caption text that aligns\nwell with both the clinical lexicon and with the natural human language used in\nCLIP's pre-training data. Starting with captions used for images in PubMed\narticles, we extend them by passing the raw captions through an LLM fine-tuned\non the field's several textbooks. We find that using captions generated by an\nexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept\nclassification performance.\n", "link": "http://arxiv.org/abs/2404.13043v1", "date": "2024-04-19", "relevancy": 2.1312, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.518}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5118}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Data%20Alignment%20for%20Zero-Shot%20Concept%20Generation%20in%20Dermatology%20AI&body=Title%3A%20Data%20Alignment%20for%20Zero-Shot%20Concept%20Generation%20in%20Dermatology%20AI%0AAuthor%3A%20Soham%20Gadgil%20and%20Mahtab%20Bigverdi%0AAbstract%3A%20%20%20AI%20in%20dermatology%20is%20evolving%20at%20a%20rapid%20pace%20but%20the%20major%20limitation%20to%0Atraining%20trustworthy%20classifiers%20is%20the%20scarcity%20of%20data%20with%20ground-truth%0Aconcept%20level%20labels%2C%20which%20are%20meta-labels%20semantically%20meaningful%20to%20humans.%0AFoundation%20models%20like%20CLIP%20providing%20zero-shot%20capabilities%20can%20help%20alleviate%0Athis%20challenge%20by%20leveraging%20vast%20amounts%20of%20image-caption%20pairs%20available%20on%0Athe%20internet.%20CLIP%20can%20be%20fine-tuned%20using%20domain%20specific%20image-caption%20pairs%0Ato%20improve%20classification%20performance.%20However%2C%20CLIP%27s%20pre-training%20data%20is%20not%0Awell-aligned%20with%20the%20medical%20jargon%20that%20clinicians%20use%20to%20perform%20diagnoses.%0AThe%20development%20of%20large%20language%20models%20%28LLMs%29%20in%20recent%20years%20has%20led%20to%20the%0Apossibility%20of%20leveraging%20the%20expressive%20nature%20of%20these%20models%20to%20generate%0Arich%20text.%20Our%20goal%20is%20to%20use%20these%20models%20to%20generate%20caption%20text%20that%20aligns%0Awell%20with%20both%20the%20clinical%20lexicon%20and%20with%20the%20natural%20human%20language%20used%20in%0ACLIP%27s%20pre-training%20data.%20Starting%20with%20captions%20used%20for%20images%20in%20PubMed%0Aarticles%2C%20we%20extend%20them%20by%20passing%20the%20raw%20captions%20through%20an%20LLM%20fine-tuned%0Aon%20the%20field%27s%20several%20textbooks.%20We%20find%20that%20using%20captions%20generated%20by%20an%0Aexpressive%20fine-tuned%20LLM%20like%20GPT-3.5%20improves%20downstream%20zero-shot%20concept%0Aclassification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13043v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Alignment%20for%20Zero-Shot%20Concept%20Generation%20in%20Dermatology%20AI&entry.906535625=Soham%20Gadgil%20and%20Mahtab%20Bigverdi&entry.1292438233=%20%20AI%20in%20dermatology%20is%20evolving%20at%20a%20rapid%20pace%20but%20the%20major%20limitation%20to%0Atraining%20trustworthy%20classifiers%20is%20the%20scarcity%20of%20data%20with%20ground-truth%0Aconcept%20level%20labels%2C%20which%20are%20meta-labels%20semantically%20meaningful%20to%20humans.%0AFoundation%20models%20like%20CLIP%20providing%20zero-shot%20capabilities%20can%20help%20alleviate%0Athis%20challenge%20by%20leveraging%20vast%20amounts%20of%20image-caption%20pairs%20available%20on%0Athe%20internet.%20CLIP%20can%20be%20fine-tuned%20using%20domain%20specific%20image-caption%20pairs%0Ato%20improve%20classification%20performance.%20However%2C%20CLIP%27s%20pre-training%20data%20is%20not%0Awell-aligned%20with%20the%20medical%20jargon%20that%20clinicians%20use%20to%20perform%20diagnoses.%0AThe%20development%20of%20large%20language%20models%20%28LLMs%29%20in%20recent%20years%20has%20led%20to%20the%0Apossibility%20of%20leveraging%20the%20expressive%20nature%20of%20these%20models%20to%20generate%0Arich%20text.%20Our%20goal%20is%20to%20use%20these%20models%20to%20generate%20caption%20text%20that%20aligns%0Awell%20with%20both%20the%20clinical%20lexicon%20and%20with%20the%20natural%20human%20language%20used%20in%0ACLIP%27s%20pre-training%20data.%20Starting%20with%20captions%20used%20for%20images%20in%20PubMed%0Aarticles%2C%20we%20extend%20them%20by%20passing%20the%20raw%20captions%20through%20an%20LLM%20fine-tuned%0Aon%20the%20field%27s%20several%20textbooks.%20We%20find%20that%20using%20captions%20generated%20by%20an%0Aexpressive%20fine-tuned%20LLM%20like%20GPT-3.5%20improves%20downstream%20zero-shot%20concept%0Aclassification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13043v1&entry.124074799=Read"},
{"title": "Efficient Backdoor Attacks for Deep Neural Networks in Real-world\n  Scenarios", "author": "Ziqiang Li and Hong Sun and Pengfei Xia and Heng Li and Beihao Xia and Yi Wu and Bin Li", "abstract": "  Recent deep neural networks (DNNs) have came to rely on vast amounts of\ntraining data, providing an opportunity for malicious attackers to exploit and\ncontaminate the data to carry out backdoor attacks. However, existing backdoor\nattack methods make unrealistic assumptions, assuming that all training data\ncomes from a single source and that attackers have full access to the training\ndata. In this paper, we introduce a more realistic attack scenario where\nvictims collect data from multiple sources, and attackers cannot access the\ncomplete training data. We refer to this scenario as data-constrained backdoor\nattacks. In such cases, previous attack methods suffer from severe efficiency\ndegradation due to the entanglement between benign and poisoning features\nduring the backdoor injection process. To tackle this problem, we introduce\nthree CLIP-based technologies from two distinct streams: Clean Feature\nSuppression and Poisoning Feature Augmentation.effective solution for\ndata-constrained backdoor attacks. The results demonstrate remarkable\nimprovements, with some settings achieving over 100% improvement compared to\nexisting attacks in data-constrained scenarios. Code is available at\nhttps://github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios\n", "link": "http://arxiv.org/abs/2306.08386v2", "date": "2024-04-19", "relevancy": 2.1257, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5598}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5179}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4943}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Backdoor%20Attacks%20for%20Deep%20Neural%20Networks%20in%20Real-world%0A%20%20Scenarios&body=Title%3A%20Efficient%20Backdoor%20Attacks%20for%20Deep%20Neural%20Networks%20in%20Real-world%0A%20%20Scenarios%0AAuthor%3A%20Ziqiang%20Li%20and%20Hong%20Sun%20and%20Pengfei%20Xia%20and%20Heng%20Li%20and%20Beihao%20Xia%20and%20Yi%20Wu%20and%20Bin%20Li%0AAbstract%3A%20%20%20Recent%20deep%20neural%20networks%20%28DNNs%29%20have%20came%20to%20rely%20on%20vast%20amounts%20of%0Atraining%20data%2C%20providing%20an%20opportunity%20for%20malicious%20attackers%20to%20exploit%20and%0Acontaminate%20the%20data%20to%20carry%20out%20backdoor%20attacks.%20However%2C%20existing%20backdoor%0Aattack%20methods%20make%20unrealistic%20assumptions%2C%20assuming%20that%20all%20training%20data%0Acomes%20from%20a%20single%20source%20and%20that%20attackers%20have%20full%20access%20to%20the%20training%0Adata.%20In%20this%20paper%2C%20we%20introduce%20a%20more%20realistic%20attack%20scenario%20where%0Avictims%20collect%20data%20from%20multiple%20sources%2C%20and%20attackers%20cannot%20access%20the%0Acomplete%20training%20data.%20We%20refer%20to%20this%20scenario%20as%20data-constrained%20backdoor%0Aattacks.%20In%20such%20cases%2C%20previous%20attack%20methods%20suffer%20from%20severe%20efficiency%0Adegradation%20due%20to%20the%20entanglement%20between%20benign%20and%20poisoning%20features%0Aduring%20the%20backdoor%20injection%20process.%20To%20tackle%20this%20problem%2C%20we%20introduce%0Athree%20CLIP-based%20technologies%20from%20two%20distinct%20streams%3A%20Clean%20Feature%0ASuppression%20and%20Poisoning%20Feature%20Augmentation.effective%20solution%20for%0Adata-constrained%20backdoor%20attacks.%20The%20results%20demonstrate%20remarkable%0Aimprovements%2C%20with%20some%20settings%20achieving%20over%20100%25%20improvement%20compared%20to%0Aexisting%20attacks%20in%20data-constrained%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.08386v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Backdoor%20Attacks%20for%20Deep%20Neural%20Networks%20in%20Real-world%0A%20%20Scenarios&entry.906535625=Ziqiang%20Li%20and%20Hong%20Sun%20and%20Pengfei%20Xia%20and%20Heng%20Li%20and%20Beihao%20Xia%20and%20Yi%20Wu%20and%20Bin%20Li&entry.1292438233=%20%20Recent%20deep%20neural%20networks%20%28DNNs%29%20have%20came%20to%20rely%20on%20vast%20amounts%20of%0Atraining%20data%2C%20providing%20an%20opportunity%20for%20malicious%20attackers%20to%20exploit%20and%0Acontaminate%20the%20data%20to%20carry%20out%20backdoor%20attacks.%20However%2C%20existing%20backdoor%0Aattack%20methods%20make%20unrealistic%20assumptions%2C%20assuming%20that%20all%20training%20data%0Acomes%20from%20a%20single%20source%20and%20that%20attackers%20have%20full%20access%20to%20the%20training%0Adata.%20In%20this%20paper%2C%20we%20introduce%20a%20more%20realistic%20attack%20scenario%20where%0Avictims%20collect%20data%20from%20multiple%20sources%2C%20and%20attackers%20cannot%20access%20the%0Acomplete%20training%20data.%20We%20refer%20to%20this%20scenario%20as%20data-constrained%20backdoor%0Aattacks.%20In%20such%20cases%2C%20previous%20attack%20methods%20suffer%20from%20severe%20efficiency%0Adegradation%20due%20to%20the%20entanglement%20between%20benign%20and%20poisoning%20features%0Aduring%20the%20backdoor%20injection%20process.%20To%20tackle%20this%20problem%2C%20we%20introduce%0Athree%20CLIP-based%20technologies%20from%20two%20distinct%20streams%3A%20Clean%20Feature%0ASuppression%20and%20Poisoning%20Feature%20Augmentation.effective%20solution%20for%0Adata-constrained%20backdoor%20attacks.%20The%20results%20demonstrate%20remarkable%0Aimprovements%2C%20with%20some%20settings%20achieving%20over%20100%25%20improvement%20compared%20to%0Aexisting%20attacks%20in%20data-constrained%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.08386v2&entry.124074799=Read"},
{"title": "When Life gives you LLMs, make LLM-ADE: Large Language Models with\n  Adaptive Data Engineering", "author": "Stephen Choi and William Gazeley", "abstract": "  This paper presents the LLM-ADE framework, a novel methodology for continued\npre-training of large language models (LLMs) that addresses the challenges of\ncatastrophic forgetting and double descent. LLM-ADE employs dynamic\narchitectural adjustments, including selective block freezing and expansion,\ntailored to specific datasets. This strategy enhances model adaptability to new\ndata while preserving previously acquired knowledge. We demonstrate LLM-ADE's\neffectiveness on the TinyLlama model across various general knowledge\nbenchmarks, showing significant performance improvements without the drawbacks\nof traditional continuous training methods. This approach promises a more\nversatile and robust way to keep LLMs current and efficient in real-world\napplications.\n", "link": "http://arxiv.org/abs/2404.13028v1", "date": "2024-04-19", "relevancy": 2.115, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5098}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5095}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20When%20Life%20gives%20you%20LLMs%2C%20make%20LLM-ADE%3A%20Large%20Language%20Models%20with%0A%20%20Adaptive%20Data%20Engineering&body=Title%3A%20When%20Life%20gives%20you%20LLMs%2C%20make%20LLM-ADE%3A%20Large%20Language%20Models%20with%0A%20%20Adaptive%20Data%20Engineering%0AAuthor%3A%20Stephen%20Choi%20and%20William%20Gazeley%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20LLM-ADE%20framework%2C%20a%20novel%20methodology%20for%20continued%0Apre-training%20of%20large%20language%20models%20%28LLMs%29%20that%20addresses%20the%20challenges%20of%0Acatastrophic%20forgetting%20and%20double%20descent.%20LLM-ADE%20employs%20dynamic%0Aarchitectural%20adjustments%2C%20including%20selective%20block%20freezing%20and%20expansion%2C%0Atailored%20to%20specific%20datasets.%20This%20strategy%20enhances%20model%20adaptability%20to%20new%0Adata%20while%20preserving%20previously%20acquired%20knowledge.%20We%20demonstrate%20LLM-ADE%27s%0Aeffectiveness%20on%20the%20TinyLlama%20model%20across%20various%20general%20knowledge%0Abenchmarks%2C%20showing%20significant%20performance%20improvements%20without%20the%20drawbacks%0Aof%20traditional%20continuous%20training%20methods.%20This%20approach%20promises%20a%20more%0Aversatile%20and%20robust%20way%20to%20keep%20LLMs%20current%20and%20efficient%20in%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13028v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Life%20gives%20you%20LLMs%2C%20make%20LLM-ADE%3A%20Large%20Language%20Models%20with%0A%20%20Adaptive%20Data%20Engineering&entry.906535625=Stephen%20Choi%20and%20William%20Gazeley&entry.1292438233=%20%20This%20paper%20presents%20the%20LLM-ADE%20framework%2C%20a%20novel%20methodology%20for%20continued%0Apre-training%20of%20large%20language%20models%20%28LLMs%29%20that%20addresses%20the%20challenges%20of%0Acatastrophic%20forgetting%20and%20double%20descent.%20LLM-ADE%20employs%20dynamic%0Aarchitectural%20adjustments%2C%20including%20selective%20block%20freezing%20and%20expansion%2C%0Atailored%20to%20specific%20datasets.%20This%20strategy%20enhances%20model%20adaptability%20to%20new%0Adata%20while%20preserving%20previously%20acquired%20knowledge.%20We%20demonstrate%20LLM-ADE%27s%0Aeffectiveness%20on%20the%20TinyLlama%20model%20across%20various%20general%20knowledge%0Abenchmarks%2C%20showing%20significant%20performance%20improvements%20without%20the%20drawbacks%0Aof%20traditional%20continuous%20training%20methods.%20This%20approach%20promises%20a%20more%0Aversatile%20and%20robust%20way%20to%20keep%20LLMs%20current%20and%20efficient%20in%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13028v1&entry.124074799=Read"},
{"title": "Disentangling ID and Modality Effects for Session-based Recommendation", "author": "Xiaokun Zhang and Bo Xu and Zhaochun Ren and Xiaochen Wang and Hongfei Lin and Fenglong Ma", "abstract": "  Session-based recommendation aims to predict intents of anonymous users based\non their limited behaviors. Modeling user behaviors involves two distinct\nrationales: co-occurrence patterns reflected by item IDs, and fine-grained\npreferences represented by item modalities (e.g., text and images). However,\nexisting methods typically entangle these causes, leading to their failure in\nachieving accurate and explainable recommendations. To this end, we propose a\nnovel framework DIMO to disentangle the effects of ID and modality in the task.\nAt the item level, we introduce a co-occurrence representation schema to\nexplicitly incorporate cooccurrence patterns into ID representations.\nSimultaneously, DIMO aligns different modalities into a unified semantic space\nto represent them uniformly. At the session level, we present a multi-view\nself-supervised disentanglement, including proxy mechanism and counterfactual\ninference, to disentangle ID and modality effects without supervised signals.\nLeveraging these disentangled causes, DIMO provides recommendations via causal\ninference and further creates two templates for generating explanations.\nExtensive experiments on multiple real-world datasets demonstrate the\nconsistent superiority of DIMO over existing methods. Further analysis also\nconfirms DIMO's effectiveness in generating explanations.\n", "link": "http://arxiv.org/abs/2404.12969v1", "date": "2024-04-19", "relevancy": 2.1132, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5317}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5301}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Disentangling%20ID%20and%20Modality%20Effects%20for%20Session-based%20Recommendation&body=Title%3A%20Disentangling%20ID%20and%20Modality%20Effects%20for%20Session-based%20Recommendation%0AAuthor%3A%20Xiaokun%20Zhang%20and%20Bo%20Xu%20and%20Zhaochun%20Ren%20and%20Xiaochen%20Wang%20and%20Hongfei%20Lin%20and%20Fenglong%20Ma%0AAbstract%3A%20%20%20Session-based%20recommendation%20aims%20to%20predict%20intents%20of%20anonymous%20users%20based%0Aon%20their%20limited%20behaviors.%20Modeling%20user%20behaviors%20involves%20two%20distinct%0Arationales%3A%20co-occurrence%20patterns%20reflected%20by%20item%20IDs%2C%20and%20fine-grained%0Apreferences%20represented%20by%20item%20modalities%20%28e.g.%2C%20text%20and%20images%29.%20However%2C%0Aexisting%20methods%20typically%20entangle%20these%20causes%2C%20leading%20to%20their%20failure%20in%0Aachieving%20accurate%20and%20explainable%20recommendations.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20framework%20DIMO%20to%20disentangle%20the%20effects%20of%20ID%20and%20modality%20in%20the%20task.%0AAt%20the%20item%20level%2C%20we%20introduce%20a%20co-occurrence%20representation%20schema%20to%0Aexplicitly%20incorporate%20cooccurrence%20patterns%20into%20ID%20representations.%0ASimultaneously%2C%20DIMO%20aligns%20different%20modalities%20into%20a%20unified%20semantic%20space%0Ato%20represent%20them%20uniformly.%20At%20the%20session%20level%2C%20we%20present%20a%20multi-view%0Aself-supervised%20disentanglement%2C%20including%20proxy%20mechanism%20and%20counterfactual%0Ainference%2C%20to%20disentangle%20ID%20and%20modality%20effects%20without%20supervised%20signals.%0ALeveraging%20these%20disentangled%20causes%2C%20DIMO%20provides%20recommendations%20via%20causal%0Ainference%20and%20further%20creates%20two%20templates%20for%20generating%20explanations.%0AExtensive%20experiments%20on%20multiple%20real-world%20datasets%20demonstrate%20the%0Aconsistent%20superiority%20of%20DIMO%20over%20existing%20methods.%20Further%20analysis%20also%0Aconfirms%20DIMO%27s%20effectiveness%20in%20generating%20explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12969v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20ID%20and%20Modality%20Effects%20for%20Session-based%20Recommendation&entry.906535625=Xiaokun%20Zhang%20and%20Bo%20Xu%20and%20Zhaochun%20Ren%20and%20Xiaochen%20Wang%20and%20Hongfei%20Lin%20and%20Fenglong%20Ma&entry.1292438233=%20%20Session-based%20recommendation%20aims%20to%20predict%20intents%20of%20anonymous%20users%20based%0Aon%20their%20limited%20behaviors.%20Modeling%20user%20behaviors%20involves%20two%20distinct%0Arationales%3A%20co-occurrence%20patterns%20reflected%20by%20item%20IDs%2C%20and%20fine-grained%0Apreferences%20represented%20by%20item%20modalities%20%28e.g.%2C%20text%20and%20images%29.%20However%2C%0Aexisting%20methods%20typically%20entangle%20these%20causes%2C%20leading%20to%20their%20failure%20in%0Aachieving%20accurate%20and%20explainable%20recommendations.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20framework%20DIMO%20to%20disentangle%20the%20effects%20of%20ID%20and%20modality%20in%20the%20task.%0AAt%20the%20item%20level%2C%20we%20introduce%20a%20co-occurrence%20representation%20schema%20to%0Aexplicitly%20incorporate%20cooccurrence%20patterns%20into%20ID%20representations.%0ASimultaneously%2C%20DIMO%20aligns%20different%20modalities%20into%20a%20unified%20semantic%20space%0Ato%20represent%20them%20uniformly.%20At%20the%20session%20level%2C%20we%20present%20a%20multi-view%0Aself-supervised%20disentanglement%2C%20including%20proxy%20mechanism%20and%20counterfactual%0Ainference%2C%20to%20disentangle%20ID%20and%20modality%20effects%20without%20supervised%20signals.%0ALeveraging%20these%20disentangled%20causes%2C%20DIMO%20provides%20recommendations%20via%20causal%0Ainference%20and%20further%20creates%20two%20templates%20for%20generating%20explanations.%0AExtensive%20experiments%20on%20multiple%20real-world%20datasets%20demonstrate%20the%0Aconsistent%20superiority%20of%20DIMO%20over%20existing%20methods.%20Further%20analysis%20also%0Aconfirms%20DIMO%27s%20effectiveness%20in%20generating%20explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12969v1&entry.124074799=Read"},
{"title": "3D Multi-frame Fusion for Video Stabilization", "author": "Zhan Peng and Xinyi Ye and Weiyue Zhao and Tianqi Liu and Huiqiang Sun and Baopu Li and Zhiguo Cao", "abstract": "  In this paper, we present RStab, a novel framework for video stabilization\nthat integrates 3D multi-frame fusion through volume rendering. Departing from\nconventional methods, we introduce a 3D multi-frame perspective to generate\nstabilized images, addressing the challenge of full-frame generation while\npreserving structure. The core of our approach lies in Stabilized Rendering\n(SR), a volume rendering module, which extends beyond the image fusion by\nincorporating feature fusion. The core of our RStab framework lies in\nStabilized Rendering (SR), a volume rendering module, fusing multi-frame\ninformation in 3D space. Specifically, SR involves warping features and colors\nfrom multiple frames by projection, fusing them into descriptors to render the\nstabilized image. However, the precision of warped information depends on the\nprojection accuracy, a factor significantly influenced by dynamic regions. In\nresponse, we introduce the Adaptive Ray Range (ARR) module to integrate depth\npriors, adaptively defining the sampling range for the projection process.\nAdditionally, we propose Color Correction (CC) assisting geometric constraints\nwith optical flow for accurate color aggregation. Thanks to the three modules,\nour RStab demonstrates superior performance compared with previous stabilizers\nin the field of view (FOV), image quality, and video stability across various\ndatasets.\n", "link": "http://arxiv.org/abs/2404.12887v1", "date": "2024-04-19", "relevancy": 2.1067, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5168}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Multi-frame%20Fusion%20for%20Video%20Stabilization&body=Title%3A%203D%20Multi-frame%20Fusion%20for%20Video%20Stabilization%0AAuthor%3A%20Zhan%20Peng%20and%20Xinyi%20Ye%20and%20Weiyue%20Zhao%20and%20Tianqi%20Liu%20and%20Huiqiang%20Sun%20and%20Baopu%20Li%20and%20Zhiguo%20Cao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20RStab%2C%20a%20novel%20framework%20for%20video%20stabilization%0Athat%20integrates%203D%20multi-frame%20fusion%20through%20volume%20rendering.%20Departing%20from%0Aconventional%20methods%2C%20we%20introduce%20a%203D%20multi-frame%20perspective%20to%20generate%0Astabilized%20images%2C%20addressing%20the%20challenge%20of%20full-frame%20generation%20while%0Apreserving%20structure.%20The%20core%20of%20our%20approach%20lies%20in%20Stabilized%20Rendering%0A%28SR%29%2C%20a%20volume%20rendering%20module%2C%20which%20extends%20beyond%20the%20image%20fusion%20by%0Aincorporating%20feature%20fusion.%20The%20core%20of%20our%20RStab%20framework%20lies%20in%0AStabilized%20Rendering%20%28SR%29%2C%20a%20volume%20rendering%20module%2C%20fusing%20multi-frame%0Ainformation%20in%203D%20space.%20Specifically%2C%20SR%20involves%20warping%20features%20and%20colors%0Afrom%20multiple%20frames%20by%20projection%2C%20fusing%20them%20into%20descriptors%20to%20render%20the%0Astabilized%20image.%20However%2C%20the%20precision%20of%20warped%20information%20depends%20on%20the%0Aprojection%20accuracy%2C%20a%20factor%20significantly%20influenced%20by%20dynamic%20regions.%20In%0Aresponse%2C%20we%20introduce%20the%20Adaptive%20Ray%20Range%20%28ARR%29%20module%20to%20integrate%20depth%0Apriors%2C%20adaptively%20defining%20the%20sampling%20range%20for%20the%20projection%20process.%0AAdditionally%2C%20we%20propose%20Color%20Correction%20%28CC%29%20assisting%20geometric%20constraints%0Awith%20optical%20flow%20for%20accurate%20color%20aggregation.%20Thanks%20to%20the%20three%20modules%2C%0Aour%20RStab%20demonstrates%20superior%20performance%20compared%20with%20previous%20stabilizers%0Ain%20the%20field%20of%20view%20%28FOV%29%2C%20image%20quality%2C%20and%20video%20stability%20across%20various%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12887v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Multi-frame%20Fusion%20for%20Video%20Stabilization&entry.906535625=Zhan%20Peng%20and%20Xinyi%20Ye%20and%20Weiyue%20Zhao%20and%20Tianqi%20Liu%20and%20Huiqiang%20Sun%20and%20Baopu%20Li%20and%20Zhiguo%20Cao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20RStab%2C%20a%20novel%20framework%20for%20video%20stabilization%0Athat%20integrates%203D%20multi-frame%20fusion%20through%20volume%20rendering.%20Departing%20from%0Aconventional%20methods%2C%20we%20introduce%20a%203D%20multi-frame%20perspective%20to%20generate%0Astabilized%20images%2C%20addressing%20the%20challenge%20of%20full-frame%20generation%20while%0Apreserving%20structure.%20The%20core%20of%20our%20approach%20lies%20in%20Stabilized%20Rendering%0A%28SR%29%2C%20a%20volume%20rendering%20module%2C%20which%20extends%20beyond%20the%20image%20fusion%20by%0Aincorporating%20feature%20fusion.%20The%20core%20of%20our%20RStab%20framework%20lies%20in%0AStabilized%20Rendering%20%28SR%29%2C%20a%20volume%20rendering%20module%2C%20fusing%20multi-frame%0Ainformation%20in%203D%20space.%20Specifically%2C%20SR%20involves%20warping%20features%20and%20colors%0Afrom%20multiple%20frames%20by%20projection%2C%20fusing%20them%20into%20descriptors%20to%20render%20the%0Astabilized%20image.%20However%2C%20the%20precision%20of%20warped%20information%20depends%20on%20the%0Aprojection%20accuracy%2C%20a%20factor%20significantly%20influenced%20by%20dynamic%20regions.%20In%0Aresponse%2C%20we%20introduce%20the%20Adaptive%20Ray%20Range%20%28ARR%29%20module%20to%20integrate%20depth%0Apriors%2C%20adaptively%20defining%20the%20sampling%20range%20for%20the%20projection%20process.%0AAdditionally%2C%20we%20propose%20Color%20Correction%20%28CC%29%20assisting%20geometric%20constraints%0Awith%20optical%20flow%20for%20accurate%20color%20aggregation.%20Thanks%20to%20the%20three%20modules%2C%0Aour%20RStab%20demonstrates%20superior%20performance%20compared%20with%20previous%20stabilizers%0Ain%20the%20field%20of%20view%20%28FOV%29%2C%20image%20quality%2C%20and%20video%20stability%20across%20various%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12887v1&entry.124074799=Read"},
{"title": "HiLo: Detailed and Robust 3D Clothed Human Reconstruction with High-and\n  Low-Frequency Information of Parametric Models", "author": "Yifan Yang and Dong Liu and Shuhai Zhang and Zeshuai Deng and Zixiong Huang and Mingkui Tan", "abstract": "  Reconstructing 3D clothed human involves creating a detailed geometry of\nindividuals in clothing, with applications ranging from virtual try-on, movies,\nto games. To enable practical and widespread applications, recent advances\npropose to generate a clothed human from an RGB image. However, they struggle\nto reconstruct detailed and robust avatars simultaneously. We empirically find\nthat the high-frequency (HF) and low-frequency (LF) information from a\nparametric model has the potential to enhance geometry details and improve\nrobustness to noise, respectively. Based on this, we propose HiLo, namely\nclothed human reconstruction with high- and low-frequency information, which\ncontains two components. 1) To recover detailed geometry using HF information,\nwe propose a progressive HF Signed Distance Function to enhance the detailed 3D\ngeometry of a clothed human. We analyze that our progressive learning manner\nalleviates large gradients that hinder model convergence. 2) To achieve robust\nreconstruction against inaccurate estimation of the parametric model by using\nLF information, we propose a spatial interaction implicit function. This\nfunction effectively exploits the complementary spatial information from a\nlow-resolution voxel grid of the parametric model. Experimental results\ndemonstrate that HiLo outperforms the state-of-the-art methods by 10.43% and\n9.54% in terms of Chamfer distance on the Thuman2.0 and CAPE datasets,\nrespectively. Additionally, HiLo demonstrates robustness to noise from the\nparametric model, challenging poses, and various clothing styles.\n", "link": "http://arxiv.org/abs/2404.04876v2", "date": "2024-04-19", "relevancy": 2.0907, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5438}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5259}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.511}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HiLo%3A%20Detailed%20and%20Robust%203D%20Clothed%20Human%20Reconstruction%20with%20High-and%0A%20%20Low-Frequency%20Information%20of%20Parametric%20Models&body=Title%3A%20HiLo%3A%20Detailed%20and%20Robust%203D%20Clothed%20Human%20Reconstruction%20with%20High-and%0A%20%20Low-Frequency%20Information%20of%20Parametric%20Models%0AAuthor%3A%20Yifan%20Yang%20and%20Dong%20Liu%20and%20Shuhai%20Zhang%20and%20Zeshuai%20Deng%20and%20Zixiong%20Huang%20and%20Mingkui%20Tan%0AAbstract%3A%20%20%20Reconstructing%203D%20clothed%20human%20involves%20creating%20a%20detailed%20geometry%20of%0Aindividuals%20in%20clothing%2C%20with%20applications%20ranging%20from%20virtual%20try-on%2C%20movies%2C%0Ato%20games.%20To%20enable%20practical%20and%20widespread%20applications%2C%20recent%20advances%0Apropose%20to%20generate%20a%20clothed%20human%20from%20an%20RGB%20image.%20However%2C%20they%20struggle%0Ato%20reconstruct%20detailed%20and%20robust%20avatars%20simultaneously.%20We%20empirically%20find%0Athat%20the%20high-frequency%20%28HF%29%20and%20low-frequency%20%28LF%29%20information%20from%20a%0Aparametric%20model%20has%20the%20potential%20to%20enhance%20geometry%20details%20and%20improve%0Arobustness%20to%20noise%2C%20respectively.%20Based%20on%20this%2C%20we%20propose%20HiLo%2C%20namely%0Aclothed%20human%20reconstruction%20with%20high-%20and%20low-frequency%20information%2C%20which%0Acontains%20two%20components.%201%29%20To%20recover%20detailed%20geometry%20using%20HF%20information%2C%0Awe%20propose%20a%20progressive%20HF%20Signed%20Distance%20Function%20to%20enhance%20the%20detailed%203D%0Ageometry%20of%20a%20clothed%20human.%20We%20analyze%20that%20our%20progressive%20learning%20manner%0Aalleviates%20large%20gradients%20that%20hinder%20model%20convergence.%202%29%20To%20achieve%20robust%0Areconstruction%20against%20inaccurate%20estimation%20of%20the%20parametric%20model%20by%20using%0ALF%20information%2C%20we%20propose%20a%20spatial%20interaction%20implicit%20function.%20This%0Afunction%20effectively%20exploits%20the%20complementary%20spatial%20information%20from%20a%0Alow-resolution%20voxel%20grid%20of%20the%20parametric%20model.%20Experimental%20results%0Ademonstrate%20that%20HiLo%20outperforms%20the%20state-of-the-art%20methods%20by%2010.43%25%20and%0A9.54%25%20in%20terms%20of%20Chamfer%20distance%20on%20the%20Thuman2.0%20and%20CAPE%20datasets%2C%0Arespectively.%20Additionally%2C%20HiLo%20demonstrates%20robustness%20to%20noise%20from%20the%0Aparametric%20model%2C%20challenging%20poses%2C%20and%20various%20clothing%20styles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04876v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiLo%3A%20Detailed%20and%20Robust%203D%20Clothed%20Human%20Reconstruction%20with%20High-and%0A%20%20Low-Frequency%20Information%20of%20Parametric%20Models&entry.906535625=Yifan%20Yang%20and%20Dong%20Liu%20and%20Shuhai%20Zhang%20and%20Zeshuai%20Deng%20and%20Zixiong%20Huang%20and%20Mingkui%20Tan&entry.1292438233=%20%20Reconstructing%203D%20clothed%20human%20involves%20creating%20a%20detailed%20geometry%20of%0Aindividuals%20in%20clothing%2C%20with%20applications%20ranging%20from%20virtual%20try-on%2C%20movies%2C%0Ato%20games.%20To%20enable%20practical%20and%20widespread%20applications%2C%20recent%20advances%0Apropose%20to%20generate%20a%20clothed%20human%20from%20an%20RGB%20image.%20However%2C%20they%20struggle%0Ato%20reconstruct%20detailed%20and%20robust%20avatars%20simultaneously.%20We%20empirically%20find%0Athat%20the%20high-frequency%20%28HF%29%20and%20low-frequency%20%28LF%29%20information%20from%20a%0Aparametric%20model%20has%20the%20potential%20to%20enhance%20geometry%20details%20and%20improve%0Arobustness%20to%20noise%2C%20respectively.%20Based%20on%20this%2C%20we%20propose%20HiLo%2C%20namely%0Aclothed%20human%20reconstruction%20with%20high-%20and%20low-frequency%20information%2C%20which%0Acontains%20two%20components.%201%29%20To%20recover%20detailed%20geometry%20using%20HF%20information%2C%0Awe%20propose%20a%20progressive%20HF%20Signed%20Distance%20Function%20to%20enhance%20the%20detailed%203D%0Ageometry%20of%20a%20clothed%20human.%20We%20analyze%20that%20our%20progressive%20learning%20manner%0Aalleviates%20large%20gradients%20that%20hinder%20model%20convergence.%202%29%20To%20achieve%20robust%0Areconstruction%20against%20inaccurate%20estimation%20of%20the%20parametric%20model%20by%20using%0ALF%20information%2C%20we%20propose%20a%20spatial%20interaction%20implicit%20function.%20This%0Afunction%20effectively%20exploits%20the%20complementary%20spatial%20information%20from%20a%0Alow-resolution%20voxel%20grid%20of%20the%20parametric%20model.%20Experimental%20results%0Ademonstrate%20that%20HiLo%20outperforms%20the%20state-of-the-art%20methods%20by%2010.43%25%20and%0A9.54%25%20in%20terms%20of%20Chamfer%20distance%20on%20the%20Thuman2.0%20and%20CAPE%20datasets%2C%0Arespectively.%20Additionally%2C%20HiLo%20demonstrates%20robustness%20to%20noise%20from%20the%0Aparametric%20model%2C%20challenging%20poses%2C%20and%20various%20clothing%20styles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04876v2&entry.124074799=Read"},
{"title": "AutoCrawler: A Progressive Understanding Web Agent for Web Crawler\n  Generation", "author": "Wenhao Huang and Chenghao Peng and Zhixu Li and Jiaqing Liang and Yanghua Xiao and Liqian Wen and Zulong Chen", "abstract": "  Web automation is a significant technique that accomplishes complicated web\ntasks by automating common web actions, enhancing operational efficiency, and\nreducing the need for manual intervention. Traditional methods, such as\nwrappers, suffer from limited adaptability and scalability when faced with a\nnew website. On the other hand, generative agents empowered by large language\nmodels (LLMs) exhibit poor performance and reusability in open-world scenarios.\nIn this work, we introduce a crawler generation task for vertical information\nweb pages and the paradigm of combining LLMs with crawlers, which helps\ncrawlers handle diverse and changing web environments more efficiently. We\npropose AutoCrawler, a two-stage framework that leverages the hierarchical\nstructure of HTML for progressive understanding. Through top-down and step-back\noperations, AutoCrawler can learn from erroneous actions and continuously prune\nHTML for better action generation. We conduct comprehensive experiments with\nmultiple LLMs and demonstrate the effectiveness of our framework. Resources of\nthis paper can be found at \\url{https://github.com/EZ-hwh/AutoCrawler}\n", "link": "http://arxiv.org/abs/2404.12753v1", "date": "2024-04-19", "relevancy": 2.0869, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5395}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5325}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5039}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AutoCrawler%3A%20A%20Progressive%20Understanding%20Web%20Agent%20for%20Web%20Crawler%0A%20%20Generation&body=Title%3A%20AutoCrawler%3A%20A%20Progressive%20Understanding%20Web%20Agent%20for%20Web%20Crawler%0A%20%20Generation%0AAuthor%3A%20Wenhao%20Huang%20and%20Chenghao%20Peng%20and%20Zhixu%20Li%20and%20Jiaqing%20Liang%20and%20Yanghua%20Xiao%20and%20Liqian%20Wen%20and%20Zulong%20Chen%0AAbstract%3A%20%20%20Web%20automation%20is%20a%20significant%20technique%20that%20accomplishes%20complicated%20web%0Atasks%20by%20automating%20common%20web%20actions%2C%20enhancing%20operational%20efficiency%2C%20and%0Areducing%20the%20need%20for%20manual%20intervention.%20Traditional%20methods%2C%20such%20as%0Awrappers%2C%20suffer%20from%20limited%20adaptability%20and%20scalability%20when%20faced%20with%20a%0Anew%20website.%20On%20the%20other%20hand%2C%20generative%20agents%20empowered%20by%20large%20language%0Amodels%20%28LLMs%29%20exhibit%20poor%20performance%20and%20reusability%20in%20open-world%20scenarios.%0AIn%20this%20work%2C%20we%20introduce%20a%20crawler%20generation%20task%20for%20vertical%20information%0Aweb%20pages%20and%20the%20paradigm%20of%20combining%20LLMs%20with%20crawlers%2C%20which%20helps%0Acrawlers%20handle%20diverse%20and%20changing%20web%20environments%20more%20efficiently.%20We%0Apropose%20AutoCrawler%2C%20a%20two-stage%20framework%20that%20leverages%20the%20hierarchical%0Astructure%20of%20HTML%20for%20progressive%20understanding.%20Through%20top-down%20and%20step-back%0Aoperations%2C%20AutoCrawler%20can%20learn%20from%20erroneous%20actions%20and%20continuously%20prune%0AHTML%20for%20better%20action%20generation.%20We%20conduct%20comprehensive%20experiments%20with%0Amultiple%20LLMs%20and%20demonstrate%20the%20effectiveness%20of%20our%20framework.%20Resources%20of%0Athis%20paper%20can%20be%20found%20at%20%5Curl%7Bhttps%3A//github.com/EZ-hwh/AutoCrawler%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12753v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoCrawler%3A%20A%20Progressive%20Understanding%20Web%20Agent%20for%20Web%20Crawler%0A%20%20Generation&entry.906535625=Wenhao%20Huang%20and%20Chenghao%20Peng%20and%20Zhixu%20Li%20and%20Jiaqing%20Liang%20and%20Yanghua%20Xiao%20and%20Liqian%20Wen%20and%20Zulong%20Chen&entry.1292438233=%20%20Web%20automation%20is%20a%20significant%20technique%20that%20accomplishes%20complicated%20web%0Atasks%20by%20automating%20common%20web%20actions%2C%20enhancing%20operational%20efficiency%2C%20and%0Areducing%20the%20need%20for%20manual%20intervention.%20Traditional%20methods%2C%20such%20as%0Awrappers%2C%20suffer%20from%20limited%20adaptability%20and%20scalability%20when%20faced%20with%20a%0Anew%20website.%20On%20the%20other%20hand%2C%20generative%20agents%20empowered%20by%20large%20language%0Amodels%20%28LLMs%29%20exhibit%20poor%20performance%20and%20reusability%20in%20open-world%20scenarios.%0AIn%20this%20work%2C%20we%20introduce%20a%20crawler%20generation%20task%20for%20vertical%20information%0Aweb%20pages%20and%20the%20paradigm%20of%20combining%20LLMs%20with%20crawlers%2C%20which%20helps%0Acrawlers%20handle%20diverse%20and%20changing%20web%20environments%20more%20efficiently.%20We%0Apropose%20AutoCrawler%2C%20a%20two-stage%20framework%20that%20leverages%20the%20hierarchical%0Astructure%20of%20HTML%20for%20progressive%20understanding.%20Through%20top-down%20and%20step-back%0Aoperations%2C%20AutoCrawler%20can%20learn%20from%20erroneous%20actions%20and%20continuously%20prune%0AHTML%20for%20better%20action%20generation.%20We%20conduct%20comprehensive%20experiments%20with%0Amultiple%20LLMs%20and%20demonstrate%20the%20effectiveness%20of%20our%20framework.%20Resources%20of%0Athis%20paper%20can%20be%20found%20at%20%5Curl%7Bhttps%3A//github.com/EZ-hwh/AutoCrawler%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12753v1&entry.124074799=Read"},
{"title": "Koala: Key frame-conditioned long video-LLM", "author": "Reuben Tan and Ximeng Sun and Ping Hu and Jui-hsien Wang and Hanieh Deilamsalehy and Bryan A. Plummer and Bryan Russell and Kate Saenko", "abstract": "  Long video question answering is a challenging task that involves recognizing\nshort-term activities and reasoning about their fine-grained relationships.\nState-of-the-art video Large Language Models (vLLMs) hold promise as a viable\nsolution due to their demonstrated emergent capabilities on new tasks. However,\ndespite being trained on millions of short seconds-long videos, vLLMs are\nunable to understand minutes-long videos and accurately answer questions about\nthem. To address this limitation, we propose a lightweight and self-supervised\napproach, Key frame-conditioned long video-LLM (Koala), that introduces\nlearnable spatiotemporal queries to adapt pretrained vLLMs for generalizing to\nlonger videos. Our approach introduces two new tokenizers that condition on\nvisual tokens computed from sparse video key frames for understanding short and\nlong video moments. We train our proposed approach on HowTo100M and demonstrate\nits effectiveness on zero-shot long video understanding benchmarks, where it\noutperforms state-of-the-art large models by 3 - 6% in absolute accuracy across\nall tasks. Surprisingly, we also empirically show that our approach not only\nhelps a pretrained vLLM to understand long videos but also improves its\naccuracy on short-term action recognition.\n", "link": "http://arxiv.org/abs/2404.04346v2", "date": "2024-04-19", "relevancy": 2.0866, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5267}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5234}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5179}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Koala%3A%20Key%20frame-conditioned%20long%20video-LLM&body=Title%3A%20Koala%3A%20Key%20frame-conditioned%20long%20video-LLM%0AAuthor%3A%20Reuben%20Tan%20and%20Ximeng%20Sun%20and%20Ping%20Hu%20and%20Jui-hsien%20Wang%20and%20Hanieh%20Deilamsalehy%20and%20Bryan%20A.%20Plummer%20and%20Bryan%20Russell%20and%20Kate%20Saenko%0AAbstract%3A%20%20%20Long%20video%20question%20answering%20is%20a%20challenging%20task%20that%20involves%20recognizing%0Ashort-term%20activities%20and%20reasoning%20about%20their%20fine-grained%20relationships.%0AState-of-the-art%20video%20Large%20Language%20Models%20%28vLLMs%29%20hold%20promise%20as%20a%20viable%0Asolution%20due%20to%20their%20demonstrated%20emergent%20capabilities%20on%20new%20tasks.%20However%2C%0Adespite%20being%20trained%20on%20millions%20of%20short%20seconds-long%20videos%2C%20vLLMs%20are%0Aunable%20to%20understand%20minutes-long%20videos%20and%20accurately%20answer%20questions%20about%0Athem.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20lightweight%20and%20self-supervised%0Aapproach%2C%20Key%20frame-conditioned%20long%20video-LLM%20%28Koala%29%2C%20that%20introduces%0Alearnable%20spatiotemporal%20queries%20to%20adapt%20pretrained%20vLLMs%20for%20generalizing%20to%0Alonger%20videos.%20Our%20approach%20introduces%20two%20new%20tokenizers%20that%20condition%20on%0Avisual%20tokens%20computed%20from%20sparse%20video%20key%20frames%20for%20understanding%20short%20and%0Along%20video%20moments.%20We%20train%20our%20proposed%20approach%20on%20HowTo100M%20and%20demonstrate%0Aits%20effectiveness%20on%20zero-shot%20long%20video%20understanding%20benchmarks%2C%20where%20it%0Aoutperforms%20state-of-the-art%20large%20models%20by%203%20-%206%25%20in%20absolute%20accuracy%20across%0Aall%20tasks.%20Surprisingly%2C%20we%20also%20empirically%20show%20that%20our%20approach%20not%20only%0Ahelps%20a%20pretrained%20vLLM%20to%20understand%20long%20videos%20but%20also%20improves%20its%0Aaccuracy%20on%20short-term%20action%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04346v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Koala%3A%20Key%20frame-conditioned%20long%20video-LLM&entry.906535625=Reuben%20Tan%20and%20Ximeng%20Sun%20and%20Ping%20Hu%20and%20Jui-hsien%20Wang%20and%20Hanieh%20Deilamsalehy%20and%20Bryan%20A.%20Plummer%20and%20Bryan%20Russell%20and%20Kate%20Saenko&entry.1292438233=%20%20Long%20video%20question%20answering%20is%20a%20challenging%20task%20that%20involves%20recognizing%0Ashort-term%20activities%20and%20reasoning%20about%20their%20fine-grained%20relationships.%0AState-of-the-art%20video%20Large%20Language%20Models%20%28vLLMs%29%20hold%20promise%20as%20a%20viable%0Asolution%20due%20to%20their%20demonstrated%20emergent%20capabilities%20on%20new%20tasks.%20However%2C%0Adespite%20being%20trained%20on%20millions%20of%20short%20seconds-long%20videos%2C%20vLLMs%20are%0Aunable%20to%20understand%20minutes-long%20videos%20and%20accurately%20answer%20questions%20about%0Athem.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20lightweight%20and%20self-supervised%0Aapproach%2C%20Key%20frame-conditioned%20long%20video-LLM%20%28Koala%29%2C%20that%20introduces%0Alearnable%20spatiotemporal%20queries%20to%20adapt%20pretrained%20vLLMs%20for%20generalizing%20to%0Alonger%20videos.%20Our%20approach%20introduces%20two%20new%20tokenizers%20that%20condition%20on%0Avisual%20tokens%20computed%20from%20sparse%20video%20key%20frames%20for%20understanding%20short%20and%0Along%20video%20moments.%20We%20train%20our%20proposed%20approach%20on%20HowTo100M%20and%20demonstrate%0Aits%20effectiveness%20on%20zero-shot%20long%20video%20understanding%20benchmarks%2C%20where%20it%0Aoutperforms%20state-of-the-art%20large%20models%20by%203%20-%206%25%20in%20absolute%20accuracy%20across%0Aall%20tasks.%20Surprisingly%2C%20we%20also%20empirically%20show%20that%20our%20approach%20not%20only%0Ahelps%20a%20pretrained%20vLLM%20to%20understand%20long%20videos%20but%20also%20improves%20its%0Aaccuracy%20on%20short-term%20action%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04346v2&entry.124074799=Read"},
{"title": "Hierarchical Position Embedding of Graphs with Landmarks and Clustering\n  for Link Prediction", "author": "Minsang Kim and Seungjun Baek", "abstract": "  Learning positional information of nodes in a graph is important for link\nprediction tasks. We propose a representation of positional information using\nrepresentative nodes called landmarks. A small number of nodes with high degree\ncentrality are selected as landmarks, which serve as reference points for the\nnodes' positions. We justify this selection strategy for well-known random\ngraph models and derive closed-form bounds on the average path lengths\ninvolving landmarks. In a model for power-law graphs, we prove that landmarks\nprovide asymptotically exact information on inter-node distances. We apply\ntheoretical insights to practical networks and propose Hierarchical Position\nembedding with Landmarks and Clustering (HPLC). HPLC combines landmark\nselection and graph clustering, where the graph is partitioned into densely\nconnected clusters in which nodes with the highest degree are selected as\nlandmarks. HPLC leverages the positional information of nodes based on\nlandmarks at various levels of hierarchy such as nodes' distances to landmarks,\ninter-landmark distances and hierarchical grouping of clusters. Experiments\nshow that HPLC achieves state-of-the-art performances of link prediction on\nvarious datasets in terms of HIT@K, MRR, and AUC. The code is available at\n\\url{https://github.com/kmswin1/HPLC}.\n", "link": "http://arxiv.org/abs/2402.08174v2", "date": "2024-04-19", "relevancy": 2.0785, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5029}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4992}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Position%20Embedding%20of%20Graphs%20with%20Landmarks%20and%20Clustering%0A%20%20for%20Link%20Prediction&body=Title%3A%20Hierarchical%20Position%20Embedding%20of%20Graphs%20with%20Landmarks%20and%20Clustering%0A%20%20for%20Link%20Prediction%0AAuthor%3A%20Minsang%20Kim%20and%20Seungjun%20Baek%0AAbstract%3A%20%20%20Learning%20positional%20information%20of%20nodes%20in%20a%20graph%20is%20important%20for%20link%0Aprediction%20tasks.%20We%20propose%20a%20representation%20of%20positional%20information%20using%0Arepresentative%20nodes%20called%20landmarks.%20A%20small%20number%20of%20nodes%20with%20high%20degree%0Acentrality%20are%20selected%20as%20landmarks%2C%20which%20serve%20as%20reference%20points%20for%20the%0Anodes%27%20positions.%20We%20justify%20this%20selection%20strategy%20for%20well-known%20random%0Agraph%20models%20and%20derive%20closed-form%20bounds%20on%20the%20average%20path%20lengths%0Ainvolving%20landmarks.%20In%20a%20model%20for%20power-law%20graphs%2C%20we%20prove%20that%20landmarks%0Aprovide%20asymptotically%20exact%20information%20on%20inter-node%20distances.%20We%20apply%0Atheoretical%20insights%20to%20practical%20networks%20and%20propose%20Hierarchical%20Position%0Aembedding%20with%20Landmarks%20and%20Clustering%20%28HPLC%29.%20HPLC%20combines%20landmark%0Aselection%20and%20graph%20clustering%2C%20where%20the%20graph%20is%20partitioned%20into%20densely%0Aconnected%20clusters%20in%20which%20nodes%20with%20the%20highest%20degree%20are%20selected%20as%0Alandmarks.%20HPLC%20leverages%20the%20positional%20information%20of%20nodes%20based%20on%0Alandmarks%20at%20various%20levels%20of%20hierarchy%20such%20as%20nodes%27%20distances%20to%20landmarks%2C%0Ainter-landmark%20distances%20and%20hierarchical%20grouping%20of%20clusters.%20Experiments%0Ashow%20that%20HPLC%20achieves%20state-of-the-art%20performances%20of%20link%20prediction%20on%0Avarious%20datasets%20in%20terms%20of%20HIT%40K%2C%20MRR%2C%20and%20AUC.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/kmswin1/HPLC%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08174v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Position%20Embedding%20of%20Graphs%20with%20Landmarks%20and%20Clustering%0A%20%20for%20Link%20Prediction&entry.906535625=Minsang%20Kim%20and%20Seungjun%20Baek&entry.1292438233=%20%20Learning%20positional%20information%20of%20nodes%20in%20a%20graph%20is%20important%20for%20link%0Aprediction%20tasks.%20We%20propose%20a%20representation%20of%20positional%20information%20using%0Arepresentative%20nodes%20called%20landmarks.%20A%20small%20number%20of%20nodes%20with%20high%20degree%0Acentrality%20are%20selected%20as%20landmarks%2C%20which%20serve%20as%20reference%20points%20for%20the%0Anodes%27%20positions.%20We%20justify%20this%20selection%20strategy%20for%20well-known%20random%0Agraph%20models%20and%20derive%20closed-form%20bounds%20on%20the%20average%20path%20lengths%0Ainvolving%20landmarks.%20In%20a%20model%20for%20power-law%20graphs%2C%20we%20prove%20that%20landmarks%0Aprovide%20asymptotically%20exact%20information%20on%20inter-node%20distances.%20We%20apply%0Atheoretical%20insights%20to%20practical%20networks%20and%20propose%20Hierarchical%20Position%0Aembedding%20with%20Landmarks%20and%20Clustering%20%28HPLC%29.%20HPLC%20combines%20landmark%0Aselection%20and%20graph%20clustering%2C%20where%20the%20graph%20is%20partitioned%20into%20densely%0Aconnected%20clusters%20in%20which%20nodes%20with%20the%20highest%20degree%20are%20selected%20as%0Alandmarks.%20HPLC%20leverages%20the%20positional%20information%20of%20nodes%20based%20on%0Alandmarks%20at%20various%20levels%20of%20hierarchy%20such%20as%20nodes%27%20distances%20to%20landmarks%2C%0Ainter-landmark%20distances%20and%20hierarchical%20grouping%20of%20clusters.%20Experiments%0Ashow%20that%20HPLC%20achieves%20state-of-the-art%20performances%20of%20link%20prediction%20on%0Avarious%20datasets%20in%20terms%20of%20HIT%40K%2C%20MRR%2C%20and%20AUC.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/kmswin1/HPLC%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08174v2&entry.124074799=Read"},
{"title": "QGen: On the Ability to Generalize in Quantization Aware Training", "author": "MohammadHossein AskariHemmat and Ahmadreza Jeddi and Reyhane Askari Hemmat and Ivan Lazarevich and Alexander Hoffman and Sudhakar Sah and Ehsan Saboori and Yvon Savaria and Jean-Pierre David", "abstract": "  Quantization lowers memory usage, computational requirements, and latency by\nutilizing fewer bits to represent model weights and activations. In this work,\nwe investigate the generalization properties of quantized neural networks, a\ncharacteristic that has received little attention despite its implications on\nmodel performance. In particular, first, we develop a theoretical model for\nquantization in neural networks and demonstrate how quantization functions as a\nform of regularization. Second, motivated by recent work connecting the\nsharpness of the loss landscape and generalization, we derive an approximate\nbound for the generalization of quantized models conditioned on the amount of\nquantization noise. We then validate our hypothesis by experimenting with over\n2000 models trained on CIFAR-10, CIFAR-100, and ImageNet datasets on\nconvolutional and transformer-based models.\n", "link": "http://arxiv.org/abs/2404.11769v2", "date": "2024-04-19", "relevancy": 2.0774, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5374}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5251}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5064}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20QGen%3A%20On%20the%20Ability%20to%20Generalize%20in%20Quantization%20Aware%20Training&body=Title%3A%20QGen%3A%20On%20the%20Ability%20to%20Generalize%20in%20Quantization%20Aware%20Training%0AAuthor%3A%20MohammadHossein%20AskariHemmat%20and%20Ahmadreza%20Jeddi%20and%20Reyhane%20Askari%20Hemmat%20and%20Ivan%20Lazarevich%20and%20Alexander%20Hoffman%20and%20Sudhakar%20Sah%20and%20Ehsan%20Saboori%20and%20Yvon%20Savaria%20and%20Jean-Pierre%20David%0AAbstract%3A%20%20%20Quantization%20lowers%20memory%20usage%2C%20computational%20requirements%2C%20and%20latency%20by%0Autilizing%20fewer%20bits%20to%20represent%20model%20weights%20and%20activations.%20In%20this%20work%2C%0Awe%20investigate%20the%20generalization%20properties%20of%20quantized%20neural%20networks%2C%20a%0Acharacteristic%20that%20has%20received%20little%20attention%20despite%20its%20implications%20on%0Amodel%20performance.%20In%20particular%2C%20first%2C%20we%20develop%20a%20theoretical%20model%20for%0Aquantization%20in%20neural%20networks%20and%20demonstrate%20how%20quantization%20functions%20as%20a%0Aform%20of%20regularization.%20Second%2C%20motivated%20by%20recent%20work%20connecting%20the%0Asharpness%20of%20the%20loss%20landscape%20and%20generalization%2C%20we%20derive%20an%20approximate%0Abound%20for%20the%20generalization%20of%20quantized%20models%20conditioned%20on%20the%20amount%20of%0Aquantization%20noise.%20We%20then%20validate%20our%20hypothesis%20by%20experimenting%20with%20over%0A2000%20models%20trained%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet%20datasets%20on%0Aconvolutional%20and%20transformer-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11769v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QGen%3A%20On%20the%20Ability%20to%20Generalize%20in%20Quantization%20Aware%20Training&entry.906535625=MohammadHossein%20AskariHemmat%20and%20Ahmadreza%20Jeddi%20and%20Reyhane%20Askari%20Hemmat%20and%20Ivan%20Lazarevich%20and%20Alexander%20Hoffman%20and%20Sudhakar%20Sah%20and%20Ehsan%20Saboori%20and%20Yvon%20Savaria%20and%20Jean-Pierre%20David&entry.1292438233=%20%20Quantization%20lowers%20memory%20usage%2C%20computational%20requirements%2C%20and%20latency%20by%0Autilizing%20fewer%20bits%20to%20represent%20model%20weights%20and%20activations.%20In%20this%20work%2C%0Awe%20investigate%20the%20generalization%20properties%20of%20quantized%20neural%20networks%2C%20a%0Acharacteristic%20that%20has%20received%20little%20attention%20despite%20its%20implications%20on%0Amodel%20performance.%20In%20particular%2C%20first%2C%20we%20develop%20a%20theoretical%20model%20for%0Aquantization%20in%20neural%20networks%20and%20demonstrate%20how%20quantization%20functions%20as%20a%0Aform%20of%20regularization.%20Second%2C%20motivated%20by%20recent%20work%20connecting%20the%0Asharpness%20of%20the%20loss%20landscape%20and%20generalization%2C%20we%20derive%20an%20approximate%0Abound%20for%20the%20generalization%20of%20quantized%20models%20conditioned%20on%20the%20amount%20of%0Aquantization%20noise.%20We%20then%20validate%20our%20hypothesis%20by%20experimenting%20with%20over%0A2000%20models%20trained%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet%20datasets%20on%0Aconvolutional%20and%20transformer-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11769v2&entry.124074799=Read"},
{"title": "Is Retain Set All You Need in Machine Unlearning? Restoring Performance\n  of Unlearned Models with Out-Of-Distribution Images", "author": "Jacopo Bonato and Marco Cotogni and Luigi Sabetta", "abstract": "  In this paper, we introduce Selective-distillation for Class and\nArchitecture-agnostic unleaRning (SCAR), a novel approximate unlearning method.\nSCAR efficiently eliminates specific information while preserving the model's\ntest accuracy without using a retain set, which is a key component in\nstate-of-the-art approximate unlearning algorithms. Our approach utilizes a\nmodified Mahalanobis distance to guide the unlearning of the feature vectors of\nthe instances to be forgotten, aligning them to the nearest wrong class\ndistribution. Moreover, we propose a distillation-trick mechanism that distills\nthe knowledge of the original model into the unlearning model with\nout-of-distribution images for retaining the original model's test performance\nwithout using any retain set. Importantly, we propose a self-forget version of\nSCAR that unlearns without having access to the forget set. We experimentally\nverified the effectiveness of our method, on three public datasets, comparing\nit with state-of-the-art methods. Our method obtains performance higher than\nmethods that operate without the retain set and comparable w.r.t the best\nmethods that rely on the retain set.\n", "link": "http://arxiv.org/abs/2404.12922v1", "date": "2024-04-19", "relevancy": 2.0724, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5374}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5153}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4767}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Is%20Retain%20Set%20All%20You%20Need%20in%20Machine%20Unlearning%3F%20Restoring%20Performance%0A%20%20of%20Unlearned%20Models%20with%20Out-Of-Distribution%20Images&body=Title%3A%20Is%20Retain%20Set%20All%20You%20Need%20in%20Machine%20Unlearning%3F%20Restoring%20Performance%0A%20%20of%20Unlearned%20Models%20with%20Out-Of-Distribution%20Images%0AAuthor%3A%20Jacopo%20Bonato%20and%20Marco%20Cotogni%20and%20Luigi%20Sabetta%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Selective-distillation%20for%20Class%20and%0AArchitecture-agnostic%20unleaRning%20%28SCAR%29%2C%20a%20novel%20approximate%20unlearning%20method.%0ASCAR%20efficiently%20eliminates%20specific%20information%20while%20preserving%20the%20model%27s%0Atest%20accuracy%20without%20using%20a%20retain%20set%2C%20which%20is%20a%20key%20component%20in%0Astate-of-the-art%20approximate%20unlearning%20algorithms.%20Our%20approach%20utilizes%20a%0Amodified%20Mahalanobis%20distance%20to%20guide%20the%20unlearning%20of%20the%20feature%20vectors%20of%0Athe%20instances%20to%20be%20forgotten%2C%20aligning%20them%20to%20the%20nearest%20wrong%20class%0Adistribution.%20Moreover%2C%20we%20propose%20a%20distillation-trick%20mechanism%20that%20distills%0Athe%20knowledge%20of%20the%20original%20model%20into%20the%20unlearning%20model%20with%0Aout-of-distribution%20images%20for%20retaining%20the%20original%20model%27s%20test%20performance%0Awithout%20using%20any%20retain%20set.%20Importantly%2C%20we%20propose%20a%20self-forget%20version%20of%0ASCAR%20that%20unlearns%20without%20having%20access%20to%20the%20forget%20set.%20We%20experimentally%0Averified%20the%20effectiveness%20of%20our%20method%2C%20on%20three%20public%20datasets%2C%20comparing%0Ait%20with%20state-of-the-art%20methods.%20Our%20method%20obtains%20performance%20higher%20than%0Amethods%20that%20operate%20without%20the%20retain%20set%20and%20comparable%20w.r.t%20the%20best%0Amethods%20that%20rely%20on%20the%20retain%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12922v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Retain%20Set%20All%20You%20Need%20in%20Machine%20Unlearning%3F%20Restoring%20Performance%0A%20%20of%20Unlearned%20Models%20with%20Out-Of-Distribution%20Images&entry.906535625=Jacopo%20Bonato%20and%20Marco%20Cotogni%20and%20Luigi%20Sabetta&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Selective-distillation%20for%20Class%20and%0AArchitecture-agnostic%20unleaRning%20%28SCAR%29%2C%20a%20novel%20approximate%20unlearning%20method.%0ASCAR%20efficiently%20eliminates%20specific%20information%20while%20preserving%20the%20model%27s%0Atest%20accuracy%20without%20using%20a%20retain%20set%2C%20which%20is%20a%20key%20component%20in%0Astate-of-the-art%20approximate%20unlearning%20algorithms.%20Our%20approach%20utilizes%20a%0Amodified%20Mahalanobis%20distance%20to%20guide%20the%20unlearning%20of%20the%20feature%20vectors%20of%0Athe%20instances%20to%20be%20forgotten%2C%20aligning%20them%20to%20the%20nearest%20wrong%20class%0Adistribution.%20Moreover%2C%20we%20propose%20a%20distillation-trick%20mechanism%20that%20distills%0Athe%20knowledge%20of%20the%20original%20model%20into%20the%20unlearning%20model%20with%0Aout-of-distribution%20images%20for%20retaining%20the%20original%20model%27s%20test%20performance%0Awithout%20using%20any%20retain%20set.%20Importantly%2C%20we%20propose%20a%20self-forget%20version%20of%0ASCAR%20that%20unlearns%20without%20having%20access%20to%20the%20forget%20set.%20We%20experimentally%0Averified%20the%20effectiveness%20of%20our%20method%2C%20on%20three%20public%20datasets%2C%20comparing%0Ait%20with%20state-of-the-art%20methods.%20Our%20method%20obtains%20performance%20higher%20than%0Amethods%20that%20operate%20without%20the%20retain%20set%20and%20comparable%20w.r.t%20the%20best%0Amethods%20that%20rely%20on%20the%20retain%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12922v1&entry.124074799=Read"},
{"title": "decoupleQ: Towards 2-bit Post-Training Uniform Quantization via\n  decoupling Parameters into Integer and Floating Points", "author": "Yi Guo and Fanliu Kong and Xiaoyang Li and Hui Li and Wei Chen and Xiaogang Tian and Jinping Cai and Yang Zhang and Shouda Liu", "abstract": "  Quantization emerges as one of the most promising compression technologies\nfor deploying efficient large models for various real time application in\nrecent years. Considering that the storage and IO of weights take up the vast\nmajority of the overhead inside a large model, weight only quantization can\nlead to large gains. However, existing quantization schemes suffer from\nsignificant accuracy degradation at very low bits, or require some additional\ncomputational overhead when deployed, making it difficult to be applied to\nlarge-scale applications in industry. In this paper, we propose decoupleQ,\nachieving a substantial increase in model accuracy, especially at very low\nbits. decoupleQ abandons the traditional heuristic quantization paradigm and\ndecouples the model parameters into integer and floating-point parts, thus\ntransforming the quantization problem into a traditional mathematical\noptimization problem with constraints, which is then solved alternatively by\noff-the-shelf optimization methods.\n  Quantization via decoupleQ is linear and uniform, making it\nhardware-friendlier than non-uniform counterpart, and enabling the idea to be\nmigrated to high-bit quantization to enhance its robustness. Our method has\nachieved well on-line accuracy near fp16/bf16 on the 2-bit quantization of\nlarge speech models in ByteDance. The code is available at\nhttps://github.com/bytedance/decoupleQ\n", "link": "http://arxiv.org/abs/2404.12759v1", "date": "2024-04-19", "relevancy": 2.0681, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5388}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5077}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.499}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20decoupleQ%3A%20Towards%202-bit%20Post-Training%20Uniform%20Quantization%20via%0A%20%20decoupling%20Parameters%20into%20Integer%20and%20Floating%20Points&body=Title%3A%20decoupleQ%3A%20Towards%202-bit%20Post-Training%20Uniform%20Quantization%20via%0A%20%20decoupling%20Parameters%20into%20Integer%20and%20Floating%20Points%0AAuthor%3A%20Yi%20Guo%20and%20Fanliu%20Kong%20and%20Xiaoyang%20Li%20and%20Hui%20Li%20and%20Wei%20Chen%20and%20Xiaogang%20Tian%20and%20Jinping%20Cai%20and%20Yang%20Zhang%20and%20Shouda%20Liu%0AAbstract%3A%20%20%20Quantization%20emerges%20as%20one%20of%20the%20most%20promising%20compression%20technologies%0Afor%20deploying%20efficient%20large%20models%20for%20various%20real%20time%20application%20in%0Arecent%20years.%20Considering%20that%20the%20storage%20and%20IO%20of%20weights%20take%20up%20the%20vast%0Amajority%20of%20the%20overhead%20inside%20a%20large%20model%2C%20weight%20only%20quantization%20can%0Alead%20to%20large%20gains.%20However%2C%20existing%20quantization%20schemes%20suffer%20from%0Asignificant%20accuracy%20degradation%20at%20very%20low%20bits%2C%20or%20require%20some%20additional%0Acomputational%20overhead%20when%20deployed%2C%20making%20it%20difficult%20to%20be%20applied%20to%0Alarge-scale%20applications%20in%20industry.%20In%20this%20paper%2C%20we%20propose%20decoupleQ%2C%0Aachieving%20a%20substantial%20increase%20in%20model%20accuracy%2C%20especially%20at%20very%20low%0Abits.%20decoupleQ%20abandons%20the%20traditional%20heuristic%20quantization%20paradigm%20and%0Adecouples%20the%20model%20parameters%20into%20integer%20and%20floating-point%20parts%2C%20thus%0Atransforming%20the%20quantization%20problem%20into%20a%20traditional%20mathematical%0Aoptimization%20problem%20with%20constraints%2C%20which%20is%20then%20solved%20alternatively%20by%0Aoff-the-shelf%20optimization%20methods.%0A%20%20Quantization%20via%20decoupleQ%20is%20linear%20and%20uniform%2C%20making%20it%0Ahardware-friendlier%20than%20non-uniform%20counterpart%2C%20and%20enabling%20the%20idea%20to%20be%0Amigrated%20to%20high-bit%20quantization%20to%20enhance%20its%20robustness.%20Our%20method%20has%0Aachieved%20well%20on-line%20accuracy%20near%20fp16/bf16%20on%20the%202-bit%20quantization%20of%0Alarge%20speech%20models%20in%20ByteDance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/bytedance/decoupleQ%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12759v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=decoupleQ%3A%20Towards%202-bit%20Post-Training%20Uniform%20Quantization%20via%0A%20%20decoupling%20Parameters%20into%20Integer%20and%20Floating%20Points&entry.906535625=Yi%20Guo%20and%20Fanliu%20Kong%20and%20Xiaoyang%20Li%20and%20Hui%20Li%20and%20Wei%20Chen%20and%20Xiaogang%20Tian%20and%20Jinping%20Cai%20and%20Yang%20Zhang%20and%20Shouda%20Liu&entry.1292438233=%20%20Quantization%20emerges%20as%20one%20of%20the%20most%20promising%20compression%20technologies%0Afor%20deploying%20efficient%20large%20models%20for%20various%20real%20time%20application%20in%0Arecent%20years.%20Considering%20that%20the%20storage%20and%20IO%20of%20weights%20take%20up%20the%20vast%0Amajority%20of%20the%20overhead%20inside%20a%20large%20model%2C%20weight%20only%20quantization%20can%0Alead%20to%20large%20gains.%20However%2C%20existing%20quantization%20schemes%20suffer%20from%0Asignificant%20accuracy%20degradation%20at%20very%20low%20bits%2C%20or%20require%20some%20additional%0Acomputational%20overhead%20when%20deployed%2C%20making%20it%20difficult%20to%20be%20applied%20to%0Alarge-scale%20applications%20in%20industry.%20In%20this%20paper%2C%20we%20propose%20decoupleQ%2C%0Aachieving%20a%20substantial%20increase%20in%20model%20accuracy%2C%20especially%20at%20very%20low%0Abits.%20decoupleQ%20abandons%20the%20traditional%20heuristic%20quantization%20paradigm%20and%0Adecouples%20the%20model%20parameters%20into%20integer%20and%20floating-point%20parts%2C%20thus%0Atransforming%20the%20quantization%20problem%20into%20a%20traditional%20mathematical%0Aoptimization%20problem%20with%20constraints%2C%20which%20is%20then%20solved%20alternatively%20by%0Aoff-the-shelf%20optimization%20methods.%0A%20%20Quantization%20via%20decoupleQ%20is%20linear%20and%20uniform%2C%20making%20it%0Ahardware-friendlier%20than%20non-uniform%20counterpart%2C%20and%20enabling%20the%20idea%20to%20be%0Amigrated%20to%20high-bit%20quantization%20to%20enhance%20its%20robustness.%20Our%20method%20has%0Aachieved%20well%20on-line%20accuracy%20near%20fp16/bf16%20on%20the%202-bit%20quantization%20of%0Alarge%20speech%20models%20in%20ByteDance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/bytedance/decoupleQ%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12759v1&entry.124074799=Read"},
{"title": "Mitigating Open-Vocabulary Caption Hallucinations", "author": "Assaf Ben-Kish and Moran Yanuka and Morris Alper and Raja Giryes and Hadar Averbuch-Elor", "abstract": "  While recent years have seen rapid progress in image-conditioned text\ngeneration, image captioning still suffers from the fundamental issue of\nhallucinations, namely, the generation of spurious details that cannot be\ninferred from the given image. Existing methods largely use closed-vocabulary\nobject lists to mitigate or evaluate hallucinations in image captioning,\nignoring the long-tailed nature of hallucinations that occur in practice. To\nthis end, we propose a framework for addressing hallucinations in image\ncaptioning in the open-vocabulary setting. Our framework includes a new\nbenchmark, OpenCHAIR, that leverages generative foundation models to evaluate\nopen-vocabulary object hallucinations for image captioning, surpassing the\npopular and similarly-sized CHAIR benchmark in both diversity and accuracy.\nFurthermore, to mitigate open-vocabulary hallucinations without using a closed\nobject list, we propose MOCHa, an approach harnessing advancements in\nreinforcement learning. Our multi-objective reward function explicitly targets\nthe trade-off between fidelity and adequacy in generations without requiring\nany strong supervision. MOCHa improves a large variety of image captioning\nmodels, as captured by our OpenCHAIR benchmark and other existing metrics. We\nwill release our code and models.\n", "link": "http://arxiv.org/abs/2312.03631v3", "date": "2024-04-19", "relevancy": 2.0401, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5118}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5085}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Open-Vocabulary%20Caption%20Hallucinations&body=Title%3A%20Mitigating%20Open-Vocabulary%20Caption%20Hallucinations%0AAuthor%3A%20Assaf%20Ben-Kish%20and%20Moran%20Yanuka%20and%20Morris%20Alper%20and%20Raja%20Giryes%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20%20%20While%20recent%20years%20have%20seen%20rapid%20progress%20in%20image-conditioned%20text%0Ageneration%2C%20image%20captioning%20still%20suffers%20from%20the%20fundamental%20issue%20of%0Ahallucinations%2C%20namely%2C%20the%20generation%20of%20spurious%20details%20that%20cannot%20be%0Ainferred%20from%20the%20given%20image.%20Existing%20methods%20largely%20use%20closed-vocabulary%0Aobject%20lists%20to%20mitigate%20or%20evaluate%20hallucinations%20in%20image%20captioning%2C%0Aignoring%20the%20long-tailed%20nature%20of%20hallucinations%20that%20occur%20in%20practice.%20To%0Athis%20end%2C%20we%20propose%20a%20framework%20for%20addressing%20hallucinations%20in%20image%0Acaptioning%20in%20the%20open-vocabulary%20setting.%20Our%20framework%20includes%20a%20new%0Abenchmark%2C%20OpenCHAIR%2C%20that%20leverages%20generative%20foundation%20models%20to%20evaluate%0Aopen-vocabulary%20object%20hallucinations%20for%20image%20captioning%2C%20surpassing%20the%0Apopular%20and%20similarly-sized%20CHAIR%20benchmark%20in%20both%20diversity%20and%20accuracy.%0AFurthermore%2C%20to%20mitigate%20open-vocabulary%20hallucinations%20without%20using%20a%20closed%0Aobject%20list%2C%20we%20propose%20MOCHa%2C%20an%20approach%20harnessing%20advancements%20in%0Areinforcement%20learning.%20Our%20multi-objective%20reward%20function%20explicitly%20targets%0Athe%20trade-off%20between%20fidelity%20and%20adequacy%20in%20generations%20without%20requiring%0Aany%20strong%20supervision.%20MOCHa%20improves%20a%20large%20variety%20of%20image%20captioning%0Amodels%2C%20as%20captured%20by%20our%20OpenCHAIR%20benchmark%20and%20other%20existing%20metrics.%20We%0Awill%20release%20our%20code%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03631v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Open-Vocabulary%20Caption%20Hallucinations&entry.906535625=Assaf%20Ben-Kish%20and%20Moran%20Yanuka%20and%20Morris%20Alper%20and%20Raja%20Giryes%20and%20Hadar%20Averbuch-Elor&entry.1292438233=%20%20While%20recent%20years%20have%20seen%20rapid%20progress%20in%20image-conditioned%20text%0Ageneration%2C%20image%20captioning%20still%20suffers%20from%20the%20fundamental%20issue%20of%0Ahallucinations%2C%20namely%2C%20the%20generation%20of%20spurious%20details%20that%20cannot%20be%0Ainferred%20from%20the%20given%20image.%20Existing%20methods%20largely%20use%20closed-vocabulary%0Aobject%20lists%20to%20mitigate%20or%20evaluate%20hallucinations%20in%20image%20captioning%2C%0Aignoring%20the%20long-tailed%20nature%20of%20hallucinations%20that%20occur%20in%20practice.%20To%0Athis%20end%2C%20we%20propose%20a%20framework%20for%20addressing%20hallucinations%20in%20image%0Acaptioning%20in%20the%20open-vocabulary%20setting.%20Our%20framework%20includes%20a%20new%0Abenchmark%2C%20OpenCHAIR%2C%20that%20leverages%20generative%20foundation%20models%20to%20evaluate%0Aopen-vocabulary%20object%20hallucinations%20for%20image%20captioning%2C%20surpassing%20the%0Apopular%20and%20similarly-sized%20CHAIR%20benchmark%20in%20both%20diversity%20and%20accuracy.%0AFurthermore%2C%20to%20mitigate%20open-vocabulary%20hallucinations%20without%20using%20a%20closed%0Aobject%20list%2C%20we%20propose%20MOCHa%2C%20an%20approach%20harnessing%20advancements%20in%0Areinforcement%20learning.%20Our%20multi-objective%20reward%20function%20explicitly%20targets%0Athe%20trade-off%20between%20fidelity%20and%20adequacy%20in%20generations%20without%20requiring%0Aany%20strong%20supervision.%20MOCHa%20improves%20a%20large%20variety%20of%20image%20captioning%0Amodels%2C%20as%20captured%20by%20our%20OpenCHAIR%20benchmark%20and%20other%20existing%20metrics.%20We%0Awill%20release%20our%20code%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03631v3&entry.124074799=Read"},
{"title": "On the Pitfalls of Batch Normalization for End-to-End Video Learning: A\n  Study on Surgical Workflow Analysis", "author": "Dominik Rivoir and Isabel Funke and Stefanie Speidel", "abstract": "  Batch Normalization's (BN) unique property of depending on other samples in a\nbatch is known to cause problems in several tasks, including sequence modeling.\nYet, BN-related issues are hardly studied for long video understanding, despite\nthe ubiquitous use of BN in CNNs (Convolutional Neural Networks) for feature\nextraction. Especially in surgical workflow analysis, where the lack of\npretrained feature extractors has led to complex, multi-stage training\npipelines, limited awareness of BN issues may have hidden the benefits of\ntraining CNNs and temporal models end to end. In this paper, we analyze\npitfalls of BN in video learning, including issues specific to online tasks\nsuch as a 'cheating' effect in anticipation. We observe that BN's properties\ncreate major obstacles for end-to-end learning. However, using BN-free\nbackbones, even simple CNN-LSTMs beat the state of the art\n{\\color{\\colorrevtwo}on three surgical workflow benchmarks} by utilizing\nadequate end-to-end training strategies which maximize temporal context. We\nconclude that awareness of BN's pitfalls is crucial for effective end-to-end\nlearning in surgical tasks. By reproducing results on natural-video datasets,\nwe hope our insights will benefit other areas of video learning as well. Code\nis available at: \\url{https://gitlab.com/nct_tso_public/pitfalls_bn}\n", "link": "http://arxiv.org/abs/2203.07976v5", "date": "2024-04-19", "relevancy": 2.0298, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.52}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5097}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5003}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Pitfalls%20of%20Batch%20Normalization%20for%20End-to-End%20Video%20Learning%3A%20A%0A%20%20Study%20on%20Surgical%20Workflow%20Analysis&body=Title%3A%20On%20the%20Pitfalls%20of%20Batch%20Normalization%20for%20End-to-End%20Video%20Learning%3A%20A%0A%20%20Study%20on%20Surgical%20Workflow%20Analysis%0AAuthor%3A%20Dominik%20Rivoir%20and%20Isabel%20Funke%20and%20Stefanie%20Speidel%0AAbstract%3A%20%20%20Batch%20Normalization%27s%20%28BN%29%20unique%20property%20of%20depending%20on%20other%20samples%20in%20a%0Abatch%20is%20known%20to%20cause%20problems%20in%20several%20tasks%2C%20including%20sequence%20modeling.%0AYet%2C%20BN-related%20issues%20are%20hardly%20studied%20for%20long%20video%20understanding%2C%20despite%0Athe%20ubiquitous%20use%20of%20BN%20in%20CNNs%20%28Convolutional%20Neural%20Networks%29%20for%20feature%0Aextraction.%20Especially%20in%20surgical%20workflow%20analysis%2C%20where%20the%20lack%20of%0Apretrained%20feature%20extractors%20has%20led%20to%20complex%2C%20multi-stage%20training%0Apipelines%2C%20limited%20awareness%20of%20BN%20issues%20may%20have%20hidden%20the%20benefits%20of%0Atraining%20CNNs%20and%20temporal%20models%20end%20to%20end.%20In%20this%20paper%2C%20we%20analyze%0Apitfalls%20of%20BN%20in%20video%20learning%2C%20including%20issues%20specific%20to%20online%20tasks%0Asuch%20as%20a%20%27cheating%27%20effect%20in%20anticipation.%20We%20observe%20that%20BN%27s%20properties%0Acreate%20major%20obstacles%20for%20end-to-end%20learning.%20However%2C%20using%20BN-free%0Abackbones%2C%20even%20simple%20CNN-LSTMs%20beat%20the%20state%20of%20the%20art%0A%7B%5Ccolor%7B%5Ccolorrevtwo%7Don%20three%20surgical%20workflow%20benchmarks%7D%20by%20utilizing%0Aadequate%20end-to-end%20training%20strategies%20which%20maximize%20temporal%20context.%20We%0Aconclude%20that%20awareness%20of%20BN%27s%20pitfalls%20is%20crucial%20for%20effective%20end-to-end%0Alearning%20in%20surgical%20tasks.%20By%20reproducing%20results%20on%20natural-video%20datasets%2C%0Awe%20hope%20our%20insights%20will%20benefit%20other%20areas%20of%20video%20learning%20as%20well.%20Code%0Ais%20available%20at%3A%20%5Curl%7Bhttps%3A//gitlab.com/nct_tso_public/pitfalls_bn%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.07976v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Pitfalls%20of%20Batch%20Normalization%20for%20End-to-End%20Video%20Learning%3A%20A%0A%20%20Study%20on%20Surgical%20Workflow%20Analysis&entry.906535625=Dominik%20Rivoir%20and%20Isabel%20Funke%20and%20Stefanie%20Speidel&entry.1292438233=%20%20Batch%20Normalization%27s%20%28BN%29%20unique%20property%20of%20depending%20on%20other%20samples%20in%20a%0Abatch%20is%20known%20to%20cause%20problems%20in%20several%20tasks%2C%20including%20sequence%20modeling.%0AYet%2C%20BN-related%20issues%20are%20hardly%20studied%20for%20long%20video%20understanding%2C%20despite%0Athe%20ubiquitous%20use%20of%20BN%20in%20CNNs%20%28Convolutional%20Neural%20Networks%29%20for%20feature%0Aextraction.%20Especially%20in%20surgical%20workflow%20analysis%2C%20where%20the%20lack%20of%0Apretrained%20feature%20extractors%20has%20led%20to%20complex%2C%20multi-stage%20training%0Apipelines%2C%20limited%20awareness%20of%20BN%20issues%20may%20have%20hidden%20the%20benefits%20of%0Atraining%20CNNs%20and%20temporal%20models%20end%20to%20end.%20In%20this%20paper%2C%20we%20analyze%0Apitfalls%20of%20BN%20in%20video%20learning%2C%20including%20issues%20specific%20to%20online%20tasks%0Asuch%20as%20a%20%27cheating%27%20effect%20in%20anticipation.%20We%20observe%20that%20BN%27s%20properties%0Acreate%20major%20obstacles%20for%20end-to-end%20learning.%20However%2C%20using%20BN-free%0Abackbones%2C%20even%20simple%20CNN-LSTMs%20beat%20the%20state%20of%20the%20art%0A%7B%5Ccolor%7B%5Ccolorrevtwo%7Don%20three%20surgical%20workflow%20benchmarks%7D%20by%20utilizing%0Aadequate%20end-to-end%20training%20strategies%20which%20maximize%20temporal%20context.%20We%0Aconclude%20that%20awareness%20of%20BN%27s%20pitfalls%20is%20crucial%20for%20effective%20end-to-end%0Alearning%20in%20surgical%20tasks.%20By%20reproducing%20results%20on%20natural-video%20datasets%2C%0Awe%20hope%20our%20insights%20will%20benefit%20other%20areas%20of%20video%20learning%20as%20well.%20Code%0Ais%20available%20at%3A%20%5Curl%7Bhttps%3A//gitlab.com/nct_tso_public/pitfalls_bn%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.07976v5&entry.124074799=Read"},
{"title": "TextSquare: Scaling up Text-Centric Visual Instruction Tuning", "author": "Jingqun Tang and Chunhui Lin and Zhen Zhao and Shu Wei and Binghong Wu and Qi Liu and Hao Feng and Yang Li and Siqi Wang and Lei Liao and Wei Shi and Yuliang Liu and Hao Liu and Yuan Xie and Xiang Bai and Can Huang", "abstract": "  Text-centric visual question answering (VQA) has made great strides with the\ndevelopment of Multimodal Large Language Models (MLLMs), yet open-source models\nstill fall short of leading models like GPT4V and Gemini, partly due to a lack\nof extensive, high-quality instruction tuning data. To this end, we introduce a\nnew approach for creating a massive, high-quality instruction-tuning dataset,\nSquare-10M, which is generated using closed-source MLLMs. The data construction\nprocess, termed Square, consists of four steps: Self-Questioning, Answering,\nReasoning, and Evaluation. Our experiments with Square-10M led to three key\nfindings: 1) Our model, TextSquare, considerably surpasses open-source previous\nstate-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%).\nIt even outperforms top-tier models like GPT4V and Gemini in 6 of 10\ntext-centric benchmarks. 2) Additionally, we demonstrate the critical role of\nVQA reasoning data in offering comprehensive contextual insights for specific\nquestions. This not only improves accuracy but also significantly mitigates\nhallucinations. Specifically, TextSquare scores an average of 75.1% across four\ngeneral VQA and hallucination evaluation datasets, outperforming previous\nstate-of-the-art models. 3) Notably, the phenomenon observed in scaling\ntext-centric VQA datasets reveals a vivid pattern: the exponential increase of\ninstruction tuning data volume is directly proportional to the improvement in\nmodel performance, thereby validating the necessity of the dataset scale and\nthe high quality of Square-10M.\n", "link": "http://arxiv.org/abs/2404.12803v1", "date": "2024-04-19", "relevancy": 2.0281, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5148}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4918}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TextSquare%3A%20Scaling%20up%20Text-Centric%20Visual%20Instruction%20Tuning&body=Title%3A%20TextSquare%3A%20Scaling%20up%20Text-Centric%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Jingqun%20Tang%20and%20Chunhui%20Lin%20and%20Zhen%20Zhao%20and%20Shu%20Wei%20and%20Binghong%20Wu%20and%20Qi%20Liu%20and%20Hao%20Feng%20and%20Yang%20Li%20and%20Siqi%20Wang%20and%20Lei%20Liao%20and%20Wei%20Shi%20and%20Yuliang%20Liu%20and%20Hao%20Liu%20and%20Yuan%20Xie%20and%20Xiang%20Bai%20and%20Can%20Huang%0AAbstract%3A%20%20%20Text-centric%20visual%20question%20answering%20%28VQA%29%20has%20made%20great%20strides%20with%20the%0Adevelopment%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20yet%20open-source%20models%0Astill%20fall%20short%20of%20leading%20models%20like%20GPT4V%20and%20Gemini%2C%20partly%20due%20to%20a%20lack%0Aof%20extensive%2C%20high-quality%20instruction%20tuning%20data.%20To%20this%20end%2C%20we%20introduce%20a%0Anew%20approach%20for%20creating%20a%20massive%2C%20high-quality%20instruction-tuning%20dataset%2C%0ASquare-10M%2C%20which%20is%20generated%20using%20closed-source%20MLLMs.%20The%20data%20construction%0Aprocess%2C%20termed%20Square%2C%20consists%20of%20four%20steps%3A%20Self-Questioning%2C%20Answering%2C%0AReasoning%2C%20and%20Evaluation.%20Our%20experiments%20with%20Square-10M%20led%20to%20three%20key%0Afindings%3A%201%29%20Our%20model%2C%20TextSquare%2C%20considerably%20surpasses%20open-source%20previous%0Astate-of-the-art%20Text-centric%20MLLMs%20and%20sets%20a%20new%20standard%20on%20OCRBench%2862.2%25%29.%0AIt%20even%20outperforms%20top-tier%20models%20like%20GPT4V%20and%20Gemini%20in%206%20of%2010%0Atext-centric%20benchmarks.%202%29%20Additionally%2C%20we%20demonstrate%20the%20critical%20role%20of%0AVQA%20reasoning%20data%20in%20offering%20comprehensive%20contextual%20insights%20for%20specific%0Aquestions.%20This%20not%20only%20improves%20accuracy%20but%20also%20significantly%20mitigates%0Ahallucinations.%20Specifically%2C%20TextSquare%20scores%20an%20average%20of%2075.1%25%20across%20four%0Ageneral%20VQA%20and%20hallucination%20evaluation%20datasets%2C%20outperforming%20previous%0Astate-of-the-art%20models.%203%29%20Notably%2C%20the%20phenomenon%20observed%20in%20scaling%0Atext-centric%20VQA%20datasets%20reveals%20a%20vivid%20pattern%3A%20the%20exponential%20increase%20of%0Ainstruction%20tuning%20data%20volume%20is%20directly%20proportional%20to%20the%20improvement%20in%0Amodel%20performance%2C%20thereby%20validating%20the%20necessity%20of%20the%20dataset%20scale%20and%0Athe%20high%20quality%20of%20Square-10M.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12803v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextSquare%3A%20Scaling%20up%20Text-Centric%20Visual%20Instruction%20Tuning&entry.906535625=Jingqun%20Tang%20and%20Chunhui%20Lin%20and%20Zhen%20Zhao%20and%20Shu%20Wei%20and%20Binghong%20Wu%20and%20Qi%20Liu%20and%20Hao%20Feng%20and%20Yang%20Li%20and%20Siqi%20Wang%20and%20Lei%20Liao%20and%20Wei%20Shi%20and%20Yuliang%20Liu%20and%20Hao%20Liu%20and%20Yuan%20Xie%20and%20Xiang%20Bai%20and%20Can%20Huang&entry.1292438233=%20%20Text-centric%20visual%20question%20answering%20%28VQA%29%20has%20made%20great%20strides%20with%20the%0Adevelopment%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20yet%20open-source%20models%0Astill%20fall%20short%20of%20leading%20models%20like%20GPT4V%20and%20Gemini%2C%20partly%20due%20to%20a%20lack%0Aof%20extensive%2C%20high-quality%20instruction%20tuning%20data.%20To%20this%20end%2C%20we%20introduce%20a%0Anew%20approach%20for%20creating%20a%20massive%2C%20high-quality%20instruction-tuning%20dataset%2C%0ASquare-10M%2C%20which%20is%20generated%20using%20closed-source%20MLLMs.%20The%20data%20construction%0Aprocess%2C%20termed%20Square%2C%20consists%20of%20four%20steps%3A%20Self-Questioning%2C%20Answering%2C%0AReasoning%2C%20and%20Evaluation.%20Our%20experiments%20with%20Square-10M%20led%20to%20three%20key%0Afindings%3A%201%29%20Our%20model%2C%20TextSquare%2C%20considerably%20surpasses%20open-source%20previous%0Astate-of-the-art%20Text-centric%20MLLMs%20and%20sets%20a%20new%20standard%20on%20OCRBench%2862.2%25%29.%0AIt%20even%20outperforms%20top-tier%20models%20like%20GPT4V%20and%20Gemini%20in%206%20of%2010%0Atext-centric%20benchmarks.%202%29%20Additionally%2C%20we%20demonstrate%20the%20critical%20role%20of%0AVQA%20reasoning%20data%20in%20offering%20comprehensive%20contextual%20insights%20for%20specific%0Aquestions.%20This%20not%20only%20improves%20accuracy%20but%20also%20significantly%20mitigates%0Ahallucinations.%20Specifically%2C%20TextSquare%20scores%20an%20average%20of%2075.1%25%20across%20four%0Ageneral%20VQA%20and%20hallucination%20evaluation%20datasets%2C%20outperforming%20previous%0Astate-of-the-art%20models.%203%29%20Notably%2C%20the%20phenomenon%20observed%20in%20scaling%0Atext-centric%20VQA%20datasets%20reveals%20a%20vivid%20pattern%3A%20the%20exponential%20increase%20of%0Ainstruction%20tuning%20data%20volume%20is%20directly%20proportional%20to%20the%20improvement%20in%0Amodel%20performance%2C%20thereby%20validating%20the%20necessity%20of%20the%20dataset%20scale%20and%0Athe%20high%20quality%20of%20Square-10M.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12803v1&entry.124074799=Read"},
{"title": "Adaptive Regularization of Representation Rank as an Implicit Constraint\n  of Bellman Equation", "author": "Qiang He and Tianyi Zhou and Meng Fang and Setareh Maghsudi", "abstract": "  Representation rank is an important concept for understanding the role of\nNeural Networks (NNs) in Deep Reinforcement learning (DRL), which measures the\nexpressive capacity of value networks. Existing studies focus on unboundedly\nmaximizing this rank; nevertheless, that approach would introduce overly\ncomplex models in the learning, thus undermining performance. Hence,\nfine-tuning representation rank presents a challenging and crucial optimization\nproblem. To address this issue, we find a guiding principle for adaptive\ncontrol of the representation rank. We employ the Bellman equation as a\ntheoretical foundation and derive an upper bound on the cosine similarity of\nconsecutive state-action pairs representations of value networks. We then\nleverage this upper bound to propose a novel regularizer, namely BEllman\nEquation-based automatic rank Regularizer (BEER). This regularizer adaptively\nregularizes the representation rank, thus improving the DRL agent's\nperformance. We first validate the effectiveness of automatic control of rank\non illustrative experiments. Then, we scale up BEER to complex continuous\ncontrol tasks by combining it with the deterministic policy gradient method.\nAmong 12 challenging DeepMind control tasks, BEER outperforms the baselines by\na large margin. Besides, BEER demonstrates significant advantages in Q-value\napproximation. Our code is available at\nhttps://github.com/sweetice/BEER-ICLR2024.\n", "link": "http://arxiv.org/abs/2404.12754v1", "date": "2024-04-19", "relevancy": 2.0214, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5275}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5202}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4816}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Regularization%20of%20Representation%20Rank%20as%20an%20Implicit%20Constraint%0A%20%20of%20Bellman%20Equation&body=Title%3A%20Adaptive%20Regularization%20of%20Representation%20Rank%20as%20an%20Implicit%20Constraint%0A%20%20of%20Bellman%20Equation%0AAuthor%3A%20Qiang%20He%20and%20Tianyi%20Zhou%20and%20Meng%20Fang%20and%20Setareh%20Maghsudi%0AAbstract%3A%20%20%20Representation%20rank%20is%20an%20important%20concept%20for%20understanding%20the%20role%20of%0ANeural%20Networks%20%28NNs%29%20in%20Deep%20Reinforcement%20learning%20%28DRL%29%2C%20which%20measures%20the%0Aexpressive%20capacity%20of%20value%20networks.%20Existing%20studies%20focus%20on%20unboundedly%0Amaximizing%20this%20rank%3B%20nevertheless%2C%20that%20approach%20would%20introduce%20overly%0Acomplex%20models%20in%20the%20learning%2C%20thus%20undermining%20performance.%20Hence%2C%0Afine-tuning%20representation%20rank%20presents%20a%20challenging%20and%20crucial%20optimization%0Aproblem.%20To%20address%20this%20issue%2C%20we%20find%20a%20guiding%20principle%20for%20adaptive%0Acontrol%20of%20the%20representation%20rank.%20We%20employ%20the%20Bellman%20equation%20as%20a%0Atheoretical%20foundation%20and%20derive%20an%20upper%20bound%20on%20the%20cosine%20similarity%20of%0Aconsecutive%20state-action%20pairs%20representations%20of%20value%20networks.%20We%20then%0Aleverage%20this%20upper%20bound%20to%20propose%20a%20novel%20regularizer%2C%20namely%20BEllman%0AEquation-based%20automatic%20rank%20Regularizer%20%28BEER%29.%20This%20regularizer%20adaptively%0Aregularizes%20the%20representation%20rank%2C%20thus%20improving%20the%20DRL%20agent%27s%0Aperformance.%20We%20first%20validate%20the%20effectiveness%20of%20automatic%20control%20of%20rank%0Aon%20illustrative%20experiments.%20Then%2C%20we%20scale%20up%20BEER%20to%20complex%20continuous%0Acontrol%20tasks%20by%20combining%20it%20with%20the%20deterministic%20policy%20gradient%20method.%0AAmong%2012%20challenging%20DeepMind%20control%20tasks%2C%20BEER%20outperforms%20the%20baselines%20by%0Aa%20large%20margin.%20Besides%2C%20BEER%20demonstrates%20significant%20advantages%20in%20Q-value%0Aapproximation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/sweetice/BEER-ICLR2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12754v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Regularization%20of%20Representation%20Rank%20as%20an%20Implicit%20Constraint%0A%20%20of%20Bellman%20Equation&entry.906535625=Qiang%20He%20and%20Tianyi%20Zhou%20and%20Meng%20Fang%20and%20Setareh%20Maghsudi&entry.1292438233=%20%20Representation%20rank%20is%20an%20important%20concept%20for%20understanding%20the%20role%20of%0ANeural%20Networks%20%28NNs%29%20in%20Deep%20Reinforcement%20learning%20%28DRL%29%2C%20which%20measures%20the%0Aexpressive%20capacity%20of%20value%20networks.%20Existing%20studies%20focus%20on%20unboundedly%0Amaximizing%20this%20rank%3B%20nevertheless%2C%20that%20approach%20would%20introduce%20overly%0Acomplex%20models%20in%20the%20learning%2C%20thus%20undermining%20performance.%20Hence%2C%0Afine-tuning%20representation%20rank%20presents%20a%20challenging%20and%20crucial%20optimization%0Aproblem.%20To%20address%20this%20issue%2C%20we%20find%20a%20guiding%20principle%20for%20adaptive%0Acontrol%20of%20the%20representation%20rank.%20We%20employ%20the%20Bellman%20equation%20as%20a%0Atheoretical%20foundation%20and%20derive%20an%20upper%20bound%20on%20the%20cosine%20similarity%20of%0Aconsecutive%20state-action%20pairs%20representations%20of%20value%20networks.%20We%20then%0Aleverage%20this%20upper%20bound%20to%20propose%20a%20novel%20regularizer%2C%20namely%20BEllman%0AEquation-based%20automatic%20rank%20Regularizer%20%28BEER%29.%20This%20regularizer%20adaptively%0Aregularizes%20the%20representation%20rank%2C%20thus%20improving%20the%20DRL%20agent%27s%0Aperformance.%20We%20first%20validate%20the%20effectiveness%20of%20automatic%20control%20of%20rank%0Aon%20illustrative%20experiments.%20Then%2C%20we%20scale%20up%20BEER%20to%20complex%20continuous%0Acontrol%20tasks%20by%20combining%20it%20with%20the%20deterministic%20policy%20gradient%20method.%0AAmong%2012%20challenging%20DeepMind%20control%20tasks%2C%20BEER%20outperforms%20the%20baselines%20by%0Aa%20large%20margin.%20Besides%2C%20BEER%20demonstrates%20significant%20advantages%20in%20Q-value%0Aapproximation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/sweetice/BEER-ICLR2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12754v1&entry.124074799=Read"},
{"title": "KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced\n  Multi-Vehicle Trajectory Forecasting at Signalized Intersections", "author": "Chuheng Wei and Guoyuan Wu and Matthew J. Barth and Amr Abdelraouf and Rohit Gupta and Kyungtae Han", "abstract": "  Reliable prediction of vehicle trajectories at signalized intersections is\ncrucial to urban traffic management and autonomous driving systems. However, it\npresents unique challenges, due to the complex roadway layout at intersections,\ninvolvement of traffic signal controls, and interactions among different types\nof road users. To address these issues, we present in this paper a novel model\ncalled Knowledge-Informed Generative Adversarial Network (KI-GAN), which\nintegrates both traffic signal information and multi-vehicle interactions to\npredict vehicle trajectories accurately. Additionally, we propose a specialized\nattention pooling method that accounts for vehicle orientation and proximity at\nintersections. Based on the SinD dataset, our KI-GAN model is able to achieve\nan Average Displacement Error (ADE) of 0.05 and a Final Displacement Error\n(FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When\nthe prediction window is extended to 9 seconds, the ADE and FDE values are\nfurther reduced to 0.11 and 0.26, respectively. These results demonstrate the\neffectiveness of the proposed KI-GAN model in vehicle trajectory prediction\nunder complex scenarios at signalized intersections, which represents a\nsignificant advancement in the target field.\n", "link": "http://arxiv.org/abs/2404.11181v2", "date": "2024-04-19", "relevancy": 2.003, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4883}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20KI-GAN%3A%20Knowledge-Informed%20Generative%20Adversarial%20Networks%20for%20Enhanced%0A%20%20Multi-Vehicle%20Trajectory%20Forecasting%20at%20Signalized%20Intersections&body=Title%3A%20KI-GAN%3A%20Knowledge-Informed%20Generative%20Adversarial%20Networks%20for%20Enhanced%0A%20%20Multi-Vehicle%20Trajectory%20Forecasting%20at%20Signalized%20Intersections%0AAuthor%3A%20Chuheng%20Wei%20and%20Guoyuan%20Wu%20and%20Matthew%20J.%20Barth%20and%20Amr%20Abdelraouf%20and%20Rohit%20Gupta%20and%20Kyungtae%20Han%0AAbstract%3A%20%20%20Reliable%20prediction%20of%20vehicle%20trajectories%20at%20signalized%20intersections%20is%0Acrucial%20to%20urban%20traffic%20management%20and%20autonomous%20driving%20systems.%20However%2C%20it%0Apresents%20unique%20challenges%2C%20due%20to%20the%20complex%20roadway%20layout%20at%20intersections%2C%0Ainvolvement%20of%20traffic%20signal%20controls%2C%20and%20interactions%20among%20different%20types%0Aof%20road%20users.%20To%20address%20these%20issues%2C%20we%20present%20in%20this%20paper%20a%20novel%20model%0Acalled%20Knowledge-Informed%20Generative%20Adversarial%20Network%20%28KI-GAN%29%2C%20which%0Aintegrates%20both%20traffic%20signal%20information%20and%20multi-vehicle%20interactions%20to%0Apredict%20vehicle%20trajectories%20accurately.%20Additionally%2C%20we%20propose%20a%20specialized%0Aattention%20pooling%20method%20that%20accounts%20for%20vehicle%20orientation%20and%20proximity%20at%0Aintersections.%20Based%20on%20the%20SinD%20dataset%2C%20our%20KI-GAN%20model%20is%20able%20to%20achieve%0Aan%20Average%20Displacement%20Error%20%28ADE%29%20of%200.05%20and%20a%20Final%20Displacement%20Error%0A%28FDE%29%20of%200.12%20for%20a%206-second%20observation%20and%206-second%20prediction%20cycle.%20When%0Athe%20prediction%20window%20is%20extended%20to%209%20seconds%2C%20the%20ADE%20and%20FDE%20values%20are%0Afurther%20reduced%20to%200.11%20and%200.26%2C%20respectively.%20These%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20KI-GAN%20model%20in%20vehicle%20trajectory%20prediction%0Aunder%20complex%20scenarios%20at%20signalized%20intersections%2C%20which%20represents%20a%0Asignificant%20advancement%20in%20the%20target%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11181v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KI-GAN%3A%20Knowledge-Informed%20Generative%20Adversarial%20Networks%20for%20Enhanced%0A%20%20Multi-Vehicle%20Trajectory%20Forecasting%20at%20Signalized%20Intersections&entry.906535625=Chuheng%20Wei%20and%20Guoyuan%20Wu%20and%20Matthew%20J.%20Barth%20and%20Amr%20Abdelraouf%20and%20Rohit%20Gupta%20and%20Kyungtae%20Han&entry.1292438233=%20%20Reliable%20prediction%20of%20vehicle%20trajectories%20at%20signalized%20intersections%20is%0Acrucial%20to%20urban%20traffic%20management%20and%20autonomous%20driving%20systems.%20However%2C%20it%0Apresents%20unique%20challenges%2C%20due%20to%20the%20complex%20roadway%20layout%20at%20intersections%2C%0Ainvolvement%20of%20traffic%20signal%20controls%2C%20and%20interactions%20among%20different%20types%0Aof%20road%20users.%20To%20address%20these%20issues%2C%20we%20present%20in%20this%20paper%20a%20novel%20model%0Acalled%20Knowledge-Informed%20Generative%20Adversarial%20Network%20%28KI-GAN%29%2C%20which%0Aintegrates%20both%20traffic%20signal%20information%20and%20multi-vehicle%20interactions%20to%0Apredict%20vehicle%20trajectories%20accurately.%20Additionally%2C%20we%20propose%20a%20specialized%0Aattention%20pooling%20method%20that%20accounts%20for%20vehicle%20orientation%20and%20proximity%20at%0Aintersections.%20Based%20on%20the%20SinD%20dataset%2C%20our%20KI-GAN%20model%20is%20able%20to%20achieve%0Aan%20Average%20Displacement%20Error%20%28ADE%29%20of%200.05%20and%20a%20Final%20Displacement%20Error%0A%28FDE%29%20of%200.12%20for%20a%206-second%20observation%20and%206-second%20prediction%20cycle.%20When%0Athe%20prediction%20window%20is%20extended%20to%209%20seconds%2C%20the%20ADE%20and%20FDE%20values%20are%0Afurther%20reduced%20to%200.11%20and%200.26%2C%20respectively.%20These%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20KI-GAN%20model%20in%20vehicle%20trajectory%20prediction%0Aunder%20complex%20scenarios%20at%20signalized%20intersections%2C%20which%20represents%20a%0Asignificant%20advancement%20in%20the%20target%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11181v2&entry.124074799=Read"},
{"title": "Explainable Deepfake Video Detection using Convolutional Neural Network\n  and CapsuleNet", "author": "Gazi Hasin Ishrak and Zalish Mahmud and MD. Zami Al Zunaed Farabe and Tahera Khanom Tinni and Tanzim Reza and Mohammad Zavid Parvez", "abstract": "  Deepfake technology, derived from deep learning, seamlessly inserts\nindividuals into digital media, irrespective of their actual participation. Its\nfoundation lies in machine learning and Artificial Intelligence (AI).\nInitially, deepfakes served research, industry, and entertainment. While the\nconcept has existed for decades, recent advancements render deepfakes nearly\nindistinguishable from reality. Accessibility has soared, empowering even\nnovices to create convincing deepfakes. However, this accessibility raises\nsecurity concerns.The primary deepfake creation algorithm, GAN (Generative\nAdversarial Network), employs machine learning to craft realistic images or\nvideos. Our objective is to utilize CNN (Convolutional Neural Network) and\nCapsuleNet with LSTM to differentiate between deepfake-generated frames and\noriginals. Furthermore, we aim to elucidate our model's decision-making process\nthrough Explainable AI, fostering transparent human-AI relationships and\noffering practical examples for real-life scenarios.\n", "link": "http://arxiv.org/abs/2404.12841v1", "date": "2024-04-19", "relevancy": 1.9687, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5073}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5014}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4734}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Explainable%20Deepfake%20Video%20Detection%20using%20Convolutional%20Neural%20Network%0A%20%20and%20CapsuleNet&body=Title%3A%20Explainable%20Deepfake%20Video%20Detection%20using%20Convolutional%20Neural%20Network%0A%20%20and%20CapsuleNet%0AAuthor%3A%20Gazi%20Hasin%20Ishrak%20and%20Zalish%20Mahmud%20and%20MD.%20Zami%20Al%20Zunaed%20Farabe%20and%20Tahera%20Khanom%20Tinni%20and%20Tanzim%20Reza%20and%20Mohammad%20Zavid%20Parvez%0AAbstract%3A%20%20%20Deepfake%20technology%2C%20derived%20from%20deep%20learning%2C%20seamlessly%20inserts%0Aindividuals%20into%20digital%20media%2C%20irrespective%20of%20their%20actual%20participation.%20Its%0Afoundation%20lies%20in%20machine%20learning%20and%20Artificial%20Intelligence%20%28AI%29.%0AInitially%2C%20deepfakes%20served%20research%2C%20industry%2C%20and%20entertainment.%20While%20the%0Aconcept%20has%20existed%20for%20decades%2C%20recent%20advancements%20render%20deepfakes%20nearly%0Aindistinguishable%20from%20reality.%20Accessibility%20has%20soared%2C%20empowering%20even%0Anovices%20to%20create%20convincing%20deepfakes.%20However%2C%20this%20accessibility%20raises%0Asecurity%20concerns.The%20primary%20deepfake%20creation%20algorithm%2C%20GAN%20%28Generative%0AAdversarial%20Network%29%2C%20employs%20machine%20learning%20to%20craft%20realistic%20images%20or%0Avideos.%20Our%20objective%20is%20to%20utilize%20CNN%20%28Convolutional%20Neural%20Network%29%20and%0ACapsuleNet%20with%20LSTM%20to%20differentiate%20between%20deepfake-generated%20frames%20and%0Aoriginals.%20Furthermore%2C%20we%20aim%20to%20elucidate%20our%20model%27s%20decision-making%20process%0Athrough%20Explainable%20AI%2C%20fostering%20transparent%20human-AI%20relationships%20and%0Aoffering%20practical%20examples%20for%20real-life%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12841v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Deepfake%20Video%20Detection%20using%20Convolutional%20Neural%20Network%0A%20%20and%20CapsuleNet&entry.906535625=Gazi%20Hasin%20Ishrak%20and%20Zalish%20Mahmud%20and%20MD.%20Zami%20Al%20Zunaed%20Farabe%20and%20Tahera%20Khanom%20Tinni%20and%20Tanzim%20Reza%20and%20Mohammad%20Zavid%20Parvez&entry.1292438233=%20%20Deepfake%20technology%2C%20derived%20from%20deep%20learning%2C%20seamlessly%20inserts%0Aindividuals%20into%20digital%20media%2C%20irrespective%20of%20their%20actual%20participation.%20Its%0Afoundation%20lies%20in%20machine%20learning%20and%20Artificial%20Intelligence%20%28AI%29.%0AInitially%2C%20deepfakes%20served%20research%2C%20industry%2C%20and%20entertainment.%20While%20the%0Aconcept%20has%20existed%20for%20decades%2C%20recent%20advancements%20render%20deepfakes%20nearly%0Aindistinguishable%20from%20reality.%20Accessibility%20has%20soared%2C%20empowering%20even%0Anovices%20to%20create%20convincing%20deepfakes.%20However%2C%20this%20accessibility%20raises%0Asecurity%20concerns.The%20primary%20deepfake%20creation%20algorithm%2C%20GAN%20%28Generative%0AAdversarial%20Network%29%2C%20employs%20machine%20learning%20to%20craft%20realistic%20images%20or%0Avideos.%20Our%20objective%20is%20to%20utilize%20CNN%20%28Convolutional%20Neural%20Network%29%20and%0ACapsuleNet%20with%20LSTM%20to%20differentiate%20between%20deepfake-generated%20frames%20and%0Aoriginals.%20Furthermore%2C%20we%20aim%20to%20elucidate%20our%20model%27s%20decision-making%20process%0Athrough%20Explainable%20AI%2C%20fostering%20transparent%20human-AI%20relationships%20and%0Aoffering%20practical%20examples%20for%20real-life%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12841v1&entry.124074799=Read"},
{"title": "Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative\n  Diseases", "author": "Mateusz Daniol and Daria Hemmerling and Jakub Sikora and Pawel Jemiolo and Marek Wodzinski and Magdalena Wojcik-Pedziwiatr", "abstract": "  Parkinson's disease ranks as the second most prevalent neurodegenerative\ndisorder globally. This research aims to develop a system leveraging Mixed\nReality capabilities for tracking and assessing eye movements. In this paper,\nwe present a medical scenario and outline the development of an application\ndesigned to capture eye-tracking signals through Mixed Reality technology for\nthe evaluation of neurodegenerative diseases. Additionally, we introduce a\npipeline for extracting clinically relevant features from eye-gaze analysis,\ndescribing the capabilities of the proposed system from a medical perspective.\nThe study involved a cohort of healthy control individuals and patients\nsuffering from Parkinson's disease, showcasing the feasibility and potential of\nthe proposed technology for non-intrusive monitoring of eye movement patterns\nfor the diagnosis of neurodegenerative diseases.\n  Clinical relevance - Developing a non-invasive biomarker for Parkinson's\ndisease is urgently needed to accurately detect the disease's onset. This would\nallow for the timely introduction of neuroprotective treatment at the earliest\nstage and enable the continuous monitoring of intervention outcomes. The\nability to detect subtle changes in eye movements allows for early diagnosis,\noffering a critical window for intervention before more pronounced symptoms\nemerge. Eye tracking provides objective and quantifiable biomarkers, ensuring\nreliable assessments of disease progression and cognitive function. The eye\ngaze analysis using Mixed Reality glasses is wireless, facilitating convenient\nassessments in both home and hospital settings. The approach offers the\nadvantage of utilizing hardware that requires no additional specialized\nattachments, enabling examinations through personal eyewear.\n", "link": "http://arxiv.org/abs/2404.12984v1", "date": "2024-04-19", "relevancy": 1.9651, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4937}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4485}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Eye-tracking%20in%20Mixed%20Reality%20for%20Diagnosis%20of%20Neurodegenerative%0A%20%20Diseases&body=Title%3A%20Eye-tracking%20in%20Mixed%20Reality%20for%20Diagnosis%20of%20Neurodegenerative%0A%20%20Diseases%0AAuthor%3A%20Mateusz%20Daniol%20and%20Daria%20Hemmerling%20and%20Jakub%20Sikora%20and%20Pawel%20Jemiolo%20and%20Marek%20Wodzinski%20and%20Magdalena%20Wojcik-Pedziwiatr%0AAbstract%3A%20%20%20Parkinson%27s%20disease%20ranks%20as%20the%20second%20most%20prevalent%20neurodegenerative%0Adisorder%20globally.%20This%20research%20aims%20to%20develop%20a%20system%20leveraging%20Mixed%0AReality%20capabilities%20for%20tracking%20and%20assessing%20eye%20movements.%20In%20this%20paper%2C%0Awe%20present%20a%20medical%20scenario%20and%20outline%20the%20development%20of%20an%20application%0Adesigned%20to%20capture%20eye-tracking%20signals%20through%20Mixed%20Reality%20technology%20for%0Athe%20evaluation%20of%20neurodegenerative%20diseases.%20Additionally%2C%20we%20introduce%20a%0Apipeline%20for%20extracting%20clinically%20relevant%20features%20from%20eye-gaze%20analysis%2C%0Adescribing%20the%20capabilities%20of%20the%20proposed%20system%20from%20a%20medical%20perspective.%0AThe%20study%20involved%20a%20cohort%20of%20healthy%20control%20individuals%20and%20patients%0Asuffering%20from%20Parkinson%27s%20disease%2C%20showcasing%20the%20feasibility%20and%20potential%20of%0Athe%20proposed%20technology%20for%20non-intrusive%20monitoring%20of%20eye%20movement%20patterns%0Afor%20the%20diagnosis%20of%20neurodegenerative%20diseases.%0A%20%20Clinical%20relevance%20-%20Developing%20a%20non-invasive%20biomarker%20for%20Parkinson%27s%0Adisease%20is%20urgently%20needed%20to%20accurately%20detect%20the%20disease%27s%20onset.%20This%20would%0Aallow%20for%20the%20timely%20introduction%20of%20neuroprotective%20treatment%20at%20the%20earliest%0Astage%20and%20enable%20the%20continuous%20monitoring%20of%20intervention%20outcomes.%20The%0Aability%20to%20detect%20subtle%20changes%20in%20eye%20movements%20allows%20for%20early%20diagnosis%2C%0Aoffering%20a%20critical%20window%20for%20intervention%20before%20more%20pronounced%20symptoms%0Aemerge.%20Eye%20tracking%20provides%20objective%20and%20quantifiable%20biomarkers%2C%20ensuring%0Areliable%20assessments%20of%20disease%20progression%20and%20cognitive%20function.%20The%20eye%0Agaze%20analysis%20using%20Mixed%20Reality%20glasses%20is%20wireless%2C%20facilitating%20convenient%0Aassessments%20in%20both%20home%20and%20hospital%20settings.%20The%20approach%20offers%20the%0Aadvantage%20of%20utilizing%20hardware%20that%20requires%20no%20additional%20specialized%0Aattachments%2C%20enabling%20examinations%20through%20personal%20eyewear.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12984v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eye-tracking%20in%20Mixed%20Reality%20for%20Diagnosis%20of%20Neurodegenerative%0A%20%20Diseases&entry.906535625=Mateusz%20Daniol%20and%20Daria%20Hemmerling%20and%20Jakub%20Sikora%20and%20Pawel%20Jemiolo%20and%20Marek%20Wodzinski%20and%20Magdalena%20Wojcik-Pedziwiatr&entry.1292438233=%20%20Parkinson%27s%20disease%20ranks%20as%20the%20second%20most%20prevalent%20neurodegenerative%0Adisorder%20globally.%20This%20research%20aims%20to%20develop%20a%20system%20leveraging%20Mixed%0AReality%20capabilities%20for%20tracking%20and%20assessing%20eye%20movements.%20In%20this%20paper%2C%0Awe%20present%20a%20medical%20scenario%20and%20outline%20the%20development%20of%20an%20application%0Adesigned%20to%20capture%20eye-tracking%20signals%20through%20Mixed%20Reality%20technology%20for%0Athe%20evaluation%20of%20neurodegenerative%20diseases.%20Additionally%2C%20we%20introduce%20a%0Apipeline%20for%20extracting%20clinically%20relevant%20features%20from%20eye-gaze%20analysis%2C%0Adescribing%20the%20capabilities%20of%20the%20proposed%20system%20from%20a%20medical%20perspective.%0AThe%20study%20involved%20a%20cohort%20of%20healthy%20control%20individuals%20and%20patients%0Asuffering%20from%20Parkinson%27s%20disease%2C%20showcasing%20the%20feasibility%20and%20potential%20of%0Athe%20proposed%20technology%20for%20non-intrusive%20monitoring%20of%20eye%20movement%20patterns%0Afor%20the%20diagnosis%20of%20neurodegenerative%20diseases.%0A%20%20Clinical%20relevance%20-%20Developing%20a%20non-invasive%20biomarker%20for%20Parkinson%27s%0Adisease%20is%20urgently%20needed%20to%20accurately%20detect%20the%20disease%27s%20onset.%20This%20would%0Aallow%20for%20the%20timely%20introduction%20of%20neuroprotective%20treatment%20at%20the%20earliest%0Astage%20and%20enable%20the%20continuous%20monitoring%20of%20intervention%20outcomes.%20The%0Aability%20to%20detect%20subtle%20changes%20in%20eye%20movements%20allows%20for%20early%20diagnosis%2C%0Aoffering%20a%20critical%20window%20for%20intervention%20before%20more%20pronounced%20symptoms%0Aemerge.%20Eye%20tracking%20provides%20objective%20and%20quantifiable%20biomarkers%2C%20ensuring%0Areliable%20assessments%20of%20disease%20progression%20and%20cognitive%20function.%20The%20eye%0Agaze%20analysis%20using%20Mixed%20Reality%20glasses%20is%20wireless%2C%20facilitating%20convenient%0Aassessments%20in%20both%20home%20and%20hospital%20settings.%20The%20approach%20offers%20the%0Aadvantage%20of%20utilizing%20hardware%20that%20requires%20no%20additional%20specialized%0Aattachments%2C%20enabling%20examinations%20through%20personal%20eyewear.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12984v1&entry.124074799=Read"},
{"title": "MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics\n  Question-Answering", "author": "Avinash Anand and Janak Kapuriya and Chhavi Kirtani and Apoorv Singh and Jay Saraf and Naman Lal and Jatin Kumar and Adarsh Raj Shivam and Astha Verma and Rajiv Ratn Shah and Roger Zimmermann", "abstract": "  Recent advancements in LLMs have shown their significant potential in tasks\nlike text summarization and generation. Yet, they often encounter difficulty\nwhile solving complex physics problems that require arithmetic calculation and\na good understanding of concepts. Moreover, many physics problems include\nimages that contain important details required to understand the problem's\ncontext. We propose an LMM-based chatbot to answer multimodal physics MCQs. For\ndomain adaptation, we utilize the MM-PhyQA dataset comprising Indian high\nschool-level multimodal physics problems. To improve the LMM's performance, we\nexperiment with two techniques, RLHF (Reinforcement Learning from Human\nFeedback) and Image Captioning. In image captioning, we add a detailed\nexplanation of the diagram in each image, minimizing hallucinations and image\nprocessing errors. We further explore the integration of Reinforcement Learning\nfrom Human Feedback (RLHF) methodology inspired by the ranking approach in RLHF\nto enhance the human-like problem-solving abilities of the models. The RLHF\napproach incorporates human feedback into the learning process of LLMs,\nimproving the model's problem-solving skills, truthfulness, and reasoning\ncapabilities, minimizing the hallucinations in the answers, and improving the\nquality instead of using vanilla-supervised fine-tuned models. We employ the\nLLaVA open-source model to answer multimodal physics MCQs and compare the\nperformance with and without using RLHF.\n", "link": "http://arxiv.org/abs/2404.12926v1", "date": "2024-04-19", "relevancy": 1.9524, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5314}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MM-PhyRLHF%3A%20Reinforcement%20Learning%20Framework%20for%20Multimodal%20Physics%0A%20%20Question-Answering&body=Title%3A%20MM-PhyRLHF%3A%20Reinforcement%20Learning%20Framework%20for%20Multimodal%20Physics%0A%20%20Question-Answering%0AAuthor%3A%20Avinash%20Anand%20and%20Janak%20Kapuriya%20and%20Chhavi%20Kirtani%20and%20Apoorv%20Singh%20and%20Jay%20Saraf%20and%20Naman%20Lal%20and%20Jatin%20Kumar%20and%20Adarsh%20Raj%20Shivam%20and%20Astha%20Verma%20and%20Rajiv%20Ratn%20Shah%20and%20Roger%20Zimmermann%0AAbstract%3A%20%20%20Recent%20advancements%20in%20LLMs%20have%20shown%20their%20significant%20potential%20in%20tasks%0Alike%20text%20summarization%20and%20generation.%20Yet%2C%20they%20often%20encounter%20difficulty%0Awhile%20solving%20complex%20physics%20problems%20that%20require%20arithmetic%20calculation%20and%0Aa%20good%20understanding%20of%20concepts.%20Moreover%2C%20many%20physics%20problems%20include%0Aimages%20that%20contain%20important%20details%20required%20to%20understand%20the%20problem%27s%0Acontext.%20We%20propose%20an%20LMM-based%20chatbot%20to%20answer%20multimodal%20physics%20MCQs.%20For%0Adomain%20adaptation%2C%20we%20utilize%20the%20MM-PhyQA%20dataset%20comprising%20Indian%20high%0Aschool-level%20multimodal%20physics%20problems.%20To%20improve%20the%20LMM%27s%20performance%2C%20we%0Aexperiment%20with%20two%20techniques%2C%20RLHF%20%28Reinforcement%20Learning%20from%20Human%0AFeedback%29%20and%20Image%20Captioning.%20In%20image%20captioning%2C%20we%20add%20a%20detailed%0Aexplanation%20of%20the%20diagram%20in%20each%20image%2C%20minimizing%20hallucinations%20and%20image%0Aprocessing%20errors.%20We%20further%20explore%20the%20integration%20of%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20methodology%20inspired%20by%20the%20ranking%20approach%20in%20RLHF%0Ato%20enhance%20the%20human-like%20problem-solving%20abilities%20of%20the%20models.%20The%20RLHF%0Aapproach%20incorporates%20human%20feedback%20into%20the%20learning%20process%20of%20LLMs%2C%0Aimproving%20the%20model%27s%20problem-solving%20skills%2C%20truthfulness%2C%20and%20reasoning%0Acapabilities%2C%20minimizing%20the%20hallucinations%20in%20the%20answers%2C%20and%20improving%20the%0Aquality%20instead%20of%20using%20vanilla-supervised%20fine-tuned%20models.%20We%20employ%20the%0ALLaVA%20open-source%20model%20to%20answer%20multimodal%20physics%20MCQs%20and%20compare%20the%0Aperformance%20with%20and%20without%20using%20RLHF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12926v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-PhyRLHF%3A%20Reinforcement%20Learning%20Framework%20for%20Multimodal%20Physics%0A%20%20Question-Answering&entry.906535625=Avinash%20Anand%20and%20Janak%20Kapuriya%20and%20Chhavi%20Kirtani%20and%20Apoorv%20Singh%20and%20Jay%20Saraf%20and%20Naman%20Lal%20and%20Jatin%20Kumar%20and%20Adarsh%20Raj%20Shivam%20and%20Astha%20Verma%20and%20Rajiv%20Ratn%20Shah%20and%20Roger%20Zimmermann&entry.1292438233=%20%20Recent%20advancements%20in%20LLMs%20have%20shown%20their%20significant%20potential%20in%20tasks%0Alike%20text%20summarization%20and%20generation.%20Yet%2C%20they%20often%20encounter%20difficulty%0Awhile%20solving%20complex%20physics%20problems%20that%20require%20arithmetic%20calculation%20and%0Aa%20good%20understanding%20of%20concepts.%20Moreover%2C%20many%20physics%20problems%20include%0Aimages%20that%20contain%20important%20details%20required%20to%20understand%20the%20problem%27s%0Acontext.%20We%20propose%20an%20LMM-based%20chatbot%20to%20answer%20multimodal%20physics%20MCQs.%20For%0Adomain%20adaptation%2C%20we%20utilize%20the%20MM-PhyQA%20dataset%20comprising%20Indian%20high%0Aschool-level%20multimodal%20physics%20problems.%20To%20improve%20the%20LMM%27s%20performance%2C%20we%0Aexperiment%20with%20two%20techniques%2C%20RLHF%20%28Reinforcement%20Learning%20from%20Human%0AFeedback%29%20and%20Image%20Captioning.%20In%20image%20captioning%2C%20we%20add%20a%20detailed%0Aexplanation%20of%20the%20diagram%20in%20each%20image%2C%20minimizing%20hallucinations%20and%20image%0Aprocessing%20errors.%20We%20further%20explore%20the%20integration%20of%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20methodology%20inspired%20by%20the%20ranking%20approach%20in%20RLHF%0Ato%20enhance%20the%20human-like%20problem-solving%20abilities%20of%20the%20models.%20The%20RLHF%0Aapproach%20incorporates%20human%20feedback%20into%20the%20learning%20process%20of%20LLMs%2C%0Aimproving%20the%20model%27s%20problem-solving%20skills%2C%20truthfulness%2C%20and%20reasoning%0Acapabilities%2C%20minimizing%20the%20hallucinations%20in%20the%20answers%2C%20and%20improving%20the%0Aquality%20instead%20of%20using%20vanilla-supervised%20fine-tuned%20models.%20We%20employ%20the%0ALLaVA%20open-source%20model%20to%20answer%20multimodal%20physics%20MCQs%20and%20compare%20the%0Aperformance%20with%20and%20without%20using%20RLHF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12926v1&entry.124074799=Read"},
{"title": "Overcoming Generic Knowledge Loss with Selective Parameter Update", "author": "Wenxuan Zhang and Paul Janson and Rahaf Aljundi and Mohamed Elhoseiny", "abstract": "  Foundation models encompass an extensive knowledge base and offer remarkable\ntransferability. However, this knowledge becomes outdated or insufficient over\ntime. The challenge lies in continuously updating foundation models to\naccommodate novel information while retaining their original capabilities.\nLeveraging the fact that foundation models have initial knowledge on various\ntasks and domains, we propose a novel approach that, instead of updating all\nparameters equally, localizes the updates to a sparse set of parameters\nrelevant to the task being learned. We strike a balance between efficiency and\nnew task performance, while maintaining the transferability and\ngeneralizability of foundation models. We extensively evaluate our method on\nfoundational vision-language models with a diverse spectrum of continual\nlearning tasks. Our method achieves improvements on the accuracy of the newly\nlearned tasks up to 7% while preserving the pretraining knowledge with a\nnegligible decrease of 0.9% on a representative control set accuracy.\n", "link": "http://arxiv.org/abs/2308.12462v4", "date": "2024-04-19", "relevancy": 1.9479, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4873}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4853}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Generic%20Knowledge%20Loss%20with%20Selective%20Parameter%20Update&body=Title%3A%20Overcoming%20Generic%20Knowledge%20Loss%20with%20Selective%20Parameter%20Update%0AAuthor%3A%20Wenxuan%20Zhang%20and%20Paul%20Janson%20and%20Rahaf%20Aljundi%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20Foundation%20models%20encompass%20an%20extensive%20knowledge%20base%20and%20offer%20remarkable%0Atransferability.%20However%2C%20this%20knowledge%20becomes%20outdated%20or%20insufficient%20over%0Atime.%20The%20challenge%20lies%20in%20continuously%20updating%20foundation%20models%20to%0Aaccommodate%20novel%20information%20while%20retaining%20their%20original%20capabilities.%0ALeveraging%20the%20fact%20that%20foundation%20models%20have%20initial%20knowledge%20on%20various%0Atasks%20and%20domains%2C%20we%20propose%20a%20novel%20approach%20that%2C%20instead%20of%20updating%20all%0Aparameters%20equally%2C%20localizes%20the%20updates%20to%20a%20sparse%20set%20of%20parameters%0Arelevant%20to%20the%20task%20being%20learned.%20We%20strike%20a%20balance%20between%20efficiency%20and%0Anew%20task%20performance%2C%20while%20maintaining%20the%20transferability%20and%0Ageneralizability%20of%20foundation%20models.%20We%20extensively%20evaluate%20our%20method%20on%0Afoundational%20vision-language%20models%20with%20a%20diverse%20spectrum%20of%20continual%0Alearning%20tasks.%20Our%20method%20achieves%20improvements%20on%20the%20accuracy%20of%20the%20newly%0Alearned%20tasks%20up%20to%207%25%20while%20preserving%20the%20pretraining%20knowledge%20with%20a%0Anegligible%20decrease%20of%200.9%25%20on%20a%20representative%20control%20set%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12462v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Generic%20Knowledge%20Loss%20with%20Selective%20Parameter%20Update&entry.906535625=Wenxuan%20Zhang%20and%20Paul%20Janson%20and%20Rahaf%20Aljundi%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20Foundation%20models%20encompass%20an%20extensive%20knowledge%20base%20and%20offer%20remarkable%0Atransferability.%20However%2C%20this%20knowledge%20becomes%20outdated%20or%20insufficient%20over%0Atime.%20The%20challenge%20lies%20in%20continuously%20updating%20foundation%20models%20to%0Aaccommodate%20novel%20information%20while%20retaining%20their%20original%20capabilities.%0ALeveraging%20the%20fact%20that%20foundation%20models%20have%20initial%20knowledge%20on%20various%0Atasks%20and%20domains%2C%20we%20propose%20a%20novel%20approach%20that%2C%20instead%20of%20updating%20all%0Aparameters%20equally%2C%20localizes%20the%20updates%20to%20a%20sparse%20set%20of%20parameters%0Arelevant%20to%20the%20task%20being%20learned.%20We%20strike%20a%20balance%20between%20efficiency%20and%0Anew%20task%20performance%2C%20while%20maintaining%20the%20transferability%20and%0Ageneralizability%20of%20foundation%20models.%20We%20extensively%20evaluate%20our%20method%20on%0Afoundational%20vision-language%20models%20with%20a%20diverse%20spectrum%20of%20continual%0Alearning%20tasks.%20Our%20method%20achieves%20improvements%20on%20the%20accuracy%20of%20the%20newly%0Alearned%20tasks%20up%20to%207%25%20while%20preserving%20the%20pretraining%20knowledge%20with%20a%0Anegligible%20decrease%20of%200.9%25%20on%20a%20representative%20control%20set%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12462v4&entry.124074799=Read"},
{"title": "One-shot skill assessment in high-stakes domains with limited data via\n  meta learning", "author": "Erim Yanik and Steven Schwaitzberg and Gene Yang and Xavier Intes and Jack Norfleet and Matthew Hackett and Suvranu De", "abstract": "  Deep Learning (DL) has achieved robust competency assessment in various\nhigh-stakes fields. However, the applicability of DL models is often hampered\nby their substantial data requirements and confinement to specific training\ndomains. This prevents them from transitioning to new tasks where data is\nscarce. Therefore, domain adaptation emerges as a critical element for the\npractical implementation of DL in real-world scenarios. Herein, we introduce\nA-VBANet, a novel meta-learning model capable of delivering domain-agnostic\nskill assessment via one-shot learning. Our methodology has been tested by\nassessing surgical skills on five laparoscopic and robotic simulators and\nreal-life laparoscopic cholecystectomy. Our model successfully adapted with\naccuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated\ntasks and 89.7% for laparoscopic cholecystectomy. This study marks the first\ninstance of a domain-agnostic methodology for skill assessment in critical\nfields setting a precedent for the broad application of DL across diverse\nreal-life domains with limited data.\n", "link": "http://arxiv.org/abs/2301.00812v5", "date": "2024-04-19", "relevancy": 1.9414, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4797}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One-shot%20skill%20assessment%20in%20high-stakes%20domains%20with%20limited%20data%20via%0A%20%20meta%20learning&body=Title%3A%20One-shot%20skill%20assessment%20in%20high-stakes%20domains%20with%20limited%20data%20via%0A%20%20meta%20learning%0AAuthor%3A%20Erim%20Yanik%20and%20Steven%20Schwaitzberg%20and%20Gene%20Yang%20and%20Xavier%20Intes%20and%20Jack%20Norfleet%20and%20Matthew%20Hackett%20and%20Suvranu%20De%0AAbstract%3A%20%20%20Deep%20Learning%20%28DL%29%20has%20achieved%20robust%20competency%20assessment%20in%20various%0Ahigh-stakes%20fields.%20However%2C%20the%20applicability%20of%20DL%20models%20is%20often%20hampered%0Aby%20their%20substantial%20data%20requirements%20and%20confinement%20to%20specific%20training%0Adomains.%20This%20prevents%20them%20from%20transitioning%20to%20new%20tasks%20where%20data%20is%0Ascarce.%20Therefore%2C%20domain%20adaptation%20emerges%20as%20a%20critical%20element%20for%20the%0Apractical%20implementation%20of%20DL%20in%20real-world%20scenarios.%20Herein%2C%20we%20introduce%0AA-VBANet%2C%20a%20novel%20meta-learning%20model%20capable%20of%20delivering%20domain-agnostic%0Askill%20assessment%20via%20one-shot%20learning.%20Our%20methodology%20has%20been%20tested%20by%0Aassessing%20surgical%20skills%20on%20five%20laparoscopic%20and%20robotic%20simulators%20and%0Areal-life%20laparoscopic%20cholecystectomy.%20Our%20model%20successfully%20adapted%20with%0Aaccuracies%20up%20to%2099.5%25%20in%20one-shot%20and%2099.9%25%20in%20few-shot%20settings%20for%20simulated%0Atasks%20and%2089.7%25%20for%20laparoscopic%20cholecystectomy.%20This%20study%20marks%20the%20first%0Ainstance%20of%20a%20domain-agnostic%20methodology%20for%20skill%20assessment%20in%20critical%0Afields%20setting%20a%20precedent%20for%20the%20broad%20application%20of%20DL%20across%20diverse%0Areal-life%20domains%20with%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.00812v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-shot%20skill%20assessment%20in%20high-stakes%20domains%20with%20limited%20data%20via%0A%20%20meta%20learning&entry.906535625=Erim%20Yanik%20and%20Steven%20Schwaitzberg%20and%20Gene%20Yang%20and%20Xavier%20Intes%20and%20Jack%20Norfleet%20and%20Matthew%20Hackett%20and%20Suvranu%20De&entry.1292438233=%20%20Deep%20Learning%20%28DL%29%20has%20achieved%20robust%20competency%20assessment%20in%20various%0Ahigh-stakes%20fields.%20However%2C%20the%20applicability%20of%20DL%20models%20is%20often%20hampered%0Aby%20their%20substantial%20data%20requirements%20and%20confinement%20to%20specific%20training%0Adomains.%20This%20prevents%20them%20from%20transitioning%20to%20new%20tasks%20where%20data%20is%0Ascarce.%20Therefore%2C%20domain%20adaptation%20emerges%20as%20a%20critical%20element%20for%20the%0Apractical%20implementation%20of%20DL%20in%20real-world%20scenarios.%20Herein%2C%20we%20introduce%0AA-VBANet%2C%20a%20novel%20meta-learning%20model%20capable%20of%20delivering%20domain-agnostic%0Askill%20assessment%20via%20one-shot%20learning.%20Our%20methodology%20has%20been%20tested%20by%0Aassessing%20surgical%20skills%20on%20five%20laparoscopic%20and%20robotic%20simulators%20and%0Areal-life%20laparoscopic%20cholecystectomy.%20Our%20model%20successfully%20adapted%20with%0Aaccuracies%20up%20to%2099.5%25%20in%20one-shot%20and%2099.9%25%20in%20few-shot%20settings%20for%20simulated%0Atasks%20and%2089.7%25%20for%20laparoscopic%20cholecystectomy.%20This%20study%20marks%20the%20first%0Ainstance%20of%20a%20domain-agnostic%20methodology%20for%20skill%20assessment%20in%20critical%0Afields%20setting%20a%20precedent%20for%20the%20broad%20application%20of%20DL%20across%20diverse%0Areal-life%20domains%20with%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.00812v5&entry.124074799=Read"},
{"title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance", "author": "Zeke Xia and Ming Hu and Dengke Yan and Xiaofei Xie and Tianlin Li and Anran Li and Junlong Zhou and Mingsong Chen", "abstract": "  Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.\n", "link": "http://arxiv.org/abs/2404.12850v1", "date": "2024-04-19", "relevancy": 1.9305, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4943}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4779}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4652}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CaBaFL%3A%20Asynchronous%20Federated%20Learning%20via%20Hierarchical%20Cache%20and%0A%20%20Feature%20Balance&body=Title%3A%20CaBaFL%3A%20Asynchronous%20Federated%20Learning%20via%20Hierarchical%20Cache%20and%0A%20%20Feature%20Balance%0AAuthor%3A%20Zeke%20Xia%20and%20Ming%20Hu%20and%20Dengke%20Yan%20and%20Xiaofei%20Xie%20and%20Tianlin%20Li%20and%20Anran%20Li%20and%20Junlong%20Zhou%20and%20Mingsong%20Chen%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20as%20a%20promising%20distributed%20machine%20learning%20paradigm%0Ahas%20been%20widely%20adopted%20in%20Artificial%20Intelligence%20of%20Things%20%28AIoT%29%0Aapplications.%20However%2C%20the%20efficiency%20and%20inference%20capability%20of%20FL%20is%0Aseriously%20limited%20due%20to%20the%20presence%20of%20stragglers%20and%20data%20imbalance%20across%0Amassive%20AIoT%20devices%2C%20respectively.%20To%20address%20the%20above%20challenges%2C%20we%20present%0Aa%20novel%20asynchronous%20FL%20approach%20named%20CaBaFL%2C%20which%20includes%20a%20hierarchical%0ACache-based%20aggregation%20mechanism%20and%20a%20feature%20Balance-guided%20device%20selection%0Astrategy.%20CaBaFL%20maintains%20multiple%20intermediate%20models%20simultaneously%20for%0Alocal%20training.%20The%20hierarchical%20cache-based%20aggregation%20mechanism%20enables%20each%0Aintermediate%20model%20to%20be%20trained%20on%20multiple%20devices%20to%20align%20the%20training%20time%0Aand%20mitigate%20the%20straggler%20issue.%20In%20specific%2C%20each%20intermediate%20model%20is%0Astored%20in%20a%20low-level%20cache%20for%20local%20training%20and%20when%20it%20is%20trained%20by%0Asufficient%20local%20devices%2C%20it%20will%20be%20stored%20in%20a%20high-level%20cache%20for%0Aaggregation.%20To%20address%20the%20problem%20of%20imbalanced%20data%2C%20the%20feature%0Abalance-guided%20device%20selection%20strategy%20in%20CaBaFL%20adopts%20the%20activation%0Adistribution%20as%20a%20metric%2C%20which%20enables%20each%20intermediate%20model%20to%20be%20trained%0Aacross%20devices%20with%20totally%20balanced%20data%20distributions%20before%20aggregation.%0AExperimental%20results%20show%20that%20compared%20with%20the%20state-of-the-art%20FL%20methods%2C%0ACaBaFL%20achieves%20up%20to%209.26X%20training%20acceleration%20and%2019.71%5C%25%20accuracy%0Aimprovements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12850v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaBaFL%3A%20Asynchronous%20Federated%20Learning%20via%20Hierarchical%20Cache%20and%0A%20%20Feature%20Balance&entry.906535625=Zeke%20Xia%20and%20Ming%20Hu%20and%20Dengke%20Yan%20and%20Xiaofei%20Xie%20and%20Tianlin%20Li%20and%20Anran%20Li%20and%20Junlong%20Zhou%20and%20Mingsong%20Chen&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20as%20a%20promising%20distributed%20machine%20learning%20paradigm%0Ahas%20been%20widely%20adopted%20in%20Artificial%20Intelligence%20of%20Things%20%28AIoT%29%0Aapplications.%20However%2C%20the%20efficiency%20and%20inference%20capability%20of%20FL%20is%0Aseriously%20limited%20due%20to%20the%20presence%20of%20stragglers%20and%20data%20imbalance%20across%0Amassive%20AIoT%20devices%2C%20respectively.%20To%20address%20the%20above%20challenges%2C%20we%20present%0Aa%20novel%20asynchronous%20FL%20approach%20named%20CaBaFL%2C%20which%20includes%20a%20hierarchical%0ACache-based%20aggregation%20mechanism%20and%20a%20feature%20Balance-guided%20device%20selection%0Astrategy.%20CaBaFL%20maintains%20multiple%20intermediate%20models%20simultaneously%20for%0Alocal%20training.%20The%20hierarchical%20cache-based%20aggregation%20mechanism%20enables%20each%0Aintermediate%20model%20to%20be%20trained%20on%20multiple%20devices%20to%20align%20the%20training%20time%0Aand%20mitigate%20the%20straggler%20issue.%20In%20specific%2C%20each%20intermediate%20model%20is%0Astored%20in%20a%20low-level%20cache%20for%20local%20training%20and%20when%20it%20is%20trained%20by%0Asufficient%20local%20devices%2C%20it%20will%20be%20stored%20in%20a%20high-level%20cache%20for%0Aaggregation.%20To%20address%20the%20problem%20of%20imbalanced%20data%2C%20the%20feature%0Abalance-guided%20device%20selection%20strategy%20in%20CaBaFL%20adopts%20the%20activation%0Adistribution%20as%20a%20metric%2C%20which%20enables%20each%20intermediate%20model%20to%20be%20trained%0Aacross%20devices%20with%20totally%20balanced%20data%20distributions%20before%20aggregation.%0AExperimental%20results%20show%20that%20compared%20with%20the%20state-of-the-art%20FL%20methods%2C%0ACaBaFL%20achieves%20up%20to%209.26X%20training%20acceleration%20and%2019.71%5C%25%20accuracy%0Aimprovements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12850v1&entry.124074799=Read"},
{"title": "Improving Pediatric Pneumonia Diagnosis with Adult Chest X-ray Images\n  Utilizing Contrastive Learning and Embedding Similarity", "author": "Mohammad Zunaed and Anwarul Hasan and Taufiq Hasan", "abstract": "  Despite the advancement of deep learning-based computer-aided diagnosis (CAD)\nmethods for pneumonia from adult chest x-ray (CXR) images, the performance of\nCAD methods applied to pediatric images remains suboptimal, mainly due to the\nlack of large-scale annotated pediatric imaging datasets. Establishing a proper\nframework to leverage existing adult large-scale CXR datasets can thus enhance\npediatric pneumonia detection performance. In this paper, we propose a\nthree-branch parallel path learning-based framework that utilizes both adult\nand pediatric datasets to improve the performance of deep learning models on\npediatric test datasets. The paths are trained with pediatric only, adult only,\nand both types of CXRs, respectively. Our proposed framework utilizes the\nmulti-positive contrastive loss to cluster the classwise embeddings and the\nembedding similarity loss among these three parallel paths to make the\nclasswise embeddings as close as possible to reduce the effect of domain shift.\nExperimental evaluations on open-access adult and pediatric CXR datasets show\nthat the proposed method achieves a superior AUROC score of 0.8464 compared to\n0.8348 obtained using the conventional approach of join training on both\ndatasets. The proposed approach thus paves the way for generalized CAD models\nthat are effective for both adult and pediatric age groups.\n", "link": "http://arxiv.org/abs/2404.12958v1", "date": "2024-04-19", "relevancy": 1.9197, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4847}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4757}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Pediatric%20Pneumonia%20Diagnosis%20with%20Adult%20Chest%20X-ray%20Images%0A%20%20Utilizing%20Contrastive%20Learning%20and%20Embedding%20Similarity&body=Title%3A%20Improving%20Pediatric%20Pneumonia%20Diagnosis%20with%20Adult%20Chest%20X-ray%20Images%0A%20%20Utilizing%20Contrastive%20Learning%20and%20Embedding%20Similarity%0AAuthor%3A%20Mohammad%20Zunaed%20and%20Anwarul%20Hasan%20and%20Taufiq%20Hasan%0AAbstract%3A%20%20%20Despite%20the%20advancement%20of%20deep%20learning-based%20computer-aided%20diagnosis%20%28CAD%29%0Amethods%20for%20pneumonia%20from%20adult%20chest%20x-ray%20%28CXR%29%20images%2C%20the%20performance%20of%0ACAD%20methods%20applied%20to%20pediatric%20images%20remains%20suboptimal%2C%20mainly%20due%20to%20the%0Alack%20of%20large-scale%20annotated%20pediatric%20imaging%20datasets.%20Establishing%20a%20proper%0Aframework%20to%20leverage%20existing%20adult%20large-scale%20CXR%20datasets%20can%20thus%20enhance%0Apediatric%20pneumonia%20detection%20performance.%20In%20this%20paper%2C%20we%20propose%20a%0Athree-branch%20parallel%20path%20learning-based%20framework%20that%20utilizes%20both%20adult%0Aand%20pediatric%20datasets%20to%20improve%20the%20performance%20of%20deep%20learning%20models%20on%0Apediatric%20test%20datasets.%20The%20paths%20are%20trained%20with%20pediatric%20only%2C%20adult%20only%2C%0Aand%20both%20types%20of%20CXRs%2C%20respectively.%20Our%20proposed%20framework%20utilizes%20the%0Amulti-positive%20contrastive%20loss%20to%20cluster%20the%20classwise%20embeddings%20and%20the%0Aembedding%20similarity%20loss%20among%20these%20three%20parallel%20paths%20to%20make%20the%0Aclasswise%20embeddings%20as%20close%20as%20possible%20to%20reduce%20the%20effect%20of%20domain%20shift.%0AExperimental%20evaluations%20on%20open-access%20adult%20and%20pediatric%20CXR%20datasets%20show%0Athat%20the%20proposed%20method%20achieves%20a%20superior%20AUROC%20score%20of%200.8464%20compared%20to%0A0.8348%20obtained%20using%20the%20conventional%20approach%20of%20join%20training%20on%20both%0Adatasets.%20The%20proposed%20approach%20thus%20paves%20the%20way%20for%20generalized%20CAD%20models%0Athat%20are%20effective%20for%20both%20adult%20and%20pediatric%20age%20groups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12958v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Pediatric%20Pneumonia%20Diagnosis%20with%20Adult%20Chest%20X-ray%20Images%0A%20%20Utilizing%20Contrastive%20Learning%20and%20Embedding%20Similarity&entry.906535625=Mohammad%20Zunaed%20and%20Anwarul%20Hasan%20and%20Taufiq%20Hasan&entry.1292438233=%20%20Despite%20the%20advancement%20of%20deep%20learning-based%20computer-aided%20diagnosis%20%28CAD%29%0Amethods%20for%20pneumonia%20from%20adult%20chest%20x-ray%20%28CXR%29%20images%2C%20the%20performance%20of%0ACAD%20methods%20applied%20to%20pediatric%20images%20remains%20suboptimal%2C%20mainly%20due%20to%20the%0Alack%20of%20large-scale%20annotated%20pediatric%20imaging%20datasets.%20Establishing%20a%20proper%0Aframework%20to%20leverage%20existing%20adult%20large-scale%20CXR%20datasets%20can%20thus%20enhance%0Apediatric%20pneumonia%20detection%20performance.%20In%20this%20paper%2C%20we%20propose%20a%0Athree-branch%20parallel%20path%20learning-based%20framework%20that%20utilizes%20both%20adult%0Aand%20pediatric%20datasets%20to%20improve%20the%20performance%20of%20deep%20learning%20models%20on%0Apediatric%20test%20datasets.%20The%20paths%20are%20trained%20with%20pediatric%20only%2C%20adult%20only%2C%0Aand%20both%20types%20of%20CXRs%2C%20respectively.%20Our%20proposed%20framework%20utilizes%20the%0Amulti-positive%20contrastive%20loss%20to%20cluster%20the%20classwise%20embeddings%20and%20the%0Aembedding%20similarity%20loss%20among%20these%20three%20parallel%20paths%20to%20make%20the%0Aclasswise%20embeddings%20as%20close%20as%20possible%20to%20reduce%20the%20effect%20of%20domain%20shift.%0AExperimental%20evaluations%20on%20open-access%20adult%20and%20pediatric%20CXR%20datasets%20show%0Athat%20the%20proposed%20method%20achieves%20a%20superior%20AUROC%20score%20of%200.8464%20compared%20to%0A0.8348%20obtained%20using%20the%20conventional%20approach%20of%20join%20training%20on%20both%0Adatasets.%20The%20proposed%20approach%20thus%20paves%20the%20way%20for%20generalized%20CAD%20models%0Athat%20are%20effective%20for%20both%20adult%20and%20pediatric%20age%20groups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12958v1&entry.124074799=Read"},
{"title": "BANF: Band-limited Neural Fields for Levels of Detail Reconstruction", "author": "Ahan Shabanov and Shrisudhan Govindarajan and Cody Reading and Lily Goli and Daniel Rebain and Kwang Moo Yi and Andrea Tagliasacchi", "abstract": "  Largely due to their implicit nature, neural fields lack a direct mechanism\nfor filtering, as Fourier analysis from discrete signal processing is not\ndirectly applicable to these representations. Effective filtering of neural\nfields is critical to enable level-of-detail processing in downstream\napplications, and support operations that involve sampling the field on regular\ngrids (e.g. marching cubes). Existing methods that attempt to decompose neural\nfields in the frequency domain either resort to heuristics or require extensive\nmodifications to the neural field architecture. We show that via a simple\nmodification, one can obtain neural fields that are low-pass filtered, and in\nturn show how this can be exploited to obtain a frequency decomposition of the\nentire signal. We demonstrate the validity of our technique by investigating\nlevel-of-detail reconstruction, and showing how coarser representations can be\ncomputed effectively.\n", "link": "http://arxiv.org/abs/2404.13024v1", "date": "2024-04-19", "relevancy": 1.9134, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.508}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4615}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4554}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BANF%3A%20Band-limited%20Neural%20Fields%20for%20Levels%20of%20Detail%20Reconstruction&body=Title%3A%20BANF%3A%20Band-limited%20Neural%20Fields%20for%20Levels%20of%20Detail%20Reconstruction%0AAuthor%3A%20Ahan%20Shabanov%20and%20Shrisudhan%20Govindarajan%20and%20Cody%20Reading%20and%20Lily%20Goli%20and%20Daniel%20Rebain%20and%20Kwang%20Moo%20Yi%20and%20Andrea%20Tagliasacchi%0AAbstract%3A%20%20%20Largely%20due%20to%20their%20implicit%20nature%2C%20neural%20fields%20lack%20a%20direct%20mechanism%0Afor%20filtering%2C%20as%20Fourier%20analysis%20from%20discrete%20signal%20processing%20is%20not%0Adirectly%20applicable%20to%20these%20representations.%20Effective%20filtering%20of%20neural%0Afields%20is%20critical%20to%20enable%20level-of-detail%20processing%20in%20downstream%0Aapplications%2C%20and%20support%20operations%20that%20involve%20sampling%20the%20field%20on%20regular%0Agrids%20%28e.g.%20marching%20cubes%29.%20Existing%20methods%20that%20attempt%20to%20decompose%20neural%0Afields%20in%20the%20frequency%20domain%20either%20resort%20to%20heuristics%20or%20require%20extensive%0Amodifications%20to%20the%20neural%20field%20architecture.%20We%20show%20that%20via%20a%20simple%0Amodification%2C%20one%20can%20obtain%20neural%20fields%20that%20are%20low-pass%20filtered%2C%20and%20in%0Aturn%20show%20how%20this%20can%20be%20exploited%20to%20obtain%20a%20frequency%20decomposition%20of%20the%0Aentire%20signal.%20We%20demonstrate%20the%20validity%20of%20our%20technique%20by%20investigating%0Alevel-of-detail%20reconstruction%2C%20and%20showing%20how%20coarser%20representations%20can%20be%0Acomputed%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13024v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BANF%3A%20Band-limited%20Neural%20Fields%20for%20Levels%20of%20Detail%20Reconstruction&entry.906535625=Ahan%20Shabanov%20and%20Shrisudhan%20Govindarajan%20and%20Cody%20Reading%20and%20Lily%20Goli%20and%20Daniel%20Rebain%20and%20Kwang%20Moo%20Yi%20and%20Andrea%20Tagliasacchi&entry.1292438233=%20%20Largely%20due%20to%20their%20implicit%20nature%2C%20neural%20fields%20lack%20a%20direct%20mechanism%0Afor%20filtering%2C%20as%20Fourier%20analysis%20from%20discrete%20signal%20processing%20is%20not%0Adirectly%20applicable%20to%20these%20representations.%20Effective%20filtering%20of%20neural%0Afields%20is%20critical%20to%20enable%20level-of-detail%20processing%20in%20downstream%0Aapplications%2C%20and%20support%20operations%20that%20involve%20sampling%20the%20field%20on%20regular%0Agrids%20%28e.g.%20marching%20cubes%29.%20Existing%20methods%20that%20attempt%20to%20decompose%20neural%0Afields%20in%20the%20frequency%20domain%20either%20resort%20to%20heuristics%20or%20require%20extensive%0Amodifications%20to%20the%20neural%20field%20architecture.%20We%20show%20that%20via%20a%20simple%0Amodification%2C%20one%20can%20obtain%20neural%20fields%20that%20are%20low-pass%20filtered%2C%20and%20in%0Aturn%20show%20how%20this%20can%20be%20exploited%20to%20obtain%20a%20frequency%20decomposition%20of%20the%0Aentire%20signal.%20We%20demonstrate%20the%20validity%20of%20our%20technique%20by%20investigating%0Alevel-of-detail%20reconstruction%2C%20and%20showing%20how%20coarser%20representations%20can%20be%0Acomputed%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13024v1&entry.124074799=Read"},
{"title": "Event-Based Contrastive Learning for Medical Time Series", "author": "Hyewon Jeong and Nassim Oufattole and Matthew Mcdermott and Aparna Balagopalan and Bryan Jangeesingh and Marzyeh Ghassemi and Collin Stultz", "abstract": "  In clinical practice, one often needs to identify whether a patient is at\nhigh risk of adverse outcomes after some key medical event. For example,\nquantifying the risk of adverse outcomes after an acute cardiovascular event\nhelps healthcare providers identify those patients at the highest risk of poor\noutcomes; i.e., patients who benefit from invasive therapies that can lower\ntheir risk. Assessing the risk of adverse outcomes, however, is challenging due\nto the complexity, variability, and heterogeneity of longitudinal medical data,\nespecially for individuals suffering from chronic diseases like heart failure.\nIn this paper, we introduce Event-Based Contrastive Learning (EBCL) - a method\nfor learning embeddings of heterogeneous patient data that preserves temporal\ninformation before and after key index events. We demonstrate that EBCL can be\nused to construct models that yield improved performance on important\ndownstream tasks relative to other pretraining methods. We develop and test the\nmethod using a cohort of heart failure patients obtained from a large hospital\nnetwork and the publicly available MIMIC-IV dataset consisting of patients in\nan intensive care unit at a large tertiary care center. On both cohorts, EBCL\npretraining yields models that are performant with respect to a number of\ndownstream tasks, including mortality, hospital readmission, and length of\nstay. In addition, unsupervised EBCL embeddings effectively cluster heart\nfailure patients into subgroups with distinct outcomes, thereby providing\ninformation that helps identify new heart failure phenotypes. The contrastive\nframework around the index event can be adapted to a wide array of time-series\ndatasets and provides information that can be used to guide personalized care.\n", "link": "http://arxiv.org/abs/2312.10308v3", "date": "2024-04-19", "relevancy": 1.8949, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4935}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4616}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Event-Based%20Contrastive%20Learning%20for%20Medical%20Time%20Series&body=Title%3A%20Event-Based%20Contrastive%20Learning%20for%20Medical%20Time%20Series%0AAuthor%3A%20Hyewon%20Jeong%20and%20Nassim%20Oufattole%20and%20Matthew%20Mcdermott%20and%20Aparna%20Balagopalan%20and%20Bryan%20Jangeesingh%20and%20Marzyeh%20Ghassemi%20and%20Collin%20Stultz%0AAbstract%3A%20%20%20In%20clinical%20practice%2C%20one%20often%20needs%20to%20identify%20whether%20a%20patient%20is%20at%0Ahigh%20risk%20of%20adverse%20outcomes%20after%20some%20key%20medical%20event.%20For%20example%2C%0Aquantifying%20the%20risk%20of%20adverse%20outcomes%20after%20an%20acute%20cardiovascular%20event%0Ahelps%20healthcare%20providers%20identify%20those%20patients%20at%20the%20highest%20risk%20of%20poor%0Aoutcomes%3B%20i.e.%2C%20patients%20who%20benefit%20from%20invasive%20therapies%20that%20can%20lower%0Atheir%20risk.%20Assessing%20the%20risk%20of%20adverse%20outcomes%2C%20however%2C%20is%20challenging%20due%0Ato%20the%20complexity%2C%20variability%2C%20and%20heterogeneity%20of%20longitudinal%20medical%20data%2C%0Aespecially%20for%20individuals%20suffering%20from%20chronic%20diseases%20like%20heart%20failure.%0AIn%20this%20paper%2C%20we%20introduce%20Event-Based%20Contrastive%20Learning%20%28EBCL%29%20-%20a%20method%0Afor%20learning%20embeddings%20of%20heterogeneous%20patient%20data%20that%20preserves%20temporal%0Ainformation%20before%20and%20after%20key%20index%20events.%20We%20demonstrate%20that%20EBCL%20can%20be%0Aused%20to%20construct%20models%20that%20yield%20improved%20performance%20on%20important%0Adownstream%20tasks%20relative%20to%20other%20pretraining%20methods.%20We%20develop%20and%20test%20the%0Amethod%20using%20a%20cohort%20of%20heart%20failure%20patients%20obtained%20from%20a%20large%20hospital%0Anetwork%20and%20the%20publicly%20available%20MIMIC-IV%20dataset%20consisting%20of%20patients%20in%0Aan%20intensive%20care%20unit%20at%20a%20large%20tertiary%20care%20center.%20On%20both%20cohorts%2C%20EBCL%0Apretraining%20yields%20models%20that%20are%20performant%20with%20respect%20to%20a%20number%20of%0Adownstream%20tasks%2C%20including%20mortality%2C%20hospital%20readmission%2C%20and%20length%20of%0Astay.%20In%20addition%2C%20unsupervised%20EBCL%20embeddings%20effectively%20cluster%20heart%0Afailure%20patients%20into%20subgroups%20with%20distinct%20outcomes%2C%20thereby%20providing%0Ainformation%20that%20helps%20identify%20new%20heart%20failure%20phenotypes.%20The%20contrastive%0Aframework%20around%20the%20index%20event%20can%20be%20adapted%20to%20a%20wide%20array%20of%20time-series%0Adatasets%20and%20provides%20information%20that%20can%20be%20used%20to%20guide%20personalized%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10308v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Based%20Contrastive%20Learning%20for%20Medical%20Time%20Series&entry.906535625=Hyewon%20Jeong%20and%20Nassim%20Oufattole%20and%20Matthew%20Mcdermott%20and%20Aparna%20Balagopalan%20and%20Bryan%20Jangeesingh%20and%20Marzyeh%20Ghassemi%20and%20Collin%20Stultz&entry.1292438233=%20%20In%20clinical%20practice%2C%20one%20often%20needs%20to%20identify%20whether%20a%20patient%20is%20at%0Ahigh%20risk%20of%20adverse%20outcomes%20after%20some%20key%20medical%20event.%20For%20example%2C%0Aquantifying%20the%20risk%20of%20adverse%20outcomes%20after%20an%20acute%20cardiovascular%20event%0Ahelps%20healthcare%20providers%20identify%20those%20patients%20at%20the%20highest%20risk%20of%20poor%0Aoutcomes%3B%20i.e.%2C%20patients%20who%20benefit%20from%20invasive%20therapies%20that%20can%20lower%0Atheir%20risk.%20Assessing%20the%20risk%20of%20adverse%20outcomes%2C%20however%2C%20is%20challenging%20due%0Ato%20the%20complexity%2C%20variability%2C%20and%20heterogeneity%20of%20longitudinal%20medical%20data%2C%0Aespecially%20for%20individuals%20suffering%20from%20chronic%20diseases%20like%20heart%20failure.%0AIn%20this%20paper%2C%20we%20introduce%20Event-Based%20Contrastive%20Learning%20%28EBCL%29%20-%20a%20method%0Afor%20learning%20embeddings%20of%20heterogeneous%20patient%20data%20that%20preserves%20temporal%0Ainformation%20before%20and%20after%20key%20index%20events.%20We%20demonstrate%20that%20EBCL%20can%20be%0Aused%20to%20construct%20models%20that%20yield%20improved%20performance%20on%20important%0Adownstream%20tasks%20relative%20to%20other%20pretraining%20methods.%20We%20develop%20and%20test%20the%0Amethod%20using%20a%20cohort%20of%20heart%20failure%20patients%20obtained%20from%20a%20large%20hospital%0Anetwork%20and%20the%20publicly%20available%20MIMIC-IV%20dataset%20consisting%20of%20patients%20in%0Aan%20intensive%20care%20unit%20at%20a%20large%20tertiary%20care%20center.%20On%20both%20cohorts%2C%20EBCL%0Apretraining%20yields%20models%20that%20are%20performant%20with%20respect%20to%20a%20number%20of%0Adownstream%20tasks%2C%20including%20mortality%2C%20hospital%20readmission%2C%20and%20length%20of%0Astay.%20In%20addition%2C%20unsupervised%20EBCL%20embeddings%20effectively%20cluster%20heart%0Afailure%20patients%20into%20subgroups%20with%20distinct%20outcomes%2C%20thereby%20providing%0Ainformation%20that%20helps%20identify%20new%20heart%20failure%20phenotypes.%20The%20contrastive%0Aframework%20around%20the%20index%20event%20can%20be%20adapted%20to%20a%20wide%20array%20of%20time-series%0Adatasets%20and%20provides%20information%20that%20can%20be%20used%20to%20guide%20personalized%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10308v3&entry.124074799=Read"},
{"title": "Probabilistic-Numeric SMC Sampling for Bayesian Nonlinear System\n  Identification in Continuous Time", "author": "Joe D. Longbottom and Max D. Champneys and Timothy J. Rogers", "abstract": "  In engineering, accurately modeling nonlinear dynamic systems from data\ncontaminated by noise is both essential and complex. Established Sequential\nMonte Carlo (SMC) methods, used for the Bayesian identification of these\nsystems, facilitate the quantification of uncertainty in the parameter\nidentification process. A significant challenge in this context is the\nnumerical integration of continuous-time ordinary differential equations\n(ODEs), crucial for aligning theoretical models with discretely sampled data.\nThis integration introduces additional numerical uncertainty, a factor that is\noften over looked. To address this issue, the field of probabilistic numerics\ncombines numerical methods, such as numerical integration, with probabilistic\nmodeling to offer a more comprehensive analysis of total uncertainty. By\nretaining the accuracy of classical deterministic methods, these probabilistic\napproaches offer a deeper understanding of the uncertainty inherent in the\ninference process. This paper demonstrates the application of a probabilistic\nnumerical method for solving ODEs in the joint parameter-state identification\nof nonlinear dynamic systems. The presented approach efficiently identifies\nlatent states and system parameters from noisy measurements. Simultaneously\nincorporating probabilistic solutions to the ODE in the identification\nchallenge. The methodology's primary advantage lies in its capability to\nproduce posterior distributions over system parameters, thereby representing\nthe inherent uncertainties in both the data and the identification process.\n", "link": "http://arxiv.org/abs/2404.12923v1", "date": "2024-04-19", "relevancy": 1.8827, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.524}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4642}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4558}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probabilistic-Numeric%20SMC%20Sampling%20for%20Bayesian%20Nonlinear%20System%0A%20%20Identification%20in%20Continuous%20Time&body=Title%3A%20Probabilistic-Numeric%20SMC%20Sampling%20for%20Bayesian%20Nonlinear%20System%0A%20%20Identification%20in%20Continuous%20Time%0AAuthor%3A%20Joe%20D.%20Longbottom%20and%20Max%20D.%20Champneys%20and%20Timothy%20J.%20Rogers%0AAbstract%3A%20%20%20In%20engineering%2C%20accurately%20modeling%20nonlinear%20dynamic%20systems%20from%20data%0Acontaminated%20by%20noise%20is%20both%20essential%20and%20complex.%20Established%20Sequential%0AMonte%20Carlo%20%28SMC%29%20methods%2C%20used%20for%20the%20Bayesian%20identification%20of%20these%0Asystems%2C%20facilitate%20the%20quantification%20of%20uncertainty%20in%20the%20parameter%0Aidentification%20process.%20A%20significant%20challenge%20in%20this%20context%20is%20the%0Anumerical%20integration%20of%20continuous-time%20ordinary%20differential%20equations%0A%28ODEs%29%2C%20crucial%20for%20aligning%20theoretical%20models%20with%20discretely%20sampled%20data.%0AThis%20integration%20introduces%20additional%20numerical%20uncertainty%2C%20a%20factor%20that%20is%0Aoften%20over%20looked.%20To%20address%20this%20issue%2C%20the%20field%20of%20probabilistic%20numerics%0Acombines%20numerical%20methods%2C%20such%20as%20numerical%20integration%2C%20with%20probabilistic%0Amodeling%20to%20offer%20a%20more%20comprehensive%20analysis%20of%20total%20uncertainty.%20By%0Aretaining%20the%20accuracy%20of%20classical%20deterministic%20methods%2C%20these%20probabilistic%0Aapproaches%20offer%20a%20deeper%20understanding%20of%20the%20uncertainty%20inherent%20in%20the%0Ainference%20process.%20This%20paper%20demonstrates%20the%20application%20of%20a%20probabilistic%0Anumerical%20method%20for%20solving%20ODEs%20in%20the%20joint%20parameter-state%20identification%0Aof%20nonlinear%20dynamic%20systems.%20The%20presented%20approach%20efficiently%20identifies%0Alatent%20states%20and%20system%20parameters%20from%20noisy%20measurements.%20Simultaneously%0Aincorporating%20probabilistic%20solutions%20to%20the%20ODE%20in%20the%20identification%0Achallenge.%20The%20methodology%27s%20primary%20advantage%20lies%20in%20its%20capability%20to%0Aproduce%20posterior%20distributions%20over%20system%20parameters%2C%20thereby%20representing%0Athe%20inherent%20uncertainties%20in%20both%20the%20data%20and%20the%20identification%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12923v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic-Numeric%20SMC%20Sampling%20for%20Bayesian%20Nonlinear%20System%0A%20%20Identification%20in%20Continuous%20Time&entry.906535625=Joe%20D.%20Longbottom%20and%20Max%20D.%20Champneys%20and%20Timothy%20J.%20Rogers&entry.1292438233=%20%20In%20engineering%2C%20accurately%20modeling%20nonlinear%20dynamic%20systems%20from%20data%0Acontaminated%20by%20noise%20is%20both%20essential%20and%20complex.%20Established%20Sequential%0AMonte%20Carlo%20%28SMC%29%20methods%2C%20used%20for%20the%20Bayesian%20identification%20of%20these%0Asystems%2C%20facilitate%20the%20quantification%20of%20uncertainty%20in%20the%20parameter%0Aidentification%20process.%20A%20significant%20challenge%20in%20this%20context%20is%20the%0Anumerical%20integration%20of%20continuous-time%20ordinary%20differential%20equations%0A%28ODEs%29%2C%20crucial%20for%20aligning%20theoretical%20models%20with%20discretely%20sampled%20data.%0AThis%20integration%20introduces%20additional%20numerical%20uncertainty%2C%20a%20factor%20that%20is%0Aoften%20over%20looked.%20To%20address%20this%20issue%2C%20the%20field%20of%20probabilistic%20numerics%0Acombines%20numerical%20methods%2C%20such%20as%20numerical%20integration%2C%20with%20probabilistic%0Amodeling%20to%20offer%20a%20more%20comprehensive%20analysis%20of%20total%20uncertainty.%20By%0Aretaining%20the%20accuracy%20of%20classical%20deterministic%20methods%2C%20these%20probabilistic%0Aapproaches%20offer%20a%20deeper%20understanding%20of%20the%20uncertainty%20inherent%20in%20the%0Ainference%20process.%20This%20paper%20demonstrates%20the%20application%20of%20a%20probabilistic%0Anumerical%20method%20for%20solving%20ODEs%20in%20the%20joint%20parameter-state%20identification%0Aof%20nonlinear%20dynamic%20systems.%20The%20presented%20approach%20efficiently%20identifies%0Alatent%20states%20and%20system%20parameters%20from%20noisy%20measurements.%20Simultaneously%0Aincorporating%20probabilistic%20solutions%20to%20the%20ODE%20in%20the%20identification%0Achallenge.%20The%20methodology%27s%20primary%20advantage%20lies%20in%20its%20capability%20to%0Aproduce%20posterior%20distributions%20over%20system%20parameters%2C%20thereby%20representing%0Athe%20inherent%20uncertainties%20in%20both%20the%20data%20and%20the%20identification%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12923v1&entry.124074799=Read"},
{"title": "What Generative Artificial Intelligence Means for Terminological\n  Definitions", "author": "Antonio San Mart\u00edn", "abstract": "  This paper examines the impact of Generative Artificial Intelligence (GenAI)\ntools like ChatGPT on the creation and consumption of terminological\ndefinitions. From the terminologist's point of view, the strategic use of GenAI\ntools can streamline the process of crafting definitions, reducing both time\nand effort, while potentially enhancing quality. GenAI tools enable AI-assisted\nterminography, notably post-editing terminography, where the machine produces a\ndefinition that the terminologist then corrects or refines. However, the\npotential of GenAI tools to fulfill all the terminological needs of a user,\nincluding term definitions, challenges the very existence of terminological\ndefinitions and resources as we know them. Unlike terminological definitions,\nGenAI tools can describe the knowledge activated by a term in a specific\ncontext. However, a main drawback of these tools is that their output can\ncontain errors. For this reason, users requiring reliability will likely still\nresort to terminological resources for definitions. Nevertheless, with the\ninevitable integration of AI into terminology work, the distinction between\nhuman-created and AI-created content will become increasingly blurred.\n", "link": "http://arxiv.org/abs/2402.16139v3", "date": "2024-04-19", "relevancy": 1.8659, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5347}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4244}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.401}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20What%20Generative%20Artificial%20Intelligence%20Means%20for%20Terminological%0A%20%20Definitions&body=Title%3A%20What%20Generative%20Artificial%20Intelligence%20Means%20for%20Terminological%0A%20%20Definitions%0AAuthor%3A%20Antonio%20San%20Mart%C3%ADn%0AAbstract%3A%20%20%20This%20paper%20examines%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%0Atools%20like%20ChatGPT%20on%20the%20creation%20and%20consumption%20of%20terminological%0Adefinitions.%20From%20the%20terminologist%27s%20point%20of%20view%2C%20the%20strategic%20use%20of%20GenAI%0Atools%20can%20streamline%20the%20process%20of%20crafting%20definitions%2C%20reducing%20both%20time%0Aand%20effort%2C%20while%20potentially%20enhancing%20quality.%20GenAI%20tools%20enable%20AI-assisted%0Aterminography%2C%20notably%20post-editing%20terminography%2C%20where%20the%20machine%20produces%20a%0Adefinition%20that%20the%20terminologist%20then%20corrects%20or%20refines.%20However%2C%20the%0Apotential%20of%20GenAI%20tools%20to%20fulfill%20all%20the%20terminological%20needs%20of%20a%20user%2C%0Aincluding%20term%20definitions%2C%20challenges%20the%20very%20existence%20of%20terminological%0Adefinitions%20and%20resources%20as%20we%20know%20them.%20Unlike%20terminological%20definitions%2C%0AGenAI%20tools%20can%20describe%20the%20knowledge%20activated%20by%20a%20term%20in%20a%20specific%0Acontext.%20However%2C%20a%20main%20drawback%20of%20these%20tools%20is%20that%20their%20output%20can%0Acontain%20errors.%20For%20this%20reason%2C%20users%20requiring%20reliability%20will%20likely%20still%0Aresort%20to%20terminological%20resources%20for%20definitions.%20Nevertheless%2C%20with%20the%0Ainevitable%20integration%20of%20AI%20into%20terminology%20work%2C%20the%20distinction%20between%0Ahuman-created%20and%20AI-created%20content%20will%20become%20increasingly%20blurred.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16139v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Generative%20Artificial%20Intelligence%20Means%20for%20Terminological%0A%20%20Definitions&entry.906535625=Antonio%20San%20Mart%C3%ADn&entry.1292438233=%20%20This%20paper%20examines%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%0Atools%20like%20ChatGPT%20on%20the%20creation%20and%20consumption%20of%20terminological%0Adefinitions.%20From%20the%20terminologist%27s%20point%20of%20view%2C%20the%20strategic%20use%20of%20GenAI%0Atools%20can%20streamline%20the%20process%20of%20crafting%20definitions%2C%20reducing%20both%20time%0Aand%20effort%2C%20while%20potentially%20enhancing%20quality.%20GenAI%20tools%20enable%20AI-assisted%0Aterminography%2C%20notably%20post-editing%20terminography%2C%20where%20the%20machine%20produces%20a%0Adefinition%20that%20the%20terminologist%20then%20corrects%20or%20refines.%20However%2C%20the%0Apotential%20of%20GenAI%20tools%20to%20fulfill%20all%20the%20terminological%20needs%20of%20a%20user%2C%0Aincluding%20term%20definitions%2C%20challenges%20the%20very%20existence%20of%20terminological%0Adefinitions%20and%20resources%20as%20we%20know%20them.%20Unlike%20terminological%20definitions%2C%0AGenAI%20tools%20can%20describe%20the%20knowledge%20activated%20by%20a%20term%20in%20a%20specific%0Acontext.%20However%2C%20a%20main%20drawback%20of%20these%20tools%20is%20that%20their%20output%20can%0Acontain%20errors.%20For%20this%20reason%2C%20users%20requiring%20reliability%20will%20likely%20still%0Aresort%20to%20terminological%20resources%20for%20definitions.%20Nevertheless%2C%20with%20the%0Ainevitable%20integration%20of%20AI%20into%20terminology%20work%2C%20the%20distinction%20between%0Ahuman-created%20and%20AI-created%20content%20will%20become%20increasingly%20blurred.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16139v3&entry.124074799=Read"},
{"title": "Cross-cultural Inspiration Detection and Analysis in Real and\n  LLM-generated Social Media Data", "author": "Oana Ignat and Gayathri Ganesh Lakshmy and Rada Mihalcea", "abstract": "  Inspiration is linked to various positive outcomes, such as increased\ncreativity, productivity, and happiness. Although inspiration has great\npotential, there has been limited effort toward identifying content that is\ninspiring, as opposed to just engaging or positive. Additionally, most research\nhas concentrated on Western data, with little attention paid to other cultures.\nThis work is the first to study cross-cultural inspiration through machine\nlearning methods. We aim to identify and analyze real and AI-generated\ncross-cultural inspiring posts. To this end, we compile and make publicly\navailable the InspAIred dataset, which consists of 2,000 real inspiring posts,\n2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly\ndistributed across India and the UK. The real posts are sourced from Reddit,\nwhile the generated posts are created using the GPT-4 model. Using this\ndataset, we conduct extensive computational linguistic analyses to (1) compare\ninspiring content across cultures, (2) compare AI-generated inspiring posts to\nreal inspiring posts, and (3) determine if detection models can accurately\ndistinguish between inspiring content across cultures and data sources.\n", "link": "http://arxiv.org/abs/2404.12933v1", "date": "2024-04-19", "relevancy": 1.8628, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4765}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4657}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4613}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-cultural%20Inspiration%20Detection%20and%20Analysis%20in%20Real%20and%0A%20%20LLM-generated%20Social%20Media%20Data&body=Title%3A%20Cross-cultural%20Inspiration%20Detection%20and%20Analysis%20in%20Real%20and%0A%20%20LLM-generated%20Social%20Media%20Data%0AAuthor%3A%20Oana%20Ignat%20and%20Gayathri%20Ganesh%20Lakshmy%20and%20Rada%20Mihalcea%0AAbstract%3A%20%20%20Inspiration%20is%20linked%20to%20various%20positive%20outcomes%2C%20such%20as%20increased%0Acreativity%2C%20productivity%2C%20and%20happiness.%20Although%20inspiration%20has%20great%0Apotential%2C%20there%20has%20been%20limited%20effort%20toward%20identifying%20content%20that%20is%0Ainspiring%2C%20as%20opposed%20to%20just%20engaging%20or%20positive.%20Additionally%2C%20most%20research%0Ahas%20concentrated%20on%20Western%20data%2C%20with%20little%20attention%20paid%20to%20other%20cultures.%0AThis%20work%20is%20the%20first%20to%20study%20cross-cultural%20inspiration%20through%20machine%0Alearning%20methods.%20We%20aim%20to%20identify%20and%20analyze%20real%20and%20AI-generated%0Across-cultural%20inspiring%20posts.%20To%20this%20end%2C%20we%20compile%20and%20make%20publicly%0Aavailable%20the%20InspAIred%20dataset%2C%20which%20consists%20of%202%2C000%20real%20inspiring%20posts%2C%0A2%2C000%20real%20non-inspiring%20posts%2C%20and%202%2C000%20generated%20inspiring%20posts%20evenly%0Adistributed%20across%20India%20and%20the%20UK.%20The%20real%20posts%20are%20sourced%20from%20Reddit%2C%0Awhile%20the%20generated%20posts%20are%20created%20using%20the%20GPT-4%20model.%20Using%20this%0Adataset%2C%20we%20conduct%20extensive%20computational%20linguistic%20analyses%20to%20%281%29%20compare%0Ainspiring%20content%20across%20cultures%2C%20%282%29%20compare%20AI-generated%20inspiring%20posts%20to%0Areal%20inspiring%20posts%2C%20and%20%283%29%20determine%20if%20detection%20models%20can%20accurately%0Adistinguish%20between%20inspiring%20content%20across%20cultures%20and%20data%20sources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12933v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-cultural%20Inspiration%20Detection%20and%20Analysis%20in%20Real%20and%0A%20%20LLM-generated%20Social%20Media%20Data&entry.906535625=Oana%20Ignat%20and%20Gayathri%20Ganesh%20Lakshmy%20and%20Rada%20Mihalcea&entry.1292438233=%20%20Inspiration%20is%20linked%20to%20various%20positive%20outcomes%2C%20such%20as%20increased%0Acreativity%2C%20productivity%2C%20and%20happiness.%20Although%20inspiration%20has%20great%0Apotential%2C%20there%20has%20been%20limited%20effort%20toward%20identifying%20content%20that%20is%0Ainspiring%2C%20as%20opposed%20to%20just%20engaging%20or%20positive.%20Additionally%2C%20most%20research%0Ahas%20concentrated%20on%20Western%20data%2C%20with%20little%20attention%20paid%20to%20other%20cultures.%0AThis%20work%20is%20the%20first%20to%20study%20cross-cultural%20inspiration%20through%20machine%0Alearning%20methods.%20We%20aim%20to%20identify%20and%20analyze%20real%20and%20AI-generated%0Across-cultural%20inspiring%20posts.%20To%20this%20end%2C%20we%20compile%20and%20make%20publicly%0Aavailable%20the%20InspAIred%20dataset%2C%20which%20consists%20of%202%2C000%20real%20inspiring%20posts%2C%0A2%2C000%20real%20non-inspiring%20posts%2C%20and%202%2C000%20generated%20inspiring%20posts%20evenly%0Adistributed%20across%20India%20and%20the%20UK.%20The%20real%20posts%20are%20sourced%20from%20Reddit%2C%0Awhile%20the%20generated%20posts%20are%20created%20using%20the%20GPT-4%20model.%20Using%20this%0Adataset%2C%20we%20conduct%20extensive%20computational%20linguistic%20analyses%20to%20%281%29%20compare%0Ainspiring%20content%20across%20cultures%2C%20%282%29%20compare%20AI-generated%20inspiring%20posts%20to%0Areal%20inspiring%20posts%2C%20and%20%283%29%20determine%20if%20detection%20models%20can%20accurately%0Adistinguish%20between%20inspiring%20content%20across%20cultures%20and%20data%20sources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12933v1&entry.124074799=Read"},
{"title": "Large Language Models for Networking: Workflow, Advances and Challenges", "author": "Chang Liu and Xiaohui Xie and Xinggong Zhang and Yong Cui", "abstract": "  The networking field is characterized by its high complexity and rapid\niteration, requiring extensive expertise to accomplish network tasks, ranging\nfrom network design, diagnosis, configuration and security. The inherent\ncomplexity of these tasks, coupled with the ever-changing landscape of\nnetworking technologies and protocols, poses significant hurdles for\ntraditional machine learning-based methods. These methods often struggle to\ngeneralize and automate complex tasks in networking, as they require extensive\nlabeled data, domain-specific feature engineering, and frequent retraining to\nadapt to new scenarios. However, the recent emergence of large language models\n(LLMs) has sparked a new wave of possibilities in addressing these challenges.\nLLMs have demonstrated remarkable capabilities in natural language\nunderstanding, generation, and reasoning. These models, trained on extensive\ndata, can benefit the networking domain. Some efforts have already explored the\napplication of LLMs in the networking domain and revealed promising results. By\nreviewing recent advances, we present an abstract workflow to describe the\nfundamental process involved in applying LLM for Networking. We introduce the\nhighlights of existing works by category and explain in detail how they operate\nat different stages of the workflow. Furthermore, we delve into the challenges\nencountered, discuss potential solutions, and outline future research\nprospects. We hope that this survey will provide insight for researchers and\npractitioners, promoting the development of this interdisciplinary research\nfield.\n", "link": "http://arxiv.org/abs/2404.12901v1", "date": "2024-04-19", "relevancy": 1.8493, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4719}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4561}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.454}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Networking%3A%20Workflow%2C%20Advances%20and%20Challenges&body=Title%3A%20Large%20Language%20Models%20for%20Networking%3A%20Workflow%2C%20Advances%20and%20Challenges%0AAuthor%3A%20Chang%20Liu%20and%20Xiaohui%20Xie%20and%20Xinggong%20Zhang%20and%20Yong%20Cui%0AAbstract%3A%20%20%20The%20networking%20field%20is%20characterized%20by%20its%20high%20complexity%20and%20rapid%0Aiteration%2C%20requiring%20extensive%20expertise%20to%20accomplish%20network%20tasks%2C%20ranging%0Afrom%20network%20design%2C%20diagnosis%2C%20configuration%20and%20security.%20The%20inherent%0Acomplexity%20of%20these%20tasks%2C%20coupled%20with%20the%20ever-changing%20landscape%20of%0Anetworking%20technologies%20and%20protocols%2C%20poses%20significant%20hurdles%20for%0Atraditional%20machine%20learning-based%20methods.%20These%20methods%20often%20struggle%20to%0Ageneralize%20and%20automate%20complex%20tasks%20in%20networking%2C%20as%20they%20require%20extensive%0Alabeled%20data%2C%20domain-specific%20feature%20engineering%2C%20and%20frequent%20retraining%20to%0Aadapt%20to%20new%20scenarios.%20However%2C%20the%20recent%20emergence%20of%20large%20language%20models%0A%28LLMs%29%20has%20sparked%20a%20new%20wave%20of%20possibilities%20in%20addressing%20these%20challenges.%0ALLMs%20have%20demonstrated%20remarkable%20capabilities%20in%20natural%20language%0Aunderstanding%2C%20generation%2C%20and%20reasoning.%20These%20models%2C%20trained%20on%20extensive%0Adata%2C%20can%20benefit%20the%20networking%20domain.%20Some%20efforts%20have%20already%20explored%20the%0Aapplication%20of%20LLMs%20in%20the%20networking%20domain%20and%20revealed%20promising%20results.%20By%0Areviewing%20recent%20advances%2C%20we%20present%20an%20abstract%20workflow%20to%20describe%20the%0Afundamental%20process%20involved%20in%20applying%20LLM%20for%20Networking.%20We%20introduce%20the%0Ahighlights%20of%20existing%20works%20by%20category%20and%20explain%20in%20detail%20how%20they%20operate%0Aat%20different%20stages%20of%20the%20workflow.%20Furthermore%2C%20we%20delve%20into%20the%20challenges%0Aencountered%2C%20discuss%20potential%20solutions%2C%20and%20outline%20future%20research%0Aprospects.%20We%20hope%20that%20this%20survey%20will%20provide%20insight%20for%20researchers%20and%0Apractitioners%2C%20promoting%20the%20development%20of%20this%20interdisciplinary%20research%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12901v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Networking%3A%20Workflow%2C%20Advances%20and%20Challenges&entry.906535625=Chang%20Liu%20and%20Xiaohui%20Xie%20and%20Xinggong%20Zhang%20and%20Yong%20Cui&entry.1292438233=%20%20The%20networking%20field%20is%20characterized%20by%20its%20high%20complexity%20and%20rapid%0Aiteration%2C%20requiring%20extensive%20expertise%20to%20accomplish%20network%20tasks%2C%20ranging%0Afrom%20network%20design%2C%20diagnosis%2C%20configuration%20and%20security.%20The%20inherent%0Acomplexity%20of%20these%20tasks%2C%20coupled%20with%20the%20ever-changing%20landscape%20of%0Anetworking%20technologies%20and%20protocols%2C%20poses%20significant%20hurdles%20for%0Atraditional%20machine%20learning-based%20methods.%20These%20methods%20often%20struggle%20to%0Ageneralize%20and%20automate%20complex%20tasks%20in%20networking%2C%20as%20they%20require%20extensive%0Alabeled%20data%2C%20domain-specific%20feature%20engineering%2C%20and%20frequent%20retraining%20to%0Aadapt%20to%20new%20scenarios.%20However%2C%20the%20recent%20emergence%20of%20large%20language%20models%0A%28LLMs%29%20has%20sparked%20a%20new%20wave%20of%20possibilities%20in%20addressing%20these%20challenges.%0ALLMs%20have%20demonstrated%20remarkable%20capabilities%20in%20natural%20language%0Aunderstanding%2C%20generation%2C%20and%20reasoning.%20These%20models%2C%20trained%20on%20extensive%0Adata%2C%20can%20benefit%20the%20networking%20domain.%20Some%20efforts%20have%20already%20explored%20the%0Aapplication%20of%20LLMs%20in%20the%20networking%20domain%20and%20revealed%20promising%20results.%20By%0Areviewing%20recent%20advances%2C%20we%20present%20an%20abstract%20workflow%20to%20describe%20the%0Afundamental%20process%20involved%20in%20applying%20LLM%20for%20Networking.%20We%20introduce%20the%0Ahighlights%20of%20existing%20works%20by%20category%20and%20explain%20in%20detail%20how%20they%20operate%0Aat%20different%20stages%20of%20the%20workflow.%20Furthermore%2C%20we%20delve%20into%20the%20challenges%0Aencountered%2C%20discuss%20potential%20solutions%2C%20and%20outline%20future%20research%0Aprospects.%20We%20hope%20that%20this%20survey%20will%20provide%20insight%20for%20researchers%20and%0Apractitioners%2C%20promoting%20the%20development%20of%20this%20interdisciplinary%20research%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12901v1&entry.124074799=Read"},
{"title": "LSP Framework: A Compensatory Model for Defeating Trigger Reverse\n  Engineering via Label Smoothing Poisoning", "author": "Beichen Li and Yuanfang Guo and Heqi Peng and Yangxi Li and Yunhong Wang", "abstract": "  Deep neural networks are vulnerable to backdoor attacks. Among the existing\nbackdoor defense methods, trigger reverse engineering based approaches, which\nreconstruct the backdoor triggers via optimizations, are the most versatile and\neffective ones compared to other types of methods. In this paper, we summarize\nand construct a generic paradigm for the typical trigger reverse engineering\nprocess. Based on this paradigm, we propose a new perspective to defeat trigger\nreverse engineering by manipulating the classification confidence of backdoor\nsamples. To determine the specific modifications of classification confidence,\nwe propose a compensatory model to compute the lower bound of the modification.\nWith proper modifications, the backdoor attack can easily bypass the trigger\nreverse engineering based methods. To achieve this objective, we propose a\nLabel Smoothing Poisoning (LSP) framework, which leverages label smoothing to\nspecifically manipulate the classification confidences of backdoor samples.\nExtensive experiments demonstrate that the proposed work can defeat the\nstate-of-the-art trigger reverse engineering based methods, and possess good\ncompatibility with a variety of existing backdoor attacks.\n", "link": "http://arxiv.org/abs/2404.12852v1", "date": "2024-04-19", "relevancy": 1.8422, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4703}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.457}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4522}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LSP%20Framework%3A%20A%20Compensatory%20Model%20for%20Defeating%20Trigger%20Reverse%0A%20%20Engineering%20via%20Label%20Smoothing%20Poisoning&body=Title%3A%20LSP%20Framework%3A%20A%20Compensatory%20Model%20for%20Defeating%20Trigger%20Reverse%0A%20%20Engineering%20via%20Label%20Smoothing%20Poisoning%0AAuthor%3A%20Beichen%20Li%20and%20Yuanfang%20Guo%20and%20Heqi%20Peng%20and%20Yangxi%20Li%20and%20Yunhong%20Wang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20vulnerable%20to%20backdoor%20attacks.%20Among%20the%20existing%0Abackdoor%20defense%20methods%2C%20trigger%20reverse%20engineering%20based%20approaches%2C%20which%0Areconstruct%20the%20backdoor%20triggers%20via%20optimizations%2C%20are%20the%20most%20versatile%20and%0Aeffective%20ones%20compared%20to%20other%20types%20of%20methods.%20In%20this%20paper%2C%20we%20summarize%0Aand%20construct%20a%20generic%20paradigm%20for%20the%20typical%20trigger%20reverse%20engineering%0Aprocess.%20Based%20on%20this%20paradigm%2C%20we%20propose%20a%20new%20perspective%20to%20defeat%20trigger%0Areverse%20engineering%20by%20manipulating%20the%20classification%20confidence%20of%20backdoor%0Asamples.%20To%20determine%20the%20specific%20modifications%20of%20classification%20confidence%2C%0Awe%20propose%20a%20compensatory%20model%20to%20compute%20the%20lower%20bound%20of%20the%20modification.%0AWith%20proper%20modifications%2C%20the%20backdoor%20attack%20can%20easily%20bypass%20the%20trigger%0Areverse%20engineering%20based%20methods.%20To%20achieve%20this%20objective%2C%20we%20propose%20a%0ALabel%20Smoothing%20Poisoning%20%28LSP%29%20framework%2C%20which%20leverages%20label%20smoothing%20to%0Aspecifically%20manipulate%20the%20classification%20confidences%20of%20backdoor%20samples.%0AExtensive%20experiments%20demonstrate%20that%20the%20proposed%20work%20can%20defeat%20the%0Astate-of-the-art%20trigger%20reverse%20engineering%20based%20methods%2C%20and%20possess%20good%0Acompatibility%20with%20a%20variety%20of%20existing%20backdoor%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12852v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSP%20Framework%3A%20A%20Compensatory%20Model%20for%20Defeating%20Trigger%20Reverse%0A%20%20Engineering%20via%20Label%20Smoothing%20Poisoning&entry.906535625=Beichen%20Li%20and%20Yuanfang%20Guo%20and%20Heqi%20Peng%20and%20Yangxi%20Li%20and%20Yunhong%20Wang&entry.1292438233=%20%20Deep%20neural%20networks%20are%20vulnerable%20to%20backdoor%20attacks.%20Among%20the%20existing%0Abackdoor%20defense%20methods%2C%20trigger%20reverse%20engineering%20based%20approaches%2C%20which%0Areconstruct%20the%20backdoor%20triggers%20via%20optimizations%2C%20are%20the%20most%20versatile%20and%0Aeffective%20ones%20compared%20to%20other%20types%20of%20methods.%20In%20this%20paper%2C%20we%20summarize%0Aand%20construct%20a%20generic%20paradigm%20for%20the%20typical%20trigger%20reverse%20engineering%0Aprocess.%20Based%20on%20this%20paradigm%2C%20we%20propose%20a%20new%20perspective%20to%20defeat%20trigger%0Areverse%20engineering%20by%20manipulating%20the%20classification%20confidence%20of%20backdoor%0Asamples.%20To%20determine%20the%20specific%20modifications%20of%20classification%20confidence%2C%0Awe%20propose%20a%20compensatory%20model%20to%20compute%20the%20lower%20bound%20of%20the%20modification.%0AWith%20proper%20modifications%2C%20the%20backdoor%20attack%20can%20easily%20bypass%20the%20trigger%0Areverse%20engineering%20based%20methods.%20To%20achieve%20this%20objective%2C%20we%20propose%20a%0ALabel%20Smoothing%20Poisoning%20%28LSP%29%20framework%2C%20which%20leverages%20label%20smoothing%20to%0Aspecifically%20manipulate%20the%20classification%20confidences%20of%20backdoor%20samples.%0AExtensive%20experiments%20demonstrate%20that%20the%20proposed%20work%20can%20defeat%20the%0Astate-of-the-art%20trigger%20reverse%20engineering%20based%20methods%2C%20and%20possess%20good%0Acompatibility%20with%20a%20variety%20of%20existing%20backdoor%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12852v1&entry.124074799=Read"},
{"title": "TRNet: Two-level Refinement Network leveraging Speech Enhancement for\n  Noise Robust Speech Emotion Recognition", "author": "Chengxin Chen and Pengyuan Zhang", "abstract": "  One persistent challenge in Speech Emotion Recognition (SER) is the\nubiquitous environmental noise, which frequently results in diminished SER\nperformance in practical use. In this paper, we introduce a Two-level\nRefinement Network, dubbed TRNet, to address this challenge. Specifically, a\npre-trained speech enhancement module is employed for front-end noise reduction\nand noise level estimation. Later, we utilize clean speech spectrograms and\ntheir corresponding deep representations as reference signals to refine the\nspectrogram distortion and representation shift of enhanced speech during model\ntraining. Experimental results validate that the proposed TRNet substantially\nincreases the system's robustness in both matched and unmatched noisy\nenvironments, without compromising its performance in clean environments.\n", "link": "http://arxiv.org/abs/2404.12979v1", "date": "2024-04-19", "relevancy": 1.8325, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.499}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4359}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4262}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TRNet%3A%20Two-level%20Refinement%20Network%20leveraging%20Speech%20Enhancement%20for%0A%20%20Noise%20Robust%20Speech%20Emotion%20Recognition&body=Title%3A%20TRNet%3A%20Two-level%20Refinement%20Network%20leveraging%20Speech%20Enhancement%20for%0A%20%20Noise%20Robust%20Speech%20Emotion%20Recognition%0AAuthor%3A%20Chengxin%20Chen%20and%20Pengyuan%20Zhang%0AAbstract%3A%20%20%20One%20persistent%20challenge%20in%20Speech%20Emotion%20Recognition%20%28SER%29%20is%20the%0Aubiquitous%20environmental%20noise%2C%20which%20frequently%20results%20in%20diminished%20SER%0Aperformance%20in%20practical%20use.%20In%20this%20paper%2C%20we%20introduce%20a%20Two-level%0ARefinement%20Network%2C%20dubbed%20TRNet%2C%20to%20address%20this%20challenge.%20Specifically%2C%20a%0Apre-trained%20speech%20enhancement%20module%20is%20employed%20for%20front-end%20noise%20reduction%0Aand%20noise%20level%20estimation.%20Later%2C%20we%20utilize%20clean%20speech%20spectrograms%20and%0Atheir%20corresponding%20deep%20representations%20as%20reference%20signals%20to%20refine%20the%0Aspectrogram%20distortion%20and%20representation%20shift%20of%20enhanced%20speech%20during%20model%0Atraining.%20Experimental%20results%20validate%20that%20the%20proposed%20TRNet%20substantially%0Aincreases%20the%20system%27s%20robustness%20in%20both%20matched%20and%20unmatched%20noisy%0Aenvironments%2C%20without%20compromising%20its%20performance%20in%20clean%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12979v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRNet%3A%20Two-level%20Refinement%20Network%20leveraging%20Speech%20Enhancement%20for%0A%20%20Noise%20Robust%20Speech%20Emotion%20Recognition&entry.906535625=Chengxin%20Chen%20and%20Pengyuan%20Zhang&entry.1292438233=%20%20One%20persistent%20challenge%20in%20Speech%20Emotion%20Recognition%20%28SER%29%20is%20the%0Aubiquitous%20environmental%20noise%2C%20which%20frequently%20results%20in%20diminished%20SER%0Aperformance%20in%20practical%20use.%20In%20this%20paper%2C%20we%20introduce%20a%20Two-level%0ARefinement%20Network%2C%20dubbed%20TRNet%2C%20to%20address%20this%20challenge.%20Specifically%2C%20a%0Apre-trained%20speech%20enhancement%20module%20is%20employed%20for%20front-end%20noise%20reduction%0Aand%20noise%20level%20estimation.%20Later%2C%20we%20utilize%20clean%20speech%20spectrograms%20and%0Atheir%20corresponding%20deep%20representations%20as%20reference%20signals%20to%20refine%20the%0Aspectrogram%20distortion%20and%20representation%20shift%20of%20enhanced%20speech%20during%20model%0Atraining.%20Experimental%20results%20validate%20that%20the%20proposed%20TRNet%20substantially%0Aincreases%20the%20system%27s%20robustness%20in%20both%20matched%20and%20unmatched%20noisy%0Aenvironments%2C%20without%20compromising%20its%20performance%20in%20clean%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12979v1&entry.124074799=Read"},
{"title": "A Quadrature Approach for General-Purpose Batch Bayesian Optimization\n  via Probabilistic Lifting", "author": "Masaki Adachi and Satoshi Hayakawa and Martin J\u00f8rgensen and Saad Hamid and Harald Oberhauser and Michael A. Osborne", "abstract": "  Parallelisation in Bayesian optimisation is a common strategy but faces\nseveral challenges: the need for flexibility in acquisition functions and\nkernel choices, flexibility dealing with discrete and continuous variables\nsimultaneously, model misspecification, and lastly fast massive\nparallelisation. To address these challenges, we introduce a versatile and\nmodular framework for batch Bayesian optimisation via probabilistic lifting\nwith kernel quadrature, called SOBER, which we present as a Python library\nbased on GPyTorch/BoTorch. Our framework offers the following unique benefits:\n(1) Versatility in downstream tasks under a unified approach. (2) A\ngradient-free sampler, which does not require the gradient of acquisition\nfunctions, offering domain-agnostic sampling (e.g., discrete and mixed\nvariables, non-Euclidean space). (3) Flexibility in domain prior distribution.\n(4) Adaptive batch size (autonomous determination of the optimal batch size).\n(5) Robustness against a misspecified reproducing kernel Hilbert space. (6)\nNatural stopping criterion.\n", "link": "http://arxiv.org/abs/2404.12219v2", "date": "2024-04-19", "relevancy": 1.8286, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5219}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4467}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4417}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Quadrature%20Approach%20for%20General-Purpose%20Batch%20Bayesian%20Optimization%0A%20%20via%20Probabilistic%20Lifting&body=Title%3A%20A%20Quadrature%20Approach%20for%20General-Purpose%20Batch%20Bayesian%20Optimization%0A%20%20via%20Probabilistic%20Lifting%0AAuthor%3A%20Masaki%20Adachi%20and%20Satoshi%20Hayakawa%20and%20Martin%20J%C3%B8rgensen%20and%20Saad%20Hamid%20and%20Harald%20Oberhauser%20and%20Michael%20A.%20Osborne%0AAbstract%3A%20%20%20Parallelisation%20in%20Bayesian%20optimisation%20is%20a%20common%20strategy%20but%20faces%0Aseveral%20challenges%3A%20the%20need%20for%20flexibility%20in%20acquisition%20functions%20and%0Akernel%20choices%2C%20flexibility%20dealing%20with%20discrete%20and%20continuous%20variables%0Asimultaneously%2C%20model%20misspecification%2C%20and%20lastly%20fast%20massive%0Aparallelisation.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20versatile%20and%0Amodular%20framework%20for%20batch%20Bayesian%20optimisation%20via%20probabilistic%20lifting%0Awith%20kernel%20quadrature%2C%20called%20SOBER%2C%20which%20we%20present%20as%20a%20Python%20library%0Abased%20on%20GPyTorch/BoTorch.%20Our%20framework%20offers%20the%20following%20unique%20benefits%3A%0A%281%29%20Versatility%20in%20downstream%20tasks%20under%20a%20unified%20approach.%20%282%29%20A%0Agradient-free%20sampler%2C%20which%20does%20not%20require%20the%20gradient%20of%20acquisition%0Afunctions%2C%20offering%20domain-agnostic%20sampling%20%28e.g.%2C%20discrete%20and%20mixed%0Avariables%2C%20non-Euclidean%20space%29.%20%283%29%20Flexibility%20in%20domain%20prior%20distribution.%0A%284%29%20Adaptive%20batch%20size%20%28autonomous%20determination%20of%20the%20optimal%20batch%20size%29.%0A%285%29%20Robustness%20against%20a%20misspecified%20reproducing%20kernel%20Hilbert%20space.%20%286%29%0ANatural%20stopping%20criterion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12219v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Quadrature%20Approach%20for%20General-Purpose%20Batch%20Bayesian%20Optimization%0A%20%20via%20Probabilistic%20Lifting&entry.906535625=Masaki%20Adachi%20and%20Satoshi%20Hayakawa%20and%20Martin%20J%C3%B8rgensen%20and%20Saad%20Hamid%20and%20Harald%20Oberhauser%20and%20Michael%20A.%20Osborne&entry.1292438233=%20%20Parallelisation%20in%20Bayesian%20optimisation%20is%20a%20common%20strategy%20but%20faces%0Aseveral%20challenges%3A%20the%20need%20for%20flexibility%20in%20acquisition%20functions%20and%0Akernel%20choices%2C%20flexibility%20dealing%20with%20discrete%20and%20continuous%20variables%0Asimultaneously%2C%20model%20misspecification%2C%20and%20lastly%20fast%20massive%0Aparallelisation.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20versatile%20and%0Amodular%20framework%20for%20batch%20Bayesian%20optimisation%20via%20probabilistic%20lifting%0Awith%20kernel%20quadrature%2C%20called%20SOBER%2C%20which%20we%20present%20as%20a%20Python%20library%0Abased%20on%20GPyTorch/BoTorch.%20Our%20framework%20offers%20the%20following%20unique%20benefits%3A%0A%281%29%20Versatility%20in%20downstream%20tasks%20under%20a%20unified%20approach.%20%282%29%20A%0Agradient-free%20sampler%2C%20which%20does%20not%20require%20the%20gradient%20of%20acquisition%0Afunctions%2C%20offering%20domain-agnostic%20sampling%20%28e.g.%2C%20discrete%20and%20mixed%0Avariables%2C%20non-Euclidean%20space%29.%20%283%29%20Flexibility%20in%20domain%20prior%20distribution.%0A%284%29%20Adaptive%20batch%20size%20%28autonomous%20determination%20of%20the%20optimal%20batch%20size%29.%0A%285%29%20Robustness%20against%20a%20misspecified%20reproducing%20kernel%20Hilbert%20space.%20%286%29%0ANatural%20stopping%20criterion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12219v2&entry.124074799=Read"},
{"title": "Stronger Random Baselines for In-Context Learning", "author": "Gregory Yauney and David Mimno", "abstract": "  Evaluating the in-context learning classification performance of language\nmodels poses challenges due to small dataset sizes, extensive prompt-selection\nusing the validation set, and intentionally difficult tasks that lead to\nnear-random performance. The standard random baseline -- the expected accuracy\nof guessing labels uniformly at random -- is stable when the evaluation set is\nused only once or when the dataset is large. We account for the common practice\nof validation set reuse and existing small datasets with a stronger random\nbaseline: the expected maximum accuracy across multiple random classifiers.\nWhen choosing the best prompt demonstrations across six quantized language\nmodels applied to 16 BIG-bench Lite tasks, more than 20\\% of the few-shot\nresults that exceed the standard baseline do not exceed this stronger random\nbaseline. When held-out test sets are available, this stronger baseline is also\na better predictor of held-out performance than the standard baseline, avoiding\nunnecessary test set evaluations. This maximum random baseline provides an\neasily calculated drop-in replacement for the standard baseline.\n", "link": "http://arxiv.org/abs/2404.13020v1", "date": "2024-04-19", "relevancy": 1.8277, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4605}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4605}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4519}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Stronger%20Random%20Baselines%20for%20In-Context%20Learning&body=Title%3A%20Stronger%20Random%20Baselines%20for%20In-Context%20Learning%0AAuthor%3A%20Gregory%20Yauney%20and%20David%20Mimno%0AAbstract%3A%20%20%20Evaluating%20the%20in-context%20learning%20classification%20performance%20of%20language%0Amodels%20poses%20challenges%20due%20to%20small%20dataset%20sizes%2C%20extensive%20prompt-selection%0Ausing%20the%20validation%20set%2C%20and%20intentionally%20difficult%20tasks%20that%20lead%20to%0Anear-random%20performance.%20The%20standard%20random%20baseline%20--%20the%20expected%20accuracy%0Aof%20guessing%20labels%20uniformly%20at%20random%20--%20is%20stable%20when%20the%20evaluation%20set%20is%0Aused%20only%20once%20or%20when%20the%20dataset%20is%20large.%20We%20account%20for%20the%20common%20practice%0Aof%20validation%20set%20reuse%20and%20existing%20small%20datasets%20with%20a%20stronger%20random%0Abaseline%3A%20the%20expected%20maximum%20accuracy%20across%20multiple%20random%20classifiers.%0AWhen%20choosing%20the%20best%20prompt%20demonstrations%20across%20six%20quantized%20language%0Amodels%20applied%20to%2016%20BIG-bench%20Lite%20tasks%2C%20more%20than%2020%5C%25%20of%20the%20few-shot%0Aresults%20that%20exceed%20the%20standard%20baseline%20do%20not%20exceed%20this%20stronger%20random%0Abaseline.%20When%20held-out%20test%20sets%20are%20available%2C%20this%20stronger%20baseline%20is%20also%0Aa%20better%20predictor%20of%20held-out%20performance%20than%20the%20standard%20baseline%2C%20avoiding%0Aunnecessary%20test%20set%20evaluations.%20This%20maximum%20random%20baseline%20provides%20an%0Aeasily%20calculated%20drop-in%20replacement%20for%20the%20standard%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13020v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stronger%20Random%20Baselines%20for%20In-Context%20Learning&entry.906535625=Gregory%20Yauney%20and%20David%20Mimno&entry.1292438233=%20%20Evaluating%20the%20in-context%20learning%20classification%20performance%20of%20language%0Amodels%20poses%20challenges%20due%20to%20small%20dataset%20sizes%2C%20extensive%20prompt-selection%0Ausing%20the%20validation%20set%2C%20and%20intentionally%20difficult%20tasks%20that%20lead%20to%0Anear-random%20performance.%20The%20standard%20random%20baseline%20--%20the%20expected%20accuracy%0Aof%20guessing%20labels%20uniformly%20at%20random%20--%20is%20stable%20when%20the%20evaluation%20set%20is%0Aused%20only%20once%20or%20when%20the%20dataset%20is%20large.%20We%20account%20for%20the%20common%20practice%0Aof%20validation%20set%20reuse%20and%20existing%20small%20datasets%20with%20a%20stronger%20random%0Abaseline%3A%20the%20expected%20maximum%20accuracy%20across%20multiple%20random%20classifiers.%0AWhen%20choosing%20the%20best%20prompt%20demonstrations%20across%20six%20quantized%20language%0Amodels%20applied%20to%2016%20BIG-bench%20Lite%20tasks%2C%20more%20than%2020%5C%25%20of%20the%20few-shot%0Aresults%20that%20exceed%20the%20standard%20baseline%20do%20not%20exceed%20this%20stronger%20random%0Abaseline.%20When%20held-out%20test%20sets%20are%20available%2C%20this%20stronger%20baseline%20is%20also%0Aa%20better%20predictor%20of%20held-out%20performance%20than%20the%20standard%20baseline%2C%20avoiding%0Aunnecessary%20test%20set%20evaluations.%20This%20maximum%20random%20baseline%20provides%20an%0Aeasily%20calculated%20drop-in%20replacement%20for%20the%20standard%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13020v1&entry.124074799=Read"},
{"title": "Interpretable Graph Neural Networks for Tabular Data", "author": "Amr Alkhatib and Sofiane Ennadir and Henrik Bostr\u00f6m and Michalis Vazirgiannis", "abstract": "  Data in tabular format is frequently occurring in real-world applications.\nGraph Neural Networks (GNNs) have recently been extended to effectively handle\nsuch data, allowing feature interactions to be captured through representation\nlearning. However, these approaches essentially produce black-box models, in\nthe form of deep neural networks, precluding users from following the logic\nbehind the model predictions. We propose an approach, called IGNNet\n(Interpretable Graph Neural Network for tabular data), which constrains the\nlearning algorithm to produce an interpretable model, where the model shows how\nthe predictions are exactly computed from the original input features. A\nlarge-scale empirical investigation is presented, showing that IGNNet is\nperforming on par with state-of-the-art machine-learning algorithms that target\ntabular data, including XGBoost, Random Forests, and TabNet. At the same time,\nthe results show that the explanations obtained from IGNNet are aligned with\nthe true Shapley values of the features without incurring any additional\ncomputational overhead.\n", "link": "http://arxiv.org/abs/2308.08945v2", "date": "2024-04-19", "relevancy": 1.8274, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4739}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.445}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4437}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Graph%20Neural%20Networks%20for%20Tabular%20Data&body=Title%3A%20Interpretable%20Graph%20Neural%20Networks%20for%20Tabular%20Data%0AAuthor%3A%20Amr%20Alkhatib%20and%20Sofiane%20Ennadir%20and%20Henrik%20Bostr%C3%B6m%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Data%20in%20tabular%20format%20is%20frequently%20occurring%20in%20real-world%20applications.%0AGraph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20been%20extended%20to%20effectively%20handle%0Asuch%20data%2C%20allowing%20feature%20interactions%20to%20be%20captured%20through%20representation%0Alearning.%20However%2C%20these%20approaches%20essentially%20produce%20black-box%20models%2C%20in%0Athe%20form%20of%20deep%20neural%20networks%2C%20precluding%20users%20from%20following%20the%20logic%0Abehind%20the%20model%20predictions.%20We%20propose%20an%20approach%2C%20called%20IGNNet%0A%28Interpretable%20Graph%20Neural%20Network%20for%20tabular%20data%29%2C%20which%20constrains%20the%0Alearning%20algorithm%20to%20produce%20an%20interpretable%20model%2C%20where%20the%20model%20shows%20how%0Athe%20predictions%20are%20exactly%20computed%20from%20the%20original%20input%20features.%20A%0Alarge-scale%20empirical%20investigation%20is%20presented%2C%20showing%20that%20IGNNet%20is%0Aperforming%20on%20par%20with%20state-of-the-art%20machine-learning%20algorithms%20that%20target%0Atabular%20data%2C%20including%20XGBoost%2C%20Random%20Forests%2C%20and%20TabNet.%20At%20the%20same%20time%2C%0Athe%20results%20show%20that%20the%20explanations%20obtained%20from%20IGNNet%20are%20aligned%20with%0Athe%20true%20Shapley%20values%20of%20the%20features%20without%20incurring%20any%20additional%0Acomputational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08945v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Graph%20Neural%20Networks%20for%20Tabular%20Data&entry.906535625=Amr%20Alkhatib%20and%20Sofiane%20Ennadir%20and%20Henrik%20Bostr%C3%B6m%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Data%20in%20tabular%20format%20is%20frequently%20occurring%20in%20real-world%20applications.%0AGraph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20been%20extended%20to%20effectively%20handle%0Asuch%20data%2C%20allowing%20feature%20interactions%20to%20be%20captured%20through%20representation%0Alearning.%20However%2C%20these%20approaches%20essentially%20produce%20black-box%20models%2C%20in%0Athe%20form%20of%20deep%20neural%20networks%2C%20precluding%20users%20from%20following%20the%20logic%0Abehind%20the%20model%20predictions.%20We%20propose%20an%20approach%2C%20called%20IGNNet%0A%28Interpretable%20Graph%20Neural%20Network%20for%20tabular%20data%29%2C%20which%20constrains%20the%0Alearning%20algorithm%20to%20produce%20an%20interpretable%20model%2C%20where%20the%20model%20shows%20how%0Athe%20predictions%20are%20exactly%20computed%20from%20the%20original%20input%20features.%20A%0Alarge-scale%20empirical%20investigation%20is%20presented%2C%20showing%20that%20IGNNet%20is%0Aperforming%20on%20par%20with%20state-of-the-art%20machine-learning%20algorithms%20that%20target%0Atabular%20data%2C%20including%20XGBoost%2C%20Random%20Forests%2C%20and%20TabNet.%20At%20the%20same%20time%2C%0Athe%20results%20show%20that%20the%20explanations%20obtained%20from%20IGNNet%20are%20aligned%20with%0Athe%20true%20Shapley%20values%20of%20the%20features%20without%20incurring%20any%20additional%0Acomputational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08945v2&entry.124074799=Read"},
{"title": "An Embodied Generalist Agent in 3D World", "author": "Jiangyong Huang and Silong Yong and Xiaojian Ma and Xiongkun Linghu and Puhao Li and Yan Wang and Qing Li and Song-Chun Zhu and Baoxiong Jia and Siyuan Huang", "abstract": "  Leveraging massive knowledge and learning schemes from large language models\n(LLMs), recent machine learning models show notable successes in building\ngeneralist agents that exhibit the capability of general-purpose task solving\nin diverse domains, including natural language processing, computer vision, and\nrobotics. However, a significant challenge remains as these models exhibit\nlimited ability in understanding and interacting with the 3D world. We argue\nthis limitation significantly hinders the current models from performing\nreal-world tasks and further achieving general intelligence. To this end, we\nintroduce an embodied multi-modal and multi-task generalist agent that excels\nin perceiving, grounding, reasoning, planning, and acting in the 3D world. Our\nproposed agent, referred to as LEO, is trained with shared LLM-based model\narchitectures, objectives, and weights in two stages: (i) 3D vision-language\nalignment and (ii) 3D vision-language-action instruction tuning. To facilitate\nthe training, we meticulously curate and generate an extensive dataset\ncomprising object-level and scene-level multi-modal tasks with exceeding scale\nand complexity, necessitating a deep understanding of and interaction with the\n3D world. Through rigorous experiments, we demonstrate LEO's remarkable\nproficiency across a wide spectrum of tasks, including 3D captioning, question\nanswering, embodied reasoning, embodied navigation, and robotic manipulation.\nOur ablation results further provide valuable insights for the development of\nfuture embodied generalist agents.\n", "link": "http://arxiv.org/abs/2311.12871v2", "date": "2024-04-19", "relevancy": 1.8232, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.622}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5914}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Embodied%20Generalist%20Agent%20in%203D%20World&body=Title%3A%20An%20Embodied%20Generalist%20Agent%20in%203D%20World%0AAuthor%3A%20Jiangyong%20Huang%20and%20Silong%20Yong%20and%20Xiaojian%20Ma%20and%20Xiongkun%20Linghu%20and%20Puhao%20Li%20and%20Yan%20Wang%20and%20Qing%20Li%20and%20Song-Chun%20Zhu%20and%20Baoxiong%20Jia%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Leveraging%20massive%20knowledge%20and%20learning%20schemes%20from%20large%20language%20models%0A%28LLMs%29%2C%20recent%20machine%20learning%20models%20show%20notable%20successes%20in%20building%0Ageneralist%20agents%20that%20exhibit%20the%20capability%20of%20general-purpose%20task%20solving%0Ain%20diverse%20domains%2C%20including%20natural%20language%20processing%2C%20computer%20vision%2C%20and%0Arobotics.%20However%2C%20a%20significant%20challenge%20remains%20as%20these%20models%20exhibit%0Alimited%20ability%20in%20understanding%20and%20interacting%20with%20the%203D%20world.%20We%20argue%0Athis%20limitation%20significantly%20hinders%20the%20current%20models%20from%20performing%0Areal-world%20tasks%20and%20further%20achieving%20general%20intelligence.%20To%20this%20end%2C%20we%0Aintroduce%20an%20embodied%20multi-modal%20and%20multi-task%20generalist%20agent%20that%20excels%0Ain%20perceiving%2C%20grounding%2C%20reasoning%2C%20planning%2C%20and%20acting%20in%20the%203D%20world.%20Our%0Aproposed%20agent%2C%20referred%20to%20as%20LEO%2C%20is%20trained%20with%20shared%20LLM-based%20model%0Aarchitectures%2C%20objectives%2C%20and%20weights%20in%20two%20stages%3A%20%28i%29%203D%20vision-language%0Aalignment%20and%20%28ii%29%203D%20vision-language-action%20instruction%20tuning.%20To%20facilitate%0Athe%20training%2C%20we%20meticulously%20curate%20and%20generate%20an%20extensive%20dataset%0Acomprising%20object-level%20and%20scene-level%20multi-modal%20tasks%20with%20exceeding%20scale%0Aand%20complexity%2C%20necessitating%20a%20deep%20understanding%20of%20and%20interaction%20with%20the%0A3D%20world.%20Through%20rigorous%20experiments%2C%20we%20demonstrate%20LEO%27s%20remarkable%0Aproficiency%20across%20a%20wide%20spectrum%20of%20tasks%2C%20including%203D%20captioning%2C%20question%0Aanswering%2C%20embodied%20reasoning%2C%20embodied%20navigation%2C%20and%20robotic%20manipulation.%0AOur%20ablation%20results%20further%20provide%20valuable%20insights%20for%20the%20development%20of%0Afuture%20embodied%20generalist%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12871v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Embodied%20Generalist%20Agent%20in%203D%20World&entry.906535625=Jiangyong%20Huang%20and%20Silong%20Yong%20and%20Xiaojian%20Ma%20and%20Xiongkun%20Linghu%20and%20Puhao%20Li%20and%20Yan%20Wang%20and%20Qing%20Li%20and%20Song-Chun%20Zhu%20and%20Baoxiong%20Jia%20and%20Siyuan%20Huang&entry.1292438233=%20%20Leveraging%20massive%20knowledge%20and%20learning%20schemes%20from%20large%20language%20models%0A%28LLMs%29%2C%20recent%20machine%20learning%20models%20show%20notable%20successes%20in%20building%0Ageneralist%20agents%20that%20exhibit%20the%20capability%20of%20general-purpose%20task%20solving%0Ain%20diverse%20domains%2C%20including%20natural%20language%20processing%2C%20computer%20vision%2C%20and%0Arobotics.%20However%2C%20a%20significant%20challenge%20remains%20as%20these%20models%20exhibit%0Alimited%20ability%20in%20understanding%20and%20interacting%20with%20the%203D%20world.%20We%20argue%0Athis%20limitation%20significantly%20hinders%20the%20current%20models%20from%20performing%0Areal-world%20tasks%20and%20further%20achieving%20general%20intelligence.%20To%20this%20end%2C%20we%0Aintroduce%20an%20embodied%20multi-modal%20and%20multi-task%20generalist%20agent%20that%20excels%0Ain%20perceiving%2C%20grounding%2C%20reasoning%2C%20planning%2C%20and%20acting%20in%20the%203D%20world.%20Our%0Aproposed%20agent%2C%20referred%20to%20as%20LEO%2C%20is%20trained%20with%20shared%20LLM-based%20model%0Aarchitectures%2C%20objectives%2C%20and%20weights%20in%20two%20stages%3A%20%28i%29%203D%20vision-language%0Aalignment%20and%20%28ii%29%203D%20vision-language-action%20instruction%20tuning.%20To%20facilitate%0Athe%20training%2C%20we%20meticulously%20curate%20and%20generate%20an%20extensive%20dataset%0Acomprising%20object-level%20and%20scene-level%20multi-modal%20tasks%20with%20exceeding%20scale%0Aand%20complexity%2C%20necessitating%20a%20deep%20understanding%20of%20and%20interaction%20with%20the%0A3D%20world.%20Through%20rigorous%20experiments%2C%20we%20demonstrate%20LEO%27s%20remarkable%0Aproficiency%20across%20a%20wide%20spectrum%20of%20tasks%2C%20including%203D%20captioning%2C%20question%0Aanswering%2C%20embodied%20reasoning%2C%20embodied%20navigation%2C%20and%20robotic%20manipulation.%0AOur%20ablation%20results%20further%20provide%20valuable%20insights%20for%20the%20development%20of%0Afuture%20embodied%20generalist%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12871v2&entry.124074799=Read"},
{"title": "Nuclei Instance Segmentation of Cryosectioned H&E Stained Histological\n  Images using Triple U-Net Architecture", "author": "Zarif Ahmed and Chowdhury Nur E Alam Siddiqi and Fardifa Fathmiul Alam and Tasnim Ahmed and Tareque Mohmud Chowdhury", "abstract": "  Nuclei instance segmentation is crucial in oncological diagnosis and cancer\npathology research. H&E stained images are commonly used for medical diagnosis,\nbut pre-processing is necessary before using them for image processing tasks.\nTwo principal pre-processing methods are formalin-fixed paraffin-embedded\nsamples (FFPE) and frozen tissue samples (FS). While FFPE is widely used, it is\ntime-consuming, while FS samples can be processed quickly. Analyzing H&E\nstained images derived from fast sample preparation, staining, and scanning can\npose difficulties due to the swift process, which can result in the degradation\nof image quality. This paper proposes a method that leverages the unique\noptical characteristics of H&E stained images. A three-branch U-Net\narchitecture has been implemented, where each branch contributes to the final\nsegmentation results. The process includes applying watershed algorithm to\nseparate overlapping regions and enhance accuracy. The Triple U-Net\narchitecture comprises an RGB branch, a Hematoxylin branch, and a Segmentation\nbranch. This study focuses on a novel dataset named CryoNuSeg. The results\nobtained through robust experiments outperform the state-of-the-art results\nacross various metrics. The benchmark score for this dataset is AJI 52.5 and PQ\n47.7, achieved through the implementation of U-Net Architecture. However, the\nproposed Triple U-Net architecture achieves an AJI score of 67.41 and PQ of\n50.56. The proposed architecture improves more on AJI than other evaluation\nmetrics, which further justifies the superiority of the Triple U-Net\narchitecture over the baseline U-Net model, as AJI is a more strict evaluation\nmetric. The use of the three-branch U-Net model, followed by watershed\npost-processing, significantly surpasses the benchmark scores, showing\nsubstantial improvement in the AJI score\n", "link": "http://arxiv.org/abs/2404.12986v1", "date": "2024-04-19", "relevancy": 1.8157, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.474}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4688}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.431}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Nuclei%20Instance%20Segmentation%20of%20Cryosectioned%20H%26E%20Stained%20Histological%0A%20%20Images%20using%20Triple%20U-Net%20Architecture&body=Title%3A%20Nuclei%20Instance%20Segmentation%20of%20Cryosectioned%20H%26E%20Stained%20Histological%0A%20%20Images%20using%20Triple%20U-Net%20Architecture%0AAuthor%3A%20Zarif%20Ahmed%20and%20Chowdhury%20Nur%20E%20Alam%20Siddiqi%20and%20Fardifa%20Fathmiul%20Alam%20and%20Tasnim%20Ahmed%20and%20Tareque%20Mohmud%20Chowdhury%0AAbstract%3A%20%20%20Nuclei%20instance%20segmentation%20is%20crucial%20in%20oncological%20diagnosis%20and%20cancer%0Apathology%20research.%20H%26E%20stained%20images%20are%20commonly%20used%20for%20medical%20diagnosis%2C%0Abut%20pre-processing%20is%20necessary%20before%20using%20them%20for%20image%20processing%20tasks.%0ATwo%20principal%20pre-processing%20methods%20are%20formalin-fixed%20paraffin-embedded%0Asamples%20%28FFPE%29%20and%20frozen%20tissue%20samples%20%28FS%29.%20While%20FFPE%20is%20widely%20used%2C%20it%20is%0Atime-consuming%2C%20while%20FS%20samples%20can%20be%20processed%20quickly.%20Analyzing%20H%26E%0Astained%20images%20derived%20from%20fast%20sample%20preparation%2C%20staining%2C%20and%20scanning%20can%0Apose%20difficulties%20due%20to%20the%20swift%20process%2C%20which%20can%20result%20in%20the%20degradation%0Aof%20image%20quality.%20This%20paper%20proposes%20a%20method%20that%20leverages%20the%20unique%0Aoptical%20characteristics%20of%20H%26E%20stained%20images.%20A%20three-branch%20U-Net%0Aarchitecture%20has%20been%20implemented%2C%20where%20each%20branch%20contributes%20to%20the%20final%0Asegmentation%20results.%20The%20process%20includes%20applying%20watershed%20algorithm%20to%0Aseparate%20overlapping%20regions%20and%20enhance%20accuracy.%20The%20Triple%20U-Net%0Aarchitecture%20comprises%20an%20RGB%20branch%2C%20a%20Hematoxylin%20branch%2C%20and%20a%20Segmentation%0Abranch.%20This%20study%20focuses%20on%20a%20novel%20dataset%20named%20CryoNuSeg.%20The%20results%0Aobtained%20through%20robust%20experiments%20outperform%20the%20state-of-the-art%20results%0Aacross%20various%20metrics.%20The%20benchmark%20score%20for%20this%20dataset%20is%20AJI%2052.5%20and%20PQ%0A47.7%2C%20achieved%20through%20the%20implementation%20of%20U-Net%20Architecture.%20However%2C%20the%0Aproposed%20Triple%20U-Net%20architecture%20achieves%20an%20AJI%20score%20of%2067.41%20and%20PQ%20of%0A50.56.%20The%20proposed%20architecture%20improves%20more%20on%20AJI%20than%20other%20evaluation%0Ametrics%2C%20which%20further%20justifies%20the%20superiority%20of%20the%20Triple%20U-Net%0Aarchitecture%20over%20the%20baseline%20U-Net%20model%2C%20as%20AJI%20is%20a%20more%20strict%20evaluation%0Ametric.%20The%20use%20of%20the%20three-branch%20U-Net%20model%2C%20followed%20by%20watershed%0Apost-processing%2C%20significantly%20surpasses%20the%20benchmark%20scores%2C%20showing%0Asubstantial%20improvement%20in%20the%20AJI%20score%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12986v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nuclei%20Instance%20Segmentation%20of%20Cryosectioned%20H%26E%20Stained%20Histological%0A%20%20Images%20using%20Triple%20U-Net%20Architecture&entry.906535625=Zarif%20Ahmed%20and%20Chowdhury%20Nur%20E%20Alam%20Siddiqi%20and%20Fardifa%20Fathmiul%20Alam%20and%20Tasnim%20Ahmed%20and%20Tareque%20Mohmud%20Chowdhury&entry.1292438233=%20%20Nuclei%20instance%20segmentation%20is%20crucial%20in%20oncological%20diagnosis%20and%20cancer%0Apathology%20research.%20H%26E%20stained%20images%20are%20commonly%20used%20for%20medical%20diagnosis%2C%0Abut%20pre-processing%20is%20necessary%20before%20using%20them%20for%20image%20processing%20tasks.%0ATwo%20principal%20pre-processing%20methods%20are%20formalin-fixed%20paraffin-embedded%0Asamples%20%28FFPE%29%20and%20frozen%20tissue%20samples%20%28FS%29.%20While%20FFPE%20is%20widely%20used%2C%20it%20is%0Atime-consuming%2C%20while%20FS%20samples%20can%20be%20processed%20quickly.%20Analyzing%20H%26E%0Astained%20images%20derived%20from%20fast%20sample%20preparation%2C%20staining%2C%20and%20scanning%20can%0Apose%20difficulties%20due%20to%20the%20swift%20process%2C%20which%20can%20result%20in%20the%20degradation%0Aof%20image%20quality.%20This%20paper%20proposes%20a%20method%20that%20leverages%20the%20unique%0Aoptical%20characteristics%20of%20H%26E%20stained%20images.%20A%20three-branch%20U-Net%0Aarchitecture%20has%20been%20implemented%2C%20where%20each%20branch%20contributes%20to%20the%20final%0Asegmentation%20results.%20The%20process%20includes%20applying%20watershed%20algorithm%20to%0Aseparate%20overlapping%20regions%20and%20enhance%20accuracy.%20The%20Triple%20U-Net%0Aarchitecture%20comprises%20an%20RGB%20branch%2C%20a%20Hematoxylin%20branch%2C%20and%20a%20Segmentation%0Abranch.%20This%20study%20focuses%20on%20a%20novel%20dataset%20named%20CryoNuSeg.%20The%20results%0Aobtained%20through%20robust%20experiments%20outperform%20the%20state-of-the-art%20results%0Aacross%20various%20metrics.%20The%20benchmark%20score%20for%20this%20dataset%20is%20AJI%2052.5%20and%20PQ%0A47.7%2C%20achieved%20through%20the%20implementation%20of%20U-Net%20Architecture.%20However%2C%20the%0Aproposed%20Triple%20U-Net%20architecture%20achieves%20an%20AJI%20score%20of%2067.41%20and%20PQ%20of%0A50.56.%20The%20proposed%20architecture%20improves%20more%20on%20AJI%20than%20other%20evaluation%0Ametrics%2C%20which%20further%20justifies%20the%20superiority%20of%20the%20Triple%20U-Net%0Aarchitecture%20over%20the%20baseline%20U-Net%20model%2C%20as%20AJI%20is%20a%20more%20strict%20evaluation%0Ametric.%20The%20use%20of%20the%20three-branch%20U-Net%20model%2C%20followed%20by%20watershed%0Apost-processing%2C%20significantly%20surpasses%20the%20benchmark%20scores%2C%20showing%0Asubstantial%20improvement%20in%20the%20AJI%20score%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12986v1&entry.124074799=Read"},
{"title": "Blind Federated Learning via Over-the-Air q-QAM", "author": "Saeed Razavikia and Jos\u00e9 Mairton Barros Da Silva J\u00fanior and Carlo Fischione", "abstract": "  In this work, we investigate federated edge learning over a fading multiple\naccess channel. To alleviate the communication burden between the edge devices\nand the access point, we introduce a pioneering digital over-the-air\ncomputation strategy employing q-ary quadrature amplitude modulation,\nculminating in a low latency communication scheme. Indeed, we propose a new\nfederated edge learning framework in which edge devices use digital modulation\nfor over-the-air uplink transmission to the edge server while they have no\naccess to the channel state information. Furthermore, we incorporate multiple\nantennas at the edge server to overcome the fading inherent in wireless\ncommunication. We analyze the number of antennas required to mitigate the\nfading impact effectively. We prove a non-asymptotic upper bound for the mean\nsquared error for the proposed federated learning with digital over-the-air\nuplink transmissions under both noisy and fading conditions. Leveraging the\nderived upper bound, we characterize the convergence rate of the learning\nprocess of a non-convex loss function in terms of the mean square error of\ngradients due to the fading channel. Furthermore, we substantiate the\ntheoretical assurances through numerical experiments concerning mean square\nerror and the convergence efficacy of the digital federated edge learning\nframework. Notably, the results demonstrate that augmenting the number of\nantennas at the edge server and adopting higher-order modulations improve the\nmodel accuracy up to 60\\%.\n", "link": "http://arxiv.org/abs/2311.04253v2", "date": "2024-04-19", "relevancy": 1.8154, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4595}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4503}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Blind%20Federated%20Learning%20via%20Over-the-Air%20q-QAM&body=Title%3A%20Blind%20Federated%20Learning%20via%20Over-the-Air%20q-QAM%0AAuthor%3A%20Saeed%20Razavikia%20and%20Jos%C3%A9%20Mairton%20Barros%20Da%20Silva%20J%C3%BAnior%20and%20Carlo%20Fischione%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20investigate%20federated%20edge%20learning%20over%20a%20fading%20multiple%0Aaccess%20channel.%20To%20alleviate%20the%20communication%20burden%20between%20the%20edge%20devices%0Aand%20the%20access%20point%2C%20we%20introduce%20a%20pioneering%20digital%20over-the-air%0Acomputation%20strategy%20employing%20q-ary%20quadrature%20amplitude%20modulation%2C%0Aculminating%20in%20a%20low%20latency%20communication%20scheme.%20Indeed%2C%20we%20propose%20a%20new%0Afederated%20edge%20learning%20framework%20in%20which%20edge%20devices%20use%20digital%20modulation%0Afor%20over-the-air%20uplink%20transmission%20to%20the%20edge%20server%20while%20they%20have%20no%0Aaccess%20to%20the%20channel%20state%20information.%20Furthermore%2C%20we%20incorporate%20multiple%0Aantennas%20at%20the%20edge%20server%20to%20overcome%20the%20fading%20inherent%20in%20wireless%0Acommunication.%20We%20analyze%20the%20number%20of%20antennas%20required%20to%20mitigate%20the%0Afading%20impact%20effectively.%20We%20prove%20a%20non-asymptotic%20upper%20bound%20for%20the%20mean%0Asquared%20error%20for%20the%20proposed%20federated%20learning%20with%20digital%20over-the-air%0Auplink%20transmissions%20under%20both%20noisy%20and%20fading%20conditions.%20Leveraging%20the%0Aderived%20upper%20bound%2C%20we%20characterize%20the%20convergence%20rate%20of%20the%20learning%0Aprocess%20of%20a%20non-convex%20loss%20function%20in%20terms%20of%20the%20mean%20square%20error%20of%0Agradients%20due%20to%20the%20fading%20channel.%20Furthermore%2C%20we%20substantiate%20the%0Atheoretical%20assurances%20through%20numerical%20experiments%20concerning%20mean%20square%0Aerror%20and%20the%20convergence%20efficacy%20of%20the%20digital%20federated%20edge%20learning%0Aframework.%20Notably%2C%20the%20results%20demonstrate%20that%20augmenting%20the%20number%20of%0Aantennas%20at%20the%20edge%20server%20and%20adopting%20higher-order%20modulations%20improve%20the%0Amodel%20accuracy%20up%20to%2060%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04253v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blind%20Federated%20Learning%20via%20Over-the-Air%20q-QAM&entry.906535625=Saeed%20Razavikia%20and%20Jos%C3%A9%20Mairton%20Barros%20Da%20Silva%20J%C3%BAnior%20and%20Carlo%20Fischione&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20federated%20edge%20learning%20over%20a%20fading%20multiple%0Aaccess%20channel.%20To%20alleviate%20the%20communication%20burden%20between%20the%20edge%20devices%0Aand%20the%20access%20point%2C%20we%20introduce%20a%20pioneering%20digital%20over-the-air%0Acomputation%20strategy%20employing%20q-ary%20quadrature%20amplitude%20modulation%2C%0Aculminating%20in%20a%20low%20latency%20communication%20scheme.%20Indeed%2C%20we%20propose%20a%20new%0Afederated%20edge%20learning%20framework%20in%20which%20edge%20devices%20use%20digital%20modulation%0Afor%20over-the-air%20uplink%20transmission%20to%20the%20edge%20server%20while%20they%20have%20no%0Aaccess%20to%20the%20channel%20state%20information.%20Furthermore%2C%20we%20incorporate%20multiple%0Aantennas%20at%20the%20edge%20server%20to%20overcome%20the%20fading%20inherent%20in%20wireless%0Acommunication.%20We%20analyze%20the%20number%20of%20antennas%20required%20to%20mitigate%20the%0Afading%20impact%20effectively.%20We%20prove%20a%20non-asymptotic%20upper%20bound%20for%20the%20mean%0Asquared%20error%20for%20the%20proposed%20federated%20learning%20with%20digital%20over-the-air%0Auplink%20transmissions%20under%20both%20noisy%20and%20fading%20conditions.%20Leveraging%20the%0Aderived%20upper%20bound%2C%20we%20characterize%20the%20convergence%20rate%20of%20the%20learning%0Aprocess%20of%20a%20non-convex%20loss%20function%20in%20terms%20of%20the%20mean%20square%20error%20of%0Agradients%20due%20to%20the%20fading%20channel.%20Furthermore%2C%20we%20substantiate%20the%0Atheoretical%20assurances%20through%20numerical%20experiments%20concerning%20mean%20square%0Aerror%20and%20the%20convergence%20efficacy%20of%20the%20digital%20federated%20edge%20learning%0Aframework.%20Notably%2C%20the%20results%20demonstrate%20that%20augmenting%20the%20number%20of%0Aantennas%20at%20the%20edge%20server%20and%20adopting%20higher-order%20modulations%20improve%20the%0Amodel%20accuracy%20up%20to%2060%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04253v2&entry.124074799=Read"},
{"title": "Re2LLM: Reflective Reinforcement Large Language Model for Session-based\n  Recommendation", "author": "Ziyan Wang and Yingpeng Du and Zhu Sun and Haoyan Chua and Kaidong Feng and Wenya Wang and Jie Zhang", "abstract": "  Large Language Models (LLMs) are emerging as promising approaches to enhance\nsession-based recommendation (SBR), where both prompt-based and\nfine-tuning-based methods have been widely investigated to align LLMs with SBR.\nHowever, the former methods struggle with optimal prompts to elicit the correct\nreasoning of LLMs due to the lack of task-specific feedback, leading to\nunsatisfactory recommendations. Although the latter methods attempt to\nfine-tune LLMs with domain-specific knowledge, they face limitations such as\nhigh computational costs and reliance on open-source backbones. To address such\nissues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for\nSBR, guiding LLMs to focus on specialized knowledge essential for more accurate\nrecommendations effectively and efficiently. In particular, we first design the\nReflective Exploration Module to effectively extract knowledge that is readily\nunderstandable and digestible by LLMs. To be specific, we direct LLMs to\nexamine recommendation errors through self-reflection and construct a knowledge\nbase (KB) comprising hints capable of rectifying these errors. To efficiently\nelicit the correct reasoning of LLMs, we further devise the Reinforcement\nUtilization Module to train a lightweight retrieval agent. It learns to select\nhints from the constructed KB based on the task-specific feedback, where the\nhints can serve as guidance to help correct LLMs reasoning for better\nrecommendations. Extensive experiments on multiple real-world datasets\ndemonstrate that our method consistently outperforms state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.16427v4", "date": "2024-04-19", "relevancy": 1.8124, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4502}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Re2LLM%3A%20Reflective%20Reinforcement%20Large%20Language%20Model%20for%20Session-based%0A%20%20Recommendation&body=Title%3A%20Re2LLM%3A%20Reflective%20Reinforcement%20Large%20Language%20Model%20for%20Session-based%0A%20%20Recommendation%0AAuthor%3A%20Ziyan%20Wang%20and%20Yingpeng%20Du%20and%20Zhu%20Sun%20and%20Haoyan%20Chua%20and%20Kaidong%20Feng%20and%20Wenya%20Wang%20and%20Jie%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20emerging%20as%20promising%20approaches%20to%20enhance%0Asession-based%20recommendation%20%28SBR%29%2C%20where%20both%20prompt-based%20and%0Afine-tuning-based%20methods%20have%20been%20widely%20investigated%20to%20align%20LLMs%20with%20SBR.%0AHowever%2C%20the%20former%20methods%20struggle%20with%20optimal%20prompts%20to%20elicit%20the%20correct%0Areasoning%20of%20LLMs%20due%20to%20the%20lack%20of%20task-specific%20feedback%2C%20leading%20to%0Aunsatisfactory%20recommendations.%20Although%20the%20latter%20methods%20attempt%20to%0Afine-tune%20LLMs%20with%20domain-specific%20knowledge%2C%20they%20face%20limitations%20such%20as%0Ahigh%20computational%20costs%20and%20reliance%20on%20open-source%20backbones.%20To%20address%20such%0Aissues%2C%20we%20propose%20a%20Reflective%20Reinforcement%20Large%20Language%20Model%20%28Re2LLM%29%20for%0ASBR%2C%20guiding%20LLMs%20to%20focus%20on%20specialized%20knowledge%20essential%20for%20more%20accurate%0Arecommendations%20effectively%20and%20efficiently.%20In%20particular%2C%20we%20first%20design%20the%0AReflective%20Exploration%20Module%20to%20effectively%20extract%20knowledge%20that%20is%20readily%0Aunderstandable%20and%20digestible%20by%20LLMs.%20To%20be%20specific%2C%20we%20direct%20LLMs%20to%0Aexamine%20recommendation%20errors%20through%20self-reflection%20and%20construct%20a%20knowledge%0Abase%20%28KB%29%20comprising%20hints%20capable%20of%20rectifying%20these%20errors.%20To%20efficiently%0Aelicit%20the%20correct%20reasoning%20of%20LLMs%2C%20we%20further%20devise%20the%20Reinforcement%0AUtilization%20Module%20to%20train%20a%20lightweight%20retrieval%20agent.%20It%20learns%20to%20select%0Ahints%20from%20the%20constructed%20KB%20based%20on%20the%20task-specific%20feedback%2C%20where%20the%0Ahints%20can%20serve%20as%20guidance%20to%20help%20correct%20LLMs%20reasoning%20for%20better%0Arecommendations.%20Extensive%20experiments%20on%20multiple%20real-world%20datasets%0Ademonstrate%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16427v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re2LLM%3A%20Reflective%20Reinforcement%20Large%20Language%20Model%20for%20Session-based%0A%20%20Recommendation&entry.906535625=Ziyan%20Wang%20and%20Yingpeng%20Du%20and%20Zhu%20Sun%20and%20Haoyan%20Chua%20and%20Kaidong%20Feng%20and%20Wenya%20Wang%20and%20Jie%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20emerging%20as%20promising%20approaches%20to%20enhance%0Asession-based%20recommendation%20%28SBR%29%2C%20where%20both%20prompt-based%20and%0Afine-tuning-based%20methods%20have%20been%20widely%20investigated%20to%20align%20LLMs%20with%20SBR.%0AHowever%2C%20the%20former%20methods%20struggle%20with%20optimal%20prompts%20to%20elicit%20the%20correct%0Areasoning%20of%20LLMs%20due%20to%20the%20lack%20of%20task-specific%20feedback%2C%20leading%20to%0Aunsatisfactory%20recommendations.%20Although%20the%20latter%20methods%20attempt%20to%0Afine-tune%20LLMs%20with%20domain-specific%20knowledge%2C%20they%20face%20limitations%20such%20as%0Ahigh%20computational%20costs%20and%20reliance%20on%20open-source%20backbones.%20To%20address%20such%0Aissues%2C%20we%20propose%20a%20Reflective%20Reinforcement%20Large%20Language%20Model%20%28Re2LLM%29%20for%0ASBR%2C%20guiding%20LLMs%20to%20focus%20on%20specialized%20knowledge%20essential%20for%20more%20accurate%0Arecommendations%20effectively%20and%20efficiently.%20In%20particular%2C%20we%20first%20design%20the%0AReflective%20Exploration%20Module%20to%20effectively%20extract%20knowledge%20that%20is%20readily%0Aunderstandable%20and%20digestible%20by%20LLMs.%20To%20be%20specific%2C%20we%20direct%20LLMs%20to%0Aexamine%20recommendation%20errors%20through%20self-reflection%20and%20construct%20a%20knowledge%0Abase%20%28KB%29%20comprising%20hints%20capable%20of%20rectifying%20these%20errors.%20To%20efficiently%0Aelicit%20the%20correct%20reasoning%20of%20LLMs%2C%20we%20further%20devise%20the%20Reinforcement%0AUtilization%20Module%20to%20train%20a%20lightweight%20retrieval%20agent.%20It%20learns%20to%20select%0Ahints%20from%20the%20constructed%20KB%20based%20on%20the%20task-specific%20feedback%2C%20where%20the%0Ahints%20can%20serve%20as%20guidance%20to%20help%20correct%20LLMs%20reasoning%20for%20better%0Arecommendations.%20Extensive%20experiments%20on%20multiple%20real-world%20datasets%0Ademonstrate%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16427v4&entry.124074799=Read"},
{"title": "Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images", "author": " Santosh and Li Lin and Irene Amerini and Xin Wang and Shu Hu", "abstract": "  Diffusion models (DMs) have revolutionized image generation, producing\nhigh-quality images with applications spanning various fields. However, their\nability to create hyper-realistic images poses significant challenges in\ndistinguishing between real and synthetic content, raising concerns about\ndigital authenticity and potential misuse in creating deepfakes. This work\nintroduces a robust detection framework that integrates image and text features\nextracted by CLIP model with a Multilayer Perceptron (MLP) classifier. We\npropose a novel loss that can improve the detector's robustness and handle\nimbalanced datasets. Additionally, we flatten the loss landscape during the\nmodel training to improve the detector's generalization capabilities. The\neffectiveness of our method, which outperforms traditional detection\ntechniques, is demonstrated through extensive experiments, underscoring its\npotential to set a new state-of-the-art approach in DM-generated image\ndetection. The code is available at\nhttps://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection.\n", "link": "http://arxiv.org/abs/2404.12908v1", "date": "2024-04-19", "relevancy": 1.8008, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6294}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5931}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5888}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20CLIP-Based%20Detector%20for%20Exposing%20Diffusion%20Model-Generated%20Images&body=Title%3A%20Robust%20CLIP-Based%20Detector%20for%20Exposing%20Diffusion%20Model-Generated%20Images%0AAuthor%3A%20%20Santosh%20and%20Li%20Lin%20and%20Irene%20Amerini%20and%20Xin%20Wang%20and%20Shu%20Hu%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20revolutionized%20image%20generation%2C%20producing%0Ahigh-quality%20images%20with%20applications%20spanning%20various%20fields.%20However%2C%20their%0Aability%20to%20create%20hyper-realistic%20images%20poses%20significant%20challenges%20in%0Adistinguishing%20between%20real%20and%20synthetic%20content%2C%20raising%20concerns%20about%0Adigital%20authenticity%20and%20potential%20misuse%20in%20creating%20deepfakes.%20This%20work%0Aintroduces%20a%20robust%20detection%20framework%20that%20integrates%20image%20and%20text%20features%0Aextracted%20by%20CLIP%20model%20with%20a%20Multilayer%20Perceptron%20%28MLP%29%20classifier.%20We%0Apropose%20a%20novel%20loss%20that%20can%20improve%20the%20detector%27s%20robustness%20and%20handle%0Aimbalanced%20datasets.%20Additionally%2C%20we%20flatten%20the%20loss%20landscape%20during%20the%0Amodel%20training%20to%20improve%20the%20detector%27s%20generalization%20capabilities.%20The%0Aeffectiveness%20of%20our%20method%2C%20which%20outperforms%20traditional%20detection%0Atechniques%2C%20is%20demonstrated%20through%20extensive%20experiments%2C%20underscoring%20its%0Apotential%20to%20set%20a%20new%20state-of-the-art%20approach%20in%20DM-generated%20image%0Adetection.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Purdue-M2/Robust_DM_Generated_Image_Detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12908v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20CLIP-Based%20Detector%20for%20Exposing%20Diffusion%20Model-Generated%20Images&entry.906535625=%20Santosh%20and%20Li%20Lin%20and%20Irene%20Amerini%20and%20Xin%20Wang%20and%20Shu%20Hu&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20revolutionized%20image%20generation%2C%20producing%0Ahigh-quality%20images%20with%20applications%20spanning%20various%20fields.%20However%2C%20their%0Aability%20to%20create%20hyper-realistic%20images%20poses%20significant%20challenges%20in%0Adistinguishing%20between%20real%20and%20synthetic%20content%2C%20raising%20concerns%20about%0Adigital%20authenticity%20and%20potential%20misuse%20in%20creating%20deepfakes.%20This%20work%0Aintroduces%20a%20robust%20detection%20framework%20that%20integrates%20image%20and%20text%20features%0Aextracted%20by%20CLIP%20model%20with%20a%20Multilayer%20Perceptron%20%28MLP%29%20classifier.%20We%0Apropose%20a%20novel%20loss%20that%20can%20improve%20the%20detector%27s%20robustness%20and%20handle%0Aimbalanced%20datasets.%20Additionally%2C%20we%20flatten%20the%20loss%20landscape%20during%20the%0Amodel%20training%20to%20improve%20the%20detector%27s%20generalization%20capabilities.%20The%0Aeffectiveness%20of%20our%20method%2C%20which%20outperforms%20traditional%20detection%0Atechniques%2C%20is%20demonstrated%20through%20extensive%20experiments%2C%20underscoring%20its%0Apotential%20to%20set%20a%20new%20state-of-the-art%20approach%20in%20DM-generated%20image%0Adetection.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Purdue-M2/Robust_DM_Generated_Image_Detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12908v1&entry.124074799=Read"},
{"title": "Set-Based Training for Neural Network Verification", "author": "Lukas Koller and Tobias Ladner and Matthias Althoff", "abstract": "  Neural networks are vulnerable to adversarial attacks, i.e., small input\nperturbations can significantly affect the outputs of a neural network. In\nsafety-critical environments, the inputs often contain noisy sensor data;\nhence, in this case, neural networks that are robust against input\nperturbations are required. To ensure safety, the robustness of a neural\nnetwork must be formally verified. However, training and formally verifying\nrobust neural networks is challenging. We address both of these challenges by\nemploying, for the first time, an end-to-end set-based training procedure that\ntrains robust neural networks for formal verification. Our training procedure\ntrains neural networks, which can be easily verified using simple\npolynomial-time verification algorithms. Moreover, our extensive evaluation\ndemonstrates that our set-based training procedure effectively trains robust\nneural networks, which are easier to verify. Set-based trained neural networks\nconsistently match or outperform those trained with state-of-the-art robust\ntraining approaches.\n", "link": "http://arxiv.org/abs/2401.14961v2", "date": "2024-04-19", "relevancy": 1.7914, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4659}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4376}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4285}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Set-Based%20Training%20for%20Neural%20Network%20Verification&body=Title%3A%20Set-Based%20Training%20for%20Neural%20Network%20Verification%0AAuthor%3A%20Lukas%20Koller%20and%20Tobias%20Ladner%20and%20Matthias%20Althoff%0AAbstract%3A%20%20%20Neural%20networks%20are%20vulnerable%20to%20adversarial%20attacks%2C%20i.e.%2C%20small%20input%0Aperturbations%20can%20significantly%20affect%20the%20outputs%20of%20a%20neural%20network.%20In%0Asafety-critical%20environments%2C%20the%20inputs%20often%20contain%20noisy%20sensor%20data%3B%0Ahence%2C%20in%20this%20case%2C%20neural%20networks%20that%20are%20robust%20against%20input%0Aperturbations%20are%20required.%20To%20ensure%20safety%2C%20the%20robustness%20of%20a%20neural%0Anetwork%20must%20be%20formally%20verified.%20However%2C%20training%20and%20formally%20verifying%0Arobust%20neural%20networks%20is%20challenging.%20We%20address%20both%20of%20these%20challenges%20by%0Aemploying%2C%20for%20the%20first%20time%2C%20an%20end-to-end%20set-based%20training%20procedure%20that%0Atrains%20robust%20neural%20networks%20for%20formal%20verification.%20Our%20training%20procedure%0Atrains%20neural%20networks%2C%20which%20can%20be%20easily%20verified%20using%20simple%0Apolynomial-time%20verification%20algorithms.%20Moreover%2C%20our%20extensive%20evaluation%0Ademonstrates%20that%20our%20set-based%20training%20procedure%20effectively%20trains%20robust%0Aneural%20networks%2C%20which%20are%20easier%20to%20verify.%20Set-based%20trained%20neural%20networks%0Aconsistently%20match%20or%20outperform%20those%20trained%20with%20state-of-the-art%20robust%0Atraining%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14961v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Set-Based%20Training%20for%20Neural%20Network%20Verification&entry.906535625=Lukas%20Koller%20and%20Tobias%20Ladner%20and%20Matthias%20Althoff&entry.1292438233=%20%20Neural%20networks%20are%20vulnerable%20to%20adversarial%20attacks%2C%20i.e.%2C%20small%20input%0Aperturbations%20can%20significantly%20affect%20the%20outputs%20of%20a%20neural%20network.%20In%0Asafety-critical%20environments%2C%20the%20inputs%20often%20contain%20noisy%20sensor%20data%3B%0Ahence%2C%20in%20this%20case%2C%20neural%20networks%20that%20are%20robust%20against%20input%0Aperturbations%20are%20required.%20To%20ensure%20safety%2C%20the%20robustness%20of%20a%20neural%0Anetwork%20must%20be%20formally%20verified.%20However%2C%20training%20and%20formally%20verifying%0Arobust%20neural%20networks%20is%20challenging.%20We%20address%20both%20of%20these%20challenges%20by%0Aemploying%2C%20for%20the%20first%20time%2C%20an%20end-to-end%20set-based%20training%20procedure%20that%0Atrains%20robust%20neural%20networks%20for%20formal%20verification.%20Our%20training%20procedure%0Atrains%20neural%20networks%2C%20which%20can%20be%20easily%20verified%20using%20simple%0Apolynomial-time%20verification%20algorithms.%20Moreover%2C%20our%20extensive%20evaluation%0Ademonstrates%20that%20our%20set-based%20training%20procedure%20effectively%20trains%20robust%0Aneural%20networks%2C%20which%20are%20easier%20to%20verify.%20Set-based%20trained%20neural%20networks%0Aconsistently%20match%20or%20outperform%20those%20trained%20with%20state-of-the-art%20robust%0Atraining%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14961v2&entry.124074799=Read"},
{"title": "LayeredMAPF: a decomposition of MAPF instance without compromising\n  solvability", "author": "Zhuo Yao and Wei Wang", "abstract": "  Generally, the calculation and memory space required for multi-agent path\nfinding (MAPF) grows exponentially as the number of agents increases. This\noften results in some MAPF instances being unsolvable under limited\ncomputational resources and memory space, thereby limiting the application of\nMAPF in complex scenarios. Hence, we propose a decomposition approach for MAPF\ninstances, which breaks down instances involving a large number of agents into\nmultiple isolated subproblems involving fewer agents. Moreover, we present a\nframework to enable general MAPF algorithms to solve each subproblem\nindependently and merge their solutions into one conflict-free final solution,\nwithout compromising on solvability. Unlike existing works that propose\nisolated methods aimed at reducing the time cost of MAPF, our method is\napplicable to all MAPF methods. In our results, we apply decomposition to\nmultiple state-of-the-art MAPF methods using a classic MAPF benchmark\n(https://movingai.com/benchmarks/mapf.html). The decomposition of MAPF\ninstances is completed on average within 1s, and its application to seven MAPF\nmethods reduces the memory usage and time cost significantly, particularly for\nserial methods. To facilitate further research within the community, we have\nmade the source code of the proposed algorithm publicly available\n(https://github.com/JoeYao-bit/LayeredMAPF).\n", "link": "http://arxiv.org/abs/2404.12773v1", "date": "2024-04-19", "relevancy": 1.7714, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4514}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.45}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4323}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LayeredMAPF%3A%20a%20decomposition%20of%20MAPF%20instance%20without%20compromising%0A%20%20solvability&body=Title%3A%20LayeredMAPF%3A%20a%20decomposition%20of%20MAPF%20instance%20without%20compromising%0A%20%20solvability%0AAuthor%3A%20Zhuo%20Yao%20and%20Wei%20Wang%0AAbstract%3A%20%20%20Generally%2C%20the%20calculation%20and%20memory%20space%20required%20for%20multi-agent%20path%0Afinding%20%28MAPF%29%20grows%20exponentially%20as%20the%20number%20of%20agents%20increases.%20This%0Aoften%20results%20in%20some%20MAPF%20instances%20being%20unsolvable%20under%20limited%0Acomputational%20resources%20and%20memory%20space%2C%20thereby%20limiting%20the%20application%20of%0AMAPF%20in%20complex%20scenarios.%20Hence%2C%20we%20propose%20a%20decomposition%20approach%20for%20MAPF%0Ainstances%2C%20which%20breaks%20down%20instances%20involving%20a%20large%20number%20of%20agents%20into%0Amultiple%20isolated%20subproblems%20involving%20fewer%20agents.%20Moreover%2C%20we%20present%20a%0Aframework%20to%20enable%20general%20MAPF%20algorithms%20to%20solve%20each%20subproblem%0Aindependently%20and%20merge%20their%20solutions%20into%20one%20conflict-free%20final%20solution%2C%0Awithout%20compromising%20on%20solvability.%20Unlike%20existing%20works%20that%20propose%0Aisolated%20methods%20aimed%20at%20reducing%20the%20time%20cost%20of%20MAPF%2C%20our%20method%20is%0Aapplicable%20to%20all%20MAPF%20methods.%20In%20our%20results%2C%20we%20apply%20decomposition%20to%0Amultiple%20state-of-the-art%20MAPF%20methods%20using%20a%20classic%20MAPF%20benchmark%0A%28https%3A//movingai.com/benchmarks/mapf.html%29.%20The%20decomposition%20of%20MAPF%0Ainstances%20is%20completed%20on%20average%20within%201s%2C%20and%20its%20application%20to%20seven%20MAPF%0Amethods%20reduces%20the%20memory%20usage%20and%20time%20cost%20significantly%2C%20particularly%20for%0Aserial%20methods.%20To%20facilitate%20further%20research%20within%20the%20community%2C%20we%20have%0Amade%20the%20source%20code%20of%20the%20proposed%20algorithm%20publicly%20available%0A%28https%3A//github.com/JoeYao-bit/LayeredMAPF%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12773v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayeredMAPF%3A%20a%20decomposition%20of%20MAPF%20instance%20without%20compromising%0A%20%20solvability&entry.906535625=Zhuo%20Yao%20and%20Wei%20Wang&entry.1292438233=%20%20Generally%2C%20the%20calculation%20and%20memory%20space%20required%20for%20multi-agent%20path%0Afinding%20%28MAPF%29%20grows%20exponentially%20as%20the%20number%20of%20agents%20increases.%20This%0Aoften%20results%20in%20some%20MAPF%20instances%20being%20unsolvable%20under%20limited%0Acomputational%20resources%20and%20memory%20space%2C%20thereby%20limiting%20the%20application%20of%0AMAPF%20in%20complex%20scenarios.%20Hence%2C%20we%20propose%20a%20decomposition%20approach%20for%20MAPF%0Ainstances%2C%20which%20breaks%20down%20instances%20involving%20a%20large%20number%20of%20agents%20into%0Amultiple%20isolated%20subproblems%20involving%20fewer%20agents.%20Moreover%2C%20we%20present%20a%0Aframework%20to%20enable%20general%20MAPF%20algorithms%20to%20solve%20each%20subproblem%0Aindependently%20and%20merge%20their%20solutions%20into%20one%20conflict-free%20final%20solution%2C%0Awithout%20compromising%20on%20solvability.%20Unlike%20existing%20works%20that%20propose%0Aisolated%20methods%20aimed%20at%20reducing%20the%20time%20cost%20of%20MAPF%2C%20our%20method%20is%0Aapplicable%20to%20all%20MAPF%20methods.%20In%20our%20results%2C%20we%20apply%20decomposition%20to%0Amultiple%20state-of-the-art%20MAPF%20methods%20using%20a%20classic%20MAPF%20benchmark%0A%28https%3A//movingai.com/benchmarks/mapf.html%29.%20The%20decomposition%20of%20MAPF%0Ainstances%20is%20completed%20on%20average%20within%201s%2C%20and%20its%20application%20to%20seven%20MAPF%0Amethods%20reduces%20the%20memory%20usage%20and%20time%20cost%20significantly%2C%20particularly%20for%0Aserial%20methods.%20To%20facilitate%20further%20research%20within%20the%20community%2C%20we%20have%0Amade%20the%20source%20code%20of%20the%20proposed%20algorithm%20publicly%20available%0A%28https%3A//github.com/JoeYao-bit/LayeredMAPF%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12773v1&entry.124074799=Read"},
{"title": "Using Graph Neural Networks to Predict Local Culture", "author": "Thiago H Silva and Daniel Silver", "abstract": "  Urban research has long recognized that neighbourhoods are dynamic and\nrelational. However, lack of data, methodologies, and computer processing power\nhave hampered a formal quantitative examination of neighbourhood relational\ndynamics. To make progress on this issue, this study proposes a graph neural\nnetwork (GNN) approach that permits combining and evaluating multiple sources\nof information about internal characteristics of neighbourhoods, their past\ncharacteristics, and flows of groups among them, potentially providing greater\nexpressive power in predictive models. By exploring a public large-scale\ndataset from Yelp, we show the potential of our approach for considering\nstructural connectedness in predicting neighbourhood attributes, specifically\nto predict local culture. Results are promising from a substantive and\nmethodologically point of view. Substantively, we find that either local area\ninformation (e.g. area demographics) or group profiles (tastes of Yelp\nreviewers) give the best results in predicting local culture, and they are\nnearly equivalent in all studied cases. Methodologically, exploring group\nprofiles could be a helpful alternative where finding local information for\nspecific areas is challenging, since they can be extracted automatically from\nmany forms of online data. Thus, our approach could empower researchers and\npolicy-makers to use a range of data sources when other local area information\nis lacking.\n", "link": "http://arxiv.org/abs/2402.17905v2", "date": "2024-04-19", "relevancy": 1.7638, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4388}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4388}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20Graph%20Neural%20Networks%20to%20Predict%20Local%20Culture&body=Title%3A%20Using%20Graph%20Neural%20Networks%20to%20Predict%20Local%20Culture%0AAuthor%3A%20Thiago%20H%20Silva%20and%20Daniel%20Silver%0AAbstract%3A%20%20%20Urban%20research%20has%20long%20recognized%20that%20neighbourhoods%20are%20dynamic%20and%0Arelational.%20However%2C%20lack%20of%20data%2C%20methodologies%2C%20and%20computer%20processing%20power%0Ahave%20hampered%20a%20formal%20quantitative%20examination%20of%20neighbourhood%20relational%0Adynamics.%20To%20make%20progress%20on%20this%20issue%2C%20this%20study%20proposes%20a%20graph%20neural%0Anetwork%20%28GNN%29%20approach%20that%20permits%20combining%20and%20evaluating%20multiple%20sources%0Aof%20information%20about%20internal%20characteristics%20of%20neighbourhoods%2C%20their%20past%0Acharacteristics%2C%20and%20flows%20of%20groups%20among%20them%2C%20potentially%20providing%20greater%0Aexpressive%20power%20in%20predictive%20models.%20By%20exploring%20a%20public%20large-scale%0Adataset%20from%20Yelp%2C%20we%20show%20the%20potential%20of%20our%20approach%20for%20considering%0Astructural%20connectedness%20in%20predicting%20neighbourhood%20attributes%2C%20specifically%0Ato%20predict%20local%20culture.%20Results%20are%20promising%20from%20a%20substantive%20and%0Amethodologically%20point%20of%20view.%20Substantively%2C%20we%20find%20that%20either%20local%20area%0Ainformation%20%28e.g.%20area%20demographics%29%20or%20group%20profiles%20%28tastes%20of%20Yelp%0Areviewers%29%20give%20the%20best%20results%20in%20predicting%20local%20culture%2C%20and%20they%20are%0Anearly%20equivalent%20in%20all%20studied%20cases.%20Methodologically%2C%20exploring%20group%0Aprofiles%20could%20be%20a%20helpful%20alternative%20where%20finding%20local%20information%20for%0Aspecific%20areas%20is%20challenging%2C%20since%20they%20can%20be%20extracted%20automatically%20from%0Amany%20forms%20of%20online%20data.%20Thus%2C%20our%20approach%20could%20empower%20researchers%20and%0Apolicy-makers%20to%20use%20a%20range%20of%20data%20sources%20when%20other%20local%20area%20information%0Ais%20lacking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17905v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Graph%20Neural%20Networks%20to%20Predict%20Local%20Culture&entry.906535625=Thiago%20H%20Silva%20and%20Daniel%20Silver&entry.1292438233=%20%20Urban%20research%20has%20long%20recognized%20that%20neighbourhoods%20are%20dynamic%20and%0Arelational.%20However%2C%20lack%20of%20data%2C%20methodologies%2C%20and%20computer%20processing%20power%0Ahave%20hampered%20a%20formal%20quantitative%20examination%20of%20neighbourhood%20relational%0Adynamics.%20To%20make%20progress%20on%20this%20issue%2C%20this%20study%20proposes%20a%20graph%20neural%0Anetwork%20%28GNN%29%20approach%20that%20permits%20combining%20and%20evaluating%20multiple%20sources%0Aof%20information%20about%20internal%20characteristics%20of%20neighbourhoods%2C%20their%20past%0Acharacteristics%2C%20and%20flows%20of%20groups%20among%20them%2C%20potentially%20providing%20greater%0Aexpressive%20power%20in%20predictive%20models.%20By%20exploring%20a%20public%20large-scale%0Adataset%20from%20Yelp%2C%20we%20show%20the%20potential%20of%20our%20approach%20for%20considering%0Astructural%20connectedness%20in%20predicting%20neighbourhood%20attributes%2C%20specifically%0Ato%20predict%20local%20culture.%20Results%20are%20promising%20from%20a%20substantive%20and%0Amethodologically%20point%20of%20view.%20Substantively%2C%20we%20find%20that%20either%20local%20area%0Ainformation%20%28e.g.%20area%20demographics%29%20or%20group%20profiles%20%28tastes%20of%20Yelp%0Areviewers%29%20give%20the%20best%20results%20in%20predicting%20local%20culture%2C%20and%20they%20are%0Anearly%20equivalent%20in%20all%20studied%20cases.%20Methodologically%2C%20exploring%20group%0Aprofiles%20could%20be%20a%20helpful%20alternative%20where%20finding%20local%20information%20for%0Aspecific%20areas%20is%20challenging%2C%20since%20they%20can%20be%20extracted%20automatically%20from%0Amany%20forms%20of%20online%20data.%20Thus%2C%20our%20approach%20could%20empower%20researchers%20and%0Apolicy-makers%20to%20use%20a%20range%20of%20data%20sources%20when%20other%20local%20area%20information%0Ais%20lacking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17905v2&entry.124074799=Read"},
{"title": "Purposer: Putting Human Motion Generation in Context", "author": "Nicolas Ugrinovic and Thomas Lucas and Fabien Baradel and Philippe Weinzaepfel and Gregory Rogez and Francesc Moreno-Noguer", "abstract": "  We present a novel method to generate human motion to populate 3D indoor\nscenes. It can be controlled with various combinations of conditioning signals\nsuch as a path in a scene, target poses, past motions, and scenes represented\nas 3D point clouds. State-of-the-art methods are either models specialized to\none single setting, require vast amounts of high-quality and diverse training\ndata, or are unconditional models that do not integrate scene or other\ncontextual information. As a consequence, they have limited applicability and\nrely on costly training data. To address these limitations, we propose a new\nmethod ,dubbed Purposer, based on neural discrete representation learning. Our\nmodel is capable of exploiting, in a flexible manner, different types of\ninformation already present in open access large-scale datasets such as AMASS.\nFirst, we encode unconditional human motion into a discrete latent space.\nSecond, an autoregressive generative model, conditioned with key contextual\ninformation, either with prompting or additive tokens, and trained for\nnext-step prediction in this space, synthesizes sequences of latent indices. We\nfurther design a novel conditioning block to handle future conditioning\ninformation in such a causal model by using a network with two branches to\ncompute separate stacks of features. In this manner, Purposer can generate\nrealistic motion sequences in diverse test scenes. Through exhaustive\nevaluation, we demonstrate that our multi-contextual solution outperforms\nexisting specialized approaches for specific contextual information, both in\nterms of quality and diversity. Our model is trained with short sequences, but\na byproduct of being able to use various conditioning signals is that at test\ntime different combinations can be used to chain short sequences together and\ngenerate long motions within a context scene.\n", "link": "http://arxiv.org/abs/2404.12942v1", "date": "2024-04-19", "relevancy": 1.763, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5968}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5785}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5741}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Purposer%3A%20Putting%20Human%20Motion%20Generation%20in%20Context&body=Title%3A%20Purposer%3A%20Putting%20Human%20Motion%20Generation%20in%20Context%0AAuthor%3A%20Nicolas%20Ugrinovic%20and%20Thomas%20Lucas%20and%20Fabien%20Baradel%20and%20Philippe%20Weinzaepfel%20and%20Gregory%20Rogez%20and%20Francesc%20Moreno-Noguer%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20to%20generate%20human%20motion%20to%20populate%203D%20indoor%0Ascenes.%20It%20can%20be%20controlled%20with%20various%20combinations%20of%20conditioning%20signals%0Asuch%20as%20a%20path%20in%20a%20scene%2C%20target%20poses%2C%20past%20motions%2C%20and%20scenes%20represented%0Aas%203D%20point%20clouds.%20State-of-the-art%20methods%20are%20either%20models%20specialized%20to%0Aone%20single%20setting%2C%20require%20vast%20amounts%20of%20high-quality%20and%20diverse%20training%0Adata%2C%20or%20are%20unconditional%20models%20that%20do%20not%20integrate%20scene%20or%20other%0Acontextual%20information.%20As%20a%20consequence%2C%20they%20have%20limited%20applicability%20and%0Arely%20on%20costly%20training%20data.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20new%0Amethod%20%2Cdubbed%20Purposer%2C%20based%20on%20neural%20discrete%20representation%20learning.%20Our%0Amodel%20is%20capable%20of%20exploiting%2C%20in%20a%20flexible%20manner%2C%20different%20types%20of%0Ainformation%20already%20present%20in%20open%20access%20large-scale%20datasets%20such%20as%20AMASS.%0AFirst%2C%20we%20encode%20unconditional%20human%20motion%20into%20a%20discrete%20latent%20space.%0ASecond%2C%20an%20autoregressive%20generative%20model%2C%20conditioned%20with%20key%20contextual%0Ainformation%2C%20either%20with%20prompting%20or%20additive%20tokens%2C%20and%20trained%20for%0Anext-step%20prediction%20in%20this%20space%2C%20synthesizes%20sequences%20of%20latent%20indices.%20We%0Afurther%20design%20a%20novel%20conditioning%20block%20to%20handle%20future%20conditioning%0Ainformation%20in%20such%20a%20causal%20model%20by%20using%20a%20network%20with%20two%20branches%20to%0Acompute%20separate%20stacks%20of%20features.%20In%20this%20manner%2C%20Purposer%20can%20generate%0Arealistic%20motion%20sequences%20in%20diverse%20test%20scenes.%20Through%20exhaustive%0Aevaluation%2C%20we%20demonstrate%20that%20our%20multi-contextual%20solution%20outperforms%0Aexisting%20specialized%20approaches%20for%20specific%20contextual%20information%2C%20both%20in%0Aterms%20of%20quality%20and%20diversity.%20Our%20model%20is%20trained%20with%20short%20sequences%2C%20but%0Aa%20byproduct%20of%20being%20able%20to%20use%20various%20conditioning%20signals%20is%20that%20at%20test%0Atime%20different%20combinations%20can%20be%20used%20to%20chain%20short%20sequences%20together%20and%0Agenerate%20long%20motions%20within%20a%20context%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12942v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Purposer%3A%20Putting%20Human%20Motion%20Generation%20in%20Context&entry.906535625=Nicolas%20Ugrinovic%20and%20Thomas%20Lucas%20and%20Fabien%20Baradel%20and%20Philippe%20Weinzaepfel%20and%20Gregory%20Rogez%20and%20Francesc%20Moreno-Noguer&entry.1292438233=%20%20We%20present%20a%20novel%20method%20to%20generate%20human%20motion%20to%20populate%203D%20indoor%0Ascenes.%20It%20can%20be%20controlled%20with%20various%20combinations%20of%20conditioning%20signals%0Asuch%20as%20a%20path%20in%20a%20scene%2C%20target%20poses%2C%20past%20motions%2C%20and%20scenes%20represented%0Aas%203D%20point%20clouds.%20State-of-the-art%20methods%20are%20either%20models%20specialized%20to%0Aone%20single%20setting%2C%20require%20vast%20amounts%20of%20high-quality%20and%20diverse%20training%0Adata%2C%20or%20are%20unconditional%20models%20that%20do%20not%20integrate%20scene%20or%20other%0Acontextual%20information.%20As%20a%20consequence%2C%20they%20have%20limited%20applicability%20and%0Arely%20on%20costly%20training%20data.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20new%0Amethod%20%2Cdubbed%20Purposer%2C%20based%20on%20neural%20discrete%20representation%20learning.%20Our%0Amodel%20is%20capable%20of%20exploiting%2C%20in%20a%20flexible%20manner%2C%20different%20types%20of%0Ainformation%20already%20present%20in%20open%20access%20large-scale%20datasets%20such%20as%20AMASS.%0AFirst%2C%20we%20encode%20unconditional%20human%20motion%20into%20a%20discrete%20latent%20space.%0ASecond%2C%20an%20autoregressive%20generative%20model%2C%20conditioned%20with%20key%20contextual%0Ainformation%2C%20either%20with%20prompting%20or%20additive%20tokens%2C%20and%20trained%20for%0Anext-step%20prediction%20in%20this%20space%2C%20synthesizes%20sequences%20of%20latent%20indices.%20We%0Afurther%20design%20a%20novel%20conditioning%20block%20to%20handle%20future%20conditioning%0Ainformation%20in%20such%20a%20causal%20model%20by%20using%20a%20network%20with%20two%20branches%20to%0Acompute%20separate%20stacks%20of%20features.%20In%20this%20manner%2C%20Purposer%20can%20generate%0Arealistic%20motion%20sequences%20in%20diverse%20test%20scenes.%20Through%20exhaustive%0Aevaluation%2C%20we%20demonstrate%20that%20our%20multi-contextual%20solution%20outperforms%0Aexisting%20specialized%20approaches%20for%20specific%20contextual%20information%2C%20both%20in%0Aterms%20of%20quality%20and%20diversity.%20Our%20model%20is%20trained%20with%20short%20sequences%2C%20but%0Aa%20byproduct%20of%20being%20able%20to%20use%20various%20conditioning%20signals%20is%20that%20at%20test%0Atime%20different%20combinations%20can%20be%20used%20to%20chain%20short%20sequences%20together%20and%0Agenerate%20long%20motions%20within%20a%20context%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12942v1&entry.124074799=Read"},
{"title": "Quantum Normalizing Flows for Anomaly Detection", "author": "Bodo Rosenhahn and Christoph Hirche", "abstract": "  A Normalizing Flow computes a bijective mapping from an arbitrary\ndistribution to a predefined (e.g. normal) distribution. Such a flow can be\nused to address different tasks, e.g. anomaly detection, once such a mapping\nhas been learned. In this work we introduce Normalizing Flows for Quantum\narchitectures, describe how to model and optimize such a flow and evaluate our\nmethod on example datasets. Our proposed models show competitive performance\nfor anomaly detection compared to classical methods, esp. those ones where\nthere are already quantum inspired algorithms available. In the experiments we\ncompare our performance to isolation forests (IF), the local outlier factor\n(LOF) or single-class SVMs.\n", "link": "http://arxiv.org/abs/2402.02866v2", "date": "2024-04-19", "relevancy": 1.7363, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5024}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4212}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4196}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Quantum%20Normalizing%20Flows%20for%20Anomaly%20Detection&body=Title%3A%20Quantum%20Normalizing%20Flows%20for%20Anomaly%20Detection%0AAuthor%3A%20Bodo%20Rosenhahn%20and%20Christoph%20Hirche%0AAbstract%3A%20%20%20A%20Normalizing%20Flow%20computes%20a%20bijective%20mapping%20from%20an%20arbitrary%0Adistribution%20to%20a%20predefined%20%28e.g.%20normal%29%20distribution.%20Such%20a%20flow%20can%20be%0Aused%20to%20address%20different%20tasks%2C%20e.g.%20anomaly%20detection%2C%20once%20such%20a%20mapping%0Ahas%20been%20learned.%20In%20this%20work%20we%20introduce%20Normalizing%20Flows%20for%20Quantum%0Aarchitectures%2C%20describe%20how%20to%20model%20and%20optimize%20such%20a%20flow%20and%20evaluate%20our%0Amethod%20on%20example%20datasets.%20Our%20proposed%20models%20show%20competitive%20performance%0Afor%20anomaly%20detection%20compared%20to%20classical%20methods%2C%20esp.%20those%20ones%20where%0Athere%20are%20already%20quantum%20inspired%20algorithms%20available.%20In%20the%20experiments%20we%0Acompare%20our%20performance%20to%20isolation%20forests%20%28IF%29%2C%20the%20local%20outlier%20factor%0A%28LOF%29%20or%20single-class%20SVMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02866v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Normalizing%20Flows%20for%20Anomaly%20Detection&entry.906535625=Bodo%20Rosenhahn%20and%20Christoph%20Hirche&entry.1292438233=%20%20A%20Normalizing%20Flow%20computes%20a%20bijective%20mapping%20from%20an%20arbitrary%0Adistribution%20to%20a%20predefined%20%28e.g.%20normal%29%20distribution.%20Such%20a%20flow%20can%20be%0Aused%20to%20address%20different%20tasks%2C%20e.g.%20anomaly%20detection%2C%20once%20such%20a%20mapping%0Ahas%20been%20learned.%20In%20this%20work%20we%20introduce%20Normalizing%20Flows%20for%20Quantum%0Aarchitectures%2C%20describe%20how%20to%20model%20and%20optimize%20such%20a%20flow%20and%20evaluate%20our%0Amethod%20on%20example%20datasets.%20Our%20proposed%20models%20show%20competitive%20performance%0Afor%20anomaly%20detection%20compared%20to%20classical%20methods%2C%20esp.%20those%20ones%20where%0Athere%20are%20already%20quantum%20inspired%20algorithms%20available.%20In%20the%20experiments%20we%0Acompare%20our%20performance%20to%20isolation%20forests%20%28IF%29%2C%20the%20local%20outlier%20factor%0A%28LOF%29%20or%20single-class%20SVMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02866v2&entry.124074799=Read"},
{"title": "MAexp: A Generic Platform for RL-based Multi-Agent Exploration", "author": "Shaohao Zhu and Jiacheng Zhou and Anjun Chen and Mingming Bai and Jiming Chen and Jinming Xu", "abstract": "  The sim-to-real gap poses a significant challenge in RL-based multi-agent\nexploration due to scene quantization and action discretization. Existing\nplatforms suffer from the inefficiency in sampling and the lack of diversity in\nMulti-Agent Reinforcement Learning (MARL) algorithms across different\nscenarios, restraining their widespread applications. To fill these gaps, we\npropose MAexp, a generic platform for multi-agent exploration that integrates a\nbroad range of state-of-the-art MARL algorithms and representative scenarios.\nMoreover, we employ point clouds to represent our exploration scenarios,\nleading to high-fidelity environment mapping and a sampling speed approximately\n40 times faster than existing platforms. Furthermore, equipped with an\nattention-based Multi-Agent Target Generator and a Single-Agent Motion Planner,\nMAexp can work with arbitrary numbers of agents and accommodate various types\nof robots. Extensive experiments are conducted to establish the first benchmark\nfeaturing several high-performance MARL algorithms across typical scenarios for\nrobots with continuous actions, which highlights the distinct strengths of each\nalgorithm in different scenarios.\n", "link": "http://arxiv.org/abs/2404.12824v1", "date": "2024-04-19", "relevancy": 1.7326, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6742}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5794}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5381}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MAexp%3A%20A%20Generic%20Platform%20for%20RL-based%20Multi-Agent%20Exploration&body=Title%3A%20MAexp%3A%20A%20Generic%20Platform%20for%20RL-based%20Multi-Agent%20Exploration%0AAuthor%3A%20Shaohao%20Zhu%20and%20Jiacheng%20Zhou%20and%20Anjun%20Chen%20and%20Mingming%20Bai%20and%20Jiming%20Chen%20and%20Jinming%20Xu%0AAbstract%3A%20%20%20The%20sim-to-real%20gap%20poses%20a%20significant%20challenge%20in%20RL-based%20multi-agent%0Aexploration%20due%20to%20scene%20quantization%20and%20action%20discretization.%20Existing%0Aplatforms%20suffer%20from%20the%20inefficiency%20in%20sampling%20and%20the%20lack%20of%20diversity%20in%0AMulti-Agent%20Reinforcement%20Learning%20%28MARL%29%20algorithms%20across%20different%0Ascenarios%2C%20restraining%20their%20widespread%20applications.%20To%20fill%20these%20gaps%2C%20we%0Apropose%20MAexp%2C%20a%20generic%20platform%20for%20multi-agent%20exploration%20that%20integrates%20a%0Abroad%20range%20of%20state-of-the-art%20MARL%20algorithms%20and%20representative%20scenarios.%0AMoreover%2C%20we%20employ%20point%20clouds%20to%20represent%20our%20exploration%20scenarios%2C%0Aleading%20to%20high-fidelity%20environment%20mapping%20and%20a%20sampling%20speed%20approximately%0A40%20times%20faster%20than%20existing%20platforms.%20Furthermore%2C%20equipped%20with%20an%0Aattention-based%20Multi-Agent%20Target%20Generator%20and%20a%20Single-Agent%20Motion%20Planner%2C%0AMAexp%20can%20work%20with%20arbitrary%20numbers%20of%20agents%20and%20accommodate%20various%20types%0Aof%20robots.%20Extensive%20experiments%20are%20conducted%20to%20establish%20the%20first%20benchmark%0Afeaturing%20several%20high-performance%20MARL%20algorithms%20across%20typical%20scenarios%20for%0Arobots%20with%20continuous%20actions%2C%20which%20highlights%20the%20distinct%20strengths%20of%20each%0Aalgorithm%20in%20different%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12824v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAexp%3A%20A%20Generic%20Platform%20for%20RL-based%20Multi-Agent%20Exploration&entry.906535625=Shaohao%20Zhu%20and%20Jiacheng%20Zhou%20and%20Anjun%20Chen%20and%20Mingming%20Bai%20and%20Jiming%20Chen%20and%20Jinming%20Xu&entry.1292438233=%20%20The%20sim-to-real%20gap%20poses%20a%20significant%20challenge%20in%20RL-based%20multi-agent%0Aexploration%20due%20to%20scene%20quantization%20and%20action%20discretization.%20Existing%0Aplatforms%20suffer%20from%20the%20inefficiency%20in%20sampling%20and%20the%20lack%20of%20diversity%20in%0AMulti-Agent%20Reinforcement%20Learning%20%28MARL%29%20algorithms%20across%20different%0Ascenarios%2C%20restraining%20their%20widespread%20applications.%20To%20fill%20these%20gaps%2C%20we%0Apropose%20MAexp%2C%20a%20generic%20platform%20for%20multi-agent%20exploration%20that%20integrates%20a%0Abroad%20range%20of%20state-of-the-art%20MARL%20algorithms%20and%20representative%20scenarios.%0AMoreover%2C%20we%20employ%20point%20clouds%20to%20represent%20our%20exploration%20scenarios%2C%0Aleading%20to%20high-fidelity%20environment%20mapping%20and%20a%20sampling%20speed%20approximately%0A40%20times%20faster%20than%20existing%20platforms.%20Furthermore%2C%20equipped%20with%20an%0Aattention-based%20Multi-Agent%20Target%20Generator%20and%20a%20Single-Agent%20Motion%20Planner%2C%0AMAexp%20can%20work%20with%20arbitrary%20numbers%20of%20agents%20and%20accommodate%20various%20types%0Aof%20robots.%20Extensive%20experiments%20are%20conducted%20to%20establish%20the%20first%20benchmark%0Afeaturing%20several%20high-performance%20MARL%20algorithms%20across%20typical%20scenarios%20for%0Arobots%20with%20continuous%20actions%2C%20which%20highlights%20the%20distinct%20strengths%20of%20each%0Aalgorithm%20in%20different%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12824v1&entry.124074799=Read"},
{"title": "Generative Modelling with High-Order Langevin Dynamics", "author": "Ziqiang Shi and Rujie Liu", "abstract": "  Diffusion generative modelling (DGM) based on stochastic\n  differential equations (SDEs) with\n  score matching has achieved unprecedented results in data\n  generation.\n  In this paper, we propose a novel fast high-quality\n  generative modelling method\n  based on high-order\n  Langevin dynamics (HOLD) with score matching.\n  This motive is proved by third-order\n  Langevin dynamics. By augmenting the\n  previous SDEs, e.g.\n  variance exploding or variance preserving SDEs\n  for single-data variable processes, HOLD can simultaneously\n  model position, velocity, and\n  acceleration, thereby improving the quality\n  and speed of the data\n  generation at the same time.\n  HOLD is composed of one Ornstein-Uhlenbeck process\n  and two Hamiltonians,\n  which reduce the mixing time by two orders of magnitude.\n  Empirical experiments for unconditional image generation on the\n  public data set CIFAR-10 and CelebA-HQ show that the effect is significant in\n  both Frechet inception distance (FID) and negative log-likelihood,\n  and achieves the\n  state-of-the-art FID of 1.85 on CIFAR-10.\n", "link": "http://arxiv.org/abs/2404.12814v1", "date": "2024-04-19", "relevancy": 1.725, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5786}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5712}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5698}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generative%20Modelling%20with%20High-Order%20Langevin%20Dynamics&body=Title%3A%20Generative%20Modelling%20with%20High-Order%20Langevin%20Dynamics%0AAuthor%3A%20Ziqiang%20Shi%20and%20Rujie%20Liu%0AAbstract%3A%20%20%20Diffusion%20generative%20modelling%20%28DGM%29%20based%20on%20stochastic%0A%20%20differential%20equations%20%28SDEs%29%20with%0A%20%20score%20matching%20has%20achieved%20unprecedented%20results%20in%20data%0A%20%20generation.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20fast%20high-quality%0A%20%20generative%20modelling%20method%0A%20%20based%20on%20high-order%0A%20%20Langevin%20dynamics%20%28HOLD%29%20with%20score%20matching.%0A%20%20This%20motive%20is%20proved%20by%20third-order%0A%20%20Langevin%20dynamics.%20By%20augmenting%20the%0A%20%20previous%20SDEs%2C%20e.g.%0A%20%20variance%20exploding%20or%20variance%20preserving%20SDEs%0A%20%20for%20single-data%20variable%20processes%2C%20HOLD%20can%20simultaneously%0A%20%20model%20position%2C%20velocity%2C%20and%0A%20%20acceleration%2C%20thereby%20improving%20the%20quality%0A%20%20and%20speed%20of%20the%20data%0A%20%20generation%20at%20the%20same%20time.%0A%20%20HOLD%20is%20composed%20of%20one%20Ornstein-Uhlenbeck%20process%0A%20%20and%20two%20Hamiltonians%2C%0A%20%20which%20reduce%20the%20mixing%20time%20by%20two%20orders%20of%20magnitude.%0A%20%20Empirical%20experiments%20for%20unconditional%20image%20generation%20on%20the%0A%20%20public%20data%20set%20CIFAR-10%20and%20CelebA-HQ%20show%20that%20the%20effect%20is%20significant%20in%0A%20%20both%20Frechet%20inception%20distance%20%28FID%29%20and%20negative%20log-likelihood%2C%0A%20%20and%20achieves%20the%0A%20%20state-of-the-art%20FID%20of%201.85%20on%20CIFAR-10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12814v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Modelling%20with%20High-Order%20Langevin%20Dynamics&entry.906535625=Ziqiang%20Shi%20and%20Rujie%20Liu&entry.1292438233=%20%20Diffusion%20generative%20modelling%20%28DGM%29%20based%20on%20stochastic%0A%20%20differential%20equations%20%28SDEs%29%20with%0A%20%20score%20matching%20has%20achieved%20unprecedented%20results%20in%20data%0A%20%20generation.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20fast%20high-quality%0A%20%20generative%20modelling%20method%0A%20%20based%20on%20high-order%0A%20%20Langevin%20dynamics%20%28HOLD%29%20with%20score%20matching.%0A%20%20This%20motive%20is%20proved%20by%20third-order%0A%20%20Langevin%20dynamics.%20By%20augmenting%20the%0A%20%20previous%20SDEs%2C%20e.g.%0A%20%20variance%20exploding%20or%20variance%20preserving%20SDEs%0A%20%20for%20single-data%20variable%20processes%2C%20HOLD%20can%20simultaneously%0A%20%20model%20position%2C%20velocity%2C%20and%0A%20%20acceleration%2C%20thereby%20improving%20the%20quality%0A%20%20and%20speed%20of%20the%20data%0A%20%20generation%20at%20the%20same%20time.%0A%20%20HOLD%20is%20composed%20of%20one%20Ornstein-Uhlenbeck%20process%0A%20%20and%20two%20Hamiltonians%2C%0A%20%20which%20reduce%20the%20mixing%20time%20by%20two%20orders%20of%20magnitude.%0A%20%20Empirical%20experiments%20for%20unconditional%20image%20generation%20on%20the%0A%20%20public%20data%20set%20CIFAR-10%20and%20CelebA-HQ%20show%20that%20the%20effect%20is%20significant%20in%0A%20%20both%20Frechet%20inception%20distance%20%28FID%29%20and%20negative%20log-likelihood%2C%0A%20%20and%20achieves%20the%0A%20%20state-of-the-art%20FID%20of%201.85%20on%20CIFAR-10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12814v1&entry.124074799=Read"},
{"title": "Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models", "author": "Konstantinos Vilouras and Pedro Sanchez and Alison Q. O'Neil and Sotirios A. Tsaftaris", "abstract": "  Localizing the exact pathological regions in a given medical scan is an\nimportant imaging problem that requires a large amount of bounding box ground\ntruth annotations to be accurately solved. However, there exist alternative,\npotentially weaker, forms of supervision, such as accompanying free-text\nreports, which are readily available. The task of performing localization with\ntextual guidance is commonly referred to as phrase grounding. In this work, we\nuse a publicly available Foundation Model, namely the Latent Diffusion Model,\nto solve this challenging task. This choice is supported by the fact that the\nLatent Diffusion Model, despite being generative in nature, contains mechanisms\n(cross-attention) that implicitly align visual and textual features, thus\nleading to intermediate representations that are suitable for the task at hand.\nIn addition, we aim to perform this task in a zero-shot manner, i.e., without\nany further training on target data, meaning that the model's weights remain\nfrozen. To this end, we devise strategies to select features and also refine\nthem via post-processing without extra learnable parameters. We compare our\nproposed method with state-of-the-art approaches which explicitly enforce\nimage-text alignment in a joint embedding space via contrastive learning.\nResults on a popular chest X-ray benchmark indicate that our method is\ncompetitive wih SOTA on different types of pathology, and even outperforms them\non average in terms of two metrics (mean IoU and AUC-ROC). Source code will be\nreleased upon acceptance.\n", "link": "http://arxiv.org/abs/2404.12920v1", "date": "2024-04-19", "relevancy": 1.7176, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6003}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5688}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.554}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Medical%20Phrase%20Grounding%20with%20Off-the-shelf%20Diffusion%20Models&body=Title%3A%20Zero-Shot%20Medical%20Phrase%20Grounding%20with%20Off-the-shelf%20Diffusion%20Models%0AAuthor%3A%20Konstantinos%20Vilouras%20and%20Pedro%20Sanchez%20and%20Alison%20Q.%20O%27Neil%20and%20Sotirios%20A.%20Tsaftaris%0AAbstract%3A%20%20%20Localizing%20the%20exact%20pathological%20regions%20in%20a%20given%20medical%20scan%20is%20an%0Aimportant%20imaging%20problem%20that%20requires%20a%20large%20amount%20of%20bounding%20box%20ground%0Atruth%20annotations%20to%20be%20accurately%20solved.%20However%2C%20there%20exist%20alternative%2C%0Apotentially%20weaker%2C%20forms%20of%20supervision%2C%20such%20as%20accompanying%20free-text%0Areports%2C%20which%20are%20readily%20available.%20The%20task%20of%20performing%20localization%20with%0Atextual%20guidance%20is%20commonly%20referred%20to%20as%20phrase%20grounding.%20In%20this%20work%2C%20we%0Ause%20a%20publicly%20available%20Foundation%20Model%2C%20namely%20the%20Latent%20Diffusion%20Model%2C%0Ato%20solve%20this%20challenging%20task.%20This%20choice%20is%20supported%20by%20the%20fact%20that%20the%0ALatent%20Diffusion%20Model%2C%20despite%20being%20generative%20in%20nature%2C%20contains%20mechanisms%0A%28cross-attention%29%20that%20implicitly%20align%20visual%20and%20textual%20features%2C%20thus%0Aleading%20to%20intermediate%20representations%20that%20are%20suitable%20for%20the%20task%20at%20hand.%0AIn%20addition%2C%20we%20aim%20to%20perform%20this%20task%20in%20a%20zero-shot%20manner%2C%20i.e.%2C%20without%0Aany%20further%20training%20on%20target%20data%2C%20meaning%20that%20the%20model%27s%20weights%20remain%0Afrozen.%20To%20this%20end%2C%20we%20devise%20strategies%20to%20select%20features%20and%20also%20refine%0Athem%20via%20post-processing%20without%20extra%20learnable%20parameters.%20We%20compare%20our%0Aproposed%20method%20with%20state-of-the-art%20approaches%20which%20explicitly%20enforce%0Aimage-text%20alignment%20in%20a%20joint%20embedding%20space%20via%20contrastive%20learning.%0AResults%20on%20a%20popular%20chest%20X-ray%20benchmark%20indicate%20that%20our%20method%20is%0Acompetitive%20wih%20SOTA%20on%20different%20types%20of%20pathology%2C%20and%20even%20outperforms%20them%0Aon%20average%20in%20terms%20of%20two%20metrics%20%28mean%20IoU%20and%20AUC-ROC%29.%20Source%20code%20will%20be%0Areleased%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12920v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Medical%20Phrase%20Grounding%20with%20Off-the-shelf%20Diffusion%20Models&entry.906535625=Konstantinos%20Vilouras%20and%20Pedro%20Sanchez%20and%20Alison%20Q.%20O%27Neil%20and%20Sotirios%20A.%20Tsaftaris&entry.1292438233=%20%20Localizing%20the%20exact%20pathological%20regions%20in%20a%20given%20medical%20scan%20is%20an%0Aimportant%20imaging%20problem%20that%20requires%20a%20large%20amount%20of%20bounding%20box%20ground%0Atruth%20annotations%20to%20be%20accurately%20solved.%20However%2C%20there%20exist%20alternative%2C%0Apotentially%20weaker%2C%20forms%20of%20supervision%2C%20such%20as%20accompanying%20free-text%0Areports%2C%20which%20are%20readily%20available.%20The%20task%20of%20performing%20localization%20with%0Atextual%20guidance%20is%20commonly%20referred%20to%20as%20phrase%20grounding.%20In%20this%20work%2C%20we%0Ause%20a%20publicly%20available%20Foundation%20Model%2C%20namely%20the%20Latent%20Diffusion%20Model%2C%0Ato%20solve%20this%20challenging%20task.%20This%20choice%20is%20supported%20by%20the%20fact%20that%20the%0ALatent%20Diffusion%20Model%2C%20despite%20being%20generative%20in%20nature%2C%20contains%20mechanisms%0A%28cross-attention%29%20that%20implicitly%20align%20visual%20and%20textual%20features%2C%20thus%0Aleading%20to%20intermediate%20representations%20that%20are%20suitable%20for%20the%20task%20at%20hand.%0AIn%20addition%2C%20we%20aim%20to%20perform%20this%20task%20in%20a%20zero-shot%20manner%2C%20i.e.%2C%20without%0Aany%20further%20training%20on%20target%20data%2C%20meaning%20that%20the%20model%27s%20weights%20remain%0Afrozen.%20To%20this%20end%2C%20we%20devise%20strategies%20to%20select%20features%20and%20also%20refine%0Athem%20via%20post-processing%20without%20extra%20learnable%20parameters.%20We%20compare%20our%0Aproposed%20method%20with%20state-of-the-art%20approaches%20which%20explicitly%20enforce%0Aimage-text%20alignment%20in%20a%20joint%20embedding%20space%20via%20contrastive%20learning.%0AResults%20on%20a%20popular%20chest%20X-ray%20benchmark%20indicate%20that%20our%20method%20is%0Acompetitive%20wih%20SOTA%20on%20different%20types%20of%20pathology%2C%20and%20even%20outperforms%20them%0Aon%20average%20in%20terms%20of%20two%20metrics%20%28mean%20IoU%20and%20AUC-ROC%29.%20Source%20code%20will%20be%0Areleased%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12920v1&entry.124074799=Read"},
{"title": "Cross-modal Diffusion Modelling for Super-resolved Spatial\n  Transcriptomics", "author": "Xiaofei Wang and Xingxu Huang and Stephen J. Price and Chao Li", "abstract": "  The recent advancement of spatial transcriptomics (ST) allows to characterize\nspatial gene expression within tissue for discovery research. However, current\nST platforms suffer from low resolution, hindering in-depth understanding of\nspatial gene expression. Super-resolution approaches promise to enhance ST maps\nby integrating histology images with gene expressions of profiled tissue spots.\nHowever, current super-resolution methods are limited by restoration\nuncertainty and mode collapse. Although diffusion models have shown promise in\ncapturing complex interactions between multi-modal conditions, it remains a\nchallenge to integrate histology images and gene expression for super-resolved\nST maps. This paper proposes a cross-modal conditional diffusion model for\nsuper-resolving ST maps with the guidance of histology images. Specifically, we\ndesign a multi-modal disentangling network with cross-modal adaptive modulation\nto utilize complementary information from histology images and spatial gene\nexpression. Moreover, we propose a dynamic cross-attention modelling strategy\nto extract hierarchical cell-to-tissue information from histology images.\nLastly, we propose a co-expression-based gene-correlation graph network to\nmodel the co-expression relationship of multiple genes. Experiments show that\nour method outperforms other state-of-the-art methods in ST super-resolution on\nthree public datasets.\n", "link": "http://arxiv.org/abs/2404.12973v1", "date": "2024-04-19", "relevancy": 1.7095, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6136}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5756}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.55}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-modal%20Diffusion%20Modelling%20for%20Super-resolved%20Spatial%0A%20%20Transcriptomics&body=Title%3A%20Cross-modal%20Diffusion%20Modelling%20for%20Super-resolved%20Spatial%0A%20%20Transcriptomics%0AAuthor%3A%20Xiaofei%20Wang%20and%20Xingxu%20Huang%20and%20Stephen%20J.%20Price%20and%20Chao%20Li%0AAbstract%3A%20%20%20The%20recent%20advancement%20of%20spatial%20transcriptomics%20%28ST%29%20allows%20to%20characterize%0Aspatial%20gene%20expression%20within%20tissue%20for%20discovery%20research.%20However%2C%20current%0AST%20platforms%20suffer%20from%20low%20resolution%2C%20hindering%20in-depth%20understanding%20of%0Aspatial%20gene%20expression.%20Super-resolution%20approaches%20promise%20to%20enhance%20ST%20maps%0Aby%20integrating%20histology%20images%20with%20gene%20expressions%20of%20profiled%20tissue%20spots.%0AHowever%2C%20current%20super-resolution%20methods%20are%20limited%20by%20restoration%0Auncertainty%20and%20mode%20collapse.%20Although%20diffusion%20models%20have%20shown%20promise%20in%0Acapturing%20complex%20interactions%20between%20multi-modal%20conditions%2C%20it%20remains%20a%0Achallenge%20to%20integrate%20histology%20images%20and%20gene%20expression%20for%20super-resolved%0AST%20maps.%20This%20paper%20proposes%20a%20cross-modal%20conditional%20diffusion%20model%20for%0Asuper-resolving%20ST%20maps%20with%20the%20guidance%20of%20histology%20images.%20Specifically%2C%20we%0Adesign%20a%20multi-modal%20disentangling%20network%20with%20cross-modal%20adaptive%20modulation%0Ato%20utilize%20complementary%20information%20from%20histology%20images%20and%20spatial%20gene%0Aexpression.%20Moreover%2C%20we%20propose%20a%20dynamic%20cross-attention%20modelling%20strategy%0Ato%20extract%20hierarchical%20cell-to-tissue%20information%20from%20histology%20images.%0ALastly%2C%20we%20propose%20a%20co-expression-based%20gene-correlation%20graph%20network%20to%0Amodel%20the%20co-expression%20relationship%20of%20multiple%20genes.%20Experiments%20show%20that%0Aour%20method%20outperforms%20other%20state-of-the-art%20methods%20in%20ST%20super-resolution%20on%0Athree%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12973v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modal%20Diffusion%20Modelling%20for%20Super-resolved%20Spatial%0A%20%20Transcriptomics&entry.906535625=Xiaofei%20Wang%20and%20Xingxu%20Huang%20and%20Stephen%20J.%20Price%20and%20Chao%20Li&entry.1292438233=%20%20The%20recent%20advancement%20of%20spatial%20transcriptomics%20%28ST%29%20allows%20to%20characterize%0Aspatial%20gene%20expression%20within%20tissue%20for%20discovery%20research.%20However%2C%20current%0AST%20platforms%20suffer%20from%20low%20resolution%2C%20hindering%20in-depth%20understanding%20of%0Aspatial%20gene%20expression.%20Super-resolution%20approaches%20promise%20to%20enhance%20ST%20maps%0Aby%20integrating%20histology%20images%20with%20gene%20expressions%20of%20profiled%20tissue%20spots.%0AHowever%2C%20current%20super-resolution%20methods%20are%20limited%20by%20restoration%0Auncertainty%20and%20mode%20collapse.%20Although%20diffusion%20models%20have%20shown%20promise%20in%0Acapturing%20complex%20interactions%20between%20multi-modal%20conditions%2C%20it%20remains%20a%0Achallenge%20to%20integrate%20histology%20images%20and%20gene%20expression%20for%20super-resolved%0AST%20maps.%20This%20paper%20proposes%20a%20cross-modal%20conditional%20diffusion%20model%20for%0Asuper-resolving%20ST%20maps%20with%20the%20guidance%20of%20histology%20images.%20Specifically%2C%20we%0Adesign%20a%20multi-modal%20disentangling%20network%20with%20cross-modal%20adaptive%20modulation%0Ato%20utilize%20complementary%20information%20from%20histology%20images%20and%20spatial%20gene%0Aexpression.%20Moreover%2C%20we%20propose%20a%20dynamic%20cross-attention%20modelling%20strategy%0Ato%20extract%20hierarchical%20cell-to-tissue%20information%20from%20histology%20images.%0ALastly%2C%20we%20propose%20a%20co-expression-based%20gene-correlation%20graph%20network%20to%0Amodel%20the%20co-expression%20relationship%20of%20multiple%20genes.%20Experiments%20show%20that%0Aour%20method%20outperforms%20other%20state-of-the-art%20methods%20in%20ST%20super-resolution%20on%0Athree%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12973v1&entry.124074799=Read"},
{"title": "RadRotator: 3D Rotation of Radiographs with Diffusion Models", "author": "Pouria Rouzrokh and Bardia Khosravi and Shahriar Faghani and Kellen L. Mulford and Michael J. Taunton and Bradley J. Erickson and Cody C. Wyles", "abstract": "  Transforming two-dimensional (2D) images into three-dimensional (3D) volumes\nis a well-known yet challenging problem for the computer vision community. In\nthe medical domain, a few previous studies attempted to convert two or more\ninput radiographs into computed tomography (CT) volumes. Following their\neffort, we introduce a diffusion model-based technology that can rotate the\nanatomical content of any input radiograph in 3D space, potentially enabling\nthe visualization of the entire anatomical content of the radiograph from any\nviewpoint in 3D. Similar to previous studies, we used CT volumes to create\nDigitally Reconstructed Radiographs (DRRs) as the training data for our model.\nHowever, we addressed two significant limitations encountered in previous\nstudies: 1. We utilized conditional diffusion models with classifier-free\nguidance instead of Generative Adversarial Networks (GANs) to achieve higher\nmode coverage and improved output image quality, with the only trade-off being\nslower inference time, which is often less critical in medical applications;\nand 2. We demonstrated that the unreliable output of style transfer deep\nlearning (DL) models, such as Cycle-GAN, to transfer the style of actual\nradiographs to DRRs could be replaced with a simple yet effective training\ntransformation that randomly changes the pixel intensity histograms of the\ninput and ground-truth imaging data during training. This transformation makes\nthe diffusion model agnostic to any distribution variations of the input data\npixel intensity, enabling the reliable training of a DL model on input DRRs and\napplying the exact same model to conventional radiographs (or DRRs) during\ninference.\n", "link": "http://arxiv.org/abs/2404.13000v1", "date": "2024-04-19", "relevancy": 1.7071, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6157}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5875}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.543}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RadRotator%3A%203D%20Rotation%20of%20Radiographs%20with%20Diffusion%20Models&body=Title%3A%20RadRotator%3A%203D%20Rotation%20of%20Radiographs%20with%20Diffusion%20Models%0AAuthor%3A%20Pouria%20Rouzrokh%20and%20Bardia%20Khosravi%20and%20Shahriar%20Faghani%20and%20Kellen%20L.%20Mulford%20and%20Michael%20J.%20Taunton%20and%20Bradley%20J.%20Erickson%20and%20Cody%20C.%20Wyles%0AAbstract%3A%20%20%20Transforming%20two-dimensional%20%282D%29%20images%20into%20three-dimensional%20%283D%29%20volumes%0Ais%20a%20well-known%20yet%20challenging%20problem%20for%20the%20computer%20vision%20community.%20In%0Athe%20medical%20domain%2C%20a%20few%20previous%20studies%20attempted%20to%20convert%20two%20or%20more%0Ainput%20radiographs%20into%20computed%20tomography%20%28CT%29%20volumes.%20Following%20their%0Aeffort%2C%20we%20introduce%20a%20diffusion%20model-based%20technology%20that%20can%20rotate%20the%0Aanatomical%20content%20of%20any%20input%20radiograph%20in%203D%20space%2C%20potentially%20enabling%0Athe%20visualization%20of%20the%20entire%20anatomical%20content%20of%20the%20radiograph%20from%20any%0Aviewpoint%20in%203D.%20Similar%20to%20previous%20studies%2C%20we%20used%20CT%20volumes%20to%20create%0ADigitally%20Reconstructed%20Radiographs%20%28DRRs%29%20as%20the%20training%20data%20for%20our%20model.%0AHowever%2C%20we%20addressed%20two%20significant%20limitations%20encountered%20in%20previous%0Astudies%3A%201.%20We%20utilized%20conditional%20diffusion%20models%20with%20classifier-free%0Aguidance%20instead%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20to%20achieve%20higher%0Amode%20coverage%20and%20improved%20output%20image%20quality%2C%20with%20the%20only%20trade-off%20being%0Aslower%20inference%20time%2C%20which%20is%20often%20less%20critical%20in%20medical%20applications%3B%0Aand%202.%20We%20demonstrated%20that%20the%20unreliable%20output%20of%20style%20transfer%20deep%0Alearning%20%28DL%29%20models%2C%20such%20as%20Cycle-GAN%2C%20to%20transfer%20the%20style%20of%20actual%0Aradiographs%20to%20DRRs%20could%20be%20replaced%20with%20a%20simple%20yet%20effective%20training%0Atransformation%20that%20randomly%20changes%20the%20pixel%20intensity%20histograms%20of%20the%0Ainput%20and%20ground-truth%20imaging%20data%20during%20training.%20This%20transformation%20makes%0Athe%20diffusion%20model%20agnostic%20to%20any%20distribution%20variations%20of%20the%20input%20data%0Apixel%20intensity%2C%20enabling%20the%20reliable%20training%20of%20a%20DL%20model%20on%20input%20DRRs%20and%0Aapplying%20the%20exact%20same%20model%20to%20conventional%20radiographs%20%28or%20DRRs%29%20during%0Ainference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13000v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadRotator%3A%203D%20Rotation%20of%20Radiographs%20with%20Diffusion%20Models&entry.906535625=Pouria%20Rouzrokh%20and%20Bardia%20Khosravi%20and%20Shahriar%20Faghani%20and%20Kellen%20L.%20Mulford%20and%20Michael%20J.%20Taunton%20and%20Bradley%20J.%20Erickson%20and%20Cody%20C.%20Wyles&entry.1292438233=%20%20Transforming%20two-dimensional%20%282D%29%20images%20into%20three-dimensional%20%283D%29%20volumes%0Ais%20a%20well-known%20yet%20challenging%20problem%20for%20the%20computer%20vision%20community.%20In%0Athe%20medical%20domain%2C%20a%20few%20previous%20studies%20attempted%20to%20convert%20two%20or%20more%0Ainput%20radiographs%20into%20computed%20tomography%20%28CT%29%20volumes.%20Following%20their%0Aeffort%2C%20we%20introduce%20a%20diffusion%20model-based%20technology%20that%20can%20rotate%20the%0Aanatomical%20content%20of%20any%20input%20radiograph%20in%203D%20space%2C%20potentially%20enabling%0Athe%20visualization%20of%20the%20entire%20anatomical%20content%20of%20the%20radiograph%20from%20any%0Aviewpoint%20in%203D.%20Similar%20to%20previous%20studies%2C%20we%20used%20CT%20volumes%20to%20create%0ADigitally%20Reconstructed%20Radiographs%20%28DRRs%29%20as%20the%20training%20data%20for%20our%20model.%0AHowever%2C%20we%20addressed%20two%20significant%20limitations%20encountered%20in%20previous%0Astudies%3A%201.%20We%20utilized%20conditional%20diffusion%20models%20with%20classifier-free%0Aguidance%20instead%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20to%20achieve%20higher%0Amode%20coverage%20and%20improved%20output%20image%20quality%2C%20with%20the%20only%20trade-off%20being%0Aslower%20inference%20time%2C%20which%20is%20often%20less%20critical%20in%20medical%20applications%3B%0Aand%202.%20We%20demonstrated%20that%20the%20unreliable%20output%20of%20style%20transfer%20deep%0Alearning%20%28DL%29%20models%2C%20such%20as%20Cycle-GAN%2C%20to%20transfer%20the%20style%20of%20actual%0Aradiographs%20to%20DRRs%20could%20be%20replaced%20with%20a%20simple%20yet%20effective%20training%0Atransformation%20that%20randomly%20changes%20the%20pixel%20intensity%20histograms%20of%20the%0Ainput%20and%20ground-truth%20imaging%20data%20during%20training.%20This%20transformation%20makes%0Athe%20diffusion%20model%20agnostic%20to%20any%20distribution%20variations%20of%20the%20input%20data%0Apixel%20intensity%2C%20enabling%20the%20reliable%20training%20of%20a%20DL%20model%20on%20input%20DRRs%20and%0Aapplying%20the%20exact%20same%20model%20to%20conventional%20radiographs%20%28or%20DRRs%29%20during%0Ainference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13000v1&entry.124074799=Read"},
{"title": "EATFormer: Improving Vision Transformer Inspired by Evolutionary\n  Algorithm", "author": "Jiangning Zhang and Xiangtai Li and Yabiao Wang and Chengjie Wang and Yibo Yang and Yong Liu and Dacheng Tao", "abstract": "  Motivated by biological evolution, this paper explains the rationality of\nVision Transformer by analogy with the proven practical Evolutionary Algorithm\n(EA) and derives that both have consistent mathematical formulation. Then\ninspired by effective EA variants, we propose a novel pyramid EATFormer\nbackbone that only contains the proposed \\emph{EA-based Transformer} (EAT)\nblock, which consists of three residual parts, i.e., \\emph{Multi-Scale Region\nAggregation} (MSRA), \\emph{Global and Local Interaction} (GLI), and\n\\emph{Feed-Forward Network} (FFN) modules, to model multi-scale, interactive,\nand individual information separately. Moreover, we design a \\emph{Task-Related\nHead} (TRH) docked with transformer backbone to complete final information\nfusion more flexibly and \\emph{improve} a \\emph{Modulated Deformable MSA}\n(MD-MSA) to dynamically model irregular locations. Massive quantitative and\nquantitative experiments on image classification, downstream tasks, and\nexplanatory experiments demonstrate the effectiveness and superiority of our\napproach over State-Of-The-Art (SOTA) methods. \\Eg, our Mobile (1.8M), Tiny\n(6.1M), Small (24.3M), and Base (49.0M) models achieve 69.4, 78.4, 83.1, and\n83.9 Top-1 only trained on ImageNet-1K with naive training recipe;\nEATFormer-Tiny/Small/Base armed Mask-R-CNN obtain 45.4/47.4/49.0 box AP and\n41.4/42.9/44.2 mask AP on COCO detection, surpassing contemporary MPViT-T,\nSwin-T, and Swin-S by 0.6/1.4/0.5 box AP and 0.4/1.3/0.9 mask AP separately\nwith less FLOPs; Our EATFormer-Small/Base achieve 47.3/49.3 mIoU on ADE20K by\nUpernet that exceeds Swin-T/S by 2.8/1.7. Code is available at\n\\url{https://github.com/zhangzjn/EATFormer}.\n", "link": "http://arxiv.org/abs/2206.09325v2", "date": "2024-04-19", "relevancy": 1.6988, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6042}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5561}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5537}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EATFormer%3A%20Improving%20Vision%20Transformer%20Inspired%20by%20Evolutionary%0A%20%20Algorithm&body=Title%3A%20EATFormer%3A%20Improving%20Vision%20Transformer%20Inspired%20by%20Evolutionary%0A%20%20Algorithm%0AAuthor%3A%20Jiangning%20Zhang%20and%20Xiangtai%20Li%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Yibo%20Yang%20and%20Yong%20Liu%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Motivated%20by%20biological%20evolution%2C%20this%20paper%20explains%20the%20rationality%20of%0AVision%20Transformer%20by%20analogy%20with%20the%20proven%20practical%20Evolutionary%20Algorithm%0A%28EA%29%20and%20derives%20that%20both%20have%20consistent%20mathematical%20formulation.%20Then%0Ainspired%20by%20effective%20EA%20variants%2C%20we%20propose%20a%20novel%20pyramid%20EATFormer%0Abackbone%20that%20only%20contains%20the%20proposed%20%5Cemph%7BEA-based%20Transformer%7D%20%28EAT%29%0Ablock%2C%20which%20consists%20of%20three%20residual%20parts%2C%20i.e.%2C%20%5Cemph%7BMulti-Scale%20Region%0AAggregation%7D%20%28MSRA%29%2C%20%5Cemph%7BGlobal%20and%20Local%20Interaction%7D%20%28GLI%29%2C%20and%0A%5Cemph%7BFeed-Forward%20Network%7D%20%28FFN%29%20modules%2C%20to%20model%20multi-scale%2C%20interactive%2C%0Aand%20individual%20information%20separately.%20Moreover%2C%20we%20design%20a%20%5Cemph%7BTask-Related%0AHead%7D%20%28TRH%29%20docked%20with%20transformer%20backbone%20to%20complete%20final%20information%0Afusion%20more%20flexibly%20and%20%5Cemph%7Bimprove%7D%20a%20%5Cemph%7BModulated%20Deformable%20MSA%7D%0A%28MD-MSA%29%20to%20dynamically%20model%20irregular%20locations.%20Massive%20quantitative%20and%0Aquantitative%20experiments%20on%20image%20classification%2C%20downstream%20tasks%2C%20and%0Aexplanatory%20experiments%20demonstrate%20the%20effectiveness%20and%20superiority%20of%20our%0Aapproach%20over%20State-Of-The-Art%20%28SOTA%29%20methods.%20%5CEg%2C%20our%20Mobile%20%281.8M%29%2C%20Tiny%0A%286.1M%29%2C%20Small%20%2824.3M%29%2C%20and%20Base%20%2849.0M%29%20models%20achieve%2069.4%2C%2078.4%2C%2083.1%2C%20and%0A83.9%20Top-1%20only%20trained%20on%20ImageNet-1K%20with%20naive%20training%20recipe%3B%0AEATFormer-Tiny/Small/Base%20armed%20Mask-R-CNN%20obtain%2045.4/47.4/49.0%20box%20AP%20and%0A41.4/42.9/44.2%20mask%20AP%20on%20COCO%20detection%2C%20surpassing%20contemporary%20MPViT-T%2C%0ASwin-T%2C%20and%20Swin-S%20by%200.6/1.4/0.5%20box%20AP%20and%200.4/1.3/0.9%20mask%20AP%20separately%0Awith%20less%20FLOPs%3B%20Our%20EATFormer-Small/Base%20achieve%2047.3/49.3%20mIoU%20on%20ADE20K%20by%0AUpernet%20that%20exceeds%20Swin-T/S%20by%202.8/1.7.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zhangzjn/EATFormer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.09325v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EATFormer%3A%20Improving%20Vision%20Transformer%20Inspired%20by%20Evolutionary%0A%20%20Algorithm&entry.906535625=Jiangning%20Zhang%20and%20Xiangtai%20Li%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Yibo%20Yang%20and%20Yong%20Liu%20and%20Dacheng%20Tao&entry.1292438233=%20%20Motivated%20by%20biological%20evolution%2C%20this%20paper%20explains%20the%20rationality%20of%0AVision%20Transformer%20by%20analogy%20with%20the%20proven%20practical%20Evolutionary%20Algorithm%0A%28EA%29%20and%20derives%20that%20both%20have%20consistent%20mathematical%20formulation.%20Then%0Ainspired%20by%20effective%20EA%20variants%2C%20we%20propose%20a%20novel%20pyramid%20EATFormer%0Abackbone%20that%20only%20contains%20the%20proposed%20%5Cemph%7BEA-based%20Transformer%7D%20%28EAT%29%0Ablock%2C%20which%20consists%20of%20three%20residual%20parts%2C%20i.e.%2C%20%5Cemph%7BMulti-Scale%20Region%0AAggregation%7D%20%28MSRA%29%2C%20%5Cemph%7BGlobal%20and%20Local%20Interaction%7D%20%28GLI%29%2C%20and%0A%5Cemph%7BFeed-Forward%20Network%7D%20%28FFN%29%20modules%2C%20to%20model%20multi-scale%2C%20interactive%2C%0Aand%20individual%20information%20separately.%20Moreover%2C%20we%20design%20a%20%5Cemph%7BTask-Related%0AHead%7D%20%28TRH%29%20docked%20with%20transformer%20backbone%20to%20complete%20final%20information%0Afusion%20more%20flexibly%20and%20%5Cemph%7Bimprove%7D%20a%20%5Cemph%7BModulated%20Deformable%20MSA%7D%0A%28MD-MSA%29%20to%20dynamically%20model%20irregular%20locations.%20Massive%20quantitative%20and%0Aquantitative%20experiments%20on%20image%20classification%2C%20downstream%20tasks%2C%20and%0Aexplanatory%20experiments%20demonstrate%20the%20effectiveness%20and%20superiority%20of%20our%0Aapproach%20over%20State-Of-The-Art%20%28SOTA%29%20methods.%20%5CEg%2C%20our%20Mobile%20%281.8M%29%2C%20Tiny%0A%286.1M%29%2C%20Small%20%2824.3M%29%2C%20and%20Base%20%2849.0M%29%20models%20achieve%2069.4%2C%2078.4%2C%2083.1%2C%20and%0A83.9%20Top-1%20only%20trained%20on%20ImageNet-1K%20with%20naive%20training%20recipe%3B%0AEATFormer-Tiny/Small/Base%20armed%20Mask-R-CNN%20obtain%2045.4/47.4/49.0%20box%20AP%20and%0A41.4/42.9/44.2%20mask%20AP%20on%20COCO%20detection%2C%20surpassing%20contemporary%20MPViT-T%2C%0ASwin-T%2C%20and%20Swin-S%20by%200.6/1.4/0.5%20box%20AP%20and%200.4/1.3/0.9%20mask%20AP%20separately%0Awith%20less%20FLOPs%3B%20Our%20EATFormer-Small/Base%20achieve%2047.3/49.3%20mIoU%20on%20ADE20K%20by%0AUpernet%20that%20exceeds%20Swin-T/S%20by%202.8/1.7.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zhangzjn/EATFormer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.09325v2&entry.124074799=Read"},
{"title": "Feature Corrective Transfer Learning: End-to-End Solutions to Object\n  Detection in Non-Ideal Visual Conditions", "author": "Chuheng Wei and Guoyuan Wu and Matthew J. Barth", "abstract": "  A significant challenge in the field of object detection lies in the system's\nperformance under non-ideal imaging conditions, such as rain, fog, low\nillumination, or raw Bayer images that lack ISP processing. Our study\nintroduces \"Feature Corrective Transfer Learning\", a novel approach that\nleverages transfer learning and a bespoke loss function to facilitate the\nend-to-end detection of objects in these challenging scenarios without the need\nto convert non-ideal images into their RGB counterparts. In our methodology, we\ninitially train a comprehensive model on a pristine RGB image dataset.\nSubsequently, non-ideal images are processed by comparing their feature maps\nagainst those from the initial ideal RGB model. This comparison employs the\nExtended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss function\ndesigned to quantify similarities and integrate them into the detection loss.\nThis approach refines the model's ability to perform object detection across\nvarying conditions through direct feature map correction, encapsulating the\nessence of Feature Corrective Transfer Learning. Experimental validation on\nvariants of the KITTI dataset demonstrates a significant improvement in mean\nAverage Precision (mAP), resulting in a 3.8-8.1% relative enhancement in\ndetection under non-ideal conditions compared to the baseline model, and a less\nmarginal performance difference within 1.3% of the mAP@[0.5:0.95] achieved\nunder ideal conditions by the standard Faster RCNN algorithm.\n", "link": "http://arxiv.org/abs/2404.11214v2", "date": "2024-04-19", "relevancy": 1.6785, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.571}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5455}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Feature%20Corrective%20Transfer%20Learning%3A%20End-to-End%20Solutions%20to%20Object%0A%20%20Detection%20in%20Non-Ideal%20Visual%20Conditions&body=Title%3A%20Feature%20Corrective%20Transfer%20Learning%3A%20End-to-End%20Solutions%20to%20Object%0A%20%20Detection%20in%20Non-Ideal%20Visual%20Conditions%0AAuthor%3A%20Chuheng%20Wei%20and%20Guoyuan%20Wu%20and%20Matthew%20J.%20Barth%0AAbstract%3A%20%20%20A%20significant%20challenge%20in%20the%20field%20of%20object%20detection%20lies%20in%20the%20system%27s%0Aperformance%20under%20non-ideal%20imaging%20conditions%2C%20such%20as%20rain%2C%20fog%2C%20low%0Aillumination%2C%20or%20raw%20Bayer%20images%20that%20lack%20ISP%20processing.%20Our%20study%0Aintroduces%20%22Feature%20Corrective%20Transfer%20Learning%22%2C%20a%20novel%20approach%20that%0Aleverages%20transfer%20learning%20and%20a%20bespoke%20loss%20function%20to%20facilitate%20the%0Aend-to-end%20detection%20of%20objects%20in%20these%20challenging%20scenarios%20without%20the%20need%0Ato%20convert%20non-ideal%20images%20into%20their%20RGB%20counterparts.%20In%20our%20methodology%2C%20we%0Ainitially%20train%20a%20comprehensive%20model%20on%20a%20pristine%20RGB%20image%20dataset.%0ASubsequently%2C%20non-ideal%20images%20are%20processed%20by%20comparing%20their%20feature%20maps%0Aagainst%20those%20from%20the%20initial%20ideal%20RGB%20model.%20This%20comparison%20employs%20the%0AExtended%20Area%20Novel%20Structural%20Discrepancy%20Loss%20%28EANSDL%29%2C%20a%20novel%20loss%20function%0Adesigned%20to%20quantify%20similarities%20and%20integrate%20them%20into%20the%20detection%20loss.%0AThis%20approach%20refines%20the%20model%27s%20ability%20to%20perform%20object%20detection%20across%0Avarying%20conditions%20through%20direct%20feature%20map%20correction%2C%20encapsulating%20the%0Aessence%20of%20Feature%20Corrective%20Transfer%20Learning.%20Experimental%20validation%20on%0Avariants%20of%20the%20KITTI%20dataset%20demonstrates%20a%20significant%20improvement%20in%20mean%0AAverage%20Precision%20%28mAP%29%2C%20resulting%20in%20a%203.8-8.1%25%20relative%20enhancement%20in%0Adetection%20under%20non-ideal%20conditions%20compared%20to%20the%20baseline%20model%2C%20and%20a%20less%0Amarginal%20performance%20difference%20within%201.3%25%20of%20the%20mAP%40%5B0.5%3A0.95%5D%20achieved%0Aunder%20ideal%20conditions%20by%20the%20standard%20Faster%20RCNN%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11214v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Corrective%20Transfer%20Learning%3A%20End-to-End%20Solutions%20to%20Object%0A%20%20Detection%20in%20Non-Ideal%20Visual%20Conditions&entry.906535625=Chuheng%20Wei%20and%20Guoyuan%20Wu%20and%20Matthew%20J.%20Barth&entry.1292438233=%20%20A%20significant%20challenge%20in%20the%20field%20of%20object%20detection%20lies%20in%20the%20system%27s%0Aperformance%20under%20non-ideal%20imaging%20conditions%2C%20such%20as%20rain%2C%20fog%2C%20low%0Aillumination%2C%20or%20raw%20Bayer%20images%20that%20lack%20ISP%20processing.%20Our%20study%0Aintroduces%20%22Feature%20Corrective%20Transfer%20Learning%22%2C%20a%20novel%20approach%20that%0Aleverages%20transfer%20learning%20and%20a%20bespoke%20loss%20function%20to%20facilitate%20the%0Aend-to-end%20detection%20of%20objects%20in%20these%20challenging%20scenarios%20without%20the%20need%0Ato%20convert%20non-ideal%20images%20into%20their%20RGB%20counterparts.%20In%20our%20methodology%2C%20we%0Ainitially%20train%20a%20comprehensive%20model%20on%20a%20pristine%20RGB%20image%20dataset.%0ASubsequently%2C%20non-ideal%20images%20are%20processed%20by%20comparing%20their%20feature%20maps%0Aagainst%20those%20from%20the%20initial%20ideal%20RGB%20model.%20This%20comparison%20employs%20the%0AExtended%20Area%20Novel%20Structural%20Discrepancy%20Loss%20%28EANSDL%29%2C%20a%20novel%20loss%20function%0Adesigned%20to%20quantify%20similarities%20and%20integrate%20them%20into%20the%20detection%20loss.%0AThis%20approach%20refines%20the%20model%27s%20ability%20to%20perform%20object%20detection%20across%0Avarying%20conditions%20through%20direct%20feature%20map%20correction%2C%20encapsulating%20the%0Aessence%20of%20Feature%20Corrective%20Transfer%20Learning.%20Experimental%20validation%20on%0Avariants%20of%20the%20KITTI%20dataset%20demonstrates%20a%20significant%20improvement%20in%20mean%0AAverage%20Precision%20%28mAP%29%2C%20resulting%20in%20a%203.8-8.1%25%20relative%20enhancement%20in%0Adetection%20under%20non-ideal%20conditions%20compared%20to%20the%20baseline%20model%2C%20and%20a%20less%0Amarginal%20performance%20difference%20within%201.3%25%20of%20the%20mAP%40%5B0.5%3A0.95%5D%20achieved%0Aunder%20ideal%20conditions%20by%20the%20standard%20Faster%20RCNN%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11214v2&entry.124074799=Read"},
{"title": "Multi-modal vision-language model for generalizable annotation-free\n  pathological lesions localization and clinical diagnosis", "author": "Hao Yang and Hong-Yu Zhou and Zhihuan Li and Yuanxu Gao and Cheng Li and Weijian Huang and Jiarun Liu and Hairong Zheng and Kang Zhang and Shanshan Wang", "abstract": "  Defining pathologies automatically from medical images aids the understanding\nof the emergence and progression of diseases, and such an ability is crucial in\nclinical diagnostics. However, existing deep learning models heavily rely on\nexpert annotations and lack generalization capabilities in open clinical\nenvironments. In this study, we present a generalizable vision-language\npre-training model for Annotation-Free pathological lesions Localization\n(AFLoc). The core strength of AFLoc lies in its extensive multi-level semantic\nstructure-based contrastive learning, which comprehensively aligns\nmulti-granularity medical concepts from reports with abundant image features,\nto adapt to the diverse expressions of pathologies and unseen pathologies\nwithout the reliance on image annotations from experts. We demonstrate the\nproof of concept on CXR images, with extensive experimental validation across 4\ndistinct external datasets, encompassing 11 types of chest pathologies. The\nresults demonstrate that AFLoc surpasses state-of-the-art methods in\npathological lesions localization and disease classification, and even\noutperforms the human benchmark in locating 5 different pathologies.\nAdditionally, we further verify its generalization ability by applying it to\nretinal fundus images. Our approach showcases AFoc versatilities and\nunderscores its suitability for clinical diagnoses in complex clinical\nenvironments.\n", "link": "http://arxiv.org/abs/2401.02044v3", "date": "2024-04-19", "relevancy": 1.6739, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5783}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5357}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20vision-language%20model%20for%20generalizable%20annotation-free%0A%20%20pathological%20lesions%20localization%20and%20clinical%20diagnosis&body=Title%3A%20Multi-modal%20vision-language%20model%20for%20generalizable%20annotation-free%0A%20%20pathological%20lesions%20localization%20and%20clinical%20diagnosis%0AAuthor%3A%20Hao%20Yang%20and%20Hong-Yu%20Zhou%20and%20Zhihuan%20Li%20and%20Yuanxu%20Gao%20and%20Cheng%20Li%20and%20Weijian%20Huang%20and%20Jiarun%20Liu%20and%20Hairong%20Zheng%20and%20Kang%20Zhang%20and%20Shanshan%20Wang%0AAbstract%3A%20%20%20Defining%20pathologies%20automatically%20from%20medical%20images%20aids%20the%20understanding%0Aof%20the%20emergence%20and%20progression%20of%20diseases%2C%20and%20such%20an%20ability%20is%20crucial%20in%0Aclinical%20diagnostics.%20However%2C%20existing%20deep%20learning%20models%20heavily%20rely%20on%0Aexpert%20annotations%20and%20lack%20generalization%20capabilities%20in%20open%20clinical%0Aenvironments.%20In%20this%20study%2C%20we%20present%20a%20generalizable%20vision-language%0Apre-training%20model%20for%20Annotation-Free%20pathological%20lesions%20Localization%0A%28AFLoc%29.%20The%20core%20strength%20of%20AFLoc%20lies%20in%20its%20extensive%20multi-level%20semantic%0Astructure-based%20contrastive%20learning%2C%20which%20comprehensively%20aligns%0Amulti-granularity%20medical%20concepts%20from%20reports%20with%20abundant%20image%20features%2C%0Ato%20adapt%20to%20the%20diverse%20expressions%20of%20pathologies%20and%20unseen%20pathologies%0Awithout%20the%20reliance%20on%20image%20annotations%20from%20experts.%20We%20demonstrate%20the%0Aproof%20of%20concept%20on%20CXR%20images%2C%20with%20extensive%20experimental%20validation%20across%204%0Adistinct%20external%20datasets%2C%20encompassing%2011%20types%20of%20chest%20pathologies.%20The%0Aresults%20demonstrate%20that%20AFLoc%20surpasses%20state-of-the-art%20methods%20in%0Apathological%20lesions%20localization%20and%20disease%20classification%2C%20and%20even%0Aoutperforms%20the%20human%20benchmark%20in%20locating%205%20different%20pathologies.%0AAdditionally%2C%20we%20further%20verify%20its%20generalization%20ability%20by%20applying%20it%20to%0Aretinal%20fundus%20images.%20Our%20approach%20showcases%20AFoc%20versatilities%20and%0Aunderscores%20its%20suitability%20for%20clinical%20diagnoses%20in%20complex%20clinical%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02044v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20vision-language%20model%20for%20generalizable%20annotation-free%0A%20%20pathological%20lesions%20localization%20and%20clinical%20diagnosis&entry.906535625=Hao%20Yang%20and%20Hong-Yu%20Zhou%20and%20Zhihuan%20Li%20and%20Yuanxu%20Gao%20and%20Cheng%20Li%20and%20Weijian%20Huang%20and%20Jiarun%20Liu%20and%20Hairong%20Zheng%20and%20Kang%20Zhang%20and%20Shanshan%20Wang&entry.1292438233=%20%20Defining%20pathologies%20automatically%20from%20medical%20images%20aids%20the%20understanding%0Aof%20the%20emergence%20and%20progression%20of%20diseases%2C%20and%20such%20an%20ability%20is%20crucial%20in%0Aclinical%20diagnostics.%20However%2C%20existing%20deep%20learning%20models%20heavily%20rely%20on%0Aexpert%20annotations%20and%20lack%20generalization%20capabilities%20in%20open%20clinical%0Aenvironments.%20In%20this%20study%2C%20we%20present%20a%20generalizable%20vision-language%0Apre-training%20model%20for%20Annotation-Free%20pathological%20lesions%20Localization%0A%28AFLoc%29.%20The%20core%20strength%20of%20AFLoc%20lies%20in%20its%20extensive%20multi-level%20semantic%0Astructure-based%20contrastive%20learning%2C%20which%20comprehensively%20aligns%0Amulti-granularity%20medical%20concepts%20from%20reports%20with%20abundant%20image%20features%2C%0Ato%20adapt%20to%20the%20diverse%20expressions%20of%20pathologies%20and%20unseen%20pathologies%0Awithout%20the%20reliance%20on%20image%20annotations%20from%20experts.%20We%20demonstrate%20the%0Aproof%20of%20concept%20on%20CXR%20images%2C%20with%20extensive%20experimental%20validation%20across%204%0Adistinct%20external%20datasets%2C%20encompassing%2011%20types%20of%20chest%20pathologies.%20The%0Aresults%20demonstrate%20that%20AFLoc%20surpasses%20state-of-the-art%20methods%20in%0Apathological%20lesions%20localization%20and%20disease%20classification%2C%20and%20even%0Aoutperforms%20the%20human%20benchmark%20in%20locating%205%20different%20pathologies.%0AAdditionally%2C%20we%20further%20verify%20its%20generalization%20ability%20by%20applying%20it%20to%0Aretinal%20fundus%20images.%20Our%20approach%20showcases%20AFoc%20versatilities%20and%0Aunderscores%20its%20suitability%20for%20clinical%20diagnoses%20in%20complex%20clinical%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02044v3&entry.124074799=Read"},
{"title": "RefinedFields: Radiance Fields Refinement for Unconstrained Scenes", "author": "Karim Kassab and Antoine Schnepf and Jean-Yves Franceschi and Laurent Caraffa and Jeremie Mary and Val\u00e9rie Gouet-Brunet", "abstract": "  Modeling large scenes from unconstrained images has proven to be a major\nchallenge in computer vision. Existing methods tackling in-the-wild scene\nmodeling operate in closed-world settings, where no conditioning on priors\nacquired from real-world images is present. We propose RefinedFields, which is,\nto the best of our knowledge, the first method leveraging pre-trained models to\nimprove in-the-wild scene modeling. We employ pre-trained networks to refine\nK-Planes representations via optimization guidance using an alternating\ntraining procedure. We carry out extensive experiments and verify the merit of\nour method on synthetic data and real tourism photo collections. RefinedFields\nenhances rendered scenes with richer details and improves upon its base\nrepresentation on the task of novel view synthesis in the wild. Our project\npage can be found at https://refinedfields.github.io.\n", "link": "http://arxiv.org/abs/2312.00639v3", "date": "2024-04-19", "relevancy": 1.6584, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5716}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5354}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5233}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RefinedFields%3A%20Radiance%20Fields%20Refinement%20for%20Unconstrained%20Scenes&body=Title%3A%20RefinedFields%3A%20Radiance%20Fields%20Refinement%20for%20Unconstrained%20Scenes%0AAuthor%3A%20Karim%20Kassab%20and%20Antoine%20Schnepf%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Jeremie%20Mary%20and%20Val%C3%A9rie%20Gouet-Brunet%0AAbstract%3A%20%20%20Modeling%20large%20scenes%20from%20unconstrained%20images%20has%20proven%20to%20be%20a%20major%0Achallenge%20in%20computer%20vision.%20Existing%20methods%20tackling%20in-the-wild%20scene%0Amodeling%20operate%20in%20closed-world%20settings%2C%20where%20no%20conditioning%20on%20priors%0Aacquired%20from%20real-world%20images%20is%20present.%20We%20propose%20RefinedFields%2C%20which%20is%2C%0Ato%20the%20best%20of%20our%20knowledge%2C%20the%20first%20method%20leveraging%20pre-trained%20models%20to%0Aimprove%20in-the-wild%20scene%20modeling.%20We%20employ%20pre-trained%20networks%20to%20refine%0AK-Planes%20representations%20via%20optimization%20guidance%20using%20an%20alternating%0Atraining%20procedure.%20We%20carry%20out%20extensive%20experiments%20and%20verify%20the%20merit%20of%0Aour%20method%20on%20synthetic%20data%20and%20real%20tourism%20photo%20collections.%20RefinedFields%0Aenhances%20rendered%20scenes%20with%20richer%20details%20and%20improves%20upon%20its%20base%0Arepresentation%20on%20the%20task%20of%20novel%20view%20synthesis%20in%20the%20wild.%20Our%20project%0Apage%20can%20be%20found%20at%20https%3A//refinedfields.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00639v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefinedFields%3A%20Radiance%20Fields%20Refinement%20for%20Unconstrained%20Scenes&entry.906535625=Karim%20Kassab%20and%20Antoine%20Schnepf%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Jeremie%20Mary%20and%20Val%C3%A9rie%20Gouet-Brunet&entry.1292438233=%20%20Modeling%20large%20scenes%20from%20unconstrained%20images%20has%20proven%20to%20be%20a%20major%0Achallenge%20in%20computer%20vision.%20Existing%20methods%20tackling%20in-the-wild%20scene%0Amodeling%20operate%20in%20closed-world%20settings%2C%20where%20no%20conditioning%20on%20priors%0Aacquired%20from%20real-world%20images%20is%20present.%20We%20propose%20RefinedFields%2C%20which%20is%2C%0Ato%20the%20best%20of%20our%20knowledge%2C%20the%20first%20method%20leveraging%20pre-trained%20models%20to%0Aimprove%20in-the-wild%20scene%20modeling.%20We%20employ%20pre-trained%20networks%20to%20refine%0AK-Planes%20representations%20via%20optimization%20guidance%20using%20an%20alternating%0Atraining%20procedure.%20We%20carry%20out%20extensive%20experiments%20and%20verify%20the%20merit%20of%0Aour%20method%20on%20synthetic%20data%20and%20real%20tourism%20photo%20collections.%20RefinedFields%0Aenhances%20rendered%20scenes%20with%20richer%20details%20and%20improves%20upon%20its%20base%0Arepresentation%20on%20the%20task%20of%20novel%20view%20synthesis%20in%20the%20wild.%20Our%20project%0Apage%20can%20be%20found%20at%20https%3A//refinedfields.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00639v3&entry.124074799=Read"},
{"title": "The maximum capability of a topological feature in link prediction", "author": "Yijun Ran and Xiao-Ke Xu and Tao Jia", "abstract": "  Networks offer a powerful approach to modeling complex systems by\nrepresenting the underlying set of pairwise interactions. Link prediction is\nthe task that predicts links of a network that are not directly visible, with\nprofound applications in biological, social, and other complex systems. Despite\nintensive utilization of the topological feature in this task, it is unclear to\nwhat extent a feature can be leveraged to infer missing links. Here, we aim to\nunveil the capability of a topological feature in link prediction by\nidentifying its prediction performance upper bound. We introduce a theoretical\nframework that is compatible with different indexes to gauge the feature,\ndifferent prediction approaches to utilize the feature, and different metrics\nto quantify the prediction performance. The maximum capability of a topological\nfeature follows a simple yet theoretically validated expression, which only\ndepends on the extent to which the feature is held in missing and nonexistent\nlinks. Because a family of indexes based on the same feature shares the same\nupper bound, the potential of all others can be estimated from one single\nindex. Furthermore, a feature's capability is lifted in the supervised\nprediction, which can be mathematically quantified, allowing us to estimate the\nbenefit of applying machine learning algorithms. The universality of the\npattern uncovered is empirically verified by 550 structurally diverse networks.\nThe findings have applications in feature and method selection, and shed light\non network characteristics that make a topological feature effective in link\nprediction.\n", "link": "http://arxiv.org/abs/2206.15101v3", "date": "2024-04-19", "relevancy": 1.6534, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.43}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4219}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3981}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20maximum%20capability%20of%20a%20topological%20feature%20in%20link%20prediction&body=Title%3A%20The%20maximum%20capability%20of%20a%20topological%20feature%20in%20link%20prediction%0AAuthor%3A%20Yijun%20Ran%20and%20Xiao-Ke%20Xu%20and%20Tao%20Jia%0AAbstract%3A%20%20%20Networks%20offer%20a%20powerful%20approach%20to%20modeling%20complex%20systems%20by%0Arepresenting%20the%20underlying%20set%20of%20pairwise%20interactions.%20Link%20prediction%20is%0Athe%20task%20that%20predicts%20links%20of%20a%20network%20that%20are%20not%20directly%20visible%2C%20with%0Aprofound%20applications%20in%20biological%2C%20social%2C%20and%20other%20complex%20systems.%20Despite%0Aintensive%20utilization%20of%20the%20topological%20feature%20in%20this%20task%2C%20it%20is%20unclear%20to%0Awhat%20extent%20a%20feature%20can%20be%20leveraged%20to%20infer%20missing%20links.%20Here%2C%20we%20aim%20to%0Aunveil%20the%20capability%20of%20a%20topological%20feature%20in%20link%20prediction%20by%0Aidentifying%20its%20prediction%20performance%20upper%20bound.%20We%20introduce%20a%20theoretical%0Aframework%20that%20is%20compatible%20with%20different%20indexes%20to%20gauge%20the%20feature%2C%0Adifferent%20prediction%20approaches%20to%20utilize%20the%20feature%2C%20and%20different%20metrics%0Ato%20quantify%20the%20prediction%20performance.%20The%20maximum%20capability%20of%20a%20topological%0Afeature%20follows%20a%20simple%20yet%20theoretically%20validated%20expression%2C%20which%20only%0Adepends%20on%20the%20extent%20to%20which%20the%20feature%20is%20held%20in%20missing%20and%20nonexistent%0Alinks.%20Because%20a%20family%20of%20indexes%20based%20on%20the%20same%20feature%20shares%20the%20same%0Aupper%20bound%2C%20the%20potential%20of%20all%20others%20can%20be%20estimated%20from%20one%20single%0Aindex.%20Furthermore%2C%20a%20feature%27s%20capability%20is%20lifted%20in%20the%20supervised%0Aprediction%2C%20which%20can%20be%20mathematically%20quantified%2C%20allowing%20us%20to%20estimate%20the%0Abenefit%20of%20applying%20machine%20learning%20algorithms.%20The%20universality%20of%20the%0Apattern%20uncovered%20is%20empirically%20verified%20by%20550%20structurally%20diverse%20networks.%0AThe%20findings%20have%20applications%20in%20feature%20and%20method%20selection%2C%20and%20shed%20light%0Aon%20network%20characteristics%20that%20make%20a%20topological%20feature%20effective%20in%20link%0Aprediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.15101v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20maximum%20capability%20of%20a%20topological%20feature%20in%20link%20prediction&entry.906535625=Yijun%20Ran%20and%20Xiao-Ke%20Xu%20and%20Tao%20Jia&entry.1292438233=%20%20Networks%20offer%20a%20powerful%20approach%20to%20modeling%20complex%20systems%20by%0Arepresenting%20the%20underlying%20set%20of%20pairwise%20interactions.%20Link%20prediction%20is%0Athe%20task%20that%20predicts%20links%20of%20a%20network%20that%20are%20not%20directly%20visible%2C%20with%0Aprofound%20applications%20in%20biological%2C%20social%2C%20and%20other%20complex%20systems.%20Despite%0Aintensive%20utilization%20of%20the%20topological%20feature%20in%20this%20task%2C%20it%20is%20unclear%20to%0Awhat%20extent%20a%20feature%20can%20be%20leveraged%20to%20infer%20missing%20links.%20Here%2C%20we%20aim%20to%0Aunveil%20the%20capability%20of%20a%20topological%20feature%20in%20link%20prediction%20by%0Aidentifying%20its%20prediction%20performance%20upper%20bound.%20We%20introduce%20a%20theoretical%0Aframework%20that%20is%20compatible%20with%20different%20indexes%20to%20gauge%20the%20feature%2C%0Adifferent%20prediction%20approaches%20to%20utilize%20the%20feature%2C%20and%20different%20metrics%0Ato%20quantify%20the%20prediction%20performance.%20The%20maximum%20capability%20of%20a%20topological%0Afeature%20follows%20a%20simple%20yet%20theoretically%20validated%20expression%2C%20which%20only%0Adepends%20on%20the%20extent%20to%20which%20the%20feature%20is%20held%20in%20missing%20and%20nonexistent%0Alinks.%20Because%20a%20family%20of%20indexes%20based%20on%20the%20same%20feature%20shares%20the%20same%0Aupper%20bound%2C%20the%20potential%20of%20all%20others%20can%20be%20estimated%20from%20one%20single%0Aindex.%20Furthermore%2C%20a%20feature%27s%20capability%20is%20lifted%20in%20the%20supervised%0Aprediction%2C%20which%20can%20be%20mathematically%20quantified%2C%20allowing%20us%20to%20estimate%20the%0Abenefit%20of%20applying%20machine%20learning%20algorithms.%20The%20universality%20of%20the%0Apattern%20uncovered%20is%20empirically%20verified%20by%20550%20structurally%20diverse%20networks.%0AThe%20findings%20have%20applications%20in%20feature%20and%20method%20selection%2C%20and%20shed%20light%0Aon%20network%20characteristics%20that%20make%20a%20topological%20feature%20effective%20in%20link%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.15101v3&entry.124074799=Read"},
{"title": "Learning Symbolic Task Representation from a Human-Led Demonstration: A\n  Memory to Store, Retrieve, Consolidate, and Forget Experiences", "author": "Luca Buoncompagni and Fulvio Mastrogiovanni", "abstract": "  We present a symbolic learning framework inspired by cognitive-like memory\nfunctionalities (i.e., storing, retrieving, consolidating and forgetting) to\ngenerate task representations to support high-level task planning and knowledge\nbootstrapping. We address a scenario involving a non-expert human, who performs\na single task demonstration, and a robot, which online learns structured\nknowledge to re-execute the task based on experiences, i.e., observations. We\nconsider a one-shot learning process based on non-annotated data to store an\nintelligible representation of the task, which can be refined through\ninteraction, e.g., via verbal or visual communication. Our general-purpose\nframework relies on fuzzy Description Logic, which has been used to extend the\npreviously developed Scene Identification and Tagging algorithm. In this paper,\nwe exploit such an algorithm to implement cognitive-like memory functionalities\nemploying scores that rank memorised observations over time based on simple\nheuristics. Our main contribution is the formalisation of a framework that can\nbe used to systematically investigate different heuristics for bootstrapping\nhierarchical knowledge representations based on robot observations. Through an\nillustrative assembly task scenario, the paper presents the performance of our\nframework to discuss its benefits and limitations.\n", "link": "http://arxiv.org/abs/2404.10591v2", "date": "2024-04-19", "relevancy": 1.6397, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5245}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Symbolic%20Task%20Representation%20from%20a%20Human-Led%20Demonstration%3A%20A%0A%20%20Memory%20to%20Store%2C%20Retrieve%2C%20Consolidate%2C%20and%20Forget%20Experiences&body=Title%3A%20Learning%20Symbolic%20Task%20Representation%20from%20a%20Human-Led%20Demonstration%3A%20A%0A%20%20Memory%20to%20Store%2C%20Retrieve%2C%20Consolidate%2C%20and%20Forget%20Experiences%0AAuthor%3A%20Luca%20Buoncompagni%20and%20Fulvio%20Mastrogiovanni%0AAbstract%3A%20%20%20We%20present%20a%20symbolic%20learning%20framework%20inspired%20by%20cognitive-like%20memory%0Afunctionalities%20%28i.e.%2C%20storing%2C%20retrieving%2C%20consolidating%20and%20forgetting%29%20to%0Agenerate%20task%20representations%20to%20support%20high-level%20task%20planning%20and%20knowledge%0Abootstrapping.%20We%20address%20a%20scenario%20involving%20a%20non-expert%20human%2C%20who%20performs%0Aa%20single%20task%20demonstration%2C%20and%20a%20robot%2C%20which%20online%20learns%20structured%0Aknowledge%20to%20re-execute%20the%20task%20based%20on%20experiences%2C%20i.e.%2C%20observations.%20We%0Aconsider%20a%20one-shot%20learning%20process%20based%20on%20non-annotated%20data%20to%20store%20an%0Aintelligible%20representation%20of%20the%20task%2C%20which%20can%20be%20refined%20through%0Ainteraction%2C%20e.g.%2C%20via%20verbal%20or%20visual%20communication.%20Our%20general-purpose%0Aframework%20relies%20on%20fuzzy%20Description%20Logic%2C%20which%20has%20been%20used%20to%20extend%20the%0Apreviously%20developed%20Scene%20Identification%20and%20Tagging%20algorithm.%20In%20this%20paper%2C%0Awe%20exploit%20such%20an%20algorithm%20to%20implement%20cognitive-like%20memory%20functionalities%0Aemploying%20scores%20that%20rank%20memorised%20observations%20over%20time%20based%20on%20simple%0Aheuristics.%20Our%20main%20contribution%20is%20the%20formalisation%20of%20a%20framework%20that%20can%0Abe%20used%20to%20systematically%20investigate%20different%20heuristics%20for%20bootstrapping%0Ahierarchical%20knowledge%20representations%20based%20on%20robot%20observations.%20Through%20an%0Aillustrative%20assembly%20task%20scenario%2C%20the%20paper%20presents%20the%20performance%20of%20our%0Aframework%20to%20discuss%20its%20benefits%20and%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10591v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Symbolic%20Task%20Representation%20from%20a%20Human-Led%20Demonstration%3A%20A%0A%20%20Memory%20to%20Store%2C%20Retrieve%2C%20Consolidate%2C%20and%20Forget%20Experiences&entry.906535625=Luca%20Buoncompagni%20and%20Fulvio%20Mastrogiovanni&entry.1292438233=%20%20We%20present%20a%20symbolic%20learning%20framework%20inspired%20by%20cognitive-like%20memory%0Afunctionalities%20%28i.e.%2C%20storing%2C%20retrieving%2C%20consolidating%20and%20forgetting%29%20to%0Agenerate%20task%20representations%20to%20support%20high-level%20task%20planning%20and%20knowledge%0Abootstrapping.%20We%20address%20a%20scenario%20involving%20a%20non-expert%20human%2C%20who%20performs%0Aa%20single%20task%20demonstration%2C%20and%20a%20robot%2C%20which%20online%20learns%20structured%0Aknowledge%20to%20re-execute%20the%20task%20based%20on%20experiences%2C%20i.e.%2C%20observations.%20We%0Aconsider%20a%20one-shot%20learning%20process%20based%20on%20non-annotated%20data%20to%20store%20an%0Aintelligible%20representation%20of%20the%20task%2C%20which%20can%20be%20refined%20through%0Ainteraction%2C%20e.g.%2C%20via%20verbal%20or%20visual%20communication.%20Our%20general-purpose%0Aframework%20relies%20on%20fuzzy%20Description%20Logic%2C%20which%20has%20been%20used%20to%20extend%20the%0Apreviously%20developed%20Scene%20Identification%20and%20Tagging%20algorithm.%20In%20this%20paper%2C%0Awe%20exploit%20such%20an%20algorithm%20to%20implement%20cognitive-like%20memory%20functionalities%0Aemploying%20scores%20that%20rank%20memorised%20observations%20over%20time%20based%20on%20simple%0Aheuristics.%20Our%20main%20contribution%20is%20the%20formalisation%20of%20a%20framework%20that%20can%0Abe%20used%20to%20systematically%20investigate%20different%20heuristics%20for%20bootstrapping%0Ahierarchical%20knowledge%20representations%20based%20on%20robot%20observations.%20Through%20an%0Aillustrative%20assembly%20task%20scenario%2C%20the%20paper%20presents%20the%20performance%20of%20our%0Aframework%20to%20discuss%20its%20benefits%20and%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10591v2&entry.124074799=Read"},
{"title": "Linearly-evolved Transformer for Pan-sharpening", "author": "Junming Hou and Zihan Cao and Naishan Zheng and Xuan Li and Xiaoyu Chen and Xinyang Liu and Xiaofeng Cong and Man Zhou and Danfeng Hong", "abstract": "  Vision transformer family has dominated the satellite pan-sharpening field\ndriven by the global-wise spatial information modeling mechanism from the core\nself-attention ingredient. The standard modeling rules within these promising\npan-sharpening methods are to roughly stack the transformer variants in a\ncascaded manner. Despite the remarkable advancement, their success may be at\nthe huge cost of model parameters and FLOPs, thus preventing its application\nover low-resource satellites.To address this challenge between favorable\nperformance and expensive computation, we tailor an efficient linearly-evolved\ntransformer variant and employ it to construct a lightweight pan-sharpening\nframework. In detail, we deepen into the popular cascaded transformer modeling\nwith cutting-edge methods and develop the alternative 1-order linearly-evolved\ntransformer variant with the 1-dimensional linear convolution chain to achieve\nthe same function. In this way, our proposed method is capable of benefiting\nthe cascaded modeling rule while achieving favorable performance in the\nefficient manner. Extensive experiments over multiple satellite datasets\nsuggest that our proposed method achieves competitive performance against other\nstate-of-the-art with fewer computational resources. Further, the consistently\nfavorable performance has been verified over the hyper-spectral image fusion\ntask. Our main focus is to provide an alternative global modeling framework\nwith an efficient structure. The code will be publicly available.\n", "link": "http://arxiv.org/abs/2404.12804v1", "date": "2024-04-19", "relevancy": 1.6355, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.615}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5293}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5236}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Linearly-evolved%20Transformer%20for%20Pan-sharpening&body=Title%3A%20Linearly-evolved%20Transformer%20for%20Pan-sharpening%0AAuthor%3A%20Junming%20Hou%20and%20Zihan%20Cao%20and%20Naishan%20Zheng%20and%20Xuan%20Li%20and%20Xiaoyu%20Chen%20and%20Xinyang%20Liu%20and%20Xiaofeng%20Cong%20and%20Man%20Zhou%20and%20Danfeng%20Hong%0AAbstract%3A%20%20%20Vision%20transformer%20family%20has%20dominated%20the%20satellite%20pan-sharpening%20field%0Adriven%20by%20the%20global-wise%20spatial%20information%20modeling%20mechanism%20from%20the%20core%0Aself-attention%20ingredient.%20The%20standard%20modeling%20rules%20within%20these%20promising%0Apan-sharpening%20methods%20are%20to%20roughly%20stack%20the%20transformer%20variants%20in%20a%0Acascaded%20manner.%20Despite%20the%20remarkable%20advancement%2C%20their%20success%20may%20be%20at%0Athe%20huge%20cost%20of%20model%20parameters%20and%20FLOPs%2C%20thus%20preventing%20its%20application%0Aover%20low-resource%20satellites.To%20address%20this%20challenge%20between%20favorable%0Aperformance%20and%20expensive%20computation%2C%20we%20tailor%20an%20efficient%20linearly-evolved%0Atransformer%20variant%20and%20employ%20it%20to%20construct%20a%20lightweight%20pan-sharpening%0Aframework.%20In%20detail%2C%20we%20deepen%20into%20the%20popular%20cascaded%20transformer%20modeling%0Awith%20cutting-edge%20methods%20and%20develop%20the%20alternative%201-order%20linearly-evolved%0Atransformer%20variant%20with%20the%201-dimensional%20linear%20convolution%20chain%20to%20achieve%0Athe%20same%20function.%20In%20this%20way%2C%20our%20proposed%20method%20is%20capable%20of%20benefiting%0Athe%20cascaded%20modeling%20rule%20while%20achieving%20favorable%20performance%20in%20the%0Aefficient%20manner.%20Extensive%20experiments%20over%20multiple%20satellite%20datasets%0Asuggest%20that%20our%20proposed%20method%20achieves%20competitive%20performance%20against%20other%0Astate-of-the-art%20with%20fewer%20computational%20resources.%20Further%2C%20the%20consistently%0Afavorable%20performance%20has%20been%20verified%20over%20the%20hyper-spectral%20image%20fusion%0Atask.%20Our%20main%20focus%20is%20to%20provide%20an%20alternative%20global%20modeling%20framework%0Awith%20an%20efficient%20structure.%20The%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12804v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linearly-evolved%20Transformer%20for%20Pan-sharpening&entry.906535625=Junming%20Hou%20and%20Zihan%20Cao%20and%20Naishan%20Zheng%20and%20Xuan%20Li%20and%20Xiaoyu%20Chen%20and%20Xinyang%20Liu%20and%20Xiaofeng%20Cong%20and%20Man%20Zhou%20and%20Danfeng%20Hong&entry.1292438233=%20%20Vision%20transformer%20family%20has%20dominated%20the%20satellite%20pan-sharpening%20field%0Adriven%20by%20the%20global-wise%20spatial%20information%20modeling%20mechanism%20from%20the%20core%0Aself-attention%20ingredient.%20The%20standard%20modeling%20rules%20within%20these%20promising%0Apan-sharpening%20methods%20are%20to%20roughly%20stack%20the%20transformer%20variants%20in%20a%0Acascaded%20manner.%20Despite%20the%20remarkable%20advancement%2C%20their%20success%20may%20be%20at%0Athe%20huge%20cost%20of%20model%20parameters%20and%20FLOPs%2C%20thus%20preventing%20its%20application%0Aover%20low-resource%20satellites.To%20address%20this%20challenge%20between%20favorable%0Aperformance%20and%20expensive%20computation%2C%20we%20tailor%20an%20efficient%20linearly-evolved%0Atransformer%20variant%20and%20employ%20it%20to%20construct%20a%20lightweight%20pan-sharpening%0Aframework.%20In%20detail%2C%20we%20deepen%20into%20the%20popular%20cascaded%20transformer%20modeling%0Awith%20cutting-edge%20methods%20and%20develop%20the%20alternative%201-order%20linearly-evolved%0Atransformer%20variant%20with%20the%201-dimensional%20linear%20convolution%20chain%20to%20achieve%0Athe%20same%20function.%20In%20this%20way%2C%20our%20proposed%20method%20is%20capable%20of%20benefiting%0Athe%20cascaded%20modeling%20rule%20while%20achieving%20favorable%20performance%20in%20the%0Aefficient%20manner.%20Extensive%20experiments%20over%20multiple%20satellite%20datasets%0Asuggest%20that%20our%20proposed%20method%20achieves%20competitive%20performance%20against%20other%0Astate-of-the-art%20with%20fewer%20computational%20resources.%20Further%2C%20the%20consistently%0Afavorable%20performance%20has%20been%20verified%20over%20the%20hyper-spectral%20image%20fusion%0Atask.%20Our%20main%20focus%20is%20to%20provide%20an%20alternative%20global%20modeling%20framework%0Awith%20an%20efficient%20structure.%20The%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12804v1&entry.124074799=Read"},
{"title": "Factorized Motion Fields for Fast Sparse Input Dynamic View Synthesis", "author": "Nagabhushan Somraj and Kapil Choudhary and Sai Harsha Mupparaju and Rajiv Soundararajan", "abstract": "  Designing a 3D representation of a dynamic scene for fast optimization and\nrendering is a challenging task. While recent explicit representations enable\nfast learning and rendering of dynamic radiance fields, they require a dense\nset of input viewpoints. In this work, we focus on learning a fast\nrepresentation for dynamic radiance fields with sparse input viewpoints.\nHowever, the optimization with sparse input is under-constrained and\nnecessitates the use of motion priors to constrain the learning. Existing fast\ndynamic scene models do not explicitly model the motion, making them difficult\nto be constrained with motion priors. We design an explicit motion model as a\nfactorized 4D representation that is fast and can exploit the spatio-temporal\ncorrelation of the motion field. We then introduce reliable flow priors\nincluding a combination of sparse flow priors across cameras and dense flow\npriors within cameras to regularize our motion model. Our model is fast,\ncompact and achieves very good performance on popular multi-view dynamic scene\ndatasets with sparse input viewpoints. The source code for our model can be\nfound on our project page:\nhttps://nagabhushansn95.github.io/publications/2024/RF-DeRF.html.\n", "link": "http://arxiv.org/abs/2404.11669v2", "date": "2024-04-19", "relevancy": 1.6312, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5511}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5473}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5394}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Factorized%20Motion%20Fields%20for%20Fast%20Sparse%20Input%20Dynamic%20View%20Synthesis&body=Title%3A%20Factorized%20Motion%20Fields%20for%20Fast%20Sparse%20Input%20Dynamic%20View%20Synthesis%0AAuthor%3A%20Nagabhushan%20Somraj%20and%20Kapil%20Choudhary%20and%20Sai%20Harsha%20Mupparaju%20and%20Rajiv%20Soundararajan%0AAbstract%3A%20%20%20Designing%20a%203D%20representation%20of%20a%20dynamic%20scene%20for%20fast%20optimization%20and%0Arendering%20is%20a%20challenging%20task.%20While%20recent%20explicit%20representations%20enable%0Afast%20learning%20and%20rendering%20of%20dynamic%20radiance%20fields%2C%20they%20require%20a%20dense%0Aset%20of%20input%20viewpoints.%20In%20this%20work%2C%20we%20focus%20on%20learning%20a%20fast%0Arepresentation%20for%20dynamic%20radiance%20fields%20with%20sparse%20input%20viewpoints.%0AHowever%2C%20the%20optimization%20with%20sparse%20input%20is%20under-constrained%20and%0Anecessitates%20the%20use%20of%20motion%20priors%20to%20constrain%20the%20learning.%20Existing%20fast%0Adynamic%20scene%20models%20do%20not%20explicitly%20model%20the%20motion%2C%20making%20them%20difficult%0Ato%20be%20constrained%20with%20motion%20priors.%20We%20design%20an%20explicit%20motion%20model%20as%20a%0Afactorized%204D%20representation%20that%20is%20fast%20and%20can%20exploit%20the%20spatio-temporal%0Acorrelation%20of%20the%20motion%20field.%20We%20then%20introduce%20reliable%20flow%20priors%0Aincluding%20a%20combination%20of%20sparse%20flow%20priors%20across%20cameras%20and%20dense%20flow%0Apriors%20within%20cameras%20to%20regularize%20our%20motion%20model.%20Our%20model%20is%20fast%2C%0Acompact%20and%20achieves%20very%20good%20performance%20on%20popular%20multi-view%20dynamic%20scene%0Adatasets%20with%20sparse%20input%20viewpoints.%20The%20source%20code%20for%20our%20model%20can%20be%0Afound%20on%20our%20project%20page%3A%0Ahttps%3A//nagabhushansn95.github.io/publications/2024/RF-DeRF.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11669v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factorized%20Motion%20Fields%20for%20Fast%20Sparse%20Input%20Dynamic%20View%20Synthesis&entry.906535625=Nagabhushan%20Somraj%20and%20Kapil%20Choudhary%20and%20Sai%20Harsha%20Mupparaju%20and%20Rajiv%20Soundararajan&entry.1292438233=%20%20Designing%20a%203D%20representation%20of%20a%20dynamic%20scene%20for%20fast%20optimization%20and%0Arendering%20is%20a%20challenging%20task.%20While%20recent%20explicit%20representations%20enable%0Afast%20learning%20and%20rendering%20of%20dynamic%20radiance%20fields%2C%20they%20require%20a%20dense%0Aset%20of%20input%20viewpoints.%20In%20this%20work%2C%20we%20focus%20on%20learning%20a%20fast%0Arepresentation%20for%20dynamic%20radiance%20fields%20with%20sparse%20input%20viewpoints.%0AHowever%2C%20the%20optimization%20with%20sparse%20input%20is%20under-constrained%20and%0Anecessitates%20the%20use%20of%20motion%20priors%20to%20constrain%20the%20learning.%20Existing%20fast%0Adynamic%20scene%20models%20do%20not%20explicitly%20model%20the%20motion%2C%20making%20them%20difficult%0Ato%20be%20constrained%20with%20motion%20priors.%20We%20design%20an%20explicit%20motion%20model%20as%20a%0Afactorized%204D%20representation%20that%20is%20fast%20and%20can%20exploit%20the%20spatio-temporal%0Acorrelation%20of%20the%20motion%20field.%20We%20then%20introduce%20reliable%20flow%20priors%0Aincluding%20a%20combination%20of%20sparse%20flow%20priors%20across%20cameras%20and%20dense%20flow%0Apriors%20within%20cameras%20to%20regularize%20our%20motion%20model.%20Our%20model%20is%20fast%2C%0Acompact%20and%20achieves%20very%20good%20performance%20on%20popular%20multi-view%20dynamic%20scene%0Adatasets%20with%20sparse%20input%20viewpoints.%20The%20source%20code%20for%20our%20model%20can%20be%0Afound%20on%20our%20project%20page%3A%0Ahttps%3A//nagabhushansn95.github.io/publications/2024/RF-DeRF.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11669v2&entry.124074799=Read"},
{"title": "PhysDreamer: Physics-Based Interaction with 3D Objects via Video\n  Generation", "author": "Tianyuan Zhang and Hong-Xing Yu and Rundi Wu and Brandon Y. Feng and Changxi Zheng and Noah Snavely and Jiajun Wu and William T. Freeman", "abstract": "  Realistic object interactions are crucial for creating immersive virtual\nexperiences, yet synthesizing realistic 3D object dynamics in response to novel\ninteractions remains a significant challenge. Unlike unconditional or\ntext-conditioned dynamics generation, action-conditioned dynamics requires\nperceiving the physical material properties of objects and grounding the 3D\nmotion prediction on these properties, such as object stiffness. However,\nestimating physical material properties is an open problem due to the lack of\nmaterial ground-truth data, as measuring these properties for real objects is\nhighly difficult. We present PhysDreamer, a physics-based approach that endows\nstatic 3D objects with interactive dynamics by leveraging the object dynamics\npriors learned by video generation models. By distilling these priors,\nPhysDreamer enables the synthesis of realistic object responses to novel\ninteractions, such as external forces or agent manipulations. We demonstrate\nour approach on diverse examples of elastic objects and evaluate the realism of\nthe synthesized interactions through a user study. PhysDreamer takes a step\ntowards more engaging and realistic virtual experiences by enabling static 3D\nobjects to dynamically respond to interactive stimuli in a physically plausible\nmanner. See our project page at https://physdreamer.github.io/.\n", "link": "http://arxiv.org/abs/2404.13026v1", "date": "2024-04-19", "relevancy": 1.6168, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5384}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5326}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PhysDreamer%3A%20Physics-Based%20Interaction%20with%203D%20Objects%20via%20Video%0A%20%20Generation&body=Title%3A%20PhysDreamer%3A%20Physics-Based%20Interaction%20with%203D%20Objects%20via%20Video%0A%20%20Generation%0AAuthor%3A%20Tianyuan%20Zhang%20and%20Hong-Xing%20Yu%20and%20Rundi%20Wu%20and%20Brandon%20Y.%20Feng%20and%20Changxi%20Zheng%20and%20Noah%20Snavely%20and%20Jiajun%20Wu%20and%20William%20T.%20Freeman%0AAbstract%3A%20%20%20Realistic%20object%20interactions%20are%20crucial%20for%20creating%20immersive%20virtual%0Aexperiences%2C%20yet%20synthesizing%20realistic%203D%20object%20dynamics%20in%20response%20to%20novel%0Ainteractions%20remains%20a%20significant%20challenge.%20Unlike%20unconditional%20or%0Atext-conditioned%20dynamics%20generation%2C%20action-conditioned%20dynamics%20requires%0Aperceiving%20the%20physical%20material%20properties%20of%20objects%20and%20grounding%20the%203D%0Amotion%20prediction%20on%20these%20properties%2C%20such%20as%20object%20stiffness.%20However%2C%0Aestimating%20physical%20material%20properties%20is%20an%20open%20problem%20due%20to%20the%20lack%20of%0Amaterial%20ground-truth%20data%2C%20as%20measuring%20these%20properties%20for%20real%20objects%20is%0Ahighly%20difficult.%20We%20present%20PhysDreamer%2C%20a%20physics-based%20approach%20that%20endows%0Astatic%203D%20objects%20with%20interactive%20dynamics%20by%20leveraging%20the%20object%20dynamics%0Apriors%20learned%20by%20video%20generation%20models.%20By%20distilling%20these%20priors%2C%0APhysDreamer%20enables%20the%20synthesis%20of%20realistic%20object%20responses%20to%20novel%0Ainteractions%2C%20such%20as%20external%20forces%20or%20agent%20manipulations.%20We%20demonstrate%0Aour%20approach%20on%20diverse%20examples%20of%20elastic%20objects%20and%20evaluate%20the%20realism%20of%0Athe%20synthesized%20interactions%20through%20a%20user%20study.%20PhysDreamer%20takes%20a%20step%0Atowards%20more%20engaging%20and%20realistic%20virtual%20experiences%20by%20enabling%20static%203D%0Aobjects%20to%20dynamically%20respond%20to%20interactive%20stimuli%20in%20a%20physically%20plausible%0Amanner.%20See%20our%20project%20page%20at%20https%3A//physdreamer.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13026v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysDreamer%3A%20Physics-Based%20Interaction%20with%203D%20Objects%20via%20Video%0A%20%20Generation&entry.906535625=Tianyuan%20Zhang%20and%20Hong-Xing%20Yu%20and%20Rundi%20Wu%20and%20Brandon%20Y.%20Feng%20and%20Changxi%20Zheng%20and%20Noah%20Snavely%20and%20Jiajun%20Wu%20and%20William%20T.%20Freeman&entry.1292438233=%20%20Realistic%20object%20interactions%20are%20crucial%20for%20creating%20immersive%20virtual%0Aexperiences%2C%20yet%20synthesizing%20realistic%203D%20object%20dynamics%20in%20response%20to%20novel%0Ainteractions%20remains%20a%20significant%20challenge.%20Unlike%20unconditional%20or%0Atext-conditioned%20dynamics%20generation%2C%20action-conditioned%20dynamics%20requires%0Aperceiving%20the%20physical%20material%20properties%20of%20objects%20and%20grounding%20the%203D%0Amotion%20prediction%20on%20these%20properties%2C%20such%20as%20object%20stiffness.%20However%2C%0Aestimating%20physical%20material%20properties%20is%20an%20open%20problem%20due%20to%20the%20lack%20of%0Amaterial%20ground-truth%20data%2C%20as%20measuring%20these%20properties%20for%20real%20objects%20is%0Ahighly%20difficult.%20We%20present%20PhysDreamer%2C%20a%20physics-based%20approach%20that%20endows%0Astatic%203D%20objects%20with%20interactive%20dynamics%20by%20leveraging%20the%20object%20dynamics%0Apriors%20learned%20by%20video%20generation%20models.%20By%20distilling%20these%20priors%2C%0APhysDreamer%20enables%20the%20synthesis%20of%20realistic%20object%20responses%20to%20novel%0Ainteractions%2C%20such%20as%20external%20forces%20or%20agent%20manipulations.%20We%20demonstrate%0Aour%20approach%20on%20diverse%20examples%20of%20elastic%20objects%20and%20evaluate%20the%20realism%20of%0Athe%20synthesized%20interactions%20through%20a%20user%20study.%20PhysDreamer%20takes%20a%20step%0Atowards%20more%20engaging%20and%20realistic%20virtual%20experiences%20by%20enabling%20static%203D%0Aobjects%20to%20dynamically%20respond%20to%20interactive%20stimuli%20in%20a%20physically%20plausible%0Amanner.%20See%20our%20project%20page%20at%20https%3A//physdreamer.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13026v1&entry.124074799=Read"},
{"title": "Unveiling the Ambiguity in Neural Inverse Rendering: A Parameter\n  Compensation Analysis", "author": "Georgios Kouros and Minye Wu and Sushruth Nagesh and Xianling Zhang and Tinne Tuytelaars", "abstract": "  Inverse rendering aims to reconstruct the scene properties of objects solely\nfrom multiview images. However, it is an ill-posed problem prone to producing\nambiguous estimations deviating from physically accurate representations. In\nthis paper, we utilize Neural Microfacet Fields (NMF), a state-of-the-art\nneural inverse rendering method to illustrate the inherent ambiguity. We\npropose an evaluation framework to assess the degree of compensation or\ninteraction between the estimated scene properties, aiming to explore the\nmechanisms behind this ill-posed problem and potential mitigation strategies.\nSpecifically, we introduce artificial perturbations to one scene property and\nexamine how adjusting another property can compensate for these perturbations.\nTo facilitate such experiments, we introduce a disentangled NMF where material\nproperties are independent. The experimental findings underscore the intrinsic\nambiguity present in neural inverse rendering and highlight the importance of\nproviding additional guidance through geometry, material, and illumination\npriors.\n", "link": "http://arxiv.org/abs/2404.12819v1", "date": "2024-04-19", "relevancy": 1.604, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5354}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5175}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Ambiguity%20in%20Neural%20Inverse%20Rendering%3A%20A%20Parameter%0A%20%20Compensation%20Analysis&body=Title%3A%20Unveiling%20the%20Ambiguity%20in%20Neural%20Inverse%20Rendering%3A%20A%20Parameter%0A%20%20Compensation%20Analysis%0AAuthor%3A%20Georgios%20Kouros%20and%20Minye%20Wu%20and%20Sushruth%20Nagesh%20and%20Xianling%20Zhang%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20%20%20Inverse%20rendering%20aims%20to%20reconstruct%20the%20scene%20properties%20of%20objects%20solely%0Afrom%20multiview%20images.%20However%2C%20it%20is%20an%20ill-posed%20problem%20prone%20to%20producing%0Aambiguous%20estimations%20deviating%20from%20physically%20accurate%20representations.%20In%0Athis%20paper%2C%20we%20utilize%20Neural%20Microfacet%20Fields%20%28NMF%29%2C%20a%20state-of-the-art%0Aneural%20inverse%20rendering%20method%20to%20illustrate%20the%20inherent%20ambiguity.%20We%0Apropose%20an%20evaluation%20framework%20to%20assess%20the%20degree%20of%20compensation%20or%0Ainteraction%20between%20the%20estimated%20scene%20properties%2C%20aiming%20to%20explore%20the%0Amechanisms%20behind%20this%20ill-posed%20problem%20and%20potential%20mitigation%20strategies.%0ASpecifically%2C%20we%20introduce%20artificial%20perturbations%20to%20one%20scene%20property%20and%0Aexamine%20how%20adjusting%20another%20property%20can%20compensate%20for%20these%20perturbations.%0ATo%20facilitate%20such%20experiments%2C%20we%20introduce%20a%20disentangled%20NMF%20where%20material%0Aproperties%20are%20independent.%20The%20experimental%20findings%20underscore%20the%20intrinsic%0Aambiguity%20present%20in%20neural%20inverse%20rendering%20and%20highlight%20the%20importance%20of%0Aproviding%20additional%20guidance%20through%20geometry%2C%20material%2C%20and%20illumination%0Apriors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12819v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Ambiguity%20in%20Neural%20Inverse%20Rendering%3A%20A%20Parameter%0A%20%20Compensation%20Analysis&entry.906535625=Georgios%20Kouros%20and%20Minye%20Wu%20and%20Sushruth%20Nagesh%20and%20Xianling%20Zhang%20and%20Tinne%20Tuytelaars&entry.1292438233=%20%20Inverse%20rendering%20aims%20to%20reconstruct%20the%20scene%20properties%20of%20objects%20solely%0Afrom%20multiview%20images.%20However%2C%20it%20is%20an%20ill-posed%20problem%20prone%20to%20producing%0Aambiguous%20estimations%20deviating%20from%20physically%20accurate%20representations.%20In%0Athis%20paper%2C%20we%20utilize%20Neural%20Microfacet%20Fields%20%28NMF%29%2C%20a%20state-of-the-art%0Aneural%20inverse%20rendering%20method%20to%20illustrate%20the%20inherent%20ambiguity.%20We%0Apropose%20an%20evaluation%20framework%20to%20assess%20the%20degree%20of%20compensation%20or%0Ainteraction%20between%20the%20estimated%20scene%20properties%2C%20aiming%20to%20explore%20the%0Amechanisms%20behind%20this%20ill-posed%20problem%20and%20potential%20mitigation%20strategies.%0ASpecifically%2C%20we%20introduce%20artificial%20perturbations%20to%20one%20scene%20property%20and%0Aexamine%20how%20adjusting%20another%20property%20can%20compensate%20for%20these%20perturbations.%0ATo%20facilitate%20such%20experiments%2C%20we%20introduce%20a%20disentangled%20NMF%20where%20material%0Aproperties%20are%20independent.%20The%20experimental%20findings%20underscore%20the%20intrinsic%0Aambiguity%20present%20in%20neural%20inverse%20rendering%20and%20highlight%20the%20importance%20of%0Aproviding%20additional%20guidance%20through%20geometry%2C%20material%2C%20and%20illumination%0Apriors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12819v1&entry.124074799=Read"},
{"title": "MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks\n  via Text Prompts", "author": "Haoqiang Guo and Sendong Zhao and Haochun Wang and Yanrui Du and Bing Qin", "abstract": "  Deep learning is now widely used in drug discovery, providing significant\nacceleration and cost reduction. As the most fundamental building block,\nmolecular representation is essential for predicting molecular properties to\nenable various downstream applications. Most existing methods attempt to\nincorporate more information to learn better representations. However, not all\nfeatures are equally important for a specific task. Ignoring this would\npotentially compromise the training efficiency and predictive accuracy. To\naddress this issue, we propose a novel approach, which treats language models\nas an agent and molecular pretraining models as a knowledge base. The agent\naccentuates task-relevant features in the molecular representation by\nunderstanding the natural language description of the task, just as a tailor\ncustomizes clothes for clients. Thus, we call this approach MolTailor.\nEvaluations demonstrate MolTailor's superior performance over baselines,\nvalidating the efficacy of enhancing relevance for molecular representation\nlearning. This illustrates the potential of language model guided optimization\nto better exploit and unleash the capabilities of existing powerful molecular\nrepresentation methods. Our code is available at\nhttps://github.com/SCIR-HI/MolTailor.\n", "link": "http://arxiv.org/abs/2401.11403v2", "date": "2024-04-19", "relevancy": 1.6015, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5541}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5153}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5016}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MolTailor%3A%20Tailoring%20Chemical%20Molecular%20Representation%20to%20Specific%20Tasks%0A%20%20via%20Text%20Prompts&body=Title%3A%20MolTailor%3A%20Tailoring%20Chemical%20Molecular%20Representation%20to%20Specific%20Tasks%0A%20%20via%20Text%20Prompts%0AAuthor%3A%20Haoqiang%20Guo%20and%20Sendong%20Zhao%20and%20Haochun%20Wang%20and%20Yanrui%20Du%20and%20Bing%20Qin%0AAbstract%3A%20%20%20Deep%20learning%20is%20now%20widely%20used%20in%20drug%20discovery%2C%20providing%20significant%0Aacceleration%20and%20cost%20reduction.%20As%20the%20most%20fundamental%20building%20block%2C%0Amolecular%20representation%20is%20essential%20for%20predicting%20molecular%20properties%20to%0Aenable%20various%20downstream%20applications.%20Most%20existing%20methods%20attempt%20to%0Aincorporate%20more%20information%20to%20learn%20better%20representations.%20However%2C%20not%20all%0Afeatures%20are%20equally%20important%20for%20a%20specific%20task.%20Ignoring%20this%20would%0Apotentially%20compromise%20the%20training%20efficiency%20and%20predictive%20accuracy.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20approach%2C%20which%20treats%20language%20models%0Aas%20an%20agent%20and%20molecular%20pretraining%20models%20as%20a%20knowledge%20base.%20The%20agent%0Aaccentuates%20task-relevant%20features%20in%20the%20molecular%20representation%20by%0Aunderstanding%20the%20natural%20language%20description%20of%20the%20task%2C%20just%20as%20a%20tailor%0Acustomizes%20clothes%20for%20clients.%20Thus%2C%20we%20call%20this%20approach%20MolTailor.%0AEvaluations%20demonstrate%20MolTailor%27s%20superior%20performance%20over%20baselines%2C%0Avalidating%20the%20efficacy%20of%20enhancing%20relevance%20for%20molecular%20representation%0Alearning.%20This%20illustrates%20the%20potential%20of%20language%20model%20guided%20optimization%0Ato%20better%20exploit%20and%20unleash%20the%20capabilities%20of%20existing%20powerful%20molecular%0Arepresentation%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/SCIR-HI/MolTailor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11403v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolTailor%3A%20Tailoring%20Chemical%20Molecular%20Representation%20to%20Specific%20Tasks%0A%20%20via%20Text%20Prompts&entry.906535625=Haoqiang%20Guo%20and%20Sendong%20Zhao%20and%20Haochun%20Wang%20and%20Yanrui%20Du%20and%20Bing%20Qin&entry.1292438233=%20%20Deep%20learning%20is%20now%20widely%20used%20in%20drug%20discovery%2C%20providing%20significant%0Aacceleration%20and%20cost%20reduction.%20As%20the%20most%20fundamental%20building%20block%2C%0Amolecular%20representation%20is%20essential%20for%20predicting%20molecular%20properties%20to%0Aenable%20various%20downstream%20applications.%20Most%20existing%20methods%20attempt%20to%0Aincorporate%20more%20information%20to%20learn%20better%20representations.%20However%2C%20not%20all%0Afeatures%20are%20equally%20important%20for%20a%20specific%20task.%20Ignoring%20this%20would%0Apotentially%20compromise%20the%20training%20efficiency%20and%20predictive%20accuracy.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20approach%2C%20which%20treats%20language%20models%0Aas%20an%20agent%20and%20molecular%20pretraining%20models%20as%20a%20knowledge%20base.%20The%20agent%0Aaccentuates%20task-relevant%20features%20in%20the%20molecular%20representation%20by%0Aunderstanding%20the%20natural%20language%20description%20of%20the%20task%2C%20just%20as%20a%20tailor%0Acustomizes%20clothes%20for%20clients.%20Thus%2C%20we%20call%20this%20approach%20MolTailor.%0AEvaluations%20demonstrate%20MolTailor%27s%20superior%20performance%20over%20baselines%2C%0Avalidating%20the%20efficacy%20of%20enhancing%20relevance%20for%20molecular%20representation%0Alearning.%20This%20illustrates%20the%20potential%20of%20language%20model%20guided%20optimization%0Ato%20better%20exploit%20and%20unleash%20the%20capabilities%20of%20existing%20powerful%20molecular%0Arepresentation%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/SCIR-HI/MolTailor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11403v2&entry.124074799=Read"},
{"title": "How Does the Textual Information Affect the Retrieval of Multimodal\n  In-Context Learning?", "author": "Yang Luo and Zangwei Zheng and Zirui Zhu and Yang You", "abstract": "  The increase in parameter size of multimodal large language models (MLLMs)\nintroduces significant capabilities, particularly in-context learning, where\nMLLMs enhance task performance without updating pre-trained parameters. This\neffectiveness, however, hinges on the appropriate selection of in-context\nexamples, a process that is currently biased towards visual data, overlooking\ntextual information. Furthermore, the area of supervised retrievers for MLLMs,\ncrucial for optimal in-context example selection, continues to be\nuninvestigated. Our study offers an in-depth evaluation of the impact of\ntextual information on the unsupervised selection of in-context examples in\nmultimodal contexts, uncovering a notable sensitivity of retriever performance\nto the employed modalities. Responding to this, we introduce a novel supervised\nMLLM-retriever MSIER that employs a neural network to select examples that\nenhance multimodal in-context learning efficiency. This approach is validated\nthrough extensive testing across three distinct tasks, demonstrating the\nmethod's effectiveness. Additionally, we investigate the influence of\nmodalities on our supervised retrieval method's training and pinpoint factors\ncontributing to our model's success. This exploration paves the way for future\nadvancements, highlighting the potential for refined in-context learning in\nMLLMs through the strategic use of multimodal data.\n", "link": "http://arxiv.org/abs/2404.12866v1", "date": "2024-04-19", "relevancy": 1.594, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5519}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.515}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20Does%20the%20Textual%20Information%20Affect%20the%20Retrieval%20of%20Multimodal%0A%20%20In-Context%20Learning%3F&body=Title%3A%20How%20Does%20the%20Textual%20Information%20Affect%20the%20Retrieval%20of%20Multimodal%0A%20%20In-Context%20Learning%3F%0AAuthor%3A%20Yang%20Luo%20and%20Zangwei%20Zheng%20and%20Zirui%20Zhu%20and%20Yang%20You%0AAbstract%3A%20%20%20The%20increase%20in%20parameter%20size%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%0Aintroduces%20significant%20capabilities%2C%20particularly%20in-context%20learning%2C%20where%0AMLLMs%20enhance%20task%20performance%20without%20updating%20pre-trained%20parameters.%20This%0Aeffectiveness%2C%20however%2C%20hinges%20on%20the%20appropriate%20selection%20of%20in-context%0Aexamples%2C%20a%20process%20that%20is%20currently%20biased%20towards%20visual%20data%2C%20overlooking%0Atextual%20information.%20Furthermore%2C%20the%20area%20of%20supervised%20retrievers%20for%20MLLMs%2C%0Acrucial%20for%20optimal%20in-context%20example%20selection%2C%20continues%20to%20be%0Auninvestigated.%20Our%20study%20offers%20an%20in-depth%20evaluation%20of%20the%20impact%20of%0Atextual%20information%20on%20the%20unsupervised%20selection%20of%20in-context%20examples%20in%0Amultimodal%20contexts%2C%20uncovering%20a%20notable%20sensitivity%20of%20retriever%20performance%0Ato%20the%20employed%20modalities.%20Responding%20to%20this%2C%20we%20introduce%20a%20novel%20supervised%0AMLLM-retriever%20MSIER%20that%20employs%20a%20neural%20network%20to%20select%20examples%20that%0Aenhance%20multimodal%20in-context%20learning%20efficiency.%20This%20approach%20is%20validated%0Athrough%20extensive%20testing%20across%20three%20distinct%20tasks%2C%20demonstrating%20the%0Amethod%27s%20effectiveness.%20Additionally%2C%20we%20investigate%20the%20influence%20of%0Amodalities%20on%20our%20supervised%20retrieval%20method%27s%20training%20and%20pinpoint%20factors%0Acontributing%20to%20our%20model%27s%20success.%20This%20exploration%20paves%20the%20way%20for%20future%0Aadvancements%2C%20highlighting%20the%20potential%20for%20refined%20in-context%20learning%20in%0AMLLMs%20through%20the%20strategic%20use%20of%20multimodal%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12866v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Does%20the%20Textual%20Information%20Affect%20the%20Retrieval%20of%20Multimodal%0A%20%20In-Context%20Learning%3F&entry.906535625=Yang%20Luo%20and%20Zangwei%20Zheng%20and%20Zirui%20Zhu%20and%20Yang%20You&entry.1292438233=%20%20The%20increase%20in%20parameter%20size%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%0Aintroduces%20significant%20capabilities%2C%20particularly%20in-context%20learning%2C%20where%0AMLLMs%20enhance%20task%20performance%20without%20updating%20pre-trained%20parameters.%20This%0Aeffectiveness%2C%20however%2C%20hinges%20on%20the%20appropriate%20selection%20of%20in-context%0Aexamples%2C%20a%20process%20that%20is%20currently%20biased%20towards%20visual%20data%2C%20overlooking%0Atextual%20information.%20Furthermore%2C%20the%20area%20of%20supervised%20retrievers%20for%20MLLMs%2C%0Acrucial%20for%20optimal%20in-context%20example%20selection%2C%20continues%20to%20be%0Auninvestigated.%20Our%20study%20offers%20an%20in-depth%20evaluation%20of%20the%20impact%20of%0Atextual%20information%20on%20the%20unsupervised%20selection%20of%20in-context%20examples%20in%0Amultimodal%20contexts%2C%20uncovering%20a%20notable%20sensitivity%20of%20retriever%20performance%0Ato%20the%20employed%20modalities.%20Responding%20to%20this%2C%20we%20introduce%20a%20novel%20supervised%0AMLLM-retriever%20MSIER%20that%20employs%20a%20neural%20network%20to%20select%20examples%20that%0Aenhance%20multimodal%20in-context%20learning%20efficiency.%20This%20approach%20is%20validated%0Athrough%20extensive%20testing%20across%20three%20distinct%20tasks%2C%20demonstrating%20the%0Amethod%27s%20effectiveness.%20Additionally%2C%20we%20investigate%20the%20influence%20of%0Amodalities%20on%20our%20supervised%20retrieval%20method%27s%20training%20and%20pinpoint%20factors%0Acontributing%20to%20our%20model%27s%20success.%20This%20exploration%20paves%20the%20way%20for%20future%0Aadvancements%2C%20highlighting%20the%20potential%20for%20refined%20in-context%20learning%20in%0AMLLMs%20through%20the%20strategic%20use%20of%20multimodal%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12866v1&entry.124074799=Read"},
{"title": "Succinct Interaction-Aware Explanations", "author": "Sascha Xu and Joscha C\u00fcppers and Jilles Vreeken", "abstract": "  SHAP is a popular approach to explain black-box models by revealing the\nimportance of individual features. As it ignores feature interactions, SHAP\nexplanations can be confusing up to misleading. NSHAP, on the other hand,\nreports the additive importance for all subsets of features. While this does\ninclude all interacting sets of features, it also leads to an exponentially\nsized, difficult to interpret explanation. In this paper, we propose to combine\nthe best of these two worlds, by partitioning the features into parts that\nsignificantly interact, and use these parts to compose a succinct,\ninterpretable, additive explanation. We derive a criterion by which to measure\nthe representativeness of such a partition for a models behavior, traded off\nagainst the complexity of the resulting explanation. To efficiently find the\nbest partition out of super-exponentially many, we show how to prune\nsub-optimal solutions using a statistical test, which not only improves runtime\nbut also helps to detect spurious interactions. Experiments on synthetic and\nreal world data show that our explanations are both more accurate resp. more\neasily interpretable than those of SHAP and NSHAP.\n", "link": "http://arxiv.org/abs/2402.05566v2", "date": "2024-04-19", "relevancy": 1.2977, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4626}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4254}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4234}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Succinct%20Interaction-Aware%20Explanations&body=Title%3A%20Succinct%20Interaction-Aware%20Explanations%0AAuthor%3A%20Sascha%20Xu%20and%20Joscha%20C%C3%BCppers%20and%20Jilles%20Vreeken%0AAbstract%3A%20%20%20SHAP%20is%20a%20popular%20approach%20to%20explain%20black-box%20models%20by%20revealing%20the%0Aimportance%20of%20individual%20features.%20As%20it%20ignores%20feature%20interactions%2C%20SHAP%0Aexplanations%20can%20be%20confusing%20up%20to%20misleading.%20NSHAP%2C%20on%20the%20other%20hand%2C%0Areports%20the%20additive%20importance%20for%20all%20subsets%20of%20features.%20While%20this%20does%0Ainclude%20all%20interacting%20sets%20of%20features%2C%20it%20also%20leads%20to%20an%20exponentially%0Asized%2C%20difficult%20to%20interpret%20explanation.%20In%20this%20paper%2C%20we%20propose%20to%20combine%0Athe%20best%20of%20these%20two%20worlds%2C%20by%20partitioning%20the%20features%20into%20parts%20that%0Asignificantly%20interact%2C%20and%20use%20these%20parts%20to%20compose%20a%20succinct%2C%0Ainterpretable%2C%20additive%20explanation.%20We%20derive%20a%20criterion%20by%20which%20to%20measure%0Athe%20representativeness%20of%20such%20a%20partition%20for%20a%20models%20behavior%2C%20traded%20off%0Aagainst%20the%20complexity%20of%20the%20resulting%20explanation.%20To%20efficiently%20find%20the%0Abest%20partition%20out%20of%20super-exponentially%20many%2C%20we%20show%20how%20to%20prune%0Asub-optimal%20solutions%20using%20a%20statistical%20test%2C%20which%20not%20only%20improves%20runtime%0Abut%20also%20helps%20to%20detect%20spurious%20interactions.%20Experiments%20on%20synthetic%20and%0Areal%20world%20data%20show%20that%20our%20explanations%20are%20both%20more%20accurate%20resp.%20more%0Aeasily%20interpretable%20than%20those%20of%20SHAP%20and%20NSHAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05566v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Succinct%20Interaction-Aware%20Explanations&entry.906535625=Sascha%20Xu%20and%20Joscha%20C%C3%BCppers%20and%20Jilles%20Vreeken&entry.1292438233=%20%20SHAP%20is%20a%20popular%20approach%20to%20explain%20black-box%20models%20by%20revealing%20the%0Aimportance%20of%20individual%20features.%20As%20it%20ignores%20feature%20interactions%2C%20SHAP%0Aexplanations%20can%20be%20confusing%20up%20to%20misleading.%20NSHAP%2C%20on%20the%20other%20hand%2C%0Areports%20the%20additive%20importance%20for%20all%20subsets%20of%20features.%20While%20this%20does%0Ainclude%20all%20interacting%20sets%20of%20features%2C%20it%20also%20leads%20to%20an%20exponentially%0Asized%2C%20difficult%20to%20interpret%20explanation.%20In%20this%20paper%2C%20we%20propose%20to%20combine%0Athe%20best%20of%20these%20two%20worlds%2C%20by%20partitioning%20the%20features%20into%20parts%20that%0Asignificantly%20interact%2C%20and%20use%20these%20parts%20to%20compose%20a%20succinct%2C%0Ainterpretable%2C%20additive%20explanation.%20We%20derive%20a%20criterion%20by%20which%20to%20measure%0Athe%20representativeness%20of%20such%20a%20partition%20for%20a%20models%20behavior%2C%20traded%20off%0Aagainst%20the%20complexity%20of%20the%20resulting%20explanation.%20To%20efficiently%20find%20the%0Abest%20partition%20out%20of%20super-exponentially%20many%2C%20we%20show%20how%20to%20prune%0Asub-optimal%20solutions%20using%20a%20statistical%20test%2C%20which%20not%20only%20improves%20runtime%0Abut%20also%20helps%20to%20detect%20spurious%20interactions.%20Experiments%20on%20synthetic%20and%0Areal%20world%20data%20show%20that%20our%20explanations%20are%20both%20more%20accurate%20resp.%20more%0Aeasily%20interpretable%20than%20those%20of%20SHAP%20and%20NSHAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05566v2&entry.124074799=Read"},
{"title": "LaPA: Latent Prompt Assist Model For Medical Visual Question Answering", "author": "Tiancheng Gu and Kaicheng Yang and Dongnan Liu and Weidong Cai", "abstract": "  Medical visual question answering (Med-VQA) aims to automate the prediction\nof correct answers for medical images and questions, thereby assisting\nphysicians in reducing repetitive tasks and alleviating their workload.\nExisting approaches primarily focus on pre-training models using additional and\ncomprehensive datasets, followed by fine-tuning to enhance performance in\ndownstream tasks. However, there is also significant value in exploring\nexisting models to extract clinically relevant information. In this paper, we\npropose the Latent Prompt Assist model (LaPA) for medical visual question\nanswering. Firstly, we design a latent prompt generation module to generate the\nlatent prompt with the constraint of the target answer. Subsequently, we\npropose a multi-modal fusion block with latent prompt fusion module that\nutilizes the latent prompt to extract clinical-relevant information from\nuni-modal and multi-modal features. Additionally, we introduce a prior\nknowledge fusion module to integrate the relationship between diseases and\norgans with the clinical-relevant information. Finally, we combine the final\nintegrated information with image-language cross-modal information to predict\nthe final answers. Experimental results on three publicly available Med-VQA\ndatasets demonstrate that LaPA outperforms the state-of-the-art model ARL,\nachieving improvements of 1.83%, 0.63%, and 1.80% on VQA-RAD, SLAKE, and\nVQA-2019, respectively. The code is publicly available at\nhttps://github.com/GaryGuTC/LaPA_model.\n", "link": "http://arxiv.org/abs/2404.13039v1", "date": "2024-04-19", "relevancy": 1.4916, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5126}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4952}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4867}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LaPA%3A%20Latent%20Prompt%20Assist%20Model%20For%20Medical%20Visual%20Question%20Answering&body=Title%3A%20LaPA%3A%20Latent%20Prompt%20Assist%20Model%20For%20Medical%20Visual%20Question%20Answering%0AAuthor%3A%20Tiancheng%20Gu%20and%20Kaicheng%20Yang%20and%20Dongnan%20Liu%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Medical%20visual%20question%20answering%20%28Med-VQA%29%20aims%20to%20automate%20the%20prediction%0Aof%20correct%20answers%20for%20medical%20images%20and%20questions%2C%20thereby%20assisting%0Aphysicians%20in%20reducing%20repetitive%20tasks%20and%20alleviating%20their%20workload.%0AExisting%20approaches%20primarily%20focus%20on%20pre-training%20models%20using%20additional%20and%0Acomprehensive%20datasets%2C%20followed%20by%20fine-tuning%20to%20enhance%20performance%20in%0Adownstream%20tasks.%20However%2C%20there%20is%20also%20significant%20value%20in%20exploring%0Aexisting%20models%20to%20extract%20clinically%20relevant%20information.%20In%20this%20paper%2C%20we%0Apropose%20the%20Latent%20Prompt%20Assist%20model%20%28LaPA%29%20for%20medical%20visual%20question%0Aanswering.%20Firstly%2C%20we%20design%20a%20latent%20prompt%20generation%20module%20to%20generate%20the%0Alatent%20prompt%20with%20the%20constraint%20of%20the%20target%20answer.%20Subsequently%2C%20we%0Apropose%20a%20multi-modal%20fusion%20block%20with%20latent%20prompt%20fusion%20module%20that%0Autilizes%20the%20latent%20prompt%20to%20extract%20clinical-relevant%20information%20from%0Auni-modal%20and%20multi-modal%20features.%20Additionally%2C%20we%20introduce%20a%20prior%0Aknowledge%20fusion%20module%20to%20integrate%20the%20relationship%20between%20diseases%20and%0Aorgans%20with%20the%20clinical-relevant%20information.%20Finally%2C%20we%20combine%20the%20final%0Aintegrated%20information%20with%20image-language%20cross-modal%20information%20to%20predict%0Athe%20final%20answers.%20Experimental%20results%20on%20three%20publicly%20available%20Med-VQA%0Adatasets%20demonstrate%20that%20LaPA%20outperforms%20the%20state-of-the-art%20model%20ARL%2C%0Aachieving%20improvements%20of%201.83%25%2C%200.63%25%2C%20and%201.80%25%20on%20VQA-RAD%2C%20SLAKE%2C%20and%0AVQA-2019%2C%20respectively.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/GaryGuTC/LaPA_model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13039v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaPA%3A%20Latent%20Prompt%20Assist%20Model%20For%20Medical%20Visual%20Question%20Answering&entry.906535625=Tiancheng%20Gu%20and%20Kaicheng%20Yang%20and%20Dongnan%20Liu%20and%20Weidong%20Cai&entry.1292438233=%20%20Medical%20visual%20question%20answering%20%28Med-VQA%29%20aims%20to%20automate%20the%20prediction%0Aof%20correct%20answers%20for%20medical%20images%20and%20questions%2C%20thereby%20assisting%0Aphysicians%20in%20reducing%20repetitive%20tasks%20and%20alleviating%20their%20workload.%0AExisting%20approaches%20primarily%20focus%20on%20pre-training%20models%20using%20additional%20and%0Acomprehensive%20datasets%2C%20followed%20by%20fine-tuning%20to%20enhance%20performance%20in%0Adownstream%20tasks.%20However%2C%20there%20is%20also%20significant%20value%20in%20exploring%0Aexisting%20models%20to%20extract%20clinically%20relevant%20information.%20In%20this%20paper%2C%20we%0Apropose%20the%20Latent%20Prompt%20Assist%20model%20%28LaPA%29%20for%20medical%20visual%20question%0Aanswering.%20Firstly%2C%20we%20design%20a%20latent%20prompt%20generation%20module%20to%20generate%20the%0Alatent%20prompt%20with%20the%20constraint%20of%20the%20target%20answer.%20Subsequently%2C%20we%0Apropose%20a%20multi-modal%20fusion%20block%20with%20latent%20prompt%20fusion%20module%20that%0Autilizes%20the%20latent%20prompt%20to%20extract%20clinical-relevant%20information%20from%0Auni-modal%20and%20multi-modal%20features.%20Additionally%2C%20we%20introduce%20a%20prior%0Aknowledge%20fusion%20module%20to%20integrate%20the%20relationship%20between%20diseases%20and%0Aorgans%20with%20the%20clinical-relevant%20information.%20Finally%2C%20we%20combine%20the%20final%0Aintegrated%20information%20with%20image-language%20cross-modal%20information%20to%20predict%0Athe%20final%20answers.%20Experimental%20results%20on%20three%20publicly%20available%20Med-VQA%0Adatasets%20demonstrate%20that%20LaPA%20outperforms%20the%20state-of-the-art%20model%20ARL%2C%0Aachieving%20improvements%20of%201.83%25%2C%200.63%25%2C%20and%201.80%25%20on%20VQA-RAD%2C%20SLAKE%2C%20and%0AVQA-2019%2C%20respectively.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/GaryGuTC/LaPA_model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13039v1&entry.124074799=Read"},
{"title": "Defining Effective Engagement For Enhancing Cancer Patients' Well-being\n  with Mobile Digital Behavior Change Interventions", "author": "Aneta Lisowska and Szymon Wilk and Laura Locati and Mimma Rizzo and Lucia Sacchi and Silvana Quaglini and Matteo Terzaghi and Valentina Tibollo and Mor Peleg", "abstract": "  Digital Behavior Change Interventions (DBCIs) are supporting development of\nnew health behaviors. Evaluating their effectiveness is crucial for their\nimprovement and understanding of success factors. However, comprehensive\nguidance for developers, particularly in small-scale studies with ethical\nconstraints, is limited. Building on the CAPABLE project, this study aims to\ndefine effective engagement with DBCIs for supporting cancer patients in\nenhancing their quality of life. We identify metrics for measuring engagement,\nexplore the interest of both patients and clinicians in DBCIs, and propose\nhypotheses for assessing the impact of DBCIs in such contexts. Our findings\nsuggest that clinician prescriptions significantly increase sustained\nengagement with mobile DBCIs. In addition, while one weekly engagement with a\nDBCI is sufficient to maintain well-being, transitioning from extrinsic to\nintrinsic motivation may require a higher level of engagement.\n", "link": "http://arxiv.org/abs/2403.12007v3", "date": "2024-04-19", "relevancy": 1.4878, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3818}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3695}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3533}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Defining%20Effective%20Engagement%20For%20Enhancing%20Cancer%20Patients%27%20Well-being%0A%20%20with%20Mobile%20Digital%20Behavior%20Change%20Interventions&body=Title%3A%20Defining%20Effective%20Engagement%20For%20Enhancing%20Cancer%20Patients%27%20Well-being%0A%20%20with%20Mobile%20Digital%20Behavior%20Change%20Interventions%0AAuthor%3A%20Aneta%20Lisowska%20and%20Szymon%20Wilk%20and%20Laura%20Locati%20and%20Mimma%20Rizzo%20and%20Lucia%20Sacchi%20and%20Silvana%20Quaglini%20and%20Matteo%20Terzaghi%20and%20Valentina%20Tibollo%20and%20Mor%20Peleg%0AAbstract%3A%20%20%20Digital%20Behavior%20Change%20Interventions%20%28DBCIs%29%20are%20supporting%20development%20of%0Anew%20health%20behaviors.%20Evaluating%20their%20effectiveness%20is%20crucial%20for%20their%0Aimprovement%20and%20understanding%20of%20success%20factors.%20However%2C%20comprehensive%0Aguidance%20for%20developers%2C%20particularly%20in%20small-scale%20studies%20with%20ethical%0Aconstraints%2C%20is%20limited.%20Building%20on%20the%20CAPABLE%20project%2C%20this%20study%20aims%20to%0Adefine%20effective%20engagement%20with%20DBCIs%20for%20supporting%20cancer%20patients%20in%0Aenhancing%20their%20quality%20of%20life.%20We%20identify%20metrics%20for%20measuring%20engagement%2C%0Aexplore%20the%20interest%20of%20both%20patients%20and%20clinicians%20in%20DBCIs%2C%20and%20propose%0Ahypotheses%20for%20assessing%20the%20impact%20of%20DBCIs%20in%20such%20contexts.%20Our%20findings%0Asuggest%20that%20clinician%20prescriptions%20significantly%20increase%20sustained%0Aengagement%20with%20mobile%20DBCIs.%20In%20addition%2C%20while%20one%20weekly%20engagement%20with%20a%0ADBCI%20is%20sufficient%20to%20maintain%20well-being%2C%20transitioning%20from%20extrinsic%20to%0Aintrinsic%20motivation%20may%20require%20a%20higher%20level%20of%20engagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12007v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defining%20Effective%20Engagement%20For%20Enhancing%20Cancer%20Patients%27%20Well-being%0A%20%20with%20Mobile%20Digital%20Behavior%20Change%20Interventions&entry.906535625=Aneta%20Lisowska%20and%20Szymon%20Wilk%20and%20Laura%20Locati%20and%20Mimma%20Rizzo%20and%20Lucia%20Sacchi%20and%20Silvana%20Quaglini%20and%20Matteo%20Terzaghi%20and%20Valentina%20Tibollo%20and%20Mor%20Peleg&entry.1292438233=%20%20Digital%20Behavior%20Change%20Interventions%20%28DBCIs%29%20are%20supporting%20development%20of%0Anew%20health%20behaviors.%20Evaluating%20their%20effectiveness%20is%20crucial%20for%20their%0Aimprovement%20and%20understanding%20of%20success%20factors.%20However%2C%20comprehensive%0Aguidance%20for%20developers%2C%20particularly%20in%20small-scale%20studies%20with%20ethical%0Aconstraints%2C%20is%20limited.%20Building%20on%20the%20CAPABLE%20project%2C%20this%20study%20aims%20to%0Adefine%20effective%20engagement%20with%20DBCIs%20for%20supporting%20cancer%20patients%20in%0Aenhancing%20their%20quality%20of%20life.%20We%20identify%20metrics%20for%20measuring%20engagement%2C%0Aexplore%20the%20interest%20of%20both%20patients%20and%20clinicians%20in%20DBCIs%2C%20and%20propose%0Ahypotheses%20for%20assessing%20the%20impact%20of%20DBCIs%20in%20such%20contexts.%20Our%20findings%0Asuggest%20that%20clinician%20prescriptions%20significantly%20increase%20sustained%0Aengagement%20with%20mobile%20DBCIs.%20In%20addition%2C%20while%20one%20weekly%20engagement%20with%20a%0ADBCI%20is%20sufficient%20to%20maintain%20well-being%2C%20transitioning%20from%20extrinsic%20to%0Aintrinsic%20motivation%20may%20require%20a%20higher%20level%20of%20engagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12007v3&entry.124074799=Read"},
{"title": "Enhancing Interval Type-2 Fuzzy Logic Systems: Learning for Precision\n  and Prediction Intervals", "author": "Ata Koklu and Yusuf Guven and Tufan Kumbasar", "abstract": "  In this paper, we tackle the task of generating Prediction Intervals (PIs) in\nhigh-risk scenarios by proposing enhancements for learning Interval Type-2\n(IT2) Fuzzy Logic Systems (FLSs) to address their learning challenges. In this\ncontext, we first provide extra design flexibility to the Karnik-Mendel (KM)\nand Nie-Tan (NT) center of sets calculation methods to increase their\nflexibility for generating PIs. These enhancements increase the flexibility of\nKM in the defuzzification stage while the NT in the fuzzification stage. To\naddress the large-scale learning challenge, we transform the IT2-FLS's\nconstraint learning problem into an unconstrained form via parameterization\ntricks, enabling the direct application of deep learning optimizers. To address\nthe curse of dimensionality issue, we expand the High-Dimensional\nTakagi-Sugeno-Kang (HTSK) method proposed for type-1 FLS to IT2-FLSs, resulting\nin the HTSK2 approach. Additionally, we introduce a framework to learn the\nenhanced IT2-FLS with a dual focus, aiming for high precision and PI\ngeneration. Through exhaustive statistical results, we reveal that HTSK2\neffectively addresses the dimensionality challenge, while the enhanced KM and\nNT methods improved learning and enhanced uncertainty quantification\nperformances of IT2-FLSs.\n", "link": "http://arxiv.org/abs/2404.12802v1", "date": "2024-04-19", "relevancy": 1.4132, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4745}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.473}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4605}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Interval%20Type-2%20Fuzzy%20Logic%20Systems%3A%20Learning%20for%20Precision%0A%20%20and%20Prediction%20Intervals&body=Title%3A%20Enhancing%20Interval%20Type-2%20Fuzzy%20Logic%20Systems%3A%20Learning%20for%20Precision%0A%20%20and%20Prediction%20Intervals%0AAuthor%3A%20Ata%20Koklu%20and%20Yusuf%20Guven%20and%20Tufan%20Kumbasar%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20tackle%20the%20task%20of%20generating%20Prediction%20Intervals%20%28PIs%29%20in%0Ahigh-risk%20scenarios%20by%20proposing%20enhancements%20for%20learning%20Interval%20Type-2%0A%28IT2%29%20Fuzzy%20Logic%20Systems%20%28FLSs%29%20to%20address%20their%20learning%20challenges.%20In%20this%0Acontext%2C%20we%20first%20provide%20extra%20design%20flexibility%20to%20the%20Karnik-Mendel%20%28KM%29%0Aand%20Nie-Tan%20%28NT%29%20center%20of%20sets%20calculation%20methods%20to%20increase%20their%0Aflexibility%20for%20generating%20PIs.%20These%20enhancements%20increase%20the%20flexibility%20of%0AKM%20in%20the%20defuzzification%20stage%20while%20the%20NT%20in%20the%20fuzzification%20stage.%20To%0Aaddress%20the%20large-scale%20learning%20challenge%2C%20we%20transform%20the%20IT2-FLS%27s%0Aconstraint%20learning%20problem%20into%20an%20unconstrained%20form%20via%20parameterization%0Atricks%2C%20enabling%20the%20direct%20application%20of%20deep%20learning%20optimizers.%20To%20address%0Athe%20curse%20of%20dimensionality%20issue%2C%20we%20expand%20the%20High-Dimensional%0ATakagi-Sugeno-Kang%20%28HTSK%29%20method%20proposed%20for%20type-1%20FLS%20to%20IT2-FLSs%2C%20resulting%0Ain%20the%20HTSK2%20approach.%20Additionally%2C%20we%20introduce%20a%20framework%20to%20learn%20the%0Aenhanced%20IT2-FLS%20with%20a%20dual%20focus%2C%20aiming%20for%20high%20precision%20and%20PI%0Ageneration.%20Through%20exhaustive%20statistical%20results%2C%20we%20reveal%20that%20HTSK2%0Aeffectively%20addresses%20the%20dimensionality%20challenge%2C%20while%20the%20enhanced%20KM%20and%0ANT%20methods%20improved%20learning%20and%20enhanced%20uncertainty%20quantification%0Aperformances%20of%20IT2-FLSs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12802v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Interval%20Type-2%20Fuzzy%20Logic%20Systems%3A%20Learning%20for%20Precision%0A%20%20and%20Prediction%20Intervals&entry.906535625=Ata%20Koklu%20and%20Yusuf%20Guven%20and%20Tufan%20Kumbasar&entry.1292438233=%20%20In%20this%20paper%2C%20we%20tackle%20the%20task%20of%20generating%20Prediction%20Intervals%20%28PIs%29%20in%0Ahigh-risk%20scenarios%20by%20proposing%20enhancements%20for%20learning%20Interval%20Type-2%0A%28IT2%29%20Fuzzy%20Logic%20Systems%20%28FLSs%29%20to%20address%20their%20learning%20challenges.%20In%20this%0Acontext%2C%20we%20first%20provide%20extra%20design%20flexibility%20to%20the%20Karnik-Mendel%20%28KM%29%0Aand%20Nie-Tan%20%28NT%29%20center%20of%20sets%20calculation%20methods%20to%20increase%20their%0Aflexibility%20for%20generating%20PIs.%20These%20enhancements%20increase%20the%20flexibility%20of%0AKM%20in%20the%20defuzzification%20stage%20while%20the%20NT%20in%20the%20fuzzification%20stage.%20To%0Aaddress%20the%20large-scale%20learning%20challenge%2C%20we%20transform%20the%20IT2-FLS%27s%0Aconstraint%20learning%20problem%20into%20an%20unconstrained%20form%20via%20parameterization%0Atricks%2C%20enabling%20the%20direct%20application%20of%20deep%20learning%20optimizers.%20To%20address%0Athe%20curse%20of%20dimensionality%20issue%2C%20we%20expand%20the%20High-Dimensional%0ATakagi-Sugeno-Kang%20%28HTSK%29%20method%20proposed%20for%20type-1%20FLS%20to%20IT2-FLSs%2C%20resulting%0Ain%20the%20HTSK2%20approach.%20Additionally%2C%20we%20introduce%20a%20framework%20to%20learn%20the%0Aenhanced%20IT2-FLS%20with%20a%20dual%20focus%2C%20aiming%20for%20high%20precision%20and%20PI%0Ageneration.%20Through%20exhaustive%20statistical%20results%2C%20we%20reveal%20that%20HTSK2%0Aeffectively%20addresses%20the%20dimensionality%20challenge%2C%20while%20the%20enhanced%20KM%20and%0ANT%20methods%20improved%20learning%20and%20enhanced%20uncertainty%20quantification%0Aperformances%20of%20IT2-FLSs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12802v1&entry.124074799=Read"},
{"title": "Neural Flow Diffusion Models: Learnable Forward Process for Improved\n  Diffusion Modelling", "author": "Grigory Bartosh and Dmitry Vetrov and Christian A. Naesseth", "abstract": "  Conventional diffusion models typically relies on a fixed forward process,\nwhich implicitly defines complex marginal distributions over latent variables.\nThis can often complicate the reverse process' task in learning generative\ntrajectories, and results in costly inference for diffusion models. To address\nthese limitations, we introduce Neural Flow Diffusion Models (NFDM), a novel\nframework that enhances diffusion models by supporting a broader range of\nforward processes beyond the fixed linear Gaussian. We also propose a novel\nparameterization technique for learning the forward process. Our framework\nprovides an end-to-end, simulation-free optimization objective, effectively\nminimizing a variational upper bound on the negative log-likelihood.\nExperimental results demonstrate NFDM's strong performance, evidenced by\nstate-of-the-art likelihood estimation. Furthermore, we investigate NFDM's\ncapacity for learning generative dynamics with specific characteristics, such\nas deterministic straight lines trajectories. This exploration underscores\nNFDM's versatility and its potential for a wide range of applications.\n", "link": "http://arxiv.org/abs/2404.12940v1", "date": "2024-04-19", "relevancy": 1.2083, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6432}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6183}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.551}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Flow%20Diffusion%20Models%3A%20Learnable%20Forward%20Process%20for%20Improved%0A%20%20Diffusion%20Modelling&body=Title%3A%20Neural%20Flow%20Diffusion%20Models%3A%20Learnable%20Forward%20Process%20for%20Improved%0A%20%20Diffusion%20Modelling%0AAuthor%3A%20Grigory%20Bartosh%20and%20Dmitry%20Vetrov%20and%20Christian%20A.%20Naesseth%0AAbstract%3A%20%20%20Conventional%20diffusion%20models%20typically%20relies%20on%20a%20fixed%20forward%20process%2C%0Awhich%20implicitly%20defines%20complex%20marginal%20distributions%20over%20latent%20variables.%0AThis%20can%20often%20complicate%20the%20reverse%20process%27%20task%20in%20learning%20generative%0Atrajectories%2C%20and%20results%20in%20costly%20inference%20for%20diffusion%20models.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20Neural%20Flow%20Diffusion%20Models%20%28NFDM%29%2C%20a%20novel%0Aframework%20that%20enhances%20diffusion%20models%20by%20supporting%20a%20broader%20range%20of%0Aforward%20processes%20beyond%20the%20fixed%20linear%20Gaussian.%20We%20also%20propose%20a%20novel%0Aparameterization%20technique%20for%20learning%20the%20forward%20process.%20Our%20framework%0Aprovides%20an%20end-to-end%2C%20simulation-free%20optimization%20objective%2C%20effectively%0Aminimizing%20a%20variational%20upper%20bound%20on%20the%20negative%20log-likelihood.%0AExperimental%20results%20demonstrate%20NFDM%27s%20strong%20performance%2C%20evidenced%20by%0Astate-of-the-art%20likelihood%20estimation.%20Furthermore%2C%20we%20investigate%20NFDM%27s%0Acapacity%20for%20learning%20generative%20dynamics%20with%20specific%20characteristics%2C%20such%0Aas%20deterministic%20straight%20lines%20trajectories.%20This%20exploration%20underscores%0ANFDM%27s%20versatility%20and%20its%20potential%20for%20a%20wide%20range%20of%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12940v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Flow%20Diffusion%20Models%3A%20Learnable%20Forward%20Process%20for%20Improved%0A%20%20Diffusion%20Modelling&entry.906535625=Grigory%20Bartosh%20and%20Dmitry%20Vetrov%20and%20Christian%20A.%20Naesseth&entry.1292438233=%20%20Conventional%20diffusion%20models%20typically%20relies%20on%20a%20fixed%20forward%20process%2C%0Awhich%20implicitly%20defines%20complex%20marginal%20distributions%20over%20latent%20variables.%0AThis%20can%20often%20complicate%20the%20reverse%20process%27%20task%20in%20learning%20generative%0Atrajectories%2C%20and%20results%20in%20costly%20inference%20for%20diffusion%20models.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20Neural%20Flow%20Diffusion%20Models%20%28NFDM%29%2C%20a%20novel%0Aframework%20that%20enhances%20diffusion%20models%20by%20supporting%20a%20broader%20range%20of%0Aforward%20processes%20beyond%20the%20fixed%20linear%20Gaussian.%20We%20also%20propose%20a%20novel%0Aparameterization%20technique%20for%20learning%20the%20forward%20process.%20Our%20framework%0Aprovides%20an%20end-to-end%2C%20simulation-free%20optimization%20objective%2C%20effectively%0Aminimizing%20a%20variational%20upper%20bound%20on%20the%20negative%20log-likelihood.%0AExperimental%20results%20demonstrate%20NFDM%27s%20strong%20performance%2C%20evidenced%20by%0Astate-of-the-art%20likelihood%20estimation.%20Furthermore%2C%20we%20investigate%20NFDM%27s%0Acapacity%20for%20learning%20generative%20dynamics%20with%20specific%20characteristics%2C%20such%0Aas%20deterministic%20straight%20lines%20trajectories.%20This%20exploration%20underscores%0ANFDM%27s%20versatility%20and%20its%20potential%20for%20a%20wide%20range%20of%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12940v1&entry.124074799=Read"},
{"title": "Learning Distributions over Trajectories for Human Behavior Prediction", "author": "Anna M\u00e9sz\u00e1ros and Julian F. Schumann and Javier Alonso-Mora and Arkady Zgonnikov and Jens Kober", "abstract": "  Predicting the future behavior of human road users is an important aspect for\nthe development of risk-aware autonomous vehicles. While many models have been\ndeveloped towards this end, effectively capturing and predicting the\nvariability inherent to human behavior still remains an open challenge. This\npaper proposes TrajFlow - a new approach for probabilistic trajectory\nprediction based on Normalizing Flows. We reformulate the problem of capturing\ndistributions over trajectories into capturing distributions over abstracted\ntrajectory features using an autoencoder, simplifying the learning task of the\nNormalizing Flows. TrajFlow outperforms state-of-the-art behavior prediction\nmodels in capturing full trajectory distributions in two synthetic benchmarks\nwith known true distributions, and is competitive on the naturalistic datasets\nETH/UCY, rounD, and nuScenes. Our results demonstrate the effectiveness of\nTrajFlow in probabilistic prediction of human behavior.\n", "link": "http://arxiv.org/abs/2304.05166v4", "date": "2024-04-19", "relevancy": 1.0706, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5413}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.494}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Distributions%20over%20Trajectories%20for%20Human%20Behavior%20Prediction&body=Title%3A%20Learning%20Distributions%20over%20Trajectories%20for%20Human%20Behavior%20Prediction%0AAuthor%3A%20Anna%20M%C3%A9sz%C3%A1ros%20and%20Julian%20F.%20Schumann%20and%20Javier%20Alonso-Mora%20and%20Arkady%20Zgonnikov%20and%20Jens%20Kober%0AAbstract%3A%20%20%20Predicting%20the%20future%20behavior%20of%20human%20road%20users%20is%20an%20important%20aspect%20for%0Athe%20development%20of%20risk-aware%20autonomous%20vehicles.%20While%20many%20models%20have%20been%0Adeveloped%20towards%20this%20end%2C%20effectively%20capturing%20and%20predicting%20the%0Avariability%20inherent%20to%20human%20behavior%20still%20remains%20an%20open%20challenge.%20This%0Apaper%20proposes%20TrajFlow%20-%20a%20new%20approach%20for%20probabilistic%20trajectory%0Aprediction%20based%20on%20Normalizing%20Flows.%20We%20reformulate%20the%20problem%20of%20capturing%0Adistributions%20over%20trajectories%20into%20capturing%20distributions%20over%20abstracted%0Atrajectory%20features%20using%20an%20autoencoder%2C%20simplifying%20the%20learning%20task%20of%20the%0ANormalizing%20Flows.%20TrajFlow%20outperforms%20state-of-the-art%20behavior%20prediction%0Amodels%20in%20capturing%20full%20trajectory%20distributions%20in%20two%20synthetic%20benchmarks%0Awith%20known%20true%20distributions%2C%20and%20is%20competitive%20on%20the%20naturalistic%20datasets%0AETH/UCY%2C%20rounD%2C%20and%20nuScenes.%20Our%20results%20demonstrate%20the%20effectiveness%20of%0ATrajFlow%20in%20probabilistic%20prediction%20of%20human%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.05166v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Distributions%20over%20Trajectories%20for%20Human%20Behavior%20Prediction&entry.906535625=Anna%20M%C3%A9sz%C3%A1ros%20and%20Julian%20F.%20Schumann%20and%20Javier%20Alonso-Mora%20and%20Arkady%20Zgonnikov%20and%20Jens%20Kober&entry.1292438233=%20%20Predicting%20the%20future%20behavior%20of%20human%20road%20users%20is%20an%20important%20aspect%20for%0Athe%20development%20of%20risk-aware%20autonomous%20vehicles.%20While%20many%20models%20have%20been%0Adeveloped%20towards%20this%20end%2C%20effectively%20capturing%20and%20predicting%20the%0Avariability%20inherent%20to%20human%20behavior%20still%20remains%20an%20open%20challenge.%20This%0Apaper%20proposes%20TrajFlow%20-%20a%20new%20approach%20for%20probabilistic%20trajectory%0Aprediction%20based%20on%20Normalizing%20Flows.%20We%20reformulate%20the%20problem%20of%20capturing%0Adistributions%20over%20trajectories%20into%20capturing%20distributions%20over%20abstracted%0Atrajectory%20features%20using%20an%20autoencoder%2C%20simplifying%20the%20learning%20task%20of%20the%0ANormalizing%20Flows.%20TrajFlow%20outperforms%20state-of-the-art%20behavior%20prediction%0Amodels%20in%20capturing%20full%20trajectory%20distributions%20in%20two%20synthetic%20benchmarks%0Awith%20known%20true%20distributions%2C%20and%20is%20competitive%20on%20the%20naturalistic%20datasets%0AETH/UCY%2C%20rounD%2C%20and%20nuScenes.%20Our%20results%20demonstrate%20the%20effectiveness%20of%0ATrajFlow%20in%20probabilistic%20prediction%20of%20human%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.05166v4&entry.124074799=Read"},
{"title": "Aquaculture field robotics: Applications, lessons learned and future\n  prospects", "author": "Herman B. Amundsen and Marios Xanthidis and Martin F\u00f8re and Sveinung J. Ohrem and Eleni Kelasidi", "abstract": "  Aquaculture is a big marine industry and contributes to securing global food\ndemands. Underwater vehicles such as remotely operated vehicles (ROVs) are\ncommonly used for inspection, maintenance, and intervention (IMR) tasks in fish\nfarms. However, underwater vehicle operations in aquaculture face several\nunique and demanding challenges, such as navigation in dynamically changing\nenvironments with time-varying sealoads and poor hydroacoustic sensor\ncapabilities, challenges yet to be properly addressed in research. This paper\nwill present various endeavors to address these questions and improve the\noverall autonomy level in aquaculture robotics, with a focus on field\nexperiments. We will also discuss lessons learned during field trials and\npotential future prospects in aquaculture robotics.\n", "link": "http://arxiv.org/abs/2404.12995v1", "date": "2024-04-19", "relevancy": 1.3912, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4938}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4521}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Aquaculture%20field%20robotics%3A%20Applications%2C%20lessons%20learned%20and%20future%0A%20%20prospects&body=Title%3A%20Aquaculture%20field%20robotics%3A%20Applications%2C%20lessons%20learned%20and%20future%0A%20%20prospects%0AAuthor%3A%20Herman%20B.%20Amundsen%20and%20Marios%20Xanthidis%20and%20Martin%20F%C3%B8re%20and%20Sveinung%20J.%20Ohrem%20and%20Eleni%20Kelasidi%0AAbstract%3A%20%20%20Aquaculture%20is%20a%20big%20marine%20industry%20and%20contributes%20to%20securing%20global%20food%0Ademands.%20Underwater%20vehicles%20such%20as%20remotely%20operated%20vehicles%20%28ROVs%29%20are%0Acommonly%20used%20for%20inspection%2C%20maintenance%2C%20and%20intervention%20%28IMR%29%20tasks%20in%20fish%0Afarms.%20However%2C%20underwater%20vehicle%20operations%20in%20aquaculture%20face%20several%0Aunique%20and%20demanding%20challenges%2C%20such%20as%20navigation%20in%20dynamically%20changing%0Aenvironments%20with%20time-varying%20sealoads%20and%20poor%20hydroacoustic%20sensor%0Acapabilities%2C%20challenges%20yet%20to%20be%20properly%20addressed%20in%20research.%20This%20paper%0Awill%20present%20various%20endeavors%20to%20address%20these%20questions%20and%20improve%20the%0Aoverall%20autonomy%20level%20in%20aquaculture%20robotics%2C%20with%20a%20focus%20on%20field%0Aexperiments.%20We%20will%20also%20discuss%20lessons%20learned%20during%20field%20trials%20and%0Apotential%20future%20prospects%20in%20aquaculture%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12995v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aquaculture%20field%20robotics%3A%20Applications%2C%20lessons%20learned%20and%20future%0A%20%20prospects&entry.906535625=Herman%20B.%20Amundsen%20and%20Marios%20Xanthidis%20and%20Martin%20F%C3%B8re%20and%20Sveinung%20J.%20Ohrem%20and%20Eleni%20Kelasidi&entry.1292438233=%20%20Aquaculture%20is%20a%20big%20marine%20industry%20and%20contributes%20to%20securing%20global%20food%0Ademands.%20Underwater%20vehicles%20such%20as%20remotely%20operated%20vehicles%20%28ROVs%29%20are%0Acommonly%20used%20for%20inspection%2C%20maintenance%2C%20and%20intervention%20%28IMR%29%20tasks%20in%20fish%0Afarms.%20However%2C%20underwater%20vehicle%20operations%20in%20aquaculture%20face%20several%0Aunique%20and%20demanding%20challenges%2C%20such%20as%20navigation%20in%20dynamically%20changing%0Aenvironments%20with%20time-varying%20sealoads%20and%20poor%20hydroacoustic%20sensor%0Acapabilities%2C%20challenges%20yet%20to%20be%20properly%20addressed%20in%20research.%20This%20paper%0Awill%20present%20various%20endeavors%20to%20address%20these%20questions%20and%20improve%20the%0Aoverall%20autonomy%20level%20in%20aquaculture%20robotics%2C%20with%20a%20focus%20on%20field%0Aexperiments.%20We%20will%20also%20discuss%20lessons%20learned%20during%20field%20trials%20and%0Apotential%20future%20prospects%20in%20aquaculture%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12995v1&entry.124074799=Read"},
{"title": "A Survey of Optimization-based Task and Motion Planning: From Classical\n  To Learning Approaches", "author": "Zhigen Zhao and Shuo Cheng and Yan Ding and Ziyi Zhou and Shiqi Zhang and Danfei Xu and Ye Zhao", "abstract": "  Task and Motion Planning (TAMP) integrates high-level task planning and\nlow-level motion planning to equip robots with the autonomy to effectively\nreason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on\nhybrid optimization approaches that define goal conditions via objective\nfunctions and are capable of handling open-ended goals, robotic dynamics, and\nphysical interaction between the robot and the environment. Therefore,\noptimization-based TAMP is particularly suited to solve highly complex,\ncontact-rich locomotion and manipulation problems. This survey provides a\ncomprehensive review on optimization-based TAMP, covering (i) planning domain\nrepresentations, including action description languages and temporal logic,\n(ii) individual solution strategies for components of TAMP, including AI\nplanning and trajectory optimization (TO), and (iii) the dynamic interplay\nbetween logic-based task planning and model-based TO. A particular focus of\nthis survey is to highlight the algorithm structures to efficiently solve TAMP,\nespecially hierarchical and distributed approaches. Additionally, the survey\nemphasizes the synergy between the classical methods and contemporary\nlearning-based innovations such as large language models. Furthermore, the\nfuture research directions for TAMP is discussed in this survey, highlighting\nboth algorithmic and application-specific challenges.\n", "link": "http://arxiv.org/abs/2404.02817v3", "date": "2024-04-19", "relevancy": 1.5431, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5294}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5039}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Optimization-based%20Task%20and%20Motion%20Planning%3A%20From%20Classical%0A%20%20To%20Learning%20Approaches&body=Title%3A%20A%20Survey%20of%20Optimization-based%20Task%20and%20Motion%20Planning%3A%20From%20Classical%0A%20%20To%20Learning%20Approaches%0AAuthor%3A%20Zhigen%20Zhao%20and%20Shuo%20Cheng%20and%20Yan%20Ding%20and%20Ziyi%20Zhou%20and%20Shiqi%20Zhang%20and%20Danfei%20Xu%20and%20Ye%20Zhao%0AAbstract%3A%20%20%20Task%20and%20Motion%20Planning%20%28TAMP%29%20integrates%20high-level%20task%20planning%20and%0Alow-level%20motion%20planning%20to%20equip%20robots%20with%20the%20autonomy%20to%20effectively%0Areason%20over%20long-horizon%2C%20dynamic%20tasks.%20Optimization-based%20TAMP%20focuses%20on%0Ahybrid%20optimization%20approaches%20that%20define%20goal%20conditions%20via%20objective%0Afunctions%20and%20are%20capable%20of%20handling%20open-ended%20goals%2C%20robotic%20dynamics%2C%20and%0Aphysical%20interaction%20between%20the%20robot%20and%20the%20environment.%20Therefore%2C%0Aoptimization-based%20TAMP%20is%20particularly%20suited%20to%20solve%20highly%20complex%2C%0Acontact-rich%20locomotion%20and%20manipulation%20problems.%20This%20survey%20provides%20a%0Acomprehensive%20review%20on%20optimization-based%20TAMP%2C%20covering%20%28i%29%20planning%20domain%0Arepresentations%2C%20including%20action%20description%20languages%20and%20temporal%20logic%2C%0A%28ii%29%20individual%20solution%20strategies%20for%20components%20of%20TAMP%2C%20including%20AI%0Aplanning%20and%20trajectory%20optimization%20%28TO%29%2C%20and%20%28iii%29%20the%20dynamic%20interplay%0Abetween%20logic-based%20task%20planning%20and%20model-based%20TO.%20A%20particular%20focus%20of%0Athis%20survey%20is%20to%20highlight%20the%20algorithm%20structures%20to%20efficiently%20solve%20TAMP%2C%0Aespecially%20hierarchical%20and%20distributed%20approaches.%20Additionally%2C%20the%20survey%0Aemphasizes%20the%20synergy%20between%20the%20classical%20methods%20and%20contemporary%0Alearning-based%20innovations%20such%20as%20large%20language%20models.%20Furthermore%2C%20the%0Afuture%20research%20directions%20for%20TAMP%20is%20discussed%20in%20this%20survey%2C%20highlighting%0Aboth%20algorithmic%20and%20application-specific%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02817v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Optimization-based%20Task%20and%20Motion%20Planning%3A%20From%20Classical%0A%20%20To%20Learning%20Approaches&entry.906535625=Zhigen%20Zhao%20and%20Shuo%20Cheng%20and%20Yan%20Ding%20and%20Ziyi%20Zhou%20and%20Shiqi%20Zhang%20and%20Danfei%20Xu%20and%20Ye%20Zhao&entry.1292438233=%20%20Task%20and%20Motion%20Planning%20%28TAMP%29%20integrates%20high-level%20task%20planning%20and%0Alow-level%20motion%20planning%20to%20equip%20robots%20with%20the%20autonomy%20to%20effectively%0Areason%20over%20long-horizon%2C%20dynamic%20tasks.%20Optimization-based%20TAMP%20focuses%20on%0Ahybrid%20optimization%20approaches%20that%20define%20goal%20conditions%20via%20objective%0Afunctions%20and%20are%20capable%20of%20handling%20open-ended%20goals%2C%20robotic%20dynamics%2C%20and%0Aphysical%20interaction%20between%20the%20robot%20and%20the%20environment.%20Therefore%2C%0Aoptimization-based%20TAMP%20is%20particularly%20suited%20to%20solve%20highly%20complex%2C%0Acontact-rich%20locomotion%20and%20manipulation%20problems.%20This%20survey%20provides%20a%0Acomprehensive%20review%20on%20optimization-based%20TAMP%2C%20covering%20%28i%29%20planning%20domain%0Arepresentations%2C%20including%20action%20description%20languages%20and%20temporal%20logic%2C%0A%28ii%29%20individual%20solution%20strategies%20for%20components%20of%20TAMP%2C%20including%20AI%0Aplanning%20and%20trajectory%20optimization%20%28TO%29%2C%20and%20%28iii%29%20the%20dynamic%20interplay%0Abetween%20logic-based%20task%20planning%20and%20model-based%20TO.%20A%20particular%20focus%20of%0Athis%20survey%20is%20to%20highlight%20the%20algorithm%20structures%20to%20efficiently%20solve%20TAMP%2C%0Aespecially%20hierarchical%20and%20distributed%20approaches.%20Additionally%2C%20the%20survey%0Aemphasizes%20the%20synergy%20between%20the%20classical%20methods%20and%20contemporary%0Alearning-based%20innovations%20such%20as%20large%20language%20models.%20Furthermore%2C%20the%0Afuture%20research%20directions%20for%20TAMP%20is%20discussed%20in%20this%20survey%2C%20highlighting%0Aboth%20algorithmic%20and%20application-specific%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02817v3&entry.124074799=Read"},
{"title": "MCM: Multi-condition Motion Synthesis Framework", "author": "Zeyu Ling and Bo Han and Yongkang Wongkan and Han Lin and Mohan Kankanhalli and Weidong Geng", "abstract": "  Conditional human motion synthesis (HMS) aims to generate human motion\nsequences that conform to specific conditions. Text and audio represent the two\npredominant modalities employed as HMS control conditions. While existing\nresearch has primarily focused on single conditions, the multi-condition human\nmotion synthesis remains underexplored. In this study, we propose a\nmulti-condition HMS framework, termed MCM, based on a dual-branch structure\ncomposed of a main branch and a control branch. This framework effectively\nextends the applicability of the diffusion model, which is initially predicated\nsolely on textual conditions, to auditory conditions. This extension\nencompasses both music-to-dance and co-speech HMS while preserving the\nintrinsic quality of motion and the capabilities for semantic association\ninherent in the original model. Furthermore, we propose the implementation of a\nTransformer-based diffusion model, designated as MWNet, as the main branch.\nThis model adeptly apprehends the spatial intricacies and inter-joint\ncorrelations inherent in motion sequences, facilitated by the integration of\nmulti-wise self-attention modules. Extensive experiments show that our method\nachieves competitive results in single-condition and multi-condition HMS tasks.\n", "link": "http://arxiv.org/abs/2404.12886v1", "date": "2024-04-19", "relevancy": 1.0411, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5292}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5264}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5061}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MCM%3A%20Multi-condition%20Motion%20Synthesis%20Framework&body=Title%3A%20MCM%3A%20Multi-condition%20Motion%20Synthesis%20Framework%0AAuthor%3A%20Zeyu%20Ling%20and%20Bo%20Han%20and%20Yongkang%20Wongkan%20and%20Han%20Lin%20and%20Mohan%20Kankanhalli%20and%20Weidong%20Geng%0AAbstract%3A%20%20%20Conditional%20human%20motion%20synthesis%20%28HMS%29%20aims%20to%20generate%20human%20motion%0Asequences%20that%20conform%20to%20specific%20conditions.%20Text%20and%20audio%20represent%20the%20two%0Apredominant%20modalities%20employed%20as%20HMS%20control%20conditions.%20While%20existing%0Aresearch%20has%20primarily%20focused%20on%20single%20conditions%2C%20the%20multi-condition%20human%0Amotion%20synthesis%20remains%20underexplored.%20In%20this%20study%2C%20we%20propose%20a%0Amulti-condition%20HMS%20framework%2C%20termed%20MCM%2C%20based%20on%20a%20dual-branch%20structure%0Acomposed%20of%20a%20main%20branch%20and%20a%20control%20branch.%20This%20framework%20effectively%0Aextends%20the%20applicability%20of%20the%20diffusion%20model%2C%20which%20is%20initially%20predicated%0Asolely%20on%20textual%20conditions%2C%20to%20auditory%20conditions.%20This%20extension%0Aencompasses%20both%20music-to-dance%20and%20co-speech%20HMS%20while%20preserving%20the%0Aintrinsic%20quality%20of%20motion%20and%20the%20capabilities%20for%20semantic%20association%0Ainherent%20in%20the%20original%20model.%20Furthermore%2C%20we%20propose%20the%20implementation%20of%20a%0ATransformer-based%20diffusion%20model%2C%20designated%20as%20MWNet%2C%20as%20the%20main%20branch.%0AThis%20model%20adeptly%20apprehends%20the%20spatial%20intricacies%20and%20inter-joint%0Acorrelations%20inherent%20in%20motion%20sequences%2C%20facilitated%20by%20the%20integration%20of%0Amulti-wise%20self-attention%20modules.%20Extensive%20experiments%20show%20that%20our%20method%0Aachieves%20competitive%20results%20in%20single-condition%20and%20multi-condition%20HMS%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12886v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCM%3A%20Multi-condition%20Motion%20Synthesis%20Framework&entry.906535625=Zeyu%20Ling%20and%20Bo%20Han%20and%20Yongkang%20Wongkan%20and%20Han%20Lin%20and%20Mohan%20Kankanhalli%20and%20Weidong%20Geng&entry.1292438233=%20%20Conditional%20human%20motion%20synthesis%20%28HMS%29%20aims%20to%20generate%20human%20motion%0Asequences%20that%20conform%20to%20specific%20conditions.%20Text%20and%20audio%20represent%20the%20two%0Apredominant%20modalities%20employed%20as%20HMS%20control%20conditions.%20While%20existing%0Aresearch%20has%20primarily%20focused%20on%20single%20conditions%2C%20the%20multi-condition%20human%0Amotion%20synthesis%20remains%20underexplored.%20In%20this%20study%2C%20we%20propose%20a%0Amulti-condition%20HMS%20framework%2C%20termed%20MCM%2C%20based%20on%20a%20dual-branch%20structure%0Acomposed%20of%20a%20main%20branch%20and%20a%20control%20branch.%20This%20framework%20effectively%0Aextends%20the%20applicability%20of%20the%20diffusion%20model%2C%20which%20is%20initially%20predicated%0Asolely%20on%20textual%20conditions%2C%20to%20auditory%20conditions.%20This%20extension%0Aencompasses%20both%20music-to-dance%20and%20co-speech%20HMS%20while%20preserving%20the%0Aintrinsic%20quality%20of%20motion%20and%20the%20capabilities%20for%20semantic%20association%0Ainherent%20in%20the%20original%20model.%20Furthermore%2C%20we%20propose%20the%20implementation%20of%20a%0ATransformer-based%20diffusion%20model%2C%20designated%20as%20MWNet%2C%20as%20the%20main%20branch.%0AThis%20model%20adeptly%20apprehends%20the%20spatial%20intricacies%20and%20inter-joint%0Acorrelations%20inherent%20in%20motion%20sequences%2C%20facilitated%20by%20the%20integration%20of%0Amulti-wise%20self-attention%20modules.%20Extensive%20experiments%20show%20that%20our%20method%0Aachieves%20competitive%20results%20in%20single-condition%20and%20multi-condition%20HMS%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12886v1&entry.124074799=Read"},
{"title": "MixLight: Borrowing the Best of both Spherical Harmonics and Gaussian\n  Models", "author": "Xinlong Ji and Fangneng Zhan and Shijian Lu and Shi-Sheng Huang and Hua Huang", "abstract": "  Accurately estimating scene lighting is critical for applications such as\nmixed reality. Existing works estimate illumination by generating illumination\nmaps or regressing illumination parameters. However, the method of generating\nillumination maps has poor generalization performance and parametric models\nsuch as Spherical Harmonic (SH) and Spherical Gaussian (SG) fall short in\ncapturing high-frequency or low-frequency components. This paper presents\nMixLight, a joint model that utilizes the complementary characteristics of SH\nand SG to achieve a more complete illumination representation, which uses SH\nand SG to capture low-frequency ambient and high-frequency light sources\nrespectively. In addition, a special spherical light source sparsemax\n(SLSparsemax) module that refers to the position and brightness relationship\nbetween spherical light sources is designed to improve their sparsity, which is\nsignificant but omitted by prior works. Extensive experiments demonstrate that\nMixLight surpasses state-of-the-art (SOTA) methods on multiple metrics. In\naddition, experiments on Web Dataset also show that MixLight as a parametric\nmethod has better generalization performance than non-parametric methods.\n", "link": "http://arxiv.org/abs/2404.12768v1", "date": "2024-04-19", "relevancy": 0.9719, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.492}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4835}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4823}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MixLight%3A%20Borrowing%20the%20Best%20of%20both%20Spherical%20Harmonics%20and%20Gaussian%0A%20%20Models&body=Title%3A%20MixLight%3A%20Borrowing%20the%20Best%20of%20both%20Spherical%20Harmonics%20and%20Gaussian%0A%20%20Models%0AAuthor%3A%20Xinlong%20Ji%20and%20Fangneng%20Zhan%20and%20Shijian%20Lu%20and%20Shi-Sheng%20Huang%20and%20Hua%20Huang%0AAbstract%3A%20%20%20Accurately%20estimating%20scene%20lighting%20is%20critical%20for%20applications%20such%20as%0Amixed%20reality.%20Existing%20works%20estimate%20illumination%20by%20generating%20illumination%0Amaps%20or%20regressing%20illumination%20parameters.%20However%2C%20the%20method%20of%20generating%0Aillumination%20maps%20has%20poor%20generalization%20performance%20and%20parametric%20models%0Asuch%20as%20Spherical%20Harmonic%20%28SH%29%20and%20Spherical%20Gaussian%20%28SG%29%20fall%20short%20in%0Acapturing%20high-frequency%20or%20low-frequency%20components.%20This%20paper%20presents%0AMixLight%2C%20a%20joint%20model%20that%20utilizes%20the%20complementary%20characteristics%20of%20SH%0Aand%20SG%20to%20achieve%20a%20more%20complete%20illumination%20representation%2C%20which%20uses%20SH%0Aand%20SG%20to%20capture%20low-frequency%20ambient%20and%20high-frequency%20light%20sources%0Arespectively.%20In%20addition%2C%20a%20special%20spherical%20light%20source%20sparsemax%0A%28SLSparsemax%29%20module%20that%20refers%20to%20the%20position%20and%20brightness%20relationship%0Abetween%20spherical%20light%20sources%20is%20designed%20to%20improve%20their%20sparsity%2C%20which%20is%0Asignificant%20but%20omitted%20by%20prior%20works.%20Extensive%20experiments%20demonstrate%20that%0AMixLight%20surpasses%20state-of-the-art%20%28SOTA%29%20methods%20on%20multiple%20metrics.%20In%0Aaddition%2C%20experiments%20on%20Web%20Dataset%20also%20show%20that%20MixLight%20as%20a%20parametric%0Amethod%20has%20better%20generalization%20performance%20than%20non-parametric%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12768v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MixLight%3A%20Borrowing%20the%20Best%20of%20both%20Spherical%20Harmonics%20and%20Gaussian%0A%20%20Models&entry.906535625=Xinlong%20Ji%20and%20Fangneng%20Zhan%20and%20Shijian%20Lu%20and%20Shi-Sheng%20Huang%20and%20Hua%20Huang&entry.1292438233=%20%20Accurately%20estimating%20scene%20lighting%20is%20critical%20for%20applications%20such%20as%0Amixed%20reality.%20Existing%20works%20estimate%20illumination%20by%20generating%20illumination%0Amaps%20or%20regressing%20illumination%20parameters.%20However%2C%20the%20method%20of%20generating%0Aillumination%20maps%20has%20poor%20generalization%20performance%20and%20parametric%20models%0Asuch%20as%20Spherical%20Harmonic%20%28SH%29%20and%20Spherical%20Gaussian%20%28SG%29%20fall%20short%20in%0Acapturing%20high-frequency%20or%20low-frequency%20components.%20This%20paper%20presents%0AMixLight%2C%20a%20joint%20model%20that%20utilizes%20the%20complementary%20characteristics%20of%20SH%0Aand%20SG%20to%20achieve%20a%20more%20complete%20illumination%20representation%2C%20which%20uses%20SH%0Aand%20SG%20to%20capture%20low-frequency%20ambient%20and%20high-frequency%20light%20sources%0Arespectively.%20In%20addition%2C%20a%20special%20spherical%20light%20source%20sparsemax%0A%28SLSparsemax%29%20module%20that%20refers%20to%20the%20position%20and%20brightness%20relationship%0Abetween%20spherical%20light%20sources%20is%20designed%20to%20improve%20their%20sparsity%2C%20which%20is%0Asignificant%20but%20omitted%20by%20prior%20works.%20Extensive%20experiments%20demonstrate%20that%0AMixLight%20surpasses%20state-of-the-art%20%28SOTA%29%20methods%20on%20multiple%20metrics.%20In%0Aaddition%2C%20experiments%20on%20Web%20Dataset%20also%20show%20that%20MixLight%20as%20a%20parametric%0Amethod%20has%20better%20generalization%20performance%20than%20non-parametric%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12768v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


