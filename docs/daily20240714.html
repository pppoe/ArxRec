<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240711.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "WildGaussians: 3D Gaussian Splatting in the Wild", "author": "Jonas Kulhanek and Songyou Peng and Zuzana Kukelova and Marc Pollefeys and Torsten Sattler", "abstract": "  While the field of 3D scene reconstruction is dominated by NeRFs due to their\nphotorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,\noffering similar quality with real-time rendering speeds. However, both methods\nprimarily excel with well-controlled 3D scenes, while in-the-wild data -\ncharacterized by occlusions, dynamic objects, and varying illumination -\nremains challenging. NeRFs can adapt to such conditions easily through\nper-image embedding vectors, but 3DGS struggles due to its explicit\nrepresentation and lack of shared parameters. To address this, we introduce\nWildGaussians, a novel approach to handle occlusions and appearance changes\nwith 3DGS. By leveraging robust DINO features and integrating an appearance\nmodeling module within 3DGS, our method achieves state-of-the-art results. We\ndemonstrate that WildGaussians matches the real-time rendering speed of 3DGS\nwhile surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all\nwithin a simple architectural framework.\n", "link": "http://arxiv.org/abs/2407.08447v1", "date": "2024-07-11", "relevancy": 3.3756, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7321}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7125}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WildGaussians%3A%203D%20Gaussian%20Splatting%20in%20the%20Wild&body=Title%3A%20WildGaussians%3A%203D%20Gaussian%20Splatting%20in%20the%20Wild%0AAuthor%3A%20Jonas%20Kulhanek%20and%20Songyou%20Peng%20and%20Zuzana%20Kukelova%20and%20Marc%20Pollefeys%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20While%20the%20field%20of%203D%20scene%20reconstruction%20is%20dominated%20by%20NeRFs%20due%20to%20their%0Aphotorealistic%20quality%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%2C%0Aoffering%20similar%20quality%20with%20real-time%20rendering%20speeds.%20However%2C%20both%20methods%0Aprimarily%20excel%20with%20well-controlled%203D%20scenes%2C%20while%20in-the-wild%20data%20-%0Acharacterized%20by%20occlusions%2C%20dynamic%20objects%2C%20and%20varying%20illumination%20-%0Aremains%20challenging.%20NeRFs%20can%20adapt%20to%20such%20conditions%20easily%20through%0Aper-image%20embedding%20vectors%2C%20but%203DGS%20struggles%20due%20to%20its%20explicit%0Arepresentation%20and%20lack%20of%20shared%20parameters.%20To%20address%20this%2C%20we%20introduce%0AWildGaussians%2C%20a%20novel%20approach%20to%20handle%20occlusions%20and%20appearance%20changes%0Awith%203DGS.%20By%20leveraging%20robust%20DINO%20features%20and%20integrating%20an%20appearance%0Amodeling%20module%20within%203DGS%2C%20our%20method%20achieves%20state-of-the-art%20results.%20We%0Ademonstrate%20that%20WildGaussians%20matches%20the%20real-time%20rendering%20speed%20of%203DGS%0Awhile%20surpassing%20both%203DGS%20and%20NeRF%20baselines%20in%20handling%20in-the-wild%20data%2C%20all%0Awithin%20a%20simple%20architectural%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWildGaussians%253A%25203D%2520Gaussian%2520Splatting%2520in%2520the%2520Wild%26entry.906535625%3DJonas%2520Kulhanek%2520and%2520Songyou%2520Peng%2520and%2520Zuzana%2520Kukelova%2520and%2520Marc%2520Pollefeys%2520and%2520Torsten%2520Sattler%26entry.1292438233%3D%2520%2520While%2520the%2520field%2520of%25203D%2520scene%2520reconstruction%2520is%2520dominated%2520by%2520NeRFs%2520due%2520to%2520their%250Aphotorealistic%2520quality%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520emerged%252C%250Aoffering%2520similar%2520quality%2520with%2520real-time%2520rendering%2520speeds.%2520However%252C%2520both%2520methods%250Aprimarily%2520excel%2520with%2520well-controlled%25203D%2520scenes%252C%2520while%2520in-the-wild%2520data%2520-%250Acharacterized%2520by%2520occlusions%252C%2520dynamic%2520objects%252C%2520and%2520varying%2520illumination%2520-%250Aremains%2520challenging.%2520NeRFs%2520can%2520adapt%2520to%2520such%2520conditions%2520easily%2520through%250Aper-image%2520embedding%2520vectors%252C%2520but%25203DGS%2520struggles%2520due%2520to%2520its%2520explicit%250Arepresentation%2520and%2520lack%2520of%2520shared%2520parameters.%2520To%2520address%2520this%252C%2520we%2520introduce%250AWildGaussians%252C%2520a%2520novel%2520approach%2520to%2520handle%2520occlusions%2520and%2520appearance%2520changes%250Awith%25203DGS.%2520By%2520leveraging%2520robust%2520DINO%2520features%2520and%2520integrating%2520an%2520appearance%250Amodeling%2520module%2520within%25203DGS%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520results.%2520We%250Ademonstrate%2520that%2520WildGaussians%2520matches%2520the%2520real-time%2520rendering%2520speed%2520of%25203DGS%250Awhile%2520surpassing%2520both%25203DGS%2520and%2520NeRF%2520baselines%2520in%2520handling%2520in-the-wild%2520data%252C%2520all%250Awithin%2520a%2520simple%2520architectural%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WildGaussians%3A%203D%20Gaussian%20Splatting%20in%20the%20Wild&entry.906535625=Jonas%20Kulhanek%20and%20Songyou%20Peng%20and%20Zuzana%20Kukelova%20and%20Marc%20Pollefeys%20and%20Torsten%20Sattler&entry.1292438233=%20%20While%20the%20field%20of%203D%20scene%20reconstruction%20is%20dominated%20by%20NeRFs%20due%20to%20their%0Aphotorealistic%20quality%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%2C%0Aoffering%20similar%20quality%20with%20real-time%20rendering%20speeds.%20However%2C%20both%20methods%0Aprimarily%20excel%20with%20well-controlled%203D%20scenes%2C%20while%20in-the-wild%20data%20-%0Acharacterized%20by%20occlusions%2C%20dynamic%20objects%2C%20and%20varying%20illumination%20-%0Aremains%20challenging.%20NeRFs%20can%20adapt%20to%20such%20conditions%20easily%20through%0Aper-image%20embedding%20vectors%2C%20but%203DGS%20struggles%20due%20to%20its%20explicit%0Arepresentation%20and%20lack%20of%20shared%20parameters.%20To%20address%20this%2C%20we%20introduce%0AWildGaussians%2C%20a%20novel%20approach%20to%20handle%20occlusions%20and%20appearance%20changes%0Awith%203DGS.%20By%20leveraging%20robust%20DINO%20features%20and%20integrating%20an%20appearance%0Amodeling%20module%20within%203DGS%2C%20our%20method%20achieves%20state-of-the-art%20results.%20We%0Ademonstrate%20that%20WildGaussians%20matches%20the%20real-time%20rendering%20speed%20of%203DGS%0Awhile%20surpassing%20both%203DGS%20and%20NeRF%20baselines%20in%20handling%20in-the-wild%20data%2C%20all%0Awithin%20a%20simple%20architectural%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08447v1&entry.124074799=Read"},
{"title": "Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using\n  Gaussian Splatting", "author": "Fang Li and Hao Zhang and Narendra Ahuja", "abstract": "  Gaussian Splatting (GS) has significantly elevated scene reconstruction\nefficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance\nFields (NeRF), particularly for dynamic scenes. However, current 4D NVS\nmethods, whether based on GS or NeRF, primarily rely on camera parameters\nprovided by COLMAP and even utilize sparse point clouds generated by COLMAP for\ninitialization, which lack accuracy as well are time-consuming. This sometimes\nresults in poor dynamic scene representation, especially in scenes with large\nobject movements, or extreme camera conditions e.g. small translations combined\nwith large rotations. Some studies simultaneously optimize the estimation of\ncamera parameters and scenes, supervised by additional information like depth,\noptical flow, etc. obtained from off-the-shelf models. Using this unverified\ninformation as ground truth can reduce robustness and accuracy, which does\nfrequently occur for long monocular videos (with e.g. > hundreds of frames). We\npropose a novel approach that learns a high-fidelity 4D GS scene representation\nwith self-calibration of camera parameters. It includes the extraction of 2D\npoint features that robustly represent 3D structure, and their use for\nsubsequent joint optimization of camera parameters and 3D structure towards\noverall 4D scene optimization. We demonstrate the accuracy and time efficiency\nof our method through extensive quantitative and qualitative experimental\nresults on several standard benchmarks. The results show significant\nimprovements over state-of-the-art methods for 4D novel view synthesis. The\nsource code will be released soon at https://github.com/fangli333/SC-4DGS.\n", "link": "http://arxiv.org/abs/2406.01042v2", "date": "2024-07-11", "relevancy": 3.3688, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.723}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6883}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Calibrating%204D%20Novel%20View%20Synthesis%20from%20Monocular%20Videos%20Using%0A%20%20Gaussian%20Splatting&body=Title%3A%20Self-Calibrating%204D%20Novel%20View%20Synthesis%20from%20Monocular%20Videos%20Using%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Fang%20Li%20and%20Hao%20Zhang%20and%20Narendra%20Ahuja%0AAbstract%3A%20%20%20Gaussian%20Splatting%20%28GS%29%20has%20significantly%20elevated%20scene%20reconstruction%0Aefficiency%20and%20novel%20view%20synthesis%20%28NVS%29%20accuracy%20compared%20to%20Neural%20Radiance%0AFields%20%28NeRF%29%2C%20particularly%20for%20dynamic%20scenes.%20However%2C%20current%204D%20NVS%0Amethods%2C%20whether%20based%20on%20GS%20or%20NeRF%2C%20primarily%20rely%20on%20camera%20parameters%0Aprovided%20by%20COLMAP%20and%20even%20utilize%20sparse%20point%20clouds%20generated%20by%20COLMAP%20for%0Ainitialization%2C%20which%20lack%20accuracy%20as%20well%20are%20time-consuming.%20This%20sometimes%0Aresults%20in%20poor%20dynamic%20scene%20representation%2C%20especially%20in%20scenes%20with%20large%0Aobject%20movements%2C%20or%20extreme%20camera%20conditions%20e.g.%20small%20translations%20combined%0Awith%20large%20rotations.%20Some%20studies%20simultaneously%20optimize%20the%20estimation%20of%0Acamera%20parameters%20and%20scenes%2C%20supervised%20by%20additional%20information%20like%20depth%2C%0Aoptical%20flow%2C%20etc.%20obtained%20from%20off-the-shelf%20models.%20Using%20this%20unverified%0Ainformation%20as%20ground%20truth%20can%20reduce%20robustness%20and%20accuracy%2C%20which%20does%0Afrequently%20occur%20for%20long%20monocular%20videos%20%28with%20e.g.%20%3E%20hundreds%20of%20frames%29.%20We%0Apropose%20a%20novel%20approach%20that%20learns%20a%20high-fidelity%204D%20GS%20scene%20representation%0Awith%20self-calibration%20of%20camera%20parameters.%20It%20includes%20the%20extraction%20of%202D%0Apoint%20features%20that%20robustly%20represent%203D%20structure%2C%20and%20their%20use%20for%0Asubsequent%20joint%20optimization%20of%20camera%20parameters%20and%203D%20structure%20towards%0Aoverall%204D%20scene%20optimization.%20We%20demonstrate%20the%20accuracy%20and%20time%20efficiency%0Aof%20our%20method%20through%20extensive%20quantitative%20and%20qualitative%20experimental%0Aresults%20on%20several%20standard%20benchmarks.%20The%20results%20show%20significant%0Aimprovements%20over%20state-of-the-art%20methods%20for%204D%20novel%20view%20synthesis.%20The%0Asource%20code%20will%20be%20released%20soon%20at%20https%3A//github.com/fangli333/SC-4DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Calibrating%25204D%2520Novel%2520View%2520Synthesis%2520from%2520Monocular%2520Videos%2520Using%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DFang%2520Li%2520and%2520Hao%2520Zhang%2520and%2520Narendra%2520Ahuja%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520%2528GS%2529%2520has%2520significantly%2520elevated%2520scene%2520reconstruction%250Aefficiency%2520and%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520accuracy%2520compared%2520to%2520Neural%2520Radiance%250AFields%2520%2528NeRF%2529%252C%2520particularly%2520for%2520dynamic%2520scenes.%2520However%252C%2520current%25204D%2520NVS%250Amethods%252C%2520whether%2520based%2520on%2520GS%2520or%2520NeRF%252C%2520primarily%2520rely%2520on%2520camera%2520parameters%250Aprovided%2520by%2520COLMAP%2520and%2520even%2520utilize%2520sparse%2520point%2520clouds%2520generated%2520by%2520COLMAP%2520for%250Ainitialization%252C%2520which%2520lack%2520accuracy%2520as%2520well%2520are%2520time-consuming.%2520This%2520sometimes%250Aresults%2520in%2520poor%2520dynamic%2520scene%2520representation%252C%2520especially%2520in%2520scenes%2520with%2520large%250Aobject%2520movements%252C%2520or%2520extreme%2520camera%2520conditions%2520e.g.%2520small%2520translations%2520combined%250Awith%2520large%2520rotations.%2520Some%2520studies%2520simultaneously%2520optimize%2520the%2520estimation%2520of%250Acamera%2520parameters%2520and%2520scenes%252C%2520supervised%2520by%2520additional%2520information%2520like%2520depth%252C%250Aoptical%2520flow%252C%2520etc.%2520obtained%2520from%2520off-the-shelf%2520models.%2520Using%2520this%2520unverified%250Ainformation%2520as%2520ground%2520truth%2520can%2520reduce%2520robustness%2520and%2520accuracy%252C%2520which%2520does%250Afrequently%2520occur%2520for%2520long%2520monocular%2520videos%2520%2528with%2520e.g.%2520%253E%2520hundreds%2520of%2520frames%2529.%2520We%250Apropose%2520a%2520novel%2520approach%2520that%2520learns%2520a%2520high-fidelity%25204D%2520GS%2520scene%2520representation%250Awith%2520self-calibration%2520of%2520camera%2520parameters.%2520It%2520includes%2520the%2520extraction%2520of%25202D%250Apoint%2520features%2520that%2520robustly%2520represent%25203D%2520structure%252C%2520and%2520their%2520use%2520for%250Asubsequent%2520joint%2520optimization%2520of%2520camera%2520parameters%2520and%25203D%2520structure%2520towards%250Aoverall%25204D%2520scene%2520optimization.%2520We%2520demonstrate%2520the%2520accuracy%2520and%2520time%2520efficiency%250Aof%2520our%2520method%2520through%2520extensive%2520quantitative%2520and%2520qualitative%2520experimental%250Aresults%2520on%2520several%2520standard%2520benchmarks.%2520The%2520results%2520show%2520significant%250Aimprovements%2520over%2520state-of-the-art%2520methods%2520for%25204D%2520novel%2520view%2520synthesis.%2520The%250Asource%2520code%2520will%2520be%2520released%2520soon%2520at%2520https%253A//github.com/fangli333/SC-4DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Calibrating%204D%20Novel%20View%20Synthesis%20from%20Monocular%20Videos%20Using%0A%20%20Gaussian%20Splatting&entry.906535625=Fang%20Li%20and%20Hao%20Zhang%20and%20Narendra%20Ahuja&entry.1292438233=%20%20Gaussian%20Splatting%20%28GS%29%20has%20significantly%20elevated%20scene%20reconstruction%0Aefficiency%20and%20novel%20view%20synthesis%20%28NVS%29%20accuracy%20compared%20to%20Neural%20Radiance%0AFields%20%28NeRF%29%2C%20particularly%20for%20dynamic%20scenes.%20However%2C%20current%204D%20NVS%0Amethods%2C%20whether%20based%20on%20GS%20or%20NeRF%2C%20primarily%20rely%20on%20camera%20parameters%0Aprovided%20by%20COLMAP%20and%20even%20utilize%20sparse%20point%20clouds%20generated%20by%20COLMAP%20for%0Ainitialization%2C%20which%20lack%20accuracy%20as%20well%20are%20time-consuming.%20This%20sometimes%0Aresults%20in%20poor%20dynamic%20scene%20representation%2C%20especially%20in%20scenes%20with%20large%0Aobject%20movements%2C%20or%20extreme%20camera%20conditions%20e.g.%20small%20translations%20combined%0Awith%20large%20rotations.%20Some%20studies%20simultaneously%20optimize%20the%20estimation%20of%0Acamera%20parameters%20and%20scenes%2C%20supervised%20by%20additional%20information%20like%20depth%2C%0Aoptical%20flow%2C%20etc.%20obtained%20from%20off-the-shelf%20models.%20Using%20this%20unverified%0Ainformation%20as%20ground%20truth%20can%20reduce%20robustness%20and%20accuracy%2C%20which%20does%0Afrequently%20occur%20for%20long%20monocular%20videos%20%28with%20e.g.%20%3E%20hundreds%20of%20frames%29.%20We%0Apropose%20a%20novel%20approach%20that%20learns%20a%20high-fidelity%204D%20GS%20scene%20representation%0Awith%20self-calibration%20of%20camera%20parameters.%20It%20includes%20the%20extraction%20of%202D%0Apoint%20features%20that%20robustly%20represent%203D%20structure%2C%20and%20their%20use%20for%0Asubsequent%20joint%20optimization%20of%20camera%20parameters%20and%203D%20structure%20towards%0Aoverall%204D%20scene%20optimization.%20We%20demonstrate%20the%20accuracy%20and%20time%20efficiency%0Aof%20our%20method%20through%20extensive%20quantitative%20and%20qualitative%20experimental%0Aresults%20on%20several%20standard%20benchmarks.%20The%20results%20show%20significant%0Aimprovements%20over%20state-of-the-art%20methods%20for%204D%20novel%20view%20synthesis.%20The%0Asource%20code%20will%20be%20released%20soon%20at%20https%3A//github.com/fangli333/SC-4DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01042v2&entry.124074799=Read"},
{"title": "MeshAvatar: Learning High-quality Triangular Human Avatars from\n  Multi-view Videos", "author": "Yushuo Chen and Zerong Zheng and Zhe Li and Chao Xu and Yebin Liu", "abstract": "  We present a novel pipeline for learning high-quality triangular human\navatars from multi-view videos. Recent methods for avatar learning are\ntypically based on neural radiance fields (NeRF), which is not compatible with\ntraditional graphics pipeline and poses great challenges for operations like\nediting or synthesizing under different environments. To overcome these\nlimitations, our method represents the avatar with an explicit triangular mesh\nextracted from an implicit SDF field, complemented by an implicit material\nfield conditioned on given poses. Leveraging this triangular avatar\nrepresentation, we incorporate physics-based rendering to accurately decompose\ngeometry and texture. To enhance both the geometric and appearance details, we\nfurther employ a 2D UNet as the network backbone and introduce pseudo normal\nground-truth as additional supervision. Experiments show that our method can\nlearn triangular avatars with high-quality geometry reconstruction and\nplausible material decomposition, inherently supporting editing, manipulation\nor relighting operations.\n", "link": "http://arxiv.org/abs/2407.08414v1", "date": "2024-07-11", "relevancy": 3.2813, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6646}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6646}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshAvatar%3A%20Learning%20High-quality%20Triangular%20Human%20Avatars%20from%0A%20%20Multi-view%20Videos&body=Title%3A%20MeshAvatar%3A%20Learning%20High-quality%20Triangular%20Human%20Avatars%20from%0A%20%20Multi-view%20Videos%0AAuthor%3A%20Yushuo%20Chen%20and%20Zerong%20Zheng%20and%20Zhe%20Li%20and%20Chao%20Xu%20and%20Yebin%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20novel%20pipeline%20for%20learning%20high-quality%20triangular%20human%0Aavatars%20from%20multi-view%20videos.%20Recent%20methods%20for%20avatar%20learning%20are%0Atypically%20based%20on%20neural%20radiance%20fields%20%28NeRF%29%2C%20which%20is%20not%20compatible%20with%0Atraditional%20graphics%20pipeline%20and%20poses%20great%20challenges%20for%20operations%20like%0Aediting%20or%20synthesizing%20under%20different%20environments.%20To%20overcome%20these%0Alimitations%2C%20our%20method%20represents%20the%20avatar%20with%20an%20explicit%20triangular%20mesh%0Aextracted%20from%20an%20implicit%20SDF%20field%2C%20complemented%20by%20an%20implicit%20material%0Afield%20conditioned%20on%20given%20poses.%20Leveraging%20this%20triangular%20avatar%0Arepresentation%2C%20we%20incorporate%20physics-based%20rendering%20to%20accurately%20decompose%0Ageometry%20and%20texture.%20To%20enhance%20both%20the%20geometric%20and%20appearance%20details%2C%20we%0Afurther%20employ%20a%202D%20UNet%20as%20the%20network%20backbone%20and%20introduce%20pseudo%20normal%0Aground-truth%20as%20additional%20supervision.%20Experiments%20show%20that%20our%20method%20can%0Alearn%20triangular%20avatars%20with%20high-quality%20geometry%20reconstruction%20and%0Aplausible%20material%20decomposition%2C%20inherently%20supporting%20editing%2C%20manipulation%0Aor%20relighting%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshAvatar%253A%2520Learning%2520High-quality%2520Triangular%2520Human%2520Avatars%2520from%250A%2520%2520Multi-view%2520Videos%26entry.906535625%3DYushuo%2520Chen%2520and%2520Zerong%2520Zheng%2520and%2520Zhe%2520Li%2520and%2520Chao%2520Xu%2520and%2520Yebin%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520pipeline%2520for%2520learning%2520high-quality%2520triangular%2520human%250Aavatars%2520from%2520multi-view%2520videos.%2520Recent%2520methods%2520for%2520avatar%2520learning%2520are%250Atypically%2520based%2520on%2520neural%2520radiance%2520fields%2520%2528NeRF%2529%252C%2520which%2520is%2520not%2520compatible%2520with%250Atraditional%2520graphics%2520pipeline%2520and%2520poses%2520great%2520challenges%2520for%2520operations%2520like%250Aediting%2520or%2520synthesizing%2520under%2520different%2520environments.%2520To%2520overcome%2520these%250Alimitations%252C%2520our%2520method%2520represents%2520the%2520avatar%2520with%2520an%2520explicit%2520triangular%2520mesh%250Aextracted%2520from%2520an%2520implicit%2520SDF%2520field%252C%2520complemented%2520by%2520an%2520implicit%2520material%250Afield%2520conditioned%2520on%2520given%2520poses.%2520Leveraging%2520this%2520triangular%2520avatar%250Arepresentation%252C%2520we%2520incorporate%2520physics-based%2520rendering%2520to%2520accurately%2520decompose%250Ageometry%2520and%2520texture.%2520To%2520enhance%2520both%2520the%2520geometric%2520and%2520appearance%2520details%252C%2520we%250Afurther%2520employ%2520a%25202D%2520UNet%2520as%2520the%2520network%2520backbone%2520and%2520introduce%2520pseudo%2520normal%250Aground-truth%2520as%2520additional%2520supervision.%2520Experiments%2520show%2520that%2520our%2520method%2520can%250Alearn%2520triangular%2520avatars%2520with%2520high-quality%2520geometry%2520reconstruction%2520and%250Aplausible%2520material%2520decomposition%252C%2520inherently%2520supporting%2520editing%252C%2520manipulation%250Aor%2520relighting%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshAvatar%3A%20Learning%20High-quality%20Triangular%20Human%20Avatars%20from%0A%20%20Multi-view%20Videos&entry.906535625=Yushuo%20Chen%20and%20Zerong%20Zheng%20and%20Zhe%20Li%20and%20Chao%20Xu%20and%20Yebin%20Liu&entry.1292438233=%20%20We%20present%20a%20novel%20pipeline%20for%20learning%20high-quality%20triangular%20human%0Aavatars%20from%20multi-view%20videos.%20Recent%20methods%20for%20avatar%20learning%20are%0Atypically%20based%20on%20neural%20radiance%20fields%20%28NeRF%29%2C%20which%20is%20not%20compatible%20with%0Atraditional%20graphics%20pipeline%20and%20poses%20great%20challenges%20for%20operations%20like%0Aediting%20or%20synthesizing%20under%20different%20environments.%20To%20overcome%20these%0Alimitations%2C%20our%20method%20represents%20the%20avatar%20with%20an%20explicit%20triangular%20mesh%0Aextracted%20from%20an%20implicit%20SDF%20field%2C%20complemented%20by%20an%20implicit%20material%0Afield%20conditioned%20on%20given%20poses.%20Leveraging%20this%20triangular%20avatar%0Arepresentation%2C%20we%20incorporate%20physics-based%20rendering%20to%20accurately%20decompose%0Ageometry%20and%20texture.%20To%20enhance%20both%20the%20geometric%20and%20appearance%20details%2C%20we%0Afurther%20employ%20a%202D%20UNet%20as%20the%20network%20backbone%20and%20introduce%20pseudo%20normal%0Aground-truth%20as%20additional%20supervision.%20Experiments%20show%20that%20our%20method%20can%0Alearn%20triangular%20avatars%20with%20high-quality%20geometry%20reconstruction%20and%0Aplausible%20material%20decomposition%2C%20inherently%20supporting%20editing%2C%20manipulation%0Aor%20relighting%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08414v1&entry.124074799=Read"},
{"title": "MapLocNet: Coarse-to-Fine Feature Registration for Visual\n  Re-Localization in Navigation Maps", "author": "Hang Wu and Zhenghao Zhang and Siyuan Lin and Xiangru Mu and Qiang Zhao and Ming Yang and Tong Qin", "abstract": "  Robust localization is the cornerstone of autonomous driving, especially in\nchallenging urban environments where GPS signals suffer from multipath errors.\nTraditional localization approaches rely on high-definition (HD) maps, which\nconsist of precisely annotated landmarks. However, building HD map is expensive\nand challenging to scale up. Given these limitations, leveraging navigation\nmaps has emerged as a promising low-cost alternative for localization. Current\napproaches based on navigation maps can achieve highly accurate localization,\nbut their complex matching strategies lead to unacceptable inference latency\nthat fails to meet the real-time demands. To address these limitations, we\npropose a novel transformer-based neural re-localization method. Inspired by\nimage registration, our approach performs a coarse-to-fine neural feature\nregistration between navigation map and visual bird's-eye view features. Our\nmethod significantly outperforms the current state-of-the-art OrienterNet on\nboth the nuScenes and Argoverse datasets, which is nearly 10%/20% localization\naccuracy and 30/16 FPS improvement on single-view and surround-view input\nsettings, separately. We highlight that our research presents an HD-map-free\nlocalization method for autonomous driving, offering cost-effective, reliable,\nand scalable performance in challenging driving environments.\n", "link": "http://arxiv.org/abs/2407.08561v1", "date": "2024-07-11", "relevancy": 3.0784, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6703}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6362}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapLocNet%3A%20Coarse-to-Fine%20Feature%20Registration%20for%20Visual%0A%20%20Re-Localization%20in%20Navigation%20Maps&body=Title%3A%20MapLocNet%3A%20Coarse-to-Fine%20Feature%20Registration%20for%20Visual%0A%20%20Re-Localization%20in%20Navigation%20Maps%0AAuthor%3A%20Hang%20Wu%20and%20Zhenghao%20Zhang%20and%20Siyuan%20Lin%20and%20Xiangru%20Mu%20and%20Qiang%20Zhao%20and%20Ming%20Yang%20and%20Tong%20Qin%0AAbstract%3A%20%20%20Robust%20localization%20is%20the%20cornerstone%20of%20autonomous%20driving%2C%20especially%20in%0Achallenging%20urban%20environments%20where%20GPS%20signals%20suffer%20from%20multipath%20errors.%0ATraditional%20localization%20approaches%20rely%20on%20high-definition%20%28HD%29%20maps%2C%20which%0Aconsist%20of%20precisely%20annotated%20landmarks.%20However%2C%20building%20HD%20map%20is%20expensive%0Aand%20challenging%20to%20scale%20up.%20Given%20these%20limitations%2C%20leveraging%20navigation%0Amaps%20has%20emerged%20as%20a%20promising%20low-cost%20alternative%20for%20localization.%20Current%0Aapproaches%20based%20on%20navigation%20maps%20can%20achieve%20highly%20accurate%20localization%2C%0Abut%20their%20complex%20matching%20strategies%20lead%20to%20unacceptable%20inference%20latency%0Athat%20fails%20to%20meet%20the%20real-time%20demands.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20novel%20transformer-based%20neural%20re-localization%20method.%20Inspired%20by%0Aimage%20registration%2C%20our%20approach%20performs%20a%20coarse-to-fine%20neural%20feature%0Aregistration%20between%20navigation%20map%20and%20visual%20bird%27s-eye%20view%20features.%20Our%0Amethod%20significantly%20outperforms%20the%20current%20state-of-the-art%20OrienterNet%20on%0Aboth%20the%20nuScenes%20and%20Argoverse%20datasets%2C%20which%20is%20nearly%2010%25/20%25%20localization%0Aaccuracy%20and%2030/16%20FPS%20improvement%20on%20single-view%20and%20surround-view%20input%0Asettings%2C%20separately.%20We%20highlight%20that%20our%20research%20presents%20an%20HD-map-free%0Alocalization%20method%20for%20autonomous%20driving%2C%20offering%20cost-effective%2C%20reliable%2C%0Aand%20scalable%20performance%20in%20challenging%20driving%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapLocNet%253A%2520Coarse-to-Fine%2520Feature%2520Registration%2520for%2520Visual%250A%2520%2520Re-Localization%2520in%2520Navigation%2520Maps%26entry.906535625%3DHang%2520Wu%2520and%2520Zhenghao%2520Zhang%2520and%2520Siyuan%2520Lin%2520and%2520Xiangru%2520Mu%2520and%2520Qiang%2520Zhao%2520and%2520Ming%2520Yang%2520and%2520Tong%2520Qin%26entry.1292438233%3D%2520%2520Robust%2520localization%2520is%2520the%2520cornerstone%2520of%2520autonomous%2520driving%252C%2520especially%2520in%250Achallenging%2520urban%2520environments%2520where%2520GPS%2520signals%2520suffer%2520from%2520multipath%2520errors.%250ATraditional%2520localization%2520approaches%2520rely%2520on%2520high-definition%2520%2528HD%2529%2520maps%252C%2520which%250Aconsist%2520of%2520precisely%2520annotated%2520landmarks.%2520However%252C%2520building%2520HD%2520map%2520is%2520expensive%250Aand%2520challenging%2520to%2520scale%2520up.%2520Given%2520these%2520limitations%252C%2520leveraging%2520navigation%250Amaps%2520has%2520emerged%2520as%2520a%2520promising%2520low-cost%2520alternative%2520for%2520localization.%2520Current%250Aapproaches%2520based%2520on%2520navigation%2520maps%2520can%2520achieve%2520highly%2520accurate%2520localization%252C%250Abut%2520their%2520complex%2520matching%2520strategies%2520lead%2520to%2520unacceptable%2520inference%2520latency%250Athat%2520fails%2520to%2520meet%2520the%2520real-time%2520demands.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520novel%2520transformer-based%2520neural%2520re-localization%2520method.%2520Inspired%2520by%250Aimage%2520registration%252C%2520our%2520approach%2520performs%2520a%2520coarse-to-fine%2520neural%2520feature%250Aregistration%2520between%2520navigation%2520map%2520and%2520visual%2520bird%2527s-eye%2520view%2520features.%2520Our%250Amethod%2520significantly%2520outperforms%2520the%2520current%2520state-of-the-art%2520OrienterNet%2520on%250Aboth%2520the%2520nuScenes%2520and%2520Argoverse%2520datasets%252C%2520which%2520is%2520nearly%252010%2525/20%2525%2520localization%250Aaccuracy%2520and%252030/16%2520FPS%2520improvement%2520on%2520single-view%2520and%2520surround-view%2520input%250Asettings%252C%2520separately.%2520We%2520highlight%2520that%2520our%2520research%2520presents%2520an%2520HD-map-free%250Alocalization%2520method%2520for%2520autonomous%2520driving%252C%2520offering%2520cost-effective%252C%2520reliable%252C%250Aand%2520scalable%2520performance%2520in%2520challenging%2520driving%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapLocNet%3A%20Coarse-to-Fine%20Feature%20Registration%20for%20Visual%0A%20%20Re-Localization%20in%20Navigation%20Maps&entry.906535625=Hang%20Wu%20and%20Zhenghao%20Zhang%20and%20Siyuan%20Lin%20and%20Xiangru%20Mu%20and%20Qiang%20Zhao%20and%20Ming%20Yang%20and%20Tong%20Qin&entry.1292438233=%20%20Robust%20localization%20is%20the%20cornerstone%20of%20autonomous%20driving%2C%20especially%20in%0Achallenging%20urban%20environments%20where%20GPS%20signals%20suffer%20from%20multipath%20errors.%0ATraditional%20localization%20approaches%20rely%20on%20high-definition%20%28HD%29%20maps%2C%20which%0Aconsist%20of%20precisely%20annotated%20landmarks.%20However%2C%20building%20HD%20map%20is%20expensive%0Aand%20challenging%20to%20scale%20up.%20Given%20these%20limitations%2C%20leveraging%20navigation%0Amaps%20has%20emerged%20as%20a%20promising%20low-cost%20alternative%20for%20localization.%20Current%0Aapproaches%20based%20on%20navigation%20maps%20can%20achieve%20highly%20accurate%20localization%2C%0Abut%20their%20complex%20matching%20strategies%20lead%20to%20unacceptable%20inference%20latency%0Athat%20fails%20to%20meet%20the%20real-time%20demands.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20novel%20transformer-based%20neural%20re-localization%20method.%20Inspired%20by%0Aimage%20registration%2C%20our%20approach%20performs%20a%20coarse-to-fine%20neural%20feature%0Aregistration%20between%20navigation%20map%20and%20visual%20bird%27s-eye%20view%20features.%20Our%0Amethod%20significantly%20outperforms%20the%20current%20state-of-the-art%20OrienterNet%20on%0Aboth%20the%20nuScenes%20and%20Argoverse%20datasets%2C%20which%20is%20nearly%2010%25/20%25%20localization%0Aaccuracy%20and%2030/16%20FPS%20improvement%20on%20single-view%20and%20surround-view%20input%0Asettings%2C%20separately.%20We%20highlight%20that%20our%20research%20presents%20an%20HD-map-free%0Alocalization%20method%20for%20autonomous%20driving%2C%20offering%20cost-effective%2C%20reliable%2C%0Aand%20scalable%20performance%20in%20challenging%20driving%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08561v1&entry.124074799=Read"},
{"title": "IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans\n  from Monocular Videos via Explicit Ray Tracing", "author": "Shaofei Wang and Bo\u017eidar Anti\u0107 and Andreas Geiger and Siyu Tang", "abstract": "  We present IntrinsicAvatar, a novel approach to recovering the intrinsic\nproperties of clothed human avatars including geometry, albedo, material, and\nenvironment lighting from only monocular videos. Recent advancements in\nhuman-based neural rendering have enabled high-quality geometry and appearance\nreconstruction of clothed humans from just monocular videos. However, these\nmethods bake intrinsic properties such as albedo, material, and environment\nlighting into a single entangled neural representation. On the other hand, only\na handful of works tackle the problem of estimating geometry and disentangled\nappearance properties of clothed humans from monocular videos. They usually\nachieve limited quality and disentanglement due to approximations of secondary\nshading effects via learned MLPs. In this work, we propose to model secondary\nshading effects explicitly via Monte-Carlo ray tracing. We model the rendering\nprocess of clothed humans as a volumetric scattering process, and combine ray\ntracing with body articulation. Our approach can recover high-quality geometry,\nalbedo, material, and lighting properties of clothed humans from a single\nmonocular video, without requiring supervised pre-training using ground truth\nmaterials. Furthermore, since we explicitly model the volumetric scattering\nprocess and ray tracing, our model naturally generalizes to novel poses,\nenabling animation of the reconstructed avatar in novel lighting conditions.\n", "link": "http://arxiv.org/abs/2312.05210v2", "date": "2024-07-11", "relevancy": 3.0102, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6175}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6175}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IntrinsicAvatar%3A%20Physically%20Based%20Inverse%20Rendering%20of%20Dynamic%20Humans%0A%20%20from%20Monocular%20Videos%20via%20Explicit%20Ray%20Tracing&body=Title%3A%20IntrinsicAvatar%3A%20Physically%20Based%20Inverse%20Rendering%20of%20Dynamic%20Humans%0A%20%20from%20Monocular%20Videos%20via%20Explicit%20Ray%20Tracing%0AAuthor%3A%20Shaofei%20Wang%20and%20Bo%C5%BEidar%20Anti%C4%87%20and%20Andreas%20Geiger%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20We%20present%20IntrinsicAvatar%2C%20a%20novel%20approach%20to%20recovering%20the%20intrinsic%0Aproperties%20of%20clothed%20human%20avatars%20including%20geometry%2C%20albedo%2C%20material%2C%20and%0Aenvironment%20lighting%20from%20only%20monocular%20videos.%20Recent%20advancements%20in%0Ahuman-based%20neural%20rendering%20have%20enabled%20high-quality%20geometry%20and%20appearance%0Areconstruction%20of%20clothed%20humans%20from%20just%20monocular%20videos.%20However%2C%20these%0Amethods%20bake%20intrinsic%20properties%20such%20as%20albedo%2C%20material%2C%20and%20environment%0Alighting%20into%20a%20single%20entangled%20neural%20representation.%20On%20the%20other%20hand%2C%20only%0Aa%20handful%20of%20works%20tackle%20the%20problem%20of%20estimating%20geometry%20and%20disentangled%0Aappearance%20properties%20of%20clothed%20humans%20from%20monocular%20videos.%20They%20usually%0Aachieve%20limited%20quality%20and%20disentanglement%20due%20to%20approximations%20of%20secondary%0Ashading%20effects%20via%20learned%20MLPs.%20In%20this%20work%2C%20we%20propose%20to%20model%20secondary%0Ashading%20effects%20explicitly%20via%20Monte-Carlo%20ray%20tracing.%20We%20model%20the%20rendering%0Aprocess%20of%20clothed%20humans%20as%20a%20volumetric%20scattering%20process%2C%20and%20combine%20ray%0Atracing%20with%20body%20articulation.%20Our%20approach%20can%20recover%20high-quality%20geometry%2C%0Aalbedo%2C%20material%2C%20and%20lighting%20properties%20of%20clothed%20humans%20from%20a%20single%0Amonocular%20video%2C%20without%20requiring%20supervised%20pre-training%20using%20ground%20truth%0Amaterials.%20Furthermore%2C%20since%20we%20explicitly%20model%20the%20volumetric%20scattering%0Aprocess%20and%20ray%20tracing%2C%20our%20model%20naturally%20generalizes%20to%20novel%20poses%2C%0Aenabling%20animation%20of%20the%20reconstructed%20avatar%20in%20novel%20lighting%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsicAvatar%253A%2520Physically%2520Based%2520Inverse%2520Rendering%2520of%2520Dynamic%2520Humans%250A%2520%2520from%2520Monocular%2520Videos%2520via%2520Explicit%2520Ray%2520Tracing%26entry.906535625%3DShaofei%2520Wang%2520and%2520Bo%25C5%25BEidar%2520Anti%25C4%2587%2520and%2520Andreas%2520Geiger%2520and%2520Siyu%2520Tang%26entry.1292438233%3D%2520%2520We%2520present%2520IntrinsicAvatar%252C%2520a%2520novel%2520approach%2520to%2520recovering%2520the%2520intrinsic%250Aproperties%2520of%2520clothed%2520human%2520avatars%2520including%2520geometry%252C%2520albedo%252C%2520material%252C%2520and%250Aenvironment%2520lighting%2520from%2520only%2520monocular%2520videos.%2520Recent%2520advancements%2520in%250Ahuman-based%2520neural%2520rendering%2520have%2520enabled%2520high-quality%2520geometry%2520and%2520appearance%250Areconstruction%2520of%2520clothed%2520humans%2520from%2520just%2520monocular%2520videos.%2520However%252C%2520these%250Amethods%2520bake%2520intrinsic%2520properties%2520such%2520as%2520albedo%252C%2520material%252C%2520and%2520environment%250Alighting%2520into%2520a%2520single%2520entangled%2520neural%2520representation.%2520On%2520the%2520other%2520hand%252C%2520only%250Aa%2520handful%2520of%2520works%2520tackle%2520the%2520problem%2520of%2520estimating%2520geometry%2520and%2520disentangled%250Aappearance%2520properties%2520of%2520clothed%2520humans%2520from%2520monocular%2520videos.%2520They%2520usually%250Aachieve%2520limited%2520quality%2520and%2520disentanglement%2520due%2520to%2520approximations%2520of%2520secondary%250Ashading%2520effects%2520via%2520learned%2520MLPs.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520model%2520secondary%250Ashading%2520effects%2520explicitly%2520via%2520Monte-Carlo%2520ray%2520tracing.%2520We%2520model%2520the%2520rendering%250Aprocess%2520of%2520clothed%2520humans%2520as%2520a%2520volumetric%2520scattering%2520process%252C%2520and%2520combine%2520ray%250Atracing%2520with%2520body%2520articulation.%2520Our%2520approach%2520can%2520recover%2520high-quality%2520geometry%252C%250Aalbedo%252C%2520material%252C%2520and%2520lighting%2520properties%2520of%2520clothed%2520humans%2520from%2520a%2520single%250Amonocular%2520video%252C%2520without%2520requiring%2520supervised%2520pre-training%2520using%2520ground%2520truth%250Amaterials.%2520Furthermore%252C%2520since%2520we%2520explicitly%2520model%2520the%2520volumetric%2520scattering%250Aprocess%2520and%2520ray%2520tracing%252C%2520our%2520model%2520naturally%2520generalizes%2520to%2520novel%2520poses%252C%250Aenabling%2520animation%2520of%2520the%2520reconstructed%2520avatar%2520in%2520novel%2520lighting%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntrinsicAvatar%3A%20Physically%20Based%20Inverse%20Rendering%20of%20Dynamic%20Humans%0A%20%20from%20Monocular%20Videos%20via%20Explicit%20Ray%20Tracing&entry.906535625=Shaofei%20Wang%20and%20Bo%C5%BEidar%20Anti%C4%87%20and%20Andreas%20Geiger%20and%20Siyu%20Tang&entry.1292438233=%20%20We%20present%20IntrinsicAvatar%2C%20a%20novel%20approach%20to%20recovering%20the%20intrinsic%0Aproperties%20of%20clothed%20human%20avatars%20including%20geometry%2C%20albedo%2C%20material%2C%20and%0Aenvironment%20lighting%20from%20only%20monocular%20videos.%20Recent%20advancements%20in%0Ahuman-based%20neural%20rendering%20have%20enabled%20high-quality%20geometry%20and%20appearance%0Areconstruction%20of%20clothed%20humans%20from%20just%20monocular%20videos.%20However%2C%20these%0Amethods%20bake%20intrinsic%20properties%20such%20as%20albedo%2C%20material%2C%20and%20environment%0Alighting%20into%20a%20single%20entangled%20neural%20representation.%20On%20the%20other%20hand%2C%20only%0Aa%20handful%20of%20works%20tackle%20the%20problem%20of%20estimating%20geometry%20and%20disentangled%0Aappearance%20properties%20of%20clothed%20humans%20from%20monocular%20videos.%20They%20usually%0Aachieve%20limited%20quality%20and%20disentanglement%20due%20to%20approximations%20of%20secondary%0Ashading%20effects%20via%20learned%20MLPs.%20In%20this%20work%2C%20we%20propose%20to%20model%20secondary%0Ashading%20effects%20explicitly%20via%20Monte-Carlo%20ray%20tracing.%20We%20model%20the%20rendering%0Aprocess%20of%20clothed%20humans%20as%20a%20volumetric%20scattering%20process%2C%20and%20combine%20ray%0Atracing%20with%20body%20articulation.%20Our%20approach%20can%20recover%20high-quality%20geometry%2C%0Aalbedo%2C%20material%2C%20and%20lighting%20properties%20of%20clothed%20humans%20from%20a%20single%0Amonocular%20video%2C%20without%20requiring%20supervised%20pre-training%20using%20ground%20truth%0Amaterials.%20Furthermore%2C%20since%20we%20explicitly%20model%20the%20volumetric%20scattering%0Aprocess%20and%20ray%20tracing%2C%20our%20model%20naturally%20generalizes%20to%20novel%20poses%2C%0Aenabling%20animation%20of%20the%20reconstructed%20avatar%20in%20novel%20lighting%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05210v2&entry.124074799=Read"},
{"title": "CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization", "author": "Jiawei Zhang and Jiahe Li and Xiaohan Yu and Lei Huang and Lin Gu and Jin Zheng and Xiao Bai", "abstract": "  3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D\nGaussians to represent a scene. With sparse training views, 3DGS easily suffers\nfrom overfitting, negatively impacting rendering. This paper introduces a new\nco-regularization perspective for improving sparse-view 3DGS. When training two\n3D Gaussian radiance fields, we observe that the two radiance fields exhibit\npoint disagreement and rendering disagreement that can unsupervisedly predict\nreconstruction quality, stemming from the randomness of densification\nimplementation. We further quantify the two disagreements and demonstrate the\nnegative correlation between them and accurate reconstruction, which allows us\nto identify inaccurate reconstruction without accessing ground-truth\ninformation. Based on the study, we propose CoR-GS, which identifies and\nsuppresses inaccurate reconstruction based on the two disagreements: (1)\nCo-pruning considers Gaussians that exhibit high point disagreement in\ninaccurate positions and prunes them. (2) Pseudo-view co-regularization\nconsiders pixels that exhibit high rendering disagreement are inaccurate and\nsuppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender\ndemonstrate that CoR-GS effectively regularizes the scene geometry,\nreconstructs the compact representations, and achieves state-of-the-art novel\nview synthesis quality under sparse training views.\n", "link": "http://arxiv.org/abs/2405.12110v2", "date": "2024-07-11", "relevancy": 2.9995, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6689}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.615}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoR-GS%3A%20Sparse-View%203D%20Gaussian%20Splatting%20via%20Co-Regularization&body=Title%3A%20CoR-GS%3A%20Sparse-View%203D%20Gaussian%20Splatting%20via%20Co-Regularization%0AAuthor%3A%20Jiawei%20Zhang%20and%20Jiahe%20Li%20and%20Xiaohan%20Yu%20and%20Lei%20Huang%20and%20Lin%20Gu%20and%20Jin%20Zheng%20and%20Xiao%20Bai%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20creates%20a%20radiance%20field%20consisting%20of%203D%0AGaussians%20to%20represent%20a%20scene.%20With%20sparse%20training%20views%2C%203DGS%20easily%20suffers%0Afrom%20overfitting%2C%20negatively%20impacting%20rendering.%20This%20paper%20introduces%20a%20new%0Aco-regularization%20perspective%20for%20improving%20sparse-view%203DGS.%20When%20training%20two%0A3D%20Gaussian%20radiance%20fields%2C%20we%20observe%20that%20the%20two%20radiance%20fields%20exhibit%0Apoint%20disagreement%20and%20rendering%20disagreement%20that%20can%20unsupervisedly%20predict%0Areconstruction%20quality%2C%20stemming%20from%20the%20randomness%20of%20densification%0Aimplementation.%20We%20further%20quantify%20the%20two%20disagreements%20and%20demonstrate%20the%0Anegative%20correlation%20between%20them%20and%20accurate%20reconstruction%2C%20which%20allows%20us%0Ato%20identify%20inaccurate%20reconstruction%20without%20accessing%20ground-truth%0Ainformation.%20Based%20on%20the%20study%2C%20we%20propose%20CoR-GS%2C%20which%20identifies%20and%0Asuppresses%20inaccurate%20reconstruction%20based%20on%20the%20two%20disagreements%3A%20%281%29%0ACo-pruning%20considers%20Gaussians%20that%20exhibit%20high%20point%20disagreement%20in%0Ainaccurate%20positions%20and%20prunes%20them.%20%282%29%20Pseudo-view%20co-regularization%0Aconsiders%20pixels%20that%20exhibit%20high%20rendering%20disagreement%20are%20inaccurate%20and%0Asuppress%20the%20disagreement.%20Results%20on%20LLFF%2C%20Mip-NeRF360%2C%20DTU%2C%20and%20Blender%0Ademonstrate%20that%20CoR-GS%20effectively%20regularizes%20the%20scene%20geometry%2C%0Areconstructs%20the%20compact%20representations%2C%20and%20achieves%20state-of-the-art%20novel%0Aview%20synthesis%20quality%20under%20sparse%20training%20views.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12110v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoR-GS%253A%2520Sparse-View%25203D%2520Gaussian%2520Splatting%2520via%2520Co-Regularization%26entry.906535625%3DJiawei%2520Zhang%2520and%2520Jiahe%2520Li%2520and%2520Xiaohan%2520Yu%2520and%2520Lei%2520Huang%2520and%2520Lin%2520Gu%2520and%2520Jin%2520Zheng%2520and%2520Xiao%2520Bai%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520creates%2520a%2520radiance%2520field%2520consisting%2520of%25203D%250AGaussians%2520to%2520represent%2520a%2520scene.%2520With%2520sparse%2520training%2520views%252C%25203DGS%2520easily%2520suffers%250Afrom%2520overfitting%252C%2520negatively%2520impacting%2520rendering.%2520This%2520paper%2520introduces%2520a%2520new%250Aco-regularization%2520perspective%2520for%2520improving%2520sparse-view%25203DGS.%2520When%2520training%2520two%250A3D%2520Gaussian%2520radiance%2520fields%252C%2520we%2520observe%2520that%2520the%2520two%2520radiance%2520fields%2520exhibit%250Apoint%2520disagreement%2520and%2520rendering%2520disagreement%2520that%2520can%2520unsupervisedly%2520predict%250Areconstruction%2520quality%252C%2520stemming%2520from%2520the%2520randomness%2520of%2520densification%250Aimplementation.%2520We%2520further%2520quantify%2520the%2520two%2520disagreements%2520and%2520demonstrate%2520the%250Anegative%2520correlation%2520between%2520them%2520and%2520accurate%2520reconstruction%252C%2520which%2520allows%2520us%250Ato%2520identify%2520inaccurate%2520reconstruction%2520without%2520accessing%2520ground-truth%250Ainformation.%2520Based%2520on%2520the%2520study%252C%2520we%2520propose%2520CoR-GS%252C%2520which%2520identifies%2520and%250Asuppresses%2520inaccurate%2520reconstruction%2520based%2520on%2520the%2520two%2520disagreements%253A%2520%25281%2529%250ACo-pruning%2520considers%2520Gaussians%2520that%2520exhibit%2520high%2520point%2520disagreement%2520in%250Ainaccurate%2520positions%2520and%2520prunes%2520them.%2520%25282%2529%2520Pseudo-view%2520co-regularization%250Aconsiders%2520pixels%2520that%2520exhibit%2520high%2520rendering%2520disagreement%2520are%2520inaccurate%2520and%250Asuppress%2520the%2520disagreement.%2520Results%2520on%2520LLFF%252C%2520Mip-NeRF360%252C%2520DTU%252C%2520and%2520Blender%250Ademonstrate%2520that%2520CoR-GS%2520effectively%2520regularizes%2520the%2520scene%2520geometry%252C%250Areconstructs%2520the%2520compact%2520representations%252C%2520and%2520achieves%2520state-of-the-art%2520novel%250Aview%2520synthesis%2520quality%2520under%2520sparse%2520training%2520views.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12110v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoR-GS%3A%20Sparse-View%203D%20Gaussian%20Splatting%20via%20Co-Regularization&entry.906535625=Jiawei%20Zhang%20and%20Jiahe%20Li%20and%20Xiaohan%20Yu%20and%20Lei%20Huang%20and%20Lin%20Gu%20and%20Jin%20Zheng%20and%20Xiao%20Bai&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20creates%20a%20radiance%20field%20consisting%20of%203D%0AGaussians%20to%20represent%20a%20scene.%20With%20sparse%20training%20views%2C%203DGS%20easily%20suffers%0Afrom%20overfitting%2C%20negatively%20impacting%20rendering.%20This%20paper%20introduces%20a%20new%0Aco-regularization%20perspective%20for%20improving%20sparse-view%203DGS.%20When%20training%20two%0A3D%20Gaussian%20radiance%20fields%2C%20we%20observe%20that%20the%20two%20radiance%20fields%20exhibit%0Apoint%20disagreement%20and%20rendering%20disagreement%20that%20can%20unsupervisedly%20predict%0Areconstruction%20quality%2C%20stemming%20from%20the%20randomness%20of%20densification%0Aimplementation.%20We%20further%20quantify%20the%20two%20disagreements%20and%20demonstrate%20the%0Anegative%20correlation%20between%20them%20and%20accurate%20reconstruction%2C%20which%20allows%20us%0Ato%20identify%20inaccurate%20reconstruction%20without%20accessing%20ground-truth%0Ainformation.%20Based%20on%20the%20study%2C%20we%20propose%20CoR-GS%2C%20which%20identifies%20and%0Asuppresses%20inaccurate%20reconstruction%20based%20on%20the%20two%20disagreements%3A%20%281%29%0ACo-pruning%20considers%20Gaussians%20that%20exhibit%20high%20point%20disagreement%20in%0Ainaccurate%20positions%20and%20prunes%20them.%20%282%29%20Pseudo-view%20co-regularization%0Aconsiders%20pixels%20that%20exhibit%20high%20rendering%20disagreement%20are%20inaccurate%20and%0Asuppress%20the%20disagreement.%20Results%20on%20LLFF%2C%20Mip-NeRF360%2C%20DTU%2C%20and%20Blender%0Ademonstrate%20that%20CoR-GS%20effectively%20regularizes%20the%20scene%20geometry%2C%0Areconstructs%20the%20compact%20representations%2C%20and%20achieves%20state-of-the-art%20novel%0Aview%20synthesis%20quality%20under%20sparse%20training%20views.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12110v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey on Human Video Generation: Challenges, Methods,\n  and Insights", "author": "Wentao Lei and Jinting Wang and Fengji Ma and Guanjie Huang and Li Liu", "abstract": "  Human video generation is a dynamic and rapidly evolving task that aims to\nsynthesize 2D human body video sequences with generative models given control\nconditions such as text, audio, and pose. With the potential for wide-ranging\napplications in film, gaming, and virtual communication, the ability to\ngenerate natural and realistic human video is critical. Recent advancements in\ngenerative models have laid a solid foundation for the growing interest in this\narea. Despite the significant progress, the task of human video generation\nremains challenging due to the consistency of characters, the complexity of\nhuman motion, and difficulties in their relationship with the environment. This\nsurvey provides a comprehensive review of the current state of human video\ngeneration, marking, to the best of our knowledge, the first extensive\nliterature review in this domain. We start with an introduction to the\nfundamentals of human video generation and the evolution of generative models\nthat have facilitated the field's growth. We then examine the main methods\nemployed for three key sub-tasks within human video generation: text-driven,\naudio-driven, and pose-driven motion generation. These areas are explored\nconcerning the conditions that guide the generation process. Furthermore, we\noffer a collection of the most commonly utilized datasets and the evaluation\nmetrics that are crucial in assessing the quality and realism of generated\nvideos. The survey concludes with a discussion of the current challenges in the\nfield and suggests possible directions for future research. The goal of this\nsurvey is to offer the research community a clear and holistic view of the\nadvancements in human video generation, highlighting the milestones achieved\nand the challenges that lie ahead.\n", "link": "http://arxiv.org/abs/2407.08428v1", "date": "2024-07-11", "relevancy": 2.9028, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5951}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5751}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20on%20Human%20Video%20Generation%3A%20Challenges%2C%20Methods%2C%0A%20%20and%20Insights&body=Title%3A%20A%20Comprehensive%20Survey%20on%20Human%20Video%20Generation%3A%20Challenges%2C%20Methods%2C%0A%20%20and%20Insights%0AAuthor%3A%20Wentao%20Lei%20and%20Jinting%20Wang%20and%20Fengji%20Ma%20and%20Guanjie%20Huang%20and%20Li%20Liu%0AAbstract%3A%20%20%20Human%20video%20generation%20is%20a%20dynamic%20and%20rapidly%20evolving%20task%20that%20aims%20to%0Asynthesize%202D%20human%20body%20video%20sequences%20with%20generative%20models%20given%20control%0Aconditions%20such%20as%20text%2C%20audio%2C%20and%20pose.%20With%20the%20potential%20for%20wide-ranging%0Aapplications%20in%20film%2C%20gaming%2C%20and%20virtual%20communication%2C%20the%20ability%20to%0Agenerate%20natural%20and%20realistic%20human%20video%20is%20critical.%20Recent%20advancements%20in%0Agenerative%20models%20have%20laid%20a%20solid%20foundation%20for%20the%20growing%20interest%20in%20this%0Aarea.%20Despite%20the%20significant%20progress%2C%20the%20task%20of%20human%20video%20generation%0Aremains%20challenging%20due%20to%20the%20consistency%20of%20characters%2C%20the%20complexity%20of%0Ahuman%20motion%2C%20and%20difficulties%20in%20their%20relationship%20with%20the%20environment.%20This%0Asurvey%20provides%20a%20comprehensive%20review%20of%20the%20current%20state%20of%20human%20video%0Ageneration%2C%20marking%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20extensive%0Aliterature%20review%20in%20this%20domain.%20We%20start%20with%20an%20introduction%20to%20the%0Afundamentals%20of%20human%20video%20generation%20and%20the%20evolution%20of%20generative%20models%0Athat%20have%20facilitated%20the%20field%27s%20growth.%20We%20then%20examine%20the%20main%20methods%0Aemployed%20for%20three%20key%20sub-tasks%20within%20human%20video%20generation%3A%20text-driven%2C%0Aaudio-driven%2C%20and%20pose-driven%20motion%20generation.%20These%20areas%20are%20explored%0Aconcerning%20the%20conditions%20that%20guide%20the%20generation%20process.%20Furthermore%2C%20we%0Aoffer%20a%20collection%20of%20the%20most%20commonly%20utilized%20datasets%20and%20the%20evaluation%0Ametrics%20that%20are%20crucial%20in%20assessing%20the%20quality%20and%20realism%20of%20generated%0Avideos.%20The%20survey%20concludes%20with%20a%20discussion%20of%20the%20current%20challenges%20in%20the%0Afield%20and%20suggests%20possible%20directions%20for%20future%20research.%20The%20goal%20of%20this%0Asurvey%20is%20to%20offer%20the%20research%20community%20a%20clear%20and%20holistic%20view%20of%20the%0Aadvancements%20in%20human%20video%20generation%2C%20highlighting%20the%20milestones%20achieved%0Aand%20the%20challenges%20that%20lie%20ahead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520on%2520Human%2520Video%2520Generation%253A%2520Challenges%252C%2520Methods%252C%250A%2520%2520and%2520Insights%26entry.906535625%3DWentao%2520Lei%2520and%2520Jinting%2520Wang%2520and%2520Fengji%2520Ma%2520and%2520Guanjie%2520Huang%2520and%2520Li%2520Liu%26entry.1292438233%3D%2520%2520Human%2520video%2520generation%2520is%2520a%2520dynamic%2520and%2520rapidly%2520evolving%2520task%2520that%2520aims%2520to%250Asynthesize%25202D%2520human%2520body%2520video%2520sequences%2520with%2520generative%2520models%2520given%2520control%250Aconditions%2520such%2520as%2520text%252C%2520audio%252C%2520and%2520pose.%2520With%2520the%2520potential%2520for%2520wide-ranging%250Aapplications%2520in%2520film%252C%2520gaming%252C%2520and%2520virtual%2520communication%252C%2520the%2520ability%2520to%250Agenerate%2520natural%2520and%2520realistic%2520human%2520video%2520is%2520critical.%2520Recent%2520advancements%2520in%250Agenerative%2520models%2520have%2520laid%2520a%2520solid%2520foundation%2520for%2520the%2520growing%2520interest%2520in%2520this%250Aarea.%2520Despite%2520the%2520significant%2520progress%252C%2520the%2520task%2520of%2520human%2520video%2520generation%250Aremains%2520challenging%2520due%2520to%2520the%2520consistency%2520of%2520characters%252C%2520the%2520complexity%2520of%250Ahuman%2520motion%252C%2520and%2520difficulties%2520in%2520their%2520relationship%2520with%2520the%2520environment.%2520This%250Asurvey%2520provides%2520a%2520comprehensive%2520review%2520of%2520the%2520current%2520state%2520of%2520human%2520video%250Ageneration%252C%2520marking%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520extensive%250Aliterature%2520review%2520in%2520this%2520domain.%2520We%2520start%2520with%2520an%2520introduction%2520to%2520the%250Afundamentals%2520of%2520human%2520video%2520generation%2520and%2520the%2520evolution%2520of%2520generative%2520models%250Athat%2520have%2520facilitated%2520the%2520field%2527s%2520growth.%2520We%2520then%2520examine%2520the%2520main%2520methods%250Aemployed%2520for%2520three%2520key%2520sub-tasks%2520within%2520human%2520video%2520generation%253A%2520text-driven%252C%250Aaudio-driven%252C%2520and%2520pose-driven%2520motion%2520generation.%2520These%2520areas%2520are%2520explored%250Aconcerning%2520the%2520conditions%2520that%2520guide%2520the%2520generation%2520process.%2520Furthermore%252C%2520we%250Aoffer%2520a%2520collection%2520of%2520the%2520most%2520commonly%2520utilized%2520datasets%2520and%2520the%2520evaluation%250Ametrics%2520that%2520are%2520crucial%2520in%2520assessing%2520the%2520quality%2520and%2520realism%2520of%2520generated%250Avideos.%2520The%2520survey%2520concludes%2520with%2520a%2520discussion%2520of%2520the%2520current%2520challenges%2520in%2520the%250Afield%2520and%2520suggests%2520possible%2520directions%2520for%2520future%2520research.%2520The%2520goal%2520of%2520this%250Asurvey%2520is%2520to%2520offer%2520the%2520research%2520community%2520a%2520clear%2520and%2520holistic%2520view%2520of%2520the%250Aadvancements%2520in%2520human%2520video%2520generation%252C%2520highlighting%2520the%2520milestones%2520achieved%250Aand%2520the%2520challenges%2520that%2520lie%2520ahead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20on%20Human%20Video%20Generation%3A%20Challenges%2C%20Methods%2C%0A%20%20and%20Insights&entry.906535625=Wentao%20Lei%20and%20Jinting%20Wang%20and%20Fengji%20Ma%20and%20Guanjie%20Huang%20and%20Li%20Liu&entry.1292438233=%20%20Human%20video%20generation%20is%20a%20dynamic%20and%20rapidly%20evolving%20task%20that%20aims%20to%0Asynthesize%202D%20human%20body%20video%20sequences%20with%20generative%20models%20given%20control%0Aconditions%20such%20as%20text%2C%20audio%2C%20and%20pose.%20With%20the%20potential%20for%20wide-ranging%0Aapplications%20in%20film%2C%20gaming%2C%20and%20virtual%20communication%2C%20the%20ability%20to%0Agenerate%20natural%20and%20realistic%20human%20video%20is%20critical.%20Recent%20advancements%20in%0Agenerative%20models%20have%20laid%20a%20solid%20foundation%20for%20the%20growing%20interest%20in%20this%0Aarea.%20Despite%20the%20significant%20progress%2C%20the%20task%20of%20human%20video%20generation%0Aremains%20challenging%20due%20to%20the%20consistency%20of%20characters%2C%20the%20complexity%20of%0Ahuman%20motion%2C%20and%20difficulties%20in%20their%20relationship%20with%20the%20environment.%20This%0Asurvey%20provides%20a%20comprehensive%20review%20of%20the%20current%20state%20of%20human%20video%0Ageneration%2C%20marking%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20extensive%0Aliterature%20review%20in%20this%20domain.%20We%20start%20with%20an%20introduction%20to%20the%0Afundamentals%20of%20human%20video%20generation%20and%20the%20evolution%20of%20generative%20models%0Athat%20have%20facilitated%20the%20field%27s%20growth.%20We%20then%20examine%20the%20main%20methods%0Aemployed%20for%20three%20key%20sub-tasks%20within%20human%20video%20generation%3A%20text-driven%2C%0Aaudio-driven%2C%20and%20pose-driven%20motion%20generation.%20These%20areas%20are%20explored%0Aconcerning%20the%20conditions%20that%20guide%20the%20generation%20process.%20Furthermore%2C%20we%0Aoffer%20a%20collection%20of%20the%20most%20commonly%20utilized%20datasets%20and%20the%20evaluation%0Ametrics%20that%20are%20crucial%20in%20assessing%20the%20quality%20and%20realism%20of%20generated%0Avideos.%20The%20survey%20concludes%20with%20a%20discussion%20of%20the%20current%20challenges%20in%20the%0Afield%20and%20suggests%20possible%20directions%20for%20future%20research.%20The%20goal%20of%20this%0Asurvey%20is%20to%20offer%20the%20research%20community%20a%20clear%20and%20holistic%20view%20of%20the%0Aadvancements%20in%20human%20video%20generation%2C%20highlighting%20the%20milestones%20achieved%0Aand%20the%20challenges%20that%20lie%20ahead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08428v1&entry.124074799=Read"},
{"title": "OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects", "author": "Akshay Krishnan and Abhijit Kundu and Kevis-Kokitsi Maninis and James Hays and Matthew Brown", "abstract": "  We propose OmniNOCS, a large-scale monocular dataset with 3D Normalized\nObject Coordinate Space (NOCS) maps, object masks, and 3D bounding box\nannotations for indoor and outdoor scenes. OmniNOCS has 20 times more object\nclasses and 200 times more instances than existing NOCS datasets (NOCS-Real275,\nWild6D). We use OmniNOCS to train a novel, transformer-based monocular NOCS\nprediction model (NOCSformer) that can predict accurate NOCS, instance masks\nand poses from 2D object detections across diverse classes. It is the first\nNOCS model that can generalize to a broad range of classes when prompted with\n2D boxes. We evaluate our model on the task of 3D oriented bounding box\nprediction, where it achieves comparable results to state-of-the-art 3D\ndetection methods such as Cube R-CNN. Unlike other 3D detection methods, our\nmodel also provides detailed and accurate 3D object shape and segmentation. We\npropose a novel benchmark for the task of NOCS prediction based on OmniNOCS,\nwhich we hope will serve as a useful baseline for future work in this area. Our\ndataset and code will be at the project website: https://omninocs.github.io.\n", "link": "http://arxiv.org/abs/2407.08711v1", "date": "2024-07-11", "relevancy": 2.8041, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5658}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5658}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniNOCS%3A%20A%20unified%20NOCS%20dataset%20and%20model%20for%203D%20lifting%20of%202D%20objects&body=Title%3A%20OmniNOCS%3A%20A%20unified%20NOCS%20dataset%20and%20model%20for%203D%20lifting%20of%202D%20objects%0AAuthor%3A%20Akshay%20Krishnan%20and%20Abhijit%20Kundu%20and%20Kevis-Kokitsi%20Maninis%20and%20James%20Hays%20and%20Matthew%20Brown%0AAbstract%3A%20%20%20We%20propose%20OmniNOCS%2C%20a%20large-scale%20monocular%20dataset%20with%203D%20Normalized%0AObject%20Coordinate%20Space%20%28NOCS%29%20maps%2C%20object%20masks%2C%20and%203D%20bounding%20box%0Aannotations%20for%20indoor%20and%20outdoor%20scenes.%20OmniNOCS%20has%2020%20times%20more%20object%0Aclasses%20and%20200%20times%20more%20instances%20than%20existing%20NOCS%20datasets%20%28NOCS-Real275%2C%0AWild6D%29.%20We%20use%20OmniNOCS%20to%20train%20a%20novel%2C%20transformer-based%20monocular%20NOCS%0Aprediction%20model%20%28NOCSformer%29%20that%20can%20predict%20accurate%20NOCS%2C%20instance%20masks%0Aand%20poses%20from%202D%20object%20detections%20across%20diverse%20classes.%20It%20is%20the%20first%0ANOCS%20model%20that%20can%20generalize%20to%20a%20broad%20range%20of%20classes%20when%20prompted%20with%0A2D%20boxes.%20We%20evaluate%20our%20model%20on%20the%20task%20of%203D%20oriented%20bounding%20box%0Aprediction%2C%20where%20it%20achieves%20comparable%20results%20to%20state-of-the-art%203D%0Adetection%20methods%20such%20as%20Cube%20R-CNN.%20Unlike%20other%203D%20detection%20methods%2C%20our%0Amodel%20also%20provides%20detailed%20and%20accurate%203D%20object%20shape%20and%20segmentation.%20We%0Apropose%20a%20novel%20benchmark%20for%20the%20task%20of%20NOCS%20prediction%20based%20on%20OmniNOCS%2C%0Awhich%20we%20hope%20will%20serve%20as%20a%20useful%20baseline%20for%20future%20work%20in%20this%20area.%20Our%0Adataset%20and%20code%20will%20be%20at%20the%20project%20website%3A%20https%3A//omninocs.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniNOCS%253A%2520A%2520unified%2520NOCS%2520dataset%2520and%2520model%2520for%25203D%2520lifting%2520of%25202D%2520objects%26entry.906535625%3DAkshay%2520Krishnan%2520and%2520Abhijit%2520Kundu%2520and%2520Kevis-Kokitsi%2520Maninis%2520and%2520James%2520Hays%2520and%2520Matthew%2520Brown%26entry.1292438233%3D%2520%2520We%2520propose%2520OmniNOCS%252C%2520a%2520large-scale%2520monocular%2520dataset%2520with%25203D%2520Normalized%250AObject%2520Coordinate%2520Space%2520%2528NOCS%2529%2520maps%252C%2520object%2520masks%252C%2520and%25203D%2520bounding%2520box%250Aannotations%2520for%2520indoor%2520and%2520outdoor%2520scenes.%2520OmniNOCS%2520has%252020%2520times%2520more%2520object%250Aclasses%2520and%2520200%2520times%2520more%2520instances%2520than%2520existing%2520NOCS%2520datasets%2520%2528NOCS-Real275%252C%250AWild6D%2529.%2520We%2520use%2520OmniNOCS%2520to%2520train%2520a%2520novel%252C%2520transformer-based%2520monocular%2520NOCS%250Aprediction%2520model%2520%2528NOCSformer%2529%2520that%2520can%2520predict%2520accurate%2520NOCS%252C%2520instance%2520masks%250Aand%2520poses%2520from%25202D%2520object%2520detections%2520across%2520diverse%2520classes.%2520It%2520is%2520the%2520first%250ANOCS%2520model%2520that%2520can%2520generalize%2520to%2520a%2520broad%2520range%2520of%2520classes%2520when%2520prompted%2520with%250A2D%2520boxes.%2520We%2520evaluate%2520our%2520model%2520on%2520the%2520task%2520of%25203D%2520oriented%2520bounding%2520box%250Aprediction%252C%2520where%2520it%2520achieves%2520comparable%2520results%2520to%2520state-of-the-art%25203D%250Adetection%2520methods%2520such%2520as%2520Cube%2520R-CNN.%2520Unlike%2520other%25203D%2520detection%2520methods%252C%2520our%250Amodel%2520also%2520provides%2520detailed%2520and%2520accurate%25203D%2520object%2520shape%2520and%2520segmentation.%2520We%250Apropose%2520a%2520novel%2520benchmark%2520for%2520the%2520task%2520of%2520NOCS%2520prediction%2520based%2520on%2520OmniNOCS%252C%250Awhich%2520we%2520hope%2520will%2520serve%2520as%2520a%2520useful%2520baseline%2520for%2520future%2520work%2520in%2520this%2520area.%2520Our%250Adataset%2520and%2520code%2520will%2520be%2520at%2520the%2520project%2520website%253A%2520https%253A//omninocs.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniNOCS%3A%20A%20unified%20NOCS%20dataset%20and%20model%20for%203D%20lifting%20of%202D%20objects&entry.906535625=Akshay%20Krishnan%20and%20Abhijit%20Kundu%20and%20Kevis-Kokitsi%20Maninis%20and%20James%20Hays%20and%20Matthew%20Brown&entry.1292438233=%20%20We%20propose%20OmniNOCS%2C%20a%20large-scale%20monocular%20dataset%20with%203D%20Normalized%0AObject%20Coordinate%20Space%20%28NOCS%29%20maps%2C%20object%20masks%2C%20and%203D%20bounding%20box%0Aannotations%20for%20indoor%20and%20outdoor%20scenes.%20OmniNOCS%20has%2020%20times%20more%20object%0Aclasses%20and%20200%20times%20more%20instances%20than%20existing%20NOCS%20datasets%20%28NOCS-Real275%2C%0AWild6D%29.%20We%20use%20OmniNOCS%20to%20train%20a%20novel%2C%20transformer-based%20monocular%20NOCS%0Aprediction%20model%20%28NOCSformer%29%20that%20can%20predict%20accurate%20NOCS%2C%20instance%20masks%0Aand%20poses%20from%202D%20object%20detections%20across%20diverse%20classes.%20It%20is%20the%20first%0ANOCS%20model%20that%20can%20generalize%20to%20a%20broad%20range%20of%20classes%20when%20prompted%20with%0A2D%20boxes.%20We%20evaluate%20our%20model%20on%20the%20task%20of%203D%20oriented%20bounding%20box%0Aprediction%2C%20where%20it%20achieves%20comparable%20results%20to%20state-of-the-art%203D%0Adetection%20methods%20such%20as%20Cube%20R-CNN.%20Unlike%20other%203D%20detection%20methods%2C%20our%0Amodel%20also%20provides%20detailed%20and%20accurate%203D%20object%20shape%20and%20segmentation.%20We%0Apropose%20a%20novel%20benchmark%20for%20the%20task%20of%20NOCS%20prediction%20based%20on%20OmniNOCS%2C%0Awhich%20we%20hope%20will%20serve%20as%20a%20useful%20baseline%20for%20future%20work%20in%20this%20area.%20Our%0Adataset%20and%20code%20will%20be%20at%20the%20project%20website%3A%20https%3A//omninocs.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08711v1&entry.124074799=Read"},
{"title": "Roadside LiDAR Assisted Cooperative Localization for Connected\n  Autonomous Vehicles", "author": "Yuze Jiang and Ehsan Javanmardi and Jin Nakazato and Manabu Tsukada and Hiroshi Esaki", "abstract": "  Advancements in LiDAR technology have led to more cost-effective production\nwhile simultaneously improving precision and resolution. As a result, LiDAR has\nbecome integral to vehicle localization, achieving centimeter-level accuracy\nthrough techniques like Normal Distributions Transform (NDT) and other advanced\n3D registration algorithms. Nonetheless, these approaches are reliant on\nhigh-definition 3D point cloud maps, the creation of which involves significant\nexpenditure. When such maps are unavailable or lack sufficient features for 3D\nregistration algorithms, localization accuracy diminishes, posing a risk to\nroad safety. To address this, we proposed to use LiDAR-equipped roadside unit\nand Vehicle-to-Infrastructure (V2I) communication to accurately estimate the\nconnected autonomous vehicle's position and help the vehicle when its\nself-localization is not accurate enough. Our simulation results indicate that\nthis method outperforms traditional NDT scan matching-based approaches in terms\nof localization accuracy.\n", "link": "http://arxiv.org/abs/2311.07913v2", "date": "2024-07-11", "relevancy": 2.7952, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6048}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5445}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Roadside%20LiDAR%20Assisted%20Cooperative%20Localization%20for%20Connected%0A%20%20Autonomous%20Vehicles&body=Title%3A%20Roadside%20LiDAR%20Assisted%20Cooperative%20Localization%20for%20Connected%0A%20%20Autonomous%20Vehicles%0AAuthor%3A%20Yuze%20Jiang%20and%20Ehsan%20Javanmardi%20and%20Jin%20Nakazato%20and%20Manabu%20Tsukada%20and%20Hiroshi%20Esaki%0AAbstract%3A%20%20%20Advancements%20in%20LiDAR%20technology%20have%20led%20to%20more%20cost-effective%20production%0Awhile%20simultaneously%20improving%20precision%20and%20resolution.%20As%20a%20result%2C%20LiDAR%20has%0Abecome%20integral%20to%20vehicle%20localization%2C%20achieving%20centimeter-level%20accuracy%0Athrough%20techniques%20like%20Normal%20Distributions%20Transform%20%28NDT%29%20and%20other%20advanced%0A3D%20registration%20algorithms.%20Nonetheless%2C%20these%20approaches%20are%20reliant%20on%0Ahigh-definition%203D%20point%20cloud%20maps%2C%20the%20creation%20of%20which%20involves%20significant%0Aexpenditure.%20When%20such%20maps%20are%20unavailable%20or%20lack%20sufficient%20features%20for%203D%0Aregistration%20algorithms%2C%20localization%20accuracy%20diminishes%2C%20posing%20a%20risk%20to%0Aroad%20safety.%20To%20address%20this%2C%20we%20proposed%20to%20use%20LiDAR-equipped%20roadside%20unit%0Aand%20Vehicle-to-Infrastructure%20%28V2I%29%20communication%20to%20accurately%20estimate%20the%0Aconnected%20autonomous%20vehicle%27s%20position%20and%20help%20the%20vehicle%20when%20its%0Aself-localization%20is%20not%20accurate%20enough.%20Our%20simulation%20results%20indicate%20that%0Athis%20method%20outperforms%20traditional%20NDT%20scan%20matching-based%20approaches%20in%20terms%0Aof%20localization%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoadside%2520LiDAR%2520Assisted%2520Cooperative%2520Localization%2520for%2520Connected%250A%2520%2520Autonomous%2520Vehicles%26entry.906535625%3DYuze%2520Jiang%2520and%2520Ehsan%2520Javanmardi%2520and%2520Jin%2520Nakazato%2520and%2520Manabu%2520Tsukada%2520and%2520Hiroshi%2520Esaki%26entry.1292438233%3D%2520%2520Advancements%2520in%2520LiDAR%2520technology%2520have%2520led%2520to%2520more%2520cost-effective%2520production%250Awhile%2520simultaneously%2520improving%2520precision%2520and%2520resolution.%2520As%2520a%2520result%252C%2520LiDAR%2520has%250Abecome%2520integral%2520to%2520vehicle%2520localization%252C%2520achieving%2520centimeter-level%2520accuracy%250Athrough%2520techniques%2520like%2520Normal%2520Distributions%2520Transform%2520%2528NDT%2529%2520and%2520other%2520advanced%250A3D%2520registration%2520algorithms.%2520Nonetheless%252C%2520these%2520approaches%2520are%2520reliant%2520on%250Ahigh-definition%25203D%2520point%2520cloud%2520maps%252C%2520the%2520creation%2520of%2520which%2520involves%2520significant%250Aexpenditure.%2520When%2520such%2520maps%2520are%2520unavailable%2520or%2520lack%2520sufficient%2520features%2520for%25203D%250Aregistration%2520algorithms%252C%2520localization%2520accuracy%2520diminishes%252C%2520posing%2520a%2520risk%2520to%250Aroad%2520safety.%2520To%2520address%2520this%252C%2520we%2520proposed%2520to%2520use%2520LiDAR-equipped%2520roadside%2520unit%250Aand%2520Vehicle-to-Infrastructure%2520%2528V2I%2529%2520communication%2520to%2520accurately%2520estimate%2520the%250Aconnected%2520autonomous%2520vehicle%2527s%2520position%2520and%2520help%2520the%2520vehicle%2520when%2520its%250Aself-localization%2520is%2520not%2520accurate%2520enough.%2520Our%2520simulation%2520results%2520indicate%2520that%250Athis%2520method%2520outperforms%2520traditional%2520NDT%2520scan%2520matching-based%2520approaches%2520in%2520terms%250Aof%2520localization%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Roadside%20LiDAR%20Assisted%20Cooperative%20Localization%20for%20Connected%0A%20%20Autonomous%20Vehicles&entry.906535625=Yuze%20Jiang%20and%20Ehsan%20Javanmardi%20and%20Jin%20Nakazato%20and%20Manabu%20Tsukada%20and%20Hiroshi%20Esaki&entry.1292438233=%20%20Advancements%20in%20LiDAR%20technology%20have%20led%20to%20more%20cost-effective%20production%0Awhile%20simultaneously%20improving%20precision%20and%20resolution.%20As%20a%20result%2C%20LiDAR%20has%0Abecome%20integral%20to%20vehicle%20localization%2C%20achieving%20centimeter-level%20accuracy%0Athrough%20techniques%20like%20Normal%20Distributions%20Transform%20%28NDT%29%20and%20other%20advanced%0A3D%20registration%20algorithms.%20Nonetheless%2C%20these%20approaches%20are%20reliant%20on%0Ahigh-definition%203D%20point%20cloud%20maps%2C%20the%20creation%20of%20which%20involves%20significant%0Aexpenditure.%20When%20such%20maps%20are%20unavailable%20or%20lack%20sufficient%20features%20for%203D%0Aregistration%20algorithms%2C%20localization%20accuracy%20diminishes%2C%20posing%20a%20risk%20to%0Aroad%20safety.%20To%20address%20this%2C%20we%20proposed%20to%20use%20LiDAR-equipped%20roadside%20unit%0Aand%20Vehicle-to-Infrastructure%20%28V2I%29%20communication%20to%20accurately%20estimate%20the%0Aconnected%20autonomous%20vehicle%27s%20position%20and%20help%20the%20vehicle%20when%20its%0Aself-localization%20is%20not%20accurate%20enough.%20Our%20simulation%20results%20indicate%20that%0Athis%20method%20outperforms%20traditional%20NDT%20scan%20matching-based%20approaches%20in%20terms%0Aof%20localization%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07913v2&entry.124074799=Read"},
{"title": "BiEquiFormer: Bi-Equivariant Representations for Global Point Cloud\n  Registration", "author": "Stefanos Pertigkiozoglou and Evangelos Chatzipantazis and Kostas Daniilidis", "abstract": "  The goal of this paper is to address the problem of \\textit{global} point\ncloud registration (PCR) i.e., finding the optimal alignment between point\nclouds irrespective of the initial poses of the scans. This problem is\nnotoriously challenging for classical optimization methods due to computational\nconstraints. First, we show that state-of-the-art deep learning methods suffer\nfrom huge performance degradation when the point clouds are arbitrarily placed\nin space. We propose that \\textit{equivariant deep learning} should be utilized\nfor solving this task and we characterize the specific type of bi-equivariance\nof PCR. Then, we design BiEquiformer a novel and scalable\n\\textit{bi-equivariant} pipeline i.e. equivariant to the independent\ntransformations of the input point clouds. While a naive approach would process\nthe point clouds independently we design expressive bi-equivariant layers that\nfuse the information from both point clouds. This allows us to extract\nhigh-quality superpoint correspondences and in turn, robust point-cloud\nregistration. Extensive comparisons against state-of-the-art methods show that\nour method achieves comparable performance in the canonical setting and\nsuperior performance in the robust setting in both the 3DMatch and the\nchallenging low-overlap 3DLoMatch dataset.\n", "link": "http://arxiv.org/abs/2407.08729v1", "date": "2024-07-11", "relevancy": 2.7901, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5938}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5457}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiEquiFormer%3A%20Bi-Equivariant%20Representations%20for%20Global%20Point%20Cloud%0A%20%20Registration&body=Title%3A%20BiEquiFormer%3A%20Bi-Equivariant%20Representations%20for%20Global%20Point%20Cloud%0A%20%20Registration%0AAuthor%3A%20Stefanos%20Pertigkiozoglou%20and%20Evangelos%20Chatzipantazis%20and%20Kostas%20Daniilidis%0AAbstract%3A%20%20%20The%20goal%20of%20this%20paper%20is%20to%20address%20the%20problem%20of%20%5Ctextit%7Bglobal%7D%20point%0Acloud%20registration%20%28PCR%29%20i.e.%2C%20finding%20the%20optimal%20alignment%20between%20point%0Aclouds%20irrespective%20of%20the%20initial%20poses%20of%20the%20scans.%20This%20problem%20is%0Anotoriously%20challenging%20for%20classical%20optimization%20methods%20due%20to%20computational%0Aconstraints.%20First%2C%20we%20show%20that%20state-of-the-art%20deep%20learning%20methods%20suffer%0Afrom%20huge%20performance%20degradation%20when%20the%20point%20clouds%20are%20arbitrarily%20placed%0Ain%20space.%20We%20propose%20that%20%5Ctextit%7Bequivariant%20deep%20learning%7D%20should%20be%20utilized%0Afor%20solving%20this%20task%20and%20we%20characterize%20the%20specific%20type%20of%20bi-equivariance%0Aof%20PCR.%20Then%2C%20we%20design%20BiEquiformer%20a%20novel%20and%20scalable%0A%5Ctextit%7Bbi-equivariant%7D%20pipeline%20i.e.%20equivariant%20to%20the%20independent%0Atransformations%20of%20the%20input%20point%20clouds.%20While%20a%20naive%20approach%20would%20process%0Athe%20point%20clouds%20independently%20we%20design%20expressive%20bi-equivariant%20layers%20that%0Afuse%20the%20information%20from%20both%20point%20clouds.%20This%20allows%20us%20to%20extract%0Ahigh-quality%20superpoint%20correspondences%20and%20in%20turn%2C%20robust%20point-cloud%0Aregistration.%20Extensive%20comparisons%20against%20state-of-the-art%20methods%20show%20that%0Aour%20method%20achieves%20comparable%20performance%20in%20the%20canonical%20setting%20and%0Asuperior%20performance%20in%20the%20robust%20setting%20in%20both%20the%203DMatch%20and%20the%0Achallenging%20low-overlap%203DLoMatch%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiEquiFormer%253A%2520Bi-Equivariant%2520Representations%2520for%2520Global%2520Point%2520Cloud%250A%2520%2520Registration%26entry.906535625%3DStefanos%2520Pertigkiozoglou%2520and%2520Evangelos%2520Chatzipantazis%2520and%2520Kostas%2520Daniilidis%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520this%2520paper%2520is%2520to%2520address%2520the%2520problem%2520of%2520%255Ctextit%257Bglobal%257D%2520point%250Acloud%2520registration%2520%2528PCR%2529%2520i.e.%252C%2520finding%2520the%2520optimal%2520alignment%2520between%2520point%250Aclouds%2520irrespective%2520of%2520the%2520initial%2520poses%2520of%2520the%2520scans.%2520This%2520problem%2520is%250Anotoriously%2520challenging%2520for%2520classical%2520optimization%2520methods%2520due%2520to%2520computational%250Aconstraints.%2520First%252C%2520we%2520show%2520that%2520state-of-the-art%2520deep%2520learning%2520methods%2520suffer%250Afrom%2520huge%2520performance%2520degradation%2520when%2520the%2520point%2520clouds%2520are%2520arbitrarily%2520placed%250Ain%2520space.%2520We%2520propose%2520that%2520%255Ctextit%257Bequivariant%2520deep%2520learning%257D%2520should%2520be%2520utilized%250Afor%2520solving%2520this%2520task%2520and%2520we%2520characterize%2520the%2520specific%2520type%2520of%2520bi-equivariance%250Aof%2520PCR.%2520Then%252C%2520we%2520design%2520BiEquiformer%2520a%2520novel%2520and%2520scalable%250A%255Ctextit%257Bbi-equivariant%257D%2520pipeline%2520i.e.%2520equivariant%2520to%2520the%2520independent%250Atransformations%2520of%2520the%2520input%2520point%2520clouds.%2520While%2520a%2520naive%2520approach%2520would%2520process%250Athe%2520point%2520clouds%2520independently%2520we%2520design%2520expressive%2520bi-equivariant%2520layers%2520that%250Afuse%2520the%2520information%2520from%2520both%2520point%2520clouds.%2520This%2520allows%2520us%2520to%2520extract%250Ahigh-quality%2520superpoint%2520correspondences%2520and%2520in%2520turn%252C%2520robust%2520point-cloud%250Aregistration.%2520Extensive%2520comparisons%2520against%2520state-of-the-art%2520methods%2520show%2520that%250Aour%2520method%2520achieves%2520comparable%2520performance%2520in%2520the%2520canonical%2520setting%2520and%250Asuperior%2520performance%2520in%2520the%2520robust%2520setting%2520in%2520both%2520the%25203DMatch%2520and%2520the%250Achallenging%2520low-overlap%25203DLoMatch%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiEquiFormer%3A%20Bi-Equivariant%20Representations%20for%20Global%20Point%20Cloud%0A%20%20Registration&entry.906535625=Stefanos%20Pertigkiozoglou%20and%20Evangelos%20Chatzipantazis%20and%20Kostas%20Daniilidis&entry.1292438233=%20%20The%20goal%20of%20this%20paper%20is%20to%20address%20the%20problem%20of%20%5Ctextit%7Bglobal%7D%20point%0Acloud%20registration%20%28PCR%29%20i.e.%2C%20finding%20the%20optimal%20alignment%20between%20point%0Aclouds%20irrespective%20of%20the%20initial%20poses%20of%20the%20scans.%20This%20problem%20is%0Anotoriously%20challenging%20for%20classical%20optimization%20methods%20due%20to%20computational%0Aconstraints.%20First%2C%20we%20show%20that%20state-of-the-art%20deep%20learning%20methods%20suffer%0Afrom%20huge%20performance%20degradation%20when%20the%20point%20clouds%20are%20arbitrarily%20placed%0Ain%20space.%20We%20propose%20that%20%5Ctextit%7Bequivariant%20deep%20learning%7D%20should%20be%20utilized%0Afor%20solving%20this%20task%20and%20we%20characterize%20the%20specific%20type%20of%20bi-equivariance%0Aof%20PCR.%20Then%2C%20we%20design%20BiEquiformer%20a%20novel%20and%20scalable%0A%5Ctextit%7Bbi-equivariant%7D%20pipeline%20i.e.%20equivariant%20to%20the%20independent%0Atransformations%20of%20the%20input%20point%20clouds.%20While%20a%20naive%20approach%20would%20process%0Athe%20point%20clouds%20independently%20we%20design%20expressive%20bi-equivariant%20layers%20that%0Afuse%20the%20information%20from%20both%20point%20clouds.%20This%20allows%20us%20to%20extract%0Ahigh-quality%20superpoint%20correspondences%20and%20in%20turn%2C%20robust%20point-cloud%0Aregistration.%20Extensive%20comparisons%20against%20state-of-the-art%20methods%20show%20that%0Aour%20method%20achieves%20comparable%20performance%20in%20the%20canonical%20setting%20and%0Asuperior%20performance%20in%20the%20robust%20setting%20in%20both%20the%203DMatch%20and%20the%0Achallenging%20low-overlap%203DLoMatch%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08729v1&entry.124074799=Read"},
{"title": "Paving the way toward foundation models for irregular and unaligned\n  Satellite Image Time Series", "author": "Iris Dumeur and Silvia Valero and Jordi Inglada", "abstract": "  Although recently several foundation models for satellite remote sensing\nimagery have been proposed, they fail to address major challenges of\nreal/operational applications. Indeed, embeddings that don't take into account\nthe spectral, spatial and temporal dimensions of the data as well as the\nirregular or unaligned temporal sampling are of little use for most real world\nuses.As a consequence, we propose an ALIgned Sits Encoder (ALISE), a novel\napproach that leverages the spatial, spectral, and temporal dimensions of\nirregular and unaligned SITS while producing aligned latent representations.\nUnlike SSL models currently available for SITS, ALISE incorporates a flexible\nquery mechanism to project the SITS into a common and learned temporal\nprojection space. Additionally, thanks to a multi-view framework, we explore\nintegration of instance discrimination along a masked autoencoding task to\nSITS. The quality of the produced representation is assessed through three\ndownstream tasks: crop segmentation (PASTIS), land cover segmentation\n(MultiSenGE), and a novel crop change detection dataset. Furthermore, the\nchange detection task is performed without supervision. The results suggest\nthat the use of aligned representations is more effective than previous SSL\nmethods for linear probing segmentation tasks.\n", "link": "http://arxiv.org/abs/2407.08448v1", "date": "2024-07-11", "relevancy": 2.7333, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paving%20the%20way%20toward%20foundation%20models%20for%20irregular%20and%20unaligned%0A%20%20Satellite%20Image%20Time%20Series&body=Title%3A%20Paving%20the%20way%20toward%20foundation%20models%20for%20irregular%20and%20unaligned%0A%20%20Satellite%20Image%20Time%20Series%0AAuthor%3A%20Iris%20Dumeur%20and%20Silvia%20Valero%20and%20Jordi%20Inglada%0AAbstract%3A%20%20%20Although%20recently%20several%20foundation%20models%20for%20satellite%20remote%20sensing%0Aimagery%20have%20been%20proposed%2C%20they%20fail%20to%20address%20major%20challenges%20of%0Areal/operational%20applications.%20Indeed%2C%20embeddings%20that%20don%27t%20take%20into%20account%0Athe%20spectral%2C%20spatial%20and%20temporal%20dimensions%20of%20the%20data%20as%20well%20as%20the%0Airregular%20or%20unaligned%20temporal%20sampling%20are%20of%20little%20use%20for%20most%20real%20world%0Auses.As%20a%20consequence%2C%20we%20propose%20an%20ALIgned%20Sits%20Encoder%20%28ALISE%29%2C%20a%20novel%0Aapproach%20that%20leverages%20the%20spatial%2C%20spectral%2C%20and%20temporal%20dimensions%20of%0Airregular%20and%20unaligned%20SITS%20while%20producing%20aligned%20latent%20representations.%0AUnlike%20SSL%20models%20currently%20available%20for%20SITS%2C%20ALISE%20incorporates%20a%20flexible%0Aquery%20mechanism%20to%20project%20the%20SITS%20into%20a%20common%20and%20learned%20temporal%0Aprojection%20space.%20Additionally%2C%20thanks%20to%20a%20multi-view%20framework%2C%20we%20explore%0Aintegration%20of%20instance%20discrimination%20along%20a%20masked%20autoencoding%20task%20to%0ASITS.%20The%20quality%20of%20the%20produced%20representation%20is%20assessed%20through%20three%0Adownstream%20tasks%3A%20crop%20segmentation%20%28PASTIS%29%2C%20land%20cover%20segmentation%0A%28MultiSenGE%29%2C%20and%20a%20novel%20crop%20change%20detection%20dataset.%20Furthermore%2C%20the%0Achange%20detection%20task%20is%20performed%20without%20supervision.%20The%20results%20suggest%0Athat%20the%20use%20of%20aligned%20representations%20is%20more%20effective%20than%20previous%20SSL%0Amethods%20for%20linear%20probing%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaving%2520the%2520way%2520toward%2520foundation%2520models%2520for%2520irregular%2520and%2520unaligned%250A%2520%2520Satellite%2520Image%2520Time%2520Series%26entry.906535625%3DIris%2520Dumeur%2520and%2520Silvia%2520Valero%2520and%2520Jordi%2520Inglada%26entry.1292438233%3D%2520%2520Although%2520recently%2520several%2520foundation%2520models%2520for%2520satellite%2520remote%2520sensing%250Aimagery%2520have%2520been%2520proposed%252C%2520they%2520fail%2520to%2520address%2520major%2520challenges%2520of%250Areal/operational%2520applications.%2520Indeed%252C%2520embeddings%2520that%2520don%2527t%2520take%2520into%2520account%250Athe%2520spectral%252C%2520spatial%2520and%2520temporal%2520dimensions%2520of%2520the%2520data%2520as%2520well%2520as%2520the%250Airregular%2520or%2520unaligned%2520temporal%2520sampling%2520are%2520of%2520little%2520use%2520for%2520most%2520real%2520world%250Auses.As%2520a%2520consequence%252C%2520we%2520propose%2520an%2520ALIgned%2520Sits%2520Encoder%2520%2528ALISE%2529%252C%2520a%2520novel%250Aapproach%2520that%2520leverages%2520the%2520spatial%252C%2520spectral%252C%2520and%2520temporal%2520dimensions%2520of%250Airregular%2520and%2520unaligned%2520SITS%2520while%2520producing%2520aligned%2520latent%2520representations.%250AUnlike%2520SSL%2520models%2520currently%2520available%2520for%2520SITS%252C%2520ALISE%2520incorporates%2520a%2520flexible%250Aquery%2520mechanism%2520to%2520project%2520the%2520SITS%2520into%2520a%2520common%2520and%2520learned%2520temporal%250Aprojection%2520space.%2520Additionally%252C%2520thanks%2520to%2520a%2520multi-view%2520framework%252C%2520we%2520explore%250Aintegration%2520of%2520instance%2520discrimination%2520along%2520a%2520masked%2520autoencoding%2520task%2520to%250ASITS.%2520The%2520quality%2520of%2520the%2520produced%2520representation%2520is%2520assessed%2520through%2520three%250Adownstream%2520tasks%253A%2520crop%2520segmentation%2520%2528PASTIS%2529%252C%2520land%2520cover%2520segmentation%250A%2528MultiSenGE%2529%252C%2520and%2520a%2520novel%2520crop%2520change%2520detection%2520dataset.%2520Furthermore%252C%2520the%250Achange%2520detection%2520task%2520is%2520performed%2520without%2520supervision.%2520The%2520results%2520suggest%250Athat%2520the%2520use%2520of%2520aligned%2520representations%2520is%2520more%2520effective%2520than%2520previous%2520SSL%250Amethods%2520for%2520linear%2520probing%2520segmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paving%20the%20way%20toward%20foundation%20models%20for%20irregular%20and%20unaligned%0A%20%20Satellite%20Image%20Time%20Series&entry.906535625=Iris%20Dumeur%20and%20Silvia%20Valero%20and%20Jordi%20Inglada&entry.1292438233=%20%20Although%20recently%20several%20foundation%20models%20for%20satellite%20remote%20sensing%0Aimagery%20have%20been%20proposed%2C%20they%20fail%20to%20address%20major%20challenges%20of%0Areal/operational%20applications.%20Indeed%2C%20embeddings%20that%20don%27t%20take%20into%20account%0Athe%20spectral%2C%20spatial%20and%20temporal%20dimensions%20of%20the%20data%20as%20well%20as%20the%0Airregular%20or%20unaligned%20temporal%20sampling%20are%20of%20little%20use%20for%20most%20real%20world%0Auses.As%20a%20consequence%2C%20we%20propose%20an%20ALIgned%20Sits%20Encoder%20%28ALISE%29%2C%20a%20novel%0Aapproach%20that%20leverages%20the%20spatial%2C%20spectral%2C%20and%20temporal%20dimensions%20of%0Airregular%20and%20unaligned%20SITS%20while%20producing%20aligned%20latent%20representations.%0AUnlike%20SSL%20models%20currently%20available%20for%20SITS%2C%20ALISE%20incorporates%20a%20flexible%0Aquery%20mechanism%20to%20project%20the%20SITS%20into%20a%20common%20and%20learned%20temporal%0Aprojection%20space.%20Additionally%2C%20thanks%20to%20a%20multi-view%20framework%2C%20we%20explore%0Aintegration%20of%20instance%20discrimination%20along%20a%20masked%20autoencoding%20task%20to%0ASITS.%20The%20quality%20of%20the%20produced%20representation%20is%20assessed%20through%20three%0Adownstream%20tasks%3A%20crop%20segmentation%20%28PASTIS%29%2C%20land%20cover%20segmentation%0A%28MultiSenGE%29%2C%20and%20a%20novel%20crop%20change%20detection%20dataset.%20Furthermore%2C%20the%0Achange%20detection%20task%20is%20performed%20without%20supervision.%20The%20results%20suggest%0Athat%20the%20use%20of%20aligned%20representations%20is%20more%20effective%20than%20previous%20SSL%0Amethods%20for%20linear%20probing%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08448v1&entry.124074799=Read"},
{"title": "DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single\n  Video", "author": "Narek Tumanyan and Assaf Singer and Shai Bagon and Tali Dekel", "abstract": "  We present DINO-Tracker -- a new framework for long-term dense tracking in\nvideo. The pillar of our approach is combining test-time training on a single\nvideo, with the powerful localized semantic features learned by a pre-trained\nDINO-ViT model. Specifically, our framework simultaneously adopts DINO's\nfeatures to fit to the motion observations of the test video, while training a\ntracker that directly leverages the refined features. The entire framework is\ntrained end-to-end using a combination of self-supervised losses, and\nregularization that allows us to retain and benefit from DINO's semantic prior.\nExtensive evaluation demonstrates that our method achieves state-of-the-art\nresults on known benchmarks. DINO-tracker significantly outperforms\nself-supervised methods and is competitive with state-of-the-art supervised\ntrackers, while outperforming them in challenging cases of tracking under\nlong-term occlusions.\n", "link": "http://arxiv.org/abs/2403.14548v2", "date": "2024-07-11", "relevancy": 2.7157, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5781}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5453}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-Tracker%3A%20Taming%20DINO%20for%20Self-Supervised%20Point%20Tracking%20in%20a%20Single%0A%20%20Video&body=Title%3A%20DINO-Tracker%3A%20Taming%20DINO%20for%20Self-Supervised%20Point%20Tracking%20in%20a%20Single%0A%20%20Video%0AAuthor%3A%20Narek%20Tumanyan%20and%20Assaf%20Singer%20and%20Shai%20Bagon%20and%20Tali%20Dekel%0AAbstract%3A%20%20%20We%20present%20DINO-Tracker%20--%20a%20new%20framework%20for%20long-term%20dense%20tracking%20in%0Avideo.%20The%20pillar%20of%20our%20approach%20is%20combining%20test-time%20training%20on%20a%20single%0Avideo%2C%20with%20the%20powerful%20localized%20semantic%20features%20learned%20by%20a%20pre-trained%0ADINO-ViT%20model.%20Specifically%2C%20our%20framework%20simultaneously%20adopts%20DINO%27s%0Afeatures%20to%20fit%20to%20the%20motion%20observations%20of%20the%20test%20video%2C%20while%20training%20a%0Atracker%20that%20directly%20leverages%20the%20refined%20features.%20The%20entire%20framework%20is%0Atrained%20end-to-end%20using%20a%20combination%20of%20self-supervised%20losses%2C%20and%0Aregularization%20that%20allows%20us%20to%20retain%20and%20benefit%20from%20DINO%27s%20semantic%20prior.%0AExtensive%20evaluation%20demonstrates%20that%20our%20method%20achieves%20state-of-the-art%0Aresults%20on%20known%20benchmarks.%20DINO-tracker%20significantly%20outperforms%0Aself-supervised%20methods%20and%20is%20competitive%20with%20state-of-the-art%20supervised%0Atrackers%2C%20while%20outperforming%20them%20in%20challenging%20cases%20of%20tracking%20under%0Along-term%20occlusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14548v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-Tracker%253A%2520Taming%2520DINO%2520for%2520Self-Supervised%2520Point%2520Tracking%2520in%2520a%2520Single%250A%2520%2520Video%26entry.906535625%3DNarek%2520Tumanyan%2520and%2520Assaf%2520Singer%2520and%2520Shai%2520Bagon%2520and%2520Tali%2520Dekel%26entry.1292438233%3D%2520%2520We%2520present%2520DINO-Tracker%2520--%2520a%2520new%2520framework%2520for%2520long-term%2520dense%2520tracking%2520in%250Avideo.%2520The%2520pillar%2520of%2520our%2520approach%2520is%2520combining%2520test-time%2520training%2520on%2520a%2520single%250Avideo%252C%2520with%2520the%2520powerful%2520localized%2520semantic%2520features%2520learned%2520by%2520a%2520pre-trained%250ADINO-ViT%2520model.%2520Specifically%252C%2520our%2520framework%2520simultaneously%2520adopts%2520DINO%2527s%250Afeatures%2520to%2520fit%2520to%2520the%2520motion%2520observations%2520of%2520the%2520test%2520video%252C%2520while%2520training%2520a%250Atracker%2520that%2520directly%2520leverages%2520the%2520refined%2520features.%2520The%2520entire%2520framework%2520is%250Atrained%2520end-to-end%2520using%2520a%2520combination%2520of%2520self-supervised%2520losses%252C%2520and%250Aregularization%2520that%2520allows%2520us%2520to%2520retain%2520and%2520benefit%2520from%2520DINO%2527s%2520semantic%2520prior.%250AExtensive%2520evaluation%2520demonstrates%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aresults%2520on%2520known%2520benchmarks.%2520DINO-tracker%2520significantly%2520outperforms%250Aself-supervised%2520methods%2520and%2520is%2520competitive%2520with%2520state-of-the-art%2520supervised%250Atrackers%252C%2520while%2520outperforming%2520them%2520in%2520challenging%2520cases%2520of%2520tracking%2520under%250Along-term%2520occlusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14548v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-Tracker%3A%20Taming%20DINO%20for%20Self-Supervised%20Point%20Tracking%20in%20a%20Single%0A%20%20Video&entry.906535625=Narek%20Tumanyan%20and%20Assaf%20Singer%20and%20Shai%20Bagon%20and%20Tali%20Dekel&entry.1292438233=%20%20We%20present%20DINO-Tracker%20--%20a%20new%20framework%20for%20long-term%20dense%20tracking%20in%0Avideo.%20The%20pillar%20of%20our%20approach%20is%20combining%20test-time%20training%20on%20a%20single%0Avideo%2C%20with%20the%20powerful%20localized%20semantic%20features%20learned%20by%20a%20pre-trained%0ADINO-ViT%20model.%20Specifically%2C%20our%20framework%20simultaneously%20adopts%20DINO%27s%0Afeatures%20to%20fit%20to%20the%20motion%20observations%20of%20the%20test%20video%2C%20while%20training%20a%0Atracker%20that%20directly%20leverages%20the%20refined%20features.%20The%20entire%20framework%20is%0Atrained%20end-to-end%20using%20a%20combination%20of%20self-supervised%20losses%2C%20and%0Aregularization%20that%20allows%20us%20to%20retain%20and%20benefit%20from%20DINO%27s%20semantic%20prior.%0AExtensive%20evaluation%20demonstrates%20that%20our%20method%20achieves%20state-of-the-art%0Aresults%20on%20known%20benchmarks.%20DINO-tracker%20significantly%20outperforms%0Aself-supervised%20methods%20and%20is%20competitive%20with%20state-of-the-art%20supervised%0Atrackers%2C%20while%20outperforming%20them%20in%20challenging%20cases%20of%20tracking%20under%0Along-term%20occlusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14548v2&entry.124074799=Read"},
{"title": "DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in\n  Multivariate Time Series", "author": "Zahra Zamanzadeh Darban and Yiyuan Yang and Geoffrey I. Webb and Charu C. Aggarwal and Qingsong Wen and Mahsa Salehi", "abstract": "  In time series anomaly detection (TSAD), the scarcity of labeled data poses a\nchallenge to the development of accurate models. Unsupervised domain adaptation\n(UDA) offers a solution by leveraging labeled data from a related domain to\ndetect anomalies in an unlabeled target domain. However, existing UDA methods\nassume consistent anomalous classes across domains. To address this limitation,\nwe propose a novel Domain Adaptation Contrastive learning model for Anomaly\nDetection in multivariate time series (DACAD), combining UDA with contrastive\nlearning. DACAD utilizes an anomaly injection mechanism that enhances\ngeneralization across unseen anomalous classes, improving adaptability and\nrobustness. Additionally, our model employs supervised contrastive loss for the\nsource domain and self-supervised contrastive triplet loss for the target\ndomain, ensuring comprehensive feature representation learning and\ndomain-invariant feature extraction. Finally, an effective Centre-based Entropy\nClassifier (CEC) accurately learns normal boundaries in the source domain.\nExtensive evaluations on multiple real-world datasets and a synthetic dataset\nhighlight DACAD's superior performance in transferring knowledge across domains\nand mitigating the challenge of limited labeled data in TSAD.\n", "link": "http://arxiv.org/abs/2404.11269v2", "date": "2024-07-11", "relevancy": 2.7028, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5527}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series&body=Title%3A%20DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series%0AAuthor%3A%20Zahra%20Zamanzadeh%20Darban%20and%20Yiyuan%20Yang%20and%20Geoffrey%20I.%20Webb%20and%20Charu%20C.%20Aggarwal%20and%20Qingsong%20Wen%20and%20Mahsa%20Salehi%0AAbstract%3A%20%20%20In%20time%20series%20anomaly%20detection%20%28TSAD%29%2C%20the%20scarcity%20of%20labeled%20data%20poses%20a%0Achallenge%20to%20the%20development%20of%20accurate%20models.%20Unsupervised%20domain%20adaptation%0A%28UDA%29%20offers%20a%20solution%20by%20leveraging%20labeled%20data%20from%20a%20related%20domain%20to%0Adetect%20anomalies%20in%20an%20unlabeled%20target%20domain.%20However%2C%20existing%20UDA%20methods%0Aassume%20consistent%20anomalous%20classes%20across%20domains.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20novel%20Domain%20Adaptation%20Contrastive%20learning%20model%20for%20Anomaly%0ADetection%20in%20multivariate%20time%20series%20%28DACAD%29%2C%20combining%20UDA%20with%20contrastive%0Alearning.%20DACAD%20utilizes%20an%20anomaly%20injection%20mechanism%20that%20enhances%0Ageneralization%20across%20unseen%20anomalous%20classes%2C%20improving%20adaptability%20and%0Arobustness.%20Additionally%2C%20our%20model%20employs%20supervised%20contrastive%20loss%20for%20the%0Asource%20domain%20and%20self-supervised%20contrastive%20triplet%20loss%20for%20the%20target%0Adomain%2C%20ensuring%20comprehensive%20feature%20representation%20learning%20and%0Adomain-invariant%20feature%20extraction.%20Finally%2C%20an%20effective%20Centre-based%20Entropy%0AClassifier%20%28CEC%29%20accurately%20learns%20normal%20boundaries%20in%20the%20source%20domain.%0AExtensive%20evaluations%20on%20multiple%20real-world%20datasets%20and%20a%20synthetic%20dataset%0Ahighlight%20DACAD%27s%20superior%20performance%20in%20transferring%20knowledge%20across%20domains%0Aand%20mitigating%20the%20challenge%20of%20limited%20labeled%20data%20in%20TSAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDACAD%253A%2520Domain%2520Adaptation%2520Contrastive%2520Learning%2520for%2520Anomaly%2520Detection%2520in%250A%2520%2520Multivariate%2520Time%2520Series%26entry.906535625%3DZahra%2520Zamanzadeh%2520Darban%2520and%2520Yiyuan%2520Yang%2520and%2520Geoffrey%2520I.%2520Webb%2520and%2520Charu%2520C.%2520Aggarwal%2520and%2520Qingsong%2520Wen%2520and%2520Mahsa%2520Salehi%26entry.1292438233%3D%2520%2520In%2520time%2520series%2520anomaly%2520detection%2520%2528TSAD%2529%252C%2520the%2520scarcity%2520of%2520labeled%2520data%2520poses%2520a%250Achallenge%2520to%2520the%2520development%2520of%2520accurate%2520models.%2520Unsupervised%2520domain%2520adaptation%250A%2528UDA%2529%2520offers%2520a%2520solution%2520by%2520leveraging%2520labeled%2520data%2520from%2520a%2520related%2520domain%2520to%250Adetect%2520anomalies%2520in%2520an%2520unlabeled%2520target%2520domain.%2520However%252C%2520existing%2520UDA%2520methods%250Aassume%2520consistent%2520anomalous%2520classes%2520across%2520domains.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520a%2520novel%2520Domain%2520Adaptation%2520Contrastive%2520learning%2520model%2520for%2520Anomaly%250ADetection%2520in%2520multivariate%2520time%2520series%2520%2528DACAD%2529%252C%2520combining%2520UDA%2520with%2520contrastive%250Alearning.%2520DACAD%2520utilizes%2520an%2520anomaly%2520injection%2520mechanism%2520that%2520enhances%250Ageneralization%2520across%2520unseen%2520anomalous%2520classes%252C%2520improving%2520adaptability%2520and%250Arobustness.%2520Additionally%252C%2520our%2520model%2520employs%2520supervised%2520contrastive%2520loss%2520for%2520the%250Asource%2520domain%2520and%2520self-supervised%2520contrastive%2520triplet%2520loss%2520for%2520the%2520target%250Adomain%252C%2520ensuring%2520comprehensive%2520feature%2520representation%2520learning%2520and%250Adomain-invariant%2520feature%2520extraction.%2520Finally%252C%2520an%2520effective%2520Centre-based%2520Entropy%250AClassifier%2520%2528CEC%2529%2520accurately%2520learns%2520normal%2520boundaries%2520in%2520the%2520source%2520domain.%250AExtensive%2520evaluations%2520on%2520multiple%2520real-world%2520datasets%2520and%2520a%2520synthetic%2520dataset%250Ahighlight%2520DACAD%2527s%2520superior%2520performance%2520in%2520transferring%2520knowledge%2520across%2520domains%250Aand%2520mitigating%2520the%2520challenge%2520of%2520limited%2520labeled%2520data%2520in%2520TSAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series&entry.906535625=Zahra%20Zamanzadeh%20Darban%20and%20Yiyuan%20Yang%20and%20Geoffrey%20I.%20Webb%20and%20Charu%20C.%20Aggarwal%20and%20Qingsong%20Wen%20and%20Mahsa%20Salehi&entry.1292438233=%20%20In%20time%20series%20anomaly%20detection%20%28TSAD%29%2C%20the%20scarcity%20of%20labeled%20data%20poses%20a%0Achallenge%20to%20the%20development%20of%20accurate%20models.%20Unsupervised%20domain%20adaptation%0A%28UDA%29%20offers%20a%20solution%20by%20leveraging%20labeled%20data%20from%20a%20related%20domain%20to%0Adetect%20anomalies%20in%20an%20unlabeled%20target%20domain.%20However%2C%20existing%20UDA%20methods%0Aassume%20consistent%20anomalous%20classes%20across%20domains.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20novel%20Domain%20Adaptation%20Contrastive%20learning%20model%20for%20Anomaly%0ADetection%20in%20multivariate%20time%20series%20%28DACAD%29%2C%20combining%20UDA%20with%20contrastive%0Alearning.%20DACAD%20utilizes%20an%20anomaly%20injection%20mechanism%20that%20enhances%0Ageneralization%20across%20unseen%20anomalous%20classes%2C%20improving%20adaptability%20and%0Arobustness.%20Additionally%2C%20our%20model%20employs%20supervised%20contrastive%20loss%20for%20the%0Asource%20domain%20and%20self-supervised%20contrastive%20triplet%20loss%20for%20the%20target%0Adomain%2C%20ensuring%20comprehensive%20feature%20representation%20learning%20and%0Adomain-invariant%20feature%20extraction.%20Finally%2C%20an%20effective%20Centre-based%20Entropy%0AClassifier%20%28CEC%29%20accurately%20learns%20normal%20boundaries%20in%20the%20source%20domain.%0AExtensive%20evaluations%20on%20multiple%20real-world%20datasets%20and%20a%20synthetic%20dataset%0Ahighlight%20DACAD%27s%20superior%20performance%20in%20transferring%20knowledge%20across%20domains%0Aand%20mitigating%20the%20challenge%20of%20limited%20labeled%20data%20in%20TSAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11269v2&entry.124074799=Read"},
{"title": "SLEDGE: Synthesizing Driving Environments with Generative Models and\n  Rule-Based Traffic", "author": "Kashyap Chitta and Daniel Dauner and Andreas Geiger", "abstract": "  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for rule-based traffic simulation. The unique properties of\nthe entities to be generated for SLEDGE, such as their connectivity and\nvariable count per scene, render the naive application of most modern\ngenerative models to this task non-trivial. Therefore, together with a\nsystematic study of existing lane graph representations, we introduce a novel\nraster-to-vector autoencoder. It encodes agents and the lane graph into\ndistinct channels in a rasterized latent map. This facilitates both\nlane-conditioned agent generation and combined generation of lanes and agents\nwith a Diffusion Transformer. Using generated entities in SLEDGE enables\ngreater control over the simulation, e.g. upsampling turns or increasing\ntraffic density. Further, SLEDGE can support 500m long routes, a capability not\nfound in existing data-driven simulators like nuPlan. It presents new\nchallenges for planning algorithms, evidenced by failure rates of over 40% for\nPDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and\ndense traffic generated by our model. Compared to nuPlan, SLEDGE requires\n500$\\times$ less storage to set up (<4 GB), making it a more accessible option\nand helping with democratizing future research in this field.\n", "link": "http://arxiv.org/abs/2403.17933v2", "date": "2024-07-11", "relevancy": 2.6964, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5726}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5227}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLEDGE%3A%20Synthesizing%20Driving%20Environments%20with%20Generative%20Models%20and%0A%20%20Rule-Based%20Traffic&body=Title%3A%20SLEDGE%3A%20Synthesizing%20Driving%20Environments%20with%20Generative%20Models%20and%0A%20%20Rule-Based%20Traffic%0AAuthor%3A%20Kashyap%20Chitta%20and%20Daniel%20Dauner%20and%20Andreas%20Geiger%0AAbstract%3A%20%20%20SLEDGE%20is%20the%20first%20generative%20simulator%20for%20vehicle%20motion%20planning%20trained%0Aon%20real-world%20driving%20logs.%20Its%20core%20component%20is%20a%20learned%20model%20that%20is%20able%0Ato%20generate%20agent%20bounding%20boxes%20and%20lane%20graphs.%20The%20model%27s%20outputs%20serve%20as%0Aan%20initial%20state%20for%20rule-based%20traffic%20simulation.%20The%20unique%20properties%20of%0Athe%20entities%20to%20be%20generated%20for%20SLEDGE%2C%20such%20as%20their%20connectivity%20and%0Avariable%20count%20per%20scene%2C%20render%20the%20naive%20application%20of%20most%20modern%0Agenerative%20models%20to%20this%20task%20non-trivial.%20Therefore%2C%20together%20with%20a%0Asystematic%20study%20of%20existing%20lane%20graph%20representations%2C%20we%20introduce%20a%20novel%0Araster-to-vector%20autoencoder.%20It%20encodes%20agents%20and%20the%20lane%20graph%20into%0Adistinct%20channels%20in%20a%20rasterized%20latent%20map.%20This%20facilitates%20both%0Alane-conditioned%20agent%20generation%20and%20combined%20generation%20of%20lanes%20and%20agents%0Awith%20a%20Diffusion%20Transformer.%20Using%20generated%20entities%20in%20SLEDGE%20enables%0Agreater%20control%20over%20the%20simulation%2C%20e.g.%20upsampling%20turns%20or%20increasing%0Atraffic%20density.%20Further%2C%20SLEDGE%20can%20support%20500m%20long%20routes%2C%20a%20capability%20not%0Afound%20in%20existing%20data-driven%20simulators%20like%20nuPlan.%20It%20presents%20new%0Achallenges%20for%20planning%20algorithms%2C%20evidenced%20by%20failure%20rates%20of%20over%2040%25%20for%0APDM%2C%20the%20winner%20of%20the%202023%20nuPlan%20challenge%2C%20when%20tested%20on%20hard%20routes%20and%0Adense%20traffic%20generated%20by%20our%20model.%20Compared%20to%20nuPlan%2C%20SLEDGE%20requires%0A500%24%5Ctimes%24%20less%20storage%20to%20set%20up%20%28%3C4%20GB%29%2C%20making%20it%20a%20more%20accessible%20option%0Aand%20helping%20with%20democratizing%20future%20research%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17933v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLEDGE%253A%2520Synthesizing%2520Driving%2520Environments%2520with%2520Generative%2520Models%2520and%250A%2520%2520Rule-Based%2520Traffic%26entry.906535625%3DKashyap%2520Chitta%2520and%2520Daniel%2520Dauner%2520and%2520Andreas%2520Geiger%26entry.1292438233%3D%2520%2520SLEDGE%2520is%2520the%2520first%2520generative%2520simulator%2520for%2520vehicle%2520motion%2520planning%2520trained%250Aon%2520real-world%2520driving%2520logs.%2520Its%2520core%2520component%2520is%2520a%2520learned%2520model%2520that%2520is%2520able%250Ato%2520generate%2520agent%2520bounding%2520boxes%2520and%2520lane%2520graphs.%2520The%2520model%2527s%2520outputs%2520serve%2520as%250Aan%2520initial%2520state%2520for%2520rule-based%2520traffic%2520simulation.%2520The%2520unique%2520properties%2520of%250Athe%2520entities%2520to%2520be%2520generated%2520for%2520SLEDGE%252C%2520such%2520as%2520their%2520connectivity%2520and%250Avariable%2520count%2520per%2520scene%252C%2520render%2520the%2520naive%2520application%2520of%2520most%2520modern%250Agenerative%2520models%2520to%2520this%2520task%2520non-trivial.%2520Therefore%252C%2520together%2520with%2520a%250Asystematic%2520study%2520of%2520existing%2520lane%2520graph%2520representations%252C%2520we%2520introduce%2520a%2520novel%250Araster-to-vector%2520autoencoder.%2520It%2520encodes%2520agents%2520and%2520the%2520lane%2520graph%2520into%250Adistinct%2520channels%2520in%2520a%2520rasterized%2520latent%2520map.%2520This%2520facilitates%2520both%250Alane-conditioned%2520agent%2520generation%2520and%2520combined%2520generation%2520of%2520lanes%2520and%2520agents%250Awith%2520a%2520Diffusion%2520Transformer.%2520Using%2520generated%2520entities%2520in%2520SLEDGE%2520enables%250Agreater%2520control%2520over%2520the%2520simulation%252C%2520e.g.%2520upsampling%2520turns%2520or%2520increasing%250Atraffic%2520density.%2520Further%252C%2520SLEDGE%2520can%2520support%2520500m%2520long%2520routes%252C%2520a%2520capability%2520not%250Afound%2520in%2520existing%2520data-driven%2520simulators%2520like%2520nuPlan.%2520It%2520presents%2520new%250Achallenges%2520for%2520planning%2520algorithms%252C%2520evidenced%2520by%2520failure%2520rates%2520of%2520over%252040%2525%2520for%250APDM%252C%2520the%2520winner%2520of%2520the%25202023%2520nuPlan%2520challenge%252C%2520when%2520tested%2520on%2520hard%2520routes%2520and%250Adense%2520traffic%2520generated%2520by%2520our%2520model.%2520Compared%2520to%2520nuPlan%252C%2520SLEDGE%2520requires%250A500%2524%255Ctimes%2524%2520less%2520storage%2520to%2520set%2520up%2520%2528%253C4%2520GB%2529%252C%2520making%2520it%2520a%2520more%2520accessible%2520option%250Aand%2520helping%2520with%2520democratizing%2520future%2520research%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17933v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLEDGE%3A%20Synthesizing%20Driving%20Environments%20with%20Generative%20Models%20and%0A%20%20Rule-Based%20Traffic&entry.906535625=Kashyap%20Chitta%20and%20Daniel%20Dauner%20and%20Andreas%20Geiger&entry.1292438233=%20%20SLEDGE%20is%20the%20first%20generative%20simulator%20for%20vehicle%20motion%20planning%20trained%0Aon%20real-world%20driving%20logs.%20Its%20core%20component%20is%20a%20learned%20model%20that%20is%20able%0Ato%20generate%20agent%20bounding%20boxes%20and%20lane%20graphs.%20The%20model%27s%20outputs%20serve%20as%0Aan%20initial%20state%20for%20rule-based%20traffic%20simulation.%20The%20unique%20properties%20of%0Athe%20entities%20to%20be%20generated%20for%20SLEDGE%2C%20such%20as%20their%20connectivity%20and%0Avariable%20count%20per%20scene%2C%20render%20the%20naive%20application%20of%20most%20modern%0Agenerative%20models%20to%20this%20task%20non-trivial.%20Therefore%2C%20together%20with%20a%0Asystematic%20study%20of%20existing%20lane%20graph%20representations%2C%20we%20introduce%20a%20novel%0Araster-to-vector%20autoencoder.%20It%20encodes%20agents%20and%20the%20lane%20graph%20into%0Adistinct%20channels%20in%20a%20rasterized%20latent%20map.%20This%20facilitates%20both%0Alane-conditioned%20agent%20generation%20and%20combined%20generation%20of%20lanes%20and%20agents%0Awith%20a%20Diffusion%20Transformer.%20Using%20generated%20entities%20in%20SLEDGE%20enables%0Agreater%20control%20over%20the%20simulation%2C%20e.g.%20upsampling%20turns%20or%20increasing%0Atraffic%20density.%20Further%2C%20SLEDGE%20can%20support%20500m%20long%20routes%2C%20a%20capability%20not%0Afound%20in%20existing%20data-driven%20simulators%20like%20nuPlan.%20It%20presents%20new%0Achallenges%20for%20planning%20algorithms%2C%20evidenced%20by%20failure%20rates%20of%20over%2040%25%20for%0APDM%2C%20the%20winner%20of%20the%202023%20nuPlan%20challenge%2C%20when%20tested%20on%20hard%20routes%20and%0Adense%20traffic%20generated%20by%20our%20model.%20Compared%20to%20nuPlan%2C%20SLEDGE%20requires%0A500%24%5Ctimes%24%20less%20storage%20to%20set%20up%20%28%3C4%20GB%29%2C%20making%20it%20a%20more%20accessible%20option%0Aand%20helping%20with%20democratizing%20future%20research%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17933v2&entry.124074799=Read"},
{"title": "Still-Moving: Customized Video Generation without Customized Video Data", "author": "Hila Chefer and Shiran Zada and Roni Paiss and Ariel Ephrat and Omer Tov and Michael Rubinstein and Lior Wolf and Tali Dekel and Tomer Michaeli and Inbar Mosseri", "abstract": "  Customizing text-to-image (T2I) models has seen tremendous progress recently,\nparticularly in areas such as personalization, stylization, and conditional\ngeneration. However, expanding this progress to video generation is still in\nits infancy, primarily due to the lack of customized video data. In this work,\nwe introduce Still-Moving, a novel generic framework for customizing a\ntext-to-video (T2V) model, without requiring any customized video data. The\nframework applies to the prominent T2V design where the video model is built\nover a text-to-image (T2I) model (e.g., via inflation). We assume access to a\ncustomized version of the T2I model, trained only on still image data (e.g.,\nusing DreamBooth or StyleDrop). Naively plugging in the weights of the\ncustomized T2I model into the T2V model often leads to significant artifacts or\ninsufficient adherence to the customization data. To overcome this issue, we\ntrain lightweight $\\textit{Spatial Adapters}$ that adjust the features produced\nby the injected T2I layers. Importantly, our adapters are trained on\n$\\textit{\"frozen videos\"}$ (i.e., repeated images), constructed from image\nsamples generated by the customized T2I model. This training is facilitated by\na novel $\\textit{Motion Adapter}$ module, which allows us to train on such\nstatic videos while preserving the motion prior of the video model. At test\ntime, we remove the Motion Adapter modules and leave in only the trained\nSpatial Adapters. This restores the motion prior of the T2V model while\nadhering to the spatial prior of the customized T2I model. We demonstrate the\neffectiveness of our approach on diverse tasks including personalized,\nstylized, and conditional generation. In all evaluated scenarios, our method\nseamlessly integrates the spatial prior of the customized T2I model with a\nmotion prior supplied by the T2V model.\n", "link": "http://arxiv.org/abs/2407.08674v1", "date": "2024-07-11", "relevancy": 2.6921, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.8019}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7101}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Still-Moving%3A%20Customized%20Video%20Generation%20without%20Customized%20Video%20Data&body=Title%3A%20Still-Moving%3A%20Customized%20Video%20Generation%20without%20Customized%20Video%20Data%0AAuthor%3A%20Hila%20Chefer%20and%20Shiran%20Zada%20and%20Roni%20Paiss%20and%20Ariel%20Ephrat%20and%20Omer%20Tov%20and%20Michael%20Rubinstein%20and%20Lior%20Wolf%20and%20Tali%20Dekel%20and%20Tomer%20Michaeli%20and%20Inbar%20Mosseri%0AAbstract%3A%20%20%20Customizing%20text-to-image%20%28T2I%29%20models%20has%20seen%20tremendous%20progress%20recently%2C%0Aparticularly%20in%20areas%20such%20as%20personalization%2C%20stylization%2C%20and%20conditional%0Ageneration.%20However%2C%20expanding%20this%20progress%20to%20video%20generation%20is%20still%20in%0Aits%20infancy%2C%20primarily%20due%20to%20the%20lack%20of%20customized%20video%20data.%20In%20this%20work%2C%0Awe%20introduce%20Still-Moving%2C%20a%20novel%20generic%20framework%20for%20customizing%20a%0Atext-to-video%20%28T2V%29%20model%2C%20without%20requiring%20any%20customized%20video%20data.%20The%0Aframework%20applies%20to%20the%20prominent%20T2V%20design%20where%20the%20video%20model%20is%20built%0Aover%20a%20text-to-image%20%28T2I%29%20model%20%28e.g.%2C%20via%20inflation%29.%20We%20assume%20access%20to%20a%0Acustomized%20version%20of%20the%20T2I%20model%2C%20trained%20only%20on%20still%20image%20data%20%28e.g.%2C%0Ausing%20DreamBooth%20or%20StyleDrop%29.%20Naively%20plugging%20in%20the%20weights%20of%20the%0Acustomized%20T2I%20model%20into%20the%20T2V%20model%20often%20leads%20to%20significant%20artifacts%20or%0Ainsufficient%20adherence%20to%20the%20customization%20data.%20To%20overcome%20this%20issue%2C%20we%0Atrain%20lightweight%20%24%5Ctextit%7BSpatial%20Adapters%7D%24%20that%20adjust%20the%20features%20produced%0Aby%20the%20injected%20T2I%20layers.%20Importantly%2C%20our%20adapters%20are%20trained%20on%0A%24%5Ctextit%7B%22frozen%20videos%22%7D%24%20%28i.e.%2C%20repeated%20images%29%2C%20constructed%20from%20image%0Asamples%20generated%20by%20the%20customized%20T2I%20model.%20This%20training%20is%20facilitated%20by%0Aa%20novel%20%24%5Ctextit%7BMotion%20Adapter%7D%24%20module%2C%20which%20allows%20us%20to%20train%20on%20such%0Astatic%20videos%20while%20preserving%20the%20motion%20prior%20of%20the%20video%20model.%20At%20test%0Atime%2C%20we%20remove%20the%20Motion%20Adapter%20modules%20and%20leave%20in%20only%20the%20trained%0ASpatial%20Adapters.%20This%20restores%20the%20motion%20prior%20of%20the%20T2V%20model%20while%0Aadhering%20to%20the%20spatial%20prior%20of%20the%20customized%20T2I%20model.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20on%20diverse%20tasks%20including%20personalized%2C%0Astylized%2C%20and%20conditional%20generation.%20In%20all%20evaluated%20scenarios%2C%20our%20method%0Aseamlessly%20integrates%20the%20spatial%20prior%20of%20the%20customized%20T2I%20model%20with%20a%0Amotion%20prior%20supplied%20by%20the%20T2V%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStill-Moving%253A%2520Customized%2520Video%2520Generation%2520without%2520Customized%2520Video%2520Data%26entry.906535625%3DHila%2520Chefer%2520and%2520Shiran%2520Zada%2520and%2520Roni%2520Paiss%2520and%2520Ariel%2520Ephrat%2520and%2520Omer%2520Tov%2520and%2520Michael%2520Rubinstein%2520and%2520Lior%2520Wolf%2520and%2520Tali%2520Dekel%2520and%2520Tomer%2520Michaeli%2520and%2520Inbar%2520Mosseri%26entry.1292438233%3D%2520%2520Customizing%2520text-to-image%2520%2528T2I%2529%2520models%2520has%2520seen%2520tremendous%2520progress%2520recently%252C%250Aparticularly%2520in%2520areas%2520such%2520as%2520personalization%252C%2520stylization%252C%2520and%2520conditional%250Ageneration.%2520However%252C%2520expanding%2520this%2520progress%2520to%2520video%2520generation%2520is%2520still%2520in%250Aits%2520infancy%252C%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520customized%2520video%2520data.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520Still-Moving%252C%2520a%2520novel%2520generic%2520framework%2520for%2520customizing%2520a%250Atext-to-video%2520%2528T2V%2529%2520model%252C%2520without%2520requiring%2520any%2520customized%2520video%2520data.%2520The%250Aframework%2520applies%2520to%2520the%2520prominent%2520T2V%2520design%2520where%2520the%2520video%2520model%2520is%2520built%250Aover%2520a%2520text-to-image%2520%2528T2I%2529%2520model%2520%2528e.g.%252C%2520via%2520inflation%2529.%2520We%2520assume%2520access%2520to%2520a%250Acustomized%2520version%2520of%2520the%2520T2I%2520model%252C%2520trained%2520only%2520on%2520still%2520image%2520data%2520%2528e.g.%252C%250Ausing%2520DreamBooth%2520or%2520StyleDrop%2529.%2520Naively%2520plugging%2520in%2520the%2520weights%2520of%2520the%250Acustomized%2520T2I%2520model%2520into%2520the%2520T2V%2520model%2520often%2520leads%2520to%2520significant%2520artifacts%2520or%250Ainsufficient%2520adherence%2520to%2520the%2520customization%2520data.%2520To%2520overcome%2520this%2520issue%252C%2520we%250Atrain%2520lightweight%2520%2524%255Ctextit%257BSpatial%2520Adapters%257D%2524%2520that%2520adjust%2520the%2520features%2520produced%250Aby%2520the%2520injected%2520T2I%2520layers.%2520Importantly%252C%2520our%2520adapters%2520are%2520trained%2520on%250A%2524%255Ctextit%257B%2522frozen%2520videos%2522%257D%2524%2520%2528i.e.%252C%2520repeated%2520images%2529%252C%2520constructed%2520from%2520image%250Asamples%2520generated%2520by%2520the%2520customized%2520T2I%2520model.%2520This%2520training%2520is%2520facilitated%2520by%250Aa%2520novel%2520%2524%255Ctextit%257BMotion%2520Adapter%257D%2524%2520module%252C%2520which%2520allows%2520us%2520to%2520train%2520on%2520such%250Astatic%2520videos%2520while%2520preserving%2520the%2520motion%2520prior%2520of%2520the%2520video%2520model.%2520At%2520test%250Atime%252C%2520we%2520remove%2520the%2520Motion%2520Adapter%2520modules%2520and%2520leave%2520in%2520only%2520the%2520trained%250ASpatial%2520Adapters.%2520This%2520restores%2520the%2520motion%2520prior%2520of%2520the%2520T2V%2520model%2520while%250Aadhering%2520to%2520the%2520spatial%2520prior%2520of%2520the%2520customized%2520T2I%2520model.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520on%2520diverse%2520tasks%2520including%2520personalized%252C%250Astylized%252C%2520and%2520conditional%2520generation.%2520In%2520all%2520evaluated%2520scenarios%252C%2520our%2520method%250Aseamlessly%2520integrates%2520the%2520spatial%2520prior%2520of%2520the%2520customized%2520T2I%2520model%2520with%2520a%250Amotion%2520prior%2520supplied%2520by%2520the%2520T2V%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Still-Moving%3A%20Customized%20Video%20Generation%20without%20Customized%20Video%20Data&entry.906535625=Hila%20Chefer%20and%20Shiran%20Zada%20and%20Roni%20Paiss%20and%20Ariel%20Ephrat%20and%20Omer%20Tov%20and%20Michael%20Rubinstein%20and%20Lior%20Wolf%20and%20Tali%20Dekel%20and%20Tomer%20Michaeli%20and%20Inbar%20Mosseri&entry.1292438233=%20%20Customizing%20text-to-image%20%28T2I%29%20models%20has%20seen%20tremendous%20progress%20recently%2C%0Aparticularly%20in%20areas%20such%20as%20personalization%2C%20stylization%2C%20and%20conditional%0Ageneration.%20However%2C%20expanding%20this%20progress%20to%20video%20generation%20is%20still%20in%0Aits%20infancy%2C%20primarily%20due%20to%20the%20lack%20of%20customized%20video%20data.%20In%20this%20work%2C%0Awe%20introduce%20Still-Moving%2C%20a%20novel%20generic%20framework%20for%20customizing%20a%0Atext-to-video%20%28T2V%29%20model%2C%20without%20requiring%20any%20customized%20video%20data.%20The%0Aframework%20applies%20to%20the%20prominent%20T2V%20design%20where%20the%20video%20model%20is%20built%0Aover%20a%20text-to-image%20%28T2I%29%20model%20%28e.g.%2C%20via%20inflation%29.%20We%20assume%20access%20to%20a%0Acustomized%20version%20of%20the%20T2I%20model%2C%20trained%20only%20on%20still%20image%20data%20%28e.g.%2C%0Ausing%20DreamBooth%20or%20StyleDrop%29.%20Naively%20plugging%20in%20the%20weights%20of%20the%0Acustomized%20T2I%20model%20into%20the%20T2V%20model%20often%20leads%20to%20significant%20artifacts%20or%0Ainsufficient%20adherence%20to%20the%20customization%20data.%20To%20overcome%20this%20issue%2C%20we%0Atrain%20lightweight%20%24%5Ctextit%7BSpatial%20Adapters%7D%24%20that%20adjust%20the%20features%20produced%0Aby%20the%20injected%20T2I%20layers.%20Importantly%2C%20our%20adapters%20are%20trained%20on%0A%24%5Ctextit%7B%22frozen%20videos%22%7D%24%20%28i.e.%2C%20repeated%20images%29%2C%20constructed%20from%20image%0Asamples%20generated%20by%20the%20customized%20T2I%20model.%20This%20training%20is%20facilitated%20by%0Aa%20novel%20%24%5Ctextit%7BMotion%20Adapter%7D%24%20module%2C%20which%20allows%20us%20to%20train%20on%20such%0Astatic%20videos%20while%20preserving%20the%20motion%20prior%20of%20the%20video%20model.%20At%20test%0Atime%2C%20we%20remove%20the%20Motion%20Adapter%20modules%20and%20leave%20in%20only%20the%20trained%0ASpatial%20Adapters.%20This%20restores%20the%20motion%20prior%20of%20the%20T2V%20model%20while%0Aadhering%20to%20the%20spatial%20prior%20of%20the%20customized%20T2I%20model.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20on%20diverse%20tasks%20including%20personalized%2C%0Astylized%2C%20and%20conditional%20generation.%20In%20all%20evaluated%20scenarios%2C%20our%20method%0Aseamlessly%20integrates%20the%20spatial%20prior%20of%20the%20customized%20T2I%20model%20with%20a%0Amotion%20prior%20supplied%20by%20the%20T2V%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08674v1&entry.124074799=Read"},
{"title": "Boosting Adversarial Transferability for Skeleton-based Action\n  Recognition via Exploring the Model Posterior Space", "author": "Yunfeng Diao and Baiqi Wu and Ruixuan Zhang and Xun Yang and Meng Wang and He Wang", "abstract": "  Skeletal motion plays a pivotal role in human activity recognition (HAR).\nRecently, attack methods have been proposed to identify the universal\nvulnerability of skeleton-based HAR(S-HAR). However, the research of\nadversarial transferability on S-HAR is largely missing. More importantly,\nexisting attacks all struggle in transfer across unknown S-HAR models. We\nobserved that the key reason is that the loss landscape of the action\nrecognizers is rugged and sharp. Given the established correlation in prior\nstudies~\\cite{qin2022boosting,wu2020towards} between loss landscape and\nadversarial transferability, we assume and empirically validate that smoothing\nthe loss landscape could potentially improve adversarial transferability on\nS-HAR. This is achieved by proposing a new post-train Dual Bayesian strategy,\nwhich can effectively explore the model posterior space for a collection of\nsurrogates without the need for re-training. Furthermore, to craft adversarial\nexamples along the motion manifold, we incorporate the attack gradient with\ninformation of the motion dynamics in a Bayesian manner. Evaluated on benchmark\ndatasets, e.g. HDM05 and NTU 60, the average transfer success rate can reach as\nhigh as 35.9\\% and 45.5\\% respectively. In comparison, current state-of-the-art\nskeletal attacks achieve only 3.6\\% and 9.8\\%. The high adversarial\ntransferability remains consistent across various surrogate, victim, and even\ndefense models. Through a comprehensive analysis of the results, we provide\ninsights on what surrogates are more likely to exhibit transferability, to shed\nlight on future research.\n", "link": "http://arxiv.org/abs/2407.08572v1", "date": "2024-07-11", "relevancy": 2.6895, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.539}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5375}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Adversarial%20Transferability%20for%20Skeleton-based%20Action%0A%20%20Recognition%20via%20Exploring%20the%20Model%20Posterior%20Space&body=Title%3A%20Boosting%20Adversarial%20Transferability%20for%20Skeleton-based%20Action%0A%20%20Recognition%20via%20Exploring%20the%20Model%20Posterior%20Space%0AAuthor%3A%20Yunfeng%20Diao%20and%20Baiqi%20Wu%20and%20Ruixuan%20Zhang%20and%20Xun%20Yang%20and%20Meng%20Wang%20and%20He%20Wang%0AAbstract%3A%20%20%20Skeletal%20motion%20plays%20a%20pivotal%20role%20in%20human%20activity%20recognition%20%28HAR%29.%0ARecently%2C%20attack%20methods%20have%20been%20proposed%20to%20identify%20the%20universal%0Avulnerability%20of%20skeleton-based%20HAR%28S-HAR%29.%20However%2C%20the%20research%20of%0Aadversarial%20transferability%20on%20S-HAR%20is%20largely%20missing.%20More%20importantly%2C%0Aexisting%20attacks%20all%20struggle%20in%20transfer%20across%20unknown%20S-HAR%20models.%20We%0Aobserved%20that%20the%20key%20reason%20is%20that%20the%20loss%20landscape%20of%20the%20action%0Arecognizers%20is%20rugged%20and%20sharp.%20Given%20the%20established%20correlation%20in%20prior%0Astudies~%5Ccite%7Bqin2022boosting%2Cwu2020towards%7D%20between%20loss%20landscape%20and%0Aadversarial%20transferability%2C%20we%20assume%20and%20empirically%20validate%20that%20smoothing%0Athe%20loss%20landscape%20could%20potentially%20improve%20adversarial%20transferability%20on%0AS-HAR.%20This%20is%20achieved%20by%20proposing%20a%20new%20post-train%20Dual%20Bayesian%20strategy%2C%0Awhich%20can%20effectively%20explore%20the%20model%20posterior%20space%20for%20a%20collection%20of%0Asurrogates%20without%20the%20need%20for%20re-training.%20Furthermore%2C%20to%20craft%20adversarial%0Aexamples%20along%20the%20motion%20manifold%2C%20we%20incorporate%20the%20attack%20gradient%20with%0Ainformation%20of%20the%20motion%20dynamics%20in%20a%20Bayesian%20manner.%20Evaluated%20on%20benchmark%0Adatasets%2C%20e.g.%20HDM05%20and%20NTU%2060%2C%20the%20average%20transfer%20success%20rate%20can%20reach%20as%0Ahigh%20as%2035.9%5C%25%20and%2045.5%5C%25%20respectively.%20In%20comparison%2C%20current%20state-of-the-art%0Askeletal%20attacks%20achieve%20only%203.6%5C%25%20and%209.8%5C%25.%20The%20high%20adversarial%0Atransferability%20remains%20consistent%20across%20various%20surrogate%2C%20victim%2C%20and%20even%0Adefense%20models.%20Through%20a%20comprehensive%20analysis%20of%20the%20results%2C%20we%20provide%0Ainsights%20on%20what%20surrogates%20are%20more%20likely%20to%20exhibit%20transferability%2C%20to%20shed%0Alight%20on%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Adversarial%2520Transferability%2520for%2520Skeleton-based%2520Action%250A%2520%2520Recognition%2520via%2520Exploring%2520the%2520Model%2520Posterior%2520Space%26entry.906535625%3DYunfeng%2520Diao%2520and%2520Baiqi%2520Wu%2520and%2520Ruixuan%2520Zhang%2520and%2520Xun%2520Yang%2520and%2520Meng%2520Wang%2520and%2520He%2520Wang%26entry.1292438233%3D%2520%2520Skeletal%2520motion%2520plays%2520a%2520pivotal%2520role%2520in%2520human%2520activity%2520recognition%2520%2528HAR%2529.%250ARecently%252C%2520attack%2520methods%2520have%2520been%2520proposed%2520to%2520identify%2520the%2520universal%250Avulnerability%2520of%2520skeleton-based%2520HAR%2528S-HAR%2529.%2520However%252C%2520the%2520research%2520of%250Aadversarial%2520transferability%2520on%2520S-HAR%2520is%2520largely%2520missing.%2520More%2520importantly%252C%250Aexisting%2520attacks%2520all%2520struggle%2520in%2520transfer%2520across%2520unknown%2520S-HAR%2520models.%2520We%250Aobserved%2520that%2520the%2520key%2520reason%2520is%2520that%2520the%2520loss%2520landscape%2520of%2520the%2520action%250Arecognizers%2520is%2520rugged%2520and%2520sharp.%2520Given%2520the%2520established%2520correlation%2520in%2520prior%250Astudies~%255Ccite%257Bqin2022boosting%252Cwu2020towards%257D%2520between%2520loss%2520landscape%2520and%250Aadversarial%2520transferability%252C%2520we%2520assume%2520and%2520empirically%2520validate%2520that%2520smoothing%250Athe%2520loss%2520landscape%2520could%2520potentially%2520improve%2520adversarial%2520transferability%2520on%250AS-HAR.%2520This%2520is%2520achieved%2520by%2520proposing%2520a%2520new%2520post-train%2520Dual%2520Bayesian%2520strategy%252C%250Awhich%2520can%2520effectively%2520explore%2520the%2520model%2520posterior%2520space%2520for%2520a%2520collection%2520of%250Asurrogates%2520without%2520the%2520need%2520for%2520re-training.%2520Furthermore%252C%2520to%2520craft%2520adversarial%250Aexamples%2520along%2520the%2520motion%2520manifold%252C%2520we%2520incorporate%2520the%2520attack%2520gradient%2520with%250Ainformation%2520of%2520the%2520motion%2520dynamics%2520in%2520a%2520Bayesian%2520manner.%2520Evaluated%2520on%2520benchmark%250Adatasets%252C%2520e.g.%2520HDM05%2520and%2520NTU%252060%252C%2520the%2520average%2520transfer%2520success%2520rate%2520can%2520reach%2520as%250Ahigh%2520as%252035.9%255C%2525%2520and%252045.5%255C%2525%2520respectively.%2520In%2520comparison%252C%2520current%2520state-of-the-art%250Askeletal%2520attacks%2520achieve%2520only%25203.6%255C%2525%2520and%25209.8%255C%2525.%2520The%2520high%2520adversarial%250Atransferability%2520remains%2520consistent%2520across%2520various%2520surrogate%252C%2520victim%252C%2520and%2520even%250Adefense%2520models.%2520Through%2520a%2520comprehensive%2520analysis%2520of%2520the%2520results%252C%2520we%2520provide%250Ainsights%2520on%2520what%2520surrogates%2520are%2520more%2520likely%2520to%2520exhibit%2520transferability%252C%2520to%2520shed%250Alight%2520on%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Adversarial%20Transferability%20for%20Skeleton-based%20Action%0A%20%20Recognition%20via%20Exploring%20the%20Model%20Posterior%20Space&entry.906535625=Yunfeng%20Diao%20and%20Baiqi%20Wu%20and%20Ruixuan%20Zhang%20and%20Xun%20Yang%20and%20Meng%20Wang%20and%20He%20Wang&entry.1292438233=%20%20Skeletal%20motion%20plays%20a%20pivotal%20role%20in%20human%20activity%20recognition%20%28HAR%29.%0ARecently%2C%20attack%20methods%20have%20been%20proposed%20to%20identify%20the%20universal%0Avulnerability%20of%20skeleton-based%20HAR%28S-HAR%29.%20However%2C%20the%20research%20of%0Aadversarial%20transferability%20on%20S-HAR%20is%20largely%20missing.%20More%20importantly%2C%0Aexisting%20attacks%20all%20struggle%20in%20transfer%20across%20unknown%20S-HAR%20models.%20We%0Aobserved%20that%20the%20key%20reason%20is%20that%20the%20loss%20landscape%20of%20the%20action%0Arecognizers%20is%20rugged%20and%20sharp.%20Given%20the%20established%20correlation%20in%20prior%0Astudies~%5Ccite%7Bqin2022boosting%2Cwu2020towards%7D%20between%20loss%20landscape%20and%0Aadversarial%20transferability%2C%20we%20assume%20and%20empirically%20validate%20that%20smoothing%0Athe%20loss%20landscape%20could%20potentially%20improve%20adversarial%20transferability%20on%0AS-HAR.%20This%20is%20achieved%20by%20proposing%20a%20new%20post-train%20Dual%20Bayesian%20strategy%2C%0Awhich%20can%20effectively%20explore%20the%20model%20posterior%20space%20for%20a%20collection%20of%0Asurrogates%20without%20the%20need%20for%20re-training.%20Furthermore%2C%20to%20craft%20adversarial%0Aexamples%20along%20the%20motion%20manifold%2C%20we%20incorporate%20the%20attack%20gradient%20with%0Ainformation%20of%20the%20motion%20dynamics%20in%20a%20Bayesian%20manner.%20Evaluated%20on%20benchmark%0Adatasets%2C%20e.g.%20HDM05%20and%20NTU%2060%2C%20the%20average%20transfer%20success%20rate%20can%20reach%20as%0Ahigh%20as%2035.9%5C%25%20and%2045.5%5C%25%20respectively.%20In%20comparison%2C%20current%20state-of-the-art%0Askeletal%20attacks%20achieve%20only%203.6%5C%25%20and%209.8%5C%25.%20The%20high%20adversarial%0Atransferability%20remains%20consistent%20across%20various%20surrogate%2C%20victim%2C%20and%20even%0Adefense%20models.%20Through%20a%20comprehensive%20analysis%20of%20the%20results%2C%20we%20provide%0Ainsights%20on%20what%20surrogates%20are%20more%20likely%20to%20exhibit%20transferability%2C%20to%20shed%0Alight%20on%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08572v1&entry.124074799=Read"},
{"title": "A Graph-Based Approach for Category-Agnostic Pose Estimation", "author": "Or Hirschorn and Shai Avidan", "abstract": "  Traditional 2D pose estimation models are limited by their category-specific\ndesign, making them suitable only for predefined object categories. This\nrestriction becomes particularly challenging when dealing with novel objects\ndue to the lack of relevant training data. To address this limitation,\ncategory-agnostic pose estimation (CAPE) was introduced. CAPE aims to enable\nkeypoint localization for arbitrary object categories using a few-shot single\nmodel, requiring minimal support images with annotated keypoints. We present a\nsignificant departure from conventional CAPE techniques, which treat keypoints\nas isolated entities, by treating the input pose data as a graph. We leverage\nthe inherent geometrical relations between keypoints through a graph-based\nnetwork to break symmetry, preserve structure, and better handle occlusions. We\nvalidate our approach on the MP-100 benchmark, a comprehensive dataset\ncomprising over 20,000 images spanning over 100 categories. Our solution boosts\nperformance by 0.98% under a 1-shot setting, achieving a new state-of-the-art\nfor CAPE. Additionally, we enhance the dataset with skeleton annotations. Our\ncode and data are publicly available.\n", "link": "http://arxiv.org/abs/2311.17891v2", "date": "2024-07-11", "relevancy": 2.6716, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.543}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5335}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Graph-Based%20Approach%20for%20Category-Agnostic%20Pose%20Estimation&body=Title%3A%20A%20Graph-Based%20Approach%20for%20Category-Agnostic%20Pose%20Estimation%0AAuthor%3A%20Or%20Hirschorn%20and%20Shai%20Avidan%0AAbstract%3A%20%20%20Traditional%202D%20pose%20estimation%20models%20are%20limited%20by%20their%20category-specific%0Adesign%2C%20making%20them%20suitable%20only%20for%20predefined%20object%20categories.%20This%0Arestriction%20becomes%20particularly%20challenging%20when%20dealing%20with%20novel%20objects%0Adue%20to%20the%20lack%20of%20relevant%20training%20data.%20To%20address%20this%20limitation%2C%0Acategory-agnostic%20pose%20estimation%20%28CAPE%29%20was%20introduced.%20CAPE%20aims%20to%20enable%0Akeypoint%20localization%20for%20arbitrary%20object%20categories%20using%20a%20few-shot%20single%0Amodel%2C%20requiring%20minimal%20support%20images%20with%20annotated%20keypoints.%20We%20present%20a%0Asignificant%20departure%20from%20conventional%20CAPE%20techniques%2C%20which%20treat%20keypoints%0Aas%20isolated%20entities%2C%20by%20treating%20the%20input%20pose%20data%20as%20a%20graph.%20We%20leverage%0Athe%20inherent%20geometrical%20relations%20between%20keypoints%20through%20a%20graph-based%0Anetwork%20to%20break%20symmetry%2C%20preserve%20structure%2C%20and%20better%20handle%20occlusions.%20We%0Avalidate%20our%20approach%20on%20the%20MP-100%20benchmark%2C%20a%20comprehensive%20dataset%0Acomprising%20over%2020%2C000%20images%20spanning%20over%20100%20categories.%20Our%20solution%20boosts%0Aperformance%20by%200.98%25%20under%20a%201-shot%20setting%2C%20achieving%20a%20new%20state-of-the-art%0Afor%20CAPE.%20Additionally%2C%20we%20enhance%20the%20dataset%20with%20skeleton%20annotations.%20Our%0Acode%20and%20data%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Graph-Based%2520Approach%2520for%2520Category-Agnostic%2520Pose%2520Estimation%26entry.906535625%3DOr%2520Hirschorn%2520and%2520Shai%2520Avidan%26entry.1292438233%3D%2520%2520Traditional%25202D%2520pose%2520estimation%2520models%2520are%2520limited%2520by%2520their%2520category-specific%250Adesign%252C%2520making%2520them%2520suitable%2520only%2520for%2520predefined%2520object%2520categories.%2520This%250Arestriction%2520becomes%2520particularly%2520challenging%2520when%2520dealing%2520with%2520novel%2520objects%250Adue%2520to%2520the%2520lack%2520of%2520relevant%2520training%2520data.%2520To%2520address%2520this%2520limitation%252C%250Acategory-agnostic%2520pose%2520estimation%2520%2528CAPE%2529%2520was%2520introduced.%2520CAPE%2520aims%2520to%2520enable%250Akeypoint%2520localization%2520for%2520arbitrary%2520object%2520categories%2520using%2520a%2520few-shot%2520single%250Amodel%252C%2520requiring%2520minimal%2520support%2520images%2520with%2520annotated%2520keypoints.%2520We%2520present%2520a%250Asignificant%2520departure%2520from%2520conventional%2520CAPE%2520techniques%252C%2520which%2520treat%2520keypoints%250Aas%2520isolated%2520entities%252C%2520by%2520treating%2520the%2520input%2520pose%2520data%2520as%2520a%2520graph.%2520We%2520leverage%250Athe%2520inherent%2520geometrical%2520relations%2520between%2520keypoints%2520through%2520a%2520graph-based%250Anetwork%2520to%2520break%2520symmetry%252C%2520preserve%2520structure%252C%2520and%2520better%2520handle%2520occlusions.%2520We%250Avalidate%2520our%2520approach%2520on%2520the%2520MP-100%2520benchmark%252C%2520a%2520comprehensive%2520dataset%250Acomprising%2520over%252020%252C000%2520images%2520spanning%2520over%2520100%2520categories.%2520Our%2520solution%2520boosts%250Aperformance%2520by%25200.98%2525%2520under%2520a%25201-shot%2520setting%252C%2520achieving%2520a%2520new%2520state-of-the-art%250Afor%2520CAPE.%2520Additionally%252C%2520we%2520enhance%2520the%2520dataset%2520with%2520skeleton%2520annotations.%2520Our%250Acode%2520and%2520data%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Graph-Based%20Approach%20for%20Category-Agnostic%20Pose%20Estimation&entry.906535625=Or%20Hirschorn%20and%20Shai%20Avidan&entry.1292438233=%20%20Traditional%202D%20pose%20estimation%20models%20are%20limited%20by%20their%20category-specific%0Adesign%2C%20making%20them%20suitable%20only%20for%20predefined%20object%20categories.%20This%0Arestriction%20becomes%20particularly%20challenging%20when%20dealing%20with%20novel%20objects%0Adue%20to%20the%20lack%20of%20relevant%20training%20data.%20To%20address%20this%20limitation%2C%0Acategory-agnostic%20pose%20estimation%20%28CAPE%29%20was%20introduced.%20CAPE%20aims%20to%20enable%0Akeypoint%20localization%20for%20arbitrary%20object%20categories%20using%20a%20few-shot%20single%0Amodel%2C%20requiring%20minimal%20support%20images%20with%20annotated%20keypoints.%20We%20present%20a%0Asignificant%20departure%20from%20conventional%20CAPE%20techniques%2C%20which%20treat%20keypoints%0Aas%20isolated%20entities%2C%20by%20treating%20the%20input%20pose%20data%20as%20a%20graph.%20We%20leverage%0Athe%20inherent%20geometrical%20relations%20between%20keypoints%20through%20a%20graph-based%0Anetwork%20to%20break%20symmetry%2C%20preserve%20structure%2C%20and%20better%20handle%20occlusions.%20We%0Avalidate%20our%20approach%20on%20the%20MP-100%20benchmark%2C%20a%20comprehensive%20dataset%0Acomprising%20over%2020%2C000%20images%20spanning%20over%20100%20categories.%20Our%20solution%20boosts%0Aperformance%20by%200.98%25%20under%20a%201-shot%20setting%2C%20achieving%20a%20new%20state-of-the-art%0Afor%20CAPE.%20Additionally%2C%20we%20enhance%20the%20dataset%20with%20skeleton%20annotations.%20Our%0Acode%20and%20data%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17891v2&entry.124074799=Read"},
{"title": "Large Pre-trained time series models for cross-domain Time series\n  analysis tasks", "author": "Harshavardhan Kamarthi and B. Aditya Prakash", "abstract": "  Large pre-trained models have been vital in recent advancements in domains\nlike language and vision, making model training for individual downstream tasks\nmore efficient and provide superior performance. However, tackling time-series\nanalysis tasks usually involves designing and training a separate model from\nscratch leveraging training data and domain expertise specific to the task. We\ntackle a significant challenge for pre-training a foundational time-series\nmodel from multi-domain time-series datasets: extracting\n  semantically useful tokenized inputs to the model\n  across heterogenous time-series from different domains. We propose Large\nPre-trained Time-series Models (LPTM) that introduces a novel method of\n\\textit{adaptive segmentation} that automatically identifies optimal\ndataset-specific\n  segmentation strategy during pre-training. This enables\n  LPTM to perform similar to or better than domain-specific state-of-art model\n  when fine-tuned to different downstream time-series analysis tasks and under\nzero-shot settings.\n  LPTM achieves superior forecasting and time-series classification results\n  taking up to 40% less data and 50% less training time\n  compared to state-of-art baselines.\n", "link": "http://arxiv.org/abs/2311.11413v2", "date": "2024-07-11", "relevancy": 2.606, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5487}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5301}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Pre-trained%20time%20series%20models%20for%20cross-domain%20Time%20series%0A%20%20analysis%20tasks&body=Title%3A%20Large%20Pre-trained%20time%20series%20models%20for%20cross-domain%20Time%20series%0A%20%20analysis%20tasks%0AAuthor%3A%20Harshavardhan%20Kamarthi%20and%20B.%20Aditya%20Prakash%0AAbstract%3A%20%20%20Large%20pre-trained%20models%20have%20been%20vital%20in%20recent%20advancements%20in%20domains%0Alike%20language%20and%20vision%2C%20making%20model%20training%20for%20individual%20downstream%20tasks%0Amore%20efficient%20and%20provide%20superior%20performance.%20However%2C%20tackling%20time-series%0Aanalysis%20tasks%20usually%20involves%20designing%20and%20training%20a%20separate%20model%20from%0Ascratch%20leveraging%20training%20data%20and%20domain%20expertise%20specific%20to%20the%20task.%20We%0Atackle%20a%20significant%20challenge%20for%20pre-training%20a%20foundational%20time-series%0Amodel%20from%20multi-domain%20time-series%20datasets%3A%20extracting%0A%20%20semantically%20useful%20tokenized%20inputs%20to%20the%20model%0A%20%20across%20heterogenous%20time-series%20from%20different%20domains.%20We%20propose%20Large%0APre-trained%20Time-series%20Models%20%28LPTM%29%20that%20introduces%20a%20novel%20method%20of%0A%5Ctextit%7Badaptive%20segmentation%7D%20that%20automatically%20identifies%20optimal%0Adataset-specific%0A%20%20segmentation%20strategy%20during%20pre-training.%20This%20enables%0A%20%20LPTM%20to%20perform%20similar%20to%20or%20better%20than%20domain-specific%20state-of-art%20model%0A%20%20when%20fine-tuned%20to%20different%20downstream%20time-series%20analysis%20tasks%20and%20under%0Azero-shot%20settings.%0A%20%20LPTM%20achieves%20superior%20forecasting%20and%20time-series%20classification%20results%0A%20%20taking%20up%20to%2040%25%20less%20data%20and%2050%25%20less%20training%20time%0A%20%20compared%20to%20state-of-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Pre-trained%2520time%2520series%2520models%2520for%2520cross-domain%2520Time%2520series%250A%2520%2520analysis%2520tasks%26entry.906535625%3DHarshavardhan%2520Kamarthi%2520and%2520B.%2520Aditya%2520Prakash%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520models%2520have%2520been%2520vital%2520in%2520recent%2520advancements%2520in%2520domains%250Alike%2520language%2520and%2520vision%252C%2520making%2520model%2520training%2520for%2520individual%2520downstream%2520tasks%250Amore%2520efficient%2520and%2520provide%2520superior%2520performance.%2520However%252C%2520tackling%2520time-series%250Aanalysis%2520tasks%2520usually%2520involves%2520designing%2520and%2520training%2520a%2520separate%2520model%2520from%250Ascratch%2520leveraging%2520training%2520data%2520and%2520domain%2520expertise%2520specific%2520to%2520the%2520task.%2520We%250Atackle%2520a%2520significant%2520challenge%2520for%2520pre-training%2520a%2520foundational%2520time-series%250Amodel%2520from%2520multi-domain%2520time-series%2520datasets%253A%2520extracting%250A%2520%2520semantically%2520useful%2520tokenized%2520inputs%2520to%2520the%2520model%250A%2520%2520across%2520heterogenous%2520time-series%2520from%2520different%2520domains.%2520We%2520propose%2520Large%250APre-trained%2520Time-series%2520Models%2520%2528LPTM%2529%2520that%2520introduces%2520a%2520novel%2520method%2520of%250A%255Ctextit%257Badaptive%2520segmentation%257D%2520that%2520automatically%2520identifies%2520optimal%250Adataset-specific%250A%2520%2520segmentation%2520strategy%2520during%2520pre-training.%2520This%2520enables%250A%2520%2520LPTM%2520to%2520perform%2520similar%2520to%2520or%2520better%2520than%2520domain-specific%2520state-of-art%2520model%250A%2520%2520when%2520fine-tuned%2520to%2520different%2520downstream%2520time-series%2520analysis%2520tasks%2520and%2520under%250Azero-shot%2520settings.%250A%2520%2520LPTM%2520achieves%2520superior%2520forecasting%2520and%2520time-series%2520classification%2520results%250A%2520%2520taking%2520up%2520to%252040%2525%2520less%2520data%2520and%252050%2525%2520less%2520training%2520time%250A%2520%2520compared%2520to%2520state-of-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Pre-trained%20time%20series%20models%20for%20cross-domain%20Time%20series%0A%20%20analysis%20tasks&entry.906535625=Harshavardhan%20Kamarthi%20and%20B.%20Aditya%20Prakash&entry.1292438233=%20%20Large%20pre-trained%20models%20have%20been%20vital%20in%20recent%20advancements%20in%20domains%0Alike%20language%20and%20vision%2C%20making%20model%20training%20for%20individual%20downstream%20tasks%0Amore%20efficient%20and%20provide%20superior%20performance.%20However%2C%20tackling%20time-series%0Aanalysis%20tasks%20usually%20involves%20designing%20and%20training%20a%20separate%20model%20from%0Ascratch%20leveraging%20training%20data%20and%20domain%20expertise%20specific%20to%20the%20task.%20We%0Atackle%20a%20significant%20challenge%20for%20pre-training%20a%20foundational%20time-series%0Amodel%20from%20multi-domain%20time-series%20datasets%3A%20extracting%0A%20%20semantically%20useful%20tokenized%20inputs%20to%20the%20model%0A%20%20across%20heterogenous%20time-series%20from%20different%20domains.%20We%20propose%20Large%0APre-trained%20Time-series%20Models%20%28LPTM%29%20that%20introduces%20a%20novel%20method%20of%0A%5Ctextit%7Badaptive%20segmentation%7D%20that%20automatically%20identifies%20optimal%0Adataset-specific%0A%20%20segmentation%20strategy%20during%20pre-training.%20This%20enables%0A%20%20LPTM%20to%20perform%20similar%20to%20or%20better%20than%20domain-specific%20state-of-art%20model%0A%20%20when%20fine-tuned%20to%20different%20downstream%20time-series%20analysis%20tasks%20and%20under%0Azero-shot%20settings.%0A%20%20LPTM%20achieves%20superior%20forecasting%20and%20time-series%20classification%20results%0A%20%20taking%20up%20to%2040%25%20less%20data%20and%2050%25%20less%20training%20time%0A%20%20compared%20to%20state-of-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11413v2&entry.124074799=Read"},
{"title": "Unsupervised Object Localization in the Era of Self-Supervised ViTs: A\n  Survey", "author": "Oriane Sim\u00e9oni and \u00c9loi Zablocki and Spyros Gidaris and Gilles Puy and Patrick P\u00e9rez", "abstract": "  The recent enthusiasm for open-world vision systems show the high interest of\nthe community to perform perception tasks outside of the closed-vocabulary\nbenchmark setups which have been so popular until now. Being able to discover\nobjects in images/videos without knowing in advance what objects populate the\ndataset is an exciting prospect. But how to find objects without knowing\nanything about them? Recent works show that it is possible to perform\nclass-agnostic unsupervised object localization by exploiting self-supervised\npre-trained features. We propose here a survey of unsupervised object\nlocalization methods that discover objects in images without requiring any\nmanual annotation in the era of self-supervised ViTs. We gather links of\ndiscussed methods in the repository\nhttps://github.com/valeoai/Awesome-Unsupervised-Object-Localization.\n", "link": "http://arxiv.org/abs/2310.12904v2", "date": "2024-07-11", "relevancy": 2.6048, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5299}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5295}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Object%20Localization%20in%20the%20Era%20of%20Self-Supervised%20ViTs%3A%20A%0A%20%20Survey&body=Title%3A%20Unsupervised%20Object%20Localization%20in%20the%20Era%20of%20Self-Supervised%20ViTs%3A%20A%0A%20%20Survey%0AAuthor%3A%20Oriane%20Sim%C3%A9oni%20and%20%C3%89loi%20Zablocki%20and%20Spyros%20Gidaris%20and%20Gilles%20Puy%20and%20Patrick%20P%C3%A9rez%0AAbstract%3A%20%20%20The%20recent%20enthusiasm%20for%20open-world%20vision%20systems%20show%20the%20high%20interest%20of%0Athe%20community%20to%20perform%20perception%20tasks%20outside%20of%20the%20closed-vocabulary%0Abenchmark%20setups%20which%20have%20been%20so%20popular%20until%20now.%20Being%20able%20to%20discover%0Aobjects%20in%20images/videos%20without%20knowing%20in%20advance%20what%20objects%20populate%20the%0Adataset%20is%20an%20exciting%20prospect.%20But%20how%20to%20find%20objects%20without%20knowing%0Aanything%20about%20them%3F%20Recent%20works%20show%20that%20it%20is%20possible%20to%20perform%0Aclass-agnostic%20unsupervised%20object%20localization%20by%20exploiting%20self-supervised%0Apre-trained%20features.%20We%20propose%20here%20a%20survey%20of%20unsupervised%20object%0Alocalization%20methods%20that%20discover%20objects%20in%20images%20without%20requiring%20any%0Amanual%20annotation%20in%20the%20era%20of%20self-supervised%20ViTs.%20We%20gather%20links%20of%0Adiscussed%20methods%20in%20the%20repository%0Ahttps%3A//github.com/valeoai/Awesome-Unsupervised-Object-Localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Object%2520Localization%2520in%2520the%2520Era%2520of%2520Self-Supervised%2520ViTs%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DOriane%2520Sim%25C3%25A9oni%2520and%2520%25C3%2589loi%2520Zablocki%2520and%2520Spyros%2520Gidaris%2520and%2520Gilles%2520Puy%2520and%2520Patrick%2520P%25C3%25A9rez%26entry.1292438233%3D%2520%2520The%2520recent%2520enthusiasm%2520for%2520open-world%2520vision%2520systems%2520show%2520the%2520high%2520interest%2520of%250Athe%2520community%2520to%2520perform%2520perception%2520tasks%2520outside%2520of%2520the%2520closed-vocabulary%250Abenchmark%2520setups%2520which%2520have%2520been%2520so%2520popular%2520until%2520now.%2520Being%2520able%2520to%2520discover%250Aobjects%2520in%2520images/videos%2520without%2520knowing%2520in%2520advance%2520what%2520objects%2520populate%2520the%250Adataset%2520is%2520an%2520exciting%2520prospect.%2520But%2520how%2520to%2520find%2520objects%2520without%2520knowing%250Aanything%2520about%2520them%253F%2520Recent%2520works%2520show%2520that%2520it%2520is%2520possible%2520to%2520perform%250Aclass-agnostic%2520unsupervised%2520object%2520localization%2520by%2520exploiting%2520self-supervised%250Apre-trained%2520features.%2520We%2520propose%2520here%2520a%2520survey%2520of%2520unsupervised%2520object%250Alocalization%2520methods%2520that%2520discover%2520objects%2520in%2520images%2520without%2520requiring%2520any%250Amanual%2520annotation%2520in%2520the%2520era%2520of%2520self-supervised%2520ViTs.%2520We%2520gather%2520links%2520of%250Adiscussed%2520methods%2520in%2520the%2520repository%250Ahttps%253A//github.com/valeoai/Awesome-Unsupervised-Object-Localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Object%20Localization%20in%20the%20Era%20of%20Self-Supervised%20ViTs%3A%20A%0A%20%20Survey&entry.906535625=Oriane%20Sim%C3%A9oni%20and%20%C3%89loi%20Zablocki%20and%20Spyros%20Gidaris%20and%20Gilles%20Puy%20and%20Patrick%20P%C3%A9rez&entry.1292438233=%20%20The%20recent%20enthusiasm%20for%20open-world%20vision%20systems%20show%20the%20high%20interest%20of%0Athe%20community%20to%20perform%20perception%20tasks%20outside%20of%20the%20closed-vocabulary%0Abenchmark%20setups%20which%20have%20been%20so%20popular%20until%20now.%20Being%20able%20to%20discover%0Aobjects%20in%20images/videos%20without%20knowing%20in%20advance%20what%20objects%20populate%20the%0Adataset%20is%20an%20exciting%20prospect.%20But%20how%20to%20find%20objects%20without%20knowing%0Aanything%20about%20them%3F%20Recent%20works%20show%20that%20it%20is%20possible%20to%20perform%0Aclass-agnostic%20unsupervised%20object%20localization%20by%20exploiting%20self-supervised%0Apre-trained%20features.%20We%20propose%20here%20a%20survey%20of%20unsupervised%20object%0Alocalization%20methods%20that%20discover%20objects%20in%20images%20without%20requiring%20any%0Amanual%20annotation%20in%20the%20era%20of%20self-supervised%20ViTs.%20We%20gather%20links%20of%0Adiscussed%20methods%20in%20the%20repository%0Ahttps%3A//github.com/valeoai/Awesome-Unsupervised-Object-Localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12904v2&entry.124074799=Read"},
{"title": "Ethics of Generating Synthetic MRI Vocal Tract Views from the Face", "author": "Muhammad Suhaib Shahid and Gleb E. Yakubov and Andrew P. French", "abstract": "  Forming oral models capable of understanding the complete dynamics of the\noral cavity is vital across research areas such as speech correction, designing\nfoods for the aging population, and dentistry. Magnetic resonance imaging (MRI)\ntechnologies, capable of capturing oral data essential for creating such\ndetailed representations, offer a powerful tool for illustrating articulatory\ndynamics. However, its real-time application is hindered by expense and\nexpertise requirements. Ever advancing generative AI approaches present\nthemselves as a way to address this barrier by leveraging multi-modal\napproaches for generating pseudo-MRI views. Nonetheless, this immediately\nsparks ethical concerns regarding the utilisation of a technology with the\ncapability to produce MRIs from facial observations.\n  This paper explores the ethical implications of external-to-internal\ncorrelation modeling (E2ICM). E2ICM utilises facial movements to infer internal\nconfigurations and provides a cost-effective supporting technology for MRI. In\nthis preliminary work, we employ Pix2PixGAN to generate pseudo-MRI views from\nexternal articulatory data, demonstrating the feasibility of this approach.\nEthical considerations concerning privacy, consent, and potential misuse, which\nare fundamental to our examination of this innovative methodology, are\ndiscussed as a result of this experimentation.\n", "link": "http://arxiv.org/abs/2407.08403v1", "date": "2024-07-11", "relevancy": 2.6045, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5546}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.504}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ethics%20of%20Generating%20Synthetic%20MRI%20Vocal%20Tract%20Views%20from%20the%20Face&body=Title%3A%20Ethics%20of%20Generating%20Synthetic%20MRI%20Vocal%20Tract%20Views%20from%20the%20Face%0AAuthor%3A%20Muhammad%20Suhaib%20Shahid%20and%20Gleb%20E.%20Yakubov%20and%20Andrew%20P.%20French%0AAbstract%3A%20%20%20Forming%20oral%20models%20capable%20of%20understanding%20the%20complete%20dynamics%20of%20the%0Aoral%20cavity%20is%20vital%20across%20research%20areas%20such%20as%20speech%20correction%2C%20designing%0Afoods%20for%20the%20aging%20population%2C%20and%20dentistry.%20Magnetic%20resonance%20imaging%20%28MRI%29%0Atechnologies%2C%20capable%20of%20capturing%20oral%20data%20essential%20for%20creating%20such%0Adetailed%20representations%2C%20offer%20a%20powerful%20tool%20for%20illustrating%20articulatory%0Adynamics.%20However%2C%20its%20real-time%20application%20is%20hindered%20by%20expense%20and%0Aexpertise%20requirements.%20Ever%20advancing%20generative%20AI%20approaches%20present%0Athemselves%20as%20a%20way%20to%20address%20this%20barrier%20by%20leveraging%20multi-modal%0Aapproaches%20for%20generating%20pseudo-MRI%20views.%20Nonetheless%2C%20this%20immediately%0Asparks%20ethical%20concerns%20regarding%20the%20utilisation%20of%20a%20technology%20with%20the%0Acapability%20to%20produce%20MRIs%20from%20facial%20observations.%0A%20%20This%20paper%20explores%20the%20ethical%20implications%20of%20external-to-internal%0Acorrelation%20modeling%20%28E2ICM%29.%20E2ICM%20utilises%20facial%20movements%20to%20infer%20internal%0Aconfigurations%20and%20provides%20a%20cost-effective%20supporting%20technology%20for%20MRI.%20In%0Athis%20preliminary%20work%2C%20we%20employ%20Pix2PixGAN%20to%20generate%20pseudo-MRI%20views%20from%0Aexternal%20articulatory%20data%2C%20demonstrating%20the%20feasibility%20of%20this%20approach.%0AEthical%20considerations%20concerning%20privacy%2C%20consent%2C%20and%20potential%20misuse%2C%20which%0Aare%20fundamental%20to%20our%20examination%20of%20this%20innovative%20methodology%2C%20are%0Adiscussed%20as%20a%20result%20of%20this%20experimentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEthics%2520of%2520Generating%2520Synthetic%2520MRI%2520Vocal%2520Tract%2520Views%2520from%2520the%2520Face%26entry.906535625%3DMuhammad%2520Suhaib%2520Shahid%2520and%2520Gleb%2520E.%2520Yakubov%2520and%2520Andrew%2520P.%2520French%26entry.1292438233%3D%2520%2520Forming%2520oral%2520models%2520capable%2520of%2520understanding%2520the%2520complete%2520dynamics%2520of%2520the%250Aoral%2520cavity%2520is%2520vital%2520across%2520research%2520areas%2520such%2520as%2520speech%2520correction%252C%2520designing%250Afoods%2520for%2520the%2520aging%2520population%252C%2520and%2520dentistry.%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%250Atechnologies%252C%2520capable%2520of%2520capturing%2520oral%2520data%2520essential%2520for%2520creating%2520such%250Adetailed%2520representations%252C%2520offer%2520a%2520powerful%2520tool%2520for%2520illustrating%2520articulatory%250Adynamics.%2520However%252C%2520its%2520real-time%2520application%2520is%2520hindered%2520by%2520expense%2520and%250Aexpertise%2520requirements.%2520Ever%2520advancing%2520generative%2520AI%2520approaches%2520present%250Athemselves%2520as%2520a%2520way%2520to%2520address%2520this%2520barrier%2520by%2520leveraging%2520multi-modal%250Aapproaches%2520for%2520generating%2520pseudo-MRI%2520views.%2520Nonetheless%252C%2520this%2520immediately%250Asparks%2520ethical%2520concerns%2520regarding%2520the%2520utilisation%2520of%2520a%2520technology%2520with%2520the%250Acapability%2520to%2520produce%2520MRIs%2520from%2520facial%2520observations.%250A%2520%2520This%2520paper%2520explores%2520the%2520ethical%2520implications%2520of%2520external-to-internal%250Acorrelation%2520modeling%2520%2528E2ICM%2529.%2520E2ICM%2520utilises%2520facial%2520movements%2520to%2520infer%2520internal%250Aconfigurations%2520and%2520provides%2520a%2520cost-effective%2520supporting%2520technology%2520for%2520MRI.%2520In%250Athis%2520preliminary%2520work%252C%2520we%2520employ%2520Pix2PixGAN%2520to%2520generate%2520pseudo-MRI%2520views%2520from%250Aexternal%2520articulatory%2520data%252C%2520demonstrating%2520the%2520feasibility%2520of%2520this%2520approach.%250AEthical%2520considerations%2520concerning%2520privacy%252C%2520consent%252C%2520and%2520potential%2520misuse%252C%2520which%250Aare%2520fundamental%2520to%2520our%2520examination%2520of%2520this%2520innovative%2520methodology%252C%2520are%250Adiscussed%2520as%2520a%2520result%2520of%2520this%2520experimentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ethics%20of%20Generating%20Synthetic%20MRI%20Vocal%20Tract%20Views%20from%20the%20Face&entry.906535625=Muhammad%20Suhaib%20Shahid%20and%20Gleb%20E.%20Yakubov%20and%20Andrew%20P.%20French&entry.1292438233=%20%20Forming%20oral%20models%20capable%20of%20understanding%20the%20complete%20dynamics%20of%20the%0Aoral%20cavity%20is%20vital%20across%20research%20areas%20such%20as%20speech%20correction%2C%20designing%0Afoods%20for%20the%20aging%20population%2C%20and%20dentistry.%20Magnetic%20resonance%20imaging%20%28MRI%29%0Atechnologies%2C%20capable%20of%20capturing%20oral%20data%20essential%20for%20creating%20such%0Adetailed%20representations%2C%20offer%20a%20powerful%20tool%20for%20illustrating%20articulatory%0Adynamics.%20However%2C%20its%20real-time%20application%20is%20hindered%20by%20expense%20and%0Aexpertise%20requirements.%20Ever%20advancing%20generative%20AI%20approaches%20present%0Athemselves%20as%20a%20way%20to%20address%20this%20barrier%20by%20leveraging%20multi-modal%0Aapproaches%20for%20generating%20pseudo-MRI%20views.%20Nonetheless%2C%20this%20immediately%0Asparks%20ethical%20concerns%20regarding%20the%20utilisation%20of%20a%20technology%20with%20the%0Acapability%20to%20produce%20MRIs%20from%20facial%20observations.%0A%20%20This%20paper%20explores%20the%20ethical%20implications%20of%20external-to-internal%0Acorrelation%20modeling%20%28E2ICM%29.%20E2ICM%20utilises%20facial%20movements%20to%20infer%20internal%0Aconfigurations%20and%20provides%20a%20cost-effective%20supporting%20technology%20for%20MRI.%20In%0Athis%20preliminary%20work%2C%20we%20employ%20Pix2PixGAN%20to%20generate%20pseudo-MRI%20views%20from%0Aexternal%20articulatory%20data%2C%20demonstrating%20the%20feasibility%20of%20this%20approach.%0AEthical%20considerations%20concerning%20privacy%2C%20consent%2C%20and%20potential%20misuse%2C%20which%0Aare%20fundamental%20to%20our%20examination%20of%20this%20innovative%20methodology%2C%20are%0Adiscussed%20as%20a%20result%20of%20this%20experimentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08403v1&entry.124074799=Read"},
{"title": "Projecting Points to Axes: Oriented Object Detection via Point-Axis\n  Representation", "author": "Zeyang Zhao and Qilong Xue and Yuhang He and Yifan Bai and Xing Wei and Yihong Gong", "abstract": "  This paper introduces the point-axis representation for oriented object\ndetection, emphasizing its flexibility and geometrically intuitive nature with\ntwo key components: points and axes. 1) Points delineate the spatial extent and\ncontours of objects, providing detailed shape descriptions. 2) Axes define the\nprimary directionalities of objects, providing essential orientation cues\ncrucial for precise detection. The point-axis representation decouples location\nand rotation, addressing the loss discontinuity issues commonly encountered in\ntraditional bounding box-based approaches. For effective optimization without\nintroducing additional annotations, we propose the max-projection loss to\nsupervise point set learning and the cross-axis loss for robust axis\nrepresentation learning. Further, leveraging this representation, we present\nthe Oriented DETR model, seamlessly integrating the DETR framework for precise\npoint-axis prediction and end-to-end detection. Experimental results\ndemonstrate significant performance improvements in oriented object detection\ntasks.\n", "link": "http://arxiv.org/abs/2407.08489v1", "date": "2024-07-11", "relevancy": 2.6017, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5541}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5037}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Projecting%20Points%20to%20Axes%3A%20Oriented%20Object%20Detection%20via%20Point-Axis%0A%20%20Representation&body=Title%3A%20Projecting%20Points%20to%20Axes%3A%20Oriented%20Object%20Detection%20via%20Point-Axis%0A%20%20Representation%0AAuthor%3A%20Zeyang%20Zhao%20and%20Qilong%20Xue%20and%20Yuhang%20He%20and%20Yifan%20Bai%20and%20Xing%20Wei%20and%20Yihong%20Gong%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20point-axis%20representation%20for%20oriented%20object%0Adetection%2C%20emphasizing%20its%20flexibility%20and%20geometrically%20intuitive%20nature%20with%0Atwo%20key%20components%3A%20points%20and%20axes.%201%29%20Points%20delineate%20the%20spatial%20extent%20and%0Acontours%20of%20objects%2C%20providing%20detailed%20shape%20descriptions.%202%29%20Axes%20define%20the%0Aprimary%20directionalities%20of%20objects%2C%20providing%20essential%20orientation%20cues%0Acrucial%20for%20precise%20detection.%20The%20point-axis%20representation%20decouples%20location%0Aand%20rotation%2C%20addressing%20the%20loss%20discontinuity%20issues%20commonly%20encountered%20in%0Atraditional%20bounding%20box-based%20approaches.%20For%20effective%20optimization%20without%0Aintroducing%20additional%20annotations%2C%20we%20propose%20the%20max-projection%20loss%20to%0Asupervise%20point%20set%20learning%20and%20the%20cross-axis%20loss%20for%20robust%20axis%0Arepresentation%20learning.%20Further%2C%20leveraging%20this%20representation%2C%20we%20present%0Athe%20Oriented%20DETR%20model%2C%20seamlessly%20integrating%20the%20DETR%20framework%20for%20precise%0Apoint-axis%20prediction%20and%20end-to-end%20detection.%20Experimental%20results%0Ademonstrate%20significant%20performance%20improvements%20in%20oriented%20object%20detection%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProjecting%2520Points%2520to%2520Axes%253A%2520Oriented%2520Object%2520Detection%2520via%2520Point-Axis%250A%2520%2520Representation%26entry.906535625%3DZeyang%2520Zhao%2520and%2520Qilong%2520Xue%2520and%2520Yuhang%2520He%2520and%2520Yifan%2520Bai%2520and%2520Xing%2520Wei%2520and%2520Yihong%2520Gong%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520point-axis%2520representation%2520for%2520oriented%2520object%250Adetection%252C%2520emphasizing%2520its%2520flexibility%2520and%2520geometrically%2520intuitive%2520nature%2520with%250Atwo%2520key%2520components%253A%2520points%2520and%2520axes.%25201%2529%2520Points%2520delineate%2520the%2520spatial%2520extent%2520and%250Acontours%2520of%2520objects%252C%2520providing%2520detailed%2520shape%2520descriptions.%25202%2529%2520Axes%2520define%2520the%250Aprimary%2520directionalities%2520of%2520objects%252C%2520providing%2520essential%2520orientation%2520cues%250Acrucial%2520for%2520precise%2520detection.%2520The%2520point-axis%2520representation%2520decouples%2520location%250Aand%2520rotation%252C%2520addressing%2520the%2520loss%2520discontinuity%2520issues%2520commonly%2520encountered%2520in%250Atraditional%2520bounding%2520box-based%2520approaches.%2520For%2520effective%2520optimization%2520without%250Aintroducing%2520additional%2520annotations%252C%2520we%2520propose%2520the%2520max-projection%2520loss%2520to%250Asupervise%2520point%2520set%2520learning%2520and%2520the%2520cross-axis%2520loss%2520for%2520robust%2520axis%250Arepresentation%2520learning.%2520Further%252C%2520leveraging%2520this%2520representation%252C%2520we%2520present%250Athe%2520Oriented%2520DETR%2520model%252C%2520seamlessly%2520integrating%2520the%2520DETR%2520framework%2520for%2520precise%250Apoint-axis%2520prediction%2520and%2520end-to-end%2520detection.%2520Experimental%2520results%250Ademonstrate%2520significant%2520performance%2520improvements%2520in%2520oriented%2520object%2520detection%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Projecting%20Points%20to%20Axes%3A%20Oriented%20Object%20Detection%20via%20Point-Axis%0A%20%20Representation&entry.906535625=Zeyang%20Zhao%20and%20Qilong%20Xue%20and%20Yuhang%20He%20and%20Yifan%20Bai%20and%20Xing%20Wei%20and%20Yihong%20Gong&entry.1292438233=%20%20This%20paper%20introduces%20the%20point-axis%20representation%20for%20oriented%20object%0Adetection%2C%20emphasizing%20its%20flexibility%20and%20geometrically%20intuitive%20nature%20with%0Atwo%20key%20components%3A%20points%20and%20axes.%201%29%20Points%20delineate%20the%20spatial%20extent%20and%0Acontours%20of%20objects%2C%20providing%20detailed%20shape%20descriptions.%202%29%20Axes%20define%20the%0Aprimary%20directionalities%20of%20objects%2C%20providing%20essential%20orientation%20cues%0Acrucial%20for%20precise%20detection.%20The%20point-axis%20representation%20decouples%20location%0Aand%20rotation%2C%20addressing%20the%20loss%20discontinuity%20issues%20commonly%20encountered%20in%0Atraditional%20bounding%20box-based%20approaches.%20For%20effective%20optimization%20without%0Aintroducing%20additional%20annotations%2C%20we%20propose%20the%20max-projection%20loss%20to%0Asupervise%20point%20set%20learning%20and%20the%20cross-axis%20loss%20for%20robust%20axis%0Arepresentation%20learning.%20Further%2C%20leveraging%20this%20representation%2C%20we%20present%0Athe%20Oriented%20DETR%20model%2C%20seamlessly%20integrating%20the%20DETR%20framework%20for%20precise%0Apoint-axis%20prediction%20and%20end-to-end%20detection.%20Experimental%20results%0Ademonstrate%20significant%20performance%20improvements%20in%20oriented%20object%20detection%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08489v1&entry.124074799=Read"},
{"title": "Exemplar-free Continual Representation Learning via Learnable Drift\n  Compensation", "author": "Alex Gomez-Villa and Dipam Goswami and Kai Wang and Andrew D. Bagdanov and Bartlomiej Twardowski and Joost van de Weijer", "abstract": "  Exemplar-free class-incremental learning using a backbone trained from\nscratch and starting from a small first task presents a significant challenge\nfor continual representation learning. Prototype-based approaches, when\ncontinually updated, face the critical issue of semantic drift due to which the\nold class prototypes drift to different positions in the new feature space.\nThrough an analysis of prototype-based continual learning, we show that\nforgetting is not due to diminished discriminative power of the feature\nextractor, and can potentially be corrected by drift compensation. To address\nthis, we propose Learnable Drift Compensation (LDC), which can effectively\nmitigate drift in any moving backbone, whether supervised or unsupervised. LDC\nis fast and straightforward to integrate on top of existing continual learning\napproaches. Furthermore, we showcase how LDC can be applied in combination with\nself-supervised CL methods, resulting in the first exemplar-free\nsemi-supervised continual learning approach. We achieve state-of-the-art\nperformance in both supervised and semi-supervised settings across multiple\ndatasets. Code is available at \\url{https://github.com/alviur/ldc}.\n", "link": "http://arxiv.org/abs/2407.08536v1", "date": "2024-07-11", "relevancy": 2.5996, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5344}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5296}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exemplar-free%20Continual%20Representation%20Learning%20via%20Learnable%20Drift%0A%20%20Compensation&body=Title%3A%20Exemplar-free%20Continual%20Representation%20Learning%20via%20Learnable%20Drift%0A%20%20Compensation%0AAuthor%3A%20Alex%20Gomez-Villa%20and%20Dipam%20Goswami%20and%20Kai%20Wang%20and%20Andrew%20D.%20Bagdanov%20and%20Bartlomiej%20Twardowski%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20%20%20Exemplar-free%20class-incremental%20learning%20using%20a%20backbone%20trained%20from%0Ascratch%20and%20starting%20from%20a%20small%20first%20task%20presents%20a%20significant%20challenge%0Afor%20continual%20representation%20learning.%20Prototype-based%20approaches%2C%20when%0Acontinually%20updated%2C%20face%20the%20critical%20issue%20of%20semantic%20drift%20due%20to%20which%20the%0Aold%20class%20prototypes%20drift%20to%20different%20positions%20in%20the%20new%20feature%20space.%0AThrough%20an%20analysis%20of%20prototype-based%20continual%20learning%2C%20we%20show%20that%0Aforgetting%20is%20not%20due%20to%20diminished%20discriminative%20power%20of%20the%20feature%0Aextractor%2C%20and%20can%20potentially%20be%20corrected%20by%20drift%20compensation.%20To%20address%0Athis%2C%20we%20propose%20Learnable%20Drift%20Compensation%20%28LDC%29%2C%20which%20can%20effectively%0Amitigate%20drift%20in%20any%20moving%20backbone%2C%20whether%20supervised%20or%20unsupervised.%20LDC%0Ais%20fast%20and%20straightforward%20to%20integrate%20on%20top%20of%20existing%20continual%20learning%0Aapproaches.%20Furthermore%2C%20we%20showcase%20how%20LDC%20can%20be%20applied%20in%20combination%20with%0Aself-supervised%20CL%20methods%2C%20resulting%20in%20the%20first%20exemplar-free%0Asemi-supervised%20continual%20learning%20approach.%20We%20achieve%20state-of-the-art%0Aperformance%20in%20both%20supervised%20and%20semi-supervised%20settings%20across%20multiple%0Adatasets.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/alviur/ldc%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExemplar-free%2520Continual%2520Representation%2520Learning%2520via%2520Learnable%2520Drift%250A%2520%2520Compensation%26entry.906535625%3DAlex%2520Gomez-Villa%2520and%2520Dipam%2520Goswami%2520and%2520Kai%2520Wang%2520and%2520Andrew%2520D.%2520Bagdanov%2520and%2520Bartlomiej%2520Twardowski%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3D%2520%2520Exemplar-free%2520class-incremental%2520learning%2520using%2520a%2520backbone%2520trained%2520from%250Ascratch%2520and%2520starting%2520from%2520a%2520small%2520first%2520task%2520presents%2520a%2520significant%2520challenge%250Afor%2520continual%2520representation%2520learning.%2520Prototype-based%2520approaches%252C%2520when%250Acontinually%2520updated%252C%2520face%2520the%2520critical%2520issue%2520of%2520semantic%2520drift%2520due%2520to%2520which%2520the%250Aold%2520class%2520prototypes%2520drift%2520to%2520different%2520positions%2520in%2520the%2520new%2520feature%2520space.%250AThrough%2520an%2520analysis%2520of%2520prototype-based%2520continual%2520learning%252C%2520we%2520show%2520that%250Aforgetting%2520is%2520not%2520due%2520to%2520diminished%2520discriminative%2520power%2520of%2520the%2520feature%250Aextractor%252C%2520and%2520can%2520potentially%2520be%2520corrected%2520by%2520drift%2520compensation.%2520To%2520address%250Athis%252C%2520we%2520propose%2520Learnable%2520Drift%2520Compensation%2520%2528LDC%2529%252C%2520which%2520can%2520effectively%250Amitigate%2520drift%2520in%2520any%2520moving%2520backbone%252C%2520whether%2520supervised%2520or%2520unsupervised.%2520LDC%250Ais%2520fast%2520and%2520straightforward%2520to%2520integrate%2520on%2520top%2520of%2520existing%2520continual%2520learning%250Aapproaches.%2520Furthermore%252C%2520we%2520showcase%2520how%2520LDC%2520can%2520be%2520applied%2520in%2520combination%2520with%250Aself-supervised%2520CL%2520methods%252C%2520resulting%2520in%2520the%2520first%2520exemplar-free%250Asemi-supervised%2520continual%2520learning%2520approach.%2520We%2520achieve%2520state-of-the-art%250Aperformance%2520in%2520both%2520supervised%2520and%2520semi-supervised%2520settings%2520across%2520multiple%250Adatasets.%2520Code%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/alviur/ldc%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exemplar-free%20Continual%20Representation%20Learning%20via%20Learnable%20Drift%0A%20%20Compensation&entry.906535625=Alex%20Gomez-Villa%20and%20Dipam%20Goswami%20and%20Kai%20Wang%20and%20Andrew%20D.%20Bagdanov%20and%20Bartlomiej%20Twardowski%20and%20Joost%20van%20de%20Weijer&entry.1292438233=%20%20Exemplar-free%20class-incremental%20learning%20using%20a%20backbone%20trained%20from%0Ascratch%20and%20starting%20from%20a%20small%20first%20task%20presents%20a%20significant%20challenge%0Afor%20continual%20representation%20learning.%20Prototype-based%20approaches%2C%20when%0Acontinually%20updated%2C%20face%20the%20critical%20issue%20of%20semantic%20drift%20due%20to%20which%20the%0Aold%20class%20prototypes%20drift%20to%20different%20positions%20in%20the%20new%20feature%20space.%0AThrough%20an%20analysis%20of%20prototype-based%20continual%20learning%2C%20we%20show%20that%0Aforgetting%20is%20not%20due%20to%20diminished%20discriminative%20power%20of%20the%20feature%0Aextractor%2C%20and%20can%20potentially%20be%20corrected%20by%20drift%20compensation.%20To%20address%0Athis%2C%20we%20propose%20Learnable%20Drift%20Compensation%20%28LDC%29%2C%20which%20can%20effectively%0Amitigate%20drift%20in%20any%20moving%20backbone%2C%20whether%20supervised%20or%20unsupervised.%20LDC%0Ais%20fast%20and%20straightforward%20to%20integrate%20on%20top%20of%20existing%20continual%20learning%0Aapproaches.%20Furthermore%2C%20we%20showcase%20how%20LDC%20can%20be%20applied%20in%20combination%20with%0Aself-supervised%20CL%20methods%2C%20resulting%20in%20the%20first%20exemplar-free%0Asemi-supervised%20continual%20learning%20approach.%20We%20achieve%20state-of-the-art%0Aperformance%20in%20both%20supervised%20and%20semi-supervised%20settings%20across%20multiple%0Adatasets.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/alviur/ldc%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08536v1&entry.124074799=Read"},
{"title": "SLoRD: Structural Low-Rank Descriptors for Shape Consistency in\n  Vertebrae Segmentation", "author": "Xin You and Yixin Lou and Minghui Zhang and Chuyan Zhang and Jie Yang and Yun Gu", "abstract": "  Automatic and precise segmentation of vertebrae from CT images is crucial for\nvarious clinical applications. However, due to a lack of explicit and strict\nconstraints, existing methods especially for single-stage methods, still suffer\nfrom the challenge of intra-vertebrae segmentation inconsistency, which refers\nto multiple label predictions inside a singular vertebra. For multi-stage\nmethods, vertebrae detection serving as the first step, is affected by the\npathology and mental implants. Thus, incorrect detections cause biased patches\nbefore segmentation, then lead to inconsistent labeling and segmentation. In\nour work, motivated by the perspective of instance segmentation, we try to\nlabel individual and complete binary masks to address this limitation.\nSpecifically, a contour-based network is proposed based on Structural Low-Rank\nDescriptors for shape consistency, termed SLoRD. These contour descriptors are\nacquired in a data-driven manner in advance. For a more precise representation\nof contour descriptors, we adopt the spherical coordinate system and devise the\nspherical centroid. Besides, the contour loss is designed to impose explicit\nconsistency constraints, facilitating regressed contour points close to\nvertebral boundaries. Quantitative and qualitative evaluations on VerSe 2019\ndemonstrate the superior performance of our framework over other single-stage\nand multi-stage state-of-the-art (SOTA) methods.\n", "link": "http://arxiv.org/abs/2407.08555v1", "date": "2024-07-11", "relevancy": 2.5763, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5281}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLoRD%3A%20Structural%20Low-Rank%20Descriptors%20for%20Shape%20Consistency%20in%0A%20%20Vertebrae%20Segmentation&body=Title%3A%20SLoRD%3A%20Structural%20Low-Rank%20Descriptors%20for%20Shape%20Consistency%20in%0A%20%20Vertebrae%20Segmentation%0AAuthor%3A%20Xin%20You%20and%20Yixin%20Lou%20and%20Minghui%20Zhang%20and%20Chuyan%20Zhang%20and%20Jie%20Yang%20and%20Yun%20Gu%0AAbstract%3A%20%20%20Automatic%20and%20precise%20segmentation%20of%20vertebrae%20from%20CT%20images%20is%20crucial%20for%0Avarious%20clinical%20applications.%20However%2C%20due%20to%20a%20lack%20of%20explicit%20and%20strict%0Aconstraints%2C%20existing%20methods%20especially%20for%20single-stage%20methods%2C%20still%20suffer%0Afrom%20the%20challenge%20of%20intra-vertebrae%20segmentation%20inconsistency%2C%20which%20refers%0Ato%20multiple%20label%20predictions%20inside%20a%20singular%20vertebra.%20For%20multi-stage%0Amethods%2C%20vertebrae%20detection%20serving%20as%20the%20first%20step%2C%20is%20affected%20by%20the%0Apathology%20and%20mental%20implants.%20Thus%2C%20incorrect%20detections%20cause%20biased%20patches%0Abefore%20segmentation%2C%20then%20lead%20to%20inconsistent%20labeling%20and%20segmentation.%20In%0Aour%20work%2C%20motivated%20by%20the%20perspective%20of%20instance%20segmentation%2C%20we%20try%20to%0Alabel%20individual%20and%20complete%20binary%20masks%20to%20address%20this%20limitation.%0ASpecifically%2C%20a%20contour-based%20network%20is%20proposed%20based%20on%20Structural%20Low-Rank%0ADescriptors%20for%20shape%20consistency%2C%20termed%20SLoRD.%20These%20contour%20descriptors%20are%0Aacquired%20in%20a%20data-driven%20manner%20in%20advance.%20For%20a%20more%20precise%20representation%0Aof%20contour%20descriptors%2C%20we%20adopt%20the%20spherical%20coordinate%20system%20and%20devise%20the%0Aspherical%20centroid.%20Besides%2C%20the%20contour%20loss%20is%20designed%20to%20impose%20explicit%0Aconsistency%20constraints%2C%20facilitating%20regressed%20contour%20points%20close%20to%0Avertebral%20boundaries.%20Quantitative%20and%20qualitative%20evaluations%20on%20VerSe%202019%0Ademonstrate%20the%20superior%20performance%20of%20our%20framework%20over%20other%20single-stage%0Aand%20multi-stage%20state-of-the-art%20%28SOTA%29%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLoRD%253A%2520Structural%2520Low-Rank%2520Descriptors%2520for%2520Shape%2520Consistency%2520in%250A%2520%2520Vertebrae%2520Segmentation%26entry.906535625%3DXin%2520You%2520and%2520Yixin%2520Lou%2520and%2520Minghui%2520Zhang%2520and%2520Chuyan%2520Zhang%2520and%2520Jie%2520Yang%2520and%2520Yun%2520Gu%26entry.1292438233%3D%2520%2520Automatic%2520and%2520precise%2520segmentation%2520of%2520vertebrae%2520from%2520CT%2520images%2520is%2520crucial%2520for%250Avarious%2520clinical%2520applications.%2520However%252C%2520due%2520to%2520a%2520lack%2520of%2520explicit%2520and%2520strict%250Aconstraints%252C%2520existing%2520methods%2520especially%2520for%2520single-stage%2520methods%252C%2520still%2520suffer%250Afrom%2520the%2520challenge%2520of%2520intra-vertebrae%2520segmentation%2520inconsistency%252C%2520which%2520refers%250Ato%2520multiple%2520label%2520predictions%2520inside%2520a%2520singular%2520vertebra.%2520For%2520multi-stage%250Amethods%252C%2520vertebrae%2520detection%2520serving%2520as%2520the%2520first%2520step%252C%2520is%2520affected%2520by%2520the%250Apathology%2520and%2520mental%2520implants.%2520Thus%252C%2520incorrect%2520detections%2520cause%2520biased%2520patches%250Abefore%2520segmentation%252C%2520then%2520lead%2520to%2520inconsistent%2520labeling%2520and%2520segmentation.%2520In%250Aour%2520work%252C%2520motivated%2520by%2520the%2520perspective%2520of%2520instance%2520segmentation%252C%2520we%2520try%2520to%250Alabel%2520individual%2520and%2520complete%2520binary%2520masks%2520to%2520address%2520this%2520limitation.%250ASpecifically%252C%2520a%2520contour-based%2520network%2520is%2520proposed%2520based%2520on%2520Structural%2520Low-Rank%250ADescriptors%2520for%2520shape%2520consistency%252C%2520termed%2520SLoRD.%2520These%2520contour%2520descriptors%2520are%250Aacquired%2520in%2520a%2520data-driven%2520manner%2520in%2520advance.%2520For%2520a%2520more%2520precise%2520representation%250Aof%2520contour%2520descriptors%252C%2520we%2520adopt%2520the%2520spherical%2520coordinate%2520system%2520and%2520devise%2520the%250Aspherical%2520centroid.%2520Besides%252C%2520the%2520contour%2520loss%2520is%2520designed%2520to%2520impose%2520explicit%250Aconsistency%2520constraints%252C%2520facilitating%2520regressed%2520contour%2520points%2520close%2520to%250Avertebral%2520boundaries.%2520Quantitative%2520and%2520qualitative%2520evaluations%2520on%2520VerSe%25202019%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520framework%2520over%2520other%2520single-stage%250Aand%2520multi-stage%2520state-of-the-art%2520%2528SOTA%2529%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLoRD%3A%20Structural%20Low-Rank%20Descriptors%20for%20Shape%20Consistency%20in%0A%20%20Vertebrae%20Segmentation&entry.906535625=Xin%20You%20and%20Yixin%20Lou%20and%20Minghui%20Zhang%20and%20Chuyan%20Zhang%20and%20Jie%20Yang%20and%20Yun%20Gu&entry.1292438233=%20%20Automatic%20and%20precise%20segmentation%20of%20vertebrae%20from%20CT%20images%20is%20crucial%20for%0Avarious%20clinical%20applications.%20However%2C%20due%20to%20a%20lack%20of%20explicit%20and%20strict%0Aconstraints%2C%20existing%20methods%20especially%20for%20single-stage%20methods%2C%20still%20suffer%0Afrom%20the%20challenge%20of%20intra-vertebrae%20segmentation%20inconsistency%2C%20which%20refers%0Ato%20multiple%20label%20predictions%20inside%20a%20singular%20vertebra.%20For%20multi-stage%0Amethods%2C%20vertebrae%20detection%20serving%20as%20the%20first%20step%2C%20is%20affected%20by%20the%0Apathology%20and%20mental%20implants.%20Thus%2C%20incorrect%20detections%20cause%20biased%20patches%0Abefore%20segmentation%2C%20then%20lead%20to%20inconsistent%20labeling%20and%20segmentation.%20In%0Aour%20work%2C%20motivated%20by%20the%20perspective%20of%20instance%20segmentation%2C%20we%20try%20to%0Alabel%20individual%20and%20complete%20binary%20masks%20to%20address%20this%20limitation.%0ASpecifically%2C%20a%20contour-based%20network%20is%20proposed%20based%20on%20Structural%20Low-Rank%0ADescriptors%20for%20shape%20consistency%2C%20termed%20SLoRD.%20These%20contour%20descriptors%20are%0Aacquired%20in%20a%20data-driven%20manner%20in%20advance.%20For%20a%20more%20precise%20representation%0Aof%20contour%20descriptors%2C%20we%20adopt%20the%20spherical%20coordinate%20system%20and%20devise%20the%0Aspherical%20centroid.%20Besides%2C%20the%20contour%20loss%20is%20designed%20to%20impose%20explicit%0Aconsistency%20constraints%2C%20facilitating%20regressed%20contour%20points%20close%20to%0Avertebral%20boundaries.%20Quantitative%20and%20qualitative%20evaluations%20on%20VerSe%202019%0Ademonstrate%20the%20superior%20performance%20of%20our%20framework%20over%20other%20single-stage%0Aand%20multi-stage%20state-of-the-art%20%28SOTA%29%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08555v1&entry.124074799=Read"},
{"title": "Generalized Low-Rank Matrix Completion Model with Overlapping Group\n  Error Representation", "author": "Wenjing Lu and Zhuang Fang and Liang Wu and Liming Tang and Hanxin Liu", "abstract": "  The low-rank matrix completion (LRMC) technology has achieved remarkable\nresults in low-level visual tasks. There is an underlying assumption that the\nreal-world matrix data is low-rank in LRMC. However, the real matrix data does\nnot satisfy the strict low-rank property, which undoubtedly present serious\nchallenges for the above-mentioned matrix recovery methods. Fortunately, there\nare feasible schemes that devise appropriate and effective priori\nrepresentations for describing the intrinsic information of real data. In this\npaper, we firstly model the matrix data ${\\bf{Y}}$ as the sum of a low-rank\napproximation component $\\bf{X}$ and an approximation error component\n$\\cal{E}$. This finer-grained data decomposition architecture enables each\ncomponent of information to be portrayed more precisely. Further, we design an\noverlapping group error representation (OGER) function to characterize the\nabove error structure and propose a generalized low-rank matrix completion\nmodel based on OGER. Specifically, the low-rank component describes the global\nstructure information of matrix data, while the OGER component not only\ncompensates for the approximation error between the low-rank component and the\nreal data but also better captures the local block sparsity information of\nmatrix data. Finally, we develop an alternating direction method of multipliers\n(ADMM) that integrates the majorization-minimization (MM) algorithm, which\nenables the efficient solution of the proposed model. And we analyze the\nconvergence of the algorithm in detail both theoretically and experimentally.\nIn addition, the results of numerical experiments demonstrate that the proposed\nmodel outperforms existing competing models in performance.\n", "link": "http://arxiv.org/abs/2407.08517v1", "date": "2024-07-11", "relevancy": 2.5762, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5339}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5211}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Low-Rank%20Matrix%20Completion%20Model%20with%20Overlapping%20Group%0A%20%20Error%20Representation&body=Title%3A%20Generalized%20Low-Rank%20Matrix%20Completion%20Model%20with%20Overlapping%20Group%0A%20%20Error%20Representation%0AAuthor%3A%20Wenjing%20Lu%20and%20Zhuang%20Fang%20and%20Liang%20Wu%20and%20Liming%20Tang%20and%20Hanxin%20Liu%0AAbstract%3A%20%20%20The%20low-rank%20matrix%20completion%20%28LRMC%29%20technology%20has%20achieved%20remarkable%0Aresults%20in%20low-level%20visual%20tasks.%20There%20is%20an%20underlying%20assumption%20that%20the%0Areal-world%20matrix%20data%20is%20low-rank%20in%20LRMC.%20However%2C%20the%20real%20matrix%20data%20does%0Anot%20satisfy%20the%20strict%20low-rank%20property%2C%20which%20undoubtedly%20present%20serious%0Achallenges%20for%20the%20above-mentioned%20matrix%20recovery%20methods.%20Fortunately%2C%20there%0Aare%20feasible%20schemes%20that%20devise%20appropriate%20and%20effective%20priori%0Arepresentations%20for%20describing%20the%20intrinsic%20information%20of%20real%20data.%20In%20this%0Apaper%2C%20we%20firstly%20model%20the%20matrix%20data%20%24%7B%5Cbf%7BY%7D%7D%24%20as%20the%20sum%20of%20a%20low-rank%0Aapproximation%20component%20%24%5Cbf%7BX%7D%24%20and%20an%20approximation%20error%20component%0A%24%5Ccal%7BE%7D%24.%20This%20finer-grained%20data%20decomposition%20architecture%20enables%20each%0Acomponent%20of%20information%20to%20be%20portrayed%20more%20precisely.%20Further%2C%20we%20design%20an%0Aoverlapping%20group%20error%20representation%20%28OGER%29%20function%20to%20characterize%20the%0Aabove%20error%20structure%20and%20propose%20a%20generalized%20low-rank%20matrix%20completion%0Amodel%20based%20on%20OGER.%20Specifically%2C%20the%20low-rank%20component%20describes%20the%20global%0Astructure%20information%20of%20matrix%20data%2C%20while%20the%20OGER%20component%20not%20only%0Acompensates%20for%20the%20approximation%20error%20between%20the%20low-rank%20component%20and%20the%0Areal%20data%20but%20also%20better%20captures%20the%20local%20block%20sparsity%20information%20of%0Amatrix%20data.%20Finally%2C%20we%20develop%20an%20alternating%20direction%20method%20of%20multipliers%0A%28ADMM%29%20that%20integrates%20the%20majorization-minimization%20%28MM%29%20algorithm%2C%20which%0Aenables%20the%20efficient%20solution%20of%20the%20proposed%20model.%20And%20we%20analyze%20the%0Aconvergence%20of%20the%20algorithm%20in%20detail%20both%20theoretically%20and%20experimentally.%0AIn%20addition%2C%20the%20results%20of%20numerical%20experiments%20demonstrate%20that%20the%20proposed%0Amodel%20outperforms%20existing%20competing%20models%20in%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Low-Rank%2520Matrix%2520Completion%2520Model%2520with%2520Overlapping%2520Group%250A%2520%2520Error%2520Representation%26entry.906535625%3DWenjing%2520Lu%2520and%2520Zhuang%2520Fang%2520and%2520Liang%2520Wu%2520and%2520Liming%2520Tang%2520and%2520Hanxin%2520Liu%26entry.1292438233%3D%2520%2520The%2520low-rank%2520matrix%2520completion%2520%2528LRMC%2529%2520technology%2520has%2520achieved%2520remarkable%250Aresults%2520in%2520low-level%2520visual%2520tasks.%2520There%2520is%2520an%2520underlying%2520assumption%2520that%2520the%250Areal-world%2520matrix%2520data%2520is%2520low-rank%2520in%2520LRMC.%2520However%252C%2520the%2520real%2520matrix%2520data%2520does%250Anot%2520satisfy%2520the%2520strict%2520low-rank%2520property%252C%2520which%2520undoubtedly%2520present%2520serious%250Achallenges%2520for%2520the%2520above-mentioned%2520matrix%2520recovery%2520methods.%2520Fortunately%252C%2520there%250Aare%2520feasible%2520schemes%2520that%2520devise%2520appropriate%2520and%2520effective%2520priori%250Arepresentations%2520for%2520describing%2520the%2520intrinsic%2520information%2520of%2520real%2520data.%2520In%2520this%250Apaper%252C%2520we%2520firstly%2520model%2520the%2520matrix%2520data%2520%2524%257B%255Cbf%257BY%257D%257D%2524%2520as%2520the%2520sum%2520of%2520a%2520low-rank%250Aapproximation%2520component%2520%2524%255Cbf%257BX%257D%2524%2520and%2520an%2520approximation%2520error%2520component%250A%2524%255Ccal%257BE%257D%2524.%2520This%2520finer-grained%2520data%2520decomposition%2520architecture%2520enables%2520each%250Acomponent%2520of%2520information%2520to%2520be%2520portrayed%2520more%2520precisely.%2520Further%252C%2520we%2520design%2520an%250Aoverlapping%2520group%2520error%2520representation%2520%2528OGER%2529%2520function%2520to%2520characterize%2520the%250Aabove%2520error%2520structure%2520and%2520propose%2520a%2520generalized%2520low-rank%2520matrix%2520completion%250Amodel%2520based%2520on%2520OGER.%2520Specifically%252C%2520the%2520low-rank%2520component%2520describes%2520the%2520global%250Astructure%2520information%2520of%2520matrix%2520data%252C%2520while%2520the%2520OGER%2520component%2520not%2520only%250Acompensates%2520for%2520the%2520approximation%2520error%2520between%2520the%2520low-rank%2520component%2520and%2520the%250Areal%2520data%2520but%2520also%2520better%2520captures%2520the%2520local%2520block%2520sparsity%2520information%2520of%250Amatrix%2520data.%2520Finally%252C%2520we%2520develop%2520an%2520alternating%2520direction%2520method%2520of%2520multipliers%250A%2528ADMM%2529%2520that%2520integrates%2520the%2520majorization-minimization%2520%2528MM%2529%2520algorithm%252C%2520which%250Aenables%2520the%2520efficient%2520solution%2520of%2520the%2520proposed%2520model.%2520And%2520we%2520analyze%2520the%250Aconvergence%2520of%2520the%2520algorithm%2520in%2520detail%2520both%2520theoretically%2520and%2520experimentally.%250AIn%2520addition%252C%2520the%2520results%2520of%2520numerical%2520experiments%2520demonstrate%2520that%2520the%2520proposed%250Amodel%2520outperforms%2520existing%2520competing%2520models%2520in%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Low-Rank%20Matrix%20Completion%20Model%20with%20Overlapping%20Group%0A%20%20Error%20Representation&entry.906535625=Wenjing%20Lu%20and%20Zhuang%20Fang%20and%20Liang%20Wu%20and%20Liming%20Tang%20and%20Hanxin%20Liu&entry.1292438233=%20%20The%20low-rank%20matrix%20completion%20%28LRMC%29%20technology%20has%20achieved%20remarkable%0Aresults%20in%20low-level%20visual%20tasks.%20There%20is%20an%20underlying%20assumption%20that%20the%0Areal-world%20matrix%20data%20is%20low-rank%20in%20LRMC.%20However%2C%20the%20real%20matrix%20data%20does%0Anot%20satisfy%20the%20strict%20low-rank%20property%2C%20which%20undoubtedly%20present%20serious%0Achallenges%20for%20the%20above-mentioned%20matrix%20recovery%20methods.%20Fortunately%2C%20there%0Aare%20feasible%20schemes%20that%20devise%20appropriate%20and%20effective%20priori%0Arepresentations%20for%20describing%20the%20intrinsic%20information%20of%20real%20data.%20In%20this%0Apaper%2C%20we%20firstly%20model%20the%20matrix%20data%20%24%7B%5Cbf%7BY%7D%7D%24%20as%20the%20sum%20of%20a%20low-rank%0Aapproximation%20component%20%24%5Cbf%7BX%7D%24%20and%20an%20approximation%20error%20component%0A%24%5Ccal%7BE%7D%24.%20This%20finer-grained%20data%20decomposition%20architecture%20enables%20each%0Acomponent%20of%20information%20to%20be%20portrayed%20more%20precisely.%20Further%2C%20we%20design%20an%0Aoverlapping%20group%20error%20representation%20%28OGER%29%20function%20to%20characterize%20the%0Aabove%20error%20structure%20and%20propose%20a%20generalized%20low-rank%20matrix%20completion%0Amodel%20based%20on%20OGER.%20Specifically%2C%20the%20low-rank%20component%20describes%20the%20global%0Astructure%20information%20of%20matrix%20data%2C%20while%20the%20OGER%20component%20not%20only%0Acompensates%20for%20the%20approximation%20error%20between%20the%20low-rank%20component%20and%20the%0Areal%20data%20but%20also%20better%20captures%20the%20local%20block%20sparsity%20information%20of%0Amatrix%20data.%20Finally%2C%20we%20develop%20an%20alternating%20direction%20method%20of%20multipliers%0A%28ADMM%29%20that%20integrates%20the%20majorization-minimization%20%28MM%29%20algorithm%2C%20which%0Aenables%20the%20efficient%20solution%20of%20the%20proposed%20model.%20And%20we%20analyze%20the%0Aconvergence%20of%20the%20algorithm%20in%20detail%20both%20theoretically%20and%20experimentally.%0AIn%20addition%2C%20the%20results%20of%20numerical%20experiments%20demonstrate%20that%20the%20proposed%0Amodel%20outperforms%20existing%20competing%20models%20in%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08517v1&entry.124074799=Read"},
{"title": "BenthicNet: A global compilation of seafloor images for deep learning\n  applications", "author": "Scott C. Lowe and Benjamin Misiuk and Isaac Xu and Shakhboz Abdulazizov and Amit R. Baroi and Alex C. Bastos and Merlin Best and Vicki Ferrini and Ariell Friedman and Deborah Hart and Ove Hoegh-Guldberg and Daniel Ierodiaconou and Julia Mackin-McLaughlin and Kathryn Markey and Pedro S. Menandro and Jacquomo Monk and Shreya Nemani and John O'Brien and Elizabeth Oh and Luba Y. Reshitnyk and Katleen Robert and Chris M. Roelfsema and Jessica A. Sameoto and Alexandre C. G. Schimel and Jordan A. Thomson and Brittany R. Wilson and Melisa C. Wong and Craig J. Brown and Thomas Trappenberg", "abstract": "  Advances in underwater imaging enable the collection of extensive seafloor\nimage datasets that are necessary for monitoring important benthic ecosystems.\nThe ability to collect seafloor imagery has outpaced our capacity to analyze\nit, hindering expedient mobilization of this crucial environmental information.\nRecent machine learning approaches provide opportunities to increase the\nefficiency with which seafloor image datasets are analyzed, yet large and\nconsistent datasets necessary to support development of such approaches are\nscarce. Here we present BenthicNet: a global compilation of seafloor imagery\ndesigned to support the training and evaluation of large-scale image\nrecognition models. An initial set of over 11.4 million images was collected\nand curated to represent a diversity of seafloor environments using a\nrepresentative subset of 1.3 million images. These are accompanied by 2.6\nmillion annotations translated to the CATAMI scheme, which span 190,000 of the\nimages. A large deep learning model was trained on this compilation and\npreliminary results suggest it has utility for automating large and small-scale\nimage analysis tasks. The compilation and model are made openly available for\nuse by the scientific community at https://doi.org/10.20383/103.0614.\n", "link": "http://arxiv.org/abs/2405.05241v2", "date": "2024-07-11", "relevancy": 2.571, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5231}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5098}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications&body=Title%3A%20BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications%0AAuthor%3A%20Scott%20C.%20Lowe%20and%20Benjamin%20Misiuk%20and%20Isaac%20Xu%20and%20Shakhboz%20Abdulazizov%20and%20Amit%20R.%20Baroi%20and%20Alex%20C.%20Bastos%20and%20Merlin%20Best%20and%20Vicki%20Ferrini%20and%20Ariell%20Friedman%20and%20Deborah%20Hart%20and%20Ove%20Hoegh-Guldberg%20and%20Daniel%20Ierodiaconou%20and%20Julia%20Mackin-McLaughlin%20and%20Kathryn%20Markey%20and%20Pedro%20S.%20Menandro%20and%20Jacquomo%20Monk%20and%20Shreya%20Nemani%20and%20John%20O%27Brien%20and%20Elizabeth%20Oh%20and%20Luba%20Y.%20Reshitnyk%20and%20Katleen%20Robert%20and%20Chris%20M.%20Roelfsema%20and%20Jessica%20A.%20Sameoto%20and%20Alexandre%20C.%20G.%20Schimel%20and%20Jordan%20A.%20Thomson%20and%20Brittany%20R.%20Wilson%20and%20Melisa%20C.%20Wong%20and%20Craig%20J.%20Brown%20and%20Thomas%20Trappenberg%0AAbstract%3A%20%20%20Advances%20in%20underwater%20imaging%20enable%20the%20collection%20of%20extensive%20seafloor%0Aimage%20datasets%20that%20are%20necessary%20for%20monitoring%20important%20benthic%20ecosystems.%0AThe%20ability%20to%20collect%20seafloor%20imagery%20has%20outpaced%20our%20capacity%20to%20analyze%0Ait%2C%20hindering%20expedient%20mobilization%20of%20this%20crucial%20environmental%20information.%0ARecent%20machine%20learning%20approaches%20provide%20opportunities%20to%20increase%20the%0Aefficiency%20with%20which%20seafloor%20image%20datasets%20are%20analyzed%2C%20yet%20large%20and%0Aconsistent%20datasets%20necessary%20to%20support%20development%20of%20such%20approaches%20are%0Ascarce.%20Here%20we%20present%20BenthicNet%3A%20a%20global%20compilation%20of%20seafloor%20imagery%0Adesigned%20to%20support%20the%20training%20and%20evaluation%20of%20large-scale%20image%0Arecognition%20models.%20An%20initial%20set%20of%20over%2011.4%20million%20images%20was%20collected%0Aand%20curated%20to%20represent%20a%20diversity%20of%20seafloor%20environments%20using%20a%0Arepresentative%20subset%20of%201.3%20million%20images.%20These%20are%20accompanied%20by%202.6%0Amillion%20annotations%20translated%20to%20the%20CATAMI%20scheme%2C%20which%20span%20190%2C000%20of%20the%0Aimages.%20A%20large%20deep%20learning%20model%20was%20trained%20on%20this%20compilation%20and%0Apreliminary%20results%20suggest%20it%20has%20utility%20for%20automating%20large%20and%20small-scale%0Aimage%20analysis%20tasks.%20The%20compilation%20and%20model%20are%20made%20openly%20available%20for%0Ause%20by%20the%20scientific%20community%20at%20https%3A//doi.org/10.20383/103.0614.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenthicNet%253A%2520A%2520global%2520compilation%2520of%2520seafloor%2520images%2520for%2520deep%2520learning%250A%2520%2520applications%26entry.906535625%3DScott%2520C.%2520Lowe%2520and%2520Benjamin%2520Misiuk%2520and%2520Isaac%2520Xu%2520and%2520Shakhboz%2520Abdulazizov%2520and%2520Amit%2520R.%2520Baroi%2520and%2520Alex%2520C.%2520Bastos%2520and%2520Merlin%2520Best%2520and%2520Vicki%2520Ferrini%2520and%2520Ariell%2520Friedman%2520and%2520Deborah%2520Hart%2520and%2520Ove%2520Hoegh-Guldberg%2520and%2520Daniel%2520Ierodiaconou%2520and%2520Julia%2520Mackin-McLaughlin%2520and%2520Kathryn%2520Markey%2520and%2520Pedro%2520S.%2520Menandro%2520and%2520Jacquomo%2520Monk%2520and%2520Shreya%2520Nemani%2520and%2520John%2520O%2527Brien%2520and%2520Elizabeth%2520Oh%2520and%2520Luba%2520Y.%2520Reshitnyk%2520and%2520Katleen%2520Robert%2520and%2520Chris%2520M.%2520Roelfsema%2520and%2520Jessica%2520A.%2520Sameoto%2520and%2520Alexandre%2520C.%2520G.%2520Schimel%2520and%2520Jordan%2520A.%2520Thomson%2520and%2520Brittany%2520R.%2520Wilson%2520and%2520Melisa%2520C.%2520Wong%2520and%2520Craig%2520J.%2520Brown%2520and%2520Thomas%2520Trappenberg%26entry.1292438233%3D%2520%2520Advances%2520in%2520underwater%2520imaging%2520enable%2520the%2520collection%2520of%2520extensive%2520seafloor%250Aimage%2520datasets%2520that%2520are%2520necessary%2520for%2520monitoring%2520important%2520benthic%2520ecosystems.%250AThe%2520ability%2520to%2520collect%2520seafloor%2520imagery%2520has%2520outpaced%2520our%2520capacity%2520to%2520analyze%250Ait%252C%2520hindering%2520expedient%2520mobilization%2520of%2520this%2520crucial%2520environmental%2520information.%250ARecent%2520machine%2520learning%2520approaches%2520provide%2520opportunities%2520to%2520increase%2520the%250Aefficiency%2520with%2520which%2520seafloor%2520image%2520datasets%2520are%2520analyzed%252C%2520yet%2520large%2520and%250Aconsistent%2520datasets%2520necessary%2520to%2520support%2520development%2520of%2520such%2520approaches%2520are%250Ascarce.%2520Here%2520we%2520present%2520BenthicNet%253A%2520a%2520global%2520compilation%2520of%2520seafloor%2520imagery%250Adesigned%2520to%2520support%2520the%2520training%2520and%2520evaluation%2520of%2520large-scale%2520image%250Arecognition%2520models.%2520An%2520initial%2520set%2520of%2520over%252011.4%2520million%2520images%2520was%2520collected%250Aand%2520curated%2520to%2520represent%2520a%2520diversity%2520of%2520seafloor%2520environments%2520using%2520a%250Arepresentative%2520subset%2520of%25201.3%2520million%2520images.%2520These%2520are%2520accompanied%2520by%25202.6%250Amillion%2520annotations%2520translated%2520to%2520the%2520CATAMI%2520scheme%252C%2520which%2520span%2520190%252C000%2520of%2520the%250Aimages.%2520A%2520large%2520deep%2520learning%2520model%2520was%2520trained%2520on%2520this%2520compilation%2520and%250Apreliminary%2520results%2520suggest%2520it%2520has%2520utility%2520for%2520automating%2520large%2520and%2520small-scale%250Aimage%2520analysis%2520tasks.%2520The%2520compilation%2520and%2520model%2520are%2520made%2520openly%2520available%2520for%250Ause%2520by%2520the%2520scientific%2520community%2520at%2520https%253A//doi.org/10.20383/103.0614.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications&entry.906535625=Scott%20C.%20Lowe%20and%20Benjamin%20Misiuk%20and%20Isaac%20Xu%20and%20Shakhboz%20Abdulazizov%20and%20Amit%20R.%20Baroi%20and%20Alex%20C.%20Bastos%20and%20Merlin%20Best%20and%20Vicki%20Ferrini%20and%20Ariell%20Friedman%20and%20Deborah%20Hart%20and%20Ove%20Hoegh-Guldberg%20and%20Daniel%20Ierodiaconou%20and%20Julia%20Mackin-McLaughlin%20and%20Kathryn%20Markey%20and%20Pedro%20S.%20Menandro%20and%20Jacquomo%20Monk%20and%20Shreya%20Nemani%20and%20John%20O%27Brien%20and%20Elizabeth%20Oh%20and%20Luba%20Y.%20Reshitnyk%20and%20Katleen%20Robert%20and%20Chris%20M.%20Roelfsema%20and%20Jessica%20A.%20Sameoto%20and%20Alexandre%20C.%20G.%20Schimel%20and%20Jordan%20A.%20Thomson%20and%20Brittany%20R.%20Wilson%20and%20Melisa%20C.%20Wong%20and%20Craig%20J.%20Brown%20and%20Thomas%20Trappenberg&entry.1292438233=%20%20Advances%20in%20underwater%20imaging%20enable%20the%20collection%20of%20extensive%20seafloor%0Aimage%20datasets%20that%20are%20necessary%20for%20monitoring%20important%20benthic%20ecosystems.%0AThe%20ability%20to%20collect%20seafloor%20imagery%20has%20outpaced%20our%20capacity%20to%20analyze%0Ait%2C%20hindering%20expedient%20mobilization%20of%20this%20crucial%20environmental%20information.%0ARecent%20machine%20learning%20approaches%20provide%20opportunities%20to%20increase%20the%0Aefficiency%20with%20which%20seafloor%20image%20datasets%20are%20analyzed%2C%20yet%20large%20and%0Aconsistent%20datasets%20necessary%20to%20support%20development%20of%20such%20approaches%20are%0Ascarce.%20Here%20we%20present%20BenthicNet%3A%20a%20global%20compilation%20of%20seafloor%20imagery%0Adesigned%20to%20support%20the%20training%20and%20evaluation%20of%20large-scale%20image%0Arecognition%20models.%20An%20initial%20set%20of%20over%2011.4%20million%20images%20was%20collected%0Aand%20curated%20to%20represent%20a%20diversity%20of%20seafloor%20environments%20using%20a%0Arepresentative%20subset%20of%201.3%20million%20images.%20These%20are%20accompanied%20by%202.6%0Amillion%20annotations%20translated%20to%20the%20CATAMI%20scheme%2C%20which%20span%20190%2C000%20of%20the%0Aimages.%20A%20large%20deep%20learning%20model%20was%20trained%20on%20this%20compilation%20and%0Apreliminary%20results%20suggest%20it%20has%20utility%20for%20automating%20large%20and%20small-scale%0Aimage%20analysis%20tasks.%20The%20compilation%20and%20model%20are%20made%20openly%20available%20for%0Ause%20by%20the%20scientific%20community%20at%20https%3A//doi.org/10.20383/103.0614.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05241v2&entry.124074799=Read"},
{"title": "SEED-Story: Multimodal Long Story Generation with Large Language Model", "author": "Shuai Yang and Yuying Ge and Yang Li and Yukang Chen and Yixiao Ge and Ying Shan and Yingcong Chen", "abstract": "  With the remarkable advancements in image generation and open-form text\ngeneration, the creation of interleaved image-text content has become an\nincreasingly intriguing field. Multimodal story generation, characterized by\nproducing narrative texts and vivid images in an interleaved manner, has\nemerged as a valuable and practical task with broad applications. However, this\ntask poses significant challenges, as it necessitates the comprehension of the\ncomplex interplay between texts and images, and the ability to generate long\nsequences of coherent, contextually relevant texts and visuals. In this work,\nwe propose SEED-Story, a novel method that leverages a Multimodal Large\nLanguage Model (MLLM) to generate extended multimodal stories. Our model, built\nupon the powerful comprehension capability of MLLM, predicts text tokens as\nwell as visual tokens, which are subsequently processed with an adapted visual\nde-tokenizer to produce images with consistent characters and styles. We\nfurther propose multimodal attention sink mechanism to enable the generation of\nstories with up to 25 sequences (only 10 for training) in a highly efficient\nautoregressive manner. Additionally, we present a large-scale and\nhigh-resolution dataset named StoryStream for training our model and\nquantitatively evaluating the task of multimodal story generation in various\naspects.\n", "link": "http://arxiv.org/abs/2407.08683v1", "date": "2024-07-11", "relevancy": 2.5331, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5138}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEED-Story%3A%20Multimodal%20Long%20Story%20Generation%20with%20Large%20Language%20Model&body=Title%3A%20SEED-Story%3A%20Multimodal%20Long%20Story%20Generation%20with%20Large%20Language%20Model%0AAuthor%3A%20Shuai%20Yang%20and%20Yuying%20Ge%20and%20Yang%20Li%20and%20Yukang%20Chen%20and%20Yixiao%20Ge%20and%20Ying%20Shan%20and%20Yingcong%20Chen%0AAbstract%3A%20%20%20With%20the%20remarkable%20advancements%20in%20image%20generation%20and%20open-form%20text%0Ageneration%2C%20the%20creation%20of%20interleaved%20image-text%20content%20has%20become%20an%0Aincreasingly%20intriguing%20field.%20Multimodal%20story%20generation%2C%20characterized%20by%0Aproducing%20narrative%20texts%20and%20vivid%20images%20in%20an%20interleaved%20manner%2C%20has%0Aemerged%20as%20a%20valuable%20and%20practical%20task%20with%20broad%20applications.%20However%2C%20this%0Atask%20poses%20significant%20challenges%2C%20as%20it%20necessitates%20the%20comprehension%20of%20the%0Acomplex%20interplay%20between%20texts%20and%20images%2C%20and%20the%20ability%20to%20generate%20long%0Asequences%20of%20coherent%2C%20contextually%20relevant%20texts%20and%20visuals.%20In%20this%20work%2C%0Awe%20propose%20SEED-Story%2C%20a%20novel%20method%20that%20leverages%20a%20Multimodal%20Large%0ALanguage%20Model%20%28MLLM%29%20to%20generate%20extended%20multimodal%20stories.%20Our%20model%2C%20built%0Aupon%20the%20powerful%20comprehension%20capability%20of%20MLLM%2C%20predicts%20text%20tokens%20as%0Awell%20as%20visual%20tokens%2C%20which%20are%20subsequently%20processed%20with%20an%20adapted%20visual%0Ade-tokenizer%20to%20produce%20images%20with%20consistent%20characters%20and%20styles.%20We%0Afurther%20propose%20multimodal%20attention%20sink%20mechanism%20to%20enable%20the%20generation%20of%0Astories%20with%20up%20to%2025%20sequences%20%28only%2010%20for%20training%29%20in%20a%20highly%20efficient%0Aautoregressive%20manner.%20Additionally%2C%20we%20present%20a%20large-scale%20and%0Ahigh-resolution%20dataset%20named%20StoryStream%20for%20training%20our%20model%20and%0Aquantitatively%20evaluating%20the%20task%20of%20multimodal%20story%20generation%20in%20various%0Aaspects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEED-Story%253A%2520Multimodal%2520Long%2520Story%2520Generation%2520with%2520Large%2520Language%2520Model%26entry.906535625%3DShuai%2520Yang%2520and%2520Yuying%2520Ge%2520and%2520Yang%2520Li%2520and%2520Yukang%2520Chen%2520and%2520Yixiao%2520Ge%2520and%2520Ying%2520Shan%2520and%2520Yingcong%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520remarkable%2520advancements%2520in%2520image%2520generation%2520and%2520open-form%2520text%250Ageneration%252C%2520the%2520creation%2520of%2520interleaved%2520image-text%2520content%2520has%2520become%2520an%250Aincreasingly%2520intriguing%2520field.%2520Multimodal%2520story%2520generation%252C%2520characterized%2520by%250Aproducing%2520narrative%2520texts%2520and%2520vivid%2520images%2520in%2520an%2520interleaved%2520manner%252C%2520has%250Aemerged%2520as%2520a%2520valuable%2520and%2520practical%2520task%2520with%2520broad%2520applications.%2520However%252C%2520this%250Atask%2520poses%2520significant%2520challenges%252C%2520as%2520it%2520necessitates%2520the%2520comprehension%2520of%2520the%250Acomplex%2520interplay%2520between%2520texts%2520and%2520images%252C%2520and%2520the%2520ability%2520to%2520generate%2520long%250Asequences%2520of%2520coherent%252C%2520contextually%2520relevant%2520texts%2520and%2520visuals.%2520In%2520this%2520work%252C%250Awe%2520propose%2520SEED-Story%252C%2520a%2520novel%2520method%2520that%2520leverages%2520a%2520Multimodal%2520Large%250ALanguage%2520Model%2520%2528MLLM%2529%2520to%2520generate%2520extended%2520multimodal%2520stories.%2520Our%2520model%252C%2520built%250Aupon%2520the%2520powerful%2520comprehension%2520capability%2520of%2520MLLM%252C%2520predicts%2520text%2520tokens%2520as%250Awell%2520as%2520visual%2520tokens%252C%2520which%2520are%2520subsequently%2520processed%2520with%2520an%2520adapted%2520visual%250Ade-tokenizer%2520to%2520produce%2520images%2520with%2520consistent%2520characters%2520and%2520styles.%2520We%250Afurther%2520propose%2520multimodal%2520attention%2520sink%2520mechanism%2520to%2520enable%2520the%2520generation%2520of%250Astories%2520with%2520up%2520to%252025%2520sequences%2520%2528only%252010%2520for%2520training%2529%2520in%2520a%2520highly%2520efficient%250Aautoregressive%2520manner.%2520Additionally%252C%2520we%2520present%2520a%2520large-scale%2520and%250Ahigh-resolution%2520dataset%2520named%2520StoryStream%2520for%2520training%2520our%2520model%2520and%250Aquantitatively%2520evaluating%2520the%2520task%2520of%2520multimodal%2520story%2520generation%2520in%2520various%250Aaspects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEED-Story%3A%20Multimodal%20Long%20Story%20Generation%20with%20Large%20Language%20Model&entry.906535625=Shuai%20Yang%20and%20Yuying%20Ge%20and%20Yang%20Li%20and%20Yukang%20Chen%20and%20Yixiao%20Ge%20and%20Ying%20Shan%20and%20Yingcong%20Chen&entry.1292438233=%20%20With%20the%20remarkable%20advancements%20in%20image%20generation%20and%20open-form%20text%0Ageneration%2C%20the%20creation%20of%20interleaved%20image-text%20content%20has%20become%20an%0Aincreasingly%20intriguing%20field.%20Multimodal%20story%20generation%2C%20characterized%20by%0Aproducing%20narrative%20texts%20and%20vivid%20images%20in%20an%20interleaved%20manner%2C%20has%0Aemerged%20as%20a%20valuable%20and%20practical%20task%20with%20broad%20applications.%20However%2C%20this%0Atask%20poses%20significant%20challenges%2C%20as%20it%20necessitates%20the%20comprehension%20of%20the%0Acomplex%20interplay%20between%20texts%20and%20images%2C%20and%20the%20ability%20to%20generate%20long%0Asequences%20of%20coherent%2C%20contextually%20relevant%20texts%20and%20visuals.%20In%20this%20work%2C%0Awe%20propose%20SEED-Story%2C%20a%20novel%20method%20that%20leverages%20a%20Multimodal%20Large%0ALanguage%20Model%20%28MLLM%29%20to%20generate%20extended%20multimodal%20stories.%20Our%20model%2C%20built%0Aupon%20the%20powerful%20comprehension%20capability%20of%20MLLM%2C%20predicts%20text%20tokens%20as%0Awell%20as%20visual%20tokens%2C%20which%20are%20subsequently%20processed%20with%20an%20adapted%20visual%0Ade-tokenizer%20to%20produce%20images%20with%20consistent%20characters%20and%20styles.%20We%0Afurther%20propose%20multimodal%20attention%20sink%20mechanism%20to%20enable%20the%20generation%20of%0Astories%20with%20up%20to%2025%20sequences%20%28only%2010%20for%20training%29%20in%20a%20highly%20efficient%0Aautoregressive%20manner.%20Additionally%2C%20we%20present%20a%20large-scale%20and%0Ahigh-resolution%20dataset%20named%20StoryStream%20for%20training%20our%20model%20and%0Aquantitatively%20evaluating%20the%20task%20of%20multimodal%20story%20generation%20in%20various%0Aaspects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08683v1&entry.124074799=Read"},
{"title": "Vision language models are blind", "author": "Pooyan Rahmanzadehgervi and Logan Bolton and Mohammad Reza Taesiri and Anh Totti Nguyen", "abstract": "  Large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro are powering countless image-text applications and scoring high\non many vision-understanding benchmarks. We propose BlindTest, a suite of 7\nvisual tasks absurdly easy to humans such as identifying (a) whether two\ncircles overlap; (b) whether two lines intersect; (c) which letter is being\ncircled in a word; and (d) counting the number of circles in a Olympic-like\nlogo. Surprisingly, four state-of-the-art VLMs are, on average, only 56.20%\naccurate on our benchmark, with \\newsonnet being the best (73.77% accuracy). On\nBlindTest, VLMs struggle with tasks that requires precise spatial information\nand counting (from 0 to 10), sometimes providing an impression of a person with\nmyopia seeing fine details as blurry and making educated guesses. Code is\navailable at: https://vlmsareblind.github.io/\n", "link": "http://arxiv.org/abs/2407.06581v2", "date": "2024-07-11", "relevancy": 2.5187, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5004}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20language%20models%20are%20blind&body=Title%3A%20Vision%20language%20models%20are%20blind%0AAuthor%3A%20Pooyan%20Rahmanzadehgervi%20and%20Logan%20Bolton%20and%20Mohammad%20Reza%20Taesiri%20and%20Anh%20Totti%20Nguyen%0AAbstract%3A%20%20%20Large%20language%20models%20with%20vision%20capabilities%20%28VLMs%29%2C%20e.g.%2C%20GPT-4o%20and%0AGemini%201.5%20Pro%20are%20powering%20countless%20image-text%20applications%20and%20scoring%20high%0Aon%20many%20vision-understanding%20benchmarks.%20We%20propose%20BlindTest%2C%20a%20suite%20of%207%0Avisual%20tasks%20absurdly%20easy%20to%20humans%20such%20as%20identifying%20%28a%29%20whether%20two%0Acircles%20overlap%3B%20%28b%29%20whether%20two%20lines%20intersect%3B%20%28c%29%20which%20letter%20is%20being%0Acircled%20in%20a%20word%3B%20and%20%28d%29%20counting%20the%20number%20of%20circles%20in%20a%20Olympic-like%0Alogo.%20Surprisingly%2C%20four%20state-of-the-art%20VLMs%20are%2C%20on%20average%2C%20only%2056.20%25%0Aaccurate%20on%20our%20benchmark%2C%20with%20%5Cnewsonnet%20being%20the%20best%20%2873.77%25%20accuracy%29.%20On%0ABlindTest%2C%20VLMs%20struggle%20with%20tasks%20that%20requires%20precise%20spatial%20information%0Aand%20counting%20%28from%200%20to%2010%29%2C%20sometimes%20providing%20an%20impression%20of%20a%20person%20with%0Amyopia%20seeing%20fine%20details%20as%20blurry%20and%20making%20educated%20guesses.%20Code%20is%0Aavailable%20at%3A%20https%3A//vlmsareblind.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06581v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520language%2520models%2520are%2520blind%26entry.906535625%3DPooyan%2520Rahmanzadehgervi%2520and%2520Logan%2520Bolton%2520and%2520Mohammad%2520Reza%2520Taesiri%2520and%2520Anh%2520Totti%2520Nguyen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520with%2520vision%2520capabilities%2520%2528VLMs%2529%252C%2520e.g.%252C%2520GPT-4o%2520and%250AGemini%25201.5%2520Pro%2520are%2520powering%2520countless%2520image-text%2520applications%2520and%2520scoring%2520high%250Aon%2520many%2520vision-understanding%2520benchmarks.%2520We%2520propose%2520BlindTest%252C%2520a%2520suite%2520of%25207%250Avisual%2520tasks%2520absurdly%2520easy%2520to%2520humans%2520such%2520as%2520identifying%2520%2528a%2529%2520whether%2520two%250Acircles%2520overlap%253B%2520%2528b%2529%2520whether%2520two%2520lines%2520intersect%253B%2520%2528c%2529%2520which%2520letter%2520is%2520being%250Acircled%2520in%2520a%2520word%253B%2520and%2520%2528d%2529%2520counting%2520the%2520number%2520of%2520circles%2520in%2520a%2520Olympic-like%250Alogo.%2520Surprisingly%252C%2520four%2520state-of-the-art%2520VLMs%2520are%252C%2520on%2520average%252C%2520only%252056.20%2525%250Aaccurate%2520on%2520our%2520benchmark%252C%2520with%2520%255Cnewsonnet%2520being%2520the%2520best%2520%252873.77%2525%2520accuracy%2529.%2520On%250ABlindTest%252C%2520VLMs%2520struggle%2520with%2520tasks%2520that%2520requires%2520precise%2520spatial%2520information%250Aand%2520counting%2520%2528from%25200%2520to%252010%2529%252C%2520sometimes%2520providing%2520an%2520impression%2520of%2520a%2520person%2520with%250Amyopia%2520seeing%2520fine%2520details%2520as%2520blurry%2520and%2520making%2520educated%2520guesses.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//vlmsareblind.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06581v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20language%20models%20are%20blind&entry.906535625=Pooyan%20Rahmanzadehgervi%20and%20Logan%20Bolton%20and%20Mohammad%20Reza%20Taesiri%20and%20Anh%20Totti%20Nguyen&entry.1292438233=%20%20Large%20language%20models%20with%20vision%20capabilities%20%28VLMs%29%2C%20e.g.%2C%20GPT-4o%20and%0AGemini%201.5%20Pro%20are%20powering%20countless%20image-text%20applications%20and%20scoring%20high%0Aon%20many%20vision-understanding%20benchmarks.%20We%20propose%20BlindTest%2C%20a%20suite%20of%207%0Avisual%20tasks%20absurdly%20easy%20to%20humans%20such%20as%20identifying%20%28a%29%20whether%20two%0Acircles%20overlap%3B%20%28b%29%20whether%20two%20lines%20intersect%3B%20%28c%29%20which%20letter%20is%20being%0Acircled%20in%20a%20word%3B%20and%20%28d%29%20counting%20the%20number%20of%20circles%20in%20a%20Olympic-like%0Alogo.%20Surprisingly%2C%20four%20state-of-the-art%20VLMs%20are%2C%20on%20average%2C%20only%2056.20%25%0Aaccurate%20on%20our%20benchmark%2C%20with%20%5Cnewsonnet%20being%20the%20best%20%2873.77%25%20accuracy%29.%20On%0ABlindTest%2C%20VLMs%20struggle%20with%20tasks%20that%20requires%20precise%20spatial%20information%0Aand%20counting%20%28from%200%20to%2010%29%2C%20sometimes%20providing%20an%20impression%20of%20a%20person%20with%0Amyopia%20seeing%20fine%20details%20as%20blurry%20and%20making%20educated%20guesses.%20Code%20is%0Aavailable%20at%3A%20https%3A//vlmsareblind.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06581v2&entry.124074799=Read"},
{"title": "Scalar Function Topology Divergence: Comparing Topology of 3D Objects", "author": "Ilya Trofimov and Daria Voronkova and Eduard Tulchinskii and Evgeny Burnaev and Serguei Barannikov", "abstract": "  We propose a new topological tool for computer vision - Scalar Function\nTopology Divergence (SFTD), which measures the dissimilarity of multi-scale\ntopology between sublevel sets of two functions having a common domain.\nFunctions can be defined on an undirected graph or Euclidean space of any\ndimensionality. Most of the existing methods for comparing topology are based\non Wasserstein distance between persistence barcodes and they don't take into\naccount the localization of topological features. On the other hand, the\nminimization of SFTD ensures that the corresponding topological features of\nscalar functions are located in the same places. The proposed tool provides\nuseful visualizations depicting areas where functions have topological\ndissimilarities. We provide applications of the proposed method to 3D computer\nvision. In particular, experiments demonstrate that SFTD improves the\nreconstruction of cellular 3D shapes from 2D fluorescence microscopy images,\nand helps to identify topological errors in 3D segmentation.\n", "link": "http://arxiv.org/abs/2407.08364v1", "date": "2024-07-11", "relevancy": 2.5146, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5128}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5128}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalar%20Function%20Topology%20Divergence%3A%20Comparing%20Topology%20of%203D%20Objects&body=Title%3A%20Scalar%20Function%20Topology%20Divergence%3A%20Comparing%20Topology%20of%203D%20Objects%0AAuthor%3A%20Ilya%20Trofimov%20and%20Daria%20Voronkova%20and%20Eduard%20Tulchinskii%20and%20Evgeny%20Burnaev%20and%20Serguei%20Barannikov%0AAbstract%3A%20%20%20We%20propose%20a%20new%20topological%20tool%20for%20computer%20vision%20-%20Scalar%20Function%0ATopology%20Divergence%20%28SFTD%29%2C%20which%20measures%20the%20dissimilarity%20of%20multi-scale%0Atopology%20between%20sublevel%20sets%20of%20two%20functions%20having%20a%20common%20domain.%0AFunctions%20can%20be%20defined%20on%20an%20undirected%20graph%20or%20Euclidean%20space%20of%20any%0Adimensionality.%20Most%20of%20the%20existing%20methods%20for%20comparing%20topology%20are%20based%0Aon%20Wasserstein%20distance%20between%20persistence%20barcodes%20and%20they%20don%27t%20take%20into%0Aaccount%20the%20localization%20of%20topological%20features.%20On%20the%20other%20hand%2C%20the%0Aminimization%20of%20SFTD%20ensures%20that%20the%20corresponding%20topological%20features%20of%0Ascalar%20functions%20are%20located%20in%20the%20same%20places.%20The%20proposed%20tool%20provides%0Auseful%20visualizations%20depicting%20areas%20where%20functions%20have%20topological%0Adissimilarities.%20We%20provide%20applications%20of%20the%20proposed%20method%20to%203D%20computer%0Avision.%20In%20particular%2C%20experiments%20demonstrate%20that%20SFTD%20improves%20the%0Areconstruction%20of%20cellular%203D%20shapes%20from%202D%20fluorescence%20microscopy%20images%2C%0Aand%20helps%20to%20identify%20topological%20errors%20in%203D%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalar%2520Function%2520Topology%2520Divergence%253A%2520Comparing%2520Topology%2520of%25203D%2520Objects%26entry.906535625%3DIlya%2520Trofimov%2520and%2520Daria%2520Voronkova%2520and%2520Eduard%2520Tulchinskii%2520and%2520Evgeny%2520Burnaev%2520and%2520Serguei%2520Barannikov%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520topological%2520tool%2520for%2520computer%2520vision%2520-%2520Scalar%2520Function%250ATopology%2520Divergence%2520%2528SFTD%2529%252C%2520which%2520measures%2520the%2520dissimilarity%2520of%2520multi-scale%250Atopology%2520between%2520sublevel%2520sets%2520of%2520two%2520functions%2520having%2520a%2520common%2520domain.%250AFunctions%2520can%2520be%2520defined%2520on%2520an%2520undirected%2520graph%2520or%2520Euclidean%2520space%2520of%2520any%250Adimensionality.%2520Most%2520of%2520the%2520existing%2520methods%2520for%2520comparing%2520topology%2520are%2520based%250Aon%2520Wasserstein%2520distance%2520between%2520persistence%2520barcodes%2520and%2520they%2520don%2527t%2520take%2520into%250Aaccount%2520the%2520localization%2520of%2520topological%2520features.%2520On%2520the%2520other%2520hand%252C%2520the%250Aminimization%2520of%2520SFTD%2520ensures%2520that%2520the%2520corresponding%2520topological%2520features%2520of%250Ascalar%2520functions%2520are%2520located%2520in%2520the%2520same%2520places.%2520The%2520proposed%2520tool%2520provides%250Auseful%2520visualizations%2520depicting%2520areas%2520where%2520functions%2520have%2520topological%250Adissimilarities.%2520We%2520provide%2520applications%2520of%2520the%2520proposed%2520method%2520to%25203D%2520computer%250Avision.%2520In%2520particular%252C%2520experiments%2520demonstrate%2520that%2520SFTD%2520improves%2520the%250Areconstruction%2520of%2520cellular%25203D%2520shapes%2520from%25202D%2520fluorescence%2520microscopy%2520images%252C%250Aand%2520helps%2520to%2520identify%2520topological%2520errors%2520in%25203D%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalar%20Function%20Topology%20Divergence%3A%20Comparing%20Topology%20of%203D%20Objects&entry.906535625=Ilya%20Trofimov%20and%20Daria%20Voronkova%20and%20Eduard%20Tulchinskii%20and%20Evgeny%20Burnaev%20and%20Serguei%20Barannikov&entry.1292438233=%20%20We%20propose%20a%20new%20topological%20tool%20for%20computer%20vision%20-%20Scalar%20Function%0ATopology%20Divergence%20%28SFTD%29%2C%20which%20measures%20the%20dissimilarity%20of%20multi-scale%0Atopology%20between%20sublevel%20sets%20of%20two%20functions%20having%20a%20common%20domain.%0AFunctions%20can%20be%20defined%20on%20an%20undirected%20graph%20or%20Euclidean%20space%20of%20any%0Adimensionality.%20Most%20of%20the%20existing%20methods%20for%20comparing%20topology%20are%20based%0Aon%20Wasserstein%20distance%20between%20persistence%20barcodes%20and%20they%20don%27t%20take%20into%0Aaccount%20the%20localization%20of%20topological%20features.%20On%20the%20other%20hand%2C%20the%0Aminimization%20of%20SFTD%20ensures%20that%20the%20corresponding%20topological%20features%20of%0Ascalar%20functions%20are%20located%20in%20the%20same%20places.%20The%20proposed%20tool%20provides%0Auseful%20visualizations%20depicting%20areas%20where%20functions%20have%20topological%0Adissimilarities.%20We%20provide%20applications%20of%20the%20proposed%20method%20to%203D%20computer%0Avision.%20In%20particular%2C%20experiments%20demonstrate%20that%20SFTD%20improves%20the%0Areconstruction%20of%20cellular%203D%20shapes%20from%202D%20fluorescence%20microscopy%20images%2C%0Aand%20helps%20to%20identify%20topological%20errors%20in%203D%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08364v1&entry.124074799=Read"},
{"title": "Large-Scale Dataset Pruning in Adversarial Training through Data\n  Importance Extrapolation", "author": "Bj\u00f6rn Nieth and Thomas Altstidl and Leo Schwinn and Bj\u00f6rn Eskofier", "abstract": "  Their vulnerability to small, imperceptible attacks limits the adoption of\ndeep learning models to real-world systems. Adversarial training has proven to\nbe one of the most promising strategies against these attacks, at the expense\nof a substantial increase in training time. With the ongoing trend of\nintegrating large-scale synthetic data this is only expected to increase even\nfurther. Thus, the need for data-centric approaches that reduce the number of\ntraining samples while maintaining accuracy and robustness arises. While data\npruning and active learning are prominent research topics in deep learning,\nthey are as of now largely unexplored in the adversarial training literature.\nWe address this gap and propose a new data pruning strategy based on\nextrapolating data importance scores from a small set of data to a larger set.\nIn an empirical evaluation, we demonstrate that extrapolation-based pruning can\nefficiently reduce dataset size while maintaining robustness.\n", "link": "http://arxiv.org/abs/2406.13283v2", "date": "2024-07-11", "relevancy": 2.5058, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5214}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5017}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Scale%20Dataset%20Pruning%20in%20Adversarial%20Training%20through%20Data%0A%20%20Importance%20Extrapolation&body=Title%3A%20Large-Scale%20Dataset%20Pruning%20in%20Adversarial%20Training%20through%20Data%0A%20%20Importance%20Extrapolation%0AAuthor%3A%20Bj%C3%B6rn%20Nieth%20and%20Thomas%20Altstidl%20and%20Leo%20Schwinn%20and%20Bj%C3%B6rn%20Eskofier%0AAbstract%3A%20%20%20Their%20vulnerability%20to%20small%2C%20imperceptible%20attacks%20limits%20the%20adoption%20of%0Adeep%20learning%20models%20to%20real-world%20systems.%20Adversarial%20training%20has%20proven%20to%0Abe%20one%20of%20the%20most%20promising%20strategies%20against%20these%20attacks%2C%20at%20the%20expense%0Aof%20a%20substantial%20increase%20in%20training%20time.%20With%20the%20ongoing%20trend%20of%0Aintegrating%20large-scale%20synthetic%20data%20this%20is%20only%20expected%20to%20increase%20even%0Afurther.%20Thus%2C%20the%20need%20for%20data-centric%20approaches%20that%20reduce%20the%20number%20of%0Atraining%20samples%20while%20maintaining%20accuracy%20and%20robustness%20arises.%20While%20data%0Apruning%20and%20active%20learning%20are%20prominent%20research%20topics%20in%20deep%20learning%2C%0Athey%20are%20as%20of%20now%20largely%20unexplored%20in%20the%20adversarial%20training%20literature.%0AWe%20address%20this%20gap%20and%20propose%20a%20new%20data%20pruning%20strategy%20based%20on%0Aextrapolating%20data%20importance%20scores%20from%20a%20small%20set%20of%20data%20to%20a%20larger%20set.%0AIn%20an%20empirical%20evaluation%2C%20we%20demonstrate%20that%20extrapolation-based%20pruning%20can%0Aefficiently%20reduce%20dataset%20size%20while%20maintaining%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Scale%2520Dataset%2520Pruning%2520in%2520Adversarial%2520Training%2520through%2520Data%250A%2520%2520Importance%2520Extrapolation%26entry.906535625%3DBj%25C3%25B6rn%2520Nieth%2520and%2520Thomas%2520Altstidl%2520and%2520Leo%2520Schwinn%2520and%2520Bj%25C3%25B6rn%2520Eskofier%26entry.1292438233%3D%2520%2520Their%2520vulnerability%2520to%2520small%252C%2520imperceptible%2520attacks%2520limits%2520the%2520adoption%2520of%250Adeep%2520learning%2520models%2520to%2520real-world%2520systems.%2520Adversarial%2520training%2520has%2520proven%2520to%250Abe%2520one%2520of%2520the%2520most%2520promising%2520strategies%2520against%2520these%2520attacks%252C%2520at%2520the%2520expense%250Aof%2520a%2520substantial%2520increase%2520in%2520training%2520time.%2520With%2520the%2520ongoing%2520trend%2520of%250Aintegrating%2520large-scale%2520synthetic%2520data%2520this%2520is%2520only%2520expected%2520to%2520increase%2520even%250Afurther.%2520Thus%252C%2520the%2520need%2520for%2520data-centric%2520approaches%2520that%2520reduce%2520the%2520number%2520of%250Atraining%2520samples%2520while%2520maintaining%2520accuracy%2520and%2520robustness%2520arises.%2520While%2520data%250Apruning%2520and%2520active%2520learning%2520are%2520prominent%2520research%2520topics%2520in%2520deep%2520learning%252C%250Athey%2520are%2520as%2520of%2520now%2520largely%2520unexplored%2520in%2520the%2520adversarial%2520training%2520literature.%250AWe%2520address%2520this%2520gap%2520and%2520propose%2520a%2520new%2520data%2520pruning%2520strategy%2520based%2520on%250Aextrapolating%2520data%2520importance%2520scores%2520from%2520a%2520small%2520set%2520of%2520data%2520to%2520a%2520larger%2520set.%250AIn%2520an%2520empirical%2520evaluation%252C%2520we%2520demonstrate%2520that%2520extrapolation-based%2520pruning%2520can%250Aefficiently%2520reduce%2520dataset%2520size%2520while%2520maintaining%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Scale%20Dataset%20Pruning%20in%20Adversarial%20Training%20through%20Data%0A%20%20Importance%20Extrapolation&entry.906535625=Bj%C3%B6rn%20Nieth%20and%20Thomas%20Altstidl%20and%20Leo%20Schwinn%20and%20Bj%C3%B6rn%20Eskofier&entry.1292438233=%20%20Their%20vulnerability%20to%20small%2C%20imperceptible%20attacks%20limits%20the%20adoption%20of%0Adeep%20learning%20models%20to%20real-world%20systems.%20Adversarial%20training%20has%20proven%20to%0Abe%20one%20of%20the%20most%20promising%20strategies%20against%20these%20attacks%2C%20at%20the%20expense%0Aof%20a%20substantial%20increase%20in%20training%20time.%20With%20the%20ongoing%20trend%20of%0Aintegrating%20large-scale%20synthetic%20data%20this%20is%20only%20expected%20to%20increase%20even%0Afurther.%20Thus%2C%20the%20need%20for%20data-centric%20approaches%20that%20reduce%20the%20number%20of%0Atraining%20samples%20while%20maintaining%20accuracy%20and%20robustness%20arises.%20While%20data%0Apruning%20and%20active%20learning%20are%20prominent%20research%20topics%20in%20deep%20learning%2C%0Athey%20are%20as%20of%20now%20largely%20unexplored%20in%20the%20adversarial%20training%20literature.%0AWe%20address%20this%20gap%20and%20propose%20a%20new%20data%20pruning%20strategy%20based%20on%0Aextrapolating%20data%20importance%20scores%20from%20a%20small%20set%20of%20data%20to%20a%20larger%20set.%0AIn%20an%20empirical%20evaluation%2C%20we%20demonstrate%20that%20extrapolation-based%20pruning%20can%0Aefficiently%20reduce%20dataset%20size%20while%20maintaining%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13283v2&entry.124074799=Read"},
{"title": "MOFA-Video: Controllable Image Animation via Generative Motion Field\n  Adaptions in Frozen Image-to-Video Diffusion Model", "author": "Muyao Niu and Xiaodong Cun and Xintao Wang and Yong Zhang and Ying Shan and Yinqiang Zheng", "abstract": "  We present MOFA-Video, an advanced controllable image animation method that\ngenerates video from the given image using various additional controllable\nsignals (such as human landmarks reference, manual trajectories, and another\neven provided video) or their combinations. This is different from previous\nmethods which only can work on a specific motion domain or show weak control\nabilities with diffusion prior. To achieve our goal, we design several\ndomain-aware motion field adapters (\\ie, MOFA-Adapters) to control the\ngenerated motions in the video generation pipeline. For MOFA-Adapters, we\nconsider the temporal motion consistency of the video and generate the dense\nmotion flow from the given sparse control conditions first, and then, the\nmulti-scale features of the given image are wrapped as a guided feature for\nstable video diffusion generation. We naively train two motion adapters for the\nmanual trajectories and the human landmarks individually since they both\ncontain sparse information about the control. After training, the MOFA-Adapters\nin different domains can also work together for more controllable video\ngeneration. Project Page: https://myniuuu.github.io/MOFA_Video/\n", "link": "http://arxiv.org/abs/2405.20222v3", "date": "2024-07-11", "relevancy": 2.4791, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6577}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.625}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOFA-Video%3A%20Controllable%20Image%20Animation%20via%20Generative%20Motion%20Field%0A%20%20Adaptions%20in%20Frozen%20Image-to-Video%20Diffusion%20Model&body=Title%3A%20MOFA-Video%3A%20Controllable%20Image%20Animation%20via%20Generative%20Motion%20Field%0A%20%20Adaptions%20in%20Frozen%20Image-to-Video%20Diffusion%20Model%0AAuthor%3A%20Muyao%20Niu%20and%20Xiaodong%20Cun%20and%20Xintao%20Wang%20and%20Yong%20Zhang%20and%20Ying%20Shan%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20We%20present%20MOFA-Video%2C%20an%20advanced%20controllable%20image%20animation%20method%20that%0Agenerates%20video%20from%20the%20given%20image%20using%20various%20additional%20controllable%0Asignals%20%28such%20as%20human%20landmarks%20reference%2C%20manual%20trajectories%2C%20and%20another%0Aeven%20provided%20video%29%20or%20their%20combinations.%20This%20is%20different%20from%20previous%0Amethods%20which%20only%20can%20work%20on%20a%20specific%20motion%20domain%20or%20show%20weak%20control%0Aabilities%20with%20diffusion%20prior.%20To%20achieve%20our%20goal%2C%20we%20design%20several%0Adomain-aware%20motion%20field%20adapters%20%28%5Cie%2C%20MOFA-Adapters%29%20to%20control%20the%0Agenerated%20motions%20in%20the%20video%20generation%20pipeline.%20For%20MOFA-Adapters%2C%20we%0Aconsider%20the%20temporal%20motion%20consistency%20of%20the%20video%20and%20generate%20the%20dense%0Amotion%20flow%20from%20the%20given%20sparse%20control%20conditions%20first%2C%20and%20then%2C%20the%0Amulti-scale%20features%20of%20the%20given%20image%20are%20wrapped%20as%20a%20guided%20feature%20for%0Astable%20video%20diffusion%20generation.%20We%20naively%20train%20two%20motion%20adapters%20for%20the%0Amanual%20trajectories%20and%20the%20human%20landmarks%20individually%20since%20they%20both%0Acontain%20sparse%20information%20about%20the%20control.%20After%20training%2C%20the%20MOFA-Adapters%0Ain%20different%20domains%20can%20also%20work%20together%20for%20more%20controllable%20video%0Ageneration.%20Project%20Page%3A%20https%3A//myniuuu.github.io/MOFA_Video/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20222v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOFA-Video%253A%2520Controllable%2520Image%2520Animation%2520via%2520Generative%2520Motion%2520Field%250A%2520%2520Adaptions%2520in%2520Frozen%2520Image-to-Video%2520Diffusion%2520Model%26entry.906535625%3DMuyao%2520Niu%2520and%2520Xiaodong%2520Cun%2520and%2520Xintao%2520Wang%2520and%2520Yong%2520Zhang%2520and%2520Ying%2520Shan%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520We%2520present%2520MOFA-Video%252C%2520an%2520advanced%2520controllable%2520image%2520animation%2520method%2520that%250Agenerates%2520video%2520from%2520the%2520given%2520image%2520using%2520various%2520additional%2520controllable%250Asignals%2520%2528such%2520as%2520human%2520landmarks%2520reference%252C%2520manual%2520trajectories%252C%2520and%2520another%250Aeven%2520provided%2520video%2529%2520or%2520their%2520combinations.%2520This%2520is%2520different%2520from%2520previous%250Amethods%2520which%2520only%2520can%2520work%2520on%2520a%2520specific%2520motion%2520domain%2520or%2520show%2520weak%2520control%250Aabilities%2520with%2520diffusion%2520prior.%2520To%2520achieve%2520our%2520goal%252C%2520we%2520design%2520several%250Adomain-aware%2520motion%2520field%2520adapters%2520%2528%255Cie%252C%2520MOFA-Adapters%2529%2520to%2520control%2520the%250Agenerated%2520motions%2520in%2520the%2520video%2520generation%2520pipeline.%2520For%2520MOFA-Adapters%252C%2520we%250Aconsider%2520the%2520temporal%2520motion%2520consistency%2520of%2520the%2520video%2520and%2520generate%2520the%2520dense%250Amotion%2520flow%2520from%2520the%2520given%2520sparse%2520control%2520conditions%2520first%252C%2520and%2520then%252C%2520the%250Amulti-scale%2520features%2520of%2520the%2520given%2520image%2520are%2520wrapped%2520as%2520a%2520guided%2520feature%2520for%250Astable%2520video%2520diffusion%2520generation.%2520We%2520naively%2520train%2520two%2520motion%2520adapters%2520for%2520the%250Amanual%2520trajectories%2520and%2520the%2520human%2520landmarks%2520individually%2520since%2520they%2520both%250Acontain%2520sparse%2520information%2520about%2520the%2520control.%2520After%2520training%252C%2520the%2520MOFA-Adapters%250Ain%2520different%2520domains%2520can%2520also%2520work%2520together%2520for%2520more%2520controllable%2520video%250Ageneration.%2520Project%2520Page%253A%2520https%253A//myniuuu.github.io/MOFA_Video/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20222v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOFA-Video%3A%20Controllable%20Image%20Animation%20via%20Generative%20Motion%20Field%0A%20%20Adaptions%20in%20Frozen%20Image-to-Video%20Diffusion%20Model&entry.906535625=Muyao%20Niu%20and%20Xiaodong%20Cun%20and%20Xintao%20Wang%20and%20Yong%20Zhang%20and%20Ying%20Shan%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20We%20present%20MOFA-Video%2C%20an%20advanced%20controllable%20image%20animation%20method%20that%0Agenerates%20video%20from%20the%20given%20image%20using%20various%20additional%20controllable%0Asignals%20%28such%20as%20human%20landmarks%20reference%2C%20manual%20trajectories%2C%20and%20another%0Aeven%20provided%20video%29%20or%20their%20combinations.%20This%20is%20different%20from%20previous%0Amethods%20which%20only%20can%20work%20on%20a%20specific%20motion%20domain%20or%20show%20weak%20control%0Aabilities%20with%20diffusion%20prior.%20To%20achieve%20our%20goal%2C%20we%20design%20several%0Adomain-aware%20motion%20field%20adapters%20%28%5Cie%2C%20MOFA-Adapters%29%20to%20control%20the%0Agenerated%20motions%20in%20the%20video%20generation%20pipeline.%20For%20MOFA-Adapters%2C%20we%0Aconsider%20the%20temporal%20motion%20consistency%20of%20the%20video%20and%20generate%20the%20dense%0Amotion%20flow%20from%20the%20given%20sparse%20control%20conditions%20first%2C%20and%20then%2C%20the%0Amulti-scale%20features%20of%20the%20given%20image%20are%20wrapped%20as%20a%20guided%20feature%20for%0Astable%20video%20diffusion%20generation.%20We%20naively%20train%20two%20motion%20adapters%20for%20the%0Amanual%20trajectories%20and%20the%20human%20landmarks%20individually%20since%20they%20both%0Acontain%20sparse%20information%20about%20the%20control.%20After%20training%2C%20the%20MOFA-Adapters%0Ain%20different%20domains%20can%20also%20work%20together%20for%20more%20controllable%20video%0Ageneration.%20Project%20Page%3A%20https%3A//myniuuu.github.io/MOFA_Video/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20222v3&entry.124074799=Read"},
{"title": "Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D\n  Scene", "author": "Ruiyang Zhang and Hu Zhang and Hang Yu and Zhedong Zheng", "abstract": "  The unsupervised 3D object detection is to accurately detect objects in\nunstructured environments with no explicit supervisory signals. This task,\ngiven sparse LiDAR point clouds, often results in compromised performance for\ndetecting distant or small objects due to the inherent sparsity and limited\nspatial resolution. In this paper, we are among the early attempts to integrate\nLiDAR data with 2D images for unsupervised 3D detection and introduce a new\nmethod, dubbed LiDAR-2D Self-paced Learning (LiSe). We argue that RGB images\nserve as a valuable complement to LiDAR data, offering precise 2D localization\ncues, particularly when scarce LiDAR points are available for certain objects.\nConsidering the unique characteristics of both modalities, our framework\ndevises a self-paced learning pipeline that incorporates adaptive sampling and\nweak model aggregation strategies. The adaptive sampling strategy dynamically\ntunes the distribution of pseudo labels during training, countering the\ntendency of models to overfit easily detected samples, such as nearby and\nlarge-sized objects. By doing so, it ensures a balanced learning trajectory\nacross varying object scales and distances. The weak model aggregation\ncomponent consolidates the strengths of models trained under different pseudo\nlabel distributions, culminating in a robust and powerful final model.\nExperimental evaluations validate the efficacy of our proposed LiSe method,\nmanifesting significant improvements of +7.1% AP$_{BEV}$ and +3.4% AP$_{3D}$ on\nnuScenes, and +8.3% AP$_{BEV}$ and +7.4% AP$_{3D}$ on Lyft compared to existing\ntechniques.\n", "link": "http://arxiv.org/abs/2407.08569v1", "date": "2024-07-11", "relevancy": 2.4733, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6205}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6168}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approaching%20Outside%3A%20Scaling%20Unsupervised%203D%20Object%20Detection%20from%202D%0A%20%20Scene&body=Title%3A%20Approaching%20Outside%3A%20Scaling%20Unsupervised%203D%20Object%20Detection%20from%202D%0A%20%20Scene%0AAuthor%3A%20Ruiyang%20Zhang%20and%20Hu%20Zhang%20and%20Hang%20Yu%20and%20Zhedong%20Zheng%0AAbstract%3A%20%20%20The%20unsupervised%203D%20object%20detection%20is%20to%20accurately%20detect%20objects%20in%0Aunstructured%20environments%20with%20no%20explicit%20supervisory%20signals.%20This%20task%2C%0Agiven%20sparse%20LiDAR%20point%20clouds%2C%20often%20results%20in%20compromised%20performance%20for%0Adetecting%20distant%20or%20small%20objects%20due%20to%20the%20inherent%20sparsity%20and%20limited%0Aspatial%20resolution.%20In%20this%20paper%2C%20we%20are%20among%20the%20early%20attempts%20to%20integrate%0ALiDAR%20data%20with%202D%20images%20for%20unsupervised%203D%20detection%20and%20introduce%20a%20new%0Amethod%2C%20dubbed%20LiDAR-2D%20Self-paced%20Learning%20%28LiSe%29.%20We%20argue%20that%20RGB%20images%0Aserve%20as%20a%20valuable%20complement%20to%20LiDAR%20data%2C%20offering%20precise%202D%20localization%0Acues%2C%20particularly%20when%20scarce%20LiDAR%20points%20are%20available%20for%20certain%20objects.%0AConsidering%20the%20unique%20characteristics%20of%20both%20modalities%2C%20our%20framework%0Adevises%20a%20self-paced%20learning%20pipeline%20that%20incorporates%20adaptive%20sampling%20and%0Aweak%20model%20aggregation%20strategies.%20The%20adaptive%20sampling%20strategy%20dynamically%0Atunes%20the%20distribution%20of%20pseudo%20labels%20during%20training%2C%20countering%20the%0Atendency%20of%20models%20to%20overfit%20easily%20detected%20samples%2C%20such%20as%20nearby%20and%0Alarge-sized%20objects.%20By%20doing%20so%2C%20it%20ensures%20a%20balanced%20learning%20trajectory%0Aacross%20varying%20object%20scales%20and%20distances.%20The%20weak%20model%20aggregation%0Acomponent%20consolidates%20the%20strengths%20of%20models%20trained%20under%20different%20pseudo%0Alabel%20distributions%2C%20culminating%20in%20a%20robust%20and%20powerful%20final%20model.%0AExperimental%20evaluations%20validate%20the%20efficacy%20of%20our%20proposed%20LiSe%20method%2C%0Amanifesting%20significant%20improvements%20of%20%2B7.1%25%20AP%24_%7BBEV%7D%24%20and%20%2B3.4%25%20AP%24_%7B3D%7D%24%20on%0AnuScenes%2C%20and%20%2B8.3%25%20AP%24_%7BBEV%7D%24%20and%20%2B7.4%25%20AP%24_%7B3D%7D%24%20on%20Lyft%20compared%20to%20existing%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproaching%2520Outside%253A%2520Scaling%2520Unsupervised%25203D%2520Object%2520Detection%2520from%25202D%250A%2520%2520Scene%26entry.906535625%3DRuiyang%2520Zhang%2520and%2520Hu%2520Zhang%2520and%2520Hang%2520Yu%2520and%2520Zhedong%2520Zheng%26entry.1292438233%3D%2520%2520The%2520unsupervised%25203D%2520object%2520detection%2520is%2520to%2520accurately%2520detect%2520objects%2520in%250Aunstructured%2520environments%2520with%2520no%2520explicit%2520supervisory%2520signals.%2520This%2520task%252C%250Agiven%2520sparse%2520LiDAR%2520point%2520clouds%252C%2520often%2520results%2520in%2520compromised%2520performance%2520for%250Adetecting%2520distant%2520or%2520small%2520objects%2520due%2520to%2520the%2520inherent%2520sparsity%2520and%2520limited%250Aspatial%2520resolution.%2520In%2520this%2520paper%252C%2520we%2520are%2520among%2520the%2520early%2520attempts%2520to%2520integrate%250ALiDAR%2520data%2520with%25202D%2520images%2520for%2520unsupervised%25203D%2520detection%2520and%2520introduce%2520a%2520new%250Amethod%252C%2520dubbed%2520LiDAR-2D%2520Self-paced%2520Learning%2520%2528LiSe%2529.%2520We%2520argue%2520that%2520RGB%2520images%250Aserve%2520as%2520a%2520valuable%2520complement%2520to%2520LiDAR%2520data%252C%2520offering%2520precise%25202D%2520localization%250Acues%252C%2520particularly%2520when%2520scarce%2520LiDAR%2520points%2520are%2520available%2520for%2520certain%2520objects.%250AConsidering%2520the%2520unique%2520characteristics%2520of%2520both%2520modalities%252C%2520our%2520framework%250Adevises%2520a%2520self-paced%2520learning%2520pipeline%2520that%2520incorporates%2520adaptive%2520sampling%2520and%250Aweak%2520model%2520aggregation%2520strategies.%2520The%2520adaptive%2520sampling%2520strategy%2520dynamically%250Atunes%2520the%2520distribution%2520of%2520pseudo%2520labels%2520during%2520training%252C%2520countering%2520the%250Atendency%2520of%2520models%2520to%2520overfit%2520easily%2520detected%2520samples%252C%2520such%2520as%2520nearby%2520and%250Alarge-sized%2520objects.%2520By%2520doing%2520so%252C%2520it%2520ensures%2520a%2520balanced%2520learning%2520trajectory%250Aacross%2520varying%2520object%2520scales%2520and%2520distances.%2520The%2520weak%2520model%2520aggregation%250Acomponent%2520consolidates%2520the%2520strengths%2520of%2520models%2520trained%2520under%2520different%2520pseudo%250Alabel%2520distributions%252C%2520culminating%2520in%2520a%2520robust%2520and%2520powerful%2520final%2520model.%250AExperimental%2520evaluations%2520validate%2520the%2520efficacy%2520of%2520our%2520proposed%2520LiSe%2520method%252C%250Amanifesting%2520significant%2520improvements%2520of%2520%252B7.1%2525%2520AP%2524_%257BBEV%257D%2524%2520and%2520%252B3.4%2525%2520AP%2524_%257B3D%257D%2524%2520on%250AnuScenes%252C%2520and%2520%252B8.3%2525%2520AP%2524_%257BBEV%257D%2524%2520and%2520%252B7.4%2525%2520AP%2524_%257B3D%257D%2524%2520on%2520Lyft%2520compared%2520to%2520existing%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approaching%20Outside%3A%20Scaling%20Unsupervised%203D%20Object%20Detection%20from%202D%0A%20%20Scene&entry.906535625=Ruiyang%20Zhang%20and%20Hu%20Zhang%20and%20Hang%20Yu%20and%20Zhedong%20Zheng&entry.1292438233=%20%20The%20unsupervised%203D%20object%20detection%20is%20to%20accurately%20detect%20objects%20in%0Aunstructured%20environments%20with%20no%20explicit%20supervisory%20signals.%20This%20task%2C%0Agiven%20sparse%20LiDAR%20point%20clouds%2C%20often%20results%20in%20compromised%20performance%20for%0Adetecting%20distant%20or%20small%20objects%20due%20to%20the%20inherent%20sparsity%20and%20limited%0Aspatial%20resolution.%20In%20this%20paper%2C%20we%20are%20among%20the%20early%20attempts%20to%20integrate%0ALiDAR%20data%20with%202D%20images%20for%20unsupervised%203D%20detection%20and%20introduce%20a%20new%0Amethod%2C%20dubbed%20LiDAR-2D%20Self-paced%20Learning%20%28LiSe%29.%20We%20argue%20that%20RGB%20images%0Aserve%20as%20a%20valuable%20complement%20to%20LiDAR%20data%2C%20offering%20precise%202D%20localization%0Acues%2C%20particularly%20when%20scarce%20LiDAR%20points%20are%20available%20for%20certain%20objects.%0AConsidering%20the%20unique%20characteristics%20of%20both%20modalities%2C%20our%20framework%0Adevises%20a%20self-paced%20learning%20pipeline%20that%20incorporates%20adaptive%20sampling%20and%0Aweak%20model%20aggregation%20strategies.%20The%20adaptive%20sampling%20strategy%20dynamically%0Atunes%20the%20distribution%20of%20pseudo%20labels%20during%20training%2C%20countering%20the%0Atendency%20of%20models%20to%20overfit%20easily%20detected%20samples%2C%20such%20as%20nearby%20and%0Alarge-sized%20objects.%20By%20doing%20so%2C%20it%20ensures%20a%20balanced%20learning%20trajectory%0Aacross%20varying%20object%20scales%20and%20distances.%20The%20weak%20model%20aggregation%0Acomponent%20consolidates%20the%20strengths%20of%20models%20trained%20under%20different%20pseudo%0Alabel%20distributions%2C%20culminating%20in%20a%20robust%20and%20powerful%20final%20model.%0AExperimental%20evaluations%20validate%20the%20efficacy%20of%20our%20proposed%20LiSe%20method%2C%0Amanifesting%20significant%20improvements%20of%20%2B7.1%25%20AP%24_%7BBEV%7D%24%20and%20%2B3.4%25%20AP%24_%7B3D%7D%24%20on%0AnuScenes%2C%20and%20%2B8.3%25%20AP%24_%7BBEV%7D%24%20and%20%2B7.4%25%20AP%24_%7B3D%7D%24%20on%20Lyft%20compared%20to%20existing%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08569v1&entry.124074799=Read"},
{"title": "Spine Vision X-Ray Image based GUI Planning of Pedicle Screws Using\n  Enhanced YOLOv5 for Vertebrae Segmentation", "author": "Yashwanth Rao and Gaurisankar S and Durga R and Aparna Purayath and Vivek Maik and Manojkumar Lakshmanan and Mohanasankar Sivaprakasm", "abstract": "  In this paper, we propose an innovative Graphical User Interface (GUI) aimed\nat improving preoperative planning and intra-operative guidance for precise\nspinal screw placement through vertebrae segmentation. The methodology\nencompasses both front-end and back-end computations. The front end comprises a\nGUI that allows surgeons to precisely adjust the placement of screws on X-Ray\nimages, thereby improving the simulation of surgical screw insertion in the\npatient's spine. On the other hand, the back-end processing involves several\nsteps, including acquiring spinal X-ray images, performing pre-processing\ntechniques to reduce noise, and training a neural network model to achieve\nreal-time segmentation of the vertebrae. The integration of vertebral\nsegmentation in the GUI ensures precise screw placement, reducing complications\nlike nerve injury and ultimately improving surgical outcomes. The Spine-Vision\nprovides a comprehensive solution with innovative features like synchronous\nAP-LP planning, accurate screw positioning via vertebrae segmentation,\neffective screw visualization, and dynamic position adjustments. This X-ray\nimage-based GUI workflow emerges as a valuable tool, enhancing precision and\nsafety in spinal screw placement and planning procedures.\n", "link": "http://arxiv.org/abs/2407.08349v1", "date": "2024-07-11", "relevancy": 2.4413, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.524}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4738}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spine%20Vision%20X-Ray%20Image%20based%20GUI%20Planning%20of%20Pedicle%20Screws%20Using%0A%20%20Enhanced%20YOLOv5%20for%20Vertebrae%20Segmentation&body=Title%3A%20Spine%20Vision%20X-Ray%20Image%20based%20GUI%20Planning%20of%20Pedicle%20Screws%20Using%0A%20%20Enhanced%20YOLOv5%20for%20Vertebrae%20Segmentation%0AAuthor%3A%20Yashwanth%20Rao%20and%20Gaurisankar%20S%20and%20Durga%20R%20and%20Aparna%20Purayath%20and%20Vivek%20Maik%20and%20Manojkumar%20Lakshmanan%20and%20Mohanasankar%20Sivaprakasm%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20innovative%20Graphical%20User%20Interface%20%28GUI%29%20aimed%0Aat%20improving%20preoperative%20planning%20and%20intra-operative%20guidance%20for%20precise%0Aspinal%20screw%20placement%20through%20vertebrae%20segmentation.%20The%20methodology%0Aencompasses%20both%20front-end%20and%20back-end%20computations.%20The%20front%20end%20comprises%20a%0AGUI%20that%20allows%20surgeons%20to%20precisely%20adjust%20the%20placement%20of%20screws%20on%20X-Ray%0Aimages%2C%20thereby%20improving%20the%20simulation%20of%20surgical%20screw%20insertion%20in%20the%0Apatient%27s%20spine.%20On%20the%20other%20hand%2C%20the%20back-end%20processing%20involves%20several%0Asteps%2C%20including%20acquiring%20spinal%20X-ray%20images%2C%20performing%20pre-processing%0Atechniques%20to%20reduce%20noise%2C%20and%20training%20a%20neural%20network%20model%20to%20achieve%0Areal-time%20segmentation%20of%20the%20vertebrae.%20The%20integration%20of%20vertebral%0Asegmentation%20in%20the%20GUI%20ensures%20precise%20screw%20placement%2C%20reducing%20complications%0Alike%20nerve%20injury%20and%20ultimately%20improving%20surgical%20outcomes.%20The%20Spine-Vision%0Aprovides%20a%20comprehensive%20solution%20with%20innovative%20features%20like%20synchronous%0AAP-LP%20planning%2C%20accurate%20screw%20positioning%20via%20vertebrae%20segmentation%2C%0Aeffective%20screw%20visualization%2C%20and%20dynamic%20position%20adjustments.%20This%20X-ray%0Aimage-based%20GUI%20workflow%20emerges%20as%20a%20valuable%20tool%2C%20enhancing%20precision%20and%0Asafety%20in%20spinal%20screw%20placement%20and%20planning%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpine%2520Vision%2520X-Ray%2520Image%2520based%2520GUI%2520Planning%2520of%2520Pedicle%2520Screws%2520Using%250A%2520%2520Enhanced%2520YOLOv5%2520for%2520Vertebrae%2520Segmentation%26entry.906535625%3DYashwanth%2520Rao%2520and%2520Gaurisankar%2520S%2520and%2520Durga%2520R%2520and%2520Aparna%2520Purayath%2520and%2520Vivek%2520Maik%2520and%2520Manojkumar%2520Lakshmanan%2520and%2520Mohanasankar%2520Sivaprakasm%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520innovative%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520aimed%250Aat%2520improving%2520preoperative%2520planning%2520and%2520intra-operative%2520guidance%2520for%2520precise%250Aspinal%2520screw%2520placement%2520through%2520vertebrae%2520segmentation.%2520The%2520methodology%250Aencompasses%2520both%2520front-end%2520and%2520back-end%2520computations.%2520The%2520front%2520end%2520comprises%2520a%250AGUI%2520that%2520allows%2520surgeons%2520to%2520precisely%2520adjust%2520the%2520placement%2520of%2520screws%2520on%2520X-Ray%250Aimages%252C%2520thereby%2520improving%2520the%2520simulation%2520of%2520surgical%2520screw%2520insertion%2520in%2520the%250Apatient%2527s%2520spine.%2520On%2520the%2520other%2520hand%252C%2520the%2520back-end%2520processing%2520involves%2520several%250Asteps%252C%2520including%2520acquiring%2520spinal%2520X-ray%2520images%252C%2520performing%2520pre-processing%250Atechniques%2520to%2520reduce%2520noise%252C%2520and%2520training%2520a%2520neural%2520network%2520model%2520to%2520achieve%250Areal-time%2520segmentation%2520of%2520the%2520vertebrae.%2520The%2520integration%2520of%2520vertebral%250Asegmentation%2520in%2520the%2520GUI%2520ensures%2520precise%2520screw%2520placement%252C%2520reducing%2520complications%250Alike%2520nerve%2520injury%2520and%2520ultimately%2520improving%2520surgical%2520outcomes.%2520The%2520Spine-Vision%250Aprovides%2520a%2520comprehensive%2520solution%2520with%2520innovative%2520features%2520like%2520synchronous%250AAP-LP%2520planning%252C%2520accurate%2520screw%2520positioning%2520via%2520vertebrae%2520segmentation%252C%250Aeffective%2520screw%2520visualization%252C%2520and%2520dynamic%2520position%2520adjustments.%2520This%2520X-ray%250Aimage-based%2520GUI%2520workflow%2520emerges%2520as%2520a%2520valuable%2520tool%252C%2520enhancing%2520precision%2520and%250Asafety%2520in%2520spinal%2520screw%2520placement%2520and%2520planning%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spine%20Vision%20X-Ray%20Image%20based%20GUI%20Planning%20of%20Pedicle%20Screws%20Using%0A%20%20Enhanced%20YOLOv5%20for%20Vertebrae%20Segmentation&entry.906535625=Yashwanth%20Rao%20and%20Gaurisankar%20S%20and%20Durga%20R%20and%20Aparna%20Purayath%20and%20Vivek%20Maik%20and%20Manojkumar%20Lakshmanan%20and%20Mohanasankar%20Sivaprakasm&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20innovative%20Graphical%20User%20Interface%20%28GUI%29%20aimed%0Aat%20improving%20preoperative%20planning%20and%20intra-operative%20guidance%20for%20precise%0Aspinal%20screw%20placement%20through%20vertebrae%20segmentation.%20The%20methodology%0Aencompasses%20both%20front-end%20and%20back-end%20computations.%20The%20front%20end%20comprises%20a%0AGUI%20that%20allows%20surgeons%20to%20precisely%20adjust%20the%20placement%20of%20screws%20on%20X-Ray%0Aimages%2C%20thereby%20improving%20the%20simulation%20of%20surgical%20screw%20insertion%20in%20the%0Apatient%27s%20spine.%20On%20the%20other%20hand%2C%20the%20back-end%20processing%20involves%20several%0Asteps%2C%20including%20acquiring%20spinal%20X-ray%20images%2C%20performing%20pre-processing%0Atechniques%20to%20reduce%20noise%2C%20and%20training%20a%20neural%20network%20model%20to%20achieve%0Areal-time%20segmentation%20of%20the%20vertebrae.%20The%20integration%20of%20vertebral%0Asegmentation%20in%20the%20GUI%20ensures%20precise%20screw%20placement%2C%20reducing%20complications%0Alike%20nerve%20injury%20and%20ultimately%20improving%20surgical%20outcomes.%20The%20Spine-Vision%0Aprovides%20a%20comprehensive%20solution%20with%20innovative%20features%20like%20synchronous%0AAP-LP%20planning%2C%20accurate%20screw%20positioning%20via%20vertebrae%20segmentation%2C%0Aeffective%20screw%20visualization%2C%20and%20dynamic%20position%20adjustments.%20This%20X-ray%0Aimage-based%20GUI%20workflow%20emerges%20as%20a%20valuable%20tool%2C%20enhancing%20precision%20and%0Asafety%20in%20spinal%20screw%20placement%20and%20planning%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08349v1&entry.124074799=Read"},
{"title": "Extracting Training Data from Document-Based VQA Models", "author": "Francesco Pinto and Nathalie Rauschmayr and Florian Tram\u00e8r and Philip Torr and Federico Tombari", "abstract": "  Vision-Language Models (VLMs) have made remarkable progress in document-based\nVisual Question Answering (i.e., responding to queries about the contents of an\ninput document provided as an image). In this work, we show these models can\nmemorize responses for training samples and regurgitate them even when the\nrelevant visual information has been removed. This includes Personal\nIdentifiable Information (PII) repeated once in the training set, indicating\nthese models could divulge memorised sensitive information and therefore pose a\nprivacy risk. We quantitatively measure the extractability of information in\ncontrolled experiments and differentiate between cases where it arises from\ngeneralization capabilities or from memorization. We further investigate the\nfactors that influence memorization across multiple state-of-the-art models and\npropose an effective heuristic countermeasure that empirically prevents the\nextractability of PII.\n", "link": "http://arxiv.org/abs/2407.08707v1", "date": "2024-07-11", "relevancy": 2.4313, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4929}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4899}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20Training%20Data%20from%20Document-Based%20VQA%20Models&body=Title%3A%20Extracting%20Training%20Data%20from%20Document-Based%20VQA%20Models%0AAuthor%3A%20Francesco%20Pinto%20and%20Nathalie%20Rauschmayr%20and%20Florian%20Tram%C3%A8r%20and%20Philip%20Torr%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20made%20remarkable%20progress%20in%20document-based%0AVisual%20Question%20Answering%20%28i.e.%2C%20responding%20to%20queries%20about%20the%20contents%20of%20an%0Ainput%20document%20provided%20as%20an%20image%29.%20In%20this%20work%2C%20we%20show%20these%20models%20can%0Amemorize%20responses%20for%20training%20samples%20and%20regurgitate%20them%20even%20when%20the%0Arelevant%20visual%20information%20has%20been%20removed.%20This%20includes%20Personal%0AIdentifiable%20Information%20%28PII%29%20repeated%20once%20in%20the%20training%20set%2C%20indicating%0Athese%20models%20could%20divulge%20memorised%20sensitive%20information%20and%20therefore%20pose%20a%0Aprivacy%20risk.%20We%20quantitatively%20measure%20the%20extractability%20of%20information%20in%0Acontrolled%20experiments%20and%20differentiate%20between%20cases%20where%20it%20arises%20from%0Ageneralization%20capabilities%20or%20from%20memorization.%20We%20further%20investigate%20the%0Afactors%20that%20influence%20memorization%20across%20multiple%20state-of-the-art%20models%20and%0Apropose%20an%20effective%20heuristic%20countermeasure%20that%20empirically%20prevents%20the%0Aextractability%20of%20PII.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520Training%2520Data%2520from%2520Document-Based%2520VQA%2520Models%26entry.906535625%3DFrancesco%2520Pinto%2520and%2520Nathalie%2520Rauschmayr%2520and%2520Florian%2520Tram%25C3%25A8r%2520and%2520Philip%2520Torr%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520made%2520remarkable%2520progress%2520in%2520document-based%250AVisual%2520Question%2520Answering%2520%2528i.e.%252C%2520responding%2520to%2520queries%2520about%2520the%2520contents%2520of%2520an%250Ainput%2520document%2520provided%2520as%2520an%2520image%2529.%2520In%2520this%2520work%252C%2520we%2520show%2520these%2520models%2520can%250Amemorize%2520responses%2520for%2520training%2520samples%2520and%2520regurgitate%2520them%2520even%2520when%2520the%250Arelevant%2520visual%2520information%2520has%2520been%2520removed.%2520This%2520includes%2520Personal%250AIdentifiable%2520Information%2520%2528PII%2529%2520repeated%2520once%2520in%2520the%2520training%2520set%252C%2520indicating%250Athese%2520models%2520could%2520divulge%2520memorised%2520sensitive%2520information%2520and%2520therefore%2520pose%2520a%250Aprivacy%2520risk.%2520We%2520quantitatively%2520measure%2520the%2520extractability%2520of%2520information%2520in%250Acontrolled%2520experiments%2520and%2520differentiate%2520between%2520cases%2520where%2520it%2520arises%2520from%250Ageneralization%2520capabilities%2520or%2520from%2520memorization.%2520We%2520further%2520investigate%2520the%250Afactors%2520that%2520influence%2520memorization%2520across%2520multiple%2520state-of-the-art%2520models%2520and%250Apropose%2520an%2520effective%2520heuristic%2520countermeasure%2520that%2520empirically%2520prevents%2520the%250Aextractability%2520of%2520PII.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Training%20Data%20from%20Document-Based%20VQA%20Models&entry.906535625=Francesco%20Pinto%20and%20Nathalie%20Rauschmayr%20and%20Florian%20Tram%C3%A8r%20and%20Philip%20Torr%20and%20Federico%20Tombari&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20made%20remarkable%20progress%20in%20document-based%0AVisual%20Question%20Answering%20%28i.e.%2C%20responding%20to%20queries%20about%20the%20contents%20of%20an%0Ainput%20document%20provided%20as%20an%20image%29.%20In%20this%20work%2C%20we%20show%20these%20models%20can%0Amemorize%20responses%20for%20training%20samples%20and%20regurgitate%20them%20even%20when%20the%0Arelevant%20visual%20information%20has%20been%20removed.%20This%20includes%20Personal%0AIdentifiable%20Information%20%28PII%29%20repeated%20once%20in%20the%20training%20set%2C%20indicating%0Athese%20models%20could%20divulge%20memorised%20sensitive%20information%20and%20therefore%20pose%20a%0Aprivacy%20risk.%20We%20quantitatively%20measure%20the%20extractability%20of%20information%20in%0Acontrolled%20experiments%20and%20differentiate%20between%20cases%20where%20it%20arises%20from%0Ageneralization%20capabilities%20or%20from%20memorization.%20We%20further%20investigate%20the%0Afactors%20that%20influence%20memorization%20across%20multiple%20state-of-the-art%20models%20and%0Apropose%20an%20effective%20heuristic%20countermeasure%20that%20empirically%20prevents%20the%0Aextractability%20of%20PII.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08707v1&entry.124074799=Read"},
{"title": "ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to\n  Video", "author": "Xinhao Li and Yuhan Zhu and Limin Wang", "abstract": "  Adapting image models to the video domain has emerged as an efficient\nparadigm for solving video recognition tasks. Due to the huge number of\nparameters and effective transferability of image models, performing full\nfine-tuning is less efficient and even unnecessary. Thus, recent research is\nshifting its focus toward parameter-efficient image-to-video adaptation.\nHowever, these adaptation strategies inevitably introduce extra computational\ncosts to deal with the domain gap and temporal modeling in videos. In this\npaper, we present a new adaptation paradigm (ZeroI2V) to transfer the image\ntransformers to video recognition tasks (i.e., introduce zero extra cost to the\noriginal models during inference). To achieve this goal, we present two core\ndesigns. First, to capture the dynamics in videos and reduce the difficulty of\nimage-to-video adaptation, we exploit the flexibility of self-attention and\nintroduce spatial-temporal dual-headed attention (STDHA). This approach\nefficiently endows the image transformers with temporal modeling capability at\nzero extra parameters and computation. Second, to handle the domain gap between\nimages and videos, we propose a linear adaption strategy that utilizes\nlightweight densely placed linear adapters to fully transfer the frozen image\nmodels to video recognition. Thanks to the customized linear design, all newly\nadded adapters could be easily merged with the original modules through\nstructural reparameterization after training, enabling zero extra cost during\ninference. Extensive experiments on representative fully-supervised and\nfew-shot video recognition benchmarks showcase that ZeroI2V can match or even\noutperform previous state-of-the-art methods while enjoying superior parameter\nand inference efficiency.\n", "link": "http://arxiv.org/abs/2310.01324v2", "date": "2024-07-11", "relevancy": 2.4199, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6153}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6094}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZeroI2V%3A%20Zero-Cost%20Adaptation%20of%20Pre-trained%20Transformers%20from%20Image%20to%0A%20%20Video&body=Title%3A%20ZeroI2V%3A%20Zero-Cost%20Adaptation%20of%20Pre-trained%20Transformers%20from%20Image%20to%0A%20%20Video%0AAuthor%3A%20Xinhao%20Li%20and%20Yuhan%20Zhu%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Adapting%20image%20models%20to%20the%20video%20domain%20has%20emerged%20as%20an%20efficient%0Aparadigm%20for%20solving%20video%20recognition%20tasks.%20Due%20to%20the%20huge%20number%20of%0Aparameters%20and%20effective%20transferability%20of%20image%20models%2C%20performing%20full%0Afine-tuning%20is%20less%20efficient%20and%20even%20unnecessary.%20Thus%2C%20recent%20research%20is%0Ashifting%20its%20focus%20toward%20parameter-efficient%20image-to-video%20adaptation.%0AHowever%2C%20these%20adaptation%20strategies%20inevitably%20introduce%20extra%20computational%0Acosts%20to%20deal%20with%20the%20domain%20gap%20and%20temporal%20modeling%20in%20videos.%20In%20this%0Apaper%2C%20we%20present%20a%20new%20adaptation%20paradigm%20%28ZeroI2V%29%20to%20transfer%20the%20image%0Atransformers%20to%20video%20recognition%20tasks%20%28i.e.%2C%20introduce%20zero%20extra%20cost%20to%20the%0Aoriginal%20models%20during%20inference%29.%20To%20achieve%20this%20goal%2C%20we%20present%20two%20core%0Adesigns.%20First%2C%20to%20capture%20the%20dynamics%20in%20videos%20and%20reduce%20the%20difficulty%20of%0Aimage-to-video%20adaptation%2C%20we%20exploit%20the%20flexibility%20of%20self-attention%20and%0Aintroduce%20spatial-temporal%20dual-headed%20attention%20%28STDHA%29.%20This%20approach%0Aefficiently%20endows%20the%20image%20transformers%20with%20temporal%20modeling%20capability%20at%0Azero%20extra%20parameters%20and%20computation.%20Second%2C%20to%20handle%20the%20domain%20gap%20between%0Aimages%20and%20videos%2C%20we%20propose%20a%20linear%20adaption%20strategy%20that%20utilizes%0Alightweight%20densely%20placed%20linear%20adapters%20to%20fully%20transfer%20the%20frozen%20image%0Amodels%20to%20video%20recognition.%20Thanks%20to%20the%20customized%20linear%20design%2C%20all%20newly%0Aadded%20adapters%20could%20be%20easily%20merged%20with%20the%20original%20modules%20through%0Astructural%20reparameterization%20after%20training%2C%20enabling%20zero%20extra%20cost%20during%0Ainference.%20Extensive%20experiments%20on%20representative%20fully-supervised%20and%0Afew-shot%20video%20recognition%20benchmarks%20showcase%20that%20ZeroI2V%20can%20match%20or%20even%0Aoutperform%20previous%20state-of-the-art%20methods%20while%20enjoying%20superior%20parameter%0Aand%20inference%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroI2V%253A%2520Zero-Cost%2520Adaptation%2520of%2520Pre-trained%2520Transformers%2520from%2520Image%2520to%250A%2520%2520Video%26entry.906535625%3DXinhao%2520Li%2520and%2520Yuhan%2520Zhu%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Adapting%2520image%2520models%2520to%2520the%2520video%2520domain%2520has%2520emerged%2520as%2520an%2520efficient%250Aparadigm%2520for%2520solving%2520video%2520recognition%2520tasks.%2520Due%2520to%2520the%2520huge%2520number%2520of%250Aparameters%2520and%2520effective%2520transferability%2520of%2520image%2520models%252C%2520performing%2520full%250Afine-tuning%2520is%2520less%2520efficient%2520and%2520even%2520unnecessary.%2520Thus%252C%2520recent%2520research%2520is%250Ashifting%2520its%2520focus%2520toward%2520parameter-efficient%2520image-to-video%2520adaptation.%250AHowever%252C%2520these%2520adaptation%2520strategies%2520inevitably%2520introduce%2520extra%2520computational%250Acosts%2520to%2520deal%2520with%2520the%2520domain%2520gap%2520and%2520temporal%2520modeling%2520in%2520videos.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520new%2520adaptation%2520paradigm%2520%2528ZeroI2V%2529%2520to%2520transfer%2520the%2520image%250Atransformers%2520to%2520video%2520recognition%2520tasks%2520%2528i.e.%252C%2520introduce%2520zero%2520extra%2520cost%2520to%2520the%250Aoriginal%2520models%2520during%2520inference%2529.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520present%2520two%2520core%250Adesigns.%2520First%252C%2520to%2520capture%2520the%2520dynamics%2520in%2520videos%2520and%2520reduce%2520the%2520difficulty%2520of%250Aimage-to-video%2520adaptation%252C%2520we%2520exploit%2520the%2520flexibility%2520of%2520self-attention%2520and%250Aintroduce%2520spatial-temporal%2520dual-headed%2520attention%2520%2528STDHA%2529.%2520This%2520approach%250Aefficiently%2520endows%2520the%2520image%2520transformers%2520with%2520temporal%2520modeling%2520capability%2520at%250Azero%2520extra%2520parameters%2520and%2520computation.%2520Second%252C%2520to%2520handle%2520the%2520domain%2520gap%2520between%250Aimages%2520and%2520videos%252C%2520we%2520propose%2520a%2520linear%2520adaption%2520strategy%2520that%2520utilizes%250Alightweight%2520densely%2520placed%2520linear%2520adapters%2520to%2520fully%2520transfer%2520the%2520frozen%2520image%250Amodels%2520to%2520video%2520recognition.%2520Thanks%2520to%2520the%2520customized%2520linear%2520design%252C%2520all%2520newly%250Aadded%2520adapters%2520could%2520be%2520easily%2520merged%2520with%2520the%2520original%2520modules%2520through%250Astructural%2520reparameterization%2520after%2520training%252C%2520enabling%2520zero%2520extra%2520cost%2520during%250Ainference.%2520Extensive%2520experiments%2520on%2520representative%2520fully-supervised%2520and%250Afew-shot%2520video%2520recognition%2520benchmarks%2520showcase%2520that%2520ZeroI2V%2520can%2520match%2520or%2520even%250Aoutperform%2520previous%2520state-of-the-art%2520methods%2520while%2520enjoying%2520superior%2520parameter%250Aand%2520inference%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZeroI2V%3A%20Zero-Cost%20Adaptation%20of%20Pre-trained%20Transformers%20from%20Image%20to%0A%20%20Video&entry.906535625=Xinhao%20Li%20and%20Yuhan%20Zhu%20and%20Limin%20Wang&entry.1292438233=%20%20Adapting%20image%20models%20to%20the%20video%20domain%20has%20emerged%20as%20an%20efficient%0Aparadigm%20for%20solving%20video%20recognition%20tasks.%20Due%20to%20the%20huge%20number%20of%0Aparameters%20and%20effective%20transferability%20of%20image%20models%2C%20performing%20full%0Afine-tuning%20is%20less%20efficient%20and%20even%20unnecessary.%20Thus%2C%20recent%20research%20is%0Ashifting%20its%20focus%20toward%20parameter-efficient%20image-to-video%20adaptation.%0AHowever%2C%20these%20adaptation%20strategies%20inevitably%20introduce%20extra%20computational%0Acosts%20to%20deal%20with%20the%20domain%20gap%20and%20temporal%20modeling%20in%20videos.%20In%20this%0Apaper%2C%20we%20present%20a%20new%20adaptation%20paradigm%20%28ZeroI2V%29%20to%20transfer%20the%20image%0Atransformers%20to%20video%20recognition%20tasks%20%28i.e.%2C%20introduce%20zero%20extra%20cost%20to%20the%0Aoriginal%20models%20during%20inference%29.%20To%20achieve%20this%20goal%2C%20we%20present%20two%20core%0Adesigns.%20First%2C%20to%20capture%20the%20dynamics%20in%20videos%20and%20reduce%20the%20difficulty%20of%0Aimage-to-video%20adaptation%2C%20we%20exploit%20the%20flexibility%20of%20self-attention%20and%0Aintroduce%20spatial-temporal%20dual-headed%20attention%20%28STDHA%29.%20This%20approach%0Aefficiently%20endows%20the%20image%20transformers%20with%20temporal%20modeling%20capability%20at%0Azero%20extra%20parameters%20and%20computation.%20Second%2C%20to%20handle%20the%20domain%20gap%20between%0Aimages%20and%20videos%2C%20we%20propose%20a%20linear%20adaption%20strategy%20that%20utilizes%0Alightweight%20densely%20placed%20linear%20adapters%20to%20fully%20transfer%20the%20frozen%20image%0Amodels%20to%20video%20recognition.%20Thanks%20to%20the%20customized%20linear%20design%2C%20all%20newly%0Aadded%20adapters%20could%20be%20easily%20merged%20with%20the%20original%20modules%20through%0Astructural%20reparameterization%20after%20training%2C%20enabling%20zero%20extra%20cost%20during%0Ainference.%20Extensive%20experiments%20on%20representative%20fully-supervised%20and%0Afew-shot%20video%20recognition%20benchmarks%20showcase%20that%20ZeroI2V%20can%20match%20or%20even%0Aoutperform%20previous%20state-of-the-art%20methods%20while%20enjoying%20superior%20parameter%0Aand%20inference%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01324v2&entry.124074799=Read"},
{"title": "Long-range Turbulence Mitigation: A Large-scale Dataset and A\n  Coarse-to-fine Framework", "author": "Shengqi Xu and Run Sun and Yi Chang and Shuning Cao and Xueyao Xiao and Luxin Yan", "abstract": "  Long-range imaging inevitably suffers from atmospheric turbulence with severe\ngeometric distortions due to random refraction of light. The further the\ndistance, the more severe the disturbance. Despite existing research has\nachieved great progress in tackling short-range turbulence, there is less\nattention paid to long-range turbulence with significant distortions. To\naddress this dilemma and advance the field, we construct a large-scale real\nlong-range atmospheric turbulence dataset (RLR-AT), including 1500 turbulence\nsequences spanning distances from 1 Km to 13 Km. The advantages of RLR-AT\ncompared to existing ones: turbulence with longer-distances and\nhigher-diversity, scenes with greater-variety and larger-scale. Moreover, most\nexisting work adopts either registration-based or decomposition-based methods\nto address distortions through one-step mitigation. However, they fail to\neffectively handle long-range turbulence due to its significant pixel\ndisplacements. In this work, we propose a coarse-to-fine framework to handle\nsevere distortions, which cooperates dynamic turbulence and static background\npriors (CDSP). On the one hand, we discover the pixel motion statistical prior\nof turbulence, and propose a frequency-aware reference frame for better\nlarge-scale distortion registration, greatly reducing the burden of refinement.\nOn the other hand, we take advantage of the static prior of background, and\npropose a subspace-based low-rank tensor refinement model to eliminate the\nmisalignments inevitably left by registration while well preserving details.\nThe dynamic and static priors complement to each other, facilitating us to\nprogressively mitigate long-range turbulence with severe distortions. Extensive\nexperiments demonstrate that the proposed method outperforms SOTA methods on\ndifferent datasets.\n", "link": "http://arxiv.org/abs/2407.08377v1", "date": "2024-07-11", "relevancy": 2.3391, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6047}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-range%20Turbulence%20Mitigation%3A%20A%20Large-scale%20Dataset%20and%20A%0A%20%20Coarse-to-fine%20Framework&body=Title%3A%20Long-range%20Turbulence%20Mitigation%3A%20A%20Large-scale%20Dataset%20and%20A%0A%20%20Coarse-to-fine%20Framework%0AAuthor%3A%20Shengqi%20Xu%20and%20Run%20Sun%20and%20Yi%20Chang%20and%20Shuning%20Cao%20and%20Xueyao%20Xiao%20and%20Luxin%20Yan%0AAbstract%3A%20%20%20Long-range%20imaging%20inevitably%20suffers%20from%20atmospheric%20turbulence%20with%20severe%0Ageometric%20distortions%20due%20to%20random%20refraction%20of%20light.%20The%20further%20the%0Adistance%2C%20the%20more%20severe%20the%20disturbance.%20Despite%20existing%20research%20has%0Aachieved%20great%20progress%20in%20tackling%20short-range%20turbulence%2C%20there%20is%20less%0Aattention%20paid%20to%20long-range%20turbulence%20with%20significant%20distortions.%20To%0Aaddress%20this%20dilemma%20and%20advance%20the%20field%2C%20we%20construct%20a%20large-scale%20real%0Along-range%20atmospheric%20turbulence%20dataset%20%28RLR-AT%29%2C%20including%201500%20turbulence%0Asequences%20spanning%20distances%20from%201%20Km%20to%2013%20Km.%20The%20advantages%20of%20RLR-AT%0Acompared%20to%20existing%20ones%3A%20turbulence%20with%20longer-distances%20and%0Ahigher-diversity%2C%20scenes%20with%20greater-variety%20and%20larger-scale.%20Moreover%2C%20most%0Aexisting%20work%20adopts%20either%20registration-based%20or%20decomposition-based%20methods%0Ato%20address%20distortions%20through%20one-step%20mitigation.%20However%2C%20they%20fail%20to%0Aeffectively%20handle%20long-range%20turbulence%20due%20to%20its%20significant%20pixel%0Adisplacements.%20In%20this%20work%2C%20we%20propose%20a%20coarse-to-fine%20framework%20to%20handle%0Asevere%20distortions%2C%20which%20cooperates%20dynamic%20turbulence%20and%20static%20background%0Apriors%20%28CDSP%29.%20On%20the%20one%20hand%2C%20we%20discover%20the%20pixel%20motion%20statistical%20prior%0Aof%20turbulence%2C%20and%20propose%20a%20frequency-aware%20reference%20frame%20for%20better%0Alarge-scale%20distortion%20registration%2C%20greatly%20reducing%20the%20burden%20of%20refinement.%0AOn%20the%20other%20hand%2C%20we%20take%20advantage%20of%20the%20static%20prior%20of%20background%2C%20and%0Apropose%20a%20subspace-based%20low-rank%20tensor%20refinement%20model%20to%20eliminate%20the%0Amisalignments%20inevitably%20left%20by%20registration%20while%20well%20preserving%20details.%0AThe%20dynamic%20and%20static%20priors%20complement%20to%20each%20other%2C%20facilitating%20us%20to%0Aprogressively%20mitigate%20long-range%20turbulence%20with%20severe%20distortions.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20method%20outperforms%20SOTA%20methods%20on%0Adifferent%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-range%2520Turbulence%2520Mitigation%253A%2520A%2520Large-scale%2520Dataset%2520and%2520A%250A%2520%2520Coarse-to-fine%2520Framework%26entry.906535625%3DShengqi%2520Xu%2520and%2520Run%2520Sun%2520and%2520Yi%2520Chang%2520and%2520Shuning%2520Cao%2520and%2520Xueyao%2520Xiao%2520and%2520Luxin%2520Yan%26entry.1292438233%3D%2520%2520Long-range%2520imaging%2520inevitably%2520suffers%2520from%2520atmospheric%2520turbulence%2520with%2520severe%250Ageometric%2520distortions%2520due%2520to%2520random%2520refraction%2520of%2520light.%2520The%2520further%2520the%250Adistance%252C%2520the%2520more%2520severe%2520the%2520disturbance.%2520Despite%2520existing%2520research%2520has%250Aachieved%2520great%2520progress%2520in%2520tackling%2520short-range%2520turbulence%252C%2520there%2520is%2520less%250Aattention%2520paid%2520to%2520long-range%2520turbulence%2520with%2520significant%2520distortions.%2520To%250Aaddress%2520this%2520dilemma%2520and%2520advance%2520the%2520field%252C%2520we%2520construct%2520a%2520large-scale%2520real%250Along-range%2520atmospheric%2520turbulence%2520dataset%2520%2528RLR-AT%2529%252C%2520including%25201500%2520turbulence%250Asequences%2520spanning%2520distances%2520from%25201%2520Km%2520to%252013%2520Km.%2520The%2520advantages%2520of%2520RLR-AT%250Acompared%2520to%2520existing%2520ones%253A%2520turbulence%2520with%2520longer-distances%2520and%250Ahigher-diversity%252C%2520scenes%2520with%2520greater-variety%2520and%2520larger-scale.%2520Moreover%252C%2520most%250Aexisting%2520work%2520adopts%2520either%2520registration-based%2520or%2520decomposition-based%2520methods%250Ato%2520address%2520distortions%2520through%2520one-step%2520mitigation.%2520However%252C%2520they%2520fail%2520to%250Aeffectively%2520handle%2520long-range%2520turbulence%2520due%2520to%2520its%2520significant%2520pixel%250Adisplacements.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520coarse-to-fine%2520framework%2520to%2520handle%250Asevere%2520distortions%252C%2520which%2520cooperates%2520dynamic%2520turbulence%2520and%2520static%2520background%250Apriors%2520%2528CDSP%2529.%2520On%2520the%2520one%2520hand%252C%2520we%2520discover%2520the%2520pixel%2520motion%2520statistical%2520prior%250Aof%2520turbulence%252C%2520and%2520propose%2520a%2520frequency-aware%2520reference%2520frame%2520for%2520better%250Alarge-scale%2520distortion%2520registration%252C%2520greatly%2520reducing%2520the%2520burden%2520of%2520refinement.%250AOn%2520the%2520other%2520hand%252C%2520we%2520take%2520advantage%2520of%2520the%2520static%2520prior%2520of%2520background%252C%2520and%250Apropose%2520a%2520subspace-based%2520low-rank%2520tensor%2520refinement%2520model%2520to%2520eliminate%2520the%250Amisalignments%2520inevitably%2520left%2520by%2520registration%2520while%2520well%2520preserving%2520details.%250AThe%2520dynamic%2520and%2520static%2520priors%2520complement%2520to%2520each%2520other%252C%2520facilitating%2520us%2520to%250Aprogressively%2520mitigate%2520long-range%2520turbulence%2520with%2520severe%2520distortions.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520SOTA%2520methods%2520on%250Adifferent%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-range%20Turbulence%20Mitigation%3A%20A%20Large-scale%20Dataset%20and%20A%0A%20%20Coarse-to-fine%20Framework&entry.906535625=Shengqi%20Xu%20and%20Run%20Sun%20and%20Yi%20Chang%20and%20Shuning%20Cao%20and%20Xueyao%20Xiao%20and%20Luxin%20Yan&entry.1292438233=%20%20Long-range%20imaging%20inevitably%20suffers%20from%20atmospheric%20turbulence%20with%20severe%0Ageometric%20distortions%20due%20to%20random%20refraction%20of%20light.%20The%20further%20the%0Adistance%2C%20the%20more%20severe%20the%20disturbance.%20Despite%20existing%20research%20has%0Aachieved%20great%20progress%20in%20tackling%20short-range%20turbulence%2C%20there%20is%20less%0Aattention%20paid%20to%20long-range%20turbulence%20with%20significant%20distortions.%20To%0Aaddress%20this%20dilemma%20and%20advance%20the%20field%2C%20we%20construct%20a%20large-scale%20real%0Along-range%20atmospheric%20turbulence%20dataset%20%28RLR-AT%29%2C%20including%201500%20turbulence%0Asequences%20spanning%20distances%20from%201%20Km%20to%2013%20Km.%20The%20advantages%20of%20RLR-AT%0Acompared%20to%20existing%20ones%3A%20turbulence%20with%20longer-distances%20and%0Ahigher-diversity%2C%20scenes%20with%20greater-variety%20and%20larger-scale.%20Moreover%2C%20most%0Aexisting%20work%20adopts%20either%20registration-based%20or%20decomposition-based%20methods%0Ato%20address%20distortions%20through%20one-step%20mitigation.%20However%2C%20they%20fail%20to%0Aeffectively%20handle%20long-range%20turbulence%20due%20to%20its%20significant%20pixel%0Adisplacements.%20In%20this%20work%2C%20we%20propose%20a%20coarse-to-fine%20framework%20to%20handle%0Asevere%20distortions%2C%20which%20cooperates%20dynamic%20turbulence%20and%20static%20background%0Apriors%20%28CDSP%29.%20On%20the%20one%20hand%2C%20we%20discover%20the%20pixel%20motion%20statistical%20prior%0Aof%20turbulence%2C%20and%20propose%20a%20frequency-aware%20reference%20frame%20for%20better%0Alarge-scale%20distortion%20registration%2C%20greatly%20reducing%20the%20burden%20of%20refinement.%0AOn%20the%20other%20hand%2C%20we%20take%20advantage%20of%20the%20static%20prior%20of%20background%2C%20and%0Apropose%20a%20subspace-based%20low-rank%20tensor%20refinement%20model%20to%20eliminate%20the%0Amisalignments%20inevitably%20left%20by%20registration%20while%20well%20preserving%20details.%0AThe%20dynamic%20and%20static%20priors%20complement%20to%20each%20other%2C%20facilitating%20us%20to%0Aprogressively%20mitigate%20long-range%20turbulence%20with%20severe%20distortions.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20method%20outperforms%20SOTA%20methods%20on%0Adifferent%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08377v1&entry.124074799=Read"},
{"title": "UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection", "author": "Yingsen Zeng and Yujie Zhong and Chengjian Feng and Lin Ma", "abstract": "  Temporal Action Detection (TAD) focuses on detecting pre-defined actions,\nwhile Moment Retrieval (MR) aims to identify the events described by open-ended\nnatural language within untrimmed videos. Despite that they focus on different\nevents, we observe they have a significant connection. For instance, most\ndescriptions in MR involve multiple actions from TAD. In this paper, we aim to\ninvestigate the potential synergy between TAD and MR. Firstly, we propose a\nunified architecture, termed Unified Moment Detection (UniMD), for both TAD and\nMR. It transforms the inputs of the two tasks, namely actions for TAD or events\nfor MR, into a common embedding space, and utilizes two novel query-dependent\ndecoders to generate a uniform output of classification score and temporal\nsegments. Secondly, we explore the efficacy of two task fusion learning\napproaches, pre-training and co-training, in order to enhance the mutual\nbenefits between TAD and MR. Extensive experiments demonstrate that the\nproposed task fusion learning scheme enables the two tasks to help each other\nand outperform the separately trained counterparts. Impressively, UniMD\nachieves state-of-the-art results on three paired datasets Ego4D, Charades-STA,\nand ActivityNet. Our code is available at https://github.com/yingsen1/UniMD.\n", "link": "http://arxiv.org/abs/2404.04933v2", "date": "2024-07-11", "relevancy": 2.3374, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5699}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMD%3A%20Towards%20Unifying%20Moment%20Retrieval%20and%20Temporal%20Action%20Detection&body=Title%3A%20UniMD%3A%20Towards%20Unifying%20Moment%20Retrieval%20and%20Temporal%20Action%20Detection%0AAuthor%3A%20Yingsen%20Zeng%20and%20Yujie%20Zhong%20and%20Chengjian%20Feng%20and%20Lin%20Ma%0AAbstract%3A%20%20%20Temporal%20Action%20Detection%20%28TAD%29%20focuses%20on%20detecting%20pre-defined%20actions%2C%0Awhile%20Moment%20Retrieval%20%28MR%29%20aims%20to%20identify%20the%20events%20described%20by%20open-ended%0Anatural%20language%20within%20untrimmed%20videos.%20Despite%20that%20they%20focus%20on%20different%0Aevents%2C%20we%20observe%20they%20have%20a%20significant%20connection.%20For%20instance%2C%20most%0Adescriptions%20in%20MR%20involve%20multiple%20actions%20from%20TAD.%20In%20this%20paper%2C%20we%20aim%20to%0Ainvestigate%20the%20potential%20synergy%20between%20TAD%20and%20MR.%20Firstly%2C%20we%20propose%20a%0Aunified%20architecture%2C%20termed%20Unified%20Moment%20Detection%20%28UniMD%29%2C%20for%20both%20TAD%20and%0AMR.%20It%20transforms%20the%20inputs%20of%20the%20two%20tasks%2C%20namely%20actions%20for%20TAD%20or%20events%0Afor%20MR%2C%20into%20a%20common%20embedding%20space%2C%20and%20utilizes%20two%20novel%20query-dependent%0Adecoders%20to%20generate%20a%20uniform%20output%20of%20classification%20score%20and%20temporal%0Asegments.%20Secondly%2C%20we%20explore%20the%20efficacy%20of%20two%20task%20fusion%20learning%0Aapproaches%2C%20pre-training%20and%20co-training%2C%20in%20order%20to%20enhance%20the%20mutual%0Abenefits%20between%20TAD%20and%20MR.%20Extensive%20experiments%20demonstrate%20that%20the%0Aproposed%20task%20fusion%20learning%20scheme%20enables%20the%20two%20tasks%20to%20help%20each%20other%0Aand%20outperform%20the%20separately%20trained%20counterparts.%20Impressively%2C%20UniMD%0Aachieves%20state-of-the-art%20results%20on%20three%20paired%20datasets%20Ego4D%2C%20Charades-STA%2C%0Aand%20ActivityNet.%20Our%20code%20is%20available%20at%20https%3A//github.com/yingsen1/UniMD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04933v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMD%253A%2520Towards%2520Unifying%2520Moment%2520Retrieval%2520and%2520Temporal%2520Action%2520Detection%26entry.906535625%3DYingsen%2520Zeng%2520and%2520Yujie%2520Zhong%2520and%2520Chengjian%2520Feng%2520and%2520Lin%2520Ma%26entry.1292438233%3D%2520%2520Temporal%2520Action%2520Detection%2520%2528TAD%2529%2520focuses%2520on%2520detecting%2520pre-defined%2520actions%252C%250Awhile%2520Moment%2520Retrieval%2520%2528MR%2529%2520aims%2520to%2520identify%2520the%2520events%2520described%2520by%2520open-ended%250Anatural%2520language%2520within%2520untrimmed%2520videos.%2520Despite%2520that%2520they%2520focus%2520on%2520different%250Aevents%252C%2520we%2520observe%2520they%2520have%2520a%2520significant%2520connection.%2520For%2520instance%252C%2520most%250Adescriptions%2520in%2520MR%2520involve%2520multiple%2520actions%2520from%2520TAD.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%250Ainvestigate%2520the%2520potential%2520synergy%2520between%2520TAD%2520and%2520MR.%2520Firstly%252C%2520we%2520propose%2520a%250Aunified%2520architecture%252C%2520termed%2520Unified%2520Moment%2520Detection%2520%2528UniMD%2529%252C%2520for%2520both%2520TAD%2520and%250AMR.%2520It%2520transforms%2520the%2520inputs%2520of%2520the%2520two%2520tasks%252C%2520namely%2520actions%2520for%2520TAD%2520or%2520events%250Afor%2520MR%252C%2520into%2520a%2520common%2520embedding%2520space%252C%2520and%2520utilizes%2520two%2520novel%2520query-dependent%250Adecoders%2520to%2520generate%2520a%2520uniform%2520output%2520of%2520classification%2520score%2520and%2520temporal%250Asegments.%2520Secondly%252C%2520we%2520explore%2520the%2520efficacy%2520of%2520two%2520task%2520fusion%2520learning%250Aapproaches%252C%2520pre-training%2520and%2520co-training%252C%2520in%2520order%2520to%2520enhance%2520the%2520mutual%250Abenefits%2520between%2520TAD%2520and%2520MR.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%250Aproposed%2520task%2520fusion%2520learning%2520scheme%2520enables%2520the%2520two%2520tasks%2520to%2520help%2520each%2520other%250Aand%2520outperform%2520the%2520separately%2520trained%2520counterparts.%2520Impressively%252C%2520UniMD%250Aachieves%2520state-of-the-art%2520results%2520on%2520three%2520paired%2520datasets%2520Ego4D%252C%2520Charades-STA%252C%250Aand%2520ActivityNet.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/yingsen1/UniMD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04933v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMD%3A%20Towards%20Unifying%20Moment%20Retrieval%20and%20Temporal%20Action%20Detection&entry.906535625=Yingsen%20Zeng%20and%20Yujie%20Zhong%20and%20Chengjian%20Feng%20and%20Lin%20Ma&entry.1292438233=%20%20Temporal%20Action%20Detection%20%28TAD%29%20focuses%20on%20detecting%20pre-defined%20actions%2C%0Awhile%20Moment%20Retrieval%20%28MR%29%20aims%20to%20identify%20the%20events%20described%20by%20open-ended%0Anatural%20language%20within%20untrimmed%20videos.%20Despite%20that%20they%20focus%20on%20different%0Aevents%2C%20we%20observe%20they%20have%20a%20significant%20connection.%20For%20instance%2C%20most%0Adescriptions%20in%20MR%20involve%20multiple%20actions%20from%20TAD.%20In%20this%20paper%2C%20we%20aim%20to%0Ainvestigate%20the%20potential%20synergy%20between%20TAD%20and%20MR.%20Firstly%2C%20we%20propose%20a%0Aunified%20architecture%2C%20termed%20Unified%20Moment%20Detection%20%28UniMD%29%2C%20for%20both%20TAD%20and%0AMR.%20It%20transforms%20the%20inputs%20of%20the%20two%20tasks%2C%20namely%20actions%20for%20TAD%20or%20events%0Afor%20MR%2C%20into%20a%20common%20embedding%20space%2C%20and%20utilizes%20two%20novel%20query-dependent%0Adecoders%20to%20generate%20a%20uniform%20output%20of%20classification%20score%20and%20temporal%0Asegments.%20Secondly%2C%20we%20explore%20the%20efficacy%20of%20two%20task%20fusion%20learning%0Aapproaches%2C%20pre-training%20and%20co-training%2C%20in%20order%20to%20enhance%20the%20mutual%0Abenefits%20between%20TAD%20and%20MR.%20Extensive%20experiments%20demonstrate%20that%20the%0Aproposed%20task%20fusion%20learning%20scheme%20enables%20the%20two%20tasks%20to%20help%20each%20other%0Aand%20outperform%20the%20separately%20trained%20counterparts.%20Impressively%2C%20UniMD%0Aachieves%20state-of-the-art%20results%20on%20three%20paired%20datasets%20Ego4D%2C%20Charades-STA%2C%0Aand%20ActivityNet.%20Our%20code%20is%20available%20at%20https%3A//github.com/yingsen1/UniMD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04933v2&entry.124074799=Read"},
{"title": "WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics", "author": "Abdollah Zakeri and Hamid Hassanpour and Mohammad Hossein Khosravi and Amir Masoud Nourollah", "abstract": "  Lip-based biometric authentication (LBBA) has attracted many researchers\nduring the last decade. The lip is specifically interesting for biometric\nresearchers because it is a twin biometric with the potential to function both\nas a physiological and a behavioral trait. Although much valuable research was\nconducted on LBBA, none of them considered the different emotions of the client\nduring the video acquisition step of LBBA, which can potentially affect the\nclient's facial expressions and speech tempo. We proposed a novel network\nstructure called WhisperNetV2, which extends our previously proposed network\ncalled WhisperNet. Our proposed network leverages a deep Siamese structure with\ntriplet loss having three identical SlowFast networks as embedding networks.\nThe SlowFast network is an excellent candidate for our task since the fast\npathway extracts motion-related features (behavioral lip movements) with a high\nframe rate and low channel capacity. The slow pathway extracts visual features\n(physiological lip appearance) with a low frame rate and high channel capacity.\nUsing an open-set protocol, we trained our network using the CREMA-D dataset\nand acquired an Equal Error Rate (EER) of 0.005 on the test set. Considering\nthat the acquired EER is less than most similar LBBA methods, our method can be\nconsidered as a state-of-the-art LBBA method.\n", "link": "http://arxiv.org/abs/2407.08717v1", "date": "2024-07-11", "relevancy": 2.3288, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4703}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4687}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WhisperNetV2%3A%20SlowFast%20Siamese%20Network%20For%20Lip-Based%20Biometrics&body=Title%3A%20WhisperNetV2%3A%20SlowFast%20Siamese%20Network%20For%20Lip-Based%20Biometrics%0AAuthor%3A%20Abdollah%20Zakeri%20and%20Hamid%20Hassanpour%20and%20Mohammad%20Hossein%20Khosravi%20and%20Amir%20Masoud%20Nourollah%0AAbstract%3A%20%20%20Lip-based%20biometric%20authentication%20%28LBBA%29%20has%20attracted%20many%20researchers%0Aduring%20the%20last%20decade.%20The%20lip%20is%20specifically%20interesting%20for%20biometric%0Aresearchers%20because%20it%20is%20a%20twin%20biometric%20with%20the%20potential%20to%20function%20both%0Aas%20a%20physiological%20and%20a%20behavioral%20trait.%20Although%20much%20valuable%20research%20was%0Aconducted%20on%20LBBA%2C%20none%20of%20them%20considered%20the%20different%20emotions%20of%20the%20client%0Aduring%20the%20video%20acquisition%20step%20of%20LBBA%2C%20which%20can%20potentially%20affect%20the%0Aclient%27s%20facial%20expressions%20and%20speech%20tempo.%20We%20proposed%20a%20novel%20network%0Astructure%20called%20WhisperNetV2%2C%20which%20extends%20our%20previously%20proposed%20network%0Acalled%20WhisperNet.%20Our%20proposed%20network%20leverages%20a%20deep%20Siamese%20structure%20with%0Atriplet%20loss%20having%20three%20identical%20SlowFast%20networks%20as%20embedding%20networks.%0AThe%20SlowFast%20network%20is%20an%20excellent%20candidate%20for%20our%20task%20since%20the%20fast%0Apathway%20extracts%20motion-related%20features%20%28behavioral%20lip%20movements%29%20with%20a%20high%0Aframe%20rate%20and%20low%20channel%20capacity.%20The%20slow%20pathway%20extracts%20visual%20features%0A%28physiological%20lip%20appearance%29%20with%20a%20low%20frame%20rate%20and%20high%20channel%20capacity.%0AUsing%20an%20open-set%20protocol%2C%20we%20trained%20our%20network%20using%20the%20CREMA-D%20dataset%0Aand%20acquired%20an%20Equal%20Error%20Rate%20%28EER%29%20of%200.005%20on%20the%20test%20set.%20Considering%0Athat%20the%20acquired%20EER%20is%20less%20than%20most%20similar%20LBBA%20methods%2C%20our%20method%20can%20be%0Aconsidered%20as%20a%20state-of-the-art%20LBBA%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhisperNetV2%253A%2520SlowFast%2520Siamese%2520Network%2520For%2520Lip-Based%2520Biometrics%26entry.906535625%3DAbdollah%2520Zakeri%2520and%2520Hamid%2520Hassanpour%2520and%2520Mohammad%2520Hossein%2520Khosravi%2520and%2520Amir%2520Masoud%2520Nourollah%26entry.1292438233%3D%2520%2520Lip-based%2520biometric%2520authentication%2520%2528LBBA%2529%2520has%2520attracted%2520many%2520researchers%250Aduring%2520the%2520last%2520decade.%2520The%2520lip%2520is%2520specifically%2520interesting%2520for%2520biometric%250Aresearchers%2520because%2520it%2520is%2520a%2520twin%2520biometric%2520with%2520the%2520potential%2520to%2520function%2520both%250Aas%2520a%2520physiological%2520and%2520a%2520behavioral%2520trait.%2520Although%2520much%2520valuable%2520research%2520was%250Aconducted%2520on%2520LBBA%252C%2520none%2520of%2520them%2520considered%2520the%2520different%2520emotions%2520of%2520the%2520client%250Aduring%2520the%2520video%2520acquisition%2520step%2520of%2520LBBA%252C%2520which%2520can%2520potentially%2520affect%2520the%250Aclient%2527s%2520facial%2520expressions%2520and%2520speech%2520tempo.%2520We%2520proposed%2520a%2520novel%2520network%250Astructure%2520called%2520WhisperNetV2%252C%2520which%2520extends%2520our%2520previously%2520proposed%2520network%250Acalled%2520WhisperNet.%2520Our%2520proposed%2520network%2520leverages%2520a%2520deep%2520Siamese%2520structure%2520with%250Atriplet%2520loss%2520having%2520three%2520identical%2520SlowFast%2520networks%2520as%2520embedding%2520networks.%250AThe%2520SlowFast%2520network%2520is%2520an%2520excellent%2520candidate%2520for%2520our%2520task%2520since%2520the%2520fast%250Apathway%2520extracts%2520motion-related%2520features%2520%2528behavioral%2520lip%2520movements%2529%2520with%2520a%2520high%250Aframe%2520rate%2520and%2520low%2520channel%2520capacity.%2520The%2520slow%2520pathway%2520extracts%2520visual%2520features%250A%2528physiological%2520lip%2520appearance%2529%2520with%2520a%2520low%2520frame%2520rate%2520and%2520high%2520channel%2520capacity.%250AUsing%2520an%2520open-set%2520protocol%252C%2520we%2520trained%2520our%2520network%2520using%2520the%2520CREMA-D%2520dataset%250Aand%2520acquired%2520an%2520Equal%2520Error%2520Rate%2520%2528EER%2529%2520of%25200.005%2520on%2520the%2520test%2520set.%2520Considering%250Athat%2520the%2520acquired%2520EER%2520is%2520less%2520than%2520most%2520similar%2520LBBA%2520methods%252C%2520our%2520method%2520can%2520be%250Aconsidered%2520as%2520a%2520state-of-the-art%2520LBBA%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WhisperNetV2%3A%20SlowFast%20Siamese%20Network%20For%20Lip-Based%20Biometrics&entry.906535625=Abdollah%20Zakeri%20and%20Hamid%20Hassanpour%20and%20Mohammad%20Hossein%20Khosravi%20and%20Amir%20Masoud%20Nourollah&entry.1292438233=%20%20Lip-based%20biometric%20authentication%20%28LBBA%29%20has%20attracted%20many%20researchers%0Aduring%20the%20last%20decade.%20The%20lip%20is%20specifically%20interesting%20for%20biometric%0Aresearchers%20because%20it%20is%20a%20twin%20biometric%20with%20the%20potential%20to%20function%20both%0Aas%20a%20physiological%20and%20a%20behavioral%20trait.%20Although%20much%20valuable%20research%20was%0Aconducted%20on%20LBBA%2C%20none%20of%20them%20considered%20the%20different%20emotions%20of%20the%20client%0Aduring%20the%20video%20acquisition%20step%20of%20LBBA%2C%20which%20can%20potentially%20affect%20the%0Aclient%27s%20facial%20expressions%20and%20speech%20tempo.%20We%20proposed%20a%20novel%20network%0Astructure%20called%20WhisperNetV2%2C%20which%20extends%20our%20previously%20proposed%20network%0Acalled%20WhisperNet.%20Our%20proposed%20network%20leverages%20a%20deep%20Siamese%20structure%20with%0Atriplet%20loss%20having%20three%20identical%20SlowFast%20networks%20as%20embedding%20networks.%0AThe%20SlowFast%20network%20is%20an%20excellent%20candidate%20for%20our%20task%20since%20the%20fast%0Apathway%20extracts%20motion-related%20features%20%28behavioral%20lip%20movements%29%20with%20a%20high%0Aframe%20rate%20and%20low%20channel%20capacity.%20The%20slow%20pathway%20extracts%20visual%20features%0A%28physiological%20lip%20appearance%29%20with%20a%20low%20frame%20rate%20and%20high%20channel%20capacity.%0AUsing%20an%20open-set%20protocol%2C%20we%20trained%20our%20network%20using%20the%20CREMA-D%20dataset%0Aand%20acquired%20an%20Equal%20Error%20Rate%20%28EER%29%20of%200.005%20on%20the%20test%20set.%20Considering%0Athat%20the%20acquired%20EER%20is%20less%20than%20most%20similar%20LBBA%20methods%2C%20our%20method%20can%20be%0Aconsidered%20as%20a%20state-of-the-art%20LBBA%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08717v1&entry.124074799=Read"},
{"title": "Accurate Cooperative Localization Utilizing LiDAR-equipped Roadside\n  Infrastructure for Autonomous Driving", "author": "Yuze Jiang and Ehsan Javanmardi and Manabu Tsukada and Hiroshi Esaki", "abstract": "  Recent advancements in LiDAR technology have significantly lowered costs and\nimproved both its precision and resolution, thereby solidifying its role as a\ncritical component in autonomous vehicle localization. Using sophisticated 3D\nregistration algorithms, LiDAR now facilitates vehicle localization with\ncentimeter-level accuracy. However, these high-precision techniques often face\nreliability challenges in environments devoid of identifiable map features. To\naddress this limitation, we propose a novel approach that utilizes road side\nunits (RSU) with vehicle-to-infrastructure (V2I) communications to assist\nvehicle self-localization. By using RSUs as stationary reference points and\nprocessing real-time LiDAR data, our method enhances localization accuracy\nthrough a cooperative localization framework. By placing RSUs in critical\nareas, our proposed method can improve the reliability and precision of vehicle\nlocalization when the traditional vehicle self-localization technique falls\nshort. Evaluation results in an end-to-end autonomous driving simulator AWSIM\nshow that the proposed method can improve localization accuracy by up to 80%\nunder vulnerable environments compared to traditional localization methods.\nAdditionally, our method also demonstrates robust resistance to network delays\nand packet loss in heterogeneous network environments.\n", "link": "http://arxiv.org/abs/2407.08384v1", "date": "2024-07-11", "relevancy": 2.3052, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6156}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accurate%20Cooperative%20Localization%20Utilizing%20LiDAR-equipped%20Roadside%0A%20%20Infrastructure%20for%20Autonomous%20Driving&body=Title%3A%20Accurate%20Cooperative%20Localization%20Utilizing%20LiDAR-equipped%20Roadside%0A%20%20Infrastructure%20for%20Autonomous%20Driving%0AAuthor%3A%20Yuze%20Jiang%20and%20Ehsan%20Javanmardi%20and%20Manabu%20Tsukada%20and%20Hiroshi%20Esaki%0AAbstract%3A%20%20%20Recent%20advancements%20in%20LiDAR%20technology%20have%20significantly%20lowered%20costs%20and%0Aimproved%20both%20its%20precision%20and%20resolution%2C%20thereby%20solidifying%20its%20role%20as%20a%0Acritical%20component%20in%20autonomous%20vehicle%20localization.%20Using%20sophisticated%203D%0Aregistration%20algorithms%2C%20LiDAR%20now%20facilitates%20vehicle%20localization%20with%0Acentimeter-level%20accuracy.%20However%2C%20these%20high-precision%20techniques%20often%20face%0Areliability%20challenges%20in%20environments%20devoid%20of%20identifiable%20map%20features.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20approach%20that%20utilizes%20road%20side%0Aunits%20%28RSU%29%20with%20vehicle-to-infrastructure%20%28V2I%29%20communications%20to%20assist%0Avehicle%20self-localization.%20By%20using%20RSUs%20as%20stationary%20reference%20points%20and%0Aprocessing%20real-time%20LiDAR%20data%2C%20our%20method%20enhances%20localization%20accuracy%0Athrough%20a%20cooperative%20localization%20framework.%20By%20placing%20RSUs%20in%20critical%0Aareas%2C%20our%20proposed%20method%20can%20improve%20the%20reliability%20and%20precision%20of%20vehicle%0Alocalization%20when%20the%20traditional%20vehicle%20self-localization%20technique%20falls%0Ashort.%20Evaluation%20results%20in%20an%20end-to-end%20autonomous%20driving%20simulator%20AWSIM%0Ashow%20that%20the%20proposed%20method%20can%20improve%20localization%20accuracy%20by%20up%20to%2080%25%0Aunder%20vulnerable%20environments%20compared%20to%20traditional%20localization%20methods.%0AAdditionally%2C%20our%20method%20also%20demonstrates%20robust%20resistance%20to%20network%20delays%0Aand%20packet%20loss%20in%20heterogeneous%20network%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccurate%2520Cooperative%2520Localization%2520Utilizing%2520LiDAR-equipped%2520Roadside%250A%2520%2520Infrastructure%2520for%2520Autonomous%2520Driving%26entry.906535625%3DYuze%2520Jiang%2520and%2520Ehsan%2520Javanmardi%2520and%2520Manabu%2520Tsukada%2520and%2520Hiroshi%2520Esaki%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520LiDAR%2520technology%2520have%2520significantly%2520lowered%2520costs%2520and%250Aimproved%2520both%2520its%2520precision%2520and%2520resolution%252C%2520thereby%2520solidifying%2520its%2520role%2520as%2520a%250Acritical%2520component%2520in%2520autonomous%2520vehicle%2520localization.%2520Using%2520sophisticated%25203D%250Aregistration%2520algorithms%252C%2520LiDAR%2520now%2520facilitates%2520vehicle%2520localization%2520with%250Acentimeter-level%2520accuracy.%2520However%252C%2520these%2520high-precision%2520techniques%2520often%2520face%250Areliability%2520challenges%2520in%2520environments%2520devoid%2520of%2520identifiable%2520map%2520features.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520utilizes%2520road%2520side%250Aunits%2520%2528RSU%2529%2520with%2520vehicle-to-infrastructure%2520%2528V2I%2529%2520communications%2520to%2520assist%250Avehicle%2520self-localization.%2520By%2520using%2520RSUs%2520as%2520stationary%2520reference%2520points%2520and%250Aprocessing%2520real-time%2520LiDAR%2520data%252C%2520our%2520method%2520enhances%2520localization%2520accuracy%250Athrough%2520a%2520cooperative%2520localization%2520framework.%2520By%2520placing%2520RSUs%2520in%2520critical%250Aareas%252C%2520our%2520proposed%2520method%2520can%2520improve%2520the%2520reliability%2520and%2520precision%2520of%2520vehicle%250Alocalization%2520when%2520the%2520traditional%2520vehicle%2520self-localization%2520technique%2520falls%250Ashort.%2520Evaluation%2520results%2520in%2520an%2520end-to-end%2520autonomous%2520driving%2520simulator%2520AWSIM%250Ashow%2520that%2520the%2520proposed%2520method%2520can%2520improve%2520localization%2520accuracy%2520by%2520up%2520to%252080%2525%250Aunder%2520vulnerable%2520environments%2520compared%2520to%2520traditional%2520localization%2520methods.%250AAdditionally%252C%2520our%2520method%2520also%2520demonstrates%2520robust%2520resistance%2520to%2520network%2520delays%250Aand%2520packet%2520loss%2520in%2520heterogeneous%2520network%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20Cooperative%20Localization%20Utilizing%20LiDAR-equipped%20Roadside%0A%20%20Infrastructure%20for%20Autonomous%20Driving&entry.906535625=Yuze%20Jiang%20and%20Ehsan%20Javanmardi%20and%20Manabu%20Tsukada%20and%20Hiroshi%20Esaki&entry.1292438233=%20%20Recent%20advancements%20in%20LiDAR%20technology%20have%20significantly%20lowered%20costs%20and%0Aimproved%20both%20its%20precision%20and%20resolution%2C%20thereby%20solidifying%20its%20role%20as%20a%0Acritical%20component%20in%20autonomous%20vehicle%20localization.%20Using%20sophisticated%203D%0Aregistration%20algorithms%2C%20LiDAR%20now%20facilitates%20vehicle%20localization%20with%0Acentimeter-level%20accuracy.%20However%2C%20these%20high-precision%20techniques%20often%20face%0Areliability%20challenges%20in%20environments%20devoid%20of%20identifiable%20map%20features.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20approach%20that%20utilizes%20road%20side%0Aunits%20%28RSU%29%20with%20vehicle-to-infrastructure%20%28V2I%29%20communications%20to%20assist%0Avehicle%20self-localization.%20By%20using%20RSUs%20as%20stationary%20reference%20points%20and%0Aprocessing%20real-time%20LiDAR%20data%2C%20our%20method%20enhances%20localization%20accuracy%0Athrough%20a%20cooperative%20localization%20framework.%20By%20placing%20RSUs%20in%20critical%0Aareas%2C%20our%20proposed%20method%20can%20improve%20the%20reliability%20and%20precision%20of%20vehicle%0Alocalization%20when%20the%20traditional%20vehicle%20self-localization%20technique%20falls%0Ashort.%20Evaluation%20results%20in%20an%20end-to-end%20autonomous%20driving%20simulator%20AWSIM%0Ashow%20that%20the%20proposed%20method%20can%20improve%20localization%20accuracy%20by%20up%20to%2080%25%0Aunder%20vulnerable%20environments%20compared%20to%20traditional%20localization%20methods.%0AAdditionally%2C%20our%20method%20also%20demonstrates%20robust%20resistance%20to%20network%20delays%0Aand%20packet%20loss%20in%20heterogeneous%20network%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08384v1&entry.124074799=Read"},
{"title": "GUI-based Pedicle Screw Planning on Fluoroscopic Images Utilizing\n  Vertebral Segmentation", "author": "Vivek Maik and Aparna Purayath and Durga R and Manojkumar Lakshmanan and Mohanasankar Sivaprakasm", "abstract": "  The proposed work establishes a novel Graphical User Interface (GUI)\nframework, primarily designed for intraoperative pedicle screw planning.\nCurrent planning workflow in Image Guided Surgeries primarily relies on\npre-operative CT planning. Intraoperative CT planning can be time-consuming and\nexpensive and thus is not a common practice. In situations where efficiency and\ncost-effectiveness are paramount, planning to utilize fluoroscopic images\nacquired for image registration emerges as the optimal choice. The methodology\nproposed in this study employs a simulated 3D pedicle screw to calculate its\ncoronal and sagittal projections for pedicle screw planning using\nanterior-posterior (AP) and lateral (LP) images. The initialization and\nplacement of pedicle screw is computed by utilizing the bounding box of\nvertebral segmentation, which is obtained by the application of enhanced\nYOLOv5. The GUI front end includes functionality that allows surgeons or\nmedical practitioners to efficiently choose, set up, and dynamically maneuver\nthe pedicle screw on AP and LP images. This is based on a novel feature called\nsynchronous planning, which involves correlating pedicle screws from the\ncoronal and sagittal planes. This correlation utilizes projective\ncorrespondence to ensure that any movement of the pedicle screw in either the\nAP or LP image will be reflected in the other image. The proposed GUI framework\nis a time-efficient and cost-effective tool for synchronizing and planning the\nmovement of pedicle screws during intraoperative surgical procedures.\n", "link": "http://arxiv.org/abs/2407.08347v1", "date": "2024-07-11", "relevancy": 2.2992, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4683}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4556}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-based%20Pedicle%20Screw%20Planning%20on%20Fluoroscopic%20Images%20Utilizing%0A%20%20Vertebral%20Segmentation&body=Title%3A%20GUI-based%20Pedicle%20Screw%20Planning%20on%20Fluoroscopic%20Images%20Utilizing%0A%20%20Vertebral%20Segmentation%0AAuthor%3A%20Vivek%20Maik%20and%20Aparna%20Purayath%20and%20Durga%20R%20and%20Manojkumar%20Lakshmanan%20and%20Mohanasankar%20Sivaprakasm%0AAbstract%3A%20%20%20The%20proposed%20work%20establishes%20a%20novel%20Graphical%20User%20Interface%20%28GUI%29%0Aframework%2C%20primarily%20designed%20for%20intraoperative%20pedicle%20screw%20planning.%0ACurrent%20planning%20workflow%20in%20Image%20Guided%20Surgeries%20primarily%20relies%20on%0Apre-operative%20CT%20planning.%20Intraoperative%20CT%20planning%20can%20be%20time-consuming%20and%0Aexpensive%20and%20thus%20is%20not%20a%20common%20practice.%20In%20situations%20where%20efficiency%20and%0Acost-effectiveness%20are%20paramount%2C%20planning%20to%20utilize%20fluoroscopic%20images%0Aacquired%20for%20image%20registration%20emerges%20as%20the%20optimal%20choice.%20The%20methodology%0Aproposed%20in%20this%20study%20employs%20a%20simulated%203D%20pedicle%20screw%20to%20calculate%20its%0Acoronal%20and%20sagittal%20projections%20for%20pedicle%20screw%20planning%20using%0Aanterior-posterior%20%28AP%29%20and%20lateral%20%28LP%29%20images.%20The%20initialization%20and%0Aplacement%20of%20pedicle%20screw%20is%20computed%20by%20utilizing%20the%20bounding%20box%20of%0Avertebral%20segmentation%2C%20which%20is%20obtained%20by%20the%20application%20of%20enhanced%0AYOLOv5.%20The%20GUI%20front%20end%20includes%20functionality%20that%20allows%20surgeons%20or%0Amedical%20practitioners%20to%20efficiently%20choose%2C%20set%20up%2C%20and%20dynamically%20maneuver%0Athe%20pedicle%20screw%20on%20AP%20and%20LP%20images.%20This%20is%20based%20on%20a%20novel%20feature%20called%0Asynchronous%20planning%2C%20which%20involves%20correlating%20pedicle%20screws%20from%20the%0Acoronal%20and%20sagittal%20planes.%20This%20correlation%20utilizes%20projective%0Acorrespondence%20to%20ensure%20that%20any%20movement%20of%20the%20pedicle%20screw%20in%20either%20the%0AAP%20or%20LP%20image%20will%20be%20reflected%20in%20the%20other%20image.%20The%20proposed%20GUI%20framework%0Ais%20a%20time-efficient%20and%20cost-effective%20tool%20for%20synchronizing%20and%20planning%20the%0Amovement%20of%20pedicle%20screws%20during%20intraoperative%20surgical%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-based%2520Pedicle%2520Screw%2520Planning%2520on%2520Fluoroscopic%2520Images%2520Utilizing%250A%2520%2520Vertebral%2520Segmentation%26entry.906535625%3DVivek%2520Maik%2520and%2520Aparna%2520Purayath%2520and%2520Durga%2520R%2520and%2520Manojkumar%2520Lakshmanan%2520and%2520Mohanasankar%2520Sivaprakasm%26entry.1292438233%3D%2520%2520The%2520proposed%2520work%2520establishes%2520a%2520novel%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%250Aframework%252C%2520primarily%2520designed%2520for%2520intraoperative%2520pedicle%2520screw%2520planning.%250ACurrent%2520planning%2520workflow%2520in%2520Image%2520Guided%2520Surgeries%2520primarily%2520relies%2520on%250Apre-operative%2520CT%2520planning.%2520Intraoperative%2520CT%2520planning%2520can%2520be%2520time-consuming%2520and%250Aexpensive%2520and%2520thus%2520is%2520not%2520a%2520common%2520practice.%2520In%2520situations%2520where%2520efficiency%2520and%250Acost-effectiveness%2520are%2520paramount%252C%2520planning%2520to%2520utilize%2520fluoroscopic%2520images%250Aacquired%2520for%2520image%2520registration%2520emerges%2520as%2520the%2520optimal%2520choice.%2520The%2520methodology%250Aproposed%2520in%2520this%2520study%2520employs%2520a%2520simulated%25203D%2520pedicle%2520screw%2520to%2520calculate%2520its%250Acoronal%2520and%2520sagittal%2520projections%2520for%2520pedicle%2520screw%2520planning%2520using%250Aanterior-posterior%2520%2528AP%2529%2520and%2520lateral%2520%2528LP%2529%2520images.%2520The%2520initialization%2520and%250Aplacement%2520of%2520pedicle%2520screw%2520is%2520computed%2520by%2520utilizing%2520the%2520bounding%2520box%2520of%250Avertebral%2520segmentation%252C%2520which%2520is%2520obtained%2520by%2520the%2520application%2520of%2520enhanced%250AYOLOv5.%2520The%2520GUI%2520front%2520end%2520includes%2520functionality%2520that%2520allows%2520surgeons%2520or%250Amedical%2520practitioners%2520to%2520efficiently%2520choose%252C%2520set%2520up%252C%2520and%2520dynamically%2520maneuver%250Athe%2520pedicle%2520screw%2520on%2520AP%2520and%2520LP%2520images.%2520This%2520is%2520based%2520on%2520a%2520novel%2520feature%2520called%250Asynchronous%2520planning%252C%2520which%2520involves%2520correlating%2520pedicle%2520screws%2520from%2520the%250Acoronal%2520and%2520sagittal%2520planes.%2520This%2520correlation%2520utilizes%2520projective%250Acorrespondence%2520to%2520ensure%2520that%2520any%2520movement%2520of%2520the%2520pedicle%2520screw%2520in%2520either%2520the%250AAP%2520or%2520LP%2520image%2520will%2520be%2520reflected%2520in%2520the%2520other%2520image.%2520The%2520proposed%2520GUI%2520framework%250Ais%2520a%2520time-efficient%2520and%2520cost-effective%2520tool%2520for%2520synchronizing%2520and%2520planning%2520the%250Amovement%2520of%2520pedicle%2520screws%2520during%2520intraoperative%2520surgical%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-based%20Pedicle%20Screw%20Planning%20on%20Fluoroscopic%20Images%20Utilizing%0A%20%20Vertebral%20Segmentation&entry.906535625=Vivek%20Maik%20and%20Aparna%20Purayath%20and%20Durga%20R%20and%20Manojkumar%20Lakshmanan%20and%20Mohanasankar%20Sivaprakasm&entry.1292438233=%20%20The%20proposed%20work%20establishes%20a%20novel%20Graphical%20User%20Interface%20%28GUI%29%0Aframework%2C%20primarily%20designed%20for%20intraoperative%20pedicle%20screw%20planning.%0ACurrent%20planning%20workflow%20in%20Image%20Guided%20Surgeries%20primarily%20relies%20on%0Apre-operative%20CT%20planning.%20Intraoperative%20CT%20planning%20can%20be%20time-consuming%20and%0Aexpensive%20and%20thus%20is%20not%20a%20common%20practice.%20In%20situations%20where%20efficiency%20and%0Acost-effectiveness%20are%20paramount%2C%20planning%20to%20utilize%20fluoroscopic%20images%0Aacquired%20for%20image%20registration%20emerges%20as%20the%20optimal%20choice.%20The%20methodology%0Aproposed%20in%20this%20study%20employs%20a%20simulated%203D%20pedicle%20screw%20to%20calculate%20its%0Acoronal%20and%20sagittal%20projections%20for%20pedicle%20screw%20planning%20using%0Aanterior-posterior%20%28AP%29%20and%20lateral%20%28LP%29%20images.%20The%20initialization%20and%0Aplacement%20of%20pedicle%20screw%20is%20computed%20by%20utilizing%20the%20bounding%20box%20of%0Avertebral%20segmentation%2C%20which%20is%20obtained%20by%20the%20application%20of%20enhanced%0AYOLOv5.%20The%20GUI%20front%20end%20includes%20functionality%20that%20allows%20surgeons%20or%0Amedical%20practitioners%20to%20efficiently%20choose%2C%20set%20up%2C%20and%20dynamically%20maneuver%0Athe%20pedicle%20screw%20on%20AP%20and%20LP%20images.%20This%20is%20based%20on%20a%20novel%20feature%20called%0Asynchronous%20planning%2C%20which%20involves%20correlating%20pedicle%20screws%20from%20the%0Acoronal%20and%20sagittal%20planes.%20This%20correlation%20utilizes%20projective%0Acorrespondence%20to%20ensure%20that%20any%20movement%20of%20the%20pedicle%20screw%20in%20either%20the%0AAP%20or%20LP%20image%20will%20be%20reflected%20in%20the%20other%20image.%20The%20proposed%20GUI%20framework%0Ais%20a%20time-efficient%20and%20cost-effective%20tool%20for%20synchronizing%20and%20planning%20the%0Amovement%20of%20pedicle%20screws%20during%20intraoperative%20surgical%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08347v1&entry.124074799=Read"},
{"title": "Neural Bipartite Matching", "author": "Dobrik Georgiev and Pietro Li\u00f2", "abstract": "  Graph neural networks (GNNs) have found application for learning in the space\nof algorithms. However, the algorithms chosen by existing research (sorting,\nBreadth-First search, shortest path finding, etc.) usually align perfectly with\na standard GNN architecture. This report describes how neural execution is\napplied to a complex algorithm, such as finding maximum bipartite matching by\nreducing it to a flow problem and using Ford-Fulkerson to find the maximum\nflow. This is achieved via neural execution based only on features generated\nfrom a single GNN. The evaluation shows strongly generalising results with the\nnetwork achieving optimal matching almost 100% of the time.\n", "link": "http://arxiv.org/abs/2005.11304v4", "date": "2024-07-11", "relevancy": 2.2916, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4799}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4585}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Bipartite%20Matching&body=Title%3A%20Neural%20Bipartite%20Matching%0AAuthor%3A%20Dobrik%20Georgiev%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20found%20application%20for%20learning%20in%20the%20space%0Aof%20algorithms.%20However%2C%20the%20algorithms%20chosen%20by%20existing%20research%20%28sorting%2C%0ABreadth-First%20search%2C%20shortest%20path%20finding%2C%20etc.%29%20usually%20align%20perfectly%20with%0Aa%20standard%20GNN%20architecture.%20This%20report%20describes%20how%20neural%20execution%20is%0Aapplied%20to%20a%20complex%20algorithm%2C%20such%20as%20finding%20maximum%20bipartite%20matching%20by%0Areducing%20it%20to%20a%20flow%20problem%20and%20using%20Ford-Fulkerson%20to%20find%20the%20maximum%0Aflow.%20This%20is%20achieved%20via%20neural%20execution%20based%20only%20on%20features%20generated%0Afrom%20a%20single%20GNN.%20The%20evaluation%20shows%20strongly%20generalising%20results%20with%20the%0Anetwork%20achieving%20optimal%20matching%20almost%20100%25%20of%20the%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2005.11304v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Bipartite%2520Matching%26entry.906535625%3DDobrik%2520Georgiev%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520found%2520application%2520for%2520learning%2520in%2520the%2520space%250Aof%2520algorithms.%2520However%252C%2520the%2520algorithms%2520chosen%2520by%2520existing%2520research%2520%2528sorting%252C%250ABreadth-First%2520search%252C%2520shortest%2520path%2520finding%252C%2520etc.%2529%2520usually%2520align%2520perfectly%2520with%250Aa%2520standard%2520GNN%2520architecture.%2520This%2520report%2520describes%2520how%2520neural%2520execution%2520is%250Aapplied%2520to%2520a%2520complex%2520algorithm%252C%2520such%2520as%2520finding%2520maximum%2520bipartite%2520matching%2520by%250Areducing%2520it%2520to%2520a%2520flow%2520problem%2520and%2520using%2520Ford-Fulkerson%2520to%2520find%2520the%2520maximum%250Aflow.%2520This%2520is%2520achieved%2520via%2520neural%2520execution%2520based%2520only%2520on%2520features%2520generated%250Afrom%2520a%2520single%2520GNN.%2520The%2520evaluation%2520shows%2520strongly%2520generalising%2520results%2520with%2520the%250Anetwork%2520achieving%2520optimal%2520matching%2520almost%2520100%2525%2520of%2520the%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2005.11304v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Bipartite%20Matching&entry.906535625=Dobrik%20Georgiev%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20found%20application%20for%20learning%20in%20the%20space%0Aof%20algorithms.%20However%2C%20the%20algorithms%20chosen%20by%20existing%20research%20%28sorting%2C%0ABreadth-First%20search%2C%20shortest%20path%20finding%2C%20etc.%29%20usually%20align%20perfectly%20with%0Aa%20standard%20GNN%20architecture.%20This%20report%20describes%20how%20neural%20execution%20is%0Aapplied%20to%20a%20complex%20algorithm%2C%20such%20as%20finding%20maximum%20bipartite%20matching%20by%0Areducing%20it%20to%20a%20flow%20problem%20and%20using%20Ford-Fulkerson%20to%20find%20the%20maximum%0Aflow.%20This%20is%20achieved%20via%20neural%20execution%20based%20only%20on%20features%20generated%0Afrom%20a%20single%20GNN.%20The%20evaluation%20shows%20strongly%20generalising%20results%20with%20the%0Anetwork%20achieving%20optimal%20matching%20almost%20100%25%20of%20the%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2005.11304v4&entry.124074799=Read"},
{"title": "Learning Localization of Body and Finger Animation Skeleton Joints on\n  Three-Dimensional Models of Human Bodies", "author": "Stefan Novakovi\u0107 and Vladimir Risojevi\u0107", "abstract": "  Contemporary approaches to solving various problems that require analyzing\nthree-dimensional (3D) meshes and point clouds have adopted the use of deep\nlearning algorithms that directly process 3D data such as point coordinates,\nnormal vectors and vertex connectivity information. Our work proposes one such\nsolution to the problem of positioning body and finger animation skeleton\njoints within 3D models of human bodies. Due to scarcity of annotated real\nhuman scans, we resort to generating synthetic samples while varying their\nshape and pose parameters. Similarly to the state-of-the-art approach, our\nmethod computes each joint location as a convex combination of input points.\nGiven only a list of point coordinates and normal vector estimates as input, a\ndynamic graph convolutional neural network is used to predict the coefficients\nof the convex combinations. By comparing our method with the state-of-the-art,\nwe show that it is possible to achieve significantly better results with a\nsimpler architecture, especially for finger joints. Since our solution requires\nfewer precomputed features, it also allows for shorter processing times.\n", "link": "http://arxiv.org/abs/2407.08484v1", "date": "2024-07-11", "relevancy": 2.269, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6016}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.553}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Localization%20of%20Body%20and%20Finger%20Animation%20Skeleton%20Joints%20on%0A%20%20Three-Dimensional%20Models%20of%20Human%20Bodies&body=Title%3A%20Learning%20Localization%20of%20Body%20and%20Finger%20Animation%20Skeleton%20Joints%20on%0A%20%20Three-Dimensional%20Models%20of%20Human%20Bodies%0AAuthor%3A%20Stefan%20Novakovi%C4%87%20and%20Vladimir%20Risojevi%C4%87%0AAbstract%3A%20%20%20Contemporary%20approaches%20to%20solving%20various%20problems%20that%20require%20analyzing%0Athree-dimensional%20%283D%29%20meshes%20and%20point%20clouds%20have%20adopted%20the%20use%20of%20deep%0Alearning%20algorithms%20that%20directly%20process%203D%20data%20such%20as%20point%20coordinates%2C%0Anormal%20vectors%20and%20vertex%20connectivity%20information.%20Our%20work%20proposes%20one%20such%0Asolution%20to%20the%20problem%20of%20positioning%20body%20and%20finger%20animation%20skeleton%0Ajoints%20within%203D%20models%20of%20human%20bodies.%20Due%20to%20scarcity%20of%20annotated%20real%0Ahuman%20scans%2C%20we%20resort%20to%20generating%20synthetic%20samples%20while%20varying%20their%0Ashape%20and%20pose%20parameters.%20Similarly%20to%20the%20state-of-the-art%20approach%2C%20our%0Amethod%20computes%20each%20joint%20location%20as%20a%20convex%20combination%20of%20input%20points.%0AGiven%20only%20a%20list%20of%20point%20coordinates%20and%20normal%20vector%20estimates%20as%20input%2C%20a%0Adynamic%20graph%20convolutional%20neural%20network%20is%20used%20to%20predict%20the%20coefficients%0Aof%20the%20convex%20combinations.%20By%20comparing%20our%20method%20with%20the%20state-of-the-art%2C%0Awe%20show%20that%20it%20is%20possible%20to%20achieve%20significantly%20better%20results%20with%20a%0Asimpler%20architecture%2C%20especially%20for%20finger%20joints.%20Since%20our%20solution%20requires%0Afewer%20precomputed%20features%2C%20it%20also%20allows%20for%20shorter%20processing%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Localization%2520of%2520Body%2520and%2520Finger%2520Animation%2520Skeleton%2520Joints%2520on%250A%2520%2520Three-Dimensional%2520Models%2520of%2520Human%2520Bodies%26entry.906535625%3DStefan%2520Novakovi%25C4%2587%2520and%2520Vladimir%2520Risojevi%25C4%2587%26entry.1292438233%3D%2520%2520Contemporary%2520approaches%2520to%2520solving%2520various%2520problems%2520that%2520require%2520analyzing%250Athree-dimensional%2520%25283D%2529%2520meshes%2520and%2520point%2520clouds%2520have%2520adopted%2520the%2520use%2520of%2520deep%250Alearning%2520algorithms%2520that%2520directly%2520process%25203D%2520data%2520such%2520as%2520point%2520coordinates%252C%250Anormal%2520vectors%2520and%2520vertex%2520connectivity%2520information.%2520Our%2520work%2520proposes%2520one%2520such%250Asolution%2520to%2520the%2520problem%2520of%2520positioning%2520body%2520and%2520finger%2520animation%2520skeleton%250Ajoints%2520within%25203D%2520models%2520of%2520human%2520bodies.%2520Due%2520to%2520scarcity%2520of%2520annotated%2520real%250Ahuman%2520scans%252C%2520we%2520resort%2520to%2520generating%2520synthetic%2520samples%2520while%2520varying%2520their%250Ashape%2520and%2520pose%2520parameters.%2520Similarly%2520to%2520the%2520state-of-the-art%2520approach%252C%2520our%250Amethod%2520computes%2520each%2520joint%2520location%2520as%2520a%2520convex%2520combination%2520of%2520input%2520points.%250AGiven%2520only%2520a%2520list%2520of%2520point%2520coordinates%2520and%2520normal%2520vector%2520estimates%2520as%2520input%252C%2520a%250Adynamic%2520graph%2520convolutional%2520neural%2520network%2520is%2520used%2520to%2520predict%2520the%2520coefficients%250Aof%2520the%2520convex%2520combinations.%2520By%2520comparing%2520our%2520method%2520with%2520the%2520state-of-the-art%252C%250Awe%2520show%2520that%2520it%2520is%2520possible%2520to%2520achieve%2520significantly%2520better%2520results%2520with%2520a%250Asimpler%2520architecture%252C%2520especially%2520for%2520finger%2520joints.%2520Since%2520our%2520solution%2520requires%250Afewer%2520precomputed%2520features%252C%2520it%2520also%2520allows%2520for%2520shorter%2520processing%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Localization%20of%20Body%20and%20Finger%20Animation%20Skeleton%20Joints%20on%0A%20%20Three-Dimensional%20Models%20of%20Human%20Bodies&entry.906535625=Stefan%20Novakovi%C4%87%20and%20Vladimir%20Risojevi%C4%87&entry.1292438233=%20%20Contemporary%20approaches%20to%20solving%20various%20problems%20that%20require%20analyzing%0Athree-dimensional%20%283D%29%20meshes%20and%20point%20clouds%20have%20adopted%20the%20use%20of%20deep%0Alearning%20algorithms%20that%20directly%20process%203D%20data%20such%20as%20point%20coordinates%2C%0Anormal%20vectors%20and%20vertex%20connectivity%20information.%20Our%20work%20proposes%20one%20such%0Asolution%20to%20the%20problem%20of%20positioning%20body%20and%20finger%20animation%20skeleton%0Ajoints%20within%203D%20models%20of%20human%20bodies.%20Due%20to%20scarcity%20of%20annotated%20real%0Ahuman%20scans%2C%20we%20resort%20to%20generating%20synthetic%20samples%20while%20varying%20their%0Ashape%20and%20pose%20parameters.%20Similarly%20to%20the%20state-of-the-art%20approach%2C%20our%0Amethod%20computes%20each%20joint%20location%20as%20a%20convex%20combination%20of%20input%20points.%0AGiven%20only%20a%20list%20of%20point%20coordinates%20and%20normal%20vector%20estimates%20as%20input%2C%20a%0Adynamic%20graph%20convolutional%20neural%20network%20is%20used%20to%20predict%20the%20coefficients%0Aof%20the%20convex%20combinations.%20By%20comparing%20our%20method%20with%20the%20state-of-the-art%2C%0Awe%20show%20that%20it%20is%20possible%20to%20achieve%20significantly%20better%20results%20with%20a%0Asimpler%20architecture%2C%20especially%20for%20finger%20joints.%20Since%20our%20solution%20requires%0Afewer%20precomputed%20features%2C%20it%20also%20allows%20for%20shorter%20processing%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08484v1&entry.124074799=Read"},
{"title": "DebSDF: Delving into the Details and Bias of Neural Indoor Scene\n  Reconstruction", "author": "Yuting Xiao and Jingwei Xu and Zehao Yu and Shenghua Gao", "abstract": "  In recent years, the neural implicit surface has emerged as a powerful\nrepresentation for multi-view surface reconstruction due to its simplicity and\nstate-of-the-art performance. However, reconstructing smooth and detailed\nsurfaces in indoor scenes from multi-view images presents unique challenges.\nIndoor scenes typically contain large texture-less regions, making the\nphotometric loss unreliable for optimizing the implicit surface. Previous work\nutilizes monocular geometry priors to improve the reconstruction in indoor\nscenes. However, monocular priors often contain substantial errors in thin\nstructure regions due to domain gaps and the inherent inconsistencies when\nderived independently from different views. This paper presents \\textbf{DebSDF}\nto address these challenges, focusing on the utilization of uncertainty in\nmonocular priors and the bias in SDF-based volume rendering. We propose an\nuncertainty modeling technique that associates larger uncertainties with larger\nerrors in the monocular priors. High-uncertainty priors are then excluded from\noptimization to prevent bias. This uncertainty measure also informs an\nimportance-guided ray sampling and adaptive smoothness regularization,\nenhancing the learning of fine structures. We further introduce a bias-aware\nsigned distance function to density transformation that takes into account the\ncurvature and the angle between the view direction and the SDF normals to\nreconstruct fine details better. Our approach has been validated through\nextensive experiments on several challenging datasets, demonstrating improved\nqualitative and quantitative results in reconstructing thin structures in\nindoor scenes, thereby outperforming previous work.\n", "link": "http://arxiv.org/abs/2308.15536v3", "date": "2024-07-11", "relevancy": 2.2665, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5748}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5654}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DebSDF%3A%20Delving%20into%20the%20Details%20and%20Bias%20of%20Neural%20Indoor%20Scene%0A%20%20Reconstruction&body=Title%3A%20DebSDF%3A%20Delving%20into%20the%20Details%20and%20Bias%20of%20Neural%20Indoor%20Scene%0A%20%20Reconstruction%0AAuthor%3A%20Yuting%20Xiao%20and%20Jingwei%20Xu%20and%20Zehao%20Yu%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20neural%20implicit%20surface%20has%20emerged%20as%20a%20powerful%0Arepresentation%20for%20multi-view%20surface%20reconstruction%20due%20to%20its%20simplicity%20and%0Astate-of-the-art%20performance.%20However%2C%20reconstructing%20smooth%20and%20detailed%0Asurfaces%20in%20indoor%20scenes%20from%20multi-view%20images%20presents%20unique%20challenges.%0AIndoor%20scenes%20typically%20contain%20large%20texture-less%20regions%2C%20making%20the%0Aphotometric%20loss%20unreliable%20for%20optimizing%20the%20implicit%20surface.%20Previous%20work%0Autilizes%20monocular%20geometry%20priors%20to%20improve%20the%20reconstruction%20in%20indoor%0Ascenes.%20However%2C%20monocular%20priors%20often%20contain%20substantial%20errors%20in%20thin%0Astructure%20regions%20due%20to%20domain%20gaps%20and%20the%20inherent%20inconsistencies%20when%0Aderived%20independently%20from%20different%20views.%20This%20paper%20presents%20%5Ctextbf%7BDebSDF%7D%0Ato%20address%20these%20challenges%2C%20focusing%20on%20the%20utilization%20of%20uncertainty%20in%0Amonocular%20priors%20and%20the%20bias%20in%20SDF-based%20volume%20rendering.%20We%20propose%20an%0Auncertainty%20modeling%20technique%20that%20associates%20larger%20uncertainties%20with%20larger%0Aerrors%20in%20the%20monocular%20priors.%20High-uncertainty%20priors%20are%20then%20excluded%20from%0Aoptimization%20to%20prevent%20bias.%20This%20uncertainty%20measure%20also%20informs%20an%0Aimportance-guided%20ray%20sampling%20and%20adaptive%20smoothness%20regularization%2C%0Aenhancing%20the%20learning%20of%20fine%20structures.%20We%20further%20introduce%20a%20bias-aware%0Asigned%20distance%20function%20to%20density%20transformation%20that%20takes%20into%20account%20the%0Acurvature%20and%20the%20angle%20between%20the%20view%20direction%20and%20the%20SDF%20normals%20to%0Areconstruct%20fine%20details%20better.%20Our%20approach%20has%20been%20validated%20through%0Aextensive%20experiments%20on%20several%20challenging%20datasets%2C%20demonstrating%20improved%0Aqualitative%20and%20quantitative%20results%20in%20reconstructing%20thin%20structures%20in%0Aindoor%20scenes%2C%20thereby%20outperforming%20previous%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.15536v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebSDF%253A%2520Delving%2520into%2520the%2520Details%2520and%2520Bias%2520of%2520Neural%2520Indoor%2520Scene%250A%2520%2520Reconstruction%26entry.906535625%3DYuting%2520Xiao%2520and%2520Jingwei%2520Xu%2520and%2520Zehao%2520Yu%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520neural%2520implicit%2520surface%2520has%2520emerged%2520as%2520a%2520powerful%250Arepresentation%2520for%2520multi-view%2520surface%2520reconstruction%2520due%2520to%2520its%2520simplicity%2520and%250Astate-of-the-art%2520performance.%2520However%252C%2520reconstructing%2520smooth%2520and%2520detailed%250Asurfaces%2520in%2520indoor%2520scenes%2520from%2520multi-view%2520images%2520presents%2520unique%2520challenges.%250AIndoor%2520scenes%2520typically%2520contain%2520large%2520texture-less%2520regions%252C%2520making%2520the%250Aphotometric%2520loss%2520unreliable%2520for%2520optimizing%2520the%2520implicit%2520surface.%2520Previous%2520work%250Autilizes%2520monocular%2520geometry%2520priors%2520to%2520improve%2520the%2520reconstruction%2520in%2520indoor%250Ascenes.%2520However%252C%2520monocular%2520priors%2520often%2520contain%2520substantial%2520errors%2520in%2520thin%250Astructure%2520regions%2520due%2520to%2520domain%2520gaps%2520and%2520the%2520inherent%2520inconsistencies%2520when%250Aderived%2520independently%2520from%2520different%2520views.%2520This%2520paper%2520presents%2520%255Ctextbf%257BDebSDF%257D%250Ato%2520address%2520these%2520challenges%252C%2520focusing%2520on%2520the%2520utilization%2520of%2520uncertainty%2520in%250Amonocular%2520priors%2520and%2520the%2520bias%2520in%2520SDF-based%2520volume%2520rendering.%2520We%2520propose%2520an%250Auncertainty%2520modeling%2520technique%2520that%2520associates%2520larger%2520uncertainties%2520with%2520larger%250Aerrors%2520in%2520the%2520monocular%2520priors.%2520High-uncertainty%2520priors%2520are%2520then%2520excluded%2520from%250Aoptimization%2520to%2520prevent%2520bias.%2520This%2520uncertainty%2520measure%2520also%2520informs%2520an%250Aimportance-guided%2520ray%2520sampling%2520and%2520adaptive%2520smoothness%2520regularization%252C%250Aenhancing%2520the%2520learning%2520of%2520fine%2520structures.%2520We%2520further%2520introduce%2520a%2520bias-aware%250Asigned%2520distance%2520function%2520to%2520density%2520transformation%2520that%2520takes%2520into%2520account%2520the%250Acurvature%2520and%2520the%2520angle%2520between%2520the%2520view%2520direction%2520and%2520the%2520SDF%2520normals%2520to%250Areconstruct%2520fine%2520details%2520better.%2520Our%2520approach%2520has%2520been%2520validated%2520through%250Aextensive%2520experiments%2520on%2520several%2520challenging%2520datasets%252C%2520demonstrating%2520improved%250Aqualitative%2520and%2520quantitative%2520results%2520in%2520reconstructing%2520thin%2520structures%2520in%250Aindoor%2520scenes%252C%2520thereby%2520outperforming%2520previous%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.15536v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DebSDF%3A%20Delving%20into%20the%20Details%20and%20Bias%20of%20Neural%20Indoor%20Scene%0A%20%20Reconstruction&entry.906535625=Yuting%20Xiao%20and%20Jingwei%20Xu%20and%20Zehao%20Yu%20and%20Shenghua%20Gao&entry.1292438233=%20%20In%20recent%20years%2C%20the%20neural%20implicit%20surface%20has%20emerged%20as%20a%20powerful%0Arepresentation%20for%20multi-view%20surface%20reconstruction%20due%20to%20its%20simplicity%20and%0Astate-of-the-art%20performance.%20However%2C%20reconstructing%20smooth%20and%20detailed%0Asurfaces%20in%20indoor%20scenes%20from%20multi-view%20images%20presents%20unique%20challenges.%0AIndoor%20scenes%20typically%20contain%20large%20texture-less%20regions%2C%20making%20the%0Aphotometric%20loss%20unreliable%20for%20optimizing%20the%20implicit%20surface.%20Previous%20work%0Autilizes%20monocular%20geometry%20priors%20to%20improve%20the%20reconstruction%20in%20indoor%0Ascenes.%20However%2C%20monocular%20priors%20often%20contain%20substantial%20errors%20in%20thin%0Astructure%20regions%20due%20to%20domain%20gaps%20and%20the%20inherent%20inconsistencies%20when%0Aderived%20independently%20from%20different%20views.%20This%20paper%20presents%20%5Ctextbf%7BDebSDF%7D%0Ato%20address%20these%20challenges%2C%20focusing%20on%20the%20utilization%20of%20uncertainty%20in%0Amonocular%20priors%20and%20the%20bias%20in%20SDF-based%20volume%20rendering.%20We%20propose%20an%0Auncertainty%20modeling%20technique%20that%20associates%20larger%20uncertainties%20with%20larger%0Aerrors%20in%20the%20monocular%20priors.%20High-uncertainty%20priors%20are%20then%20excluded%20from%0Aoptimization%20to%20prevent%20bias.%20This%20uncertainty%20measure%20also%20informs%20an%0Aimportance-guided%20ray%20sampling%20and%20adaptive%20smoothness%20regularization%2C%0Aenhancing%20the%20learning%20of%20fine%20structures.%20We%20further%20introduce%20a%20bias-aware%0Asigned%20distance%20function%20to%20density%20transformation%20that%20takes%20into%20account%20the%0Acurvature%20and%20the%20angle%20between%20the%20view%20direction%20and%20the%20SDF%20normals%20to%0Areconstruct%20fine%20details%20better.%20Our%20approach%20has%20been%20validated%20through%0Aextensive%20experiments%20on%20several%20challenging%20datasets%2C%20demonstrating%20improved%0Aqualitative%20and%20quantitative%20results%20in%20reconstructing%20thin%20structures%20in%0Aindoor%20scenes%2C%20thereby%20outperforming%20previous%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15536v3&entry.124074799=Read"},
{"title": "High-resolution open-vocabulary object 6D pose estimation", "author": "Jaime Corsetti and Davide Boscaini and Francesco Giuliari and Changjae Oh and Andrea Cavallaro and Fabio Poiesi", "abstract": "  The generalisation to unseen objects in the 6D pose estimation task is very\nchallenging. While Vision-Language Models (VLMs) enable using natural language\ndescriptions to support 6D pose estimation of unseen objects, these solutions\nunderperform compared to model-based methods. In this work we present Horyon,\nan open-vocabulary VLM-based architecture that addresses relative pose\nestimation between two scenes of an unseen object, described by a textual\nprompt only. We use the textual prompt to identify the unseen object in the\nscenes and then obtain high-resolution multi-scale features. These features are\nused to extract cross-scene matches for registration. We evaluate our model on\na benchmark with a large variety of unseen objects across four datasets, namely\nREAL275, Toyota-Light, Linemod, and YCB-Video. Our method achieves\nstate-of-the-art performance on all datasets, outperforming by 12.6 in Average\nRecall the previous best-performing approach.\n", "link": "http://arxiv.org/abs/2406.16384v2", "date": "2024-07-11", "relevancy": 2.2443, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-resolution%20open-vocabulary%20object%206D%20pose%20estimation&body=Title%3A%20High-resolution%20open-vocabulary%20object%206D%20pose%20estimation%0AAuthor%3A%20Jaime%20Corsetti%20and%20Davide%20Boscaini%20and%20Francesco%20Giuliari%20and%20Changjae%20Oh%20and%20Andrea%20Cavallaro%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20The%20generalisation%20to%20unseen%20objects%20in%20the%206D%20pose%20estimation%20task%20is%20very%0Achallenging.%20While%20Vision-Language%20Models%20%28VLMs%29%20enable%20using%20natural%20language%0Adescriptions%20to%20support%206D%20pose%20estimation%20of%20unseen%20objects%2C%20these%20solutions%0Aunderperform%20compared%20to%20model-based%20methods.%20In%20this%20work%20we%20present%20Horyon%2C%0Aan%20open-vocabulary%20VLM-based%20architecture%20that%20addresses%20relative%20pose%0Aestimation%20between%20two%20scenes%20of%20an%20unseen%20object%2C%20described%20by%20a%20textual%0Aprompt%20only.%20We%20use%20the%20textual%20prompt%20to%20identify%20the%20unseen%20object%20in%20the%0Ascenes%20and%20then%20obtain%20high-resolution%20multi-scale%20features.%20These%20features%20are%0Aused%20to%20extract%20cross-scene%20matches%20for%20registration.%20We%20evaluate%20our%20model%20on%0Aa%20benchmark%20with%20a%20large%20variety%20of%20unseen%20objects%20across%20four%20datasets%2C%20namely%0AREAL275%2C%20Toyota-Light%2C%20Linemod%2C%20and%20YCB-Video.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20on%20all%20datasets%2C%20outperforming%20by%2012.6%20in%20Average%0ARecall%20the%20previous%20best-performing%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16384v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-resolution%2520open-vocabulary%2520object%25206D%2520pose%2520estimation%26entry.906535625%3DJaime%2520Corsetti%2520and%2520Davide%2520Boscaini%2520and%2520Francesco%2520Giuliari%2520and%2520Changjae%2520Oh%2520and%2520Andrea%2520Cavallaro%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3D%2520%2520The%2520generalisation%2520to%2520unseen%2520objects%2520in%2520the%25206D%2520pose%2520estimation%2520task%2520is%2520very%250Achallenging.%2520While%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520enable%2520using%2520natural%2520language%250Adescriptions%2520to%2520support%25206D%2520pose%2520estimation%2520of%2520unseen%2520objects%252C%2520these%2520solutions%250Aunderperform%2520compared%2520to%2520model-based%2520methods.%2520In%2520this%2520work%2520we%2520present%2520Horyon%252C%250Aan%2520open-vocabulary%2520VLM-based%2520architecture%2520that%2520addresses%2520relative%2520pose%250Aestimation%2520between%2520two%2520scenes%2520of%2520an%2520unseen%2520object%252C%2520described%2520by%2520a%2520textual%250Aprompt%2520only.%2520We%2520use%2520the%2520textual%2520prompt%2520to%2520identify%2520the%2520unseen%2520object%2520in%2520the%250Ascenes%2520and%2520then%2520obtain%2520high-resolution%2520multi-scale%2520features.%2520These%2520features%2520are%250Aused%2520to%2520extract%2520cross-scene%2520matches%2520for%2520registration.%2520We%2520evaluate%2520our%2520model%2520on%250Aa%2520benchmark%2520with%2520a%2520large%2520variety%2520of%2520unseen%2520objects%2520across%2520four%2520datasets%252C%2520namely%250AREAL275%252C%2520Toyota-Light%252C%2520Linemod%252C%2520and%2520YCB-Video.%2520Our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520on%2520all%2520datasets%252C%2520outperforming%2520by%252012.6%2520in%2520Average%250ARecall%2520the%2520previous%2520best-performing%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16384v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-resolution%20open-vocabulary%20object%206D%20pose%20estimation&entry.906535625=Jaime%20Corsetti%20and%20Davide%20Boscaini%20and%20Francesco%20Giuliari%20and%20Changjae%20Oh%20and%20Andrea%20Cavallaro%20and%20Fabio%20Poiesi&entry.1292438233=%20%20The%20generalisation%20to%20unseen%20objects%20in%20the%206D%20pose%20estimation%20task%20is%20very%0Achallenging.%20While%20Vision-Language%20Models%20%28VLMs%29%20enable%20using%20natural%20language%0Adescriptions%20to%20support%206D%20pose%20estimation%20of%20unseen%20objects%2C%20these%20solutions%0Aunderperform%20compared%20to%20model-based%20methods.%20In%20this%20work%20we%20present%20Horyon%2C%0Aan%20open-vocabulary%20VLM-based%20architecture%20that%20addresses%20relative%20pose%0Aestimation%20between%20two%20scenes%20of%20an%20unseen%20object%2C%20described%20by%20a%20textual%0Aprompt%20only.%20We%20use%20the%20textual%20prompt%20to%20identify%20the%20unseen%20object%20in%20the%0Ascenes%20and%20then%20obtain%20high-resolution%20multi-scale%20features.%20These%20features%20are%0Aused%20to%20extract%20cross-scene%20matches%20for%20registration.%20We%20evaluate%20our%20model%20on%0Aa%20benchmark%20with%20a%20large%20variety%20of%20unseen%20objects%20across%20four%20datasets%2C%20namely%0AREAL275%2C%20Toyota-Light%2C%20Linemod%2C%20and%20YCB-Video.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20on%20all%20datasets%2C%20outperforming%20by%2012.6%20in%20Average%0ARecall%20the%20previous%20best-performing%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16384v2&entry.124074799=Read"},
{"title": "Global Spatial-Temporal Information-based Residual ConvLSTM for Video\n  Space-Time Super-Resolution", "author": "Congrui Fu and Hui Yuan and Shiqi Jiang and Guanghui Zhang and Liquan Shen and Raouf Hamzaoui", "abstract": "  By converting low-frame-rate, low-resolution videos into high-frame-rate,\nhigh-resolution ones, space-time video super-resolution techniques can enhance\nvisual experiences and facilitate more efficient information dissemination. We\npropose a convolutional neural network (CNN) for space-time video\nsuper-resolution, namely GIRNet. To generate highly accurate features and thus\nimprove performance, the proposed network integrates a feature-level temporal\ninterpolation module with deformable convolutions and a global spatial-temporal\ninformation-based residual convolutional long short-term memory (convLSTM)\nmodule. In the feature-level temporal interpolation module, we leverage\ndeformable convolution, which adapts to deformations and scale variations of\nobjects across different scene locations. This presents a more efficient\nsolution than conventional convolution for extracting features from moving\nobjects. Our network effectively uses forward and backward feature information\nto determine inter-frame offsets, leading to the direct generation of\ninterpolated frame features. In the global spatial-temporal information-based\nresidual convLSTM module, the first convLSTM is used to derive global\nspatial-temporal information from the input features, and the second convLSTM\nuses the previously computed global spatial-temporal information feature as its\ninitial cell state. This second convLSTM adopts residual connections to\npreserve spatial information, thereby enhancing the output features.\nExperiments on the Vimeo90K dataset show that the proposed method outperforms\nstate-of-the-art techniques in peak signal-to-noise-ratio (by 1.45 dB, 1.14 dB,\nand 0.02 dB over STARnet, TMNet, and 3DAttGAN, respectively), structural\nsimilarity index(by 0.027, 0.023, and 0.006 over STARnet, TMNet, and 3DAttGAN,\nrespectively), and visually.\n", "link": "http://arxiv.org/abs/2407.08466v1", "date": "2024-07-11", "relevancy": 2.2421, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5663}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5643}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Spatial-Temporal%20Information-based%20Residual%20ConvLSTM%20for%20Video%0A%20%20Space-Time%20Super-Resolution&body=Title%3A%20Global%20Spatial-Temporal%20Information-based%20Residual%20ConvLSTM%20for%20Video%0A%20%20Space-Time%20Super-Resolution%0AAuthor%3A%20Congrui%20Fu%20and%20Hui%20Yuan%20and%20Shiqi%20Jiang%20and%20Guanghui%20Zhang%20and%20Liquan%20Shen%20and%20Raouf%20Hamzaoui%0AAbstract%3A%20%20%20By%20converting%20low-frame-rate%2C%20low-resolution%20videos%20into%20high-frame-rate%2C%0Ahigh-resolution%20ones%2C%20space-time%20video%20super-resolution%20techniques%20can%20enhance%0Avisual%20experiences%20and%20facilitate%20more%20efficient%20information%20dissemination.%20We%0Apropose%20a%20convolutional%20neural%20network%20%28CNN%29%20for%20space-time%20video%0Asuper-resolution%2C%20namely%20GIRNet.%20To%20generate%20highly%20accurate%20features%20and%20thus%0Aimprove%20performance%2C%20the%20proposed%20network%20integrates%20a%20feature-level%20temporal%0Ainterpolation%20module%20with%20deformable%20convolutions%20and%20a%20global%20spatial-temporal%0Ainformation-based%20residual%20convolutional%20long%20short-term%20memory%20%28convLSTM%29%0Amodule.%20In%20the%20feature-level%20temporal%20interpolation%20module%2C%20we%20leverage%0Adeformable%20convolution%2C%20which%20adapts%20to%20deformations%20and%20scale%20variations%20of%0Aobjects%20across%20different%20scene%20locations.%20This%20presents%20a%20more%20efficient%0Asolution%20than%20conventional%20convolution%20for%20extracting%20features%20from%20moving%0Aobjects.%20Our%20network%20effectively%20uses%20forward%20and%20backward%20feature%20information%0Ato%20determine%20inter-frame%20offsets%2C%20leading%20to%20the%20direct%20generation%20of%0Ainterpolated%20frame%20features.%20In%20the%20global%20spatial-temporal%20information-based%0Aresidual%20convLSTM%20module%2C%20the%20first%20convLSTM%20is%20used%20to%20derive%20global%0Aspatial-temporal%20information%20from%20the%20input%20features%2C%20and%20the%20second%20convLSTM%0Auses%20the%20previously%20computed%20global%20spatial-temporal%20information%20feature%20as%20its%0Ainitial%20cell%20state.%20This%20second%20convLSTM%20adopts%20residual%20connections%20to%0Apreserve%20spatial%20information%2C%20thereby%20enhancing%20the%20output%20features.%0AExperiments%20on%20the%20Vimeo90K%20dataset%20show%20that%20the%20proposed%20method%20outperforms%0Astate-of-the-art%20techniques%20in%20peak%20signal-to-noise-ratio%20%28by%201.45%20dB%2C%201.14%20dB%2C%0Aand%200.02%20dB%20over%20STARnet%2C%20TMNet%2C%20and%203DAttGAN%2C%20respectively%29%2C%20structural%0Asimilarity%20index%28by%200.027%2C%200.023%2C%20and%200.006%20over%20STARnet%2C%20TMNet%2C%20and%203DAttGAN%2C%0Arespectively%29%2C%20and%20visually.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Spatial-Temporal%2520Information-based%2520Residual%2520ConvLSTM%2520for%2520Video%250A%2520%2520Space-Time%2520Super-Resolution%26entry.906535625%3DCongrui%2520Fu%2520and%2520Hui%2520Yuan%2520and%2520Shiqi%2520Jiang%2520and%2520Guanghui%2520Zhang%2520and%2520Liquan%2520Shen%2520and%2520Raouf%2520Hamzaoui%26entry.1292438233%3D%2520%2520By%2520converting%2520low-frame-rate%252C%2520low-resolution%2520videos%2520into%2520high-frame-rate%252C%250Ahigh-resolution%2520ones%252C%2520space-time%2520video%2520super-resolution%2520techniques%2520can%2520enhance%250Avisual%2520experiences%2520and%2520facilitate%2520more%2520efficient%2520information%2520dissemination.%2520We%250Apropose%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520for%2520space-time%2520video%250Asuper-resolution%252C%2520namely%2520GIRNet.%2520To%2520generate%2520highly%2520accurate%2520features%2520and%2520thus%250Aimprove%2520performance%252C%2520the%2520proposed%2520network%2520integrates%2520a%2520feature-level%2520temporal%250Ainterpolation%2520module%2520with%2520deformable%2520convolutions%2520and%2520a%2520global%2520spatial-temporal%250Ainformation-based%2520residual%2520convolutional%2520long%2520short-term%2520memory%2520%2528convLSTM%2529%250Amodule.%2520In%2520the%2520feature-level%2520temporal%2520interpolation%2520module%252C%2520we%2520leverage%250Adeformable%2520convolution%252C%2520which%2520adapts%2520to%2520deformations%2520and%2520scale%2520variations%2520of%250Aobjects%2520across%2520different%2520scene%2520locations.%2520This%2520presents%2520a%2520more%2520efficient%250Asolution%2520than%2520conventional%2520convolution%2520for%2520extracting%2520features%2520from%2520moving%250Aobjects.%2520Our%2520network%2520effectively%2520uses%2520forward%2520and%2520backward%2520feature%2520information%250Ato%2520determine%2520inter-frame%2520offsets%252C%2520leading%2520to%2520the%2520direct%2520generation%2520of%250Ainterpolated%2520frame%2520features.%2520In%2520the%2520global%2520spatial-temporal%2520information-based%250Aresidual%2520convLSTM%2520module%252C%2520the%2520first%2520convLSTM%2520is%2520used%2520to%2520derive%2520global%250Aspatial-temporal%2520information%2520from%2520the%2520input%2520features%252C%2520and%2520the%2520second%2520convLSTM%250Auses%2520the%2520previously%2520computed%2520global%2520spatial-temporal%2520information%2520feature%2520as%2520its%250Ainitial%2520cell%2520state.%2520This%2520second%2520convLSTM%2520adopts%2520residual%2520connections%2520to%250Apreserve%2520spatial%2520information%252C%2520thereby%2520enhancing%2520the%2520output%2520features.%250AExperiments%2520on%2520the%2520Vimeo90K%2520dataset%2520show%2520that%2520the%2520proposed%2520method%2520outperforms%250Astate-of-the-art%2520techniques%2520in%2520peak%2520signal-to-noise-ratio%2520%2528by%25201.45%2520dB%252C%25201.14%2520dB%252C%250Aand%25200.02%2520dB%2520over%2520STARnet%252C%2520TMNet%252C%2520and%25203DAttGAN%252C%2520respectively%2529%252C%2520structural%250Asimilarity%2520index%2528by%25200.027%252C%25200.023%252C%2520and%25200.006%2520over%2520STARnet%252C%2520TMNet%252C%2520and%25203DAttGAN%252C%250Arespectively%2529%252C%2520and%2520visually.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Spatial-Temporal%20Information-based%20Residual%20ConvLSTM%20for%20Video%0A%20%20Space-Time%20Super-Resolution&entry.906535625=Congrui%20Fu%20and%20Hui%20Yuan%20and%20Shiqi%20Jiang%20and%20Guanghui%20Zhang%20and%20Liquan%20Shen%20and%20Raouf%20Hamzaoui&entry.1292438233=%20%20By%20converting%20low-frame-rate%2C%20low-resolution%20videos%20into%20high-frame-rate%2C%0Ahigh-resolution%20ones%2C%20space-time%20video%20super-resolution%20techniques%20can%20enhance%0Avisual%20experiences%20and%20facilitate%20more%20efficient%20information%20dissemination.%20We%0Apropose%20a%20convolutional%20neural%20network%20%28CNN%29%20for%20space-time%20video%0Asuper-resolution%2C%20namely%20GIRNet.%20To%20generate%20highly%20accurate%20features%20and%20thus%0Aimprove%20performance%2C%20the%20proposed%20network%20integrates%20a%20feature-level%20temporal%0Ainterpolation%20module%20with%20deformable%20convolutions%20and%20a%20global%20spatial-temporal%0Ainformation-based%20residual%20convolutional%20long%20short-term%20memory%20%28convLSTM%29%0Amodule.%20In%20the%20feature-level%20temporal%20interpolation%20module%2C%20we%20leverage%0Adeformable%20convolution%2C%20which%20adapts%20to%20deformations%20and%20scale%20variations%20of%0Aobjects%20across%20different%20scene%20locations.%20This%20presents%20a%20more%20efficient%0Asolution%20than%20conventional%20convolution%20for%20extracting%20features%20from%20moving%0Aobjects.%20Our%20network%20effectively%20uses%20forward%20and%20backward%20feature%20information%0Ato%20determine%20inter-frame%20offsets%2C%20leading%20to%20the%20direct%20generation%20of%0Ainterpolated%20frame%20features.%20In%20the%20global%20spatial-temporal%20information-based%0Aresidual%20convLSTM%20module%2C%20the%20first%20convLSTM%20is%20used%20to%20derive%20global%0Aspatial-temporal%20information%20from%20the%20input%20features%2C%20and%20the%20second%20convLSTM%0Auses%20the%20previously%20computed%20global%20spatial-temporal%20information%20feature%20as%20its%0Ainitial%20cell%20state.%20This%20second%20convLSTM%20adopts%20residual%20connections%20to%0Apreserve%20spatial%20information%2C%20thereby%20enhancing%20the%20output%20features.%0AExperiments%20on%20the%20Vimeo90K%20dataset%20show%20that%20the%20proposed%20method%20outperforms%0Astate-of-the-art%20techniques%20in%20peak%20signal-to-noise-ratio%20%28by%201.45%20dB%2C%201.14%20dB%2C%0Aand%200.02%20dB%20over%20STARnet%2C%20TMNet%2C%20and%203DAttGAN%2C%20respectively%29%2C%20structural%0Asimilarity%20index%28by%200.027%2C%200.023%2C%20and%200.006%20over%20STARnet%2C%20TMNet%2C%20and%203DAttGAN%2C%0Arespectively%29%2C%20and%20visually.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08466v1&entry.124074799=Read"},
{"title": "PPTFormer: Pseudo Multi-Perspective Transformer for UAV Segmentation", "author": "Deyi Ji and Wenwei Jin and Hongtao Lu and Feng Zhao", "abstract": "  The ascension of Unmanned Aerial Vehicles (UAVs) in various fields\nnecessitates effective UAV image segmentation, which faces challenges due to\nthe dynamic perspectives of UAV-captured images. Traditional segmentation\nalgorithms falter as they cannot accurately mimic the complexity of UAV\nperspectives, and the cost of obtaining multi-perspective labeled datasets is\nprohibitive. To address these issues, we introduce the PPTFormer, a novel\n\\textbf{P}seudo Multi-\\textbf{P}erspective \\textbf{T}rans\\textbf{former}\nnetwork that revolutionizes UAV image segmentation. Our approach circumvents\nthe need for actual multi-perspective data by creating pseudo perspectives for\nenhanced multi-perspective learning. The PPTFormer network boasts Perspective\nRepresentation, novel Perspective Prototypes, and a specialized encoder and\ndecoder that together achieve superior segmentation results through Pseudo\nMulti-Perspective Attention (PMP Attention) and fusion. Our experiments\ndemonstrate that PPTFormer achieves state-of-the-art performance across five\nUAV segmentation datasets, confirming its capability to effectively simulate\nUAV flight perspectives and significantly advance segmentation precision. This\nwork presents a pioneering leap in UAV scene understanding and sets a new\nbenchmark for future developments in semantic segmentation.\n", "link": "http://arxiv.org/abs/2406.19632v2", "date": "2024-07-11", "relevancy": 2.2406, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5436}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PPTFormer%3A%20Pseudo%20Multi-Perspective%20Transformer%20for%20UAV%20Segmentation&body=Title%3A%20PPTFormer%3A%20Pseudo%20Multi-Perspective%20Transformer%20for%20UAV%20Segmentation%0AAuthor%3A%20Deyi%20Ji%20and%20Wenwei%20Jin%20and%20Hongtao%20Lu%20and%20Feng%20Zhao%0AAbstract%3A%20%20%20The%20ascension%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20in%20various%20fields%0Anecessitates%20effective%20UAV%20image%20segmentation%2C%20which%20faces%20challenges%20due%20to%0Athe%20dynamic%20perspectives%20of%20UAV-captured%20images.%20Traditional%20segmentation%0Aalgorithms%20falter%20as%20they%20cannot%20accurately%20mimic%20the%20complexity%20of%20UAV%0Aperspectives%2C%20and%20the%20cost%20of%20obtaining%20multi-perspective%20labeled%20datasets%20is%0Aprohibitive.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20PPTFormer%2C%20a%20novel%0A%5Ctextbf%7BP%7Dseudo%20Multi-%5Ctextbf%7BP%7Derspective%20%5Ctextbf%7BT%7Drans%5Ctextbf%7Bformer%7D%0Anetwork%20that%20revolutionizes%20UAV%20image%20segmentation.%20Our%20approach%20circumvents%0Athe%20need%20for%20actual%20multi-perspective%20data%20by%20creating%20pseudo%20perspectives%20for%0Aenhanced%20multi-perspective%20learning.%20The%20PPTFormer%20network%20boasts%20Perspective%0ARepresentation%2C%20novel%20Perspective%20Prototypes%2C%20and%20a%20specialized%20encoder%20and%0Adecoder%20that%20together%20achieve%20superior%20segmentation%20results%20through%20Pseudo%0AMulti-Perspective%20Attention%20%28PMP%20Attention%29%20and%20fusion.%20Our%20experiments%0Ademonstrate%20that%20PPTFormer%20achieves%20state-of-the-art%20performance%20across%20five%0AUAV%20segmentation%20datasets%2C%20confirming%20its%20capability%20to%20effectively%20simulate%0AUAV%20flight%20perspectives%20and%20significantly%20advance%20segmentation%20precision.%20This%0Awork%20presents%20a%20pioneering%20leap%20in%20UAV%20scene%20understanding%20and%20sets%20a%20new%0Abenchmark%20for%20future%20developments%20in%20semantic%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPPTFormer%253A%2520Pseudo%2520Multi-Perspective%2520Transformer%2520for%2520UAV%2520Segmentation%26entry.906535625%3DDeyi%2520Ji%2520and%2520Wenwei%2520Jin%2520and%2520Hongtao%2520Lu%2520and%2520Feng%2520Zhao%26entry.1292438233%3D%2520%2520The%2520ascension%2520of%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520in%2520various%2520fields%250Anecessitates%2520effective%2520UAV%2520image%2520segmentation%252C%2520which%2520faces%2520challenges%2520due%2520to%250Athe%2520dynamic%2520perspectives%2520of%2520UAV-captured%2520images.%2520Traditional%2520segmentation%250Aalgorithms%2520falter%2520as%2520they%2520cannot%2520accurately%2520mimic%2520the%2520complexity%2520of%2520UAV%250Aperspectives%252C%2520and%2520the%2520cost%2520of%2520obtaining%2520multi-perspective%2520labeled%2520datasets%2520is%250Aprohibitive.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520the%2520PPTFormer%252C%2520a%2520novel%250A%255Ctextbf%257BP%257Dseudo%2520Multi-%255Ctextbf%257BP%257Derspective%2520%255Ctextbf%257BT%257Drans%255Ctextbf%257Bformer%257D%250Anetwork%2520that%2520revolutionizes%2520UAV%2520image%2520segmentation.%2520Our%2520approach%2520circumvents%250Athe%2520need%2520for%2520actual%2520multi-perspective%2520data%2520by%2520creating%2520pseudo%2520perspectives%2520for%250Aenhanced%2520multi-perspective%2520learning.%2520The%2520PPTFormer%2520network%2520boasts%2520Perspective%250ARepresentation%252C%2520novel%2520Perspective%2520Prototypes%252C%2520and%2520a%2520specialized%2520encoder%2520and%250Adecoder%2520that%2520together%2520achieve%2520superior%2520segmentation%2520results%2520through%2520Pseudo%250AMulti-Perspective%2520Attention%2520%2528PMP%2520Attention%2529%2520and%2520fusion.%2520Our%2520experiments%250Ademonstrate%2520that%2520PPTFormer%2520achieves%2520state-of-the-art%2520performance%2520across%2520five%250AUAV%2520segmentation%2520datasets%252C%2520confirming%2520its%2520capability%2520to%2520effectively%2520simulate%250AUAV%2520flight%2520perspectives%2520and%2520significantly%2520advance%2520segmentation%2520precision.%2520This%250Awork%2520presents%2520a%2520pioneering%2520leap%2520in%2520UAV%2520scene%2520understanding%2520and%2520sets%2520a%2520new%250Abenchmark%2520for%2520future%2520developments%2520in%2520semantic%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PPTFormer%3A%20Pseudo%20Multi-Perspective%20Transformer%20for%20UAV%20Segmentation&entry.906535625=Deyi%20Ji%20and%20Wenwei%20Jin%20and%20Hongtao%20Lu%20and%20Feng%20Zhao&entry.1292438233=%20%20The%20ascension%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20in%20various%20fields%0Anecessitates%20effective%20UAV%20image%20segmentation%2C%20which%20faces%20challenges%20due%20to%0Athe%20dynamic%20perspectives%20of%20UAV-captured%20images.%20Traditional%20segmentation%0Aalgorithms%20falter%20as%20they%20cannot%20accurately%20mimic%20the%20complexity%20of%20UAV%0Aperspectives%2C%20and%20the%20cost%20of%20obtaining%20multi-perspective%20labeled%20datasets%20is%0Aprohibitive.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20PPTFormer%2C%20a%20novel%0A%5Ctextbf%7BP%7Dseudo%20Multi-%5Ctextbf%7BP%7Derspective%20%5Ctextbf%7BT%7Drans%5Ctextbf%7Bformer%7D%0Anetwork%20that%20revolutionizes%20UAV%20image%20segmentation.%20Our%20approach%20circumvents%0Athe%20need%20for%20actual%20multi-perspective%20data%20by%20creating%20pseudo%20perspectives%20for%0Aenhanced%20multi-perspective%20learning.%20The%20PPTFormer%20network%20boasts%20Perspective%0ARepresentation%2C%20novel%20Perspective%20Prototypes%2C%20and%20a%20specialized%20encoder%20and%0Adecoder%20that%20together%20achieve%20superior%20segmentation%20results%20through%20Pseudo%0AMulti-Perspective%20Attention%20%28PMP%20Attention%29%20and%20fusion.%20Our%20experiments%0Ademonstrate%20that%20PPTFormer%20achieves%20state-of-the-art%20performance%20across%20five%0AUAV%20segmentation%20datasets%2C%20confirming%20its%20capability%20to%20effectively%20simulate%0AUAV%20flight%20perspectives%20and%20significantly%20advance%20segmentation%20precision.%20This%0Awork%20presents%20a%20pioneering%20leap%20in%20UAV%20scene%20understanding%20and%20sets%20a%20new%0Abenchmark%20for%20future%20developments%20in%20semantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19632v2&entry.124074799=Read"},
{"title": "Robust Generalization of Graph Neural Networks for Carrier Scheduling", "author": "Daniel F. Perez-Ramirez and Carlos P\u00e9rez-Penichet and Nicolas Tsiftes and Dejan Kostic and Magnus Boman and Thiemo Voigt", "abstract": "  Battery-free sensor tags are devices that leverage backscatter techniques to\ncommunicate with standard IoT devices, thereby augmenting a network's sensing\ncapabilities in a scalable way. For communicating, a sensor tag relies on an\nunmodulated carrier provided by a neighboring IoT device, with a schedule\ncoordinating this provisioning across the network. Carrier\nscheduling--computing schedules to interrogate all sensor tags while minimizing\nenergy, spectrum utilization, and latency--is an NP-Hard optimization problem.\nRecent work introduces learning-based schedulers that achieve resource savings\nover a carefully-crafted heuristic, generalizing to networks of up to 60 nodes.\nHowever, we find that their advantage diminishes in networks with hundreds of\nnodes, and degrades further in larger setups. This paper introduces\nRobustGANTT, a GNN-based scheduler that improves generalization (without\nre-training) to networks up to 1000 nodes (100x training topology sizes).\nRobustGANTT not only achieves better and more consistent generalization, but\nalso computes schedules requiring up to 2x less resources than existing\nsystems. Our scheduler exhibits average runtimes of hundreds of milliseconds,\nallowing it to react fast to changing network conditions. Our work not only\nimproves resource utilization in large-scale backscatter networks, but also\noffers valuable insights in learning-based scheduling.\n", "link": "http://arxiv.org/abs/2407.08479v1", "date": "2024-07-11", "relevancy": 2.2389, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4511}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4504}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Generalization%20of%20Graph%20Neural%20Networks%20for%20Carrier%20Scheduling&body=Title%3A%20Robust%20Generalization%20of%20Graph%20Neural%20Networks%20for%20Carrier%20Scheduling%0AAuthor%3A%20Daniel%20F.%20Perez-Ramirez%20and%20Carlos%20P%C3%A9rez-Penichet%20and%20Nicolas%20Tsiftes%20and%20Dejan%20Kostic%20and%20Magnus%20Boman%20and%20Thiemo%20Voigt%0AAbstract%3A%20%20%20Battery-free%20sensor%20tags%20are%20devices%20that%20leverage%20backscatter%20techniques%20to%0Acommunicate%20with%20standard%20IoT%20devices%2C%20thereby%20augmenting%20a%20network%27s%20sensing%0Acapabilities%20in%20a%20scalable%20way.%20For%20communicating%2C%20a%20sensor%20tag%20relies%20on%20an%0Aunmodulated%20carrier%20provided%20by%20a%20neighboring%20IoT%20device%2C%20with%20a%20schedule%0Acoordinating%20this%20provisioning%20across%20the%20network.%20Carrier%0Ascheduling--computing%20schedules%20to%20interrogate%20all%20sensor%20tags%20while%20minimizing%0Aenergy%2C%20spectrum%20utilization%2C%20and%20latency--is%20an%20NP-Hard%20optimization%20problem.%0ARecent%20work%20introduces%20learning-based%20schedulers%20that%20achieve%20resource%20savings%0Aover%20a%20carefully-crafted%20heuristic%2C%20generalizing%20to%20networks%20of%20up%20to%2060%20nodes.%0AHowever%2C%20we%20find%20that%20their%20advantage%20diminishes%20in%20networks%20with%20hundreds%20of%0Anodes%2C%20and%20degrades%20further%20in%20larger%20setups.%20This%20paper%20introduces%0ARobustGANTT%2C%20a%20GNN-based%20scheduler%20that%20improves%20generalization%20%28without%0Are-training%29%20to%20networks%20up%20to%201000%20nodes%20%28100x%20training%20topology%20sizes%29.%0ARobustGANTT%20not%20only%20achieves%20better%20and%20more%20consistent%20generalization%2C%20but%0Aalso%20computes%20schedules%20requiring%20up%20to%202x%20less%20resources%20than%20existing%0Asystems.%20Our%20scheduler%20exhibits%20average%20runtimes%20of%20hundreds%20of%20milliseconds%2C%0Aallowing%20it%20to%20react%20fast%20to%20changing%20network%20conditions.%20Our%20work%20not%20only%0Aimproves%20resource%20utilization%20in%20large-scale%20backscatter%20networks%2C%20but%20also%0Aoffers%20valuable%20insights%20in%20learning-based%20scheduling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Generalization%2520of%2520Graph%2520Neural%2520Networks%2520for%2520Carrier%2520Scheduling%26entry.906535625%3DDaniel%2520F.%2520Perez-Ramirez%2520and%2520Carlos%2520P%25C3%25A9rez-Penichet%2520and%2520Nicolas%2520Tsiftes%2520and%2520Dejan%2520Kostic%2520and%2520Magnus%2520Boman%2520and%2520Thiemo%2520Voigt%26entry.1292438233%3D%2520%2520Battery-free%2520sensor%2520tags%2520are%2520devices%2520that%2520leverage%2520backscatter%2520techniques%2520to%250Acommunicate%2520with%2520standard%2520IoT%2520devices%252C%2520thereby%2520augmenting%2520a%2520network%2527s%2520sensing%250Acapabilities%2520in%2520a%2520scalable%2520way.%2520For%2520communicating%252C%2520a%2520sensor%2520tag%2520relies%2520on%2520an%250Aunmodulated%2520carrier%2520provided%2520by%2520a%2520neighboring%2520IoT%2520device%252C%2520with%2520a%2520schedule%250Acoordinating%2520this%2520provisioning%2520across%2520the%2520network.%2520Carrier%250Ascheduling--computing%2520schedules%2520to%2520interrogate%2520all%2520sensor%2520tags%2520while%2520minimizing%250Aenergy%252C%2520spectrum%2520utilization%252C%2520and%2520latency--is%2520an%2520NP-Hard%2520optimization%2520problem.%250ARecent%2520work%2520introduces%2520learning-based%2520schedulers%2520that%2520achieve%2520resource%2520savings%250Aover%2520a%2520carefully-crafted%2520heuristic%252C%2520generalizing%2520to%2520networks%2520of%2520up%2520to%252060%2520nodes.%250AHowever%252C%2520we%2520find%2520that%2520their%2520advantage%2520diminishes%2520in%2520networks%2520with%2520hundreds%2520of%250Anodes%252C%2520and%2520degrades%2520further%2520in%2520larger%2520setups.%2520This%2520paper%2520introduces%250ARobustGANTT%252C%2520a%2520GNN-based%2520scheduler%2520that%2520improves%2520generalization%2520%2528without%250Are-training%2529%2520to%2520networks%2520up%2520to%25201000%2520nodes%2520%2528100x%2520training%2520topology%2520sizes%2529.%250ARobustGANTT%2520not%2520only%2520achieves%2520better%2520and%2520more%2520consistent%2520generalization%252C%2520but%250Aalso%2520computes%2520schedules%2520requiring%2520up%2520to%25202x%2520less%2520resources%2520than%2520existing%250Asystems.%2520Our%2520scheduler%2520exhibits%2520average%2520runtimes%2520of%2520hundreds%2520of%2520milliseconds%252C%250Aallowing%2520it%2520to%2520react%2520fast%2520to%2520changing%2520network%2520conditions.%2520Our%2520work%2520not%2520only%250Aimproves%2520resource%2520utilization%2520in%2520large-scale%2520backscatter%2520networks%252C%2520but%2520also%250Aoffers%2520valuable%2520insights%2520in%2520learning-based%2520scheduling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Generalization%20of%20Graph%20Neural%20Networks%20for%20Carrier%20Scheduling&entry.906535625=Daniel%20F.%20Perez-Ramirez%20and%20Carlos%20P%C3%A9rez-Penichet%20and%20Nicolas%20Tsiftes%20and%20Dejan%20Kostic%20and%20Magnus%20Boman%20and%20Thiemo%20Voigt&entry.1292438233=%20%20Battery-free%20sensor%20tags%20are%20devices%20that%20leverage%20backscatter%20techniques%20to%0Acommunicate%20with%20standard%20IoT%20devices%2C%20thereby%20augmenting%20a%20network%27s%20sensing%0Acapabilities%20in%20a%20scalable%20way.%20For%20communicating%2C%20a%20sensor%20tag%20relies%20on%20an%0Aunmodulated%20carrier%20provided%20by%20a%20neighboring%20IoT%20device%2C%20with%20a%20schedule%0Acoordinating%20this%20provisioning%20across%20the%20network.%20Carrier%0Ascheduling--computing%20schedules%20to%20interrogate%20all%20sensor%20tags%20while%20minimizing%0Aenergy%2C%20spectrum%20utilization%2C%20and%20latency--is%20an%20NP-Hard%20optimization%20problem.%0ARecent%20work%20introduces%20learning-based%20schedulers%20that%20achieve%20resource%20savings%0Aover%20a%20carefully-crafted%20heuristic%2C%20generalizing%20to%20networks%20of%20up%20to%2060%20nodes.%0AHowever%2C%20we%20find%20that%20their%20advantage%20diminishes%20in%20networks%20with%20hundreds%20of%0Anodes%2C%20and%20degrades%20further%20in%20larger%20setups.%20This%20paper%20introduces%0ARobustGANTT%2C%20a%20GNN-based%20scheduler%20that%20improves%20generalization%20%28without%0Are-training%29%20to%20networks%20up%20to%201000%20nodes%20%28100x%20training%20topology%20sizes%29.%0ARobustGANTT%20not%20only%20achieves%20better%20and%20more%20consistent%20generalization%2C%20but%0Aalso%20computes%20schedules%20requiring%20up%20to%202x%20less%20resources%20than%20existing%0Asystems.%20Our%20scheduler%20exhibits%20average%20runtimes%20of%20hundreds%20of%20milliseconds%2C%0Aallowing%20it%20to%20react%20fast%20to%20changing%20network%20conditions.%20Our%20work%20not%20only%0Aimproves%20resource%20utilization%20in%20large-scale%20backscatter%20networks%2C%20but%20also%0Aoffers%20valuable%20insights%20in%20learning-based%20scheduling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08479v1&entry.124074799=Read"},
{"title": "Infinite Motion: Extended Motion Generation via Long Text Instructions", "author": "Mengtian Li and Chengshuo Zhai and Shengxiang Yao and Zhifeng Xie and Keyu Chen Yu-Gang Jiang", "abstract": "  In the realm of motion generation, the creation of long-duration,\nhigh-quality motion sequences remains a significant challenge. This paper\npresents our groundbreaking work on \"Infinite Motion\", a novel approach that\nleverages long text to extended motion generation, effectively bridging the gap\nbetween short and long-duration motion synthesis. Our core insight is the\nstrategic extension and reassembly of existing high-quality text-motion\ndatasets, which has led to the creation of a novel benchmark dataset to\nfacilitate the training of models for extended motion sequences. A key\ninnovation of our model is its ability to accept arbitrary lengths of text as\ninput, enabling the generation of motion sequences tailored to specific\nnarratives or scenarios. Furthermore, we incorporate the timestamp design for\ntext which allows precise editing of local segments within the generated\nsequences, offering unparalleled control and flexibility in motion synthesis.\nWe further demonstrate the versatility and practical utility of \"Infinite\nMotion\" through three specific applications: natural language interactive\nediting, motion sequence editing within long sequences and splicing of\nindependent motion sequences. Each application highlights the adaptability of\nour approach and broadens the spectrum of possibilities for research and\ndevelopment in motion generation. Through extensive experiments, we demonstrate\nthe superior performance of our model in generating long sequence motions\ncompared to existing methods.Project page:\nhttps://shuochengzhai.github.io/Infinite-motion.github.io/\n", "link": "http://arxiv.org/abs/2407.08443v1", "date": "2024-07-11", "relevancy": 2.2371, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6089}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5554}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinite%20Motion%3A%20Extended%20Motion%20Generation%20via%20Long%20Text%20Instructions&body=Title%3A%20Infinite%20Motion%3A%20Extended%20Motion%20Generation%20via%20Long%20Text%20Instructions%0AAuthor%3A%20Mengtian%20Li%20and%20Chengshuo%20Zhai%20and%20Shengxiang%20Yao%20and%20Zhifeng%20Xie%20and%20Keyu%20Chen%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20In%20the%20realm%20of%20motion%20generation%2C%20the%20creation%20of%20long-duration%2C%0Ahigh-quality%20motion%20sequences%20remains%20a%20significant%20challenge.%20This%20paper%0Apresents%20our%20groundbreaking%20work%20on%20%22Infinite%20Motion%22%2C%20a%20novel%20approach%20that%0Aleverages%20long%20text%20to%20extended%20motion%20generation%2C%20effectively%20bridging%20the%20gap%0Abetween%20short%20and%20long-duration%20motion%20synthesis.%20Our%20core%20insight%20is%20the%0Astrategic%20extension%20and%20reassembly%20of%20existing%20high-quality%20text-motion%0Adatasets%2C%20which%20has%20led%20to%20the%20creation%20of%20a%20novel%20benchmark%20dataset%20to%0Afacilitate%20the%20training%20of%20models%20for%20extended%20motion%20sequences.%20A%20key%0Ainnovation%20of%20our%20model%20is%20its%20ability%20to%20accept%20arbitrary%20lengths%20of%20text%20as%0Ainput%2C%20enabling%20the%20generation%20of%20motion%20sequences%20tailored%20to%20specific%0Anarratives%20or%20scenarios.%20Furthermore%2C%20we%20incorporate%20the%20timestamp%20design%20for%0Atext%20which%20allows%20precise%20editing%20of%20local%20segments%20within%20the%20generated%0Asequences%2C%20offering%20unparalleled%20control%20and%20flexibility%20in%20motion%20synthesis.%0AWe%20further%20demonstrate%20the%20versatility%20and%20practical%20utility%20of%20%22Infinite%0AMotion%22%20through%20three%20specific%20applications%3A%20natural%20language%20interactive%0Aediting%2C%20motion%20sequence%20editing%20within%20long%20sequences%20and%20splicing%20of%0Aindependent%20motion%20sequences.%20Each%20application%20highlights%20the%20adaptability%20of%0Aour%20approach%20and%20broadens%20the%20spectrum%20of%20possibilities%20for%20research%20and%0Adevelopment%20in%20motion%20generation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%0Athe%20superior%20performance%20of%20our%20model%20in%20generating%20long%20sequence%20motions%0Acompared%20to%20existing%20methods.Project%20page%3A%0Ahttps%3A//shuochengzhai.github.io/Infinite-motion.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinite%2520Motion%253A%2520Extended%2520Motion%2520Generation%2520via%2520Long%2520Text%2520Instructions%26entry.906535625%3DMengtian%2520Li%2520and%2520Chengshuo%2520Zhai%2520and%2520Shengxiang%2520Yao%2520and%2520Zhifeng%2520Xie%2520and%2520Keyu%2520Chen%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520motion%2520generation%252C%2520the%2520creation%2520of%2520long-duration%252C%250Ahigh-quality%2520motion%2520sequences%2520remains%2520a%2520significant%2520challenge.%2520This%2520paper%250Apresents%2520our%2520groundbreaking%2520work%2520on%2520%2522Infinite%2520Motion%2522%252C%2520a%2520novel%2520approach%2520that%250Aleverages%2520long%2520text%2520to%2520extended%2520motion%2520generation%252C%2520effectively%2520bridging%2520the%2520gap%250Abetween%2520short%2520and%2520long-duration%2520motion%2520synthesis.%2520Our%2520core%2520insight%2520is%2520the%250Astrategic%2520extension%2520and%2520reassembly%2520of%2520existing%2520high-quality%2520text-motion%250Adatasets%252C%2520which%2520has%2520led%2520to%2520the%2520creation%2520of%2520a%2520novel%2520benchmark%2520dataset%2520to%250Afacilitate%2520the%2520training%2520of%2520models%2520for%2520extended%2520motion%2520sequences.%2520A%2520key%250Ainnovation%2520of%2520our%2520model%2520is%2520its%2520ability%2520to%2520accept%2520arbitrary%2520lengths%2520of%2520text%2520as%250Ainput%252C%2520enabling%2520the%2520generation%2520of%2520motion%2520sequences%2520tailored%2520to%2520specific%250Anarratives%2520or%2520scenarios.%2520Furthermore%252C%2520we%2520incorporate%2520the%2520timestamp%2520design%2520for%250Atext%2520which%2520allows%2520precise%2520editing%2520of%2520local%2520segments%2520within%2520the%2520generated%250Asequences%252C%2520offering%2520unparalleled%2520control%2520and%2520flexibility%2520in%2520motion%2520synthesis.%250AWe%2520further%2520demonstrate%2520the%2520versatility%2520and%2520practical%2520utility%2520of%2520%2522Infinite%250AMotion%2522%2520through%2520three%2520specific%2520applications%253A%2520natural%2520language%2520interactive%250Aediting%252C%2520motion%2520sequence%2520editing%2520within%2520long%2520sequences%2520and%2520splicing%2520of%250Aindependent%2520motion%2520sequences.%2520Each%2520application%2520highlights%2520the%2520adaptability%2520of%250Aour%2520approach%2520and%2520broadens%2520the%2520spectrum%2520of%2520possibilities%2520for%2520research%2520and%250Adevelopment%2520in%2520motion%2520generation.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%250Athe%2520superior%2520performance%2520of%2520our%2520model%2520in%2520generating%2520long%2520sequence%2520motions%250Acompared%2520to%2520existing%2520methods.Project%2520page%253A%250Ahttps%253A//shuochengzhai.github.io/Infinite-motion.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinite%20Motion%3A%20Extended%20Motion%20Generation%20via%20Long%20Text%20Instructions&entry.906535625=Mengtian%20Li%20and%20Chengshuo%20Zhai%20and%20Shengxiang%20Yao%20and%20Zhifeng%20Xie%20and%20Keyu%20Chen%20Yu-Gang%20Jiang&entry.1292438233=%20%20In%20the%20realm%20of%20motion%20generation%2C%20the%20creation%20of%20long-duration%2C%0Ahigh-quality%20motion%20sequences%20remains%20a%20significant%20challenge.%20This%20paper%0Apresents%20our%20groundbreaking%20work%20on%20%22Infinite%20Motion%22%2C%20a%20novel%20approach%20that%0Aleverages%20long%20text%20to%20extended%20motion%20generation%2C%20effectively%20bridging%20the%20gap%0Abetween%20short%20and%20long-duration%20motion%20synthesis.%20Our%20core%20insight%20is%20the%0Astrategic%20extension%20and%20reassembly%20of%20existing%20high-quality%20text-motion%0Adatasets%2C%20which%20has%20led%20to%20the%20creation%20of%20a%20novel%20benchmark%20dataset%20to%0Afacilitate%20the%20training%20of%20models%20for%20extended%20motion%20sequences.%20A%20key%0Ainnovation%20of%20our%20model%20is%20its%20ability%20to%20accept%20arbitrary%20lengths%20of%20text%20as%0Ainput%2C%20enabling%20the%20generation%20of%20motion%20sequences%20tailored%20to%20specific%0Anarratives%20or%20scenarios.%20Furthermore%2C%20we%20incorporate%20the%20timestamp%20design%20for%0Atext%20which%20allows%20precise%20editing%20of%20local%20segments%20within%20the%20generated%0Asequences%2C%20offering%20unparalleled%20control%20and%20flexibility%20in%20motion%20synthesis.%0AWe%20further%20demonstrate%20the%20versatility%20and%20practical%20utility%20of%20%22Infinite%0AMotion%22%20through%20three%20specific%20applications%3A%20natural%20language%20interactive%0Aediting%2C%20motion%20sequence%20editing%20within%20long%20sequences%20and%20splicing%20of%0Aindependent%20motion%20sequences.%20Each%20application%20highlights%20the%20adaptability%20of%0Aour%20approach%20and%20broadens%20the%20spectrum%20of%20possibilities%20for%20research%20and%0Adevelopment%20in%20motion%20generation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%0Athe%20superior%20performance%20of%20our%20model%20in%20generating%20long%20sequence%20motions%0Acompared%20to%20existing%20methods.Project%20page%3A%0Ahttps%3A//shuochengzhai.github.io/Infinite-motion.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08443v1&entry.124074799=Read"},
{"title": "Industrial Anomaly Detection and Localization Using Weakly-Supervised\n  Residual Transformers", "author": "Hanxi Li and Jingqi Wu and Lin Yuanbo Wu and Hao Chen and Deyin Liu and Mingwen Wang and Peng Wang", "abstract": "  Recent advancements in industrial Anomaly Detection (AD) have shown that\nincorporating a few anomalous samples during training can significantly boost\naccuracy. However, this performance improvement comes at a high cost: extensive\nannotation efforts, which are often impractical in real-world applications. In\nthis work, we propose a novel framework called \"Weakly-supervised RESidual\nTransformer\" (WeakREST), which aims to achieve high AD accuracy while\nminimizing the need for extensive annotations. First, we reformulate the\npixel-wise anomaly localization task into a block-wise classification problem.\nBy shifting the focus to block-wise level, we can drastically reduce the amount\nof required annotations without compromising on the accuracy of anomaly\ndetection Secondly, we design a residual-based transformer model, termed\n\"Positional Fast Anomaly Residuals\" (PosFAR), to classify the image blocks in\nreal time. We further propose to label the anomalous regions using only\nbounding boxes or image tags as weaker labels, leading to a semi-supervised\nlearning setting. On the benchmark dataset MVTec-AD, our proposed WeakREST\nframework achieves a remarkable Average Precision (AP) of 83.0%, significantly\noutperforming the previous best result of 75.8% in the unsupervised setting. In\nthe supervised AD setting, WeakREST further improves performance, attaining an\nAP of 87.6% compared to the previous best of 78.6%. Notably, even when\nutilizing weaker labels based on bounding boxes, WeakREST surpasses recent\nleading methods that rely on pixel-wise supervision, achieving an AP of 87.1%\nagainst the prior best of 78.6% on MVTec-AD. This precision advantage is also\nconsistently observed on other well-known AD datasets, such as BTAD and KSDD2.\n", "link": "http://arxiv.org/abs/2306.03492v5", "date": "2024-07-11", "relevancy": 2.2315, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5943}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.55}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Industrial%20Anomaly%20Detection%20and%20Localization%20Using%20Weakly-Supervised%0A%20%20Residual%20Transformers&body=Title%3A%20Industrial%20Anomaly%20Detection%20and%20Localization%20Using%20Weakly-Supervised%0A%20%20Residual%20Transformers%0AAuthor%3A%20Hanxi%20Li%20and%20Jingqi%20Wu%20and%20Lin%20Yuanbo%20Wu%20and%20Hao%20Chen%20and%20Deyin%20Liu%20and%20Mingwen%20Wang%20and%20Peng%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20industrial%20Anomaly%20Detection%20%28AD%29%20have%20shown%20that%0Aincorporating%20a%20few%20anomalous%20samples%20during%20training%20can%20significantly%20boost%0Aaccuracy.%20However%2C%20this%20performance%20improvement%20comes%20at%20a%20high%20cost%3A%20extensive%0Aannotation%20efforts%2C%20which%20are%20often%20impractical%20in%20real-world%20applications.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20framework%20called%20%22Weakly-supervised%20RESidual%0ATransformer%22%20%28WeakREST%29%2C%20which%20aims%20to%20achieve%20high%20AD%20accuracy%20while%0Aminimizing%20the%20need%20for%20extensive%20annotations.%20First%2C%20we%20reformulate%20the%0Apixel-wise%20anomaly%20localization%20task%20into%20a%20block-wise%20classification%20problem.%0ABy%20shifting%20the%20focus%20to%20block-wise%20level%2C%20we%20can%20drastically%20reduce%20the%20amount%0Aof%20required%20annotations%20without%20compromising%20on%20the%20accuracy%20of%20anomaly%0Adetection%20Secondly%2C%20we%20design%20a%20residual-based%20transformer%20model%2C%20termed%0A%22Positional%20Fast%20Anomaly%20Residuals%22%20%28PosFAR%29%2C%20to%20classify%20the%20image%20blocks%20in%0Areal%20time.%20We%20further%20propose%20to%20label%20the%20anomalous%20regions%20using%20only%0Abounding%20boxes%20or%20image%20tags%20as%20weaker%20labels%2C%20leading%20to%20a%20semi-supervised%0Alearning%20setting.%20On%20the%20benchmark%20dataset%20MVTec-AD%2C%20our%20proposed%20WeakREST%0Aframework%20achieves%20a%20remarkable%20Average%20Precision%20%28AP%29%20of%2083.0%25%2C%20significantly%0Aoutperforming%20the%20previous%20best%20result%20of%2075.8%25%20in%20the%20unsupervised%20setting.%20In%0Athe%20supervised%20AD%20setting%2C%20WeakREST%20further%20improves%20performance%2C%20attaining%20an%0AAP%20of%2087.6%25%20compared%20to%20the%20previous%20best%20of%2078.6%25.%20Notably%2C%20even%20when%0Autilizing%20weaker%20labels%20based%20on%20bounding%20boxes%2C%20WeakREST%20surpasses%20recent%0Aleading%20methods%20that%20rely%20on%20pixel-wise%20supervision%2C%20achieving%20an%20AP%20of%2087.1%25%0Aagainst%20the%20prior%20best%20of%2078.6%25%20on%20MVTec-AD.%20This%20precision%20advantage%20is%20also%0Aconsistently%20observed%20on%20other%20well-known%20AD%20datasets%2C%20such%20as%20BTAD%20and%20KSDD2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.03492v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndustrial%2520Anomaly%2520Detection%2520and%2520Localization%2520Using%2520Weakly-Supervised%250A%2520%2520Residual%2520Transformers%26entry.906535625%3DHanxi%2520Li%2520and%2520Jingqi%2520Wu%2520and%2520Lin%2520Yuanbo%2520Wu%2520and%2520Hao%2520Chen%2520and%2520Deyin%2520Liu%2520and%2520Mingwen%2520Wang%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520industrial%2520Anomaly%2520Detection%2520%2528AD%2529%2520have%2520shown%2520that%250Aincorporating%2520a%2520few%2520anomalous%2520samples%2520during%2520training%2520can%2520significantly%2520boost%250Aaccuracy.%2520However%252C%2520this%2520performance%2520improvement%2520comes%2520at%2520a%2520high%2520cost%253A%2520extensive%250Aannotation%2520efforts%252C%2520which%2520are%2520often%2520impractical%2520in%2520real-world%2520applications.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%2520%2522Weakly-supervised%2520RESidual%250ATransformer%2522%2520%2528WeakREST%2529%252C%2520which%2520aims%2520to%2520achieve%2520high%2520AD%2520accuracy%2520while%250Aminimizing%2520the%2520need%2520for%2520extensive%2520annotations.%2520First%252C%2520we%2520reformulate%2520the%250Apixel-wise%2520anomaly%2520localization%2520task%2520into%2520a%2520block-wise%2520classification%2520problem.%250ABy%2520shifting%2520the%2520focus%2520to%2520block-wise%2520level%252C%2520we%2520can%2520drastically%2520reduce%2520the%2520amount%250Aof%2520required%2520annotations%2520without%2520compromising%2520on%2520the%2520accuracy%2520of%2520anomaly%250Adetection%2520Secondly%252C%2520we%2520design%2520a%2520residual-based%2520transformer%2520model%252C%2520termed%250A%2522Positional%2520Fast%2520Anomaly%2520Residuals%2522%2520%2528PosFAR%2529%252C%2520to%2520classify%2520the%2520image%2520blocks%2520in%250Areal%2520time.%2520We%2520further%2520propose%2520to%2520label%2520the%2520anomalous%2520regions%2520using%2520only%250Abounding%2520boxes%2520or%2520image%2520tags%2520as%2520weaker%2520labels%252C%2520leading%2520to%2520a%2520semi-supervised%250Alearning%2520setting.%2520On%2520the%2520benchmark%2520dataset%2520MVTec-AD%252C%2520our%2520proposed%2520WeakREST%250Aframework%2520achieves%2520a%2520remarkable%2520Average%2520Precision%2520%2528AP%2529%2520of%252083.0%2525%252C%2520significantly%250Aoutperforming%2520the%2520previous%2520best%2520result%2520of%252075.8%2525%2520in%2520the%2520unsupervised%2520setting.%2520In%250Athe%2520supervised%2520AD%2520setting%252C%2520WeakREST%2520further%2520improves%2520performance%252C%2520attaining%2520an%250AAP%2520of%252087.6%2525%2520compared%2520to%2520the%2520previous%2520best%2520of%252078.6%2525.%2520Notably%252C%2520even%2520when%250Autilizing%2520weaker%2520labels%2520based%2520on%2520bounding%2520boxes%252C%2520WeakREST%2520surpasses%2520recent%250Aleading%2520methods%2520that%2520rely%2520on%2520pixel-wise%2520supervision%252C%2520achieving%2520an%2520AP%2520of%252087.1%2525%250Aagainst%2520the%2520prior%2520best%2520of%252078.6%2525%2520on%2520MVTec-AD.%2520This%2520precision%2520advantage%2520is%2520also%250Aconsistently%2520observed%2520on%2520other%2520well-known%2520AD%2520datasets%252C%2520such%2520as%2520BTAD%2520and%2520KSDD2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.03492v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Industrial%20Anomaly%20Detection%20and%20Localization%20Using%20Weakly-Supervised%0A%20%20Residual%20Transformers&entry.906535625=Hanxi%20Li%20and%20Jingqi%20Wu%20and%20Lin%20Yuanbo%20Wu%20and%20Hao%20Chen%20and%20Deyin%20Liu%20and%20Mingwen%20Wang%20and%20Peng%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20industrial%20Anomaly%20Detection%20%28AD%29%20have%20shown%20that%0Aincorporating%20a%20few%20anomalous%20samples%20during%20training%20can%20significantly%20boost%0Aaccuracy.%20However%2C%20this%20performance%20improvement%20comes%20at%20a%20high%20cost%3A%20extensive%0Aannotation%20efforts%2C%20which%20are%20often%20impractical%20in%20real-world%20applications.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20framework%20called%20%22Weakly-supervised%20RESidual%0ATransformer%22%20%28WeakREST%29%2C%20which%20aims%20to%20achieve%20high%20AD%20accuracy%20while%0Aminimizing%20the%20need%20for%20extensive%20annotations.%20First%2C%20we%20reformulate%20the%0Apixel-wise%20anomaly%20localization%20task%20into%20a%20block-wise%20classification%20problem.%0ABy%20shifting%20the%20focus%20to%20block-wise%20level%2C%20we%20can%20drastically%20reduce%20the%20amount%0Aof%20required%20annotations%20without%20compromising%20on%20the%20accuracy%20of%20anomaly%0Adetection%20Secondly%2C%20we%20design%20a%20residual-based%20transformer%20model%2C%20termed%0A%22Positional%20Fast%20Anomaly%20Residuals%22%20%28PosFAR%29%2C%20to%20classify%20the%20image%20blocks%20in%0Areal%20time.%20We%20further%20propose%20to%20label%20the%20anomalous%20regions%20using%20only%0Abounding%20boxes%20or%20image%20tags%20as%20weaker%20labels%2C%20leading%20to%20a%20semi-supervised%0Alearning%20setting.%20On%20the%20benchmark%20dataset%20MVTec-AD%2C%20our%20proposed%20WeakREST%0Aframework%20achieves%20a%20remarkable%20Average%20Precision%20%28AP%29%20of%2083.0%25%2C%20significantly%0Aoutperforming%20the%20previous%20best%20result%20of%2075.8%25%20in%20the%20unsupervised%20setting.%20In%0Athe%20supervised%20AD%20setting%2C%20WeakREST%20further%20improves%20performance%2C%20attaining%20an%0AAP%20of%2087.6%25%20compared%20to%20the%20previous%20best%20of%2078.6%25.%20Notably%2C%20even%20when%0Autilizing%20weaker%20labels%20based%20on%20bounding%20boxes%2C%20WeakREST%20surpasses%20recent%0Aleading%20methods%20that%20rely%20on%20pixel-wise%20supervision%2C%20achieving%20an%20AP%20of%2087.1%25%0Aagainst%20the%20prior%20best%20of%2078.6%25%20on%20MVTec-AD.%20This%20precision%20advantage%20is%20also%0Aconsistently%20observed%20on%20other%20well-known%20AD%20datasets%2C%20such%20as%20BTAD%20and%20KSDD2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.03492v5&entry.124074799=Read"},
{"title": "BLOS-BEV: Navigation Map Enhanced Lane Segmentation Network, Beyond Line\n  of Sight", "author": "Hang Wu and Zhenghao Zhang and Siyuan Lin and Tong Qin and Jin Pan and Qiang Zhao and Chunjing Xu and Ming Yang", "abstract": "  Bird's-eye-view (BEV) representation is crucial for the perception function\nin autonomous driving tasks. It is difficult to balance the accuracy,\nefficiency and range of BEV representation. The existing works are restricted\nto a limited perception range within 50 meters. Extending the BEV\nrepresentation range can greatly benefit downstream tasks such as topology\nreasoning, scene understanding, and planning by offering more comprehensive\ninformation and reaction time. The Standard-Definition (SD) navigation maps can\nprovide a lightweight representation of road structure topology, characterized\nby ease of acquisition and low maintenance costs. An intuitive idea is to\ncombine the close-range visual information from onboard cameras with the beyond\nline-of-sight (BLOS) environmental priors from SD maps to realize expanded\nperceptual capabilities. In this paper, we propose BLOS-BEV, a novel BEV\nsegmentation model that incorporates SD maps for accurate beyond line-of-sight\nperception, up to 200m. Our approach is applicable to common BEV architectures\nand can achieve excellent results by incorporating information derived from SD\nmaps. We explore various feature fusion schemes to effectively integrate the\nvisual BEV representations and semantic features from the SD map, aiming to\nleverage the complementary information from both sources optimally. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nin BEV segmentation on nuScenes and Argoverse benchmark. Through multi-modal\ninputs, BEV segmentation is significantly enhanced at close ranges below 50m,\nwhile also demonstrating superior performance in long-range scenarios,\nsurpassing other methods by over 20% mIoU at distances ranging from 50-200m.\n", "link": "http://arxiv.org/abs/2407.08526v1", "date": "2024-07-11", "relevancy": 2.2288, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5954}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5518}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLOS-BEV%3A%20Navigation%20Map%20Enhanced%20Lane%20Segmentation%20Network%2C%20Beyond%20Line%0A%20%20of%20Sight&body=Title%3A%20BLOS-BEV%3A%20Navigation%20Map%20Enhanced%20Lane%20Segmentation%20Network%2C%20Beyond%20Line%0A%20%20of%20Sight%0AAuthor%3A%20Hang%20Wu%20and%20Zhenghao%20Zhang%20and%20Siyuan%20Lin%20and%20Tong%20Qin%20and%20Jin%20Pan%20and%20Qiang%20Zhao%20and%20Chunjing%20Xu%20and%20Ming%20Yang%0AAbstract%3A%20%20%20Bird%27s-eye-view%20%28BEV%29%20representation%20is%20crucial%20for%20the%20perception%20function%0Ain%20autonomous%20driving%20tasks.%20It%20is%20difficult%20to%20balance%20the%20accuracy%2C%0Aefficiency%20and%20range%20of%20BEV%20representation.%20The%20existing%20works%20are%20restricted%0Ato%20a%20limited%20perception%20range%20within%2050%20meters.%20Extending%20the%20BEV%0Arepresentation%20range%20can%20greatly%20benefit%20downstream%20tasks%20such%20as%20topology%0Areasoning%2C%20scene%20understanding%2C%20and%20planning%20by%20offering%20more%20comprehensive%0Ainformation%20and%20reaction%20time.%20The%20Standard-Definition%20%28SD%29%20navigation%20maps%20can%0Aprovide%20a%20lightweight%20representation%20of%20road%20structure%20topology%2C%20characterized%0Aby%20ease%20of%20acquisition%20and%20low%20maintenance%20costs.%20An%20intuitive%20idea%20is%20to%0Acombine%20the%20close-range%20visual%20information%20from%20onboard%20cameras%20with%20the%20beyond%0Aline-of-sight%20%28BLOS%29%20environmental%20priors%20from%20SD%20maps%20to%20realize%20expanded%0Aperceptual%20capabilities.%20In%20this%20paper%2C%20we%20propose%20BLOS-BEV%2C%20a%20novel%20BEV%0Asegmentation%20model%20that%20incorporates%20SD%20maps%20for%20accurate%20beyond%20line-of-sight%0Aperception%2C%20up%20to%20200m.%20Our%20approach%20is%20applicable%20to%20common%20BEV%20architectures%0Aand%20can%20achieve%20excellent%20results%20by%20incorporating%20information%20derived%20from%20SD%0Amaps.%20We%20explore%20various%20feature%20fusion%20schemes%20to%20effectively%20integrate%20the%0Avisual%20BEV%20representations%20and%20semantic%20features%20from%20the%20SD%20map%2C%20aiming%20to%0Aleverage%20the%20complementary%20information%20from%20both%20sources%20optimally.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%0Ain%20BEV%20segmentation%20on%20nuScenes%20and%20Argoverse%20benchmark.%20Through%20multi-modal%0Ainputs%2C%20BEV%20segmentation%20is%20significantly%20enhanced%20at%20close%20ranges%20below%2050m%2C%0Awhile%20also%20demonstrating%20superior%20performance%20in%20long-range%20scenarios%2C%0Asurpassing%20other%20methods%20by%20over%2020%25%20mIoU%20at%20distances%20ranging%20from%2050-200m.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLOS-BEV%253A%2520Navigation%2520Map%2520Enhanced%2520Lane%2520Segmentation%2520Network%252C%2520Beyond%2520Line%250A%2520%2520of%2520Sight%26entry.906535625%3DHang%2520Wu%2520and%2520Zhenghao%2520Zhang%2520and%2520Siyuan%2520Lin%2520and%2520Tong%2520Qin%2520and%2520Jin%2520Pan%2520and%2520Qiang%2520Zhao%2520and%2520Chunjing%2520Xu%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520Bird%2527s-eye-view%2520%2528BEV%2529%2520representation%2520is%2520crucial%2520for%2520the%2520perception%2520function%250Ain%2520autonomous%2520driving%2520tasks.%2520It%2520is%2520difficult%2520to%2520balance%2520the%2520accuracy%252C%250Aefficiency%2520and%2520range%2520of%2520BEV%2520representation.%2520The%2520existing%2520works%2520are%2520restricted%250Ato%2520a%2520limited%2520perception%2520range%2520within%252050%2520meters.%2520Extending%2520the%2520BEV%250Arepresentation%2520range%2520can%2520greatly%2520benefit%2520downstream%2520tasks%2520such%2520as%2520topology%250Areasoning%252C%2520scene%2520understanding%252C%2520and%2520planning%2520by%2520offering%2520more%2520comprehensive%250Ainformation%2520and%2520reaction%2520time.%2520The%2520Standard-Definition%2520%2528SD%2529%2520navigation%2520maps%2520can%250Aprovide%2520a%2520lightweight%2520representation%2520of%2520road%2520structure%2520topology%252C%2520characterized%250Aby%2520ease%2520of%2520acquisition%2520and%2520low%2520maintenance%2520costs.%2520An%2520intuitive%2520idea%2520is%2520to%250Acombine%2520the%2520close-range%2520visual%2520information%2520from%2520onboard%2520cameras%2520with%2520the%2520beyond%250Aline-of-sight%2520%2528BLOS%2529%2520environmental%2520priors%2520from%2520SD%2520maps%2520to%2520realize%2520expanded%250Aperceptual%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520BLOS-BEV%252C%2520a%2520novel%2520BEV%250Asegmentation%2520model%2520that%2520incorporates%2520SD%2520maps%2520for%2520accurate%2520beyond%2520line-of-sight%250Aperception%252C%2520up%2520to%2520200m.%2520Our%2520approach%2520is%2520applicable%2520to%2520common%2520BEV%2520architectures%250Aand%2520can%2520achieve%2520excellent%2520results%2520by%2520incorporating%2520information%2520derived%2520from%2520SD%250Amaps.%2520We%2520explore%2520various%2520feature%2520fusion%2520schemes%2520to%2520effectively%2520integrate%2520the%250Avisual%2520BEV%2520representations%2520and%2520semantic%2520features%2520from%2520the%2520SD%2520map%252C%2520aiming%2520to%250Aleverage%2520the%2520complementary%2520information%2520from%2520both%2520sources%2520optimally.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%250Ain%2520BEV%2520segmentation%2520on%2520nuScenes%2520and%2520Argoverse%2520benchmark.%2520Through%2520multi-modal%250Ainputs%252C%2520BEV%2520segmentation%2520is%2520significantly%2520enhanced%2520at%2520close%2520ranges%2520below%252050m%252C%250Awhile%2520also%2520demonstrating%2520superior%2520performance%2520in%2520long-range%2520scenarios%252C%250Asurpassing%2520other%2520methods%2520by%2520over%252020%2525%2520mIoU%2520at%2520distances%2520ranging%2520from%252050-200m.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLOS-BEV%3A%20Navigation%20Map%20Enhanced%20Lane%20Segmentation%20Network%2C%20Beyond%20Line%0A%20%20of%20Sight&entry.906535625=Hang%20Wu%20and%20Zhenghao%20Zhang%20and%20Siyuan%20Lin%20and%20Tong%20Qin%20and%20Jin%20Pan%20and%20Qiang%20Zhao%20and%20Chunjing%20Xu%20and%20Ming%20Yang&entry.1292438233=%20%20Bird%27s-eye-view%20%28BEV%29%20representation%20is%20crucial%20for%20the%20perception%20function%0Ain%20autonomous%20driving%20tasks.%20It%20is%20difficult%20to%20balance%20the%20accuracy%2C%0Aefficiency%20and%20range%20of%20BEV%20representation.%20The%20existing%20works%20are%20restricted%0Ato%20a%20limited%20perception%20range%20within%2050%20meters.%20Extending%20the%20BEV%0Arepresentation%20range%20can%20greatly%20benefit%20downstream%20tasks%20such%20as%20topology%0Areasoning%2C%20scene%20understanding%2C%20and%20planning%20by%20offering%20more%20comprehensive%0Ainformation%20and%20reaction%20time.%20The%20Standard-Definition%20%28SD%29%20navigation%20maps%20can%0Aprovide%20a%20lightweight%20representation%20of%20road%20structure%20topology%2C%20characterized%0Aby%20ease%20of%20acquisition%20and%20low%20maintenance%20costs.%20An%20intuitive%20idea%20is%20to%0Acombine%20the%20close-range%20visual%20information%20from%20onboard%20cameras%20with%20the%20beyond%0Aline-of-sight%20%28BLOS%29%20environmental%20priors%20from%20SD%20maps%20to%20realize%20expanded%0Aperceptual%20capabilities.%20In%20this%20paper%2C%20we%20propose%20BLOS-BEV%2C%20a%20novel%20BEV%0Asegmentation%20model%20that%20incorporates%20SD%20maps%20for%20accurate%20beyond%20line-of-sight%0Aperception%2C%20up%20to%20200m.%20Our%20approach%20is%20applicable%20to%20common%20BEV%20architectures%0Aand%20can%20achieve%20excellent%20results%20by%20incorporating%20information%20derived%20from%20SD%0Amaps.%20We%20explore%20various%20feature%20fusion%20schemes%20to%20effectively%20integrate%20the%0Avisual%20BEV%20representations%20and%20semantic%20features%20from%20the%20SD%20map%2C%20aiming%20to%0Aleverage%20the%20complementary%20information%20from%20both%20sources%20optimally.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%0Ain%20BEV%20segmentation%20on%20nuScenes%20and%20Argoverse%20benchmark.%20Through%20multi-modal%0Ainputs%2C%20BEV%20segmentation%20is%20significantly%20enhanced%20at%20close%20ranges%20below%2050m%2C%0Awhile%20also%20demonstrating%20superior%20performance%20in%20long-range%20scenarios%2C%0Asurpassing%20other%20methods%20by%20over%2020%25%20mIoU%20at%20distances%20ranging%20from%2050-200m.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08526v1&entry.124074799=Read"},
{"title": "Emergent Visual-Semantic Hierarchies in Image-Text Representations", "author": "Morris Alper and Hadar Averbuch-Elor", "abstract": "  While recent vision-and-language models (VLMs) like CLIP are a powerful tool\nfor analyzing text and images in a shared semantic space, they do not\nexplicitly model the hierarchical nature of the set of texts which may describe\nan image. Conversely, existing multimodal hierarchical representation learning\nmethods require costly training from scratch, failing to leverage the knowledge\nencoded by state-of-the-art multimodal foundation models. In this work, we\nstudy the knowledge of existing foundation models, finding that they exhibit\nemergent understanding of visual-semantic hierarchies despite not being\ndirectly trained for this purpose. We propose the Radial Embedding (RE)\nframework for probing and optimizing hierarchical understanding, and contribute\nthe HierarCaps dataset, a benchmark facilitating the study of hierarchical\nknowledge in image--text representations, constructed automatically via large\nlanguage models. Our results show that foundation VLMs exhibit zero-shot\nhierarchical understanding, surpassing the performance of prior models\nexplicitly designed for this purpose. Furthermore, we show that foundation\nmodels may be better aligned to hierarchical reasoning via a text-only\nfine-tuning phase, while retaining pretraining knowledge.\n", "link": "http://arxiv.org/abs/2407.08521v1", "date": "2024-07-11", "relevancy": 2.2201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5841}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5479}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Visual-Semantic%20Hierarchies%20in%20Image-Text%20Representations&body=Title%3A%20Emergent%20Visual-Semantic%20Hierarchies%20in%20Image-Text%20Representations%0AAuthor%3A%20Morris%20Alper%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20%20%20While%20recent%20vision-and-language%20models%20%28VLMs%29%20like%20CLIP%20are%20a%20powerful%20tool%0Afor%20analyzing%20text%20and%20images%20in%20a%20shared%20semantic%20space%2C%20they%20do%20not%0Aexplicitly%20model%20the%20hierarchical%20nature%20of%20the%20set%20of%20texts%20which%20may%20describe%0Aan%20image.%20Conversely%2C%20existing%20multimodal%20hierarchical%20representation%20learning%0Amethods%20require%20costly%20training%20from%20scratch%2C%20failing%20to%20leverage%20the%20knowledge%0Aencoded%20by%20state-of-the-art%20multimodal%20foundation%20models.%20In%20this%20work%2C%20we%0Astudy%20the%20knowledge%20of%20existing%20foundation%20models%2C%20finding%20that%20they%20exhibit%0Aemergent%20understanding%20of%20visual-semantic%20hierarchies%20despite%20not%20being%0Adirectly%20trained%20for%20this%20purpose.%20We%20propose%20the%20Radial%20Embedding%20%28RE%29%0Aframework%20for%20probing%20and%20optimizing%20hierarchical%20understanding%2C%20and%20contribute%0Athe%20HierarCaps%20dataset%2C%20a%20benchmark%20facilitating%20the%20study%20of%20hierarchical%0Aknowledge%20in%20image--text%20representations%2C%20constructed%20automatically%20via%20large%0Alanguage%20models.%20Our%20results%20show%20that%20foundation%20VLMs%20exhibit%20zero-shot%0Ahierarchical%20understanding%2C%20surpassing%20the%20performance%20of%20prior%20models%0Aexplicitly%20designed%20for%20this%20purpose.%20Furthermore%2C%20we%20show%20that%20foundation%0Amodels%20may%20be%20better%20aligned%20to%20hierarchical%20reasoning%20via%20a%20text-only%0Afine-tuning%20phase%2C%20while%20retaining%20pretraining%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Visual-Semantic%2520Hierarchies%2520in%2520Image-Text%2520Representations%26entry.906535625%3DMorris%2520Alper%2520and%2520Hadar%2520Averbuch-Elor%26entry.1292438233%3D%2520%2520While%2520recent%2520vision-and-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%2520are%2520a%2520powerful%2520tool%250Afor%2520analyzing%2520text%2520and%2520images%2520in%2520a%2520shared%2520semantic%2520space%252C%2520they%2520do%2520not%250Aexplicitly%2520model%2520the%2520hierarchical%2520nature%2520of%2520the%2520set%2520of%2520texts%2520which%2520may%2520describe%250Aan%2520image.%2520Conversely%252C%2520existing%2520multimodal%2520hierarchical%2520representation%2520learning%250Amethods%2520require%2520costly%2520training%2520from%2520scratch%252C%2520failing%2520to%2520leverage%2520the%2520knowledge%250Aencoded%2520by%2520state-of-the-art%2520multimodal%2520foundation%2520models.%2520In%2520this%2520work%252C%2520we%250Astudy%2520the%2520knowledge%2520of%2520existing%2520foundation%2520models%252C%2520finding%2520that%2520they%2520exhibit%250Aemergent%2520understanding%2520of%2520visual-semantic%2520hierarchies%2520despite%2520not%2520being%250Adirectly%2520trained%2520for%2520this%2520purpose.%2520We%2520propose%2520the%2520Radial%2520Embedding%2520%2528RE%2529%250Aframework%2520for%2520probing%2520and%2520optimizing%2520hierarchical%2520understanding%252C%2520and%2520contribute%250Athe%2520HierarCaps%2520dataset%252C%2520a%2520benchmark%2520facilitating%2520the%2520study%2520of%2520hierarchical%250Aknowledge%2520in%2520image--text%2520representations%252C%2520constructed%2520automatically%2520via%2520large%250Alanguage%2520models.%2520Our%2520results%2520show%2520that%2520foundation%2520VLMs%2520exhibit%2520zero-shot%250Ahierarchical%2520understanding%252C%2520surpassing%2520the%2520performance%2520of%2520prior%2520models%250Aexplicitly%2520designed%2520for%2520this%2520purpose.%2520Furthermore%252C%2520we%2520show%2520that%2520foundation%250Amodels%2520may%2520be%2520better%2520aligned%2520to%2520hierarchical%2520reasoning%2520via%2520a%2520text-only%250Afine-tuning%2520phase%252C%2520while%2520retaining%2520pretraining%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Visual-Semantic%20Hierarchies%20in%20Image-Text%20Representations&entry.906535625=Morris%20Alper%20and%20Hadar%20Averbuch-Elor&entry.1292438233=%20%20While%20recent%20vision-and-language%20models%20%28VLMs%29%20like%20CLIP%20are%20a%20powerful%20tool%0Afor%20analyzing%20text%20and%20images%20in%20a%20shared%20semantic%20space%2C%20they%20do%20not%0Aexplicitly%20model%20the%20hierarchical%20nature%20of%20the%20set%20of%20texts%20which%20may%20describe%0Aan%20image.%20Conversely%2C%20existing%20multimodal%20hierarchical%20representation%20learning%0Amethods%20require%20costly%20training%20from%20scratch%2C%20failing%20to%20leverage%20the%20knowledge%0Aencoded%20by%20state-of-the-art%20multimodal%20foundation%20models.%20In%20this%20work%2C%20we%0Astudy%20the%20knowledge%20of%20existing%20foundation%20models%2C%20finding%20that%20they%20exhibit%0Aemergent%20understanding%20of%20visual-semantic%20hierarchies%20despite%20not%20being%0Adirectly%20trained%20for%20this%20purpose.%20We%20propose%20the%20Radial%20Embedding%20%28RE%29%0Aframework%20for%20probing%20and%20optimizing%20hierarchical%20understanding%2C%20and%20contribute%0Athe%20HierarCaps%20dataset%2C%20a%20benchmark%20facilitating%20the%20study%20of%20hierarchical%0Aknowledge%20in%20image--text%20representations%2C%20constructed%20automatically%20via%20large%0Alanguage%20models.%20Our%20results%20show%20that%20foundation%20VLMs%20exhibit%20zero-shot%0Ahierarchical%20understanding%2C%20surpassing%20the%20performance%20of%20prior%20models%0Aexplicitly%20designed%20for%20this%20purpose.%20Furthermore%2C%20we%20show%20that%20foundation%0Amodels%20may%20be%20better%20aligned%20to%20hierarchical%20reasoning%20via%20a%20text-only%0Afine-tuning%20phase%2C%20while%20retaining%20pretraining%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08521v1&entry.124074799=Read"},
{"title": "Chunking: Continual Learning is not just about Distribution Shift", "author": "Thomas L. Lee and Amos Storkey", "abstract": "  Work on continual learning (CL) has thus far largely focused on the problems\narising from shifts in the data distribution. However, CL can be decomposed\ninto two sub-problems: (a) shifts in the data distribution, and (b) dealing\nwith the fact that the data is split into chunks and so only a part of the data\nis available to be trained on at any point in time. In this work, we look at\nthe latter sub-problem, the chunking of data. We show that chunking is an\nimportant part of CL, accounting for around half of the performance drop from\noffline learning in our experiments. Furthermore, our results reveal that\ncurrent CL algorithms do not address the chunking sub-problem, only performing\nas well as plain SGD training when there is no shift in the data distribution.\nTherefore, we show that chunking is both an important and currently unaddressed\nsub-problem and until it is addressed CL methods will be capped in performance.\nAdditionally, we analyse why performance drops when learning occurs on\nidentically distributed chunks of data, and find that forgetting, which is\noften seen to be a problem due to distribution shift, still arises and is a\nsignificant problem. We also show that performance on the chunking sub-problem\ncan be increased and that this performance transfers to the full CL setting,\nwhere there is distribution shift. Hence, we argue that work on chunking can\nhelp advance CL in general.\n", "link": "http://arxiv.org/abs/2310.02206v2", "date": "2024-07-11", "relevancy": 2.2168, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4393}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chunking%3A%20Continual%20Learning%20is%20not%20just%20about%20Distribution%20Shift&body=Title%3A%20Chunking%3A%20Continual%20Learning%20is%20not%20just%20about%20Distribution%20Shift%0AAuthor%3A%20Thomas%20L.%20Lee%20and%20Amos%20Storkey%0AAbstract%3A%20%20%20Work%20on%20continual%20learning%20%28CL%29%20has%20thus%20far%20largely%20focused%20on%20the%20problems%0Aarising%20from%20shifts%20in%20the%20data%20distribution.%20However%2C%20CL%20can%20be%20decomposed%0Ainto%20two%20sub-problems%3A%20%28a%29%20shifts%20in%20the%20data%20distribution%2C%20and%20%28b%29%20dealing%0Awith%20the%20fact%20that%20the%20data%20is%20split%20into%20chunks%20and%20so%20only%20a%20part%20of%20the%20data%0Ais%20available%20to%20be%20trained%20on%20at%20any%20point%20in%20time.%20In%20this%20work%2C%20we%20look%20at%0Athe%20latter%20sub-problem%2C%20the%20chunking%20of%20data.%20We%20show%20that%20chunking%20is%20an%0Aimportant%20part%20of%20CL%2C%20accounting%20for%20around%20half%20of%20the%20performance%20drop%20from%0Aoffline%20learning%20in%20our%20experiments.%20Furthermore%2C%20our%20results%20reveal%20that%0Acurrent%20CL%20algorithms%20do%20not%20address%20the%20chunking%20sub-problem%2C%20only%20performing%0Aas%20well%20as%20plain%20SGD%20training%20when%20there%20is%20no%20shift%20in%20the%20data%20distribution.%0ATherefore%2C%20we%20show%20that%20chunking%20is%20both%20an%20important%20and%20currently%20unaddressed%0Asub-problem%20and%20until%20it%20is%20addressed%20CL%20methods%20will%20be%20capped%20in%20performance.%0AAdditionally%2C%20we%20analyse%20why%20performance%20drops%20when%20learning%20occurs%20on%0Aidentically%20distributed%20chunks%20of%20data%2C%20and%20find%20that%20forgetting%2C%20which%20is%0Aoften%20seen%20to%20be%20a%20problem%20due%20to%20distribution%20shift%2C%20still%20arises%20and%20is%20a%0Asignificant%20problem.%20We%20also%20show%20that%20performance%20on%20the%20chunking%20sub-problem%0Acan%20be%20increased%20and%20that%20this%20performance%20transfers%20to%20the%20full%20CL%20setting%2C%0Awhere%20there%20is%20distribution%20shift.%20Hence%2C%20we%20argue%20that%20work%20on%20chunking%20can%0Ahelp%20advance%20CL%20in%20general.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChunking%253A%2520Continual%2520Learning%2520is%2520not%2520just%2520about%2520Distribution%2520Shift%26entry.906535625%3DThomas%2520L.%2520Lee%2520and%2520Amos%2520Storkey%26entry.1292438233%3D%2520%2520Work%2520on%2520continual%2520learning%2520%2528CL%2529%2520has%2520thus%2520far%2520largely%2520focused%2520on%2520the%2520problems%250Aarising%2520from%2520shifts%2520in%2520the%2520data%2520distribution.%2520However%252C%2520CL%2520can%2520be%2520decomposed%250Ainto%2520two%2520sub-problems%253A%2520%2528a%2529%2520shifts%2520in%2520the%2520data%2520distribution%252C%2520and%2520%2528b%2529%2520dealing%250Awith%2520the%2520fact%2520that%2520the%2520data%2520is%2520split%2520into%2520chunks%2520and%2520so%2520only%2520a%2520part%2520of%2520the%2520data%250Ais%2520available%2520to%2520be%2520trained%2520on%2520at%2520any%2520point%2520in%2520time.%2520In%2520this%2520work%252C%2520we%2520look%2520at%250Athe%2520latter%2520sub-problem%252C%2520the%2520chunking%2520of%2520data.%2520We%2520show%2520that%2520chunking%2520is%2520an%250Aimportant%2520part%2520of%2520CL%252C%2520accounting%2520for%2520around%2520half%2520of%2520the%2520performance%2520drop%2520from%250Aoffline%2520learning%2520in%2520our%2520experiments.%2520Furthermore%252C%2520our%2520results%2520reveal%2520that%250Acurrent%2520CL%2520algorithms%2520do%2520not%2520address%2520the%2520chunking%2520sub-problem%252C%2520only%2520performing%250Aas%2520well%2520as%2520plain%2520SGD%2520training%2520when%2520there%2520is%2520no%2520shift%2520in%2520the%2520data%2520distribution.%250ATherefore%252C%2520we%2520show%2520that%2520chunking%2520is%2520both%2520an%2520important%2520and%2520currently%2520unaddressed%250Asub-problem%2520and%2520until%2520it%2520is%2520addressed%2520CL%2520methods%2520will%2520be%2520capped%2520in%2520performance.%250AAdditionally%252C%2520we%2520analyse%2520why%2520performance%2520drops%2520when%2520learning%2520occurs%2520on%250Aidentically%2520distributed%2520chunks%2520of%2520data%252C%2520and%2520find%2520that%2520forgetting%252C%2520which%2520is%250Aoften%2520seen%2520to%2520be%2520a%2520problem%2520due%2520to%2520distribution%2520shift%252C%2520still%2520arises%2520and%2520is%2520a%250Asignificant%2520problem.%2520We%2520also%2520show%2520that%2520performance%2520on%2520the%2520chunking%2520sub-problem%250Acan%2520be%2520increased%2520and%2520that%2520this%2520performance%2520transfers%2520to%2520the%2520full%2520CL%2520setting%252C%250Awhere%2520there%2520is%2520distribution%2520shift.%2520Hence%252C%2520we%2520argue%2520that%2520work%2520on%2520chunking%2520can%250Ahelp%2520advance%2520CL%2520in%2520general.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chunking%3A%20Continual%20Learning%20is%20not%20just%20about%20Distribution%20Shift&entry.906535625=Thomas%20L.%20Lee%20and%20Amos%20Storkey&entry.1292438233=%20%20Work%20on%20continual%20learning%20%28CL%29%20has%20thus%20far%20largely%20focused%20on%20the%20problems%0Aarising%20from%20shifts%20in%20the%20data%20distribution.%20However%2C%20CL%20can%20be%20decomposed%0Ainto%20two%20sub-problems%3A%20%28a%29%20shifts%20in%20the%20data%20distribution%2C%20and%20%28b%29%20dealing%0Awith%20the%20fact%20that%20the%20data%20is%20split%20into%20chunks%20and%20so%20only%20a%20part%20of%20the%20data%0Ais%20available%20to%20be%20trained%20on%20at%20any%20point%20in%20time.%20In%20this%20work%2C%20we%20look%20at%0Athe%20latter%20sub-problem%2C%20the%20chunking%20of%20data.%20We%20show%20that%20chunking%20is%20an%0Aimportant%20part%20of%20CL%2C%20accounting%20for%20around%20half%20of%20the%20performance%20drop%20from%0Aoffline%20learning%20in%20our%20experiments.%20Furthermore%2C%20our%20results%20reveal%20that%0Acurrent%20CL%20algorithms%20do%20not%20address%20the%20chunking%20sub-problem%2C%20only%20performing%0Aas%20well%20as%20plain%20SGD%20training%20when%20there%20is%20no%20shift%20in%20the%20data%20distribution.%0ATherefore%2C%20we%20show%20that%20chunking%20is%20both%20an%20important%20and%20currently%20unaddressed%0Asub-problem%20and%20until%20it%20is%20addressed%20CL%20methods%20will%20be%20capped%20in%20performance.%0AAdditionally%2C%20we%20analyse%20why%20performance%20drops%20when%20learning%20occurs%20on%0Aidentically%20distributed%20chunks%20of%20data%2C%20and%20find%20that%20forgetting%2C%20which%20is%0Aoften%20seen%20to%20be%20a%20problem%20due%20to%20distribution%20shift%2C%20still%20arises%20and%20is%20a%0Asignificant%20problem.%20We%20also%20show%20that%20performance%20on%20the%20chunking%20sub-problem%0Acan%20be%20increased%20and%20that%20this%20performance%20transfers%20to%20the%20full%20CL%20setting%2C%0Awhere%20there%20is%20distribution%20shift.%20Hence%2C%20we%20argue%20that%20work%20on%20chunking%20can%0Ahelp%20advance%20CL%20in%20general.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02206v2&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Scalable Vector Graphics-Driven\n  Image Understanding", "author": "Mu Cai and Zeyi Huang and Yuheng Li and Utkarsh Ojha and Haohan Wang and Yong Jae Lee", "abstract": "  Large language models (LLMs) have made significant advancements in natural\nlanguage understanding. However, through that enormous semantic representation\nthat the LLM has learnt, is it somehow possible for it to understand images as\nwell? This work investigates this question. To enable the LLM to process\nimages, we convert them into a representation given by Scalable Vector Graphics\n(SVG). To study what the LLM can do with this XML-based textual description of\nimages, we test the LLM on three broad computer vision tasks: (i) visual\nreasoning and question answering, (ii) image classification under distribution\nshift, few-shot learning, and (iii) generating new images using visual\nprompting. Even though we do not naturally associate LLMs with any visual\nunderstanding capabilities, our results indicate that the LLM can often do a\ndecent job in many of these tasks, potentially opening new avenues for research\ninto LLMs' ability to understand image data. Our code, data, and models can be\nfound here https://github.com/mu-cai/svg-llm.\n", "link": "http://arxiv.org/abs/2306.06094v2", "date": "2024-07-11", "relevancy": 2.2109, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5679}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5498}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Scalable%20Vector%20Graphics-Driven%0A%20%20Image%20Understanding&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Scalable%20Vector%20Graphics-Driven%0A%20%20Image%20Understanding%0AAuthor%3A%20Mu%20Cai%20and%20Zeyi%20Huang%20and%20Yuheng%20Li%20and%20Utkarsh%20Ojha%20and%20Haohan%20Wang%20and%20Yong%20Jae%20Lee%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20made%20significant%20advancements%20in%20natural%0Alanguage%20understanding.%20However%2C%20through%20that%20enormous%20semantic%20representation%0Athat%20the%20LLM%20has%20learnt%2C%20is%20it%20somehow%20possible%20for%20it%20to%20understand%20images%20as%0Awell%3F%20This%20work%20investigates%20this%20question.%20To%20enable%20the%20LLM%20to%20process%0Aimages%2C%20we%20convert%20them%20into%20a%20representation%20given%20by%20Scalable%20Vector%20Graphics%0A%28SVG%29.%20To%20study%20what%20the%20LLM%20can%20do%20with%20this%20XML-based%20textual%20description%20of%0Aimages%2C%20we%20test%20the%20LLM%20on%20three%20broad%20computer%20vision%20tasks%3A%20%28i%29%20visual%0Areasoning%20and%20question%20answering%2C%20%28ii%29%20image%20classification%20under%20distribution%0Ashift%2C%20few-shot%20learning%2C%20and%20%28iii%29%20generating%20new%20images%20using%20visual%0Aprompting.%20Even%20though%20we%20do%20not%20naturally%20associate%20LLMs%20with%20any%20visual%0Aunderstanding%20capabilities%2C%20our%20results%20indicate%20that%20the%20LLM%20can%20often%20do%20a%0Adecent%20job%20in%20many%20of%20these%20tasks%2C%20potentially%20opening%20new%20avenues%20for%20research%0Ainto%20LLMs%27%20ability%20to%20understand%20image%20data.%20Our%20code%2C%20data%2C%20and%20models%20can%20be%0Afound%20here%20https%3A//github.com/mu-cai/svg-llm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Scalable%2520Vector%2520Graphics-Driven%250A%2520%2520Image%2520Understanding%26entry.906535625%3DMu%2520Cai%2520and%2520Zeyi%2520Huang%2520and%2520Yuheng%2520Li%2520and%2520Utkarsh%2520Ojha%2520and%2520Haohan%2520Wang%2520and%2520Yong%2520Jae%2520Lee%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520advancements%2520in%2520natural%250Alanguage%2520understanding.%2520However%252C%2520through%2520that%2520enormous%2520semantic%2520representation%250Athat%2520the%2520LLM%2520has%2520learnt%252C%2520is%2520it%2520somehow%2520possible%2520for%2520it%2520to%2520understand%2520images%2520as%250Awell%253F%2520This%2520work%2520investigates%2520this%2520question.%2520To%2520enable%2520the%2520LLM%2520to%2520process%250Aimages%252C%2520we%2520convert%2520them%2520into%2520a%2520representation%2520given%2520by%2520Scalable%2520Vector%2520Graphics%250A%2528SVG%2529.%2520To%2520study%2520what%2520the%2520LLM%2520can%2520do%2520with%2520this%2520XML-based%2520textual%2520description%2520of%250Aimages%252C%2520we%2520test%2520the%2520LLM%2520on%2520three%2520broad%2520computer%2520vision%2520tasks%253A%2520%2528i%2529%2520visual%250Areasoning%2520and%2520question%2520answering%252C%2520%2528ii%2529%2520image%2520classification%2520under%2520distribution%250Ashift%252C%2520few-shot%2520learning%252C%2520and%2520%2528iii%2529%2520generating%2520new%2520images%2520using%2520visual%250Aprompting.%2520Even%2520though%2520we%2520do%2520not%2520naturally%2520associate%2520LLMs%2520with%2520any%2520visual%250Aunderstanding%2520capabilities%252C%2520our%2520results%2520indicate%2520that%2520the%2520LLM%2520can%2520often%2520do%2520a%250Adecent%2520job%2520in%2520many%2520of%2520these%2520tasks%252C%2520potentially%2520opening%2520new%2520avenues%2520for%2520research%250Ainto%2520LLMs%2527%2520ability%2520to%2520understand%2520image%2520data.%2520Our%2520code%252C%2520data%252C%2520and%2520models%2520can%2520be%250Afound%2520here%2520https%253A//github.com/mu-cai/svg-llm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Scalable%20Vector%20Graphics-Driven%0A%20%20Image%20Understanding&entry.906535625=Mu%20Cai%20and%20Zeyi%20Huang%20and%20Yuheng%20Li%20and%20Utkarsh%20Ojha%20and%20Haohan%20Wang%20and%20Yong%20Jae%20Lee&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20made%20significant%20advancements%20in%20natural%0Alanguage%20understanding.%20However%2C%20through%20that%20enormous%20semantic%20representation%0Athat%20the%20LLM%20has%20learnt%2C%20is%20it%20somehow%20possible%20for%20it%20to%20understand%20images%20as%0Awell%3F%20This%20work%20investigates%20this%20question.%20To%20enable%20the%20LLM%20to%20process%0Aimages%2C%20we%20convert%20them%20into%20a%20representation%20given%20by%20Scalable%20Vector%20Graphics%0A%28SVG%29.%20To%20study%20what%20the%20LLM%20can%20do%20with%20this%20XML-based%20textual%20description%20of%0Aimages%2C%20we%20test%20the%20LLM%20on%20three%20broad%20computer%20vision%20tasks%3A%20%28i%29%20visual%0Areasoning%20and%20question%20answering%2C%20%28ii%29%20image%20classification%20under%20distribution%0Ashift%2C%20few-shot%20learning%2C%20and%20%28iii%29%20generating%20new%20images%20using%20visual%0Aprompting.%20Even%20though%20we%20do%20not%20naturally%20associate%20LLMs%20with%20any%20visual%0Aunderstanding%20capabilities%2C%20our%20results%20indicate%20that%20the%20LLM%20can%20often%20do%20a%0Adecent%20job%20in%20many%20of%20these%20tasks%2C%20potentially%20opening%20new%20avenues%20for%20research%0Ainto%20LLMs%27%20ability%20to%20understand%20image%20data.%20Our%20code%2C%20data%2C%20and%20models%20can%20be%0Afound%20here%20https%3A//github.com/mu-cai/svg-llm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06094v2&entry.124074799=Read"},
{"title": "Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic\n  Segmentation", "author": "Hairong Shi and Songhao Han and Shaofei Huang and Yue Liao and Guanbin Li and Xiangxing Kong and Hua Zhu and Xiaomu Wang and Si Liu", "abstract": "  Tumor lesion segmentation on CT or MRI images plays a critical role in cancer\ndiagnosis and treatment planning. Considering the inherent differences in tumor\nlesion segmentation data across various medical imaging modalities and\nequipment, integrating medical knowledge into the Segment Anything Model (SAM)\npresents promising capability due to its versatility and generalization\npotential. Recent studies have attempted to enhance SAM with medical expertise\nby pre-training on large-scale medical segmentation datasets. However,\nchallenges still exist in 3D tumor lesion segmentation owing to tumor\ncomplexity and the imbalance in foreground and background regions. Therefore,\nwe introduce Mask-Enhanced SAM (M-SAM), an innovative architecture tailored for\n3D tumor lesion segmentation. We propose a novel Mask-Enhanced Adapter (MEA)\nwithin M-SAM that enriches the semantic information of medical images with\npositional data from coarse segmentation masks, facilitating the generation of\nmore precise segmentation masks. Furthermore, an iterative refinement scheme is\nimplemented in M-SAM to refine the segmentation masks progressively, leading to\nimproved performance. Extensive experiments on seven tumor lesion segmentation\ndatasets indicate that our M-SAM not only achieves high segmentation accuracy\nbut also exhibits robust generalization. The code is available at\nhttps://github.com/nanase1025/M-SAM.\n", "link": "http://arxiv.org/abs/2403.05912v2", "date": "2024-07-11", "relevancy": 2.1992, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5892}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask-Enhanced%20Segment%20Anything%20Model%20for%20Tumor%20Lesion%20Semantic%0A%20%20Segmentation&body=Title%3A%20Mask-Enhanced%20Segment%20Anything%20Model%20for%20Tumor%20Lesion%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Hairong%20Shi%20and%20Songhao%20Han%20and%20Shaofei%20Huang%20and%20Yue%20Liao%20and%20Guanbin%20Li%20and%20Xiangxing%20Kong%20and%20Hua%20Zhu%20and%20Xiaomu%20Wang%20and%20Si%20Liu%0AAbstract%3A%20%20%20Tumor%20lesion%20segmentation%20on%20CT%20or%20MRI%20images%20plays%20a%20critical%20role%20in%20cancer%0Adiagnosis%20and%20treatment%20planning.%20Considering%20the%20inherent%20differences%20in%20tumor%0Alesion%20segmentation%20data%20across%20various%20medical%20imaging%20modalities%20and%0Aequipment%2C%20integrating%20medical%20knowledge%20into%20the%20Segment%20Anything%20Model%20%28SAM%29%0Apresents%20promising%20capability%20due%20to%20its%20versatility%20and%20generalization%0Apotential.%20Recent%20studies%20have%20attempted%20to%20enhance%20SAM%20with%20medical%20expertise%0Aby%20pre-training%20on%20large-scale%20medical%20segmentation%20datasets.%20However%2C%0Achallenges%20still%20exist%20in%203D%20tumor%20lesion%20segmentation%20owing%20to%20tumor%0Acomplexity%20and%20the%20imbalance%20in%20foreground%20and%20background%20regions.%20Therefore%2C%0Awe%20introduce%20Mask-Enhanced%20SAM%20%28M-SAM%29%2C%20an%20innovative%20architecture%20tailored%20for%0A3D%20tumor%20lesion%20segmentation.%20We%20propose%20a%20novel%20Mask-Enhanced%20Adapter%20%28MEA%29%0Awithin%20M-SAM%20that%20enriches%20the%20semantic%20information%20of%20medical%20images%20with%0Apositional%20data%20from%20coarse%20segmentation%20masks%2C%20facilitating%20the%20generation%20of%0Amore%20precise%20segmentation%20masks.%20Furthermore%2C%20an%20iterative%20refinement%20scheme%20is%0Aimplemented%20in%20M-SAM%20to%20refine%20the%20segmentation%20masks%20progressively%2C%20leading%20to%0Aimproved%20performance.%20Extensive%20experiments%20on%20seven%20tumor%20lesion%20segmentation%0Adatasets%20indicate%20that%20our%20M-SAM%20not%20only%20achieves%20high%20segmentation%20accuracy%0Abut%20also%20exhibits%20robust%20generalization.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/nanase1025/M-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask-Enhanced%2520Segment%2520Anything%2520Model%2520for%2520Tumor%2520Lesion%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DHairong%2520Shi%2520and%2520Songhao%2520Han%2520and%2520Shaofei%2520Huang%2520and%2520Yue%2520Liao%2520and%2520Guanbin%2520Li%2520and%2520Xiangxing%2520Kong%2520and%2520Hua%2520Zhu%2520and%2520Xiaomu%2520Wang%2520and%2520Si%2520Liu%26entry.1292438233%3D%2520%2520Tumor%2520lesion%2520segmentation%2520on%2520CT%2520or%2520MRI%2520images%2520plays%2520a%2520critical%2520role%2520in%2520cancer%250Adiagnosis%2520and%2520treatment%2520planning.%2520Considering%2520the%2520inherent%2520differences%2520in%2520tumor%250Alesion%2520segmentation%2520data%2520across%2520various%2520medical%2520imaging%2520modalities%2520and%250Aequipment%252C%2520integrating%2520medical%2520knowledge%2520into%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%250Apresents%2520promising%2520capability%2520due%2520to%2520its%2520versatility%2520and%2520generalization%250Apotential.%2520Recent%2520studies%2520have%2520attempted%2520to%2520enhance%2520SAM%2520with%2520medical%2520expertise%250Aby%2520pre-training%2520on%2520large-scale%2520medical%2520segmentation%2520datasets.%2520However%252C%250Achallenges%2520still%2520exist%2520in%25203D%2520tumor%2520lesion%2520segmentation%2520owing%2520to%2520tumor%250Acomplexity%2520and%2520the%2520imbalance%2520in%2520foreground%2520and%2520background%2520regions.%2520Therefore%252C%250Awe%2520introduce%2520Mask-Enhanced%2520SAM%2520%2528M-SAM%2529%252C%2520an%2520innovative%2520architecture%2520tailored%2520for%250A3D%2520tumor%2520lesion%2520segmentation.%2520We%2520propose%2520a%2520novel%2520Mask-Enhanced%2520Adapter%2520%2528MEA%2529%250Awithin%2520M-SAM%2520that%2520enriches%2520the%2520semantic%2520information%2520of%2520medical%2520images%2520with%250Apositional%2520data%2520from%2520coarse%2520segmentation%2520masks%252C%2520facilitating%2520the%2520generation%2520of%250Amore%2520precise%2520segmentation%2520masks.%2520Furthermore%252C%2520an%2520iterative%2520refinement%2520scheme%2520is%250Aimplemented%2520in%2520M-SAM%2520to%2520refine%2520the%2520segmentation%2520masks%2520progressively%252C%2520leading%2520to%250Aimproved%2520performance.%2520Extensive%2520experiments%2520on%2520seven%2520tumor%2520lesion%2520segmentation%250Adatasets%2520indicate%2520that%2520our%2520M-SAM%2520not%2520only%2520achieves%2520high%2520segmentation%2520accuracy%250Abut%2520also%2520exhibits%2520robust%2520generalization.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/nanase1025/M-SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask-Enhanced%20Segment%20Anything%20Model%20for%20Tumor%20Lesion%20Semantic%0A%20%20Segmentation&entry.906535625=Hairong%20Shi%20and%20Songhao%20Han%20and%20Shaofei%20Huang%20and%20Yue%20Liao%20and%20Guanbin%20Li%20and%20Xiangxing%20Kong%20and%20Hua%20Zhu%20and%20Xiaomu%20Wang%20and%20Si%20Liu&entry.1292438233=%20%20Tumor%20lesion%20segmentation%20on%20CT%20or%20MRI%20images%20plays%20a%20critical%20role%20in%20cancer%0Adiagnosis%20and%20treatment%20planning.%20Considering%20the%20inherent%20differences%20in%20tumor%0Alesion%20segmentation%20data%20across%20various%20medical%20imaging%20modalities%20and%0Aequipment%2C%20integrating%20medical%20knowledge%20into%20the%20Segment%20Anything%20Model%20%28SAM%29%0Apresents%20promising%20capability%20due%20to%20its%20versatility%20and%20generalization%0Apotential.%20Recent%20studies%20have%20attempted%20to%20enhance%20SAM%20with%20medical%20expertise%0Aby%20pre-training%20on%20large-scale%20medical%20segmentation%20datasets.%20However%2C%0Achallenges%20still%20exist%20in%203D%20tumor%20lesion%20segmentation%20owing%20to%20tumor%0Acomplexity%20and%20the%20imbalance%20in%20foreground%20and%20background%20regions.%20Therefore%2C%0Awe%20introduce%20Mask-Enhanced%20SAM%20%28M-SAM%29%2C%20an%20innovative%20architecture%20tailored%20for%0A3D%20tumor%20lesion%20segmentation.%20We%20propose%20a%20novel%20Mask-Enhanced%20Adapter%20%28MEA%29%0Awithin%20M-SAM%20that%20enriches%20the%20semantic%20information%20of%20medical%20images%20with%0Apositional%20data%20from%20coarse%20segmentation%20masks%2C%20facilitating%20the%20generation%20of%0Amore%20precise%20segmentation%20masks.%20Furthermore%2C%20an%20iterative%20refinement%20scheme%20is%0Aimplemented%20in%20M-SAM%20to%20refine%20the%20segmentation%20masks%20progressively%2C%20leading%20to%0Aimproved%20performance.%20Extensive%20experiments%20on%20seven%20tumor%20lesion%20segmentation%0Adatasets%20indicate%20that%20our%20M-SAM%20not%20only%20achieves%20high%20segmentation%20accuracy%0Abut%20also%20exhibits%20robust%20generalization.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/nanase1025/M-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05912v2&entry.124074799=Read"},
{"title": "Controlling the Fidelity and Diversity of Deep Generative Models via\n  Pseudo Density", "author": "Shuangqi Li and Chen Liu and Tong Zhang and Hieu Le and Sabine S\u00fcsstrunk and Mathieu Salzmann", "abstract": "  We introduce an approach to bias deep generative models, such as GANs and\ndiffusion models, towards generating data with either enhanced fidelity or\nincreased diversity. Our approach involves manipulating the distribution of\ntraining and generated data through a novel metric for individual samples,\nnamed pseudo density, which is based on the nearest-neighbor information from\nreal samples. Our approach offers three distinct techniques to adjust the\nfidelity and diversity of deep generative models: 1) Per-sample perturbation,\nenabling precise adjustments for individual samples towards either more common\nor more unique characteristics; 2) Importance sampling during model inference\nto enhance either fidelity or diversity in the generated data; 3) Fine-tuning\nwith importance sampling, which guides the generative model to learn an\nadjusted distribution, thus controlling fidelity and diversity. Furthermore,\nour fine-tuning method demonstrates the ability to improve the Frechet\nInception Distance (FID) for pre-trained generative models with minimal\niterations.\n", "link": "http://arxiv.org/abs/2407.08659v1", "date": "2024-07-11", "relevancy": 2.1982, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5661}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5396}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20the%20Fidelity%20and%20Diversity%20of%20Deep%20Generative%20Models%20via%0A%20%20Pseudo%20Density&body=Title%3A%20Controlling%20the%20Fidelity%20and%20Diversity%20of%20Deep%20Generative%20Models%20via%0A%20%20Pseudo%20Density%0AAuthor%3A%20Shuangqi%20Li%20and%20Chen%20Liu%20and%20Tong%20Zhang%20and%20Hieu%20Le%20and%20Sabine%20S%C3%BCsstrunk%20and%20Mathieu%20Salzmann%0AAbstract%3A%20%20%20We%20introduce%20an%20approach%20to%20bias%20deep%20generative%20models%2C%20such%20as%20GANs%20and%0Adiffusion%20models%2C%20towards%20generating%20data%20with%20either%20enhanced%20fidelity%20or%0Aincreased%20diversity.%20Our%20approach%20involves%20manipulating%20the%20distribution%20of%0Atraining%20and%20generated%20data%20through%20a%20novel%20metric%20for%20individual%20samples%2C%0Anamed%20pseudo%20density%2C%20which%20is%20based%20on%20the%20nearest-neighbor%20information%20from%0Areal%20samples.%20Our%20approach%20offers%20three%20distinct%20techniques%20to%20adjust%20the%0Afidelity%20and%20diversity%20of%20deep%20generative%20models%3A%201%29%20Per-sample%20perturbation%2C%0Aenabling%20precise%20adjustments%20for%20individual%20samples%20towards%20either%20more%20common%0Aor%20more%20unique%20characteristics%3B%202%29%20Importance%20sampling%20during%20model%20inference%0Ato%20enhance%20either%20fidelity%20or%20diversity%20in%20the%20generated%20data%3B%203%29%20Fine-tuning%0Awith%20importance%20sampling%2C%20which%20guides%20the%20generative%20model%20to%20learn%20an%0Aadjusted%20distribution%2C%20thus%20controlling%20fidelity%20and%20diversity.%20Furthermore%2C%0Aour%20fine-tuning%20method%20demonstrates%20the%20ability%20to%20improve%20the%20Frechet%0AInception%20Distance%20%28FID%29%20for%20pre-trained%20generative%20models%20with%20minimal%0Aiterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520the%2520Fidelity%2520and%2520Diversity%2520of%2520Deep%2520Generative%2520Models%2520via%250A%2520%2520Pseudo%2520Density%26entry.906535625%3DShuangqi%2520Li%2520and%2520Chen%2520Liu%2520and%2520Tong%2520Zhang%2520and%2520Hieu%2520Le%2520and%2520Sabine%2520S%25C3%25BCsstrunk%2520and%2520Mathieu%2520Salzmann%26entry.1292438233%3D%2520%2520We%2520introduce%2520an%2520approach%2520to%2520bias%2520deep%2520generative%2520models%252C%2520such%2520as%2520GANs%2520and%250Adiffusion%2520models%252C%2520towards%2520generating%2520data%2520with%2520either%2520enhanced%2520fidelity%2520or%250Aincreased%2520diversity.%2520Our%2520approach%2520involves%2520manipulating%2520the%2520distribution%2520of%250Atraining%2520and%2520generated%2520data%2520through%2520a%2520novel%2520metric%2520for%2520individual%2520samples%252C%250Anamed%2520pseudo%2520density%252C%2520which%2520is%2520based%2520on%2520the%2520nearest-neighbor%2520information%2520from%250Areal%2520samples.%2520Our%2520approach%2520offers%2520three%2520distinct%2520techniques%2520to%2520adjust%2520the%250Afidelity%2520and%2520diversity%2520of%2520deep%2520generative%2520models%253A%25201%2529%2520Per-sample%2520perturbation%252C%250Aenabling%2520precise%2520adjustments%2520for%2520individual%2520samples%2520towards%2520either%2520more%2520common%250Aor%2520more%2520unique%2520characteristics%253B%25202%2529%2520Importance%2520sampling%2520during%2520model%2520inference%250Ato%2520enhance%2520either%2520fidelity%2520or%2520diversity%2520in%2520the%2520generated%2520data%253B%25203%2529%2520Fine-tuning%250Awith%2520importance%2520sampling%252C%2520which%2520guides%2520the%2520generative%2520model%2520to%2520learn%2520an%250Aadjusted%2520distribution%252C%2520thus%2520controlling%2520fidelity%2520and%2520diversity.%2520Furthermore%252C%250Aour%2520fine-tuning%2520method%2520demonstrates%2520the%2520ability%2520to%2520improve%2520the%2520Frechet%250AInception%2520Distance%2520%2528FID%2529%2520for%2520pre-trained%2520generative%2520models%2520with%2520minimal%250Aiterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20the%20Fidelity%20and%20Diversity%20of%20Deep%20Generative%20Models%20via%0A%20%20Pseudo%20Density&entry.906535625=Shuangqi%20Li%20and%20Chen%20Liu%20and%20Tong%20Zhang%20and%20Hieu%20Le%20and%20Sabine%20S%C3%BCsstrunk%20and%20Mathieu%20Salzmann&entry.1292438233=%20%20We%20introduce%20an%20approach%20to%20bias%20deep%20generative%20models%2C%20such%20as%20GANs%20and%0Adiffusion%20models%2C%20towards%20generating%20data%20with%20either%20enhanced%20fidelity%20or%0Aincreased%20diversity.%20Our%20approach%20involves%20manipulating%20the%20distribution%20of%0Atraining%20and%20generated%20data%20through%20a%20novel%20metric%20for%20individual%20samples%2C%0Anamed%20pseudo%20density%2C%20which%20is%20based%20on%20the%20nearest-neighbor%20information%20from%0Areal%20samples.%20Our%20approach%20offers%20three%20distinct%20techniques%20to%20adjust%20the%0Afidelity%20and%20diversity%20of%20deep%20generative%20models%3A%201%29%20Per-sample%20perturbation%2C%0Aenabling%20precise%20adjustments%20for%20individual%20samples%20towards%20either%20more%20common%0Aor%20more%20unique%20characteristics%3B%202%29%20Importance%20sampling%20during%20model%20inference%0Ato%20enhance%20either%20fidelity%20or%20diversity%20in%20the%20generated%20data%3B%203%29%20Fine-tuning%0Awith%20importance%20sampling%2C%20which%20guides%20the%20generative%20model%20to%20learn%20an%0Aadjusted%20distribution%2C%20thus%20controlling%20fidelity%20and%20diversity.%20Furthermore%2C%0Aour%20fine-tuning%20method%20demonstrates%20the%20ability%20to%20improve%20the%20Frechet%0AInception%20Distance%20%28FID%29%20for%20pre-trained%20generative%20models%20with%20minimal%0Aiterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08659v1&entry.124074799=Read"},
{"title": "VideoMamba: Spatio-Temporal Selective State Space Model", "author": "Jinyoung Park and Hee-Seon Kim and Kangwook Ko and Minbeom Kim and Changick Kim", "abstract": "  We introduce VideoMamba, a novel adaptation of the pure Mamba architecture,\nspecifically designed for video recognition. Unlike transformers that rely on\nself-attention mechanisms leading to high computational costs by quadratic\ncomplexity, VideoMamba leverages Mamba's linear complexity and selective SSM\nmechanism for more efficient processing. The proposed Spatio-Temporal Forward\nand Backward SSM allows the model to effectively capture the complex\nrelationship between non-sequential spatial and sequential temporal information\nin video. Consequently, VideoMamba is not only resource-efficient but also\neffective in capturing long-range dependency in videos, demonstrated by\ncompetitive performance and outstanding efficiency on a variety of video\nunderstanding benchmarks. Our work highlights the potential of VideoMamba as a\npowerful tool for video understanding, offering a simple yet effective baseline\nfor future research in video analysis.\n", "link": "http://arxiv.org/abs/2407.08476v1", "date": "2024-07-11", "relevancy": 2.1844, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5846}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5441}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoMamba%3A%20Spatio-Temporal%20Selective%20State%20Space%20Model&body=Title%3A%20VideoMamba%3A%20Spatio-Temporal%20Selective%20State%20Space%20Model%0AAuthor%3A%20Jinyoung%20Park%20and%20Hee-Seon%20Kim%20and%20Kangwook%20Ko%20and%20Minbeom%20Kim%20and%20Changick%20Kim%0AAbstract%3A%20%20%20We%20introduce%20VideoMamba%2C%20a%20novel%20adaptation%20of%20the%20pure%20Mamba%20architecture%2C%0Aspecifically%20designed%20for%20video%20recognition.%20Unlike%20transformers%20that%20rely%20on%0Aself-attention%20mechanisms%20leading%20to%20high%20computational%20costs%20by%20quadratic%0Acomplexity%2C%20VideoMamba%20leverages%20Mamba%27s%20linear%20complexity%20and%20selective%20SSM%0Amechanism%20for%20more%20efficient%20processing.%20The%20proposed%20Spatio-Temporal%20Forward%0Aand%20Backward%20SSM%20allows%20the%20model%20to%20effectively%20capture%20the%20complex%0Arelationship%20between%20non-sequential%20spatial%20and%20sequential%20temporal%20information%0Ain%20video.%20Consequently%2C%20VideoMamba%20is%20not%20only%20resource-efficient%20but%20also%0Aeffective%20in%20capturing%20long-range%20dependency%20in%20videos%2C%20demonstrated%20by%0Acompetitive%20performance%20and%20outstanding%20efficiency%20on%20a%20variety%20of%20video%0Aunderstanding%20benchmarks.%20Our%20work%20highlights%20the%20potential%20of%20VideoMamba%20as%20a%0Apowerful%20tool%20for%20video%20understanding%2C%20offering%20a%20simple%20yet%20effective%20baseline%0Afor%20future%20research%20in%20video%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoMamba%253A%2520Spatio-Temporal%2520Selective%2520State%2520Space%2520Model%26entry.906535625%3DJinyoung%2520Park%2520and%2520Hee-Seon%2520Kim%2520and%2520Kangwook%2520Ko%2520and%2520Minbeom%2520Kim%2520and%2520Changick%2520Kim%26entry.1292438233%3D%2520%2520We%2520introduce%2520VideoMamba%252C%2520a%2520novel%2520adaptation%2520of%2520the%2520pure%2520Mamba%2520architecture%252C%250Aspecifically%2520designed%2520for%2520video%2520recognition.%2520Unlike%2520transformers%2520that%2520rely%2520on%250Aself-attention%2520mechanisms%2520leading%2520to%2520high%2520computational%2520costs%2520by%2520quadratic%250Acomplexity%252C%2520VideoMamba%2520leverages%2520Mamba%2527s%2520linear%2520complexity%2520and%2520selective%2520SSM%250Amechanism%2520for%2520more%2520efficient%2520processing.%2520The%2520proposed%2520Spatio-Temporal%2520Forward%250Aand%2520Backward%2520SSM%2520allows%2520the%2520model%2520to%2520effectively%2520capture%2520the%2520complex%250Arelationship%2520between%2520non-sequential%2520spatial%2520and%2520sequential%2520temporal%2520information%250Ain%2520video.%2520Consequently%252C%2520VideoMamba%2520is%2520not%2520only%2520resource-efficient%2520but%2520also%250Aeffective%2520in%2520capturing%2520long-range%2520dependency%2520in%2520videos%252C%2520demonstrated%2520by%250Acompetitive%2520performance%2520and%2520outstanding%2520efficiency%2520on%2520a%2520variety%2520of%2520video%250Aunderstanding%2520benchmarks.%2520Our%2520work%2520highlights%2520the%2520potential%2520of%2520VideoMamba%2520as%2520a%250Apowerful%2520tool%2520for%2520video%2520understanding%252C%2520offering%2520a%2520simple%2520yet%2520effective%2520baseline%250Afor%2520future%2520research%2520in%2520video%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoMamba%3A%20Spatio-Temporal%20Selective%20State%20Space%20Model&entry.906535625=Jinyoung%20Park%20and%20Hee-Seon%20Kim%20and%20Kangwook%20Ko%20and%20Minbeom%20Kim%20and%20Changick%20Kim&entry.1292438233=%20%20We%20introduce%20VideoMamba%2C%20a%20novel%20adaptation%20of%20the%20pure%20Mamba%20architecture%2C%0Aspecifically%20designed%20for%20video%20recognition.%20Unlike%20transformers%20that%20rely%20on%0Aself-attention%20mechanisms%20leading%20to%20high%20computational%20costs%20by%20quadratic%0Acomplexity%2C%20VideoMamba%20leverages%20Mamba%27s%20linear%20complexity%20and%20selective%20SSM%0Amechanism%20for%20more%20efficient%20processing.%20The%20proposed%20Spatio-Temporal%20Forward%0Aand%20Backward%20SSM%20allows%20the%20model%20to%20effectively%20capture%20the%20complex%0Arelationship%20between%20non-sequential%20spatial%20and%20sequential%20temporal%20information%0Ain%20video.%20Consequently%2C%20VideoMamba%20is%20not%20only%20resource-efficient%20but%20also%0Aeffective%20in%20capturing%20long-range%20dependency%20in%20videos%2C%20demonstrated%20by%0Acompetitive%20performance%20and%20outstanding%20efficiency%20on%20a%20variety%20of%20video%0Aunderstanding%20benchmarks.%20Our%20work%20highlights%20the%20potential%20of%20VideoMamba%20as%20a%0Apowerful%20tool%20for%20video%20understanding%2C%20offering%20a%20simple%20yet%20effective%20baseline%0Afor%20future%20research%20in%20video%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08476v1&entry.124074799=Read"},
{"title": "Beyond Aesthetics: Cultural Competence in Text-to-Image Models", "author": "Nithish Kannen and Arif Ahmad and Marco Andreetto and Vinodkumar Prabhakaran and Utsav Prabhu and Adji Bousso Dieng and Pushpak Bhattacharyya and Shachi Dave", "abstract": "  Text-to-Image (T2I) models are being increasingly adopted in diverse global\ncommunities where they create visual representations of their unique cultures.\nCurrent T2I benchmarks primarily focus on faithfulness, aesthetics, and realism\nof generated images, overlooking the critical dimension of cultural competence.\nIn this work, we introduce a framework to evaluate cultural competence of T2I\nmodels along two crucial dimensions: cultural awareness and cultural diversity,\nand present a scalable approach using a combination of structured knowledge\nbases and large language models to build a large dataset of cultural artifacts\nto enable this evaluation. In particular, we apply this approach to build CUBE\n(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to\nevaluate cultural competence of T2I models. CUBE covers cultural artifacts\nassociated with 8 countries across different geo-cultural regions and along 3\nconcepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of\nhigh-quality prompts that enable the evaluation of cultural awareness, and 2)\nCUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to\nevaluate cultural diversity. We also introduce cultural diversity as a novel\nT2I evaluation component, leveraging quality-weighted Vendi score. Our\nevaluations reveal significant gaps in the cultural awareness of existing\nmodels across countries and provide valuable insights into the cultural\ndiversity of T2I outputs for under-specified prompts. Our methodology is\nextendable to other cultural regions and concepts, and can facilitate the\ndevelopment of T2I models that better cater to the global population.\n", "link": "http://arxiv.org/abs/2407.06863v2", "date": "2024-07-11", "relevancy": 2.1703, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5503}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5475}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Aesthetics%3A%20Cultural%20Competence%20in%20Text-to-Image%20Models&body=Title%3A%20Beyond%20Aesthetics%3A%20Cultural%20Competence%20in%20Text-to-Image%20Models%0AAuthor%3A%20Nithish%20Kannen%20and%20Arif%20Ahmad%20and%20Marco%20Andreetto%20and%20Vinodkumar%20Prabhakaran%20and%20Utsav%20Prabhu%20and%20Adji%20Bousso%20Dieng%20and%20Pushpak%20Bhattacharyya%20and%20Shachi%20Dave%0AAbstract%3A%20%20%20Text-to-Image%20%28T2I%29%20models%20are%20being%20increasingly%20adopted%20in%20diverse%20global%0Acommunities%20where%20they%20create%20visual%20representations%20of%20their%20unique%20cultures.%0ACurrent%20T2I%20benchmarks%20primarily%20focus%20on%20faithfulness%2C%20aesthetics%2C%20and%20realism%0Aof%20generated%20images%2C%20overlooking%20the%20critical%20dimension%20of%20cultural%20competence.%0AIn%20this%20work%2C%20we%20introduce%20a%20framework%20to%20evaluate%20cultural%20competence%20of%20T2I%0Amodels%20along%20two%20crucial%20dimensions%3A%20cultural%20awareness%20and%20cultural%20diversity%2C%0Aand%20present%20a%20scalable%20approach%20using%20a%20combination%20of%20structured%20knowledge%0Abases%20and%20large%20language%20models%20to%20build%20a%20large%20dataset%20of%20cultural%20artifacts%0Ato%20enable%20this%20evaluation.%20In%20particular%2C%20we%20apply%20this%20approach%20to%20build%20CUBE%0A%28CUltural%20BEnchmark%20for%20Text-to-Image%20models%29%2C%20a%20first-of-its-kind%20benchmark%20to%0Aevaluate%20cultural%20competence%20of%20T2I%20models.%20CUBE%20covers%20cultural%20artifacts%0Aassociated%20with%208%20countries%20across%20different%20geo-cultural%20regions%20and%20along%203%0Aconcepts%3A%20cuisine%2C%20landmarks%2C%20and%20art.%20CUBE%20consists%20of%201%29%20CUBE-1K%2C%20a%20set%20of%0Ahigh-quality%20prompts%20that%20enable%20the%20evaluation%20of%20cultural%20awareness%2C%20and%202%29%0ACUBE-CSpace%2C%20a%20larger%20dataset%20of%20cultural%20artifacts%20that%20serves%20as%20grounding%20to%0Aevaluate%20cultural%20diversity.%20We%20also%20introduce%20cultural%20diversity%20as%20a%20novel%0AT2I%20evaluation%20component%2C%20leveraging%20quality-weighted%20Vendi%20score.%20Our%0Aevaluations%20reveal%20significant%20gaps%20in%20the%20cultural%20awareness%20of%20existing%0Amodels%20across%20countries%20and%20provide%20valuable%20insights%20into%20the%20cultural%0Adiversity%20of%20T2I%20outputs%20for%20under-specified%20prompts.%20Our%20methodology%20is%0Aextendable%20to%20other%20cultural%20regions%20and%20concepts%2C%20and%20can%20facilitate%20the%0Adevelopment%20of%20T2I%20models%20that%20better%20cater%20to%20the%20global%20population.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Aesthetics%253A%2520Cultural%2520Competence%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DNithish%2520Kannen%2520and%2520Arif%2520Ahmad%2520and%2520Marco%2520Andreetto%2520and%2520Vinodkumar%2520Prabhakaran%2520and%2520Utsav%2520Prabhu%2520and%2520Adji%2520Bousso%2520Dieng%2520and%2520Pushpak%2520Bhattacharyya%2520and%2520Shachi%2520Dave%26entry.1292438233%3D%2520%2520Text-to-Image%2520%2528T2I%2529%2520models%2520are%2520being%2520increasingly%2520adopted%2520in%2520diverse%2520global%250Acommunities%2520where%2520they%2520create%2520visual%2520representations%2520of%2520their%2520unique%2520cultures.%250ACurrent%2520T2I%2520benchmarks%2520primarily%2520focus%2520on%2520faithfulness%252C%2520aesthetics%252C%2520and%2520realism%250Aof%2520generated%2520images%252C%2520overlooking%2520the%2520critical%2520dimension%2520of%2520cultural%2520competence.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520framework%2520to%2520evaluate%2520cultural%2520competence%2520of%2520T2I%250Amodels%2520along%2520two%2520crucial%2520dimensions%253A%2520cultural%2520awareness%2520and%2520cultural%2520diversity%252C%250Aand%2520present%2520a%2520scalable%2520approach%2520using%2520a%2520combination%2520of%2520structured%2520knowledge%250Abases%2520and%2520large%2520language%2520models%2520to%2520build%2520a%2520large%2520dataset%2520of%2520cultural%2520artifacts%250Ato%2520enable%2520this%2520evaluation.%2520In%2520particular%252C%2520we%2520apply%2520this%2520approach%2520to%2520build%2520CUBE%250A%2528CUltural%2520BEnchmark%2520for%2520Text-to-Image%2520models%2529%252C%2520a%2520first-of-its-kind%2520benchmark%2520to%250Aevaluate%2520cultural%2520competence%2520of%2520T2I%2520models.%2520CUBE%2520covers%2520cultural%2520artifacts%250Aassociated%2520with%25208%2520countries%2520across%2520different%2520geo-cultural%2520regions%2520and%2520along%25203%250Aconcepts%253A%2520cuisine%252C%2520landmarks%252C%2520and%2520art.%2520CUBE%2520consists%2520of%25201%2529%2520CUBE-1K%252C%2520a%2520set%2520of%250Ahigh-quality%2520prompts%2520that%2520enable%2520the%2520evaluation%2520of%2520cultural%2520awareness%252C%2520and%25202%2529%250ACUBE-CSpace%252C%2520a%2520larger%2520dataset%2520of%2520cultural%2520artifacts%2520that%2520serves%2520as%2520grounding%2520to%250Aevaluate%2520cultural%2520diversity.%2520We%2520also%2520introduce%2520cultural%2520diversity%2520as%2520a%2520novel%250AT2I%2520evaluation%2520component%252C%2520leveraging%2520quality-weighted%2520Vendi%2520score.%2520Our%250Aevaluations%2520reveal%2520significant%2520gaps%2520in%2520the%2520cultural%2520awareness%2520of%2520existing%250Amodels%2520across%2520countries%2520and%2520provide%2520valuable%2520insights%2520into%2520the%2520cultural%250Adiversity%2520of%2520T2I%2520outputs%2520for%2520under-specified%2520prompts.%2520Our%2520methodology%2520is%250Aextendable%2520to%2520other%2520cultural%2520regions%2520and%2520concepts%252C%2520and%2520can%2520facilitate%2520the%250Adevelopment%2520of%2520T2I%2520models%2520that%2520better%2520cater%2520to%2520the%2520global%2520population.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Aesthetics%3A%20Cultural%20Competence%20in%20Text-to-Image%20Models&entry.906535625=Nithish%20Kannen%20and%20Arif%20Ahmad%20and%20Marco%20Andreetto%20and%20Vinodkumar%20Prabhakaran%20and%20Utsav%20Prabhu%20and%20Adji%20Bousso%20Dieng%20and%20Pushpak%20Bhattacharyya%20and%20Shachi%20Dave&entry.1292438233=%20%20Text-to-Image%20%28T2I%29%20models%20are%20being%20increasingly%20adopted%20in%20diverse%20global%0Acommunities%20where%20they%20create%20visual%20representations%20of%20their%20unique%20cultures.%0ACurrent%20T2I%20benchmarks%20primarily%20focus%20on%20faithfulness%2C%20aesthetics%2C%20and%20realism%0Aof%20generated%20images%2C%20overlooking%20the%20critical%20dimension%20of%20cultural%20competence.%0AIn%20this%20work%2C%20we%20introduce%20a%20framework%20to%20evaluate%20cultural%20competence%20of%20T2I%0Amodels%20along%20two%20crucial%20dimensions%3A%20cultural%20awareness%20and%20cultural%20diversity%2C%0Aand%20present%20a%20scalable%20approach%20using%20a%20combination%20of%20structured%20knowledge%0Abases%20and%20large%20language%20models%20to%20build%20a%20large%20dataset%20of%20cultural%20artifacts%0Ato%20enable%20this%20evaluation.%20In%20particular%2C%20we%20apply%20this%20approach%20to%20build%20CUBE%0A%28CUltural%20BEnchmark%20for%20Text-to-Image%20models%29%2C%20a%20first-of-its-kind%20benchmark%20to%0Aevaluate%20cultural%20competence%20of%20T2I%20models.%20CUBE%20covers%20cultural%20artifacts%0Aassociated%20with%208%20countries%20across%20different%20geo-cultural%20regions%20and%20along%203%0Aconcepts%3A%20cuisine%2C%20landmarks%2C%20and%20art.%20CUBE%20consists%20of%201%29%20CUBE-1K%2C%20a%20set%20of%0Ahigh-quality%20prompts%20that%20enable%20the%20evaluation%20of%20cultural%20awareness%2C%20and%202%29%0ACUBE-CSpace%2C%20a%20larger%20dataset%20of%20cultural%20artifacts%20that%20serves%20as%20grounding%20to%0Aevaluate%20cultural%20diversity.%20We%20also%20introduce%20cultural%20diversity%20as%20a%20novel%0AT2I%20evaluation%20component%2C%20leveraging%20quality-weighted%20Vendi%20score.%20Our%0Aevaluations%20reveal%20significant%20gaps%20in%20the%20cultural%20awareness%20of%20existing%0Amodels%20across%20countries%20and%20provide%20valuable%20insights%20into%20the%20cultural%0Adiversity%20of%20T2I%20outputs%20for%20under-specified%20prompts.%20Our%20methodology%20is%0Aextendable%20to%20other%20cultural%20regions%20and%20concepts%2C%20and%20can%20facilitate%20the%0Adevelopment%20of%20T2I%20models%20that%20better%20cater%20to%20the%20global%20population.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06863v2&entry.124074799=Read"},
{"title": "Generalization Error Matters in Decentralized Learning Under Byzantine\n  Attacks", "author": "Haoxiang Ye and Qing Ling", "abstract": "  Recently, decentralized learning has emerged as a popular peer-to-peer signal\nand information processing paradigm that enables model training across\ngeographically distributed agents in a scalable manner, without the presence of\nany central server. When some of the agents are malicious (also termed as\nByzantine), resilient decentralized learning algorithms are able to limit the\nimpact of these Byzantine agents without knowing their number and identities,\nand have guaranteed optimization errors. However, analysis of the\ngeneralization errors, which are critical to implementations of the trained\nmodels, is still lacking. In this paper, we provide the first analysis of the\ngeneralization errors for a class of popular Byzantine-resilient decentralized\nstochastic gradient descent (DSGD) algorithms. Our theoretical results reveal\nthat the generalization errors cannot be entirely eliminated because of the\npresence of the Byzantine agents, even if the number of training samples are\ninfinitely large. Numerical experiments are conducted to confirm our\ntheoretical results.\n", "link": "http://arxiv.org/abs/2407.08632v1", "date": "2024-07-11", "relevancy": 2.1594, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.445}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.426}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Error%20Matters%20in%20Decentralized%20Learning%20Under%20Byzantine%0A%20%20Attacks&body=Title%3A%20Generalization%20Error%20Matters%20in%20Decentralized%20Learning%20Under%20Byzantine%0A%20%20Attacks%0AAuthor%3A%20Haoxiang%20Ye%20and%20Qing%20Ling%0AAbstract%3A%20%20%20Recently%2C%20decentralized%20learning%20has%20emerged%20as%20a%20popular%20peer-to-peer%20signal%0Aand%20information%20processing%20paradigm%20that%20enables%20model%20training%20across%0Ageographically%20distributed%20agents%20in%20a%20scalable%20manner%2C%20without%20the%20presence%20of%0Aany%20central%20server.%20When%20some%20of%20the%20agents%20are%20malicious%20%28also%20termed%20as%0AByzantine%29%2C%20resilient%20decentralized%20learning%20algorithms%20are%20able%20to%20limit%20the%0Aimpact%20of%20these%20Byzantine%20agents%20without%20knowing%20their%20number%20and%20identities%2C%0Aand%20have%20guaranteed%20optimization%20errors.%20However%2C%20analysis%20of%20the%0Ageneralization%20errors%2C%20which%20are%20critical%20to%20implementations%20of%20the%20trained%0Amodels%2C%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20provide%20the%20first%20analysis%20of%20the%0Ageneralization%20errors%20for%20a%20class%20of%20popular%20Byzantine-resilient%20decentralized%0Astochastic%20gradient%20descent%20%28DSGD%29%20algorithms.%20Our%20theoretical%20results%20reveal%0Athat%20the%20generalization%20errors%20cannot%20be%20entirely%20eliminated%20because%20of%20the%0Apresence%20of%20the%20Byzantine%20agents%2C%20even%20if%20the%20number%20of%20training%20samples%20are%0Ainfinitely%20large.%20Numerical%20experiments%20are%20conducted%20to%20confirm%20our%0Atheoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Error%2520Matters%2520in%2520Decentralized%2520Learning%2520Under%2520Byzantine%250A%2520%2520Attacks%26entry.906535625%3DHaoxiang%2520Ye%2520and%2520Qing%2520Ling%26entry.1292438233%3D%2520%2520Recently%252C%2520decentralized%2520learning%2520has%2520emerged%2520as%2520a%2520popular%2520peer-to-peer%2520signal%250Aand%2520information%2520processing%2520paradigm%2520that%2520enables%2520model%2520training%2520across%250Ageographically%2520distributed%2520agents%2520in%2520a%2520scalable%2520manner%252C%2520without%2520the%2520presence%2520of%250Aany%2520central%2520server.%2520When%2520some%2520of%2520the%2520agents%2520are%2520malicious%2520%2528also%2520termed%2520as%250AByzantine%2529%252C%2520resilient%2520decentralized%2520learning%2520algorithms%2520are%2520able%2520to%2520limit%2520the%250Aimpact%2520of%2520these%2520Byzantine%2520agents%2520without%2520knowing%2520their%2520number%2520and%2520identities%252C%250Aand%2520have%2520guaranteed%2520optimization%2520errors.%2520However%252C%2520analysis%2520of%2520the%250Ageneralization%2520errors%252C%2520which%2520are%2520critical%2520to%2520implementations%2520of%2520the%2520trained%250Amodels%252C%2520is%2520still%2520lacking.%2520In%2520this%2520paper%252C%2520we%2520provide%2520the%2520first%2520analysis%2520of%2520the%250Ageneralization%2520errors%2520for%2520a%2520class%2520of%2520popular%2520Byzantine-resilient%2520decentralized%250Astochastic%2520gradient%2520descent%2520%2528DSGD%2529%2520algorithms.%2520Our%2520theoretical%2520results%2520reveal%250Athat%2520the%2520generalization%2520errors%2520cannot%2520be%2520entirely%2520eliminated%2520because%2520of%2520the%250Apresence%2520of%2520the%2520Byzantine%2520agents%252C%2520even%2520if%2520the%2520number%2520of%2520training%2520samples%2520are%250Ainfinitely%2520large.%2520Numerical%2520experiments%2520are%2520conducted%2520to%2520confirm%2520our%250Atheoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Error%20Matters%20in%20Decentralized%20Learning%20Under%20Byzantine%0A%20%20Attacks&entry.906535625=Haoxiang%20Ye%20and%20Qing%20Ling&entry.1292438233=%20%20Recently%2C%20decentralized%20learning%20has%20emerged%20as%20a%20popular%20peer-to-peer%20signal%0Aand%20information%20processing%20paradigm%20that%20enables%20model%20training%20across%0Ageographically%20distributed%20agents%20in%20a%20scalable%20manner%2C%20without%20the%20presence%20of%0Aany%20central%20server.%20When%20some%20of%20the%20agents%20are%20malicious%20%28also%20termed%20as%0AByzantine%29%2C%20resilient%20decentralized%20learning%20algorithms%20are%20able%20to%20limit%20the%0Aimpact%20of%20these%20Byzantine%20agents%20without%20knowing%20their%20number%20and%20identities%2C%0Aand%20have%20guaranteed%20optimization%20errors.%20However%2C%20analysis%20of%20the%0Ageneralization%20errors%2C%20which%20are%20critical%20to%20implementations%20of%20the%20trained%0Amodels%2C%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20provide%20the%20first%20analysis%20of%20the%0Ageneralization%20errors%20for%20a%20class%20of%20popular%20Byzantine-resilient%20decentralized%0Astochastic%20gradient%20descent%20%28DSGD%29%20algorithms.%20Our%20theoretical%20results%20reveal%0Athat%20the%20generalization%20errors%20cannot%20be%20entirely%20eliminated%20because%20of%20the%0Apresence%20of%20the%20Byzantine%20agents%2C%20even%20if%20the%20number%20of%20training%20samples%20are%0Ainfinitely%20large.%20Numerical%20experiments%20are%20conducted%20to%20confirm%20our%0Atheoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08632v1&entry.124074799=Read"},
{"title": "AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position\n  and Scale", "author": "Adam Pardyl and Micha\u0142 Wronka and Maciej Wo\u0142czyk and Kamil Adamczewski and Tomasz Trzci\u0144ski and Bartosz Zieli\u0144ski", "abstract": "  Active Visual Exploration (AVE) is a task that involves dynamically selecting\nobservations (glimpses), which is critical to facilitate comprehension and\nnavigation within an environment. While modern AVE methods have demonstrated\nimpressive performance, they are constrained to fixed-scale glimpses from rigid\ngrids. In contrast, existing mobile platforms equipped with optical zoom\ncapabilities can capture glimpses of arbitrary positions and scales. To address\nthis gap between software and hardware capabilities, we introduce AdaGlimpse.\nIt uses Soft Actor-Critic, a reinforcement learning algorithm tailored for\nexploration tasks, to select glimpses of arbitrary position and scale. This\napproach enables our model to rapidly establish a general awareness of the\nenvironment before zooming in for detailed analysis. Experimental results\ndemonstrate that AdaGlimpse surpasses previous methods across various visual\ntasks while maintaining greater applicability in realistic AVE scenarios.\n", "link": "http://arxiv.org/abs/2404.03482v2", "date": "2024-07-11", "relevancy": 2.1561, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5339}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaGlimpse%3A%20Active%20Visual%20Exploration%20with%20Arbitrary%20Glimpse%20Position%0A%20%20and%20Scale&body=Title%3A%20AdaGlimpse%3A%20Active%20Visual%20Exploration%20with%20Arbitrary%20Glimpse%20Position%0A%20%20and%20Scale%0AAuthor%3A%20Adam%20Pardyl%20and%20Micha%C5%82%20Wronka%20and%20Maciej%20Wo%C5%82czyk%20and%20Kamil%20Adamczewski%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bartosz%20Zieli%C5%84ski%0AAbstract%3A%20%20%20Active%20Visual%20Exploration%20%28AVE%29%20is%20a%20task%20that%20involves%20dynamically%20selecting%0Aobservations%20%28glimpses%29%2C%20which%20is%20critical%20to%20facilitate%20comprehension%20and%0Anavigation%20within%20an%20environment.%20While%20modern%20AVE%20methods%20have%20demonstrated%0Aimpressive%20performance%2C%20they%20are%20constrained%20to%20fixed-scale%20glimpses%20from%20rigid%0Agrids.%20In%20contrast%2C%20existing%20mobile%20platforms%20equipped%20with%20optical%20zoom%0Acapabilities%20can%20capture%20glimpses%20of%20arbitrary%20positions%20and%20scales.%20To%20address%0Athis%20gap%20between%20software%20and%20hardware%20capabilities%2C%20we%20introduce%20AdaGlimpse.%0AIt%20uses%20Soft%20Actor-Critic%2C%20a%20reinforcement%20learning%20algorithm%20tailored%20for%0Aexploration%20tasks%2C%20to%20select%20glimpses%20of%20arbitrary%20position%20and%20scale.%20This%0Aapproach%20enables%20our%20model%20to%20rapidly%20establish%20a%20general%20awareness%20of%20the%0Aenvironment%20before%20zooming%20in%20for%20detailed%20analysis.%20Experimental%20results%0Ademonstrate%20that%20AdaGlimpse%20surpasses%20previous%20methods%20across%20various%20visual%0Atasks%20while%20maintaining%20greater%20applicability%20in%20realistic%20AVE%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaGlimpse%253A%2520Active%2520Visual%2520Exploration%2520with%2520Arbitrary%2520Glimpse%2520Position%250A%2520%2520and%2520Scale%26entry.906535625%3DAdam%2520Pardyl%2520and%2520Micha%25C5%2582%2520Wronka%2520and%2520Maciej%2520Wo%25C5%2582czyk%2520and%2520Kamil%2520Adamczewski%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Bartosz%2520Zieli%25C5%2584ski%26entry.1292438233%3D%2520%2520Active%2520Visual%2520Exploration%2520%2528AVE%2529%2520is%2520a%2520task%2520that%2520involves%2520dynamically%2520selecting%250Aobservations%2520%2528glimpses%2529%252C%2520which%2520is%2520critical%2520to%2520facilitate%2520comprehension%2520and%250Anavigation%2520within%2520an%2520environment.%2520While%2520modern%2520AVE%2520methods%2520have%2520demonstrated%250Aimpressive%2520performance%252C%2520they%2520are%2520constrained%2520to%2520fixed-scale%2520glimpses%2520from%2520rigid%250Agrids.%2520In%2520contrast%252C%2520existing%2520mobile%2520platforms%2520equipped%2520with%2520optical%2520zoom%250Acapabilities%2520can%2520capture%2520glimpses%2520of%2520arbitrary%2520positions%2520and%2520scales.%2520To%2520address%250Athis%2520gap%2520between%2520software%2520and%2520hardware%2520capabilities%252C%2520we%2520introduce%2520AdaGlimpse.%250AIt%2520uses%2520Soft%2520Actor-Critic%252C%2520a%2520reinforcement%2520learning%2520algorithm%2520tailored%2520for%250Aexploration%2520tasks%252C%2520to%2520select%2520glimpses%2520of%2520arbitrary%2520position%2520and%2520scale.%2520This%250Aapproach%2520enables%2520our%2520model%2520to%2520rapidly%2520establish%2520a%2520general%2520awareness%2520of%2520the%250Aenvironment%2520before%2520zooming%2520in%2520for%2520detailed%2520analysis.%2520Experimental%2520results%250Ademonstrate%2520that%2520AdaGlimpse%2520surpasses%2520previous%2520methods%2520across%2520various%2520visual%250Atasks%2520while%2520maintaining%2520greater%2520applicability%2520in%2520realistic%2520AVE%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaGlimpse%3A%20Active%20Visual%20Exploration%20with%20Arbitrary%20Glimpse%20Position%0A%20%20and%20Scale&entry.906535625=Adam%20Pardyl%20and%20Micha%C5%82%20Wronka%20and%20Maciej%20Wo%C5%82czyk%20and%20Kamil%20Adamczewski%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bartosz%20Zieli%C5%84ski&entry.1292438233=%20%20Active%20Visual%20Exploration%20%28AVE%29%20is%20a%20task%20that%20involves%20dynamically%20selecting%0Aobservations%20%28glimpses%29%2C%20which%20is%20critical%20to%20facilitate%20comprehension%20and%0Anavigation%20within%20an%20environment.%20While%20modern%20AVE%20methods%20have%20demonstrated%0Aimpressive%20performance%2C%20they%20are%20constrained%20to%20fixed-scale%20glimpses%20from%20rigid%0Agrids.%20In%20contrast%2C%20existing%20mobile%20platforms%20equipped%20with%20optical%20zoom%0Acapabilities%20can%20capture%20glimpses%20of%20arbitrary%20positions%20and%20scales.%20To%20address%0Athis%20gap%20between%20software%20and%20hardware%20capabilities%2C%20we%20introduce%20AdaGlimpse.%0AIt%20uses%20Soft%20Actor-Critic%2C%20a%20reinforcement%20learning%20algorithm%20tailored%20for%0Aexploration%20tasks%2C%20to%20select%20glimpses%20of%20arbitrary%20position%20and%20scale.%20This%0Aapproach%20enables%20our%20model%20to%20rapidly%20establish%20a%20general%20awareness%20of%20the%0Aenvironment%20before%20zooming%20in%20for%20detailed%20analysis.%20Experimental%20results%0Ademonstrate%20that%20AdaGlimpse%20surpasses%20previous%20methods%20across%20various%20visual%0Atasks%20while%20maintaining%20greater%20applicability%20in%20realistic%20AVE%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03482v2&entry.124074799=Read"},
{"title": "On Onboard LiDAR-based Flying Object Detection", "author": "Matou\u0161 Vrba and Viktor Walter and V\u00e1clav Pritzl and Michal Pliska and Tom\u00e1\u0161 B\u00e1\u010da and Vojt\u011bch Spurn\u00fd and Daniel He\u0159t and Martin Saska", "abstract": "  A new robust and accurate approach for the detection and localization of\nflying objects with the purpose of highly dynamic aerial interception and agile\nmulti-robot interaction is presented in this paper. The approach is proposed\nfor use onboard an autonomous aerial vehicle equipped with a 3D LiDAR sensor\nproviding input data for the algorithm. It relies on a novel 3D occupancy voxel\nmapping method for the target detection and a cluster-based multiple hypothesis\ntracker to compensate uncertainty of the sensory data. When compared to\nstate-of-the-art methods of onboard detection of other flying objects, the\npresented approach provides superior localization accuracy and robustness to\ndifferent environments and appearance changes of the target, as well as a\ngreater detection range. Furthermore, in combination with the proposed\nmulti-target tracker, sporadic false positives are suppressed, state estimation\nof the target is provided and the detection latency is negligible. This makes\nthe detector suitable for tasks of agile multi-robot interaction, such as\nautonomous aerial interception or formation control where precise, robust, and\nfast relative localization of other robots is crucial. We demonstrate the\npractical usability and performance of the system in simulated and real-world\nexperiments.\n", "link": "http://arxiv.org/abs/2303.05404v3", "date": "2024-07-11", "relevancy": 2.1557, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5562}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Onboard%20LiDAR-based%20Flying%20Object%20Detection&body=Title%3A%20On%20Onboard%20LiDAR-based%20Flying%20Object%20Detection%0AAuthor%3A%20Matou%C5%A1%20Vrba%20and%20Viktor%20Walter%20and%20V%C3%A1clav%20Pritzl%20and%20Michal%20Pliska%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Vojt%C4%9Bch%20Spurn%C3%BD%20and%20Daniel%20He%C5%99t%20and%20Martin%20Saska%0AAbstract%3A%20%20%20A%20new%20robust%20and%20accurate%20approach%20for%20the%20detection%20and%20localization%20of%0Aflying%20objects%20with%20the%20purpose%20of%20highly%20dynamic%20aerial%20interception%20and%20agile%0Amulti-robot%20interaction%20is%20presented%20in%20this%20paper.%20The%20approach%20is%20proposed%0Afor%20use%20onboard%20an%20autonomous%20aerial%20vehicle%20equipped%20with%20a%203D%20LiDAR%20sensor%0Aproviding%20input%20data%20for%20the%20algorithm.%20It%20relies%20on%20a%20novel%203D%20occupancy%20voxel%0Amapping%20method%20for%20the%20target%20detection%20and%20a%20cluster-based%20multiple%20hypothesis%0Atracker%20to%20compensate%20uncertainty%20of%20the%20sensory%20data.%20When%20compared%20to%0Astate-of-the-art%20methods%20of%20onboard%20detection%20of%20other%20flying%20objects%2C%20the%0Apresented%20approach%20provides%20superior%20localization%20accuracy%20and%20robustness%20to%0Adifferent%20environments%20and%20appearance%20changes%20of%20the%20target%2C%20as%20well%20as%20a%0Agreater%20detection%20range.%20Furthermore%2C%20in%20combination%20with%20the%20proposed%0Amulti-target%20tracker%2C%20sporadic%20false%20positives%20are%20suppressed%2C%20state%20estimation%0Aof%20the%20target%20is%20provided%20and%20the%20detection%20latency%20is%20negligible.%20This%20makes%0Athe%20detector%20suitable%20for%20tasks%20of%20agile%20multi-robot%20interaction%2C%20such%20as%0Aautonomous%20aerial%20interception%20or%20formation%20control%20where%20precise%2C%20robust%2C%20and%0Afast%20relative%20localization%20of%20other%20robots%20is%20crucial.%20We%20demonstrate%20the%0Apractical%20usability%20and%20performance%20of%20the%20system%20in%20simulated%20and%20real-world%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.05404v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Onboard%2520LiDAR-based%2520Flying%2520Object%2520Detection%26entry.906535625%3DMatou%25C5%25A1%2520Vrba%2520and%2520Viktor%2520Walter%2520and%2520V%25C3%25A1clav%2520Pritzl%2520and%2520Michal%2520Pliska%2520and%2520Tom%25C3%25A1%25C5%25A1%2520B%25C3%25A1%25C4%258Da%2520and%2520Vojt%25C4%259Bch%2520Spurn%25C3%25BD%2520and%2520Daniel%2520He%25C5%2599t%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520A%2520new%2520robust%2520and%2520accurate%2520approach%2520for%2520the%2520detection%2520and%2520localization%2520of%250Aflying%2520objects%2520with%2520the%2520purpose%2520of%2520highly%2520dynamic%2520aerial%2520interception%2520and%2520agile%250Amulti-robot%2520interaction%2520is%2520presented%2520in%2520this%2520paper.%2520The%2520approach%2520is%2520proposed%250Afor%2520use%2520onboard%2520an%2520autonomous%2520aerial%2520vehicle%2520equipped%2520with%2520a%25203D%2520LiDAR%2520sensor%250Aproviding%2520input%2520data%2520for%2520the%2520algorithm.%2520It%2520relies%2520on%2520a%2520novel%25203D%2520occupancy%2520voxel%250Amapping%2520method%2520for%2520the%2520target%2520detection%2520and%2520a%2520cluster-based%2520multiple%2520hypothesis%250Atracker%2520to%2520compensate%2520uncertainty%2520of%2520the%2520sensory%2520data.%2520When%2520compared%2520to%250Astate-of-the-art%2520methods%2520of%2520onboard%2520detection%2520of%2520other%2520flying%2520objects%252C%2520the%250Apresented%2520approach%2520provides%2520superior%2520localization%2520accuracy%2520and%2520robustness%2520to%250Adifferent%2520environments%2520and%2520appearance%2520changes%2520of%2520the%2520target%252C%2520as%2520well%2520as%2520a%250Agreater%2520detection%2520range.%2520Furthermore%252C%2520in%2520combination%2520with%2520the%2520proposed%250Amulti-target%2520tracker%252C%2520sporadic%2520false%2520positives%2520are%2520suppressed%252C%2520state%2520estimation%250Aof%2520the%2520target%2520is%2520provided%2520and%2520the%2520detection%2520latency%2520is%2520negligible.%2520This%2520makes%250Athe%2520detector%2520suitable%2520for%2520tasks%2520of%2520agile%2520multi-robot%2520interaction%252C%2520such%2520as%250Aautonomous%2520aerial%2520interception%2520or%2520formation%2520control%2520where%2520precise%252C%2520robust%252C%2520and%250Afast%2520relative%2520localization%2520of%2520other%2520robots%2520is%2520crucial.%2520We%2520demonstrate%2520the%250Apractical%2520usability%2520and%2520performance%2520of%2520the%2520system%2520in%2520simulated%2520and%2520real-world%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.05404v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Onboard%20LiDAR-based%20Flying%20Object%20Detection&entry.906535625=Matou%C5%A1%20Vrba%20and%20Viktor%20Walter%20and%20V%C3%A1clav%20Pritzl%20and%20Michal%20Pliska%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Vojt%C4%9Bch%20Spurn%C3%BD%20and%20Daniel%20He%C5%99t%20and%20Martin%20Saska&entry.1292438233=%20%20A%20new%20robust%20and%20accurate%20approach%20for%20the%20detection%20and%20localization%20of%0Aflying%20objects%20with%20the%20purpose%20of%20highly%20dynamic%20aerial%20interception%20and%20agile%0Amulti-robot%20interaction%20is%20presented%20in%20this%20paper.%20The%20approach%20is%20proposed%0Afor%20use%20onboard%20an%20autonomous%20aerial%20vehicle%20equipped%20with%20a%203D%20LiDAR%20sensor%0Aproviding%20input%20data%20for%20the%20algorithm.%20It%20relies%20on%20a%20novel%203D%20occupancy%20voxel%0Amapping%20method%20for%20the%20target%20detection%20and%20a%20cluster-based%20multiple%20hypothesis%0Atracker%20to%20compensate%20uncertainty%20of%20the%20sensory%20data.%20When%20compared%20to%0Astate-of-the-art%20methods%20of%20onboard%20detection%20of%20other%20flying%20objects%2C%20the%0Apresented%20approach%20provides%20superior%20localization%20accuracy%20and%20robustness%20to%0Adifferent%20environments%20and%20appearance%20changes%20of%20the%20target%2C%20as%20well%20as%20a%0Agreater%20detection%20range.%20Furthermore%2C%20in%20combination%20with%20the%20proposed%0Amulti-target%20tracker%2C%20sporadic%20false%20positives%20are%20suppressed%2C%20state%20estimation%0Aof%20the%20target%20is%20provided%20and%20the%20detection%20latency%20is%20negligible.%20This%20makes%0Athe%20detector%20suitable%20for%20tasks%20of%20agile%20multi-robot%20interaction%2C%20such%20as%0Aautonomous%20aerial%20interception%20or%20formation%20control%20where%20precise%2C%20robust%2C%20and%0Afast%20relative%20localization%20of%20other%20robots%20is%20crucial.%20We%20demonstrate%20the%0Apractical%20usability%20and%20performance%20of%20the%20system%20in%20simulated%20and%20real-world%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.05404v3&entry.124074799=Read"},
{"title": "Bootstrapping Vision-language Models for Self-supervised Remote\n  Physiological Measurement", "author": "Zijie Yue and Miaojing Shi and Hanli Wang and Shuai Ding and Qijun Chen and Shanlin Yang", "abstract": "  Facial video-based remote physiological measurement is a promising research\narea for detecting human vital signs (e.g., heart rate, respiration frequency)\nin a non-contact way. Conventional approaches are mostly supervised learning,\nrequiring extensive collections of facial videos and synchronously recorded\nphotoplethysmography (PPG) signals. To tackle it, self-supervised learning has\nrecently gained attentions; due to the lack of ground truth PPG signals, its\nperformance is however limited. In this paper, we propose a novel\nself-supervised framework that successfully integrates the popular\nvision-language models (VLMs) into the remote physiological measurement task.\nGiven a facial video, we first augment its positive and negative video samples\nwith varying rPPG signal frequencies. Next, we introduce a frequency-oriented\nvision-text pair generation method by carefully creating contrastive\nspatio-temporal maps from positive and negative samples and designing proper\ntext prompts to describe their relative ratios of signal frequencies. A\npre-trained VLM is employed to extract features for these formed vision-text\npairs and estimate rPPG signals thereafter. We develop a series of generative\nand contrastive learning mechanisms to optimize the VLM, including the\ntext-guided visual map reconstruction task, the vision-text contrastive\nlearning task, and the frequency contrastive and ranking task. Overall, our\nmethod for the first time adapts VLMs to digest and align the frequency-related\nknowledge in vision and text modalities. Extensive experiments on four\nbenchmark datasets demonstrate that it significantly outperforms state of the\nart self-supervised methods.\n", "link": "http://arxiv.org/abs/2407.08507v1", "date": "2024-07-11", "relevancy": 2.155, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5442}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5355}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20Vision-language%20Models%20for%20Self-supervised%20Remote%0A%20%20Physiological%20Measurement&body=Title%3A%20Bootstrapping%20Vision-language%20Models%20for%20Self-supervised%20Remote%0A%20%20Physiological%20Measurement%0AAuthor%3A%20Zijie%20Yue%20and%20Miaojing%20Shi%20and%20Hanli%20Wang%20and%20Shuai%20Ding%20and%20Qijun%20Chen%20and%20Shanlin%20Yang%0AAbstract%3A%20%20%20Facial%20video-based%20remote%20physiological%20measurement%20is%20a%20promising%20research%0Aarea%20for%20detecting%20human%20vital%20signs%20%28e.g.%2C%20heart%20rate%2C%20respiration%20frequency%29%0Ain%20a%20non-contact%20way.%20Conventional%20approaches%20are%20mostly%20supervised%20learning%2C%0Arequiring%20extensive%20collections%20of%20facial%20videos%20and%20synchronously%20recorded%0Aphotoplethysmography%20%28PPG%29%20signals.%20To%20tackle%20it%2C%20self-supervised%20learning%20has%0Arecently%20gained%20attentions%3B%20due%20to%20the%20lack%20of%20ground%20truth%20PPG%20signals%2C%20its%0Aperformance%20is%20however%20limited.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aself-supervised%20framework%20that%20successfully%20integrates%20the%20popular%0Avision-language%20models%20%28VLMs%29%20into%20the%20remote%20physiological%20measurement%20task.%0AGiven%20a%20facial%20video%2C%20we%20first%20augment%20its%20positive%20and%20negative%20video%20samples%0Awith%20varying%20rPPG%20signal%20frequencies.%20Next%2C%20we%20introduce%20a%20frequency-oriented%0Avision-text%20pair%20generation%20method%20by%20carefully%20creating%20contrastive%0Aspatio-temporal%20maps%20from%20positive%20and%20negative%20samples%20and%20designing%20proper%0Atext%20prompts%20to%20describe%20their%20relative%20ratios%20of%20signal%20frequencies.%20A%0Apre-trained%20VLM%20is%20employed%20to%20extract%20features%20for%20these%20formed%20vision-text%0Apairs%20and%20estimate%20rPPG%20signals%20thereafter.%20We%20develop%20a%20series%20of%20generative%0Aand%20contrastive%20learning%20mechanisms%20to%20optimize%20the%20VLM%2C%20including%20the%0Atext-guided%20visual%20map%20reconstruction%20task%2C%20the%20vision-text%20contrastive%0Alearning%20task%2C%20and%20the%20frequency%20contrastive%20and%20ranking%20task.%20Overall%2C%20our%0Amethod%20for%20the%20first%20time%20adapts%20VLMs%20to%20digest%20and%20align%20the%20frequency-related%0Aknowledge%20in%20vision%20and%20text%20modalities.%20Extensive%20experiments%20on%20four%0Abenchmark%20datasets%20demonstrate%20that%20it%20significantly%20outperforms%20state%20of%20the%0Aart%20self-supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520Vision-language%2520Models%2520for%2520Self-supervised%2520Remote%250A%2520%2520Physiological%2520Measurement%26entry.906535625%3DZijie%2520Yue%2520and%2520Miaojing%2520Shi%2520and%2520Hanli%2520Wang%2520and%2520Shuai%2520Ding%2520and%2520Qijun%2520Chen%2520and%2520Shanlin%2520Yang%26entry.1292438233%3D%2520%2520Facial%2520video-based%2520remote%2520physiological%2520measurement%2520is%2520a%2520promising%2520research%250Aarea%2520for%2520detecting%2520human%2520vital%2520signs%2520%2528e.g.%252C%2520heart%2520rate%252C%2520respiration%2520frequency%2529%250Ain%2520a%2520non-contact%2520way.%2520Conventional%2520approaches%2520are%2520mostly%2520supervised%2520learning%252C%250Arequiring%2520extensive%2520collections%2520of%2520facial%2520videos%2520and%2520synchronously%2520recorded%250Aphotoplethysmography%2520%2528PPG%2529%2520signals.%2520To%2520tackle%2520it%252C%2520self-supervised%2520learning%2520has%250Arecently%2520gained%2520attentions%253B%2520due%2520to%2520the%2520lack%2520of%2520ground%2520truth%2520PPG%2520signals%252C%2520its%250Aperformance%2520is%2520however%2520limited.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aself-supervised%2520framework%2520that%2520successfully%2520integrates%2520the%2520popular%250Avision-language%2520models%2520%2528VLMs%2529%2520into%2520the%2520remote%2520physiological%2520measurement%2520task.%250AGiven%2520a%2520facial%2520video%252C%2520we%2520first%2520augment%2520its%2520positive%2520and%2520negative%2520video%2520samples%250Awith%2520varying%2520rPPG%2520signal%2520frequencies.%2520Next%252C%2520we%2520introduce%2520a%2520frequency-oriented%250Avision-text%2520pair%2520generation%2520method%2520by%2520carefully%2520creating%2520contrastive%250Aspatio-temporal%2520maps%2520from%2520positive%2520and%2520negative%2520samples%2520and%2520designing%2520proper%250Atext%2520prompts%2520to%2520describe%2520their%2520relative%2520ratios%2520of%2520signal%2520frequencies.%2520A%250Apre-trained%2520VLM%2520is%2520employed%2520to%2520extract%2520features%2520for%2520these%2520formed%2520vision-text%250Apairs%2520and%2520estimate%2520rPPG%2520signals%2520thereafter.%2520We%2520develop%2520a%2520series%2520of%2520generative%250Aand%2520contrastive%2520learning%2520mechanisms%2520to%2520optimize%2520the%2520VLM%252C%2520including%2520the%250Atext-guided%2520visual%2520map%2520reconstruction%2520task%252C%2520the%2520vision-text%2520contrastive%250Alearning%2520task%252C%2520and%2520the%2520frequency%2520contrastive%2520and%2520ranking%2520task.%2520Overall%252C%2520our%250Amethod%2520for%2520the%2520first%2520time%2520adapts%2520VLMs%2520to%2520digest%2520and%2520align%2520the%2520frequency-related%250Aknowledge%2520in%2520vision%2520and%2520text%2520modalities.%2520Extensive%2520experiments%2520on%2520four%250Abenchmark%2520datasets%2520demonstrate%2520that%2520it%2520significantly%2520outperforms%2520state%2520of%2520the%250Aart%2520self-supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20Vision-language%20Models%20for%20Self-supervised%20Remote%0A%20%20Physiological%20Measurement&entry.906535625=Zijie%20Yue%20and%20Miaojing%20Shi%20and%20Hanli%20Wang%20and%20Shuai%20Ding%20and%20Qijun%20Chen%20and%20Shanlin%20Yang&entry.1292438233=%20%20Facial%20video-based%20remote%20physiological%20measurement%20is%20a%20promising%20research%0Aarea%20for%20detecting%20human%20vital%20signs%20%28e.g.%2C%20heart%20rate%2C%20respiration%20frequency%29%0Ain%20a%20non-contact%20way.%20Conventional%20approaches%20are%20mostly%20supervised%20learning%2C%0Arequiring%20extensive%20collections%20of%20facial%20videos%20and%20synchronously%20recorded%0Aphotoplethysmography%20%28PPG%29%20signals.%20To%20tackle%20it%2C%20self-supervised%20learning%20has%0Arecently%20gained%20attentions%3B%20due%20to%20the%20lack%20of%20ground%20truth%20PPG%20signals%2C%20its%0Aperformance%20is%20however%20limited.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aself-supervised%20framework%20that%20successfully%20integrates%20the%20popular%0Avision-language%20models%20%28VLMs%29%20into%20the%20remote%20physiological%20measurement%20task.%0AGiven%20a%20facial%20video%2C%20we%20first%20augment%20its%20positive%20and%20negative%20video%20samples%0Awith%20varying%20rPPG%20signal%20frequencies.%20Next%2C%20we%20introduce%20a%20frequency-oriented%0Avision-text%20pair%20generation%20method%20by%20carefully%20creating%20contrastive%0Aspatio-temporal%20maps%20from%20positive%20and%20negative%20samples%20and%20designing%20proper%0Atext%20prompts%20to%20describe%20their%20relative%20ratios%20of%20signal%20frequencies.%20A%0Apre-trained%20VLM%20is%20employed%20to%20extract%20features%20for%20these%20formed%20vision-text%0Apairs%20and%20estimate%20rPPG%20signals%20thereafter.%20We%20develop%20a%20series%20of%20generative%0Aand%20contrastive%20learning%20mechanisms%20to%20optimize%20the%20VLM%2C%20including%20the%0Atext-guided%20visual%20map%20reconstruction%20task%2C%20the%20vision-text%20contrastive%0Alearning%20task%2C%20and%20the%20frequency%20contrastive%20and%20ranking%20task.%20Overall%2C%20our%0Amethod%20for%20the%20first%20time%20adapts%20VLMs%20to%20digest%20and%20align%20the%20frequency-related%0Aknowledge%20in%20vision%20and%20text%20modalities.%20Extensive%20experiments%20on%20four%0Abenchmark%20datasets%20demonstrate%20that%20it%20significantly%20outperforms%20state%20of%20the%0Aart%20self-supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08507v1&entry.124074799=Read"},
{"title": "RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation", "author": "Tao Jiang and Xinchen Xie and Yining Li", "abstract": "  Whole-body pose estimation is a challenging task that requires simultaneous\nprediction of keypoints for the body, hands, face, and feet. Whole-body pose\nestimation aims to predict fine-grained pose information for the human body,\nincluding the face, torso, hands, and feet, which plays an important role in\nthe study of human-centric perception and generation and in various\napplications. In this work, we present RTMW (Real-Time Multi-person Whole-body\npose estimation models), a series of high-performance models for 2D/3D\nwhole-body pose estimation. We incorporate RTMPose model architecture with FPN\nand HEM (Hierarchical Encoding Module) to better capture pose information from\ndifferent body parts with various scales. The model is trained with a rich\ncollection of open-source human keypoint datasets with manually aligned\nannotations and further enhanced via a two-stage distillation strategy. RTMW\ndemonstrates strong performance on multiple whole-body pose estimation\nbenchmarks while maintaining high inference efficiency and deployment\nfriendliness. We release three sizes: m/l/x, with RTMW-l achieving a 70.2 mAP\non the COCO-Wholebody benchmark, making it the first open-source model to\nexceed 70 mAP on this benchmark. Meanwhile, we explored the performance of RTMW\nin the task of 3D whole-body pose estimation, conducting image-based monocular\n3D whole-body pose estimation in a coordinate classification manner. We hope\nthis work can benefit both academic research and industrial applications. The\ncode and models have been made publicly available at:\nhttps://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose\n", "link": "http://arxiv.org/abs/2407.08634v1", "date": "2024-07-11", "relevancy": 2.1473, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5505}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5368}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RTMW%3A%20Real-Time%20Multi-Person%202D%20and%203D%20Whole-body%20Pose%20Estimation&body=Title%3A%20RTMW%3A%20Real-Time%20Multi-Person%202D%20and%203D%20Whole-body%20Pose%20Estimation%0AAuthor%3A%20Tao%20Jiang%20and%20Xinchen%20Xie%20and%20Yining%20Li%0AAbstract%3A%20%20%20Whole-body%20pose%20estimation%20is%20a%20challenging%20task%20that%20requires%20simultaneous%0Aprediction%20of%20keypoints%20for%20the%20body%2C%20hands%2C%20face%2C%20and%20feet.%20Whole-body%20pose%0Aestimation%20aims%20to%20predict%20fine-grained%20pose%20information%20for%20the%20human%20body%2C%0Aincluding%20the%20face%2C%20torso%2C%20hands%2C%20and%20feet%2C%20which%20plays%20an%20important%20role%20in%0Athe%20study%20of%20human-centric%20perception%20and%20generation%20and%20in%20various%0Aapplications.%20In%20this%20work%2C%20we%20present%20RTMW%20%28Real-Time%20Multi-person%20Whole-body%0Apose%20estimation%20models%29%2C%20a%20series%20of%20high-performance%20models%20for%202D/3D%0Awhole-body%20pose%20estimation.%20We%20incorporate%20RTMPose%20model%20architecture%20with%20FPN%0Aand%20HEM%20%28Hierarchical%20Encoding%20Module%29%20to%20better%20capture%20pose%20information%20from%0Adifferent%20body%20parts%20with%20various%20scales.%20The%20model%20is%20trained%20with%20a%20rich%0Acollection%20of%20open-source%20human%20keypoint%20datasets%20with%20manually%20aligned%0Aannotations%20and%20further%20enhanced%20via%20a%20two-stage%20distillation%20strategy.%20RTMW%0Ademonstrates%20strong%20performance%20on%20multiple%20whole-body%20pose%20estimation%0Abenchmarks%20while%20maintaining%20high%20inference%20efficiency%20and%20deployment%0Afriendliness.%20We%20release%20three%20sizes%3A%20m/l/x%2C%20with%20RTMW-l%20achieving%20a%2070.2%20mAP%0Aon%20the%20COCO-Wholebody%20benchmark%2C%20making%20it%20the%20first%20open-source%20model%20to%0Aexceed%2070%20mAP%20on%20this%20benchmark.%20Meanwhile%2C%20we%20explored%20the%20performance%20of%20RTMW%0Ain%20the%20task%20of%203D%20whole-body%20pose%20estimation%2C%20conducting%20image-based%20monocular%0A3D%20whole-body%20pose%20estimation%20in%20a%20coordinate%20classification%20manner.%20We%20hope%0Athis%20work%20can%20benefit%20both%20academic%20research%20and%20industrial%20applications.%20The%0Acode%20and%20models%20have%20been%20made%20publicly%20available%20at%3A%0Ahttps%3A//github.com/open-mmlab/mmpose/tree/main/projects/rtmpose%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRTMW%253A%2520Real-Time%2520Multi-Person%25202D%2520and%25203D%2520Whole-body%2520Pose%2520Estimation%26entry.906535625%3DTao%2520Jiang%2520and%2520Xinchen%2520Xie%2520and%2520Yining%2520Li%26entry.1292438233%3D%2520%2520Whole-body%2520pose%2520estimation%2520is%2520a%2520challenging%2520task%2520that%2520requires%2520simultaneous%250Aprediction%2520of%2520keypoints%2520for%2520the%2520body%252C%2520hands%252C%2520face%252C%2520and%2520feet.%2520Whole-body%2520pose%250Aestimation%2520aims%2520to%2520predict%2520fine-grained%2520pose%2520information%2520for%2520the%2520human%2520body%252C%250Aincluding%2520the%2520face%252C%2520torso%252C%2520hands%252C%2520and%2520feet%252C%2520which%2520plays%2520an%2520important%2520role%2520in%250Athe%2520study%2520of%2520human-centric%2520perception%2520and%2520generation%2520and%2520in%2520various%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520present%2520RTMW%2520%2528Real-Time%2520Multi-person%2520Whole-body%250Apose%2520estimation%2520models%2529%252C%2520a%2520series%2520of%2520high-performance%2520models%2520for%25202D/3D%250Awhole-body%2520pose%2520estimation.%2520We%2520incorporate%2520RTMPose%2520model%2520architecture%2520with%2520FPN%250Aand%2520HEM%2520%2528Hierarchical%2520Encoding%2520Module%2529%2520to%2520better%2520capture%2520pose%2520information%2520from%250Adifferent%2520body%2520parts%2520with%2520various%2520scales.%2520The%2520model%2520is%2520trained%2520with%2520a%2520rich%250Acollection%2520of%2520open-source%2520human%2520keypoint%2520datasets%2520with%2520manually%2520aligned%250Aannotations%2520and%2520further%2520enhanced%2520via%2520a%2520two-stage%2520distillation%2520strategy.%2520RTMW%250Ademonstrates%2520strong%2520performance%2520on%2520multiple%2520whole-body%2520pose%2520estimation%250Abenchmarks%2520while%2520maintaining%2520high%2520inference%2520efficiency%2520and%2520deployment%250Afriendliness.%2520We%2520release%2520three%2520sizes%253A%2520m/l/x%252C%2520with%2520RTMW-l%2520achieving%2520a%252070.2%2520mAP%250Aon%2520the%2520COCO-Wholebody%2520benchmark%252C%2520making%2520it%2520the%2520first%2520open-source%2520model%2520to%250Aexceed%252070%2520mAP%2520on%2520this%2520benchmark.%2520Meanwhile%252C%2520we%2520explored%2520the%2520performance%2520of%2520RTMW%250Ain%2520the%2520task%2520of%25203D%2520whole-body%2520pose%2520estimation%252C%2520conducting%2520image-based%2520monocular%250A3D%2520whole-body%2520pose%2520estimation%2520in%2520a%2520coordinate%2520classification%2520manner.%2520We%2520hope%250Athis%2520work%2520can%2520benefit%2520both%2520academic%2520research%2520and%2520industrial%2520applications.%2520The%250Acode%2520and%2520models%2520have%2520been%2520made%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/open-mmlab/mmpose/tree/main/projects/rtmpose%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTMW%3A%20Real-Time%20Multi-Person%202D%20and%203D%20Whole-body%20Pose%20Estimation&entry.906535625=Tao%20Jiang%20and%20Xinchen%20Xie%20and%20Yining%20Li&entry.1292438233=%20%20Whole-body%20pose%20estimation%20is%20a%20challenging%20task%20that%20requires%20simultaneous%0Aprediction%20of%20keypoints%20for%20the%20body%2C%20hands%2C%20face%2C%20and%20feet.%20Whole-body%20pose%0Aestimation%20aims%20to%20predict%20fine-grained%20pose%20information%20for%20the%20human%20body%2C%0Aincluding%20the%20face%2C%20torso%2C%20hands%2C%20and%20feet%2C%20which%20plays%20an%20important%20role%20in%0Athe%20study%20of%20human-centric%20perception%20and%20generation%20and%20in%20various%0Aapplications.%20In%20this%20work%2C%20we%20present%20RTMW%20%28Real-Time%20Multi-person%20Whole-body%0Apose%20estimation%20models%29%2C%20a%20series%20of%20high-performance%20models%20for%202D/3D%0Awhole-body%20pose%20estimation.%20We%20incorporate%20RTMPose%20model%20architecture%20with%20FPN%0Aand%20HEM%20%28Hierarchical%20Encoding%20Module%29%20to%20better%20capture%20pose%20information%20from%0Adifferent%20body%20parts%20with%20various%20scales.%20The%20model%20is%20trained%20with%20a%20rich%0Acollection%20of%20open-source%20human%20keypoint%20datasets%20with%20manually%20aligned%0Aannotations%20and%20further%20enhanced%20via%20a%20two-stage%20distillation%20strategy.%20RTMW%0Ademonstrates%20strong%20performance%20on%20multiple%20whole-body%20pose%20estimation%0Abenchmarks%20while%20maintaining%20high%20inference%20efficiency%20and%20deployment%0Afriendliness.%20We%20release%20three%20sizes%3A%20m/l/x%2C%20with%20RTMW-l%20achieving%20a%2070.2%20mAP%0Aon%20the%20COCO-Wholebody%20benchmark%2C%20making%20it%20the%20first%20open-source%20model%20to%0Aexceed%2070%20mAP%20on%20this%20benchmark.%20Meanwhile%2C%20we%20explored%20the%20performance%20of%20RTMW%0Ain%20the%20task%20of%203D%20whole-body%20pose%20estimation%2C%20conducting%20image-based%20monocular%0A3D%20whole-body%20pose%20estimation%20in%20a%20coordinate%20classification%20manner.%20We%20hope%0Athis%20work%20can%20benefit%20both%20academic%20research%20and%20industrial%20applications.%20The%0Acode%20and%20models%20have%20been%20made%20publicly%20available%20at%3A%0Ahttps%3A//github.com/open-mmlab/mmpose/tree/main/projects/rtmpose%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08634v1&entry.124074799=Read"},
{"title": "Visible and Clear: Finding Tiny Objects in Difference Map", "author": "Bing Cao and Haiyu Yao and Pengfei Zhu and Qinghua Hu", "abstract": "  Tiny object detection is one of the key challenges in the field of object\ndetection. The performance of most generic detectors dramatically decreases in\ntiny object detection tasks. The main challenge lies in extracting effective\nfeatures of tiny objects. Existing methods usually perform generation-based\nfeature enhancement, which is seriously affected by spurious textures and\nartifacts, making it difficult to make the tiny-object-specific features\nvisible and clear for detection. To address this issue, we propose a\nself-reconstructed tiny object detection (SR-TOD) framework. We for the first\ntime introduce a self-reconstruction mechanism in the detection model, and\ndiscover the strong correlation between it and the tiny objects. Specifically,\nwe impose a reconstruction head in-between the neck of a detector, constructing\na difference map of the reconstructed image and the input, which shows high\nsensitivity to tiny objects. This inspires us to enhance the weak\nrepresentations of tiny objects under the guidance of the difference maps.\nThus, improving the visibility of tiny objects for the detectors. Building on\nthis, we further develop a Difference Map Guided Feature Enhancement (DGFE)\nmodule to make the tiny feature representation more clear. In addition, we\nfurther propose a new multi-instance anti-UAV dataset, which is called\nDroneSwarms dataset and contains a large number of tiny drones with the\nsmallest average size to date. Extensive experiments on the DroneSwarms dataset\nand other datasets demonstrate the effectiveness of the proposed method. The\ncode and dataset will be publicly available.\n", "link": "http://arxiv.org/abs/2405.11276v2", "date": "2024-07-11", "relevancy": 2.1375, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5359}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5356}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visible%20and%20Clear%3A%20Finding%20Tiny%20Objects%20in%20Difference%20Map&body=Title%3A%20Visible%20and%20Clear%3A%20Finding%20Tiny%20Objects%20in%20Difference%20Map%0AAuthor%3A%20Bing%20Cao%20and%20Haiyu%20Yao%20and%20Pengfei%20Zhu%20and%20Qinghua%20Hu%0AAbstract%3A%20%20%20Tiny%20object%20detection%20is%20one%20of%20the%20key%20challenges%20in%20the%20field%20of%20object%0Adetection.%20The%20performance%20of%20most%20generic%20detectors%20dramatically%20decreases%20in%0Atiny%20object%20detection%20tasks.%20The%20main%20challenge%20lies%20in%20extracting%20effective%0Afeatures%20of%20tiny%20objects.%20Existing%20methods%20usually%20perform%20generation-based%0Afeature%20enhancement%2C%20which%20is%20seriously%20affected%20by%20spurious%20textures%20and%0Aartifacts%2C%20making%20it%20difficult%20to%20make%20the%20tiny-object-specific%20features%0Avisible%20and%20clear%20for%20detection.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Aself-reconstructed%20tiny%20object%20detection%20%28SR-TOD%29%20framework.%20We%20for%20the%20first%0Atime%20introduce%20a%20self-reconstruction%20mechanism%20in%20the%20detection%20model%2C%20and%0Adiscover%20the%20strong%20correlation%20between%20it%20and%20the%20tiny%20objects.%20Specifically%2C%0Awe%20impose%20a%20reconstruction%20head%20in-between%20the%20neck%20of%20a%20detector%2C%20constructing%0Aa%20difference%20map%20of%20the%20reconstructed%20image%20and%20the%20input%2C%20which%20shows%20high%0Asensitivity%20to%20tiny%20objects.%20This%20inspires%20us%20to%20enhance%20the%20weak%0Arepresentations%20of%20tiny%20objects%20under%20the%20guidance%20of%20the%20difference%20maps.%0AThus%2C%20improving%20the%20visibility%20of%20tiny%20objects%20for%20the%20detectors.%20Building%20on%0Athis%2C%20we%20further%20develop%20a%20Difference%20Map%20Guided%20Feature%20Enhancement%20%28DGFE%29%0Amodule%20to%20make%20the%20tiny%20feature%20representation%20more%20clear.%20In%20addition%2C%20we%0Afurther%20propose%20a%20new%20multi-instance%20anti-UAV%20dataset%2C%20which%20is%20called%0ADroneSwarms%20dataset%20and%20contains%20a%20large%20number%20of%20tiny%20drones%20with%20the%0Asmallest%20average%20size%20to%20date.%20Extensive%20experiments%20on%20the%20DroneSwarms%20dataset%0Aand%20other%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%20The%0Acode%20and%20dataset%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisible%2520and%2520Clear%253A%2520Finding%2520Tiny%2520Objects%2520in%2520Difference%2520Map%26entry.906535625%3DBing%2520Cao%2520and%2520Haiyu%2520Yao%2520and%2520Pengfei%2520Zhu%2520and%2520Qinghua%2520Hu%26entry.1292438233%3D%2520%2520Tiny%2520object%2520detection%2520is%2520one%2520of%2520the%2520key%2520challenges%2520in%2520the%2520field%2520of%2520object%250Adetection.%2520The%2520performance%2520of%2520most%2520generic%2520detectors%2520dramatically%2520decreases%2520in%250Atiny%2520object%2520detection%2520tasks.%2520The%2520main%2520challenge%2520lies%2520in%2520extracting%2520effective%250Afeatures%2520of%2520tiny%2520objects.%2520Existing%2520methods%2520usually%2520perform%2520generation-based%250Afeature%2520enhancement%252C%2520which%2520is%2520seriously%2520affected%2520by%2520spurious%2520textures%2520and%250Aartifacts%252C%2520making%2520it%2520difficult%2520to%2520make%2520the%2520tiny-object-specific%2520features%250Avisible%2520and%2520clear%2520for%2520detection.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Aself-reconstructed%2520tiny%2520object%2520detection%2520%2528SR-TOD%2529%2520framework.%2520We%2520for%2520the%2520first%250Atime%2520introduce%2520a%2520self-reconstruction%2520mechanism%2520in%2520the%2520detection%2520model%252C%2520and%250Adiscover%2520the%2520strong%2520correlation%2520between%2520it%2520and%2520the%2520tiny%2520objects.%2520Specifically%252C%250Awe%2520impose%2520a%2520reconstruction%2520head%2520in-between%2520the%2520neck%2520of%2520a%2520detector%252C%2520constructing%250Aa%2520difference%2520map%2520of%2520the%2520reconstructed%2520image%2520and%2520the%2520input%252C%2520which%2520shows%2520high%250Asensitivity%2520to%2520tiny%2520objects.%2520This%2520inspires%2520us%2520to%2520enhance%2520the%2520weak%250Arepresentations%2520of%2520tiny%2520objects%2520under%2520the%2520guidance%2520of%2520the%2520difference%2520maps.%250AThus%252C%2520improving%2520the%2520visibility%2520of%2520tiny%2520objects%2520for%2520the%2520detectors.%2520Building%2520on%250Athis%252C%2520we%2520further%2520develop%2520a%2520Difference%2520Map%2520Guided%2520Feature%2520Enhancement%2520%2528DGFE%2529%250Amodule%2520to%2520make%2520the%2520tiny%2520feature%2520representation%2520more%2520clear.%2520In%2520addition%252C%2520we%250Afurther%2520propose%2520a%2520new%2520multi-instance%2520anti-UAV%2520dataset%252C%2520which%2520is%2520called%250ADroneSwarms%2520dataset%2520and%2520contains%2520a%2520large%2520number%2520of%2520tiny%2520drones%2520with%2520the%250Asmallest%2520average%2520size%2520to%2520date.%2520Extensive%2520experiments%2520on%2520the%2520DroneSwarms%2520dataset%250Aand%2520other%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%2520The%250Acode%2520and%2520dataset%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visible%20and%20Clear%3A%20Finding%20Tiny%20Objects%20in%20Difference%20Map&entry.906535625=Bing%20Cao%20and%20Haiyu%20Yao%20and%20Pengfei%20Zhu%20and%20Qinghua%20Hu&entry.1292438233=%20%20Tiny%20object%20detection%20is%20one%20of%20the%20key%20challenges%20in%20the%20field%20of%20object%0Adetection.%20The%20performance%20of%20most%20generic%20detectors%20dramatically%20decreases%20in%0Atiny%20object%20detection%20tasks.%20The%20main%20challenge%20lies%20in%20extracting%20effective%0Afeatures%20of%20tiny%20objects.%20Existing%20methods%20usually%20perform%20generation-based%0Afeature%20enhancement%2C%20which%20is%20seriously%20affected%20by%20spurious%20textures%20and%0Aartifacts%2C%20making%20it%20difficult%20to%20make%20the%20tiny-object-specific%20features%0Avisible%20and%20clear%20for%20detection.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Aself-reconstructed%20tiny%20object%20detection%20%28SR-TOD%29%20framework.%20We%20for%20the%20first%0Atime%20introduce%20a%20self-reconstruction%20mechanism%20in%20the%20detection%20model%2C%20and%0Adiscover%20the%20strong%20correlation%20between%20it%20and%20the%20tiny%20objects.%20Specifically%2C%0Awe%20impose%20a%20reconstruction%20head%20in-between%20the%20neck%20of%20a%20detector%2C%20constructing%0Aa%20difference%20map%20of%20the%20reconstructed%20image%20and%20the%20input%2C%20which%20shows%20high%0Asensitivity%20to%20tiny%20objects.%20This%20inspires%20us%20to%20enhance%20the%20weak%0Arepresentations%20of%20tiny%20objects%20under%20the%20guidance%20of%20the%20difference%20maps.%0AThus%2C%20improving%20the%20visibility%20of%20tiny%20objects%20for%20the%20detectors.%20Building%20on%0Athis%2C%20we%20further%20develop%20a%20Difference%20Map%20Guided%20Feature%20Enhancement%20%28DGFE%29%0Amodule%20to%20make%20the%20tiny%20feature%20representation%20more%20clear.%20In%20addition%2C%20we%0Afurther%20propose%20a%20new%20multi-instance%20anti-UAV%20dataset%2C%20which%20is%20called%0ADroneSwarms%20dataset%20and%20contains%20a%20large%20number%20of%20tiny%20drones%20with%20the%0Asmallest%20average%20size%20to%20date.%20Extensive%20experiments%20on%20the%20DroneSwarms%20dataset%0Aand%20other%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%20The%0Acode%20and%20dataset%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11276v2&entry.124074799=Read"},
{"title": "Natural language is not enough: Benchmarking multi-modal generative AI\n  for Verilog generation", "author": "Kaiyan Chang and Zhirong Chen and Yunhao Zhou and Wenlong Zhu and kun wang and Haobo Xu and Cangyuan Li and Mengdi Wang and Shengwen Liang and Huawei Li and Yinhe Han and Ying Wang", "abstract": "  Natural language interfaces have exhibited considerable potential in the\nautomation of Verilog generation derived from high-level specifications through\nthe utilization of large language models, garnering significant attention.\nNevertheless, this paper elucidates that visual representations contribute\nessential contextual information critical to design intent for hardware\narchitectures possessing spatial complexity, potentially surpassing the\nefficacy of natural-language-only inputs. Expanding upon this premise, our\npaper introduces an open-source benchmark for multi-modal generative models\ntailored for Verilog synthesis from visual-linguistic inputs, addressing both\nsingular and complex modules. Additionally, we introduce an open-source visual\nand natural language Verilog query language framework to facilitate efficient\nand user-friendly multi-modal queries. To evaluate the performance of the\nproposed multi-modal hardware generative AI in Verilog generation tasks, we\ncompare it with a popular method that relies solely on natural language. Our\nresults demonstrate a significant accuracy improvement in the multi-modal\ngenerated Verilog compared to queries based solely on natural language. We hope\nto reveal a new approach to hardware design in the large-hardware-design-model\nera, thereby fostering a more diversified and productive approach to hardware\ndesign.\n", "link": "http://arxiv.org/abs/2407.08473v1", "date": "2024-07-11", "relevancy": 2.1334, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5349}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20language%20is%20not%20enough%3A%20Benchmarking%20multi-modal%20generative%20AI%0A%20%20for%20Verilog%20generation&body=Title%3A%20Natural%20language%20is%20not%20enough%3A%20Benchmarking%20multi-modal%20generative%20AI%0A%20%20for%20Verilog%20generation%0AAuthor%3A%20Kaiyan%20Chang%20and%20Zhirong%20Chen%20and%20Yunhao%20Zhou%20and%20Wenlong%20Zhu%20and%20kun%20wang%20and%20Haobo%20Xu%20and%20Cangyuan%20Li%20and%20Mengdi%20Wang%20and%20Shengwen%20Liang%20and%20Huawei%20Li%20and%20Yinhe%20Han%20and%20Ying%20Wang%0AAbstract%3A%20%20%20Natural%20language%20interfaces%20have%20exhibited%20considerable%20potential%20in%20the%0Aautomation%20of%20Verilog%20generation%20derived%20from%20high-level%20specifications%20through%0Athe%20utilization%20of%20large%20language%20models%2C%20garnering%20significant%20attention.%0ANevertheless%2C%20this%20paper%20elucidates%20that%20visual%20representations%20contribute%0Aessential%20contextual%20information%20critical%20to%20design%20intent%20for%20hardware%0Aarchitectures%20possessing%20spatial%20complexity%2C%20potentially%20surpassing%20the%0Aefficacy%20of%20natural-language-only%20inputs.%20Expanding%20upon%20this%20premise%2C%20our%0Apaper%20introduces%20an%20open-source%20benchmark%20for%20multi-modal%20generative%20models%0Atailored%20for%20Verilog%20synthesis%20from%20visual-linguistic%20inputs%2C%20addressing%20both%0Asingular%20and%20complex%20modules.%20Additionally%2C%20we%20introduce%20an%20open-source%20visual%0Aand%20natural%20language%20Verilog%20query%20language%20framework%20to%20facilitate%20efficient%0Aand%20user-friendly%20multi-modal%20queries.%20To%20evaluate%20the%20performance%20of%20the%0Aproposed%20multi-modal%20hardware%20generative%20AI%20in%20Verilog%20generation%20tasks%2C%20we%0Acompare%20it%20with%20a%20popular%20method%20that%20relies%20solely%20on%20natural%20language.%20Our%0Aresults%20demonstrate%20a%20significant%20accuracy%20improvement%20in%20the%20multi-modal%0Agenerated%20Verilog%20compared%20to%20queries%20based%20solely%20on%20natural%20language.%20We%20hope%0Ato%20reveal%20a%20new%20approach%20to%20hardware%20design%20in%20the%20large-hardware-design-model%0Aera%2C%20thereby%20fostering%20a%20more%20diversified%20and%20productive%20approach%20to%20hardware%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520language%2520is%2520not%2520enough%253A%2520Benchmarking%2520multi-modal%2520generative%2520AI%250A%2520%2520for%2520Verilog%2520generation%26entry.906535625%3DKaiyan%2520Chang%2520and%2520Zhirong%2520Chen%2520and%2520Yunhao%2520Zhou%2520and%2520Wenlong%2520Zhu%2520and%2520kun%2520wang%2520and%2520Haobo%2520Xu%2520and%2520Cangyuan%2520Li%2520and%2520Mengdi%2520Wang%2520and%2520Shengwen%2520Liang%2520and%2520Huawei%2520Li%2520and%2520Yinhe%2520Han%2520and%2520Ying%2520Wang%26entry.1292438233%3D%2520%2520Natural%2520language%2520interfaces%2520have%2520exhibited%2520considerable%2520potential%2520in%2520the%250Aautomation%2520of%2520Verilog%2520generation%2520derived%2520from%2520high-level%2520specifications%2520through%250Athe%2520utilization%2520of%2520large%2520language%2520models%252C%2520garnering%2520significant%2520attention.%250ANevertheless%252C%2520this%2520paper%2520elucidates%2520that%2520visual%2520representations%2520contribute%250Aessential%2520contextual%2520information%2520critical%2520to%2520design%2520intent%2520for%2520hardware%250Aarchitectures%2520possessing%2520spatial%2520complexity%252C%2520potentially%2520surpassing%2520the%250Aefficacy%2520of%2520natural-language-only%2520inputs.%2520Expanding%2520upon%2520this%2520premise%252C%2520our%250Apaper%2520introduces%2520an%2520open-source%2520benchmark%2520for%2520multi-modal%2520generative%2520models%250Atailored%2520for%2520Verilog%2520synthesis%2520from%2520visual-linguistic%2520inputs%252C%2520addressing%2520both%250Asingular%2520and%2520complex%2520modules.%2520Additionally%252C%2520we%2520introduce%2520an%2520open-source%2520visual%250Aand%2520natural%2520language%2520Verilog%2520query%2520language%2520framework%2520to%2520facilitate%2520efficient%250Aand%2520user-friendly%2520multi-modal%2520queries.%2520To%2520evaluate%2520the%2520performance%2520of%2520the%250Aproposed%2520multi-modal%2520hardware%2520generative%2520AI%2520in%2520Verilog%2520generation%2520tasks%252C%2520we%250Acompare%2520it%2520with%2520a%2520popular%2520method%2520that%2520relies%2520solely%2520on%2520natural%2520language.%2520Our%250Aresults%2520demonstrate%2520a%2520significant%2520accuracy%2520improvement%2520in%2520the%2520multi-modal%250Agenerated%2520Verilog%2520compared%2520to%2520queries%2520based%2520solely%2520on%2520natural%2520language.%2520We%2520hope%250Ato%2520reveal%2520a%2520new%2520approach%2520to%2520hardware%2520design%2520in%2520the%2520large-hardware-design-model%250Aera%252C%2520thereby%2520fostering%2520a%2520more%2520diversified%2520and%2520productive%2520approach%2520to%2520hardware%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20language%20is%20not%20enough%3A%20Benchmarking%20multi-modal%20generative%20AI%0A%20%20for%20Verilog%20generation&entry.906535625=Kaiyan%20Chang%20and%20Zhirong%20Chen%20and%20Yunhao%20Zhou%20and%20Wenlong%20Zhu%20and%20kun%20wang%20and%20Haobo%20Xu%20and%20Cangyuan%20Li%20and%20Mengdi%20Wang%20and%20Shengwen%20Liang%20and%20Huawei%20Li%20and%20Yinhe%20Han%20and%20Ying%20Wang&entry.1292438233=%20%20Natural%20language%20interfaces%20have%20exhibited%20considerable%20potential%20in%20the%0Aautomation%20of%20Verilog%20generation%20derived%20from%20high-level%20specifications%20through%0Athe%20utilization%20of%20large%20language%20models%2C%20garnering%20significant%20attention.%0ANevertheless%2C%20this%20paper%20elucidates%20that%20visual%20representations%20contribute%0Aessential%20contextual%20information%20critical%20to%20design%20intent%20for%20hardware%0Aarchitectures%20possessing%20spatial%20complexity%2C%20potentially%20surpassing%20the%0Aefficacy%20of%20natural-language-only%20inputs.%20Expanding%20upon%20this%20premise%2C%20our%0Apaper%20introduces%20an%20open-source%20benchmark%20for%20multi-modal%20generative%20models%0Atailored%20for%20Verilog%20synthesis%20from%20visual-linguistic%20inputs%2C%20addressing%20both%0Asingular%20and%20complex%20modules.%20Additionally%2C%20we%20introduce%20an%20open-source%20visual%0Aand%20natural%20language%20Verilog%20query%20language%20framework%20to%20facilitate%20efficient%0Aand%20user-friendly%20multi-modal%20queries.%20To%20evaluate%20the%20performance%20of%20the%0Aproposed%20multi-modal%20hardware%20generative%20AI%20in%20Verilog%20generation%20tasks%2C%20we%0Acompare%20it%20with%20a%20popular%20method%20that%20relies%20solely%20on%20natural%20language.%20Our%0Aresults%20demonstrate%20a%20significant%20accuracy%20improvement%20in%20the%20multi-modal%0Agenerated%20Verilog%20compared%20to%20queries%20based%20solely%20on%20natural%20language.%20We%20hope%0Ato%20reveal%20a%20new%20approach%20to%20hardware%20design%20in%20the%20large-hardware-design-model%0Aera%2C%20thereby%20fostering%20a%20more%20diversified%20and%20productive%20approach%20to%20hardware%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08473v1&entry.124074799=Read"},
{"title": "Shedding More Light on Robust Classifiers under the lens of Energy-based\n  Models", "author": "Mujtaba Hussain Mirza and Maria Rosaria Briglia and Senad Beadini and Iacopo Masi", "abstract": "  By reinterpreting a robust discriminative classifier as Energy-based Model\n(EBM), we offer a new take on the dynamics of adversarial training (AT). Our\nanalysis of the energy landscape during AT reveals that untargeted attacks\ngenerate adversarial images much more in-distribution (lower energy) than the\noriginal data from the point of view of the model. Conversely, we observe the\nopposite for targeted attacks. On the ground of our thorough analysis, we\npresent new theoretical and practical results that show how interpreting AT\nenergy dynamics unlocks a better understanding: (1) AT dynamic is governed by\nthree phases and robust overfitting occurs in the third phase with a drastic\ndivergence between natural and adversarial energies (2) by rewriting the loss\nof TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization\n(TRADES) in terms of energies, we show that TRADES implicitly alleviates\noverfitting by means of aligning the natural energy with the adversarial one\n(3) we empirically show that all recent state-of-the-art robust classifiers are\nsmoothing the energy landscape and we reconcile a variety of studies about\nunderstanding AT and weighting the loss function under the umbrella of EBMs.\nMotivated by rigorous evidence, we propose Weighted Energy Adversarial Training\n(WEAT), a novel sample weighting scheme that yields robust accuracy matching\nthe state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going\nbeyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers\nvary in the intensity and quality of their generative capabilities, and offer a\nsimple method to push this capability, reaching a remarkable Inception Score\n(IS) and FID using a robust classifier without training for generative\nmodeling. The code to reproduce our results is available at\nhttp://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/ .\n", "link": "http://arxiv.org/abs/2407.06315v2", "date": "2024-07-11", "relevancy": 2.1234, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5551}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5181}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shedding%20More%20Light%20on%20Robust%20Classifiers%20under%20the%20lens%20of%20Energy-based%0A%20%20Models&body=Title%3A%20Shedding%20More%20Light%20on%20Robust%20Classifiers%20under%20the%20lens%20of%20Energy-based%0A%20%20Models%0AAuthor%3A%20Mujtaba%20Hussain%20Mirza%20and%20Maria%20Rosaria%20Briglia%20and%20Senad%20Beadini%20and%20Iacopo%20Masi%0AAbstract%3A%20%20%20By%20reinterpreting%20a%20robust%20discriminative%20classifier%20as%20Energy-based%20Model%0A%28EBM%29%2C%20we%20offer%20a%20new%20take%20on%20the%20dynamics%20of%20adversarial%20training%20%28AT%29.%20Our%0Aanalysis%20of%20the%20energy%20landscape%20during%20AT%20reveals%20that%20untargeted%20attacks%0Agenerate%20adversarial%20images%20much%20more%20in-distribution%20%28lower%20energy%29%20than%20the%0Aoriginal%20data%20from%20the%20point%20of%20view%20of%20the%20model.%20Conversely%2C%20we%20observe%20the%0Aopposite%20for%20targeted%20attacks.%20On%20the%20ground%20of%20our%20thorough%20analysis%2C%20we%0Apresent%20new%20theoretical%20and%20practical%20results%20that%20show%20how%20interpreting%20AT%0Aenergy%20dynamics%20unlocks%20a%20better%20understanding%3A%20%281%29%20AT%20dynamic%20is%20governed%20by%0Athree%20phases%20and%20robust%20overfitting%20occurs%20in%20the%20third%20phase%20with%20a%20drastic%0Adivergence%20between%20natural%20and%20adversarial%20energies%20%282%29%20by%20rewriting%20the%20loss%0Aof%20TRadeoff-inspired%20Adversarial%20DEfense%20via%20Surrogate-loss%20minimization%0A%28TRADES%29%20in%20terms%20of%20energies%2C%20we%20show%20that%20TRADES%20implicitly%20alleviates%0Aoverfitting%20by%20means%20of%20aligning%20the%20natural%20energy%20with%20the%20adversarial%20one%0A%283%29%20we%20empirically%20show%20that%20all%20recent%20state-of-the-art%20robust%20classifiers%20are%0Asmoothing%20the%20energy%20landscape%20and%20we%20reconcile%20a%20variety%20of%20studies%20about%0Aunderstanding%20AT%20and%20weighting%20the%20loss%20function%20under%20the%20umbrella%20of%20EBMs.%0AMotivated%20by%20rigorous%20evidence%2C%20we%20propose%20Weighted%20Energy%20Adversarial%20Training%0A%28WEAT%29%2C%20a%20novel%20sample%20weighting%20scheme%20that%20yields%20robust%20accuracy%20matching%0Athe%20state-of-the-art%20on%20multiple%20benchmarks%20such%20as%20CIFAR-10%20and%20SVHN%20and%20going%0Abeyond%20in%20CIFAR-100%20and%20Tiny-ImageNet.%20We%20further%20show%20that%20robust%20classifiers%0Avary%20in%20the%20intensity%20and%20quality%20of%20their%20generative%20capabilities%2C%20and%20offer%20a%0Asimple%20method%20to%20push%20this%20capability%2C%20reaching%20a%20remarkable%20Inception%20Score%0A%28IS%29%20and%20FID%20using%20a%20robust%20classifier%20without%20training%20for%20generative%0Amodeling.%20The%20code%20to%20reproduce%20our%20results%20is%20available%20at%0Ahttp%3A//github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShedding%2520More%2520Light%2520on%2520Robust%2520Classifiers%2520under%2520the%2520lens%2520of%2520Energy-based%250A%2520%2520Models%26entry.906535625%3DMujtaba%2520Hussain%2520Mirza%2520and%2520Maria%2520Rosaria%2520Briglia%2520and%2520Senad%2520Beadini%2520and%2520Iacopo%2520Masi%26entry.1292438233%3D%2520%2520By%2520reinterpreting%2520a%2520robust%2520discriminative%2520classifier%2520as%2520Energy-based%2520Model%250A%2528EBM%2529%252C%2520we%2520offer%2520a%2520new%2520take%2520on%2520the%2520dynamics%2520of%2520adversarial%2520training%2520%2528AT%2529.%2520Our%250Aanalysis%2520of%2520the%2520energy%2520landscape%2520during%2520AT%2520reveals%2520that%2520untargeted%2520attacks%250Agenerate%2520adversarial%2520images%2520much%2520more%2520in-distribution%2520%2528lower%2520energy%2529%2520than%2520the%250Aoriginal%2520data%2520from%2520the%2520point%2520of%2520view%2520of%2520the%2520model.%2520Conversely%252C%2520we%2520observe%2520the%250Aopposite%2520for%2520targeted%2520attacks.%2520On%2520the%2520ground%2520of%2520our%2520thorough%2520analysis%252C%2520we%250Apresent%2520new%2520theoretical%2520and%2520practical%2520results%2520that%2520show%2520how%2520interpreting%2520AT%250Aenergy%2520dynamics%2520unlocks%2520a%2520better%2520understanding%253A%2520%25281%2529%2520AT%2520dynamic%2520is%2520governed%2520by%250Athree%2520phases%2520and%2520robust%2520overfitting%2520occurs%2520in%2520the%2520third%2520phase%2520with%2520a%2520drastic%250Adivergence%2520between%2520natural%2520and%2520adversarial%2520energies%2520%25282%2529%2520by%2520rewriting%2520the%2520loss%250Aof%2520TRadeoff-inspired%2520Adversarial%2520DEfense%2520via%2520Surrogate-loss%2520minimization%250A%2528TRADES%2529%2520in%2520terms%2520of%2520energies%252C%2520we%2520show%2520that%2520TRADES%2520implicitly%2520alleviates%250Aoverfitting%2520by%2520means%2520of%2520aligning%2520the%2520natural%2520energy%2520with%2520the%2520adversarial%2520one%250A%25283%2529%2520we%2520empirically%2520show%2520that%2520all%2520recent%2520state-of-the-art%2520robust%2520classifiers%2520are%250Asmoothing%2520the%2520energy%2520landscape%2520and%2520we%2520reconcile%2520a%2520variety%2520of%2520studies%2520about%250Aunderstanding%2520AT%2520and%2520weighting%2520the%2520loss%2520function%2520under%2520the%2520umbrella%2520of%2520EBMs.%250AMotivated%2520by%2520rigorous%2520evidence%252C%2520we%2520propose%2520Weighted%2520Energy%2520Adversarial%2520Training%250A%2528WEAT%2529%252C%2520a%2520novel%2520sample%2520weighting%2520scheme%2520that%2520yields%2520robust%2520accuracy%2520matching%250Athe%2520state-of-the-art%2520on%2520multiple%2520benchmarks%2520such%2520as%2520CIFAR-10%2520and%2520SVHN%2520and%2520going%250Abeyond%2520in%2520CIFAR-100%2520and%2520Tiny-ImageNet.%2520We%2520further%2520show%2520that%2520robust%2520classifiers%250Avary%2520in%2520the%2520intensity%2520and%2520quality%2520of%2520their%2520generative%2520capabilities%252C%2520and%2520offer%2520a%250Asimple%2520method%2520to%2520push%2520this%2520capability%252C%2520reaching%2520a%2520remarkable%2520Inception%2520Score%250A%2528IS%2529%2520and%2520FID%2520using%2520a%2520robust%2520classifier%2520without%2520training%2520for%2520generative%250Amodeling.%2520The%2520code%2520to%2520reproduce%2520our%2520results%2520is%2520available%2520at%250Ahttp%253A//github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shedding%20More%20Light%20on%20Robust%20Classifiers%20under%20the%20lens%20of%20Energy-based%0A%20%20Models&entry.906535625=Mujtaba%20Hussain%20Mirza%20and%20Maria%20Rosaria%20Briglia%20and%20Senad%20Beadini%20and%20Iacopo%20Masi&entry.1292438233=%20%20By%20reinterpreting%20a%20robust%20discriminative%20classifier%20as%20Energy-based%20Model%0A%28EBM%29%2C%20we%20offer%20a%20new%20take%20on%20the%20dynamics%20of%20adversarial%20training%20%28AT%29.%20Our%0Aanalysis%20of%20the%20energy%20landscape%20during%20AT%20reveals%20that%20untargeted%20attacks%0Agenerate%20adversarial%20images%20much%20more%20in-distribution%20%28lower%20energy%29%20than%20the%0Aoriginal%20data%20from%20the%20point%20of%20view%20of%20the%20model.%20Conversely%2C%20we%20observe%20the%0Aopposite%20for%20targeted%20attacks.%20On%20the%20ground%20of%20our%20thorough%20analysis%2C%20we%0Apresent%20new%20theoretical%20and%20practical%20results%20that%20show%20how%20interpreting%20AT%0Aenergy%20dynamics%20unlocks%20a%20better%20understanding%3A%20%281%29%20AT%20dynamic%20is%20governed%20by%0Athree%20phases%20and%20robust%20overfitting%20occurs%20in%20the%20third%20phase%20with%20a%20drastic%0Adivergence%20between%20natural%20and%20adversarial%20energies%20%282%29%20by%20rewriting%20the%20loss%0Aof%20TRadeoff-inspired%20Adversarial%20DEfense%20via%20Surrogate-loss%20minimization%0A%28TRADES%29%20in%20terms%20of%20energies%2C%20we%20show%20that%20TRADES%20implicitly%20alleviates%0Aoverfitting%20by%20means%20of%20aligning%20the%20natural%20energy%20with%20the%20adversarial%20one%0A%283%29%20we%20empirically%20show%20that%20all%20recent%20state-of-the-art%20robust%20classifiers%20are%0Asmoothing%20the%20energy%20landscape%20and%20we%20reconcile%20a%20variety%20of%20studies%20about%0Aunderstanding%20AT%20and%20weighting%20the%20loss%20function%20under%20the%20umbrella%20of%20EBMs.%0AMotivated%20by%20rigorous%20evidence%2C%20we%20propose%20Weighted%20Energy%20Adversarial%20Training%0A%28WEAT%29%2C%20a%20novel%20sample%20weighting%20scheme%20that%20yields%20robust%20accuracy%20matching%0Athe%20state-of-the-art%20on%20multiple%20benchmarks%20such%20as%20CIFAR-10%20and%20SVHN%20and%20going%0Abeyond%20in%20CIFAR-100%20and%20Tiny-ImageNet.%20We%20further%20show%20that%20robust%20classifiers%0Avary%20in%20the%20intensity%20and%20quality%20of%20their%20generative%20capabilities%2C%20and%20offer%20a%0Asimple%20method%20to%20push%20this%20capability%2C%20reaching%20a%20remarkable%20Inception%20Score%0A%28IS%29%20and%20FID%20using%20a%20robust%20classifier%20without%20training%20for%20generative%0Amodeling.%20The%20code%20to%20reproduce%20our%20results%20is%20available%20at%0Ahttp%3A//github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06315v2&entry.124074799=Read"},
{"title": "15M Multimodal Facial Image-Text Dataset", "author": "Dawei Dai and YuTang Li and YingGe Liu and Mingming Jia and Zhang YuanHui and Guoyin Wang", "abstract": "  Currently, image-text-driven multi-modal deep learning models have\ndemonstrated their outstanding potential in many fields. In practice, tasks\ncentered around facial images have broad application prospects. This paper\npresents \\textbf{FaceCaption-15M}, a large-scale, diverse, and high-quality\ndataset of facial images accompanied by their natural language descriptions\n(facial image-to-text). This dataset aims to facilitate a study on\nface-centered tasks. FaceCaption-15M comprises over 15 million pairs of facial\nimages and their corresponding natural language descriptions of facial\nfeatures, making it the largest facial image-caption dataset to date. We\nconducted a comprehensive analysis of image quality, text naturalness, text\ncomplexity, and text-image relevance to demonstrate the superiority of\nFaceCaption-15M. To validate the effectiveness of FaceCaption-15M, we first\ntrained a facial language-image pre-training model (FLIP, similar to CLIP) to\nalign facial image with its corresponding captions in feature space.\nSubsequently, using both image and text encoders and fine-tuning only the\nlinear layer, our FLIP-based models achieved state-of-the-art results on two\nchallenging face-centered tasks. The purpose is to promote research in the\nfield of face-related tasks through the availability of the proposed\nFaceCaption-15M dataset. All data, codes, and models are publicly available.\nhttps://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M\n", "link": "http://arxiv.org/abs/2407.08515v1", "date": "2024-07-11", "relevancy": 2.1229, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5581}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%2015M%20Multimodal%20Facial%20Image-Text%20Dataset&body=Title%3A%2015M%20Multimodal%20Facial%20Image-Text%20Dataset%0AAuthor%3A%20Dawei%20Dai%20and%20YuTang%20Li%20and%20YingGe%20Liu%20and%20Mingming%20Jia%20and%20Zhang%20YuanHui%20and%20Guoyin%20Wang%0AAbstract%3A%20%20%20Currently%2C%20image-text-driven%20multi-modal%20deep%20learning%20models%20have%0Ademonstrated%20their%20outstanding%20potential%20in%20many%20fields.%20In%20practice%2C%20tasks%0Acentered%20around%20facial%20images%20have%20broad%20application%20prospects.%20This%20paper%0Apresents%20%5Ctextbf%7BFaceCaption-15M%7D%2C%20a%20large-scale%2C%20diverse%2C%20and%20high-quality%0Adataset%20of%20facial%20images%20accompanied%20by%20their%20natural%20language%20descriptions%0A%28facial%20image-to-text%29.%20This%20dataset%20aims%20to%20facilitate%20a%20study%20on%0Aface-centered%20tasks.%20FaceCaption-15M%20comprises%20over%2015%20million%20pairs%20of%20facial%0Aimages%20and%20their%20corresponding%20natural%20language%20descriptions%20of%20facial%0Afeatures%2C%20making%20it%20the%20largest%20facial%20image-caption%20dataset%20to%20date.%20We%0Aconducted%20a%20comprehensive%20analysis%20of%20image%20quality%2C%20text%20naturalness%2C%20text%0Acomplexity%2C%20and%20text-image%20relevance%20to%20demonstrate%20the%20superiority%20of%0AFaceCaption-15M.%20To%20validate%20the%20effectiveness%20of%20FaceCaption-15M%2C%20we%20first%0Atrained%20a%20facial%20language-image%20pre-training%20model%20%28FLIP%2C%20similar%20to%20CLIP%29%20to%0Aalign%20facial%20image%20with%20its%20corresponding%20captions%20in%20feature%20space.%0ASubsequently%2C%20using%20both%20image%20and%20text%20encoders%20and%20fine-tuning%20only%20the%0Alinear%20layer%2C%20our%20FLIP-based%20models%20achieved%20state-of-the-art%20results%20on%20two%0Achallenging%20face-centered%20tasks.%20The%20purpose%20is%20to%20promote%20research%20in%20the%0Afield%20of%20face-related%20tasks%20through%20the%20availability%20of%20the%20proposed%0AFaceCaption-15M%20dataset.%20All%20data%2C%20codes%2C%20and%20models%20are%20publicly%20available.%0Ahttps%3A//huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D15M%2520Multimodal%2520Facial%2520Image-Text%2520Dataset%26entry.906535625%3DDawei%2520Dai%2520and%2520YuTang%2520Li%2520and%2520YingGe%2520Liu%2520and%2520Mingming%2520Jia%2520and%2520Zhang%2520YuanHui%2520and%2520Guoyin%2520Wang%26entry.1292438233%3D%2520%2520Currently%252C%2520image-text-driven%2520multi-modal%2520deep%2520learning%2520models%2520have%250Ademonstrated%2520their%2520outstanding%2520potential%2520in%2520many%2520fields.%2520In%2520practice%252C%2520tasks%250Acentered%2520around%2520facial%2520images%2520have%2520broad%2520application%2520prospects.%2520This%2520paper%250Apresents%2520%255Ctextbf%257BFaceCaption-15M%257D%252C%2520a%2520large-scale%252C%2520diverse%252C%2520and%2520high-quality%250Adataset%2520of%2520facial%2520images%2520accompanied%2520by%2520their%2520natural%2520language%2520descriptions%250A%2528facial%2520image-to-text%2529.%2520This%2520dataset%2520aims%2520to%2520facilitate%2520a%2520study%2520on%250Aface-centered%2520tasks.%2520FaceCaption-15M%2520comprises%2520over%252015%2520million%2520pairs%2520of%2520facial%250Aimages%2520and%2520their%2520corresponding%2520natural%2520language%2520descriptions%2520of%2520facial%250Afeatures%252C%2520making%2520it%2520the%2520largest%2520facial%2520image-caption%2520dataset%2520to%2520date.%2520We%250Aconducted%2520a%2520comprehensive%2520analysis%2520of%2520image%2520quality%252C%2520text%2520naturalness%252C%2520text%250Acomplexity%252C%2520and%2520text-image%2520relevance%2520to%2520demonstrate%2520the%2520superiority%2520of%250AFaceCaption-15M.%2520To%2520validate%2520the%2520effectiveness%2520of%2520FaceCaption-15M%252C%2520we%2520first%250Atrained%2520a%2520facial%2520language-image%2520pre-training%2520model%2520%2528FLIP%252C%2520similar%2520to%2520CLIP%2529%2520to%250Aalign%2520facial%2520image%2520with%2520its%2520corresponding%2520captions%2520in%2520feature%2520space.%250ASubsequently%252C%2520using%2520both%2520image%2520and%2520text%2520encoders%2520and%2520fine-tuning%2520only%2520the%250Alinear%2520layer%252C%2520our%2520FLIP-based%2520models%2520achieved%2520state-of-the-art%2520results%2520on%2520two%250Achallenging%2520face-centered%2520tasks.%2520The%2520purpose%2520is%2520to%2520promote%2520research%2520in%2520the%250Afield%2520of%2520face-related%2520tasks%2520through%2520the%2520availability%2520of%2520the%2520proposed%250AFaceCaption-15M%2520dataset.%2520All%2520data%252C%2520codes%252C%2520and%2520models%2520are%2520publicly%2520available.%250Ahttps%253A//huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=15M%20Multimodal%20Facial%20Image-Text%20Dataset&entry.906535625=Dawei%20Dai%20and%20YuTang%20Li%20and%20YingGe%20Liu%20and%20Mingming%20Jia%20and%20Zhang%20YuanHui%20and%20Guoyin%20Wang&entry.1292438233=%20%20Currently%2C%20image-text-driven%20multi-modal%20deep%20learning%20models%20have%0Ademonstrated%20their%20outstanding%20potential%20in%20many%20fields.%20In%20practice%2C%20tasks%0Acentered%20around%20facial%20images%20have%20broad%20application%20prospects.%20This%20paper%0Apresents%20%5Ctextbf%7BFaceCaption-15M%7D%2C%20a%20large-scale%2C%20diverse%2C%20and%20high-quality%0Adataset%20of%20facial%20images%20accompanied%20by%20their%20natural%20language%20descriptions%0A%28facial%20image-to-text%29.%20This%20dataset%20aims%20to%20facilitate%20a%20study%20on%0Aface-centered%20tasks.%20FaceCaption-15M%20comprises%20over%2015%20million%20pairs%20of%20facial%0Aimages%20and%20their%20corresponding%20natural%20language%20descriptions%20of%20facial%0Afeatures%2C%20making%20it%20the%20largest%20facial%20image-caption%20dataset%20to%20date.%20We%0Aconducted%20a%20comprehensive%20analysis%20of%20image%20quality%2C%20text%20naturalness%2C%20text%0Acomplexity%2C%20and%20text-image%20relevance%20to%20demonstrate%20the%20superiority%20of%0AFaceCaption-15M.%20To%20validate%20the%20effectiveness%20of%20FaceCaption-15M%2C%20we%20first%0Atrained%20a%20facial%20language-image%20pre-training%20model%20%28FLIP%2C%20similar%20to%20CLIP%29%20to%0Aalign%20facial%20image%20with%20its%20corresponding%20captions%20in%20feature%20space.%0ASubsequently%2C%20using%20both%20image%20and%20text%20encoders%20and%20fine-tuning%20only%20the%0Alinear%20layer%2C%20our%20FLIP-based%20models%20achieved%20state-of-the-art%20results%20on%20two%0Achallenging%20face-centered%20tasks.%20The%20purpose%20is%20to%20promote%20research%20in%20the%0Afield%20of%20face-related%20tasks%20through%20the%20availability%20of%20the%20proposed%0AFaceCaption-15M%20dataset.%20All%20data%2C%20codes%2C%20and%20models%20are%20publicly%20available.%0Ahttps%3A//huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08515v1&entry.124074799=Read"},
{"title": "TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware\n  Representations", "author": "Junik Bae and Kwanyoung Park and Youngwoon Lee", "abstract": "  Unsupervised goal-conditioned reinforcement learning (GCRL) is a promising\nparadigm for developing diverse robotic skills without external supervision.\nHowever, existing unsupervised GCRL methods often struggle to cover a wide\nrange of states in complex environments due to their limited exploration and\nsparse or noisy rewards for GCRL. To overcome these challenges, we propose a\nnovel unsupervised GCRL method that leverages TemporaL Distance-aware\nRepresentations (TLDR). TLDR selects faraway goals to initiate exploration and\ncomputes intrinsic exploration rewards and goal-reaching rewards, based on\ntemporal distance. Specifically, our exploration policy seeks states with large\ntemporal distances (i.e. covering a large state space), while the\ngoal-conditioned policy learns to minimize the temporal distance to the goal\n(i.e. reaching the goal). Our experimental results in six simulated robotic\nlocomotion environments demonstrate that our method significantly outperforms\nprevious unsupervised GCRL methods in achieving a wide variety of states.\n", "link": "http://arxiv.org/abs/2407.08464v1", "date": "2024-07-11", "relevancy": 2.1169, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5965}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5216}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TLDR%3A%20Unsupervised%20Goal-Conditioned%20RL%20via%20Temporal%20Distance-Aware%0A%20%20Representations&body=Title%3A%20TLDR%3A%20Unsupervised%20Goal-Conditioned%20RL%20via%20Temporal%20Distance-Aware%0A%20%20Representations%0AAuthor%3A%20Junik%20Bae%20and%20Kwanyoung%20Park%20and%20Youngwoon%20Lee%0AAbstract%3A%20%20%20Unsupervised%20goal-conditioned%20reinforcement%20learning%20%28GCRL%29%20is%20a%20promising%0Aparadigm%20for%20developing%20diverse%20robotic%20skills%20without%20external%20supervision.%0AHowever%2C%20existing%20unsupervised%20GCRL%20methods%20often%20struggle%20to%20cover%20a%20wide%0Arange%20of%20states%20in%20complex%20environments%20due%20to%20their%20limited%20exploration%20and%0Asparse%20or%20noisy%20rewards%20for%20GCRL.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%0Anovel%20unsupervised%20GCRL%20method%20that%20leverages%20TemporaL%20Distance-aware%0ARepresentations%20%28TLDR%29.%20TLDR%20selects%20faraway%20goals%20to%20initiate%20exploration%20and%0Acomputes%20intrinsic%20exploration%20rewards%20and%20goal-reaching%20rewards%2C%20based%20on%0Atemporal%20distance.%20Specifically%2C%20our%20exploration%20policy%20seeks%20states%20with%20large%0Atemporal%20distances%20%28i.e.%20covering%20a%20large%20state%20space%29%2C%20while%20the%0Agoal-conditioned%20policy%20learns%20to%20minimize%20the%20temporal%20distance%20to%20the%20goal%0A%28i.e.%20reaching%20the%20goal%29.%20Our%20experimental%20results%20in%20six%20simulated%20robotic%0Alocomotion%20environments%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aprevious%20unsupervised%20GCRL%20methods%20in%20achieving%20a%20wide%20variety%20of%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTLDR%253A%2520Unsupervised%2520Goal-Conditioned%2520RL%2520via%2520Temporal%2520Distance-Aware%250A%2520%2520Representations%26entry.906535625%3DJunik%2520Bae%2520and%2520Kwanyoung%2520Park%2520and%2520Youngwoon%2520Lee%26entry.1292438233%3D%2520%2520Unsupervised%2520goal-conditioned%2520reinforcement%2520learning%2520%2528GCRL%2529%2520is%2520a%2520promising%250Aparadigm%2520for%2520developing%2520diverse%2520robotic%2520skills%2520without%2520external%2520supervision.%250AHowever%252C%2520existing%2520unsupervised%2520GCRL%2520methods%2520often%2520struggle%2520to%2520cover%2520a%2520wide%250Arange%2520of%2520states%2520in%2520complex%2520environments%2520due%2520to%2520their%2520limited%2520exploration%2520and%250Asparse%2520or%2520noisy%2520rewards%2520for%2520GCRL.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%250Anovel%2520unsupervised%2520GCRL%2520method%2520that%2520leverages%2520TemporaL%2520Distance-aware%250ARepresentations%2520%2528TLDR%2529.%2520TLDR%2520selects%2520faraway%2520goals%2520to%2520initiate%2520exploration%2520and%250Acomputes%2520intrinsic%2520exploration%2520rewards%2520and%2520goal-reaching%2520rewards%252C%2520based%2520on%250Atemporal%2520distance.%2520Specifically%252C%2520our%2520exploration%2520policy%2520seeks%2520states%2520with%2520large%250Atemporal%2520distances%2520%2528i.e.%2520covering%2520a%2520large%2520state%2520space%2529%252C%2520while%2520the%250Agoal-conditioned%2520policy%2520learns%2520to%2520minimize%2520the%2520temporal%2520distance%2520to%2520the%2520goal%250A%2528i.e.%2520reaching%2520the%2520goal%2529.%2520Our%2520experimental%2520results%2520in%2520six%2520simulated%2520robotic%250Alocomotion%2520environments%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%250Aprevious%2520unsupervised%2520GCRL%2520methods%2520in%2520achieving%2520a%2520wide%2520variety%2520of%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TLDR%3A%20Unsupervised%20Goal-Conditioned%20RL%20via%20Temporal%20Distance-Aware%0A%20%20Representations&entry.906535625=Junik%20Bae%20and%20Kwanyoung%20Park%20and%20Youngwoon%20Lee&entry.1292438233=%20%20Unsupervised%20goal-conditioned%20reinforcement%20learning%20%28GCRL%29%20is%20a%20promising%0Aparadigm%20for%20developing%20diverse%20robotic%20skills%20without%20external%20supervision.%0AHowever%2C%20existing%20unsupervised%20GCRL%20methods%20often%20struggle%20to%20cover%20a%20wide%0Arange%20of%20states%20in%20complex%20environments%20due%20to%20their%20limited%20exploration%20and%0Asparse%20or%20noisy%20rewards%20for%20GCRL.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%0Anovel%20unsupervised%20GCRL%20method%20that%20leverages%20TemporaL%20Distance-aware%0ARepresentations%20%28TLDR%29.%20TLDR%20selects%20faraway%20goals%20to%20initiate%20exploration%20and%0Acomputes%20intrinsic%20exploration%20rewards%20and%20goal-reaching%20rewards%2C%20based%20on%0Atemporal%20distance.%20Specifically%2C%20our%20exploration%20policy%20seeks%20states%20with%20large%0Atemporal%20distances%20%28i.e.%20covering%20a%20large%20state%20space%29%2C%20while%20the%0Agoal-conditioned%20policy%20learns%20to%20minimize%20the%20temporal%20distance%20to%20the%20goal%0A%28i.e.%20reaching%20the%20goal%29.%20Our%20experimental%20results%20in%20six%20simulated%20robotic%0Alocomotion%20environments%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aprevious%20unsupervised%20GCRL%20methods%20in%20achieving%20a%20wide%20variety%20of%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08464v1&entry.124074799=Read"},
{"title": "UP-FacE: User-predictable Fine-grained Face Shape Editing", "author": "Florian Strohm and Mihai B\u00e2ce and Andreas Bulling", "abstract": "  We present User-predictable Face Editing (UP-FacE) -- a novel method for\npredictable face shape editing. In stark contrast to existing methods for face\nediting using trial and error, edits with UP-FacE are predictable by the human\nuser. That is, users can control the desired degree of change precisely and\ndeterministically and know upfront the amount of change required to achieve a\ncertain editing result. Our method leverages facial landmarks to precisely\nmeasure facial feature values, facilitating the training of UP-FacE without\nmanually annotated attribute labels. At the core of UP-FacE is a\ntransformer-based network that takes as input a latent vector from a\npre-trained generative model and a facial feature embedding, and predicts a\nsuitable manipulation vector. To enable user-predictable editing, a scaling\nlayer adjusts the manipulation vector to achieve the precise desired degree of\nchange. To ensure that the desired feature is manipulated towards the target\nvalue without altering uncorrelated features, we further introduce a novel\nsemantic face feature loss. Qualitative and quantitative results demonstrate\nthat UP-FacE enables precise and fine-grained control over 23 face shape\nfeatures.\n", "link": "http://arxiv.org/abs/2403.13972v3", "date": "2024-07-11", "relevancy": 2.1141, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.544}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5254}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UP-FacE%3A%20User-predictable%20Fine-grained%20Face%20Shape%20Editing&body=Title%3A%20UP-FacE%3A%20User-predictable%20Fine-grained%20Face%20Shape%20Editing%0AAuthor%3A%20Florian%20Strohm%20and%20Mihai%20B%C3%A2ce%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20We%20present%20User-predictable%20Face%20Editing%20%28UP-FacE%29%20--%20a%20novel%20method%20for%0Apredictable%20face%20shape%20editing.%20In%20stark%20contrast%20to%20existing%20methods%20for%20face%0Aediting%20using%20trial%20and%20error%2C%20edits%20with%20UP-FacE%20are%20predictable%20by%20the%20human%0Auser.%20That%20is%2C%20users%20can%20control%20the%20desired%20degree%20of%20change%20precisely%20and%0Adeterministically%20and%20know%20upfront%20the%20amount%20of%20change%20required%20to%20achieve%20a%0Acertain%20editing%20result.%20Our%20method%20leverages%20facial%20landmarks%20to%20precisely%0Ameasure%20facial%20feature%20values%2C%20facilitating%20the%20training%20of%20UP-FacE%20without%0Amanually%20annotated%20attribute%20labels.%20At%20the%20core%20of%20UP-FacE%20is%20a%0Atransformer-based%20network%20that%20takes%20as%20input%20a%20latent%20vector%20from%20a%0Apre-trained%20generative%20model%20and%20a%20facial%20feature%20embedding%2C%20and%20predicts%20a%0Asuitable%20manipulation%20vector.%20To%20enable%20user-predictable%20editing%2C%20a%20scaling%0Alayer%20adjusts%20the%20manipulation%20vector%20to%20achieve%20the%20precise%20desired%20degree%20of%0Achange.%20To%20ensure%20that%20the%20desired%20feature%20is%20manipulated%20towards%20the%20target%0Avalue%20without%20altering%20uncorrelated%20features%2C%20we%20further%20introduce%20a%20novel%0Asemantic%20face%20feature%20loss.%20Qualitative%20and%20quantitative%20results%20demonstrate%0Athat%20UP-FacE%20enables%20precise%20and%20fine-grained%20control%20over%2023%20face%20shape%0Afeatures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13972v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUP-FacE%253A%2520User-predictable%2520Fine-grained%2520Face%2520Shape%2520Editing%26entry.906535625%3DFlorian%2520Strohm%2520and%2520Mihai%2520B%25C3%25A2ce%2520and%2520Andreas%2520Bulling%26entry.1292438233%3D%2520%2520We%2520present%2520User-predictable%2520Face%2520Editing%2520%2528UP-FacE%2529%2520--%2520a%2520novel%2520method%2520for%250Apredictable%2520face%2520shape%2520editing.%2520In%2520stark%2520contrast%2520to%2520existing%2520methods%2520for%2520face%250Aediting%2520using%2520trial%2520and%2520error%252C%2520edits%2520with%2520UP-FacE%2520are%2520predictable%2520by%2520the%2520human%250Auser.%2520That%2520is%252C%2520users%2520can%2520control%2520the%2520desired%2520degree%2520of%2520change%2520precisely%2520and%250Adeterministically%2520and%2520know%2520upfront%2520the%2520amount%2520of%2520change%2520required%2520to%2520achieve%2520a%250Acertain%2520editing%2520result.%2520Our%2520method%2520leverages%2520facial%2520landmarks%2520to%2520precisely%250Ameasure%2520facial%2520feature%2520values%252C%2520facilitating%2520the%2520training%2520of%2520UP-FacE%2520without%250Amanually%2520annotated%2520attribute%2520labels.%2520At%2520the%2520core%2520of%2520UP-FacE%2520is%2520a%250Atransformer-based%2520network%2520that%2520takes%2520as%2520input%2520a%2520latent%2520vector%2520from%2520a%250Apre-trained%2520generative%2520model%2520and%2520a%2520facial%2520feature%2520embedding%252C%2520and%2520predicts%2520a%250Asuitable%2520manipulation%2520vector.%2520To%2520enable%2520user-predictable%2520editing%252C%2520a%2520scaling%250Alayer%2520adjusts%2520the%2520manipulation%2520vector%2520to%2520achieve%2520the%2520precise%2520desired%2520degree%2520of%250Achange.%2520To%2520ensure%2520that%2520the%2520desired%2520feature%2520is%2520manipulated%2520towards%2520the%2520target%250Avalue%2520without%2520altering%2520uncorrelated%2520features%252C%2520we%2520further%2520introduce%2520a%2520novel%250Asemantic%2520face%2520feature%2520loss.%2520Qualitative%2520and%2520quantitative%2520results%2520demonstrate%250Athat%2520UP-FacE%2520enables%2520precise%2520and%2520fine-grained%2520control%2520over%252023%2520face%2520shape%250Afeatures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13972v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UP-FacE%3A%20User-predictable%20Fine-grained%20Face%20Shape%20Editing&entry.906535625=Florian%20Strohm%20and%20Mihai%20B%C3%A2ce%20and%20Andreas%20Bulling&entry.1292438233=%20%20We%20present%20User-predictable%20Face%20Editing%20%28UP-FacE%29%20--%20a%20novel%20method%20for%0Apredictable%20face%20shape%20editing.%20In%20stark%20contrast%20to%20existing%20methods%20for%20face%0Aediting%20using%20trial%20and%20error%2C%20edits%20with%20UP-FacE%20are%20predictable%20by%20the%20human%0Auser.%20That%20is%2C%20users%20can%20control%20the%20desired%20degree%20of%20change%20precisely%20and%0Adeterministically%20and%20know%20upfront%20the%20amount%20of%20change%20required%20to%20achieve%20a%0Acertain%20editing%20result.%20Our%20method%20leverages%20facial%20landmarks%20to%20precisely%0Ameasure%20facial%20feature%20values%2C%20facilitating%20the%20training%20of%20UP-FacE%20without%0Amanually%20annotated%20attribute%20labels.%20At%20the%20core%20of%20UP-FacE%20is%20a%0Atransformer-based%20network%20that%20takes%20as%20input%20a%20latent%20vector%20from%20a%0Apre-trained%20generative%20model%20and%20a%20facial%20feature%20embedding%2C%20and%20predicts%20a%0Asuitable%20manipulation%20vector.%20To%20enable%20user-predictable%20editing%2C%20a%20scaling%0Alayer%20adjusts%20the%20manipulation%20vector%20to%20achieve%20the%20precise%20desired%20degree%20of%0Achange.%20To%20ensure%20that%20the%20desired%20feature%20is%20manipulated%20towards%20the%20target%0Avalue%20without%20altering%20uncorrelated%20features%2C%20we%20further%20introduce%20a%20novel%0Asemantic%20face%20feature%20loss.%20Qualitative%20and%20quantitative%20results%20demonstrate%0Athat%20UP-FacE%20enables%20precise%20and%20fine-grained%20control%20over%2023%20face%20shape%0Afeatures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13972v3&entry.124074799=Read"},
{"title": "BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human\n  Racing Gameplay", "author": "Catherine Weaver and Chen Tang and Ce Hao and Kenta Kawamoto and Masayoshi Tomizuka and Wei Zhan", "abstract": "  Imitation learning learns a policy from demonstrations without requiring\nhand-designed reward functions. In many robotic tasks, such as autonomous\nracing, imitated policies must model complex environment dynamics and human\ndecision-making. Sequence modeling is highly effective in capturing intricate\npatterns of motion sequences but struggles to adapt to new environments or\ndistribution shifts that are common in real-world robotics tasks. In contrast,\nAdversarial Imitation Learning (AIL) can mitigate this effect, but struggles\nwith sample inefficiency and handling complex motion patterns. Thus, we propose\nBeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a\nBehavior Transformer (BeT) policy from human demonstrations with online AIL.\nBeTAIL adds an AIL residual policy to the BeT policy to model the sequential\ndecision-making process of human experts and correct for out-of-distribution\nstates or shifts in environment dynamics. We test BeTAIL on three challenges\nwith expert-level demonstrations of real human gameplay in Gran Turismo Sport.\nOur proposed residual BeTAIL reduces environment interactions and improves\nracing performance and stability, even when the BeT is pretrained on different\ntracks than downstream learning. Videos and code available at:\nhttps://sites.google.com/berkeley.edu/BeTAIL/home.\n", "link": "http://arxiv.org/abs/2402.14194v2", "date": "2024-07-11", "relevancy": 2.1104, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5391}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5248}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeTAIL%3A%20Behavior%20Transformer%20Adversarial%20Imitation%20Learning%20from%20Human%0A%20%20Racing%20Gameplay&body=Title%3A%20BeTAIL%3A%20Behavior%20Transformer%20Adversarial%20Imitation%20Learning%20from%20Human%0A%20%20Racing%20Gameplay%0AAuthor%3A%20Catherine%20Weaver%20and%20Chen%20Tang%20and%20Ce%20Hao%20and%20Kenta%20Kawamoto%20and%20Masayoshi%20Tomizuka%20and%20Wei%20Zhan%0AAbstract%3A%20%20%20Imitation%20learning%20learns%20a%20policy%20from%20demonstrations%20without%20requiring%0Ahand-designed%20reward%20functions.%20In%20many%20robotic%20tasks%2C%20such%20as%20autonomous%0Aracing%2C%20imitated%20policies%20must%20model%20complex%20environment%20dynamics%20and%20human%0Adecision-making.%20Sequence%20modeling%20is%20highly%20effective%20in%20capturing%20intricate%0Apatterns%20of%20motion%20sequences%20but%20struggles%20to%20adapt%20to%20new%20environments%20or%0Adistribution%20shifts%20that%20are%20common%20in%20real-world%20robotics%20tasks.%20In%20contrast%2C%0AAdversarial%20Imitation%20Learning%20%28AIL%29%20can%20mitigate%20this%20effect%2C%20but%20struggles%0Awith%20sample%20inefficiency%20and%20handling%20complex%20motion%20patterns.%20Thus%2C%20we%20propose%0ABeTAIL%3A%20Behavior%20Transformer%20Adversarial%20Imitation%20Learning%2C%20which%20combines%20a%0ABehavior%20Transformer%20%28BeT%29%20policy%20from%20human%20demonstrations%20with%20online%20AIL.%0ABeTAIL%20adds%20an%20AIL%20residual%20policy%20to%20the%20BeT%20policy%20to%20model%20the%20sequential%0Adecision-making%20process%20of%20human%20experts%20and%20correct%20for%20out-of-distribution%0Astates%20or%20shifts%20in%20environment%20dynamics.%20We%20test%20BeTAIL%20on%20three%20challenges%0Awith%20expert-level%20demonstrations%20of%20real%20human%20gameplay%20in%20Gran%20Turismo%20Sport.%0AOur%20proposed%20residual%20BeTAIL%20reduces%20environment%20interactions%20and%20improves%0Aracing%20performance%20and%20stability%2C%20even%20when%20the%20BeT%20is%20pretrained%20on%20different%0Atracks%20than%20downstream%20learning.%20Videos%20and%20code%20available%20at%3A%0Ahttps%3A//sites.google.com/berkeley.edu/BeTAIL/home.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14194v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeTAIL%253A%2520Behavior%2520Transformer%2520Adversarial%2520Imitation%2520Learning%2520from%2520Human%250A%2520%2520Racing%2520Gameplay%26entry.906535625%3DCatherine%2520Weaver%2520and%2520Chen%2520Tang%2520and%2520Ce%2520Hao%2520and%2520Kenta%2520Kawamoto%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Wei%2520Zhan%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520learns%2520a%2520policy%2520from%2520demonstrations%2520without%2520requiring%250Ahand-designed%2520reward%2520functions.%2520In%2520many%2520robotic%2520tasks%252C%2520such%2520as%2520autonomous%250Aracing%252C%2520imitated%2520policies%2520must%2520model%2520complex%2520environment%2520dynamics%2520and%2520human%250Adecision-making.%2520Sequence%2520modeling%2520is%2520highly%2520effective%2520in%2520capturing%2520intricate%250Apatterns%2520of%2520motion%2520sequences%2520but%2520struggles%2520to%2520adapt%2520to%2520new%2520environments%2520or%250Adistribution%2520shifts%2520that%2520are%2520common%2520in%2520real-world%2520robotics%2520tasks.%2520In%2520contrast%252C%250AAdversarial%2520Imitation%2520Learning%2520%2528AIL%2529%2520can%2520mitigate%2520this%2520effect%252C%2520but%2520struggles%250Awith%2520sample%2520inefficiency%2520and%2520handling%2520complex%2520motion%2520patterns.%2520Thus%252C%2520we%2520propose%250ABeTAIL%253A%2520Behavior%2520Transformer%2520Adversarial%2520Imitation%2520Learning%252C%2520which%2520combines%2520a%250ABehavior%2520Transformer%2520%2528BeT%2529%2520policy%2520from%2520human%2520demonstrations%2520with%2520online%2520AIL.%250ABeTAIL%2520adds%2520an%2520AIL%2520residual%2520policy%2520to%2520the%2520BeT%2520policy%2520to%2520model%2520the%2520sequential%250Adecision-making%2520process%2520of%2520human%2520experts%2520and%2520correct%2520for%2520out-of-distribution%250Astates%2520or%2520shifts%2520in%2520environment%2520dynamics.%2520We%2520test%2520BeTAIL%2520on%2520three%2520challenges%250Awith%2520expert-level%2520demonstrations%2520of%2520real%2520human%2520gameplay%2520in%2520Gran%2520Turismo%2520Sport.%250AOur%2520proposed%2520residual%2520BeTAIL%2520reduces%2520environment%2520interactions%2520and%2520improves%250Aracing%2520performance%2520and%2520stability%252C%2520even%2520when%2520the%2520BeT%2520is%2520pretrained%2520on%2520different%250Atracks%2520than%2520downstream%2520learning.%2520Videos%2520and%2520code%2520available%2520at%253A%250Ahttps%253A//sites.google.com/berkeley.edu/BeTAIL/home.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14194v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeTAIL%3A%20Behavior%20Transformer%20Adversarial%20Imitation%20Learning%20from%20Human%0A%20%20Racing%20Gameplay&entry.906535625=Catherine%20Weaver%20and%20Chen%20Tang%20and%20Ce%20Hao%20and%20Kenta%20Kawamoto%20and%20Masayoshi%20Tomizuka%20and%20Wei%20Zhan&entry.1292438233=%20%20Imitation%20learning%20learns%20a%20policy%20from%20demonstrations%20without%20requiring%0Ahand-designed%20reward%20functions.%20In%20many%20robotic%20tasks%2C%20such%20as%20autonomous%0Aracing%2C%20imitated%20policies%20must%20model%20complex%20environment%20dynamics%20and%20human%0Adecision-making.%20Sequence%20modeling%20is%20highly%20effective%20in%20capturing%20intricate%0Apatterns%20of%20motion%20sequences%20but%20struggles%20to%20adapt%20to%20new%20environments%20or%0Adistribution%20shifts%20that%20are%20common%20in%20real-world%20robotics%20tasks.%20In%20contrast%2C%0AAdversarial%20Imitation%20Learning%20%28AIL%29%20can%20mitigate%20this%20effect%2C%20but%20struggles%0Awith%20sample%20inefficiency%20and%20handling%20complex%20motion%20patterns.%20Thus%2C%20we%20propose%0ABeTAIL%3A%20Behavior%20Transformer%20Adversarial%20Imitation%20Learning%2C%20which%20combines%20a%0ABehavior%20Transformer%20%28BeT%29%20policy%20from%20human%20demonstrations%20with%20online%20AIL.%0ABeTAIL%20adds%20an%20AIL%20residual%20policy%20to%20the%20BeT%20policy%20to%20model%20the%20sequential%0Adecision-making%20process%20of%20human%20experts%20and%20correct%20for%20out-of-distribution%0Astates%20or%20shifts%20in%20environment%20dynamics.%20We%20test%20BeTAIL%20on%20three%20challenges%0Awith%20expert-level%20demonstrations%20of%20real%20human%20gameplay%20in%20Gran%20Turismo%20Sport.%0AOur%20proposed%20residual%20BeTAIL%20reduces%20environment%20interactions%20and%20improves%0Aracing%20performance%20and%20stability%2C%20even%20when%20the%20BeT%20is%20pretrained%20on%20different%0Atracks%20than%20downstream%20learning.%20Videos%20and%20code%20available%20at%3A%0Ahttps%3A//sites.google.com/berkeley.edu/BeTAIL/home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14194v2&entry.124074799=Read"},
{"title": "Trainable Highly-expressive Activation Functions", "author": "Irit Chelly and Shahaf E. Finder and Shira Ifergane and Oren Freifeld", "abstract": "  Nonlinear activation functions are pivotal to the success of deep neural\nnets, and choosing the appropriate activation function can significantly affect\ntheir performance. Most networks use fixed activation functions (e.g., ReLU,\nGELU, etc.), and this choice might limit their expressiveness. Furthermore,\ndifferent layers may benefit from diverse activation functions. Consequently,\nthere has been a growing interest in trainable activation functions. In this\npaper, we introduce DiTAC, a trainable highly-expressive activation function\nbased on an efficient diffeomorphic transformation (called CPAB). Despite\nintroducing only a negligible number of trainable parameters, DiTAC enhances\nmodel expressiveness and performance, often yielding substantial improvements.\nIt also outperforms existing activation functions (regardless whether the\nlatter are fixed or trainable) in tasks such as semantic segmentation, image\ngeneration, regression problems, and image classification. Our code is\navailable at https://github.com/BGU-CS-VIL/DiTAC.\n", "link": "http://arxiv.org/abs/2407.07564v2", "date": "2024-07-11", "relevancy": 2.1046, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.532}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5282}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trainable%20Highly-expressive%20Activation%20Functions&body=Title%3A%20Trainable%20Highly-expressive%20Activation%20Functions%0AAuthor%3A%20Irit%20Chelly%20and%20Shahaf%20E.%20Finder%20and%20Shira%20Ifergane%20and%20Oren%20Freifeld%0AAbstract%3A%20%20%20Nonlinear%20activation%20functions%20are%20pivotal%20to%20the%20success%20of%20deep%20neural%0Anets%2C%20and%20choosing%20the%20appropriate%20activation%20function%20can%20significantly%20affect%0Atheir%20performance.%20Most%20networks%20use%20fixed%20activation%20functions%20%28e.g.%2C%20ReLU%2C%0AGELU%2C%20etc.%29%2C%20and%20this%20choice%20might%20limit%20their%20expressiveness.%20Furthermore%2C%0Adifferent%20layers%20may%20benefit%20from%20diverse%20activation%20functions.%20Consequently%2C%0Athere%20has%20been%20a%20growing%20interest%20in%20trainable%20activation%20functions.%20In%20this%0Apaper%2C%20we%20introduce%20DiTAC%2C%20a%20trainable%20highly-expressive%20activation%20function%0Abased%20on%20an%20efficient%20diffeomorphic%20transformation%20%28called%20CPAB%29.%20Despite%0Aintroducing%20only%20a%20negligible%20number%20of%20trainable%20parameters%2C%20DiTAC%20enhances%0Amodel%20expressiveness%20and%20performance%2C%20often%20yielding%20substantial%20improvements.%0AIt%20also%20outperforms%20existing%20activation%20functions%20%28regardless%20whether%20the%0Alatter%20are%20fixed%20or%20trainable%29%20in%20tasks%20such%20as%20semantic%20segmentation%2C%20image%0Ageneration%2C%20regression%20problems%2C%20and%20image%20classification.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/BGU-CS-VIL/DiTAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07564v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrainable%2520Highly-expressive%2520Activation%2520Functions%26entry.906535625%3DIrit%2520Chelly%2520and%2520Shahaf%2520E.%2520Finder%2520and%2520Shira%2520Ifergane%2520and%2520Oren%2520Freifeld%26entry.1292438233%3D%2520%2520Nonlinear%2520activation%2520functions%2520are%2520pivotal%2520to%2520the%2520success%2520of%2520deep%2520neural%250Anets%252C%2520and%2520choosing%2520the%2520appropriate%2520activation%2520function%2520can%2520significantly%2520affect%250Atheir%2520performance.%2520Most%2520networks%2520use%2520fixed%2520activation%2520functions%2520%2528e.g.%252C%2520ReLU%252C%250AGELU%252C%2520etc.%2529%252C%2520and%2520this%2520choice%2520might%2520limit%2520their%2520expressiveness.%2520Furthermore%252C%250Adifferent%2520layers%2520may%2520benefit%2520from%2520diverse%2520activation%2520functions.%2520Consequently%252C%250Athere%2520has%2520been%2520a%2520growing%2520interest%2520in%2520trainable%2520activation%2520functions.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520DiTAC%252C%2520a%2520trainable%2520highly-expressive%2520activation%2520function%250Abased%2520on%2520an%2520efficient%2520diffeomorphic%2520transformation%2520%2528called%2520CPAB%2529.%2520Despite%250Aintroducing%2520only%2520a%2520negligible%2520number%2520of%2520trainable%2520parameters%252C%2520DiTAC%2520enhances%250Amodel%2520expressiveness%2520and%2520performance%252C%2520often%2520yielding%2520substantial%2520improvements.%250AIt%2520also%2520outperforms%2520existing%2520activation%2520functions%2520%2528regardless%2520whether%2520the%250Alatter%2520are%2520fixed%2520or%2520trainable%2529%2520in%2520tasks%2520such%2520as%2520semantic%2520segmentation%252C%2520image%250Ageneration%252C%2520regression%2520problems%252C%2520and%2520image%2520classification.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/BGU-CS-VIL/DiTAC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07564v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trainable%20Highly-expressive%20Activation%20Functions&entry.906535625=Irit%20Chelly%20and%20Shahaf%20E.%20Finder%20and%20Shira%20Ifergane%20and%20Oren%20Freifeld&entry.1292438233=%20%20Nonlinear%20activation%20functions%20are%20pivotal%20to%20the%20success%20of%20deep%20neural%0Anets%2C%20and%20choosing%20the%20appropriate%20activation%20function%20can%20significantly%20affect%0Atheir%20performance.%20Most%20networks%20use%20fixed%20activation%20functions%20%28e.g.%2C%20ReLU%2C%0AGELU%2C%20etc.%29%2C%20and%20this%20choice%20might%20limit%20their%20expressiveness.%20Furthermore%2C%0Adifferent%20layers%20may%20benefit%20from%20diverse%20activation%20functions.%20Consequently%2C%0Athere%20has%20been%20a%20growing%20interest%20in%20trainable%20activation%20functions.%20In%20this%0Apaper%2C%20we%20introduce%20DiTAC%2C%20a%20trainable%20highly-expressive%20activation%20function%0Abased%20on%20an%20efficient%20diffeomorphic%20transformation%20%28called%20CPAB%29.%20Despite%0Aintroducing%20only%20a%20negligible%20number%20of%20trainable%20parameters%2C%20DiTAC%20enhances%0Amodel%20expressiveness%20and%20performance%2C%20often%20yielding%20substantial%20improvements.%0AIt%20also%20outperforms%20existing%20activation%20functions%20%28regardless%20whether%20the%0Alatter%20are%20fixed%20or%20trainable%29%20in%20tasks%20such%20as%20semantic%20segmentation%2C%20image%0Ageneration%2C%20regression%20problems%2C%20and%20image%20classification.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/BGU-CS-VIL/DiTAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07564v2&entry.124074799=Read"},
{"title": "SPOCKMIP: Segmentation of Vessels in MRAs with Enhanced Continuity using\n  Maximum Intensity Projection as Loss", "author": "Chethan Radhakrishna and Karthikesh Varma Chintalapati and Sri Chandana Hudukula Ram Kumar and Raviteja Sutrave and Hendrik Mattern and Oliver Speck and Andreas N\u00fcrnberger and Soumick Chatterjee", "abstract": "  Identification of vessel structures of different sizes in biomedical images\nis crucial in the diagnosis of many neurodegenerative diseases. However, the\nsparsity of good-quality annotations of such images makes the task of vessel\nsegmentation challenging. Deep learning offers an efficient way to segment\nvessels of different sizes by learning their high-level feature representations\nand the spatial continuity of such features across dimensions. Semi-supervised\npatch-based approaches have been effective in identifying small vessels of one\nto two voxels in diameter. This study focuses on improving the segmentation\nquality by considering the spatial correlation of the features using the\nMaximum Intensity Projection~(MIP) as an additional loss criterion. Two methods\nare proposed with the incorporation of MIPs of label segmentation on the\nsingle~(z-axis) and multiple perceivable axes of the 3D volume. The proposed\nMIP-based methods produce segmentations with improved vessel continuity, which\nis evident in visual examinations of ROIs. Patch-based training is improved by\nintroducing an additional loss term, MIP loss, to penalise the predicted\ndiscontinuity of vessels. A training set of 14 volumes is selected from the\nStudyForrest dataset comprising of 18 7-Tesla 3D Time-of-Flight~(ToF) Magnetic\nResonance Angiography (MRA) images. The generalisation performance of the\nmethod is evaluated using the other unseen volumes in the dataset. It is\nobserved that the proposed method with multi-axes MIP loss produces better\nquality segmentations with a median Dice of $80.245 \\pm 0.129$. Also, the\nmethod with single-axis MIP loss produces segmentations with a median Dice of\n$79.749 \\pm 0.109$. Furthermore, a visual comparison of the ROIs in the\npredicted segmentation reveals a significant improvement in the continuity of\nthe vessels when MIP loss is incorporated into training.\n", "link": "http://arxiv.org/abs/2407.08655v1", "date": "2024-07-11", "relevancy": 2.1009, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5096}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPOCKMIP%3A%20Segmentation%20of%20Vessels%20in%20MRAs%20with%20Enhanced%20Continuity%20using%0A%20%20Maximum%20Intensity%20Projection%20as%20Loss&body=Title%3A%20SPOCKMIP%3A%20Segmentation%20of%20Vessels%20in%20MRAs%20with%20Enhanced%20Continuity%20using%0A%20%20Maximum%20Intensity%20Projection%20as%20Loss%0AAuthor%3A%20Chethan%20Radhakrishna%20and%20Karthikesh%20Varma%20Chintalapati%20and%20Sri%20Chandana%20Hudukula%20Ram%20Kumar%20and%20Raviteja%20Sutrave%20and%20Hendrik%20Mattern%20and%20Oliver%20Speck%20and%20Andreas%20N%C3%BCrnberger%20and%20Soumick%20Chatterjee%0AAbstract%3A%20%20%20Identification%20of%20vessel%20structures%20of%20different%20sizes%20in%20biomedical%20images%0Ais%20crucial%20in%20the%20diagnosis%20of%20many%20neurodegenerative%20diseases.%20However%2C%20the%0Asparsity%20of%20good-quality%20annotations%20of%20such%20images%20makes%20the%20task%20of%20vessel%0Asegmentation%20challenging.%20Deep%20learning%20offers%20an%20efficient%20way%20to%20segment%0Avessels%20of%20different%20sizes%20by%20learning%20their%20high-level%20feature%20representations%0Aand%20the%20spatial%20continuity%20of%20such%20features%20across%20dimensions.%20Semi-supervised%0Apatch-based%20approaches%20have%20been%20effective%20in%20identifying%20small%20vessels%20of%20one%0Ato%20two%20voxels%20in%20diameter.%20This%20study%20focuses%20on%20improving%20the%20segmentation%0Aquality%20by%20considering%20the%20spatial%20correlation%20of%20the%20features%20using%20the%0AMaximum%20Intensity%20Projection~%28MIP%29%20as%20an%20additional%20loss%20criterion.%20Two%20methods%0Aare%20proposed%20with%20the%20incorporation%20of%20MIPs%20of%20label%20segmentation%20on%20the%0Asingle~%28z-axis%29%20and%20multiple%20perceivable%20axes%20of%20the%203D%20volume.%20The%20proposed%0AMIP-based%20methods%20produce%20segmentations%20with%20improved%20vessel%20continuity%2C%20which%0Ais%20evident%20in%20visual%20examinations%20of%20ROIs.%20Patch-based%20training%20is%20improved%20by%0Aintroducing%20an%20additional%20loss%20term%2C%20MIP%20loss%2C%20to%20penalise%20the%20predicted%0Adiscontinuity%20of%20vessels.%20A%20training%20set%20of%2014%20volumes%20is%20selected%20from%20the%0AStudyForrest%20dataset%20comprising%20of%2018%207-Tesla%203D%20Time-of-Flight~%28ToF%29%20Magnetic%0AResonance%20Angiography%20%28MRA%29%20images.%20The%20generalisation%20performance%20of%20the%0Amethod%20is%20evaluated%20using%20the%20other%20unseen%20volumes%20in%20the%20dataset.%20It%20is%0Aobserved%20that%20the%20proposed%20method%20with%20multi-axes%20MIP%20loss%20produces%20better%0Aquality%20segmentations%20with%20a%20median%20Dice%20of%20%2480.245%20%5Cpm%200.129%24.%20Also%2C%20the%0Amethod%20with%20single-axis%20MIP%20loss%20produces%20segmentations%20with%20a%20median%20Dice%20of%0A%2479.749%20%5Cpm%200.109%24.%20Furthermore%2C%20a%20visual%20comparison%20of%20the%20ROIs%20in%20the%0Apredicted%20segmentation%20reveals%20a%20significant%20improvement%20in%20the%20continuity%20of%0Athe%20vessels%20when%20MIP%20loss%20is%20incorporated%20into%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPOCKMIP%253A%2520Segmentation%2520of%2520Vessels%2520in%2520MRAs%2520with%2520Enhanced%2520Continuity%2520using%250A%2520%2520Maximum%2520Intensity%2520Projection%2520as%2520Loss%26entry.906535625%3DChethan%2520Radhakrishna%2520and%2520Karthikesh%2520Varma%2520Chintalapati%2520and%2520Sri%2520Chandana%2520Hudukula%2520Ram%2520Kumar%2520and%2520Raviteja%2520Sutrave%2520and%2520Hendrik%2520Mattern%2520and%2520Oliver%2520Speck%2520and%2520Andreas%2520N%25C3%25BCrnberger%2520and%2520Soumick%2520Chatterjee%26entry.1292438233%3D%2520%2520Identification%2520of%2520vessel%2520structures%2520of%2520different%2520sizes%2520in%2520biomedical%2520images%250Ais%2520crucial%2520in%2520the%2520diagnosis%2520of%2520many%2520neurodegenerative%2520diseases.%2520However%252C%2520the%250Asparsity%2520of%2520good-quality%2520annotations%2520of%2520such%2520images%2520makes%2520the%2520task%2520of%2520vessel%250Asegmentation%2520challenging.%2520Deep%2520learning%2520offers%2520an%2520efficient%2520way%2520to%2520segment%250Avessels%2520of%2520different%2520sizes%2520by%2520learning%2520their%2520high-level%2520feature%2520representations%250Aand%2520the%2520spatial%2520continuity%2520of%2520such%2520features%2520across%2520dimensions.%2520Semi-supervised%250Apatch-based%2520approaches%2520have%2520been%2520effective%2520in%2520identifying%2520small%2520vessels%2520of%2520one%250Ato%2520two%2520voxels%2520in%2520diameter.%2520This%2520study%2520focuses%2520on%2520improving%2520the%2520segmentation%250Aquality%2520by%2520considering%2520the%2520spatial%2520correlation%2520of%2520the%2520features%2520using%2520the%250AMaximum%2520Intensity%2520Projection~%2528MIP%2529%2520as%2520an%2520additional%2520loss%2520criterion.%2520Two%2520methods%250Aare%2520proposed%2520with%2520the%2520incorporation%2520of%2520MIPs%2520of%2520label%2520segmentation%2520on%2520the%250Asingle~%2528z-axis%2529%2520and%2520multiple%2520perceivable%2520axes%2520of%2520the%25203D%2520volume.%2520The%2520proposed%250AMIP-based%2520methods%2520produce%2520segmentations%2520with%2520improved%2520vessel%2520continuity%252C%2520which%250Ais%2520evident%2520in%2520visual%2520examinations%2520of%2520ROIs.%2520Patch-based%2520training%2520is%2520improved%2520by%250Aintroducing%2520an%2520additional%2520loss%2520term%252C%2520MIP%2520loss%252C%2520to%2520penalise%2520the%2520predicted%250Adiscontinuity%2520of%2520vessels.%2520A%2520training%2520set%2520of%252014%2520volumes%2520is%2520selected%2520from%2520the%250AStudyForrest%2520dataset%2520comprising%2520of%252018%25207-Tesla%25203D%2520Time-of-Flight~%2528ToF%2529%2520Magnetic%250AResonance%2520Angiography%2520%2528MRA%2529%2520images.%2520The%2520generalisation%2520performance%2520of%2520the%250Amethod%2520is%2520evaluated%2520using%2520the%2520other%2520unseen%2520volumes%2520in%2520the%2520dataset.%2520It%2520is%250Aobserved%2520that%2520the%2520proposed%2520method%2520with%2520multi-axes%2520MIP%2520loss%2520produces%2520better%250Aquality%2520segmentations%2520with%2520a%2520median%2520Dice%2520of%2520%252480.245%2520%255Cpm%25200.129%2524.%2520Also%252C%2520the%250Amethod%2520with%2520single-axis%2520MIP%2520loss%2520produces%2520segmentations%2520with%2520a%2520median%2520Dice%2520of%250A%252479.749%2520%255Cpm%25200.109%2524.%2520Furthermore%252C%2520a%2520visual%2520comparison%2520of%2520the%2520ROIs%2520in%2520the%250Apredicted%2520segmentation%2520reveals%2520a%2520significant%2520improvement%2520in%2520the%2520continuity%2520of%250Athe%2520vessels%2520when%2520MIP%2520loss%2520is%2520incorporated%2520into%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPOCKMIP%3A%20Segmentation%20of%20Vessels%20in%20MRAs%20with%20Enhanced%20Continuity%20using%0A%20%20Maximum%20Intensity%20Projection%20as%20Loss&entry.906535625=Chethan%20Radhakrishna%20and%20Karthikesh%20Varma%20Chintalapati%20and%20Sri%20Chandana%20Hudukula%20Ram%20Kumar%20and%20Raviteja%20Sutrave%20and%20Hendrik%20Mattern%20and%20Oliver%20Speck%20and%20Andreas%20N%C3%BCrnberger%20and%20Soumick%20Chatterjee&entry.1292438233=%20%20Identification%20of%20vessel%20structures%20of%20different%20sizes%20in%20biomedical%20images%0Ais%20crucial%20in%20the%20diagnosis%20of%20many%20neurodegenerative%20diseases.%20However%2C%20the%0Asparsity%20of%20good-quality%20annotations%20of%20such%20images%20makes%20the%20task%20of%20vessel%0Asegmentation%20challenging.%20Deep%20learning%20offers%20an%20efficient%20way%20to%20segment%0Avessels%20of%20different%20sizes%20by%20learning%20their%20high-level%20feature%20representations%0Aand%20the%20spatial%20continuity%20of%20such%20features%20across%20dimensions.%20Semi-supervised%0Apatch-based%20approaches%20have%20been%20effective%20in%20identifying%20small%20vessels%20of%20one%0Ato%20two%20voxels%20in%20diameter.%20This%20study%20focuses%20on%20improving%20the%20segmentation%0Aquality%20by%20considering%20the%20spatial%20correlation%20of%20the%20features%20using%20the%0AMaximum%20Intensity%20Projection~%28MIP%29%20as%20an%20additional%20loss%20criterion.%20Two%20methods%0Aare%20proposed%20with%20the%20incorporation%20of%20MIPs%20of%20label%20segmentation%20on%20the%0Asingle~%28z-axis%29%20and%20multiple%20perceivable%20axes%20of%20the%203D%20volume.%20The%20proposed%0AMIP-based%20methods%20produce%20segmentations%20with%20improved%20vessel%20continuity%2C%20which%0Ais%20evident%20in%20visual%20examinations%20of%20ROIs.%20Patch-based%20training%20is%20improved%20by%0Aintroducing%20an%20additional%20loss%20term%2C%20MIP%20loss%2C%20to%20penalise%20the%20predicted%0Adiscontinuity%20of%20vessels.%20A%20training%20set%20of%2014%20volumes%20is%20selected%20from%20the%0AStudyForrest%20dataset%20comprising%20of%2018%207-Tesla%203D%20Time-of-Flight~%28ToF%29%20Magnetic%0AResonance%20Angiography%20%28MRA%29%20images.%20The%20generalisation%20performance%20of%20the%0Amethod%20is%20evaluated%20using%20the%20other%20unseen%20volumes%20in%20the%20dataset.%20It%20is%0Aobserved%20that%20the%20proposed%20method%20with%20multi-axes%20MIP%20loss%20produces%20better%0Aquality%20segmentations%20with%20a%20median%20Dice%20of%20%2480.245%20%5Cpm%200.129%24.%20Also%2C%20the%0Amethod%20with%20single-axis%20MIP%20loss%20produces%20segmentations%20with%20a%20median%20Dice%20of%0A%2479.749%20%5Cpm%200.109%24.%20Furthermore%2C%20a%20visual%20comparison%20of%20the%20ROIs%20in%20the%0Apredicted%20segmentation%20reveals%20a%20significant%20improvement%20in%20the%20continuity%20of%0Athe%20vessels%20when%20MIP%20loss%20is%20incorporated%20into%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08655v1&entry.124074799=Read"},
{"title": "Uncertainty Estimation of Large Language Models in Medical Question\n  Answering", "author": "Jiaxin Wu and Yizhou Yu and Hong-Yu Zhou", "abstract": "  Large Language Models (LLMs) show promise for natural language generation in\nhealthcare, but risk hallucinating factually incorrect information. Deploying\nLLMs for medical question answering necessitates reliable uncertainty\nestimation (UE) methods to detect hallucinations. In this work, we benchmark\npopular UE methods with different model sizes on medical question-answering\ndatasets. Our results show that current approaches generally perform poorly in\nthis domain, highlighting the challenge of UE for medical applications. We also\nobserve that larger models tend to yield better results, suggesting a\ncorrelation between model size and the reliability of UE. To address these\nchallenges, we propose Two-phase Verification, a probability-free Uncertainty\nEstimation approach. First, an LLM generates a step-by-step explanation\nalongside its initial answer, followed by formulating verification questions to\ncheck the factual claims in the explanation. The model then answers these\nquestions twice: first independently, and then referencing the explanation.\nInconsistencies between the two sets of answers measure the uncertainty in the\noriginal response. We evaluate our approach on three biomedical\nquestion-answering datasets using Llama 2 Chat models and compare it against\nthe benchmarked baseline methods. The results show that our Two-phase\nVerification method achieves the best overall accuracy and stability across\nvarious datasets and model sizes, and its performance scales as the model size\nincreases.\n", "link": "http://arxiv.org/abs/2407.08662v1", "date": "2024-07-11", "relevancy": 2.0975, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6427}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5173}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Estimation%20of%20Large%20Language%20Models%20in%20Medical%20Question%0A%20%20Answering&body=Title%3A%20Uncertainty%20Estimation%20of%20Large%20Language%20Models%20in%20Medical%20Question%0A%20%20Answering%0AAuthor%3A%20Jiaxin%20Wu%20and%20Yizhou%20Yu%20and%20Hong-Yu%20Zhou%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20show%20promise%20for%20natural%20language%20generation%20in%0Ahealthcare%2C%20but%20risk%20hallucinating%20factually%20incorrect%20information.%20Deploying%0ALLMs%20for%20medical%20question%20answering%20necessitates%20reliable%20uncertainty%0Aestimation%20%28UE%29%20methods%20to%20detect%20hallucinations.%20In%20this%20work%2C%20we%20benchmark%0Apopular%20UE%20methods%20with%20different%20model%20sizes%20on%20medical%20question-answering%0Adatasets.%20Our%20results%20show%20that%20current%20approaches%20generally%20perform%20poorly%20in%0Athis%20domain%2C%20highlighting%20the%20challenge%20of%20UE%20for%20medical%20applications.%20We%20also%0Aobserve%20that%20larger%20models%20tend%20to%20yield%20better%20results%2C%20suggesting%20a%0Acorrelation%20between%20model%20size%20and%20the%20reliability%20of%20UE.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Two-phase%20Verification%2C%20a%20probability-free%20Uncertainty%0AEstimation%20approach.%20First%2C%20an%20LLM%20generates%20a%20step-by-step%20explanation%0Aalongside%20its%20initial%20answer%2C%20followed%20by%20formulating%20verification%20questions%20to%0Acheck%20the%20factual%20claims%20in%20the%20explanation.%20The%20model%20then%20answers%20these%0Aquestions%20twice%3A%20first%20independently%2C%20and%20then%20referencing%20the%20explanation.%0AInconsistencies%20between%20the%20two%20sets%20of%20answers%20measure%20the%20uncertainty%20in%20the%0Aoriginal%20response.%20We%20evaluate%20our%20approach%20on%20three%20biomedical%0Aquestion-answering%20datasets%20using%20Llama%202%20Chat%20models%20and%20compare%20it%20against%0Athe%20benchmarked%20baseline%20methods.%20The%20results%20show%20that%20our%20Two-phase%0AVerification%20method%20achieves%20the%20best%20overall%20accuracy%20and%20stability%20across%0Avarious%20datasets%20and%20model%20sizes%2C%20and%20its%20performance%20scales%20as%20the%20model%20size%0Aincreases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Estimation%2520of%2520Large%2520Language%2520Models%2520in%2520Medical%2520Question%250A%2520%2520Answering%26entry.906535625%3DJiaxin%2520Wu%2520and%2520Yizhou%2520Yu%2520and%2520Hong-Yu%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520promise%2520for%2520natural%2520language%2520generation%2520in%250Ahealthcare%252C%2520but%2520risk%2520hallucinating%2520factually%2520incorrect%2520information.%2520Deploying%250ALLMs%2520for%2520medical%2520question%2520answering%2520necessitates%2520reliable%2520uncertainty%250Aestimation%2520%2528UE%2529%2520methods%2520to%2520detect%2520hallucinations.%2520In%2520this%2520work%252C%2520we%2520benchmark%250Apopular%2520UE%2520methods%2520with%2520different%2520model%2520sizes%2520on%2520medical%2520question-answering%250Adatasets.%2520Our%2520results%2520show%2520that%2520current%2520approaches%2520generally%2520perform%2520poorly%2520in%250Athis%2520domain%252C%2520highlighting%2520the%2520challenge%2520of%2520UE%2520for%2520medical%2520applications.%2520We%2520also%250Aobserve%2520that%2520larger%2520models%2520tend%2520to%2520yield%2520better%2520results%252C%2520suggesting%2520a%250Acorrelation%2520between%2520model%2520size%2520and%2520the%2520reliability%2520of%2520UE.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520Two-phase%2520Verification%252C%2520a%2520probability-free%2520Uncertainty%250AEstimation%2520approach.%2520First%252C%2520an%2520LLM%2520generates%2520a%2520step-by-step%2520explanation%250Aalongside%2520its%2520initial%2520answer%252C%2520followed%2520by%2520formulating%2520verification%2520questions%2520to%250Acheck%2520the%2520factual%2520claims%2520in%2520the%2520explanation.%2520The%2520model%2520then%2520answers%2520these%250Aquestions%2520twice%253A%2520first%2520independently%252C%2520and%2520then%2520referencing%2520the%2520explanation.%250AInconsistencies%2520between%2520the%2520two%2520sets%2520of%2520answers%2520measure%2520the%2520uncertainty%2520in%2520the%250Aoriginal%2520response.%2520We%2520evaluate%2520our%2520approach%2520on%2520three%2520biomedical%250Aquestion-answering%2520datasets%2520using%2520Llama%25202%2520Chat%2520models%2520and%2520compare%2520it%2520against%250Athe%2520benchmarked%2520baseline%2520methods.%2520The%2520results%2520show%2520that%2520our%2520Two-phase%250AVerification%2520method%2520achieves%2520the%2520best%2520overall%2520accuracy%2520and%2520stability%2520across%250Avarious%2520datasets%2520and%2520model%2520sizes%252C%2520and%2520its%2520performance%2520scales%2520as%2520the%2520model%2520size%250Aincreases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Estimation%20of%20Large%20Language%20Models%20in%20Medical%20Question%0A%20%20Answering&entry.906535625=Jiaxin%20Wu%20and%20Yizhou%20Yu%20and%20Hong-Yu%20Zhou&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20show%20promise%20for%20natural%20language%20generation%20in%0Ahealthcare%2C%20but%20risk%20hallucinating%20factually%20incorrect%20information.%20Deploying%0ALLMs%20for%20medical%20question%20answering%20necessitates%20reliable%20uncertainty%0Aestimation%20%28UE%29%20methods%20to%20detect%20hallucinations.%20In%20this%20work%2C%20we%20benchmark%0Apopular%20UE%20methods%20with%20different%20model%20sizes%20on%20medical%20question-answering%0Adatasets.%20Our%20results%20show%20that%20current%20approaches%20generally%20perform%20poorly%20in%0Athis%20domain%2C%20highlighting%20the%20challenge%20of%20UE%20for%20medical%20applications.%20We%20also%0Aobserve%20that%20larger%20models%20tend%20to%20yield%20better%20results%2C%20suggesting%20a%0Acorrelation%20between%20model%20size%20and%20the%20reliability%20of%20UE.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Two-phase%20Verification%2C%20a%20probability-free%20Uncertainty%0AEstimation%20approach.%20First%2C%20an%20LLM%20generates%20a%20step-by-step%20explanation%0Aalongside%20its%20initial%20answer%2C%20followed%20by%20formulating%20verification%20questions%20to%0Acheck%20the%20factual%20claims%20in%20the%20explanation.%20The%20model%20then%20answers%20these%0Aquestions%20twice%3A%20first%20independently%2C%20and%20then%20referencing%20the%20explanation.%0AInconsistencies%20between%20the%20two%20sets%20of%20answers%20measure%20the%20uncertainty%20in%20the%0Aoriginal%20response.%20We%20evaluate%20our%20approach%20on%20three%20biomedical%0Aquestion-answering%20datasets%20using%20Llama%202%20Chat%20models%20and%20compare%20it%20against%0Athe%20benchmarked%20baseline%20methods.%20The%20results%20show%20that%20our%20Two-phase%0AVerification%20method%20achieves%20the%20best%20overall%20accuracy%20and%20stability%20across%0Avarious%20datasets%20and%20model%20sizes%2C%20and%20its%20performance%20scales%20as%20the%20model%20size%0Aincreases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08662v1&entry.124074799=Read"},
{"title": "Neural Poisson Solver: A Universal and Continuous Framework for Natural\n  Signal Blending", "author": "Delong Wu and Hao Zhu and Qi Zhang and You Li and Zhan Ma and Xun Cao", "abstract": "  Implicit Neural Representation (INR) has become a popular method for\nrepresenting visual signals (e.g., 2D images and 3D scenes), demonstrating\npromising results in various downstream applications. Given its potential as a\nmedium for visual signals, exploring the development of a neural blending\nmethod that utilizes INRs is a natural progression. Neural blending involves\nmerging two INRs to create a new INR that encapsulates information from both\noriginal representations. A direct approach involves applying traditional image\nediting methods to the INR rendering process. However, this method often\nresults in blending distortions, artifacts, and color shifts, primarily due to\nthe discretization of the underlying pixel grid and the introduction of\nboundary conditions for solving variational problems. To tackle this issue, we\nintroduce the Neural Poisson Solver, a plug-and-play and universally applicable\nframework across different signal dimensions for blending visual signals\nrepresented by INRs. Our Neural Poisson Solver offers a variational\nproblem-solving approach based on the continuous Poisson equation,\ndemonstrating exceptional performance across various domains. Specifically, we\npropose a gradient-guided neural solver to represent the solution process of\nthe variational problem, refining the target signal to achieve natural blending\nresults. We also develop a Poisson equation-based loss and optimization scheme\nto train our solver, ensuring it effectively blends the input INR scenes while\npreserving their inherent structure and semantic content. The lack of\ndependence on additional prior knowledge makes our method easily adaptable to\nvarious task categories, highlighting its versatility. Comprehensive\nexperimental results validate the robustness of our approach across multiple\ndimensions and blending tasks.\n", "link": "http://arxiv.org/abs/2407.08457v1", "date": "2024-07-11", "relevancy": 2.0937, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5323}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5181}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Poisson%20Solver%3A%20A%20Universal%20and%20Continuous%20Framework%20for%20Natural%0A%20%20Signal%20Blending&body=Title%3A%20Neural%20Poisson%20Solver%3A%20A%20Universal%20and%20Continuous%20Framework%20for%20Natural%0A%20%20Signal%20Blending%0AAuthor%3A%20Delong%20Wu%20and%20Hao%20Zhu%20and%20Qi%20Zhang%20and%20You%20Li%20and%20Zhan%20Ma%20and%20Xun%20Cao%0AAbstract%3A%20%20%20Implicit%20Neural%20Representation%20%28INR%29%20has%20become%20a%20popular%20method%20for%0Arepresenting%20visual%20signals%20%28e.g.%2C%202D%20images%20and%203D%20scenes%29%2C%20demonstrating%0Apromising%20results%20in%20various%20downstream%20applications.%20Given%20its%20potential%20as%20a%0Amedium%20for%20visual%20signals%2C%20exploring%20the%20development%20of%20a%20neural%20blending%0Amethod%20that%20utilizes%20INRs%20is%20a%20natural%20progression.%20Neural%20blending%20involves%0Amerging%20two%20INRs%20to%20create%20a%20new%20INR%20that%20encapsulates%20information%20from%20both%0Aoriginal%20representations.%20A%20direct%20approach%20involves%20applying%20traditional%20image%0Aediting%20methods%20to%20the%20INR%20rendering%20process.%20However%2C%20this%20method%20often%0Aresults%20in%20blending%20distortions%2C%20artifacts%2C%20and%20color%20shifts%2C%20primarily%20due%20to%0Athe%20discretization%20of%20the%20underlying%20pixel%20grid%20and%20the%20introduction%20of%0Aboundary%20conditions%20for%20solving%20variational%20problems.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20the%20Neural%20Poisson%20Solver%2C%20a%20plug-and-play%20and%20universally%20applicable%0Aframework%20across%20different%20signal%20dimensions%20for%20blending%20visual%20signals%0Arepresented%20by%20INRs.%20Our%20Neural%20Poisson%20Solver%20offers%20a%20variational%0Aproblem-solving%20approach%20based%20on%20the%20continuous%20Poisson%20equation%2C%0Ademonstrating%20exceptional%20performance%20across%20various%20domains.%20Specifically%2C%20we%0Apropose%20a%20gradient-guided%20neural%20solver%20to%20represent%20the%20solution%20process%20of%0Athe%20variational%20problem%2C%20refining%20the%20target%20signal%20to%20achieve%20natural%20blending%0Aresults.%20We%20also%20develop%20a%20Poisson%20equation-based%20loss%20and%20optimization%20scheme%0Ato%20train%20our%20solver%2C%20ensuring%20it%20effectively%20blends%20the%20input%20INR%20scenes%20while%0Apreserving%20their%20inherent%20structure%20and%20semantic%20content.%20The%20lack%20of%0Adependence%20on%20additional%20prior%20knowledge%20makes%20our%20method%20easily%20adaptable%20to%0Avarious%20task%20categories%2C%20highlighting%20its%20versatility.%20Comprehensive%0Aexperimental%20results%20validate%20the%20robustness%20of%20our%20approach%20across%20multiple%0Adimensions%20and%20blending%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Poisson%2520Solver%253A%2520A%2520Universal%2520and%2520Continuous%2520Framework%2520for%2520Natural%250A%2520%2520Signal%2520Blending%26entry.906535625%3DDelong%2520Wu%2520and%2520Hao%2520Zhu%2520and%2520Qi%2520Zhang%2520and%2520You%2520Li%2520and%2520Zhan%2520Ma%2520and%2520Xun%2520Cao%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representation%2520%2528INR%2529%2520has%2520become%2520a%2520popular%2520method%2520for%250Arepresenting%2520visual%2520signals%2520%2528e.g.%252C%25202D%2520images%2520and%25203D%2520scenes%2529%252C%2520demonstrating%250Apromising%2520results%2520in%2520various%2520downstream%2520applications.%2520Given%2520its%2520potential%2520as%2520a%250Amedium%2520for%2520visual%2520signals%252C%2520exploring%2520the%2520development%2520of%2520a%2520neural%2520blending%250Amethod%2520that%2520utilizes%2520INRs%2520is%2520a%2520natural%2520progression.%2520Neural%2520blending%2520involves%250Amerging%2520two%2520INRs%2520to%2520create%2520a%2520new%2520INR%2520that%2520encapsulates%2520information%2520from%2520both%250Aoriginal%2520representations.%2520A%2520direct%2520approach%2520involves%2520applying%2520traditional%2520image%250Aediting%2520methods%2520to%2520the%2520INR%2520rendering%2520process.%2520However%252C%2520this%2520method%2520often%250Aresults%2520in%2520blending%2520distortions%252C%2520artifacts%252C%2520and%2520color%2520shifts%252C%2520primarily%2520due%2520to%250Athe%2520discretization%2520of%2520the%2520underlying%2520pixel%2520grid%2520and%2520the%2520introduction%2520of%250Aboundary%2520conditions%2520for%2520solving%2520variational%2520problems.%2520To%2520tackle%2520this%2520issue%252C%2520we%250Aintroduce%2520the%2520Neural%2520Poisson%2520Solver%252C%2520a%2520plug-and-play%2520and%2520universally%2520applicable%250Aframework%2520across%2520different%2520signal%2520dimensions%2520for%2520blending%2520visual%2520signals%250Arepresented%2520by%2520INRs.%2520Our%2520Neural%2520Poisson%2520Solver%2520offers%2520a%2520variational%250Aproblem-solving%2520approach%2520based%2520on%2520the%2520continuous%2520Poisson%2520equation%252C%250Ademonstrating%2520exceptional%2520performance%2520across%2520various%2520domains.%2520Specifically%252C%2520we%250Apropose%2520a%2520gradient-guided%2520neural%2520solver%2520to%2520represent%2520the%2520solution%2520process%2520of%250Athe%2520variational%2520problem%252C%2520refining%2520the%2520target%2520signal%2520to%2520achieve%2520natural%2520blending%250Aresults.%2520We%2520also%2520develop%2520a%2520Poisson%2520equation-based%2520loss%2520and%2520optimization%2520scheme%250Ato%2520train%2520our%2520solver%252C%2520ensuring%2520it%2520effectively%2520blends%2520the%2520input%2520INR%2520scenes%2520while%250Apreserving%2520their%2520inherent%2520structure%2520and%2520semantic%2520content.%2520The%2520lack%2520of%250Adependence%2520on%2520additional%2520prior%2520knowledge%2520makes%2520our%2520method%2520easily%2520adaptable%2520to%250Avarious%2520task%2520categories%252C%2520highlighting%2520its%2520versatility.%2520Comprehensive%250Aexperimental%2520results%2520validate%2520the%2520robustness%2520of%2520our%2520approach%2520across%2520multiple%250Adimensions%2520and%2520blending%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Poisson%20Solver%3A%20A%20Universal%20and%20Continuous%20Framework%20for%20Natural%0A%20%20Signal%20Blending&entry.906535625=Delong%20Wu%20and%20Hao%20Zhu%20and%20Qi%20Zhang%20and%20You%20Li%20and%20Zhan%20Ma%20and%20Xun%20Cao&entry.1292438233=%20%20Implicit%20Neural%20Representation%20%28INR%29%20has%20become%20a%20popular%20method%20for%0Arepresenting%20visual%20signals%20%28e.g.%2C%202D%20images%20and%203D%20scenes%29%2C%20demonstrating%0Apromising%20results%20in%20various%20downstream%20applications.%20Given%20its%20potential%20as%20a%0Amedium%20for%20visual%20signals%2C%20exploring%20the%20development%20of%20a%20neural%20blending%0Amethod%20that%20utilizes%20INRs%20is%20a%20natural%20progression.%20Neural%20blending%20involves%0Amerging%20two%20INRs%20to%20create%20a%20new%20INR%20that%20encapsulates%20information%20from%20both%0Aoriginal%20representations.%20A%20direct%20approach%20involves%20applying%20traditional%20image%0Aediting%20methods%20to%20the%20INR%20rendering%20process.%20However%2C%20this%20method%20often%0Aresults%20in%20blending%20distortions%2C%20artifacts%2C%20and%20color%20shifts%2C%20primarily%20due%20to%0Athe%20discretization%20of%20the%20underlying%20pixel%20grid%20and%20the%20introduction%20of%0Aboundary%20conditions%20for%20solving%20variational%20problems.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20the%20Neural%20Poisson%20Solver%2C%20a%20plug-and-play%20and%20universally%20applicable%0Aframework%20across%20different%20signal%20dimensions%20for%20blending%20visual%20signals%0Arepresented%20by%20INRs.%20Our%20Neural%20Poisson%20Solver%20offers%20a%20variational%0Aproblem-solving%20approach%20based%20on%20the%20continuous%20Poisson%20equation%2C%0Ademonstrating%20exceptional%20performance%20across%20various%20domains.%20Specifically%2C%20we%0Apropose%20a%20gradient-guided%20neural%20solver%20to%20represent%20the%20solution%20process%20of%0Athe%20variational%20problem%2C%20refining%20the%20target%20signal%20to%20achieve%20natural%20blending%0Aresults.%20We%20also%20develop%20a%20Poisson%20equation-based%20loss%20and%20optimization%20scheme%0Ato%20train%20our%20solver%2C%20ensuring%20it%20effectively%20blends%20the%20input%20INR%20scenes%20while%0Apreserving%20their%20inherent%20structure%20and%20semantic%20content.%20The%20lack%20of%0Adependence%20on%20additional%20prior%20knowledge%20makes%20our%20method%20easily%20adaptable%20to%0Avarious%20task%20categories%2C%20highlighting%20its%20versatility.%20Comprehensive%0Aexperimental%20results%20validate%20the%20robustness%20of%20our%20approach%20across%20multiple%0Adimensions%20and%20blending%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08457v1&entry.124074799=Read"},
{"title": "Modality Agnostic Heterogeneous Face Recognition with Switch Style\n  Modulators", "author": "Anjith George and Sebastien Marcel", "abstract": "  Heterogeneous Face Recognition (HFR) systems aim to enhance the capability of\nface recognition in challenging cross-modal authentication scenarios. However,\nthe significant domain gap between the source and target modalities poses a\nconsiderable challenge for cross-domain matching. Existing literature primarily\nfocuses on developing HFR approaches for specific pairs of face modalities,\nnecessitating the explicit training of models for each source-target\ncombination. In this work, we introduce a novel framework designed to train a\nmodality-agnostic HFR method capable of handling multiple modalities during\ninference, all without explicit knowledge of the target modality labels. We\nachieve this by implementing a computationally efficient automatic routing\nmechanism called Switch Style Modulation Blocks (SSMB) that trains various\ndomain expert modulators which transform the feature maps adaptively reducing\nthe domain gap. Our proposed SSMB can be trained end-to-end and seamlessly\nintegrated into pre-trained face recognition models, transforming them into\nmodality-agnostic HFR models. We have performed extensive evaluations on HFR\nbenchmark datasets to demonstrate its effectiveness. The source code and\nprotocols will be made publicly available.\n", "link": "http://arxiv.org/abs/2407.08640v1", "date": "2024-07-11", "relevancy": 2.0932, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5343}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5231}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality%20Agnostic%20Heterogeneous%20Face%20Recognition%20with%20Switch%20Style%0A%20%20Modulators&body=Title%3A%20Modality%20Agnostic%20Heterogeneous%20Face%20Recognition%20with%20Switch%20Style%0A%20%20Modulators%0AAuthor%3A%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20Heterogeneous%20Face%20Recognition%20%28HFR%29%20systems%20aim%20to%20enhance%20the%20capability%20of%0Aface%20recognition%20in%20challenging%20cross-modal%20authentication%20scenarios.%20However%2C%0Athe%20significant%20domain%20gap%20between%20the%20source%20and%20target%20modalities%20poses%20a%0Aconsiderable%20challenge%20for%20cross-domain%20matching.%20Existing%20literature%20primarily%0Afocuses%20on%20developing%20HFR%20approaches%20for%20specific%20pairs%20of%20face%20modalities%2C%0Anecessitating%20the%20explicit%20training%20of%20models%20for%20each%20source-target%0Acombination.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20designed%20to%20train%20a%0Amodality-agnostic%20HFR%20method%20capable%20of%20handling%20multiple%20modalities%20during%0Ainference%2C%20all%20without%20explicit%20knowledge%20of%20the%20target%20modality%20labels.%20We%0Aachieve%20this%20by%20implementing%20a%20computationally%20efficient%20automatic%20routing%0Amechanism%20called%20Switch%20Style%20Modulation%20Blocks%20%28SSMB%29%20that%20trains%20various%0Adomain%20expert%20modulators%20which%20transform%20the%20feature%20maps%20adaptively%20reducing%0Athe%20domain%20gap.%20Our%20proposed%20SSMB%20can%20be%20trained%20end-to-end%20and%20seamlessly%0Aintegrated%20into%20pre-trained%20face%20recognition%20models%2C%20transforming%20them%20into%0Amodality-agnostic%20HFR%20models.%20We%20have%20performed%20extensive%20evaluations%20on%20HFR%0Abenchmark%20datasets%20to%20demonstrate%20its%20effectiveness.%20The%20source%20code%20and%0Aprotocols%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality%2520Agnostic%2520Heterogeneous%2520Face%2520Recognition%2520with%2520Switch%2520Style%250A%2520%2520Modulators%26entry.906535625%3DAnjith%2520George%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520Heterogeneous%2520Face%2520Recognition%2520%2528HFR%2529%2520systems%2520aim%2520to%2520enhance%2520the%2520capability%2520of%250Aface%2520recognition%2520in%2520challenging%2520cross-modal%2520authentication%2520scenarios.%2520However%252C%250Athe%2520significant%2520domain%2520gap%2520between%2520the%2520source%2520and%2520target%2520modalities%2520poses%2520a%250Aconsiderable%2520challenge%2520for%2520cross-domain%2520matching.%2520Existing%2520literature%2520primarily%250Afocuses%2520on%2520developing%2520HFR%2520approaches%2520for%2520specific%2520pairs%2520of%2520face%2520modalities%252C%250Anecessitating%2520the%2520explicit%2520training%2520of%2520models%2520for%2520each%2520source-target%250Acombination.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520designed%2520to%2520train%2520a%250Amodality-agnostic%2520HFR%2520method%2520capable%2520of%2520handling%2520multiple%2520modalities%2520during%250Ainference%252C%2520all%2520without%2520explicit%2520knowledge%2520of%2520the%2520target%2520modality%2520labels.%2520We%250Aachieve%2520this%2520by%2520implementing%2520a%2520computationally%2520efficient%2520automatic%2520routing%250Amechanism%2520called%2520Switch%2520Style%2520Modulation%2520Blocks%2520%2528SSMB%2529%2520that%2520trains%2520various%250Adomain%2520expert%2520modulators%2520which%2520transform%2520the%2520feature%2520maps%2520adaptively%2520reducing%250Athe%2520domain%2520gap.%2520Our%2520proposed%2520SSMB%2520can%2520be%2520trained%2520end-to-end%2520and%2520seamlessly%250Aintegrated%2520into%2520pre-trained%2520face%2520recognition%2520models%252C%2520transforming%2520them%2520into%250Amodality-agnostic%2520HFR%2520models.%2520We%2520have%2520performed%2520extensive%2520evaluations%2520on%2520HFR%250Abenchmark%2520datasets%2520to%2520demonstrate%2520its%2520effectiveness.%2520The%2520source%2520code%2520and%250Aprotocols%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality%20Agnostic%20Heterogeneous%20Face%20Recognition%20with%20Switch%20Style%0A%20%20Modulators&entry.906535625=Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20Heterogeneous%20Face%20Recognition%20%28HFR%29%20systems%20aim%20to%20enhance%20the%20capability%20of%0Aface%20recognition%20in%20challenging%20cross-modal%20authentication%20scenarios.%20However%2C%0Athe%20significant%20domain%20gap%20between%20the%20source%20and%20target%20modalities%20poses%20a%0Aconsiderable%20challenge%20for%20cross-domain%20matching.%20Existing%20literature%20primarily%0Afocuses%20on%20developing%20HFR%20approaches%20for%20specific%20pairs%20of%20face%20modalities%2C%0Anecessitating%20the%20explicit%20training%20of%20models%20for%20each%20source-target%0Acombination.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20designed%20to%20train%20a%0Amodality-agnostic%20HFR%20method%20capable%20of%20handling%20multiple%20modalities%20during%0Ainference%2C%20all%20without%20explicit%20knowledge%20of%20the%20target%20modality%20labels.%20We%0Aachieve%20this%20by%20implementing%20a%20computationally%20efficient%20automatic%20routing%0Amechanism%20called%20Switch%20Style%20Modulation%20Blocks%20%28SSMB%29%20that%20trains%20various%0Adomain%20expert%20modulators%20which%20transform%20the%20feature%20maps%20adaptively%20reducing%0Athe%20domain%20gap.%20Our%20proposed%20SSMB%20can%20be%20trained%20end-to-end%20and%20seamlessly%0Aintegrated%20into%20pre-trained%20face%20recognition%20models%2C%20transforming%20them%20into%0Amodality-agnostic%20HFR%20models.%20We%20have%20performed%20extensive%20evaluations%20on%20HFR%0Abenchmark%20datasets%20to%20demonstrate%20its%20effectiveness.%20The%20source%20code%20and%0Aprotocols%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08640v1&entry.124074799=Read"},
{"title": "HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large\n  Vision-Language Models", "author": "Runhui Huang and Xinpeng Ding and Chunwei Wang and Jianhua Han and Yulong Liu and Hengshuang Zhao and Hang Xu and Lu Hou and Wei Zhang and Xiaodan Liang", "abstract": "  High-resolution inputs enable Large Vision-Language Models (LVLMs) to discern\nfiner visual details, enhancing their comprehension capabilities. To reduce the\ntraining and computation costs caused by high-resolution input, one promising\ndirection is to use sliding windows to slice the input into uniform patches,\neach matching the input size of the well-trained vision encoder. Although\nefficient, this slicing strategy leads to the fragmentation of original input,\ni.e., the continuity of contextual information and spatial geometry is lost\nacross patches, adversely affecting performance in cross-patch context\nperception and position-specific tasks. To overcome these shortcomings, we\nintroduce HiRes-LLaVA, a novel framework designed to efficiently process any\nsize of high-resolution input without altering the original contextual and\ngeometric information. HiRes-LLaVA comprises two innovative components: (i) a\nSliceRestore adapter that reconstructs sliced patches into their original form,\nefficiently extracting both global and local features via down-up-sampling and\nconvolution layers, and (ii) a Self-Mining Sampler to compresses the vision\ntokens based on themselves, preserving the original context and positional\ninformation while reducing training overhead. To assess the ability of handling\ncontext fragmentation, we construct a new benchmark, EntityGrid-QA, consisting\nof edge-related and position-related tasks. Our comprehensive experiments\ndemonstrate the superiority of HiRes-LLaVA on both existing public benchmarks\nand on EntityGrid-QA, particularly on document-oriented tasks, establishing new\nstandards for handling high-resolution inputs.\n", "link": "http://arxiv.org/abs/2407.08706v1", "date": "2024-07-11", "relevancy": 2.0908, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5307}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5227}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiRes-LLaVA%3A%20Restoring%20Fragmentation%20Input%20in%20High-Resolution%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20HiRes-LLaVA%3A%20Restoring%20Fragmentation%20Input%20in%20High-Resolution%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Runhui%20Huang%20and%20Xinpeng%20Ding%20and%20Chunwei%20Wang%20and%20Jianhua%20Han%20and%20Yulong%20Liu%20and%20Hengshuang%20Zhao%20and%20Hang%20Xu%20and%20Lu%20Hou%20and%20Wei%20Zhang%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20High-resolution%20inputs%20enable%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20discern%0Afiner%20visual%20details%2C%20enhancing%20their%20comprehension%20capabilities.%20To%20reduce%20the%0Atraining%20and%20computation%20costs%20caused%20by%20high-resolution%20input%2C%20one%20promising%0Adirection%20is%20to%20use%20sliding%20windows%20to%20slice%20the%20input%20into%20uniform%20patches%2C%0Aeach%20matching%20the%20input%20size%20of%20the%20well-trained%20vision%20encoder.%20Although%0Aefficient%2C%20this%20slicing%20strategy%20leads%20to%20the%20fragmentation%20of%20original%20input%2C%0Ai.e.%2C%20the%20continuity%20of%20contextual%20information%20and%20spatial%20geometry%20is%20lost%0Aacross%20patches%2C%20adversely%20affecting%20performance%20in%20cross-patch%20context%0Aperception%20and%20position-specific%20tasks.%20To%20overcome%20these%20shortcomings%2C%20we%0Aintroduce%20HiRes-LLaVA%2C%20a%20novel%20framework%20designed%20to%20efficiently%20process%20any%0Asize%20of%20high-resolution%20input%20without%20altering%20the%20original%20contextual%20and%0Ageometric%20information.%20HiRes-LLaVA%20comprises%20two%20innovative%20components%3A%20%28i%29%20a%0ASliceRestore%20adapter%20that%20reconstructs%20sliced%20patches%20into%20their%20original%20form%2C%0Aefficiently%20extracting%20both%20global%20and%20local%20features%20via%20down-up-sampling%20and%0Aconvolution%20layers%2C%20and%20%28ii%29%20a%20Self-Mining%20Sampler%20to%20compresses%20the%20vision%0Atokens%20based%20on%20themselves%2C%20preserving%20the%20original%20context%20and%20positional%0Ainformation%20while%20reducing%20training%20overhead.%20To%20assess%20the%20ability%20of%20handling%0Acontext%20fragmentation%2C%20we%20construct%20a%20new%20benchmark%2C%20EntityGrid-QA%2C%20consisting%0Aof%20edge-related%20and%20position-related%20tasks.%20Our%20comprehensive%20experiments%0Ademonstrate%20the%20superiority%20of%20HiRes-LLaVA%20on%20both%20existing%20public%20benchmarks%0Aand%20on%20EntityGrid-QA%2C%20particularly%20on%20document-oriented%20tasks%2C%20establishing%20new%0Astandards%20for%20handling%20high-resolution%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiRes-LLaVA%253A%2520Restoring%2520Fragmentation%2520Input%2520in%2520High-Resolution%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DRunhui%2520Huang%2520and%2520Xinpeng%2520Ding%2520and%2520Chunwei%2520Wang%2520and%2520Jianhua%2520Han%2520and%2520Yulong%2520Liu%2520and%2520Hengshuang%2520Zhao%2520and%2520Hang%2520Xu%2520and%2520Lu%2520Hou%2520and%2520Wei%2520Zhang%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520High-resolution%2520inputs%2520enable%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520discern%250Afiner%2520visual%2520details%252C%2520enhancing%2520their%2520comprehension%2520capabilities.%2520To%2520reduce%2520the%250Atraining%2520and%2520computation%2520costs%2520caused%2520by%2520high-resolution%2520input%252C%2520one%2520promising%250Adirection%2520is%2520to%2520use%2520sliding%2520windows%2520to%2520slice%2520the%2520input%2520into%2520uniform%2520patches%252C%250Aeach%2520matching%2520the%2520input%2520size%2520of%2520the%2520well-trained%2520vision%2520encoder.%2520Although%250Aefficient%252C%2520this%2520slicing%2520strategy%2520leads%2520to%2520the%2520fragmentation%2520of%2520original%2520input%252C%250Ai.e.%252C%2520the%2520continuity%2520of%2520contextual%2520information%2520and%2520spatial%2520geometry%2520is%2520lost%250Aacross%2520patches%252C%2520adversely%2520affecting%2520performance%2520in%2520cross-patch%2520context%250Aperception%2520and%2520position-specific%2520tasks.%2520To%2520overcome%2520these%2520shortcomings%252C%2520we%250Aintroduce%2520HiRes-LLaVA%252C%2520a%2520novel%2520framework%2520designed%2520to%2520efficiently%2520process%2520any%250Asize%2520of%2520high-resolution%2520input%2520without%2520altering%2520the%2520original%2520contextual%2520and%250Ageometric%2520information.%2520HiRes-LLaVA%2520comprises%2520two%2520innovative%2520components%253A%2520%2528i%2529%2520a%250ASliceRestore%2520adapter%2520that%2520reconstructs%2520sliced%2520patches%2520into%2520their%2520original%2520form%252C%250Aefficiently%2520extracting%2520both%2520global%2520and%2520local%2520features%2520via%2520down-up-sampling%2520and%250Aconvolution%2520layers%252C%2520and%2520%2528ii%2529%2520a%2520Self-Mining%2520Sampler%2520to%2520compresses%2520the%2520vision%250Atokens%2520based%2520on%2520themselves%252C%2520preserving%2520the%2520original%2520context%2520and%2520positional%250Ainformation%2520while%2520reducing%2520training%2520overhead.%2520To%2520assess%2520the%2520ability%2520of%2520handling%250Acontext%2520fragmentation%252C%2520we%2520construct%2520a%2520new%2520benchmark%252C%2520EntityGrid-QA%252C%2520consisting%250Aof%2520edge-related%2520and%2520position-related%2520tasks.%2520Our%2520comprehensive%2520experiments%250Ademonstrate%2520the%2520superiority%2520of%2520HiRes-LLaVA%2520on%2520both%2520existing%2520public%2520benchmarks%250Aand%2520on%2520EntityGrid-QA%252C%2520particularly%2520on%2520document-oriented%2520tasks%252C%2520establishing%2520new%250Astandards%2520for%2520handling%2520high-resolution%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiRes-LLaVA%3A%20Restoring%20Fragmentation%20Input%20in%20High-Resolution%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Runhui%20Huang%20and%20Xinpeng%20Ding%20and%20Chunwei%20Wang%20and%20Jianhua%20Han%20and%20Yulong%20Liu%20and%20Hengshuang%20Zhao%20and%20Hang%20Xu%20and%20Lu%20Hou%20and%20Wei%20Zhang%20and%20Xiaodan%20Liang&entry.1292438233=%20%20High-resolution%20inputs%20enable%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20discern%0Afiner%20visual%20details%2C%20enhancing%20their%20comprehension%20capabilities.%20To%20reduce%20the%0Atraining%20and%20computation%20costs%20caused%20by%20high-resolution%20input%2C%20one%20promising%0Adirection%20is%20to%20use%20sliding%20windows%20to%20slice%20the%20input%20into%20uniform%20patches%2C%0Aeach%20matching%20the%20input%20size%20of%20the%20well-trained%20vision%20encoder.%20Although%0Aefficient%2C%20this%20slicing%20strategy%20leads%20to%20the%20fragmentation%20of%20original%20input%2C%0Ai.e.%2C%20the%20continuity%20of%20contextual%20information%20and%20spatial%20geometry%20is%20lost%0Aacross%20patches%2C%20adversely%20affecting%20performance%20in%20cross-patch%20context%0Aperception%20and%20position-specific%20tasks.%20To%20overcome%20these%20shortcomings%2C%20we%0Aintroduce%20HiRes-LLaVA%2C%20a%20novel%20framework%20designed%20to%20efficiently%20process%20any%0Asize%20of%20high-resolution%20input%20without%20altering%20the%20original%20contextual%20and%0Ageometric%20information.%20HiRes-LLaVA%20comprises%20two%20innovative%20components%3A%20%28i%29%20a%0ASliceRestore%20adapter%20that%20reconstructs%20sliced%20patches%20into%20their%20original%20form%2C%0Aefficiently%20extracting%20both%20global%20and%20local%20features%20via%20down-up-sampling%20and%0Aconvolution%20layers%2C%20and%20%28ii%29%20a%20Self-Mining%20Sampler%20to%20compresses%20the%20vision%0Atokens%20based%20on%20themselves%2C%20preserving%20the%20original%20context%20and%20positional%0Ainformation%20while%20reducing%20training%20overhead.%20To%20assess%20the%20ability%20of%20handling%0Acontext%20fragmentation%2C%20we%20construct%20a%20new%20benchmark%2C%20EntityGrid-QA%2C%20consisting%0Aof%20edge-related%20and%20position-related%20tasks.%20Our%20comprehensive%20experiments%0Ademonstrate%20the%20superiority%20of%20HiRes-LLaVA%20on%20both%20existing%20public%20benchmarks%0Aand%20on%20EntityGrid-QA%2C%20particularly%20on%20document-oriented%20tasks%2C%20establishing%20new%0Astandards%20for%20handling%20high-resolution%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08706v1&entry.124074799=Read"},
{"title": "Enhancing Robustness of Vision-Language Models through Orthogonality\n  Learning and Cross-Regularization", "author": "Jinlong Li and Zequn Jie and Elisa Ricci and Lin Ma and Nicu Sebe", "abstract": "  Efficient finetuning of vision-language models (VLMs) like CLIP for specific\ndownstream tasks is gaining significant attention. Previous works primarily\nfocus on prompt learning to adapt the CLIP into a variety of downstream tasks,\nhowever, suffering from task overfitting when finetuned on a small data set. In\nthis paper, we introduce an orthogonal finetuning method for efficiently\nupdating pretrained weights which enhances robustness and generalization, while\na cross-regularization strategy is further exploited to maintain the stability\nin terms of zero-shot generalization of VLMs, dubbed \\textbf{\\textit{OrthCR}}.\nSpecifically, trainable orthogonal matrices are injected seamlessly into the\ntransformer architecture and enforced with orthogonality constraint using\nCayley parameterization, benefiting from the norm-preserving property and thus\nleading to stable and faster convergence. To alleviate deviation from\northogonal constraint during training, a cross-regularization strategy is\nfurther employed with initial pretrained weights within a bypass manner. In\naddition, to enrich the sample diversity for downstream tasks, we first explore\nCutout data augmentation to boost the efficient finetuning and comprehend how\nour approach improves the specific downstream performance and maintains the\ngeneralizability in the perspective of Orthogonality Learning. Beyond existing\nprompt learning techniques, we conduct extensive experiments to demonstrate\nthat our method explicitly steers pretrained weight space to represent the\ntask-specific knowledge and presents competitive generalizability under\n\\textit{base-to-base/base-to-new}, \\textit{cross-dataset transfer} and\n\\textit{domain generalization} evaluations.\n", "link": "http://arxiv.org/abs/2407.08374v1", "date": "2024-07-11", "relevancy": 2.0878, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5068}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Robustness%20of%20Vision-Language%20Models%20through%20Orthogonality%0A%20%20Learning%20and%20Cross-Regularization&body=Title%3A%20Enhancing%20Robustness%20of%20Vision-Language%20Models%20through%20Orthogonality%0A%20%20Learning%20and%20Cross-Regularization%0AAuthor%3A%20Jinlong%20Li%20and%20Zequn%20Jie%20and%20Elisa%20Ricci%20and%20Lin%20Ma%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Efficient%20finetuning%20of%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20for%20specific%0Adownstream%20tasks%20is%20gaining%20significant%20attention.%20Previous%20works%20primarily%0Afocus%20on%20prompt%20learning%20to%20adapt%20the%20CLIP%20into%20a%20variety%20of%20downstream%20tasks%2C%0Ahowever%2C%20suffering%20from%20task%20overfitting%20when%20finetuned%20on%20a%20small%20data%20set.%20In%0Athis%20paper%2C%20we%20introduce%20an%20orthogonal%20finetuning%20method%20for%20efficiently%0Aupdating%20pretrained%20weights%20which%20enhances%20robustness%20and%20generalization%2C%20while%0Aa%20cross-regularization%20strategy%20is%20further%20exploited%20to%20maintain%20the%20stability%0Ain%20terms%20of%20zero-shot%20generalization%20of%20VLMs%2C%20dubbed%20%5Ctextbf%7B%5Ctextit%7BOrthCR%7D%7D.%0ASpecifically%2C%20trainable%20orthogonal%20matrices%20are%20injected%20seamlessly%20into%20the%0Atransformer%20architecture%20and%20enforced%20with%20orthogonality%20constraint%20using%0ACayley%20parameterization%2C%20benefiting%20from%20the%20norm-preserving%20property%20and%20thus%0Aleading%20to%20stable%20and%20faster%20convergence.%20To%20alleviate%20deviation%20from%0Aorthogonal%20constraint%20during%20training%2C%20a%20cross-regularization%20strategy%20is%0Afurther%20employed%20with%20initial%20pretrained%20weights%20within%20a%20bypass%20manner.%20In%0Aaddition%2C%20to%20enrich%20the%20sample%20diversity%20for%20downstream%20tasks%2C%20we%20first%20explore%0ACutout%20data%20augmentation%20to%20boost%20the%20efficient%20finetuning%20and%20comprehend%20how%0Aour%20approach%20improves%20the%20specific%20downstream%20performance%20and%20maintains%20the%0Ageneralizability%20in%20the%20perspective%20of%20Orthogonality%20Learning.%20Beyond%20existing%0Aprompt%20learning%20techniques%2C%20we%20conduct%20extensive%20experiments%20to%20demonstrate%0Athat%20our%20method%20explicitly%20steers%20pretrained%20weight%20space%20to%20represent%20the%0Atask-specific%20knowledge%20and%20presents%20competitive%20generalizability%20under%0A%5Ctextit%7Bbase-to-base/base-to-new%7D%2C%20%5Ctextit%7Bcross-dataset%20transfer%7D%20and%0A%5Ctextit%7Bdomain%20generalization%7D%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Robustness%2520of%2520Vision-Language%2520Models%2520through%2520Orthogonality%250A%2520%2520Learning%2520and%2520Cross-Regularization%26entry.906535625%3DJinlong%2520Li%2520and%2520Zequn%2520Jie%2520and%2520Elisa%2520Ricci%2520and%2520Lin%2520Ma%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Efficient%2520finetuning%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%2520for%2520specific%250Adownstream%2520tasks%2520is%2520gaining%2520significant%2520attention.%2520Previous%2520works%2520primarily%250Afocus%2520on%2520prompt%2520learning%2520to%2520adapt%2520the%2520CLIP%2520into%2520a%2520variety%2520of%2520downstream%2520tasks%252C%250Ahowever%252C%2520suffering%2520from%2520task%2520overfitting%2520when%2520finetuned%2520on%2520a%2520small%2520data%2520set.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520an%2520orthogonal%2520finetuning%2520method%2520for%2520efficiently%250Aupdating%2520pretrained%2520weights%2520which%2520enhances%2520robustness%2520and%2520generalization%252C%2520while%250Aa%2520cross-regularization%2520strategy%2520is%2520further%2520exploited%2520to%2520maintain%2520the%2520stability%250Ain%2520terms%2520of%2520zero-shot%2520generalization%2520of%2520VLMs%252C%2520dubbed%2520%255Ctextbf%257B%255Ctextit%257BOrthCR%257D%257D.%250ASpecifically%252C%2520trainable%2520orthogonal%2520matrices%2520are%2520injected%2520seamlessly%2520into%2520the%250Atransformer%2520architecture%2520and%2520enforced%2520with%2520orthogonality%2520constraint%2520using%250ACayley%2520parameterization%252C%2520benefiting%2520from%2520the%2520norm-preserving%2520property%2520and%2520thus%250Aleading%2520to%2520stable%2520and%2520faster%2520convergence.%2520To%2520alleviate%2520deviation%2520from%250Aorthogonal%2520constraint%2520during%2520training%252C%2520a%2520cross-regularization%2520strategy%2520is%250Afurther%2520employed%2520with%2520initial%2520pretrained%2520weights%2520within%2520a%2520bypass%2520manner.%2520In%250Aaddition%252C%2520to%2520enrich%2520the%2520sample%2520diversity%2520for%2520downstream%2520tasks%252C%2520we%2520first%2520explore%250ACutout%2520data%2520augmentation%2520to%2520boost%2520the%2520efficient%2520finetuning%2520and%2520comprehend%2520how%250Aour%2520approach%2520improves%2520the%2520specific%2520downstream%2520performance%2520and%2520maintains%2520the%250Ageneralizability%2520in%2520the%2520perspective%2520of%2520Orthogonality%2520Learning.%2520Beyond%2520existing%250Aprompt%2520learning%2520techniques%252C%2520we%2520conduct%2520extensive%2520experiments%2520to%2520demonstrate%250Athat%2520our%2520method%2520explicitly%2520steers%2520pretrained%2520weight%2520space%2520to%2520represent%2520the%250Atask-specific%2520knowledge%2520and%2520presents%2520competitive%2520generalizability%2520under%250A%255Ctextit%257Bbase-to-base/base-to-new%257D%252C%2520%255Ctextit%257Bcross-dataset%2520transfer%257D%2520and%250A%255Ctextit%257Bdomain%2520generalization%257D%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Robustness%20of%20Vision-Language%20Models%20through%20Orthogonality%0A%20%20Learning%20and%20Cross-Regularization&entry.906535625=Jinlong%20Li%20and%20Zequn%20Jie%20and%20Elisa%20Ricci%20and%20Lin%20Ma%20and%20Nicu%20Sebe&entry.1292438233=%20%20Efficient%20finetuning%20of%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20for%20specific%0Adownstream%20tasks%20is%20gaining%20significant%20attention.%20Previous%20works%20primarily%0Afocus%20on%20prompt%20learning%20to%20adapt%20the%20CLIP%20into%20a%20variety%20of%20downstream%20tasks%2C%0Ahowever%2C%20suffering%20from%20task%20overfitting%20when%20finetuned%20on%20a%20small%20data%20set.%20In%0Athis%20paper%2C%20we%20introduce%20an%20orthogonal%20finetuning%20method%20for%20efficiently%0Aupdating%20pretrained%20weights%20which%20enhances%20robustness%20and%20generalization%2C%20while%0Aa%20cross-regularization%20strategy%20is%20further%20exploited%20to%20maintain%20the%20stability%0Ain%20terms%20of%20zero-shot%20generalization%20of%20VLMs%2C%20dubbed%20%5Ctextbf%7B%5Ctextit%7BOrthCR%7D%7D.%0ASpecifically%2C%20trainable%20orthogonal%20matrices%20are%20injected%20seamlessly%20into%20the%0Atransformer%20architecture%20and%20enforced%20with%20orthogonality%20constraint%20using%0ACayley%20parameterization%2C%20benefiting%20from%20the%20norm-preserving%20property%20and%20thus%0Aleading%20to%20stable%20and%20faster%20convergence.%20To%20alleviate%20deviation%20from%0Aorthogonal%20constraint%20during%20training%2C%20a%20cross-regularization%20strategy%20is%0Afurther%20employed%20with%20initial%20pretrained%20weights%20within%20a%20bypass%20manner.%20In%0Aaddition%2C%20to%20enrich%20the%20sample%20diversity%20for%20downstream%20tasks%2C%20we%20first%20explore%0ACutout%20data%20augmentation%20to%20boost%20the%20efficient%20finetuning%20and%20comprehend%20how%0Aour%20approach%20improves%20the%20specific%20downstream%20performance%20and%20maintains%20the%0Ageneralizability%20in%20the%20perspective%20of%20Orthogonality%20Learning.%20Beyond%20existing%0Aprompt%20learning%20techniques%2C%20we%20conduct%20extensive%20experiments%20to%20demonstrate%0Athat%20our%20method%20explicitly%20steers%20pretrained%20weight%20space%20to%20represent%20the%0Atask-specific%20knowledge%20and%20presents%20competitive%20generalizability%20under%0A%5Ctextit%7Bbase-to-base/base-to-new%7D%2C%20%5Ctextit%7Bcross-dataset%20transfer%7D%20and%0A%5Ctextit%7Bdomain%20generalization%7D%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08374v1&entry.124074799=Read"},
{"title": "Semi-Supervised Object Detection: A Survey on Progress from CNN to\n  Transformer", "author": "Tahira Shehzadi and  Ifza and Didier Stricker and Muhammad Zeshan Afzal", "abstract": "  The impressive advancements in semi-supervised learning have driven\nresearchers to explore its potential in object detection tasks within the field\nof computer vision. Semi-Supervised Object Detection (SSOD) leverages a\ncombination of a small labeled dataset and a larger, unlabeled dataset. This\napproach effectively reduces the dependence on large labeled datasets, which\nare often expensive and time-consuming to obtain. Initially, SSOD models\nencountered challenges in effectively leveraging unlabeled data and managing\nnoise in generated pseudo-labels for unlabeled data. However, numerous recent\nadvancements have addressed these issues, resulting in substantial improvements\nin SSOD performance. This paper presents a comprehensive review of 27\ncutting-edge developments in SSOD methodologies, from Convolutional Neural\nNetworks (CNNs) to Transformers. We delve into the core components of\nsemi-supervised learning and its integration into object detection frameworks,\ncovering data augmentation techniques, pseudo-labeling strategies, consistency\nregularization, and adversarial training methods. Furthermore, we conduct a\ncomparative analysis of various SSOD models, evaluating their performance and\narchitectural differences. We aim to ignite further research interest in\novercoming existing challenges and exploring new directions in semi-supervised\nlearning for object detection.\n", "link": "http://arxiv.org/abs/2407.08460v1", "date": "2024-07-11", "relevancy": 2.0747, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5249}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.522}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Object%20Detection%3A%20A%20Survey%20on%20Progress%20from%20CNN%20to%0A%20%20Transformer&body=Title%3A%20Semi-Supervised%20Object%20Detection%3A%20A%20Survey%20on%20Progress%20from%20CNN%20to%0A%20%20Transformer%0AAuthor%3A%20Tahira%20Shehzadi%20and%20%20Ifza%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal%0AAbstract%3A%20%20%20The%20impressive%20advancements%20in%20semi-supervised%20learning%20have%20driven%0Aresearchers%20to%20explore%20its%20potential%20in%20object%20detection%20tasks%20within%20the%20field%0Aof%20computer%20vision.%20Semi-Supervised%20Object%20Detection%20%28SSOD%29%20leverages%20a%0Acombination%20of%20a%20small%20labeled%20dataset%20and%20a%20larger%2C%20unlabeled%20dataset.%20This%0Aapproach%20effectively%20reduces%20the%20dependence%20on%20large%20labeled%20datasets%2C%20which%0Aare%20often%20expensive%20and%20time-consuming%20to%20obtain.%20Initially%2C%20SSOD%20models%0Aencountered%20challenges%20in%20effectively%20leveraging%20unlabeled%20data%20and%20managing%0Anoise%20in%20generated%20pseudo-labels%20for%20unlabeled%20data.%20However%2C%20numerous%20recent%0Aadvancements%20have%20addressed%20these%20issues%2C%20resulting%20in%20substantial%20improvements%0Ain%20SSOD%20performance.%20This%20paper%20presents%20a%20comprehensive%20review%20of%2027%0Acutting-edge%20developments%20in%20SSOD%20methodologies%2C%20from%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20to%20Transformers.%20We%20delve%20into%20the%20core%20components%20of%0Asemi-supervised%20learning%20and%20its%20integration%20into%20object%20detection%20frameworks%2C%0Acovering%20data%20augmentation%20techniques%2C%20pseudo-labeling%20strategies%2C%20consistency%0Aregularization%2C%20and%20adversarial%20training%20methods.%20Furthermore%2C%20we%20conduct%20a%0Acomparative%20analysis%20of%20various%20SSOD%20models%2C%20evaluating%20their%20performance%20and%0Aarchitectural%20differences.%20We%20aim%20to%20ignite%20further%20research%20interest%20in%0Aovercoming%20existing%20challenges%20and%20exploring%20new%20directions%20in%20semi-supervised%0Alearning%20for%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Object%2520Detection%253A%2520A%2520Survey%2520on%2520Progress%2520from%2520CNN%2520to%250A%2520%2520Transformer%26entry.906535625%3DTahira%2520Shehzadi%2520and%2520%2520Ifza%2520and%2520Didier%2520Stricker%2520and%2520Muhammad%2520Zeshan%2520Afzal%26entry.1292438233%3D%2520%2520The%2520impressive%2520advancements%2520in%2520semi-supervised%2520learning%2520have%2520driven%250Aresearchers%2520to%2520explore%2520its%2520potential%2520in%2520object%2520detection%2520tasks%2520within%2520the%2520field%250Aof%2520computer%2520vision.%2520Semi-Supervised%2520Object%2520Detection%2520%2528SSOD%2529%2520leverages%2520a%250Acombination%2520of%2520a%2520small%2520labeled%2520dataset%2520and%2520a%2520larger%252C%2520unlabeled%2520dataset.%2520This%250Aapproach%2520effectively%2520reduces%2520the%2520dependence%2520on%2520large%2520labeled%2520datasets%252C%2520which%250Aare%2520often%2520expensive%2520and%2520time-consuming%2520to%2520obtain.%2520Initially%252C%2520SSOD%2520models%250Aencountered%2520challenges%2520in%2520effectively%2520leveraging%2520unlabeled%2520data%2520and%2520managing%250Anoise%2520in%2520generated%2520pseudo-labels%2520for%2520unlabeled%2520data.%2520However%252C%2520numerous%2520recent%250Aadvancements%2520have%2520addressed%2520these%2520issues%252C%2520resulting%2520in%2520substantial%2520improvements%250Ain%2520SSOD%2520performance.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520review%2520of%252027%250Acutting-edge%2520developments%2520in%2520SSOD%2520methodologies%252C%2520from%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%2520to%2520Transformers.%2520We%2520delve%2520into%2520the%2520core%2520components%2520of%250Asemi-supervised%2520learning%2520and%2520its%2520integration%2520into%2520object%2520detection%2520frameworks%252C%250Acovering%2520data%2520augmentation%2520techniques%252C%2520pseudo-labeling%2520strategies%252C%2520consistency%250Aregularization%252C%2520and%2520adversarial%2520training%2520methods.%2520Furthermore%252C%2520we%2520conduct%2520a%250Acomparative%2520analysis%2520of%2520various%2520SSOD%2520models%252C%2520evaluating%2520their%2520performance%2520and%250Aarchitectural%2520differences.%2520We%2520aim%2520to%2520ignite%2520further%2520research%2520interest%2520in%250Aovercoming%2520existing%2520challenges%2520and%2520exploring%2520new%2520directions%2520in%2520semi-supervised%250Alearning%2520for%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Object%20Detection%3A%20A%20Survey%20on%20Progress%20from%20CNN%20to%0A%20%20Transformer&entry.906535625=Tahira%20Shehzadi%20and%20%20Ifza%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal&entry.1292438233=%20%20The%20impressive%20advancements%20in%20semi-supervised%20learning%20have%20driven%0Aresearchers%20to%20explore%20its%20potential%20in%20object%20detection%20tasks%20within%20the%20field%0Aof%20computer%20vision.%20Semi-Supervised%20Object%20Detection%20%28SSOD%29%20leverages%20a%0Acombination%20of%20a%20small%20labeled%20dataset%20and%20a%20larger%2C%20unlabeled%20dataset.%20This%0Aapproach%20effectively%20reduces%20the%20dependence%20on%20large%20labeled%20datasets%2C%20which%0Aare%20often%20expensive%20and%20time-consuming%20to%20obtain.%20Initially%2C%20SSOD%20models%0Aencountered%20challenges%20in%20effectively%20leveraging%20unlabeled%20data%20and%20managing%0Anoise%20in%20generated%20pseudo-labels%20for%20unlabeled%20data.%20However%2C%20numerous%20recent%0Aadvancements%20have%20addressed%20these%20issues%2C%20resulting%20in%20substantial%20improvements%0Ain%20SSOD%20performance.%20This%20paper%20presents%20a%20comprehensive%20review%20of%2027%0Acutting-edge%20developments%20in%20SSOD%20methodologies%2C%20from%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20to%20Transformers.%20We%20delve%20into%20the%20core%20components%20of%0Asemi-supervised%20learning%20and%20its%20integration%20into%20object%20detection%20frameworks%2C%0Acovering%20data%20augmentation%20techniques%2C%20pseudo-labeling%20strategies%2C%20consistency%0Aregularization%2C%20and%20adversarial%20training%20methods.%20Furthermore%2C%20we%20conduct%20a%0Acomparative%20analysis%20of%20various%20SSOD%20models%2C%20evaluating%20their%20performance%20and%0Aarchitectural%20differences.%20We%20aim%20to%20ignite%20further%20research%20interest%20in%0Aovercoming%20existing%20challenges%20and%20exploring%20new%20directions%20in%20semi-supervised%0Alearning%20for%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08460v1&entry.124074799=Read"},
{"title": "Brain Tumor Segmentation in MRI Images with 3D U-Net and Contextual\n  Transformer", "author": "Thien-Qua T. Nguyen and Hieu-Nghia Nguyen and Thanh-Hieu Bui and Thien B. Nguyen-Tat and Vuong M. Ngo", "abstract": "  This research presents an enhanced approach for precise segmentation of brain\ntumor masses in magnetic resonance imaging (MRI) using an advanced 3D-UNet\nmodel combined with a Context Transformer (CoT). By architectural expansion\nCoT, the proposed model extends its architecture to a 3D format, integrates it\nsmoothly with the base model to utilize the complex contextual information\nfound in MRI scans, emphasizing how elements rely on each other across an\nextended spatial range. The proposed model synchronizes tumor mass\ncharacteristics from CoT, mutually reinforcing feature extraction, facilitating\nthe precise capture of detailed tumor mass structures, including location,\nsize, and boundaries. Several experimental results present the outstanding\nsegmentation performance of the proposed method in comparison to current\nstate-of-the-art approaches, achieving Dice score of 82.0%, 81.5%, 89.0% for\nEnhancing Tumor, Tumor Core and Whole Tumor, respectively, on BraTS2019.\n", "link": "http://arxiv.org/abs/2407.08470v1", "date": "2024-07-11", "relevancy": 2.0706, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5194}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Tumor%20Segmentation%20in%20MRI%20Images%20with%203D%20U-Net%20and%20Contextual%0A%20%20Transformer&body=Title%3A%20Brain%20Tumor%20Segmentation%20in%20MRI%20Images%20with%203D%20U-Net%20and%20Contextual%0A%20%20Transformer%0AAuthor%3A%20Thien-Qua%20T.%20Nguyen%20and%20Hieu-Nghia%20Nguyen%20and%20Thanh-Hieu%20Bui%20and%20Thien%20B.%20Nguyen-Tat%20and%20Vuong%20M.%20Ngo%0AAbstract%3A%20%20%20This%20research%20presents%20an%20enhanced%20approach%20for%20precise%20segmentation%20of%20brain%0Atumor%20masses%20in%20magnetic%20resonance%20imaging%20%28MRI%29%20using%20an%20advanced%203D-UNet%0Amodel%20combined%20with%20a%20Context%20Transformer%20%28CoT%29.%20By%20architectural%20expansion%0ACoT%2C%20the%20proposed%20model%20extends%20its%20architecture%20to%20a%203D%20format%2C%20integrates%20it%0Asmoothly%20with%20the%20base%20model%20to%20utilize%20the%20complex%20contextual%20information%0Afound%20in%20MRI%20scans%2C%20emphasizing%20how%20elements%20rely%20on%20each%20other%20across%20an%0Aextended%20spatial%20range.%20The%20proposed%20model%20synchronizes%20tumor%20mass%0Acharacteristics%20from%20CoT%2C%20mutually%20reinforcing%20feature%20extraction%2C%20facilitating%0Athe%20precise%20capture%20of%20detailed%20tumor%20mass%20structures%2C%20including%20location%2C%0Asize%2C%20and%20boundaries.%20Several%20experimental%20results%20present%20the%20outstanding%0Asegmentation%20performance%20of%20the%20proposed%20method%20in%20comparison%20to%20current%0Astate-of-the-art%20approaches%2C%20achieving%20Dice%20score%20of%2082.0%25%2C%2081.5%25%2C%2089.0%25%20for%0AEnhancing%20Tumor%2C%20Tumor%20Core%20and%20Whole%20Tumor%2C%20respectively%2C%20on%20BraTS2019.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Tumor%2520Segmentation%2520in%2520MRI%2520Images%2520with%25203D%2520U-Net%2520and%2520Contextual%250A%2520%2520Transformer%26entry.906535625%3DThien-Qua%2520T.%2520Nguyen%2520and%2520Hieu-Nghia%2520Nguyen%2520and%2520Thanh-Hieu%2520Bui%2520and%2520Thien%2520B.%2520Nguyen-Tat%2520and%2520Vuong%2520M.%2520Ngo%26entry.1292438233%3D%2520%2520This%2520research%2520presents%2520an%2520enhanced%2520approach%2520for%2520precise%2520segmentation%2520of%2520brain%250Atumor%2520masses%2520in%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520using%2520an%2520advanced%25203D-UNet%250Amodel%2520combined%2520with%2520a%2520Context%2520Transformer%2520%2528CoT%2529.%2520By%2520architectural%2520expansion%250ACoT%252C%2520the%2520proposed%2520model%2520extends%2520its%2520architecture%2520to%2520a%25203D%2520format%252C%2520integrates%2520it%250Asmoothly%2520with%2520the%2520base%2520model%2520to%2520utilize%2520the%2520complex%2520contextual%2520information%250Afound%2520in%2520MRI%2520scans%252C%2520emphasizing%2520how%2520elements%2520rely%2520on%2520each%2520other%2520across%2520an%250Aextended%2520spatial%2520range.%2520The%2520proposed%2520model%2520synchronizes%2520tumor%2520mass%250Acharacteristics%2520from%2520CoT%252C%2520mutually%2520reinforcing%2520feature%2520extraction%252C%2520facilitating%250Athe%2520precise%2520capture%2520of%2520detailed%2520tumor%2520mass%2520structures%252C%2520including%2520location%252C%250Asize%252C%2520and%2520boundaries.%2520Several%2520experimental%2520results%2520present%2520the%2520outstanding%250Asegmentation%2520performance%2520of%2520the%2520proposed%2520method%2520in%2520comparison%2520to%2520current%250Astate-of-the-art%2520approaches%252C%2520achieving%2520Dice%2520score%2520of%252082.0%2525%252C%252081.5%2525%252C%252089.0%2525%2520for%250AEnhancing%2520Tumor%252C%2520Tumor%2520Core%2520and%2520Whole%2520Tumor%252C%2520respectively%252C%2520on%2520BraTS2019.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Tumor%20Segmentation%20in%20MRI%20Images%20with%203D%20U-Net%20and%20Contextual%0A%20%20Transformer&entry.906535625=Thien-Qua%20T.%20Nguyen%20and%20Hieu-Nghia%20Nguyen%20and%20Thanh-Hieu%20Bui%20and%20Thien%20B.%20Nguyen-Tat%20and%20Vuong%20M.%20Ngo&entry.1292438233=%20%20This%20research%20presents%20an%20enhanced%20approach%20for%20precise%20segmentation%20of%20brain%0Atumor%20masses%20in%20magnetic%20resonance%20imaging%20%28MRI%29%20using%20an%20advanced%203D-UNet%0Amodel%20combined%20with%20a%20Context%20Transformer%20%28CoT%29.%20By%20architectural%20expansion%0ACoT%2C%20the%20proposed%20model%20extends%20its%20architecture%20to%20a%203D%20format%2C%20integrates%20it%0Asmoothly%20with%20the%20base%20model%20to%20utilize%20the%20complex%20contextual%20information%0Afound%20in%20MRI%20scans%2C%20emphasizing%20how%20elements%20rely%20on%20each%20other%20across%20an%0Aextended%20spatial%20range.%20The%20proposed%20model%20synchronizes%20tumor%20mass%0Acharacteristics%20from%20CoT%2C%20mutually%20reinforcing%20feature%20extraction%2C%20facilitating%0Athe%20precise%20capture%20of%20detailed%20tumor%20mass%20structures%2C%20including%20location%2C%0Asize%2C%20and%20boundaries.%20Several%20experimental%20results%20present%20the%20outstanding%0Asegmentation%20performance%20of%20the%20proposed%20method%20in%20comparison%20to%20current%0Astate-of-the-art%20approaches%2C%20achieving%20Dice%20score%20of%2082.0%25%2C%2081.5%25%2C%2089.0%25%20for%0AEnhancing%20Tumor%2C%20Tumor%20Core%20and%20Whole%20Tumor%2C%20respectively%2C%20on%20BraTS2019.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08470v1&entry.124074799=Read"},
{"title": "EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary\n  Algorithms", "author": "Siyu Yuan and Kaitao Song and Jiangjie Chen and Xu Tan and Dongsheng Li and Deqing Yang", "abstract": "  The rise of powerful large language models (LLMs) has spurred a new trend in\nbuilding LLM-based autonomous agents for solving complex tasks, especially\nmulti-agent systems. Despite the remarkable progress, we notice that existing\nworks are heavily dependent on human-designed frameworks, which greatly limits\nthe functional scope and scalability of agent systems. How to automatically\nextend the specialized agent to multi-agent systems to improve task-solving\ncapability still remains a significant challenge. In this paper, we introduce\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\nframeworks as the initial individual and then apply a series of evolutionary\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\nagents with diverse agent settings. EvoAgent can be generalized to any\nLLM-based agent framework, and can automatically extend the existing agent\nframework to multi-agent systems without any extra human designs. Experimental\nresults across various tasks have shown that EvoAgent can automatically\ngenerate multiple expert agents and significantly enhance the task-solving\ncapabilities of LLM-based agents.\n", "link": "http://arxiv.org/abs/2406.14228v2", "date": "2024-07-11", "relevancy": 2.0629, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5334}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5205}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoAgent%3A%20Towards%20Automatic%20Multi-Agent%20Generation%20via%20Evolutionary%0A%20%20Algorithms&body=Title%3A%20EvoAgent%3A%20Towards%20Automatic%20Multi-Agent%20Generation%20via%20Evolutionary%0A%20%20Algorithms%0AAuthor%3A%20Siyu%20Yuan%20and%20Kaitao%20Song%20and%20Jiangjie%20Chen%20and%20Xu%20Tan%20and%20Dongsheng%20Li%20and%20Deqing%20Yang%0AAbstract%3A%20%20%20The%20rise%20of%20powerful%20large%20language%20models%20%28LLMs%29%20has%20spurred%20a%20new%20trend%20in%0Abuilding%20LLM-based%20autonomous%20agents%20for%20solving%20complex%20tasks%2C%20especially%0Amulti-agent%20systems.%20Despite%20the%20remarkable%20progress%2C%20we%20notice%20that%20existing%0Aworks%20are%20heavily%20dependent%20on%20human-designed%20frameworks%2C%20which%20greatly%20limits%0Athe%20functional%20scope%20and%20scalability%20of%20agent%20systems.%20How%20to%20automatically%0Aextend%20the%20specialized%20agent%20to%20multi-agent%20systems%20to%20improve%20task-solving%0Acapability%20still%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20introduce%0AEvoAgent%2C%20a%20generic%20method%20to%20automatically%20extend%20expert%20agents%20to%20multi-agent%0Asystems%20via%20the%20evolutionary%20algorithm%2C%20thereby%20improving%20the%20effectiveness%20of%0ALLM-based%20agents%20in%20solving%20tasks.%20Specifically%2C%20we%20consider%20the%20existing%20agent%0Aframeworks%20as%20the%20initial%20individual%20and%20then%20apply%20a%20series%20of%20evolutionary%0Aoperators%20%28e.g.%2C%20mutation%2C%20crossover%2C%20selection%2C%20etc.%29%20to%20generate%20multiple%0Aagents%20with%20diverse%20agent%20settings.%20EvoAgent%20can%20be%20generalized%20to%20any%0ALLM-based%20agent%20framework%2C%20and%20can%20automatically%20extend%20the%20existing%20agent%0Aframework%20to%20multi-agent%20systems%20without%20any%20extra%20human%20designs.%20Experimental%0Aresults%20across%20various%20tasks%20have%20shown%20that%20EvoAgent%20can%20automatically%0Agenerate%20multiple%20expert%20agents%20and%20significantly%20enhance%20the%20task-solving%0Acapabilities%20of%20LLM-based%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoAgent%253A%2520Towards%2520Automatic%2520Multi-Agent%2520Generation%2520via%2520Evolutionary%250A%2520%2520Algorithms%26entry.906535625%3DSiyu%2520Yuan%2520and%2520Kaitao%2520Song%2520and%2520Jiangjie%2520Chen%2520and%2520Xu%2520Tan%2520and%2520Dongsheng%2520Li%2520and%2520Deqing%2520Yang%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520powerful%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520spurred%2520a%2520new%2520trend%2520in%250Abuilding%2520LLM-based%2520autonomous%2520agents%2520for%2520solving%2520complex%2520tasks%252C%2520especially%250Amulti-agent%2520systems.%2520Despite%2520the%2520remarkable%2520progress%252C%2520we%2520notice%2520that%2520existing%250Aworks%2520are%2520heavily%2520dependent%2520on%2520human-designed%2520frameworks%252C%2520which%2520greatly%2520limits%250Athe%2520functional%2520scope%2520and%2520scalability%2520of%2520agent%2520systems.%2520How%2520to%2520automatically%250Aextend%2520the%2520specialized%2520agent%2520to%2520multi-agent%2520systems%2520to%2520improve%2520task-solving%250Acapability%2520still%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AEvoAgent%252C%2520a%2520generic%2520method%2520to%2520automatically%2520extend%2520expert%2520agents%2520to%2520multi-agent%250Asystems%2520via%2520the%2520evolutionary%2520algorithm%252C%2520thereby%2520improving%2520the%2520effectiveness%2520of%250ALLM-based%2520agents%2520in%2520solving%2520tasks.%2520Specifically%252C%2520we%2520consider%2520the%2520existing%2520agent%250Aframeworks%2520as%2520the%2520initial%2520individual%2520and%2520then%2520apply%2520a%2520series%2520of%2520evolutionary%250Aoperators%2520%2528e.g.%252C%2520mutation%252C%2520crossover%252C%2520selection%252C%2520etc.%2529%2520to%2520generate%2520multiple%250Aagents%2520with%2520diverse%2520agent%2520settings.%2520EvoAgent%2520can%2520be%2520generalized%2520to%2520any%250ALLM-based%2520agent%2520framework%252C%2520and%2520can%2520automatically%2520extend%2520the%2520existing%2520agent%250Aframework%2520to%2520multi-agent%2520systems%2520without%2520any%2520extra%2520human%2520designs.%2520Experimental%250Aresults%2520across%2520various%2520tasks%2520have%2520shown%2520that%2520EvoAgent%2520can%2520automatically%250Agenerate%2520multiple%2520expert%2520agents%2520and%2520significantly%2520enhance%2520the%2520task-solving%250Acapabilities%2520of%2520LLM-based%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoAgent%3A%20Towards%20Automatic%20Multi-Agent%20Generation%20via%20Evolutionary%0A%20%20Algorithms&entry.906535625=Siyu%20Yuan%20and%20Kaitao%20Song%20and%20Jiangjie%20Chen%20and%20Xu%20Tan%20and%20Dongsheng%20Li%20and%20Deqing%20Yang&entry.1292438233=%20%20The%20rise%20of%20powerful%20large%20language%20models%20%28LLMs%29%20has%20spurred%20a%20new%20trend%20in%0Abuilding%20LLM-based%20autonomous%20agents%20for%20solving%20complex%20tasks%2C%20especially%0Amulti-agent%20systems.%20Despite%20the%20remarkable%20progress%2C%20we%20notice%20that%20existing%0Aworks%20are%20heavily%20dependent%20on%20human-designed%20frameworks%2C%20which%20greatly%20limits%0Athe%20functional%20scope%20and%20scalability%20of%20agent%20systems.%20How%20to%20automatically%0Aextend%20the%20specialized%20agent%20to%20multi-agent%20systems%20to%20improve%20task-solving%0Acapability%20still%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20introduce%0AEvoAgent%2C%20a%20generic%20method%20to%20automatically%20extend%20expert%20agents%20to%20multi-agent%0Asystems%20via%20the%20evolutionary%20algorithm%2C%20thereby%20improving%20the%20effectiveness%20of%0ALLM-based%20agents%20in%20solving%20tasks.%20Specifically%2C%20we%20consider%20the%20existing%20agent%0Aframeworks%20as%20the%20initial%20individual%20and%20then%20apply%20a%20series%20of%20evolutionary%0Aoperators%20%28e.g.%2C%20mutation%2C%20crossover%2C%20selection%2C%20etc.%29%20to%20generate%20multiple%0Aagents%20with%20diverse%20agent%20settings.%20EvoAgent%20can%20be%20generalized%20to%20any%0ALLM-based%20agent%20framework%2C%20and%20can%20automatically%20extend%20the%20existing%20agent%0Aframework%20to%20multi-agent%20systems%20without%20any%20extra%20human%20designs.%20Experimental%0Aresults%20across%20various%20tasks%20have%20shown%20that%20EvoAgent%20can%20automatically%0Agenerate%20multiple%20expert%20agents%20and%20significantly%20enhance%20the%20task-solving%0Acapabilities%20of%20LLM-based%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14228v2&entry.124074799=Read"},
{"title": "How more data can hurt: Instability and regularization in\n  next-generation reservoir computing", "author": "Yuanzhao Zhang and Sean P. Cornelius", "abstract": "  It has been found recently that more data can, counter-intuitively, hurt the\nperformance of deep neural networks. Here, we show that a more extreme version\nof the phenomenon occurs in data-driven models of dynamical systems. To\nelucidate the underlying mechanism, we focus on next-generation reservoir\ncomputing (NGRC) -- a popular framework for learning dynamics from data. We\nfind that, despite learning a better representation of the flow map with more\ntraining data, NGRC can adopt an ill-conditioned ``integrator'' and lose\nstability. We link this data-induced instability to the auxiliary dimensions\ncreated by the delayed states in NGRC. Based on these findings, we propose\nsimple strategies to mitigate the instability, either by increasing\nregularization strength in tandem with data size, or by carefully introducing\nnoise during training. Our results highlight the importance of proper\nregularization in data-driven modeling of dynamical systems.\n", "link": "http://arxiv.org/abs/2407.08641v1", "date": "2024-07-11", "relevancy": 2.056, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5519}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5189}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20more%20data%20can%20hurt%3A%20Instability%20and%20regularization%20in%0A%20%20next-generation%20reservoir%20computing&body=Title%3A%20How%20more%20data%20can%20hurt%3A%20Instability%20and%20regularization%20in%0A%20%20next-generation%20reservoir%20computing%0AAuthor%3A%20Yuanzhao%20Zhang%20and%20Sean%20P.%20Cornelius%0AAbstract%3A%20%20%20It%20has%20been%20found%20recently%20that%20more%20data%20can%2C%20counter-intuitively%2C%20hurt%20the%0Aperformance%20of%20deep%20neural%20networks.%20Here%2C%20we%20show%20that%20a%20more%20extreme%20version%0Aof%20the%20phenomenon%20occurs%20in%20data-driven%20models%20of%20dynamical%20systems.%20To%0Aelucidate%20the%20underlying%20mechanism%2C%20we%20focus%20on%20next-generation%20reservoir%0Acomputing%20%28NGRC%29%20--%20a%20popular%20framework%20for%20learning%20dynamics%20from%20data.%20We%0Afind%20that%2C%20despite%20learning%20a%20better%20representation%20of%20the%20flow%20map%20with%20more%0Atraining%20data%2C%20NGRC%20can%20adopt%20an%20ill-conditioned%20%60%60integrator%27%27%20and%20lose%0Astability.%20We%20link%20this%20data-induced%20instability%20to%20the%20auxiliary%20dimensions%0Acreated%20by%20the%20delayed%20states%20in%20NGRC.%20Based%20on%20these%20findings%2C%20we%20propose%0Asimple%20strategies%20to%20mitigate%20the%20instability%2C%20either%20by%20increasing%0Aregularization%20strength%20in%20tandem%20with%20data%20size%2C%20or%20by%20carefully%20introducing%0Anoise%20during%20training.%20Our%20results%20highlight%20the%20importance%20of%20proper%0Aregularization%20in%20data-driven%20modeling%20of%20dynamical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520more%2520data%2520can%2520hurt%253A%2520Instability%2520and%2520regularization%2520in%250A%2520%2520next-generation%2520reservoir%2520computing%26entry.906535625%3DYuanzhao%2520Zhang%2520and%2520Sean%2520P.%2520Cornelius%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520found%2520recently%2520that%2520more%2520data%2520can%252C%2520counter-intuitively%252C%2520hurt%2520the%250Aperformance%2520of%2520deep%2520neural%2520networks.%2520Here%252C%2520we%2520show%2520that%2520a%2520more%2520extreme%2520version%250Aof%2520the%2520phenomenon%2520occurs%2520in%2520data-driven%2520models%2520of%2520dynamical%2520systems.%2520To%250Aelucidate%2520the%2520underlying%2520mechanism%252C%2520we%2520focus%2520on%2520next-generation%2520reservoir%250Acomputing%2520%2528NGRC%2529%2520--%2520a%2520popular%2520framework%2520for%2520learning%2520dynamics%2520from%2520data.%2520We%250Afind%2520that%252C%2520despite%2520learning%2520a%2520better%2520representation%2520of%2520the%2520flow%2520map%2520with%2520more%250Atraining%2520data%252C%2520NGRC%2520can%2520adopt%2520an%2520ill-conditioned%2520%2560%2560integrator%2527%2527%2520and%2520lose%250Astability.%2520We%2520link%2520this%2520data-induced%2520instability%2520to%2520the%2520auxiliary%2520dimensions%250Acreated%2520by%2520the%2520delayed%2520states%2520in%2520NGRC.%2520Based%2520on%2520these%2520findings%252C%2520we%2520propose%250Asimple%2520strategies%2520to%2520mitigate%2520the%2520instability%252C%2520either%2520by%2520increasing%250Aregularization%2520strength%2520in%2520tandem%2520with%2520data%2520size%252C%2520or%2520by%2520carefully%2520introducing%250Anoise%2520during%2520training.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520proper%250Aregularization%2520in%2520data-driven%2520modeling%2520of%2520dynamical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20more%20data%20can%20hurt%3A%20Instability%20and%20regularization%20in%0A%20%20next-generation%20reservoir%20computing&entry.906535625=Yuanzhao%20Zhang%20and%20Sean%20P.%20Cornelius&entry.1292438233=%20%20It%20has%20been%20found%20recently%20that%20more%20data%20can%2C%20counter-intuitively%2C%20hurt%20the%0Aperformance%20of%20deep%20neural%20networks.%20Here%2C%20we%20show%20that%20a%20more%20extreme%20version%0Aof%20the%20phenomenon%20occurs%20in%20data-driven%20models%20of%20dynamical%20systems.%20To%0Aelucidate%20the%20underlying%20mechanism%2C%20we%20focus%20on%20next-generation%20reservoir%0Acomputing%20%28NGRC%29%20--%20a%20popular%20framework%20for%20learning%20dynamics%20from%20data.%20We%0Afind%20that%2C%20despite%20learning%20a%20better%20representation%20of%20the%20flow%20map%20with%20more%0Atraining%20data%2C%20NGRC%20can%20adopt%20an%20ill-conditioned%20%60%60integrator%27%27%20and%20lose%0Astability.%20We%20link%20this%20data-induced%20instability%20to%20the%20auxiliary%20dimensions%0Acreated%20by%20the%20delayed%20states%20in%20NGRC.%20Based%20on%20these%20findings%2C%20we%20propose%0Asimple%20strategies%20to%20mitigate%20the%20instability%2C%20either%20by%20increasing%0Aregularization%20strength%20in%20tandem%20with%20data%20size%2C%20or%20by%20carefully%20introducing%0Anoise%20during%20training.%20Our%20results%20highlight%20the%20importance%20of%20proper%0Aregularization%20in%20data-driven%20modeling%20of%20dynamical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08641v1&entry.124074799=Read"},
{"title": "Using deep neural networks to detect non-analytically defined expert\n  event labels in canoe sprint force sensor signals", "author": "Sarah Rockstrok and Patrick Frenzel and Daniel Matthes and Kay Schubert and David Wollburg and Mirco Fuchs", "abstract": "  Assessing an athlete's performance in canoe sprint is often established by\nmeasuring a variety of kinematic parameters during training sessions. Many of\nthese parameters are related to single or multiple paddle stroke cycles.\nDetermining on- and offset of these cycles in force sensor signals is usually\nnot straightforward and requires human interaction. This paper explores\nconvolutional neural networks (CNNs) and recurrent neural networks (RNNs) in\nterms of their ability to automatically predict these events. In addition, our\nwork proposes an extension to the recently published SoftED metric for event\ndetection in order to properly assess the model performance on time windows. In\nour results, an RNN based on bidirectional gated recurrent units (BGRUs) turned\nout to be the most suitable model for paddle stroke detection.\n", "link": "http://arxiv.org/abs/2407.08395v1", "date": "2024-07-11", "relevancy": 2.0536, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5243}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20deep%20neural%20networks%20to%20detect%20non-analytically%20defined%20expert%0A%20%20event%20labels%20in%20canoe%20sprint%20force%20sensor%20signals&body=Title%3A%20Using%20deep%20neural%20networks%20to%20detect%20non-analytically%20defined%20expert%0A%20%20event%20labels%20in%20canoe%20sprint%20force%20sensor%20signals%0AAuthor%3A%20Sarah%20Rockstrok%20and%20Patrick%20Frenzel%20and%20Daniel%20Matthes%20and%20Kay%20Schubert%20and%20David%20Wollburg%20and%20Mirco%20Fuchs%0AAbstract%3A%20%20%20Assessing%20an%20athlete%27s%20performance%20in%20canoe%20sprint%20is%20often%20established%20by%0Ameasuring%20a%20variety%20of%20kinematic%20parameters%20during%20training%20sessions.%20Many%20of%0Athese%20parameters%20are%20related%20to%20single%20or%20multiple%20paddle%20stroke%20cycles.%0ADetermining%20on-%20and%20offset%20of%20these%20cycles%20in%20force%20sensor%20signals%20is%20usually%0Anot%20straightforward%20and%20requires%20human%20interaction.%20This%20paper%20explores%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20recurrent%20neural%20networks%20%28RNNs%29%20in%0Aterms%20of%20their%20ability%20to%20automatically%20predict%20these%20events.%20In%20addition%2C%20our%0Awork%20proposes%20an%20extension%20to%20the%20recently%20published%20SoftED%20metric%20for%20event%0Adetection%20in%20order%20to%20properly%20assess%20the%20model%20performance%20on%20time%20windows.%20In%0Aour%20results%2C%20an%20RNN%20based%20on%20bidirectional%20gated%20recurrent%20units%20%28BGRUs%29%20turned%0Aout%20to%20be%20the%20most%20suitable%20model%20for%20paddle%20stroke%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520deep%2520neural%2520networks%2520to%2520detect%2520non-analytically%2520defined%2520expert%250A%2520%2520event%2520labels%2520in%2520canoe%2520sprint%2520force%2520sensor%2520signals%26entry.906535625%3DSarah%2520Rockstrok%2520and%2520Patrick%2520Frenzel%2520and%2520Daniel%2520Matthes%2520and%2520Kay%2520Schubert%2520and%2520David%2520Wollburg%2520and%2520Mirco%2520Fuchs%26entry.1292438233%3D%2520%2520Assessing%2520an%2520athlete%2527s%2520performance%2520in%2520canoe%2520sprint%2520is%2520often%2520established%2520by%250Ameasuring%2520a%2520variety%2520of%2520kinematic%2520parameters%2520during%2520training%2520sessions.%2520Many%2520of%250Athese%2520parameters%2520are%2520related%2520to%2520single%2520or%2520multiple%2520paddle%2520stroke%2520cycles.%250ADetermining%2520on-%2520and%2520offset%2520of%2520these%2520cycles%2520in%2520force%2520sensor%2520signals%2520is%2520usually%250Anot%2520straightforward%2520and%2520requires%2520human%2520interaction.%2520This%2520paper%2520explores%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520in%250Aterms%2520of%2520their%2520ability%2520to%2520automatically%2520predict%2520these%2520events.%2520In%2520addition%252C%2520our%250Awork%2520proposes%2520an%2520extension%2520to%2520the%2520recently%2520published%2520SoftED%2520metric%2520for%2520event%250Adetection%2520in%2520order%2520to%2520properly%2520assess%2520the%2520model%2520performance%2520on%2520time%2520windows.%2520In%250Aour%2520results%252C%2520an%2520RNN%2520based%2520on%2520bidirectional%2520gated%2520recurrent%2520units%2520%2528BGRUs%2529%2520turned%250Aout%2520to%2520be%2520the%2520most%2520suitable%2520model%2520for%2520paddle%2520stroke%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20deep%20neural%20networks%20to%20detect%20non-analytically%20defined%20expert%0A%20%20event%20labels%20in%20canoe%20sprint%20force%20sensor%20signals&entry.906535625=Sarah%20Rockstrok%20and%20Patrick%20Frenzel%20and%20Daniel%20Matthes%20and%20Kay%20Schubert%20and%20David%20Wollburg%20and%20Mirco%20Fuchs&entry.1292438233=%20%20Assessing%20an%20athlete%27s%20performance%20in%20canoe%20sprint%20is%20often%20established%20by%0Ameasuring%20a%20variety%20of%20kinematic%20parameters%20during%20training%20sessions.%20Many%20of%0Athese%20parameters%20are%20related%20to%20single%20or%20multiple%20paddle%20stroke%20cycles.%0ADetermining%20on-%20and%20offset%20of%20these%20cycles%20in%20force%20sensor%20signals%20is%20usually%0Anot%20straightforward%20and%20requires%20human%20interaction.%20This%20paper%20explores%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20recurrent%20neural%20networks%20%28RNNs%29%20in%0Aterms%20of%20their%20ability%20to%20automatically%20predict%20these%20events.%20In%20addition%2C%20our%0Awork%20proposes%20an%20extension%20to%20the%20recently%20published%20SoftED%20metric%20for%20event%0Adetection%20in%20order%20to%20properly%20assess%20the%20model%20performance%20on%20time%20windows.%20In%0Aour%20results%2C%20an%20RNN%20based%20on%20bidirectional%20gated%20recurrent%20units%20%28BGRUs%29%20turned%0Aout%20to%20be%20the%20most%20suitable%20model%20for%20paddle%20stroke%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08395v1&entry.124074799=Read"},
{"title": "DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis\n  through Structure Guidance", "author": "Younghyun Kim and Geunmin Hwang and Junyu Zhang and Eunbyung Park", "abstract": "  Recent surge in large-scale generative models has spurred the development of\nvast fields in computer vision. In particular, text-to-image diffusion models\nhave garnered widespread adoption across diverse domain due to their potential\nfor high-fidelity image generation. Nonetheless, existing large-scale diffusion\nmodels are confined to generate images of up to 1K resolution, which is far\nfrom meeting the demands of contemporary commercial applications. Directly\nsampling higher-resolution images often yields results marred by artifacts such\nas object repetition and distorted shapes. Addressing the aforementioned issues\ntypically necessitates training or fine-tuning models on higher resolution\ndatasets. However, this undertaking poses a formidable challenge due to the\ndifficulty in collecting large-scale high-resolution contents and substantial\ncomputational resources. While several preceding works have proposed\nalternatives, they often fail to produce convincing results. In this work, we\nprobe the generative ability of diffusion models at higher resolution beyond\nits original capability and propose a novel progressive approach that fully\nutilizes generated low-resolution image to guide the generation of higher\nresolution image. Our method obviates the need for additional training or\nfine-tuning which significantly lowers the burden of computational costs.\nExtensive experiments and results validate the efficiency and efficacy of our\nmethod. Project page: https://yhyun225.github.io/DiffuseHigh/\n", "link": "http://arxiv.org/abs/2406.18459v4", "date": "2024-07-11", "relevancy": 2.0489, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.695}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6801}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffuseHigh%3A%20Training-free%20Progressive%20High-Resolution%20Image%20Synthesis%0A%20%20through%20Structure%20Guidance&body=Title%3A%20DiffuseHigh%3A%20Training-free%20Progressive%20High-Resolution%20Image%20Synthesis%0A%20%20through%20Structure%20Guidance%0AAuthor%3A%20Younghyun%20Kim%20and%20Geunmin%20Hwang%20and%20Junyu%20Zhang%20and%20Eunbyung%20Park%0AAbstract%3A%20%20%20Recent%20surge%20in%20large-scale%20generative%20models%20has%20spurred%20the%20development%20of%0Avast%20fields%20in%20computer%20vision.%20In%20particular%2C%20text-to-image%20diffusion%20models%0Ahave%20garnered%20widespread%20adoption%20across%20diverse%20domain%20due%20to%20their%20potential%0Afor%20high-fidelity%20image%20generation.%20Nonetheless%2C%20existing%20large-scale%20diffusion%0Amodels%20are%20confined%20to%20generate%20images%20of%20up%20to%201K%20resolution%2C%20which%20is%20far%0Afrom%20meeting%20the%20demands%20of%20contemporary%20commercial%20applications.%20Directly%0Asampling%20higher-resolution%20images%20often%20yields%20results%20marred%20by%20artifacts%20such%0Aas%20object%20repetition%20and%20distorted%20shapes.%20Addressing%20the%20aforementioned%20issues%0Atypically%20necessitates%20training%20or%20fine-tuning%20models%20on%20higher%20resolution%0Adatasets.%20However%2C%20this%20undertaking%20poses%20a%20formidable%20challenge%20due%20to%20the%0Adifficulty%20in%20collecting%20large-scale%20high-resolution%20contents%20and%20substantial%0Acomputational%20resources.%20While%20several%20preceding%20works%20have%20proposed%0Aalternatives%2C%20they%20often%20fail%20to%20produce%20convincing%20results.%20In%20this%20work%2C%20we%0Aprobe%20the%20generative%20ability%20of%20diffusion%20models%20at%20higher%20resolution%20beyond%0Aits%20original%20capability%20and%20propose%20a%20novel%20progressive%20approach%20that%20fully%0Autilizes%20generated%20low-resolution%20image%20to%20guide%20the%20generation%20of%20higher%0Aresolution%20image.%20Our%20method%20obviates%20the%20need%20for%20additional%20training%20or%0Afine-tuning%20which%20significantly%20lowers%20the%20burden%20of%20computational%20costs.%0AExtensive%20experiments%20and%20results%20validate%20the%20efficiency%20and%20efficacy%20of%20our%0Amethod.%20Project%20page%3A%20https%3A//yhyun225.github.io/DiffuseHigh/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18459v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffuseHigh%253A%2520Training-free%2520Progressive%2520High-Resolution%2520Image%2520Synthesis%250A%2520%2520through%2520Structure%2520Guidance%26entry.906535625%3DYounghyun%2520Kim%2520and%2520Geunmin%2520Hwang%2520and%2520Junyu%2520Zhang%2520and%2520Eunbyung%2520Park%26entry.1292438233%3D%2520%2520Recent%2520surge%2520in%2520large-scale%2520generative%2520models%2520has%2520spurred%2520the%2520development%2520of%250Avast%2520fields%2520in%2520computer%2520vision.%2520In%2520particular%252C%2520text-to-image%2520diffusion%2520models%250Ahave%2520garnered%2520widespread%2520adoption%2520across%2520diverse%2520domain%2520due%2520to%2520their%2520potential%250Afor%2520high-fidelity%2520image%2520generation.%2520Nonetheless%252C%2520existing%2520large-scale%2520diffusion%250Amodels%2520are%2520confined%2520to%2520generate%2520images%2520of%2520up%2520to%25201K%2520resolution%252C%2520which%2520is%2520far%250Afrom%2520meeting%2520the%2520demands%2520of%2520contemporary%2520commercial%2520applications.%2520Directly%250Asampling%2520higher-resolution%2520images%2520often%2520yields%2520results%2520marred%2520by%2520artifacts%2520such%250Aas%2520object%2520repetition%2520and%2520distorted%2520shapes.%2520Addressing%2520the%2520aforementioned%2520issues%250Atypically%2520necessitates%2520training%2520or%2520fine-tuning%2520models%2520on%2520higher%2520resolution%250Adatasets.%2520However%252C%2520this%2520undertaking%2520poses%2520a%2520formidable%2520challenge%2520due%2520to%2520the%250Adifficulty%2520in%2520collecting%2520large-scale%2520high-resolution%2520contents%2520and%2520substantial%250Acomputational%2520resources.%2520While%2520several%2520preceding%2520works%2520have%2520proposed%250Aalternatives%252C%2520they%2520often%2520fail%2520to%2520produce%2520convincing%2520results.%2520In%2520this%2520work%252C%2520we%250Aprobe%2520the%2520generative%2520ability%2520of%2520diffusion%2520models%2520at%2520higher%2520resolution%2520beyond%250Aits%2520original%2520capability%2520and%2520propose%2520a%2520novel%2520progressive%2520approach%2520that%2520fully%250Autilizes%2520generated%2520low-resolution%2520image%2520to%2520guide%2520the%2520generation%2520of%2520higher%250Aresolution%2520image.%2520Our%2520method%2520obviates%2520the%2520need%2520for%2520additional%2520training%2520or%250Afine-tuning%2520which%2520significantly%2520lowers%2520the%2520burden%2520of%2520computational%2520costs.%250AExtensive%2520experiments%2520and%2520results%2520validate%2520the%2520efficiency%2520and%2520efficacy%2520of%2520our%250Amethod.%2520Project%2520page%253A%2520https%253A//yhyun225.github.io/DiffuseHigh/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18459v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffuseHigh%3A%20Training-free%20Progressive%20High-Resolution%20Image%20Synthesis%0A%20%20through%20Structure%20Guidance&entry.906535625=Younghyun%20Kim%20and%20Geunmin%20Hwang%20and%20Junyu%20Zhang%20and%20Eunbyung%20Park&entry.1292438233=%20%20Recent%20surge%20in%20large-scale%20generative%20models%20has%20spurred%20the%20development%20of%0Avast%20fields%20in%20computer%20vision.%20In%20particular%2C%20text-to-image%20diffusion%20models%0Ahave%20garnered%20widespread%20adoption%20across%20diverse%20domain%20due%20to%20their%20potential%0Afor%20high-fidelity%20image%20generation.%20Nonetheless%2C%20existing%20large-scale%20diffusion%0Amodels%20are%20confined%20to%20generate%20images%20of%20up%20to%201K%20resolution%2C%20which%20is%20far%0Afrom%20meeting%20the%20demands%20of%20contemporary%20commercial%20applications.%20Directly%0Asampling%20higher-resolution%20images%20often%20yields%20results%20marred%20by%20artifacts%20such%0Aas%20object%20repetition%20and%20distorted%20shapes.%20Addressing%20the%20aforementioned%20issues%0Atypically%20necessitates%20training%20or%20fine-tuning%20models%20on%20higher%20resolution%0Adatasets.%20However%2C%20this%20undertaking%20poses%20a%20formidable%20challenge%20due%20to%20the%0Adifficulty%20in%20collecting%20large-scale%20high-resolution%20contents%20and%20substantial%0Acomputational%20resources.%20While%20several%20preceding%20works%20have%20proposed%0Aalternatives%2C%20they%20often%20fail%20to%20produce%20convincing%20results.%20In%20this%20work%2C%20we%0Aprobe%20the%20generative%20ability%20of%20diffusion%20models%20at%20higher%20resolution%20beyond%0Aits%20original%20capability%20and%20propose%20a%20novel%20progressive%20approach%20that%20fully%0Autilizes%20generated%20low-resolution%20image%20to%20guide%20the%20generation%20of%20higher%0Aresolution%20image.%20Our%20method%20obviates%20the%20need%20for%20additional%20training%20or%0Afine-tuning%20which%20significantly%20lowers%20the%20burden%20of%20computational%20costs.%0AExtensive%20experiments%20and%20results%20validate%20the%20efficiency%20and%20efficacy%20of%20our%0Amethod.%20Project%20page%3A%20https%3A//yhyun225.github.io/DiffuseHigh/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18459v4&entry.124074799=Read"},
{"title": "OMR-NET: a two-stage octave multi-scale residual network for screen\n  content image compression", "author": "Shiqi Jiang and Ting Ren and Congrui Fu and Shuai Li and Hui Yuan", "abstract": "  Screen content (SC) differs from natural scene (NS) with unique\ncharacteristics such as noise-free, repetitive patterns, and high contrast.\nAiming at addressing the inadequacies of current learned image compression\n(LIC) methods for SC, we propose an improved two-stage octave convolutional\nresidual blocks (IToRB) for high and low-frequency feature extraction and a\ncascaded two-stage multi-scale residual blocks (CTMSRB) for improved\nmulti-scale learning and nonlinearity in SC. Additionally, we employ a\nwindow-based attention module (WAM) to capture pixel correlations, especially\nfor high contrast regions in the image. We also construct a diverse SC image\ncompression dataset (SDU-SCICD2K) for training, including text, charts,\ngraphics, animation, movie, game and mixture of SC images and NS images.\nExperimental results show our method, more suited for SC than NS data,\noutperforms existing LIC methods in rate-distortion performance on SC images.\nThe code is publicly available at https://github.com/SunshineSki/OMR Net.git.\n", "link": "http://arxiv.org/abs/2407.08545v1", "date": "2024-07-11", "relevancy": 2.0366, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5249}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5088}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMR-NET%3A%20a%20two-stage%20octave%20multi-scale%20residual%20network%20for%20screen%0A%20%20content%20image%20compression&body=Title%3A%20OMR-NET%3A%20a%20two-stage%20octave%20multi-scale%20residual%20network%20for%20screen%0A%20%20content%20image%20compression%0AAuthor%3A%20Shiqi%20Jiang%20and%20Ting%20Ren%20and%20Congrui%20Fu%20and%20Shuai%20Li%20and%20Hui%20Yuan%0AAbstract%3A%20%20%20Screen%20content%20%28SC%29%20differs%20from%20natural%20scene%20%28NS%29%20with%20unique%0Acharacteristics%20such%20as%20noise-free%2C%20repetitive%20patterns%2C%20and%20high%20contrast.%0AAiming%20at%20addressing%20the%20inadequacies%20of%20current%20learned%20image%20compression%0A%28LIC%29%20methods%20for%20SC%2C%20we%20propose%20an%20improved%20two-stage%20octave%20convolutional%0Aresidual%20blocks%20%28IToRB%29%20for%20high%20and%20low-frequency%20feature%20extraction%20and%20a%0Acascaded%20two-stage%20multi-scale%20residual%20blocks%20%28CTMSRB%29%20for%20improved%0Amulti-scale%20learning%20and%20nonlinearity%20in%20SC.%20Additionally%2C%20we%20employ%20a%0Awindow-based%20attention%20module%20%28WAM%29%20to%20capture%20pixel%20correlations%2C%20especially%0Afor%20high%20contrast%20regions%20in%20the%20image.%20We%20also%20construct%20a%20diverse%20SC%20image%0Acompression%20dataset%20%28SDU-SCICD2K%29%20for%20training%2C%20including%20text%2C%20charts%2C%0Agraphics%2C%20animation%2C%20movie%2C%20game%20and%20mixture%20of%20SC%20images%20and%20NS%20images.%0AExperimental%20results%20show%20our%20method%2C%20more%20suited%20for%20SC%20than%20NS%20data%2C%0Aoutperforms%20existing%20LIC%20methods%20in%20rate-distortion%20performance%20on%20SC%20images.%0AThe%20code%20is%20publicly%20available%20at%20https%3A//github.com/SunshineSki/OMR%20Net.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMR-NET%253A%2520a%2520two-stage%2520octave%2520multi-scale%2520residual%2520network%2520for%2520screen%250A%2520%2520content%2520image%2520compression%26entry.906535625%3DShiqi%2520Jiang%2520and%2520Ting%2520Ren%2520and%2520Congrui%2520Fu%2520and%2520Shuai%2520Li%2520and%2520Hui%2520Yuan%26entry.1292438233%3D%2520%2520Screen%2520content%2520%2528SC%2529%2520differs%2520from%2520natural%2520scene%2520%2528NS%2529%2520with%2520unique%250Acharacteristics%2520such%2520as%2520noise-free%252C%2520repetitive%2520patterns%252C%2520and%2520high%2520contrast.%250AAiming%2520at%2520addressing%2520the%2520inadequacies%2520of%2520current%2520learned%2520image%2520compression%250A%2528LIC%2529%2520methods%2520for%2520SC%252C%2520we%2520propose%2520an%2520improved%2520two-stage%2520octave%2520convolutional%250Aresidual%2520blocks%2520%2528IToRB%2529%2520for%2520high%2520and%2520low-frequency%2520feature%2520extraction%2520and%2520a%250Acascaded%2520two-stage%2520multi-scale%2520residual%2520blocks%2520%2528CTMSRB%2529%2520for%2520improved%250Amulti-scale%2520learning%2520and%2520nonlinearity%2520in%2520SC.%2520Additionally%252C%2520we%2520employ%2520a%250Awindow-based%2520attention%2520module%2520%2528WAM%2529%2520to%2520capture%2520pixel%2520correlations%252C%2520especially%250Afor%2520high%2520contrast%2520regions%2520in%2520the%2520image.%2520We%2520also%2520construct%2520a%2520diverse%2520SC%2520image%250Acompression%2520dataset%2520%2528SDU-SCICD2K%2529%2520for%2520training%252C%2520including%2520text%252C%2520charts%252C%250Agraphics%252C%2520animation%252C%2520movie%252C%2520game%2520and%2520mixture%2520of%2520SC%2520images%2520and%2520NS%2520images.%250AExperimental%2520results%2520show%2520our%2520method%252C%2520more%2520suited%2520for%2520SC%2520than%2520NS%2520data%252C%250Aoutperforms%2520existing%2520LIC%2520methods%2520in%2520rate-distortion%2520performance%2520on%2520SC%2520images.%250AThe%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/SunshineSki/OMR%2520Net.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMR-NET%3A%20a%20two-stage%20octave%20multi-scale%20residual%20network%20for%20screen%0A%20%20content%20image%20compression&entry.906535625=Shiqi%20Jiang%20and%20Ting%20Ren%20and%20Congrui%20Fu%20and%20Shuai%20Li%20and%20Hui%20Yuan&entry.1292438233=%20%20Screen%20content%20%28SC%29%20differs%20from%20natural%20scene%20%28NS%29%20with%20unique%0Acharacteristics%20such%20as%20noise-free%2C%20repetitive%20patterns%2C%20and%20high%20contrast.%0AAiming%20at%20addressing%20the%20inadequacies%20of%20current%20learned%20image%20compression%0A%28LIC%29%20methods%20for%20SC%2C%20we%20propose%20an%20improved%20two-stage%20octave%20convolutional%0Aresidual%20blocks%20%28IToRB%29%20for%20high%20and%20low-frequency%20feature%20extraction%20and%20a%0Acascaded%20two-stage%20multi-scale%20residual%20blocks%20%28CTMSRB%29%20for%20improved%0Amulti-scale%20learning%20and%20nonlinearity%20in%20SC.%20Additionally%2C%20we%20employ%20a%0Awindow-based%20attention%20module%20%28WAM%29%20to%20capture%20pixel%20correlations%2C%20especially%0Afor%20high%20contrast%20regions%20in%20the%20image.%20We%20also%20construct%20a%20diverse%20SC%20image%0Acompression%20dataset%20%28SDU-SCICD2K%29%20for%20training%2C%20including%20text%2C%20charts%2C%0Agraphics%2C%20animation%2C%20movie%2C%20game%20and%20mixture%20of%20SC%20images%20and%20NS%20images.%0AExperimental%20results%20show%20our%20method%2C%20more%20suited%20for%20SC%20than%20NS%20data%2C%0Aoutperforms%20existing%20LIC%20methods%20in%20rate-distortion%20performance%20on%20SC%20images.%0AThe%20code%20is%20publicly%20available%20at%20https%3A//github.com/SunshineSki/OMR%20Net.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08545v1&entry.124074799=Read"},
{"title": "BiasPruner: Debiased Continual Learning for Medical Image Classification", "author": "Nourhan Bayasi and Jamil Fayyad and Alceu Bissoto and Ghassan Hamarneh and Rafeef Garbi", "abstract": "  Continual Learning (CL) is crucial for enabling networks to dynamically adapt\nas they learn new tasks sequentially, accommodating new data and classes\nwithout catastrophic forgetting. Diverging from conventional perspectives on\nCL, our paper introduces a new perspective wherein forgetting could actually\nbenefit the sequential learning paradigm. Specifically, we present BiasPruner,\na CL framework that intentionally forgets spurious correlations in the training\ndata that could lead to shortcut learning. Utilizing a new bias score that\nmeasures the contribution of each unit in the network to learning spurious\nfeatures, BiasPruner prunes those units with the highest bias scores to form a\ndebiased subnetwork preserved for a given task. As BiasPruner learns a new\ntask, it constructs a new debiased subnetwork, potentially incorporating units\nfrom previous subnetworks, which improves adaptation and performance on the new\ntask. During inference, BiasPruner employs a simple task-agnostic approach to\nselect the best debiased subnetwork for predictions. We conduct experiments on\nthree medical datasets for skin lesion classification and chest X-Ray\nclassification and demonstrate that BiasPruner consistently outperforms SOTA CL\nmethods in terms of classification performance and fairness. Our code is\navailable here.\n", "link": "http://arxiv.org/abs/2407.08609v1", "date": "2024-07-11", "relevancy": 2.0269, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5178}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5018}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiasPruner%3A%20Debiased%20Continual%20Learning%20for%20Medical%20Image%20Classification&body=Title%3A%20BiasPruner%3A%20Debiased%20Continual%20Learning%20for%20Medical%20Image%20Classification%0AAuthor%3A%20Nourhan%20Bayasi%20and%20Jamil%20Fayyad%20and%20Alceu%20Bissoto%20and%20Ghassan%20Hamarneh%20and%20Rafeef%20Garbi%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20is%20crucial%20for%20enabling%20networks%20to%20dynamically%20adapt%0Aas%20they%20learn%20new%20tasks%20sequentially%2C%20accommodating%20new%20data%20and%20classes%0Awithout%20catastrophic%20forgetting.%20Diverging%20from%20conventional%20perspectives%20on%0ACL%2C%20our%20paper%20introduces%20a%20new%20perspective%20wherein%20forgetting%20could%20actually%0Abenefit%20the%20sequential%20learning%20paradigm.%20Specifically%2C%20we%20present%20BiasPruner%2C%0Aa%20CL%20framework%20that%20intentionally%20forgets%20spurious%20correlations%20in%20the%20training%0Adata%20that%20could%20lead%20to%20shortcut%20learning.%20Utilizing%20a%20new%20bias%20score%20that%0Ameasures%20the%20contribution%20of%20each%20unit%20in%20the%20network%20to%20learning%20spurious%0Afeatures%2C%20BiasPruner%20prunes%20those%20units%20with%20the%20highest%20bias%20scores%20to%20form%20a%0Adebiased%20subnetwork%20preserved%20for%20a%20given%20task.%20As%20BiasPruner%20learns%20a%20new%0Atask%2C%20it%20constructs%20a%20new%20debiased%20subnetwork%2C%20potentially%20incorporating%20units%0Afrom%20previous%20subnetworks%2C%20which%20improves%20adaptation%20and%20performance%20on%20the%20new%0Atask.%20During%20inference%2C%20BiasPruner%20employs%20a%20simple%20task-agnostic%20approach%20to%0Aselect%20the%20best%20debiased%20subnetwork%20for%20predictions.%20We%20conduct%20experiments%20on%0Athree%20medical%20datasets%20for%20skin%20lesion%20classification%20and%20chest%20X-Ray%0Aclassification%20and%20demonstrate%20that%20BiasPruner%20consistently%20outperforms%20SOTA%20CL%0Amethods%20in%20terms%20of%20classification%20performance%20and%20fairness.%20Our%20code%20is%0Aavailable%20here.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiasPruner%253A%2520Debiased%2520Continual%2520Learning%2520for%2520Medical%2520Image%2520Classification%26entry.906535625%3DNourhan%2520Bayasi%2520and%2520Jamil%2520Fayyad%2520and%2520Alceu%2520Bissoto%2520and%2520Ghassan%2520Hamarneh%2520and%2520Rafeef%2520Garbi%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520is%2520crucial%2520for%2520enabling%2520networks%2520to%2520dynamically%2520adapt%250Aas%2520they%2520learn%2520new%2520tasks%2520sequentially%252C%2520accommodating%2520new%2520data%2520and%2520classes%250Awithout%2520catastrophic%2520forgetting.%2520Diverging%2520from%2520conventional%2520perspectives%2520on%250ACL%252C%2520our%2520paper%2520introduces%2520a%2520new%2520perspective%2520wherein%2520forgetting%2520could%2520actually%250Abenefit%2520the%2520sequential%2520learning%2520paradigm.%2520Specifically%252C%2520we%2520present%2520BiasPruner%252C%250Aa%2520CL%2520framework%2520that%2520intentionally%2520forgets%2520spurious%2520correlations%2520in%2520the%2520training%250Adata%2520that%2520could%2520lead%2520to%2520shortcut%2520learning.%2520Utilizing%2520a%2520new%2520bias%2520score%2520that%250Ameasures%2520the%2520contribution%2520of%2520each%2520unit%2520in%2520the%2520network%2520to%2520learning%2520spurious%250Afeatures%252C%2520BiasPruner%2520prunes%2520those%2520units%2520with%2520the%2520highest%2520bias%2520scores%2520to%2520form%2520a%250Adebiased%2520subnetwork%2520preserved%2520for%2520a%2520given%2520task.%2520As%2520BiasPruner%2520learns%2520a%2520new%250Atask%252C%2520it%2520constructs%2520a%2520new%2520debiased%2520subnetwork%252C%2520potentially%2520incorporating%2520units%250Afrom%2520previous%2520subnetworks%252C%2520which%2520improves%2520adaptation%2520and%2520performance%2520on%2520the%2520new%250Atask.%2520During%2520inference%252C%2520BiasPruner%2520employs%2520a%2520simple%2520task-agnostic%2520approach%2520to%250Aselect%2520the%2520best%2520debiased%2520subnetwork%2520for%2520predictions.%2520We%2520conduct%2520experiments%2520on%250Athree%2520medical%2520datasets%2520for%2520skin%2520lesion%2520classification%2520and%2520chest%2520X-Ray%250Aclassification%2520and%2520demonstrate%2520that%2520BiasPruner%2520consistently%2520outperforms%2520SOTA%2520CL%250Amethods%2520in%2520terms%2520of%2520classification%2520performance%2520and%2520fairness.%2520Our%2520code%2520is%250Aavailable%2520here.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiasPruner%3A%20Debiased%20Continual%20Learning%20for%20Medical%20Image%20Classification&entry.906535625=Nourhan%20Bayasi%20and%20Jamil%20Fayyad%20and%20Alceu%20Bissoto%20and%20Ghassan%20Hamarneh%20and%20Rafeef%20Garbi&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20is%20crucial%20for%20enabling%20networks%20to%20dynamically%20adapt%0Aas%20they%20learn%20new%20tasks%20sequentially%2C%20accommodating%20new%20data%20and%20classes%0Awithout%20catastrophic%20forgetting.%20Diverging%20from%20conventional%20perspectives%20on%0ACL%2C%20our%20paper%20introduces%20a%20new%20perspective%20wherein%20forgetting%20could%20actually%0Abenefit%20the%20sequential%20learning%20paradigm.%20Specifically%2C%20we%20present%20BiasPruner%2C%0Aa%20CL%20framework%20that%20intentionally%20forgets%20spurious%20correlations%20in%20the%20training%0Adata%20that%20could%20lead%20to%20shortcut%20learning.%20Utilizing%20a%20new%20bias%20score%20that%0Ameasures%20the%20contribution%20of%20each%20unit%20in%20the%20network%20to%20learning%20spurious%0Afeatures%2C%20BiasPruner%20prunes%20those%20units%20with%20the%20highest%20bias%20scores%20to%20form%20a%0Adebiased%20subnetwork%20preserved%20for%20a%20given%20task.%20As%20BiasPruner%20learns%20a%20new%0Atask%2C%20it%20constructs%20a%20new%20debiased%20subnetwork%2C%20potentially%20incorporating%20units%0Afrom%20previous%20subnetworks%2C%20which%20improves%20adaptation%20and%20performance%20on%20the%20new%0Atask.%20During%20inference%2C%20BiasPruner%20employs%20a%20simple%20task-agnostic%20approach%20to%0Aselect%20the%20best%20debiased%20subnetwork%20for%20predictions.%20We%20conduct%20experiments%20on%0Athree%20medical%20datasets%20for%20skin%20lesion%20classification%20and%20chest%20X-Ray%0Aclassification%20and%20demonstrate%20that%20BiasPruner%20consistently%20outperforms%20SOTA%20CL%0Amethods%20in%20terms%20of%20classification%20performance%20and%20fairness.%20Our%20code%20is%0Aavailable%20here.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08609v1&entry.124074799=Read"},
{"title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural\n  Networks", "author": "Rui-Jie Zhu and Qihang Zhao and Guoqi Li and Jason K. Eshraghian", "abstract": "  As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking Neural Networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the Receptance Weighted Key Value (RWKV)\nlanguage model, we successfully implement `SpikeGPT', a generative language\nmodel with binary, event-driven spiking activation units. We train the proposed\nmodel on two model variants: 45M and 216M parameters. To the best of our\nknowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,\nrendering it suitable for both the generation and comprehension of natural\nlanguage. We achieve this by modifying the transformer block to replace\nmulti-head self attention to reduce quadratic computational complexity O(N^2)\nto linear complexity O(N) with increasing sequence length. Input tokens are\ninstead streamed in sequentially to our attention mechanism (as with typical\nSNNs). Our preliminary experiments show that SpikeGPT remains competitive with\nnon-spiking models on tested benchmarks, while maintaining 20x fewer operations\nwhen processed on neuromorphic hardware that can leverage sparse, event-driven\nactivations. Our code implementation is available at\nhttps://github.com/ridgerchu/SpikeGPT.\n", "link": "http://arxiv.org/abs/2302.13939v5", "date": "2024-07-11", "relevancy": 2.0252, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5294}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5115}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeGPT%3A%20Generative%20Pre-trained%20Language%20Model%20with%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20SpikeGPT%3A%20Generative%20Pre-trained%20Language%20Model%20with%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Rui-Jie%20Zhu%20and%20Qihang%20Zhao%20and%20Guoqi%20Li%20and%20Jason%20K.%20Eshraghian%0AAbstract%3A%20%20%20As%20the%20size%20of%20large%20language%20models%20continue%20to%20scale%2C%20so%20does%20the%0Acomputational%20resources%20required%20to%20run%20it.%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%0Aemerged%20as%20an%20energy-efficient%20approach%20to%20deep%20learning%20that%20leverage%20sparse%0Aand%20event-driven%20activations%20to%20reduce%20the%20computational%20overhead%20associated%0Awith%20model%20inference.%20While%20they%20have%20become%20competitive%20with%20non-spiking%0Amodels%20on%20many%20computer%20vision%20tasks%2C%20SNNs%20have%20also%20proven%20to%20be%20more%0Achallenging%20to%20train.%20As%20a%20result%2C%20their%20performance%20lags%20behind%20modern%20deep%0Alearning%2C%20and%20we%20are%20yet%20to%20see%20the%20effectiveness%20of%20SNNs%20in%20language%0Ageneration.%20In%20this%20paper%2C%20inspired%20by%20the%20Receptance%20Weighted%20Key%20Value%20%28RWKV%29%0Alanguage%20model%2C%20we%20successfully%20implement%20%60SpikeGPT%27%2C%20a%20generative%20language%0Amodel%20with%20binary%2C%20event-driven%20spiking%20activation%20units.%20We%20train%20the%20proposed%0Amodel%20on%20two%20model%20variants%3A%2045M%20and%20216M%20parameters.%20To%20the%20best%20of%20our%0Aknowledge%2C%20SpikeGPT%20is%20the%20largest%20backpropagation-trained%20SNN%20model%20to%20date%2C%0Arendering%20it%20suitable%20for%20both%20the%20generation%20and%20comprehension%20of%20natural%0Alanguage.%20We%20achieve%20this%20by%20modifying%20the%20transformer%20block%20to%20replace%0Amulti-head%20self%20attention%20to%20reduce%20quadratic%20computational%20complexity%20O%28N%5E2%29%0Ato%20linear%20complexity%20O%28N%29%20with%20increasing%20sequence%20length.%20Input%20tokens%20are%0Ainstead%20streamed%20in%20sequentially%20to%20our%20attention%20mechanism%20%28as%20with%20typical%0ASNNs%29.%20Our%20preliminary%20experiments%20show%20that%20SpikeGPT%20remains%20competitive%20with%0Anon-spiking%20models%20on%20tested%20benchmarks%2C%20while%20maintaining%2020x%20fewer%20operations%0Awhen%20processed%20on%20neuromorphic%20hardware%20that%20can%20leverage%20sparse%2C%20event-driven%0Aactivations.%20Our%20code%20implementation%20is%20available%20at%0Ahttps%3A//github.com/ridgerchu/SpikeGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.13939v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeGPT%253A%2520Generative%2520Pre-trained%2520Language%2520Model%2520with%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DRui-Jie%2520Zhu%2520and%2520Qihang%2520Zhao%2520and%2520Guoqi%2520Li%2520and%2520Jason%2520K.%2520Eshraghian%26entry.1292438233%3D%2520%2520As%2520the%2520size%2520of%2520large%2520language%2520models%2520continue%2520to%2520scale%252C%2520so%2520does%2520the%250Acomputational%2520resources%2520required%2520to%2520run%2520it.%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520have%250Aemerged%2520as%2520an%2520energy-efficient%2520approach%2520to%2520deep%2520learning%2520that%2520leverage%2520sparse%250Aand%2520event-driven%2520activations%2520to%2520reduce%2520the%2520computational%2520overhead%2520associated%250Awith%2520model%2520inference.%2520While%2520they%2520have%2520become%2520competitive%2520with%2520non-spiking%250Amodels%2520on%2520many%2520computer%2520vision%2520tasks%252C%2520SNNs%2520have%2520also%2520proven%2520to%2520be%2520more%250Achallenging%2520to%2520train.%2520As%2520a%2520result%252C%2520their%2520performance%2520lags%2520behind%2520modern%2520deep%250Alearning%252C%2520and%2520we%2520are%2520yet%2520to%2520see%2520the%2520effectiveness%2520of%2520SNNs%2520in%2520language%250Ageneration.%2520In%2520this%2520paper%252C%2520inspired%2520by%2520the%2520Receptance%2520Weighted%2520Key%2520Value%2520%2528RWKV%2529%250Alanguage%2520model%252C%2520we%2520successfully%2520implement%2520%2560SpikeGPT%2527%252C%2520a%2520generative%2520language%250Amodel%2520with%2520binary%252C%2520event-driven%2520spiking%2520activation%2520units.%2520We%2520train%2520the%2520proposed%250Amodel%2520on%2520two%2520model%2520variants%253A%252045M%2520and%2520216M%2520parameters.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520SpikeGPT%2520is%2520the%2520largest%2520backpropagation-trained%2520SNN%2520model%2520to%2520date%252C%250Arendering%2520it%2520suitable%2520for%2520both%2520the%2520generation%2520and%2520comprehension%2520of%2520natural%250Alanguage.%2520We%2520achieve%2520this%2520by%2520modifying%2520the%2520transformer%2520block%2520to%2520replace%250Amulti-head%2520self%2520attention%2520to%2520reduce%2520quadratic%2520computational%2520complexity%2520O%2528N%255E2%2529%250Ato%2520linear%2520complexity%2520O%2528N%2529%2520with%2520increasing%2520sequence%2520length.%2520Input%2520tokens%2520are%250Ainstead%2520streamed%2520in%2520sequentially%2520to%2520our%2520attention%2520mechanism%2520%2528as%2520with%2520typical%250ASNNs%2529.%2520Our%2520preliminary%2520experiments%2520show%2520that%2520SpikeGPT%2520remains%2520competitive%2520with%250Anon-spiking%2520models%2520on%2520tested%2520benchmarks%252C%2520while%2520maintaining%252020x%2520fewer%2520operations%250Awhen%2520processed%2520on%2520neuromorphic%2520hardware%2520that%2520can%2520leverage%2520sparse%252C%2520event-driven%250Aactivations.%2520Our%2520code%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/ridgerchu/SpikeGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.13939v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeGPT%3A%20Generative%20Pre-trained%20Language%20Model%20with%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Rui-Jie%20Zhu%20and%20Qihang%20Zhao%20and%20Guoqi%20Li%20and%20Jason%20K.%20Eshraghian&entry.1292438233=%20%20As%20the%20size%20of%20large%20language%20models%20continue%20to%20scale%2C%20so%20does%20the%0Acomputational%20resources%20required%20to%20run%20it.%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%0Aemerged%20as%20an%20energy-efficient%20approach%20to%20deep%20learning%20that%20leverage%20sparse%0Aand%20event-driven%20activations%20to%20reduce%20the%20computational%20overhead%20associated%0Awith%20model%20inference.%20While%20they%20have%20become%20competitive%20with%20non-spiking%0Amodels%20on%20many%20computer%20vision%20tasks%2C%20SNNs%20have%20also%20proven%20to%20be%20more%0Achallenging%20to%20train.%20As%20a%20result%2C%20their%20performance%20lags%20behind%20modern%20deep%0Alearning%2C%20and%20we%20are%20yet%20to%20see%20the%20effectiveness%20of%20SNNs%20in%20language%0Ageneration.%20In%20this%20paper%2C%20inspired%20by%20the%20Receptance%20Weighted%20Key%20Value%20%28RWKV%29%0Alanguage%20model%2C%20we%20successfully%20implement%20%60SpikeGPT%27%2C%20a%20generative%20language%0Amodel%20with%20binary%2C%20event-driven%20spiking%20activation%20units.%20We%20train%20the%20proposed%0Amodel%20on%20two%20model%20variants%3A%2045M%20and%20216M%20parameters.%20To%20the%20best%20of%20our%0Aknowledge%2C%20SpikeGPT%20is%20the%20largest%20backpropagation-trained%20SNN%20model%20to%20date%2C%0Arendering%20it%20suitable%20for%20both%20the%20generation%20and%20comprehension%20of%20natural%0Alanguage.%20We%20achieve%20this%20by%20modifying%20the%20transformer%20block%20to%20replace%0Amulti-head%20self%20attention%20to%20reduce%20quadratic%20computational%20complexity%20O%28N%5E2%29%0Ato%20linear%20complexity%20O%28N%29%20with%20increasing%20sequence%20length.%20Input%20tokens%20are%0Ainstead%20streamed%20in%20sequentially%20to%20our%20attention%20mechanism%20%28as%20with%20typical%0ASNNs%29.%20Our%20preliminary%20experiments%20show%20that%20SpikeGPT%20remains%20competitive%20with%0Anon-spiking%20models%20on%20tested%20benchmarks%2C%20while%20maintaining%2020x%20fewer%20operations%0Awhen%20processed%20on%20neuromorphic%20hardware%20that%20can%20leverage%20sparse%2C%20event-driven%0Aactivations.%20Our%20code%20implementation%20is%20available%20at%0Ahttps%3A//github.com/ridgerchu/SpikeGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.13939v5&entry.124074799=Read"},
{"title": "CLEO: Continual Learning of Evolving Ontologies", "author": "Shishir Muralidhara and Saqib Bukhari and Georg Schneider and Didier Stricker and Ren\u00e9 Schuster", "abstract": "  Continual learning (CL) addresses the problem of catastrophic forgetting in\nneural networks, which occurs when a trained model tends to overwrite\npreviously learned information, when presented with a new task. CL aims to\ninstill the lifelong learning characteristic of humans in intelligent systems,\nmaking them capable of learning continuously while retaining what was already\nlearned. Current CL problems involve either learning new domains\n(domain-incremental) or new and previously unseen classes (class-incremental).\nHowever, general learning processes are not just limited to learning\ninformation, but also refinement of existing information. In this paper, we\ndefine CLEO - Continual Learning of Evolving Ontologies, as a new incremental\nlearning setting under CL to tackle evolving classes. CLEO is motivated by the\nneed for intelligent systems to adapt to real-world ontologies that change over\ntime, such as those in autonomous driving. We use Cityscapes, PASCAL VOC, and\nMapillary Vistas to define the task settings and demonstrate the applicability\nof CLEO. We highlight the shortcomings of existing CIL methods in adapting to\nCLEO and propose a baseline solution, called Modelling Ontologies (MoOn). CLEO\nis a promising new approach to CL that addresses the challenge of evolving\nontologies in real-world applications. MoOn surpasses previous CL approaches in\nthe context of CLEO.\n", "link": "http://arxiv.org/abs/2407.08411v1", "date": "2024-07-11", "relevancy": 2.0214, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5106}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLEO%3A%20Continual%20Learning%20of%20Evolving%20Ontologies&body=Title%3A%20CLEO%3A%20Continual%20Learning%20of%20Evolving%20Ontologies%0AAuthor%3A%20Shishir%20Muralidhara%20and%20Saqib%20Bukhari%20and%20Georg%20Schneider%20and%20Didier%20Stricker%20and%20Ren%C3%A9%20Schuster%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20addresses%20the%20problem%20of%20catastrophic%20forgetting%20in%0Aneural%20networks%2C%20which%20occurs%20when%20a%20trained%20model%20tends%20to%20overwrite%0Apreviously%20learned%20information%2C%20when%20presented%20with%20a%20new%20task.%20CL%20aims%20to%0Ainstill%20the%20lifelong%20learning%20characteristic%20of%20humans%20in%20intelligent%20systems%2C%0Amaking%20them%20capable%20of%20learning%20continuously%20while%20retaining%20what%20was%20already%0Alearned.%20Current%20CL%20problems%20involve%20either%20learning%20new%20domains%0A%28domain-incremental%29%20or%20new%20and%20previously%20unseen%20classes%20%28class-incremental%29.%0AHowever%2C%20general%20learning%20processes%20are%20not%20just%20limited%20to%20learning%0Ainformation%2C%20but%20also%20refinement%20of%20existing%20information.%20In%20this%20paper%2C%20we%0Adefine%20CLEO%20-%20Continual%20Learning%20of%20Evolving%20Ontologies%2C%20as%20a%20new%20incremental%0Alearning%20setting%20under%20CL%20to%20tackle%20evolving%20classes.%20CLEO%20is%20motivated%20by%20the%0Aneed%20for%20intelligent%20systems%20to%20adapt%20to%20real-world%20ontologies%20that%20change%20over%0Atime%2C%20such%20as%20those%20in%20autonomous%20driving.%20We%20use%20Cityscapes%2C%20PASCAL%20VOC%2C%20and%0AMapillary%20Vistas%20to%20define%20the%20task%20settings%20and%20demonstrate%20the%20applicability%0Aof%20CLEO.%20We%20highlight%20the%20shortcomings%20of%20existing%20CIL%20methods%20in%20adapting%20to%0ACLEO%20and%20propose%20a%20baseline%20solution%2C%20called%20Modelling%20Ontologies%20%28MoOn%29.%20CLEO%0Ais%20a%20promising%20new%20approach%20to%20CL%20that%20addresses%20the%20challenge%20of%20evolving%0Aontologies%20in%20real-world%20applications.%20MoOn%20surpasses%20previous%20CL%20approaches%20in%0Athe%20context%20of%20CLEO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLEO%253A%2520Continual%2520Learning%2520of%2520Evolving%2520Ontologies%26entry.906535625%3DShishir%2520Muralidhara%2520and%2520Saqib%2520Bukhari%2520and%2520Georg%2520Schneider%2520and%2520Didier%2520Stricker%2520and%2520Ren%25C3%25A9%2520Schuster%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520addresses%2520the%2520problem%2520of%2520catastrophic%2520forgetting%2520in%250Aneural%2520networks%252C%2520which%2520occurs%2520when%2520a%2520trained%2520model%2520tends%2520to%2520overwrite%250Apreviously%2520learned%2520information%252C%2520when%2520presented%2520with%2520a%2520new%2520task.%2520CL%2520aims%2520to%250Ainstill%2520the%2520lifelong%2520learning%2520characteristic%2520of%2520humans%2520in%2520intelligent%2520systems%252C%250Amaking%2520them%2520capable%2520of%2520learning%2520continuously%2520while%2520retaining%2520what%2520was%2520already%250Alearned.%2520Current%2520CL%2520problems%2520involve%2520either%2520learning%2520new%2520domains%250A%2528domain-incremental%2529%2520or%2520new%2520and%2520previously%2520unseen%2520classes%2520%2528class-incremental%2529.%250AHowever%252C%2520general%2520learning%2520processes%2520are%2520not%2520just%2520limited%2520to%2520learning%250Ainformation%252C%2520but%2520also%2520refinement%2520of%2520existing%2520information.%2520In%2520this%2520paper%252C%2520we%250Adefine%2520CLEO%2520-%2520Continual%2520Learning%2520of%2520Evolving%2520Ontologies%252C%2520as%2520a%2520new%2520incremental%250Alearning%2520setting%2520under%2520CL%2520to%2520tackle%2520evolving%2520classes.%2520CLEO%2520is%2520motivated%2520by%2520the%250Aneed%2520for%2520intelligent%2520systems%2520to%2520adapt%2520to%2520real-world%2520ontologies%2520that%2520change%2520over%250Atime%252C%2520such%2520as%2520those%2520in%2520autonomous%2520driving.%2520We%2520use%2520Cityscapes%252C%2520PASCAL%2520VOC%252C%2520and%250AMapillary%2520Vistas%2520to%2520define%2520the%2520task%2520settings%2520and%2520demonstrate%2520the%2520applicability%250Aof%2520CLEO.%2520We%2520highlight%2520the%2520shortcomings%2520of%2520existing%2520CIL%2520methods%2520in%2520adapting%2520to%250ACLEO%2520and%2520propose%2520a%2520baseline%2520solution%252C%2520called%2520Modelling%2520Ontologies%2520%2528MoOn%2529.%2520CLEO%250Ais%2520a%2520promising%2520new%2520approach%2520to%2520CL%2520that%2520addresses%2520the%2520challenge%2520of%2520evolving%250Aontologies%2520in%2520real-world%2520applications.%2520MoOn%2520surpasses%2520previous%2520CL%2520approaches%2520in%250Athe%2520context%2520of%2520CLEO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLEO%3A%20Continual%20Learning%20of%20Evolving%20Ontologies&entry.906535625=Shishir%20Muralidhara%20and%20Saqib%20Bukhari%20and%20Georg%20Schneider%20and%20Didier%20Stricker%20and%20Ren%C3%A9%20Schuster&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20addresses%20the%20problem%20of%20catastrophic%20forgetting%20in%0Aneural%20networks%2C%20which%20occurs%20when%20a%20trained%20model%20tends%20to%20overwrite%0Apreviously%20learned%20information%2C%20when%20presented%20with%20a%20new%20task.%20CL%20aims%20to%0Ainstill%20the%20lifelong%20learning%20characteristic%20of%20humans%20in%20intelligent%20systems%2C%0Amaking%20them%20capable%20of%20learning%20continuously%20while%20retaining%20what%20was%20already%0Alearned.%20Current%20CL%20problems%20involve%20either%20learning%20new%20domains%0A%28domain-incremental%29%20or%20new%20and%20previously%20unseen%20classes%20%28class-incremental%29.%0AHowever%2C%20general%20learning%20processes%20are%20not%20just%20limited%20to%20learning%0Ainformation%2C%20but%20also%20refinement%20of%20existing%20information.%20In%20this%20paper%2C%20we%0Adefine%20CLEO%20-%20Continual%20Learning%20of%20Evolving%20Ontologies%2C%20as%20a%20new%20incremental%0Alearning%20setting%20under%20CL%20to%20tackle%20evolving%20classes.%20CLEO%20is%20motivated%20by%20the%0Aneed%20for%20intelligent%20systems%20to%20adapt%20to%20real-world%20ontologies%20that%20change%20over%0Atime%2C%20such%20as%20those%20in%20autonomous%20driving.%20We%20use%20Cityscapes%2C%20PASCAL%20VOC%2C%20and%0AMapillary%20Vistas%20to%20define%20the%20task%20settings%20and%20demonstrate%20the%20applicability%0Aof%20CLEO.%20We%20highlight%20the%20shortcomings%20of%20existing%20CIL%20methods%20in%20adapting%20to%0ACLEO%20and%20propose%20a%20baseline%20solution%2C%20called%20Modelling%20Ontologies%20%28MoOn%29.%20CLEO%0Ais%20a%20promising%20new%20approach%20to%20CL%20that%20addresses%20the%20challenge%20of%20evolving%0Aontologies%20in%20real-world%20applications.%20MoOn%20surpasses%20previous%20CL%20approaches%20in%0Athe%20context%20of%20CLEO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08411v1&entry.124074799=Read"},
{"title": "Event-based vision on FPGAs -- a survey", "author": "Tomasz Kryjak", "abstract": "  In recent years there has been a growing interest in event cameras, i.e.\nvision sensors that record changes in illumination independently for each\npixel. This type of operation ensures that acquisition is possible in very\nadverse lighting conditions, both in low light and high dynamic range, and\nreduces average power consumption. In addition, the independent operation of\neach pixel results in low latency, which is desirable for robotic solutions.\nNowadays, Field Programmable Gate Arrays (FPGAs), along with general-purpose\nprocessors (GPPs/CPUs) and programmable graphics processing units (GPUs), are\npopular architectures for implementing and accelerating computing tasks. In\nparticular, their usefulness in the embedded vision domain has been repeatedly\ndemonstrated over the past 30 years, where they have enabled fast data\nprocessing (even in real-time) and energy efficiency. Hence, the combination of\nevent cameras and reconfigurable devices seems to be a good solution,\nespecially in the context of energy-efficient real-time embedded systems. This\npaper gives an overview of the most important works, where FPGAs have been used\nin different contexts to process event data. It covers applications in the\nfollowing areas: filtering, stereovision, optical flow, acceleration of\nAI-based algorithms (including spiking neural networks) for object\nclassification, detection and tracking, and applications in robotics and\ninspection systems. Current trends and challenges for such systems are also\ndiscussed.\n", "link": "http://arxiv.org/abs/2407.08356v1", "date": "2024-07-11", "relevancy": 2.0187, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-based%20vision%20on%20FPGAs%20--%20a%20survey&body=Title%3A%20Event-based%20vision%20on%20FPGAs%20--%20a%20survey%0AAuthor%3A%20Tomasz%20Kryjak%0AAbstract%3A%20%20%20In%20recent%20years%20there%20has%20been%20a%20growing%20interest%20in%20event%20cameras%2C%20i.e.%0Avision%20sensors%20that%20record%20changes%20in%20illumination%20independently%20for%20each%0Apixel.%20This%20type%20of%20operation%20ensures%20that%20acquisition%20is%20possible%20in%20very%0Aadverse%20lighting%20conditions%2C%20both%20in%20low%20light%20and%20high%20dynamic%20range%2C%20and%0Areduces%20average%20power%20consumption.%20In%20addition%2C%20the%20independent%20operation%20of%0Aeach%20pixel%20results%20in%20low%20latency%2C%20which%20is%20desirable%20for%20robotic%20solutions.%0ANowadays%2C%20Field%20Programmable%20Gate%20Arrays%20%28FPGAs%29%2C%20along%20with%20general-purpose%0Aprocessors%20%28GPPs/CPUs%29%20and%20programmable%20graphics%20processing%20units%20%28GPUs%29%2C%20are%0Apopular%20architectures%20for%20implementing%20and%20accelerating%20computing%20tasks.%20In%0Aparticular%2C%20their%20usefulness%20in%20the%20embedded%20vision%20domain%20has%20been%20repeatedly%0Ademonstrated%20over%20the%20past%2030%20years%2C%20where%20they%20have%20enabled%20fast%20data%0Aprocessing%20%28even%20in%20real-time%29%20and%20energy%20efficiency.%20Hence%2C%20the%20combination%20of%0Aevent%20cameras%20and%20reconfigurable%20devices%20seems%20to%20be%20a%20good%20solution%2C%0Aespecially%20in%20the%20context%20of%20energy-efficient%20real-time%20embedded%20systems.%20This%0Apaper%20gives%20an%20overview%20of%20the%20most%20important%20works%2C%20where%20FPGAs%20have%20been%20used%0Ain%20different%20contexts%20to%20process%20event%20data.%20It%20covers%20applications%20in%20the%0Afollowing%20areas%3A%20filtering%2C%20stereovision%2C%20optical%20flow%2C%20acceleration%20of%0AAI-based%20algorithms%20%28including%20spiking%20neural%20networks%29%20for%20object%0Aclassification%2C%20detection%20and%20tracking%2C%20and%20applications%20in%20robotics%20and%0Ainspection%20systems.%20Current%20trends%20and%20challenges%20for%20such%20systems%20are%20also%0Adiscussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-based%2520vision%2520on%2520FPGAs%2520--%2520a%2520survey%26entry.906535625%3DTomasz%2520Kryjak%26entry.1292438233%3D%2520%2520In%2520recent%2520years%2520there%2520has%2520been%2520a%2520growing%2520interest%2520in%2520event%2520cameras%252C%2520i.e.%250Avision%2520sensors%2520that%2520record%2520changes%2520in%2520illumination%2520independently%2520for%2520each%250Apixel.%2520This%2520type%2520of%2520operation%2520ensures%2520that%2520acquisition%2520is%2520possible%2520in%2520very%250Aadverse%2520lighting%2520conditions%252C%2520both%2520in%2520low%2520light%2520and%2520high%2520dynamic%2520range%252C%2520and%250Areduces%2520average%2520power%2520consumption.%2520In%2520addition%252C%2520the%2520independent%2520operation%2520of%250Aeach%2520pixel%2520results%2520in%2520low%2520latency%252C%2520which%2520is%2520desirable%2520for%2520robotic%2520solutions.%250ANowadays%252C%2520Field%2520Programmable%2520Gate%2520Arrays%2520%2528FPGAs%2529%252C%2520along%2520with%2520general-purpose%250Aprocessors%2520%2528GPPs/CPUs%2529%2520and%2520programmable%2520graphics%2520processing%2520units%2520%2528GPUs%2529%252C%2520are%250Apopular%2520architectures%2520for%2520implementing%2520and%2520accelerating%2520computing%2520tasks.%2520In%250Aparticular%252C%2520their%2520usefulness%2520in%2520the%2520embedded%2520vision%2520domain%2520has%2520been%2520repeatedly%250Ademonstrated%2520over%2520the%2520past%252030%2520years%252C%2520where%2520they%2520have%2520enabled%2520fast%2520data%250Aprocessing%2520%2528even%2520in%2520real-time%2529%2520and%2520energy%2520efficiency.%2520Hence%252C%2520the%2520combination%2520of%250Aevent%2520cameras%2520and%2520reconfigurable%2520devices%2520seems%2520to%2520be%2520a%2520good%2520solution%252C%250Aespecially%2520in%2520the%2520context%2520of%2520energy-efficient%2520real-time%2520embedded%2520systems.%2520This%250Apaper%2520gives%2520an%2520overview%2520of%2520the%2520most%2520important%2520works%252C%2520where%2520FPGAs%2520have%2520been%2520used%250Ain%2520different%2520contexts%2520to%2520process%2520event%2520data.%2520It%2520covers%2520applications%2520in%2520the%250Afollowing%2520areas%253A%2520filtering%252C%2520stereovision%252C%2520optical%2520flow%252C%2520acceleration%2520of%250AAI-based%2520algorithms%2520%2528including%2520spiking%2520neural%2520networks%2529%2520for%2520object%250Aclassification%252C%2520detection%2520and%2520tracking%252C%2520and%2520applications%2520in%2520robotics%2520and%250Ainspection%2520systems.%2520Current%2520trends%2520and%2520challenges%2520for%2520such%2520systems%2520are%2520also%250Adiscussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-based%20vision%20on%20FPGAs%20--%20a%20survey&entry.906535625=Tomasz%20Kryjak&entry.1292438233=%20%20In%20recent%20years%20there%20has%20been%20a%20growing%20interest%20in%20event%20cameras%2C%20i.e.%0Avision%20sensors%20that%20record%20changes%20in%20illumination%20independently%20for%20each%0Apixel.%20This%20type%20of%20operation%20ensures%20that%20acquisition%20is%20possible%20in%20very%0Aadverse%20lighting%20conditions%2C%20both%20in%20low%20light%20and%20high%20dynamic%20range%2C%20and%0Areduces%20average%20power%20consumption.%20In%20addition%2C%20the%20independent%20operation%20of%0Aeach%20pixel%20results%20in%20low%20latency%2C%20which%20is%20desirable%20for%20robotic%20solutions.%0ANowadays%2C%20Field%20Programmable%20Gate%20Arrays%20%28FPGAs%29%2C%20along%20with%20general-purpose%0Aprocessors%20%28GPPs/CPUs%29%20and%20programmable%20graphics%20processing%20units%20%28GPUs%29%2C%20are%0Apopular%20architectures%20for%20implementing%20and%20accelerating%20computing%20tasks.%20In%0Aparticular%2C%20their%20usefulness%20in%20the%20embedded%20vision%20domain%20has%20been%20repeatedly%0Ademonstrated%20over%20the%20past%2030%20years%2C%20where%20they%20have%20enabled%20fast%20data%0Aprocessing%20%28even%20in%20real-time%29%20and%20energy%20efficiency.%20Hence%2C%20the%20combination%20of%0Aevent%20cameras%20and%20reconfigurable%20devices%20seems%20to%20be%20a%20good%20solution%2C%0Aespecially%20in%20the%20context%20of%20energy-efficient%20real-time%20embedded%20systems.%20This%0Apaper%20gives%20an%20overview%20of%20the%20most%20important%20works%2C%20where%20FPGAs%20have%20been%20used%0Ain%20different%20contexts%20to%20process%20event%20data.%20It%20covers%20applications%20in%20the%0Afollowing%20areas%3A%20filtering%2C%20stereovision%2C%20optical%20flow%2C%20acceleration%20of%0AAI-based%20algorithms%20%28including%20spiking%20neural%20networks%29%20for%20object%0Aclassification%2C%20detection%20and%20tracking%2C%20and%20applications%20in%20robotics%20and%0Ainspection%20systems.%20Current%20trends%20and%20challenges%20for%20such%20systems%20are%20also%0Adiscussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08356v1&entry.124074799=Read"},
{"title": "ST-Mamba: Spatial-Temporal Mamba for Traffic Flow Estimation Recovery\n  using Limited Data", "author": "Doncheng Yuan and Jianzhe Xue and Jinshan Su and Wenchao Xu and Haibo Zhou", "abstract": "  Traffic flow estimation (TFE) is crucial for urban intelligent traffic\nsystems. While traditional on-road detectors are hindered by limited coverage\nand high costs, cloud computing and data mining of vehicular network data, such\nas driving speeds and GPS coordinates, present a promising and cost-effective\nalternative. Furthermore, minimizing data collection can significantly reduce\noverhead. However, limited data can lead to inaccuracies and instability in\nTFE. To address this, we introduce the spatial-temporal Mamba (ST-Mamba), a\ndeep learning model combining a convolutional neural network (CNN) with a Mamba\nframework. ST-Mamba is designed to enhance TFE accuracy and stability by\neffectively capturing the spatial-temporal patterns within traffic flow. Our\nmodel aims to achieve results comparable to those from extensive data sets\nwhile only utilizing minimal data. Simulations using real-world datasets have\nvalidated our model's ability to deliver precise and stable TFE across an urban\nlandscape based on limited data, establishing a cost-efficient solution for\nTFE.\n", "link": "http://arxiv.org/abs/2407.08558v1", "date": "2024-07-11", "relevancy": 2.0179, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5265}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.493}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ST-Mamba%3A%20Spatial-Temporal%20Mamba%20for%20Traffic%20Flow%20Estimation%20Recovery%0A%20%20using%20Limited%20Data&body=Title%3A%20ST-Mamba%3A%20Spatial-Temporal%20Mamba%20for%20Traffic%20Flow%20Estimation%20Recovery%0A%20%20using%20Limited%20Data%0AAuthor%3A%20Doncheng%20Yuan%20and%20Jianzhe%20Xue%20and%20Jinshan%20Su%20and%20Wenchao%20Xu%20and%20Haibo%20Zhou%0AAbstract%3A%20%20%20Traffic%20flow%20estimation%20%28TFE%29%20is%20crucial%20for%20urban%20intelligent%20traffic%0Asystems.%20While%20traditional%20on-road%20detectors%20are%20hindered%20by%20limited%20coverage%0Aand%20high%20costs%2C%20cloud%20computing%20and%20data%20mining%20of%20vehicular%20network%20data%2C%20such%0Aas%20driving%20speeds%20and%20GPS%20coordinates%2C%20present%20a%20promising%20and%20cost-effective%0Aalternative.%20Furthermore%2C%20minimizing%20data%20collection%20can%20significantly%20reduce%0Aoverhead.%20However%2C%20limited%20data%20can%20lead%20to%20inaccuracies%20and%20instability%20in%0ATFE.%20To%20address%20this%2C%20we%20introduce%20the%20spatial-temporal%20Mamba%20%28ST-Mamba%29%2C%20a%0Adeep%20learning%20model%20combining%20a%20convolutional%20neural%20network%20%28CNN%29%20with%20a%20Mamba%0Aframework.%20ST-Mamba%20is%20designed%20to%20enhance%20TFE%20accuracy%20and%20stability%20by%0Aeffectively%20capturing%20the%20spatial-temporal%20patterns%20within%20traffic%20flow.%20Our%0Amodel%20aims%20to%20achieve%20results%20comparable%20to%20those%20from%20extensive%20data%20sets%0Awhile%20only%20utilizing%20minimal%20data.%20Simulations%20using%20real-world%20datasets%20have%0Avalidated%20our%20model%27s%20ability%20to%20deliver%20precise%20and%20stable%20TFE%20across%20an%20urban%0Alandscape%20based%20on%20limited%20data%2C%20establishing%20a%20cost-efficient%20solution%20for%0ATFE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DST-Mamba%253A%2520Spatial-Temporal%2520Mamba%2520for%2520Traffic%2520Flow%2520Estimation%2520Recovery%250A%2520%2520using%2520Limited%2520Data%26entry.906535625%3DDoncheng%2520Yuan%2520and%2520Jianzhe%2520Xue%2520and%2520Jinshan%2520Su%2520and%2520Wenchao%2520Xu%2520and%2520Haibo%2520Zhou%26entry.1292438233%3D%2520%2520Traffic%2520flow%2520estimation%2520%2528TFE%2529%2520is%2520crucial%2520for%2520urban%2520intelligent%2520traffic%250Asystems.%2520While%2520traditional%2520on-road%2520detectors%2520are%2520hindered%2520by%2520limited%2520coverage%250Aand%2520high%2520costs%252C%2520cloud%2520computing%2520and%2520data%2520mining%2520of%2520vehicular%2520network%2520data%252C%2520such%250Aas%2520driving%2520speeds%2520and%2520GPS%2520coordinates%252C%2520present%2520a%2520promising%2520and%2520cost-effective%250Aalternative.%2520Furthermore%252C%2520minimizing%2520data%2520collection%2520can%2520significantly%2520reduce%250Aoverhead.%2520However%252C%2520limited%2520data%2520can%2520lead%2520to%2520inaccuracies%2520and%2520instability%2520in%250ATFE.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520spatial-temporal%2520Mamba%2520%2528ST-Mamba%2529%252C%2520a%250Adeep%2520learning%2520model%2520combining%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520with%2520a%2520Mamba%250Aframework.%2520ST-Mamba%2520is%2520designed%2520to%2520enhance%2520TFE%2520accuracy%2520and%2520stability%2520by%250Aeffectively%2520capturing%2520the%2520spatial-temporal%2520patterns%2520within%2520traffic%2520flow.%2520Our%250Amodel%2520aims%2520to%2520achieve%2520results%2520comparable%2520to%2520those%2520from%2520extensive%2520data%2520sets%250Awhile%2520only%2520utilizing%2520minimal%2520data.%2520Simulations%2520using%2520real-world%2520datasets%2520have%250Avalidated%2520our%2520model%2527s%2520ability%2520to%2520deliver%2520precise%2520and%2520stable%2520TFE%2520across%2520an%2520urban%250Alandscape%2520based%2520on%2520limited%2520data%252C%2520establishing%2520a%2520cost-efficient%2520solution%2520for%250ATFE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ST-Mamba%3A%20Spatial-Temporal%20Mamba%20for%20Traffic%20Flow%20Estimation%20Recovery%0A%20%20using%20Limited%20Data&entry.906535625=Doncheng%20Yuan%20and%20Jianzhe%20Xue%20and%20Jinshan%20Su%20and%20Wenchao%20Xu%20and%20Haibo%20Zhou&entry.1292438233=%20%20Traffic%20flow%20estimation%20%28TFE%29%20is%20crucial%20for%20urban%20intelligent%20traffic%0Asystems.%20While%20traditional%20on-road%20detectors%20are%20hindered%20by%20limited%20coverage%0Aand%20high%20costs%2C%20cloud%20computing%20and%20data%20mining%20of%20vehicular%20network%20data%2C%20such%0Aas%20driving%20speeds%20and%20GPS%20coordinates%2C%20present%20a%20promising%20and%20cost-effective%0Aalternative.%20Furthermore%2C%20minimizing%20data%20collection%20can%20significantly%20reduce%0Aoverhead.%20However%2C%20limited%20data%20can%20lead%20to%20inaccuracies%20and%20instability%20in%0ATFE.%20To%20address%20this%2C%20we%20introduce%20the%20spatial-temporal%20Mamba%20%28ST-Mamba%29%2C%20a%0Adeep%20learning%20model%20combining%20a%20convolutional%20neural%20network%20%28CNN%29%20with%20a%20Mamba%0Aframework.%20ST-Mamba%20is%20designed%20to%20enhance%20TFE%20accuracy%20and%20stability%20by%0Aeffectively%20capturing%20the%20spatial-temporal%20patterns%20within%20traffic%20flow.%20Our%0Amodel%20aims%20to%20achieve%20results%20comparable%20to%20those%20from%20extensive%20data%20sets%0Awhile%20only%20utilizing%20minimal%20data.%20Simulations%20using%20real-world%20datasets%20have%0Avalidated%20our%20model%27s%20ability%20to%20deliver%20precise%20and%20stable%20TFE%20across%20an%20urban%0Alandscape%20based%20on%20limited%20data%2C%20establishing%20a%20cost-efficient%20solution%20for%0ATFE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08558v1&entry.124074799=Read"},
{"title": "PredBench: Benchmarking Spatio-Temporal Prediction across Diverse\n  Disciplines", "author": "ZiDong Wang and Zeyu Lu and Di Huang and Tong He and Xihui Liu and Wanli Ouyang and Lei Bai", "abstract": "  In this paper, we introduce PredBench, a benchmark tailored for the holistic\nevaluation of spatio-temporal prediction networks. Despite significant progress\nin this field, there remains a lack of a standardized framework for a detailed\nand comparative analysis of various prediction network architectures. PredBench\naddresses this gap by conducting large-scale experiments, upholding\nstandardized and appropriate experimental settings, and implementing\nmulti-dimensional evaluations. This benchmark integrates 12 widely adopted\nmethods with 15 diverse datasets across multiple application domains, offering\nextensive evaluation of contemporary spatio-temporal prediction networks.\nThrough meticulous calibration of prediction settings across various\napplications, PredBench ensures evaluations relevant to their intended use and\nenables fair comparisons. Moreover, its multi-dimensional evaluation framework\nbroadens the analysis with a comprehensive set of metrics, providing deep\ninsights into the capabilities of models. The findings from our research offer\nstrategic directions for future developments in the field. Our codebase is\navailable at https://github.com/WZDTHU/PredBench.\n", "link": "http://arxiv.org/abs/2407.08418v1", "date": "2024-07-11", "relevancy": 2.0157, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PredBench%3A%20Benchmarking%20Spatio-Temporal%20Prediction%20across%20Diverse%0A%20%20Disciplines&body=Title%3A%20PredBench%3A%20Benchmarking%20Spatio-Temporal%20Prediction%20across%20Diverse%0A%20%20Disciplines%0AAuthor%3A%20ZiDong%20Wang%20and%20Zeyu%20Lu%20and%20Di%20Huang%20and%20Tong%20He%20and%20Xihui%20Liu%20and%20Wanli%20Ouyang%20and%20Lei%20Bai%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20PredBench%2C%20a%20benchmark%20tailored%20for%20the%20holistic%0Aevaluation%20of%20spatio-temporal%20prediction%20networks.%20Despite%20significant%20progress%0Ain%20this%20field%2C%20there%20remains%20a%20lack%20of%20a%20standardized%20framework%20for%20a%20detailed%0Aand%20comparative%20analysis%20of%20various%20prediction%20network%20architectures.%20PredBench%0Aaddresses%20this%20gap%20by%20conducting%20large-scale%20experiments%2C%20upholding%0Astandardized%20and%20appropriate%20experimental%20settings%2C%20and%20implementing%0Amulti-dimensional%20evaluations.%20This%20benchmark%20integrates%2012%20widely%20adopted%0Amethods%20with%2015%20diverse%20datasets%20across%20multiple%20application%20domains%2C%20offering%0Aextensive%20evaluation%20of%20contemporary%20spatio-temporal%20prediction%20networks.%0AThrough%20meticulous%20calibration%20of%20prediction%20settings%20across%20various%0Aapplications%2C%20PredBench%20ensures%20evaluations%20relevant%20to%20their%20intended%20use%20and%0Aenables%20fair%20comparisons.%20Moreover%2C%20its%20multi-dimensional%20evaluation%20framework%0Abroadens%20the%20analysis%20with%20a%20comprehensive%20set%20of%20metrics%2C%20providing%20deep%0Ainsights%20into%20the%20capabilities%20of%20models.%20The%20findings%20from%20our%20research%20offer%0Astrategic%20directions%20for%20future%20developments%20in%20the%20field.%20Our%20codebase%20is%0Aavailable%20at%20https%3A//github.com/WZDTHU/PredBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredBench%253A%2520Benchmarking%2520Spatio-Temporal%2520Prediction%2520across%2520Diverse%250A%2520%2520Disciplines%26entry.906535625%3DZiDong%2520Wang%2520and%2520Zeyu%2520Lu%2520and%2520Di%2520Huang%2520and%2520Tong%2520He%2520and%2520Xihui%2520Liu%2520and%2520Wanli%2520Ouyang%2520and%2520Lei%2520Bai%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PredBench%252C%2520a%2520benchmark%2520tailored%2520for%2520the%2520holistic%250Aevaluation%2520of%2520spatio-temporal%2520prediction%2520networks.%2520Despite%2520significant%2520progress%250Ain%2520this%2520field%252C%2520there%2520remains%2520a%2520lack%2520of%2520a%2520standardized%2520framework%2520for%2520a%2520detailed%250Aand%2520comparative%2520analysis%2520of%2520various%2520prediction%2520network%2520architectures.%2520PredBench%250Aaddresses%2520this%2520gap%2520by%2520conducting%2520large-scale%2520experiments%252C%2520upholding%250Astandardized%2520and%2520appropriate%2520experimental%2520settings%252C%2520and%2520implementing%250Amulti-dimensional%2520evaluations.%2520This%2520benchmark%2520integrates%252012%2520widely%2520adopted%250Amethods%2520with%252015%2520diverse%2520datasets%2520across%2520multiple%2520application%2520domains%252C%2520offering%250Aextensive%2520evaluation%2520of%2520contemporary%2520spatio-temporal%2520prediction%2520networks.%250AThrough%2520meticulous%2520calibration%2520of%2520prediction%2520settings%2520across%2520various%250Aapplications%252C%2520PredBench%2520ensures%2520evaluations%2520relevant%2520to%2520their%2520intended%2520use%2520and%250Aenables%2520fair%2520comparisons.%2520Moreover%252C%2520its%2520multi-dimensional%2520evaluation%2520framework%250Abroadens%2520the%2520analysis%2520with%2520a%2520comprehensive%2520set%2520of%2520metrics%252C%2520providing%2520deep%250Ainsights%2520into%2520the%2520capabilities%2520of%2520models.%2520The%2520findings%2520from%2520our%2520research%2520offer%250Astrategic%2520directions%2520for%2520future%2520developments%2520in%2520the%2520field.%2520Our%2520codebase%2520is%250Aavailable%2520at%2520https%253A//github.com/WZDTHU/PredBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PredBench%3A%20Benchmarking%20Spatio-Temporal%20Prediction%20across%20Diverse%0A%20%20Disciplines&entry.906535625=ZiDong%20Wang%20and%20Zeyu%20Lu%20and%20Di%20Huang%20and%20Tong%20He%20and%20Xihui%20Liu%20and%20Wanli%20Ouyang%20and%20Lei%20Bai&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20PredBench%2C%20a%20benchmark%20tailored%20for%20the%20holistic%0Aevaluation%20of%20spatio-temporal%20prediction%20networks.%20Despite%20significant%20progress%0Ain%20this%20field%2C%20there%20remains%20a%20lack%20of%20a%20standardized%20framework%20for%20a%20detailed%0Aand%20comparative%20analysis%20of%20various%20prediction%20network%20architectures.%20PredBench%0Aaddresses%20this%20gap%20by%20conducting%20large-scale%20experiments%2C%20upholding%0Astandardized%20and%20appropriate%20experimental%20settings%2C%20and%20implementing%0Amulti-dimensional%20evaluations.%20This%20benchmark%20integrates%2012%20widely%20adopted%0Amethods%20with%2015%20diverse%20datasets%20across%20multiple%20application%20domains%2C%20offering%0Aextensive%20evaluation%20of%20contemporary%20spatio-temporal%20prediction%20networks.%0AThrough%20meticulous%20calibration%20of%20prediction%20settings%20across%20various%0Aapplications%2C%20PredBench%20ensures%20evaluations%20relevant%20to%20their%20intended%20use%20and%0Aenables%20fair%20comparisons.%20Moreover%2C%20its%20multi-dimensional%20evaluation%20framework%0Abroadens%20the%20analysis%20with%20a%20comprehensive%20set%20of%20metrics%2C%20providing%20deep%0Ainsights%20into%20the%20capabilities%20of%20models.%20The%20findings%20from%20our%20research%20offer%0Astrategic%20directions%20for%20future%20developments%20in%20the%20field.%20Our%20codebase%20is%0Aavailable%20at%20https%3A//github.com/WZDTHU/PredBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08418v1&entry.124074799=Read"},
{"title": "Collaborative Object Manipulation on the Water Surface by a UAV-USV Team\n  Using Tethers", "author": "Filip Nov\u00e1k and Tom\u00e1\u0161 B\u00e1\u010da and Martin Saska", "abstract": "  This paper introduces an innovative methodology for object manipulation on\nthe surface of water through the collaboration of an Unmanned Aerial Vehicle\n(UAV) and an Unmanned Surface Vehicle (USV) connected to the object by tethers.\nWe propose a novel mathematical model of a robotic system that combines the\nUAV, USV, and the tethered floating object. A novel Model Predictive Control\n(MPC) framework is designed for using this model to achieve precise control and\nguidance for this collaborative robotic system. Extensive simulations in the\nrealistic robotic simulator Gazebo demonstrate the system's readiness for\nreal-world deployment, highlighting its versatility and effectiveness. Our\nmulti-robot system overcomes the state-of-the-art single-robot approach,\nexhibiting smaller control errors during the tracking of the floating object's\nreference. Additionally, our multi-robot system demonstrates a shorter recovery\ntime from a disturbance compared to the single-robot approach.\n", "link": "http://arxiv.org/abs/2407.08580v1", "date": "2024-07-11", "relevancy": 2.0106, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5145}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Object%20Manipulation%20on%20the%20Water%20Surface%20by%20a%20UAV-USV%20Team%0A%20%20Using%20Tethers&body=Title%3A%20Collaborative%20Object%20Manipulation%20on%20the%20Water%20Surface%20by%20a%20UAV-USV%20Team%0A%20%20Using%20Tethers%0AAuthor%3A%20Filip%20Nov%C3%A1k%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Martin%20Saska%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20innovative%20methodology%20for%20object%20manipulation%20on%0Athe%20surface%20of%20water%20through%20the%20collaboration%20of%20an%20Unmanned%20Aerial%20Vehicle%0A%28UAV%29%20and%20an%20Unmanned%20Surface%20Vehicle%20%28USV%29%20connected%20to%20the%20object%20by%20tethers.%0AWe%20propose%20a%20novel%20mathematical%20model%20of%20a%20robotic%20system%20that%20combines%20the%0AUAV%2C%20USV%2C%20and%20the%20tethered%20floating%20object.%20A%20novel%20Model%20Predictive%20Control%0A%28MPC%29%20framework%20is%20designed%20for%20using%20this%20model%20to%20achieve%20precise%20control%20and%0Aguidance%20for%20this%20collaborative%20robotic%20system.%20Extensive%20simulations%20in%20the%0Arealistic%20robotic%20simulator%20Gazebo%20demonstrate%20the%20system%27s%20readiness%20for%0Areal-world%20deployment%2C%20highlighting%20its%20versatility%20and%20effectiveness.%20Our%0Amulti-robot%20system%20overcomes%20the%20state-of-the-art%20single-robot%20approach%2C%0Aexhibiting%20smaller%20control%20errors%20during%20the%20tracking%20of%20the%20floating%20object%27s%0Areference.%20Additionally%2C%20our%20multi-robot%20system%20demonstrates%20a%20shorter%20recovery%0Atime%20from%20a%20disturbance%20compared%20to%20the%20single-robot%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Object%2520Manipulation%2520on%2520the%2520Water%2520Surface%2520by%2520a%2520UAV-USV%2520Team%250A%2520%2520Using%2520Tethers%26entry.906535625%3DFilip%2520Nov%25C3%25A1k%2520and%2520Tom%25C3%25A1%25C5%25A1%2520B%25C3%25A1%25C4%258Da%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520innovative%2520methodology%2520for%2520object%2520manipulation%2520on%250Athe%2520surface%2520of%2520water%2520through%2520the%2520collaboration%2520of%2520an%2520Unmanned%2520Aerial%2520Vehicle%250A%2528UAV%2529%2520and%2520an%2520Unmanned%2520Surface%2520Vehicle%2520%2528USV%2529%2520connected%2520to%2520the%2520object%2520by%2520tethers.%250AWe%2520propose%2520a%2520novel%2520mathematical%2520model%2520of%2520a%2520robotic%2520system%2520that%2520combines%2520the%250AUAV%252C%2520USV%252C%2520and%2520the%2520tethered%2520floating%2520object.%2520A%2520novel%2520Model%2520Predictive%2520Control%250A%2528MPC%2529%2520framework%2520is%2520designed%2520for%2520using%2520this%2520model%2520to%2520achieve%2520precise%2520control%2520and%250Aguidance%2520for%2520this%2520collaborative%2520robotic%2520system.%2520Extensive%2520simulations%2520in%2520the%250Arealistic%2520robotic%2520simulator%2520Gazebo%2520demonstrate%2520the%2520system%2527s%2520readiness%2520for%250Areal-world%2520deployment%252C%2520highlighting%2520its%2520versatility%2520and%2520effectiveness.%2520Our%250Amulti-robot%2520system%2520overcomes%2520the%2520state-of-the-art%2520single-robot%2520approach%252C%250Aexhibiting%2520smaller%2520control%2520errors%2520during%2520the%2520tracking%2520of%2520the%2520floating%2520object%2527s%250Areference.%2520Additionally%252C%2520our%2520multi-robot%2520system%2520demonstrates%2520a%2520shorter%2520recovery%250Atime%2520from%2520a%2520disturbance%2520compared%2520to%2520the%2520single-robot%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Object%20Manipulation%20on%20the%20Water%20Surface%20by%20a%20UAV-USV%20Team%0A%20%20Using%20Tethers&entry.906535625=Filip%20Nov%C3%A1k%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Martin%20Saska&entry.1292438233=%20%20This%20paper%20introduces%20an%20innovative%20methodology%20for%20object%20manipulation%20on%0Athe%20surface%20of%20water%20through%20the%20collaboration%20of%20an%20Unmanned%20Aerial%20Vehicle%0A%28UAV%29%20and%20an%20Unmanned%20Surface%20Vehicle%20%28USV%29%20connected%20to%20the%20object%20by%20tethers.%0AWe%20propose%20a%20novel%20mathematical%20model%20of%20a%20robotic%20system%20that%20combines%20the%0AUAV%2C%20USV%2C%20and%20the%20tethered%20floating%20object.%20A%20novel%20Model%20Predictive%20Control%0A%28MPC%29%20framework%20is%20designed%20for%20using%20this%20model%20to%20achieve%20precise%20control%20and%0Aguidance%20for%20this%20collaborative%20robotic%20system.%20Extensive%20simulations%20in%20the%0Arealistic%20robotic%20simulator%20Gazebo%20demonstrate%20the%20system%27s%20readiness%20for%0Areal-world%20deployment%2C%20highlighting%20its%20versatility%20and%20effectiveness.%20Our%0Amulti-robot%20system%20overcomes%20the%20state-of-the-art%20single-robot%20approach%2C%0Aexhibiting%20smaller%20control%20errors%20during%20the%20tracking%20of%20the%20floating%20object%27s%0Areference.%20Additionally%2C%20our%20multi-robot%20system%20demonstrates%20a%20shorter%20recovery%0Atime%20from%20a%20disturbance%20compared%20to%20the%20single-robot%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08580v1&entry.124074799=Read"},
{"title": "Enhancing context models for point cloud geometry compression with\n  context feature residuals and multi-loss", "author": "Chang Sun and Hui Yuan and Shuai Li and Xin Lu and Raouf Hamzaoui", "abstract": "  In point cloud geometry compression, context models usually use the one-hot\nencoding of node occupancy as the label, and the cross-entropy between the\none-hot encoding and the probability distribution predicted by the context\nmodel as the loss function. However, this approach has two main weaknesses.\nFirst, the differences between contexts of different nodes are not significant,\nmaking it difficult for the context model to accurately predict the probability\ndistribution of node occupancy. Second, as the one-hot encoding is not the\nactual probability distribution of node occupancy, the cross-entropy loss\nfunction is inaccurate. To address these problems, we propose a general\nstructure that can enhance existing context models. We introduce the context\nfeature residuals into the context model to amplify the differences between\ncontexts. We also add a multi-layer perception branch, that uses the mean\nsquared error between its output and node occupancy as a loss function to\nprovide accurate gradients in backpropagation. We validate our method by\nshowing that it can improve the performance of an octree-based model\n(OctAttention) and a voxel-based model (VoxelDNN) on the object point cloud\ndatasets MPEG 8i and MVUB, as well as the LiDAR point cloud dataset\nSemanticKITTI.\n", "link": "http://arxiv.org/abs/2407.08520v1", "date": "2024-07-11", "relevancy": 2.0041, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5054}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5009}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20context%20models%20for%20point%20cloud%20geometry%20compression%20with%0A%20%20context%20feature%20residuals%20and%20multi-loss&body=Title%3A%20Enhancing%20context%20models%20for%20point%20cloud%20geometry%20compression%20with%0A%20%20context%20feature%20residuals%20and%20multi-loss%0AAuthor%3A%20Chang%20Sun%20and%20Hui%20Yuan%20and%20Shuai%20Li%20and%20Xin%20Lu%20and%20Raouf%20Hamzaoui%0AAbstract%3A%20%20%20In%20point%20cloud%20geometry%20compression%2C%20context%20models%20usually%20use%20the%20one-hot%0Aencoding%20of%20node%20occupancy%20as%20the%20label%2C%20and%20the%20cross-entropy%20between%20the%0Aone-hot%20encoding%20and%20the%20probability%20distribution%20predicted%20by%20the%20context%0Amodel%20as%20the%20loss%20function.%20However%2C%20this%20approach%20has%20two%20main%20weaknesses.%0AFirst%2C%20the%20differences%20between%20contexts%20of%20different%20nodes%20are%20not%20significant%2C%0Amaking%20it%20difficult%20for%20the%20context%20model%20to%20accurately%20predict%20the%20probability%0Adistribution%20of%20node%20occupancy.%20Second%2C%20as%20the%20one-hot%20encoding%20is%20not%20the%0Aactual%20probability%20distribution%20of%20node%20occupancy%2C%20the%20cross-entropy%20loss%0Afunction%20is%20inaccurate.%20To%20address%20these%20problems%2C%20we%20propose%20a%20general%0Astructure%20that%20can%20enhance%20existing%20context%20models.%20We%20introduce%20the%20context%0Afeature%20residuals%20into%20the%20context%20model%20to%20amplify%20the%20differences%20between%0Acontexts.%20We%20also%20add%20a%20multi-layer%20perception%20branch%2C%20that%20uses%20the%20mean%0Asquared%20error%20between%20its%20output%20and%20node%20occupancy%20as%20a%20loss%20function%20to%0Aprovide%20accurate%20gradients%20in%20backpropagation.%20We%20validate%20our%20method%20by%0Ashowing%20that%20it%20can%20improve%20the%20performance%20of%20an%20octree-based%20model%0A%28OctAttention%29%20and%20a%20voxel-based%20model%20%28VoxelDNN%29%20on%20the%20object%20point%20cloud%0Adatasets%20MPEG%208i%20and%20MVUB%2C%20as%20well%20as%20the%20LiDAR%20point%20cloud%20dataset%0ASemanticKITTI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520context%2520models%2520for%2520point%2520cloud%2520geometry%2520compression%2520with%250A%2520%2520context%2520feature%2520residuals%2520and%2520multi-loss%26entry.906535625%3DChang%2520Sun%2520and%2520Hui%2520Yuan%2520and%2520Shuai%2520Li%2520and%2520Xin%2520Lu%2520and%2520Raouf%2520Hamzaoui%26entry.1292438233%3D%2520%2520In%2520point%2520cloud%2520geometry%2520compression%252C%2520context%2520models%2520usually%2520use%2520the%2520one-hot%250Aencoding%2520of%2520node%2520occupancy%2520as%2520the%2520label%252C%2520and%2520the%2520cross-entropy%2520between%2520the%250Aone-hot%2520encoding%2520and%2520the%2520probability%2520distribution%2520predicted%2520by%2520the%2520context%250Amodel%2520as%2520the%2520loss%2520function.%2520However%252C%2520this%2520approach%2520has%2520two%2520main%2520weaknesses.%250AFirst%252C%2520the%2520differences%2520between%2520contexts%2520of%2520different%2520nodes%2520are%2520not%2520significant%252C%250Amaking%2520it%2520difficult%2520for%2520the%2520context%2520model%2520to%2520accurately%2520predict%2520the%2520probability%250Adistribution%2520of%2520node%2520occupancy.%2520Second%252C%2520as%2520the%2520one-hot%2520encoding%2520is%2520not%2520the%250Aactual%2520probability%2520distribution%2520of%2520node%2520occupancy%252C%2520the%2520cross-entropy%2520loss%250Afunction%2520is%2520inaccurate.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520general%250Astructure%2520that%2520can%2520enhance%2520existing%2520context%2520models.%2520We%2520introduce%2520the%2520context%250Afeature%2520residuals%2520into%2520the%2520context%2520model%2520to%2520amplify%2520the%2520differences%2520between%250Acontexts.%2520We%2520also%2520add%2520a%2520multi-layer%2520perception%2520branch%252C%2520that%2520uses%2520the%2520mean%250Asquared%2520error%2520between%2520its%2520output%2520and%2520node%2520occupancy%2520as%2520a%2520loss%2520function%2520to%250Aprovide%2520accurate%2520gradients%2520in%2520backpropagation.%2520We%2520validate%2520our%2520method%2520by%250Ashowing%2520that%2520it%2520can%2520improve%2520the%2520performance%2520of%2520an%2520octree-based%2520model%250A%2528OctAttention%2529%2520and%2520a%2520voxel-based%2520model%2520%2528VoxelDNN%2529%2520on%2520the%2520object%2520point%2520cloud%250Adatasets%2520MPEG%25208i%2520and%2520MVUB%252C%2520as%2520well%2520as%2520the%2520LiDAR%2520point%2520cloud%2520dataset%250ASemanticKITTI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20context%20models%20for%20point%20cloud%20geometry%20compression%20with%0A%20%20context%20feature%20residuals%20and%20multi-loss&entry.906535625=Chang%20Sun%20and%20Hui%20Yuan%20and%20Shuai%20Li%20and%20Xin%20Lu%20and%20Raouf%20Hamzaoui&entry.1292438233=%20%20In%20point%20cloud%20geometry%20compression%2C%20context%20models%20usually%20use%20the%20one-hot%0Aencoding%20of%20node%20occupancy%20as%20the%20label%2C%20and%20the%20cross-entropy%20between%20the%0Aone-hot%20encoding%20and%20the%20probability%20distribution%20predicted%20by%20the%20context%0Amodel%20as%20the%20loss%20function.%20However%2C%20this%20approach%20has%20two%20main%20weaknesses.%0AFirst%2C%20the%20differences%20between%20contexts%20of%20different%20nodes%20are%20not%20significant%2C%0Amaking%20it%20difficult%20for%20the%20context%20model%20to%20accurately%20predict%20the%20probability%0Adistribution%20of%20node%20occupancy.%20Second%2C%20as%20the%20one-hot%20encoding%20is%20not%20the%0Aactual%20probability%20distribution%20of%20node%20occupancy%2C%20the%20cross-entropy%20loss%0Afunction%20is%20inaccurate.%20To%20address%20these%20problems%2C%20we%20propose%20a%20general%0Astructure%20that%20can%20enhance%20existing%20context%20models.%20We%20introduce%20the%20context%0Afeature%20residuals%20into%20the%20context%20model%20to%20amplify%20the%20differences%20between%0Acontexts.%20We%20also%20add%20a%20multi-layer%20perception%20branch%2C%20that%20uses%20the%20mean%0Asquared%20error%20between%20its%20output%20and%20node%20occupancy%20as%20a%20loss%20function%20to%0Aprovide%20accurate%20gradients%20in%20backpropagation.%20We%20validate%20our%20method%20by%0Ashowing%20that%20it%20can%20improve%20the%20performance%20of%20an%20octree-based%20model%0A%28OctAttention%29%20and%20a%20voxel-based%20model%20%28VoxelDNN%29%20on%20the%20object%20point%20cloud%0Adatasets%20MPEG%208i%20and%20MVUB%2C%20as%20well%20as%20the%20LiDAR%20point%20cloud%20dataset%0ASemanticKITTI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08520v1&entry.124074799=Read"},
{"title": "CAD-Prompted Generative Models: A Pathway to Feasible and Novel\n  Engineering Designs", "author": "Leah Chong and Jude Rayan and Steven Dow and Ioanna Lykourentzou and Faez Ahmed", "abstract": "  Text-to-image generative models have increasingly been used to assist\ndesigners during concept generation in various creative domains, such as\ngraphic design, user interface design, and fashion design. However, their\napplications in engineering design remain limited due to the models' challenges\nin generating images of feasible designs concepts. To address this issue, this\npaper introduces a method that improves the design feasibility by prompting the\ngeneration with feasible CAD images. In this work, the usefulness of this\nmethod is investigated through a case study with a bike design task using an\noff-the-shelf text-to-image model, Stable Diffusion 2.1. A diverse set of bike\ndesigns are produced in seven different generation settings with varying CAD\nimage prompting weights, and these designs are evaluated on their perceived\nfeasibility and novelty. Results demonstrate that the CAD image prompting\nsuccessfully helps text-to-image models like Stable Diffusion 2.1 create\nvisibly more feasible design images. While a general tradeoff is observed\nbetween feasibility and novelty, when the prompting weight is kept low around\n0.35, the design feasibility is significantly improved while its novelty\nremains on par with those generated by text prompts alone. The insights from\nthis case study offer some guidelines for selecting the appropriate CAD image\nprompting weight for different stages of the engineering design process. When\nutilized effectively, our CAD image prompting method opens doors to a wider\nrange of applications of text-to-image models in engineering design.\n", "link": "http://arxiv.org/abs/2407.08675v1", "date": "2024-07-11", "relevancy": 1.7546, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5986}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5881}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Prompted%20Generative%20Models%3A%20A%20Pathway%20to%20Feasible%20and%20Novel%0A%20%20Engineering%20Designs&body=Title%3A%20CAD-Prompted%20Generative%20Models%3A%20A%20Pathway%20to%20Feasible%20and%20Novel%0A%20%20Engineering%20Designs%0AAuthor%3A%20Leah%20Chong%20and%20Jude%20Rayan%20and%20Steven%20Dow%20and%20Ioanna%20Lykourentzou%20and%20Faez%20Ahmed%0AAbstract%3A%20%20%20Text-to-image%20generative%20models%20have%20increasingly%20been%20used%20to%20assist%0Adesigners%20during%20concept%20generation%20in%20various%20creative%20domains%2C%20such%20as%0Agraphic%20design%2C%20user%20interface%20design%2C%20and%20fashion%20design.%20However%2C%20their%0Aapplications%20in%20engineering%20design%20remain%20limited%20due%20to%20the%20models%27%20challenges%0Ain%20generating%20images%20of%20feasible%20designs%20concepts.%20To%20address%20this%20issue%2C%20this%0Apaper%20introduces%20a%20method%20that%20improves%20the%20design%20feasibility%20by%20prompting%20the%0Ageneration%20with%20feasible%20CAD%20images.%20In%20this%20work%2C%20the%20usefulness%20of%20this%0Amethod%20is%20investigated%20through%20a%20case%20study%20with%20a%20bike%20design%20task%20using%20an%0Aoff-the-shelf%20text-to-image%20model%2C%20Stable%20Diffusion%202.1.%20A%20diverse%20set%20of%20bike%0Adesigns%20are%20produced%20in%20seven%20different%20generation%20settings%20with%20varying%20CAD%0Aimage%20prompting%20weights%2C%20and%20these%20designs%20are%20evaluated%20on%20their%20perceived%0Afeasibility%20and%20novelty.%20Results%20demonstrate%20that%20the%20CAD%20image%20prompting%0Asuccessfully%20helps%20text-to-image%20models%20like%20Stable%20Diffusion%202.1%20create%0Avisibly%20more%20feasible%20design%20images.%20While%20a%20general%20tradeoff%20is%20observed%0Abetween%20feasibility%20and%20novelty%2C%20when%20the%20prompting%20weight%20is%20kept%20low%20around%0A0.35%2C%20the%20design%20feasibility%20is%20significantly%20improved%20while%20its%20novelty%0Aremains%20on%20par%20with%20those%20generated%20by%20text%20prompts%20alone.%20The%20insights%20from%0Athis%20case%20study%20offer%20some%20guidelines%20for%20selecting%20the%20appropriate%20CAD%20image%0Aprompting%20weight%20for%20different%20stages%20of%20the%20engineering%20design%20process.%20When%0Autilized%20effectively%2C%20our%20CAD%20image%20prompting%20method%20opens%20doors%20to%20a%20wider%0Arange%20of%20applications%20of%20text-to-image%20models%20in%20engineering%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Prompted%2520Generative%2520Models%253A%2520A%2520Pathway%2520to%2520Feasible%2520and%2520Novel%250A%2520%2520Engineering%2520Designs%26entry.906535625%3DLeah%2520Chong%2520and%2520Jude%2520Rayan%2520and%2520Steven%2520Dow%2520and%2520Ioanna%2520Lykourentzou%2520and%2520Faez%2520Ahmed%26entry.1292438233%3D%2520%2520Text-to-image%2520generative%2520models%2520have%2520increasingly%2520been%2520used%2520to%2520assist%250Adesigners%2520during%2520concept%2520generation%2520in%2520various%2520creative%2520domains%252C%2520such%2520as%250Agraphic%2520design%252C%2520user%2520interface%2520design%252C%2520and%2520fashion%2520design.%2520However%252C%2520their%250Aapplications%2520in%2520engineering%2520design%2520remain%2520limited%2520due%2520to%2520the%2520models%2527%2520challenges%250Ain%2520generating%2520images%2520of%2520feasible%2520designs%2520concepts.%2520To%2520address%2520this%2520issue%252C%2520this%250Apaper%2520introduces%2520a%2520method%2520that%2520improves%2520the%2520design%2520feasibility%2520by%2520prompting%2520the%250Ageneration%2520with%2520feasible%2520CAD%2520images.%2520In%2520this%2520work%252C%2520the%2520usefulness%2520of%2520this%250Amethod%2520is%2520investigated%2520through%2520a%2520case%2520study%2520with%2520a%2520bike%2520design%2520task%2520using%2520an%250Aoff-the-shelf%2520text-to-image%2520model%252C%2520Stable%2520Diffusion%25202.1.%2520A%2520diverse%2520set%2520of%2520bike%250Adesigns%2520are%2520produced%2520in%2520seven%2520different%2520generation%2520settings%2520with%2520varying%2520CAD%250Aimage%2520prompting%2520weights%252C%2520and%2520these%2520designs%2520are%2520evaluated%2520on%2520their%2520perceived%250Afeasibility%2520and%2520novelty.%2520Results%2520demonstrate%2520that%2520the%2520CAD%2520image%2520prompting%250Asuccessfully%2520helps%2520text-to-image%2520models%2520like%2520Stable%2520Diffusion%25202.1%2520create%250Avisibly%2520more%2520feasible%2520design%2520images.%2520While%2520a%2520general%2520tradeoff%2520is%2520observed%250Abetween%2520feasibility%2520and%2520novelty%252C%2520when%2520the%2520prompting%2520weight%2520is%2520kept%2520low%2520around%250A0.35%252C%2520the%2520design%2520feasibility%2520is%2520significantly%2520improved%2520while%2520its%2520novelty%250Aremains%2520on%2520par%2520with%2520those%2520generated%2520by%2520text%2520prompts%2520alone.%2520The%2520insights%2520from%250Athis%2520case%2520study%2520offer%2520some%2520guidelines%2520for%2520selecting%2520the%2520appropriate%2520CAD%2520image%250Aprompting%2520weight%2520for%2520different%2520stages%2520of%2520the%2520engineering%2520design%2520process.%2520When%250Autilized%2520effectively%252C%2520our%2520CAD%2520image%2520prompting%2520method%2520opens%2520doors%2520to%2520a%2520wider%250Arange%2520of%2520applications%2520of%2520text-to-image%2520models%2520in%2520engineering%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Prompted%20Generative%20Models%3A%20A%20Pathway%20to%20Feasible%20and%20Novel%0A%20%20Engineering%20Designs&entry.906535625=Leah%20Chong%20and%20Jude%20Rayan%20and%20Steven%20Dow%20and%20Ioanna%20Lykourentzou%20and%20Faez%20Ahmed&entry.1292438233=%20%20Text-to-image%20generative%20models%20have%20increasingly%20been%20used%20to%20assist%0Adesigners%20during%20concept%20generation%20in%20various%20creative%20domains%2C%20such%20as%0Agraphic%20design%2C%20user%20interface%20design%2C%20and%20fashion%20design.%20However%2C%20their%0Aapplications%20in%20engineering%20design%20remain%20limited%20due%20to%20the%20models%27%20challenges%0Ain%20generating%20images%20of%20feasible%20designs%20concepts.%20To%20address%20this%20issue%2C%20this%0Apaper%20introduces%20a%20method%20that%20improves%20the%20design%20feasibility%20by%20prompting%20the%0Ageneration%20with%20feasible%20CAD%20images.%20In%20this%20work%2C%20the%20usefulness%20of%20this%0Amethod%20is%20investigated%20through%20a%20case%20study%20with%20a%20bike%20design%20task%20using%20an%0Aoff-the-shelf%20text-to-image%20model%2C%20Stable%20Diffusion%202.1.%20A%20diverse%20set%20of%20bike%0Adesigns%20are%20produced%20in%20seven%20different%20generation%20settings%20with%20varying%20CAD%0Aimage%20prompting%20weights%2C%20and%20these%20designs%20are%20evaluated%20on%20their%20perceived%0Afeasibility%20and%20novelty.%20Results%20demonstrate%20that%20the%20CAD%20image%20prompting%0Asuccessfully%20helps%20text-to-image%20models%20like%20Stable%20Diffusion%202.1%20create%0Avisibly%20more%20feasible%20design%20images.%20While%20a%20general%20tradeoff%20is%20observed%0Abetween%20feasibility%20and%20novelty%2C%20when%20the%20prompting%20weight%20is%20kept%20low%20around%0A0.35%2C%20the%20design%20feasibility%20is%20significantly%20improved%20while%20its%20novelty%0Aremains%20on%20par%20with%20those%20generated%20by%20text%20prompts%20alone.%20The%20insights%20from%0Athis%20case%20study%20offer%20some%20guidelines%20for%20selecting%20the%20appropriate%20CAD%20image%0Aprompting%20weight%20for%20different%20stages%20of%20the%20engineering%20design%20process.%20When%0Autilized%20effectively%2C%20our%20CAD%20image%20prompting%20method%20opens%20doors%20to%20a%20wider%0Arange%20of%20applications%20of%20text-to-image%20models%20in%20engineering%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08675v1&entry.124074799=Read"},
{"title": "CAR-MFL: Cross-Modal Augmentation by Retrieval for Multimodal Federated\n  Learning with Missing Modalities", "author": "Pranav Poudel and Prashant Shrestha and Sanskar Amgain and Yash Raj Shrestha and Prashnna Gyawali and Binod Bhattarai", "abstract": "  Multimodal AI has demonstrated superior performance over unimodal approaches\nby leveraging diverse data sources for more comprehensive analysis. However,\napplying this effectiveness in healthcare is challenging due to the limited\navailability of public datasets. Federated learning presents an exciting\nsolution, allowing the use of extensive databases from hospitals and health\ncenters without centralizing sensitive data, thus maintaining privacy and\nsecurity. Yet, research in multimodal federated learning, particularly in\nscenarios with missing modalities a common issue in healthcare datasets remains\nscarce, highlighting a critical area for future exploration. Toward this, we\npropose a novel method for multimodal federated learning with missing\nmodalities. Our contribution lies in a novel cross-modal data augmentation by\nretrieval, leveraging the small publicly available dataset to fill the missing\nmodalities in the clients. Our method learns the parameters in a federated\nmanner, ensuring privacy protection and improving performance in multiple\nchallenging multimodal benchmarks in the medical domain, surpassing several\ncompetitive baselines. Code Available: https://github.com/bhattarailab/CAR-MFL\n", "link": "http://arxiv.org/abs/2407.08648v1", "date": "2024-07-11", "relevancy": 1.6655, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5694}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAR-MFL%3A%20Cross-Modal%20Augmentation%20by%20Retrieval%20for%20Multimodal%20Federated%0A%20%20Learning%20with%20Missing%20Modalities&body=Title%3A%20CAR-MFL%3A%20Cross-Modal%20Augmentation%20by%20Retrieval%20for%20Multimodal%20Federated%0A%20%20Learning%20with%20Missing%20Modalities%0AAuthor%3A%20Pranav%20Poudel%20and%20Prashant%20Shrestha%20and%20Sanskar%20Amgain%20and%20Yash%20Raj%20Shrestha%20and%20Prashnna%20Gyawali%20and%20Binod%20Bhattarai%0AAbstract%3A%20%20%20Multimodal%20AI%20has%20demonstrated%20superior%20performance%20over%20unimodal%20approaches%0Aby%20leveraging%20diverse%20data%20sources%20for%20more%20comprehensive%20analysis.%20However%2C%0Aapplying%20this%20effectiveness%20in%20healthcare%20is%20challenging%20due%20to%20the%20limited%0Aavailability%20of%20public%20datasets.%20Federated%20learning%20presents%20an%20exciting%0Asolution%2C%20allowing%20the%20use%20of%20extensive%20databases%20from%20hospitals%20and%20health%0Acenters%20without%20centralizing%20sensitive%20data%2C%20thus%20maintaining%20privacy%20and%0Asecurity.%20Yet%2C%20research%20in%20multimodal%20federated%20learning%2C%20particularly%20in%0Ascenarios%20with%20missing%20modalities%20a%20common%20issue%20in%20healthcare%20datasets%20remains%0Ascarce%2C%20highlighting%20a%20critical%20area%20for%20future%20exploration.%20Toward%20this%2C%20we%0Apropose%20a%20novel%20method%20for%20multimodal%20federated%20learning%20with%20missing%0Amodalities.%20Our%20contribution%20lies%20in%20a%20novel%20cross-modal%20data%20augmentation%20by%0Aretrieval%2C%20leveraging%20the%20small%20publicly%20available%20dataset%20to%20fill%20the%20missing%0Amodalities%20in%20the%20clients.%20Our%20method%20learns%20the%20parameters%20in%20a%20federated%0Amanner%2C%20ensuring%20privacy%20protection%20and%20improving%20performance%20in%20multiple%0Achallenging%20multimodal%20benchmarks%20in%20the%20medical%20domain%2C%20surpassing%20several%0Acompetitive%20baselines.%20Code%20Available%3A%20https%3A//github.com/bhattarailab/CAR-MFL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAR-MFL%253A%2520Cross-Modal%2520Augmentation%2520by%2520Retrieval%2520for%2520Multimodal%2520Federated%250A%2520%2520Learning%2520with%2520Missing%2520Modalities%26entry.906535625%3DPranav%2520Poudel%2520and%2520Prashant%2520Shrestha%2520and%2520Sanskar%2520Amgain%2520and%2520Yash%2520Raj%2520Shrestha%2520and%2520Prashnna%2520Gyawali%2520and%2520Binod%2520Bhattarai%26entry.1292438233%3D%2520%2520Multimodal%2520AI%2520has%2520demonstrated%2520superior%2520performance%2520over%2520unimodal%2520approaches%250Aby%2520leveraging%2520diverse%2520data%2520sources%2520for%2520more%2520comprehensive%2520analysis.%2520However%252C%250Aapplying%2520this%2520effectiveness%2520in%2520healthcare%2520is%2520challenging%2520due%2520to%2520the%2520limited%250Aavailability%2520of%2520public%2520datasets.%2520Federated%2520learning%2520presents%2520an%2520exciting%250Asolution%252C%2520allowing%2520the%2520use%2520of%2520extensive%2520databases%2520from%2520hospitals%2520and%2520health%250Acenters%2520without%2520centralizing%2520sensitive%2520data%252C%2520thus%2520maintaining%2520privacy%2520and%250Asecurity.%2520Yet%252C%2520research%2520in%2520multimodal%2520federated%2520learning%252C%2520particularly%2520in%250Ascenarios%2520with%2520missing%2520modalities%2520a%2520common%2520issue%2520in%2520healthcare%2520datasets%2520remains%250Ascarce%252C%2520highlighting%2520a%2520critical%2520area%2520for%2520future%2520exploration.%2520Toward%2520this%252C%2520we%250Apropose%2520a%2520novel%2520method%2520for%2520multimodal%2520federated%2520learning%2520with%2520missing%250Amodalities.%2520Our%2520contribution%2520lies%2520in%2520a%2520novel%2520cross-modal%2520data%2520augmentation%2520by%250Aretrieval%252C%2520leveraging%2520the%2520small%2520publicly%2520available%2520dataset%2520to%2520fill%2520the%2520missing%250Amodalities%2520in%2520the%2520clients.%2520Our%2520method%2520learns%2520the%2520parameters%2520in%2520a%2520federated%250Amanner%252C%2520ensuring%2520privacy%2520protection%2520and%2520improving%2520performance%2520in%2520multiple%250Achallenging%2520multimodal%2520benchmarks%2520in%2520the%2520medical%2520domain%252C%2520surpassing%2520several%250Acompetitive%2520baselines.%2520Code%2520Available%253A%2520https%253A//github.com/bhattarailab/CAR-MFL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAR-MFL%3A%20Cross-Modal%20Augmentation%20by%20Retrieval%20for%20Multimodal%20Federated%0A%20%20Learning%20with%20Missing%20Modalities&entry.906535625=Pranav%20Poudel%20and%20Prashant%20Shrestha%20and%20Sanskar%20Amgain%20and%20Yash%20Raj%20Shrestha%20and%20Prashnna%20Gyawali%20and%20Binod%20Bhattarai&entry.1292438233=%20%20Multimodal%20AI%20has%20demonstrated%20superior%20performance%20over%20unimodal%20approaches%0Aby%20leveraging%20diverse%20data%20sources%20for%20more%20comprehensive%20analysis.%20However%2C%0Aapplying%20this%20effectiveness%20in%20healthcare%20is%20challenging%20due%20to%20the%20limited%0Aavailability%20of%20public%20datasets.%20Federated%20learning%20presents%20an%20exciting%0Asolution%2C%20allowing%20the%20use%20of%20extensive%20databases%20from%20hospitals%20and%20health%0Acenters%20without%20centralizing%20sensitive%20data%2C%20thus%20maintaining%20privacy%20and%0Asecurity.%20Yet%2C%20research%20in%20multimodal%20federated%20learning%2C%20particularly%20in%0Ascenarios%20with%20missing%20modalities%20a%20common%20issue%20in%20healthcare%20datasets%20remains%0Ascarce%2C%20highlighting%20a%20critical%20area%20for%20future%20exploration.%20Toward%20this%2C%20we%0Apropose%20a%20novel%20method%20for%20multimodal%20federated%20learning%20with%20missing%0Amodalities.%20Our%20contribution%20lies%20in%20a%20novel%20cross-modal%20data%20augmentation%20by%0Aretrieval%2C%20leveraging%20the%20small%20publicly%20available%20dataset%20to%20fill%20the%20missing%0Amodalities%20in%20the%20clients.%20Our%20method%20learns%20the%20parameters%20in%20a%20federated%0Amanner%2C%20ensuring%20privacy%20protection%20and%20improving%20performance%20in%20multiple%0Achallenging%20multimodal%20benchmarks%20in%20the%20medical%20domain%2C%20surpassing%20several%0Acompetitive%20baselines.%20Code%20Available%3A%20https%3A//github.com/bhattarailab/CAR-MFL%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08648v1&entry.124074799=Read"},
{"title": "MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces", "author": "Wayne Wu and Honglin He and Yiran Wang and Chenda Duan and Jack He and Zhizheng Liu and Quanyi Li and Bolei Zhou", "abstract": "  Public urban spaces like streetscapes and plazas serve residents and\naccommodate social life in all its vibrant variations. Recent advances in\nRobotics and Embodied AI make public urban spaces no longer exclusive to\nhumans. Food delivery bots and electric wheelchairs have started sharing\nsidewalks with pedestrians, while diverse robot dogs and humanoids have\nrecently emerged in the street. Ensuring the generalizability and safety of\nthese forthcoming mobile machines is crucial when navigating through the\nbustling streets in urban spaces. In this work, we present MetaUrban, a\ncompositional simulation platform for Embodied AI research in urban spaces.\nMetaUrban can construct an infinite number of interactive urban scenes from\ncompositional elements, covering a vast array of ground plans, object\nplacements, pedestrians, vulnerable road users, and other mobile agents'\nappearances and dynamics. We design point navigation and social navigation\ntasks as the pilot study using MetaUrban for embodied AI research and establish\nvarious baselines of Reinforcement Learning and Imitation Learning. Experiments\ndemonstrate that the compositional nature of the simulated environments can\nsubstantially improve the generalizability and safety of the trained mobile\nagents. MetaUrban will be made publicly available to provide more research\nopportunities and foster safe and trustworthy embodied AI in urban spaces.\n", "link": "http://arxiv.org/abs/2407.08725v1", "date": "2024-07-11", "relevancy": 1.7117, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5897}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaUrban%3A%20A%20Simulation%20Platform%20for%20Embodied%20AI%20in%20Urban%20Spaces&body=Title%3A%20MetaUrban%3A%20A%20Simulation%20Platform%20for%20Embodied%20AI%20in%20Urban%20Spaces%0AAuthor%3A%20Wayne%20Wu%20and%20Honglin%20He%20and%20Yiran%20Wang%20and%20Chenda%20Duan%20and%20Jack%20He%20and%20Zhizheng%20Liu%20and%20Quanyi%20Li%20and%20Bolei%20Zhou%0AAbstract%3A%20%20%20Public%20urban%20spaces%20like%20streetscapes%20and%20plazas%20serve%20residents%20and%0Aaccommodate%20social%20life%20in%20all%20its%20vibrant%20variations.%20Recent%20advances%20in%0ARobotics%20and%20Embodied%20AI%20make%20public%20urban%20spaces%20no%20longer%20exclusive%20to%0Ahumans.%20Food%20delivery%20bots%20and%20electric%20wheelchairs%20have%20started%20sharing%0Asidewalks%20with%20pedestrians%2C%20while%20diverse%20robot%20dogs%20and%20humanoids%20have%0Arecently%20emerged%20in%20the%20street.%20Ensuring%20the%20generalizability%20and%20safety%20of%0Athese%20forthcoming%20mobile%20machines%20is%20crucial%20when%20navigating%20through%20the%0Abustling%20streets%20in%20urban%20spaces.%20In%20this%20work%2C%20we%20present%20MetaUrban%2C%20a%0Acompositional%20simulation%20platform%20for%20Embodied%20AI%20research%20in%20urban%20spaces.%0AMetaUrban%20can%20construct%20an%20infinite%20number%20of%20interactive%20urban%20scenes%20from%0Acompositional%20elements%2C%20covering%20a%20vast%20array%20of%20ground%20plans%2C%20object%0Aplacements%2C%20pedestrians%2C%20vulnerable%20road%20users%2C%20and%20other%20mobile%20agents%27%0Aappearances%20and%20dynamics.%20We%20design%20point%20navigation%20and%20social%20navigation%0Atasks%20as%20the%20pilot%20study%20using%20MetaUrban%20for%20embodied%20AI%20research%20and%20establish%0Avarious%20baselines%20of%20Reinforcement%20Learning%20and%20Imitation%20Learning.%20Experiments%0Ademonstrate%20that%20the%20compositional%20nature%20of%20the%20simulated%20environments%20can%0Asubstantially%20improve%20the%20generalizability%20and%20safety%20of%20the%20trained%20mobile%0Aagents.%20MetaUrban%20will%20be%20made%20publicly%20available%20to%20provide%20more%20research%0Aopportunities%20and%20foster%20safe%20and%20trustworthy%20embodied%20AI%20in%20urban%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaUrban%253A%2520A%2520Simulation%2520Platform%2520for%2520Embodied%2520AI%2520in%2520Urban%2520Spaces%26entry.906535625%3DWayne%2520Wu%2520and%2520Honglin%2520He%2520and%2520Yiran%2520Wang%2520and%2520Chenda%2520Duan%2520and%2520Jack%2520He%2520and%2520Zhizheng%2520Liu%2520and%2520Quanyi%2520Li%2520and%2520Bolei%2520Zhou%26entry.1292438233%3D%2520%2520Public%2520urban%2520spaces%2520like%2520streetscapes%2520and%2520plazas%2520serve%2520residents%2520and%250Aaccommodate%2520social%2520life%2520in%2520all%2520its%2520vibrant%2520variations.%2520Recent%2520advances%2520in%250ARobotics%2520and%2520Embodied%2520AI%2520make%2520public%2520urban%2520spaces%2520no%2520longer%2520exclusive%2520to%250Ahumans.%2520Food%2520delivery%2520bots%2520and%2520electric%2520wheelchairs%2520have%2520started%2520sharing%250Asidewalks%2520with%2520pedestrians%252C%2520while%2520diverse%2520robot%2520dogs%2520and%2520humanoids%2520have%250Arecently%2520emerged%2520in%2520the%2520street.%2520Ensuring%2520the%2520generalizability%2520and%2520safety%2520of%250Athese%2520forthcoming%2520mobile%2520machines%2520is%2520crucial%2520when%2520navigating%2520through%2520the%250Abustling%2520streets%2520in%2520urban%2520spaces.%2520In%2520this%2520work%252C%2520we%2520present%2520MetaUrban%252C%2520a%250Acompositional%2520simulation%2520platform%2520for%2520Embodied%2520AI%2520research%2520in%2520urban%2520spaces.%250AMetaUrban%2520can%2520construct%2520an%2520infinite%2520number%2520of%2520interactive%2520urban%2520scenes%2520from%250Acompositional%2520elements%252C%2520covering%2520a%2520vast%2520array%2520of%2520ground%2520plans%252C%2520object%250Aplacements%252C%2520pedestrians%252C%2520vulnerable%2520road%2520users%252C%2520and%2520other%2520mobile%2520agents%2527%250Aappearances%2520and%2520dynamics.%2520We%2520design%2520point%2520navigation%2520and%2520social%2520navigation%250Atasks%2520as%2520the%2520pilot%2520study%2520using%2520MetaUrban%2520for%2520embodied%2520AI%2520research%2520and%2520establish%250Avarious%2520baselines%2520of%2520Reinforcement%2520Learning%2520and%2520Imitation%2520Learning.%2520Experiments%250Ademonstrate%2520that%2520the%2520compositional%2520nature%2520of%2520the%2520simulated%2520environments%2520can%250Asubstantially%2520improve%2520the%2520generalizability%2520and%2520safety%2520of%2520the%2520trained%2520mobile%250Aagents.%2520MetaUrban%2520will%2520be%2520made%2520publicly%2520available%2520to%2520provide%2520more%2520research%250Aopportunities%2520and%2520foster%2520safe%2520and%2520trustworthy%2520embodied%2520AI%2520in%2520urban%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaUrban%3A%20A%20Simulation%20Platform%20for%20Embodied%20AI%20in%20Urban%20Spaces&entry.906535625=Wayne%20Wu%20and%20Honglin%20He%20and%20Yiran%20Wang%20and%20Chenda%20Duan%20and%20Jack%20He%20and%20Zhizheng%20Liu%20and%20Quanyi%20Li%20and%20Bolei%20Zhou&entry.1292438233=%20%20Public%20urban%20spaces%20like%20streetscapes%20and%20plazas%20serve%20residents%20and%0Aaccommodate%20social%20life%20in%20all%20its%20vibrant%20variations.%20Recent%20advances%20in%0ARobotics%20and%20Embodied%20AI%20make%20public%20urban%20spaces%20no%20longer%20exclusive%20to%0Ahumans.%20Food%20delivery%20bots%20and%20electric%20wheelchairs%20have%20started%20sharing%0Asidewalks%20with%20pedestrians%2C%20while%20diverse%20robot%20dogs%20and%20humanoids%20have%0Arecently%20emerged%20in%20the%20street.%20Ensuring%20the%20generalizability%20and%20safety%20of%0Athese%20forthcoming%20mobile%20machines%20is%20crucial%20when%20navigating%20through%20the%0Abustling%20streets%20in%20urban%20spaces.%20In%20this%20work%2C%20we%20present%20MetaUrban%2C%20a%0Acompositional%20simulation%20platform%20for%20Embodied%20AI%20research%20in%20urban%20spaces.%0AMetaUrban%20can%20construct%20an%20infinite%20number%20of%20interactive%20urban%20scenes%20from%0Acompositional%20elements%2C%20covering%20a%20vast%20array%20of%20ground%20plans%2C%20object%0Aplacements%2C%20pedestrians%2C%20vulnerable%20road%20users%2C%20and%20other%20mobile%20agents%27%0Aappearances%20and%20dynamics.%20We%20design%20point%20navigation%20and%20social%20navigation%0Atasks%20as%20the%20pilot%20study%20using%20MetaUrban%20for%20embodied%20AI%20research%20and%20establish%0Avarious%20baselines%20of%20Reinforcement%20Learning%20and%20Imitation%20Learning.%20Experiments%0Ademonstrate%20that%20the%20compositional%20nature%20of%20the%20simulated%20environments%20can%0Asubstantially%20improve%20the%20generalizability%20and%20safety%20of%20the%20trained%20mobile%0Aagents.%20MetaUrban%20will%20be%20made%20publicly%20available%20to%20provide%20more%20research%0Aopportunities%20and%20foster%20safe%20and%20trustworthy%20embodied%20AI%20in%20urban%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08725v1&entry.124074799=Read"},
{"title": "Haar Nuclear Norms with Applications to Remote Sensing Imagery\n  Restoration", "author": "Shuang Xu and Chang Yu and Jiangjun Peng and Xiangyong Cao", "abstract": "  Remote sensing image restoration aims to reconstruct missing or corrupted\nareas within images. To date, low-rank based models have garnered significant\ninterest in this field. This paper proposes a novel low-rank regularization\nterm, named the Haar nuclear norm (HNN), for efficient and effective remote\nsensing image restoration.\n  It leverages the low-rank properties of wavelet coefficients derived from the\n2-D frontal slice-wise Haar discrete wavelet transform, effectively modeling\nthe low-rank prior for separated coarse-grained structure and fine-grained\ntextures in the image. Experimental evaluations conducted on hyperspectral\nimage inpainting, multi-temporal image cloud removal, and hyperspectral image\ndenoising have revealed the HNN's potential. Typically, HNN achieves a\nperformance improvement of 1-4 dB and a speedup of 10-28x compared to some\nstate-of-the-art methods (e.g., tensor correlated total variation, and\nfully-connected tensor network) for inpainting tasks.\n", "link": "http://arxiv.org/abs/2407.08509v1", "date": "2024-07-11", "relevancy": 1.5292, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5333}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4837}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Haar%20Nuclear%20Norms%20with%20Applications%20to%20Remote%20Sensing%20Imagery%0A%20%20Restoration&body=Title%3A%20Haar%20Nuclear%20Norms%20with%20Applications%20to%20Remote%20Sensing%20Imagery%0A%20%20Restoration%0AAuthor%3A%20Shuang%20Xu%20and%20Chang%20Yu%20and%20Jiangjun%20Peng%20and%20Xiangyong%20Cao%0AAbstract%3A%20%20%20Remote%20sensing%20image%20restoration%20aims%20to%20reconstruct%20missing%20or%20corrupted%0Aareas%20within%20images.%20To%20date%2C%20low-rank%20based%20models%20have%20garnered%20significant%0Ainterest%20in%20this%20field.%20This%20paper%20proposes%20a%20novel%20low-rank%20regularization%0Aterm%2C%20named%20the%20Haar%20nuclear%20norm%20%28HNN%29%2C%20for%20efficient%20and%20effective%20remote%0Asensing%20image%20restoration.%0A%20%20It%20leverages%20the%20low-rank%20properties%20of%20wavelet%20coefficients%20derived%20from%20the%0A2-D%20frontal%20slice-wise%20Haar%20discrete%20wavelet%20transform%2C%20effectively%20modeling%0Athe%20low-rank%20prior%20for%20separated%20coarse-grained%20structure%20and%20fine-grained%0Atextures%20in%20the%20image.%20Experimental%20evaluations%20conducted%20on%20hyperspectral%0Aimage%20inpainting%2C%20multi-temporal%20image%20cloud%20removal%2C%20and%20hyperspectral%20image%0Adenoising%20have%20revealed%20the%20HNN%27s%20potential.%20Typically%2C%20HNN%20achieves%20a%0Aperformance%20improvement%20of%201-4%20dB%20and%20a%20speedup%20of%2010-28x%20compared%20to%20some%0Astate-of-the-art%20methods%20%28e.g.%2C%20tensor%20correlated%20total%20variation%2C%20and%0Afully-connected%20tensor%20network%29%20for%20inpainting%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHaar%2520Nuclear%2520Norms%2520with%2520Applications%2520to%2520Remote%2520Sensing%2520Imagery%250A%2520%2520Restoration%26entry.906535625%3DShuang%2520Xu%2520and%2520Chang%2520Yu%2520and%2520Jiangjun%2520Peng%2520and%2520Xiangyong%2520Cao%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image%2520restoration%2520aims%2520to%2520reconstruct%2520missing%2520or%2520corrupted%250Aareas%2520within%2520images.%2520To%2520date%252C%2520low-rank%2520based%2520models%2520have%2520garnered%2520significant%250Ainterest%2520in%2520this%2520field.%2520This%2520paper%2520proposes%2520a%2520novel%2520low-rank%2520regularization%250Aterm%252C%2520named%2520the%2520Haar%2520nuclear%2520norm%2520%2528HNN%2529%252C%2520for%2520efficient%2520and%2520effective%2520remote%250Asensing%2520image%2520restoration.%250A%2520%2520It%2520leverages%2520the%2520low-rank%2520properties%2520of%2520wavelet%2520coefficients%2520derived%2520from%2520the%250A2-D%2520frontal%2520slice-wise%2520Haar%2520discrete%2520wavelet%2520transform%252C%2520effectively%2520modeling%250Athe%2520low-rank%2520prior%2520for%2520separated%2520coarse-grained%2520structure%2520and%2520fine-grained%250Atextures%2520in%2520the%2520image.%2520Experimental%2520evaluations%2520conducted%2520on%2520hyperspectral%250Aimage%2520inpainting%252C%2520multi-temporal%2520image%2520cloud%2520removal%252C%2520and%2520hyperspectral%2520image%250Adenoising%2520have%2520revealed%2520the%2520HNN%2527s%2520potential.%2520Typically%252C%2520HNN%2520achieves%2520a%250Aperformance%2520improvement%2520of%25201-4%2520dB%2520and%2520a%2520speedup%2520of%252010-28x%2520compared%2520to%2520some%250Astate-of-the-art%2520methods%2520%2528e.g.%252C%2520tensor%2520correlated%2520total%2520variation%252C%2520and%250Afully-connected%2520tensor%2520network%2529%2520for%2520inpainting%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Haar%20Nuclear%20Norms%20with%20Applications%20to%20Remote%20Sensing%20Imagery%0A%20%20Restoration&entry.906535625=Shuang%20Xu%20and%20Chang%20Yu%20and%20Jiangjun%20Peng%20and%20Xiangyong%20Cao&entry.1292438233=%20%20Remote%20sensing%20image%20restoration%20aims%20to%20reconstruct%20missing%20or%20corrupted%0Aareas%20within%20images.%20To%20date%2C%20low-rank%20based%20models%20have%20garnered%20significant%0Ainterest%20in%20this%20field.%20This%20paper%20proposes%20a%20novel%20low-rank%20regularization%0Aterm%2C%20named%20the%20Haar%20nuclear%20norm%20%28HNN%29%2C%20for%20efficient%20and%20effective%20remote%0Asensing%20image%20restoration.%0A%20%20It%20leverages%20the%20low-rank%20properties%20of%20wavelet%20coefficients%20derived%20from%20the%0A2-D%20frontal%20slice-wise%20Haar%20discrete%20wavelet%20transform%2C%20effectively%20modeling%0Athe%20low-rank%20prior%20for%20separated%20coarse-grained%20structure%20and%20fine-grained%0Atextures%20in%20the%20image.%20Experimental%20evaluations%20conducted%20on%20hyperspectral%0Aimage%20inpainting%2C%20multi-temporal%20image%20cloud%20removal%2C%20and%20hyperspectral%20image%0Adenoising%20have%20revealed%20the%20HNN%27s%20potential.%20Typically%2C%20HNN%20achieves%20a%0Aperformance%20improvement%20of%201-4%20dB%20and%20a%20speedup%20of%2010-28x%20compared%20to%20some%0Astate-of-the-art%20methods%20%28e.g.%2C%20tensor%20correlated%20total%20variation%2C%20and%0Afully-connected%20tensor%20network%29%20for%20inpainting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08509v1&entry.124074799=Read"},
{"title": "The Career Interests of Large Language Models", "author": "Meng Hua and Yuan Cheng and Hengshu Zhu", "abstract": "  Recent advancements in Large Language Models (LLMs) have significantly\nextended their capabilities, evolving from basic text generation to complex,\nhuman-like interactions. In light of the possibilities that LLMs could assume\nsignificant workplace responsibilities, it becomes imminently necessary to\nexplore LLMs' capacities as professional assistants. This study focuses on the\naspect of career interests by applying the Occupation Network's Interest\nProfiler short form to LLMs as if they were human participants and investigates\ntheir hypothetical career interests and competence, examining how these vary\nwith language changes and model advancements. We analyzed the answers using a\ngeneral linear mixed model approach and found distinct career interest\ninclinations among LLMs, particularly towards the social and artistic domains.\nInterestingly, these preferences did not align with the occupations where LLMs\nexhibited higher competence. This novel approach of using psychometric\ninstruments and sophisticated statistical tools on LLMs unveils fresh\nperspectives on their integration into professional environments, highlighting\nhuman-like tendencies and promoting a reevaluation of LLMs' self-perception and\ncompetency alignment in the workforce.\n", "link": "http://arxiv.org/abs/2407.08564v1", "date": "2024-07-11", "relevancy": 1.2442, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4218}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4149}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Career%20Interests%20of%20Large%20Language%20Models&body=Title%3A%20The%20Career%20Interests%20of%20Large%20Language%20Models%0AAuthor%3A%20Meng%20Hua%20and%20Yuan%20Cheng%20and%20Hengshu%20Zhu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%0Aextended%20their%20capabilities%2C%20evolving%20from%20basic%20text%20generation%20to%20complex%2C%0Ahuman-like%20interactions.%20In%20light%20of%20the%20possibilities%20that%20LLMs%20could%20assume%0Asignificant%20workplace%20responsibilities%2C%20it%20becomes%20imminently%20necessary%20to%0Aexplore%20LLMs%27%20capacities%20as%20professional%20assistants.%20This%20study%20focuses%20on%20the%0Aaspect%20of%20career%20interests%20by%20applying%20the%20Occupation%20Network%27s%20Interest%0AProfiler%20short%20form%20to%20LLMs%20as%20if%20they%20were%20human%20participants%20and%20investigates%0Atheir%20hypothetical%20career%20interests%20and%20competence%2C%20examining%20how%20these%20vary%0Awith%20language%20changes%20and%20model%20advancements.%20We%20analyzed%20the%20answers%20using%20a%0Ageneral%20linear%20mixed%20model%20approach%20and%20found%20distinct%20career%20interest%0Ainclinations%20among%20LLMs%2C%20particularly%20towards%20the%20social%20and%20artistic%20domains.%0AInterestingly%2C%20these%20preferences%20did%20not%20align%20with%20the%20occupations%20where%20LLMs%0Aexhibited%20higher%20competence.%20This%20novel%20approach%20of%20using%20psychometric%0Ainstruments%20and%20sophisticated%20statistical%20tools%20on%20LLMs%20unveils%20fresh%0Aperspectives%20on%20their%20integration%20into%20professional%20environments%2C%20highlighting%0Ahuman-like%20tendencies%20and%20promoting%20a%20reevaluation%20of%20LLMs%27%20self-perception%20and%0Acompetency%20alignment%20in%20the%20workforce.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Career%2520Interests%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DMeng%2520Hua%2520and%2520Yuan%2520Cheng%2520and%2520Hengshu%2520Zhu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520significantly%250Aextended%2520their%2520capabilities%252C%2520evolving%2520from%2520basic%2520text%2520generation%2520to%2520complex%252C%250Ahuman-like%2520interactions.%2520In%2520light%2520of%2520the%2520possibilities%2520that%2520LLMs%2520could%2520assume%250Asignificant%2520workplace%2520responsibilities%252C%2520it%2520becomes%2520imminently%2520necessary%2520to%250Aexplore%2520LLMs%2527%2520capacities%2520as%2520professional%2520assistants.%2520This%2520study%2520focuses%2520on%2520the%250Aaspect%2520of%2520career%2520interests%2520by%2520applying%2520the%2520Occupation%2520Network%2527s%2520Interest%250AProfiler%2520short%2520form%2520to%2520LLMs%2520as%2520if%2520they%2520were%2520human%2520participants%2520and%2520investigates%250Atheir%2520hypothetical%2520career%2520interests%2520and%2520competence%252C%2520examining%2520how%2520these%2520vary%250Awith%2520language%2520changes%2520and%2520model%2520advancements.%2520We%2520analyzed%2520the%2520answers%2520using%2520a%250Ageneral%2520linear%2520mixed%2520model%2520approach%2520and%2520found%2520distinct%2520career%2520interest%250Ainclinations%2520among%2520LLMs%252C%2520particularly%2520towards%2520the%2520social%2520and%2520artistic%2520domains.%250AInterestingly%252C%2520these%2520preferences%2520did%2520not%2520align%2520with%2520the%2520occupations%2520where%2520LLMs%250Aexhibited%2520higher%2520competence.%2520This%2520novel%2520approach%2520of%2520using%2520psychometric%250Ainstruments%2520and%2520sophisticated%2520statistical%2520tools%2520on%2520LLMs%2520unveils%2520fresh%250Aperspectives%2520on%2520their%2520integration%2520into%2520professional%2520environments%252C%2520highlighting%250Ahuman-like%2520tendencies%2520and%2520promoting%2520a%2520reevaluation%2520of%2520LLMs%2527%2520self-perception%2520and%250Acompetency%2520alignment%2520in%2520the%2520workforce.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Career%20Interests%20of%20Large%20Language%20Models&entry.906535625=Meng%20Hua%20and%20Yuan%20Cheng%20and%20Hengshu%20Zhu&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%0Aextended%20their%20capabilities%2C%20evolving%20from%20basic%20text%20generation%20to%20complex%2C%0Ahuman-like%20interactions.%20In%20light%20of%20the%20possibilities%20that%20LLMs%20could%20assume%0Asignificant%20workplace%20responsibilities%2C%20it%20becomes%20imminently%20necessary%20to%0Aexplore%20LLMs%27%20capacities%20as%20professional%20assistants.%20This%20study%20focuses%20on%20the%0Aaspect%20of%20career%20interests%20by%20applying%20the%20Occupation%20Network%27s%20Interest%0AProfiler%20short%20form%20to%20LLMs%20as%20if%20they%20were%20human%20participants%20and%20investigates%0Atheir%20hypothetical%20career%20interests%20and%20competence%2C%20examining%20how%20these%20vary%0Awith%20language%20changes%20and%20model%20advancements.%20We%20analyzed%20the%20answers%20using%20a%0Ageneral%20linear%20mixed%20model%20approach%20and%20found%20distinct%20career%20interest%0Ainclinations%20among%20LLMs%2C%20particularly%20towards%20the%20social%20and%20artistic%20domains.%0AInterestingly%2C%20these%20preferences%20did%20not%20align%20with%20the%20occupations%20where%20LLMs%0Aexhibited%20higher%20competence.%20This%20novel%20approach%20of%20using%20psychometric%0Ainstruments%20and%20sophisticated%20statistical%20tools%20on%20LLMs%20unveils%20fresh%0Aperspectives%20on%20their%20integration%20into%20professional%20environments%2C%20highlighting%0Ahuman-like%20tendencies%20and%20promoting%20a%20reevaluation%20of%20LLMs%27%20self-perception%20and%0Acompetency%20alignment%20in%20the%20workforce.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08564v1&entry.124074799=Read"},
{"title": "Adaptive Smooth Non-Stationary Bandits", "author": "Joe Suk", "abstract": "  We study a $K$-armed non-stationary bandit model where rewards change\nsmoothly, as captured by H\\\"{o}lder class assumptions on rewards as functions\nof time. Such smooth changes are parametrized by a H\\\"{o}lder exponent $\\beta$\nand coefficient $\\lambda$. While various sub-cases of this general model have\nbeen studied in isolation, we first establish the minimax dynamic regret rate\ngenerally for all $K,\\beta,\\lambda$. Next, we show this optimal dynamic regret\ncan be attained adaptively, without knowledge of $\\beta,\\lambda$. To contrast,\neven with parameter knowledge, upper bounds were only previously known for\nlimited regimes $\\beta\\leq 1$ and $\\beta=2$ (Slivkins, 2014; Krishnamurthy and\nGopalan, 2021; Manegueu et al., 2021; Jia et al.,2023). Thus, our work resolves\nopen questions raised by these disparate threads of the literature.\n  We also study the problem of attaining faster gap-dependent regret rates in\nnon-stationary bandits. While such rates are long known to be impossible in\ngeneral (Garivier and Moulines, 2011), we show that environments admitting a\nsafe arm (Suk and Kpotufe, 2022) allow for much faster rates than the\nworst-case scaling with $\\sqrt{T}$. While previous works in this direction\nfocused on attaining the usual logarithmic regret bounds, as summed over\nstationary periods, our new gap-dependent rates reveal new optimistic regimes\nof non-stationarity where even the logarithmic bounds are pessimistic. We show\nour new gap-dependent rate is tight and that its achievability (i.e., as made\npossible by a safe arm) has a surprisingly simple and clean characterization\nwithin the smooth H\\\"{o}lder class model.\n", "link": "http://arxiv.org/abs/2407.08654v1", "date": "2024-07-11", "relevancy": 1.7408, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4424}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Smooth%20Non-Stationary%20Bandits&body=Title%3A%20Adaptive%20Smooth%20Non-Stationary%20Bandits%0AAuthor%3A%20Joe%20Suk%0AAbstract%3A%20%20%20We%20study%20a%20%24K%24-armed%20non-stationary%20bandit%20model%20where%20rewards%20change%0Asmoothly%2C%20as%20captured%20by%20H%5C%22%7Bo%7Dlder%20class%20assumptions%20on%20rewards%20as%20functions%0Aof%20time.%20Such%20smooth%20changes%20are%20parametrized%20by%20a%20H%5C%22%7Bo%7Dlder%20exponent%20%24%5Cbeta%24%0Aand%20coefficient%20%24%5Clambda%24.%20While%20various%20sub-cases%20of%20this%20general%20model%20have%0Abeen%20studied%20in%20isolation%2C%20we%20first%20establish%20the%20minimax%20dynamic%20regret%20rate%0Agenerally%20for%20all%20%24K%2C%5Cbeta%2C%5Clambda%24.%20Next%2C%20we%20show%20this%20optimal%20dynamic%20regret%0Acan%20be%20attained%20adaptively%2C%20without%20knowledge%20of%20%24%5Cbeta%2C%5Clambda%24.%20To%20contrast%2C%0Aeven%20with%20parameter%20knowledge%2C%20upper%20bounds%20were%20only%20previously%20known%20for%0Alimited%20regimes%20%24%5Cbeta%5Cleq%201%24%20and%20%24%5Cbeta%3D2%24%20%28Slivkins%2C%202014%3B%20Krishnamurthy%20and%0AGopalan%2C%202021%3B%20Manegueu%20et%20al.%2C%202021%3B%20Jia%20et%20al.%2C2023%29.%20Thus%2C%20our%20work%20resolves%0Aopen%20questions%20raised%20by%20these%20disparate%20threads%20of%20the%20literature.%0A%20%20We%20also%20study%20the%20problem%20of%20attaining%20faster%20gap-dependent%20regret%20rates%20in%0Anon-stationary%20bandits.%20While%20such%20rates%20are%20long%20known%20to%20be%20impossible%20in%0Ageneral%20%28Garivier%20and%20Moulines%2C%202011%29%2C%20we%20show%20that%20environments%20admitting%20a%0Asafe%20arm%20%28Suk%20and%20Kpotufe%2C%202022%29%20allow%20for%20much%20faster%20rates%20than%20the%0Aworst-case%20scaling%20with%20%24%5Csqrt%7BT%7D%24.%20While%20previous%20works%20in%20this%20direction%0Afocused%20on%20attaining%20the%20usual%20logarithmic%20regret%20bounds%2C%20as%20summed%20over%0Astationary%20periods%2C%20our%20new%20gap-dependent%20rates%20reveal%20new%20optimistic%20regimes%0Aof%20non-stationarity%20where%20even%20the%20logarithmic%20bounds%20are%20pessimistic.%20We%20show%0Aour%20new%20gap-dependent%20rate%20is%20tight%20and%20that%20its%20achievability%20%28i.e.%2C%20as%20made%0Apossible%20by%20a%20safe%20arm%29%20has%20a%20surprisingly%20simple%20and%20clean%20characterization%0Awithin%20the%20smooth%20H%5C%22%7Bo%7Dlder%20class%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Smooth%2520Non-Stationary%2520Bandits%26entry.906535625%3DJoe%2520Suk%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520%2524K%2524-armed%2520non-stationary%2520bandit%2520model%2520where%2520rewards%2520change%250Asmoothly%252C%2520as%2520captured%2520by%2520H%255C%2522%257Bo%257Dlder%2520class%2520assumptions%2520on%2520rewards%2520as%2520functions%250Aof%2520time.%2520Such%2520smooth%2520changes%2520are%2520parametrized%2520by%2520a%2520H%255C%2522%257Bo%257Dlder%2520exponent%2520%2524%255Cbeta%2524%250Aand%2520coefficient%2520%2524%255Clambda%2524.%2520While%2520various%2520sub-cases%2520of%2520this%2520general%2520model%2520have%250Abeen%2520studied%2520in%2520isolation%252C%2520we%2520first%2520establish%2520the%2520minimax%2520dynamic%2520regret%2520rate%250Agenerally%2520for%2520all%2520%2524K%252C%255Cbeta%252C%255Clambda%2524.%2520Next%252C%2520we%2520show%2520this%2520optimal%2520dynamic%2520regret%250Acan%2520be%2520attained%2520adaptively%252C%2520without%2520knowledge%2520of%2520%2524%255Cbeta%252C%255Clambda%2524.%2520To%2520contrast%252C%250Aeven%2520with%2520parameter%2520knowledge%252C%2520upper%2520bounds%2520were%2520only%2520previously%2520known%2520for%250Alimited%2520regimes%2520%2524%255Cbeta%255Cleq%25201%2524%2520and%2520%2524%255Cbeta%253D2%2524%2520%2528Slivkins%252C%25202014%253B%2520Krishnamurthy%2520and%250AGopalan%252C%25202021%253B%2520Manegueu%2520et%2520al.%252C%25202021%253B%2520Jia%2520et%2520al.%252C2023%2529.%2520Thus%252C%2520our%2520work%2520resolves%250Aopen%2520questions%2520raised%2520by%2520these%2520disparate%2520threads%2520of%2520the%2520literature.%250A%2520%2520We%2520also%2520study%2520the%2520problem%2520of%2520attaining%2520faster%2520gap-dependent%2520regret%2520rates%2520in%250Anon-stationary%2520bandits.%2520While%2520such%2520rates%2520are%2520long%2520known%2520to%2520be%2520impossible%2520in%250Ageneral%2520%2528Garivier%2520and%2520Moulines%252C%25202011%2529%252C%2520we%2520show%2520that%2520environments%2520admitting%2520a%250Asafe%2520arm%2520%2528Suk%2520and%2520Kpotufe%252C%25202022%2529%2520allow%2520for%2520much%2520faster%2520rates%2520than%2520the%250Aworst-case%2520scaling%2520with%2520%2524%255Csqrt%257BT%257D%2524.%2520While%2520previous%2520works%2520in%2520this%2520direction%250Afocused%2520on%2520attaining%2520the%2520usual%2520logarithmic%2520regret%2520bounds%252C%2520as%2520summed%2520over%250Astationary%2520periods%252C%2520our%2520new%2520gap-dependent%2520rates%2520reveal%2520new%2520optimistic%2520regimes%250Aof%2520non-stationarity%2520where%2520even%2520the%2520logarithmic%2520bounds%2520are%2520pessimistic.%2520We%2520show%250Aour%2520new%2520gap-dependent%2520rate%2520is%2520tight%2520and%2520that%2520its%2520achievability%2520%2528i.e.%252C%2520as%2520made%250Apossible%2520by%2520a%2520safe%2520arm%2529%2520has%2520a%2520surprisingly%2520simple%2520and%2520clean%2520characterization%250Awithin%2520the%2520smooth%2520H%255C%2522%257Bo%257Dlder%2520class%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Smooth%20Non-Stationary%20Bandits&entry.906535625=Joe%20Suk&entry.1292438233=%20%20We%20study%20a%20%24K%24-armed%20non-stationary%20bandit%20model%20where%20rewards%20change%0Asmoothly%2C%20as%20captured%20by%20H%5C%22%7Bo%7Dlder%20class%20assumptions%20on%20rewards%20as%20functions%0Aof%20time.%20Such%20smooth%20changes%20are%20parametrized%20by%20a%20H%5C%22%7Bo%7Dlder%20exponent%20%24%5Cbeta%24%0Aand%20coefficient%20%24%5Clambda%24.%20While%20various%20sub-cases%20of%20this%20general%20model%20have%0Abeen%20studied%20in%20isolation%2C%20we%20first%20establish%20the%20minimax%20dynamic%20regret%20rate%0Agenerally%20for%20all%20%24K%2C%5Cbeta%2C%5Clambda%24.%20Next%2C%20we%20show%20this%20optimal%20dynamic%20regret%0Acan%20be%20attained%20adaptively%2C%20without%20knowledge%20of%20%24%5Cbeta%2C%5Clambda%24.%20To%20contrast%2C%0Aeven%20with%20parameter%20knowledge%2C%20upper%20bounds%20were%20only%20previously%20known%20for%0Alimited%20regimes%20%24%5Cbeta%5Cleq%201%24%20and%20%24%5Cbeta%3D2%24%20%28Slivkins%2C%202014%3B%20Krishnamurthy%20and%0AGopalan%2C%202021%3B%20Manegueu%20et%20al.%2C%202021%3B%20Jia%20et%20al.%2C2023%29.%20Thus%2C%20our%20work%20resolves%0Aopen%20questions%20raised%20by%20these%20disparate%20threads%20of%20the%20literature.%0A%20%20We%20also%20study%20the%20problem%20of%20attaining%20faster%20gap-dependent%20regret%20rates%20in%0Anon-stationary%20bandits.%20While%20such%20rates%20are%20long%20known%20to%20be%20impossible%20in%0Ageneral%20%28Garivier%20and%20Moulines%2C%202011%29%2C%20we%20show%20that%20environments%20admitting%20a%0Asafe%20arm%20%28Suk%20and%20Kpotufe%2C%202022%29%20allow%20for%20much%20faster%20rates%20than%20the%0Aworst-case%20scaling%20with%20%24%5Csqrt%7BT%7D%24.%20While%20previous%20works%20in%20this%20direction%0Afocused%20on%20attaining%20the%20usual%20logarithmic%20regret%20bounds%2C%20as%20summed%20over%0Astationary%20periods%2C%20our%20new%20gap-dependent%20rates%20reveal%20new%20optimistic%20regimes%0Aof%20non-stationarity%20where%20even%20the%20logarithmic%20bounds%20are%20pessimistic.%20We%20show%0Aour%20new%20gap-dependent%20rate%20is%20tight%20and%20that%20its%20achievability%20%28i.e.%2C%20as%20made%0Apossible%20by%20a%20safe%20arm%29%20has%20a%20surprisingly%20simple%20and%20clean%20characterization%0Awithin%20the%20smooth%20H%5C%22%7Bo%7Dlder%20class%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08654v1&entry.124074799=Read"},
{"title": "High-Precision, Fair University Course Scheduling During a Pandemic", "author": "Matthew E. H. Petering and Mohammad Khamechian", "abstract": "  Scheduling university courses is extra challenging when classroom capacities\nare reduced because of social distancing requirements that are implemented in\nresponse to a pandemic such as COVID-19. In this work, we propose an expanded\ntaxonomy of course delivery modes, present an integer program, and develop a\ncourse scheduling algorithm to enable all course sections -- even the largest\n-- to have a significant classroom learning component during a pandemic. Our\napproach is fair by ensuring that a certain fraction of the instruction in\nevery course section occurs in the classroom. Unlike previous studies, we do\nnot allow rotating attendance and instead require simultaneous attendance in\nwhich all students in a section meet in 1-5 rooms at the same time but less\noften than in a normal semester. These mass meetings, which create\nopportunities for in-person midterm exams and group activities, are scheduled\nat high precision across all days of the semester rather than a single,\nrepeating week. A fast heuristic algorithm makes the schedule in an hour.\nResults: We consider the 1834 in-person course sections, 172 classrooms, and 96\ndays in the fall 2022 semester at [UniversityXYZ]. If average classroom\ncapacity is reduced by 75% due to a pandemic, our approach still allows at\nleast 25% of the instruction in every section, and more than 49% of all\ninstruction across the entire campus, to be in the classroom. Our method also\nproduces excellent results for regular classroom assignment. Managerial\nimplications: An algorithm based on the principles of fairness and simultaneous\nattendance can significantly improve university course schedules during a\npandemic and in normal times. High-precision schedules that prepare a campus\nfor various pandemic possibilities can be created with minimal administrative\neffort and activated at a moment's notice before or during a semester if an\noutbreak occurs.\n", "link": "http://arxiv.org/abs/2407.07355v2", "date": "2024-07-11", "relevancy": 1.5575, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3924}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3893}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Precision%2C%20Fair%20University%20Course%20Scheduling%20During%20a%20Pandemic&body=Title%3A%20High-Precision%2C%20Fair%20University%20Course%20Scheduling%20During%20a%20Pandemic%0AAuthor%3A%20Matthew%20E.%20H.%20Petering%20and%20Mohammad%20Khamechian%0AAbstract%3A%20%20%20Scheduling%20university%20courses%20is%20extra%20challenging%20when%20classroom%20capacities%0Aare%20reduced%20because%20of%20social%20distancing%20requirements%20that%20are%20implemented%20in%0Aresponse%20to%20a%20pandemic%20such%20as%20COVID-19.%20In%20this%20work%2C%20we%20propose%20an%20expanded%0Ataxonomy%20of%20course%20delivery%20modes%2C%20present%20an%20integer%20program%2C%20and%20develop%20a%0Acourse%20scheduling%20algorithm%20to%20enable%20all%20course%20sections%20--%20even%20the%20largest%0A--%20to%20have%20a%20significant%20classroom%20learning%20component%20during%20a%20pandemic.%20Our%0Aapproach%20is%20fair%20by%20ensuring%20that%20a%20certain%20fraction%20of%20the%20instruction%20in%0Aevery%20course%20section%20occurs%20in%20the%20classroom.%20Unlike%20previous%20studies%2C%20we%20do%0Anot%20allow%20rotating%20attendance%20and%20instead%20require%20simultaneous%20attendance%20in%0Awhich%20all%20students%20in%20a%20section%20meet%20in%201-5%20rooms%20at%20the%20same%20time%20but%20less%0Aoften%20than%20in%20a%20normal%20semester.%20These%20mass%20meetings%2C%20which%20create%0Aopportunities%20for%20in-person%20midterm%20exams%20and%20group%20activities%2C%20are%20scheduled%0Aat%20high%20precision%20across%20all%20days%20of%20the%20semester%20rather%20than%20a%20single%2C%0Arepeating%20week.%20A%20fast%20heuristic%20algorithm%20makes%20the%20schedule%20in%20an%20hour.%0AResults%3A%20We%20consider%20the%201834%20in-person%20course%20sections%2C%20172%20classrooms%2C%20and%2096%0Adays%20in%20the%20fall%202022%20semester%20at%20%5BUniversityXYZ%5D.%20If%20average%20classroom%0Acapacity%20is%20reduced%20by%2075%25%20due%20to%20a%20pandemic%2C%20our%20approach%20still%20allows%20at%0Aleast%2025%25%20of%20the%20instruction%20in%20every%20section%2C%20and%20more%20than%2049%25%20of%20all%0Ainstruction%20across%20the%20entire%20campus%2C%20to%20be%20in%20the%20classroom.%20Our%20method%20also%0Aproduces%20excellent%20results%20for%20regular%20classroom%20assignment.%20Managerial%0Aimplications%3A%20An%20algorithm%20based%20on%20the%20principles%20of%20fairness%20and%20simultaneous%0Aattendance%20can%20significantly%20improve%20university%20course%20schedules%20during%20a%0Apandemic%20and%20in%20normal%20times.%20High-precision%20schedules%20that%20prepare%20a%20campus%0Afor%20various%20pandemic%20possibilities%20can%20be%20created%20with%20minimal%20administrative%0Aeffort%20and%20activated%20at%20a%20moment%27s%20notice%20before%20or%20during%20a%20semester%20if%20an%0Aoutbreak%20occurs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Precision%252C%2520Fair%2520University%2520Course%2520Scheduling%2520During%2520a%2520Pandemic%26entry.906535625%3DMatthew%2520E.%2520H.%2520Petering%2520and%2520Mohammad%2520Khamechian%26entry.1292438233%3D%2520%2520Scheduling%2520university%2520courses%2520is%2520extra%2520challenging%2520when%2520classroom%2520capacities%250Aare%2520reduced%2520because%2520of%2520social%2520distancing%2520requirements%2520that%2520are%2520implemented%2520in%250Aresponse%2520to%2520a%2520pandemic%2520such%2520as%2520COVID-19.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520expanded%250Ataxonomy%2520of%2520course%2520delivery%2520modes%252C%2520present%2520an%2520integer%2520program%252C%2520and%2520develop%2520a%250Acourse%2520scheduling%2520algorithm%2520to%2520enable%2520all%2520course%2520sections%2520--%2520even%2520the%2520largest%250A--%2520to%2520have%2520a%2520significant%2520classroom%2520learning%2520component%2520during%2520a%2520pandemic.%2520Our%250Aapproach%2520is%2520fair%2520by%2520ensuring%2520that%2520a%2520certain%2520fraction%2520of%2520the%2520instruction%2520in%250Aevery%2520course%2520section%2520occurs%2520in%2520the%2520classroom.%2520Unlike%2520previous%2520studies%252C%2520we%2520do%250Anot%2520allow%2520rotating%2520attendance%2520and%2520instead%2520require%2520simultaneous%2520attendance%2520in%250Awhich%2520all%2520students%2520in%2520a%2520section%2520meet%2520in%25201-5%2520rooms%2520at%2520the%2520same%2520time%2520but%2520less%250Aoften%2520than%2520in%2520a%2520normal%2520semester.%2520These%2520mass%2520meetings%252C%2520which%2520create%250Aopportunities%2520for%2520in-person%2520midterm%2520exams%2520and%2520group%2520activities%252C%2520are%2520scheduled%250Aat%2520high%2520precision%2520across%2520all%2520days%2520of%2520the%2520semester%2520rather%2520than%2520a%2520single%252C%250Arepeating%2520week.%2520A%2520fast%2520heuristic%2520algorithm%2520makes%2520the%2520schedule%2520in%2520an%2520hour.%250AResults%253A%2520We%2520consider%2520the%25201834%2520in-person%2520course%2520sections%252C%2520172%2520classrooms%252C%2520and%252096%250Adays%2520in%2520the%2520fall%25202022%2520semester%2520at%2520%255BUniversityXYZ%255D.%2520If%2520average%2520classroom%250Acapacity%2520is%2520reduced%2520by%252075%2525%2520due%2520to%2520a%2520pandemic%252C%2520our%2520approach%2520still%2520allows%2520at%250Aleast%252025%2525%2520of%2520the%2520instruction%2520in%2520every%2520section%252C%2520and%2520more%2520than%252049%2525%2520of%2520all%250Ainstruction%2520across%2520the%2520entire%2520campus%252C%2520to%2520be%2520in%2520the%2520classroom.%2520Our%2520method%2520also%250Aproduces%2520excellent%2520results%2520for%2520regular%2520classroom%2520assignment.%2520Managerial%250Aimplications%253A%2520An%2520algorithm%2520based%2520on%2520the%2520principles%2520of%2520fairness%2520and%2520simultaneous%250Aattendance%2520can%2520significantly%2520improve%2520university%2520course%2520schedules%2520during%2520a%250Apandemic%2520and%2520in%2520normal%2520times.%2520High-precision%2520schedules%2520that%2520prepare%2520a%2520campus%250Afor%2520various%2520pandemic%2520possibilities%2520can%2520be%2520created%2520with%2520minimal%2520administrative%250Aeffort%2520and%2520activated%2520at%2520a%2520moment%2527s%2520notice%2520before%2520or%2520during%2520a%2520semester%2520if%2520an%250Aoutbreak%2520occurs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Precision%2C%20Fair%20University%20Course%20Scheduling%20During%20a%20Pandemic&entry.906535625=Matthew%20E.%20H.%20Petering%20and%20Mohammad%20Khamechian&entry.1292438233=%20%20Scheduling%20university%20courses%20is%20extra%20challenging%20when%20classroom%20capacities%0Aare%20reduced%20because%20of%20social%20distancing%20requirements%20that%20are%20implemented%20in%0Aresponse%20to%20a%20pandemic%20such%20as%20COVID-19.%20In%20this%20work%2C%20we%20propose%20an%20expanded%0Ataxonomy%20of%20course%20delivery%20modes%2C%20present%20an%20integer%20program%2C%20and%20develop%20a%0Acourse%20scheduling%20algorithm%20to%20enable%20all%20course%20sections%20--%20even%20the%20largest%0A--%20to%20have%20a%20significant%20classroom%20learning%20component%20during%20a%20pandemic.%20Our%0Aapproach%20is%20fair%20by%20ensuring%20that%20a%20certain%20fraction%20of%20the%20instruction%20in%0Aevery%20course%20section%20occurs%20in%20the%20classroom.%20Unlike%20previous%20studies%2C%20we%20do%0Anot%20allow%20rotating%20attendance%20and%20instead%20require%20simultaneous%20attendance%20in%0Awhich%20all%20students%20in%20a%20section%20meet%20in%201-5%20rooms%20at%20the%20same%20time%20but%20less%0Aoften%20than%20in%20a%20normal%20semester.%20These%20mass%20meetings%2C%20which%20create%0Aopportunities%20for%20in-person%20midterm%20exams%20and%20group%20activities%2C%20are%20scheduled%0Aat%20high%20precision%20across%20all%20days%20of%20the%20semester%20rather%20than%20a%20single%2C%0Arepeating%20week.%20A%20fast%20heuristic%20algorithm%20makes%20the%20schedule%20in%20an%20hour.%0AResults%3A%20We%20consider%20the%201834%20in-person%20course%20sections%2C%20172%20classrooms%2C%20and%2096%0Adays%20in%20the%20fall%202022%20semester%20at%20%5BUniversityXYZ%5D.%20If%20average%20classroom%0Acapacity%20is%20reduced%20by%2075%25%20due%20to%20a%20pandemic%2C%20our%20approach%20still%20allows%20at%0Aleast%2025%25%20of%20the%20instruction%20in%20every%20section%2C%20and%20more%20than%2049%25%20of%20all%0Ainstruction%20across%20the%20entire%20campus%2C%20to%20be%20in%20the%20classroom.%20Our%20method%20also%0Aproduces%20excellent%20results%20for%20regular%20classroom%20assignment.%20Managerial%0Aimplications%3A%20An%20algorithm%20based%20on%20the%20principles%20of%20fairness%20and%20simultaneous%0Aattendance%20can%20significantly%20improve%20university%20course%20schedules%20during%20a%0Apandemic%20and%20in%20normal%20times.%20High-precision%20schedules%20that%20prepare%20a%20campus%0Afor%20various%20pandemic%20possibilities%20can%20be%20created%20with%20minimal%20administrative%0Aeffort%20and%20activated%20at%20a%20moment%27s%20notice%20before%20or%20during%20a%20semester%20if%20an%0Aoutbreak%20occurs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07355v2&entry.124074799=Read"},
{"title": "Cloud Atlas: Efficient Fault Localization for Cloud Systems using\n  Language Models and Causal Insight", "author": "Zhiqiang Xie and Yujia Zheng and Lizi Ottens and Kun Zhang and Christos Kozyrakis and Jonathan Mace", "abstract": "  Runtime failure and performance degradation is commonplace in modern cloud\nsystems. For cloud providers, automatically determining the root cause of\nincidents is paramount to ensuring high reliability and availability as prompt\nfault localization can enable faster diagnosis and triage for timely\nresolution. A compelling solution explored in recent work is causal reasoning\nusing causal graphs to capture relationships between varied cloud system\nperformance metrics. To be effective, however, systems developers must\ncorrectly define the causal graph of their system, which is a time-consuming,\nbrittle, and challenging task that increases in difficulty for large and\ndynamic systems and requires domain expertise. Alternatively, automated\ndata-driven approaches have limited efficacy for cloud systems due to the\ninherent rarity of incidents. In this work, we present Atlas, a novel approach\nto automatically synthesizing causal graphs for cloud systems. Atlas leverages\nlarge language models (LLMs) to generate causal graphs using system\ndocumentation, telemetry, and deployment feedback. Atlas is complementary to\ndata-driven causal discovery techniques, and we further enhance Atlas with a\ndata-driven validation step. We evaluate Atlas across a range of fault\nlocalization scenarios and demonstrate that Atlas is capable of generating\ncausal graphs in a scalable and generalizable manner, with performance that far\nsurpasses that of data-driven algorithms and is commensurate to the\nground-truth baseline.\n", "link": "http://arxiv.org/abs/2407.08694v1", "date": "2024-07-11", "relevancy": 1.3439, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4588}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4348}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cloud%20Atlas%3A%20Efficient%20Fault%20Localization%20for%20Cloud%20Systems%20using%0A%20%20Language%20Models%20and%20Causal%20Insight&body=Title%3A%20Cloud%20Atlas%3A%20Efficient%20Fault%20Localization%20for%20Cloud%20Systems%20using%0A%20%20Language%20Models%20and%20Causal%20Insight%0AAuthor%3A%20Zhiqiang%20Xie%20and%20Yujia%20Zheng%20and%20Lizi%20Ottens%20and%20Kun%20Zhang%20and%20Christos%20Kozyrakis%20and%20Jonathan%20Mace%0AAbstract%3A%20%20%20Runtime%20failure%20and%20performance%20degradation%20is%20commonplace%20in%20modern%20cloud%0Asystems.%20For%20cloud%20providers%2C%20automatically%20determining%20the%20root%20cause%20of%0Aincidents%20is%20paramount%20to%20ensuring%20high%20reliability%20and%20availability%20as%20prompt%0Afault%20localization%20can%20enable%20faster%20diagnosis%20and%20triage%20for%20timely%0Aresolution.%20A%20compelling%20solution%20explored%20in%20recent%20work%20is%20causal%20reasoning%0Ausing%20causal%20graphs%20to%20capture%20relationships%20between%20varied%20cloud%20system%0Aperformance%20metrics.%20To%20be%20effective%2C%20however%2C%20systems%20developers%20must%0Acorrectly%20define%20the%20causal%20graph%20of%20their%20system%2C%20which%20is%20a%20time-consuming%2C%0Abrittle%2C%20and%20challenging%20task%20that%20increases%20in%20difficulty%20for%20large%20and%0Adynamic%20systems%20and%20requires%20domain%20expertise.%20Alternatively%2C%20automated%0Adata-driven%20approaches%20have%20limited%20efficacy%20for%20cloud%20systems%20due%20to%20the%0Ainherent%20rarity%20of%20incidents.%20In%20this%20work%2C%20we%20present%20Atlas%2C%20a%20novel%20approach%0Ato%20automatically%20synthesizing%20causal%20graphs%20for%20cloud%20systems.%20Atlas%20leverages%0Alarge%20language%20models%20%28LLMs%29%20to%20generate%20causal%20graphs%20using%20system%0Adocumentation%2C%20telemetry%2C%20and%20deployment%20feedback.%20Atlas%20is%20complementary%20to%0Adata-driven%20causal%20discovery%20techniques%2C%20and%20we%20further%20enhance%20Atlas%20with%20a%0Adata-driven%20validation%20step.%20We%20evaluate%20Atlas%20across%20a%20range%20of%20fault%0Alocalization%20scenarios%20and%20demonstrate%20that%20Atlas%20is%20capable%20of%20generating%0Acausal%20graphs%20in%20a%20scalable%20and%20generalizable%20manner%2C%20with%20performance%20that%20far%0Asurpasses%20that%20of%20data-driven%20algorithms%20and%20is%20commensurate%20to%20the%0Aground-truth%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCloud%2520Atlas%253A%2520Efficient%2520Fault%2520Localization%2520for%2520Cloud%2520Systems%2520using%250A%2520%2520Language%2520Models%2520and%2520Causal%2520Insight%26entry.906535625%3DZhiqiang%2520Xie%2520and%2520Yujia%2520Zheng%2520and%2520Lizi%2520Ottens%2520and%2520Kun%2520Zhang%2520and%2520Christos%2520Kozyrakis%2520and%2520Jonathan%2520Mace%26entry.1292438233%3D%2520%2520Runtime%2520failure%2520and%2520performance%2520degradation%2520is%2520commonplace%2520in%2520modern%2520cloud%250Asystems.%2520For%2520cloud%2520providers%252C%2520automatically%2520determining%2520the%2520root%2520cause%2520of%250Aincidents%2520is%2520paramount%2520to%2520ensuring%2520high%2520reliability%2520and%2520availability%2520as%2520prompt%250Afault%2520localization%2520can%2520enable%2520faster%2520diagnosis%2520and%2520triage%2520for%2520timely%250Aresolution.%2520A%2520compelling%2520solution%2520explored%2520in%2520recent%2520work%2520is%2520causal%2520reasoning%250Ausing%2520causal%2520graphs%2520to%2520capture%2520relationships%2520between%2520varied%2520cloud%2520system%250Aperformance%2520metrics.%2520To%2520be%2520effective%252C%2520however%252C%2520systems%2520developers%2520must%250Acorrectly%2520define%2520the%2520causal%2520graph%2520of%2520their%2520system%252C%2520which%2520is%2520a%2520time-consuming%252C%250Abrittle%252C%2520and%2520challenging%2520task%2520that%2520increases%2520in%2520difficulty%2520for%2520large%2520and%250Adynamic%2520systems%2520and%2520requires%2520domain%2520expertise.%2520Alternatively%252C%2520automated%250Adata-driven%2520approaches%2520have%2520limited%2520efficacy%2520for%2520cloud%2520systems%2520due%2520to%2520the%250Ainherent%2520rarity%2520of%2520incidents.%2520In%2520this%2520work%252C%2520we%2520present%2520Atlas%252C%2520a%2520novel%2520approach%250Ato%2520automatically%2520synthesizing%2520causal%2520graphs%2520for%2520cloud%2520systems.%2520Atlas%2520leverages%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520generate%2520causal%2520graphs%2520using%2520system%250Adocumentation%252C%2520telemetry%252C%2520and%2520deployment%2520feedback.%2520Atlas%2520is%2520complementary%2520to%250Adata-driven%2520causal%2520discovery%2520techniques%252C%2520and%2520we%2520further%2520enhance%2520Atlas%2520with%2520a%250Adata-driven%2520validation%2520step.%2520We%2520evaluate%2520Atlas%2520across%2520a%2520range%2520of%2520fault%250Alocalization%2520scenarios%2520and%2520demonstrate%2520that%2520Atlas%2520is%2520capable%2520of%2520generating%250Acausal%2520graphs%2520in%2520a%2520scalable%2520and%2520generalizable%2520manner%252C%2520with%2520performance%2520that%2520far%250Asurpasses%2520that%2520of%2520data-driven%2520algorithms%2520and%2520is%2520commensurate%2520to%2520the%250Aground-truth%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cloud%20Atlas%3A%20Efficient%20Fault%20Localization%20for%20Cloud%20Systems%20using%0A%20%20Language%20Models%20and%20Causal%20Insight&entry.906535625=Zhiqiang%20Xie%20and%20Yujia%20Zheng%20and%20Lizi%20Ottens%20and%20Kun%20Zhang%20and%20Christos%20Kozyrakis%20and%20Jonathan%20Mace&entry.1292438233=%20%20Runtime%20failure%20and%20performance%20degradation%20is%20commonplace%20in%20modern%20cloud%0Asystems.%20For%20cloud%20providers%2C%20automatically%20determining%20the%20root%20cause%20of%0Aincidents%20is%20paramount%20to%20ensuring%20high%20reliability%20and%20availability%20as%20prompt%0Afault%20localization%20can%20enable%20faster%20diagnosis%20and%20triage%20for%20timely%0Aresolution.%20A%20compelling%20solution%20explored%20in%20recent%20work%20is%20causal%20reasoning%0Ausing%20causal%20graphs%20to%20capture%20relationships%20between%20varied%20cloud%20system%0Aperformance%20metrics.%20To%20be%20effective%2C%20however%2C%20systems%20developers%20must%0Acorrectly%20define%20the%20causal%20graph%20of%20their%20system%2C%20which%20is%20a%20time-consuming%2C%0Abrittle%2C%20and%20challenging%20task%20that%20increases%20in%20difficulty%20for%20large%20and%0Adynamic%20systems%20and%20requires%20domain%20expertise.%20Alternatively%2C%20automated%0Adata-driven%20approaches%20have%20limited%20efficacy%20for%20cloud%20systems%20due%20to%20the%0Ainherent%20rarity%20of%20incidents.%20In%20this%20work%2C%20we%20present%20Atlas%2C%20a%20novel%20approach%0Ato%20automatically%20synthesizing%20causal%20graphs%20for%20cloud%20systems.%20Atlas%20leverages%0Alarge%20language%20models%20%28LLMs%29%20to%20generate%20causal%20graphs%20using%20system%0Adocumentation%2C%20telemetry%2C%20and%20deployment%20feedback.%20Atlas%20is%20complementary%20to%0Adata-driven%20causal%20discovery%20techniques%2C%20and%20we%20further%20enhance%20Atlas%20with%20a%0Adata-driven%20validation%20step.%20We%20evaluate%20Atlas%20across%20a%20range%20of%20fault%0Alocalization%20scenarios%20and%20demonstrate%20that%20Atlas%20is%20capable%20of%20generating%0Acausal%20graphs%20in%20a%20scalable%20and%20generalizable%20manner%2C%20with%20performance%20that%20far%0Asurpasses%20that%20of%20data-driven%20algorithms%20and%20is%20commensurate%20to%20the%0Aground-truth%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08694v1&entry.124074799=Read"},
{"title": "Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers", "author": "Zhengbo Zhang and Li Xu and Duo Peng and Hossein Rahmani and Jun Liu", "abstract": "  We introduce Diff-Tracker, a novel approach for the challenging unsupervised\nvisual tracking task leveraging the pre-trained text-to-image diffusion model.\nOur main idea is to leverage the rich knowledge encapsulated within the\npre-trained diffusion model, such as the understanding of image semantics and\nstructural information, to address unsupervised visual tracking. To this end,\nwe design an initial prompt learner to enable the diffusion model to recognize\nthe tracking target by learning a prompt representing the target. Furthermore,\nto facilitate dynamic adaptation of the prompt to the target's movements, we\npropose an online prompt updater. Extensive experiments on five benchmark\ndatasets demonstrate the effectiveness of our proposed method, which also\nachieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2407.08394v1", "date": "2024-07-11", "relevancy": 1.7548, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.619}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5866}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diff-Tracker%3A%20Text-to-Image%20Diffusion%20Models%20are%20Unsupervised%20Trackers&body=Title%3A%20Diff-Tracker%3A%20Text-to-Image%20Diffusion%20Models%20are%20Unsupervised%20Trackers%0AAuthor%3A%20Zhengbo%20Zhang%20and%20Li%20Xu%20and%20Duo%20Peng%20and%20Hossein%20Rahmani%20and%20Jun%20Liu%0AAbstract%3A%20%20%20We%20introduce%20Diff-Tracker%2C%20a%20novel%20approach%20for%20the%20challenging%20unsupervised%0Avisual%20tracking%20task%20leveraging%20the%20pre-trained%20text-to-image%20diffusion%20model.%0AOur%20main%20idea%20is%20to%20leverage%20the%20rich%20knowledge%20encapsulated%20within%20the%0Apre-trained%20diffusion%20model%2C%20such%20as%20the%20understanding%20of%20image%20semantics%20and%0Astructural%20information%2C%20to%20address%20unsupervised%20visual%20tracking.%20To%20this%20end%2C%0Awe%20design%20an%20initial%20prompt%20learner%20to%20enable%20the%20diffusion%20model%20to%20recognize%0Athe%20tracking%20target%20by%20learning%20a%20prompt%20representing%20the%20target.%20Furthermore%2C%0Ato%20facilitate%20dynamic%20adaptation%20of%20the%20prompt%20to%20the%20target%27s%20movements%2C%20we%0Apropose%20an%20online%20prompt%20updater.%20Extensive%20experiments%20on%20five%20benchmark%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20which%20also%0Aachieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiff-Tracker%253A%2520Text-to-Image%2520Diffusion%2520Models%2520are%2520Unsupervised%2520Trackers%26entry.906535625%3DZhengbo%2520Zhang%2520and%2520Li%2520Xu%2520and%2520Duo%2520Peng%2520and%2520Hossein%2520Rahmani%2520and%2520Jun%2520Liu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Diff-Tracker%252C%2520a%2520novel%2520approach%2520for%2520the%2520challenging%2520unsupervised%250Avisual%2520tracking%2520task%2520leveraging%2520the%2520pre-trained%2520text-to-image%2520diffusion%2520model.%250AOur%2520main%2520idea%2520is%2520to%2520leverage%2520the%2520rich%2520knowledge%2520encapsulated%2520within%2520the%250Apre-trained%2520diffusion%2520model%252C%2520such%2520as%2520the%2520understanding%2520of%2520image%2520semantics%2520and%250Astructural%2520information%252C%2520to%2520address%2520unsupervised%2520visual%2520tracking.%2520To%2520this%2520end%252C%250Awe%2520design%2520an%2520initial%2520prompt%2520learner%2520to%2520enable%2520the%2520diffusion%2520model%2520to%2520recognize%250Athe%2520tracking%2520target%2520by%2520learning%2520a%2520prompt%2520representing%2520the%2520target.%2520Furthermore%252C%250Ato%2520facilitate%2520dynamic%2520adaptation%2520of%2520the%2520prompt%2520to%2520the%2520target%2527s%2520movements%252C%2520we%250Apropose%2520an%2520online%2520prompt%2520updater.%2520Extensive%2520experiments%2520on%2520five%2520benchmark%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%252C%2520which%2520also%250Aachieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diff-Tracker%3A%20Text-to-Image%20Diffusion%20Models%20are%20Unsupervised%20Trackers&entry.906535625=Zhengbo%20Zhang%20and%20Li%20Xu%20and%20Duo%20Peng%20and%20Hossein%20Rahmani%20and%20Jun%20Liu&entry.1292438233=%20%20We%20introduce%20Diff-Tracker%2C%20a%20novel%20approach%20for%20the%20challenging%20unsupervised%0Avisual%20tracking%20task%20leveraging%20the%20pre-trained%20text-to-image%20diffusion%20model.%0AOur%20main%20idea%20is%20to%20leverage%20the%20rich%20knowledge%20encapsulated%20within%20the%0Apre-trained%20diffusion%20model%2C%20such%20as%20the%20understanding%20of%20image%20semantics%20and%0Astructural%20information%2C%20to%20address%20unsupervised%20visual%20tracking.%20To%20this%20end%2C%0Awe%20design%20an%20initial%20prompt%20learner%20to%20enable%20the%20diffusion%20model%20to%20recognize%0Athe%20tracking%20target%20by%20learning%20a%20prompt%20representing%20the%20target.%20Furthermore%2C%0Ato%20facilitate%20dynamic%20adaptation%20of%20the%20prompt%20to%20the%20target%27s%20movements%2C%20we%0Apropose%20an%20online%20prompt%20updater.%20Extensive%20experiments%20on%20five%20benchmark%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20which%20also%0Aachieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08394v1&entry.124074799=Read"},
{"title": "Leveraging Latent Diffusion Models for Training-Free In-Distribution\n  Data Augmentation for Surface Defect Detection", "author": "Federico Girella and Ziyue Liu and Franco Fummi and Francesco Setti and Marco Cristani and Luigi Capogrosso", "abstract": "  Defect detection is the task of identifying defects in production samples.\nUsually, defect detection classifiers are trained on ground-truth data formed\nby normal samples (negative data) and samples with defects (positive data),\nwhere the latter are consistently fewer than normal samples. State-of-the-art\ndata augmentation procedures add synthetic defect data by superimposing\nartifacts to normal samples to mitigate problems related to unbalanced training\ndata. These techniques often produce out-of-distribution images, resulting in\nsystems that learn what is not a normal sample but cannot accurately identify\nwhat a defect looks like. In this work, we introduce DIAG, a training-free\nDiffusion-based In-distribution Anomaly Generation pipeline for data\naugmentation. Unlike conventional image generation techniques, we implement a\nhuman-in-the-loop pipeline, where domain experts provide multimodal guidance to\nthe model through text descriptions and region localization of the possible\nanomalies. This strategic shift enhances the interpretability of results and\nfosters a more robust human feedback loop, facilitating iterative improvements\nof the generated outputs. Remarkably, our approach operates in a zero-shot\nmanner, avoiding time-consuming fine-tuning procedures while achieving superior\nperformance. We demonstrate the efficacy and versatility of DIAG with respect\nto state-of-the-art data augmentation approaches on the challenging KSDD2\ndataset, with an improvement in AP of approximately 18% when positive samples\nare available and 28% when they are missing. The source code is available at\nhttps://github.com/intelligolabs/DIAG.\n", "link": "http://arxiv.org/abs/2407.03961v2", "date": "2024-07-11", "relevancy": 1.7357, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6299}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5839}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Latent%20Diffusion%20Models%20for%20Training-Free%20In-Distribution%0A%20%20Data%20Augmentation%20for%20Surface%20Defect%20Detection&body=Title%3A%20Leveraging%20Latent%20Diffusion%20Models%20for%20Training-Free%20In-Distribution%0A%20%20Data%20Augmentation%20for%20Surface%20Defect%20Detection%0AAuthor%3A%20Federico%20Girella%20and%20Ziyue%20Liu%20and%20Franco%20Fummi%20and%20Francesco%20Setti%20and%20Marco%20Cristani%20and%20Luigi%20Capogrosso%0AAbstract%3A%20%20%20Defect%20detection%20is%20the%20task%20of%20identifying%20defects%20in%20production%20samples.%0AUsually%2C%20defect%20detection%20classifiers%20are%20trained%20on%20ground-truth%20data%20formed%0Aby%20normal%20samples%20%28negative%20data%29%20and%20samples%20with%20defects%20%28positive%20data%29%2C%0Awhere%20the%20latter%20are%20consistently%20fewer%20than%20normal%20samples.%20State-of-the-art%0Adata%20augmentation%20procedures%20add%20synthetic%20defect%20data%20by%20superimposing%0Aartifacts%20to%20normal%20samples%20to%20mitigate%20problems%20related%20to%20unbalanced%20training%0Adata.%20These%20techniques%20often%20produce%20out-of-distribution%20images%2C%20resulting%20in%0Asystems%20that%20learn%20what%20is%20not%20a%20normal%20sample%20but%20cannot%20accurately%20identify%0Awhat%20a%20defect%20looks%20like.%20In%20this%20work%2C%20we%20introduce%20DIAG%2C%20a%20training-free%0ADiffusion-based%20In-distribution%20Anomaly%20Generation%20pipeline%20for%20data%0Aaugmentation.%20Unlike%20conventional%20image%20generation%20techniques%2C%20we%20implement%20a%0Ahuman-in-the-loop%20pipeline%2C%20where%20domain%20experts%20provide%20multimodal%20guidance%20to%0Athe%20model%20through%20text%20descriptions%20and%20region%20localization%20of%20the%20possible%0Aanomalies.%20This%20strategic%20shift%20enhances%20the%20interpretability%20of%20results%20and%0Afosters%20a%20more%20robust%20human%20feedback%20loop%2C%20facilitating%20iterative%20improvements%0Aof%20the%20generated%20outputs.%20Remarkably%2C%20our%20approach%20operates%20in%20a%20zero-shot%0Amanner%2C%20avoiding%20time-consuming%20fine-tuning%20procedures%20while%20achieving%20superior%0Aperformance.%20We%20demonstrate%20the%20efficacy%20and%20versatility%20of%20DIAG%20with%20respect%0Ato%20state-of-the-art%20data%20augmentation%20approaches%20on%20the%20challenging%20KSDD2%0Adataset%2C%20with%20an%20improvement%20in%20AP%20of%20approximately%2018%25%20when%20positive%20samples%0Aare%20available%20and%2028%25%20when%20they%20are%20missing.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/intelligolabs/DIAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Latent%2520Diffusion%2520Models%2520for%2520Training-Free%2520In-Distribution%250A%2520%2520Data%2520Augmentation%2520for%2520Surface%2520Defect%2520Detection%26entry.906535625%3DFederico%2520Girella%2520and%2520Ziyue%2520Liu%2520and%2520Franco%2520Fummi%2520and%2520Francesco%2520Setti%2520and%2520Marco%2520Cristani%2520and%2520Luigi%2520Capogrosso%26entry.1292438233%3D%2520%2520Defect%2520detection%2520is%2520the%2520task%2520of%2520identifying%2520defects%2520in%2520production%2520samples.%250AUsually%252C%2520defect%2520detection%2520classifiers%2520are%2520trained%2520on%2520ground-truth%2520data%2520formed%250Aby%2520normal%2520samples%2520%2528negative%2520data%2529%2520and%2520samples%2520with%2520defects%2520%2528positive%2520data%2529%252C%250Awhere%2520the%2520latter%2520are%2520consistently%2520fewer%2520than%2520normal%2520samples.%2520State-of-the-art%250Adata%2520augmentation%2520procedures%2520add%2520synthetic%2520defect%2520data%2520by%2520superimposing%250Aartifacts%2520to%2520normal%2520samples%2520to%2520mitigate%2520problems%2520related%2520to%2520unbalanced%2520training%250Adata.%2520These%2520techniques%2520often%2520produce%2520out-of-distribution%2520images%252C%2520resulting%2520in%250Asystems%2520that%2520learn%2520what%2520is%2520not%2520a%2520normal%2520sample%2520but%2520cannot%2520accurately%2520identify%250Awhat%2520a%2520defect%2520looks%2520like.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DIAG%252C%2520a%2520training-free%250ADiffusion-based%2520In-distribution%2520Anomaly%2520Generation%2520pipeline%2520for%2520data%250Aaugmentation.%2520Unlike%2520conventional%2520image%2520generation%2520techniques%252C%2520we%2520implement%2520a%250Ahuman-in-the-loop%2520pipeline%252C%2520where%2520domain%2520experts%2520provide%2520multimodal%2520guidance%2520to%250Athe%2520model%2520through%2520text%2520descriptions%2520and%2520region%2520localization%2520of%2520the%2520possible%250Aanomalies.%2520This%2520strategic%2520shift%2520enhances%2520the%2520interpretability%2520of%2520results%2520and%250Afosters%2520a%2520more%2520robust%2520human%2520feedback%2520loop%252C%2520facilitating%2520iterative%2520improvements%250Aof%2520the%2520generated%2520outputs.%2520Remarkably%252C%2520our%2520approach%2520operates%2520in%2520a%2520zero-shot%250Amanner%252C%2520avoiding%2520time-consuming%2520fine-tuning%2520procedures%2520while%2520achieving%2520superior%250Aperformance.%2520We%2520demonstrate%2520the%2520efficacy%2520and%2520versatility%2520of%2520DIAG%2520with%2520respect%250Ato%2520state-of-the-art%2520data%2520augmentation%2520approaches%2520on%2520the%2520challenging%2520KSDD2%250Adataset%252C%2520with%2520an%2520improvement%2520in%2520AP%2520of%2520approximately%252018%2525%2520when%2520positive%2520samples%250Aare%2520available%2520and%252028%2525%2520when%2520they%2520are%2520missing.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/intelligolabs/DIAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Latent%20Diffusion%20Models%20for%20Training-Free%20In-Distribution%0A%20%20Data%20Augmentation%20for%20Surface%20Defect%20Detection&entry.906535625=Federico%20Girella%20and%20Ziyue%20Liu%20and%20Franco%20Fummi%20and%20Francesco%20Setti%20and%20Marco%20Cristani%20and%20Luigi%20Capogrosso&entry.1292438233=%20%20Defect%20detection%20is%20the%20task%20of%20identifying%20defects%20in%20production%20samples.%0AUsually%2C%20defect%20detection%20classifiers%20are%20trained%20on%20ground-truth%20data%20formed%0Aby%20normal%20samples%20%28negative%20data%29%20and%20samples%20with%20defects%20%28positive%20data%29%2C%0Awhere%20the%20latter%20are%20consistently%20fewer%20than%20normal%20samples.%20State-of-the-art%0Adata%20augmentation%20procedures%20add%20synthetic%20defect%20data%20by%20superimposing%0Aartifacts%20to%20normal%20samples%20to%20mitigate%20problems%20related%20to%20unbalanced%20training%0Adata.%20These%20techniques%20often%20produce%20out-of-distribution%20images%2C%20resulting%20in%0Asystems%20that%20learn%20what%20is%20not%20a%20normal%20sample%20but%20cannot%20accurately%20identify%0Awhat%20a%20defect%20looks%20like.%20In%20this%20work%2C%20we%20introduce%20DIAG%2C%20a%20training-free%0ADiffusion-based%20In-distribution%20Anomaly%20Generation%20pipeline%20for%20data%0Aaugmentation.%20Unlike%20conventional%20image%20generation%20techniques%2C%20we%20implement%20a%0Ahuman-in-the-loop%20pipeline%2C%20where%20domain%20experts%20provide%20multimodal%20guidance%20to%0Athe%20model%20through%20text%20descriptions%20and%20region%20localization%20of%20the%20possible%0Aanomalies.%20This%20strategic%20shift%20enhances%20the%20interpretability%20of%20results%20and%0Afosters%20a%20more%20robust%20human%20feedback%20loop%2C%20facilitating%20iterative%20improvements%0Aof%20the%20generated%20outputs.%20Remarkably%2C%20our%20approach%20operates%20in%20a%20zero-shot%0Amanner%2C%20avoiding%20time-consuming%20fine-tuning%20procedures%20while%20achieving%20superior%0Aperformance.%20We%20demonstrate%20the%20efficacy%20and%20versatility%20of%20DIAG%20with%20respect%0Ato%20state-of-the-art%20data%20augmentation%20approaches%20on%20the%20challenging%20KSDD2%0Adataset%2C%20with%20an%20improvement%20in%20AP%20of%20approximately%2018%25%20when%20positive%20samples%0Aare%20available%20and%2028%25%20when%20they%20are%20missing.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/intelligolabs/DIAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03961v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


