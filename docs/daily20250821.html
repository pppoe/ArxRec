<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250820.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in\n  Milliseconds", "author": "Jia Lu and Taoran Yi and Jiemin Fang and Chen Yang and Chuiyun Wu and Wei Shen and Wenyu Liu and Qi Tian and Xinggang Wang", "abstract": "  Reconstructing 3D human bodies from sparse views has been an appealing topic,\nwhich is crucial to broader the related applications. In this paper, we propose\na quite challenging but valuable task to reconstruct the human body from only\ntwo images, i.e., the front and back view, which can largely lower the barrier\nfor users to create their own 3D digital humans. The main challenges lie in the\ndifficulty of building 3D consistency and recovering missing information from\nthe highly sparse input. We redesign a geometry reconstruction model based on\nfoundation reconstruction models to predict consistent point clouds even input\nimages have scarce overlaps with extensive human data training. Furthermore, an\nenhancement algorithm is applied to supplement the missing color information,\nand then the complete human point clouds with colors can be obtained, which are\ndirectly transformed into 3D Gaussians for better rendering quality.\nExperiments show that our method can reconstruct the entire human in 190 ms on\na single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,\ndemonstrating state-of-the-art performance on the THuman2.0 and cross-domain\ndatasets. Additionally, our method can complete human reconstruction even with\nimages captured by low-cost mobile devices, reducing the requirements for data\ncollection. Demos and code are available at\nhttps://hustvl.github.io/Snap-Snap/.\n", "link": "http://arxiv.org/abs/2508.14892v1", "date": "2025-08-20", "relevancy": 3.5257, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7592}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6874}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snap-Snap%3A%20Taking%20Two%20Images%20to%20Reconstruct%203D%20Human%20Gaussians%20in%0A%20%20Milliseconds&body=Title%3A%20Snap-Snap%3A%20Taking%20Two%20Images%20to%20Reconstruct%203D%20Human%20Gaussians%20in%0A%20%20Milliseconds%0AAuthor%3A%20Jia%20Lu%20and%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Chuiyun%20Wu%20and%20Wei%20Shen%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Reconstructing%203D%20human%20bodies%20from%20sparse%20views%20has%20been%20an%20appealing%20topic%2C%0Awhich%20is%20crucial%20to%20broader%20the%20related%20applications.%20In%20this%20paper%2C%20we%20propose%0Aa%20quite%20challenging%20but%20valuable%20task%20to%20reconstruct%20the%20human%20body%20from%20only%0Atwo%20images%2C%20i.e.%2C%20the%20front%20and%20back%20view%2C%20which%20can%20largely%20lower%20the%20barrier%0Afor%20users%20to%20create%20their%20own%203D%20digital%20humans.%20The%20main%20challenges%20lie%20in%20the%0Adifficulty%20of%20building%203D%20consistency%20and%20recovering%20missing%20information%20from%0Athe%20highly%20sparse%20input.%20We%20redesign%20a%20geometry%20reconstruction%20model%20based%20on%0Afoundation%20reconstruction%20models%20to%20predict%20consistent%20point%20clouds%20even%20input%0Aimages%20have%20scarce%20overlaps%20with%20extensive%20human%20data%20training.%20Furthermore%2C%20an%0Aenhancement%20algorithm%20is%20applied%20to%20supplement%20the%20missing%20color%20information%2C%0Aand%20then%20the%20complete%20human%20point%20clouds%20with%20colors%20can%20be%20obtained%2C%20which%20are%0Adirectly%20transformed%20into%203D%20Gaussians%20for%20better%20rendering%20quality.%0AExperiments%20show%20that%20our%20method%20can%20reconstruct%20the%20entire%20human%20in%20190%20ms%20on%0Aa%20single%20NVIDIA%20RTX%204090%2C%20with%20two%20images%20at%20a%20resolution%20of%201024x1024%2C%0Ademonstrating%20state-of-the-art%20performance%20on%20the%20THuman2.0%20and%20cross-domain%0Adatasets.%20Additionally%2C%20our%20method%20can%20complete%20human%20reconstruction%20even%20with%0Aimages%20captured%20by%20low-cost%20mobile%20devices%2C%20reducing%20the%20requirements%20for%20data%0Acollection.%20Demos%20and%20code%20are%20available%20at%0Ahttps%3A//hustvl.github.io/Snap-Snap/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnap-Snap%253A%2520Taking%2520Two%2520Images%2520to%2520Reconstruct%25203D%2520Human%2520Gaussians%2520in%250A%2520%2520Milliseconds%26entry.906535625%3DJia%2520Lu%2520and%2520Taoran%2520Yi%2520and%2520Jiemin%2520Fang%2520and%2520Chen%2520Yang%2520and%2520Chuiyun%2520Wu%2520and%2520Wei%2520Shen%2520and%2520Wenyu%2520Liu%2520and%2520Qi%2520Tian%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520human%2520bodies%2520from%2520sparse%2520views%2520has%2520been%2520an%2520appealing%2520topic%252C%250Awhich%2520is%2520crucial%2520to%2520broader%2520the%2520related%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520quite%2520challenging%2520but%2520valuable%2520task%2520to%2520reconstruct%2520the%2520human%2520body%2520from%2520only%250Atwo%2520images%252C%2520i.e.%252C%2520the%2520front%2520and%2520back%2520view%252C%2520which%2520can%2520largely%2520lower%2520the%2520barrier%250Afor%2520users%2520to%2520create%2520their%2520own%25203D%2520digital%2520humans.%2520The%2520main%2520challenges%2520lie%2520in%2520the%250Adifficulty%2520of%2520building%25203D%2520consistency%2520and%2520recovering%2520missing%2520information%2520from%250Athe%2520highly%2520sparse%2520input.%2520We%2520redesign%2520a%2520geometry%2520reconstruction%2520model%2520based%2520on%250Afoundation%2520reconstruction%2520models%2520to%2520predict%2520consistent%2520point%2520clouds%2520even%2520input%250Aimages%2520have%2520scarce%2520overlaps%2520with%2520extensive%2520human%2520data%2520training.%2520Furthermore%252C%2520an%250Aenhancement%2520algorithm%2520is%2520applied%2520to%2520supplement%2520the%2520missing%2520color%2520information%252C%250Aand%2520then%2520the%2520complete%2520human%2520point%2520clouds%2520with%2520colors%2520can%2520be%2520obtained%252C%2520which%2520are%250Adirectly%2520transformed%2520into%25203D%2520Gaussians%2520for%2520better%2520rendering%2520quality.%250AExperiments%2520show%2520that%2520our%2520method%2520can%2520reconstruct%2520the%2520entire%2520human%2520in%2520190%2520ms%2520on%250Aa%2520single%2520NVIDIA%2520RTX%25204090%252C%2520with%2520two%2520images%2520at%2520a%2520resolution%2520of%25201024x1024%252C%250Ademonstrating%2520state-of-the-art%2520performance%2520on%2520the%2520THuman2.0%2520and%2520cross-domain%250Adatasets.%2520Additionally%252C%2520our%2520method%2520can%2520complete%2520human%2520reconstruction%2520even%2520with%250Aimages%2520captured%2520by%2520low-cost%2520mobile%2520devices%252C%2520reducing%2520the%2520requirements%2520for%2520data%250Acollection.%2520Demos%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//hustvl.github.io/Snap-Snap/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snap-Snap%3A%20Taking%20Two%20Images%20to%20Reconstruct%203D%20Human%20Gaussians%20in%0A%20%20Milliseconds&entry.906535625=Jia%20Lu%20and%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Chuiyun%20Wu%20and%20Wei%20Shen%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang&entry.1292438233=%20%20Reconstructing%203D%20human%20bodies%20from%20sparse%20views%20has%20been%20an%20appealing%20topic%2C%0Awhich%20is%20crucial%20to%20broader%20the%20related%20applications.%20In%20this%20paper%2C%20we%20propose%0Aa%20quite%20challenging%20but%20valuable%20task%20to%20reconstruct%20the%20human%20body%20from%20only%0Atwo%20images%2C%20i.e.%2C%20the%20front%20and%20back%20view%2C%20which%20can%20largely%20lower%20the%20barrier%0Afor%20users%20to%20create%20their%20own%203D%20digital%20humans.%20The%20main%20challenges%20lie%20in%20the%0Adifficulty%20of%20building%203D%20consistency%20and%20recovering%20missing%20information%20from%0Athe%20highly%20sparse%20input.%20We%20redesign%20a%20geometry%20reconstruction%20model%20based%20on%0Afoundation%20reconstruction%20models%20to%20predict%20consistent%20point%20clouds%20even%20input%0Aimages%20have%20scarce%20overlaps%20with%20extensive%20human%20data%20training.%20Furthermore%2C%20an%0Aenhancement%20algorithm%20is%20applied%20to%20supplement%20the%20missing%20color%20information%2C%0Aand%20then%20the%20complete%20human%20point%20clouds%20with%20colors%20can%20be%20obtained%2C%20which%20are%0Adirectly%20transformed%20into%203D%20Gaussians%20for%20better%20rendering%20quality.%0AExperiments%20show%20that%20our%20method%20can%20reconstruct%20the%20entire%20human%20in%20190%20ms%20on%0Aa%20single%20NVIDIA%20RTX%204090%2C%20with%20two%20images%20at%20a%20resolution%20of%201024x1024%2C%0Ademonstrating%20state-of-the-art%20performance%20on%20the%20THuman2.0%20and%20cross-domain%0Adatasets.%20Additionally%2C%20our%20method%20can%20complete%20human%20reconstruction%20even%20with%0Aimages%20captured%20by%20low-cost%20mobile%20devices%2C%20reducing%20the%20requirements%20for%20data%0Acollection.%20Demos%20and%20code%20are%20available%20at%0Ahttps%3A//hustvl.github.io/Snap-Snap/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14892v1&entry.124074799=Read"},
{"title": "Gaussian-LIC: Real-Time Photo-Realistic SLAM with Gaussian Splatting and\n  LiDAR-Inertial-Camera Fusion", "author": "Xiaolei Lang and Laijian Li and Chenming Wu and Chen Zhao and Lina Liu and Yong Liu and Jiajun Lv and Xingxing Zuo", "abstract": "  In this paper, we present a real-time photo-realistic SLAM method based on\nmarrying Gaussian Splatting with LiDAR-Inertial-Camera SLAM. Most existing\nradiance-field-based SLAM systems mainly focus on bounded indoor environments,\nequipped with RGB-D or RGB sensors. However, they are prone to decline when\nexpanding to unbounded scenes or encountering adverse conditions, such as\nviolent motions and changing illumination. In contrast, oriented to general\nscenarios, our approach additionally tightly fuses LiDAR, IMU, and camera for\nrobust pose estimation and photo-realistic online mapping. To compensate for\nregions unobserved by the LiDAR, we propose to integrate both the triangulated\nvisual points from images and LiDAR points for initializing 3D Gaussians. In\naddition, the modeling of the sky and varying camera exposure have been\nrealized for high-quality rendering. Notably, we implement our system purely\nwith C++ and CUDA, and meticulously design a series of strategies to accelerate\nthe online optimization of the Gaussian-based scene representation. Extensive\nexperiments demonstrate that our method outperforms its counterparts while\nmaintaining real-time capability. Impressively, regarding photo-realistic\nmapping, our method with our estimated poses even surpasses all the compared\napproaches that utilize privileged ground-truth poses for mapping. Our code has\nbeen released on https://github.com/APRIL-ZJU/Gaussian-LIC.\n", "link": "http://arxiv.org/abs/2404.06926v3", "date": "2025-08-20", "relevancy": 3.4233, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.791}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.635}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian-LIC%3A%20Real-Time%20Photo-Realistic%20SLAM%20with%20Gaussian%20Splatting%20and%0A%20%20LiDAR-Inertial-Camera%20Fusion&body=Title%3A%20Gaussian-LIC%3A%20Real-Time%20Photo-Realistic%20SLAM%20with%20Gaussian%20Splatting%20and%0A%20%20LiDAR-Inertial-Camera%20Fusion%0AAuthor%3A%20Xiaolei%20Lang%20and%20Laijian%20Li%20and%20Chenming%20Wu%20and%20Chen%20Zhao%20and%20Lina%20Liu%20and%20Yong%20Liu%20and%20Jiajun%20Lv%20and%20Xingxing%20Zuo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20real-time%20photo-realistic%20SLAM%20method%20based%20on%0Amarrying%20Gaussian%20Splatting%20with%20LiDAR-Inertial-Camera%20SLAM.%20Most%20existing%0Aradiance-field-based%20SLAM%20systems%20mainly%20focus%20on%20bounded%20indoor%20environments%2C%0Aequipped%20with%20RGB-D%20or%20RGB%20sensors.%20However%2C%20they%20are%20prone%20to%20decline%20when%0Aexpanding%20to%20unbounded%20scenes%20or%20encountering%20adverse%20conditions%2C%20such%20as%0Aviolent%20motions%20and%20changing%20illumination.%20In%20contrast%2C%20oriented%20to%20general%0Ascenarios%2C%20our%20approach%20additionally%20tightly%20fuses%20LiDAR%2C%20IMU%2C%20and%20camera%20for%0Arobust%20pose%20estimation%20and%20photo-realistic%20online%20mapping.%20To%20compensate%20for%0Aregions%20unobserved%20by%20the%20LiDAR%2C%20we%20propose%20to%20integrate%20both%20the%20triangulated%0Avisual%20points%20from%20images%20and%20LiDAR%20points%20for%20initializing%203D%20Gaussians.%20In%0Aaddition%2C%20the%20modeling%20of%20the%20sky%20and%20varying%20camera%20exposure%20have%20been%0Arealized%20for%20high-quality%20rendering.%20Notably%2C%20we%20implement%20our%20system%20purely%0Awith%20C%2B%2B%20and%20CUDA%2C%20and%20meticulously%20design%20a%20series%20of%20strategies%20to%20accelerate%0Athe%20online%20optimization%20of%20the%20Gaussian-based%20scene%20representation.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20outperforms%20its%20counterparts%20while%0Amaintaining%20real-time%20capability.%20Impressively%2C%20regarding%20photo-realistic%0Amapping%2C%20our%20method%20with%20our%20estimated%20poses%20even%20surpasses%20all%20the%20compared%0Aapproaches%20that%20utilize%20privileged%20ground-truth%20poses%20for%20mapping.%20Our%20code%20has%0Abeen%20released%20on%20https%3A//github.com/APRIL-ZJU/Gaussian-LIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06926v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian-LIC%253A%2520Real-Time%2520Photo-Realistic%2520SLAM%2520with%2520Gaussian%2520Splatting%2520and%250A%2520%2520LiDAR-Inertial-Camera%2520Fusion%26entry.906535625%3DXiaolei%2520Lang%2520and%2520Laijian%2520Li%2520and%2520Chenming%2520Wu%2520and%2520Chen%2520Zhao%2520and%2520Lina%2520Liu%2520and%2520Yong%2520Liu%2520and%2520Jiajun%2520Lv%2520and%2520Xingxing%2520Zuo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520real-time%2520photo-realistic%2520SLAM%2520method%2520based%2520on%250Amarrying%2520Gaussian%2520Splatting%2520with%2520LiDAR-Inertial-Camera%2520SLAM.%2520Most%2520existing%250Aradiance-field-based%2520SLAM%2520systems%2520mainly%2520focus%2520on%2520bounded%2520indoor%2520environments%252C%250Aequipped%2520with%2520RGB-D%2520or%2520RGB%2520sensors.%2520However%252C%2520they%2520are%2520prone%2520to%2520decline%2520when%250Aexpanding%2520to%2520unbounded%2520scenes%2520or%2520encountering%2520adverse%2520conditions%252C%2520such%2520as%250Aviolent%2520motions%2520and%2520changing%2520illumination.%2520In%2520contrast%252C%2520oriented%2520to%2520general%250Ascenarios%252C%2520our%2520approach%2520additionally%2520tightly%2520fuses%2520LiDAR%252C%2520IMU%252C%2520and%2520camera%2520for%250Arobust%2520pose%2520estimation%2520and%2520photo-realistic%2520online%2520mapping.%2520To%2520compensate%2520for%250Aregions%2520unobserved%2520by%2520the%2520LiDAR%252C%2520we%2520propose%2520to%2520integrate%2520both%2520the%2520triangulated%250Avisual%2520points%2520from%2520images%2520and%2520LiDAR%2520points%2520for%2520initializing%25203D%2520Gaussians.%2520In%250Aaddition%252C%2520the%2520modeling%2520of%2520the%2520sky%2520and%2520varying%2520camera%2520exposure%2520have%2520been%250Arealized%2520for%2520high-quality%2520rendering.%2520Notably%252C%2520we%2520implement%2520our%2520system%2520purely%250Awith%2520C%252B%252B%2520and%2520CUDA%252C%2520and%2520meticulously%2520design%2520a%2520series%2520of%2520strategies%2520to%2520accelerate%250Athe%2520online%2520optimization%2520of%2520the%2520Gaussian-based%2520scene%2520representation.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520its%2520counterparts%2520while%250Amaintaining%2520real-time%2520capability.%2520Impressively%252C%2520regarding%2520photo-realistic%250Amapping%252C%2520our%2520method%2520with%2520our%2520estimated%2520poses%2520even%2520surpasses%2520all%2520the%2520compared%250Aapproaches%2520that%2520utilize%2520privileged%2520ground-truth%2520poses%2520for%2520mapping.%2520Our%2520code%2520has%250Abeen%2520released%2520on%2520https%253A//github.com/APRIL-ZJU/Gaussian-LIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06926v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian-LIC%3A%20Real-Time%20Photo-Realistic%20SLAM%20with%20Gaussian%20Splatting%20and%0A%20%20LiDAR-Inertial-Camera%20Fusion&entry.906535625=Xiaolei%20Lang%20and%20Laijian%20Li%20and%20Chenming%20Wu%20and%20Chen%20Zhao%20and%20Lina%20Liu%20and%20Yong%20Liu%20and%20Jiajun%20Lv%20and%20Xingxing%20Zuo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20real-time%20photo-realistic%20SLAM%20method%20based%20on%0Amarrying%20Gaussian%20Splatting%20with%20LiDAR-Inertial-Camera%20SLAM.%20Most%20existing%0Aradiance-field-based%20SLAM%20systems%20mainly%20focus%20on%20bounded%20indoor%20environments%2C%0Aequipped%20with%20RGB-D%20or%20RGB%20sensors.%20However%2C%20they%20are%20prone%20to%20decline%20when%0Aexpanding%20to%20unbounded%20scenes%20or%20encountering%20adverse%20conditions%2C%20such%20as%0Aviolent%20motions%20and%20changing%20illumination.%20In%20contrast%2C%20oriented%20to%20general%0Ascenarios%2C%20our%20approach%20additionally%20tightly%20fuses%20LiDAR%2C%20IMU%2C%20and%20camera%20for%0Arobust%20pose%20estimation%20and%20photo-realistic%20online%20mapping.%20To%20compensate%20for%0Aregions%20unobserved%20by%20the%20LiDAR%2C%20we%20propose%20to%20integrate%20both%20the%20triangulated%0Avisual%20points%20from%20images%20and%20LiDAR%20points%20for%20initializing%203D%20Gaussians.%20In%0Aaddition%2C%20the%20modeling%20of%20the%20sky%20and%20varying%20camera%20exposure%20have%20been%0Arealized%20for%20high-quality%20rendering.%20Notably%2C%20we%20implement%20our%20system%20purely%0Awith%20C%2B%2B%20and%20CUDA%2C%20and%20meticulously%20design%20a%20series%20of%20strategies%20to%20accelerate%0Athe%20online%20optimization%20of%20the%20Gaussian-based%20scene%20representation.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20outperforms%20its%20counterparts%20while%0Amaintaining%20real-time%20capability.%20Impressively%2C%20regarding%20photo-realistic%0Amapping%2C%20our%20method%20with%20our%20estimated%20poses%20even%20surpasses%20all%20the%20compared%0Aapproaches%20that%20utilize%20privileged%20ground-truth%20poses%20for%20mapping.%20Our%20code%20has%0Abeen%20released%20on%20https%3A//github.com/APRIL-ZJU/Gaussian-LIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06926v3&entry.124074799=Read"},
{"title": "GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting", "author": "Jiaxin Wei and Stefan Leutenegger and Simon Schaefer", "abstract": "  Recent developments in 3D Gaussian Splatting have significantly enhanced\nnovel view synthesis, yet generating high-quality renderings from extreme novel\nviewpoints or partially observed regions remains challenging. Meanwhile,\ndiffusion models exhibit strong generative capabilities, but their reliance on\ntext prompts and lack of awareness of specific scene information hinder\naccurate 3D reconstruction tasks. To address these limitations, we introduce\nGSFix3D, a novel framework that improves the visual fidelity in\nunder-constrained regions by distilling prior knowledge from diffusion models\ninto 3D representations, while preserving consistency with observed scene\ndetails. At its core is GSFixer, a latent diffusion model obtained via our\ncustomized fine-tuning protocol that can leverage both mesh and 3D Gaussians to\nadapt pretrained generative models to a variety of environments and artifact\ntypes from different reconstruction methods, enabling robust novel view repair\nfor unseen camera poses. Moreover, we propose a random mask augmentation\nstrategy that empowers GSFixer to plausibly inpaint missing regions.\nExperiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer\nachieve state-of-the-art performance, requiring only minimal scene-specific\nfine-tuning on captured data. Real-world test further confirms its resilience\nto potential pose errors. Our code and data will be made publicly available.\nProject page: https://gsfix3d.github.io.\n", "link": "http://arxiv.org/abs/2508.14717v1", "date": "2025-08-20", "relevancy": 3.3469, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6865}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6676}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSFix3D%3A%20Diffusion-Guided%20Repair%20of%20Novel%20Views%20in%20Gaussian%20Splatting&body=Title%3A%20GSFix3D%3A%20Diffusion-Guided%20Repair%20of%20Novel%20Views%20in%20Gaussian%20Splatting%0AAuthor%3A%20Jiaxin%20Wei%20and%20Stefan%20Leutenegger%20and%20Simon%20Schaefer%0AAbstract%3A%20%20%20Recent%20developments%20in%203D%20Gaussian%20Splatting%20have%20significantly%20enhanced%0Anovel%20view%20synthesis%2C%20yet%20generating%20high-quality%20renderings%20from%20extreme%20novel%0Aviewpoints%20or%20partially%20observed%20regions%20remains%20challenging.%20Meanwhile%2C%0Adiffusion%20models%20exhibit%20strong%20generative%20capabilities%2C%20but%20their%20reliance%20on%0Atext%20prompts%20and%20lack%20of%20awareness%20of%20specific%20scene%20information%20hinder%0Aaccurate%203D%20reconstruction%20tasks.%20To%20address%20these%20limitations%2C%20we%20introduce%0AGSFix3D%2C%20a%20novel%20framework%20that%20improves%20the%20visual%20fidelity%20in%0Aunder-constrained%20regions%20by%20distilling%20prior%20knowledge%20from%20diffusion%20models%0Ainto%203D%20representations%2C%20while%20preserving%20consistency%20with%20observed%20scene%0Adetails.%20At%20its%20core%20is%20GSFixer%2C%20a%20latent%20diffusion%20model%20obtained%20via%20our%0Acustomized%20fine-tuning%20protocol%20that%20can%20leverage%20both%20mesh%20and%203D%20Gaussians%20to%0Aadapt%20pretrained%20generative%20models%20to%20a%20variety%20of%20environments%20and%20artifact%0Atypes%20from%20different%20reconstruction%20methods%2C%20enabling%20robust%20novel%20view%20repair%0Afor%20unseen%20camera%20poses.%20Moreover%2C%20we%20propose%20a%20random%20mask%20augmentation%0Astrategy%20that%20empowers%20GSFixer%20to%20plausibly%20inpaint%20missing%20regions.%0AExperiments%20on%20challenging%20benchmarks%20demonstrate%20that%20our%20GSFix3D%20and%20GSFixer%0Aachieve%20state-of-the-art%20performance%2C%20requiring%20only%20minimal%20scene-specific%0Afine-tuning%20on%20captured%20data.%20Real-world%20test%20further%20confirms%20its%20resilience%0Ato%20potential%20pose%20errors.%20Our%20code%20and%20data%20will%20be%20made%20publicly%20available.%0AProject%20page%3A%20https%3A//gsfix3d.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSFix3D%253A%2520Diffusion-Guided%2520Repair%2520of%2520Novel%2520Views%2520in%2520Gaussian%2520Splatting%26entry.906535625%3DJiaxin%2520Wei%2520and%2520Stefan%2520Leutenegger%2520and%2520Simon%2520Schaefer%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%25203D%2520Gaussian%2520Splatting%2520have%2520significantly%2520enhanced%250Anovel%2520view%2520synthesis%252C%2520yet%2520generating%2520high-quality%2520renderings%2520from%2520extreme%2520novel%250Aviewpoints%2520or%2520partially%2520observed%2520regions%2520remains%2520challenging.%2520Meanwhile%252C%250Adiffusion%2520models%2520exhibit%2520strong%2520generative%2520capabilities%252C%2520but%2520their%2520reliance%2520on%250Atext%2520prompts%2520and%2520lack%2520of%2520awareness%2520of%2520specific%2520scene%2520information%2520hinder%250Aaccurate%25203D%2520reconstruction%2520tasks.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250AGSFix3D%252C%2520a%2520novel%2520framework%2520that%2520improves%2520the%2520visual%2520fidelity%2520in%250Aunder-constrained%2520regions%2520by%2520distilling%2520prior%2520knowledge%2520from%2520diffusion%2520models%250Ainto%25203D%2520representations%252C%2520while%2520preserving%2520consistency%2520with%2520observed%2520scene%250Adetails.%2520At%2520its%2520core%2520is%2520GSFixer%252C%2520a%2520latent%2520diffusion%2520model%2520obtained%2520via%2520our%250Acustomized%2520fine-tuning%2520protocol%2520that%2520can%2520leverage%2520both%2520mesh%2520and%25203D%2520Gaussians%2520to%250Aadapt%2520pretrained%2520generative%2520models%2520to%2520a%2520variety%2520of%2520environments%2520and%2520artifact%250Atypes%2520from%2520different%2520reconstruction%2520methods%252C%2520enabling%2520robust%2520novel%2520view%2520repair%250Afor%2520unseen%2520camera%2520poses.%2520Moreover%252C%2520we%2520propose%2520a%2520random%2520mask%2520augmentation%250Astrategy%2520that%2520empowers%2520GSFixer%2520to%2520plausibly%2520inpaint%2520missing%2520regions.%250AExperiments%2520on%2520challenging%2520benchmarks%2520demonstrate%2520that%2520our%2520GSFix3D%2520and%2520GSFixer%250Aachieve%2520state-of-the-art%2520performance%252C%2520requiring%2520only%2520minimal%2520scene-specific%250Afine-tuning%2520on%2520captured%2520data.%2520Real-world%2520test%2520further%2520confirms%2520its%2520resilience%250Ato%2520potential%2520pose%2520errors.%2520Our%2520code%2520and%2520data%2520will%2520be%2520made%2520publicly%2520available.%250AProject%2520page%253A%2520https%253A//gsfix3d.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSFix3D%3A%20Diffusion-Guided%20Repair%20of%20Novel%20Views%20in%20Gaussian%20Splatting&entry.906535625=Jiaxin%20Wei%20and%20Stefan%20Leutenegger%20and%20Simon%20Schaefer&entry.1292438233=%20%20Recent%20developments%20in%203D%20Gaussian%20Splatting%20have%20significantly%20enhanced%0Anovel%20view%20synthesis%2C%20yet%20generating%20high-quality%20renderings%20from%20extreme%20novel%0Aviewpoints%20or%20partially%20observed%20regions%20remains%20challenging.%20Meanwhile%2C%0Adiffusion%20models%20exhibit%20strong%20generative%20capabilities%2C%20but%20their%20reliance%20on%0Atext%20prompts%20and%20lack%20of%20awareness%20of%20specific%20scene%20information%20hinder%0Aaccurate%203D%20reconstruction%20tasks.%20To%20address%20these%20limitations%2C%20we%20introduce%0AGSFix3D%2C%20a%20novel%20framework%20that%20improves%20the%20visual%20fidelity%20in%0Aunder-constrained%20regions%20by%20distilling%20prior%20knowledge%20from%20diffusion%20models%0Ainto%203D%20representations%2C%20while%20preserving%20consistency%20with%20observed%20scene%0Adetails.%20At%20its%20core%20is%20GSFixer%2C%20a%20latent%20diffusion%20model%20obtained%20via%20our%0Acustomized%20fine-tuning%20protocol%20that%20can%20leverage%20both%20mesh%20and%203D%20Gaussians%20to%0Aadapt%20pretrained%20generative%20models%20to%20a%20variety%20of%20environments%20and%20artifact%0Atypes%20from%20different%20reconstruction%20methods%2C%20enabling%20robust%20novel%20view%20repair%0Afor%20unseen%20camera%20poses.%20Moreover%2C%20we%20propose%20a%20random%20mask%20augmentation%0Astrategy%20that%20empowers%20GSFixer%20to%20plausibly%20inpaint%20missing%20regions.%0AExperiments%20on%20challenging%20benchmarks%20demonstrate%20that%20our%20GSFix3D%20and%20GSFixer%0Aachieve%20state-of-the-art%20performance%2C%20requiring%20only%20minimal%20scene-specific%0Afine-tuning%20on%20captured%20data.%20Real-world%20test%20further%20confirms%20its%20resilience%0Ato%20potential%20pose%20errors.%20Our%20code%20and%20data%20will%20be%20made%20publicly%20available.%0AProject%20page%3A%20https%3A//gsfix3d.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14717v1&entry.124074799=Read"},
{"title": "GeMS: Efficient Gaussian Splatting for Extreme Motion Blur", "author": "Gopi Raju Matta and Trisha Reddypalli and Vemunuri Divya Madhuri and Kaushik Mitra", "abstract": "  We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to\nhandle severely motion-blurred images. State-of-the-art deblurring methods for\nextreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches\nlike Deblur-GS, typically assume access to sharp images for camera pose\nestimation and point cloud generation, an unrealistic assumption. Methods\nrelying on COLMAP initialization, such as BAD-Gaussians, also fail due to\nunreliable feature correspondences under severe blur. To address these\nchallenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly\nfrom extremely blurred images. GeMS integrates: (1) VGGSfM, a deep\nlearning-based Structure-from-Motion pipeline that estimates poses and\ngenerates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which\nenables robust scene initialization by treating Gaussians as samples from a\nprobability distribution, eliminating heuristic densification and pruning; and\n(3) joint optimization of camera trajectories and Gaussian parameters for\nstable reconstruction. While this pipeline produces strong results,\ninaccuracies may remain when all inputs are severely blurred. To mitigate this,\nwe propose GeMS-E, which integrates a progressive refinement step using events:\n(4) Event-based Double Integral (EDI) deblurring restores sharper images that\nare then fed into GeMS, improving pose estimation, point cloud generation, and\noverall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art\nperformance on synthetic and real-world datasets. To our knowledge, this is the\nfirst framework to address extreme motion blur within 3DGS directly from\nseverely blurred inputs.\n", "link": "http://arxiv.org/abs/2508.14682v1", "date": "2025-08-20", "relevancy": 3.228, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.649}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6487}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeMS%3A%20Efficient%20Gaussian%20Splatting%20for%20Extreme%20Motion%20Blur&body=Title%3A%20GeMS%3A%20Efficient%20Gaussian%20Splatting%20for%20Extreme%20Motion%20Blur%0AAuthor%3A%20Gopi%20Raju%20Matta%20and%20Trisha%20Reddypalli%20and%20Vemunuri%20Divya%20Madhuri%20and%20Kaushik%20Mitra%0AAbstract%3A%20%20%20We%20introduce%20GeMS%2C%20a%20framework%20for%203D%20Gaussian%20Splatting%20%283DGS%29%20designed%20to%0Ahandle%20severely%20motion-blurred%20images.%20State-of-the-art%20deblurring%20methods%20for%0Aextreme%20blur%2C%20such%20as%20ExBluRF%2C%20as%20well%20as%20Gaussian%20Splatting-based%20approaches%0Alike%20Deblur-GS%2C%20typically%20assume%20access%20to%20sharp%20images%20for%20camera%20pose%0Aestimation%20and%20point%20cloud%20generation%2C%20an%20unrealistic%20assumption.%20Methods%0Arelying%20on%20COLMAP%20initialization%2C%20such%20as%20BAD-Gaussians%2C%20also%20fail%20due%20to%0Aunreliable%20feature%20correspondences%20under%20severe%20blur.%20To%20address%20these%0Achallenges%2C%20we%20propose%20GeMS%2C%20a%203DGS%20framework%20that%20reconstructs%20scenes%20directly%0Afrom%20extremely%20blurred%20images.%20GeMS%20integrates%3A%20%281%29%20VGGSfM%2C%20a%20deep%0Alearning-based%20Structure-from-Motion%20pipeline%20that%20estimates%20poses%20and%0Agenerates%20point%20clouds%20directly%20from%20blurred%20inputs%3B%20%282%29%203DGS-MCMC%2C%20which%0Aenables%20robust%20scene%20initialization%20by%20treating%20Gaussians%20as%20samples%20from%20a%0Aprobability%20distribution%2C%20eliminating%20heuristic%20densification%20and%20pruning%3B%20and%0A%283%29%20joint%20optimization%20of%20camera%20trajectories%20and%20Gaussian%20parameters%20for%0Astable%20reconstruction.%20While%20this%20pipeline%20produces%20strong%20results%2C%0Ainaccuracies%20may%20remain%20when%20all%20inputs%20are%20severely%20blurred.%20To%20mitigate%20this%2C%0Awe%20propose%20GeMS-E%2C%20which%20integrates%20a%20progressive%20refinement%20step%20using%20events%3A%0A%284%29%20Event-based%20Double%20Integral%20%28EDI%29%20deblurring%20restores%20sharper%20images%20that%0Aare%20then%20fed%20into%20GeMS%2C%20improving%20pose%20estimation%2C%20point%20cloud%20generation%2C%20and%0Aoverall%20reconstruction.%20Both%20GeMS%20and%20GeMS-E%20achieve%20state-of-the-art%0Aperformance%20on%20synthetic%20and%20real-world%20datasets.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20framework%20to%20address%20extreme%20motion%20blur%20within%203DGS%20directly%20from%0Aseverely%20blurred%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeMS%253A%2520Efficient%2520Gaussian%2520Splatting%2520for%2520Extreme%2520Motion%2520Blur%26entry.906535625%3DGopi%2520Raju%2520Matta%2520and%2520Trisha%2520Reddypalli%2520and%2520Vemunuri%2520Divya%2520Madhuri%2520and%2520Kaushik%2520Mitra%26entry.1292438233%3D%2520%2520We%2520introduce%2520GeMS%252C%2520a%2520framework%2520for%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520designed%2520to%250Ahandle%2520severely%2520motion-blurred%2520images.%2520State-of-the-art%2520deblurring%2520methods%2520for%250Aextreme%2520blur%252C%2520such%2520as%2520ExBluRF%252C%2520as%2520well%2520as%2520Gaussian%2520Splatting-based%2520approaches%250Alike%2520Deblur-GS%252C%2520typically%2520assume%2520access%2520to%2520sharp%2520images%2520for%2520camera%2520pose%250Aestimation%2520and%2520point%2520cloud%2520generation%252C%2520an%2520unrealistic%2520assumption.%2520Methods%250Arelying%2520on%2520COLMAP%2520initialization%252C%2520such%2520as%2520BAD-Gaussians%252C%2520also%2520fail%2520due%2520to%250Aunreliable%2520feature%2520correspondences%2520under%2520severe%2520blur.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520GeMS%252C%2520a%25203DGS%2520framework%2520that%2520reconstructs%2520scenes%2520directly%250Afrom%2520extremely%2520blurred%2520images.%2520GeMS%2520integrates%253A%2520%25281%2529%2520VGGSfM%252C%2520a%2520deep%250Alearning-based%2520Structure-from-Motion%2520pipeline%2520that%2520estimates%2520poses%2520and%250Agenerates%2520point%2520clouds%2520directly%2520from%2520blurred%2520inputs%253B%2520%25282%2529%25203DGS-MCMC%252C%2520which%250Aenables%2520robust%2520scene%2520initialization%2520by%2520treating%2520Gaussians%2520as%2520samples%2520from%2520a%250Aprobability%2520distribution%252C%2520eliminating%2520heuristic%2520densification%2520and%2520pruning%253B%2520and%250A%25283%2529%2520joint%2520optimization%2520of%2520camera%2520trajectories%2520and%2520Gaussian%2520parameters%2520for%250Astable%2520reconstruction.%2520While%2520this%2520pipeline%2520produces%2520strong%2520results%252C%250Ainaccuracies%2520may%2520remain%2520when%2520all%2520inputs%2520are%2520severely%2520blurred.%2520To%2520mitigate%2520this%252C%250Awe%2520propose%2520GeMS-E%252C%2520which%2520integrates%2520a%2520progressive%2520refinement%2520step%2520using%2520events%253A%250A%25284%2529%2520Event-based%2520Double%2520Integral%2520%2528EDI%2529%2520deblurring%2520restores%2520sharper%2520images%2520that%250Aare%2520then%2520fed%2520into%2520GeMS%252C%2520improving%2520pose%2520estimation%252C%2520point%2520cloud%2520generation%252C%2520and%250Aoverall%2520reconstruction.%2520Both%2520GeMS%2520and%2520GeMS-E%2520achieve%2520state-of-the-art%250Aperformance%2520on%2520synthetic%2520and%2520real-world%2520datasets.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520framework%2520to%2520address%2520extreme%2520motion%2520blur%2520within%25203DGS%2520directly%2520from%250Aseverely%2520blurred%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeMS%3A%20Efficient%20Gaussian%20Splatting%20for%20Extreme%20Motion%20Blur&entry.906535625=Gopi%20Raju%20Matta%20and%20Trisha%20Reddypalli%20and%20Vemunuri%20Divya%20Madhuri%20and%20Kaushik%20Mitra&entry.1292438233=%20%20We%20introduce%20GeMS%2C%20a%20framework%20for%203D%20Gaussian%20Splatting%20%283DGS%29%20designed%20to%0Ahandle%20severely%20motion-blurred%20images.%20State-of-the-art%20deblurring%20methods%20for%0Aextreme%20blur%2C%20such%20as%20ExBluRF%2C%20as%20well%20as%20Gaussian%20Splatting-based%20approaches%0Alike%20Deblur-GS%2C%20typically%20assume%20access%20to%20sharp%20images%20for%20camera%20pose%0Aestimation%20and%20point%20cloud%20generation%2C%20an%20unrealistic%20assumption.%20Methods%0Arelying%20on%20COLMAP%20initialization%2C%20such%20as%20BAD-Gaussians%2C%20also%20fail%20due%20to%0Aunreliable%20feature%20correspondences%20under%20severe%20blur.%20To%20address%20these%0Achallenges%2C%20we%20propose%20GeMS%2C%20a%203DGS%20framework%20that%20reconstructs%20scenes%20directly%0Afrom%20extremely%20blurred%20images.%20GeMS%20integrates%3A%20%281%29%20VGGSfM%2C%20a%20deep%0Alearning-based%20Structure-from-Motion%20pipeline%20that%20estimates%20poses%20and%0Agenerates%20point%20clouds%20directly%20from%20blurred%20inputs%3B%20%282%29%203DGS-MCMC%2C%20which%0Aenables%20robust%20scene%20initialization%20by%20treating%20Gaussians%20as%20samples%20from%20a%0Aprobability%20distribution%2C%20eliminating%20heuristic%20densification%20and%20pruning%3B%20and%0A%283%29%20joint%20optimization%20of%20camera%20trajectories%20and%20Gaussian%20parameters%20for%0Astable%20reconstruction.%20While%20this%20pipeline%20produces%20strong%20results%2C%0Ainaccuracies%20may%20remain%20when%20all%20inputs%20are%20severely%20blurred.%20To%20mitigate%20this%2C%0Awe%20propose%20GeMS-E%2C%20which%20integrates%20a%20progressive%20refinement%20step%20using%20events%3A%0A%284%29%20Event-based%20Double%20Integral%20%28EDI%29%20deblurring%20restores%20sharper%20images%20that%0Aare%20then%20fed%20into%20GeMS%2C%20improving%20pose%20estimation%2C%20point%20cloud%20generation%2C%20and%0Aoverall%20reconstruction.%20Both%20GeMS%20and%20GeMS-E%20achieve%20state-of-the-art%0Aperformance%20on%20synthetic%20and%20real-world%20datasets.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20framework%20to%20address%20extreme%20motion%20blur%20within%203DGS%20directly%20from%0Aseverely%20blurred%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14682v1&entry.124074799=Read"},
{"title": "Identity Preserving 3D Head Stylization with Multiview Score\n  Distillation", "author": "Bahri Batuhan Bilecen and Ahmet Berke Gokmen and Furkan Guzelant and Aysegul Dundar", "abstract": "  3D head stylization transforms realistic facial features into artistic\nrepresentations, enhancing user engagement across gaming and virtual reality\napplications. While 3D-aware generators have made significant advancements,\nmany 3D stylization methods primarily provide near-frontal views and struggle\nto preserve the unique identities of original subjects, often resulting in\noutputs that lack diversity and individuality. This paper addresses these\nchallenges by leveraging the PanoHead model, synthesizing images from a\ncomprehensive 360-degree perspective. We propose a novel framework that employs\nnegative log-likelihood distillation (LD) to enhance identity preservation and\nimprove stylization quality. By integrating multi-view grid score and mirror\ngradients within the 3D GAN architecture and introducing a score rank weighing\ntechnique, our approach achieves substantial qualitative and quantitative\nimprovements. Our findings not only advance the state of 3D head stylization\nbut also provide valuable insights into effective distillation processes\nbetween diffusion models and GANs, focusing on the critical issue of identity\npreservation. Please visit the https://three-bee.github.io/head_stylization for\nmore visuals.\n", "link": "http://arxiv.org/abs/2411.13536v3", "date": "2025-08-20", "relevancy": 3.1833, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6407}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6407}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity%20Preserving%203D%20Head%20Stylization%20with%20Multiview%20Score%0A%20%20Distillation&body=Title%3A%20Identity%20Preserving%203D%20Head%20Stylization%20with%20Multiview%20Score%0A%20%20Distillation%0AAuthor%3A%20Bahri%20Batuhan%20Bilecen%20and%20Ahmet%20Berke%20Gokmen%20and%20Furkan%20Guzelant%20and%20Aysegul%20Dundar%0AAbstract%3A%20%20%203D%20head%20stylization%20transforms%20realistic%20facial%20features%20into%20artistic%0Arepresentations%2C%20enhancing%20user%20engagement%20across%20gaming%20and%20virtual%20reality%0Aapplications.%20While%203D-aware%20generators%20have%20made%20significant%20advancements%2C%0Amany%203D%20stylization%20methods%20primarily%20provide%20near-frontal%20views%20and%20struggle%0Ato%20preserve%20the%20unique%20identities%20of%20original%20subjects%2C%20often%20resulting%20in%0Aoutputs%20that%20lack%20diversity%20and%20individuality.%20This%20paper%20addresses%20these%0Achallenges%20by%20leveraging%20the%20PanoHead%20model%2C%20synthesizing%20images%20from%20a%0Acomprehensive%20360-degree%20perspective.%20We%20propose%20a%20novel%20framework%20that%20employs%0Anegative%20log-likelihood%20distillation%20%28LD%29%20to%20enhance%20identity%20preservation%20and%0Aimprove%20stylization%20quality.%20By%20integrating%20multi-view%20grid%20score%20and%20mirror%0Agradients%20within%20the%203D%20GAN%20architecture%20and%20introducing%20a%20score%20rank%20weighing%0Atechnique%2C%20our%20approach%20achieves%20substantial%20qualitative%20and%20quantitative%0Aimprovements.%20Our%20findings%20not%20only%20advance%20the%20state%20of%203D%20head%20stylization%0Abut%20also%20provide%20valuable%20insights%20into%20effective%20distillation%20processes%0Abetween%20diffusion%20models%20and%20GANs%2C%20focusing%20on%20the%20critical%20issue%20of%20identity%0Apreservation.%20Please%20visit%20the%20https%3A//three-bee.github.io/head_stylization%20for%0Amore%20visuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13536v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity%2520Preserving%25203D%2520Head%2520Stylization%2520with%2520Multiview%2520Score%250A%2520%2520Distillation%26entry.906535625%3DBahri%2520Batuhan%2520Bilecen%2520and%2520Ahmet%2520Berke%2520Gokmen%2520and%2520Furkan%2520Guzelant%2520and%2520Aysegul%2520Dundar%26entry.1292438233%3D%2520%25203D%2520head%2520stylization%2520transforms%2520realistic%2520facial%2520features%2520into%2520artistic%250Arepresentations%252C%2520enhancing%2520user%2520engagement%2520across%2520gaming%2520and%2520virtual%2520reality%250Aapplications.%2520While%25203D-aware%2520generators%2520have%2520made%2520significant%2520advancements%252C%250Amany%25203D%2520stylization%2520methods%2520primarily%2520provide%2520near-frontal%2520views%2520and%2520struggle%250Ato%2520preserve%2520the%2520unique%2520identities%2520of%2520original%2520subjects%252C%2520often%2520resulting%2520in%250Aoutputs%2520that%2520lack%2520diversity%2520and%2520individuality.%2520This%2520paper%2520addresses%2520these%250Achallenges%2520by%2520leveraging%2520the%2520PanoHead%2520model%252C%2520synthesizing%2520images%2520from%2520a%250Acomprehensive%2520360-degree%2520perspective.%2520We%2520propose%2520a%2520novel%2520framework%2520that%2520employs%250Anegative%2520log-likelihood%2520distillation%2520%2528LD%2529%2520to%2520enhance%2520identity%2520preservation%2520and%250Aimprove%2520stylization%2520quality.%2520By%2520integrating%2520multi-view%2520grid%2520score%2520and%2520mirror%250Agradients%2520within%2520the%25203D%2520GAN%2520architecture%2520and%2520introducing%2520a%2520score%2520rank%2520weighing%250Atechnique%252C%2520our%2520approach%2520achieves%2520substantial%2520qualitative%2520and%2520quantitative%250Aimprovements.%2520Our%2520findings%2520not%2520only%2520advance%2520the%2520state%2520of%25203D%2520head%2520stylization%250Abut%2520also%2520provide%2520valuable%2520insights%2520into%2520effective%2520distillation%2520processes%250Abetween%2520diffusion%2520models%2520and%2520GANs%252C%2520focusing%2520on%2520the%2520critical%2520issue%2520of%2520identity%250Apreservation.%2520Please%2520visit%2520the%2520https%253A//three-bee.github.io/head_stylization%2520for%250Amore%2520visuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13536v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity%20Preserving%203D%20Head%20Stylization%20with%20Multiview%20Score%0A%20%20Distillation&entry.906535625=Bahri%20Batuhan%20Bilecen%20and%20Ahmet%20Berke%20Gokmen%20and%20Furkan%20Guzelant%20and%20Aysegul%20Dundar&entry.1292438233=%20%203D%20head%20stylization%20transforms%20realistic%20facial%20features%20into%20artistic%0Arepresentations%2C%20enhancing%20user%20engagement%20across%20gaming%20and%20virtual%20reality%0Aapplications.%20While%203D-aware%20generators%20have%20made%20significant%20advancements%2C%0Amany%203D%20stylization%20methods%20primarily%20provide%20near-frontal%20views%20and%20struggle%0Ato%20preserve%20the%20unique%20identities%20of%20original%20subjects%2C%20often%20resulting%20in%0Aoutputs%20that%20lack%20diversity%20and%20individuality.%20This%20paper%20addresses%20these%0Achallenges%20by%20leveraging%20the%20PanoHead%20model%2C%20synthesizing%20images%20from%20a%0Acomprehensive%20360-degree%20perspective.%20We%20propose%20a%20novel%20framework%20that%20employs%0Anegative%20log-likelihood%20distillation%20%28LD%29%20to%20enhance%20identity%20preservation%20and%0Aimprove%20stylization%20quality.%20By%20integrating%20multi-view%20grid%20score%20and%20mirror%0Agradients%20within%20the%203D%20GAN%20architecture%20and%20introducing%20a%20score%20rank%20weighing%0Atechnique%2C%20our%20approach%20achieves%20substantial%20qualitative%20and%20quantitative%0Aimprovements.%20Our%20findings%20not%20only%20advance%20the%20state%20of%203D%20head%20stylization%0Abut%20also%20provide%20valuable%20insights%20into%20effective%20distillation%20processes%0Abetween%20diffusion%20models%20and%20GANs%2C%20focusing%20on%20the%20critical%20issue%20of%20identity%0Apreservation.%20Please%20visit%20the%20https%3A//three-bee.github.io/head_stylization%20for%0Amore%20visuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13536v3&entry.124074799=Read"},
{"title": "GaussianArt: Unified Modeling of Geometry and Motion for Articulated\n  Objects", "author": "Licheng Shen and Saining Zhang and Honghan Li and Peilin Yang and Zihao Huang and Zongzheng Zhang and Hao Zhao", "abstract": "  Reconstructing articulated objects is essential for building digital twins of\ninteractive environments. However, prior methods typically decouple geometry\nand motion by first reconstructing object shape in distinct states and then\nestimating articulation through post-hoc alignment. This separation complicates\nthe reconstruction pipeline and restricts scalability, especially for objects\nwith complex, multi-part articulation. We introduce a unified representation\nthat jointly models geometry and motion using articulated 3D Gaussians. This\nformulation improves robustness in motion decomposition and supports\narticulated objects with up to 20 parts, significantly outperforming prior\napproaches that often struggle beyond 2--3 parts due to brittle initialization.\nTo systematically assess scalability and generalization, we propose MPArt-90, a\nnew benchmark consisting of 90 articulated objects across 20 categories, each\nwith diverse part counts and motion configurations. Extensive experiments show\nthat our method consistently achieves superior accuracy in part-level geometry\nreconstruction and motion estimation across a broad range of object types. We\nfurther demonstrate applicability to downstream tasks such as robotic\nsimulation and human-scene interaction modeling, highlighting the potential of\nunified articulated representations in scalable physical modeling.\n", "link": "http://arxiv.org/abs/2508.14891v1", "date": "2025-08-20", "relevancy": 3.1476, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6806}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6121}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianArt%3A%20Unified%20Modeling%20of%20Geometry%20and%20Motion%20for%20Articulated%0A%20%20Objects&body=Title%3A%20GaussianArt%3A%20Unified%20Modeling%20of%20Geometry%20and%20Motion%20for%20Articulated%0A%20%20Objects%0AAuthor%3A%20Licheng%20Shen%20and%20Saining%20Zhang%20and%20Honghan%20Li%20and%20Peilin%20Yang%20and%20Zihao%20Huang%20and%20Zongzheng%20Zhang%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Reconstructing%20articulated%20objects%20is%20essential%20for%20building%20digital%20twins%20of%0Ainteractive%20environments.%20However%2C%20prior%20methods%20typically%20decouple%20geometry%0Aand%20motion%20by%20first%20reconstructing%20object%20shape%20in%20distinct%20states%20and%20then%0Aestimating%20articulation%20through%20post-hoc%20alignment.%20This%20separation%20complicates%0Athe%20reconstruction%20pipeline%20and%20restricts%20scalability%2C%20especially%20for%20objects%0Awith%20complex%2C%20multi-part%20articulation.%20We%20introduce%20a%20unified%20representation%0Athat%20jointly%20models%20geometry%20and%20motion%20using%20articulated%203D%20Gaussians.%20This%0Aformulation%20improves%20robustness%20in%20motion%20decomposition%20and%20supports%0Aarticulated%20objects%20with%20up%20to%2020%20parts%2C%20significantly%20outperforming%20prior%0Aapproaches%20that%20often%20struggle%20beyond%202--3%20parts%20due%20to%20brittle%20initialization.%0ATo%20systematically%20assess%20scalability%20and%20generalization%2C%20we%20propose%20MPArt-90%2C%20a%0Anew%20benchmark%20consisting%20of%2090%20articulated%20objects%20across%2020%20categories%2C%20each%0Awith%20diverse%20part%20counts%20and%20motion%20configurations.%20Extensive%20experiments%20show%0Athat%20our%20method%20consistently%20achieves%20superior%20accuracy%20in%20part-level%20geometry%0Areconstruction%20and%20motion%20estimation%20across%20a%20broad%20range%20of%20object%20types.%20We%0Afurther%20demonstrate%20applicability%20to%20downstream%20tasks%20such%20as%20robotic%0Asimulation%20and%20human-scene%20interaction%20modeling%2C%20highlighting%20the%20potential%20of%0Aunified%20articulated%20representations%20in%20scalable%20physical%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianArt%253A%2520Unified%2520Modeling%2520of%2520Geometry%2520and%2520Motion%2520for%2520Articulated%250A%2520%2520Objects%26entry.906535625%3DLicheng%2520Shen%2520and%2520Saining%2520Zhang%2520and%2520Honghan%2520Li%2520and%2520Peilin%2520Yang%2520and%2520Zihao%2520Huang%2520and%2520Zongzheng%2520Zhang%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Reconstructing%2520articulated%2520objects%2520is%2520essential%2520for%2520building%2520digital%2520twins%2520of%250Ainteractive%2520environments.%2520However%252C%2520prior%2520methods%2520typically%2520decouple%2520geometry%250Aand%2520motion%2520by%2520first%2520reconstructing%2520object%2520shape%2520in%2520distinct%2520states%2520and%2520then%250Aestimating%2520articulation%2520through%2520post-hoc%2520alignment.%2520This%2520separation%2520complicates%250Athe%2520reconstruction%2520pipeline%2520and%2520restricts%2520scalability%252C%2520especially%2520for%2520objects%250Awith%2520complex%252C%2520multi-part%2520articulation.%2520We%2520introduce%2520a%2520unified%2520representation%250Athat%2520jointly%2520models%2520geometry%2520and%2520motion%2520using%2520articulated%25203D%2520Gaussians.%2520This%250Aformulation%2520improves%2520robustness%2520in%2520motion%2520decomposition%2520and%2520supports%250Aarticulated%2520objects%2520with%2520up%2520to%252020%2520parts%252C%2520significantly%2520outperforming%2520prior%250Aapproaches%2520that%2520often%2520struggle%2520beyond%25202--3%2520parts%2520due%2520to%2520brittle%2520initialization.%250ATo%2520systematically%2520assess%2520scalability%2520and%2520generalization%252C%2520we%2520propose%2520MPArt-90%252C%2520a%250Anew%2520benchmark%2520consisting%2520of%252090%2520articulated%2520objects%2520across%252020%2520categories%252C%2520each%250Awith%2520diverse%2520part%2520counts%2520and%2520motion%2520configurations.%2520Extensive%2520experiments%2520show%250Athat%2520our%2520method%2520consistently%2520achieves%2520superior%2520accuracy%2520in%2520part-level%2520geometry%250Areconstruction%2520and%2520motion%2520estimation%2520across%2520a%2520broad%2520range%2520of%2520object%2520types.%2520We%250Afurther%2520demonstrate%2520applicability%2520to%2520downstream%2520tasks%2520such%2520as%2520robotic%250Asimulation%2520and%2520human-scene%2520interaction%2520modeling%252C%2520highlighting%2520the%2520potential%2520of%250Aunified%2520articulated%2520representations%2520in%2520scalable%2520physical%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianArt%3A%20Unified%20Modeling%20of%20Geometry%20and%20Motion%20for%20Articulated%0A%20%20Objects&entry.906535625=Licheng%20Shen%20and%20Saining%20Zhang%20and%20Honghan%20Li%20and%20Peilin%20Yang%20and%20Zihao%20Huang%20and%20Zongzheng%20Zhang%20and%20Hao%20Zhao&entry.1292438233=%20%20Reconstructing%20articulated%20objects%20is%20essential%20for%20building%20digital%20twins%20of%0Ainteractive%20environments.%20However%2C%20prior%20methods%20typically%20decouple%20geometry%0Aand%20motion%20by%20first%20reconstructing%20object%20shape%20in%20distinct%20states%20and%20then%0Aestimating%20articulation%20through%20post-hoc%20alignment.%20This%20separation%20complicates%0Athe%20reconstruction%20pipeline%20and%20restricts%20scalability%2C%20especially%20for%20objects%0Awith%20complex%2C%20multi-part%20articulation.%20We%20introduce%20a%20unified%20representation%0Athat%20jointly%20models%20geometry%20and%20motion%20using%20articulated%203D%20Gaussians.%20This%0Aformulation%20improves%20robustness%20in%20motion%20decomposition%20and%20supports%0Aarticulated%20objects%20with%20up%20to%2020%20parts%2C%20significantly%20outperforming%20prior%0Aapproaches%20that%20often%20struggle%20beyond%202--3%20parts%20due%20to%20brittle%20initialization.%0ATo%20systematically%20assess%20scalability%20and%20generalization%2C%20we%20propose%20MPArt-90%2C%20a%0Anew%20benchmark%20consisting%20of%2090%20articulated%20objects%20across%2020%20categories%2C%20each%0Awith%20diverse%20part%20counts%20and%20motion%20configurations.%20Extensive%20experiments%20show%0Athat%20our%20method%20consistently%20achieves%20superior%20accuracy%20in%20part-level%20geometry%0Areconstruction%20and%20motion%20estimation%20across%20a%20broad%20range%20of%20object%20types.%20We%0Afurther%20demonstrate%20applicability%20to%20downstream%20tasks%20such%20as%20robotic%0Asimulation%20and%20human-scene%20interaction%20modeling%2C%20highlighting%20the%20potential%20of%0Aunified%20articulated%20representations%20in%20scalable%20physical%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14891v1&entry.124074799=Read"},
{"title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness", "author": "Dian Zheng and Ziqi Huang and Hongbo Liu and Kai Zou and Yinan He and Fan Zhang and Lulu Gu and Yuanhan Zhang and Jingwen He and Wei-Shi Zheng and Yu Qiao and Ziwei Liu", "abstract": "  Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nto individual dimensions, our evaluation framework integrates generalists such\nas SOTA VLMs and LLMs, and specialists, including anomaly detection methods\nproposed for video generation. We conduct extensive human annotations to ensure\nevaluation alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.\n", "link": "http://arxiv.org/abs/2503.21755v2", "date": "2025-08-20", "relevancy": 3.025, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6129}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6018}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VBench-2.0%3A%20Advancing%20Video%20Generation%20Benchmark%20Suite%20for%20Intrinsic%0A%20%20Faithfulness&body=Title%3A%20VBench-2.0%3A%20Advancing%20Video%20Generation%20Benchmark%20Suite%20for%20Intrinsic%0A%20%20Faithfulness%0AAuthor%3A%20Dian%20Zheng%20and%20Ziqi%20Huang%20and%20Hongbo%20Liu%20and%20Kai%20Zou%20and%20Yinan%20He%20and%20Fan%20Zhang%20and%20Lulu%20Gu%20and%20Yuanhan%20Zhang%20and%20Jingwen%20He%20and%20Wei-Shi%20Zheng%20and%20Yu%20Qiao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Video%20generation%20has%20advanced%20significantly%2C%20evolving%20from%20producing%0Aunrealistic%20outputs%20to%20generating%20videos%20that%20appear%20visually%20convincing%20and%0Atemporally%20coherent.%20To%20evaluate%20these%20video%20generative%20models%2C%20benchmarks%20such%0Aas%20VBench%20have%20been%20developed%20to%20assess%20their%20faithfulness%2C%20measuring%20factors%0Alike%20per-frame%20aesthetics%2C%20temporal%20consistency%2C%20and%20basic%20prompt%20adherence.%0AHowever%2C%20these%20aspects%20mainly%20represent%20superficial%20faithfulness%2C%20which%20focus%0Aon%20whether%20the%20video%20appears%20visually%20convincing%20rather%20than%20whether%20it%20adheres%0Ato%20real-world%20principles.%20While%20recent%20models%20perform%20increasingly%20well%20on%0Athese%20metrics%2C%20they%20still%20struggle%20to%20generate%20videos%20that%20are%20not%20just%0Avisually%20plausible%20but%20fundamentally%20realistic.%20To%20achieve%20real%20%22world%20models%22%0Athrough%20video%20generation%2C%20the%20next%20frontier%20lies%20in%20intrinsic%20faithfulness%20to%0Aensure%20that%20generated%20videos%20adhere%20to%20physical%20laws%2C%20commonsense%20reasoning%2C%0Aanatomical%20correctness%2C%20and%20compositional%20integrity.%20Achieving%20this%20level%20of%0Arealism%20is%20essential%20for%20applications%20such%20as%20AI-assisted%20filmmaking%20and%0Asimulated%20world%20modeling.%20To%20bridge%20this%20gap%2C%20we%20introduce%20VBench-2.0%2C%20a%0Anext-generation%20benchmark%20designed%20to%20automatically%20evaluate%20video%20generative%0Amodels%20for%20their%20intrinsic%20faithfulness.%20VBench-2.0%20assesses%20five%20key%0Adimensions%3A%20Human%20Fidelity%2C%20Controllability%2C%20Creativity%2C%20Physics%2C%20and%0ACommonsense%2C%20each%20further%20broken%20down%20into%20fine-grained%20capabilities.%20Tailored%0Ato%20individual%20dimensions%2C%20our%20evaluation%20framework%20integrates%20generalists%20such%0Aas%20SOTA%20VLMs%20and%20LLMs%2C%20and%20specialists%2C%20including%20anomaly%20detection%20methods%0Aproposed%20for%20video%20generation.%20We%20conduct%20extensive%20human%20annotations%20to%20ensure%0Aevaluation%20alignment%20with%20human%20judgment.%20By%20pushing%20beyond%20superficial%0Afaithfulness%20toward%20intrinsic%20faithfulness%2C%20VBench-2.0%20aims%20to%20set%20a%20new%0Astandard%20for%20the%20next%20generation%20of%20video%20generative%20models%20in%20pursuit%20of%0Aintrinsic%20faithfulness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVBench-2.0%253A%2520Advancing%2520Video%2520Generation%2520Benchmark%2520Suite%2520for%2520Intrinsic%250A%2520%2520Faithfulness%26entry.906535625%3DDian%2520Zheng%2520and%2520Ziqi%2520Huang%2520and%2520Hongbo%2520Liu%2520and%2520Kai%2520Zou%2520and%2520Yinan%2520He%2520and%2520Fan%2520Zhang%2520and%2520Lulu%2520Gu%2520and%2520Yuanhan%2520Zhang%2520and%2520Jingwen%2520He%2520and%2520Wei-Shi%2520Zheng%2520and%2520Yu%2520Qiao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Video%2520generation%2520has%2520advanced%2520significantly%252C%2520evolving%2520from%2520producing%250Aunrealistic%2520outputs%2520to%2520generating%2520videos%2520that%2520appear%2520visually%2520convincing%2520and%250Atemporally%2520coherent.%2520To%2520evaluate%2520these%2520video%2520generative%2520models%252C%2520benchmarks%2520such%250Aas%2520VBench%2520have%2520been%2520developed%2520to%2520assess%2520their%2520faithfulness%252C%2520measuring%2520factors%250Alike%2520per-frame%2520aesthetics%252C%2520temporal%2520consistency%252C%2520and%2520basic%2520prompt%2520adherence.%250AHowever%252C%2520these%2520aspects%2520mainly%2520represent%2520superficial%2520faithfulness%252C%2520which%2520focus%250Aon%2520whether%2520the%2520video%2520appears%2520visually%2520convincing%2520rather%2520than%2520whether%2520it%2520adheres%250Ato%2520real-world%2520principles.%2520While%2520recent%2520models%2520perform%2520increasingly%2520well%2520on%250Athese%2520metrics%252C%2520they%2520still%2520struggle%2520to%2520generate%2520videos%2520that%2520are%2520not%2520just%250Avisually%2520plausible%2520but%2520fundamentally%2520realistic.%2520To%2520achieve%2520real%2520%2522world%2520models%2522%250Athrough%2520video%2520generation%252C%2520the%2520next%2520frontier%2520lies%2520in%2520intrinsic%2520faithfulness%2520to%250Aensure%2520that%2520generated%2520videos%2520adhere%2520to%2520physical%2520laws%252C%2520commonsense%2520reasoning%252C%250Aanatomical%2520correctness%252C%2520and%2520compositional%2520integrity.%2520Achieving%2520this%2520level%2520of%250Arealism%2520is%2520essential%2520for%2520applications%2520such%2520as%2520AI-assisted%2520filmmaking%2520and%250Asimulated%2520world%2520modeling.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520VBench-2.0%252C%2520a%250Anext-generation%2520benchmark%2520designed%2520to%2520automatically%2520evaluate%2520video%2520generative%250Amodels%2520for%2520their%2520intrinsic%2520faithfulness.%2520VBench-2.0%2520assesses%2520five%2520key%250Adimensions%253A%2520Human%2520Fidelity%252C%2520Controllability%252C%2520Creativity%252C%2520Physics%252C%2520and%250ACommonsense%252C%2520each%2520further%2520broken%2520down%2520into%2520fine-grained%2520capabilities.%2520Tailored%250Ato%2520individual%2520dimensions%252C%2520our%2520evaluation%2520framework%2520integrates%2520generalists%2520such%250Aas%2520SOTA%2520VLMs%2520and%2520LLMs%252C%2520and%2520specialists%252C%2520including%2520anomaly%2520detection%2520methods%250Aproposed%2520for%2520video%2520generation.%2520We%2520conduct%2520extensive%2520human%2520annotations%2520to%2520ensure%250Aevaluation%2520alignment%2520with%2520human%2520judgment.%2520By%2520pushing%2520beyond%2520superficial%250Afaithfulness%2520toward%2520intrinsic%2520faithfulness%252C%2520VBench-2.0%2520aims%2520to%2520set%2520a%2520new%250Astandard%2520for%2520the%2520next%2520generation%2520of%2520video%2520generative%2520models%2520in%2520pursuit%2520of%250Aintrinsic%2520faithfulness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VBench-2.0%3A%20Advancing%20Video%20Generation%20Benchmark%20Suite%20for%20Intrinsic%0A%20%20Faithfulness&entry.906535625=Dian%20Zheng%20and%20Ziqi%20Huang%20and%20Hongbo%20Liu%20and%20Kai%20Zou%20and%20Yinan%20He%20and%20Fan%20Zhang%20and%20Lulu%20Gu%20and%20Yuanhan%20Zhang%20and%20Jingwen%20He%20and%20Wei-Shi%20Zheng%20and%20Yu%20Qiao%20and%20Ziwei%20Liu&entry.1292438233=%20%20Video%20generation%20has%20advanced%20significantly%2C%20evolving%20from%20producing%0Aunrealistic%20outputs%20to%20generating%20videos%20that%20appear%20visually%20convincing%20and%0Atemporally%20coherent.%20To%20evaluate%20these%20video%20generative%20models%2C%20benchmarks%20such%0Aas%20VBench%20have%20been%20developed%20to%20assess%20their%20faithfulness%2C%20measuring%20factors%0Alike%20per-frame%20aesthetics%2C%20temporal%20consistency%2C%20and%20basic%20prompt%20adherence.%0AHowever%2C%20these%20aspects%20mainly%20represent%20superficial%20faithfulness%2C%20which%20focus%0Aon%20whether%20the%20video%20appears%20visually%20convincing%20rather%20than%20whether%20it%20adheres%0Ato%20real-world%20principles.%20While%20recent%20models%20perform%20increasingly%20well%20on%0Athese%20metrics%2C%20they%20still%20struggle%20to%20generate%20videos%20that%20are%20not%20just%0Avisually%20plausible%20but%20fundamentally%20realistic.%20To%20achieve%20real%20%22world%20models%22%0Athrough%20video%20generation%2C%20the%20next%20frontier%20lies%20in%20intrinsic%20faithfulness%20to%0Aensure%20that%20generated%20videos%20adhere%20to%20physical%20laws%2C%20commonsense%20reasoning%2C%0Aanatomical%20correctness%2C%20and%20compositional%20integrity.%20Achieving%20this%20level%20of%0Arealism%20is%20essential%20for%20applications%20such%20as%20AI-assisted%20filmmaking%20and%0Asimulated%20world%20modeling.%20To%20bridge%20this%20gap%2C%20we%20introduce%20VBench-2.0%2C%20a%0Anext-generation%20benchmark%20designed%20to%20automatically%20evaluate%20video%20generative%0Amodels%20for%20their%20intrinsic%20faithfulness.%20VBench-2.0%20assesses%20five%20key%0Adimensions%3A%20Human%20Fidelity%2C%20Controllability%2C%20Creativity%2C%20Physics%2C%20and%0ACommonsense%2C%20each%20further%20broken%20down%20into%20fine-grained%20capabilities.%20Tailored%0Ato%20individual%20dimensions%2C%20our%20evaluation%20framework%20integrates%20generalists%20such%0Aas%20SOTA%20VLMs%20and%20LLMs%2C%20and%20specialists%2C%20including%20anomaly%20detection%20methods%0Aproposed%20for%20video%20generation.%20We%20conduct%20extensive%20human%20annotations%20to%20ensure%0Aevaluation%20alignment%20with%20human%20judgment.%20By%20pushing%20beyond%20superficial%0Afaithfulness%20toward%20intrinsic%20faithfulness%2C%20VBench-2.0%20aims%20to%20set%20a%20new%0Astandard%20for%20the%20next%20generation%20of%20video%20generative%20models%20in%20pursuit%20of%0Aintrinsic%20faithfulness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21755v2&entry.124074799=Read"},
{"title": "Endo-FASt3r: Endoscopic Foundation model Adaptation for Structure from\n  motion", "author": "Mona Sheikh Zeinoddin and Mobarak I. Hoque and Zafer Tandogdu and Greg Shaw and Matthew J. Clarkson and Evangelos Mazomenos and Danail Stoyanov", "abstract": "  Accurate depth and camera pose estimation is essential for achieving\nhigh-quality 3D visualisations in robotic-assisted surgery. Despite recent\nadvancements in foundation model adaptation to monocular depth estimation of\nendoscopic scenes via self-supervised learning (SSL), no prior work has\nexplored their use for pose estimation. These methods rely on low rank-based\nadaptation approaches, which constrain model updates to a low-rank space. We\npropose Endo-FASt3r, the first monocular SSL depth and pose estimation\nframework that uses foundation models for both tasks. We extend the Reloc3r\nrelative pose estimation foundation model by designing Reloc3rX, introducing\nmodifications necessary for convergence in SSL. We also present DoMoRA, a novel\nadaptation technique that enables higher-rank updates and faster convergence.\nExperiments on the SCARED dataset show that Endo-FASt3r achieves a substantial\n$10\\%$ improvement in pose estimation and a $2\\%$ improvement in depth\nestimation over prior work. Similar performance gains on the Hamlyn and\nStereoMIS datasets reinforce the generalisability of Endo-FASt3r across\ndifferent datasets.\n", "link": "http://arxiv.org/abs/2503.07204v4", "date": "2025-08-20", "relevancy": 2.9243, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Endo-FASt3r%3A%20Endoscopic%20Foundation%20model%20Adaptation%20for%20Structure%20from%0A%20%20motion&body=Title%3A%20Endo-FASt3r%3A%20Endoscopic%20Foundation%20model%20Adaptation%20for%20Structure%20from%0A%20%20motion%0AAuthor%3A%20Mona%20Sheikh%20Zeinoddin%20and%20Mobarak%20I.%20Hoque%20and%20Zafer%20Tandogdu%20and%20Greg%20Shaw%20and%20Matthew%20J.%20Clarkson%20and%20Evangelos%20Mazomenos%20and%20Danail%20Stoyanov%0AAbstract%3A%20%20%20Accurate%20depth%20and%20camera%20pose%20estimation%20is%20essential%20for%20achieving%0Ahigh-quality%203D%20visualisations%20in%20robotic-assisted%20surgery.%20Despite%20recent%0Aadvancements%20in%20foundation%20model%20adaptation%20to%20monocular%20depth%20estimation%20of%0Aendoscopic%20scenes%20via%20self-supervised%20learning%20%28SSL%29%2C%20no%20prior%20work%20has%0Aexplored%20their%20use%20for%20pose%20estimation.%20These%20methods%20rely%20on%20low%20rank-based%0Aadaptation%20approaches%2C%20which%20constrain%20model%20updates%20to%20a%20low-rank%20space.%20We%0Apropose%20Endo-FASt3r%2C%20the%20first%20monocular%20SSL%20depth%20and%20pose%20estimation%0Aframework%20that%20uses%20foundation%20models%20for%20both%20tasks.%20We%20extend%20the%20Reloc3r%0Arelative%20pose%20estimation%20foundation%20model%20by%20designing%20Reloc3rX%2C%20introducing%0Amodifications%20necessary%20for%20convergence%20in%20SSL.%20We%20also%20present%20DoMoRA%2C%20a%20novel%0Aadaptation%20technique%20that%20enables%20higher-rank%20updates%20and%20faster%20convergence.%0AExperiments%20on%20the%20SCARED%20dataset%20show%20that%20Endo-FASt3r%20achieves%20a%20substantial%0A%2410%5C%25%24%20improvement%20in%20pose%20estimation%20and%20a%20%242%5C%25%24%20improvement%20in%20depth%0Aestimation%20over%20prior%20work.%20Similar%20performance%20gains%20on%20the%20Hamlyn%20and%0AStereoMIS%20datasets%20reinforce%20the%20generalisability%20of%20Endo-FASt3r%20across%0Adifferent%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07204v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndo-FASt3r%253A%2520Endoscopic%2520Foundation%2520model%2520Adaptation%2520for%2520Structure%2520from%250A%2520%2520motion%26entry.906535625%3DMona%2520Sheikh%2520Zeinoddin%2520and%2520Mobarak%2520I.%2520Hoque%2520and%2520Zafer%2520Tandogdu%2520and%2520Greg%2520Shaw%2520and%2520Matthew%2520J.%2520Clarkson%2520and%2520Evangelos%2520Mazomenos%2520and%2520Danail%2520Stoyanov%26entry.1292438233%3D%2520%2520Accurate%2520depth%2520and%2520camera%2520pose%2520estimation%2520is%2520essential%2520for%2520achieving%250Ahigh-quality%25203D%2520visualisations%2520in%2520robotic-assisted%2520surgery.%2520Despite%2520recent%250Aadvancements%2520in%2520foundation%2520model%2520adaptation%2520to%2520monocular%2520depth%2520estimation%2520of%250Aendoscopic%2520scenes%2520via%2520self-supervised%2520learning%2520%2528SSL%2529%252C%2520no%2520prior%2520work%2520has%250Aexplored%2520their%2520use%2520for%2520pose%2520estimation.%2520These%2520methods%2520rely%2520on%2520low%2520rank-based%250Aadaptation%2520approaches%252C%2520which%2520constrain%2520model%2520updates%2520to%2520a%2520low-rank%2520space.%2520We%250Apropose%2520Endo-FASt3r%252C%2520the%2520first%2520monocular%2520SSL%2520depth%2520and%2520pose%2520estimation%250Aframework%2520that%2520uses%2520foundation%2520models%2520for%2520both%2520tasks.%2520We%2520extend%2520the%2520Reloc3r%250Arelative%2520pose%2520estimation%2520foundation%2520model%2520by%2520designing%2520Reloc3rX%252C%2520introducing%250Amodifications%2520necessary%2520for%2520convergence%2520in%2520SSL.%2520We%2520also%2520present%2520DoMoRA%252C%2520a%2520novel%250Aadaptation%2520technique%2520that%2520enables%2520higher-rank%2520updates%2520and%2520faster%2520convergence.%250AExperiments%2520on%2520the%2520SCARED%2520dataset%2520show%2520that%2520Endo-FASt3r%2520achieves%2520a%2520substantial%250A%252410%255C%2525%2524%2520improvement%2520in%2520pose%2520estimation%2520and%2520a%2520%25242%255C%2525%2524%2520improvement%2520in%2520depth%250Aestimation%2520over%2520prior%2520work.%2520Similar%2520performance%2520gains%2520on%2520the%2520Hamlyn%2520and%250AStereoMIS%2520datasets%2520reinforce%2520the%2520generalisability%2520of%2520Endo-FASt3r%2520across%250Adifferent%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07204v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Endo-FASt3r%3A%20Endoscopic%20Foundation%20model%20Adaptation%20for%20Structure%20from%0A%20%20motion&entry.906535625=Mona%20Sheikh%20Zeinoddin%20and%20Mobarak%20I.%20Hoque%20and%20Zafer%20Tandogdu%20and%20Greg%20Shaw%20and%20Matthew%20J.%20Clarkson%20and%20Evangelos%20Mazomenos%20and%20Danail%20Stoyanov&entry.1292438233=%20%20Accurate%20depth%20and%20camera%20pose%20estimation%20is%20essential%20for%20achieving%0Ahigh-quality%203D%20visualisations%20in%20robotic-assisted%20surgery.%20Despite%20recent%0Aadvancements%20in%20foundation%20model%20adaptation%20to%20monocular%20depth%20estimation%20of%0Aendoscopic%20scenes%20via%20self-supervised%20learning%20%28SSL%29%2C%20no%20prior%20work%20has%0Aexplored%20their%20use%20for%20pose%20estimation.%20These%20methods%20rely%20on%20low%20rank-based%0Aadaptation%20approaches%2C%20which%20constrain%20model%20updates%20to%20a%20low-rank%20space.%20We%0Apropose%20Endo-FASt3r%2C%20the%20first%20monocular%20SSL%20depth%20and%20pose%20estimation%0Aframework%20that%20uses%20foundation%20models%20for%20both%20tasks.%20We%20extend%20the%20Reloc3r%0Arelative%20pose%20estimation%20foundation%20model%20by%20designing%20Reloc3rX%2C%20introducing%0Amodifications%20necessary%20for%20convergence%20in%20SSL.%20We%20also%20present%20DoMoRA%2C%20a%20novel%0Aadaptation%20technique%20that%20enables%20higher-rank%20updates%20and%20faster%20convergence.%0AExperiments%20on%20the%20SCARED%20dataset%20show%20that%20Endo-FASt3r%20achieves%20a%20substantial%0A%2410%5C%25%24%20improvement%20in%20pose%20estimation%20and%20a%20%242%5C%25%24%20improvement%20in%20depth%0Aestimation%20over%20prior%20work.%20Similar%20performance%20gains%20on%20the%20Hamlyn%20and%0AStereoMIS%20datasets%20reinforce%20the%20generalisability%20of%20Endo-FASt3r%20across%0Adifferent%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07204v4&entry.124074799=Read"},
{"title": "Superpixel-informed Continuous Low-Rank Tensor Representation for\n  Multi-Dimensional Data Recovery", "author": "Zhizhou Wang and Jianli Wang and Ruijing Zheng and Zhenyu Wu", "abstract": "  Low-rank tensor representation (LRTR) has emerged as a powerful tool for\nmulti-dimensional data processing. However, classical LRTR-based methods face\ntwo critical limitations: (1) they typically assume that the holistic data is\nlow-rank, this assumption is often violated in real-world scenarios with\nsignificant spatial variations; and (2) they are constrained to discrete\nmeshgrid data, limiting their flexibility and applicability. To overcome these\nlimitations, we propose a Superpixel-informed Continuous low-rank Tensor\nRepresentation (SCTR) framework, which enables continuous and flexible modeling\nof multi-dimensional data beyond traditional grid-based constraints. Our\napproach introduces two main innovations: First, motivated by the observation\nthat semantically coherent regions exhibit stronger low-rank characteristics\nthan holistic data, we employ superpixels as the basic modeling units. This\ndesign not only encodes rich semantic information, but also enhances\nadaptability to diverse forms of data streams. Second, we propose a novel\nasymmetric low-rank tensor factorization (ALTF) where superpixel-specific\nfactor matrices are parameterized by a shared neural network with specialized\nheads. By strategically separating global pattern learning from local\nadaptation, this framework efficiently captures both cross-superpixel\ncommonalities and within-superpixel variations. This yields a representation\nthat is both highly expressive and compact, balancing model efficiency with\nadaptability. Extensive experiments on several benchmark datasets demonstrate\nthat SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods\nacross multispectral images, videos, and color images.\n", "link": "http://arxiv.org/abs/2508.12261v2", "date": "2025-08-20", "relevancy": 2.8085, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5982}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5494}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superpixel-informed%20Continuous%20Low-Rank%20Tensor%20Representation%20for%0A%20%20Multi-Dimensional%20Data%20Recovery&body=Title%3A%20Superpixel-informed%20Continuous%20Low-Rank%20Tensor%20Representation%20for%0A%20%20Multi-Dimensional%20Data%20Recovery%0AAuthor%3A%20Zhizhou%20Wang%20and%20Jianli%20Wang%20and%20Ruijing%20Zheng%20and%20Zhenyu%20Wu%0AAbstract%3A%20%20%20Low-rank%20tensor%20representation%20%28LRTR%29%20has%20emerged%20as%20a%20powerful%20tool%20for%0Amulti-dimensional%20data%20processing.%20However%2C%20classical%20LRTR-based%20methods%20face%0Atwo%20critical%20limitations%3A%20%281%29%20they%20typically%20assume%20that%20the%20holistic%20data%20is%0Alow-rank%2C%20this%20assumption%20is%20often%20violated%20in%20real-world%20scenarios%20with%0Asignificant%20spatial%20variations%3B%20and%20%282%29%20they%20are%20constrained%20to%20discrete%0Ameshgrid%20data%2C%20limiting%20their%20flexibility%20and%20applicability.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20Superpixel-informed%20Continuous%20low-rank%20Tensor%0ARepresentation%20%28SCTR%29%20framework%2C%20which%20enables%20continuous%20and%20flexible%20modeling%0Aof%20multi-dimensional%20data%20beyond%20traditional%20grid-based%20constraints.%20Our%0Aapproach%20introduces%20two%20main%20innovations%3A%20First%2C%20motivated%20by%20the%20observation%0Athat%20semantically%20coherent%20regions%20exhibit%20stronger%20low-rank%20characteristics%0Athan%20holistic%20data%2C%20we%20employ%20superpixels%20as%20the%20basic%20modeling%20units.%20This%0Adesign%20not%20only%20encodes%20rich%20semantic%20information%2C%20but%20also%20enhances%0Aadaptability%20to%20diverse%20forms%20of%20data%20streams.%20Second%2C%20we%20propose%20a%20novel%0Aasymmetric%20low-rank%20tensor%20factorization%20%28ALTF%29%20where%20superpixel-specific%0Afactor%20matrices%20are%20parameterized%20by%20a%20shared%20neural%20network%20with%20specialized%0Aheads.%20By%20strategically%20separating%20global%20pattern%20learning%20from%20local%0Aadaptation%2C%20this%20framework%20efficiently%20captures%20both%20cross-superpixel%0Acommonalities%20and%20within-superpixel%20variations.%20This%20yields%20a%20representation%0Athat%20is%20both%20highly%20expressive%20and%20compact%2C%20balancing%20model%20efficiency%20with%0Aadaptability.%20Extensive%20experiments%20on%20several%20benchmark%20datasets%20demonstrate%0Athat%20SCTR%20achieves%203-5%20dB%20PSNR%20improvements%20over%20existing%20LRTR-based%20methods%0Aacross%20multispectral%20images%2C%20videos%2C%20and%20color%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12261v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperpixel-informed%2520Continuous%2520Low-Rank%2520Tensor%2520Representation%2520for%250A%2520%2520Multi-Dimensional%2520Data%2520Recovery%26entry.906535625%3DZhizhou%2520Wang%2520and%2520Jianli%2520Wang%2520and%2520Ruijing%2520Zheng%2520and%2520Zhenyu%2520Wu%26entry.1292438233%3D%2520%2520Low-rank%2520tensor%2520representation%2520%2528LRTR%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%250Amulti-dimensional%2520data%2520processing.%2520However%252C%2520classical%2520LRTR-based%2520methods%2520face%250Atwo%2520critical%2520limitations%253A%2520%25281%2529%2520they%2520typically%2520assume%2520that%2520the%2520holistic%2520data%2520is%250Alow-rank%252C%2520this%2520assumption%2520is%2520often%2520violated%2520in%2520real-world%2520scenarios%2520with%250Asignificant%2520spatial%2520variations%253B%2520and%2520%25282%2529%2520they%2520are%2520constrained%2520to%2520discrete%250Ameshgrid%2520data%252C%2520limiting%2520their%2520flexibility%2520and%2520applicability.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520Superpixel-informed%2520Continuous%2520low-rank%2520Tensor%250ARepresentation%2520%2528SCTR%2529%2520framework%252C%2520which%2520enables%2520continuous%2520and%2520flexible%2520modeling%250Aof%2520multi-dimensional%2520data%2520beyond%2520traditional%2520grid-based%2520constraints.%2520Our%250Aapproach%2520introduces%2520two%2520main%2520innovations%253A%2520First%252C%2520motivated%2520by%2520the%2520observation%250Athat%2520semantically%2520coherent%2520regions%2520exhibit%2520stronger%2520low-rank%2520characteristics%250Athan%2520holistic%2520data%252C%2520we%2520employ%2520superpixels%2520as%2520the%2520basic%2520modeling%2520units.%2520This%250Adesign%2520not%2520only%2520encodes%2520rich%2520semantic%2520information%252C%2520but%2520also%2520enhances%250Aadaptability%2520to%2520diverse%2520forms%2520of%2520data%2520streams.%2520Second%252C%2520we%2520propose%2520a%2520novel%250Aasymmetric%2520low-rank%2520tensor%2520factorization%2520%2528ALTF%2529%2520where%2520superpixel-specific%250Afactor%2520matrices%2520are%2520parameterized%2520by%2520a%2520shared%2520neural%2520network%2520with%2520specialized%250Aheads.%2520By%2520strategically%2520separating%2520global%2520pattern%2520learning%2520from%2520local%250Aadaptation%252C%2520this%2520framework%2520efficiently%2520captures%2520both%2520cross-superpixel%250Acommonalities%2520and%2520within-superpixel%2520variations.%2520This%2520yields%2520a%2520representation%250Athat%2520is%2520both%2520highly%2520expressive%2520and%2520compact%252C%2520balancing%2520model%2520efficiency%2520with%250Aadaptability.%2520Extensive%2520experiments%2520on%2520several%2520benchmark%2520datasets%2520demonstrate%250Athat%2520SCTR%2520achieves%25203-5%2520dB%2520PSNR%2520improvements%2520over%2520existing%2520LRTR-based%2520methods%250Aacross%2520multispectral%2520images%252C%2520videos%252C%2520and%2520color%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12261v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superpixel-informed%20Continuous%20Low-Rank%20Tensor%20Representation%20for%0A%20%20Multi-Dimensional%20Data%20Recovery&entry.906535625=Zhizhou%20Wang%20and%20Jianli%20Wang%20and%20Ruijing%20Zheng%20and%20Zhenyu%20Wu&entry.1292438233=%20%20Low-rank%20tensor%20representation%20%28LRTR%29%20has%20emerged%20as%20a%20powerful%20tool%20for%0Amulti-dimensional%20data%20processing.%20However%2C%20classical%20LRTR-based%20methods%20face%0Atwo%20critical%20limitations%3A%20%281%29%20they%20typically%20assume%20that%20the%20holistic%20data%20is%0Alow-rank%2C%20this%20assumption%20is%20often%20violated%20in%20real-world%20scenarios%20with%0Asignificant%20spatial%20variations%3B%20and%20%282%29%20they%20are%20constrained%20to%20discrete%0Ameshgrid%20data%2C%20limiting%20their%20flexibility%20and%20applicability.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20Superpixel-informed%20Continuous%20low-rank%20Tensor%0ARepresentation%20%28SCTR%29%20framework%2C%20which%20enables%20continuous%20and%20flexible%20modeling%0Aof%20multi-dimensional%20data%20beyond%20traditional%20grid-based%20constraints.%20Our%0Aapproach%20introduces%20two%20main%20innovations%3A%20First%2C%20motivated%20by%20the%20observation%0Athat%20semantically%20coherent%20regions%20exhibit%20stronger%20low-rank%20characteristics%0Athan%20holistic%20data%2C%20we%20employ%20superpixels%20as%20the%20basic%20modeling%20units.%20This%0Adesign%20not%20only%20encodes%20rich%20semantic%20information%2C%20but%20also%20enhances%0Aadaptability%20to%20diverse%20forms%20of%20data%20streams.%20Second%2C%20we%20propose%20a%20novel%0Aasymmetric%20low-rank%20tensor%20factorization%20%28ALTF%29%20where%20superpixel-specific%0Afactor%20matrices%20are%20parameterized%20by%20a%20shared%20neural%20network%20with%20specialized%0Aheads.%20By%20strategically%20separating%20global%20pattern%20learning%20from%20local%0Aadaptation%2C%20this%20framework%20efficiently%20captures%20both%20cross-superpixel%0Acommonalities%20and%20within-superpixel%20variations.%20This%20yields%20a%20representation%0Athat%20is%20both%20highly%20expressive%20and%20compact%2C%20balancing%20model%20efficiency%20with%0Aadaptability.%20Extensive%20experiments%20on%20several%20benchmark%20datasets%20demonstrate%0Athat%20SCTR%20achieves%203-5%20dB%20PSNR%20improvements%20over%20existing%20LRTR-based%20methods%0Aacross%20multispectral%20images%2C%20videos%2C%20and%20color%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12261v2&entry.124074799=Read"},
{"title": "CoMatcher: Multi-View Collaborative Feature Matching", "author": "Jintao Zhang and Zimin Xia and Mingyue Dong and Shuhan Shen and Linwei Yue and Xianwei Zheng", "abstract": "  This paper proposes a multi-view collaborative matching strategy for reliable\ntrack construction in complex scenarios. We observe that the pairwise matching\nparadigms applied to image set matching often result in ambiguous estimation\nwhen the selected independent pairs exhibit significant occlusions or extreme\nviewpoint changes. This challenge primarily stems from the inherent uncertainty\nin interpreting intricate 3D structures based on limited two-view observations,\nas the 3D-to-2D projection leads to significant information loss. To address\nthis, we introduce CoMatcher, a deep multi-view matcher to (i) leverage\ncomplementary context cues from different views to form a holistic 3D scene\nunderstanding and (ii) utilize cross-view projection consistency to infer a\nreliable global solution. Building on CoMatcher, we develop a groupwise\nframework that fully exploits cross-view relationships for large-scale matching\ntasks. Extensive experiments on various complex scenarios demonstrate the\nsuperiority of our method over the mainstream two-view matching paradigm.\n", "link": "http://arxiv.org/abs/2504.01872v2", "date": "2025-08-20", "relevancy": 2.8035, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5787}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5547}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoMatcher%3A%20Multi-View%20Collaborative%20Feature%20Matching&body=Title%3A%20CoMatcher%3A%20Multi-View%20Collaborative%20Feature%20Matching%0AAuthor%3A%20Jintao%20Zhang%20and%20Zimin%20Xia%20and%20Mingyue%20Dong%20and%20Shuhan%20Shen%20and%20Linwei%20Yue%20and%20Xianwei%20Zheng%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20multi-view%20collaborative%20matching%20strategy%20for%20reliable%0Atrack%20construction%20in%20complex%20scenarios.%20We%20observe%20that%20the%20pairwise%20matching%0Aparadigms%20applied%20to%20image%20set%20matching%20often%20result%20in%20ambiguous%20estimation%0Awhen%20the%20selected%20independent%20pairs%20exhibit%20significant%20occlusions%20or%20extreme%0Aviewpoint%20changes.%20This%20challenge%20primarily%20stems%20from%20the%20inherent%20uncertainty%0Ain%20interpreting%20intricate%203D%20structures%20based%20on%20limited%20two-view%20observations%2C%0Aas%20the%203D-to-2D%20projection%20leads%20to%20significant%20information%20loss.%20To%20address%0Athis%2C%20we%20introduce%20CoMatcher%2C%20a%20deep%20multi-view%20matcher%20to%20%28i%29%20leverage%0Acomplementary%20context%20cues%20from%20different%20views%20to%20form%20a%20holistic%203D%20scene%0Aunderstanding%20and%20%28ii%29%20utilize%20cross-view%20projection%20consistency%20to%20infer%20a%0Areliable%20global%20solution.%20Building%20on%20CoMatcher%2C%20we%20develop%20a%20groupwise%0Aframework%20that%20fully%20exploits%20cross-view%20relationships%20for%20large-scale%20matching%0Atasks.%20Extensive%20experiments%20on%20various%20complex%20scenarios%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20the%20mainstream%20two-view%20matching%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01872v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoMatcher%253A%2520Multi-View%2520Collaborative%2520Feature%2520Matching%26entry.906535625%3DJintao%2520Zhang%2520and%2520Zimin%2520Xia%2520and%2520Mingyue%2520Dong%2520and%2520Shuhan%2520Shen%2520and%2520Linwei%2520Yue%2520and%2520Xianwei%2520Zheng%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520multi-view%2520collaborative%2520matching%2520strategy%2520for%2520reliable%250Atrack%2520construction%2520in%2520complex%2520scenarios.%2520We%2520observe%2520that%2520the%2520pairwise%2520matching%250Aparadigms%2520applied%2520to%2520image%2520set%2520matching%2520often%2520result%2520in%2520ambiguous%2520estimation%250Awhen%2520the%2520selected%2520independent%2520pairs%2520exhibit%2520significant%2520occlusions%2520or%2520extreme%250Aviewpoint%2520changes.%2520This%2520challenge%2520primarily%2520stems%2520from%2520the%2520inherent%2520uncertainty%250Ain%2520interpreting%2520intricate%25203D%2520structures%2520based%2520on%2520limited%2520two-view%2520observations%252C%250Aas%2520the%25203D-to-2D%2520projection%2520leads%2520to%2520significant%2520information%2520loss.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520CoMatcher%252C%2520a%2520deep%2520multi-view%2520matcher%2520to%2520%2528i%2529%2520leverage%250Acomplementary%2520context%2520cues%2520from%2520different%2520views%2520to%2520form%2520a%2520holistic%25203D%2520scene%250Aunderstanding%2520and%2520%2528ii%2529%2520utilize%2520cross-view%2520projection%2520consistency%2520to%2520infer%2520a%250Areliable%2520global%2520solution.%2520Building%2520on%2520CoMatcher%252C%2520we%2520develop%2520a%2520groupwise%250Aframework%2520that%2520fully%2520exploits%2520cross-view%2520relationships%2520for%2520large-scale%2520matching%250Atasks.%2520Extensive%2520experiments%2520on%2520various%2520complex%2520scenarios%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520method%2520over%2520the%2520mainstream%2520two-view%2520matching%2520paradigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01872v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoMatcher%3A%20Multi-View%20Collaborative%20Feature%20Matching&entry.906535625=Jintao%20Zhang%20and%20Zimin%20Xia%20and%20Mingyue%20Dong%20and%20Shuhan%20Shen%20and%20Linwei%20Yue%20and%20Xianwei%20Zheng&entry.1292438233=%20%20This%20paper%20proposes%20a%20multi-view%20collaborative%20matching%20strategy%20for%20reliable%0Atrack%20construction%20in%20complex%20scenarios.%20We%20observe%20that%20the%20pairwise%20matching%0Aparadigms%20applied%20to%20image%20set%20matching%20often%20result%20in%20ambiguous%20estimation%0Awhen%20the%20selected%20independent%20pairs%20exhibit%20significant%20occlusions%20or%20extreme%0Aviewpoint%20changes.%20This%20challenge%20primarily%20stems%20from%20the%20inherent%20uncertainty%0Ain%20interpreting%20intricate%203D%20structures%20based%20on%20limited%20two-view%20observations%2C%0Aas%20the%203D-to-2D%20projection%20leads%20to%20significant%20information%20loss.%20To%20address%0Athis%2C%20we%20introduce%20CoMatcher%2C%20a%20deep%20multi-view%20matcher%20to%20%28i%29%20leverage%0Acomplementary%20context%20cues%20from%20different%20views%20to%20form%20a%20holistic%203D%20scene%0Aunderstanding%20and%20%28ii%29%20utilize%20cross-view%20projection%20consistency%20to%20infer%20a%0Areliable%20global%20solution.%20Building%20on%20CoMatcher%2C%20we%20develop%20a%20groupwise%0Aframework%20that%20fully%20exploits%20cross-view%20relationships%20for%20large-scale%20matching%0Atasks.%20Extensive%20experiments%20on%20various%20complex%20scenarios%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20the%20mainstream%20two-view%20matching%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01872v2&entry.124074799=Read"},
{"title": "Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation\n  Dataset for Marine Vessels", "author": "Fabian Holst and Emre G\u00fclsoylu and Simone Frintrop", "abstract": "  The paper presents a novel technique for creating a 6D pose estimation\ndataset for marine vessels by fusing monocular RGB images with Automatic\nIdentification System (AIS) data. The proposed technique addresses the\nlimitations of relying purely on AIS for location information, caused by issues\nlike equipment reliability, data manipulation, and transmission delays. By\ncombining vessel detections from monocular RGB images, obtained using an object\ndetection network (YOLOX-X), with AIS messages, the technique generates 3D\nbounding boxes that represent the vessels' 6D poses, i.e. spatial and\nrotational dimensions. The paper evaluates different object detection models to\nlocate vessels in image space. We also compare two transformation methods\n(homography and Perspective-n-Point) for aligning AIS data with image\ncoordinates. The results of our work demonstrate that the Perspective-n-Point\n(PnP) method achieves a significantly lower projection error compared to\nhomography-based approaches used before, and the YOLOX-X model achieves a mean\nAverage Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold\nof 0.5 for relevant vessel classes. We show indication that our approach allows\nthe creation of a 6D pose estimation dataset without needing manual annotation.\nAdditionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a\npublicly available dataset comprising 3753 images with 3D bounding box\nannotations for pose estimation, created by our data fusion approach. This\ndataset can be used for training and evaluating 6D pose estimation networks. In\naddition we introduce a set of 1000 images with 2D bounding box annotations for\nship detection from the same scene.\n", "link": "http://arxiv.org/abs/2508.14767v1", "date": "2025-08-20", "relevancy": 2.782, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5703}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5603}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusing%20Monocular%20RGB%20Images%20with%20AIS%20Data%20to%20Create%20a%206D%20Pose%20Estimation%0A%20%20Dataset%20for%20Marine%20Vessels&body=Title%3A%20Fusing%20Monocular%20RGB%20Images%20with%20AIS%20Data%20to%20Create%20a%206D%20Pose%20Estimation%0A%20%20Dataset%20for%20Marine%20Vessels%0AAuthor%3A%20Fabian%20Holst%20and%20Emre%20G%C3%BClsoylu%20and%20Simone%20Frintrop%0AAbstract%3A%20%20%20The%20paper%20presents%20a%20novel%20technique%20for%20creating%20a%206D%20pose%20estimation%0Adataset%20for%20marine%20vessels%20by%20fusing%20monocular%20RGB%20images%20with%20Automatic%0AIdentification%20System%20%28AIS%29%20data.%20The%20proposed%20technique%20addresses%20the%0Alimitations%20of%20relying%20purely%20on%20AIS%20for%20location%20information%2C%20caused%20by%20issues%0Alike%20equipment%20reliability%2C%20data%20manipulation%2C%20and%20transmission%20delays.%20By%0Acombining%20vessel%20detections%20from%20monocular%20RGB%20images%2C%20obtained%20using%20an%20object%0Adetection%20network%20%28YOLOX-X%29%2C%20with%20AIS%20messages%2C%20the%20technique%20generates%203D%0Abounding%20boxes%20that%20represent%20the%20vessels%27%206D%20poses%2C%20i.e.%20spatial%20and%0Arotational%20dimensions.%20The%20paper%20evaluates%20different%20object%20detection%20models%20to%0Alocate%20vessels%20in%20image%20space.%20We%20also%20compare%20two%20transformation%20methods%0A%28homography%20and%20Perspective-n-Point%29%20for%20aligning%20AIS%20data%20with%20image%0Acoordinates.%20The%20results%20of%20our%20work%20demonstrate%20that%20the%20Perspective-n-Point%0A%28PnP%29%20method%20achieves%20a%20significantly%20lower%20projection%20error%20compared%20to%0Ahomography-based%20approaches%20used%20before%2C%20and%20the%20YOLOX-X%20model%20achieves%20a%20mean%0AAverage%20Precision%20%28mAP%29%20of%200.80%20at%20an%20Intersection%20over%20Union%20%28IoU%29%20threshold%0Aof%200.5%20for%20relevant%20vessel%20classes.%20We%20show%20indication%20that%20our%20approach%20allows%0Athe%20creation%20of%20a%206D%20pose%20estimation%20dataset%20without%20needing%20manual%20annotation.%0AAdditionally%2C%20we%20introduce%20the%20Boats%20on%20Nordelbe%20Kehrwieder%20%28BONK-pose%29%2C%20a%0Apublicly%20available%20dataset%20comprising%203753%20images%20with%203D%20bounding%20box%0Aannotations%20for%20pose%20estimation%2C%20created%20by%20our%20data%20fusion%20approach.%20This%0Adataset%20can%20be%20used%20for%20training%20and%20evaluating%206D%20pose%20estimation%20networks.%20In%0Aaddition%20we%20introduce%20a%20set%20of%201000%20images%20with%202D%20bounding%20box%20annotations%20for%0Aship%20detection%20from%20the%20same%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusing%2520Monocular%2520RGB%2520Images%2520with%2520AIS%2520Data%2520to%2520Create%2520a%25206D%2520Pose%2520Estimation%250A%2520%2520Dataset%2520for%2520Marine%2520Vessels%26entry.906535625%3DFabian%2520Holst%2520and%2520Emre%2520G%25C3%25BClsoylu%2520and%2520Simone%2520Frintrop%26entry.1292438233%3D%2520%2520The%2520paper%2520presents%2520a%2520novel%2520technique%2520for%2520creating%2520a%25206D%2520pose%2520estimation%250Adataset%2520for%2520marine%2520vessels%2520by%2520fusing%2520monocular%2520RGB%2520images%2520with%2520Automatic%250AIdentification%2520System%2520%2528AIS%2529%2520data.%2520The%2520proposed%2520technique%2520addresses%2520the%250Alimitations%2520of%2520relying%2520purely%2520on%2520AIS%2520for%2520location%2520information%252C%2520caused%2520by%2520issues%250Alike%2520equipment%2520reliability%252C%2520data%2520manipulation%252C%2520and%2520transmission%2520delays.%2520By%250Acombining%2520vessel%2520detections%2520from%2520monocular%2520RGB%2520images%252C%2520obtained%2520using%2520an%2520object%250Adetection%2520network%2520%2528YOLOX-X%2529%252C%2520with%2520AIS%2520messages%252C%2520the%2520technique%2520generates%25203D%250Abounding%2520boxes%2520that%2520represent%2520the%2520vessels%2527%25206D%2520poses%252C%2520i.e.%2520spatial%2520and%250Arotational%2520dimensions.%2520The%2520paper%2520evaluates%2520different%2520object%2520detection%2520models%2520to%250Alocate%2520vessels%2520in%2520image%2520space.%2520We%2520also%2520compare%2520two%2520transformation%2520methods%250A%2528homography%2520and%2520Perspective-n-Point%2529%2520for%2520aligning%2520AIS%2520data%2520with%2520image%250Acoordinates.%2520The%2520results%2520of%2520our%2520work%2520demonstrate%2520that%2520the%2520Perspective-n-Point%250A%2528PnP%2529%2520method%2520achieves%2520a%2520significantly%2520lower%2520projection%2520error%2520compared%2520to%250Ahomography-based%2520approaches%2520used%2520before%252C%2520and%2520the%2520YOLOX-X%2520model%2520achieves%2520a%2520mean%250AAverage%2520Precision%2520%2528mAP%2529%2520of%25200.80%2520at%2520an%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520threshold%250Aof%25200.5%2520for%2520relevant%2520vessel%2520classes.%2520We%2520show%2520indication%2520that%2520our%2520approach%2520allows%250Athe%2520creation%2520of%2520a%25206D%2520pose%2520estimation%2520dataset%2520without%2520needing%2520manual%2520annotation.%250AAdditionally%252C%2520we%2520introduce%2520the%2520Boats%2520on%2520Nordelbe%2520Kehrwieder%2520%2528BONK-pose%2529%252C%2520a%250Apublicly%2520available%2520dataset%2520comprising%25203753%2520images%2520with%25203D%2520bounding%2520box%250Aannotations%2520for%2520pose%2520estimation%252C%2520created%2520by%2520our%2520data%2520fusion%2520approach.%2520This%250Adataset%2520can%2520be%2520used%2520for%2520training%2520and%2520evaluating%25206D%2520pose%2520estimation%2520networks.%2520In%250Aaddition%2520we%2520introduce%2520a%2520set%2520of%25201000%2520images%2520with%25202D%2520bounding%2520box%2520annotations%2520for%250Aship%2520detection%2520from%2520the%2520same%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusing%20Monocular%20RGB%20Images%20with%20AIS%20Data%20to%20Create%20a%206D%20Pose%20Estimation%0A%20%20Dataset%20for%20Marine%20Vessels&entry.906535625=Fabian%20Holst%20and%20Emre%20G%C3%BClsoylu%20and%20Simone%20Frintrop&entry.1292438233=%20%20The%20paper%20presents%20a%20novel%20technique%20for%20creating%20a%206D%20pose%20estimation%0Adataset%20for%20marine%20vessels%20by%20fusing%20monocular%20RGB%20images%20with%20Automatic%0AIdentification%20System%20%28AIS%29%20data.%20The%20proposed%20technique%20addresses%20the%0Alimitations%20of%20relying%20purely%20on%20AIS%20for%20location%20information%2C%20caused%20by%20issues%0Alike%20equipment%20reliability%2C%20data%20manipulation%2C%20and%20transmission%20delays.%20By%0Acombining%20vessel%20detections%20from%20monocular%20RGB%20images%2C%20obtained%20using%20an%20object%0Adetection%20network%20%28YOLOX-X%29%2C%20with%20AIS%20messages%2C%20the%20technique%20generates%203D%0Abounding%20boxes%20that%20represent%20the%20vessels%27%206D%20poses%2C%20i.e.%20spatial%20and%0Arotational%20dimensions.%20The%20paper%20evaluates%20different%20object%20detection%20models%20to%0Alocate%20vessels%20in%20image%20space.%20We%20also%20compare%20two%20transformation%20methods%0A%28homography%20and%20Perspective-n-Point%29%20for%20aligning%20AIS%20data%20with%20image%0Acoordinates.%20The%20results%20of%20our%20work%20demonstrate%20that%20the%20Perspective-n-Point%0A%28PnP%29%20method%20achieves%20a%20significantly%20lower%20projection%20error%20compared%20to%0Ahomography-based%20approaches%20used%20before%2C%20and%20the%20YOLOX-X%20model%20achieves%20a%20mean%0AAverage%20Precision%20%28mAP%29%20of%200.80%20at%20an%20Intersection%20over%20Union%20%28IoU%29%20threshold%0Aof%200.5%20for%20relevant%20vessel%20classes.%20We%20show%20indication%20that%20our%20approach%20allows%0Athe%20creation%20of%20a%206D%20pose%20estimation%20dataset%20without%20needing%20manual%20annotation.%0AAdditionally%2C%20we%20introduce%20the%20Boats%20on%20Nordelbe%20Kehrwieder%20%28BONK-pose%29%2C%20a%0Apublicly%20available%20dataset%20comprising%203753%20images%20with%203D%20bounding%20box%0Aannotations%20for%20pose%20estimation%2C%20created%20by%20our%20data%20fusion%20approach.%20This%0Adataset%20can%20be%20used%20for%20training%20and%20evaluating%206D%20pose%20estimation%20networks.%20In%0Aaddition%20we%20introduce%20a%20set%20of%201000%20images%20with%202D%20bounding%20box%20annotations%20for%0Aship%20detection%20from%20the%20same%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14767v1&entry.124074799=Read"},
{"title": "Seeing Further on the Shoulders of Giants: Knowledge Inheritance for\n  Vision Foundation Models", "author": "Jiabo Huang and Chen Chen and Lingjuan Lyu", "abstract": "  Vision foundation models (VFMs) are predominantly developed using\ndata-centric methods. These methods require training on vast amounts of data\nusually with high-quality labels, which poses a bottleneck for most\ninstitutions that lack both large-scale data and high-end GPUs. On the other\nhand, many open-source vision models have been pretrained on domain-specific\ndata, enabling them to distill and represent core knowledge in a form that is\ntransferable across diverse applications. Even though these models are highly\nvaluable assets, they remain largely under-explored in empowering the\ndevelopment of a general-purpose VFM. In this paper, we presents a new\nmodel-driven approach for training VFMs through joint knowledge transfer and\npreservation. Our method unifies multiple pre-trained teacher models in a\nshared latent space to mitigate the ``imbalanced transfer'' issue caused by\ntheir distributional gaps. Besides, we introduce a knowledge preservation\nstrategy to take a general-purpose teacher as a knowledge base for integrating\nknowledge from the remaining purpose-specific teachers using an adapter module.\nBy unifying and aggregating existing models, we build a powerful VFM to inherit\nteachers' expertise without needing to train on a large amount of labeled data.\nOur model not only provides generalizable visual features, but also inherently\nsupports multiple downstream tasks. Extensive experiments demonstrate that our\nVFM outperforms existing data-centric models across four fundamental vision\ntasks, including image classification, object detection, semantic and instance\nsegmentation.\n", "link": "http://arxiv.org/abs/2508.14707v1", "date": "2025-08-20", "relevancy": 2.7472, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5656}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Further%20on%20the%20Shoulders%20of%20Giants%3A%20Knowledge%20Inheritance%20for%0A%20%20Vision%20Foundation%20Models&body=Title%3A%20Seeing%20Further%20on%20the%20Shoulders%20of%20Giants%3A%20Knowledge%20Inheritance%20for%0A%20%20Vision%20Foundation%20Models%0AAuthor%3A%20Jiabo%20Huang%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu%0AAbstract%3A%20%20%20Vision%20foundation%20models%20%28VFMs%29%20are%20predominantly%20developed%20using%0Adata-centric%20methods.%20These%20methods%20require%20training%20on%20vast%20amounts%20of%20data%0Ausually%20with%20high-quality%20labels%2C%20which%20poses%20a%20bottleneck%20for%20most%0Ainstitutions%20that%20lack%20both%20large-scale%20data%20and%20high-end%20GPUs.%20On%20the%20other%0Ahand%2C%20many%20open-source%20vision%20models%20have%20been%20pretrained%20on%20domain-specific%0Adata%2C%20enabling%20them%20to%20distill%20and%20represent%20core%20knowledge%20in%20a%20form%20that%20is%0Atransferable%20across%20diverse%20applications.%20Even%20though%20these%20models%20are%20highly%0Avaluable%20assets%2C%20they%20remain%20largely%20under-explored%20in%20empowering%20the%0Adevelopment%20of%20a%20general-purpose%20VFM.%20In%20this%20paper%2C%20we%20presents%20a%20new%0Amodel-driven%20approach%20for%20training%20VFMs%20through%20joint%20knowledge%20transfer%20and%0Apreservation.%20Our%20method%20unifies%20multiple%20pre-trained%20teacher%20models%20in%20a%0Ashared%20latent%20space%20to%20mitigate%20the%20%60%60imbalanced%20transfer%27%27%20issue%20caused%20by%0Atheir%20distributional%20gaps.%20Besides%2C%20we%20introduce%20a%20knowledge%20preservation%0Astrategy%20to%20take%20a%20general-purpose%20teacher%20as%20a%20knowledge%20base%20for%20integrating%0Aknowledge%20from%20the%20remaining%20purpose-specific%20teachers%20using%20an%20adapter%20module.%0ABy%20unifying%20and%20aggregating%20existing%20models%2C%20we%20build%20a%20powerful%20VFM%20to%20inherit%0Ateachers%27%20expertise%20without%20needing%20to%20train%20on%20a%20large%20amount%20of%20labeled%20data.%0AOur%20model%20not%20only%20provides%20generalizable%20visual%20features%2C%20but%20also%20inherently%0Asupports%20multiple%20downstream%20tasks.%20Extensive%20experiments%20demonstrate%20that%20our%0AVFM%20outperforms%20existing%20data-centric%20models%20across%20four%20fundamental%20vision%0Atasks%2C%20including%20image%20classification%2C%20object%20detection%2C%20semantic%20and%20instance%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Further%2520on%2520the%2520Shoulders%2520of%2520Giants%253A%2520Knowledge%2520Inheritance%2520for%250A%2520%2520Vision%2520Foundation%2520Models%26entry.906535625%3DJiabo%2520Huang%2520and%2520Chen%2520Chen%2520and%2520Lingjuan%2520Lyu%26entry.1292438233%3D%2520%2520Vision%2520foundation%2520models%2520%2528VFMs%2529%2520are%2520predominantly%2520developed%2520using%250Adata-centric%2520methods.%2520These%2520methods%2520require%2520training%2520on%2520vast%2520amounts%2520of%2520data%250Ausually%2520with%2520high-quality%2520labels%252C%2520which%2520poses%2520a%2520bottleneck%2520for%2520most%250Ainstitutions%2520that%2520lack%2520both%2520large-scale%2520data%2520and%2520high-end%2520GPUs.%2520On%2520the%2520other%250Ahand%252C%2520many%2520open-source%2520vision%2520models%2520have%2520been%2520pretrained%2520on%2520domain-specific%250Adata%252C%2520enabling%2520them%2520to%2520distill%2520and%2520represent%2520core%2520knowledge%2520in%2520a%2520form%2520that%2520is%250Atransferable%2520across%2520diverse%2520applications.%2520Even%2520though%2520these%2520models%2520are%2520highly%250Avaluable%2520assets%252C%2520they%2520remain%2520largely%2520under-explored%2520in%2520empowering%2520the%250Adevelopment%2520of%2520a%2520general-purpose%2520VFM.%2520In%2520this%2520paper%252C%2520we%2520presents%2520a%2520new%250Amodel-driven%2520approach%2520for%2520training%2520VFMs%2520through%2520joint%2520knowledge%2520transfer%2520and%250Apreservation.%2520Our%2520method%2520unifies%2520multiple%2520pre-trained%2520teacher%2520models%2520in%2520a%250Ashared%2520latent%2520space%2520to%2520mitigate%2520the%2520%2560%2560imbalanced%2520transfer%2527%2527%2520issue%2520caused%2520by%250Atheir%2520distributional%2520gaps.%2520Besides%252C%2520we%2520introduce%2520a%2520knowledge%2520preservation%250Astrategy%2520to%2520take%2520a%2520general-purpose%2520teacher%2520as%2520a%2520knowledge%2520base%2520for%2520integrating%250Aknowledge%2520from%2520the%2520remaining%2520purpose-specific%2520teachers%2520using%2520an%2520adapter%2520module.%250ABy%2520unifying%2520and%2520aggregating%2520existing%2520models%252C%2520we%2520build%2520a%2520powerful%2520VFM%2520to%2520inherit%250Ateachers%2527%2520expertise%2520without%2520needing%2520to%2520train%2520on%2520a%2520large%2520amount%2520of%2520labeled%2520data.%250AOur%2520model%2520not%2520only%2520provides%2520generalizable%2520visual%2520features%252C%2520but%2520also%2520inherently%250Asupports%2520multiple%2520downstream%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250AVFM%2520outperforms%2520existing%2520data-centric%2520models%2520across%2520four%2520fundamental%2520vision%250Atasks%252C%2520including%2520image%2520classification%252C%2520object%2520detection%252C%2520semantic%2520and%2520instance%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Further%20on%20the%20Shoulders%20of%20Giants%3A%20Knowledge%20Inheritance%20for%0A%20%20Vision%20Foundation%20Models&entry.906535625=Jiabo%20Huang%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu&entry.1292438233=%20%20Vision%20foundation%20models%20%28VFMs%29%20are%20predominantly%20developed%20using%0Adata-centric%20methods.%20These%20methods%20require%20training%20on%20vast%20amounts%20of%20data%0Ausually%20with%20high-quality%20labels%2C%20which%20poses%20a%20bottleneck%20for%20most%0Ainstitutions%20that%20lack%20both%20large-scale%20data%20and%20high-end%20GPUs.%20On%20the%20other%0Ahand%2C%20many%20open-source%20vision%20models%20have%20been%20pretrained%20on%20domain-specific%0Adata%2C%20enabling%20them%20to%20distill%20and%20represent%20core%20knowledge%20in%20a%20form%20that%20is%0Atransferable%20across%20diverse%20applications.%20Even%20though%20these%20models%20are%20highly%0Avaluable%20assets%2C%20they%20remain%20largely%20under-explored%20in%20empowering%20the%0Adevelopment%20of%20a%20general-purpose%20VFM.%20In%20this%20paper%2C%20we%20presents%20a%20new%0Amodel-driven%20approach%20for%20training%20VFMs%20through%20joint%20knowledge%20transfer%20and%0Apreservation.%20Our%20method%20unifies%20multiple%20pre-trained%20teacher%20models%20in%20a%0Ashared%20latent%20space%20to%20mitigate%20the%20%60%60imbalanced%20transfer%27%27%20issue%20caused%20by%0Atheir%20distributional%20gaps.%20Besides%2C%20we%20introduce%20a%20knowledge%20preservation%0Astrategy%20to%20take%20a%20general-purpose%20teacher%20as%20a%20knowledge%20base%20for%20integrating%0Aknowledge%20from%20the%20remaining%20purpose-specific%20teachers%20using%20an%20adapter%20module.%0ABy%20unifying%20and%20aggregating%20existing%20models%2C%20we%20build%20a%20powerful%20VFM%20to%20inherit%0Ateachers%27%20expertise%20without%20needing%20to%20train%20on%20a%20large%20amount%20of%20labeled%20data.%0AOur%20model%20not%20only%20provides%20generalizable%20visual%20features%2C%20but%20also%20inherently%0Asupports%20multiple%20downstream%20tasks.%20Extensive%20experiments%20demonstrate%20that%20our%0AVFM%20outperforms%20existing%20data-centric%20models%20across%20four%20fundamental%20vision%0Atasks%2C%20including%20image%20classification%2C%20object%20detection%2C%20semantic%20and%20instance%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14707v1&entry.124074799=Read"},
{"title": "RotBench: Evaluating Multimodal Large Language Models on Identifying\n  Image Rotation", "author": "Tianyi Niu and Jaemin Cho and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  We investigate to what extent Multimodal Large Language Models (MLLMs) can\naccurately identify the orientation of input images rotated 0{\\deg}, 90{\\deg},\n180{\\deg}, and 270{\\deg}. This task demands robust visual reasoning\ncapabilities to detect rotational cues and contextualize spatial relationships\nwithin images, regardless of their orientation. To evaluate MLLMs on these\nabilities, we introduce RotBench -- a 350-image manually-filtered benchmark\ncomprising lifestyle, portrait, and landscape images. Despite the relatively\nsimple nature of this task, we show that several state-of-the-art open and\nproprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably\nidentify rotation in input images. Providing models with auxiliary information\n-- including captions, depth maps, and more -- or using chain-of-thought\nprompting offers only small and inconsistent improvements. Our results indicate\nthat most models are able to reliably identify right-side-up (0{\\deg}) images,\nwhile certain models are able to identify upside-down (180{\\deg}) images. None\ncan reliably distinguish between 90{\\deg} and 270{\\deg}. Simultaneously showing\nthe image rotated in different orientations leads to moderate performance gains\nfor reasoning models, while a modified setup using voting improves the\nperformance of weaker models. We further show that fine-tuning does not improve\nmodels' ability to distinguish 90{\\deg} and 270{\\deg} rotations, despite\nsubstantially improving the identification of 180{\\deg} images. Together, these\nresults reveal a significant gap between MLLMs' spatial reasoning capabilities\nand human perception in identifying rotation.\n", "link": "http://arxiv.org/abs/2508.13968v2", "date": "2025-08-20", "relevancy": 2.7282, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RotBench%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Identifying%0A%20%20Image%20Rotation&body=Title%3A%20RotBench%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Identifying%0A%20%20Image%20Rotation%0AAuthor%3A%20Tianyi%20Niu%20and%20Jaemin%20Cho%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20We%20investigate%20to%20what%20extent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20can%0Aaccurately%20identify%20the%20orientation%20of%20input%20images%20rotated%200%7B%5Cdeg%7D%2C%2090%7B%5Cdeg%7D%2C%0A180%7B%5Cdeg%7D%2C%20and%20270%7B%5Cdeg%7D.%20This%20task%20demands%20robust%20visual%20reasoning%0Acapabilities%20to%20detect%20rotational%20cues%20and%20contextualize%20spatial%20relationships%0Awithin%20images%2C%20regardless%20of%20their%20orientation.%20To%20evaluate%20MLLMs%20on%20these%0Aabilities%2C%20we%20introduce%20RotBench%20--%20a%20350-image%20manually-filtered%20benchmark%0Acomprising%20lifestyle%2C%20portrait%2C%20and%20landscape%20images.%20Despite%20the%20relatively%0Asimple%20nature%20of%20this%20task%2C%20we%20show%20that%20several%20state-of-the-art%20open%20and%0Aproprietary%20MLLMs%2C%20including%20GPT-5%2C%20o3%2C%20and%20Gemini-2.5-Pro%2C%20do%20not%20reliably%0Aidentify%20rotation%20in%20input%20images.%20Providing%20models%20with%20auxiliary%20information%0A--%20including%20captions%2C%20depth%20maps%2C%20and%20more%20--%20or%20using%20chain-of-thought%0Aprompting%20offers%20only%20small%20and%20inconsistent%20improvements.%20Our%20results%20indicate%0Athat%20most%20models%20are%20able%20to%20reliably%20identify%20right-side-up%20%280%7B%5Cdeg%7D%29%20images%2C%0Awhile%20certain%20models%20are%20able%20to%20identify%20upside-down%20%28180%7B%5Cdeg%7D%29%20images.%20None%0Acan%20reliably%20distinguish%20between%2090%7B%5Cdeg%7D%20and%20270%7B%5Cdeg%7D.%20Simultaneously%20showing%0Athe%20image%20rotated%20in%20different%20orientations%20leads%20to%20moderate%20performance%20gains%0Afor%20reasoning%20models%2C%20while%20a%20modified%20setup%20using%20voting%20improves%20the%0Aperformance%20of%20weaker%20models.%20We%20further%20show%20that%20fine-tuning%20does%20not%20improve%0Amodels%27%20ability%20to%20distinguish%2090%7B%5Cdeg%7D%20and%20270%7B%5Cdeg%7D%20rotations%2C%20despite%0Asubstantially%20improving%20the%20identification%20of%20180%7B%5Cdeg%7D%20images.%20Together%2C%20these%0Aresults%20reveal%20a%20significant%20gap%20between%20MLLMs%27%20spatial%20reasoning%20capabilities%0Aand%20human%20perception%20in%20identifying%20rotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRotBench%253A%2520Evaluating%2520Multimodal%2520Large%2520Language%2520Models%2520on%2520Identifying%250A%2520%2520Image%2520Rotation%26entry.906535625%3DTianyi%2520Niu%2520and%2520Jaemin%2520Cho%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520We%2520investigate%2520to%2520what%2520extent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520can%250Aaccurately%2520identify%2520the%2520orientation%2520of%2520input%2520images%2520rotated%25200%257B%255Cdeg%257D%252C%252090%257B%255Cdeg%257D%252C%250A180%257B%255Cdeg%257D%252C%2520and%2520270%257B%255Cdeg%257D.%2520This%2520task%2520demands%2520robust%2520visual%2520reasoning%250Acapabilities%2520to%2520detect%2520rotational%2520cues%2520and%2520contextualize%2520spatial%2520relationships%250Awithin%2520images%252C%2520regardless%2520of%2520their%2520orientation.%2520To%2520evaluate%2520MLLMs%2520on%2520these%250Aabilities%252C%2520we%2520introduce%2520RotBench%2520--%2520a%2520350-image%2520manually-filtered%2520benchmark%250Acomprising%2520lifestyle%252C%2520portrait%252C%2520and%2520landscape%2520images.%2520Despite%2520the%2520relatively%250Asimple%2520nature%2520of%2520this%2520task%252C%2520we%2520show%2520that%2520several%2520state-of-the-art%2520open%2520and%250Aproprietary%2520MLLMs%252C%2520including%2520GPT-5%252C%2520o3%252C%2520and%2520Gemini-2.5-Pro%252C%2520do%2520not%2520reliably%250Aidentify%2520rotation%2520in%2520input%2520images.%2520Providing%2520models%2520with%2520auxiliary%2520information%250A--%2520including%2520captions%252C%2520depth%2520maps%252C%2520and%2520more%2520--%2520or%2520using%2520chain-of-thought%250Aprompting%2520offers%2520only%2520small%2520and%2520inconsistent%2520improvements.%2520Our%2520results%2520indicate%250Athat%2520most%2520models%2520are%2520able%2520to%2520reliably%2520identify%2520right-side-up%2520%25280%257B%255Cdeg%257D%2529%2520images%252C%250Awhile%2520certain%2520models%2520are%2520able%2520to%2520identify%2520upside-down%2520%2528180%257B%255Cdeg%257D%2529%2520images.%2520None%250Acan%2520reliably%2520distinguish%2520between%252090%257B%255Cdeg%257D%2520and%2520270%257B%255Cdeg%257D.%2520Simultaneously%2520showing%250Athe%2520image%2520rotated%2520in%2520different%2520orientations%2520leads%2520to%2520moderate%2520performance%2520gains%250Afor%2520reasoning%2520models%252C%2520while%2520a%2520modified%2520setup%2520using%2520voting%2520improves%2520the%250Aperformance%2520of%2520weaker%2520models.%2520We%2520further%2520show%2520that%2520fine-tuning%2520does%2520not%2520improve%250Amodels%2527%2520ability%2520to%2520distinguish%252090%257B%255Cdeg%257D%2520and%2520270%257B%255Cdeg%257D%2520rotations%252C%2520despite%250Asubstantially%2520improving%2520the%2520identification%2520of%2520180%257B%255Cdeg%257D%2520images.%2520Together%252C%2520these%250Aresults%2520reveal%2520a%2520significant%2520gap%2520between%2520MLLMs%2527%2520spatial%2520reasoning%2520capabilities%250Aand%2520human%2520perception%2520in%2520identifying%2520rotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RotBench%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Identifying%0A%20%20Image%20Rotation&entry.906535625=Tianyi%20Niu%20and%20Jaemin%20Cho%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20We%20investigate%20to%20what%20extent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20can%0Aaccurately%20identify%20the%20orientation%20of%20input%20images%20rotated%200%7B%5Cdeg%7D%2C%2090%7B%5Cdeg%7D%2C%0A180%7B%5Cdeg%7D%2C%20and%20270%7B%5Cdeg%7D.%20This%20task%20demands%20robust%20visual%20reasoning%0Acapabilities%20to%20detect%20rotational%20cues%20and%20contextualize%20spatial%20relationships%0Awithin%20images%2C%20regardless%20of%20their%20orientation.%20To%20evaluate%20MLLMs%20on%20these%0Aabilities%2C%20we%20introduce%20RotBench%20--%20a%20350-image%20manually-filtered%20benchmark%0Acomprising%20lifestyle%2C%20portrait%2C%20and%20landscape%20images.%20Despite%20the%20relatively%0Asimple%20nature%20of%20this%20task%2C%20we%20show%20that%20several%20state-of-the-art%20open%20and%0Aproprietary%20MLLMs%2C%20including%20GPT-5%2C%20o3%2C%20and%20Gemini-2.5-Pro%2C%20do%20not%20reliably%0Aidentify%20rotation%20in%20input%20images.%20Providing%20models%20with%20auxiliary%20information%0A--%20including%20captions%2C%20depth%20maps%2C%20and%20more%20--%20or%20using%20chain-of-thought%0Aprompting%20offers%20only%20small%20and%20inconsistent%20improvements.%20Our%20results%20indicate%0Athat%20most%20models%20are%20able%20to%20reliably%20identify%20right-side-up%20%280%7B%5Cdeg%7D%29%20images%2C%0Awhile%20certain%20models%20are%20able%20to%20identify%20upside-down%20%28180%7B%5Cdeg%7D%29%20images.%20None%0Acan%20reliably%20distinguish%20between%2090%7B%5Cdeg%7D%20and%20270%7B%5Cdeg%7D.%20Simultaneously%20showing%0Athe%20image%20rotated%20in%20different%20orientations%20leads%20to%20moderate%20performance%20gains%0Afor%20reasoning%20models%2C%20while%20a%20modified%20setup%20using%20voting%20improves%20the%0Aperformance%20of%20weaker%20models.%20We%20further%20show%20that%20fine-tuning%20does%20not%20improve%0Amodels%27%20ability%20to%20distinguish%2090%7B%5Cdeg%7D%20and%20270%7B%5Cdeg%7D%20rotations%2C%20despite%0Asubstantially%20improving%20the%20identification%20of%20180%7B%5Cdeg%7D%20images.%20Together%2C%20these%0Aresults%20reveal%20a%20significant%20gap%20between%20MLLMs%27%20spatial%20reasoning%20capabilities%0Aand%20human%20perception%20in%20identifying%20rotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13968v2&entry.124074799=Read"},
{"title": "MissionHD: Data-Driven Refinement of Reasoning Graph Structure through\n  Hyperdimensional Causal Path Encoding and Decoding", "author": "Sanggeon Yun and Raheeb Hassan and Ryozo Masukawa and Mohsen Imani", "abstract": "  Reasoning graphs from Large Language Models (LLMs) are often misaligned with\ndownstream visual tasks such as video anomaly detection (VAD). Existing Graph\nStructure Refinement (GSR) methods are ill-suited for these novel, dataset-less\ngraphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly\noptimizes graph structure using downstream task data, and propose MissionHD, a\nhyperdimensional computing (HDC) framework to operationalize it. MissionHD uses\nan efficient encode-decode process to refine the graph, guided by the\ndownstream task signal. Experiments on challenging VAD and VAR benchmarks show\nsignificant performance improvements when using our refined graphs, validating\nour approach as an effective pre-processing step.\n", "link": "http://arxiv.org/abs/2508.14746v1", "date": "2025-08-20", "relevancy": 2.7124, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MissionHD%3A%20Data-Driven%20Refinement%20of%20Reasoning%20Graph%20Structure%20through%0A%20%20Hyperdimensional%20Causal%20Path%20Encoding%20and%20Decoding&body=Title%3A%20MissionHD%3A%20Data-Driven%20Refinement%20of%20Reasoning%20Graph%20Structure%20through%0A%20%20Hyperdimensional%20Causal%20Path%20Encoding%20and%20Decoding%0AAuthor%3A%20Sanggeon%20Yun%20and%20Raheeb%20Hassan%20and%20Ryozo%20Masukawa%20and%20Mohsen%20Imani%0AAbstract%3A%20%20%20Reasoning%20graphs%20from%20Large%20Language%20Models%20%28LLMs%29%20are%20often%20misaligned%20with%0Adownstream%20visual%20tasks%20such%20as%20video%20anomaly%20detection%20%28VAD%29.%20Existing%20Graph%0AStructure%20Refinement%20%28GSR%29%20methods%20are%20ill-suited%20for%20these%20novel%2C%20dataset-less%0Agraphs.%20We%20introduce%20Data-driven%20GSR%20%28D-GSR%29%2C%20a%20new%20paradigm%20that%20directly%0Aoptimizes%20graph%20structure%20using%20downstream%20task%20data%2C%20and%20propose%20MissionHD%2C%20a%0Ahyperdimensional%20computing%20%28HDC%29%20framework%20to%20operationalize%20it.%20MissionHD%20uses%0Aan%20efficient%20encode-decode%20process%20to%20refine%20the%20graph%2C%20guided%20by%20the%0Adownstream%20task%20signal.%20Experiments%20on%20challenging%20VAD%20and%20VAR%20benchmarks%20show%0Asignificant%20performance%20improvements%20when%20using%20our%20refined%20graphs%2C%20validating%0Aour%20approach%20as%20an%20effective%20pre-processing%20step.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMissionHD%253A%2520Data-Driven%2520Refinement%2520of%2520Reasoning%2520Graph%2520Structure%2520through%250A%2520%2520Hyperdimensional%2520Causal%2520Path%2520Encoding%2520and%2520Decoding%26entry.906535625%3DSanggeon%2520Yun%2520and%2520Raheeb%2520Hassan%2520and%2520Ryozo%2520Masukawa%2520and%2520Mohsen%2520Imani%26entry.1292438233%3D%2520%2520Reasoning%2520graphs%2520from%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520often%2520misaligned%2520with%250Adownstream%2520visual%2520tasks%2520such%2520as%2520video%2520anomaly%2520detection%2520%2528VAD%2529.%2520Existing%2520Graph%250AStructure%2520Refinement%2520%2528GSR%2529%2520methods%2520are%2520ill-suited%2520for%2520these%2520novel%252C%2520dataset-less%250Agraphs.%2520We%2520introduce%2520Data-driven%2520GSR%2520%2528D-GSR%2529%252C%2520a%2520new%2520paradigm%2520that%2520directly%250Aoptimizes%2520graph%2520structure%2520using%2520downstream%2520task%2520data%252C%2520and%2520propose%2520MissionHD%252C%2520a%250Ahyperdimensional%2520computing%2520%2528HDC%2529%2520framework%2520to%2520operationalize%2520it.%2520MissionHD%2520uses%250Aan%2520efficient%2520encode-decode%2520process%2520to%2520refine%2520the%2520graph%252C%2520guided%2520by%2520the%250Adownstream%2520task%2520signal.%2520Experiments%2520on%2520challenging%2520VAD%2520and%2520VAR%2520benchmarks%2520show%250Asignificant%2520performance%2520improvements%2520when%2520using%2520our%2520refined%2520graphs%252C%2520validating%250Aour%2520approach%2520as%2520an%2520effective%2520pre-processing%2520step.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MissionHD%3A%20Data-Driven%20Refinement%20of%20Reasoning%20Graph%20Structure%20through%0A%20%20Hyperdimensional%20Causal%20Path%20Encoding%20and%20Decoding&entry.906535625=Sanggeon%20Yun%20and%20Raheeb%20Hassan%20and%20Ryozo%20Masukawa%20and%20Mohsen%20Imani&entry.1292438233=%20%20Reasoning%20graphs%20from%20Large%20Language%20Models%20%28LLMs%29%20are%20often%20misaligned%20with%0Adownstream%20visual%20tasks%20such%20as%20video%20anomaly%20detection%20%28VAD%29.%20Existing%20Graph%0AStructure%20Refinement%20%28GSR%29%20methods%20are%20ill-suited%20for%20these%20novel%2C%20dataset-less%0Agraphs.%20We%20introduce%20Data-driven%20GSR%20%28D-GSR%29%2C%20a%20new%20paradigm%20that%20directly%0Aoptimizes%20graph%20structure%20using%20downstream%20task%20data%2C%20and%20propose%20MissionHD%2C%20a%0Ahyperdimensional%20computing%20%28HDC%29%20framework%20to%20operationalize%20it.%20MissionHD%20uses%0Aan%20efficient%20encode-decode%20process%20to%20refine%20the%20graph%2C%20guided%20by%20the%0Adownstream%20task%20signal.%20Experiments%20on%20challenging%20VAD%20and%20VAR%20benchmarks%20show%0Asignificant%20performance%20improvements%20when%20using%20our%20refined%20graphs%2C%20validating%0Aour%20approach%20as%20an%20effective%20pre-processing%20step.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14746v1&entry.124074799=Read"},
{"title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From\n  Sparse Inputs without Per-Scene Optimization", "author": "Canyu Zhao and Xiaoman Li and Tianjian Feng and Zhiyue Zhao and Hao Chen and Chunhua Shen", "abstract": "  We introduce Tinker, a versatile framework for high-fidelity 3D editing that\noperates in both one-shot and few-shot regimes without any per-scene\nfinetuning. Unlike prior techniques that demand extensive per-scene\noptimization to ensure multi-view consistency or to produce dozens of\nconsistent edited input views, Tinker delivers robust, multi-view consistent\nedits from as few as one or two images. This capability stems from repurposing\npretrained diffusion models, which unlocks their latent 3D awareness. To drive\nresearch in this space, we curate the first large-scale multi-view editing\ndataset and data pipeline, spanning diverse scenes and styles. Building on this\ndataset, we develop our framework capable of generating multi-view consistent\nedited views without per-scene training, which consists of two novel\ncomponents: (1) Referring multi-view editor: Enables precise, reference-driven\nedits that remain coherent across all viewpoints. (2) Any-view-to-video\nsynthesizer: Leverages spatial-temporal priors from video diffusion to perform\nhigh-quality scene completion and novel-view generation even from sparse\ninputs. Through extensive experiments, Tinker significantly reduces the barrier\nto generalizable 3D content creation, achieving state-of-the-art performance on\nediting, novel-view synthesis, and rendering enhancement tasks. We believe that\nTinker represents a key step towards truly scalable, zero-shot 3D editing.\nProject webpage: https://aim-uofa.github.io/Tinker\n", "link": "http://arxiv.org/abs/2508.14811v1", "date": "2025-08-20", "relevancy": 2.6985, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6848}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6848}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tinker%3A%20Diffusion%27s%20Gift%20to%203D--Multi-View%20Consistent%20Editing%20From%0A%20%20Sparse%20Inputs%20without%20Per-Scene%20Optimization&body=Title%3A%20Tinker%3A%20Diffusion%27s%20Gift%20to%203D--Multi-View%20Consistent%20Editing%20From%0A%20%20Sparse%20Inputs%20without%20Per-Scene%20Optimization%0AAuthor%3A%20Canyu%20Zhao%20and%20Xiaoman%20Li%20and%20Tianjian%20Feng%20and%20Zhiyue%20Zhao%20and%20Hao%20Chen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20We%20introduce%20Tinker%2C%20a%20versatile%20framework%20for%20high-fidelity%203D%20editing%20that%0Aoperates%20in%20both%20one-shot%20and%20few-shot%20regimes%20without%20any%20per-scene%0Afinetuning.%20Unlike%20prior%20techniques%20that%20demand%20extensive%20per-scene%0Aoptimization%20to%20ensure%20multi-view%20consistency%20or%20to%20produce%20dozens%20of%0Aconsistent%20edited%20input%20views%2C%20Tinker%20delivers%20robust%2C%20multi-view%20consistent%0Aedits%20from%20as%20few%20as%20one%20or%20two%20images.%20This%20capability%20stems%20from%20repurposing%0Apretrained%20diffusion%20models%2C%20which%20unlocks%20their%20latent%203D%20awareness.%20To%20drive%0Aresearch%20in%20this%20space%2C%20we%20curate%20the%20first%20large-scale%20multi-view%20editing%0Adataset%20and%20data%20pipeline%2C%20spanning%20diverse%20scenes%20and%20styles.%20Building%20on%20this%0Adataset%2C%20we%20develop%20our%20framework%20capable%20of%20generating%20multi-view%20consistent%0Aedited%20views%20without%20per-scene%20training%2C%20which%20consists%20of%20two%20novel%0Acomponents%3A%20%281%29%20Referring%20multi-view%20editor%3A%20Enables%20precise%2C%20reference-driven%0Aedits%20that%20remain%20coherent%20across%20all%20viewpoints.%20%282%29%20Any-view-to-video%0Asynthesizer%3A%20Leverages%20spatial-temporal%20priors%20from%20video%20diffusion%20to%20perform%0Ahigh-quality%20scene%20completion%20and%20novel-view%20generation%20even%20from%20sparse%0Ainputs.%20Through%20extensive%20experiments%2C%20Tinker%20significantly%20reduces%20the%20barrier%0Ato%20generalizable%203D%20content%20creation%2C%20achieving%20state-of-the-art%20performance%20on%0Aediting%2C%20novel-view%20synthesis%2C%20and%20rendering%20enhancement%20tasks.%20We%20believe%20that%0ATinker%20represents%20a%20key%20step%20towards%20truly%20scalable%2C%20zero-shot%203D%20editing.%0AProject%20webpage%3A%20https%3A//aim-uofa.github.io/Tinker%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinker%253A%2520Diffusion%2527s%2520Gift%2520to%25203D--Multi-View%2520Consistent%2520Editing%2520From%250A%2520%2520Sparse%2520Inputs%2520without%2520Per-Scene%2520Optimization%26entry.906535625%3DCanyu%2520Zhao%2520and%2520Xiaoman%2520Li%2520and%2520Tianjian%2520Feng%2520and%2520Zhiyue%2520Zhao%2520and%2520Hao%2520Chen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520We%2520introduce%2520Tinker%252C%2520a%2520versatile%2520framework%2520for%2520high-fidelity%25203D%2520editing%2520that%250Aoperates%2520in%2520both%2520one-shot%2520and%2520few-shot%2520regimes%2520without%2520any%2520per-scene%250Afinetuning.%2520Unlike%2520prior%2520techniques%2520that%2520demand%2520extensive%2520per-scene%250Aoptimization%2520to%2520ensure%2520multi-view%2520consistency%2520or%2520to%2520produce%2520dozens%2520of%250Aconsistent%2520edited%2520input%2520views%252C%2520Tinker%2520delivers%2520robust%252C%2520multi-view%2520consistent%250Aedits%2520from%2520as%2520few%2520as%2520one%2520or%2520two%2520images.%2520This%2520capability%2520stems%2520from%2520repurposing%250Apretrained%2520diffusion%2520models%252C%2520which%2520unlocks%2520their%2520latent%25203D%2520awareness.%2520To%2520drive%250Aresearch%2520in%2520this%2520space%252C%2520we%2520curate%2520the%2520first%2520large-scale%2520multi-view%2520editing%250Adataset%2520and%2520data%2520pipeline%252C%2520spanning%2520diverse%2520scenes%2520and%2520styles.%2520Building%2520on%2520this%250Adataset%252C%2520we%2520develop%2520our%2520framework%2520capable%2520of%2520generating%2520multi-view%2520consistent%250Aedited%2520views%2520without%2520per-scene%2520training%252C%2520which%2520consists%2520of%2520two%2520novel%250Acomponents%253A%2520%25281%2529%2520Referring%2520multi-view%2520editor%253A%2520Enables%2520precise%252C%2520reference-driven%250Aedits%2520that%2520remain%2520coherent%2520across%2520all%2520viewpoints.%2520%25282%2529%2520Any-view-to-video%250Asynthesizer%253A%2520Leverages%2520spatial-temporal%2520priors%2520from%2520video%2520diffusion%2520to%2520perform%250Ahigh-quality%2520scene%2520completion%2520and%2520novel-view%2520generation%2520even%2520from%2520sparse%250Ainputs.%2520Through%2520extensive%2520experiments%252C%2520Tinker%2520significantly%2520reduces%2520the%2520barrier%250Ato%2520generalizable%25203D%2520content%2520creation%252C%2520achieving%2520state-of-the-art%2520performance%2520on%250Aediting%252C%2520novel-view%2520synthesis%252C%2520and%2520rendering%2520enhancement%2520tasks.%2520We%2520believe%2520that%250ATinker%2520represents%2520a%2520key%2520step%2520towards%2520truly%2520scalable%252C%2520zero-shot%25203D%2520editing.%250AProject%2520webpage%253A%2520https%253A//aim-uofa.github.io/Tinker%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tinker%3A%20Diffusion%27s%20Gift%20to%203D--Multi-View%20Consistent%20Editing%20From%0A%20%20Sparse%20Inputs%20without%20Per-Scene%20Optimization&entry.906535625=Canyu%20Zhao%20and%20Xiaoman%20Li%20and%20Tianjian%20Feng%20and%20Zhiyue%20Zhao%20and%20Hao%20Chen%20and%20Chunhua%20Shen&entry.1292438233=%20%20We%20introduce%20Tinker%2C%20a%20versatile%20framework%20for%20high-fidelity%203D%20editing%20that%0Aoperates%20in%20both%20one-shot%20and%20few-shot%20regimes%20without%20any%20per-scene%0Afinetuning.%20Unlike%20prior%20techniques%20that%20demand%20extensive%20per-scene%0Aoptimization%20to%20ensure%20multi-view%20consistency%20or%20to%20produce%20dozens%20of%0Aconsistent%20edited%20input%20views%2C%20Tinker%20delivers%20robust%2C%20multi-view%20consistent%0Aedits%20from%20as%20few%20as%20one%20or%20two%20images.%20This%20capability%20stems%20from%20repurposing%0Apretrained%20diffusion%20models%2C%20which%20unlocks%20their%20latent%203D%20awareness.%20To%20drive%0Aresearch%20in%20this%20space%2C%20we%20curate%20the%20first%20large-scale%20multi-view%20editing%0Adataset%20and%20data%20pipeline%2C%20spanning%20diverse%20scenes%20and%20styles.%20Building%20on%20this%0Adataset%2C%20we%20develop%20our%20framework%20capable%20of%20generating%20multi-view%20consistent%0Aedited%20views%20without%20per-scene%20training%2C%20which%20consists%20of%20two%20novel%0Acomponents%3A%20%281%29%20Referring%20multi-view%20editor%3A%20Enables%20precise%2C%20reference-driven%0Aedits%20that%20remain%20coherent%20across%20all%20viewpoints.%20%282%29%20Any-view-to-video%0Asynthesizer%3A%20Leverages%20spatial-temporal%20priors%20from%20video%20diffusion%20to%20perform%0Ahigh-quality%20scene%20completion%20and%20novel-view%20generation%20even%20from%20sparse%0Ainputs.%20Through%20extensive%20experiments%2C%20Tinker%20significantly%20reduces%20the%20barrier%0Ato%20generalizable%203D%20content%20creation%2C%20achieving%20state-of-the-art%20performance%20on%0Aediting%2C%20novel-view%20synthesis%2C%20and%20rendering%20enhancement%20tasks.%20We%20believe%20that%0ATinker%20represents%20a%20key%20step%20towards%20truly%20scalable%2C%20zero-shot%203D%20editing.%0AProject%20webpage%3A%20https%3A//aim-uofa.github.io/Tinker%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14811v1&entry.124074799=Read"},
{"title": "ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal", "author": "Yucong Zhang and Juan Liu and Ming Li", "abstract": "  Pre-trained foundation models have demonstrated remarkable success in vision\nand language, yet their potential for general machine signal modeling-covering\nacoustic, vibration, and other industrial sensor data-remains under-explored.\nExisting approach using sub-band-based encoders has achieved competitive\nresults but are limited by fixed input lengths, and the absence of explicit\nfrequency positional encoding. In this work, we propose a novel foundation\nmodel that integrates an advanced band-split architecture with relative\nfrequency positional embeddings, enabling precise spectral localization across\narbitrary sampling configurations. The model supports inputs of arbitrary\nlength without padding or segmentation, producing a concise embedding that\nretains both temporal and spectral fidelity. We evaluate our method on SIREN\n(https://github.com/yucongzh/SIREN), a newly introduced large-scale benchmark\nfor machine signal encoding that unifies multiple datasets, including all DCASE\ntask 2 challenges (2020-2025) and widely-used industrial signal corpora.\nExperimental results demonstrate consistent state-of-the-art performance in\nanomaly detection and fault identification, confirming the effectiveness and\ngeneralization capability of the proposed model. We open-sourced ECHO on\nhttps://github.com/yucongzh/ECHO.\n", "link": "http://arxiv.org/abs/2508.14689v1", "date": "2025-08-20", "relevancy": 2.6805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECHO%3A%20Frequency-aware%20Hierarchical%20Encoding%20for%20Variable-length%20Signal&body=Title%3A%20ECHO%3A%20Frequency-aware%20Hierarchical%20Encoding%20for%20Variable-length%20Signal%0AAuthor%3A%20Yucong%20Zhang%20and%20Juan%20Liu%20and%20Ming%20Li%0AAbstract%3A%20%20%20Pre-trained%20foundation%20models%20have%20demonstrated%20remarkable%20success%20in%20vision%0Aand%20language%2C%20yet%20their%20potential%20for%20general%20machine%20signal%20modeling-covering%0Aacoustic%2C%20vibration%2C%20and%20other%20industrial%20sensor%20data-remains%20under-explored.%0AExisting%20approach%20using%20sub-band-based%20encoders%20has%20achieved%20competitive%0Aresults%20but%20are%20limited%20by%20fixed%20input%20lengths%2C%20and%20the%20absence%20of%20explicit%0Afrequency%20positional%20encoding.%20In%20this%20work%2C%20we%20propose%20a%20novel%20foundation%0Amodel%20that%20integrates%20an%20advanced%20band-split%20architecture%20with%20relative%0Afrequency%20positional%20embeddings%2C%20enabling%20precise%20spectral%20localization%20across%0Aarbitrary%20sampling%20configurations.%20The%20model%20supports%20inputs%20of%20arbitrary%0Alength%20without%20padding%20or%20segmentation%2C%20producing%20a%20concise%20embedding%20that%0Aretains%20both%20temporal%20and%20spectral%20fidelity.%20We%20evaluate%20our%20method%20on%20SIREN%0A%28https%3A//github.com/yucongzh/SIREN%29%2C%20a%20newly%20introduced%20large-scale%20benchmark%0Afor%20machine%20signal%20encoding%20that%20unifies%20multiple%20datasets%2C%20including%20all%20DCASE%0Atask%202%20challenges%20%282020-2025%29%20and%20widely-used%20industrial%20signal%20corpora.%0AExperimental%20results%20demonstrate%20consistent%20state-of-the-art%20performance%20in%0Aanomaly%20detection%20and%20fault%20identification%2C%20confirming%20the%20effectiveness%20and%0Ageneralization%20capability%20of%20the%20proposed%20model.%20We%20open-sourced%20ECHO%20on%0Ahttps%3A//github.com/yucongzh/ECHO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECHO%253A%2520Frequency-aware%2520Hierarchical%2520Encoding%2520for%2520Variable-length%2520Signal%26entry.906535625%3DYucong%2520Zhang%2520and%2520Juan%2520Liu%2520and%2520Ming%2520Li%26entry.1292438233%3D%2520%2520Pre-trained%2520foundation%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520vision%250Aand%2520language%252C%2520yet%2520their%2520potential%2520for%2520general%2520machine%2520signal%2520modeling-covering%250Aacoustic%252C%2520vibration%252C%2520and%2520other%2520industrial%2520sensor%2520data-remains%2520under-explored.%250AExisting%2520approach%2520using%2520sub-band-based%2520encoders%2520has%2520achieved%2520competitive%250Aresults%2520but%2520are%2520limited%2520by%2520fixed%2520input%2520lengths%252C%2520and%2520the%2520absence%2520of%2520explicit%250Afrequency%2520positional%2520encoding.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520foundation%250Amodel%2520that%2520integrates%2520an%2520advanced%2520band-split%2520architecture%2520with%2520relative%250Afrequency%2520positional%2520embeddings%252C%2520enabling%2520precise%2520spectral%2520localization%2520across%250Aarbitrary%2520sampling%2520configurations.%2520The%2520model%2520supports%2520inputs%2520of%2520arbitrary%250Alength%2520without%2520padding%2520or%2520segmentation%252C%2520producing%2520a%2520concise%2520embedding%2520that%250Aretains%2520both%2520temporal%2520and%2520spectral%2520fidelity.%2520We%2520evaluate%2520our%2520method%2520on%2520SIREN%250A%2528https%253A//github.com/yucongzh/SIREN%2529%252C%2520a%2520newly%2520introduced%2520large-scale%2520benchmark%250Afor%2520machine%2520signal%2520encoding%2520that%2520unifies%2520multiple%2520datasets%252C%2520including%2520all%2520DCASE%250Atask%25202%2520challenges%2520%25282020-2025%2529%2520and%2520widely-used%2520industrial%2520signal%2520corpora.%250AExperimental%2520results%2520demonstrate%2520consistent%2520state-of-the-art%2520performance%2520in%250Aanomaly%2520detection%2520and%2520fault%2520identification%252C%2520confirming%2520the%2520effectiveness%2520and%250Ageneralization%2520capability%2520of%2520the%2520proposed%2520model.%2520We%2520open-sourced%2520ECHO%2520on%250Ahttps%253A//github.com/yucongzh/ECHO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECHO%3A%20Frequency-aware%20Hierarchical%20Encoding%20for%20Variable-length%20Signal&entry.906535625=Yucong%20Zhang%20and%20Juan%20Liu%20and%20Ming%20Li&entry.1292438233=%20%20Pre-trained%20foundation%20models%20have%20demonstrated%20remarkable%20success%20in%20vision%0Aand%20language%2C%20yet%20their%20potential%20for%20general%20machine%20signal%20modeling-covering%0Aacoustic%2C%20vibration%2C%20and%20other%20industrial%20sensor%20data-remains%20under-explored.%0AExisting%20approach%20using%20sub-band-based%20encoders%20has%20achieved%20competitive%0Aresults%20but%20are%20limited%20by%20fixed%20input%20lengths%2C%20and%20the%20absence%20of%20explicit%0Afrequency%20positional%20encoding.%20In%20this%20work%2C%20we%20propose%20a%20novel%20foundation%0Amodel%20that%20integrates%20an%20advanced%20band-split%20architecture%20with%20relative%0Afrequency%20positional%20embeddings%2C%20enabling%20precise%20spectral%20localization%20across%0Aarbitrary%20sampling%20configurations.%20The%20model%20supports%20inputs%20of%20arbitrary%0Alength%20without%20padding%20or%20segmentation%2C%20producing%20a%20concise%20embedding%20that%0Aretains%20both%20temporal%20and%20spectral%20fidelity.%20We%20evaluate%20our%20method%20on%20SIREN%0A%28https%3A//github.com/yucongzh/SIREN%29%2C%20a%20newly%20introduced%20large-scale%20benchmark%0Afor%20machine%20signal%20encoding%20that%20unifies%20multiple%20datasets%2C%20including%20all%20DCASE%0Atask%202%20challenges%20%282020-2025%29%20and%20widely-used%20industrial%20signal%20corpora.%0AExperimental%20results%20demonstrate%20consistent%20state-of-the-art%20performance%20in%0Aanomaly%20detection%20and%20fault%20identification%2C%20confirming%20the%20effectiveness%20and%0Ageneralization%20capability%20of%20the%20proposed%20model.%20We%20open-sourced%20ECHO%20on%0Ahttps%3A//github.com/yucongzh/ECHO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14689v1&entry.124074799=Read"},
{"title": "MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition", "author": "Mert Kiray and Alvaro Ritter and Nassir Navab and Benjamin Busam", "abstract": "  Contrastive learning has gained significant attention in skeleton-based\naction recognition for its ability to learn robust representations from\nunlabeled data. However, existing methods rely on a single skeleton convention,\nwhich limits their ability to generalize across datasets with diverse joint\nstructures and anatomical coverage. We propose Multi-Skeleton Contrastive\nLearning (MS-CLR), a general self-supervised framework that aligns pose\nrepresentations across multiple skeleton conventions extracted from the same\nsequence. This encourages the model to learn structural invariances and capture\ndiverse anatomical cues, resulting in more expressive and generalizable\nfeatures. To support this, we adapt the ST-GCN architecture to handle skeletons\nwith varying joint layouts and scales through a unified representation scheme.\nExperiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLR\nconsistently improves performance over strong single-skeleton contrastive\nlearning baselines. A multi-skeleton ensemble further boosts performance,\nsetting new state-of-the-art results on both datasets.\n", "link": "http://arxiv.org/abs/2508.14889v1", "date": "2025-08-20", "relevancy": 2.6343, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5286}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-CLR%3A%20Multi-Skeleton%20Contrastive%20Learning%20for%20Human%20Action%20Recognition&body=Title%3A%20MS-CLR%3A%20Multi-Skeleton%20Contrastive%20Learning%20for%20Human%20Action%20Recognition%0AAuthor%3A%20Mert%20Kiray%20and%20Alvaro%20Ritter%20and%20Nassir%20Navab%20and%20Benjamin%20Busam%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20gained%20significant%20attention%20in%20skeleton-based%0Aaction%20recognition%20for%20its%20ability%20to%20learn%20robust%20representations%20from%0Aunlabeled%20data.%20However%2C%20existing%20methods%20rely%20on%20a%20single%20skeleton%20convention%2C%0Awhich%20limits%20their%20ability%20to%20generalize%20across%20datasets%20with%20diverse%20joint%0Astructures%20and%20anatomical%20coverage.%20We%20propose%20Multi-Skeleton%20Contrastive%0ALearning%20%28MS-CLR%29%2C%20a%20general%20self-supervised%20framework%20that%20aligns%20pose%0Arepresentations%20across%20multiple%20skeleton%20conventions%20extracted%20from%20the%20same%0Asequence.%20This%20encourages%20the%20model%20to%20learn%20structural%20invariances%20and%20capture%0Adiverse%20anatomical%20cues%2C%20resulting%20in%20more%20expressive%20and%20generalizable%0Afeatures.%20To%20support%20this%2C%20we%20adapt%20the%20ST-GCN%20architecture%20to%20handle%20skeletons%0Awith%20varying%20joint%20layouts%20and%20scales%20through%20a%20unified%20representation%20scheme.%0AExperiments%20on%20the%20NTU%20RGB%2BD%2060%20and%20120%20datasets%20demonstrate%20that%20MS-CLR%0Aconsistently%20improves%20performance%20over%20strong%20single-skeleton%20contrastive%0Alearning%20baselines.%20A%20multi-skeleton%20ensemble%20further%20boosts%20performance%2C%0Asetting%20new%20state-of-the-art%20results%20on%20both%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-CLR%253A%2520Multi-Skeleton%2520Contrastive%2520Learning%2520for%2520Human%2520Action%2520Recognition%26entry.906535625%3DMert%2520Kiray%2520and%2520Alvaro%2520Ritter%2520and%2520Nassir%2520Navab%2520and%2520Benjamin%2520Busam%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520has%2520gained%2520significant%2520attention%2520in%2520skeleton-based%250Aaction%2520recognition%2520for%2520its%2520ability%2520to%2520learn%2520robust%2520representations%2520from%250Aunlabeled%2520data.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520a%2520single%2520skeleton%2520convention%252C%250Awhich%2520limits%2520their%2520ability%2520to%2520generalize%2520across%2520datasets%2520with%2520diverse%2520joint%250Astructures%2520and%2520anatomical%2520coverage.%2520We%2520propose%2520Multi-Skeleton%2520Contrastive%250ALearning%2520%2528MS-CLR%2529%252C%2520a%2520general%2520self-supervised%2520framework%2520that%2520aligns%2520pose%250Arepresentations%2520across%2520multiple%2520skeleton%2520conventions%2520extracted%2520from%2520the%2520same%250Asequence.%2520This%2520encourages%2520the%2520model%2520to%2520learn%2520structural%2520invariances%2520and%2520capture%250Adiverse%2520anatomical%2520cues%252C%2520resulting%2520in%2520more%2520expressive%2520and%2520generalizable%250Afeatures.%2520To%2520support%2520this%252C%2520we%2520adapt%2520the%2520ST-GCN%2520architecture%2520to%2520handle%2520skeletons%250Awith%2520varying%2520joint%2520layouts%2520and%2520scales%2520through%2520a%2520unified%2520representation%2520scheme.%250AExperiments%2520on%2520the%2520NTU%2520RGB%252BD%252060%2520and%2520120%2520datasets%2520demonstrate%2520that%2520MS-CLR%250Aconsistently%2520improves%2520performance%2520over%2520strong%2520single-skeleton%2520contrastive%250Alearning%2520baselines.%2520A%2520multi-skeleton%2520ensemble%2520further%2520boosts%2520performance%252C%250Asetting%2520new%2520state-of-the-art%2520results%2520on%2520both%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-CLR%3A%20Multi-Skeleton%20Contrastive%20Learning%20for%20Human%20Action%20Recognition&entry.906535625=Mert%20Kiray%20and%20Alvaro%20Ritter%20and%20Nassir%20Navab%20and%20Benjamin%20Busam&entry.1292438233=%20%20Contrastive%20learning%20has%20gained%20significant%20attention%20in%20skeleton-based%0Aaction%20recognition%20for%20its%20ability%20to%20learn%20robust%20representations%20from%0Aunlabeled%20data.%20However%2C%20existing%20methods%20rely%20on%20a%20single%20skeleton%20convention%2C%0Awhich%20limits%20their%20ability%20to%20generalize%20across%20datasets%20with%20diverse%20joint%0Astructures%20and%20anatomical%20coverage.%20We%20propose%20Multi-Skeleton%20Contrastive%0ALearning%20%28MS-CLR%29%2C%20a%20general%20self-supervised%20framework%20that%20aligns%20pose%0Arepresentations%20across%20multiple%20skeleton%20conventions%20extracted%20from%20the%20same%0Asequence.%20This%20encourages%20the%20model%20to%20learn%20structural%20invariances%20and%20capture%0Adiverse%20anatomical%20cues%2C%20resulting%20in%20more%20expressive%20and%20generalizable%0Afeatures.%20To%20support%20this%2C%20we%20adapt%20the%20ST-GCN%20architecture%20to%20handle%20skeletons%0Awith%20varying%20joint%20layouts%20and%20scales%20through%20a%20unified%20representation%20scheme.%0AExperiments%20on%20the%20NTU%20RGB%2BD%2060%20and%20120%20datasets%20demonstrate%20that%20MS-CLR%0Aconsistently%20improves%20performance%20over%20strong%20single-skeleton%20contrastive%0Alearning%20baselines.%20A%20multi-skeleton%20ensemble%20further%20boosts%20performance%2C%0Asetting%20new%20state-of-the-art%20results%20on%20both%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14889v1&entry.124074799=Read"},
{"title": "Rule-based Key-Point Extraction for MR-Guided Biomechanical Digital\n  Twins of the Spine", "author": "Robert Graf and Tanja Lerchl and Kati Nispel and Hendrik M\u00f6ller and Matan Atad and Julian McGinnis and Julius Maria Watrinet and Johannes Paetzold and Daniel Rueckert and Jan S. Kirschke", "abstract": "  Digital twins offer a powerful framework for subject-specific simulation and\nclinical decision support, yet their development often hinges on accurate,\nindividualized anatomical modeling. In this work, we present a rule-based\napproach for subpixel-accurate key-point extraction from MRI, adapted from\nprior CT-based methods. Our approach incorporates robust image alignment and\nvertebra-specific orientation estimation to generate anatomically meaningful\nlandmarks that serve as boundary conditions and force application points, like\nmuscle and ligament insertions in biomechanical models. These models enable the\nsimulation of spinal mechanics considering the subject's individual anatomy,\nand thus support the development of tailored approaches in clinical diagnostics\nand treatment planning. By leveraging MR imaging, our method is radiation-free\nand well-suited for large-scale studies and use in underrepresented\npopulations. This work contributes to the digital twin ecosystem by bridging\nthe gap between precise medical image analysis with biomechanical simulation,\nand aligns with key themes in personalized modeling for healthcare.\n", "link": "http://arxiv.org/abs/2508.14708v1", "date": "2025-08-20", "relevancy": 2.6049, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5308}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5292}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rule-based%20Key-Point%20Extraction%20for%20MR-Guided%20Biomechanical%20Digital%0A%20%20Twins%20of%20the%20Spine&body=Title%3A%20Rule-based%20Key-Point%20Extraction%20for%20MR-Guided%20Biomechanical%20Digital%0A%20%20Twins%20of%20the%20Spine%0AAuthor%3A%20Robert%20Graf%20and%20Tanja%20Lerchl%20and%20Kati%20Nispel%20and%20Hendrik%20M%C3%B6ller%20and%20Matan%20Atad%20and%20Julian%20McGinnis%20and%20Julius%20Maria%20Watrinet%20and%20Johannes%20Paetzold%20and%20Daniel%20Rueckert%20and%20Jan%20S.%20Kirschke%0AAbstract%3A%20%20%20Digital%20twins%20offer%20a%20powerful%20framework%20for%20subject-specific%20simulation%20and%0Aclinical%20decision%20support%2C%20yet%20their%20development%20often%20hinges%20on%20accurate%2C%0Aindividualized%20anatomical%20modeling.%20In%20this%20work%2C%20we%20present%20a%20rule-based%0Aapproach%20for%20subpixel-accurate%20key-point%20extraction%20from%20MRI%2C%20adapted%20from%0Aprior%20CT-based%20methods.%20Our%20approach%20incorporates%20robust%20image%20alignment%20and%0Avertebra-specific%20orientation%20estimation%20to%20generate%20anatomically%20meaningful%0Alandmarks%20that%20serve%20as%20boundary%20conditions%20and%20force%20application%20points%2C%20like%0Amuscle%20and%20ligament%20insertions%20in%20biomechanical%20models.%20These%20models%20enable%20the%0Asimulation%20of%20spinal%20mechanics%20considering%20the%20subject%27s%20individual%20anatomy%2C%0Aand%20thus%20support%20the%20development%20of%20tailored%20approaches%20in%20clinical%20diagnostics%0Aand%20treatment%20planning.%20By%20leveraging%20MR%20imaging%2C%20our%20method%20is%20radiation-free%0Aand%20well-suited%20for%20large-scale%20studies%20and%20use%20in%20underrepresented%0Apopulations.%20This%20work%20contributes%20to%20the%20digital%20twin%20ecosystem%20by%20bridging%0Athe%20gap%20between%20precise%20medical%20image%20analysis%20with%20biomechanical%20simulation%2C%0Aand%20aligns%20with%20key%20themes%20in%20personalized%20modeling%20for%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRule-based%2520Key-Point%2520Extraction%2520for%2520MR-Guided%2520Biomechanical%2520Digital%250A%2520%2520Twins%2520of%2520the%2520Spine%26entry.906535625%3DRobert%2520Graf%2520and%2520Tanja%2520Lerchl%2520and%2520Kati%2520Nispel%2520and%2520Hendrik%2520M%25C3%25B6ller%2520and%2520Matan%2520Atad%2520and%2520Julian%2520McGinnis%2520and%2520Julius%2520Maria%2520Watrinet%2520and%2520Johannes%2520Paetzold%2520and%2520Daniel%2520Rueckert%2520and%2520Jan%2520S.%2520Kirschke%26entry.1292438233%3D%2520%2520Digital%2520twins%2520offer%2520a%2520powerful%2520framework%2520for%2520subject-specific%2520simulation%2520and%250Aclinical%2520decision%2520support%252C%2520yet%2520their%2520development%2520often%2520hinges%2520on%2520accurate%252C%250Aindividualized%2520anatomical%2520modeling.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520rule-based%250Aapproach%2520for%2520subpixel-accurate%2520key-point%2520extraction%2520from%2520MRI%252C%2520adapted%2520from%250Aprior%2520CT-based%2520methods.%2520Our%2520approach%2520incorporates%2520robust%2520image%2520alignment%2520and%250Avertebra-specific%2520orientation%2520estimation%2520to%2520generate%2520anatomically%2520meaningful%250Alandmarks%2520that%2520serve%2520as%2520boundary%2520conditions%2520and%2520force%2520application%2520points%252C%2520like%250Amuscle%2520and%2520ligament%2520insertions%2520in%2520biomechanical%2520models.%2520These%2520models%2520enable%2520the%250Asimulation%2520of%2520spinal%2520mechanics%2520considering%2520the%2520subject%2527s%2520individual%2520anatomy%252C%250Aand%2520thus%2520support%2520the%2520development%2520of%2520tailored%2520approaches%2520in%2520clinical%2520diagnostics%250Aand%2520treatment%2520planning.%2520By%2520leveraging%2520MR%2520imaging%252C%2520our%2520method%2520is%2520radiation-free%250Aand%2520well-suited%2520for%2520large-scale%2520studies%2520and%2520use%2520in%2520underrepresented%250Apopulations.%2520This%2520work%2520contributes%2520to%2520the%2520digital%2520twin%2520ecosystem%2520by%2520bridging%250Athe%2520gap%2520between%2520precise%2520medical%2520image%2520analysis%2520with%2520biomechanical%2520simulation%252C%250Aand%2520aligns%2520with%2520key%2520themes%2520in%2520personalized%2520modeling%2520for%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rule-based%20Key-Point%20Extraction%20for%20MR-Guided%20Biomechanical%20Digital%0A%20%20Twins%20of%20the%20Spine&entry.906535625=Robert%20Graf%20and%20Tanja%20Lerchl%20and%20Kati%20Nispel%20and%20Hendrik%20M%C3%B6ller%20and%20Matan%20Atad%20and%20Julian%20McGinnis%20and%20Julius%20Maria%20Watrinet%20and%20Johannes%20Paetzold%20and%20Daniel%20Rueckert%20and%20Jan%20S.%20Kirschke&entry.1292438233=%20%20Digital%20twins%20offer%20a%20powerful%20framework%20for%20subject-specific%20simulation%20and%0Aclinical%20decision%20support%2C%20yet%20their%20development%20often%20hinges%20on%20accurate%2C%0Aindividualized%20anatomical%20modeling.%20In%20this%20work%2C%20we%20present%20a%20rule-based%0Aapproach%20for%20subpixel-accurate%20key-point%20extraction%20from%20MRI%2C%20adapted%20from%0Aprior%20CT-based%20methods.%20Our%20approach%20incorporates%20robust%20image%20alignment%20and%0Avertebra-specific%20orientation%20estimation%20to%20generate%20anatomically%20meaningful%0Alandmarks%20that%20serve%20as%20boundary%20conditions%20and%20force%20application%20points%2C%20like%0Amuscle%20and%20ligament%20insertions%20in%20biomechanical%20models.%20These%20models%20enable%20the%0Asimulation%20of%20spinal%20mechanics%20considering%20the%20subject%27s%20individual%20anatomy%2C%0Aand%20thus%20support%20the%20development%20of%20tailored%20approaches%20in%20clinical%20diagnostics%0Aand%20treatment%20planning.%20By%20leveraging%20MR%20imaging%2C%20our%20method%20is%20radiation-free%0Aand%20well-suited%20for%20large-scale%20studies%20and%20use%20in%20underrepresented%0Apopulations.%20This%20work%20contributes%20to%20the%20digital%20twin%20ecosystem%20by%20bridging%0Athe%20gap%20between%20precise%20medical%20image%20analysis%20with%20biomechanical%20simulation%2C%0Aand%20aligns%20with%20key%20themes%20in%20personalized%20modeling%20for%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14708v1&entry.124074799=Read"},
{"title": "GenVC: Self-Supervised Zero-Shot Voice Conversion", "author": "Zexin Cai and Henry Li Xinyuan and Ashi Garg and Leibny Paola Garc\u00eda-Perera and Kevin Duh and Sanjeev Khudanpur and Matthew Wiesner and Nicholas Andrews", "abstract": "  Most current zero-shot voice conversion methods rely on externally supervised\ncomponents, particularly speaker encoders, for training. To explore\nalternatives that eliminate this dependency, this paper introduces GenVC, a\nnovel framework that disentangles speaker identity and linguistic content from\nspeech signals in a self-supervised manner. GenVC leverages speech tokenizers\nand an autoregressive, Transformer-based language model as its backbone for\nspeech generation. This design supports large-scale training while enhancing\nboth source speaker privacy protection and target speaker cloning fidelity.\nExperimental results demonstrate that GenVC achieves notably higher speaker\nsimilarity, with naturalness on par with leading zero-shot approaches.\nMoreover, due to its autoregressive formulation, GenVC introduces flexibility\nin temporal alignment, reducing the preservation of source prosody and\nspeaker-specific traits, and making it highly effective for voice\nanonymization.\n", "link": "http://arxiv.org/abs/2502.04519v2", "date": "2025-08-20", "relevancy": 2.5982, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5479}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5223}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenVC%3A%20Self-Supervised%20Zero-Shot%20Voice%20Conversion&body=Title%3A%20GenVC%3A%20Self-Supervised%20Zero-Shot%20Voice%20Conversion%0AAuthor%3A%20Zexin%20Cai%20and%20Henry%20Li%20Xinyuan%20and%20Ashi%20Garg%20and%20Leibny%20Paola%20Garc%C3%ADa-Perera%20and%20Kevin%20Duh%20and%20Sanjeev%20Khudanpur%20and%20Matthew%20Wiesner%20and%20Nicholas%20Andrews%0AAbstract%3A%20%20%20Most%20current%20zero-shot%20voice%20conversion%20methods%20rely%20on%20externally%20supervised%0Acomponents%2C%20particularly%20speaker%20encoders%2C%20for%20training.%20To%20explore%0Aalternatives%20that%20eliminate%20this%20dependency%2C%20this%20paper%20introduces%20GenVC%2C%20a%0Anovel%20framework%20that%20disentangles%20speaker%20identity%20and%20linguistic%20content%20from%0Aspeech%20signals%20in%20a%20self-supervised%20manner.%20GenVC%20leverages%20speech%20tokenizers%0Aand%20an%20autoregressive%2C%20Transformer-based%20language%20model%20as%20its%20backbone%20for%0Aspeech%20generation.%20This%20design%20supports%20large-scale%20training%20while%20enhancing%0Aboth%20source%20speaker%20privacy%20protection%20and%20target%20speaker%20cloning%20fidelity.%0AExperimental%20results%20demonstrate%20that%20GenVC%20achieves%20notably%20higher%20speaker%0Asimilarity%2C%20with%20naturalness%20on%20par%20with%20leading%20zero-shot%20approaches.%0AMoreover%2C%20due%20to%20its%20autoregressive%20formulation%2C%20GenVC%20introduces%20flexibility%0Ain%20temporal%20alignment%2C%20reducing%20the%20preservation%20of%20source%20prosody%20and%0Aspeaker-specific%20traits%2C%20and%20making%20it%20highly%20effective%20for%20voice%0Aanonymization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenVC%253A%2520Self-Supervised%2520Zero-Shot%2520Voice%2520Conversion%26entry.906535625%3DZexin%2520Cai%2520and%2520Henry%2520Li%2520Xinyuan%2520and%2520Ashi%2520Garg%2520and%2520Leibny%2520Paola%2520Garc%25C3%25ADa-Perera%2520and%2520Kevin%2520Duh%2520and%2520Sanjeev%2520Khudanpur%2520and%2520Matthew%2520Wiesner%2520and%2520Nicholas%2520Andrews%26entry.1292438233%3D%2520%2520Most%2520current%2520zero-shot%2520voice%2520conversion%2520methods%2520rely%2520on%2520externally%2520supervised%250Acomponents%252C%2520particularly%2520speaker%2520encoders%252C%2520for%2520training.%2520To%2520explore%250Aalternatives%2520that%2520eliminate%2520this%2520dependency%252C%2520this%2520paper%2520introduces%2520GenVC%252C%2520a%250Anovel%2520framework%2520that%2520disentangles%2520speaker%2520identity%2520and%2520linguistic%2520content%2520from%250Aspeech%2520signals%2520in%2520a%2520self-supervised%2520manner.%2520GenVC%2520leverages%2520speech%2520tokenizers%250Aand%2520an%2520autoregressive%252C%2520Transformer-based%2520language%2520model%2520as%2520its%2520backbone%2520for%250Aspeech%2520generation.%2520This%2520design%2520supports%2520large-scale%2520training%2520while%2520enhancing%250Aboth%2520source%2520speaker%2520privacy%2520protection%2520and%2520target%2520speaker%2520cloning%2520fidelity.%250AExperimental%2520results%2520demonstrate%2520that%2520GenVC%2520achieves%2520notably%2520higher%2520speaker%250Asimilarity%252C%2520with%2520naturalness%2520on%2520par%2520with%2520leading%2520zero-shot%2520approaches.%250AMoreover%252C%2520due%2520to%2520its%2520autoregressive%2520formulation%252C%2520GenVC%2520introduces%2520flexibility%250Ain%2520temporal%2520alignment%252C%2520reducing%2520the%2520preservation%2520of%2520source%2520prosody%2520and%250Aspeaker-specific%2520traits%252C%2520and%2520making%2520it%2520highly%2520effective%2520for%2520voice%250Aanonymization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenVC%3A%20Self-Supervised%20Zero-Shot%20Voice%20Conversion&entry.906535625=Zexin%20Cai%20and%20Henry%20Li%20Xinyuan%20and%20Ashi%20Garg%20and%20Leibny%20Paola%20Garc%C3%ADa-Perera%20and%20Kevin%20Duh%20and%20Sanjeev%20Khudanpur%20and%20Matthew%20Wiesner%20and%20Nicholas%20Andrews&entry.1292438233=%20%20Most%20current%20zero-shot%20voice%20conversion%20methods%20rely%20on%20externally%20supervised%0Acomponents%2C%20particularly%20speaker%20encoders%2C%20for%20training.%20To%20explore%0Aalternatives%20that%20eliminate%20this%20dependency%2C%20this%20paper%20introduces%20GenVC%2C%20a%0Anovel%20framework%20that%20disentangles%20speaker%20identity%20and%20linguistic%20content%20from%0Aspeech%20signals%20in%20a%20self-supervised%20manner.%20GenVC%20leverages%20speech%20tokenizers%0Aand%20an%20autoregressive%2C%20Transformer-based%20language%20model%20as%20its%20backbone%20for%0Aspeech%20generation.%20This%20design%20supports%20large-scale%20training%20while%20enhancing%0Aboth%20source%20speaker%20privacy%20protection%20and%20target%20speaker%20cloning%20fidelity.%0AExperimental%20results%20demonstrate%20that%20GenVC%20achieves%20notably%20higher%20speaker%0Asimilarity%2C%20with%20naturalness%20on%20par%20with%20leading%20zero-shot%20approaches.%0AMoreover%2C%20due%20to%20its%20autoregressive%20formulation%2C%20GenVC%20introduces%20flexibility%0Ain%20temporal%20alignment%2C%20reducing%20the%20preservation%20of%20source%20prosody%20and%0Aspeaker-specific%20traits%2C%20and%20making%20it%20highly%20effective%20for%20voice%0Aanonymization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04519v2&entry.124074799=Read"},
{"title": "What Makes for Good Image Captions?", "author": "Delong Chen and Samuel Cahyawijaya and Etsuko Ishii and Ho Shu Chan and Yejin Bang and Pascale Fung", "abstract": "  This paper establishes a formal information-theoretic framework for image\ncaptioning, conceptualizing captions as compressed linguistic representations\nthat selectively encode semantic units in images. Our framework posits that\ngood image captions should balance three key aspects: informationally\nsufficient, minimally redundant, and readily comprehensible by humans. By\nformulating these aspects as quantitative measures with adjustable weights, our\nframework provides a flexible foundation for analyzing and optimizing image\ncaptioning systems across diverse task requirements. To demonstrate its\napplicability, we introduce the Pyramid of Captions (PoCa) method, which\ngenerates enriched captions by integrating local and global visual information.\nWe present both theoretical proof that PoCa improves caption quality under\ncertain assumptions, and empirical validation of its effectiveness across\nvarious image captioning models and datasets.\n", "link": "http://arxiv.org/abs/2405.00485v3", "date": "2025-08-20", "relevancy": 2.5806, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20for%20Good%20Image%20Captions%3F&body=Title%3A%20What%20Makes%20for%20Good%20Image%20Captions%3F%0AAuthor%3A%20Delong%20Chen%20and%20Samuel%20Cahyawijaya%20and%20Etsuko%20Ishii%20and%20Ho%20Shu%20Chan%20and%20Yejin%20Bang%20and%20Pascale%20Fung%0AAbstract%3A%20%20%20This%20paper%20establishes%20a%20formal%20information-theoretic%20framework%20for%20image%0Acaptioning%2C%20conceptualizing%20captions%20as%20compressed%20linguistic%20representations%0Athat%20selectively%20encode%20semantic%20units%20in%20images.%20Our%20framework%20posits%20that%0Agood%20image%20captions%20should%20balance%20three%20key%20aspects%3A%20informationally%0Asufficient%2C%20minimally%20redundant%2C%20and%20readily%20comprehensible%20by%20humans.%20By%0Aformulating%20these%20aspects%20as%20quantitative%20measures%20with%20adjustable%20weights%2C%20our%0Aframework%20provides%20a%20flexible%20foundation%20for%20analyzing%20and%20optimizing%20image%0Acaptioning%20systems%20across%20diverse%20task%20requirements.%20To%20demonstrate%20its%0Aapplicability%2C%20we%20introduce%20the%20Pyramid%20of%20Captions%20%28PoCa%29%20method%2C%20which%0Agenerates%20enriched%20captions%20by%20integrating%20local%20and%20global%20visual%20information.%0AWe%20present%20both%20theoretical%20proof%20that%20PoCa%20improves%20caption%20quality%20under%0Acertain%20assumptions%2C%20and%20empirical%20validation%20of%20its%20effectiveness%20across%0Avarious%20image%20captioning%20models%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520for%2520Good%2520Image%2520Captions%253F%26entry.906535625%3DDelong%2520Chen%2520and%2520Samuel%2520Cahyawijaya%2520and%2520Etsuko%2520Ishii%2520and%2520Ho%2520Shu%2520Chan%2520and%2520Yejin%2520Bang%2520and%2520Pascale%2520Fung%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520a%2520formal%2520information-theoretic%2520framework%2520for%2520image%250Acaptioning%252C%2520conceptualizing%2520captions%2520as%2520compressed%2520linguistic%2520representations%250Athat%2520selectively%2520encode%2520semantic%2520units%2520in%2520images.%2520Our%2520framework%2520posits%2520that%250Agood%2520image%2520captions%2520should%2520balance%2520three%2520key%2520aspects%253A%2520informationally%250Asufficient%252C%2520minimally%2520redundant%252C%2520and%2520readily%2520comprehensible%2520by%2520humans.%2520By%250Aformulating%2520these%2520aspects%2520as%2520quantitative%2520measures%2520with%2520adjustable%2520weights%252C%2520our%250Aframework%2520provides%2520a%2520flexible%2520foundation%2520for%2520analyzing%2520and%2520optimizing%2520image%250Acaptioning%2520systems%2520across%2520diverse%2520task%2520requirements.%2520To%2520demonstrate%2520its%250Aapplicability%252C%2520we%2520introduce%2520the%2520Pyramid%2520of%2520Captions%2520%2528PoCa%2529%2520method%252C%2520which%250Agenerates%2520enriched%2520captions%2520by%2520integrating%2520local%2520and%2520global%2520visual%2520information.%250AWe%2520present%2520both%2520theoretical%2520proof%2520that%2520PoCa%2520improves%2520caption%2520quality%2520under%250Acertain%2520assumptions%252C%2520and%2520empirical%2520validation%2520of%2520its%2520effectiveness%2520across%250Avarious%2520image%2520captioning%2520models%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20for%20Good%20Image%20Captions%3F&entry.906535625=Delong%20Chen%20and%20Samuel%20Cahyawijaya%20and%20Etsuko%20Ishii%20and%20Ho%20Shu%20Chan%20and%20Yejin%20Bang%20and%20Pascale%20Fung&entry.1292438233=%20%20This%20paper%20establishes%20a%20formal%20information-theoretic%20framework%20for%20image%0Acaptioning%2C%20conceptualizing%20captions%20as%20compressed%20linguistic%20representations%0Athat%20selectively%20encode%20semantic%20units%20in%20images.%20Our%20framework%20posits%20that%0Agood%20image%20captions%20should%20balance%20three%20key%20aspects%3A%20informationally%0Asufficient%2C%20minimally%20redundant%2C%20and%20readily%20comprehensible%20by%20humans.%20By%0Aformulating%20these%20aspects%20as%20quantitative%20measures%20with%20adjustable%20weights%2C%20our%0Aframework%20provides%20a%20flexible%20foundation%20for%20analyzing%20and%20optimizing%20image%0Acaptioning%20systems%20across%20diverse%20task%20requirements.%20To%20demonstrate%20its%0Aapplicability%2C%20we%20introduce%20the%20Pyramid%20of%20Captions%20%28PoCa%29%20method%2C%20which%0Agenerates%20enriched%20captions%20by%20integrating%20local%20and%20global%20visual%20information.%0AWe%20present%20both%20theoretical%20proof%20that%20PoCa%20improves%20caption%20quality%20under%0Acertain%20assumptions%2C%20and%20empirical%20validation%20of%20its%20effectiveness%20across%0Avarious%20image%20captioning%20models%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00485v3&entry.124074799=Read"},
{"title": "Context Steering: A New Paradigm for Compression-based Embeddings by\n  Synthesizing Relevant Information Features", "author": "Guillermo Sarasa Dur\u00e1n and Ana Granados Fontecha and Francisco de Borja Rodr\u00edguez Ort\u00edz", "abstract": "  Compression-based distances (CD) offer a flexible and domain-agnostic means\nof measuring similarity by identifying implicit information through\nredundancies between data objects. However, as similarity features are derived\nfrom the data, rather than defined as an input, it often proves difficult to\nalign with the task at hand, particularly in complex clustering or\nclassification settings. To address this issue, we introduce \"context\nsteering,\" a novel methodology that actively guides the feature-shaping\nprocess. Instead of passively accepting the emergent data structure (typically\na hierarchy derived from clustering CDs), our approach \"steers\" the process by\nsystematically analyzing how each object influences the relational context\nwithin a clustering framework. This process generates a custom-tailored\nembedding that isolates and amplifies class-distinctive information. We\nvalidate the capabilities of this strategy using Normalized Compression\nDistance (NCD) and Relative Compression Distance (NRC) with common hierarchical\nclustering, providing an effective alternative to common transductive methods.\nExperimental results across heterogeneous datasets-from text to real-world\naudio-validate the robustness and generality of context steering, marking a\nfundamental shift in their application: from merely discovering inherent data\nstructures to actively shaping a feature space tailored to a specific\nobjective.\n", "link": "http://arxiv.org/abs/2508.14780v1", "date": "2025-08-20", "relevancy": 2.561, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20Steering%3A%20A%20New%20Paradigm%20for%20Compression-based%20Embeddings%20by%0A%20%20Synthesizing%20Relevant%20Information%20Features&body=Title%3A%20Context%20Steering%3A%20A%20New%20Paradigm%20for%20Compression-based%20Embeddings%20by%0A%20%20Synthesizing%20Relevant%20Information%20Features%0AAuthor%3A%20Guillermo%20Sarasa%20Dur%C3%A1n%20and%20Ana%20Granados%20Fontecha%20and%20Francisco%20de%20Borja%20Rodr%C3%ADguez%20Ort%C3%ADz%0AAbstract%3A%20%20%20Compression-based%20distances%20%28CD%29%20offer%20a%20flexible%20and%20domain-agnostic%20means%0Aof%20measuring%20similarity%20by%20identifying%20implicit%20information%20through%0Aredundancies%20between%20data%20objects.%20However%2C%20as%20similarity%20features%20are%20derived%0Afrom%20the%20data%2C%20rather%20than%20defined%20as%20an%20input%2C%20it%20often%20proves%20difficult%20to%0Aalign%20with%20the%20task%20at%20hand%2C%20particularly%20in%20complex%20clustering%20or%0Aclassification%20settings.%20To%20address%20this%20issue%2C%20we%20introduce%20%22context%0Asteering%2C%22%20a%20novel%20methodology%20that%20actively%20guides%20the%20feature-shaping%0Aprocess.%20Instead%20of%20passively%20accepting%20the%20emergent%20data%20structure%20%28typically%0Aa%20hierarchy%20derived%20from%20clustering%20CDs%29%2C%20our%20approach%20%22steers%22%20the%20process%20by%0Asystematically%20analyzing%20how%20each%20object%20influences%20the%20relational%20context%0Awithin%20a%20clustering%20framework.%20This%20process%20generates%20a%20custom-tailored%0Aembedding%20that%20isolates%20and%20amplifies%20class-distinctive%20information.%20We%0Avalidate%20the%20capabilities%20of%20this%20strategy%20using%20Normalized%20Compression%0ADistance%20%28NCD%29%20and%20Relative%20Compression%20Distance%20%28NRC%29%20with%20common%20hierarchical%0Aclustering%2C%20providing%20an%20effective%20alternative%20to%20common%20transductive%20methods.%0AExperimental%20results%20across%20heterogeneous%20datasets-from%20text%20to%20real-world%0Aaudio-validate%20the%20robustness%20and%20generality%20of%20context%20steering%2C%20marking%20a%0Afundamental%20shift%20in%20their%20application%3A%20from%20merely%20discovering%20inherent%20data%0Astructures%20to%20actively%20shaping%20a%20feature%20space%20tailored%20to%20a%20specific%0Aobjective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520Steering%253A%2520A%2520New%2520Paradigm%2520for%2520Compression-based%2520Embeddings%2520by%250A%2520%2520Synthesizing%2520Relevant%2520Information%2520Features%26entry.906535625%3DGuillermo%2520Sarasa%2520Dur%25C3%25A1n%2520and%2520Ana%2520Granados%2520Fontecha%2520and%2520Francisco%2520de%2520Borja%2520Rodr%25C3%25ADguez%2520Ort%25C3%25ADz%26entry.1292438233%3D%2520%2520Compression-based%2520distances%2520%2528CD%2529%2520offer%2520a%2520flexible%2520and%2520domain-agnostic%2520means%250Aof%2520measuring%2520similarity%2520by%2520identifying%2520implicit%2520information%2520through%250Aredundancies%2520between%2520data%2520objects.%2520However%252C%2520as%2520similarity%2520features%2520are%2520derived%250Afrom%2520the%2520data%252C%2520rather%2520than%2520defined%2520as%2520an%2520input%252C%2520it%2520often%2520proves%2520difficult%2520to%250Aalign%2520with%2520the%2520task%2520at%2520hand%252C%2520particularly%2520in%2520complex%2520clustering%2520or%250Aclassification%2520settings.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520%2522context%250Asteering%252C%2522%2520a%2520novel%2520methodology%2520that%2520actively%2520guides%2520the%2520feature-shaping%250Aprocess.%2520Instead%2520of%2520passively%2520accepting%2520the%2520emergent%2520data%2520structure%2520%2528typically%250Aa%2520hierarchy%2520derived%2520from%2520clustering%2520CDs%2529%252C%2520our%2520approach%2520%2522steers%2522%2520the%2520process%2520by%250Asystematically%2520analyzing%2520how%2520each%2520object%2520influences%2520the%2520relational%2520context%250Awithin%2520a%2520clustering%2520framework.%2520This%2520process%2520generates%2520a%2520custom-tailored%250Aembedding%2520that%2520isolates%2520and%2520amplifies%2520class-distinctive%2520information.%2520We%250Avalidate%2520the%2520capabilities%2520of%2520this%2520strategy%2520using%2520Normalized%2520Compression%250ADistance%2520%2528NCD%2529%2520and%2520Relative%2520Compression%2520Distance%2520%2528NRC%2529%2520with%2520common%2520hierarchical%250Aclustering%252C%2520providing%2520an%2520effective%2520alternative%2520to%2520common%2520transductive%2520methods.%250AExperimental%2520results%2520across%2520heterogeneous%2520datasets-from%2520text%2520to%2520real-world%250Aaudio-validate%2520the%2520robustness%2520and%2520generality%2520of%2520context%2520steering%252C%2520marking%2520a%250Afundamental%2520shift%2520in%2520their%2520application%253A%2520from%2520merely%2520discovering%2520inherent%2520data%250Astructures%2520to%2520actively%2520shaping%2520a%2520feature%2520space%2520tailored%2520to%2520a%2520specific%250Aobjective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20Steering%3A%20A%20New%20Paradigm%20for%20Compression-based%20Embeddings%20by%0A%20%20Synthesizing%20Relevant%20Information%20Features&entry.906535625=Guillermo%20Sarasa%20Dur%C3%A1n%20and%20Ana%20Granados%20Fontecha%20and%20Francisco%20de%20Borja%20Rodr%C3%ADguez%20Ort%C3%ADz&entry.1292438233=%20%20Compression-based%20distances%20%28CD%29%20offer%20a%20flexible%20and%20domain-agnostic%20means%0Aof%20measuring%20similarity%20by%20identifying%20implicit%20information%20through%0Aredundancies%20between%20data%20objects.%20However%2C%20as%20similarity%20features%20are%20derived%0Afrom%20the%20data%2C%20rather%20than%20defined%20as%20an%20input%2C%20it%20often%20proves%20difficult%20to%0Aalign%20with%20the%20task%20at%20hand%2C%20particularly%20in%20complex%20clustering%20or%0Aclassification%20settings.%20To%20address%20this%20issue%2C%20we%20introduce%20%22context%0Asteering%2C%22%20a%20novel%20methodology%20that%20actively%20guides%20the%20feature-shaping%0Aprocess.%20Instead%20of%20passively%20accepting%20the%20emergent%20data%20structure%20%28typically%0Aa%20hierarchy%20derived%20from%20clustering%20CDs%29%2C%20our%20approach%20%22steers%22%20the%20process%20by%0Asystematically%20analyzing%20how%20each%20object%20influences%20the%20relational%20context%0Awithin%20a%20clustering%20framework.%20This%20process%20generates%20a%20custom-tailored%0Aembedding%20that%20isolates%20and%20amplifies%20class-distinctive%20information.%20We%0Avalidate%20the%20capabilities%20of%20this%20strategy%20using%20Normalized%20Compression%0ADistance%20%28NCD%29%20and%20Relative%20Compression%20Distance%20%28NRC%29%20with%20common%20hierarchical%0Aclustering%2C%20providing%20an%20effective%20alternative%20to%20common%20transductive%20methods.%0AExperimental%20results%20across%20heterogeneous%20datasets-from%20text%20to%20real-world%0Aaudio-validate%20the%20robustness%20and%20generality%20of%20context%20steering%2C%20marking%20a%0Afundamental%20shift%20in%20their%20application%3A%20from%20merely%20discovering%20inherent%20data%0Astructures%20to%20actively%20shaping%20a%20feature%20space%20tailored%20to%20a%20specific%0Aobjective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14780v1&entry.124074799=Read"},
{"title": "Enhancing Contrastive Link Prediction With Edge Balancing Augmentation", "author": "Chen-Hao Chang and Hui-Ju Hung and Chia-Hsun Lu and Chih-Ya Shen", "abstract": "  Link prediction is one of the most fundamental tasks in graph mining, which\nmotivates the recent studies of leveraging contrastive learning to enhance the\nperformance. However, we observe two major weaknesses of these studies: i) the\nlack of theoretical analysis for contrastive learning on link prediction, and\nii) inadequate consideration of node degrees in contrastive learning. To\naddress the above weaknesses, we provide the first formal theoretical analysis\nfor contrastive learning on link prediction, where our analysis results can\ngeneralize to the autoencoder-based link prediction models with contrastive\nlearning. Motivated by our analysis results, we propose a new graph\naugmentation approach, Edge Balancing Augmentation (EBA), which adjusts the\nnode degrees in the graph as the augmentation. We then propose a new approach,\nnamed Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA),\nthat integrates the proposed EBA and the proposed new contrastive losses to\nimprove the model performance. We conduct experiments on 8 benchmark datasets.\nThe results demonstrate that our proposed CoEBA significantly outperforms the\nother state-of-the-art link prediction models.\n", "link": "http://arxiv.org/abs/2508.14808v1", "date": "2025-08-20", "relevancy": 2.5576, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5324}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.51}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Contrastive%20Link%20Prediction%20With%20Edge%20Balancing%20Augmentation&body=Title%3A%20Enhancing%20Contrastive%20Link%20Prediction%20With%20Edge%20Balancing%20Augmentation%0AAuthor%3A%20Chen-Hao%20Chang%20and%20Hui-Ju%20Hung%20and%20Chia-Hsun%20Lu%20and%20Chih-Ya%20Shen%0AAbstract%3A%20%20%20Link%20prediction%20is%20one%20of%20the%20most%20fundamental%20tasks%20in%20graph%20mining%2C%20which%0Amotivates%20the%20recent%20studies%20of%20leveraging%20contrastive%20learning%20to%20enhance%20the%0Aperformance.%20However%2C%20we%20observe%20two%20major%20weaknesses%20of%20these%20studies%3A%20i%29%20the%0Alack%20of%20theoretical%20analysis%20for%20contrastive%20learning%20on%20link%20prediction%2C%20and%0Aii%29%20inadequate%20consideration%20of%20node%20degrees%20in%20contrastive%20learning.%20To%0Aaddress%20the%20above%20weaknesses%2C%20we%20provide%20the%20first%20formal%20theoretical%20analysis%0Afor%20contrastive%20learning%20on%20link%20prediction%2C%20where%20our%20analysis%20results%20can%0Ageneralize%20to%20the%20autoencoder-based%20link%20prediction%20models%20with%20contrastive%0Alearning.%20Motivated%20by%20our%20analysis%20results%2C%20we%20propose%20a%20new%20graph%0Aaugmentation%20approach%2C%20Edge%20Balancing%20Augmentation%20%28EBA%29%2C%20which%20adjusts%20the%0Anode%20degrees%20in%20the%20graph%20as%20the%20augmentation.%20We%20then%20propose%20a%20new%20approach%2C%0Anamed%20Contrastive%20Link%20Prediction%20with%20Edge%20Balancing%20Augmentation%20%28CoEBA%29%2C%0Athat%20integrates%20the%20proposed%20EBA%20and%20the%20proposed%20new%20contrastive%20losses%20to%0Aimprove%20the%20model%20performance.%20We%20conduct%20experiments%20on%208%20benchmark%20datasets.%0AThe%20results%20demonstrate%20that%20our%20proposed%20CoEBA%20significantly%20outperforms%20the%0Aother%20state-of-the-art%20link%20prediction%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Contrastive%2520Link%2520Prediction%2520With%2520Edge%2520Balancing%2520Augmentation%26entry.906535625%3DChen-Hao%2520Chang%2520and%2520Hui-Ju%2520Hung%2520and%2520Chia-Hsun%2520Lu%2520and%2520Chih-Ya%2520Shen%26entry.1292438233%3D%2520%2520Link%2520prediction%2520is%2520one%2520of%2520the%2520most%2520fundamental%2520tasks%2520in%2520graph%2520mining%252C%2520which%250Amotivates%2520the%2520recent%2520studies%2520of%2520leveraging%2520contrastive%2520learning%2520to%2520enhance%2520the%250Aperformance.%2520However%252C%2520we%2520observe%2520two%2520major%2520weaknesses%2520of%2520these%2520studies%253A%2520i%2529%2520the%250Alack%2520of%2520theoretical%2520analysis%2520for%2520contrastive%2520learning%2520on%2520link%2520prediction%252C%2520and%250Aii%2529%2520inadequate%2520consideration%2520of%2520node%2520degrees%2520in%2520contrastive%2520learning.%2520To%250Aaddress%2520the%2520above%2520weaknesses%252C%2520we%2520provide%2520the%2520first%2520formal%2520theoretical%2520analysis%250Afor%2520contrastive%2520learning%2520on%2520link%2520prediction%252C%2520where%2520our%2520analysis%2520results%2520can%250Ageneralize%2520to%2520the%2520autoencoder-based%2520link%2520prediction%2520models%2520with%2520contrastive%250Alearning.%2520Motivated%2520by%2520our%2520analysis%2520results%252C%2520we%2520propose%2520a%2520new%2520graph%250Aaugmentation%2520approach%252C%2520Edge%2520Balancing%2520Augmentation%2520%2528EBA%2529%252C%2520which%2520adjusts%2520the%250Anode%2520degrees%2520in%2520the%2520graph%2520as%2520the%2520augmentation.%2520We%2520then%2520propose%2520a%2520new%2520approach%252C%250Anamed%2520Contrastive%2520Link%2520Prediction%2520with%2520Edge%2520Balancing%2520Augmentation%2520%2528CoEBA%2529%252C%250Athat%2520integrates%2520the%2520proposed%2520EBA%2520and%2520the%2520proposed%2520new%2520contrastive%2520losses%2520to%250Aimprove%2520the%2520model%2520performance.%2520We%2520conduct%2520experiments%2520on%25208%2520benchmark%2520datasets.%250AThe%2520results%2520demonstrate%2520that%2520our%2520proposed%2520CoEBA%2520significantly%2520outperforms%2520the%250Aother%2520state-of-the-art%2520link%2520prediction%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Contrastive%20Link%20Prediction%20With%20Edge%20Balancing%20Augmentation&entry.906535625=Chen-Hao%20Chang%20and%20Hui-Ju%20Hung%20and%20Chia-Hsun%20Lu%20and%20Chih-Ya%20Shen&entry.1292438233=%20%20Link%20prediction%20is%20one%20of%20the%20most%20fundamental%20tasks%20in%20graph%20mining%2C%20which%0Amotivates%20the%20recent%20studies%20of%20leveraging%20contrastive%20learning%20to%20enhance%20the%0Aperformance.%20However%2C%20we%20observe%20two%20major%20weaknesses%20of%20these%20studies%3A%20i%29%20the%0Alack%20of%20theoretical%20analysis%20for%20contrastive%20learning%20on%20link%20prediction%2C%20and%0Aii%29%20inadequate%20consideration%20of%20node%20degrees%20in%20contrastive%20learning.%20To%0Aaddress%20the%20above%20weaknesses%2C%20we%20provide%20the%20first%20formal%20theoretical%20analysis%0Afor%20contrastive%20learning%20on%20link%20prediction%2C%20where%20our%20analysis%20results%20can%0Ageneralize%20to%20the%20autoencoder-based%20link%20prediction%20models%20with%20contrastive%0Alearning.%20Motivated%20by%20our%20analysis%20results%2C%20we%20propose%20a%20new%20graph%0Aaugmentation%20approach%2C%20Edge%20Balancing%20Augmentation%20%28EBA%29%2C%20which%20adjusts%20the%0Anode%20degrees%20in%20the%20graph%20as%20the%20augmentation.%20We%20then%20propose%20a%20new%20approach%2C%0Anamed%20Contrastive%20Link%20Prediction%20with%20Edge%20Balancing%20Augmentation%20%28CoEBA%29%2C%0Athat%20integrates%20the%20proposed%20EBA%20and%20the%20proposed%20new%20contrastive%20losses%20to%0Aimprove%20the%20model%20performance.%20We%20conduct%20experiments%20on%208%20benchmark%20datasets.%0AThe%20results%20demonstrate%20that%20our%20proposed%20CoEBA%20significantly%20outperforms%20the%0Aother%20state-of-the-art%20link%20prediction%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14808v1&entry.124074799=Read"},
{"title": "Incremental Object Detection with Prompt-based Methods", "author": "Matthias Neuwirth-Trapp and Maarten Bieshaar and Danda Pani Paudel and Luc Van Gool", "abstract": "  Visual prompt-based methods have seen growing interest in incremental\nlearning (IL) for image classification. These approaches learn additional\nembedding vectors while keeping the model frozen, making them efficient to\ntrain. However, no prior work has applied such methods to incremental object\ndetection (IOD), leaving their generalizability unclear. In this paper, we\nanalyze three different prompt-based methods under a complex domain-incremental\nlearning setting. We additionally provide a wide range of reference baselines\nfor comparison. Empirically, we show that the prompt-based approaches we tested\nunderperform in this setting. However, a strong yet practical method, combining\nvisual prompts with replaying a small portion of previous data, achieves the\nbest results. Together with additional experiments on prompt length and\ninitialization, our findings offer valuable insights for advancing prompt-based\nIL in IOD.\n", "link": "http://arxiv.org/abs/2508.14599v1", "date": "2025-08-20", "relevancy": 2.5536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Object%20Detection%20with%20Prompt-based%20Methods&body=Title%3A%20Incremental%20Object%20Detection%20with%20Prompt-based%20Methods%0AAuthor%3A%20Matthias%20Neuwirth-Trapp%20and%20Maarten%20Bieshaar%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Visual%20prompt-based%20methods%20have%20seen%20growing%20interest%20in%20incremental%0Alearning%20%28IL%29%20for%20image%20classification.%20These%20approaches%20learn%20additional%0Aembedding%20vectors%20while%20keeping%20the%20model%20frozen%2C%20making%20them%20efficient%20to%0Atrain.%20However%2C%20no%20prior%20work%20has%20applied%20such%20methods%20to%20incremental%20object%0Adetection%20%28IOD%29%2C%20leaving%20their%20generalizability%20unclear.%20In%20this%20paper%2C%20we%0Aanalyze%20three%20different%20prompt-based%20methods%20under%20a%20complex%20domain-incremental%0Alearning%20setting.%20We%20additionally%20provide%20a%20wide%20range%20of%20reference%20baselines%0Afor%20comparison.%20Empirically%2C%20we%20show%20that%20the%20prompt-based%20approaches%20we%20tested%0Aunderperform%20in%20this%20setting.%20However%2C%20a%20strong%20yet%20practical%20method%2C%20combining%0Avisual%20prompts%20with%20replaying%20a%20small%20portion%20of%20previous%20data%2C%20achieves%20the%0Abest%20results.%20Together%20with%20additional%20experiments%20on%20prompt%20length%20and%0Ainitialization%2C%20our%20findings%20offer%20valuable%20insights%20for%20advancing%20prompt-based%0AIL%20in%20IOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Object%2520Detection%2520with%2520Prompt-based%2520Methods%26entry.906535625%3DMatthias%2520Neuwirth-Trapp%2520and%2520Maarten%2520Bieshaar%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Visual%2520prompt-based%2520methods%2520have%2520seen%2520growing%2520interest%2520in%2520incremental%250Alearning%2520%2528IL%2529%2520for%2520image%2520classification.%2520These%2520approaches%2520learn%2520additional%250Aembedding%2520vectors%2520while%2520keeping%2520the%2520model%2520frozen%252C%2520making%2520them%2520efficient%2520to%250Atrain.%2520However%252C%2520no%2520prior%2520work%2520has%2520applied%2520such%2520methods%2520to%2520incremental%2520object%250Adetection%2520%2528IOD%2529%252C%2520leaving%2520their%2520generalizability%2520unclear.%2520In%2520this%2520paper%252C%2520we%250Aanalyze%2520three%2520different%2520prompt-based%2520methods%2520under%2520a%2520complex%2520domain-incremental%250Alearning%2520setting.%2520We%2520additionally%2520provide%2520a%2520wide%2520range%2520of%2520reference%2520baselines%250Afor%2520comparison.%2520Empirically%252C%2520we%2520show%2520that%2520the%2520prompt-based%2520approaches%2520we%2520tested%250Aunderperform%2520in%2520this%2520setting.%2520However%252C%2520a%2520strong%2520yet%2520practical%2520method%252C%2520combining%250Avisual%2520prompts%2520with%2520replaying%2520a%2520small%2520portion%2520of%2520previous%2520data%252C%2520achieves%2520the%250Abest%2520results.%2520Together%2520with%2520additional%2520experiments%2520on%2520prompt%2520length%2520and%250Ainitialization%252C%2520our%2520findings%2520offer%2520valuable%2520insights%2520for%2520advancing%2520prompt-based%250AIL%2520in%2520IOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Object%20Detection%20with%20Prompt-based%20Methods&entry.906535625=Matthias%20Neuwirth-Trapp%20and%20Maarten%20Bieshaar%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Visual%20prompt-based%20methods%20have%20seen%20growing%20interest%20in%20incremental%0Alearning%20%28IL%29%20for%20image%20classification.%20These%20approaches%20learn%20additional%0Aembedding%20vectors%20while%20keeping%20the%20model%20frozen%2C%20making%20them%20efficient%20to%0Atrain.%20However%2C%20no%20prior%20work%20has%20applied%20such%20methods%20to%20incremental%20object%0Adetection%20%28IOD%29%2C%20leaving%20their%20generalizability%20unclear.%20In%20this%20paper%2C%20we%0Aanalyze%20three%20different%20prompt-based%20methods%20under%20a%20complex%20domain-incremental%0Alearning%20setting.%20We%20additionally%20provide%20a%20wide%20range%20of%20reference%20baselines%0Afor%20comparison.%20Empirically%2C%20we%20show%20that%20the%20prompt-based%20approaches%20we%20tested%0Aunderperform%20in%20this%20setting.%20However%2C%20a%20strong%20yet%20practical%20method%2C%20combining%0Avisual%20prompts%20with%20replaying%20a%20small%20portion%20of%20previous%20data%2C%20achieves%20the%0Abest%20results.%20Together%20with%20additional%20experiments%20on%20prompt%20length%20and%0Ainitialization%2C%20our%20findings%20offer%20valuable%20insights%20for%20advancing%20prompt-based%0AIL%20in%20IOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14599v1&entry.124074799=Read"},
{"title": "STEM: Efficient Relative Capability Evaluation of LLMs through\n  Structured Transition Samples", "author": "Haiquan Hu and Jiazhi Jiang and Shiyou Xu and Ruhan Zeng and Tian Wang", "abstract": "  Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.\n", "link": "http://arxiv.org/abs/2508.12096v2", "date": "2025-08-20", "relevancy": 2.543, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STEM%3A%20Efficient%20Relative%20Capability%20Evaluation%20of%20LLMs%20through%0A%20%20Structured%20Transition%20Samples&body=Title%3A%20STEM%3A%20Efficient%20Relative%20Capability%20Evaluation%20of%20LLMs%20through%0A%20%20Structured%20Transition%20Samples%0AAuthor%3A%20Haiquan%20Hu%20and%20Jiazhi%20Jiang%20and%20Shiyou%20Xu%20and%20Ruhan%20Zeng%20and%20Tian%20Wang%0AAbstract%3A%20%20%20Evaluating%20large%20language%20models%20%28LLMs%29%20has%20become%20increasingly%20challenging%0Aas%20model%20capabilities%20advance%20rapidly.%20While%20recent%20models%20often%20achieve%20higher%0Ascores%20on%20standard%20benchmarks%2C%20these%20improvements%20do%20not%20consistently%20reflect%0Aenhanced%20real-world%20reasoning%20capabilities.%20Moreover%2C%20widespread%20overfitting%20to%0Apublic%20benchmarks%20and%20the%20high%20computational%20cost%20of%20full%20evaluations%20have%20made%0Ait%20both%20expensive%20and%20less%20effective%20to%20distinguish%20meaningful%20differences%0Abetween%20models.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20%5Ctextbf%7BS%7Dtructured%0A%5Ctextbf%7BT%7Dransition%20%5Ctextbf%7BE%7Dvaluation%20%5Ctextbf%7BM%7Dethod%20%28STEM%29%2C%20a%20lightweight%0Aand%20interpretable%20evaluation%20framework%20for%20efficiently%20estimating%20the%20relative%0Acapabilities%20of%20LLMs.%20STEM%20identifies%20%5Ctextit%7Bsignificant%20transition%20samples%7D%0A%28STS%29%20by%20analyzing%20consistent%20performance%20transitions%20among%20LLMs%20of%20the%20same%0Aarchitecture%20but%20varying%20parameter%20scales.%20These%20samples%20enable%20STEM%20to%0Aeffectively%20estimate%20the%20capability%20position%20of%20an%20unknown%20model.%20Qwen3%20model%0Afamily%20is%20applied%20to%20construct%20the%20STS%20pool%20on%20six%20diverse%20and%20representative%0Abenchmarks.%20To%20assess%20generalizability.%20Experimental%20results%20indicate%20that%20STEM%0Areliably%20captures%20performance%20trends%2C%20aligns%20with%20ground-truth%20rankings%20of%0Amodel%20capability.%20These%20findings%20highlight%20STEM%20as%20a%20practical%20and%20scalable%0Amethod%20for%20fine-grained%2C%20architecture-agnostic%20evaluation%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTEM%253A%2520Efficient%2520Relative%2520Capability%2520Evaluation%2520of%2520LLMs%2520through%250A%2520%2520Structured%2520Transition%2520Samples%26entry.906535625%3DHaiquan%2520Hu%2520and%2520Jiazhi%2520Jiang%2520and%2520Shiyou%2520Xu%2520and%2520Ruhan%2520Zeng%2520and%2520Tian%2520Wang%26entry.1292438233%3D%2520%2520Evaluating%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520become%2520increasingly%2520challenging%250Aas%2520model%2520capabilities%2520advance%2520rapidly.%2520While%2520recent%2520models%2520often%2520achieve%2520higher%250Ascores%2520on%2520standard%2520benchmarks%252C%2520these%2520improvements%2520do%2520not%2520consistently%2520reflect%250Aenhanced%2520real-world%2520reasoning%2520capabilities.%2520Moreover%252C%2520widespread%2520overfitting%2520to%250Apublic%2520benchmarks%2520and%2520the%2520high%2520computational%2520cost%2520of%2520full%2520evaluations%2520have%2520made%250Ait%2520both%2520expensive%2520and%2520less%2520effective%2520to%2520distinguish%2520meaningful%2520differences%250Abetween%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520%255Ctextbf%257BS%257Dtructured%250A%255Ctextbf%257BT%257Dransition%2520%255Ctextbf%257BE%257Dvaluation%2520%255Ctextbf%257BM%257Dethod%2520%2528STEM%2529%252C%2520a%2520lightweight%250Aand%2520interpretable%2520evaluation%2520framework%2520for%2520efficiently%2520estimating%2520the%2520relative%250Acapabilities%2520of%2520LLMs.%2520STEM%2520identifies%2520%255Ctextit%257Bsignificant%2520transition%2520samples%257D%250A%2528STS%2529%2520by%2520analyzing%2520consistent%2520performance%2520transitions%2520among%2520LLMs%2520of%2520the%2520same%250Aarchitecture%2520but%2520varying%2520parameter%2520scales.%2520These%2520samples%2520enable%2520STEM%2520to%250Aeffectively%2520estimate%2520the%2520capability%2520position%2520of%2520an%2520unknown%2520model.%2520Qwen3%2520model%250Afamily%2520is%2520applied%2520to%2520construct%2520the%2520STS%2520pool%2520on%2520six%2520diverse%2520and%2520representative%250Abenchmarks.%2520To%2520assess%2520generalizability.%2520Experimental%2520results%2520indicate%2520that%2520STEM%250Areliably%2520captures%2520performance%2520trends%252C%2520aligns%2520with%2520ground-truth%2520rankings%2520of%250Amodel%2520capability.%2520These%2520findings%2520highlight%2520STEM%2520as%2520a%2520practical%2520and%2520scalable%250Amethod%2520for%2520fine-grained%252C%2520architecture-agnostic%2520evaluation%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STEM%3A%20Efficient%20Relative%20Capability%20Evaluation%20of%20LLMs%20through%0A%20%20Structured%20Transition%20Samples&entry.906535625=Haiquan%20Hu%20and%20Jiazhi%20Jiang%20and%20Shiyou%20Xu%20and%20Ruhan%20Zeng%20and%20Tian%20Wang&entry.1292438233=%20%20Evaluating%20large%20language%20models%20%28LLMs%29%20has%20become%20increasingly%20challenging%0Aas%20model%20capabilities%20advance%20rapidly.%20While%20recent%20models%20often%20achieve%20higher%0Ascores%20on%20standard%20benchmarks%2C%20these%20improvements%20do%20not%20consistently%20reflect%0Aenhanced%20real-world%20reasoning%20capabilities.%20Moreover%2C%20widespread%20overfitting%20to%0Apublic%20benchmarks%20and%20the%20high%20computational%20cost%20of%20full%20evaluations%20have%20made%0Ait%20both%20expensive%20and%20less%20effective%20to%20distinguish%20meaningful%20differences%0Abetween%20models.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20%5Ctextbf%7BS%7Dtructured%0A%5Ctextbf%7BT%7Dransition%20%5Ctextbf%7BE%7Dvaluation%20%5Ctextbf%7BM%7Dethod%20%28STEM%29%2C%20a%20lightweight%0Aand%20interpretable%20evaluation%20framework%20for%20efficiently%20estimating%20the%20relative%0Acapabilities%20of%20LLMs.%20STEM%20identifies%20%5Ctextit%7Bsignificant%20transition%20samples%7D%0A%28STS%29%20by%20analyzing%20consistent%20performance%20transitions%20among%20LLMs%20of%20the%20same%0Aarchitecture%20but%20varying%20parameter%20scales.%20These%20samples%20enable%20STEM%20to%0Aeffectively%20estimate%20the%20capability%20position%20of%20an%20unknown%20model.%20Qwen3%20model%0Afamily%20is%20applied%20to%20construct%20the%20STS%20pool%20on%20six%20diverse%20and%20representative%0Abenchmarks.%20To%20assess%20generalizability.%20Experimental%20results%20indicate%20that%20STEM%0Areliably%20captures%20performance%20trends%2C%20aligns%20with%20ground-truth%20rankings%20of%0Amodel%20capability.%20These%20findings%20highlight%20STEM%20as%20a%20practical%20and%20scalable%0Amethod%20for%20fine-grained%2C%20architecture-agnostic%20evaluation%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12096v2&entry.124074799=Read"},
{"title": "TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation\n  and Analysis", "author": "Zhengpeng Feng and Clement Atzberger and Sadiq Jaffer and Jovana Knezevic and Silja Sormunen and Robin Young and Madeline C Lisaius and Markus Immitzer and James Ball and David A. Coomes and Anil Madhavapeddy and Andrew Blake and Srinivasan Keshav", "abstract": "  Satellite remote sensing enables a wide range of downstream applications,\nincluding habitat mapping, carbon accounting, and strategies for conservation\nand sustainable land use. However, satellite time series are voluminous and\noften corrupted, making them challenging to use. We present TESSERA, an open,\nglobal, land-oriented remote sensing foundation model that uses self-supervised\nlearning to generate `ready-to-use' embeddings at 10~m scale from pixel-level\nsatellite time-series data. TESSERA uses two encoders to combine optical data\nwith synthetic aperture radar backscatter coefficients at 10~m resolution to\ncreate embeddings that are fused with a multilayer perceptron to create annual\nglobal embedding maps. We compare our work with state-of-the-art task-specific\nmodels and other foundation models in five diverse downstream tasks and find\nthat TESSERA closely matches or outperforms these baselines. We believe that\nTESSERA's ease of use, state-of-the-art performance, openness, and computation-\nand labelled data-efficiency will prove transformative in a wide range of\necological applications.\n", "link": "http://arxiv.org/abs/2506.20380v4", "date": "2025-08-20", "relevancy": 2.5101, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TESSERA%3A%20Temporal%20Embeddings%20of%20Surface%20Spectra%20for%20Earth%20Representation%0A%20%20and%20Analysis&body=Title%3A%20TESSERA%3A%20Temporal%20Embeddings%20of%20Surface%20Spectra%20for%20Earth%20Representation%0A%20%20and%20Analysis%0AAuthor%3A%20Zhengpeng%20Feng%20and%20Clement%20Atzberger%20and%20Sadiq%20Jaffer%20and%20Jovana%20Knezevic%20and%20Silja%20Sormunen%20and%20Robin%20Young%20and%20Madeline%20C%20Lisaius%20and%20Markus%20Immitzer%20and%20James%20Ball%20and%20David%20A.%20Coomes%20and%20Anil%20Madhavapeddy%20and%20Andrew%20Blake%20and%20Srinivasan%20Keshav%0AAbstract%3A%20%20%20Satellite%20remote%20sensing%20enables%20a%20wide%20range%20of%20downstream%20applications%2C%0Aincluding%20habitat%20mapping%2C%20carbon%20accounting%2C%20and%20strategies%20for%20conservation%0Aand%20sustainable%20land%20use.%20However%2C%20satellite%20time%20series%20are%20voluminous%20and%0Aoften%20corrupted%2C%20making%20them%20challenging%20to%20use.%20We%20present%20TESSERA%2C%20an%20open%2C%0Aglobal%2C%20land-oriented%20remote%20sensing%20foundation%20model%20that%20uses%20self-supervised%0Alearning%20to%20generate%20%60ready-to-use%27%20embeddings%20at%2010~m%20scale%20from%20pixel-level%0Asatellite%20time-series%20data.%20TESSERA%20uses%20two%20encoders%20to%20combine%20optical%20data%0Awith%20synthetic%20aperture%20radar%20backscatter%20coefficients%20at%2010~m%20resolution%20to%0Acreate%20embeddings%20that%20are%20fused%20with%20a%20multilayer%20perceptron%20to%20create%20annual%0Aglobal%20embedding%20maps.%20We%20compare%20our%20work%20with%20state-of-the-art%20task-specific%0Amodels%20and%20other%20foundation%20models%20in%20five%20diverse%20downstream%20tasks%20and%20find%0Athat%20TESSERA%20closely%20matches%20or%20outperforms%20these%20baselines.%20We%20believe%20that%0ATESSERA%27s%20ease%20of%20use%2C%20state-of-the-art%20performance%2C%20openness%2C%20and%20computation-%0Aand%20labelled%20data-efficiency%20will%20prove%20transformative%20in%20a%20wide%20range%20of%0Aecological%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20380v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTESSERA%253A%2520Temporal%2520Embeddings%2520of%2520Surface%2520Spectra%2520for%2520Earth%2520Representation%250A%2520%2520and%2520Analysis%26entry.906535625%3DZhengpeng%2520Feng%2520and%2520Clement%2520Atzberger%2520and%2520Sadiq%2520Jaffer%2520and%2520Jovana%2520Knezevic%2520and%2520Silja%2520Sormunen%2520and%2520Robin%2520Young%2520and%2520Madeline%2520C%2520Lisaius%2520and%2520Markus%2520Immitzer%2520and%2520James%2520Ball%2520and%2520David%2520A.%2520Coomes%2520and%2520Anil%2520Madhavapeddy%2520and%2520Andrew%2520Blake%2520and%2520Srinivasan%2520Keshav%26entry.1292438233%3D%2520%2520Satellite%2520remote%2520sensing%2520enables%2520a%2520wide%2520range%2520of%2520downstream%2520applications%252C%250Aincluding%2520habitat%2520mapping%252C%2520carbon%2520accounting%252C%2520and%2520strategies%2520for%2520conservation%250Aand%2520sustainable%2520land%2520use.%2520However%252C%2520satellite%2520time%2520series%2520are%2520voluminous%2520and%250Aoften%2520corrupted%252C%2520making%2520them%2520challenging%2520to%2520use.%2520We%2520present%2520TESSERA%252C%2520an%2520open%252C%250Aglobal%252C%2520land-oriented%2520remote%2520sensing%2520foundation%2520model%2520that%2520uses%2520self-supervised%250Alearning%2520to%2520generate%2520%2560ready-to-use%2527%2520embeddings%2520at%252010~m%2520scale%2520from%2520pixel-level%250Asatellite%2520time-series%2520data.%2520TESSERA%2520uses%2520two%2520encoders%2520to%2520combine%2520optical%2520data%250Awith%2520synthetic%2520aperture%2520radar%2520backscatter%2520coefficients%2520at%252010~m%2520resolution%2520to%250Acreate%2520embeddings%2520that%2520are%2520fused%2520with%2520a%2520multilayer%2520perceptron%2520to%2520create%2520annual%250Aglobal%2520embedding%2520maps.%2520We%2520compare%2520our%2520work%2520with%2520state-of-the-art%2520task-specific%250Amodels%2520and%2520other%2520foundation%2520models%2520in%2520five%2520diverse%2520downstream%2520tasks%2520and%2520find%250Athat%2520TESSERA%2520closely%2520matches%2520or%2520outperforms%2520these%2520baselines.%2520We%2520believe%2520that%250ATESSERA%2527s%2520ease%2520of%2520use%252C%2520state-of-the-art%2520performance%252C%2520openness%252C%2520and%2520computation-%250Aand%2520labelled%2520data-efficiency%2520will%2520prove%2520transformative%2520in%2520a%2520wide%2520range%2520of%250Aecological%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20380v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TESSERA%3A%20Temporal%20Embeddings%20of%20Surface%20Spectra%20for%20Earth%20Representation%0A%20%20and%20Analysis&entry.906535625=Zhengpeng%20Feng%20and%20Clement%20Atzberger%20and%20Sadiq%20Jaffer%20and%20Jovana%20Knezevic%20and%20Silja%20Sormunen%20and%20Robin%20Young%20and%20Madeline%20C%20Lisaius%20and%20Markus%20Immitzer%20and%20James%20Ball%20and%20David%20A.%20Coomes%20and%20Anil%20Madhavapeddy%20and%20Andrew%20Blake%20and%20Srinivasan%20Keshav&entry.1292438233=%20%20Satellite%20remote%20sensing%20enables%20a%20wide%20range%20of%20downstream%20applications%2C%0Aincluding%20habitat%20mapping%2C%20carbon%20accounting%2C%20and%20strategies%20for%20conservation%0Aand%20sustainable%20land%20use.%20However%2C%20satellite%20time%20series%20are%20voluminous%20and%0Aoften%20corrupted%2C%20making%20them%20challenging%20to%20use.%20We%20present%20TESSERA%2C%20an%20open%2C%0Aglobal%2C%20land-oriented%20remote%20sensing%20foundation%20model%20that%20uses%20self-supervised%0Alearning%20to%20generate%20%60ready-to-use%27%20embeddings%20at%2010~m%20scale%20from%20pixel-level%0Asatellite%20time-series%20data.%20TESSERA%20uses%20two%20encoders%20to%20combine%20optical%20data%0Awith%20synthetic%20aperture%20radar%20backscatter%20coefficients%20at%2010~m%20resolution%20to%0Acreate%20embeddings%20that%20are%20fused%20with%20a%20multilayer%20perceptron%20to%20create%20annual%0Aglobal%20embedding%20maps.%20We%20compare%20our%20work%20with%20state-of-the-art%20task-specific%0Amodels%20and%20other%20foundation%20models%20in%20five%20diverse%20downstream%20tasks%20and%20find%0Athat%20TESSERA%20closely%20matches%20or%20outperforms%20these%20baselines.%20We%20believe%20that%0ATESSERA%27s%20ease%20of%20use%2C%20state-of-the-art%20performance%2C%20openness%2C%20and%20computation-%0Aand%20labelled%20data-efficiency%20will%20prove%20transformative%20in%20a%20wide%20range%20of%0Aecological%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20380v4&entry.124074799=Read"},
{"title": "Long Chain-of-Thought Reasoning Across Languages", "author": "Josh Barua and Seun Eisape and Kayo Yin and Alane Suhr", "abstract": "  Scaling inference through long chains-of-thought (CoTs) has unlocked\nimpressive reasoning capabilities in large language models (LLMs), yet the\nreasoning process remains almost exclusively English-centric. We construct\ntranslated versions of two popular English reasoning datasets, fine-tune Qwen\n2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT\ngeneration across French, Japanese, Latvian, and Swahili. Our experiments\nreveal three key findings. First, the efficacy of using English as a pivot\nlanguage varies by language: it provides no benefit for French, improves\nperformance when used as the reasoning language for Japanese and Latvian, and\nproves insufficient for Swahili where both task comprehension and reasoning\nremain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but\ndoes not eliminate the cross-lingual performance gap. A lightweight fine-tune\nusing only 1k traces still improves performance by over 30\\% in Swahili. Third,\ndata quality versus scale trade-offs are language dependent: small, carefully\ncurated datasets suffice for English and French, whereas larger but noisier\ncorpora prove more effective for Swahili and Latvian. Together, these results\nclarify when and why long CoTs transfer across languages and provide translated\ndatasets to foster equitable multilingual reasoning research.\n", "link": "http://arxiv.org/abs/2508.14828v1", "date": "2025-08-20", "relevancy": 2.4975, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20Chain-of-Thought%20Reasoning%20Across%20Languages&body=Title%3A%20Long%20Chain-of-Thought%20Reasoning%20Across%20Languages%0AAuthor%3A%20Josh%20Barua%20and%20Seun%20Eisape%20and%20Kayo%20Yin%20and%20Alane%20Suhr%0AAbstract%3A%20%20%20Scaling%20inference%20through%20long%20chains-of-thought%20%28CoTs%29%20has%20unlocked%0Aimpressive%20reasoning%20capabilities%20in%20large%20language%20models%20%28LLMs%29%2C%20yet%20the%0Areasoning%20process%20remains%20almost%20exclusively%20English-centric.%20We%20construct%0Atranslated%20versions%20of%20two%20popular%20English%20reasoning%20datasets%2C%20fine-tune%20Qwen%0A2.5%20%287B%29%20and%20Qwen%203%20%288B%29%20models%2C%20and%20present%20a%20systematic%20study%20of%20long%20CoT%0Ageneration%20across%20French%2C%20Japanese%2C%20Latvian%2C%20and%20Swahili.%20Our%20experiments%0Areveal%20three%20key%20findings.%20First%2C%20the%20efficacy%20of%20using%20English%20as%20a%20pivot%0Alanguage%20varies%20by%20language%3A%20it%20provides%20no%20benefit%20for%20French%2C%20improves%0Aperformance%20when%20used%20as%20the%20reasoning%20language%20for%20Japanese%20and%20Latvian%2C%20and%0Aproves%20insufficient%20for%20Swahili%20where%20both%20task%20comprehension%20and%20reasoning%0Aremain%20poor.%20Second%2C%20extensive%20multilingual%20pretraining%20in%20Qwen%203%20narrows%20but%0Adoes%20not%20eliminate%20the%20cross-lingual%20performance%20gap.%20A%20lightweight%20fine-tune%0Ausing%20only%201k%20traces%20still%20improves%20performance%20by%20over%2030%5C%25%20in%20Swahili.%20Third%2C%0Adata%20quality%20versus%20scale%20trade-offs%20are%20language%20dependent%3A%20small%2C%20carefully%0Acurated%20datasets%20suffice%20for%20English%20and%20French%2C%20whereas%20larger%20but%20noisier%0Acorpora%20prove%20more%20effective%20for%20Swahili%20and%20Latvian.%20Together%2C%20these%20results%0Aclarify%20when%20and%20why%20long%20CoTs%20transfer%20across%20languages%20and%20provide%20translated%0Adatasets%20to%20foster%20equitable%20multilingual%20reasoning%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520Chain-of-Thought%2520Reasoning%2520Across%2520Languages%26entry.906535625%3DJosh%2520Barua%2520and%2520Seun%2520Eisape%2520and%2520Kayo%2520Yin%2520and%2520Alane%2520Suhr%26entry.1292438233%3D%2520%2520Scaling%2520inference%2520through%2520long%2520chains-of-thought%2520%2528CoTs%2529%2520has%2520unlocked%250Aimpressive%2520reasoning%2520capabilities%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520yet%2520the%250Areasoning%2520process%2520remains%2520almost%2520exclusively%2520English-centric.%2520We%2520construct%250Atranslated%2520versions%2520of%2520two%2520popular%2520English%2520reasoning%2520datasets%252C%2520fine-tune%2520Qwen%250A2.5%2520%25287B%2529%2520and%2520Qwen%25203%2520%25288B%2529%2520models%252C%2520and%2520present%2520a%2520systematic%2520study%2520of%2520long%2520CoT%250Ageneration%2520across%2520French%252C%2520Japanese%252C%2520Latvian%252C%2520and%2520Swahili.%2520Our%2520experiments%250Areveal%2520three%2520key%2520findings.%2520First%252C%2520the%2520efficacy%2520of%2520using%2520English%2520as%2520a%2520pivot%250Alanguage%2520varies%2520by%2520language%253A%2520it%2520provides%2520no%2520benefit%2520for%2520French%252C%2520improves%250Aperformance%2520when%2520used%2520as%2520the%2520reasoning%2520language%2520for%2520Japanese%2520and%2520Latvian%252C%2520and%250Aproves%2520insufficient%2520for%2520Swahili%2520where%2520both%2520task%2520comprehension%2520and%2520reasoning%250Aremain%2520poor.%2520Second%252C%2520extensive%2520multilingual%2520pretraining%2520in%2520Qwen%25203%2520narrows%2520but%250Adoes%2520not%2520eliminate%2520the%2520cross-lingual%2520performance%2520gap.%2520A%2520lightweight%2520fine-tune%250Ausing%2520only%25201k%2520traces%2520still%2520improves%2520performance%2520by%2520over%252030%255C%2525%2520in%2520Swahili.%2520Third%252C%250Adata%2520quality%2520versus%2520scale%2520trade-offs%2520are%2520language%2520dependent%253A%2520small%252C%2520carefully%250Acurated%2520datasets%2520suffice%2520for%2520English%2520and%2520French%252C%2520whereas%2520larger%2520but%2520noisier%250Acorpora%2520prove%2520more%2520effective%2520for%2520Swahili%2520and%2520Latvian.%2520Together%252C%2520these%2520results%250Aclarify%2520when%2520and%2520why%2520long%2520CoTs%2520transfer%2520across%2520languages%2520and%2520provide%2520translated%250Adatasets%2520to%2520foster%2520equitable%2520multilingual%2520reasoning%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20Chain-of-Thought%20Reasoning%20Across%20Languages&entry.906535625=Josh%20Barua%20and%20Seun%20Eisape%20and%20Kayo%20Yin%20and%20Alane%20Suhr&entry.1292438233=%20%20Scaling%20inference%20through%20long%20chains-of-thought%20%28CoTs%29%20has%20unlocked%0Aimpressive%20reasoning%20capabilities%20in%20large%20language%20models%20%28LLMs%29%2C%20yet%20the%0Areasoning%20process%20remains%20almost%20exclusively%20English-centric.%20We%20construct%0Atranslated%20versions%20of%20two%20popular%20English%20reasoning%20datasets%2C%20fine-tune%20Qwen%0A2.5%20%287B%29%20and%20Qwen%203%20%288B%29%20models%2C%20and%20present%20a%20systematic%20study%20of%20long%20CoT%0Ageneration%20across%20French%2C%20Japanese%2C%20Latvian%2C%20and%20Swahili.%20Our%20experiments%0Areveal%20three%20key%20findings.%20First%2C%20the%20efficacy%20of%20using%20English%20as%20a%20pivot%0Alanguage%20varies%20by%20language%3A%20it%20provides%20no%20benefit%20for%20French%2C%20improves%0Aperformance%20when%20used%20as%20the%20reasoning%20language%20for%20Japanese%20and%20Latvian%2C%20and%0Aproves%20insufficient%20for%20Swahili%20where%20both%20task%20comprehension%20and%20reasoning%0Aremain%20poor.%20Second%2C%20extensive%20multilingual%20pretraining%20in%20Qwen%203%20narrows%20but%0Adoes%20not%20eliminate%20the%20cross-lingual%20performance%20gap.%20A%20lightweight%20fine-tune%0Ausing%20only%201k%20traces%20still%20improves%20performance%20by%20over%2030%5C%25%20in%20Swahili.%20Third%2C%0Adata%20quality%20versus%20scale%20trade-offs%20are%20language%20dependent%3A%20small%2C%20carefully%0Acurated%20datasets%20suffice%20for%20English%20and%20French%2C%20whereas%20larger%20but%20noisier%0Acorpora%20prove%20more%20effective%20for%20Swahili%20and%20Latvian.%20Together%2C%20these%20results%0Aclarify%20when%20and%20why%20long%20CoTs%20transfer%20across%20languages%20and%20provide%20translated%0Adatasets%20to%20foster%20equitable%20multilingual%20reasoning%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14828v1&entry.124074799=Read"},
{"title": "MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting\n  Mitigation in GFSCIL", "author": "Jinhui Pang and Changqing Lin and Hao Lin and Zhihui Zhang and Weiping Ding and Yu Liu and Xiaoshuai Hao", "abstract": "  Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to\ncontinually learn from limited samples of novel tasks after initial training on\na large base dataset. Existing GFSCIL approaches typically utilize Prototypical\nNetworks (PNs) for metric-based class representations and fine-tune the model\nduring the incremental learning stage. However, these PN-based methods\noversimplify learning via novel query set fine-tuning and fail to integrate\nGraph Continual Learning (GCL) techniques due to architectural constraints. To\naddress these challenges, we propose a more rigorous and practical setting for\nGFSCIL that excludes query sets during the incremental training phase. Building\non this foundation, we introduce Model-Agnostic Meta Graph Continual Learning\n(MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL.\nSpecifically, by calculating the incremental second-order gradient during the\nmeta-training stage, we endow the model to learn high-quality priors that\nenhance incremental learning by aligning its behaviors across both the\nmeta-training and incremental learning stages. Extensive experiments on four\nmainstream graph datasets demonstrate that MEGA achieves state-of-the-art\nresults and enhances the effectiveness of various GCL methods in GFSCIL. We\nbelieve that our proposed MEGA serves as a model-agnostic GFSCIL paradigm,\npaving the way for future research.\n", "link": "http://arxiv.org/abs/2504.13691v3", "date": "2025-08-20", "relevancy": 2.4397, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4956}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4889}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL&body=Title%3A%20MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL%0AAuthor%3A%20Jinhui%20Pang%20and%20Changqing%20Lin%20and%20Hao%20Lin%20and%20Zhihui%20Zhang%20and%20Weiping%20Ding%20and%20Yu%20Liu%20and%20Xiaoshuai%20Hao%0AAbstract%3A%20%20%20Graph%20Few-Shot%20Class-Incremental%20Learning%20%28GFSCIL%29%20enables%20models%20to%0Acontinually%20learn%20from%20limited%20samples%20of%20novel%20tasks%20after%20initial%20training%20on%0Aa%20large%20base%20dataset.%20Existing%20GFSCIL%20approaches%20typically%20utilize%20Prototypical%0ANetworks%20%28PNs%29%20for%20metric-based%20class%20representations%20and%20fine-tune%20the%20model%0Aduring%20the%20incremental%20learning%20stage.%20However%2C%20these%20PN-based%20methods%0Aoversimplify%20learning%20via%20novel%20query%20set%20fine-tuning%20and%20fail%20to%20integrate%0AGraph%20Continual%20Learning%20%28GCL%29%20techniques%20due%20to%20architectural%20constraints.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20more%20rigorous%20and%20practical%20setting%20for%0AGFSCIL%20that%20excludes%20query%20sets%20during%20the%20incremental%20training%20phase.%20Building%0Aon%20this%20foundation%2C%20we%20introduce%20Model-Agnostic%20Meta%20Graph%20Continual%20Learning%0A%28MEGA%29%2C%20aimed%20at%20effectively%20alleviating%20catastrophic%20forgetting%20for%20GFSCIL.%0ASpecifically%2C%20by%20calculating%20the%20incremental%20second-order%20gradient%20during%20the%0Ameta-training%20stage%2C%20we%20endow%20the%20model%20to%20learn%20high-quality%20priors%20that%0Aenhance%20incremental%20learning%20by%20aligning%20its%20behaviors%20across%20both%20the%0Ameta-training%20and%20incremental%20learning%20stages.%20Extensive%20experiments%20on%20four%0Amainstream%20graph%20datasets%20demonstrate%20that%20MEGA%20achieves%20state-of-the-art%0Aresults%20and%20enhances%20the%20effectiveness%20of%20various%20GCL%20methods%20in%20GFSCIL.%20We%0Abelieve%20that%20our%20proposed%20MEGA%20serves%20as%20a%20model-agnostic%20GFSCIL%20paradigm%2C%0Apaving%20the%20way%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13691v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGA%253A%2520Second-Order%2520Gradient%2520Alignment%2520for%2520Catastrophic%2520Forgetting%250A%2520%2520Mitigation%2520in%2520GFSCIL%26entry.906535625%3DJinhui%2520Pang%2520and%2520Changqing%2520Lin%2520and%2520Hao%2520Lin%2520and%2520Zhihui%2520Zhang%2520and%2520Weiping%2520Ding%2520and%2520Yu%2520Liu%2520and%2520Xiaoshuai%2520Hao%26entry.1292438233%3D%2520%2520Graph%2520Few-Shot%2520Class-Incremental%2520Learning%2520%2528GFSCIL%2529%2520enables%2520models%2520to%250Acontinually%2520learn%2520from%2520limited%2520samples%2520of%2520novel%2520tasks%2520after%2520initial%2520training%2520on%250Aa%2520large%2520base%2520dataset.%2520Existing%2520GFSCIL%2520approaches%2520typically%2520utilize%2520Prototypical%250ANetworks%2520%2528PNs%2529%2520for%2520metric-based%2520class%2520representations%2520and%2520fine-tune%2520the%2520model%250Aduring%2520the%2520incremental%2520learning%2520stage.%2520However%252C%2520these%2520PN-based%2520methods%250Aoversimplify%2520learning%2520via%2520novel%2520query%2520set%2520fine-tuning%2520and%2520fail%2520to%2520integrate%250AGraph%2520Continual%2520Learning%2520%2528GCL%2529%2520techniques%2520due%2520to%2520architectural%2520constraints.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520more%2520rigorous%2520and%2520practical%2520setting%2520for%250AGFSCIL%2520that%2520excludes%2520query%2520sets%2520during%2520the%2520incremental%2520training%2520phase.%2520Building%250Aon%2520this%2520foundation%252C%2520we%2520introduce%2520Model-Agnostic%2520Meta%2520Graph%2520Continual%2520Learning%250A%2528MEGA%2529%252C%2520aimed%2520at%2520effectively%2520alleviating%2520catastrophic%2520forgetting%2520for%2520GFSCIL.%250ASpecifically%252C%2520by%2520calculating%2520the%2520incremental%2520second-order%2520gradient%2520during%2520the%250Ameta-training%2520stage%252C%2520we%2520endow%2520the%2520model%2520to%2520learn%2520high-quality%2520priors%2520that%250Aenhance%2520incremental%2520learning%2520by%2520aligning%2520its%2520behaviors%2520across%2520both%2520the%250Ameta-training%2520and%2520incremental%2520learning%2520stages.%2520Extensive%2520experiments%2520on%2520four%250Amainstream%2520graph%2520datasets%2520demonstrate%2520that%2520MEGA%2520achieves%2520state-of-the-art%250Aresults%2520and%2520enhances%2520the%2520effectiveness%2520of%2520various%2520GCL%2520methods%2520in%2520GFSCIL.%2520We%250Abelieve%2520that%2520our%2520proposed%2520MEGA%2520serves%2520as%2520a%2520model-agnostic%2520GFSCIL%2520paradigm%252C%250Apaving%2520the%2520way%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13691v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL&entry.906535625=Jinhui%20Pang%20and%20Changqing%20Lin%20and%20Hao%20Lin%20and%20Zhihui%20Zhang%20and%20Weiping%20Ding%20and%20Yu%20Liu%20and%20Xiaoshuai%20Hao&entry.1292438233=%20%20Graph%20Few-Shot%20Class-Incremental%20Learning%20%28GFSCIL%29%20enables%20models%20to%0Acontinually%20learn%20from%20limited%20samples%20of%20novel%20tasks%20after%20initial%20training%20on%0Aa%20large%20base%20dataset.%20Existing%20GFSCIL%20approaches%20typically%20utilize%20Prototypical%0ANetworks%20%28PNs%29%20for%20metric-based%20class%20representations%20and%20fine-tune%20the%20model%0Aduring%20the%20incremental%20learning%20stage.%20However%2C%20these%20PN-based%20methods%0Aoversimplify%20learning%20via%20novel%20query%20set%20fine-tuning%20and%20fail%20to%20integrate%0AGraph%20Continual%20Learning%20%28GCL%29%20techniques%20due%20to%20architectural%20constraints.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20more%20rigorous%20and%20practical%20setting%20for%0AGFSCIL%20that%20excludes%20query%20sets%20during%20the%20incremental%20training%20phase.%20Building%0Aon%20this%20foundation%2C%20we%20introduce%20Model-Agnostic%20Meta%20Graph%20Continual%20Learning%0A%28MEGA%29%2C%20aimed%20at%20effectively%20alleviating%20catastrophic%20forgetting%20for%20GFSCIL.%0ASpecifically%2C%20by%20calculating%20the%20incremental%20second-order%20gradient%20during%20the%0Ameta-training%20stage%2C%20we%20endow%20the%20model%20to%20learn%20high-quality%20priors%20that%0Aenhance%20incremental%20learning%20by%20aligning%20its%20behaviors%20across%20both%20the%0Ameta-training%20and%20incremental%20learning%20stages.%20Extensive%20experiments%20on%20four%0Amainstream%20graph%20datasets%20demonstrate%20that%20MEGA%20achieves%20state-of-the-art%0Aresults%20and%20enhances%20the%20effectiveness%20of%20various%20GCL%20methods%20in%20GFSCIL.%20We%0Abelieve%20that%20our%20proposed%20MEGA%20serves%20as%20a%20model-agnostic%20GFSCIL%20paradigm%2C%0Apaving%20the%20way%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13691v3&entry.124074799=Read"},
{"title": "VisioPhysioENet: Visual Physiological Engagement Detection Network", "author": "Alakhsimar Singh and Kanav Goyal and Nischay Verma and Puneet Kumar and Xiaobai Li and Amritpal Singh", "abstract": "  This paper presents VisioPhysioENet, a novel multimodal system that leverages\nvisual and physiological signals to detect learner engagement. It employs a\ntwo-level approach for extracting both visual and physiological features. For\nvisual feature extraction, Dlib is used to detect facial landmarks, while\nOpenCV provides additional estimations. The face recognition library, built on\nDlib, is used to identify the facial region of interest specifically for\nphysiological signal extraction. Physiological signals are then extracted using\nthe plane-orthogonal-toskin method to assess cardiovascular activity. These\nfeatures are integrated using advanced machine learning classifiers, enhancing\nthe detection of various levels of engagement. We thoroughly tested\nVisioPhysioENet on the DAiSEE dataset. It achieved an accuracy of 63.09%. This\nshows it can better identify different levels of engagement compared to many\nexisting methods. It performed 8.6% better than the only other model that uses\nboth physiological and visual features.\n", "link": "http://arxiv.org/abs/2409.16126v3", "date": "2025-08-20", "relevancy": 2.4362, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisioPhysioENet%3A%20Visual%20Physiological%20Engagement%20Detection%20Network&body=Title%3A%20VisioPhysioENet%3A%20Visual%20Physiological%20Engagement%20Detection%20Network%0AAuthor%3A%20Alakhsimar%20Singh%20and%20Kanav%20Goyal%20and%20Nischay%20Verma%20and%20Puneet%20Kumar%20and%20Xiaobai%20Li%20and%20Amritpal%20Singh%0AAbstract%3A%20%20%20This%20paper%20presents%20VisioPhysioENet%2C%20a%20novel%20multimodal%20system%20that%20leverages%0Avisual%20and%20physiological%20signals%20to%20detect%20learner%20engagement.%20It%20employs%20a%0Atwo-level%20approach%20for%20extracting%20both%20visual%20and%20physiological%20features.%20For%0Avisual%20feature%20extraction%2C%20Dlib%20is%20used%20to%20detect%20facial%20landmarks%2C%20while%0AOpenCV%20provides%20additional%20estimations.%20The%20face%20recognition%20library%2C%20built%20on%0ADlib%2C%20is%20used%20to%20identify%20the%20facial%20region%20of%20interest%20specifically%20for%0Aphysiological%20signal%20extraction.%20Physiological%20signals%20are%20then%20extracted%20using%0Athe%20plane-orthogonal-toskin%20method%20to%20assess%20cardiovascular%20activity.%20These%0Afeatures%20are%20integrated%20using%20advanced%20machine%20learning%20classifiers%2C%20enhancing%0Athe%20detection%20of%20various%20levels%20of%20engagement.%20We%20thoroughly%20tested%0AVisioPhysioENet%20on%20the%20DAiSEE%20dataset.%20It%20achieved%20an%20accuracy%20of%2063.09%25.%20This%0Ashows%20it%20can%20better%20identify%20different%20levels%20of%20engagement%20compared%20to%20many%0Aexisting%20methods.%20It%20performed%208.6%25%20better%20than%20the%20only%20other%20model%20that%20uses%0Aboth%20physiological%20and%20visual%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16126v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisioPhysioENet%253A%2520Visual%2520Physiological%2520Engagement%2520Detection%2520Network%26entry.906535625%3DAlakhsimar%2520Singh%2520and%2520Kanav%2520Goyal%2520and%2520Nischay%2520Verma%2520and%2520Puneet%2520Kumar%2520and%2520Xiaobai%2520Li%2520and%2520Amritpal%2520Singh%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520VisioPhysioENet%252C%2520a%2520novel%2520multimodal%2520system%2520that%2520leverages%250Avisual%2520and%2520physiological%2520signals%2520to%2520detect%2520learner%2520engagement.%2520It%2520employs%2520a%250Atwo-level%2520approach%2520for%2520extracting%2520both%2520visual%2520and%2520physiological%2520features.%2520For%250Avisual%2520feature%2520extraction%252C%2520Dlib%2520is%2520used%2520to%2520detect%2520facial%2520landmarks%252C%2520while%250AOpenCV%2520provides%2520additional%2520estimations.%2520The%2520face%2520recognition%2520library%252C%2520built%2520on%250ADlib%252C%2520is%2520used%2520to%2520identify%2520the%2520facial%2520region%2520of%2520interest%2520specifically%2520for%250Aphysiological%2520signal%2520extraction.%2520Physiological%2520signals%2520are%2520then%2520extracted%2520using%250Athe%2520plane-orthogonal-toskin%2520method%2520to%2520assess%2520cardiovascular%2520activity.%2520These%250Afeatures%2520are%2520integrated%2520using%2520advanced%2520machine%2520learning%2520classifiers%252C%2520enhancing%250Athe%2520detection%2520of%2520various%2520levels%2520of%2520engagement.%2520We%2520thoroughly%2520tested%250AVisioPhysioENet%2520on%2520the%2520DAiSEE%2520dataset.%2520It%2520achieved%2520an%2520accuracy%2520of%252063.09%2525.%2520This%250Ashows%2520it%2520can%2520better%2520identify%2520different%2520levels%2520of%2520engagement%2520compared%2520to%2520many%250Aexisting%2520methods.%2520It%2520performed%25208.6%2525%2520better%2520than%2520the%2520only%2520other%2520model%2520that%2520uses%250Aboth%2520physiological%2520and%2520visual%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16126v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisioPhysioENet%3A%20Visual%20Physiological%20Engagement%20Detection%20Network&entry.906535625=Alakhsimar%20Singh%20and%20Kanav%20Goyal%20and%20Nischay%20Verma%20and%20Puneet%20Kumar%20and%20Xiaobai%20Li%20and%20Amritpal%20Singh&entry.1292438233=%20%20This%20paper%20presents%20VisioPhysioENet%2C%20a%20novel%20multimodal%20system%20that%20leverages%0Avisual%20and%20physiological%20signals%20to%20detect%20learner%20engagement.%20It%20employs%20a%0Atwo-level%20approach%20for%20extracting%20both%20visual%20and%20physiological%20features.%20For%0Avisual%20feature%20extraction%2C%20Dlib%20is%20used%20to%20detect%20facial%20landmarks%2C%20while%0AOpenCV%20provides%20additional%20estimations.%20The%20face%20recognition%20library%2C%20built%20on%0ADlib%2C%20is%20used%20to%20identify%20the%20facial%20region%20of%20interest%20specifically%20for%0Aphysiological%20signal%20extraction.%20Physiological%20signals%20are%20then%20extracted%20using%0Athe%20plane-orthogonal-toskin%20method%20to%20assess%20cardiovascular%20activity.%20These%0Afeatures%20are%20integrated%20using%20advanced%20machine%20learning%20classifiers%2C%20enhancing%0Athe%20detection%20of%20various%20levels%20of%20engagement.%20We%20thoroughly%20tested%0AVisioPhysioENet%20on%20the%20DAiSEE%20dataset.%20It%20achieved%20an%20accuracy%20of%2063.09%25.%20This%0Ashows%20it%20can%20better%20identify%20different%20levels%20of%20engagement%20compared%20to%20many%0Aexisting%20methods.%20It%20performed%208.6%25%20better%20than%20the%20only%20other%20model%20that%20uses%0Aboth%20physiological%20and%20visual%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16126v3&entry.124074799=Read"},
{"title": "Learnable Kernel Density Estimation for Graphs", "author": "Xudong Wang and Ziheng Sun and Chris Ding and Jicong Fan", "abstract": "  This work proposes a framework LGKDE that learns kernel density estimation\nfor graphs. The key challenge in graph density estimation lies in effectively\ncapturing both structural patterns and semantic variations while maintaining\ntheoretical guarantees. Combining graph kernels and kernel density estimation\n(KDE) is a standard approach to graph density estimation, but has\nunsatisfactory performance due to the handcrafted and fixed features of\nkernels. Our method LGKDE leverages graph neural networks to represent each\ngraph as a discrete distribution and utilizes maximum mean discrepancy to learn\nthe graph metric for multi-scale KDE, where all parameters are learned by\nmaximizing the density of graphs relative to the density of their well-designed\nperturbed counterparts. The perturbations are conducted on both node features\nand graph spectra, which helps better characterize the boundary of normal\ndensity regions. Theoretically, we establish consistency and convergence\nguarantees for LGKDE, including bounds on the mean integrated squared error,\nrobustness, and complexity. We validate LGKDE by demonstrating its\neffectiveness in recovering the underlying density of synthetic graph\ndistributions and applying it to graph anomaly detection across diverse\nbenchmark datasets. Extensive empirical evaluation shows that LGKDE\ndemonstrates superior performance compared to state-of-the-art baselines on\nmost benchmark datasets.\n", "link": "http://arxiv.org/abs/2505.21285v2", "date": "2025-08-20", "relevancy": 2.4349, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5156}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4756}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Kernel%20Density%20Estimation%20for%20Graphs&body=Title%3A%20Learnable%20Kernel%20Density%20Estimation%20for%20Graphs%0AAuthor%3A%20Xudong%20Wang%20and%20Ziheng%20Sun%20and%20Chris%20Ding%20and%20Jicong%20Fan%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20framework%20LGKDE%20that%20learns%20kernel%20density%20estimation%0Afor%20graphs.%20The%20key%20challenge%20in%20graph%20density%20estimation%20lies%20in%20effectively%0Acapturing%20both%20structural%20patterns%20and%20semantic%20variations%20while%20maintaining%0Atheoretical%20guarantees.%20Combining%20graph%20kernels%20and%20kernel%20density%20estimation%0A%28KDE%29%20is%20a%20standard%20approach%20to%20graph%20density%20estimation%2C%20but%20has%0Aunsatisfactory%20performance%20due%20to%20the%20handcrafted%20and%20fixed%20features%20of%0Akernels.%20Our%20method%20LGKDE%20leverages%20graph%20neural%20networks%20to%20represent%20each%0Agraph%20as%20a%20discrete%20distribution%20and%20utilizes%20maximum%20mean%20discrepancy%20to%20learn%0Athe%20graph%20metric%20for%20multi-scale%20KDE%2C%20where%20all%20parameters%20are%20learned%20by%0Amaximizing%20the%20density%20of%20graphs%20relative%20to%20the%20density%20of%20their%20well-designed%0Aperturbed%20counterparts.%20The%20perturbations%20are%20conducted%20on%20both%20node%20features%0Aand%20graph%20spectra%2C%20which%20helps%20better%20characterize%20the%20boundary%20of%20normal%0Adensity%20regions.%20Theoretically%2C%20we%20establish%20consistency%20and%20convergence%0Aguarantees%20for%20LGKDE%2C%20including%20bounds%20on%20the%20mean%20integrated%20squared%20error%2C%0Arobustness%2C%20and%20complexity.%20We%20validate%20LGKDE%20by%20demonstrating%20its%0Aeffectiveness%20in%20recovering%20the%20underlying%20density%20of%20synthetic%20graph%0Adistributions%20and%20applying%20it%20to%20graph%20anomaly%20detection%20across%20diverse%0Abenchmark%20datasets.%20Extensive%20empirical%20evaluation%20shows%20that%20LGKDE%0Ademonstrates%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Amost%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Kernel%2520Density%2520Estimation%2520for%2520Graphs%26entry.906535625%3DXudong%2520Wang%2520and%2520Ziheng%2520Sun%2520and%2520Chris%2520Ding%2520and%2520Jicong%2520Fan%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520a%2520framework%2520LGKDE%2520that%2520learns%2520kernel%2520density%2520estimation%250Afor%2520graphs.%2520The%2520key%2520challenge%2520in%2520graph%2520density%2520estimation%2520lies%2520in%2520effectively%250Acapturing%2520both%2520structural%2520patterns%2520and%2520semantic%2520variations%2520while%2520maintaining%250Atheoretical%2520guarantees.%2520Combining%2520graph%2520kernels%2520and%2520kernel%2520density%2520estimation%250A%2528KDE%2529%2520is%2520a%2520standard%2520approach%2520to%2520graph%2520density%2520estimation%252C%2520but%2520has%250Aunsatisfactory%2520performance%2520due%2520to%2520the%2520handcrafted%2520and%2520fixed%2520features%2520of%250Akernels.%2520Our%2520method%2520LGKDE%2520leverages%2520graph%2520neural%2520networks%2520to%2520represent%2520each%250Agraph%2520as%2520a%2520discrete%2520distribution%2520and%2520utilizes%2520maximum%2520mean%2520discrepancy%2520to%2520learn%250Athe%2520graph%2520metric%2520for%2520multi-scale%2520KDE%252C%2520where%2520all%2520parameters%2520are%2520learned%2520by%250Amaximizing%2520the%2520density%2520of%2520graphs%2520relative%2520to%2520the%2520density%2520of%2520their%2520well-designed%250Aperturbed%2520counterparts.%2520The%2520perturbations%2520are%2520conducted%2520on%2520both%2520node%2520features%250Aand%2520graph%2520spectra%252C%2520which%2520helps%2520better%2520characterize%2520the%2520boundary%2520of%2520normal%250Adensity%2520regions.%2520Theoretically%252C%2520we%2520establish%2520consistency%2520and%2520convergence%250Aguarantees%2520for%2520LGKDE%252C%2520including%2520bounds%2520on%2520the%2520mean%2520integrated%2520squared%2520error%252C%250Arobustness%252C%2520and%2520complexity.%2520We%2520validate%2520LGKDE%2520by%2520demonstrating%2520its%250Aeffectiveness%2520in%2520recovering%2520the%2520underlying%2520density%2520of%2520synthetic%2520graph%250Adistributions%2520and%2520applying%2520it%2520to%2520graph%2520anomaly%2520detection%2520across%2520diverse%250Abenchmark%2520datasets.%2520Extensive%2520empirical%2520evaluation%2520shows%2520that%2520LGKDE%250Ademonstrates%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520baselines%2520on%250Amost%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Kernel%20Density%20Estimation%20for%20Graphs&entry.906535625=Xudong%20Wang%20and%20Ziheng%20Sun%20and%20Chris%20Ding%20and%20Jicong%20Fan&entry.1292438233=%20%20This%20work%20proposes%20a%20framework%20LGKDE%20that%20learns%20kernel%20density%20estimation%0Afor%20graphs.%20The%20key%20challenge%20in%20graph%20density%20estimation%20lies%20in%20effectively%0Acapturing%20both%20structural%20patterns%20and%20semantic%20variations%20while%20maintaining%0Atheoretical%20guarantees.%20Combining%20graph%20kernels%20and%20kernel%20density%20estimation%0A%28KDE%29%20is%20a%20standard%20approach%20to%20graph%20density%20estimation%2C%20but%20has%0Aunsatisfactory%20performance%20due%20to%20the%20handcrafted%20and%20fixed%20features%20of%0Akernels.%20Our%20method%20LGKDE%20leverages%20graph%20neural%20networks%20to%20represent%20each%0Agraph%20as%20a%20discrete%20distribution%20and%20utilizes%20maximum%20mean%20discrepancy%20to%20learn%0Athe%20graph%20metric%20for%20multi-scale%20KDE%2C%20where%20all%20parameters%20are%20learned%20by%0Amaximizing%20the%20density%20of%20graphs%20relative%20to%20the%20density%20of%20their%20well-designed%0Aperturbed%20counterparts.%20The%20perturbations%20are%20conducted%20on%20both%20node%20features%0Aand%20graph%20spectra%2C%20which%20helps%20better%20characterize%20the%20boundary%20of%20normal%0Adensity%20regions.%20Theoretically%2C%20we%20establish%20consistency%20and%20convergence%0Aguarantees%20for%20LGKDE%2C%20including%20bounds%20on%20the%20mean%20integrated%20squared%20error%2C%0Arobustness%2C%20and%20complexity.%20We%20validate%20LGKDE%20by%20demonstrating%20its%0Aeffectiveness%20in%20recovering%20the%20underlying%20density%20of%20synthetic%20graph%0Adistributions%20and%20applying%20it%20to%20graph%20anomaly%20detection%20across%20diverse%0Abenchmark%20datasets.%20Extensive%20empirical%20evaluation%20shows%20that%20LGKDE%0Ademonstrates%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Amost%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21285v2&entry.124074799=Read"},
{"title": "OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service", "author": "Zhipeng Wei and Kuo Cai and Junda She and Jie Chen and Minghao Chen and Yang Zeng and Qiang Luo and Wencong Zeng and Ruiming Tang and Kun Gai and Guorui Zhou", "abstract": "  Local life service is a vital scenario in Kuaishou App, where video\nrecommendation is intrinsically linked with store's location information. Thus,\nrecommendation in our scenario is challenging because we should take into\naccount user's interest and real-time location at the same time. In the face of\nsuch complex scenarios, end-to-end generative recommendation has emerged as a\nnew paradigm, such as OneRec in the short video scenario, OneSug in the search\nscenario, and EGA in the advertising scenario. However, in local life service,\nan end-to-end generative recommendation model has not yet been developed as\nthere are some key challenges to be solved. The first challenge is how to make\nfull use of geographic information. The second challenge is how to balance\nmultiple objectives, including user interests, the distance between user and\nstores, and some other business objectives. To address the challenges, we\npropose OneLoc. Specifically, we leverage geographic information from different\nperspectives: (1) geo-aware semantic ID incorporates both video and geographic\ninformation for tokenization, (2) geo-aware self-attention in the encoder\nleverages both video location similarity and user's real-time location, and (3)\nneighbor-aware prompt captures rich context information surrounding users for\ngeneration. To balance multiple objectives, we use reinforcement learning and\npropose two reward functions, i.e., geographic reward and GMV reward. With the\nabove design, OneLoc achieves outstanding offline and online performance. In\nfact, OneLoc has been deployed in local life service of Kuaishou App. It serves\n400 million active users daily, achieving 21.016% and 17.891% improvements in\nterms of gross merchandise value (GMV) and orders numbers.\n", "link": "http://arxiv.org/abs/2508.14646v1", "date": "2025-08-20", "relevancy": 2.4302, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4914}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneLoc%3A%20Geo-Aware%20Generative%20Recommender%20Systems%20for%20Local%20Life%20Service&body=Title%3A%20OneLoc%3A%20Geo-Aware%20Generative%20Recommender%20Systems%20for%20Local%20Life%20Service%0AAuthor%3A%20Zhipeng%20Wei%20and%20Kuo%20Cai%20and%20Junda%20She%20and%20Jie%20Chen%20and%20Minghao%20Chen%20and%20Yang%20Zeng%20and%20Qiang%20Luo%20and%20Wencong%20Zeng%20and%20Ruiming%20Tang%20and%20Kun%20Gai%20and%20Guorui%20Zhou%0AAbstract%3A%20%20%20Local%20life%20service%20is%20a%20vital%20scenario%20in%20Kuaishou%20App%2C%20where%20video%0Arecommendation%20is%20intrinsically%20linked%20with%20store%27s%20location%20information.%20Thus%2C%0Arecommendation%20in%20our%20scenario%20is%20challenging%20because%20we%20should%20take%20into%0Aaccount%20user%27s%20interest%20and%20real-time%20location%20at%20the%20same%20time.%20In%20the%20face%20of%0Asuch%20complex%20scenarios%2C%20end-to-end%20generative%20recommendation%20has%20emerged%20as%20a%0Anew%20paradigm%2C%20such%20as%20OneRec%20in%20the%20short%20video%20scenario%2C%20OneSug%20in%20the%20search%0Ascenario%2C%20and%20EGA%20in%20the%20advertising%20scenario.%20However%2C%20in%20local%20life%20service%2C%0Aan%20end-to-end%20generative%20recommendation%20model%20has%20not%20yet%20been%20developed%20as%0Athere%20are%20some%20key%20challenges%20to%20be%20solved.%20The%20first%20challenge%20is%20how%20to%20make%0Afull%20use%20of%20geographic%20information.%20The%20second%20challenge%20is%20how%20to%20balance%0Amultiple%20objectives%2C%20including%20user%20interests%2C%20the%20distance%20between%20user%20and%0Astores%2C%20and%20some%20other%20business%20objectives.%20To%20address%20the%20challenges%2C%20we%0Apropose%20OneLoc.%20Specifically%2C%20we%20leverage%20geographic%20information%20from%20different%0Aperspectives%3A%20%281%29%20geo-aware%20semantic%20ID%20incorporates%20both%20video%20and%20geographic%0Ainformation%20for%20tokenization%2C%20%282%29%20geo-aware%20self-attention%20in%20the%20encoder%0Aleverages%20both%20video%20location%20similarity%20and%20user%27s%20real-time%20location%2C%20and%20%283%29%0Aneighbor-aware%20prompt%20captures%20rich%20context%20information%20surrounding%20users%20for%0Ageneration.%20To%20balance%20multiple%20objectives%2C%20we%20use%20reinforcement%20learning%20and%0Apropose%20two%20reward%20functions%2C%20i.e.%2C%20geographic%20reward%20and%20GMV%20reward.%20With%20the%0Aabove%20design%2C%20OneLoc%20achieves%20outstanding%20offline%20and%20online%20performance.%20In%0Afact%2C%20OneLoc%20has%20been%20deployed%20in%20local%20life%20service%20of%20Kuaishou%20App.%20It%20serves%0A400%20million%20active%20users%20daily%2C%20achieving%2021.016%25%20and%2017.891%25%20improvements%20in%0Aterms%20of%20gross%20merchandise%20value%20%28GMV%29%20and%20orders%20numbers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneLoc%253A%2520Geo-Aware%2520Generative%2520Recommender%2520Systems%2520for%2520Local%2520Life%2520Service%26entry.906535625%3DZhipeng%2520Wei%2520and%2520Kuo%2520Cai%2520and%2520Junda%2520She%2520and%2520Jie%2520Chen%2520and%2520Minghao%2520Chen%2520and%2520Yang%2520Zeng%2520and%2520Qiang%2520Luo%2520and%2520Wencong%2520Zeng%2520and%2520Ruiming%2520Tang%2520and%2520Kun%2520Gai%2520and%2520Guorui%2520Zhou%26entry.1292438233%3D%2520%2520Local%2520life%2520service%2520is%2520a%2520vital%2520scenario%2520in%2520Kuaishou%2520App%252C%2520where%2520video%250Arecommendation%2520is%2520intrinsically%2520linked%2520with%2520store%2527s%2520location%2520information.%2520Thus%252C%250Arecommendation%2520in%2520our%2520scenario%2520is%2520challenging%2520because%2520we%2520should%2520take%2520into%250Aaccount%2520user%2527s%2520interest%2520and%2520real-time%2520location%2520at%2520the%2520same%2520time.%2520In%2520the%2520face%2520of%250Asuch%2520complex%2520scenarios%252C%2520end-to-end%2520generative%2520recommendation%2520has%2520emerged%2520as%2520a%250Anew%2520paradigm%252C%2520such%2520as%2520OneRec%2520in%2520the%2520short%2520video%2520scenario%252C%2520OneSug%2520in%2520the%2520search%250Ascenario%252C%2520and%2520EGA%2520in%2520the%2520advertising%2520scenario.%2520However%252C%2520in%2520local%2520life%2520service%252C%250Aan%2520end-to-end%2520generative%2520recommendation%2520model%2520has%2520not%2520yet%2520been%2520developed%2520as%250Athere%2520are%2520some%2520key%2520challenges%2520to%2520be%2520solved.%2520The%2520first%2520challenge%2520is%2520how%2520to%2520make%250Afull%2520use%2520of%2520geographic%2520information.%2520The%2520second%2520challenge%2520is%2520how%2520to%2520balance%250Amultiple%2520objectives%252C%2520including%2520user%2520interests%252C%2520the%2520distance%2520between%2520user%2520and%250Astores%252C%2520and%2520some%2520other%2520business%2520objectives.%2520To%2520address%2520the%2520challenges%252C%2520we%250Apropose%2520OneLoc.%2520Specifically%252C%2520we%2520leverage%2520geographic%2520information%2520from%2520different%250Aperspectives%253A%2520%25281%2529%2520geo-aware%2520semantic%2520ID%2520incorporates%2520both%2520video%2520and%2520geographic%250Ainformation%2520for%2520tokenization%252C%2520%25282%2529%2520geo-aware%2520self-attention%2520in%2520the%2520encoder%250Aleverages%2520both%2520video%2520location%2520similarity%2520and%2520user%2527s%2520real-time%2520location%252C%2520and%2520%25283%2529%250Aneighbor-aware%2520prompt%2520captures%2520rich%2520context%2520information%2520surrounding%2520users%2520for%250Ageneration.%2520To%2520balance%2520multiple%2520objectives%252C%2520we%2520use%2520reinforcement%2520learning%2520and%250Apropose%2520two%2520reward%2520functions%252C%2520i.e.%252C%2520geographic%2520reward%2520and%2520GMV%2520reward.%2520With%2520the%250Aabove%2520design%252C%2520OneLoc%2520achieves%2520outstanding%2520offline%2520and%2520online%2520performance.%2520In%250Afact%252C%2520OneLoc%2520has%2520been%2520deployed%2520in%2520local%2520life%2520service%2520of%2520Kuaishou%2520App.%2520It%2520serves%250A400%2520million%2520active%2520users%2520daily%252C%2520achieving%252021.016%2525%2520and%252017.891%2525%2520improvements%2520in%250Aterms%2520of%2520gross%2520merchandise%2520value%2520%2528GMV%2529%2520and%2520orders%2520numbers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneLoc%3A%20Geo-Aware%20Generative%20Recommender%20Systems%20for%20Local%20Life%20Service&entry.906535625=Zhipeng%20Wei%20and%20Kuo%20Cai%20and%20Junda%20She%20and%20Jie%20Chen%20and%20Minghao%20Chen%20and%20Yang%20Zeng%20and%20Qiang%20Luo%20and%20Wencong%20Zeng%20and%20Ruiming%20Tang%20and%20Kun%20Gai%20and%20Guorui%20Zhou&entry.1292438233=%20%20Local%20life%20service%20is%20a%20vital%20scenario%20in%20Kuaishou%20App%2C%20where%20video%0Arecommendation%20is%20intrinsically%20linked%20with%20store%27s%20location%20information.%20Thus%2C%0Arecommendation%20in%20our%20scenario%20is%20challenging%20because%20we%20should%20take%20into%0Aaccount%20user%27s%20interest%20and%20real-time%20location%20at%20the%20same%20time.%20In%20the%20face%20of%0Asuch%20complex%20scenarios%2C%20end-to-end%20generative%20recommendation%20has%20emerged%20as%20a%0Anew%20paradigm%2C%20such%20as%20OneRec%20in%20the%20short%20video%20scenario%2C%20OneSug%20in%20the%20search%0Ascenario%2C%20and%20EGA%20in%20the%20advertising%20scenario.%20However%2C%20in%20local%20life%20service%2C%0Aan%20end-to-end%20generative%20recommendation%20model%20has%20not%20yet%20been%20developed%20as%0Athere%20are%20some%20key%20challenges%20to%20be%20solved.%20The%20first%20challenge%20is%20how%20to%20make%0Afull%20use%20of%20geographic%20information.%20The%20second%20challenge%20is%20how%20to%20balance%0Amultiple%20objectives%2C%20including%20user%20interests%2C%20the%20distance%20between%20user%20and%0Astores%2C%20and%20some%20other%20business%20objectives.%20To%20address%20the%20challenges%2C%20we%0Apropose%20OneLoc.%20Specifically%2C%20we%20leverage%20geographic%20information%20from%20different%0Aperspectives%3A%20%281%29%20geo-aware%20semantic%20ID%20incorporates%20both%20video%20and%20geographic%0Ainformation%20for%20tokenization%2C%20%282%29%20geo-aware%20self-attention%20in%20the%20encoder%0Aleverages%20both%20video%20location%20similarity%20and%20user%27s%20real-time%20location%2C%20and%20%283%29%0Aneighbor-aware%20prompt%20captures%20rich%20context%20information%20surrounding%20users%20for%0Ageneration.%20To%20balance%20multiple%20objectives%2C%20we%20use%20reinforcement%20learning%20and%0Apropose%20two%20reward%20functions%2C%20i.e.%2C%20geographic%20reward%20and%20GMV%20reward.%20With%20the%0Aabove%20design%2C%20OneLoc%20achieves%20outstanding%20offline%20and%20online%20performance.%20In%0Afact%2C%20OneLoc%20has%20been%20deployed%20in%20local%20life%20service%20of%20Kuaishou%20App.%20It%20serves%0A400%20million%20active%20users%20daily%2C%20achieving%2021.016%25%20and%2017.891%25%20improvements%20in%0Aterms%20of%20gross%20merchandise%20value%20%28GMV%29%20and%20orders%20numbers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14646v1&entry.124074799=Read"},
{"title": "LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and\n  Optimization", "author": "Xujia Wang and Yunjia Qi and Bin Xu", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly\nreduce the number of trainable parameters by introducing low-rank decomposition\nmatrices. However, existing methods perform extensive matrix multiplications in\ndomain specialization tasks, resulting in computational inefficiency and\nsub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources\nSubnet Integration Adaptation), an innovative method that dynamically localizes\nand optimizes critical parameters during the training process. Specifically, it\nidentifies a sub-network using gradient sparsity analysis and optimizes it as\nthe trainable target. This design enables effective high-rank adaptation by\nupdating only the sub-network parameters, reducing the additional matrix\nmultiplication. We also present LoSiA-Pro, a faster implementation of LoSiA,\nwhich reduces the training latency by about $27\\%$ compared to LoRA. Extensive\nevaluations show that our method achieves minimal performance drop compared to\nfull fine-tuning, while requiring the least training time across domain\nspecialization and common-sense reasoning tasks. Further analysis shows that\nLoSiA also reduces forgetting during continued training. The source code is\navailable at https://github.com/KlozeWang/LoSiA.\n", "link": "http://arxiv.org/abs/2507.04487v3", "date": "2025-08-20", "relevancy": 2.4006, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4901}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4826}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoSiA%3A%20Efficient%20High-Rank%20Fine-Tuning%20via%20Subnet%20Localization%20and%0A%20%20Optimization&body=Title%3A%20LoSiA%3A%20Efficient%20High-Rank%20Fine-Tuning%20via%20Subnet%20Localization%20and%0A%20%20Optimization%0AAuthor%3A%20Xujia%20Wang%20and%20Yunjia%20Qi%20and%20Bin%20Xu%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%20significantly%0Areduce%20the%20number%20of%20trainable%20parameters%20by%20introducing%20low-rank%20decomposition%0Amatrices.%20However%2C%20existing%20methods%20perform%20extensive%20matrix%20multiplications%20in%0Adomain%20specialization%20tasks%2C%20resulting%20in%20computational%20inefficiency%20and%0Asub-optimal%20fine-tuning%20performance.%20Hence%2C%20we%20propose%20LoSiA%28Low-Resources%0ASubnet%20Integration%20Adaptation%29%2C%20an%20innovative%20method%20that%20dynamically%20localizes%0Aand%20optimizes%20critical%20parameters%20during%20the%20training%20process.%20Specifically%2C%20it%0Aidentifies%20a%20sub-network%20using%20gradient%20sparsity%20analysis%20and%20optimizes%20it%20as%0Athe%20trainable%20target.%20This%20design%20enables%20effective%20high-rank%20adaptation%20by%0Aupdating%20only%20the%20sub-network%20parameters%2C%20reducing%20the%20additional%20matrix%0Amultiplication.%20We%20also%20present%20LoSiA-Pro%2C%20a%20faster%20implementation%20of%20LoSiA%2C%0Awhich%20reduces%20the%20training%20latency%20by%20about%20%2427%5C%25%24%20compared%20to%20LoRA.%20Extensive%0Aevaluations%20show%20that%20our%20method%20achieves%20minimal%20performance%20drop%20compared%20to%0Afull%20fine-tuning%2C%20while%20requiring%20the%20least%20training%20time%20across%20domain%0Aspecialization%20and%20common-sense%20reasoning%20tasks.%20Further%20analysis%20shows%20that%0ALoSiA%20also%20reduces%20forgetting%20during%20continued%20training.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/KlozeWang/LoSiA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04487v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoSiA%253A%2520Efficient%2520High-Rank%2520Fine-Tuning%2520via%2520Subnet%2520Localization%2520and%250A%2520%2520Optimization%26entry.906535625%3DXujia%2520Wang%2520and%2520Yunjia%2520Qi%2520and%2520Bin%2520Xu%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%252C%2520such%2520as%2520LoRA%252C%2520significantly%250Areduce%2520the%2520number%2520of%2520trainable%2520parameters%2520by%2520introducing%2520low-rank%2520decomposition%250Amatrices.%2520However%252C%2520existing%2520methods%2520perform%2520extensive%2520matrix%2520multiplications%2520in%250Adomain%2520specialization%2520tasks%252C%2520resulting%2520in%2520computational%2520inefficiency%2520and%250Asub-optimal%2520fine-tuning%2520performance.%2520Hence%252C%2520we%2520propose%2520LoSiA%2528Low-Resources%250ASubnet%2520Integration%2520Adaptation%2529%252C%2520an%2520innovative%2520method%2520that%2520dynamically%2520localizes%250Aand%2520optimizes%2520critical%2520parameters%2520during%2520the%2520training%2520process.%2520Specifically%252C%2520it%250Aidentifies%2520a%2520sub-network%2520using%2520gradient%2520sparsity%2520analysis%2520and%2520optimizes%2520it%2520as%250Athe%2520trainable%2520target.%2520This%2520design%2520enables%2520effective%2520high-rank%2520adaptation%2520by%250Aupdating%2520only%2520the%2520sub-network%2520parameters%252C%2520reducing%2520the%2520additional%2520matrix%250Amultiplication.%2520We%2520also%2520present%2520LoSiA-Pro%252C%2520a%2520faster%2520implementation%2520of%2520LoSiA%252C%250Awhich%2520reduces%2520the%2520training%2520latency%2520by%2520about%2520%252427%255C%2525%2524%2520compared%2520to%2520LoRA.%2520Extensive%250Aevaluations%2520show%2520that%2520our%2520method%2520achieves%2520minimal%2520performance%2520drop%2520compared%2520to%250Afull%2520fine-tuning%252C%2520while%2520requiring%2520the%2520least%2520training%2520time%2520across%2520domain%250Aspecialization%2520and%2520common-sense%2520reasoning%2520tasks.%2520Further%2520analysis%2520shows%2520that%250ALoSiA%2520also%2520reduces%2520forgetting%2520during%2520continued%2520training.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/KlozeWang/LoSiA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04487v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoSiA%3A%20Efficient%20High-Rank%20Fine-Tuning%20via%20Subnet%20Localization%20and%0A%20%20Optimization&entry.906535625=Xujia%20Wang%20and%20Yunjia%20Qi%20and%20Bin%20Xu&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%20significantly%0Areduce%20the%20number%20of%20trainable%20parameters%20by%20introducing%20low-rank%20decomposition%0Amatrices.%20However%2C%20existing%20methods%20perform%20extensive%20matrix%20multiplications%20in%0Adomain%20specialization%20tasks%2C%20resulting%20in%20computational%20inefficiency%20and%0Asub-optimal%20fine-tuning%20performance.%20Hence%2C%20we%20propose%20LoSiA%28Low-Resources%0ASubnet%20Integration%20Adaptation%29%2C%20an%20innovative%20method%20that%20dynamically%20localizes%0Aand%20optimizes%20critical%20parameters%20during%20the%20training%20process.%20Specifically%2C%20it%0Aidentifies%20a%20sub-network%20using%20gradient%20sparsity%20analysis%20and%20optimizes%20it%20as%0Athe%20trainable%20target.%20This%20design%20enables%20effective%20high-rank%20adaptation%20by%0Aupdating%20only%20the%20sub-network%20parameters%2C%20reducing%20the%20additional%20matrix%0Amultiplication.%20We%20also%20present%20LoSiA-Pro%2C%20a%20faster%20implementation%20of%20LoSiA%2C%0Awhich%20reduces%20the%20training%20latency%20by%20about%20%2427%5C%25%24%20compared%20to%20LoRA.%20Extensive%0Aevaluations%20show%20that%20our%20method%20achieves%20minimal%20performance%20drop%20compared%20to%0Afull%20fine-tuning%2C%20while%20requiring%20the%20least%20training%20time%20across%20domain%0Aspecialization%20and%20common-sense%20reasoning%20tasks.%20Further%20analysis%20shows%20that%0ALoSiA%20also%20reduces%20forgetting%20during%20continued%20training.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/KlozeWang/LoSiA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04487v3&entry.124074799=Read"},
{"title": "Adversarial Hospital-Invariant Feature Learning for WSI Patch\n  Classification", "author": "Mengliang Zhang and Jacob M. Luber", "abstract": "  Pathology foundation models (PFMs) have demonstrated remarkable potential in\nwhole-slide image (WSI) diagnosis. However, pathology images from different\nhospitals often vary due to differences in scanning hardware and preprocessing\nstyles, which may lead PFMs to inadvertently learn hospital-specific features,\nposing risks for clinical deployment. In this work, we present the first\nsystematic study of domain bias in PFMs arising from hospital source\ncharacteristics. Specifically, we (1) construct a pipeline for quantifying\ndomain bias in PFMs, (2) evaluate and compare the performance of multiple\nmodels, and (3) propose a lightweight adversarial framework that removes latent\nhospital-specific features from frozen representations without modifying the\nencoder itself. By introducing a trainable adapter and a domain classifier\nconnected through a gradient reversal layer (GRL), our method learns\ntask-discriminative yet domain-invariant representations. Experiments on\nmulti-center histopathology datasets demonstrate that our approach\nsubstantially reduces domain predictability while maintaining or even improving\ndisease classification performance, particularly in out-of-domain (unseen\nhospital) scenarios. Further analyses, including hospital detection and feature\nspace visualization, confirm the effectiveness of our method in mitigating\nhospital bias. We will provide our code based on acceptance.\n", "link": "http://arxiv.org/abs/2508.14779v1", "date": "2025-08-20", "relevancy": 2.3572, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4688}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Hospital-Invariant%20Feature%20Learning%20for%20WSI%20Patch%0A%20%20Classification&body=Title%3A%20Adversarial%20Hospital-Invariant%20Feature%20Learning%20for%20WSI%20Patch%0A%20%20Classification%0AAuthor%3A%20Mengliang%20Zhang%20and%20Jacob%20M.%20Luber%0AAbstract%3A%20%20%20Pathology%20foundation%20models%20%28PFMs%29%20have%20demonstrated%20remarkable%20potential%20in%0Awhole-slide%20image%20%28WSI%29%20diagnosis.%20However%2C%20pathology%20images%20from%20different%0Ahospitals%20often%20vary%20due%20to%20differences%20in%20scanning%20hardware%20and%20preprocessing%0Astyles%2C%20which%20may%20lead%20PFMs%20to%20inadvertently%20learn%20hospital-specific%20features%2C%0Aposing%20risks%20for%20clinical%20deployment.%20In%20this%20work%2C%20we%20present%20the%20first%0Asystematic%20study%20of%20domain%20bias%20in%20PFMs%20arising%20from%20hospital%20source%0Acharacteristics.%20Specifically%2C%20we%20%281%29%20construct%20a%20pipeline%20for%20quantifying%0Adomain%20bias%20in%20PFMs%2C%20%282%29%20evaluate%20and%20compare%20the%20performance%20of%20multiple%0Amodels%2C%20and%20%283%29%20propose%20a%20lightweight%20adversarial%20framework%20that%20removes%20latent%0Ahospital-specific%20features%20from%20frozen%20representations%20without%20modifying%20the%0Aencoder%20itself.%20By%20introducing%20a%20trainable%20adapter%20and%20a%20domain%20classifier%0Aconnected%20through%20a%20gradient%20reversal%20layer%20%28GRL%29%2C%20our%20method%20learns%0Atask-discriminative%20yet%20domain-invariant%20representations.%20Experiments%20on%0Amulti-center%20histopathology%20datasets%20demonstrate%20that%20our%20approach%0Asubstantially%20reduces%20domain%20predictability%20while%20maintaining%20or%20even%20improving%0Adisease%20classification%20performance%2C%20particularly%20in%20out-of-domain%20%28unseen%0Ahospital%29%20scenarios.%20Further%20analyses%2C%20including%20hospital%20detection%20and%20feature%0Aspace%20visualization%2C%20confirm%20the%20effectiveness%20of%20our%20method%20in%20mitigating%0Ahospital%20bias.%20We%20will%20provide%20our%20code%20based%20on%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Hospital-Invariant%2520Feature%2520Learning%2520for%2520WSI%2520Patch%250A%2520%2520Classification%26entry.906535625%3DMengliang%2520Zhang%2520and%2520Jacob%2520M.%2520Luber%26entry.1292438233%3D%2520%2520Pathology%2520foundation%2520models%2520%2528PFMs%2529%2520have%2520demonstrated%2520remarkable%2520potential%2520in%250Awhole-slide%2520image%2520%2528WSI%2529%2520diagnosis.%2520However%252C%2520pathology%2520images%2520from%2520different%250Ahospitals%2520often%2520vary%2520due%2520to%2520differences%2520in%2520scanning%2520hardware%2520and%2520preprocessing%250Astyles%252C%2520which%2520may%2520lead%2520PFMs%2520to%2520inadvertently%2520learn%2520hospital-specific%2520features%252C%250Aposing%2520risks%2520for%2520clinical%2520deployment.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%250Asystematic%2520study%2520of%2520domain%2520bias%2520in%2520PFMs%2520arising%2520from%2520hospital%2520source%250Acharacteristics.%2520Specifically%252C%2520we%2520%25281%2529%2520construct%2520a%2520pipeline%2520for%2520quantifying%250Adomain%2520bias%2520in%2520PFMs%252C%2520%25282%2529%2520evaluate%2520and%2520compare%2520the%2520performance%2520of%2520multiple%250Amodels%252C%2520and%2520%25283%2529%2520propose%2520a%2520lightweight%2520adversarial%2520framework%2520that%2520removes%2520latent%250Ahospital-specific%2520features%2520from%2520frozen%2520representations%2520without%2520modifying%2520the%250Aencoder%2520itself.%2520By%2520introducing%2520a%2520trainable%2520adapter%2520and%2520a%2520domain%2520classifier%250Aconnected%2520through%2520a%2520gradient%2520reversal%2520layer%2520%2528GRL%2529%252C%2520our%2520method%2520learns%250Atask-discriminative%2520yet%2520domain-invariant%2520representations.%2520Experiments%2520on%250Amulti-center%2520histopathology%2520datasets%2520demonstrate%2520that%2520our%2520approach%250Asubstantially%2520reduces%2520domain%2520predictability%2520while%2520maintaining%2520or%2520even%2520improving%250Adisease%2520classification%2520performance%252C%2520particularly%2520in%2520out-of-domain%2520%2528unseen%250Ahospital%2529%2520scenarios.%2520Further%2520analyses%252C%2520including%2520hospital%2520detection%2520and%2520feature%250Aspace%2520visualization%252C%2520confirm%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520mitigating%250Ahospital%2520bias.%2520We%2520will%2520provide%2520our%2520code%2520based%2520on%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Hospital-Invariant%20Feature%20Learning%20for%20WSI%20Patch%0A%20%20Classification&entry.906535625=Mengliang%20Zhang%20and%20Jacob%20M.%20Luber&entry.1292438233=%20%20Pathology%20foundation%20models%20%28PFMs%29%20have%20demonstrated%20remarkable%20potential%20in%0Awhole-slide%20image%20%28WSI%29%20diagnosis.%20However%2C%20pathology%20images%20from%20different%0Ahospitals%20often%20vary%20due%20to%20differences%20in%20scanning%20hardware%20and%20preprocessing%0Astyles%2C%20which%20may%20lead%20PFMs%20to%20inadvertently%20learn%20hospital-specific%20features%2C%0Aposing%20risks%20for%20clinical%20deployment.%20In%20this%20work%2C%20we%20present%20the%20first%0Asystematic%20study%20of%20domain%20bias%20in%20PFMs%20arising%20from%20hospital%20source%0Acharacteristics.%20Specifically%2C%20we%20%281%29%20construct%20a%20pipeline%20for%20quantifying%0Adomain%20bias%20in%20PFMs%2C%20%282%29%20evaluate%20and%20compare%20the%20performance%20of%20multiple%0Amodels%2C%20and%20%283%29%20propose%20a%20lightweight%20adversarial%20framework%20that%20removes%20latent%0Ahospital-specific%20features%20from%20frozen%20representations%20without%20modifying%20the%0Aencoder%20itself.%20By%20introducing%20a%20trainable%20adapter%20and%20a%20domain%20classifier%0Aconnected%20through%20a%20gradient%20reversal%20layer%20%28GRL%29%2C%20our%20method%20learns%0Atask-discriminative%20yet%20domain-invariant%20representations.%20Experiments%20on%0Amulti-center%20histopathology%20datasets%20demonstrate%20that%20our%20approach%0Asubstantially%20reduces%20domain%20predictability%20while%20maintaining%20or%20even%20improving%0Adisease%20classification%20performance%2C%20particularly%20in%20out-of-domain%20%28unseen%0Ahospital%29%20scenarios.%20Further%20analyses%2C%20including%20hospital%20detection%20and%20feature%0Aspace%20visualization%2C%20confirm%20the%20effectiveness%20of%20our%20method%20in%20mitigating%0Ahospital%20bias.%20We%20will%20provide%20our%20code%20based%20on%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14779v1&entry.124074799=Read"},
{"title": "MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling", "author": "Qian Wang and Ziqi Huang and Ruoxi Jia and Paul Debevec and Ning Yu", "abstract": "  Despite recent advances, long-sequence video generation frameworks still\nsuffer from significant limitations: poor assistive capability, suboptimal\nvisual quality, and limited expressiveness. To mitigate these limitations, we\npropose MAViS, an end-to-end multi-agent collaborative framework for\nlong-sequence video storytelling. MAViS orchestrates specialized agents across\nmultiple stages, including script writing, shot designing, character modeling,\nkeyframe generation, video animation, and audio generation. In each stage,\nagents operate under the 3E Principle -- Explore, Examine, and Enhance -- to\nensure the completeness of intermediate outputs. Considering the capability\nlimitations of current generative models, we propose the Script Writing\nGuidelines to optimize compatibility between scripts and generative tools.\nExperimental results demonstrate that MAViS achieves state-of-the-art\nperformance in assistive capability, visual quality, and video expressiveness.\nIts modular framework further enables scalability with diverse generative\nmodels and tools. With just a brief user prompt, MAViS is capable of producing\nhigh-quality, expressive long-sequence video storytelling, enriching\ninspirations and creativity for users. To the best of our knowledge, MAViS is\nthe only framework that provides multimodal design output -- videos with\nnarratives and background music.\n", "link": "http://arxiv.org/abs/2508.08487v3", "date": "2025-08-20", "relevancy": 2.3279, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6029}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5711}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAViS%3A%20A%20Multi-Agent%20Framework%20for%20Long-Sequence%20Video%20Storytelling&body=Title%3A%20MAViS%3A%20A%20Multi-Agent%20Framework%20for%20Long-Sequence%20Video%20Storytelling%0AAuthor%3A%20Qian%20Wang%20and%20Ziqi%20Huang%20and%20Ruoxi%20Jia%20and%20Paul%20Debevec%20and%20Ning%20Yu%0AAbstract%3A%20%20%20Despite%20recent%20advances%2C%20long-sequence%20video%20generation%20frameworks%20still%0Asuffer%20from%20significant%20limitations%3A%20poor%20assistive%20capability%2C%20suboptimal%0Avisual%20quality%2C%20and%20limited%20expressiveness.%20To%20mitigate%20these%20limitations%2C%20we%0Apropose%20MAViS%2C%20an%20end-to-end%20multi-agent%20collaborative%20framework%20for%0Along-sequence%20video%20storytelling.%20MAViS%20orchestrates%20specialized%20agents%20across%0Amultiple%20stages%2C%20including%20script%20writing%2C%20shot%20designing%2C%20character%20modeling%2C%0Akeyframe%20generation%2C%20video%20animation%2C%20and%20audio%20generation.%20In%20each%20stage%2C%0Aagents%20operate%20under%20the%203E%20Principle%20--%20Explore%2C%20Examine%2C%20and%20Enhance%20--%20to%0Aensure%20the%20completeness%20of%20intermediate%20outputs.%20Considering%20the%20capability%0Alimitations%20of%20current%20generative%20models%2C%20we%20propose%20the%20Script%20Writing%0AGuidelines%20to%20optimize%20compatibility%20between%20scripts%20and%20generative%20tools.%0AExperimental%20results%20demonstrate%20that%20MAViS%20achieves%20state-of-the-art%0Aperformance%20in%20assistive%20capability%2C%20visual%20quality%2C%20and%20video%20expressiveness.%0AIts%20modular%20framework%20further%20enables%20scalability%20with%20diverse%20generative%0Amodels%20and%20tools.%20With%20just%20a%20brief%20user%20prompt%2C%20MAViS%20is%20capable%20of%20producing%0Ahigh-quality%2C%20expressive%20long-sequence%20video%20storytelling%2C%20enriching%0Ainspirations%20and%20creativity%20for%20users.%20To%20the%20best%20of%20our%20knowledge%2C%20MAViS%20is%0Athe%20only%20framework%20that%20provides%20multimodal%20design%20output%20--%20videos%20with%0Anarratives%20and%20background%20music.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08487v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAViS%253A%2520A%2520Multi-Agent%2520Framework%2520for%2520Long-Sequence%2520Video%2520Storytelling%26entry.906535625%3DQian%2520Wang%2520and%2520Ziqi%2520Huang%2520and%2520Ruoxi%2520Jia%2520and%2520Paul%2520Debevec%2520and%2520Ning%2520Yu%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%252C%2520long-sequence%2520video%2520generation%2520frameworks%2520still%250Asuffer%2520from%2520significant%2520limitations%253A%2520poor%2520assistive%2520capability%252C%2520suboptimal%250Avisual%2520quality%252C%2520and%2520limited%2520expressiveness.%2520To%2520mitigate%2520these%2520limitations%252C%2520we%250Apropose%2520MAViS%252C%2520an%2520end-to-end%2520multi-agent%2520collaborative%2520framework%2520for%250Along-sequence%2520video%2520storytelling.%2520MAViS%2520orchestrates%2520specialized%2520agents%2520across%250Amultiple%2520stages%252C%2520including%2520script%2520writing%252C%2520shot%2520designing%252C%2520character%2520modeling%252C%250Akeyframe%2520generation%252C%2520video%2520animation%252C%2520and%2520audio%2520generation.%2520In%2520each%2520stage%252C%250Aagents%2520operate%2520under%2520the%25203E%2520Principle%2520--%2520Explore%252C%2520Examine%252C%2520and%2520Enhance%2520--%2520to%250Aensure%2520the%2520completeness%2520of%2520intermediate%2520outputs.%2520Considering%2520the%2520capability%250Alimitations%2520of%2520current%2520generative%2520models%252C%2520we%2520propose%2520the%2520Script%2520Writing%250AGuidelines%2520to%2520optimize%2520compatibility%2520between%2520scripts%2520and%2520generative%2520tools.%250AExperimental%2520results%2520demonstrate%2520that%2520MAViS%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520assistive%2520capability%252C%2520visual%2520quality%252C%2520and%2520video%2520expressiveness.%250AIts%2520modular%2520framework%2520further%2520enables%2520scalability%2520with%2520diverse%2520generative%250Amodels%2520and%2520tools.%2520With%2520just%2520a%2520brief%2520user%2520prompt%252C%2520MAViS%2520is%2520capable%2520of%2520producing%250Ahigh-quality%252C%2520expressive%2520long-sequence%2520video%2520storytelling%252C%2520enriching%250Ainspirations%2520and%2520creativity%2520for%2520users.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520MAViS%2520is%250Athe%2520only%2520framework%2520that%2520provides%2520multimodal%2520design%2520output%2520--%2520videos%2520with%250Anarratives%2520and%2520background%2520music.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08487v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAViS%3A%20A%20Multi-Agent%20Framework%20for%20Long-Sequence%20Video%20Storytelling&entry.906535625=Qian%20Wang%20and%20Ziqi%20Huang%20and%20Ruoxi%20Jia%20and%20Paul%20Debevec%20and%20Ning%20Yu&entry.1292438233=%20%20Despite%20recent%20advances%2C%20long-sequence%20video%20generation%20frameworks%20still%0Asuffer%20from%20significant%20limitations%3A%20poor%20assistive%20capability%2C%20suboptimal%0Avisual%20quality%2C%20and%20limited%20expressiveness.%20To%20mitigate%20these%20limitations%2C%20we%0Apropose%20MAViS%2C%20an%20end-to-end%20multi-agent%20collaborative%20framework%20for%0Along-sequence%20video%20storytelling.%20MAViS%20orchestrates%20specialized%20agents%20across%0Amultiple%20stages%2C%20including%20script%20writing%2C%20shot%20designing%2C%20character%20modeling%2C%0Akeyframe%20generation%2C%20video%20animation%2C%20and%20audio%20generation.%20In%20each%20stage%2C%0Aagents%20operate%20under%20the%203E%20Principle%20--%20Explore%2C%20Examine%2C%20and%20Enhance%20--%20to%0Aensure%20the%20completeness%20of%20intermediate%20outputs.%20Considering%20the%20capability%0Alimitations%20of%20current%20generative%20models%2C%20we%20propose%20the%20Script%20Writing%0AGuidelines%20to%20optimize%20compatibility%20between%20scripts%20and%20generative%20tools.%0AExperimental%20results%20demonstrate%20that%20MAViS%20achieves%20state-of-the-art%0Aperformance%20in%20assistive%20capability%2C%20visual%20quality%2C%20and%20video%20expressiveness.%0AIts%20modular%20framework%20further%20enables%20scalability%20with%20diverse%20generative%0Amodels%20and%20tools.%20With%20just%20a%20brief%20user%20prompt%2C%20MAViS%20is%20capable%20of%20producing%0Ahigh-quality%2C%20expressive%20long-sequence%20video%20storytelling%2C%20enriching%0Ainspirations%20and%20creativity%20for%20users.%20To%20the%20best%20of%20our%20knowledge%2C%20MAViS%20is%0Athe%20only%20framework%20that%20provides%20multimodal%20design%20output%20--%20videos%20with%0Anarratives%20and%20background%20music.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08487v3&entry.124074799=Read"},
{"title": "UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud\n  Video Modeling", "author": "Peiming Li and Ziyi Wang and Yulin Yuan and Hong Liu and Xiangming Meng and Junsong Yuan and Mengyuan Liu", "abstract": "  Point cloud videos capture dynamic 3D motion while reducing the effects of\nlighting and viewpoint variations, making them highly effective for recognizing\nsubtle and continuous human actions. Although Selective State Space Models\n(SSMs) have shown good performance in sequence modeling with linear complexity,\nthe spatio-temporal disorder of point cloud videos hinders their unidirectional\nmodeling when directly unfolding the point cloud video into a 1D sequence\nthrough temporally sequential scanning. To address this challenge, we propose\nthe Unified Spatio-Temporal State Space Model (UST-SSM), which extends the\nlatest advancements in SSMs to point cloud videos. Specifically, we introduce\nSpatial-Temporal Selection Scanning (STSS), which reorganizes unordered points\ninto semantic-aware sequences through prompt-guided clustering, thereby\nenabling the effective utilization of points that are spatially and temporally\ndistant yet similar within the sequence. For missing 4D geometric and motion\ndetails, Spatio-Temporal Structure Aggregation (STSA) aggregates\nspatio-temporal features and compensates. To improve temporal interaction\nwithin the sampled sequence, Temporal Interaction Sampling (TIS) enhances\nfine-grained temporal dependencies through non-anchor frame utilization and\nexpanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D,\nand Synthia 4D datasets validate the effectiveness of our method. Our code is\navailable at https://github.com/wangzy01/UST-SSM.\n", "link": "http://arxiv.org/abs/2508.14604v1", "date": "2025-08-20", "relevancy": 2.3273, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.591}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5853}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UST-SSM%3A%20Unified%20Spatio-Temporal%20State%20Space%20Models%20for%20Point%20Cloud%0A%20%20Video%20Modeling&body=Title%3A%20UST-SSM%3A%20Unified%20Spatio-Temporal%20State%20Space%20Models%20for%20Point%20Cloud%0A%20%20Video%20Modeling%0AAuthor%3A%20Peiming%20Li%20and%20Ziyi%20Wang%20and%20Yulin%20Yuan%20and%20Hong%20Liu%20and%20Xiangming%20Meng%20and%20Junsong%20Yuan%20and%20Mengyuan%20Liu%0AAbstract%3A%20%20%20Point%20cloud%20videos%20capture%20dynamic%203D%20motion%20while%20reducing%20the%20effects%20of%0Alighting%20and%20viewpoint%20variations%2C%20making%20them%20highly%20effective%20for%20recognizing%0Asubtle%20and%20continuous%20human%20actions.%20Although%20Selective%20State%20Space%20Models%0A%28SSMs%29%20have%20shown%20good%20performance%20in%20sequence%20modeling%20with%20linear%20complexity%2C%0Athe%20spatio-temporal%20disorder%20of%20point%20cloud%20videos%20hinders%20their%20unidirectional%0Amodeling%20when%20directly%20unfolding%20the%20point%20cloud%20video%20into%20a%201D%20sequence%0Athrough%20temporally%20sequential%20scanning.%20To%20address%20this%20challenge%2C%20we%20propose%0Athe%20Unified%20Spatio-Temporal%20State%20Space%20Model%20%28UST-SSM%29%2C%20which%20extends%20the%0Alatest%20advancements%20in%20SSMs%20to%20point%20cloud%20videos.%20Specifically%2C%20we%20introduce%0ASpatial-Temporal%20Selection%20Scanning%20%28STSS%29%2C%20which%20reorganizes%20unordered%20points%0Ainto%20semantic-aware%20sequences%20through%20prompt-guided%20clustering%2C%20thereby%0Aenabling%20the%20effective%20utilization%20of%20points%20that%20are%20spatially%20and%20temporally%0Adistant%20yet%20similar%20within%20the%20sequence.%20For%20missing%204D%20geometric%20and%20motion%0Adetails%2C%20Spatio-Temporal%20Structure%20Aggregation%20%28STSA%29%20aggregates%0Aspatio-temporal%20features%20and%20compensates.%20To%20improve%20temporal%20interaction%0Awithin%20the%20sampled%20sequence%2C%20Temporal%20Interaction%20Sampling%20%28TIS%29%20enhances%0Afine-grained%20temporal%20dependencies%20through%20non-anchor%20frame%20utilization%20and%0Aexpanded%20receptive%20fields.%20Experimental%20results%20on%20the%20MSR-Action3D%2C%20NTU%20RGB%2BD%2C%0Aand%20Synthia%204D%20datasets%20validate%20the%20effectiveness%20of%20our%20method.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/wangzy01/UST-SSM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUST-SSM%253A%2520Unified%2520Spatio-Temporal%2520State%2520Space%2520Models%2520for%2520Point%2520Cloud%250A%2520%2520Video%2520Modeling%26entry.906535625%3DPeiming%2520Li%2520and%2520Ziyi%2520Wang%2520and%2520Yulin%2520Yuan%2520and%2520Hong%2520Liu%2520and%2520Xiangming%2520Meng%2520and%2520Junsong%2520Yuan%2520and%2520Mengyuan%2520Liu%26entry.1292438233%3D%2520%2520Point%2520cloud%2520videos%2520capture%2520dynamic%25203D%2520motion%2520while%2520reducing%2520the%2520effects%2520of%250Alighting%2520and%2520viewpoint%2520variations%252C%2520making%2520them%2520highly%2520effective%2520for%2520recognizing%250Asubtle%2520and%2520continuous%2520human%2520actions.%2520Although%2520Selective%2520State%2520Space%2520Models%250A%2528SSMs%2529%2520have%2520shown%2520good%2520performance%2520in%2520sequence%2520modeling%2520with%2520linear%2520complexity%252C%250Athe%2520spatio-temporal%2520disorder%2520of%2520point%2520cloud%2520videos%2520hinders%2520their%2520unidirectional%250Amodeling%2520when%2520directly%2520unfolding%2520the%2520point%2520cloud%2520video%2520into%2520a%25201D%2520sequence%250Athrough%2520temporally%2520sequential%2520scanning.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250Athe%2520Unified%2520Spatio-Temporal%2520State%2520Space%2520Model%2520%2528UST-SSM%2529%252C%2520which%2520extends%2520the%250Alatest%2520advancements%2520in%2520SSMs%2520to%2520point%2520cloud%2520videos.%2520Specifically%252C%2520we%2520introduce%250ASpatial-Temporal%2520Selection%2520Scanning%2520%2528STSS%2529%252C%2520which%2520reorganizes%2520unordered%2520points%250Ainto%2520semantic-aware%2520sequences%2520through%2520prompt-guided%2520clustering%252C%2520thereby%250Aenabling%2520the%2520effective%2520utilization%2520of%2520points%2520that%2520are%2520spatially%2520and%2520temporally%250Adistant%2520yet%2520similar%2520within%2520the%2520sequence.%2520For%2520missing%25204D%2520geometric%2520and%2520motion%250Adetails%252C%2520Spatio-Temporal%2520Structure%2520Aggregation%2520%2528STSA%2529%2520aggregates%250Aspatio-temporal%2520features%2520and%2520compensates.%2520To%2520improve%2520temporal%2520interaction%250Awithin%2520the%2520sampled%2520sequence%252C%2520Temporal%2520Interaction%2520Sampling%2520%2528TIS%2529%2520enhances%250Afine-grained%2520temporal%2520dependencies%2520through%2520non-anchor%2520frame%2520utilization%2520and%250Aexpanded%2520receptive%2520fields.%2520Experimental%2520results%2520on%2520the%2520MSR-Action3D%252C%2520NTU%2520RGB%252BD%252C%250Aand%2520Synthia%25204D%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520our%2520method.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/wangzy01/UST-SSM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UST-SSM%3A%20Unified%20Spatio-Temporal%20State%20Space%20Models%20for%20Point%20Cloud%0A%20%20Video%20Modeling&entry.906535625=Peiming%20Li%20and%20Ziyi%20Wang%20and%20Yulin%20Yuan%20and%20Hong%20Liu%20and%20Xiangming%20Meng%20and%20Junsong%20Yuan%20and%20Mengyuan%20Liu&entry.1292438233=%20%20Point%20cloud%20videos%20capture%20dynamic%203D%20motion%20while%20reducing%20the%20effects%20of%0Alighting%20and%20viewpoint%20variations%2C%20making%20them%20highly%20effective%20for%20recognizing%0Asubtle%20and%20continuous%20human%20actions.%20Although%20Selective%20State%20Space%20Models%0A%28SSMs%29%20have%20shown%20good%20performance%20in%20sequence%20modeling%20with%20linear%20complexity%2C%0Athe%20spatio-temporal%20disorder%20of%20point%20cloud%20videos%20hinders%20their%20unidirectional%0Amodeling%20when%20directly%20unfolding%20the%20point%20cloud%20video%20into%20a%201D%20sequence%0Athrough%20temporally%20sequential%20scanning.%20To%20address%20this%20challenge%2C%20we%20propose%0Athe%20Unified%20Spatio-Temporal%20State%20Space%20Model%20%28UST-SSM%29%2C%20which%20extends%20the%0Alatest%20advancements%20in%20SSMs%20to%20point%20cloud%20videos.%20Specifically%2C%20we%20introduce%0ASpatial-Temporal%20Selection%20Scanning%20%28STSS%29%2C%20which%20reorganizes%20unordered%20points%0Ainto%20semantic-aware%20sequences%20through%20prompt-guided%20clustering%2C%20thereby%0Aenabling%20the%20effective%20utilization%20of%20points%20that%20are%20spatially%20and%20temporally%0Adistant%20yet%20similar%20within%20the%20sequence.%20For%20missing%204D%20geometric%20and%20motion%0Adetails%2C%20Spatio-Temporal%20Structure%20Aggregation%20%28STSA%29%20aggregates%0Aspatio-temporal%20features%20and%20compensates.%20To%20improve%20temporal%20interaction%0Awithin%20the%20sampled%20sequence%2C%20Temporal%20Interaction%20Sampling%20%28TIS%29%20enhances%0Afine-grained%20temporal%20dependencies%20through%20non-anchor%20frame%20utilization%20and%0Aexpanded%20receptive%20fields.%20Experimental%20results%20on%20the%20MSR-Action3D%2C%20NTU%20RGB%2BD%2C%0Aand%20Synthia%204D%20datasets%20validate%20the%20effectiveness%20of%20our%20method.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/wangzy01/UST-SSM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14604v1&entry.124074799=Read"},
{"title": "CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via\n  Chain-of-Thought Reasoning", "author": "Jeonghyo Song and Kimin Yun and DaeUng Jo and Jinyoung Kim and Youngjoon Yoo", "abstract": "  Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the\nreliability of semantic segmentation models, particularly in complex road\nenvironments where safety and accuracy are paramount. Despite recent\nadvancements in large language models (LLMs), notably GPT-4, which\nsignificantly enhanced multimodal reasoning through Chain-of-Thought (CoT)\nprompting, the application of CoT-based visual reasoning for OOD semantic\nsegmentation remains largely unexplored. In this paper, through extensive\nanalyses of the road scene anomalies, we identify three challenging scenarios\nwhere current state-of-the-art OOD segmentation methods consistently struggle:\n(1) densely packed and overlapping objects, (2) distant scenes with small\nobjects, and (3) large foreground-dominant objects. To address the presented\nchallenges, we propose a novel CoT-based framework targeting OOD detection in\nroad anomaly scenes. Our method leverages the extensive knowledge and reasoning\ncapabilities of foundation models, such as GPT-4, to enhance OOD detection\nthrough improved image understanding and prompt-based reasoning aligned with\nobserved problematic scene attributes. Extensive experiments show that our\nframework consistently outperforms state-of-the-art methods on both standard\nbenchmarks and our newly defined challenging subset of the RoadAnomaly dataset,\noffering a robust and interpretable solution for OOD semantic segmentation in\ncomplex driving environments.\n", "link": "http://arxiv.org/abs/2507.03984v2", "date": "2025-08-20", "relevancy": 2.2887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoT-Segmenter%3A%20Enhancing%20OOD%20Detection%20in%20Dense%20Road%20Scenes%20via%0A%20%20Chain-of-Thought%20Reasoning&body=Title%3A%20CoT-Segmenter%3A%20Enhancing%20OOD%20Detection%20in%20Dense%20Road%20Scenes%20via%0A%20%20Chain-of-Thought%20Reasoning%0AAuthor%3A%20Jeonghyo%20Song%20and%20Kimin%20Yun%20and%20DaeUng%20Jo%20and%20Jinyoung%20Kim%20and%20Youngjoon%20Yoo%0AAbstract%3A%20%20%20Effective%20Out-of-Distribution%20%28OOD%29%20detection%20is%20criti-cal%20for%20ensuring%20the%0Areliability%20of%20semantic%20segmentation%20models%2C%20particularly%20in%20complex%20road%0Aenvironments%20where%20safety%20and%20accuracy%20are%20paramount.%20Despite%20recent%0Aadvancements%20in%20large%20language%20models%20%28LLMs%29%2C%20notably%20GPT-4%2C%20which%0Asignificantly%20enhanced%20multimodal%20reasoning%20through%20Chain-of-Thought%20%28CoT%29%0Aprompting%2C%20the%20application%20of%20CoT-based%20visual%20reasoning%20for%20OOD%20semantic%0Asegmentation%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20through%20extensive%0Aanalyses%20of%20the%20road%20scene%20anomalies%2C%20we%20identify%20three%20challenging%20scenarios%0Awhere%20current%20state-of-the-art%20OOD%20segmentation%20methods%20consistently%20struggle%3A%0A%281%29%20densely%20packed%20and%20overlapping%20objects%2C%20%282%29%20distant%20scenes%20with%20small%0Aobjects%2C%20and%20%283%29%20large%20foreground-dominant%20objects.%20To%20address%20the%20presented%0Achallenges%2C%20we%20propose%20a%20novel%20CoT-based%20framework%20targeting%20OOD%20detection%20in%0Aroad%20anomaly%20scenes.%20Our%20method%20leverages%20the%20extensive%20knowledge%20and%20reasoning%0Acapabilities%20of%20foundation%20models%2C%20such%20as%20GPT-4%2C%20to%20enhance%20OOD%20detection%0Athrough%20improved%20image%20understanding%20and%20prompt-based%20reasoning%20aligned%20with%0Aobserved%20problematic%20scene%20attributes.%20Extensive%20experiments%20show%20that%20our%0Aframework%20consistently%20outperforms%20state-of-the-art%20methods%20on%20both%20standard%0Abenchmarks%20and%20our%20newly%20defined%20challenging%20subset%20of%20the%20RoadAnomaly%20dataset%2C%0Aoffering%20a%20robust%20and%20interpretable%20solution%20for%20OOD%20semantic%20segmentation%20in%0Acomplex%20driving%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03984v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoT-Segmenter%253A%2520Enhancing%2520OOD%2520Detection%2520in%2520Dense%2520Road%2520Scenes%2520via%250A%2520%2520Chain-of-Thought%2520Reasoning%26entry.906535625%3DJeonghyo%2520Song%2520and%2520Kimin%2520Yun%2520and%2520DaeUng%2520Jo%2520and%2520Jinyoung%2520Kim%2520and%2520Youngjoon%2520Yoo%26entry.1292438233%3D%2520%2520Effective%2520Out-of-Distribution%2520%2528OOD%2529%2520detection%2520is%2520criti-cal%2520for%2520ensuring%2520the%250Areliability%2520of%2520semantic%2520segmentation%2520models%252C%2520particularly%2520in%2520complex%2520road%250Aenvironments%2520where%2520safety%2520and%2520accuracy%2520are%2520paramount.%2520Despite%2520recent%250Aadvancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520notably%2520GPT-4%252C%2520which%250Asignificantly%2520enhanced%2520multimodal%2520reasoning%2520through%2520Chain-of-Thought%2520%2528CoT%2529%250Aprompting%252C%2520the%2520application%2520of%2520CoT-based%2520visual%2520reasoning%2520for%2520OOD%2520semantic%250Asegmentation%2520remains%2520largely%2520unexplored.%2520In%2520this%2520paper%252C%2520through%2520extensive%250Aanalyses%2520of%2520the%2520road%2520scene%2520anomalies%252C%2520we%2520identify%2520three%2520challenging%2520scenarios%250Awhere%2520current%2520state-of-the-art%2520OOD%2520segmentation%2520methods%2520consistently%2520struggle%253A%250A%25281%2529%2520densely%2520packed%2520and%2520overlapping%2520objects%252C%2520%25282%2529%2520distant%2520scenes%2520with%2520small%250Aobjects%252C%2520and%2520%25283%2529%2520large%2520foreground-dominant%2520objects.%2520To%2520address%2520the%2520presented%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520CoT-based%2520framework%2520targeting%2520OOD%2520detection%2520in%250Aroad%2520anomaly%2520scenes.%2520Our%2520method%2520leverages%2520the%2520extensive%2520knowledge%2520and%2520reasoning%250Acapabilities%2520of%2520foundation%2520models%252C%2520such%2520as%2520GPT-4%252C%2520to%2520enhance%2520OOD%2520detection%250Athrough%2520improved%2520image%2520understanding%2520and%2520prompt-based%2520reasoning%2520aligned%2520with%250Aobserved%2520problematic%2520scene%2520attributes.%2520Extensive%2520experiments%2520show%2520that%2520our%250Aframework%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520on%2520both%2520standard%250Abenchmarks%2520and%2520our%2520newly%2520defined%2520challenging%2520subset%2520of%2520the%2520RoadAnomaly%2520dataset%252C%250Aoffering%2520a%2520robust%2520and%2520interpretable%2520solution%2520for%2520OOD%2520semantic%2520segmentation%2520in%250Acomplex%2520driving%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03984v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoT-Segmenter%3A%20Enhancing%20OOD%20Detection%20in%20Dense%20Road%20Scenes%20via%0A%20%20Chain-of-Thought%20Reasoning&entry.906535625=Jeonghyo%20Song%20and%20Kimin%20Yun%20and%20DaeUng%20Jo%20and%20Jinyoung%20Kim%20and%20Youngjoon%20Yoo&entry.1292438233=%20%20Effective%20Out-of-Distribution%20%28OOD%29%20detection%20is%20criti-cal%20for%20ensuring%20the%0Areliability%20of%20semantic%20segmentation%20models%2C%20particularly%20in%20complex%20road%0Aenvironments%20where%20safety%20and%20accuracy%20are%20paramount.%20Despite%20recent%0Aadvancements%20in%20large%20language%20models%20%28LLMs%29%2C%20notably%20GPT-4%2C%20which%0Asignificantly%20enhanced%20multimodal%20reasoning%20through%20Chain-of-Thought%20%28CoT%29%0Aprompting%2C%20the%20application%20of%20CoT-based%20visual%20reasoning%20for%20OOD%20semantic%0Asegmentation%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20through%20extensive%0Aanalyses%20of%20the%20road%20scene%20anomalies%2C%20we%20identify%20three%20challenging%20scenarios%0Awhere%20current%20state-of-the-art%20OOD%20segmentation%20methods%20consistently%20struggle%3A%0A%281%29%20densely%20packed%20and%20overlapping%20objects%2C%20%282%29%20distant%20scenes%20with%20small%0Aobjects%2C%20and%20%283%29%20large%20foreground-dominant%20objects.%20To%20address%20the%20presented%0Achallenges%2C%20we%20propose%20a%20novel%20CoT-based%20framework%20targeting%20OOD%20detection%20in%0Aroad%20anomaly%20scenes.%20Our%20method%20leverages%20the%20extensive%20knowledge%20and%20reasoning%0Acapabilities%20of%20foundation%20models%2C%20such%20as%20GPT-4%2C%20to%20enhance%20OOD%20detection%0Athrough%20improved%20image%20understanding%20and%20prompt-based%20reasoning%20aligned%20with%0Aobserved%20problematic%20scene%20attributes.%20Extensive%20experiments%20show%20that%20our%0Aframework%20consistently%20outperforms%20state-of-the-art%20methods%20on%20both%20standard%0Abenchmarks%20and%20our%20newly%20defined%20challenging%20subset%20of%20the%20RoadAnomaly%20dataset%2C%0Aoffering%20a%20robust%20and%20interpretable%20solution%20for%20OOD%20semantic%20segmentation%20in%0Acomplex%20driving%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03984v2&entry.124074799=Read"},
{"title": "Security Concerns for Large Language Models: A Survey", "author": "Miles Q. Li and Benjamin C. M. Fung", "abstract": "  Large Language Models (LLMs) such as ChatGPT and its competitors have caused\na revolution in natural language processing, but their capabilities also\nintroduce new security vulnerabilities. This survey provides a comprehensive\noverview of these emerging concerns, categorizing threats into several key\nareas: prompt injection and jailbreaking; adversarial attacks, including input\nperturbations and data poisoning; misuse by malicious actors to generate\ndisinformation, phishing emails, and malware; and the worrisome risks inherent\nin autonomous LLM agents. Recently, a significant focus is increasingly being\nplaced on the latter, exploring goal misalignment, emergent deception,\nself-preservation instincts, and the potential for LLMs to develop and pursue\ncovert, misaligned objectives, a behavior known as scheming, which may even\npersist through safety training. We summarize recent academic and industrial\nstudies from 2022 to 2025 that exemplify each threat, analyze proposed defenses\nand their limitations, and identify open challenges in securing LLM-based\napplications. We conclude by emphasizing the importance of advancing robust,\nmulti-layered security strategies to ensure LLMs are safe and beneficial.\n", "link": "http://arxiv.org/abs/2505.18889v4", "date": "2025-08-20", "relevancy": 2.2833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Security%20Concerns%20for%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Security%20Concerns%20for%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Miles%20Q.%20Li%20and%20Benjamin%20C.%20M.%20Fung%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20ChatGPT%20and%20its%20competitors%20have%20caused%0Aa%20revolution%20in%20natural%20language%20processing%2C%20but%20their%20capabilities%20also%0Aintroduce%20new%20security%20vulnerabilities.%20This%20survey%20provides%20a%20comprehensive%0Aoverview%20of%20these%20emerging%20concerns%2C%20categorizing%20threats%20into%20several%20key%0Aareas%3A%20prompt%20injection%20and%20jailbreaking%3B%20adversarial%20attacks%2C%20including%20input%0Aperturbations%20and%20data%20poisoning%3B%20misuse%20by%20malicious%20actors%20to%20generate%0Adisinformation%2C%20phishing%20emails%2C%20and%20malware%3B%20and%20the%20worrisome%20risks%20inherent%0Ain%20autonomous%20LLM%20agents.%20Recently%2C%20a%20significant%20focus%20is%20increasingly%20being%0Aplaced%20on%20the%20latter%2C%20exploring%20goal%20misalignment%2C%20emergent%20deception%2C%0Aself-preservation%20instincts%2C%20and%20the%20potential%20for%20LLMs%20to%20develop%20and%20pursue%0Acovert%2C%20misaligned%20objectives%2C%20a%20behavior%20known%20as%20scheming%2C%20which%20may%20even%0Apersist%20through%20safety%20training.%20We%20summarize%20recent%20academic%20and%20industrial%0Astudies%20from%202022%20to%202025%20that%20exemplify%20each%20threat%2C%20analyze%20proposed%20defenses%0Aand%20their%20limitations%2C%20and%20identify%20open%20challenges%20in%20securing%20LLM-based%0Aapplications.%20We%20conclude%20by%20emphasizing%20the%20importance%20of%20advancing%20robust%2C%0Amulti-layered%20security%20strategies%20to%20ensure%20LLMs%20are%20safe%20and%20beneficial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18889v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecurity%2520Concerns%2520for%2520Large%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DMiles%2520Q.%2520Li%2520and%2520Benjamin%2520C.%2520M.%2520Fung%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520such%2520as%2520ChatGPT%2520and%2520its%2520competitors%2520have%2520caused%250Aa%2520revolution%2520in%2520natural%2520language%2520processing%252C%2520but%2520their%2520capabilities%2520also%250Aintroduce%2520new%2520security%2520vulnerabilities.%2520This%2520survey%2520provides%2520a%2520comprehensive%250Aoverview%2520of%2520these%2520emerging%2520concerns%252C%2520categorizing%2520threats%2520into%2520several%2520key%250Aareas%253A%2520prompt%2520injection%2520and%2520jailbreaking%253B%2520adversarial%2520attacks%252C%2520including%2520input%250Aperturbations%2520and%2520data%2520poisoning%253B%2520misuse%2520by%2520malicious%2520actors%2520to%2520generate%250Adisinformation%252C%2520phishing%2520emails%252C%2520and%2520malware%253B%2520and%2520the%2520worrisome%2520risks%2520inherent%250Ain%2520autonomous%2520LLM%2520agents.%2520Recently%252C%2520a%2520significant%2520focus%2520is%2520increasingly%2520being%250Aplaced%2520on%2520the%2520latter%252C%2520exploring%2520goal%2520misalignment%252C%2520emergent%2520deception%252C%250Aself-preservation%2520instincts%252C%2520and%2520the%2520potential%2520for%2520LLMs%2520to%2520develop%2520and%2520pursue%250Acovert%252C%2520misaligned%2520objectives%252C%2520a%2520behavior%2520known%2520as%2520scheming%252C%2520which%2520may%2520even%250Apersist%2520through%2520safety%2520training.%2520We%2520summarize%2520recent%2520academic%2520and%2520industrial%250Astudies%2520from%25202022%2520to%25202025%2520that%2520exemplify%2520each%2520threat%252C%2520analyze%2520proposed%2520defenses%250Aand%2520their%2520limitations%252C%2520and%2520identify%2520open%2520challenges%2520in%2520securing%2520LLM-based%250Aapplications.%2520We%2520conclude%2520by%2520emphasizing%2520the%2520importance%2520of%2520advancing%2520robust%252C%250Amulti-layered%2520security%2520strategies%2520to%2520ensure%2520LLMs%2520are%2520safe%2520and%2520beneficial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18889v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Security%20Concerns%20for%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Miles%20Q.%20Li%20and%20Benjamin%20C.%20M.%20Fung&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20ChatGPT%20and%20its%20competitors%20have%20caused%0Aa%20revolution%20in%20natural%20language%20processing%2C%20but%20their%20capabilities%20also%0Aintroduce%20new%20security%20vulnerabilities.%20This%20survey%20provides%20a%20comprehensive%0Aoverview%20of%20these%20emerging%20concerns%2C%20categorizing%20threats%20into%20several%20key%0Aareas%3A%20prompt%20injection%20and%20jailbreaking%3B%20adversarial%20attacks%2C%20including%20input%0Aperturbations%20and%20data%20poisoning%3B%20misuse%20by%20malicious%20actors%20to%20generate%0Adisinformation%2C%20phishing%20emails%2C%20and%20malware%3B%20and%20the%20worrisome%20risks%20inherent%0Ain%20autonomous%20LLM%20agents.%20Recently%2C%20a%20significant%20focus%20is%20increasingly%20being%0Aplaced%20on%20the%20latter%2C%20exploring%20goal%20misalignment%2C%20emergent%20deception%2C%0Aself-preservation%20instincts%2C%20and%20the%20potential%20for%20LLMs%20to%20develop%20and%20pursue%0Acovert%2C%20misaligned%20objectives%2C%20a%20behavior%20known%20as%20scheming%2C%20which%20may%20even%0Apersist%20through%20safety%20training.%20We%20summarize%20recent%20academic%20and%20industrial%0Astudies%20from%202022%20to%202025%20that%20exemplify%20each%20threat%2C%20analyze%20proposed%20defenses%0Aand%20their%20limitations%2C%20and%20identify%20open%20challenges%20in%20securing%20LLM-based%0Aapplications.%20We%20conclude%20by%20emphasizing%20the%20importance%20of%20advancing%20robust%2C%0Amulti-layered%20security%20strategies%20to%20ensure%20LLMs%20are%20safe%20and%20beneficial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18889v4&entry.124074799=Read"},
{"title": "Repeating Words for Video-Language Retrieval with Coarse-to-Fine\n  Objectives", "author": "Haoyu Zhao and Jiaxi Gu and Shicong Wang and Xing Zhang and Hang Xu and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  The explosive growth of video streaming presents challenges in achieving high\naccuracy and low training costs for video-language retrieval. However, existing\nmethods rely on large-scale pre-training to improve video retrieval\nperformance, resulting in significant computational demands. Additionally, the\nfine-grained information in videos and texts remains underexplored. To\nalleviate these problems, we propose a novel framework to learn fine-grained\nfeatures for better alignment and introduce an inference pipeline to improve\nperformance without additional training. Specifically, we employ coarse-to-fine\nobjectives to understand the semantic information of video-text pairs,\nincluding contrastive and matching learning. The fine-grained data used for\ntraining is obtained through the Granularity-Aware Representation module, which\nis designed based on similarity analysis between video frames and words in\ncaptions. Furthermore, we observe that the repetition of keywords in the\noriginal captions, referred to as \"Repetition\", can enhance retrieval\nperformance and improve alignment between video and text. Based on this\ninsight, we propose a novel and effective inference pipeline that incorporates\na voting mechanism and a new Matching Entropy metric to achieve better\nretrieval performance without requiring additional pre-training. Experimental\nresults on four benchmarks demonstrate that the proposed method outperforms\nprevious approaches. Additionally, our inference pipeline achieves significant\nperformance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT\ndataset and a 1.6% increase on the DiDeMo dataset.\n", "link": "http://arxiv.org/abs/2508.14812v1", "date": "2025-08-20", "relevancy": 2.2793, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5707}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Repeating%20Words%20for%20Video-Language%20Retrieval%20with%20Coarse-to-Fine%0A%20%20Objectives&body=Title%3A%20Repeating%20Words%20for%20Video-Language%20Retrieval%20with%20Coarse-to-Fine%0A%20%20Objectives%0AAuthor%3A%20Haoyu%20Zhao%20and%20Jiaxi%20Gu%20and%20Shicong%20Wang%20and%20Xing%20Zhang%20and%20Hang%20Xu%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20The%20explosive%20growth%20of%20video%20streaming%20presents%20challenges%20in%20achieving%20high%0Aaccuracy%20and%20low%20training%20costs%20for%20video-language%20retrieval.%20However%2C%20existing%0Amethods%20rely%20on%20large-scale%20pre-training%20to%20improve%20video%20retrieval%0Aperformance%2C%20resulting%20in%20significant%20computational%20demands.%20Additionally%2C%20the%0Afine-grained%20information%20in%20videos%20and%20texts%20remains%20underexplored.%20To%0Aalleviate%20these%20problems%2C%20we%20propose%20a%20novel%20framework%20to%20learn%20fine-grained%0Afeatures%20for%20better%20alignment%20and%20introduce%20an%20inference%20pipeline%20to%20improve%0Aperformance%20without%20additional%20training.%20Specifically%2C%20we%20employ%20coarse-to-fine%0Aobjectives%20to%20understand%20the%20semantic%20information%20of%20video-text%20pairs%2C%0Aincluding%20contrastive%20and%20matching%20learning.%20The%20fine-grained%20data%20used%20for%0Atraining%20is%20obtained%20through%20the%20Granularity-Aware%20Representation%20module%2C%20which%0Ais%20designed%20based%20on%20similarity%20analysis%20between%20video%20frames%20and%20words%20in%0Acaptions.%20Furthermore%2C%20we%20observe%20that%20the%20repetition%20of%20keywords%20in%20the%0Aoriginal%20captions%2C%20referred%20to%20as%20%22Repetition%22%2C%20can%20enhance%20retrieval%0Aperformance%20and%20improve%20alignment%20between%20video%20and%20text.%20Based%20on%20this%0Ainsight%2C%20we%20propose%20a%20novel%20and%20effective%20inference%20pipeline%20that%20incorporates%0Aa%20voting%20mechanism%20and%20a%20new%20Matching%20Entropy%20metric%20to%20achieve%20better%0Aretrieval%20performance%20without%20requiring%20additional%20pre-training.%20Experimental%0Aresults%20on%20four%20benchmarks%20demonstrate%20that%20the%20proposed%20method%20outperforms%0Aprevious%20approaches.%20Additionally%2C%20our%20inference%20pipeline%20achieves%20significant%0Aperformance%20improvements%2C%20with%20a%202.1%25%20increase%20in%20Recall%401%20on%20the%20MSR-VTT%0Adataset%20and%20a%201.6%25%20increase%20on%20the%20DiDeMo%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepeating%2520Words%2520for%2520Video-Language%2520Retrieval%2520with%2520Coarse-to-Fine%250A%2520%2520Objectives%26entry.906535625%3DHaoyu%2520Zhao%2520and%2520Jiaxi%2520Gu%2520and%2520Shicong%2520Wang%2520and%2520Xing%2520Zhang%2520and%2520Hang%2520Xu%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520The%2520explosive%2520growth%2520of%2520video%2520streaming%2520presents%2520challenges%2520in%2520achieving%2520high%250Aaccuracy%2520and%2520low%2520training%2520costs%2520for%2520video-language%2520retrieval.%2520However%252C%2520existing%250Amethods%2520rely%2520on%2520large-scale%2520pre-training%2520to%2520improve%2520video%2520retrieval%250Aperformance%252C%2520resulting%2520in%2520significant%2520computational%2520demands.%2520Additionally%252C%2520the%250Afine-grained%2520information%2520in%2520videos%2520and%2520texts%2520remains%2520underexplored.%2520To%250Aalleviate%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%2520framework%2520to%2520learn%2520fine-grained%250Afeatures%2520for%2520better%2520alignment%2520and%2520introduce%2520an%2520inference%2520pipeline%2520to%2520improve%250Aperformance%2520without%2520additional%2520training.%2520Specifically%252C%2520we%2520employ%2520coarse-to-fine%250Aobjectives%2520to%2520understand%2520the%2520semantic%2520information%2520of%2520video-text%2520pairs%252C%250Aincluding%2520contrastive%2520and%2520matching%2520learning.%2520The%2520fine-grained%2520data%2520used%2520for%250Atraining%2520is%2520obtained%2520through%2520the%2520Granularity-Aware%2520Representation%2520module%252C%2520which%250Ais%2520designed%2520based%2520on%2520similarity%2520analysis%2520between%2520video%2520frames%2520and%2520words%2520in%250Acaptions.%2520Furthermore%252C%2520we%2520observe%2520that%2520the%2520repetition%2520of%2520keywords%2520in%2520the%250Aoriginal%2520captions%252C%2520referred%2520to%2520as%2520%2522Repetition%2522%252C%2520can%2520enhance%2520retrieval%250Aperformance%2520and%2520improve%2520alignment%2520between%2520video%2520and%2520text.%2520Based%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520a%2520novel%2520and%2520effective%2520inference%2520pipeline%2520that%2520incorporates%250Aa%2520voting%2520mechanism%2520and%2520a%2520new%2520Matching%2520Entropy%2520metric%2520to%2520achieve%2520better%250Aretrieval%2520performance%2520without%2520requiring%2520additional%2520pre-training.%2520Experimental%250Aresults%2520on%2520four%2520benchmarks%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%250Aprevious%2520approaches.%2520Additionally%252C%2520our%2520inference%2520pipeline%2520achieves%2520significant%250Aperformance%2520improvements%252C%2520with%2520a%25202.1%2525%2520increase%2520in%2520Recall%25401%2520on%2520the%2520MSR-VTT%250Adataset%2520and%2520a%25201.6%2525%2520increase%2520on%2520the%2520DiDeMo%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Repeating%20Words%20for%20Video-Language%20Retrieval%20with%20Coarse-to-Fine%0A%20%20Objectives&entry.906535625=Haoyu%20Zhao%20and%20Jiaxi%20Gu%20and%20Shicong%20Wang%20and%20Xing%20Zhang%20and%20Hang%20Xu%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20The%20explosive%20growth%20of%20video%20streaming%20presents%20challenges%20in%20achieving%20high%0Aaccuracy%20and%20low%20training%20costs%20for%20video-language%20retrieval.%20However%2C%20existing%0Amethods%20rely%20on%20large-scale%20pre-training%20to%20improve%20video%20retrieval%0Aperformance%2C%20resulting%20in%20significant%20computational%20demands.%20Additionally%2C%20the%0Afine-grained%20information%20in%20videos%20and%20texts%20remains%20underexplored.%20To%0Aalleviate%20these%20problems%2C%20we%20propose%20a%20novel%20framework%20to%20learn%20fine-grained%0Afeatures%20for%20better%20alignment%20and%20introduce%20an%20inference%20pipeline%20to%20improve%0Aperformance%20without%20additional%20training.%20Specifically%2C%20we%20employ%20coarse-to-fine%0Aobjectives%20to%20understand%20the%20semantic%20information%20of%20video-text%20pairs%2C%0Aincluding%20contrastive%20and%20matching%20learning.%20The%20fine-grained%20data%20used%20for%0Atraining%20is%20obtained%20through%20the%20Granularity-Aware%20Representation%20module%2C%20which%0Ais%20designed%20based%20on%20similarity%20analysis%20between%20video%20frames%20and%20words%20in%0Acaptions.%20Furthermore%2C%20we%20observe%20that%20the%20repetition%20of%20keywords%20in%20the%0Aoriginal%20captions%2C%20referred%20to%20as%20%22Repetition%22%2C%20can%20enhance%20retrieval%0Aperformance%20and%20improve%20alignment%20between%20video%20and%20text.%20Based%20on%20this%0Ainsight%2C%20we%20propose%20a%20novel%20and%20effective%20inference%20pipeline%20that%20incorporates%0Aa%20voting%20mechanism%20and%20a%20new%20Matching%20Entropy%20metric%20to%20achieve%20better%0Aretrieval%20performance%20without%20requiring%20additional%20pre-training.%20Experimental%0Aresults%20on%20four%20benchmarks%20demonstrate%20that%20the%20proposed%20method%20outperforms%0Aprevious%20approaches.%20Additionally%2C%20our%20inference%20pipeline%20achieves%20significant%0Aperformance%20improvements%2C%20with%20a%202.1%25%20increase%20in%20Recall%401%20on%20the%20MSR-VTT%0Adataset%20and%20a%201.6%25%20increase%20on%20the%20DiDeMo%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14812v1&entry.124074799=Read"},
{"title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds", "author": "Bingquan Dai and Li Ray Luo and Qihong Tang and Jie Wang and Xinyu Lian and Hao Xu and Minghan Qin and Xudong Xu and Bo Dai and Haoqian Wang and Zhaoyang Lyu and Jiangmiao Pang", "abstract": "  Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.\n", "link": "http://arxiv.org/abs/2508.14879v1", "date": "2025-08-20", "relevancy": 2.2698, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6045}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5448}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshCoder%3A%20LLM-Powered%20Structured%20Mesh%20Code%20Generation%20from%20Point%20Clouds&body=Title%3A%20MeshCoder%3A%20LLM-Powered%20Structured%20Mesh%20Code%20Generation%20from%20Point%20Clouds%0AAuthor%3A%20Bingquan%20Dai%20and%20Li%20Ray%20Luo%20and%20Qihong%20Tang%20and%20Jie%20Wang%20and%20Xinyu%20Lian%20and%20Hao%20Xu%20and%20Minghan%20Qin%20and%20Xudong%20Xu%20and%20Bo%20Dai%20and%20Haoqian%20Wang%20and%20Zhaoyang%20Lyu%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Reconstructing%203D%20objects%20into%20editable%20programs%20is%20pivotal%20for%20applications%0Alike%20reverse%20engineering%20and%20shape%20editing.%20However%2C%20existing%20methods%20often%0Arely%20on%20limited%20domain-specific%20languages%20%28DSLs%29%20and%20small-scale%20datasets%2C%0Arestricting%20their%20ability%20to%20model%20complex%20geometries%20and%20structures.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20MeshCoder%2C%20a%20novel%20framework%20that%0Areconstructs%20complex%203D%20objects%20from%20point%20clouds%20into%20editable%20Blender%20Python%0Ascripts.%20We%20develop%20a%20comprehensive%20set%20of%20expressive%20Blender%20Python%20APIs%0Acapable%20of%20synthesizing%20intricate%20geometries.%20Leveraging%20these%20APIs%2C%20we%0Aconstruct%20a%20large-scale%20paired%20object-code%20dataset%2C%20where%20the%20code%20for%20each%0Aobject%20is%20decomposed%20into%20distinct%20semantic%20parts.%20Subsequently%2C%20we%20train%20a%0Amultimodal%20large%20language%20model%20%28LLM%29%20that%20translates%203D%20point%20cloud%20into%0Aexecutable%20Blender%20Python%20scripts.%20Our%20approach%20not%20only%20achieves%20superior%0Aperformance%20in%20shape-to-code%20reconstruction%20tasks%20but%20also%20facilitates%0Aintuitive%20geometric%20and%20topological%20editing%20through%20convenient%20code%0Amodifications.%20Furthermore%2C%20our%20code-based%20representation%20enhances%20the%0Areasoning%20capabilities%20of%20LLMs%20in%203D%20shape%20understanding%20tasks.%20Together%2C%20these%0Acontributions%20establish%20MeshCoder%20as%20a%20powerful%20and%20flexible%20solution%20for%0Aprogrammatic%203D%20shape%20reconstruction%20and%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshCoder%253A%2520LLM-Powered%2520Structured%2520Mesh%2520Code%2520Generation%2520from%2520Point%2520Clouds%26entry.906535625%3DBingquan%2520Dai%2520and%2520Li%2520Ray%2520Luo%2520and%2520Qihong%2520Tang%2520and%2520Jie%2520Wang%2520and%2520Xinyu%2520Lian%2520and%2520Hao%2520Xu%2520and%2520Minghan%2520Qin%2520and%2520Xudong%2520Xu%2520and%2520Bo%2520Dai%2520and%2520Haoqian%2520Wang%2520and%2520Zhaoyang%2520Lyu%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520objects%2520into%2520editable%2520programs%2520is%2520pivotal%2520for%2520applications%250Alike%2520reverse%2520engineering%2520and%2520shape%2520editing.%2520However%252C%2520existing%2520methods%2520often%250Arely%2520on%2520limited%2520domain-specific%2520languages%2520%2528DSLs%2529%2520and%2520small-scale%2520datasets%252C%250Arestricting%2520their%2520ability%2520to%2520model%2520complex%2520geometries%2520and%2520structures.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520MeshCoder%252C%2520a%2520novel%2520framework%2520that%250Areconstructs%2520complex%25203D%2520objects%2520from%2520point%2520clouds%2520into%2520editable%2520Blender%2520Python%250Ascripts.%2520We%2520develop%2520a%2520comprehensive%2520set%2520of%2520expressive%2520Blender%2520Python%2520APIs%250Acapable%2520of%2520synthesizing%2520intricate%2520geometries.%2520Leveraging%2520these%2520APIs%252C%2520we%250Aconstruct%2520a%2520large-scale%2520paired%2520object-code%2520dataset%252C%2520where%2520the%2520code%2520for%2520each%250Aobject%2520is%2520decomposed%2520into%2520distinct%2520semantic%2520parts.%2520Subsequently%252C%2520we%2520train%2520a%250Amultimodal%2520large%2520language%2520model%2520%2528LLM%2529%2520that%2520translates%25203D%2520point%2520cloud%2520into%250Aexecutable%2520Blender%2520Python%2520scripts.%2520Our%2520approach%2520not%2520only%2520achieves%2520superior%250Aperformance%2520in%2520shape-to-code%2520reconstruction%2520tasks%2520but%2520also%2520facilitates%250Aintuitive%2520geometric%2520and%2520topological%2520editing%2520through%2520convenient%2520code%250Amodifications.%2520Furthermore%252C%2520our%2520code-based%2520representation%2520enhances%2520the%250Areasoning%2520capabilities%2520of%2520LLMs%2520in%25203D%2520shape%2520understanding%2520tasks.%2520Together%252C%2520these%250Acontributions%2520establish%2520MeshCoder%2520as%2520a%2520powerful%2520and%2520flexible%2520solution%2520for%250Aprogrammatic%25203D%2520shape%2520reconstruction%2520and%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshCoder%3A%20LLM-Powered%20Structured%20Mesh%20Code%20Generation%20from%20Point%20Clouds&entry.906535625=Bingquan%20Dai%20and%20Li%20Ray%20Luo%20and%20Qihong%20Tang%20and%20Jie%20Wang%20and%20Xinyu%20Lian%20and%20Hao%20Xu%20and%20Minghan%20Qin%20and%20Xudong%20Xu%20and%20Bo%20Dai%20and%20Haoqian%20Wang%20and%20Zhaoyang%20Lyu%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Reconstructing%203D%20objects%20into%20editable%20programs%20is%20pivotal%20for%20applications%0Alike%20reverse%20engineering%20and%20shape%20editing.%20However%2C%20existing%20methods%20often%0Arely%20on%20limited%20domain-specific%20languages%20%28DSLs%29%20and%20small-scale%20datasets%2C%0Arestricting%20their%20ability%20to%20model%20complex%20geometries%20and%20structures.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20MeshCoder%2C%20a%20novel%20framework%20that%0Areconstructs%20complex%203D%20objects%20from%20point%20clouds%20into%20editable%20Blender%20Python%0Ascripts.%20We%20develop%20a%20comprehensive%20set%20of%20expressive%20Blender%20Python%20APIs%0Acapable%20of%20synthesizing%20intricate%20geometries.%20Leveraging%20these%20APIs%2C%20we%0Aconstruct%20a%20large-scale%20paired%20object-code%20dataset%2C%20where%20the%20code%20for%20each%0Aobject%20is%20decomposed%20into%20distinct%20semantic%20parts.%20Subsequently%2C%20we%20train%20a%0Amultimodal%20large%20language%20model%20%28LLM%29%20that%20translates%203D%20point%20cloud%20into%0Aexecutable%20Blender%20Python%20scripts.%20Our%20approach%20not%20only%20achieves%20superior%0Aperformance%20in%20shape-to-code%20reconstruction%20tasks%20but%20also%20facilitates%0Aintuitive%20geometric%20and%20topological%20editing%20through%20convenient%20code%0Amodifications.%20Furthermore%2C%20our%20code-based%20representation%20enhances%20the%0Areasoning%20capabilities%20of%20LLMs%20in%203D%20shape%20understanding%20tasks.%20Together%2C%20these%0Acontributions%20establish%20MeshCoder%20as%20a%20powerful%20and%20flexible%20solution%20for%0Aprogrammatic%203D%20shape%20reconstruction%20and%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14879v1&entry.124074799=Read"},
{"title": "Addressing Graph Anomaly Detection via Causal Edge Separation and\n  Spectrum", "author": "Zengyi Wo and Wenjun Wang and Minglai Shao and Chang Liu and Yumeng Wang and Yueheng Sun", "abstract": "  In the real world, anomalous entities often add more legitimate connections\nwhile hiding direct links with other anomalous entities, leading to\nheterophilic structures in anomalous networks that most GNN-based techniques\nfail to address. Several works have been proposed to tackle this issue in the\nspatial domain. However, these methods overlook the complex relationships\nbetween node structure encoding, node features, and their contextual\nenvironment and rely on principled guidance, research on solving spectral\ndomain heterophilic problems remains limited. This study analyzes the spectral\ndistribution of nodes with different heterophilic degrees and discovers that\nthe heterophily of anomalous nodes causes the spectral energy to shift from low\nto high frequencies. To address the above challenges, we propose a spectral\nneural network CES2-GAD based on causal edge separation for anomaly detection\non heterophilic graphs. Firstly, CES2-GAD will separate the original graph into\nhomophilic and heterophilic edges using causal interventions. Subsequently,\nvarious hybrid-spectrum filters are used to capture signals from the segmented\ngraphs. Finally, representations from multiple signals are concatenated and\ninput into a classifier to predict anomalies. Extensive experiments with\nreal-world datasets have proven the effectiveness of the method we proposed.\n", "link": "http://arxiv.org/abs/2508.14684v1", "date": "2025-08-20", "relevancy": 2.2607, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4642}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4533}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Graph%20Anomaly%20Detection%20via%20Causal%20Edge%20Separation%20and%0A%20%20Spectrum&body=Title%3A%20Addressing%20Graph%20Anomaly%20Detection%20via%20Causal%20Edge%20Separation%20and%0A%20%20Spectrum%0AAuthor%3A%20Zengyi%20Wo%20and%20Wenjun%20Wang%20and%20Minglai%20Shao%20and%20Chang%20Liu%20and%20Yumeng%20Wang%20and%20Yueheng%20Sun%0AAbstract%3A%20%20%20In%20the%20real%20world%2C%20anomalous%20entities%20often%20add%20more%20legitimate%20connections%0Awhile%20hiding%20direct%20links%20with%20other%20anomalous%20entities%2C%20leading%20to%0Aheterophilic%20structures%20in%20anomalous%20networks%20that%20most%20GNN-based%20techniques%0Afail%20to%20address.%20Several%20works%20have%20been%20proposed%20to%20tackle%20this%20issue%20in%20the%0Aspatial%20domain.%20However%2C%20these%20methods%20overlook%20the%20complex%20relationships%0Abetween%20node%20structure%20encoding%2C%20node%20features%2C%20and%20their%20contextual%0Aenvironment%20and%20rely%20on%20principled%20guidance%2C%20research%20on%20solving%20spectral%0Adomain%20heterophilic%20problems%20remains%20limited.%20This%20study%20analyzes%20the%20spectral%0Adistribution%20of%20nodes%20with%20different%20heterophilic%20degrees%20and%20discovers%20that%0Athe%20heterophily%20of%20anomalous%20nodes%20causes%20the%20spectral%20energy%20to%20shift%20from%20low%0Ato%20high%20frequencies.%20To%20address%20the%20above%20challenges%2C%20we%20propose%20a%20spectral%0Aneural%20network%20CES2-GAD%20based%20on%20causal%20edge%20separation%20for%20anomaly%20detection%0Aon%20heterophilic%20graphs.%20Firstly%2C%20CES2-GAD%20will%20separate%20the%20original%20graph%20into%0Ahomophilic%20and%20heterophilic%20edges%20using%20causal%20interventions.%20Subsequently%2C%0Avarious%20hybrid-spectrum%20filters%20are%20used%20to%20capture%20signals%20from%20the%20segmented%0Agraphs.%20Finally%2C%20representations%20from%20multiple%20signals%20are%20concatenated%20and%0Ainput%20into%20a%20classifier%20to%20predict%20anomalies.%20Extensive%20experiments%20with%0Areal-world%20datasets%20have%20proven%20the%20effectiveness%20of%20the%20method%20we%20proposed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Graph%2520Anomaly%2520Detection%2520via%2520Causal%2520Edge%2520Separation%2520and%250A%2520%2520Spectrum%26entry.906535625%3DZengyi%2520Wo%2520and%2520Wenjun%2520Wang%2520and%2520Minglai%2520Shao%2520and%2520Chang%2520Liu%2520and%2520Yumeng%2520Wang%2520and%2520Yueheng%2520Sun%26entry.1292438233%3D%2520%2520In%2520the%2520real%2520world%252C%2520anomalous%2520entities%2520often%2520add%2520more%2520legitimate%2520connections%250Awhile%2520hiding%2520direct%2520links%2520with%2520other%2520anomalous%2520entities%252C%2520leading%2520to%250Aheterophilic%2520structures%2520in%2520anomalous%2520networks%2520that%2520most%2520GNN-based%2520techniques%250Afail%2520to%2520address.%2520Several%2520works%2520have%2520been%2520proposed%2520to%2520tackle%2520this%2520issue%2520in%2520the%250Aspatial%2520domain.%2520However%252C%2520these%2520methods%2520overlook%2520the%2520complex%2520relationships%250Abetween%2520node%2520structure%2520encoding%252C%2520node%2520features%252C%2520and%2520their%2520contextual%250Aenvironment%2520and%2520rely%2520on%2520principled%2520guidance%252C%2520research%2520on%2520solving%2520spectral%250Adomain%2520heterophilic%2520problems%2520remains%2520limited.%2520This%2520study%2520analyzes%2520the%2520spectral%250Adistribution%2520of%2520nodes%2520with%2520different%2520heterophilic%2520degrees%2520and%2520discovers%2520that%250Athe%2520heterophily%2520of%2520anomalous%2520nodes%2520causes%2520the%2520spectral%2520energy%2520to%2520shift%2520from%2520low%250Ato%2520high%2520frequencies.%2520To%2520address%2520the%2520above%2520challenges%252C%2520we%2520propose%2520a%2520spectral%250Aneural%2520network%2520CES2-GAD%2520based%2520on%2520causal%2520edge%2520separation%2520for%2520anomaly%2520detection%250Aon%2520heterophilic%2520graphs.%2520Firstly%252C%2520CES2-GAD%2520will%2520separate%2520the%2520original%2520graph%2520into%250Ahomophilic%2520and%2520heterophilic%2520edges%2520using%2520causal%2520interventions.%2520Subsequently%252C%250Avarious%2520hybrid-spectrum%2520filters%2520are%2520used%2520to%2520capture%2520signals%2520from%2520the%2520segmented%250Agraphs.%2520Finally%252C%2520representations%2520from%2520multiple%2520signals%2520are%2520concatenated%2520and%250Ainput%2520into%2520a%2520classifier%2520to%2520predict%2520anomalies.%2520Extensive%2520experiments%2520with%250Areal-world%2520datasets%2520have%2520proven%2520the%2520effectiveness%2520of%2520the%2520method%2520we%2520proposed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Graph%20Anomaly%20Detection%20via%20Causal%20Edge%20Separation%20and%0A%20%20Spectrum&entry.906535625=Zengyi%20Wo%20and%20Wenjun%20Wang%20and%20Minglai%20Shao%20and%20Chang%20Liu%20and%20Yumeng%20Wang%20and%20Yueheng%20Sun&entry.1292438233=%20%20In%20the%20real%20world%2C%20anomalous%20entities%20often%20add%20more%20legitimate%20connections%0Awhile%20hiding%20direct%20links%20with%20other%20anomalous%20entities%2C%20leading%20to%0Aheterophilic%20structures%20in%20anomalous%20networks%20that%20most%20GNN-based%20techniques%0Afail%20to%20address.%20Several%20works%20have%20been%20proposed%20to%20tackle%20this%20issue%20in%20the%0Aspatial%20domain.%20However%2C%20these%20methods%20overlook%20the%20complex%20relationships%0Abetween%20node%20structure%20encoding%2C%20node%20features%2C%20and%20their%20contextual%0Aenvironment%20and%20rely%20on%20principled%20guidance%2C%20research%20on%20solving%20spectral%0Adomain%20heterophilic%20problems%20remains%20limited.%20This%20study%20analyzes%20the%20spectral%0Adistribution%20of%20nodes%20with%20different%20heterophilic%20degrees%20and%20discovers%20that%0Athe%20heterophily%20of%20anomalous%20nodes%20causes%20the%20spectral%20energy%20to%20shift%20from%20low%0Ato%20high%20frequencies.%20To%20address%20the%20above%20challenges%2C%20we%20propose%20a%20spectral%0Aneural%20network%20CES2-GAD%20based%20on%20causal%20edge%20separation%20for%20anomaly%20detection%0Aon%20heterophilic%20graphs.%20Firstly%2C%20CES2-GAD%20will%20separate%20the%20original%20graph%20into%0Ahomophilic%20and%20heterophilic%20edges%20using%20causal%20interventions.%20Subsequently%2C%0Avarious%20hybrid-spectrum%20filters%20are%20used%20to%20capture%20signals%20from%20the%20segmented%0Agraphs.%20Finally%2C%20representations%20from%20multiple%20signals%20are%20concatenated%20and%0Ainput%20into%20a%20classifier%20to%20predict%20anomalies.%20Extensive%20experiments%20with%0Areal-world%20datasets%20have%20proven%20the%20effectiveness%20of%20the%20method%20we%20proposed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14684v1&entry.124074799=Read"},
{"title": "LeanGeo: Formalizing Competitional Geometry problems in Lean", "author": "Chendong Song and Zihan Wang and Frederick Pu and Haiming Wang and Xiaohan Lin and Junqi Liu and Jia Li and Zhengying Liu", "abstract": "  Geometry problems are a crucial testbed for AI reasoning capabilities. Most\nexisting geometry solving systems cannot express problems within a unified\nframework, thus are difficult to integrate with other mathematical fields.\nBesides, since most geometric proofs rely on intuitive diagrams, verifying\ngeometry problems is particularly challenging. To address these gaps, we\nintroduce LeanGeo, a unified formal system for formalizing and solving\ncompetition-level geometry problems within the Lean 4 theorem prover. LeanGeo\nfeatures a comprehensive library of high-level geometric theorems with Lean's\nfoundational logic, enabling rigorous proof verification and seamless\nintegration with Mathlib. We also present LeanGeo-Bench, a formal geometry\nbenchmark in LeanGeo, comprising problems from the International Mathematical\nOlympiad (IMO) and other advanced sources. Our evaluation demonstrates the\ncapabilities and limitations of state-of-the-art Large Language Models on this\nbenchmark, highlighting the need for further advancements in automated\ngeometric reasoning. We open source the theorem library and the benchmark of\nLeanGeo at https://github.com/project-numina/LeanGeo/tree/master.\n", "link": "http://arxiv.org/abs/2508.14644v1", "date": "2025-08-20", "relevancy": 2.2521, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeanGeo%3A%20Formalizing%20Competitional%20Geometry%20problems%20in%20Lean&body=Title%3A%20LeanGeo%3A%20Formalizing%20Competitional%20Geometry%20problems%20in%20Lean%0AAuthor%3A%20Chendong%20Song%20and%20Zihan%20Wang%20and%20Frederick%20Pu%20and%20Haiming%20Wang%20and%20Xiaohan%20Lin%20and%20Junqi%20Liu%20and%20Jia%20Li%20and%20Zhengying%20Liu%0AAbstract%3A%20%20%20Geometry%20problems%20are%20a%20crucial%20testbed%20for%20AI%20reasoning%20capabilities.%20Most%0Aexisting%20geometry%20solving%20systems%20cannot%20express%20problems%20within%20a%20unified%0Aframework%2C%20thus%20are%20difficult%20to%20integrate%20with%20other%20mathematical%20fields.%0ABesides%2C%20since%20most%20geometric%20proofs%20rely%20on%20intuitive%20diagrams%2C%20verifying%0Ageometry%20problems%20is%20particularly%20challenging.%20To%20address%20these%20gaps%2C%20we%0Aintroduce%20LeanGeo%2C%20a%20unified%20formal%20system%20for%20formalizing%20and%20solving%0Acompetition-level%20geometry%20problems%20within%20the%20Lean%204%20theorem%20prover.%20LeanGeo%0Afeatures%20a%20comprehensive%20library%20of%20high-level%20geometric%20theorems%20with%20Lean%27s%0Afoundational%20logic%2C%20enabling%20rigorous%20proof%20verification%20and%20seamless%0Aintegration%20with%20Mathlib.%20We%20also%20present%20LeanGeo-Bench%2C%20a%20formal%20geometry%0Abenchmark%20in%20LeanGeo%2C%20comprising%20problems%20from%20the%20International%20Mathematical%0AOlympiad%20%28IMO%29%20and%20other%20advanced%20sources.%20Our%20evaluation%20demonstrates%20the%0Acapabilities%20and%20limitations%20of%20state-of-the-art%20Large%20Language%20Models%20on%20this%0Abenchmark%2C%20highlighting%20the%20need%20for%20further%20advancements%20in%20automated%0Ageometric%20reasoning.%20We%20open%20source%20the%20theorem%20library%20and%20the%20benchmark%20of%0ALeanGeo%20at%20https%3A//github.com/project-numina/LeanGeo/tree/master.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeanGeo%253A%2520Formalizing%2520Competitional%2520Geometry%2520problems%2520in%2520Lean%26entry.906535625%3DChendong%2520Song%2520and%2520Zihan%2520Wang%2520and%2520Frederick%2520Pu%2520and%2520Haiming%2520Wang%2520and%2520Xiaohan%2520Lin%2520and%2520Junqi%2520Liu%2520and%2520Jia%2520Li%2520and%2520Zhengying%2520Liu%26entry.1292438233%3D%2520%2520Geometry%2520problems%2520are%2520a%2520crucial%2520testbed%2520for%2520AI%2520reasoning%2520capabilities.%2520Most%250Aexisting%2520geometry%2520solving%2520systems%2520cannot%2520express%2520problems%2520within%2520a%2520unified%250Aframework%252C%2520thus%2520are%2520difficult%2520to%2520integrate%2520with%2520other%2520mathematical%2520fields.%250ABesides%252C%2520since%2520most%2520geometric%2520proofs%2520rely%2520on%2520intuitive%2520diagrams%252C%2520verifying%250Ageometry%2520problems%2520is%2520particularly%2520challenging.%2520To%2520address%2520these%2520gaps%252C%2520we%250Aintroduce%2520LeanGeo%252C%2520a%2520unified%2520formal%2520system%2520for%2520formalizing%2520and%2520solving%250Acompetition-level%2520geometry%2520problems%2520within%2520the%2520Lean%25204%2520theorem%2520prover.%2520LeanGeo%250Afeatures%2520a%2520comprehensive%2520library%2520of%2520high-level%2520geometric%2520theorems%2520with%2520Lean%2527s%250Afoundational%2520logic%252C%2520enabling%2520rigorous%2520proof%2520verification%2520and%2520seamless%250Aintegration%2520with%2520Mathlib.%2520We%2520also%2520present%2520LeanGeo-Bench%252C%2520a%2520formal%2520geometry%250Abenchmark%2520in%2520LeanGeo%252C%2520comprising%2520problems%2520from%2520the%2520International%2520Mathematical%250AOlympiad%2520%2528IMO%2529%2520and%2520other%2520advanced%2520sources.%2520Our%2520evaluation%2520demonstrates%2520the%250Acapabilities%2520and%2520limitations%2520of%2520state-of-the-art%2520Large%2520Language%2520Models%2520on%2520this%250Abenchmark%252C%2520highlighting%2520the%2520need%2520for%2520further%2520advancements%2520in%2520automated%250Ageometric%2520reasoning.%2520We%2520open%2520source%2520the%2520theorem%2520library%2520and%2520the%2520benchmark%2520of%250ALeanGeo%2520at%2520https%253A//github.com/project-numina/LeanGeo/tree/master.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeanGeo%3A%20Formalizing%20Competitional%20Geometry%20problems%20in%20Lean&entry.906535625=Chendong%20Song%20and%20Zihan%20Wang%20and%20Frederick%20Pu%20and%20Haiming%20Wang%20and%20Xiaohan%20Lin%20and%20Junqi%20Liu%20and%20Jia%20Li%20and%20Zhengying%20Liu&entry.1292438233=%20%20Geometry%20problems%20are%20a%20crucial%20testbed%20for%20AI%20reasoning%20capabilities.%20Most%0Aexisting%20geometry%20solving%20systems%20cannot%20express%20problems%20within%20a%20unified%0Aframework%2C%20thus%20are%20difficult%20to%20integrate%20with%20other%20mathematical%20fields.%0ABesides%2C%20since%20most%20geometric%20proofs%20rely%20on%20intuitive%20diagrams%2C%20verifying%0Ageometry%20problems%20is%20particularly%20challenging.%20To%20address%20these%20gaps%2C%20we%0Aintroduce%20LeanGeo%2C%20a%20unified%20formal%20system%20for%20formalizing%20and%20solving%0Acompetition-level%20geometry%20problems%20within%20the%20Lean%204%20theorem%20prover.%20LeanGeo%0Afeatures%20a%20comprehensive%20library%20of%20high-level%20geometric%20theorems%20with%20Lean%27s%0Afoundational%20logic%2C%20enabling%20rigorous%20proof%20verification%20and%20seamless%0Aintegration%20with%20Mathlib.%20We%20also%20present%20LeanGeo-Bench%2C%20a%20formal%20geometry%0Abenchmark%20in%20LeanGeo%2C%20comprising%20problems%20from%20the%20International%20Mathematical%0AOlympiad%20%28IMO%29%20and%20other%20advanced%20sources.%20Our%20evaluation%20demonstrates%20the%0Acapabilities%20and%20limitations%20of%20state-of-the-art%20Large%20Language%20Models%20on%20this%0Abenchmark%2C%20highlighting%20the%20need%20for%20further%20advancements%20in%20automated%0Ageometric%20reasoning.%20We%20open%20source%20the%20theorem%20library%20and%20the%20benchmark%20of%0ALeanGeo%20at%20https%3A//github.com/project-numina/LeanGeo/tree/master.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14644v1&entry.124074799=Read"},
{"title": "Dark Miner: Defend against undesirable generation for text-to-image\n  diffusion models", "author": "Zheling Meng and Bo Peng and Xiaochuan Jin and Yue Jiang and Wei Wang and Jing Dong and Tieniu Tan", "abstract": "  Text-to-image diffusion models have been demonstrated with undesired\ngeneration due to unfiltered large-scale training data, such as sexual images\nand copyrights, necessitating the erasure of undesired concepts. Most existing\nmethods focus on modifying the generation probabilities conditioned on the\ntexts containing target concepts. However, they fail to guarantee the desired\ngeneration of texts unseen in the training phase, especially for the\nadversarial texts from malicious attacks. In this paper, we analyze the erasure\ntask and point out that existing methods cannot guarantee the minimization of\nthe total probabilities of undesired generation. To tackle this problem, we\npropose Dark Miner. It entails a recurring three-stage process that comprises\nmining, verifying, and circumventing. This method greedily mines embeddings\nwith maximum generation probabilities of target concepts and more effectively\nreduces their generation. In the experiments, we evaluate its performance on\nthe inappropriateness, object, and style concepts. Compared with the previous\nmethods, our method achieves better erasure and defense results, especially\nunder multiple adversarial attacks, while preserving the native generation\ncapability of the models. Our code will be available on GitHub.\n", "link": "http://arxiv.org/abs/2409.17682v3", "date": "2025-08-20", "relevancy": 2.2235, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5692}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5472}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dark%20Miner%3A%20Defend%20against%20undesirable%20generation%20for%20text-to-image%0A%20%20diffusion%20models&body=Title%3A%20Dark%20Miner%3A%20Defend%20against%20undesirable%20generation%20for%20text-to-image%0A%20%20diffusion%20models%0AAuthor%3A%20Zheling%20Meng%20and%20Bo%20Peng%20and%20Xiaochuan%20Jin%20and%20Yue%20Jiang%20and%20Wei%20Wang%20and%20Jing%20Dong%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20been%20demonstrated%20with%20undesired%0Ageneration%20due%20to%20unfiltered%20large-scale%20training%20data%2C%20such%20as%20sexual%20images%0Aand%20copyrights%2C%20necessitating%20the%20erasure%20of%20undesired%20concepts.%20Most%20existing%0Amethods%20focus%20on%20modifying%20the%20generation%20probabilities%20conditioned%20on%20the%0Atexts%20containing%20target%20concepts.%20However%2C%20they%20fail%20to%20guarantee%20the%20desired%0Ageneration%20of%20texts%20unseen%20in%20the%20training%20phase%2C%20especially%20for%20the%0Aadversarial%20texts%20from%20malicious%20attacks.%20In%20this%20paper%2C%20we%20analyze%20the%20erasure%0Atask%20and%20point%20out%20that%20existing%20methods%20cannot%20guarantee%20the%20minimization%20of%0Athe%20total%20probabilities%20of%20undesired%20generation.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20Dark%20Miner.%20It%20entails%20a%20recurring%20three-stage%20process%20that%20comprises%0Amining%2C%20verifying%2C%20and%20circumventing.%20This%20method%20greedily%20mines%20embeddings%0Awith%20maximum%20generation%20probabilities%20of%20target%20concepts%20and%20more%20effectively%0Areduces%20their%20generation.%20In%20the%20experiments%2C%20we%20evaluate%20its%20performance%20on%0Athe%20inappropriateness%2C%20object%2C%20and%20style%20concepts.%20Compared%20with%20the%20previous%0Amethods%2C%20our%20method%20achieves%20better%20erasure%20and%20defense%20results%2C%20especially%0Aunder%20multiple%20adversarial%20attacks%2C%20while%20preserving%20the%20native%20generation%0Acapability%20of%20the%20models.%20Our%20code%20will%20be%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17682v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDark%2520Miner%253A%2520Defend%2520against%2520undesirable%2520generation%2520for%2520text-to-image%250A%2520%2520diffusion%2520models%26entry.906535625%3DZheling%2520Meng%2520and%2520Bo%2520Peng%2520and%2520Xiaochuan%2520Jin%2520and%2520Yue%2520Jiang%2520and%2520Wei%2520Wang%2520and%2520Jing%2520Dong%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520been%2520demonstrated%2520with%2520undesired%250Ageneration%2520due%2520to%2520unfiltered%2520large-scale%2520training%2520data%252C%2520such%2520as%2520sexual%2520images%250Aand%2520copyrights%252C%2520necessitating%2520the%2520erasure%2520of%2520undesired%2520concepts.%2520Most%2520existing%250Amethods%2520focus%2520on%2520modifying%2520the%2520generation%2520probabilities%2520conditioned%2520on%2520the%250Atexts%2520containing%2520target%2520concepts.%2520However%252C%2520they%2520fail%2520to%2520guarantee%2520the%2520desired%250Ageneration%2520of%2520texts%2520unseen%2520in%2520the%2520training%2520phase%252C%2520especially%2520for%2520the%250Aadversarial%2520texts%2520from%2520malicious%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520erasure%250Atask%2520and%2520point%2520out%2520that%2520existing%2520methods%2520cannot%2520guarantee%2520the%2520minimization%2520of%250Athe%2520total%2520probabilities%2520of%2520undesired%2520generation.%2520To%2520tackle%2520this%2520problem%252C%2520we%250Apropose%2520Dark%2520Miner.%2520It%2520entails%2520a%2520recurring%2520three-stage%2520process%2520that%2520comprises%250Amining%252C%2520verifying%252C%2520and%2520circumventing.%2520This%2520method%2520greedily%2520mines%2520embeddings%250Awith%2520maximum%2520generation%2520probabilities%2520of%2520target%2520concepts%2520and%2520more%2520effectively%250Areduces%2520their%2520generation.%2520In%2520the%2520experiments%252C%2520we%2520evaluate%2520its%2520performance%2520on%250Athe%2520inappropriateness%252C%2520object%252C%2520and%2520style%2520concepts.%2520Compared%2520with%2520the%2520previous%250Amethods%252C%2520our%2520method%2520achieves%2520better%2520erasure%2520and%2520defense%2520results%252C%2520especially%250Aunder%2520multiple%2520adversarial%2520attacks%252C%2520while%2520preserving%2520the%2520native%2520generation%250Acapability%2520of%2520the%2520models.%2520Our%2520code%2520will%2520be%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17682v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dark%20Miner%3A%20Defend%20against%20undesirable%20generation%20for%20text-to-image%0A%20%20diffusion%20models&entry.906535625=Zheling%20Meng%20and%20Bo%20Peng%20and%20Xiaochuan%20Jin%20and%20Yue%20Jiang%20and%20Wei%20Wang%20and%20Jing%20Dong%20and%20Tieniu%20Tan&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20been%20demonstrated%20with%20undesired%0Ageneration%20due%20to%20unfiltered%20large-scale%20training%20data%2C%20such%20as%20sexual%20images%0Aand%20copyrights%2C%20necessitating%20the%20erasure%20of%20undesired%20concepts.%20Most%20existing%0Amethods%20focus%20on%20modifying%20the%20generation%20probabilities%20conditioned%20on%20the%0Atexts%20containing%20target%20concepts.%20However%2C%20they%20fail%20to%20guarantee%20the%20desired%0Ageneration%20of%20texts%20unseen%20in%20the%20training%20phase%2C%20especially%20for%20the%0Aadversarial%20texts%20from%20malicious%20attacks.%20In%20this%20paper%2C%20we%20analyze%20the%20erasure%0Atask%20and%20point%20out%20that%20existing%20methods%20cannot%20guarantee%20the%20minimization%20of%0Athe%20total%20probabilities%20of%20undesired%20generation.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20Dark%20Miner.%20It%20entails%20a%20recurring%20three-stage%20process%20that%20comprises%0Amining%2C%20verifying%2C%20and%20circumventing.%20This%20method%20greedily%20mines%20embeddings%0Awith%20maximum%20generation%20probabilities%20of%20target%20concepts%20and%20more%20effectively%0Areduces%20their%20generation.%20In%20the%20experiments%2C%20we%20evaluate%20its%20performance%20on%0Athe%20inappropriateness%2C%20object%2C%20and%20style%20concepts.%20Compared%20with%20the%20previous%0Amethods%2C%20our%20method%20achieves%20better%20erasure%20and%20defense%20results%2C%20especially%0Aunder%20multiple%20adversarial%20attacks%2C%20while%20preserving%20the%20native%20generation%0Acapability%20of%20the%20models.%20Our%20code%20will%20be%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17682v3&entry.124074799=Read"},
{"title": "AnchorSync: Global Consistency Optimization for Long Video Editing", "author": "Zichi Liu and Yinggui Wang and Tao Wei and Chao Ma", "abstract": "  Editing long videos remains a challenging task due to the need for\nmaintaining both global consistency and temporal coherence across thousands of\nframes. Existing methods often suffer from structural drift or temporal\nartifacts, particularly in minute-long sequences. We introduce AnchorSync, a\nnovel diffusion-based framework that enables high-quality, long-term video\nediting by decoupling the task into sparse anchor frame editing and smooth\nintermediate frame interpolation. Our approach enforces structural consistency\nthrough a progressive denoising process and preserves temporal dynamics via\nmultimodal guidance. Extensive experiments show that AnchorSync produces\ncoherent, high-fidelity edits, surpassing prior methods in visual quality and\ntemporal stability.\n", "link": "http://arxiv.org/abs/2508.14609v1", "date": "2025-08-20", "relevancy": 2.2203, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5959}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5547}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnchorSync%3A%20Global%20Consistency%20Optimization%20for%20Long%20Video%20Editing&body=Title%3A%20AnchorSync%3A%20Global%20Consistency%20Optimization%20for%20Long%20Video%20Editing%0AAuthor%3A%20Zichi%20Liu%20and%20Yinggui%20Wang%20and%20Tao%20Wei%20and%20Chao%20Ma%0AAbstract%3A%20%20%20Editing%20long%20videos%20remains%20a%20challenging%20task%20due%20to%20the%20need%20for%0Amaintaining%20both%20global%20consistency%20and%20temporal%20coherence%20across%20thousands%20of%0Aframes.%20Existing%20methods%20often%20suffer%20from%20structural%20drift%20or%20temporal%0Aartifacts%2C%20particularly%20in%20minute-long%20sequences.%20We%20introduce%20AnchorSync%2C%20a%0Anovel%20diffusion-based%20framework%20that%20enables%20high-quality%2C%20long-term%20video%0Aediting%20by%20decoupling%20the%20task%20into%20sparse%20anchor%20frame%20editing%20and%20smooth%0Aintermediate%20frame%20interpolation.%20Our%20approach%20enforces%20structural%20consistency%0Athrough%20a%20progressive%20denoising%20process%20and%20preserves%20temporal%20dynamics%20via%0Amultimodal%20guidance.%20Extensive%20experiments%20show%20that%20AnchorSync%20produces%0Acoherent%2C%20high-fidelity%20edits%2C%20surpassing%20prior%20methods%20in%20visual%20quality%20and%0Atemporal%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnchorSync%253A%2520Global%2520Consistency%2520Optimization%2520for%2520Long%2520Video%2520Editing%26entry.906535625%3DZichi%2520Liu%2520and%2520Yinggui%2520Wang%2520and%2520Tao%2520Wei%2520and%2520Chao%2520Ma%26entry.1292438233%3D%2520%2520Editing%2520long%2520videos%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520need%2520for%250Amaintaining%2520both%2520global%2520consistency%2520and%2520temporal%2520coherence%2520across%2520thousands%2520of%250Aframes.%2520Existing%2520methods%2520often%2520suffer%2520from%2520structural%2520drift%2520or%2520temporal%250Aartifacts%252C%2520particularly%2520in%2520minute-long%2520sequences.%2520We%2520introduce%2520AnchorSync%252C%2520a%250Anovel%2520diffusion-based%2520framework%2520that%2520enables%2520high-quality%252C%2520long-term%2520video%250Aediting%2520by%2520decoupling%2520the%2520task%2520into%2520sparse%2520anchor%2520frame%2520editing%2520and%2520smooth%250Aintermediate%2520frame%2520interpolation.%2520Our%2520approach%2520enforces%2520structural%2520consistency%250Athrough%2520a%2520progressive%2520denoising%2520process%2520and%2520preserves%2520temporal%2520dynamics%2520via%250Amultimodal%2520guidance.%2520Extensive%2520experiments%2520show%2520that%2520AnchorSync%2520produces%250Acoherent%252C%2520high-fidelity%2520edits%252C%2520surpassing%2520prior%2520methods%2520in%2520visual%2520quality%2520and%250Atemporal%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnchorSync%3A%20Global%20Consistency%20Optimization%20for%20Long%20Video%20Editing&entry.906535625=Zichi%20Liu%20and%20Yinggui%20Wang%20and%20Tao%20Wei%20and%20Chao%20Ma&entry.1292438233=%20%20Editing%20long%20videos%20remains%20a%20challenging%20task%20due%20to%20the%20need%20for%0Amaintaining%20both%20global%20consistency%20and%20temporal%20coherence%20across%20thousands%20of%0Aframes.%20Existing%20methods%20often%20suffer%20from%20structural%20drift%20or%20temporal%0Aartifacts%2C%20particularly%20in%20minute-long%20sequences.%20We%20introduce%20AnchorSync%2C%20a%0Anovel%20diffusion-based%20framework%20that%20enables%20high-quality%2C%20long-term%20video%0Aediting%20by%20decoupling%20the%20task%20into%20sparse%20anchor%20frame%20editing%20and%20smooth%0Aintermediate%20frame%20interpolation.%20Our%20approach%20enforces%20structural%20consistency%0Athrough%20a%20progressive%20denoising%20process%20and%20preserves%20temporal%20dynamics%20via%0Amultimodal%20guidance.%20Extensive%20experiments%20show%20that%20AnchorSync%20produces%0Acoherent%2C%20high-fidelity%20edits%2C%20surpassing%20prior%20methods%20in%20visual%20quality%20and%0Atemporal%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14609v1&entry.124074799=Read"},
{"title": "Privileged Self-Access Matters for Introspection in AI", "author": "Siyuan Song and Harvey Lederman and Jennifer Hu and Kyle Mahowald", "abstract": "  Whether AI models can introspect is an increasingly important practical\nquestion. But there is no consensus on how introspection is to be defined.\nBeginning from a recently proposed ''lightweight'' definition, we argue instead\nfor a thicker one. According to our proposal, introspection in AI is any\nprocess which yields information about internal states through a process more\nreliable than one with equal or lower computational cost available to a third\nparty. Using experiments where LLMs reason about their internal temperature\nparameters, we show they can appear to have lightweight introspection while\nfailing to meaningfully introspect per our proposed definition.\n", "link": "http://arxiv.org/abs/2508.14802v1", "date": "2025-08-20", "relevancy": 2.2178, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4635}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privileged%20Self-Access%20Matters%20for%20Introspection%20in%20AI&body=Title%3A%20Privileged%20Self-Access%20Matters%20for%20Introspection%20in%20AI%0AAuthor%3A%20Siyuan%20Song%20and%20Harvey%20Lederman%20and%20Jennifer%20Hu%20and%20Kyle%20Mahowald%0AAbstract%3A%20%20%20Whether%20AI%20models%20can%20introspect%20is%20an%20increasingly%20important%20practical%0Aquestion.%20But%20there%20is%20no%20consensus%20on%20how%20introspection%20is%20to%20be%20defined.%0ABeginning%20from%20a%20recently%20proposed%20%27%27lightweight%27%27%20definition%2C%20we%20argue%20instead%0Afor%20a%20thicker%20one.%20According%20to%20our%20proposal%2C%20introspection%20in%20AI%20is%20any%0Aprocess%20which%20yields%20information%20about%20internal%20states%20through%20a%20process%20more%0Areliable%20than%20one%20with%20equal%20or%20lower%20computational%20cost%20available%20to%20a%20third%0Aparty.%20Using%20experiments%20where%20LLMs%20reason%20about%20their%20internal%20temperature%0Aparameters%2C%20we%20show%20they%20can%20appear%20to%20have%20lightweight%20introspection%20while%0Afailing%20to%20meaningfully%20introspect%20per%20our%20proposed%20definition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivileged%2520Self-Access%2520Matters%2520for%2520Introspection%2520in%2520AI%26entry.906535625%3DSiyuan%2520Song%2520and%2520Harvey%2520Lederman%2520and%2520Jennifer%2520Hu%2520and%2520Kyle%2520Mahowald%26entry.1292438233%3D%2520%2520Whether%2520AI%2520models%2520can%2520introspect%2520is%2520an%2520increasingly%2520important%2520practical%250Aquestion.%2520But%2520there%2520is%2520no%2520consensus%2520on%2520how%2520introspection%2520is%2520to%2520be%2520defined.%250ABeginning%2520from%2520a%2520recently%2520proposed%2520%2527%2527lightweight%2527%2527%2520definition%252C%2520we%2520argue%2520instead%250Afor%2520a%2520thicker%2520one.%2520According%2520to%2520our%2520proposal%252C%2520introspection%2520in%2520AI%2520is%2520any%250Aprocess%2520which%2520yields%2520information%2520about%2520internal%2520states%2520through%2520a%2520process%2520more%250Areliable%2520than%2520one%2520with%2520equal%2520or%2520lower%2520computational%2520cost%2520available%2520to%2520a%2520third%250Aparty.%2520Using%2520experiments%2520where%2520LLMs%2520reason%2520about%2520their%2520internal%2520temperature%250Aparameters%252C%2520we%2520show%2520they%2520can%2520appear%2520to%2520have%2520lightweight%2520introspection%2520while%250Afailing%2520to%2520meaningfully%2520introspect%2520per%2520our%2520proposed%2520definition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privileged%20Self-Access%20Matters%20for%20Introspection%20in%20AI&entry.906535625=Siyuan%20Song%20and%20Harvey%20Lederman%20and%20Jennifer%20Hu%20and%20Kyle%20Mahowald&entry.1292438233=%20%20Whether%20AI%20models%20can%20introspect%20is%20an%20increasingly%20important%20practical%0Aquestion.%20But%20there%20is%20no%20consensus%20on%20how%20introspection%20is%20to%20be%20defined.%0ABeginning%20from%20a%20recently%20proposed%20%27%27lightweight%27%27%20definition%2C%20we%20argue%20instead%0Afor%20a%20thicker%20one.%20According%20to%20our%20proposal%2C%20introspection%20in%20AI%20is%20any%0Aprocess%20which%20yields%20information%20about%20internal%20states%20through%20a%20process%20more%0Areliable%20than%20one%20with%20equal%20or%20lower%20computational%20cost%20available%20to%20a%20third%0Aparty.%20Using%20experiments%20where%20LLMs%20reason%20about%20their%20internal%20temperature%0Aparameters%2C%20we%20show%20they%20can%20appear%20to%20have%20lightweight%20introspection%20while%0Afailing%20to%20meaningfully%20introspect%20per%20our%20proposed%20definition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14802v1&entry.124074799=Read"},
{"title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban\n  Transportation via Learnable Prompting", "author": "Jiaming Leng and Yunying Bi and Chuan Qin and Bing Yin and Yanyong Zhang and Chao Wang", "abstract": "  Urban transportation systems encounter diverse challenges across multiple\ntasks, such as traffic forecasting, electric vehicle (EV) charging demand\nprediction, and taxi dispatch. Existing approaches suffer from two key\nlimitations: small-scale deep learning models are task-specific and\ndata-hungry, limiting their generalizability across diverse scenarios, while\nlarge language models (LLMs), despite offering flexibility through natural\nlanguage interfaces, struggle with structured spatiotemporal data and numerical\nreasoning in transportation domains. To address these limitations, we propose\nTransLLM, a unified foundation framework that integrates spatiotemporal\nmodeling with large language models through learnable prompt composition. Our\napproach features a lightweight spatiotemporal encoder that captures complex\ndependencies via dilated temporal convolutions and dual-adjacency graph\nattention networks, seamlessly interfacing with LLMs through structured\nembeddings. A novel instance-level prompt routing mechanism, trained via\nreinforcement learning, dynamically personalizes prompts based on input\ncharacteristics, moving beyond fixed task-specific templates. The framework\noperates by encoding spatiotemporal patterns into contextual representations,\ndynamically composing personalized prompts to guide LLM reasoning, and\nprojecting the resulting representations through specialized output layers to\ngenerate task-specific predictions. Experiments across seven datasets and three\ntasks demonstrate the exceptional effectiveness of TransLLM in both supervised\nand zero-shot settings. Compared to ten baseline models, it delivers\ncompetitive performance on both regression and planning problems, showing\nstrong generalization and cross-task adaptability. Our code is available at\nhttps://github.com/BiYunying/TransLLM.\n", "link": "http://arxiv.org/abs/2508.14782v1", "date": "2025-08-20", "relevancy": 2.2146, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5698}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransLLM%3A%20A%20Unified%20Multi-Task%20Foundation%20Framework%20for%20Urban%0A%20%20Transportation%20via%20Learnable%20Prompting&body=Title%3A%20TransLLM%3A%20A%20Unified%20Multi-Task%20Foundation%20Framework%20for%20Urban%0A%20%20Transportation%20via%20Learnable%20Prompting%0AAuthor%3A%20Jiaming%20Leng%20and%20Yunying%20Bi%20and%20Chuan%20Qin%20and%20Bing%20Yin%20and%20Yanyong%20Zhang%20and%20Chao%20Wang%0AAbstract%3A%20%20%20Urban%20transportation%20systems%20encounter%20diverse%20challenges%20across%20multiple%0Atasks%2C%20such%20as%20traffic%20forecasting%2C%20electric%20vehicle%20%28EV%29%20charging%20demand%0Aprediction%2C%20and%20taxi%20dispatch.%20Existing%20approaches%20suffer%20from%20two%20key%0Alimitations%3A%20small-scale%20deep%20learning%20models%20are%20task-specific%20and%0Adata-hungry%2C%20limiting%20their%20generalizability%20across%20diverse%20scenarios%2C%20while%0Alarge%20language%20models%20%28LLMs%29%2C%20despite%20offering%20flexibility%20through%20natural%0Alanguage%20interfaces%2C%20struggle%20with%20structured%20spatiotemporal%20data%20and%20numerical%0Areasoning%20in%20transportation%20domains.%20To%20address%20these%20limitations%2C%20we%20propose%0ATransLLM%2C%20a%20unified%20foundation%20framework%20that%20integrates%20spatiotemporal%0Amodeling%20with%20large%20language%20models%20through%20learnable%20prompt%20composition.%20Our%0Aapproach%20features%20a%20lightweight%20spatiotemporal%20encoder%20that%20captures%20complex%0Adependencies%20via%20dilated%20temporal%20convolutions%20and%20dual-adjacency%20graph%0Aattention%20networks%2C%20seamlessly%20interfacing%20with%20LLMs%20through%20structured%0Aembeddings.%20A%20novel%20instance-level%20prompt%20routing%20mechanism%2C%20trained%20via%0Areinforcement%20learning%2C%20dynamically%20personalizes%20prompts%20based%20on%20input%0Acharacteristics%2C%20moving%20beyond%20fixed%20task-specific%20templates.%20The%20framework%0Aoperates%20by%20encoding%20spatiotemporal%20patterns%20into%20contextual%20representations%2C%0Adynamically%20composing%20personalized%20prompts%20to%20guide%20LLM%20reasoning%2C%20and%0Aprojecting%20the%20resulting%20representations%20through%20specialized%20output%20layers%20to%0Agenerate%20task-specific%20predictions.%20Experiments%20across%20seven%20datasets%20and%20three%0Atasks%20demonstrate%20the%20exceptional%20effectiveness%20of%20TransLLM%20in%20both%20supervised%0Aand%20zero-shot%20settings.%20Compared%20to%20ten%20baseline%20models%2C%20it%20delivers%0Acompetitive%20performance%20on%20both%20regression%20and%20planning%20problems%2C%20showing%0Astrong%20generalization%20and%20cross-task%20adaptability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/BiYunying/TransLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransLLM%253A%2520A%2520Unified%2520Multi-Task%2520Foundation%2520Framework%2520for%2520Urban%250A%2520%2520Transportation%2520via%2520Learnable%2520Prompting%26entry.906535625%3DJiaming%2520Leng%2520and%2520Yunying%2520Bi%2520and%2520Chuan%2520Qin%2520and%2520Bing%2520Yin%2520and%2520Yanyong%2520Zhang%2520and%2520Chao%2520Wang%26entry.1292438233%3D%2520%2520Urban%2520transportation%2520systems%2520encounter%2520diverse%2520challenges%2520across%2520multiple%250Atasks%252C%2520such%2520as%2520traffic%2520forecasting%252C%2520electric%2520vehicle%2520%2528EV%2529%2520charging%2520demand%250Aprediction%252C%2520and%2520taxi%2520dispatch.%2520Existing%2520approaches%2520suffer%2520from%2520two%2520key%250Alimitations%253A%2520small-scale%2520deep%2520learning%2520models%2520are%2520task-specific%2520and%250Adata-hungry%252C%2520limiting%2520their%2520generalizability%2520across%2520diverse%2520scenarios%252C%2520while%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520despite%2520offering%2520flexibility%2520through%2520natural%250Alanguage%2520interfaces%252C%2520struggle%2520with%2520structured%2520spatiotemporal%2520data%2520and%2520numerical%250Areasoning%2520in%2520transportation%2520domains.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250ATransLLM%252C%2520a%2520unified%2520foundation%2520framework%2520that%2520integrates%2520spatiotemporal%250Amodeling%2520with%2520large%2520language%2520models%2520through%2520learnable%2520prompt%2520composition.%2520Our%250Aapproach%2520features%2520a%2520lightweight%2520spatiotemporal%2520encoder%2520that%2520captures%2520complex%250Adependencies%2520via%2520dilated%2520temporal%2520convolutions%2520and%2520dual-adjacency%2520graph%250Aattention%2520networks%252C%2520seamlessly%2520interfacing%2520with%2520LLMs%2520through%2520structured%250Aembeddings.%2520A%2520novel%2520instance-level%2520prompt%2520routing%2520mechanism%252C%2520trained%2520via%250Areinforcement%2520learning%252C%2520dynamically%2520personalizes%2520prompts%2520based%2520on%2520input%250Acharacteristics%252C%2520moving%2520beyond%2520fixed%2520task-specific%2520templates.%2520The%2520framework%250Aoperates%2520by%2520encoding%2520spatiotemporal%2520patterns%2520into%2520contextual%2520representations%252C%250Adynamically%2520composing%2520personalized%2520prompts%2520to%2520guide%2520LLM%2520reasoning%252C%2520and%250Aprojecting%2520the%2520resulting%2520representations%2520through%2520specialized%2520output%2520layers%2520to%250Agenerate%2520task-specific%2520predictions.%2520Experiments%2520across%2520seven%2520datasets%2520and%2520three%250Atasks%2520demonstrate%2520the%2520exceptional%2520effectiveness%2520of%2520TransLLM%2520in%2520both%2520supervised%250Aand%2520zero-shot%2520settings.%2520Compared%2520to%2520ten%2520baseline%2520models%252C%2520it%2520delivers%250Acompetitive%2520performance%2520on%2520both%2520regression%2520and%2520planning%2520problems%252C%2520showing%250Astrong%2520generalization%2520and%2520cross-task%2520adaptability.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/BiYunying/TransLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransLLM%3A%20A%20Unified%20Multi-Task%20Foundation%20Framework%20for%20Urban%0A%20%20Transportation%20via%20Learnable%20Prompting&entry.906535625=Jiaming%20Leng%20and%20Yunying%20Bi%20and%20Chuan%20Qin%20and%20Bing%20Yin%20and%20Yanyong%20Zhang%20and%20Chao%20Wang&entry.1292438233=%20%20Urban%20transportation%20systems%20encounter%20diverse%20challenges%20across%20multiple%0Atasks%2C%20such%20as%20traffic%20forecasting%2C%20electric%20vehicle%20%28EV%29%20charging%20demand%0Aprediction%2C%20and%20taxi%20dispatch.%20Existing%20approaches%20suffer%20from%20two%20key%0Alimitations%3A%20small-scale%20deep%20learning%20models%20are%20task-specific%20and%0Adata-hungry%2C%20limiting%20their%20generalizability%20across%20diverse%20scenarios%2C%20while%0Alarge%20language%20models%20%28LLMs%29%2C%20despite%20offering%20flexibility%20through%20natural%0Alanguage%20interfaces%2C%20struggle%20with%20structured%20spatiotemporal%20data%20and%20numerical%0Areasoning%20in%20transportation%20domains.%20To%20address%20these%20limitations%2C%20we%20propose%0ATransLLM%2C%20a%20unified%20foundation%20framework%20that%20integrates%20spatiotemporal%0Amodeling%20with%20large%20language%20models%20through%20learnable%20prompt%20composition.%20Our%0Aapproach%20features%20a%20lightweight%20spatiotemporal%20encoder%20that%20captures%20complex%0Adependencies%20via%20dilated%20temporal%20convolutions%20and%20dual-adjacency%20graph%0Aattention%20networks%2C%20seamlessly%20interfacing%20with%20LLMs%20through%20structured%0Aembeddings.%20A%20novel%20instance-level%20prompt%20routing%20mechanism%2C%20trained%20via%0Areinforcement%20learning%2C%20dynamically%20personalizes%20prompts%20based%20on%20input%0Acharacteristics%2C%20moving%20beyond%20fixed%20task-specific%20templates.%20The%20framework%0Aoperates%20by%20encoding%20spatiotemporal%20patterns%20into%20contextual%20representations%2C%0Adynamically%20composing%20personalized%20prompts%20to%20guide%20LLM%20reasoning%2C%20and%0Aprojecting%20the%20resulting%20representations%20through%20specialized%20output%20layers%20to%0Agenerate%20task-specific%20predictions.%20Experiments%20across%20seven%20datasets%20and%20three%0Atasks%20demonstrate%20the%20exceptional%20effectiveness%20of%20TransLLM%20in%20both%20supervised%0Aand%20zero-shot%20settings.%20Compared%20to%20ten%20baseline%20models%2C%20it%20delivers%0Acompetitive%20performance%20on%20both%20regression%20and%20planning%20problems%2C%20showing%0Astrong%20generalization%20and%20cross-task%20adaptability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/BiYunying/TransLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14782v1&entry.124074799=Read"},
{"title": "Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte\n  Carlo Approximations", "author": "Elia Trevisan and Khaled A. Mustafa and Godert Notten and Xinwei Wang and Javier Alonso-Mora", "abstract": "  Deploying mobile robots safely among humans requires the motion planner to\naccount for the uncertainty in the other agents' predicted trajectories. This\nremains challenging in traditional approaches, especially with arbitrarily\nshaped predictions and real-time constraints. To address these challenges, we\npropose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI),\na motion planner that incorporates uncertain future motions modelled with\npotentially non-Gaussian stochastic predictions. By leveraging MPPI's\ngradient-free nature, we propose a method that efficiently approximates the\njoint Collision Probability (CP) among multiple dynamic obstacles for several\nhundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This\nenables the rejection of samples exceeding a predefined CP threshold or the\nintegration of CP as a weighted objective within the navigation cost function.\nConsequently, DRA-MPPI mitigates the freezing robot problem while enhancing\nsafety. Real-world and simulated experiments with multiple dynamic obstacles\ndemonstrate DRA-MPPI's superior performance compared to state-of-the-art\napproaches, including Scenario-based Model Predictive Control (S-MPC), Frenet\nplanner, and vanilla MPPI.\n", "link": "http://arxiv.org/abs/2506.21205v2", "date": "2025-08-20", "relevancy": 2.2103, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6018}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5781}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Risk-Aware%20MPPI%20for%20Mobile%20Robots%20in%20Crowds%20via%20Efficient%20Monte%0A%20%20Carlo%20Approximations&body=Title%3A%20Dynamic%20Risk-Aware%20MPPI%20for%20Mobile%20Robots%20in%20Crowds%20via%20Efficient%20Monte%0A%20%20Carlo%20Approximations%0AAuthor%3A%20Elia%20Trevisan%20and%20Khaled%20A.%20Mustafa%20and%20Godert%20Notten%20and%20Xinwei%20Wang%20and%20Javier%20Alonso-Mora%0AAbstract%3A%20%20%20Deploying%20mobile%20robots%20safely%20among%20humans%20requires%20the%20motion%20planner%20to%0Aaccount%20for%20the%20uncertainty%20in%20the%20other%20agents%27%20predicted%20trajectories.%20This%0Aremains%20challenging%20in%20traditional%20approaches%2C%20especially%20with%20arbitrarily%0Ashaped%20predictions%20and%20real-time%20constraints.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20Dynamic%20Risk-Aware%20Model%20Predictive%20Path%20Integral%20control%20%28DRA-MPPI%29%2C%0Aa%20motion%20planner%20that%20incorporates%20uncertain%20future%20motions%20modelled%20with%0Apotentially%20non-Gaussian%20stochastic%20predictions.%20By%20leveraging%20MPPI%27s%0Agradient-free%20nature%2C%20we%20propose%20a%20method%20that%20efficiently%20approximates%20the%0Ajoint%20Collision%20Probability%20%28CP%29%20among%20multiple%20dynamic%20obstacles%20for%20several%0Ahundred%20sampled%20trajectories%20in%20real-time%20via%20a%20Monte%20Carlo%20%28MC%29%20approach.%20This%0Aenables%20the%20rejection%20of%20samples%20exceeding%20a%20predefined%20CP%20threshold%20or%20the%0Aintegration%20of%20CP%20as%20a%20weighted%20objective%20within%20the%20navigation%20cost%20function.%0AConsequently%2C%20DRA-MPPI%20mitigates%20the%20freezing%20robot%20problem%20while%20enhancing%0Asafety.%20Real-world%20and%20simulated%20experiments%20with%20multiple%20dynamic%20obstacles%0Ademonstrate%20DRA-MPPI%27s%20superior%20performance%20compared%20to%20state-of-the-art%0Aapproaches%2C%20including%20Scenario-based%20Model%20Predictive%20Control%20%28S-MPC%29%2C%20Frenet%0Aplanner%2C%20and%20vanilla%20MPPI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Risk-Aware%2520MPPI%2520for%2520Mobile%2520Robots%2520in%2520Crowds%2520via%2520Efficient%2520Monte%250A%2520%2520Carlo%2520Approximations%26entry.906535625%3DElia%2520Trevisan%2520and%2520Khaled%2520A.%2520Mustafa%2520and%2520Godert%2520Notten%2520and%2520Xinwei%2520Wang%2520and%2520Javier%2520Alonso-Mora%26entry.1292438233%3D%2520%2520Deploying%2520mobile%2520robots%2520safely%2520among%2520humans%2520requires%2520the%2520motion%2520planner%2520to%250Aaccount%2520for%2520the%2520uncertainty%2520in%2520the%2520other%2520agents%2527%2520predicted%2520trajectories.%2520This%250Aremains%2520challenging%2520in%2520traditional%2520approaches%252C%2520especially%2520with%2520arbitrarily%250Ashaped%2520predictions%2520and%2520real-time%2520constraints.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520Dynamic%2520Risk-Aware%2520Model%2520Predictive%2520Path%2520Integral%2520control%2520%2528DRA-MPPI%2529%252C%250Aa%2520motion%2520planner%2520that%2520incorporates%2520uncertain%2520future%2520motions%2520modelled%2520with%250Apotentially%2520non-Gaussian%2520stochastic%2520predictions.%2520By%2520leveraging%2520MPPI%2527s%250Agradient-free%2520nature%252C%2520we%2520propose%2520a%2520method%2520that%2520efficiently%2520approximates%2520the%250Ajoint%2520Collision%2520Probability%2520%2528CP%2529%2520among%2520multiple%2520dynamic%2520obstacles%2520for%2520several%250Ahundred%2520sampled%2520trajectories%2520in%2520real-time%2520via%2520a%2520Monte%2520Carlo%2520%2528MC%2529%2520approach.%2520This%250Aenables%2520the%2520rejection%2520of%2520samples%2520exceeding%2520a%2520predefined%2520CP%2520threshold%2520or%2520the%250Aintegration%2520of%2520CP%2520as%2520a%2520weighted%2520objective%2520within%2520the%2520navigation%2520cost%2520function.%250AConsequently%252C%2520DRA-MPPI%2520mitigates%2520the%2520freezing%2520robot%2520problem%2520while%2520enhancing%250Asafety.%2520Real-world%2520and%2520simulated%2520experiments%2520with%2520multiple%2520dynamic%2520obstacles%250Ademonstrate%2520DRA-MPPI%2527s%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%250Aapproaches%252C%2520including%2520Scenario-based%2520Model%2520Predictive%2520Control%2520%2528S-MPC%2529%252C%2520Frenet%250Aplanner%252C%2520and%2520vanilla%2520MPPI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Risk-Aware%20MPPI%20for%20Mobile%20Robots%20in%20Crowds%20via%20Efficient%20Monte%0A%20%20Carlo%20Approximations&entry.906535625=Elia%20Trevisan%20and%20Khaled%20A.%20Mustafa%20and%20Godert%20Notten%20and%20Xinwei%20Wang%20and%20Javier%20Alonso-Mora&entry.1292438233=%20%20Deploying%20mobile%20robots%20safely%20among%20humans%20requires%20the%20motion%20planner%20to%0Aaccount%20for%20the%20uncertainty%20in%20the%20other%20agents%27%20predicted%20trajectories.%20This%0Aremains%20challenging%20in%20traditional%20approaches%2C%20especially%20with%20arbitrarily%0Ashaped%20predictions%20and%20real-time%20constraints.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20Dynamic%20Risk-Aware%20Model%20Predictive%20Path%20Integral%20control%20%28DRA-MPPI%29%2C%0Aa%20motion%20planner%20that%20incorporates%20uncertain%20future%20motions%20modelled%20with%0Apotentially%20non-Gaussian%20stochastic%20predictions.%20By%20leveraging%20MPPI%27s%0Agradient-free%20nature%2C%20we%20propose%20a%20method%20that%20efficiently%20approximates%20the%0Ajoint%20Collision%20Probability%20%28CP%29%20among%20multiple%20dynamic%20obstacles%20for%20several%0Ahundred%20sampled%20trajectories%20in%20real-time%20via%20a%20Monte%20Carlo%20%28MC%29%20approach.%20This%0Aenables%20the%20rejection%20of%20samples%20exceeding%20a%20predefined%20CP%20threshold%20or%20the%0Aintegration%20of%20CP%20as%20a%20weighted%20objective%20within%20the%20navigation%20cost%20function.%0AConsequently%2C%20DRA-MPPI%20mitigates%20the%20freezing%20robot%20problem%20while%20enhancing%0Asafety.%20Real-world%20and%20simulated%20experiments%20with%20multiple%20dynamic%20obstacles%0Ademonstrate%20DRA-MPPI%27s%20superior%20performance%20compared%20to%20state-of-the-art%0Aapproaches%2C%20including%20Scenario-based%20Model%20Predictive%20Control%20%28S-MPC%29%2C%20Frenet%0Aplanner%2C%20and%20vanilla%20MPPI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21205v2&entry.124074799=Read"},
{"title": "Towards PerSense++: Advancing Training-Free Personalized Instance\n  Segmentation in Dense Images", "author": "Muhammad Ibraheem Siddiqui and Muhammad Umer Sheikh and Hassan Abid and Kevin Henry and Muhammad Haris Khan", "abstract": "  Segmentation in dense visual scenes poses significant challenges due to\nocclusions, background clutter, and scale variations. To address this, we\nintroduce PerSense, an end-to-end, training-free, and model-agnostic one-shot\nframework for Personalized instance Segmentation in dense images. PerSense\nemploys a novel Instance Detection Module (IDM) that leverages density maps\n(DMs) to generate instance-level candidate point prompts, followed by a Point\nPrompt Selection Module (PPSM) that filters false positives via adaptive\nthresholding and spatial gating. A feedback mechanism further enhances\nsegmentation by automatically selecting effective exemplars to improve DM\nquality. We additionally present PerSense++, an enhanced variant that\nincorporates three additional components to improve robustness in cluttered\nscenes: (i) a diversity-aware exemplar selection strategy that leverages\nfeature and scale diversity for better DM generation; (ii) a hybrid IDM\ncombining contour and peak-based prompt generation for improved instance\nseparation within complex density patterns; and (iii) an Irrelevant Mask\nRejection Module (IMRM) that discards spatially inconsistent masks using\noutlier analysis. Finally, to support this underexplored task, we introduce\nPerSense-D, a dedicated benchmark for personalized segmentation in dense\nimages. Extensive experiments across multiple benchmarks demonstrate that\nPerSense++ outperforms existing methods in dense settings.\n", "link": "http://arxiv.org/abs/2508.14660v1", "date": "2025-08-20", "relevancy": 2.1917, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20PerSense%2B%2B%3A%20Advancing%20Training-Free%20Personalized%20Instance%0A%20%20Segmentation%20in%20Dense%20Images&body=Title%3A%20Towards%20PerSense%2B%2B%3A%20Advancing%20Training-Free%20Personalized%20Instance%0A%20%20Segmentation%20in%20Dense%20Images%0AAuthor%3A%20Muhammad%20Ibraheem%20Siddiqui%20and%20Muhammad%20Umer%20Sheikh%20and%20Hassan%20Abid%20and%20Kevin%20Henry%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20Segmentation%20in%20dense%20visual%20scenes%20poses%20significant%20challenges%20due%20to%0Aocclusions%2C%20background%20clutter%2C%20and%20scale%20variations.%20To%20address%20this%2C%20we%0Aintroduce%20PerSense%2C%20an%20end-to-end%2C%20training-free%2C%20and%20model-agnostic%20one-shot%0Aframework%20for%20Personalized%20instance%20Segmentation%20in%20dense%20images.%20PerSense%0Aemploys%20a%20novel%20Instance%20Detection%20Module%20%28IDM%29%20that%20leverages%20density%20maps%0A%28DMs%29%20to%20generate%20instance-level%20candidate%20point%20prompts%2C%20followed%20by%20a%20Point%0APrompt%20Selection%20Module%20%28PPSM%29%20that%20filters%20false%20positives%20via%20adaptive%0Athresholding%20and%20spatial%20gating.%20A%20feedback%20mechanism%20further%20enhances%0Asegmentation%20by%20automatically%20selecting%20effective%20exemplars%20to%20improve%20DM%0Aquality.%20We%20additionally%20present%20PerSense%2B%2B%2C%20an%20enhanced%20variant%20that%0Aincorporates%20three%20additional%20components%20to%20improve%20robustness%20in%20cluttered%0Ascenes%3A%20%28i%29%20a%20diversity-aware%20exemplar%20selection%20strategy%20that%20leverages%0Afeature%20and%20scale%20diversity%20for%20better%20DM%20generation%3B%20%28ii%29%20a%20hybrid%20IDM%0Acombining%20contour%20and%20peak-based%20prompt%20generation%20for%20improved%20instance%0Aseparation%20within%20complex%20density%20patterns%3B%20and%20%28iii%29%20an%20Irrelevant%20Mask%0ARejection%20Module%20%28IMRM%29%20that%20discards%20spatially%20inconsistent%20masks%20using%0Aoutlier%20analysis.%20Finally%2C%20to%20support%20this%20underexplored%20task%2C%20we%20introduce%0APerSense-D%2C%20a%20dedicated%20benchmark%20for%20personalized%20segmentation%20in%20dense%0Aimages.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%0APerSense%2B%2B%20outperforms%20existing%20methods%20in%20dense%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520PerSense%252B%252B%253A%2520Advancing%2520Training-Free%2520Personalized%2520Instance%250A%2520%2520Segmentation%2520in%2520Dense%2520Images%26entry.906535625%3DMuhammad%2520Ibraheem%2520Siddiqui%2520and%2520Muhammad%2520Umer%2520Sheikh%2520and%2520Hassan%2520Abid%2520and%2520Kevin%2520Henry%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520Segmentation%2520in%2520dense%2520visual%2520scenes%2520poses%2520significant%2520challenges%2520due%2520to%250Aocclusions%252C%2520background%2520clutter%252C%2520and%2520scale%2520variations.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520PerSense%252C%2520an%2520end-to-end%252C%2520training-free%252C%2520and%2520model-agnostic%2520one-shot%250Aframework%2520for%2520Personalized%2520instance%2520Segmentation%2520in%2520dense%2520images.%2520PerSense%250Aemploys%2520a%2520novel%2520Instance%2520Detection%2520Module%2520%2528IDM%2529%2520that%2520leverages%2520density%2520maps%250A%2528DMs%2529%2520to%2520generate%2520instance-level%2520candidate%2520point%2520prompts%252C%2520followed%2520by%2520a%2520Point%250APrompt%2520Selection%2520Module%2520%2528PPSM%2529%2520that%2520filters%2520false%2520positives%2520via%2520adaptive%250Athresholding%2520and%2520spatial%2520gating.%2520A%2520feedback%2520mechanism%2520further%2520enhances%250Asegmentation%2520by%2520automatically%2520selecting%2520effective%2520exemplars%2520to%2520improve%2520DM%250Aquality.%2520We%2520additionally%2520present%2520PerSense%252B%252B%252C%2520an%2520enhanced%2520variant%2520that%250Aincorporates%2520three%2520additional%2520components%2520to%2520improve%2520robustness%2520in%2520cluttered%250Ascenes%253A%2520%2528i%2529%2520a%2520diversity-aware%2520exemplar%2520selection%2520strategy%2520that%2520leverages%250Afeature%2520and%2520scale%2520diversity%2520for%2520better%2520DM%2520generation%253B%2520%2528ii%2529%2520a%2520hybrid%2520IDM%250Acombining%2520contour%2520and%2520peak-based%2520prompt%2520generation%2520for%2520improved%2520instance%250Aseparation%2520within%2520complex%2520density%2520patterns%253B%2520and%2520%2528iii%2529%2520an%2520Irrelevant%2520Mask%250ARejection%2520Module%2520%2528IMRM%2529%2520that%2520discards%2520spatially%2520inconsistent%2520masks%2520using%250Aoutlier%2520analysis.%2520Finally%252C%2520to%2520support%2520this%2520underexplored%2520task%252C%2520we%2520introduce%250APerSense-D%252C%2520a%2520dedicated%2520benchmark%2520for%2520personalized%2520segmentation%2520in%2520dense%250Aimages.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%250APerSense%252B%252B%2520outperforms%2520existing%2520methods%2520in%2520dense%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20PerSense%2B%2B%3A%20Advancing%20Training-Free%20Personalized%20Instance%0A%20%20Segmentation%20in%20Dense%20Images&entry.906535625=Muhammad%20Ibraheem%20Siddiqui%20and%20Muhammad%20Umer%20Sheikh%20and%20Hassan%20Abid%20and%20Kevin%20Henry%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20Segmentation%20in%20dense%20visual%20scenes%20poses%20significant%20challenges%20due%20to%0Aocclusions%2C%20background%20clutter%2C%20and%20scale%20variations.%20To%20address%20this%2C%20we%0Aintroduce%20PerSense%2C%20an%20end-to-end%2C%20training-free%2C%20and%20model-agnostic%20one-shot%0Aframework%20for%20Personalized%20instance%20Segmentation%20in%20dense%20images.%20PerSense%0Aemploys%20a%20novel%20Instance%20Detection%20Module%20%28IDM%29%20that%20leverages%20density%20maps%0A%28DMs%29%20to%20generate%20instance-level%20candidate%20point%20prompts%2C%20followed%20by%20a%20Point%0APrompt%20Selection%20Module%20%28PPSM%29%20that%20filters%20false%20positives%20via%20adaptive%0Athresholding%20and%20spatial%20gating.%20A%20feedback%20mechanism%20further%20enhances%0Asegmentation%20by%20automatically%20selecting%20effective%20exemplars%20to%20improve%20DM%0Aquality.%20We%20additionally%20present%20PerSense%2B%2B%2C%20an%20enhanced%20variant%20that%0Aincorporates%20three%20additional%20components%20to%20improve%20robustness%20in%20cluttered%0Ascenes%3A%20%28i%29%20a%20diversity-aware%20exemplar%20selection%20strategy%20that%20leverages%0Afeature%20and%20scale%20diversity%20for%20better%20DM%20generation%3B%20%28ii%29%20a%20hybrid%20IDM%0Acombining%20contour%20and%20peak-based%20prompt%20generation%20for%20improved%20instance%0Aseparation%20within%20complex%20density%20patterns%3B%20and%20%28iii%29%20an%20Irrelevant%20Mask%0ARejection%20Module%20%28IMRM%29%20that%20discards%20spatially%20inconsistent%20masks%20using%0Aoutlier%20analysis.%20Finally%2C%20to%20support%20this%20underexplored%20task%2C%20we%20introduce%0APerSense-D%2C%20a%20dedicated%20benchmark%20for%20personalized%20segmentation%20in%20dense%0Aimages.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%0APerSense%2B%2B%20outperforms%20existing%20methods%20in%20dense%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14660v1&entry.124074799=Read"},
{"title": "SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object\n  Tracking in RGB Videos", "author": "Pengzhi Zhong and Xinzhe Wang and Dan Zeng and Qihua Zhou and Feixiang He and Shuiwang Li", "abstract": "  Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential\nfor low-power computation, yet their application in visual tasks remains\nlargely confined to image classification, object detection, and event-based\ntracking. In contrast, real-world vision systems still widely use conventional\nRGB video streams, where the potential of directly-trained SNNs for complex\ntemporal tasks such as multi-object tracking (MOT) remains underexplored. To\naddress this challenge, we propose SMTrack-the first directly trained deep SNN\nframework for end-to-end multi-object tracking on standard RGB videos. SMTrack\nintroduces an adaptive and scale-aware Normalized Wasserstein Distance loss\n(Asa-NWDLoss) to improve detection and localization performance under varying\nobject scales and densities. Specifically, the method computes the average\nobject size within each training batch and dynamically adjusts the\nnormalization factor, thereby enhancing sensitivity to small objects. For the\nassociation stage, we incorporate the TrackTrack identity module to maintain\nrobust and consistent object trajectories. Extensive evaluations on BEE24,\nMOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with\nleading ANN-based MOT methods, advancing robust and accurate SNN-based tracking\nin complex scenarios.\n", "link": "http://arxiv.org/abs/2508.14607v1", "date": "2025-08-20", "relevancy": 2.1872, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5507}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.545}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMTrack%3A%20End-to-End%20Trained%20Spiking%20Neural%20Networks%20for%20Multi-Object%0A%20%20Tracking%20in%20RGB%20Videos&body=Title%3A%20SMTrack%3A%20End-to-End%20Trained%20Spiking%20Neural%20Networks%20for%20Multi-Object%0A%20%20Tracking%20in%20RGB%20Videos%0AAuthor%3A%20Pengzhi%20Zhong%20and%20Xinzhe%20Wang%20and%20Dan%20Zeng%20and%20Qihua%20Zhou%20and%20Feixiang%20He%20and%20Shuiwang%20Li%0AAbstract%3A%20%20%20Brain-inspired%20Spiking%20Neural%20Networks%20%28SNNs%29%20exhibit%20significant%20potential%0Afor%20low-power%20computation%2C%20yet%20their%20application%20in%20visual%20tasks%20remains%0Alargely%20confined%20to%20image%20classification%2C%20object%20detection%2C%20and%20event-based%0Atracking.%20In%20contrast%2C%20real-world%20vision%20systems%20still%20widely%20use%20conventional%0ARGB%20video%20streams%2C%20where%20the%20potential%20of%20directly-trained%20SNNs%20for%20complex%0Atemporal%20tasks%20such%20as%20multi-object%20tracking%20%28MOT%29%20remains%20underexplored.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20SMTrack-the%20first%20directly%20trained%20deep%20SNN%0Aframework%20for%20end-to-end%20multi-object%20tracking%20on%20standard%20RGB%20videos.%20SMTrack%0Aintroduces%20an%20adaptive%20and%20scale-aware%20Normalized%20Wasserstein%20Distance%20loss%0A%28Asa-NWDLoss%29%20to%20improve%20detection%20and%20localization%20performance%20under%20varying%0Aobject%20scales%20and%20densities.%20Specifically%2C%20the%20method%20computes%20the%20average%0Aobject%20size%20within%20each%20training%20batch%20and%20dynamically%20adjusts%20the%0Anormalization%20factor%2C%20thereby%20enhancing%20sensitivity%20to%20small%20objects.%20For%20the%0Aassociation%20stage%2C%20we%20incorporate%20the%20TrackTrack%20identity%20module%20to%20maintain%0Arobust%20and%20consistent%20object%20trajectories.%20Extensive%20evaluations%20on%20BEE24%2C%0AMOT17%2C%20MOT20%2C%20and%20DanceTrack%20show%20that%20SMTrack%20achieves%20performance%20on%20par%20with%0Aleading%20ANN-based%20MOT%20methods%2C%20advancing%20robust%20and%20accurate%20SNN-based%20tracking%0Ain%20complex%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMTrack%253A%2520End-to-End%2520Trained%2520Spiking%2520Neural%2520Networks%2520for%2520Multi-Object%250A%2520%2520Tracking%2520in%2520RGB%2520Videos%26entry.906535625%3DPengzhi%2520Zhong%2520and%2520Xinzhe%2520Wang%2520and%2520Dan%2520Zeng%2520and%2520Qihua%2520Zhou%2520and%2520Feixiang%2520He%2520and%2520Shuiwang%2520Li%26entry.1292438233%3D%2520%2520Brain-inspired%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520exhibit%2520significant%2520potential%250Afor%2520low-power%2520computation%252C%2520yet%2520their%2520application%2520in%2520visual%2520tasks%2520remains%250Alargely%2520confined%2520to%2520image%2520classification%252C%2520object%2520detection%252C%2520and%2520event-based%250Atracking.%2520In%2520contrast%252C%2520real-world%2520vision%2520systems%2520still%2520widely%2520use%2520conventional%250ARGB%2520video%2520streams%252C%2520where%2520the%2520potential%2520of%2520directly-trained%2520SNNs%2520for%2520complex%250Atemporal%2520tasks%2520such%2520as%2520multi-object%2520tracking%2520%2528MOT%2529%2520remains%2520underexplored.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520SMTrack-the%2520first%2520directly%2520trained%2520deep%2520SNN%250Aframework%2520for%2520end-to-end%2520multi-object%2520tracking%2520on%2520standard%2520RGB%2520videos.%2520SMTrack%250Aintroduces%2520an%2520adaptive%2520and%2520scale-aware%2520Normalized%2520Wasserstein%2520Distance%2520loss%250A%2528Asa-NWDLoss%2529%2520to%2520improve%2520detection%2520and%2520localization%2520performance%2520under%2520varying%250Aobject%2520scales%2520and%2520densities.%2520Specifically%252C%2520the%2520method%2520computes%2520the%2520average%250Aobject%2520size%2520within%2520each%2520training%2520batch%2520and%2520dynamically%2520adjusts%2520the%250Anormalization%2520factor%252C%2520thereby%2520enhancing%2520sensitivity%2520to%2520small%2520objects.%2520For%2520the%250Aassociation%2520stage%252C%2520we%2520incorporate%2520the%2520TrackTrack%2520identity%2520module%2520to%2520maintain%250Arobust%2520and%2520consistent%2520object%2520trajectories.%2520Extensive%2520evaluations%2520on%2520BEE24%252C%250AMOT17%252C%2520MOT20%252C%2520and%2520DanceTrack%2520show%2520that%2520SMTrack%2520achieves%2520performance%2520on%2520par%2520with%250Aleading%2520ANN-based%2520MOT%2520methods%252C%2520advancing%2520robust%2520and%2520accurate%2520SNN-based%2520tracking%250Ain%2520complex%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMTrack%3A%20End-to-End%20Trained%20Spiking%20Neural%20Networks%20for%20Multi-Object%0A%20%20Tracking%20in%20RGB%20Videos&entry.906535625=Pengzhi%20Zhong%20and%20Xinzhe%20Wang%20and%20Dan%20Zeng%20and%20Qihua%20Zhou%20and%20Feixiang%20He%20and%20Shuiwang%20Li&entry.1292438233=%20%20Brain-inspired%20Spiking%20Neural%20Networks%20%28SNNs%29%20exhibit%20significant%20potential%0Afor%20low-power%20computation%2C%20yet%20their%20application%20in%20visual%20tasks%20remains%0Alargely%20confined%20to%20image%20classification%2C%20object%20detection%2C%20and%20event-based%0Atracking.%20In%20contrast%2C%20real-world%20vision%20systems%20still%20widely%20use%20conventional%0ARGB%20video%20streams%2C%20where%20the%20potential%20of%20directly-trained%20SNNs%20for%20complex%0Atemporal%20tasks%20such%20as%20multi-object%20tracking%20%28MOT%29%20remains%20underexplored.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20SMTrack-the%20first%20directly%20trained%20deep%20SNN%0Aframework%20for%20end-to-end%20multi-object%20tracking%20on%20standard%20RGB%20videos.%20SMTrack%0Aintroduces%20an%20adaptive%20and%20scale-aware%20Normalized%20Wasserstein%20Distance%20loss%0A%28Asa-NWDLoss%29%20to%20improve%20detection%20and%20localization%20performance%20under%20varying%0Aobject%20scales%20and%20densities.%20Specifically%2C%20the%20method%20computes%20the%20average%0Aobject%20size%20within%20each%20training%20batch%20and%20dynamically%20adjusts%20the%0Anormalization%20factor%2C%20thereby%20enhancing%20sensitivity%20to%20small%20objects.%20For%20the%0Aassociation%20stage%2C%20we%20incorporate%20the%20TrackTrack%20identity%20module%20to%20maintain%0Arobust%20and%20consistent%20object%20trajectories.%20Extensive%20evaluations%20on%20BEE24%2C%0AMOT17%2C%20MOT20%2C%20and%20DanceTrack%20show%20that%20SMTrack%20achieves%20performance%20on%20par%20with%0Aleading%20ANN-based%20MOT%20methods%2C%20advancing%20robust%20and%20accurate%20SNN-based%20tracking%0Ain%20complex%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14607v1&entry.124074799=Read"},
{"title": "Multimodal Quantum Vision Transformer for Enzyme Commission\n  Classification from Biochemical Representations", "author": "Murat Isik and Mandeep Kaur Saggi and Humaira Gowher and Sabre Kais", "abstract": "  Accurately predicting enzyme functionality remains one of the major\nchallenges in computational biology, particularly for enzymes with limited\nstructural annotations or sequence homology. We present a novel multimodal\nQuantum Machine Learning (QML) framework that enhances Enzyme Commission (EC)\nclassification by integrating four complementary biochemical modalities:\nprotein sequence embeddings, quantum-derived electronic descriptors, molecular\ngraph structures, and 2D molecular image representations. Quantum Vision\nTransformer (QVT) backbone equipped with modality-specific encoders and a\nunified cross-attention fusion module. By integrating graph features and\nspatial patterns, our method captures key stereoelectronic interactions behind\nenzyme function. Experimental results demonstrate that our multimodal QVT model\nachieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a\nsubstantial margin and achieving better performance results compared to other\nQML models.\n", "link": "http://arxiv.org/abs/2508.14844v1", "date": "2025-08-20", "relevancy": 2.1838, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Quantum%20Vision%20Transformer%20for%20Enzyme%20Commission%0A%20%20Classification%20from%20Biochemical%20Representations&body=Title%3A%20Multimodal%20Quantum%20Vision%20Transformer%20for%20Enzyme%20Commission%0A%20%20Classification%20from%20Biochemical%20Representations%0AAuthor%3A%20Murat%20Isik%20and%20Mandeep%20Kaur%20Saggi%20and%20Humaira%20Gowher%20and%20Sabre%20Kais%0AAbstract%3A%20%20%20Accurately%20predicting%20enzyme%20functionality%20remains%20one%20of%20the%20major%0Achallenges%20in%20computational%20biology%2C%20particularly%20for%20enzymes%20with%20limited%0Astructural%20annotations%20or%20sequence%20homology.%20We%20present%20a%20novel%20multimodal%0AQuantum%20Machine%20Learning%20%28QML%29%20framework%20that%20enhances%20Enzyme%20Commission%20%28EC%29%0Aclassification%20by%20integrating%20four%20complementary%20biochemical%20modalities%3A%0Aprotein%20sequence%20embeddings%2C%20quantum-derived%20electronic%20descriptors%2C%20molecular%0Agraph%20structures%2C%20and%202D%20molecular%20image%20representations.%20Quantum%20Vision%0ATransformer%20%28QVT%29%20backbone%20equipped%20with%20modality-specific%20encoders%20and%20a%0Aunified%20cross-attention%20fusion%20module.%20By%20integrating%20graph%20features%20and%0Aspatial%20patterns%2C%20our%20method%20captures%20key%20stereoelectronic%20interactions%20behind%0Aenzyme%20function.%20Experimental%20results%20demonstrate%20that%20our%20multimodal%20QVT%20model%0Aachieves%20a%20top-1%20accuracy%20of%2085.1%25%2C%20outperforming%20sequence-only%20baselines%20by%20a%0Asubstantial%20margin%20and%20achieving%20better%20performance%20results%20compared%20to%20other%0AQML%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Quantum%2520Vision%2520Transformer%2520for%2520Enzyme%2520Commission%250A%2520%2520Classification%2520from%2520Biochemical%2520Representations%26entry.906535625%3DMurat%2520Isik%2520and%2520Mandeep%2520Kaur%2520Saggi%2520and%2520Humaira%2520Gowher%2520and%2520Sabre%2520Kais%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520enzyme%2520functionality%2520remains%2520one%2520of%2520the%2520major%250Achallenges%2520in%2520computational%2520biology%252C%2520particularly%2520for%2520enzymes%2520with%2520limited%250Astructural%2520annotations%2520or%2520sequence%2520homology.%2520We%2520present%2520a%2520novel%2520multimodal%250AQuantum%2520Machine%2520Learning%2520%2528QML%2529%2520framework%2520that%2520enhances%2520Enzyme%2520Commission%2520%2528EC%2529%250Aclassification%2520by%2520integrating%2520four%2520complementary%2520biochemical%2520modalities%253A%250Aprotein%2520sequence%2520embeddings%252C%2520quantum-derived%2520electronic%2520descriptors%252C%2520molecular%250Agraph%2520structures%252C%2520and%25202D%2520molecular%2520image%2520representations.%2520Quantum%2520Vision%250ATransformer%2520%2528QVT%2529%2520backbone%2520equipped%2520with%2520modality-specific%2520encoders%2520and%2520a%250Aunified%2520cross-attention%2520fusion%2520module.%2520By%2520integrating%2520graph%2520features%2520and%250Aspatial%2520patterns%252C%2520our%2520method%2520captures%2520key%2520stereoelectronic%2520interactions%2520behind%250Aenzyme%2520function.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520multimodal%2520QVT%2520model%250Aachieves%2520a%2520top-1%2520accuracy%2520of%252085.1%2525%252C%2520outperforming%2520sequence-only%2520baselines%2520by%2520a%250Asubstantial%2520margin%2520and%2520achieving%2520better%2520performance%2520results%2520compared%2520to%2520other%250AQML%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Quantum%20Vision%20Transformer%20for%20Enzyme%20Commission%0A%20%20Classification%20from%20Biochemical%20Representations&entry.906535625=Murat%20Isik%20and%20Mandeep%20Kaur%20Saggi%20and%20Humaira%20Gowher%20and%20Sabre%20Kais&entry.1292438233=%20%20Accurately%20predicting%20enzyme%20functionality%20remains%20one%20of%20the%20major%0Achallenges%20in%20computational%20biology%2C%20particularly%20for%20enzymes%20with%20limited%0Astructural%20annotations%20or%20sequence%20homology.%20We%20present%20a%20novel%20multimodal%0AQuantum%20Machine%20Learning%20%28QML%29%20framework%20that%20enhances%20Enzyme%20Commission%20%28EC%29%0Aclassification%20by%20integrating%20four%20complementary%20biochemical%20modalities%3A%0Aprotein%20sequence%20embeddings%2C%20quantum-derived%20electronic%20descriptors%2C%20molecular%0Agraph%20structures%2C%20and%202D%20molecular%20image%20representations.%20Quantum%20Vision%0ATransformer%20%28QVT%29%20backbone%20equipped%20with%20modality-specific%20encoders%20and%20a%0Aunified%20cross-attention%20fusion%20module.%20By%20integrating%20graph%20features%20and%0Aspatial%20patterns%2C%20our%20method%20captures%20key%20stereoelectronic%20interactions%20behind%0Aenzyme%20function.%20Experimental%20results%20demonstrate%20that%20our%20multimodal%20QVT%20model%0Aachieves%20a%20top-1%20accuracy%20of%2085.1%25%2C%20outperforming%20sequence-only%20baselines%20by%20a%0Asubstantial%20margin%20and%20achieving%20better%20performance%20results%20compared%20to%20other%0AQML%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14844v1&entry.124074799=Read"},
{"title": "EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic\n  Attention", "author": "Lakshmi Annamalai and Chetan Singh Thakur", "abstract": "  Road segmentation is pivotal for autonomous vehicles, yet achieving low\nlatency and low compute solutions using frame based cameras remains a\nchallenge. Event cameras offer a promising alternative. To leverage their low\npower sensing, we introduce EventSSEG, a method for road segmentation that uses\nevent only computing and a probabilistic attention mechanism. Event only\ncomputing poses a challenge in transferring pretrained weights from the\nconventional camera domain, requiring abundant labeled data, which is scarce.\nTo overcome this, EventSSEG employs event-based self supervised learning,\neliminating the need for extensive labeled data. Experiments on DSEC-Semantic\nand DDD17 show that EventSSEG achieves state of the art performance with\nminimal labeled events. This approach maximizes event cameras capabilities and\naddresses the lack of labeled events.\n", "link": "http://arxiv.org/abs/2508.14856v1", "date": "2025-08-20", "relevancy": 2.1797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5404}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventSSEG%3A%20Event-driven%20Self-Supervised%20Segmentation%20with%20Probabilistic%0A%20%20Attention&body=Title%3A%20EventSSEG%3A%20Event-driven%20Self-Supervised%20Segmentation%20with%20Probabilistic%0A%20%20Attention%0AAuthor%3A%20Lakshmi%20Annamalai%20and%20Chetan%20Singh%20Thakur%0AAbstract%3A%20%20%20Road%20segmentation%20is%20pivotal%20for%20autonomous%20vehicles%2C%20yet%20achieving%20low%0Alatency%20and%20low%20compute%20solutions%20using%20frame%20based%20cameras%20remains%20a%0Achallenge.%20Event%20cameras%20offer%20a%20promising%20alternative.%20To%20leverage%20their%20low%0Apower%20sensing%2C%20we%20introduce%20EventSSEG%2C%20a%20method%20for%20road%20segmentation%20that%20uses%0Aevent%20only%20computing%20and%20a%20probabilistic%20attention%20mechanism.%20Event%20only%0Acomputing%20poses%20a%20challenge%20in%20transferring%20pretrained%20weights%20from%20the%0Aconventional%20camera%20domain%2C%20requiring%20abundant%20labeled%20data%2C%20which%20is%20scarce.%0ATo%20overcome%20this%2C%20EventSSEG%20employs%20event-based%20self%20supervised%20learning%2C%0Aeliminating%20the%20need%20for%20extensive%20labeled%20data.%20Experiments%20on%20DSEC-Semantic%0Aand%20DDD17%20show%20that%20EventSSEG%20achieves%20state%20of%20the%20art%20performance%20with%0Aminimal%20labeled%20events.%20This%20approach%20maximizes%20event%20cameras%20capabilities%20and%0Aaddresses%20the%20lack%20of%20labeled%20events.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventSSEG%253A%2520Event-driven%2520Self-Supervised%2520Segmentation%2520with%2520Probabilistic%250A%2520%2520Attention%26entry.906535625%3DLakshmi%2520Annamalai%2520and%2520Chetan%2520Singh%2520Thakur%26entry.1292438233%3D%2520%2520Road%2520segmentation%2520is%2520pivotal%2520for%2520autonomous%2520vehicles%252C%2520yet%2520achieving%2520low%250Alatency%2520and%2520low%2520compute%2520solutions%2520using%2520frame%2520based%2520cameras%2520remains%2520a%250Achallenge.%2520Event%2520cameras%2520offer%2520a%2520promising%2520alternative.%2520To%2520leverage%2520their%2520low%250Apower%2520sensing%252C%2520we%2520introduce%2520EventSSEG%252C%2520a%2520method%2520for%2520road%2520segmentation%2520that%2520uses%250Aevent%2520only%2520computing%2520and%2520a%2520probabilistic%2520attention%2520mechanism.%2520Event%2520only%250Acomputing%2520poses%2520a%2520challenge%2520in%2520transferring%2520pretrained%2520weights%2520from%2520the%250Aconventional%2520camera%2520domain%252C%2520requiring%2520abundant%2520labeled%2520data%252C%2520which%2520is%2520scarce.%250ATo%2520overcome%2520this%252C%2520EventSSEG%2520employs%2520event-based%2520self%2520supervised%2520learning%252C%250Aeliminating%2520the%2520need%2520for%2520extensive%2520labeled%2520data.%2520Experiments%2520on%2520DSEC-Semantic%250Aand%2520DDD17%2520show%2520that%2520EventSSEG%2520achieves%2520state%2520of%2520the%2520art%2520performance%2520with%250Aminimal%2520labeled%2520events.%2520This%2520approach%2520maximizes%2520event%2520cameras%2520capabilities%2520and%250Aaddresses%2520the%2520lack%2520of%2520labeled%2520events.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventSSEG%3A%20Event-driven%20Self-Supervised%20Segmentation%20with%20Probabilistic%0A%20%20Attention&entry.906535625=Lakshmi%20Annamalai%20and%20Chetan%20Singh%20Thakur&entry.1292438233=%20%20Road%20segmentation%20is%20pivotal%20for%20autonomous%20vehicles%2C%20yet%20achieving%20low%0Alatency%20and%20low%20compute%20solutions%20using%20frame%20based%20cameras%20remains%20a%0Achallenge.%20Event%20cameras%20offer%20a%20promising%20alternative.%20To%20leverage%20their%20low%0Apower%20sensing%2C%20we%20introduce%20EventSSEG%2C%20a%20method%20for%20road%20segmentation%20that%20uses%0Aevent%20only%20computing%20and%20a%20probabilistic%20attention%20mechanism.%20Event%20only%0Acomputing%20poses%20a%20challenge%20in%20transferring%20pretrained%20weights%20from%20the%0Aconventional%20camera%20domain%2C%20requiring%20abundant%20labeled%20data%2C%20which%20is%20scarce.%0ATo%20overcome%20this%2C%20EventSSEG%20employs%20event-based%20self%20supervised%20learning%2C%0Aeliminating%20the%20need%20for%20extensive%20labeled%20data.%20Experiments%20on%20DSEC-Semantic%0Aand%20DDD17%20show%20that%20EventSSEG%20achieves%20state%20of%20the%20art%20performance%20with%0Aminimal%20labeled%20events.%20This%20approach%20maximizes%20event%20cameras%20capabilities%20and%0Aaddresses%20the%20lack%20of%20labeled%20events.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14856v1&entry.124074799=Read"},
{"title": "The importance of visual modelling languages in generative software\n  engineering", "author": "Roberto Rossi", "abstract": "  Multimodal GPTs represent a watershed in the interplay between Software\nEngineering and Generative Artificial Intelligence. GPT-4 accepts image and\ntext inputs, rather than simply natural language. We investigate relevant use\ncases stemming from these enhanced capabilities of GPT-4. To the best of our\nknowledge, no other work has investigated similar use cases involving Software\nEngineering tasks carried out via multimodal GPTs prompted with a mix of\ndiagrams and natural language.\n", "link": "http://arxiv.org/abs/2411.17976v4", "date": "2025-08-20", "relevancy": 2.177, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5653}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.533}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20importance%20of%20visual%20modelling%20languages%20in%20generative%20software%0A%20%20engineering&body=Title%3A%20The%20importance%20of%20visual%20modelling%20languages%20in%20generative%20software%0A%20%20engineering%0AAuthor%3A%20Roberto%20Rossi%0AAbstract%3A%20%20%20Multimodal%20GPTs%20represent%20a%20watershed%20in%20the%20interplay%20between%20Software%0AEngineering%20and%20Generative%20Artificial%20Intelligence.%20GPT-4%20accepts%20image%20and%0Atext%20inputs%2C%20rather%20than%20simply%20natural%20language.%20We%20investigate%20relevant%20use%0Acases%20stemming%20from%20these%20enhanced%20capabilities%20of%20GPT-4.%20To%20the%20best%20of%20our%0Aknowledge%2C%20no%20other%20work%20has%20investigated%20similar%20use%20cases%20involving%20Software%0AEngineering%20tasks%20carried%20out%20via%20multimodal%20GPTs%20prompted%20with%20a%20mix%20of%0Adiagrams%20and%20natural%20language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17976v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520importance%2520of%2520visual%2520modelling%2520languages%2520in%2520generative%2520software%250A%2520%2520engineering%26entry.906535625%3DRoberto%2520Rossi%26entry.1292438233%3D%2520%2520Multimodal%2520GPTs%2520represent%2520a%2520watershed%2520in%2520the%2520interplay%2520between%2520Software%250AEngineering%2520and%2520Generative%2520Artificial%2520Intelligence.%2520GPT-4%2520accepts%2520image%2520and%250Atext%2520inputs%252C%2520rather%2520than%2520simply%2520natural%2520language.%2520We%2520investigate%2520relevant%2520use%250Acases%2520stemming%2520from%2520these%2520enhanced%2520capabilities%2520of%2520GPT-4.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520no%2520other%2520work%2520has%2520investigated%2520similar%2520use%2520cases%2520involving%2520Software%250AEngineering%2520tasks%2520carried%2520out%2520via%2520multimodal%2520GPTs%2520prompted%2520with%2520a%2520mix%2520of%250Adiagrams%2520and%2520natural%2520language.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17976v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20importance%20of%20visual%20modelling%20languages%20in%20generative%20software%0A%20%20engineering&entry.906535625=Roberto%20Rossi&entry.1292438233=%20%20Multimodal%20GPTs%20represent%20a%20watershed%20in%20the%20interplay%20between%20Software%0AEngineering%20and%20Generative%20Artificial%20Intelligence.%20GPT-4%20accepts%20image%20and%0Atext%20inputs%2C%20rather%20than%20simply%20natural%20language.%20We%20investigate%20relevant%20use%0Acases%20stemming%20from%20these%20enhanced%20capabilities%20of%20GPT-4.%20To%20the%20best%20of%20our%0Aknowledge%2C%20no%20other%20work%20has%20investigated%20similar%20use%20cases%20involving%20Software%0AEngineering%20tasks%20carried%20out%20via%20multimodal%20GPTs%20prompted%20with%20a%20mix%20of%0Adiagrams%20and%20natural%20language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17976v4&entry.124074799=Read"},
{"title": "The NordDRG AI Benchmark for Large Language Models", "author": "Tapio Pitk\u00e4ranta", "abstract": "  Large language models (LLMs) are being piloted for clinical coding and\ndecision support, yet no open benchmark targets the hospital-funding layer\nwhere Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD\nsystems, DRGs route a substantial share of multi-trillion-dollar health\nspending through governed grouper software, making transparency and\nauditability first-order concerns. We release NordDRG-AI-Benchmark, the first\npublic, rule-complete test bed for DRG reasoning. The package includes (i)\nmachine-readable approximately 20-sheet NordDRG definition tables and (ii)\nexpert manuals and change-log templates that capture governance workflows. It\nexposes two suites: a 13-task Logic benchmark (code lookup, cross-table\ninference, grouping features, multilingual terminology, and CC/MCC validity\nchecks) and a 13-task Grouper benchmark that requires full DRG grouper\nemulation with strict exact-match scoring on both the DRG and the triggering\ndrg_logic.id. Lightweight reference agents (LogicAgent, GrouperAgent) enable\nartefact-only evaluation. Under an artefact-only (no web) setting, on the 13\nLogic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier\nmodels (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining\nmodels score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5\nThinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13,\nand all other tested endpoints score 0/13. To our knowledge, this is the first\npublic report of an LLM partially emulating the complete NordDRG grouper logic\nwith governance-grade traceability. Coupling a rule-complete release with\nexact-match tasks and open scoring provides a reproducible yardstick for\nhead-to-head and longitudinal evaluation in hospital funding. Benchmark\nmaterials available in Github.\n", "link": "http://arxiv.org/abs/2506.13790v3", "date": "2025-08-20", "relevancy": 2.1615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4399}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20NordDRG%20AI%20Benchmark%20for%20Large%20Language%20Models&body=Title%3A%20The%20NordDRG%20AI%20Benchmark%20for%20Large%20Language%20Models%0AAuthor%3A%20Tapio%20Pitk%C3%A4ranta%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20being%20piloted%20for%20clinical%20coding%20and%0Adecision%20support%2C%20yet%20no%20open%20benchmark%20targets%20the%20hospital-funding%20layer%0Awhere%20Diagnosis-Related%20Groups%20%28DRGs%29%20determine%20reimbursement.%20In%20most%20OECD%0Asystems%2C%20DRGs%20route%20a%20substantial%20share%20of%20multi-trillion-dollar%20health%0Aspending%20through%20governed%20grouper%20software%2C%20making%20transparency%20and%0Aauditability%20first-order%20concerns.%20We%20release%20NordDRG-AI-Benchmark%2C%20the%20first%0Apublic%2C%20rule-complete%20test%20bed%20for%20DRG%20reasoning.%20The%20package%20includes%20%28i%29%0Amachine-readable%20approximately%2020-sheet%20NordDRG%20definition%20tables%20and%20%28ii%29%0Aexpert%20manuals%20and%20change-log%20templates%20that%20capture%20governance%20workflows.%20It%0Aexposes%20two%20suites%3A%20a%2013-task%20Logic%20benchmark%20%28code%20lookup%2C%20cross-table%0Ainference%2C%20grouping%20features%2C%20multilingual%20terminology%2C%20and%20CC/MCC%20validity%0Achecks%29%20and%20a%2013-task%20Grouper%20benchmark%20that%20requires%20full%20DRG%20grouper%0Aemulation%20with%20strict%20exact-match%20scoring%20on%20both%20the%20DRG%20and%20the%20triggering%0Adrg_logic.id.%20Lightweight%20reference%20agents%20%28LogicAgent%2C%20GrouperAgent%29%20enable%0Aartefact-only%20evaluation.%20Under%20an%20artefact-only%20%28no%20web%29%20setting%2C%20on%20the%2013%0ALogic%20tasks%20GPT-5%20Thinking%20and%20Opus%204.1%20score%2013/13%2C%20o3%20scores%2012/13%3B%20mid-tier%0Amodels%20%28GPT-5%20Thinking%20Mini%2C%20o4-mini%2C%20GPT-5%20Fast%29%20achieve%206-8/13%2C%20and%20remaining%0Amodels%20score%205/13%20or%20below.%20On%20full%20grouper%20emulation%20across%2013%20tasks%2C%20GPT-5%0AThinking%20solves%207/13%2C%20o3%206/13%2C%20o4-mini%203/13%3B%20GPT-5%20Thinking%20Mini%20solves%201/13%2C%0Aand%20all%20other%20tested%20endpoints%20score%200/13.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Apublic%20report%20of%20an%20LLM%20partially%20emulating%20the%20complete%20NordDRG%20grouper%20logic%0Awith%20governance-grade%20traceability.%20Coupling%20a%20rule-complete%20release%20with%0Aexact-match%20tasks%20and%20open%20scoring%20provides%20a%20reproducible%20yardstick%20for%0Ahead-to-head%20and%20longitudinal%20evaluation%20in%20hospital%20funding.%20Benchmark%0Amaterials%20available%20in%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13790v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520NordDRG%2520AI%2520Benchmark%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DTapio%2520Pitk%25C3%25A4ranta%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520being%2520piloted%2520for%2520clinical%2520coding%2520and%250Adecision%2520support%252C%2520yet%2520no%2520open%2520benchmark%2520targets%2520the%2520hospital-funding%2520layer%250Awhere%2520Diagnosis-Related%2520Groups%2520%2528DRGs%2529%2520determine%2520reimbursement.%2520In%2520most%2520OECD%250Asystems%252C%2520DRGs%2520route%2520a%2520substantial%2520share%2520of%2520multi-trillion-dollar%2520health%250Aspending%2520through%2520governed%2520grouper%2520software%252C%2520making%2520transparency%2520and%250Aauditability%2520first-order%2520concerns.%2520We%2520release%2520NordDRG-AI-Benchmark%252C%2520the%2520first%250Apublic%252C%2520rule-complete%2520test%2520bed%2520for%2520DRG%2520reasoning.%2520The%2520package%2520includes%2520%2528i%2529%250Amachine-readable%2520approximately%252020-sheet%2520NordDRG%2520definition%2520tables%2520and%2520%2528ii%2529%250Aexpert%2520manuals%2520and%2520change-log%2520templates%2520that%2520capture%2520governance%2520workflows.%2520It%250Aexposes%2520two%2520suites%253A%2520a%252013-task%2520Logic%2520benchmark%2520%2528code%2520lookup%252C%2520cross-table%250Ainference%252C%2520grouping%2520features%252C%2520multilingual%2520terminology%252C%2520and%2520CC/MCC%2520validity%250Achecks%2529%2520and%2520a%252013-task%2520Grouper%2520benchmark%2520that%2520requires%2520full%2520DRG%2520grouper%250Aemulation%2520with%2520strict%2520exact-match%2520scoring%2520on%2520both%2520the%2520DRG%2520and%2520the%2520triggering%250Adrg_logic.id.%2520Lightweight%2520reference%2520agents%2520%2528LogicAgent%252C%2520GrouperAgent%2529%2520enable%250Aartefact-only%2520evaluation.%2520Under%2520an%2520artefact-only%2520%2528no%2520web%2529%2520setting%252C%2520on%2520the%252013%250ALogic%2520tasks%2520GPT-5%2520Thinking%2520and%2520Opus%25204.1%2520score%252013/13%252C%2520o3%2520scores%252012/13%253B%2520mid-tier%250Amodels%2520%2528GPT-5%2520Thinking%2520Mini%252C%2520o4-mini%252C%2520GPT-5%2520Fast%2529%2520achieve%25206-8/13%252C%2520and%2520remaining%250Amodels%2520score%25205/13%2520or%2520below.%2520On%2520full%2520grouper%2520emulation%2520across%252013%2520tasks%252C%2520GPT-5%250AThinking%2520solves%25207/13%252C%2520o3%25206/13%252C%2520o4-mini%25203/13%253B%2520GPT-5%2520Thinking%2520Mini%2520solves%25201/13%252C%250Aand%2520all%2520other%2520tested%2520endpoints%2520score%25200/13.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Apublic%2520report%2520of%2520an%2520LLM%2520partially%2520emulating%2520the%2520complete%2520NordDRG%2520grouper%2520logic%250Awith%2520governance-grade%2520traceability.%2520Coupling%2520a%2520rule-complete%2520release%2520with%250Aexact-match%2520tasks%2520and%2520open%2520scoring%2520provides%2520a%2520reproducible%2520yardstick%2520for%250Ahead-to-head%2520and%2520longitudinal%2520evaluation%2520in%2520hospital%2520funding.%2520Benchmark%250Amaterials%2520available%2520in%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13790v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20NordDRG%20AI%20Benchmark%20for%20Large%20Language%20Models&entry.906535625=Tapio%20Pitk%C3%A4ranta&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20being%20piloted%20for%20clinical%20coding%20and%0Adecision%20support%2C%20yet%20no%20open%20benchmark%20targets%20the%20hospital-funding%20layer%0Awhere%20Diagnosis-Related%20Groups%20%28DRGs%29%20determine%20reimbursement.%20In%20most%20OECD%0Asystems%2C%20DRGs%20route%20a%20substantial%20share%20of%20multi-trillion-dollar%20health%0Aspending%20through%20governed%20grouper%20software%2C%20making%20transparency%20and%0Aauditability%20first-order%20concerns.%20We%20release%20NordDRG-AI-Benchmark%2C%20the%20first%0Apublic%2C%20rule-complete%20test%20bed%20for%20DRG%20reasoning.%20The%20package%20includes%20%28i%29%0Amachine-readable%20approximately%2020-sheet%20NordDRG%20definition%20tables%20and%20%28ii%29%0Aexpert%20manuals%20and%20change-log%20templates%20that%20capture%20governance%20workflows.%20It%0Aexposes%20two%20suites%3A%20a%2013-task%20Logic%20benchmark%20%28code%20lookup%2C%20cross-table%0Ainference%2C%20grouping%20features%2C%20multilingual%20terminology%2C%20and%20CC/MCC%20validity%0Achecks%29%20and%20a%2013-task%20Grouper%20benchmark%20that%20requires%20full%20DRG%20grouper%0Aemulation%20with%20strict%20exact-match%20scoring%20on%20both%20the%20DRG%20and%20the%20triggering%0Adrg_logic.id.%20Lightweight%20reference%20agents%20%28LogicAgent%2C%20GrouperAgent%29%20enable%0Aartefact-only%20evaluation.%20Under%20an%20artefact-only%20%28no%20web%29%20setting%2C%20on%20the%2013%0ALogic%20tasks%20GPT-5%20Thinking%20and%20Opus%204.1%20score%2013/13%2C%20o3%20scores%2012/13%3B%20mid-tier%0Amodels%20%28GPT-5%20Thinking%20Mini%2C%20o4-mini%2C%20GPT-5%20Fast%29%20achieve%206-8/13%2C%20and%20remaining%0Amodels%20score%205/13%20or%20below.%20On%20full%20grouper%20emulation%20across%2013%20tasks%2C%20GPT-5%0AThinking%20solves%207/13%2C%20o3%206/13%2C%20o4-mini%203/13%3B%20GPT-5%20Thinking%20Mini%20solves%201/13%2C%0Aand%20all%20other%20tested%20endpoints%20score%200/13.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Apublic%20report%20of%20an%20LLM%20partially%20emulating%20the%20complete%20NordDRG%20grouper%20logic%0Awith%20governance-grade%20traceability.%20Coupling%20a%20rule-complete%20release%20with%0Aexact-match%20tasks%20and%20open%20scoring%20provides%20a%20reproducible%20yardstick%20for%0Ahead-to-head%20and%20longitudinal%20evaluation%20in%20hospital%20funding.%20Benchmark%0Amaterials%20available%20in%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13790v3&entry.124074799=Read"},
{"title": "Hybrid-Hierarchical Fashion Graph Attention Network for\n  Compatibility-Oriented and Personalized Outfit Recommendation", "author": "Sajjad Saed and Babak Teimourpour", "abstract": "  The rapid expansion of the fashion industry and the growing variety of\nproducts have made it increasingly challenging for users to identify compatible\nitems on e-commerce platforms. Effective fashion recommendation systems are\ntherefore crucial for filtering irrelevant options and suggesting suitable\nones. However, simultaneously addressing outfit compatibility and personalized\nrecommendations remains a significant challenge, as these aspects are typically\ntreated independently in existing studies, thereby overlooking the complex\ninteractions between items and user preferences. This research introduces a new\nframework named FGAT, which leverages a hierarchical graph representation\ntogether with graph attention mechanisms to address this problem. The framework\nconstructs a three-tier graph of users, outfits, and items, integrating visual\nand textual features to jointly model outfit compatibility and user\npreferences. By dynamically weighting node importance during representation\npropagation, the graph attention mechanism captures key interactions and\nproduces precise embeddings for both user preferences and outfit compatibility.\nEvaluated on the POG dataset, FGAT outperforms strong baselines such as HFGN,\nachieving notable improvements in accuracy, precision, HR, recall, and NDCG.\nThese results demonstrate that combining multimodal visual and textual features\nwith a hierarchical graph structure and attention mechanisms significantly\nenhances the effectiveness and efficiency of personalized fashion\nrecommendation systems.\n", "link": "http://arxiv.org/abs/2508.11105v2", "date": "2025-08-20", "relevancy": 2.143, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.578}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5565}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid-Hierarchical%20Fashion%20Graph%20Attention%20Network%20for%0A%20%20Compatibility-Oriented%20and%20Personalized%20Outfit%20Recommendation&body=Title%3A%20Hybrid-Hierarchical%20Fashion%20Graph%20Attention%20Network%20for%0A%20%20Compatibility-Oriented%20and%20Personalized%20Outfit%20Recommendation%0AAuthor%3A%20Sajjad%20Saed%20and%20Babak%20Teimourpour%0AAbstract%3A%20%20%20The%20rapid%20expansion%20of%20the%20fashion%20industry%20and%20the%20growing%20variety%20of%0Aproducts%20have%20made%20it%20increasingly%20challenging%20for%20users%20to%20identify%20compatible%0Aitems%20on%20e-commerce%20platforms.%20Effective%20fashion%20recommendation%20systems%20are%0Atherefore%20crucial%20for%20filtering%20irrelevant%20options%20and%20suggesting%20suitable%0Aones.%20However%2C%20simultaneously%20addressing%20outfit%20compatibility%20and%20personalized%0Arecommendations%20remains%20a%20significant%20challenge%2C%20as%20these%20aspects%20are%20typically%0Atreated%20independently%20in%20existing%20studies%2C%20thereby%20overlooking%20the%20complex%0Ainteractions%20between%20items%20and%20user%20preferences.%20This%20research%20introduces%20a%20new%0Aframework%20named%20FGAT%2C%20which%20leverages%20a%20hierarchical%20graph%20representation%0Atogether%20with%20graph%20attention%20mechanisms%20to%20address%20this%20problem.%20The%20framework%0Aconstructs%20a%20three-tier%20graph%20of%20users%2C%20outfits%2C%20and%20items%2C%20integrating%20visual%0Aand%20textual%20features%20to%20jointly%20model%20outfit%20compatibility%20and%20user%0Apreferences.%20By%20dynamically%20weighting%20node%20importance%20during%20representation%0Apropagation%2C%20the%20graph%20attention%20mechanism%20captures%20key%20interactions%20and%0Aproduces%20precise%20embeddings%20for%20both%20user%20preferences%20and%20outfit%20compatibility.%0AEvaluated%20on%20the%20POG%20dataset%2C%20FGAT%20outperforms%20strong%20baselines%20such%20as%20HFGN%2C%0Aachieving%20notable%20improvements%20in%20accuracy%2C%20precision%2C%20HR%2C%20recall%2C%20and%20NDCG.%0AThese%20results%20demonstrate%20that%20combining%20multimodal%20visual%20and%20textual%20features%0Awith%20a%20hierarchical%20graph%20structure%20and%20attention%20mechanisms%20significantly%0Aenhances%20the%20effectiveness%20and%20efficiency%20of%20personalized%20fashion%0Arecommendation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid-Hierarchical%2520Fashion%2520Graph%2520Attention%2520Network%2520for%250A%2520%2520Compatibility-Oriented%2520and%2520Personalized%2520Outfit%2520Recommendation%26entry.906535625%3DSajjad%2520Saed%2520and%2520Babak%2520Teimourpour%26entry.1292438233%3D%2520%2520The%2520rapid%2520expansion%2520of%2520the%2520fashion%2520industry%2520and%2520the%2520growing%2520variety%2520of%250Aproducts%2520have%2520made%2520it%2520increasingly%2520challenging%2520for%2520users%2520to%2520identify%2520compatible%250Aitems%2520on%2520e-commerce%2520platforms.%2520Effective%2520fashion%2520recommendation%2520systems%2520are%250Atherefore%2520crucial%2520for%2520filtering%2520irrelevant%2520options%2520and%2520suggesting%2520suitable%250Aones.%2520However%252C%2520simultaneously%2520addressing%2520outfit%2520compatibility%2520and%2520personalized%250Arecommendations%2520remains%2520a%2520significant%2520challenge%252C%2520as%2520these%2520aspects%2520are%2520typically%250Atreated%2520independently%2520in%2520existing%2520studies%252C%2520thereby%2520overlooking%2520the%2520complex%250Ainteractions%2520between%2520items%2520and%2520user%2520preferences.%2520This%2520research%2520introduces%2520a%2520new%250Aframework%2520named%2520FGAT%252C%2520which%2520leverages%2520a%2520hierarchical%2520graph%2520representation%250Atogether%2520with%2520graph%2520attention%2520mechanisms%2520to%2520address%2520this%2520problem.%2520The%2520framework%250Aconstructs%2520a%2520three-tier%2520graph%2520of%2520users%252C%2520outfits%252C%2520and%2520items%252C%2520integrating%2520visual%250Aand%2520textual%2520features%2520to%2520jointly%2520model%2520outfit%2520compatibility%2520and%2520user%250Apreferences.%2520By%2520dynamically%2520weighting%2520node%2520importance%2520during%2520representation%250Apropagation%252C%2520the%2520graph%2520attention%2520mechanism%2520captures%2520key%2520interactions%2520and%250Aproduces%2520precise%2520embeddings%2520for%2520both%2520user%2520preferences%2520and%2520outfit%2520compatibility.%250AEvaluated%2520on%2520the%2520POG%2520dataset%252C%2520FGAT%2520outperforms%2520strong%2520baselines%2520such%2520as%2520HFGN%252C%250Aachieving%2520notable%2520improvements%2520in%2520accuracy%252C%2520precision%252C%2520HR%252C%2520recall%252C%2520and%2520NDCG.%250AThese%2520results%2520demonstrate%2520that%2520combining%2520multimodal%2520visual%2520and%2520textual%2520features%250Awith%2520a%2520hierarchical%2520graph%2520structure%2520and%2520attention%2520mechanisms%2520significantly%250Aenhances%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520personalized%2520fashion%250Arecommendation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid-Hierarchical%20Fashion%20Graph%20Attention%20Network%20for%0A%20%20Compatibility-Oriented%20and%20Personalized%20Outfit%20Recommendation&entry.906535625=Sajjad%20Saed%20and%20Babak%20Teimourpour&entry.1292438233=%20%20The%20rapid%20expansion%20of%20the%20fashion%20industry%20and%20the%20growing%20variety%20of%0Aproducts%20have%20made%20it%20increasingly%20challenging%20for%20users%20to%20identify%20compatible%0Aitems%20on%20e-commerce%20platforms.%20Effective%20fashion%20recommendation%20systems%20are%0Atherefore%20crucial%20for%20filtering%20irrelevant%20options%20and%20suggesting%20suitable%0Aones.%20However%2C%20simultaneously%20addressing%20outfit%20compatibility%20and%20personalized%0Arecommendations%20remains%20a%20significant%20challenge%2C%20as%20these%20aspects%20are%20typically%0Atreated%20independently%20in%20existing%20studies%2C%20thereby%20overlooking%20the%20complex%0Ainteractions%20between%20items%20and%20user%20preferences.%20This%20research%20introduces%20a%20new%0Aframework%20named%20FGAT%2C%20which%20leverages%20a%20hierarchical%20graph%20representation%0Atogether%20with%20graph%20attention%20mechanisms%20to%20address%20this%20problem.%20The%20framework%0Aconstructs%20a%20three-tier%20graph%20of%20users%2C%20outfits%2C%20and%20items%2C%20integrating%20visual%0Aand%20textual%20features%20to%20jointly%20model%20outfit%20compatibility%20and%20user%0Apreferences.%20By%20dynamically%20weighting%20node%20importance%20during%20representation%0Apropagation%2C%20the%20graph%20attention%20mechanism%20captures%20key%20interactions%20and%0Aproduces%20precise%20embeddings%20for%20both%20user%20preferences%20and%20outfit%20compatibility.%0AEvaluated%20on%20the%20POG%20dataset%2C%20FGAT%20outperforms%20strong%20baselines%20such%20as%20HFGN%2C%0Aachieving%20notable%20improvements%20in%20accuracy%2C%20precision%2C%20HR%2C%20recall%2C%20and%20NDCG.%0AThese%20results%20demonstrate%20that%20combining%20multimodal%20visual%20and%20textual%20features%0Awith%20a%20hierarchical%20graph%20structure%20and%20attention%20mechanisms%20significantly%0Aenhances%20the%20effectiveness%20and%20efficiency%20of%20personalized%20fashion%0Arecommendation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11105v2&entry.124074799=Read"},
{"title": "MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face\n  Forgery Detection", "author": "Chenqi Kong and Anwei Luo and Peijun Bao and Yi Yu and Haoliang Li and Zengwei Zheng and Shiqi Wang and Alex C. Kot", "abstract": "  Deepfakes have recently raised significant trust issues and security concerns\namong the public. Compared to CNN face forgery detectors, ViT-based methods\ntake advantage of the expressivity of transformers, achieving superior\ndetection performance. However, these approaches still exhibit the following\nlimitations: (1) Fully fine-tuning ViT-based models from ImageNet weights\ndemands substantial computational and storage resources; (2) ViT-based methods\nstruggle to capture local forgery clues, leading to model bias; (3) These\nmethods limit their scope on only one or few face forgery features, resulting\nin limited generalizability. To tackle these challenges, this work introduces\nMixture-of-Experts modules for Face Forgery Detection (MoE-FFD), a generalized\nyet parameter-efficient ViT-based approach. MoE-FFD only updates lightweight\nLow-Rank Adaptation (LoRA) and Adapter layers while keeping the ViT backbone\nfrozen, thereby achieving parameter-efficient training. Moreover, MoE-FFD\nleverages the expressivity of transformers and local priors of CNNs to\nsimultaneously extract global and local forgery clues. Additionally, novel MoE\nmodules are designed to scale the model's capacity and smartly select optimal\nforgery experts, further enhancing forgery detection performance. Our proposed\nlearning scheme can be seamlessly adapted to various transformer backbones in a\nplug-and-play manner. Extensive experimental results demonstrate that the\nproposed method achieves state-of-the-art face forgery detection performance\nwith significantly reduced parameter overhead. The code is released at:\nhttps://github.com/LoveSiameseCat/MoE-FFD.\n", "link": "http://arxiv.org/abs/2404.08452v3", "date": "2025-08-20", "relevancy": 2.1425, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5674}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5141}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoE-FFD%3A%20Mixture%20of%20Experts%20for%20Generalized%20and%20Parameter-Efficient%20Face%0A%20%20Forgery%20Detection&body=Title%3A%20MoE-FFD%3A%20Mixture%20of%20Experts%20for%20Generalized%20and%20Parameter-Efficient%20Face%0A%20%20Forgery%20Detection%0AAuthor%3A%20Chenqi%20Kong%20and%20Anwei%20Luo%20and%20Peijun%20Bao%20and%20Yi%20Yu%20and%20Haoliang%20Li%20and%20Zengwei%20Zheng%20and%20Shiqi%20Wang%20and%20Alex%20C.%20Kot%0AAbstract%3A%20%20%20Deepfakes%20have%20recently%20raised%20significant%20trust%20issues%20and%20security%20concerns%0Aamong%20the%20public.%20Compared%20to%20CNN%20face%20forgery%20detectors%2C%20ViT-based%20methods%0Atake%20advantage%20of%20the%20expressivity%20of%20transformers%2C%20achieving%20superior%0Adetection%20performance.%20However%2C%20these%20approaches%20still%20exhibit%20the%20following%0Alimitations%3A%20%281%29%20Fully%20fine-tuning%20ViT-based%20models%20from%20ImageNet%20weights%0Ademands%20substantial%20computational%20and%20storage%20resources%3B%20%282%29%20ViT-based%20methods%0Astruggle%20to%20capture%20local%20forgery%20clues%2C%20leading%20to%20model%20bias%3B%20%283%29%20These%0Amethods%20limit%20their%20scope%20on%20only%20one%20or%20few%20face%20forgery%20features%2C%20resulting%0Ain%20limited%20generalizability.%20To%20tackle%20these%20challenges%2C%20this%20work%20introduces%0AMixture-of-Experts%20modules%20for%20Face%20Forgery%20Detection%20%28MoE-FFD%29%2C%20a%20generalized%0Ayet%20parameter-efficient%20ViT-based%20approach.%20MoE-FFD%20only%20updates%20lightweight%0ALow-Rank%20Adaptation%20%28LoRA%29%20and%20Adapter%20layers%20while%20keeping%20the%20ViT%20backbone%0Afrozen%2C%20thereby%20achieving%20parameter-efficient%20training.%20Moreover%2C%20MoE-FFD%0Aleverages%20the%20expressivity%20of%20transformers%20and%20local%20priors%20of%20CNNs%20to%0Asimultaneously%20extract%20global%20and%20local%20forgery%20clues.%20Additionally%2C%20novel%20MoE%0Amodules%20are%20designed%20to%20scale%20the%20model%27s%20capacity%20and%20smartly%20select%20optimal%0Aforgery%20experts%2C%20further%20enhancing%20forgery%20detection%20performance.%20Our%20proposed%0Alearning%20scheme%20can%20be%20seamlessly%20adapted%20to%20various%20transformer%20backbones%20in%20a%0Aplug-and-play%20manner.%20Extensive%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20achieves%20state-of-the-art%20face%20forgery%20detection%20performance%0Awith%20significantly%20reduced%20parameter%20overhead.%20The%20code%20is%20released%20at%3A%0Ahttps%3A//github.com/LoveSiameseCat/MoE-FFD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08452v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoE-FFD%253A%2520Mixture%2520of%2520Experts%2520for%2520Generalized%2520and%2520Parameter-Efficient%2520Face%250A%2520%2520Forgery%2520Detection%26entry.906535625%3DChenqi%2520Kong%2520and%2520Anwei%2520Luo%2520and%2520Peijun%2520Bao%2520and%2520Yi%2520Yu%2520and%2520Haoliang%2520Li%2520and%2520Zengwei%2520Zheng%2520and%2520Shiqi%2520Wang%2520and%2520Alex%2520C.%2520Kot%26entry.1292438233%3D%2520%2520Deepfakes%2520have%2520recently%2520raised%2520significant%2520trust%2520issues%2520and%2520security%2520concerns%250Aamong%2520the%2520public.%2520Compared%2520to%2520CNN%2520face%2520forgery%2520detectors%252C%2520ViT-based%2520methods%250Atake%2520advantage%2520of%2520the%2520expressivity%2520of%2520transformers%252C%2520achieving%2520superior%250Adetection%2520performance.%2520However%252C%2520these%2520approaches%2520still%2520exhibit%2520the%2520following%250Alimitations%253A%2520%25281%2529%2520Fully%2520fine-tuning%2520ViT-based%2520models%2520from%2520ImageNet%2520weights%250Ademands%2520substantial%2520computational%2520and%2520storage%2520resources%253B%2520%25282%2529%2520ViT-based%2520methods%250Astruggle%2520to%2520capture%2520local%2520forgery%2520clues%252C%2520leading%2520to%2520model%2520bias%253B%2520%25283%2529%2520These%250Amethods%2520limit%2520their%2520scope%2520on%2520only%2520one%2520or%2520few%2520face%2520forgery%2520features%252C%2520resulting%250Ain%2520limited%2520generalizability.%2520To%2520tackle%2520these%2520challenges%252C%2520this%2520work%2520introduces%250AMixture-of-Experts%2520modules%2520for%2520Face%2520Forgery%2520Detection%2520%2528MoE-FFD%2529%252C%2520a%2520generalized%250Ayet%2520parameter-efficient%2520ViT-based%2520approach.%2520MoE-FFD%2520only%2520updates%2520lightweight%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%2520and%2520Adapter%2520layers%2520while%2520keeping%2520the%2520ViT%2520backbone%250Afrozen%252C%2520thereby%2520achieving%2520parameter-efficient%2520training.%2520Moreover%252C%2520MoE-FFD%250Aleverages%2520the%2520expressivity%2520of%2520transformers%2520and%2520local%2520priors%2520of%2520CNNs%2520to%250Asimultaneously%2520extract%2520global%2520and%2520local%2520forgery%2520clues.%2520Additionally%252C%2520novel%2520MoE%250Amodules%2520are%2520designed%2520to%2520scale%2520the%2520model%2527s%2520capacity%2520and%2520smartly%2520select%2520optimal%250Aforgery%2520experts%252C%2520further%2520enhancing%2520forgery%2520detection%2520performance.%2520Our%2520proposed%250Alearning%2520scheme%2520can%2520be%2520seamlessly%2520adapted%2520to%2520various%2520transformer%2520backbones%2520in%2520a%250Aplug-and-play%2520manner.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520achieves%2520state-of-the-art%2520face%2520forgery%2520detection%2520performance%250Awith%2520significantly%2520reduced%2520parameter%2520overhead.%2520The%2520code%2520is%2520released%2520at%253A%250Ahttps%253A//github.com/LoveSiameseCat/MoE-FFD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08452v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE-FFD%3A%20Mixture%20of%20Experts%20for%20Generalized%20and%20Parameter-Efficient%20Face%0A%20%20Forgery%20Detection&entry.906535625=Chenqi%20Kong%20and%20Anwei%20Luo%20and%20Peijun%20Bao%20and%20Yi%20Yu%20and%20Haoliang%20Li%20and%20Zengwei%20Zheng%20and%20Shiqi%20Wang%20and%20Alex%20C.%20Kot&entry.1292438233=%20%20Deepfakes%20have%20recently%20raised%20significant%20trust%20issues%20and%20security%20concerns%0Aamong%20the%20public.%20Compared%20to%20CNN%20face%20forgery%20detectors%2C%20ViT-based%20methods%0Atake%20advantage%20of%20the%20expressivity%20of%20transformers%2C%20achieving%20superior%0Adetection%20performance.%20However%2C%20these%20approaches%20still%20exhibit%20the%20following%0Alimitations%3A%20%281%29%20Fully%20fine-tuning%20ViT-based%20models%20from%20ImageNet%20weights%0Ademands%20substantial%20computational%20and%20storage%20resources%3B%20%282%29%20ViT-based%20methods%0Astruggle%20to%20capture%20local%20forgery%20clues%2C%20leading%20to%20model%20bias%3B%20%283%29%20These%0Amethods%20limit%20their%20scope%20on%20only%20one%20or%20few%20face%20forgery%20features%2C%20resulting%0Ain%20limited%20generalizability.%20To%20tackle%20these%20challenges%2C%20this%20work%20introduces%0AMixture-of-Experts%20modules%20for%20Face%20Forgery%20Detection%20%28MoE-FFD%29%2C%20a%20generalized%0Ayet%20parameter-efficient%20ViT-based%20approach.%20MoE-FFD%20only%20updates%20lightweight%0ALow-Rank%20Adaptation%20%28LoRA%29%20and%20Adapter%20layers%20while%20keeping%20the%20ViT%20backbone%0Afrozen%2C%20thereby%20achieving%20parameter-efficient%20training.%20Moreover%2C%20MoE-FFD%0Aleverages%20the%20expressivity%20of%20transformers%20and%20local%20priors%20of%20CNNs%20to%0Asimultaneously%20extract%20global%20and%20local%20forgery%20clues.%20Additionally%2C%20novel%20MoE%0Amodules%20are%20designed%20to%20scale%20the%20model%27s%20capacity%20and%20smartly%20select%20optimal%0Aforgery%20experts%2C%20further%20enhancing%20forgery%20detection%20performance.%20Our%20proposed%0Alearning%20scheme%20can%20be%20seamlessly%20adapted%20to%20various%20transformer%20backbones%20in%20a%0Aplug-and-play%20manner.%20Extensive%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20achieves%20state-of-the-art%20face%20forgery%20detection%20performance%0Awith%20significantly%20reduced%20parameter%20overhead.%20The%20code%20is%20released%20at%3A%0Ahttps%3A//github.com/LoveSiameseCat/MoE-FFD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08452v3&entry.124074799=Read"},
{"title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model\n  Context Protocol Servers", "author": "Ziyang Luo and Zhiqi Shen and Wenzhuo Yang and Zirui Zhao and Prathyusha Jwalapuram and Amrita Saha and Doyen Sahoo and Silvio Savarese and Caiming Xiong and Junnan Li", "abstract": "  The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem.\n", "link": "http://arxiv.org/abs/2508.14704v1", "date": "2025-08-20", "relevancy": 2.1393, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCP-Universe%3A%20Benchmarking%20Large%20Language%20Models%20with%20Real-World%20Model%0A%20%20Context%20Protocol%20Servers&body=Title%3A%20MCP-Universe%3A%20Benchmarking%20Large%20Language%20Models%20with%20Real-World%20Model%0A%20%20Context%20Protocol%20Servers%0AAuthor%3A%20Ziyang%20Luo%20and%20Zhiqi%20Shen%20and%20Wenzhuo%20Yang%20and%20Zirui%20Zhao%20and%20Prathyusha%20Jwalapuram%20and%20Amrita%20Saha%20and%20Doyen%20Sahoo%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Junnan%20Li%0AAbstract%3A%20%20%20The%20Model%20Context%20Protocol%20has%20emerged%20as%20a%20transformative%20standard%20for%0Aconnecting%20large%20language%20models%20to%20external%20data%20sources%20and%20tools%2C%20rapidly%0Againing%20adoption%20across%20major%20AI%20providers%20and%20development%20platforms.%20However%2C%0Aexisting%20benchmarks%20are%20overly%20simplistic%20and%20fail%20to%20capture%20real%20application%0Achallenges%20such%20as%20long-horizon%20reasoning%20and%20large%2C%20unfamiliar%20tool%20spaces.%20To%0Aaddress%20this%20critical%20gap%2C%20we%20introduce%20MCP-Universe%2C%20the%20first%20comprehensive%0Abenchmark%20specifically%20designed%20to%20evaluate%20LLMs%20in%20realistic%20and%20hard%20tasks%0Athrough%20interaction%20with%20real-world%20MCP%20servers.%20Our%20benchmark%20encompasses%206%0Acore%20domains%20spanning%2011%20different%20MCP%20servers%3A%20Location%20Navigation%2C%20Repository%0AManagement%2C%20Financial%20Analysis%2C%203D%20Design%2C%20Browser%20Automation%2C%20and%20Web%0ASearching.%20To%20ensure%20rigorous%20evaluation%2C%20we%20implement%20execution-based%0Aevaluators%2C%20including%20format%20evaluators%20for%20agent%20format%20compliance%2C%20static%0Aevaluators%20for%20time-invariant%20content%20matching%2C%20and%20dynamic%20evaluators%20that%0Aautomatically%20retrieve%20real-time%20ground%20truth%20for%20temporally%20sensitive%20tasks.%0AThrough%20extensive%20evaluation%20of%20leading%20LLMs%2C%20we%20find%20that%20even%20SOTA%20models%0Asuch%20as%20GPT-5%20%2843.72%25%29%2C%20Grok-4%20%2833.33%25%29%20and%20Claude-4.0-Sonnet%20%2829.44%25%29%20exhibit%0Asignificant%20performance%20limitations.%20In%20addition%2C%20our%20benchmark%20poses%20a%0Asignificant%20long-context%20challenge%20for%20LLM%20agents%2C%20as%20the%20number%20of%20input%0Atokens%20increases%20rapidly%20with%20the%20number%20of%20interaction%20steps.%20Moreover%2C%20it%0Aintroduces%20an%20unknown-tools%20challenge%2C%20as%20LLM%20agents%20often%20lack%20familiarity%0Awith%20the%20precise%20usage%20of%20the%20MCP%20servers.%20Notably%2C%20enterprise-level%20agents%0Alike%20Cursor%20cannot%20achieve%20better%20performance%20than%20standard%20ReAct%20frameworks.%0ABeyond%20evaluation%2C%20we%20open-source%20our%20extensible%20evaluation%20framework%20with%20UI%0Asupport%2C%20enabling%20researchers%20and%20practitioners%20to%20seamlessly%20integrate%20new%0Aagents%20and%20MCP%20servers%20while%20fostering%20innovation%20in%20the%20rapidly%20evolving%20MCP%0Aecosystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCP-Universe%253A%2520Benchmarking%2520Large%2520Language%2520Models%2520with%2520Real-World%2520Model%250A%2520%2520Context%2520Protocol%2520Servers%26entry.906535625%3DZiyang%2520Luo%2520and%2520Zhiqi%2520Shen%2520and%2520Wenzhuo%2520Yang%2520and%2520Zirui%2520Zhao%2520and%2520Prathyusha%2520Jwalapuram%2520and%2520Amrita%2520Saha%2520and%2520Doyen%2520Sahoo%2520and%2520Silvio%2520Savarese%2520and%2520Caiming%2520Xiong%2520and%2520Junnan%2520Li%26entry.1292438233%3D%2520%2520The%2520Model%2520Context%2520Protocol%2520has%2520emerged%2520as%2520a%2520transformative%2520standard%2520for%250Aconnecting%2520large%2520language%2520models%2520to%2520external%2520data%2520sources%2520and%2520tools%252C%2520rapidly%250Againing%2520adoption%2520across%2520major%2520AI%2520providers%2520and%2520development%2520platforms.%2520However%252C%250Aexisting%2520benchmarks%2520are%2520overly%2520simplistic%2520and%2520fail%2520to%2520capture%2520real%2520application%250Achallenges%2520such%2520as%2520long-horizon%2520reasoning%2520and%2520large%252C%2520unfamiliar%2520tool%2520spaces.%2520To%250Aaddress%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520MCP-Universe%252C%2520the%2520first%2520comprehensive%250Abenchmark%2520specifically%2520designed%2520to%2520evaluate%2520LLMs%2520in%2520realistic%2520and%2520hard%2520tasks%250Athrough%2520interaction%2520with%2520real-world%2520MCP%2520servers.%2520Our%2520benchmark%2520encompasses%25206%250Acore%2520domains%2520spanning%252011%2520different%2520MCP%2520servers%253A%2520Location%2520Navigation%252C%2520Repository%250AManagement%252C%2520Financial%2520Analysis%252C%25203D%2520Design%252C%2520Browser%2520Automation%252C%2520and%2520Web%250ASearching.%2520To%2520ensure%2520rigorous%2520evaluation%252C%2520we%2520implement%2520execution-based%250Aevaluators%252C%2520including%2520format%2520evaluators%2520for%2520agent%2520format%2520compliance%252C%2520static%250Aevaluators%2520for%2520time-invariant%2520content%2520matching%252C%2520and%2520dynamic%2520evaluators%2520that%250Aautomatically%2520retrieve%2520real-time%2520ground%2520truth%2520for%2520temporally%2520sensitive%2520tasks.%250AThrough%2520extensive%2520evaluation%2520of%2520leading%2520LLMs%252C%2520we%2520find%2520that%2520even%2520SOTA%2520models%250Asuch%2520as%2520GPT-5%2520%252843.72%2525%2529%252C%2520Grok-4%2520%252833.33%2525%2529%2520and%2520Claude-4.0-Sonnet%2520%252829.44%2525%2529%2520exhibit%250Asignificant%2520performance%2520limitations.%2520In%2520addition%252C%2520our%2520benchmark%2520poses%2520a%250Asignificant%2520long-context%2520challenge%2520for%2520LLM%2520agents%252C%2520as%2520the%2520number%2520of%2520input%250Atokens%2520increases%2520rapidly%2520with%2520the%2520number%2520of%2520interaction%2520steps.%2520Moreover%252C%2520it%250Aintroduces%2520an%2520unknown-tools%2520challenge%252C%2520as%2520LLM%2520agents%2520often%2520lack%2520familiarity%250Awith%2520the%2520precise%2520usage%2520of%2520the%2520MCP%2520servers.%2520Notably%252C%2520enterprise-level%2520agents%250Alike%2520Cursor%2520cannot%2520achieve%2520better%2520performance%2520than%2520standard%2520ReAct%2520frameworks.%250ABeyond%2520evaluation%252C%2520we%2520open-source%2520our%2520extensible%2520evaluation%2520framework%2520with%2520UI%250Asupport%252C%2520enabling%2520researchers%2520and%2520practitioners%2520to%2520seamlessly%2520integrate%2520new%250Aagents%2520and%2520MCP%2520servers%2520while%2520fostering%2520innovation%2520in%2520the%2520rapidly%2520evolving%2520MCP%250Aecosystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCP-Universe%3A%20Benchmarking%20Large%20Language%20Models%20with%20Real-World%20Model%0A%20%20Context%20Protocol%20Servers&entry.906535625=Ziyang%20Luo%20and%20Zhiqi%20Shen%20and%20Wenzhuo%20Yang%20and%20Zirui%20Zhao%20and%20Prathyusha%20Jwalapuram%20and%20Amrita%20Saha%20and%20Doyen%20Sahoo%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Junnan%20Li&entry.1292438233=%20%20The%20Model%20Context%20Protocol%20has%20emerged%20as%20a%20transformative%20standard%20for%0Aconnecting%20large%20language%20models%20to%20external%20data%20sources%20and%20tools%2C%20rapidly%0Againing%20adoption%20across%20major%20AI%20providers%20and%20development%20platforms.%20However%2C%0Aexisting%20benchmarks%20are%20overly%20simplistic%20and%20fail%20to%20capture%20real%20application%0Achallenges%20such%20as%20long-horizon%20reasoning%20and%20large%2C%20unfamiliar%20tool%20spaces.%20To%0Aaddress%20this%20critical%20gap%2C%20we%20introduce%20MCP-Universe%2C%20the%20first%20comprehensive%0Abenchmark%20specifically%20designed%20to%20evaluate%20LLMs%20in%20realistic%20and%20hard%20tasks%0Athrough%20interaction%20with%20real-world%20MCP%20servers.%20Our%20benchmark%20encompasses%206%0Acore%20domains%20spanning%2011%20different%20MCP%20servers%3A%20Location%20Navigation%2C%20Repository%0AManagement%2C%20Financial%20Analysis%2C%203D%20Design%2C%20Browser%20Automation%2C%20and%20Web%0ASearching.%20To%20ensure%20rigorous%20evaluation%2C%20we%20implement%20execution-based%0Aevaluators%2C%20including%20format%20evaluators%20for%20agent%20format%20compliance%2C%20static%0Aevaluators%20for%20time-invariant%20content%20matching%2C%20and%20dynamic%20evaluators%20that%0Aautomatically%20retrieve%20real-time%20ground%20truth%20for%20temporally%20sensitive%20tasks.%0AThrough%20extensive%20evaluation%20of%20leading%20LLMs%2C%20we%20find%20that%20even%20SOTA%20models%0Asuch%20as%20GPT-5%20%2843.72%25%29%2C%20Grok-4%20%2833.33%25%29%20and%20Claude-4.0-Sonnet%20%2829.44%25%29%20exhibit%0Asignificant%20performance%20limitations.%20In%20addition%2C%20our%20benchmark%20poses%20a%0Asignificant%20long-context%20challenge%20for%20LLM%20agents%2C%20as%20the%20number%20of%20input%0Atokens%20increases%20rapidly%20with%20the%20number%20of%20interaction%20steps.%20Moreover%2C%20it%0Aintroduces%20an%20unknown-tools%20challenge%2C%20as%20LLM%20agents%20often%20lack%20familiarity%0Awith%20the%20precise%20usage%20of%20the%20MCP%20servers.%20Notably%2C%20enterprise-level%20agents%0Alike%20Cursor%20cannot%20achieve%20better%20performance%20than%20standard%20ReAct%20frameworks.%0ABeyond%20evaluation%2C%20we%20open-source%20our%20extensible%20evaluation%20framework%20with%20UI%0Asupport%2C%20enabling%20researchers%20and%20practitioners%20to%20seamlessly%20integrate%20new%0Aagents%20and%20MCP%20servers%20while%20fostering%20innovation%20in%20the%20rapidly%20evolving%20MCP%0Aecosystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14704v1&entry.124074799=Read"},
{"title": "Improved Mapping Between Illuminations and Sensors for RAW Images", "author": "Abhijith Punnappurath and Luxi Zhao and Hoang Le and Abdelrahman Abdelhamed and SaiKiran Kumar Tedla and Michael S. Brown", "abstract": "  RAW images are unprocessed camera sensor output with sensor-specific RGB\nvalues based on the sensor's color filter spectral sensitivities. RAW images\nalso incur strong color casts due to the sensor's response to the spectral\nproperties of scene illumination. The sensor- and illumination-specific nature\nof RAW images makes it challenging to capture RAW datasets for deep learning\nmethods, as scenes need to be captured for each sensor and under a wide range\nof illumination. Methods for illumination augmentation for a given sensor and\nthe ability to map RAW images between sensors are important for reducing the\nburden of data capture. To explore this problem, we introduce the\nfirst-of-its-kind dataset comprising carefully captured scenes under a wide\nrange of illumination. Specifically, we use a customized lightbox with tunable\nillumination spectra to capture several scenes with different cameras. Our\nillumination and sensor mapping dataset has 390 illuminations, four cameras,\nand 18 scenes. Using this dataset, we introduce a lightweight neural network\napproach for illumination and sensor mapping that outperforms competing\nmethods. We demonstrate the utility of our approach on the downstream task of\ntraining a neural ISP. Link to project page:\nhttps://github.com/SamsungLabs/illum-sensor-mapping.\n", "link": "http://arxiv.org/abs/2508.14730v1", "date": "2025-08-20", "relevancy": 2.1209, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5549}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5256}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Mapping%20Between%20Illuminations%20and%20Sensors%20for%20RAW%20Images&body=Title%3A%20Improved%20Mapping%20Between%20Illuminations%20and%20Sensors%20for%20RAW%20Images%0AAuthor%3A%20Abhijith%20Punnappurath%20and%20Luxi%20Zhao%20and%20Hoang%20Le%20and%20Abdelrahman%20Abdelhamed%20and%20SaiKiran%20Kumar%20Tedla%20and%20Michael%20S.%20Brown%0AAbstract%3A%20%20%20RAW%20images%20are%20unprocessed%20camera%20sensor%20output%20with%20sensor-specific%20RGB%0Avalues%20based%20on%20the%20sensor%27s%20color%20filter%20spectral%20sensitivities.%20RAW%20images%0Aalso%20incur%20strong%20color%20casts%20due%20to%20the%20sensor%27s%20response%20to%20the%20spectral%0Aproperties%20of%20scene%20illumination.%20The%20sensor-%20and%20illumination-specific%20nature%0Aof%20RAW%20images%20makes%20it%20challenging%20to%20capture%20RAW%20datasets%20for%20deep%20learning%0Amethods%2C%20as%20scenes%20need%20to%20be%20captured%20for%20each%20sensor%20and%20under%20a%20wide%20range%0Aof%20illumination.%20Methods%20for%20illumination%20augmentation%20for%20a%20given%20sensor%20and%0Athe%20ability%20to%20map%20RAW%20images%20between%20sensors%20are%20important%20for%20reducing%20the%0Aburden%20of%20data%20capture.%20To%20explore%20this%20problem%2C%20we%20introduce%20the%0Afirst-of-its-kind%20dataset%20comprising%20carefully%20captured%20scenes%20under%20a%20wide%0Arange%20of%20illumination.%20Specifically%2C%20we%20use%20a%20customized%20lightbox%20with%20tunable%0Aillumination%20spectra%20to%20capture%20several%20scenes%20with%20different%20cameras.%20Our%0Aillumination%20and%20sensor%20mapping%20dataset%20has%20390%20illuminations%2C%20four%20cameras%2C%0Aand%2018%20scenes.%20Using%20this%20dataset%2C%20we%20introduce%20a%20lightweight%20neural%20network%0Aapproach%20for%20illumination%20and%20sensor%20mapping%20that%20outperforms%20competing%0Amethods.%20We%20demonstrate%20the%20utility%20of%20our%20approach%20on%20the%20downstream%20task%20of%0Atraining%20a%20neural%20ISP.%20Link%20to%20project%20page%3A%0Ahttps%3A//github.com/SamsungLabs/illum-sensor-mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Mapping%2520Between%2520Illuminations%2520and%2520Sensors%2520for%2520RAW%2520Images%26entry.906535625%3DAbhijith%2520Punnappurath%2520and%2520Luxi%2520Zhao%2520and%2520Hoang%2520Le%2520and%2520Abdelrahman%2520Abdelhamed%2520and%2520SaiKiran%2520Kumar%2520Tedla%2520and%2520Michael%2520S.%2520Brown%26entry.1292438233%3D%2520%2520RAW%2520images%2520are%2520unprocessed%2520camera%2520sensor%2520output%2520with%2520sensor-specific%2520RGB%250Avalues%2520based%2520on%2520the%2520sensor%2527s%2520color%2520filter%2520spectral%2520sensitivities.%2520RAW%2520images%250Aalso%2520incur%2520strong%2520color%2520casts%2520due%2520to%2520the%2520sensor%2527s%2520response%2520to%2520the%2520spectral%250Aproperties%2520of%2520scene%2520illumination.%2520The%2520sensor-%2520and%2520illumination-specific%2520nature%250Aof%2520RAW%2520images%2520makes%2520it%2520challenging%2520to%2520capture%2520RAW%2520datasets%2520for%2520deep%2520learning%250Amethods%252C%2520as%2520scenes%2520need%2520to%2520be%2520captured%2520for%2520each%2520sensor%2520and%2520under%2520a%2520wide%2520range%250Aof%2520illumination.%2520Methods%2520for%2520illumination%2520augmentation%2520for%2520a%2520given%2520sensor%2520and%250Athe%2520ability%2520to%2520map%2520RAW%2520images%2520between%2520sensors%2520are%2520important%2520for%2520reducing%2520the%250Aburden%2520of%2520data%2520capture.%2520To%2520explore%2520this%2520problem%252C%2520we%2520introduce%2520the%250Afirst-of-its-kind%2520dataset%2520comprising%2520carefully%2520captured%2520scenes%2520under%2520a%2520wide%250Arange%2520of%2520illumination.%2520Specifically%252C%2520we%2520use%2520a%2520customized%2520lightbox%2520with%2520tunable%250Aillumination%2520spectra%2520to%2520capture%2520several%2520scenes%2520with%2520different%2520cameras.%2520Our%250Aillumination%2520and%2520sensor%2520mapping%2520dataset%2520has%2520390%2520illuminations%252C%2520four%2520cameras%252C%250Aand%252018%2520scenes.%2520Using%2520this%2520dataset%252C%2520we%2520introduce%2520a%2520lightweight%2520neural%2520network%250Aapproach%2520for%2520illumination%2520and%2520sensor%2520mapping%2520that%2520outperforms%2520competing%250Amethods.%2520We%2520demonstrate%2520the%2520utility%2520of%2520our%2520approach%2520on%2520the%2520downstream%2520task%2520of%250Atraining%2520a%2520neural%2520ISP.%2520Link%2520to%2520project%2520page%253A%250Ahttps%253A//github.com/SamsungLabs/illum-sensor-mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Mapping%20Between%20Illuminations%20and%20Sensors%20for%20RAW%20Images&entry.906535625=Abhijith%20Punnappurath%20and%20Luxi%20Zhao%20and%20Hoang%20Le%20and%20Abdelrahman%20Abdelhamed%20and%20SaiKiran%20Kumar%20Tedla%20and%20Michael%20S.%20Brown&entry.1292438233=%20%20RAW%20images%20are%20unprocessed%20camera%20sensor%20output%20with%20sensor-specific%20RGB%0Avalues%20based%20on%20the%20sensor%27s%20color%20filter%20spectral%20sensitivities.%20RAW%20images%0Aalso%20incur%20strong%20color%20casts%20due%20to%20the%20sensor%27s%20response%20to%20the%20spectral%0Aproperties%20of%20scene%20illumination.%20The%20sensor-%20and%20illumination-specific%20nature%0Aof%20RAW%20images%20makes%20it%20challenging%20to%20capture%20RAW%20datasets%20for%20deep%20learning%0Amethods%2C%20as%20scenes%20need%20to%20be%20captured%20for%20each%20sensor%20and%20under%20a%20wide%20range%0Aof%20illumination.%20Methods%20for%20illumination%20augmentation%20for%20a%20given%20sensor%20and%0Athe%20ability%20to%20map%20RAW%20images%20between%20sensors%20are%20important%20for%20reducing%20the%0Aburden%20of%20data%20capture.%20To%20explore%20this%20problem%2C%20we%20introduce%20the%0Afirst-of-its-kind%20dataset%20comprising%20carefully%20captured%20scenes%20under%20a%20wide%0Arange%20of%20illumination.%20Specifically%2C%20we%20use%20a%20customized%20lightbox%20with%20tunable%0Aillumination%20spectra%20to%20capture%20several%20scenes%20with%20different%20cameras.%20Our%0Aillumination%20and%20sensor%20mapping%20dataset%20has%20390%20illuminations%2C%20four%20cameras%2C%0Aand%2018%20scenes.%20Using%20this%20dataset%2C%20we%20introduce%20a%20lightweight%20neural%20network%0Aapproach%20for%20illumination%20and%20sensor%20mapping%20that%20outperforms%20competing%0Amethods.%20We%20demonstrate%20the%20utility%20of%20our%20approach%20on%20the%20downstream%20task%20of%0Atraining%20a%20neural%20ISP.%20Link%20to%20project%20page%3A%0Ahttps%3A//github.com/SamsungLabs/illum-sensor-mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14730v1&entry.124074799=Read"},
{"title": "Uncertainty Quantification for Language Models: A Suite of Black-Box,\n  White-Box, LLM Judge, and Ensemble Scorers", "author": "Dylan Bouchard and Mohit Singh Chauhan", "abstract": "  Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we outline a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we propose a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.\n", "link": "http://arxiv.org/abs/2504.19254v3", "date": "2025-08-20", "relevancy": 2.1133, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.597}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20for%20Language%20Models%3A%20A%20Suite%20of%20Black-Box%2C%0A%20%20White-Box%2C%20LLM%20Judge%2C%20and%20Ensemble%20Scorers&body=Title%3A%20Uncertainty%20Quantification%20for%20Language%20Models%3A%20A%20Suite%20of%20Black-Box%2C%0A%20%20White-Box%2C%20LLM%20Judge%2C%20and%20Ensemble%20Scorers%0AAuthor%3A%20Dylan%20Bouchard%20and%20Mohit%20Singh%20Chauhan%0AAbstract%3A%20%20%20Hallucinations%20are%20a%20persistent%20problem%20with%20Large%20Language%20Models%20%28LLMs%29.%20As%0Athese%20models%20become%20increasingly%20used%20in%20high-stakes%20domains%2C%20such%20as%0Ahealthcare%20and%20finance%2C%20the%20need%20for%20effective%20hallucination%20detection%20is%0Acrucial.%20To%20this%20end%2C%20we%20outline%20a%20versatile%20framework%20for%20zero-resource%0Ahallucination%20detection%20that%20practitioners%20can%20apply%20to%20real-world%20use%20cases.%0ATo%20achieve%20this%2C%20we%20adapt%20a%20variety%20of%20existing%20uncertainty%20quantification%20%28UQ%29%0Atechniques%2C%20including%20black-box%20UQ%2C%20white-box%20UQ%2C%20and%20LLM-as-a-Judge%2C%0Atransforming%20them%20as%20necessary%20into%20standardized%20response-level%20confidence%0Ascores%20ranging%20from%200%20to%201.%20To%20enhance%20flexibility%2C%20we%20propose%20a%20tunable%0Aensemble%20approach%20that%20incorporates%20any%20combination%20of%20the%20individual%0Aconfidence%20scores.%20This%20approach%20enables%20practitioners%20to%20optimize%20the%20ensemble%0Afor%20a%20specific%20use%20case%20for%20improved%20performance.%20To%20streamline%20implementation%2C%0Athe%20full%20suite%20of%20scorers%20is%20offered%20in%20this%20paper%27s%20companion%20Python%20toolkit%2C%0AUQLM.%20To%20evaluate%20the%20performance%20of%20the%20various%20scorers%2C%20we%20conduct%20an%0Aextensive%20set%20of%20experiments%20using%20several%20LLM%20question-answering%20benchmarks.%0AWe%20find%20that%20our%20tunable%20ensemble%20typically%20surpasses%20its%20individual%20components%0Aand%20outperforms%20existing%20hallucination%20detection%20methods.%20Our%20results%0Ademonstrate%20the%20benefits%20of%20customized%20hallucination%20detection%20strategies%20for%0Aimproving%20the%20accuracy%20and%20reliability%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19254v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520for%2520Language%2520Models%253A%2520A%2520Suite%2520of%2520Black-Box%252C%250A%2520%2520White-Box%252C%2520LLM%2520Judge%252C%2520and%2520Ensemble%2520Scorers%26entry.906535625%3DDylan%2520Bouchard%2520and%2520Mohit%2520Singh%2520Chauhan%26entry.1292438233%3D%2520%2520Hallucinations%2520are%2520a%2520persistent%2520problem%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520As%250Athese%2520models%2520become%2520increasingly%2520used%2520in%2520high-stakes%2520domains%252C%2520such%2520as%250Ahealthcare%2520and%2520finance%252C%2520the%2520need%2520for%2520effective%2520hallucination%2520detection%2520is%250Acrucial.%2520To%2520this%2520end%252C%2520we%2520outline%2520a%2520versatile%2520framework%2520for%2520zero-resource%250Ahallucination%2520detection%2520that%2520practitioners%2520can%2520apply%2520to%2520real-world%2520use%2520cases.%250ATo%2520achieve%2520this%252C%2520we%2520adapt%2520a%2520variety%2520of%2520existing%2520uncertainty%2520quantification%2520%2528UQ%2529%250Atechniques%252C%2520including%2520black-box%2520UQ%252C%2520white-box%2520UQ%252C%2520and%2520LLM-as-a-Judge%252C%250Atransforming%2520them%2520as%2520necessary%2520into%2520standardized%2520response-level%2520confidence%250Ascores%2520ranging%2520from%25200%2520to%25201.%2520To%2520enhance%2520flexibility%252C%2520we%2520propose%2520a%2520tunable%250Aensemble%2520approach%2520that%2520incorporates%2520any%2520combination%2520of%2520the%2520individual%250Aconfidence%2520scores.%2520This%2520approach%2520enables%2520practitioners%2520to%2520optimize%2520the%2520ensemble%250Afor%2520a%2520specific%2520use%2520case%2520for%2520improved%2520performance.%2520To%2520streamline%2520implementation%252C%250Athe%2520full%2520suite%2520of%2520scorers%2520is%2520offered%2520in%2520this%2520paper%2527s%2520companion%2520Python%2520toolkit%252C%250AUQLM.%2520To%2520evaluate%2520the%2520performance%2520of%2520the%2520various%2520scorers%252C%2520we%2520conduct%2520an%250Aextensive%2520set%2520of%2520experiments%2520using%2520several%2520LLM%2520question-answering%2520benchmarks.%250AWe%2520find%2520that%2520our%2520tunable%2520ensemble%2520typically%2520surpasses%2520its%2520individual%2520components%250Aand%2520outperforms%2520existing%2520hallucination%2520detection%2520methods.%2520Our%2520results%250Ademonstrate%2520the%2520benefits%2520of%2520customized%2520hallucination%2520detection%2520strategies%2520for%250Aimproving%2520the%2520accuracy%2520and%2520reliability%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19254v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20for%20Language%20Models%3A%20A%20Suite%20of%20Black-Box%2C%0A%20%20White-Box%2C%20LLM%20Judge%2C%20and%20Ensemble%20Scorers&entry.906535625=Dylan%20Bouchard%20and%20Mohit%20Singh%20Chauhan&entry.1292438233=%20%20Hallucinations%20are%20a%20persistent%20problem%20with%20Large%20Language%20Models%20%28LLMs%29.%20As%0Athese%20models%20become%20increasingly%20used%20in%20high-stakes%20domains%2C%20such%20as%0Ahealthcare%20and%20finance%2C%20the%20need%20for%20effective%20hallucination%20detection%20is%0Acrucial.%20To%20this%20end%2C%20we%20outline%20a%20versatile%20framework%20for%20zero-resource%0Ahallucination%20detection%20that%20practitioners%20can%20apply%20to%20real-world%20use%20cases.%0ATo%20achieve%20this%2C%20we%20adapt%20a%20variety%20of%20existing%20uncertainty%20quantification%20%28UQ%29%0Atechniques%2C%20including%20black-box%20UQ%2C%20white-box%20UQ%2C%20and%20LLM-as-a-Judge%2C%0Atransforming%20them%20as%20necessary%20into%20standardized%20response-level%20confidence%0Ascores%20ranging%20from%200%20to%201.%20To%20enhance%20flexibility%2C%20we%20propose%20a%20tunable%0Aensemble%20approach%20that%20incorporates%20any%20combination%20of%20the%20individual%0Aconfidence%20scores.%20This%20approach%20enables%20practitioners%20to%20optimize%20the%20ensemble%0Afor%20a%20specific%20use%20case%20for%20improved%20performance.%20To%20streamline%20implementation%2C%0Athe%20full%20suite%20of%20scorers%20is%20offered%20in%20this%20paper%27s%20companion%20Python%20toolkit%2C%0AUQLM.%20To%20evaluate%20the%20performance%20of%20the%20various%20scorers%2C%20we%20conduct%20an%0Aextensive%20set%20of%20experiments%20using%20several%20LLM%20question-answering%20benchmarks.%0AWe%20find%20that%20our%20tunable%20ensemble%20typically%20surpasses%20its%20individual%20components%0Aand%20outperforms%20existing%20hallucination%20detection%20methods.%20Our%20results%0Ademonstrate%20the%20benefits%20of%20customized%20hallucination%20detection%20strategies%20for%0Aimproving%20the%20accuracy%20and%20reliability%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19254v3&entry.124074799=Read"},
{"title": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models", "author": "Anindya Bijoy Das and Shibbir Ahmed and Shahnewaz Karim Sakib", "abstract": "  Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization.\n", "link": "http://arxiv.org/abs/2504.19061v3", "date": "2025-08-20", "relevancy": 2.1121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallucinations%20and%20Key%20Information%20Extraction%20in%20Medical%20Texts%3A%20A%0A%20%20Comprehensive%20Assessment%20of%20Open-Source%20Large%20Language%20Models&body=Title%3A%20Hallucinations%20and%20Key%20Information%20Extraction%20in%20Medical%20Texts%3A%20A%0A%20%20Comprehensive%20Assessment%20of%20Open-Source%20Large%20Language%20Models%0AAuthor%3A%20Anindya%20Bijoy%20Das%20and%20Shibbir%20Ahmed%20and%20Shahnewaz%20Karim%20Sakib%0AAbstract%3A%20%20%20Clinical%20summarization%20is%20crucial%20in%20healthcare%20as%20it%20distills%20complex%0Amedical%20data%20into%20digestible%20information%2C%20enhancing%20patient%20understanding%20and%0Acare%20management.%20Large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20potential%0Ain%20automating%20and%20improving%20the%20accuracy%20of%20such%20summarizations%20due%20to%20their%0Aadvanced%20natural%20language%20understanding%20capabilities.%20These%20models%20are%0Aparticularly%20applicable%20in%20the%20context%20of%20summarizing%20medical/clinical%20texts%2C%0Awhere%20precise%20and%20concise%20information%20transfer%20is%20essential.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20effectiveness%20of%20open-source%20LLMs%20in%20extracting%20key%20events%20from%0Adischarge%20reports%2C%20including%20admission%20reasons%2C%20major%20in-hospital%20events%2C%20and%0Acritical%20follow-up%20actions.%20In%20addition%2C%20we%20also%20assess%20the%20prevalence%20of%0Avarious%20types%20of%20hallucinations%20in%20the%20summaries%20produced%20by%20these%20models.%0ADetecting%20hallucinations%20is%20vital%20as%20it%20directly%20influences%20the%20reliability%20of%0Athe%20information%2C%20potentially%20affecting%20patient%20care%20and%20treatment%20outcomes.%20We%0Aconduct%20comprehensive%20simulations%20to%20rigorously%20evaluate%20the%20performance%20of%0Athese%20models%2C%20further%20probing%20the%20accuracy%20and%20fidelity%20of%20the%20extracted%0Acontent%20in%20clinical%20summarization.%20Our%20results%20reveal%20that%20while%20the%20LLMs%0A%28e.g.%2C%20Qwen2.5%20and%20DeepSeek-v2%29%20perform%20quite%20well%20in%20capturing%20admission%0Areasons%20and%20hospitalization%20events%2C%20they%20are%20generally%20less%20consistent%20when%20it%0Acomes%20to%20identifying%20follow-up%20recommendations%2C%20highlighting%20broader%20challenges%0Ain%20leveraging%20LLMs%20for%20comprehensive%20summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19061v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallucinations%2520and%2520Key%2520Information%2520Extraction%2520in%2520Medical%2520Texts%253A%2520A%250A%2520%2520Comprehensive%2520Assessment%2520of%2520Open-Source%2520Large%2520Language%2520Models%26entry.906535625%3DAnindya%2520Bijoy%2520Das%2520and%2520Shibbir%2520Ahmed%2520and%2520Shahnewaz%2520Karim%2520Sakib%26entry.1292438233%3D%2520%2520Clinical%2520summarization%2520is%2520crucial%2520in%2520healthcare%2520as%2520it%2520distills%2520complex%250Amedical%2520data%2520into%2520digestible%2520information%252C%2520enhancing%2520patient%2520understanding%2520and%250Acare%2520management.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520significant%2520potential%250Ain%2520automating%2520and%2520improving%2520the%2520accuracy%2520of%2520such%2520summarizations%2520due%2520to%2520their%250Aadvanced%2520natural%2520language%2520understanding%2520capabilities.%2520These%2520models%2520are%250Aparticularly%2520applicable%2520in%2520the%2520context%2520of%2520summarizing%2520medical/clinical%2520texts%252C%250Awhere%2520precise%2520and%2520concise%2520information%2520transfer%2520is%2520essential.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520the%2520effectiveness%2520of%2520open-source%2520LLMs%2520in%2520extracting%2520key%2520events%2520from%250Adischarge%2520reports%252C%2520including%2520admission%2520reasons%252C%2520major%2520in-hospital%2520events%252C%2520and%250Acritical%2520follow-up%2520actions.%2520In%2520addition%252C%2520we%2520also%2520assess%2520the%2520prevalence%2520of%250Avarious%2520types%2520of%2520hallucinations%2520in%2520the%2520summaries%2520produced%2520by%2520these%2520models.%250ADetecting%2520hallucinations%2520is%2520vital%2520as%2520it%2520directly%2520influences%2520the%2520reliability%2520of%250Athe%2520information%252C%2520potentially%2520affecting%2520patient%2520care%2520and%2520treatment%2520outcomes.%2520We%250Aconduct%2520comprehensive%2520simulations%2520to%2520rigorously%2520evaluate%2520the%2520performance%2520of%250Athese%2520models%252C%2520further%2520probing%2520the%2520accuracy%2520and%2520fidelity%2520of%2520the%2520extracted%250Acontent%2520in%2520clinical%2520summarization.%2520Our%2520results%2520reveal%2520that%2520while%2520the%2520LLMs%250A%2528e.g.%252C%2520Qwen2.5%2520and%2520DeepSeek-v2%2529%2520perform%2520quite%2520well%2520in%2520capturing%2520admission%250Areasons%2520and%2520hospitalization%2520events%252C%2520they%2520are%2520generally%2520less%2520consistent%2520when%2520it%250Acomes%2520to%2520identifying%2520follow-up%2520recommendations%252C%2520highlighting%2520broader%2520challenges%250Ain%2520leveraging%2520LLMs%2520for%2520comprehensive%2520summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19061v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallucinations%20and%20Key%20Information%20Extraction%20in%20Medical%20Texts%3A%20A%0A%20%20Comprehensive%20Assessment%20of%20Open-Source%20Large%20Language%20Models&entry.906535625=Anindya%20Bijoy%20Das%20and%20Shibbir%20Ahmed%20and%20Shahnewaz%20Karim%20Sakib&entry.1292438233=%20%20Clinical%20summarization%20is%20crucial%20in%20healthcare%20as%20it%20distills%20complex%0Amedical%20data%20into%20digestible%20information%2C%20enhancing%20patient%20understanding%20and%0Acare%20management.%20Large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20potential%0Ain%20automating%20and%20improving%20the%20accuracy%20of%20such%20summarizations%20due%20to%20their%0Aadvanced%20natural%20language%20understanding%20capabilities.%20These%20models%20are%0Aparticularly%20applicable%20in%20the%20context%20of%20summarizing%20medical/clinical%20texts%2C%0Awhere%20precise%20and%20concise%20information%20transfer%20is%20essential.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20effectiveness%20of%20open-source%20LLMs%20in%20extracting%20key%20events%20from%0Adischarge%20reports%2C%20including%20admission%20reasons%2C%20major%20in-hospital%20events%2C%20and%0Acritical%20follow-up%20actions.%20In%20addition%2C%20we%20also%20assess%20the%20prevalence%20of%0Avarious%20types%20of%20hallucinations%20in%20the%20summaries%20produced%20by%20these%20models.%0ADetecting%20hallucinations%20is%20vital%20as%20it%20directly%20influences%20the%20reliability%20of%0Athe%20information%2C%20potentially%20affecting%20patient%20care%20and%20treatment%20outcomes.%20We%0Aconduct%20comprehensive%20simulations%20to%20rigorously%20evaluate%20the%20performance%20of%0Athese%20models%2C%20further%20probing%20the%20accuracy%20and%20fidelity%20of%20the%20extracted%0Acontent%20in%20clinical%20summarization.%20Our%20results%20reveal%20that%20while%20the%20LLMs%0A%28e.g.%2C%20Qwen2.5%20and%20DeepSeek-v2%29%20perform%20quite%20well%20in%20capturing%20admission%0Areasons%20and%20hospitalization%20events%2C%20they%20are%20generally%20less%20consistent%20when%20it%0Acomes%20to%20identifying%20follow-up%20recommendations%2C%20highlighting%20broader%20challenges%0Ain%20leveraging%20LLMs%20for%20comprehensive%20summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19061v3&entry.124074799=Read"},
{"title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "author": "Guangzhan Wang and Hongyu Zhang and Beijun Shen and Xiaodong Gu", "abstract": "  Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.\n", "link": "http://arxiv.org/abs/2508.14723v1", "date": "2025-08-20", "relevancy": 2.1118, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5656}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5326}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transplant%20Then%20Regenerate%3A%20A%20New%20Paradigm%20for%20Text%20Data%20Augmentation&body=Title%3A%20Transplant%20Then%20Regenerate%3A%20A%20New%20Paradigm%20for%20Text%20Data%20Augmentation%0AAuthor%3A%20Guangzhan%20Wang%20and%20Hongyu%20Zhang%20and%20Beijun%20Shen%20and%20Xiaodong%20Gu%0AAbstract%3A%20%20%20Data%20augmentation%20is%20a%20critical%20technique%20in%20deep%20learning.%20Traditional%0Amethods%20like%20Back-translation%20typically%20focus%20on%20lexical-level%20rephrasing%2C%0Awhich%20primarily%20produces%20variations%20with%20the%20same%20semantics.%20While%20large%0Alanguage%20models%20%28LLMs%29%20have%20enhanced%20text%20augmentation%20by%20their%20%22knowledge%0Aemergence%22%20capability%2C%20controlling%20the%20style%20and%20structure%20of%20these%20outputs%0Aremains%20challenging%20and%20requires%20meticulous%20prompt%20engineering.%20In%20this%20paper%2C%0Awe%20propose%20LMTransplant%2C%20a%20novel%20text%20augmentation%20paradigm%20leveraging%20LLMs.%0AThe%20core%20idea%20of%20LMTransplant%20is%20transplant-then-regenerate%3A%20incorporating%20seed%0Atext%20into%20a%20context%20expanded%20by%20LLM%2C%20and%20asking%20the%20LLM%20to%20regenerate%20a%20variant%0Abased%20on%20the%20expanded%20context.%20This%20strategy%20allows%20the%20model%20to%20create%20more%0Adiverse%20and%20creative%20content-level%20variants%20by%20fully%20leveraging%20the%20knowledge%0Aembedded%20in%20LLMs%2C%20while%20preserving%20the%20core%20attributes%20of%20the%20original%20text.%20We%0Aevaluate%20LMTransplant%20across%20various%20text-related%20tasks%2C%20demonstrating%20its%0Asuperior%20performance%20over%20existing%20text%20augmentation%20methods.%20Moreover%2C%0ALMTransplant%20demonstrates%20exceptional%20scalability%20as%20the%20size%20of%20augmented%20data%0Agrows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransplant%2520Then%2520Regenerate%253A%2520A%2520New%2520Paradigm%2520for%2520Text%2520Data%2520Augmentation%26entry.906535625%3DGuangzhan%2520Wang%2520and%2520Hongyu%2520Zhang%2520and%2520Beijun%2520Shen%2520and%2520Xiaodong%2520Gu%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520a%2520critical%2520technique%2520in%2520deep%2520learning.%2520Traditional%250Amethods%2520like%2520Back-translation%2520typically%2520focus%2520on%2520lexical-level%2520rephrasing%252C%250Awhich%2520primarily%2520produces%2520variations%2520with%2520the%2520same%2520semantics.%2520While%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520enhanced%2520text%2520augmentation%2520by%2520their%2520%2522knowledge%250Aemergence%2522%2520capability%252C%2520controlling%2520the%2520style%2520and%2520structure%2520of%2520these%2520outputs%250Aremains%2520challenging%2520and%2520requires%2520meticulous%2520prompt%2520engineering.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520LMTransplant%252C%2520a%2520novel%2520text%2520augmentation%2520paradigm%2520leveraging%2520LLMs.%250AThe%2520core%2520idea%2520of%2520LMTransplant%2520is%2520transplant-then-regenerate%253A%2520incorporating%2520seed%250Atext%2520into%2520a%2520context%2520expanded%2520by%2520LLM%252C%2520and%2520asking%2520the%2520LLM%2520to%2520regenerate%2520a%2520variant%250Abased%2520on%2520the%2520expanded%2520context.%2520This%2520strategy%2520allows%2520the%2520model%2520to%2520create%2520more%250Adiverse%2520and%2520creative%2520content-level%2520variants%2520by%2520fully%2520leveraging%2520the%2520knowledge%250Aembedded%2520in%2520LLMs%252C%2520while%2520preserving%2520the%2520core%2520attributes%2520of%2520the%2520original%2520text.%2520We%250Aevaluate%2520LMTransplant%2520across%2520various%2520text-related%2520tasks%252C%2520demonstrating%2520its%250Asuperior%2520performance%2520over%2520existing%2520text%2520augmentation%2520methods.%2520Moreover%252C%250ALMTransplant%2520demonstrates%2520exceptional%2520scalability%2520as%2520the%2520size%2520of%2520augmented%2520data%250Agrows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transplant%20Then%20Regenerate%3A%20A%20New%20Paradigm%20for%20Text%20Data%20Augmentation&entry.906535625=Guangzhan%20Wang%20and%20Hongyu%20Zhang%20and%20Beijun%20Shen%20and%20Xiaodong%20Gu&entry.1292438233=%20%20Data%20augmentation%20is%20a%20critical%20technique%20in%20deep%20learning.%20Traditional%0Amethods%20like%20Back-translation%20typically%20focus%20on%20lexical-level%20rephrasing%2C%0Awhich%20primarily%20produces%20variations%20with%20the%20same%20semantics.%20While%20large%0Alanguage%20models%20%28LLMs%29%20have%20enhanced%20text%20augmentation%20by%20their%20%22knowledge%0Aemergence%22%20capability%2C%20controlling%20the%20style%20and%20structure%20of%20these%20outputs%0Aremains%20challenging%20and%20requires%20meticulous%20prompt%20engineering.%20In%20this%20paper%2C%0Awe%20propose%20LMTransplant%2C%20a%20novel%20text%20augmentation%20paradigm%20leveraging%20LLMs.%0AThe%20core%20idea%20of%20LMTransplant%20is%20transplant-then-regenerate%3A%20incorporating%20seed%0Atext%20into%20a%20context%20expanded%20by%20LLM%2C%20and%20asking%20the%20LLM%20to%20regenerate%20a%20variant%0Abased%20on%20the%20expanded%20context.%20This%20strategy%20allows%20the%20model%20to%20create%20more%0Adiverse%20and%20creative%20content-level%20variants%20by%20fully%20leveraging%20the%20knowledge%0Aembedded%20in%20LLMs%2C%20while%20preserving%20the%20core%20attributes%20of%20the%20original%20text.%20We%0Aevaluate%20LMTransplant%20across%20various%20text-related%20tasks%2C%20demonstrating%20its%0Asuperior%20performance%20over%20existing%20text%20augmentation%20methods.%20Moreover%2C%0ALMTransplant%20demonstrates%20exceptional%20scalability%20as%20the%20size%20of%20augmented%20data%0Agrows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14723v1&entry.124074799=Read"},
{"title": "TRUST-Planner: Topology-guided Robust Trajectory Planner for AAVs with\n  Uncertain Obstacle Spatial-temporal Avoidance", "author": "Junzhi Li and Teng Long and Jingliang Sun and Jianxin Zhong", "abstract": "  Despite extensive developments in motion planning of autonomous aerial\nvehicles (AAVs), existing frameworks faces the challenges of local minima and\ndeadlock in complex dynamic environments, leading to increased collision risks.\nTo address these challenges, we present TRUST-Planner, a topology-guided\nhierarchical planning framework for robust spatial-temporal obstacle avoidance.\nIn the frontend, a dynamic enhanced visible probabilistic roadmap (DEV-PRM) is\nproposed to rapidly explore topological paths for global guidance. The backend\nutilizes a uniform terminal-free minimum control polynomial (UTF-MINCO) and\ndynamic distance field (DDF) to enable efficient predictive obstacle avoidance\nand fast parallel computation. Furthermore, an incremental multi-branch\ntrajectory management framework is introduced to enable spatio-temporal\ntopological decision-making, while efficiently leveraging historical\ninformation to reduce replanning time. Simulation results show that\nTRUST-Planner outperforms baseline competitors, achieving a 96\\% success rate\nand millisecond-level computation efficiency in tested complex environments.\nReal-world experiments further validate the feasibility and practicality of the\nproposed method.\n", "link": "http://arxiv.org/abs/2508.14610v1", "date": "2025-08-20", "relevancy": 2.1118, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5655}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRUST-Planner%3A%20Topology-guided%20Robust%20Trajectory%20Planner%20for%20AAVs%20with%0A%20%20Uncertain%20Obstacle%20Spatial-temporal%20Avoidance&body=Title%3A%20TRUST-Planner%3A%20Topology-guided%20Robust%20Trajectory%20Planner%20for%20AAVs%20with%0A%20%20Uncertain%20Obstacle%20Spatial-temporal%20Avoidance%0AAuthor%3A%20Junzhi%20Li%20and%20Teng%20Long%20and%20Jingliang%20Sun%20and%20Jianxin%20Zhong%0AAbstract%3A%20%20%20Despite%20extensive%20developments%20in%20motion%20planning%20of%20autonomous%20aerial%0Avehicles%20%28AAVs%29%2C%20existing%20frameworks%20faces%20the%20challenges%20of%20local%20minima%20and%0Adeadlock%20in%20complex%20dynamic%20environments%2C%20leading%20to%20increased%20collision%20risks.%0ATo%20address%20these%20challenges%2C%20we%20present%20TRUST-Planner%2C%20a%20topology-guided%0Ahierarchical%20planning%20framework%20for%20robust%20spatial-temporal%20obstacle%20avoidance.%0AIn%20the%20frontend%2C%20a%20dynamic%20enhanced%20visible%20probabilistic%20roadmap%20%28DEV-PRM%29%20is%0Aproposed%20to%20rapidly%20explore%20topological%20paths%20for%20global%20guidance.%20The%20backend%0Autilizes%20a%20uniform%20terminal-free%20minimum%20control%20polynomial%20%28UTF-MINCO%29%20and%0Adynamic%20distance%20field%20%28DDF%29%20to%20enable%20efficient%20predictive%20obstacle%20avoidance%0Aand%20fast%20parallel%20computation.%20Furthermore%2C%20an%20incremental%20multi-branch%0Atrajectory%20management%20framework%20is%20introduced%20to%20enable%20spatio-temporal%0Atopological%20decision-making%2C%20while%20efficiently%20leveraging%20historical%0Ainformation%20to%20reduce%20replanning%20time.%20Simulation%20results%20show%20that%0ATRUST-Planner%20outperforms%20baseline%20competitors%2C%20achieving%20a%2096%5C%25%20success%20rate%0Aand%20millisecond-level%20computation%20efficiency%20in%20tested%20complex%20environments.%0AReal-world%20experiments%20further%20validate%20the%20feasibility%20and%20practicality%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRUST-Planner%253A%2520Topology-guided%2520Robust%2520Trajectory%2520Planner%2520for%2520AAVs%2520with%250A%2520%2520Uncertain%2520Obstacle%2520Spatial-temporal%2520Avoidance%26entry.906535625%3DJunzhi%2520Li%2520and%2520Teng%2520Long%2520and%2520Jingliang%2520Sun%2520and%2520Jianxin%2520Zhong%26entry.1292438233%3D%2520%2520Despite%2520extensive%2520developments%2520in%2520motion%2520planning%2520of%2520autonomous%2520aerial%250Avehicles%2520%2528AAVs%2529%252C%2520existing%2520frameworks%2520faces%2520the%2520challenges%2520of%2520local%2520minima%2520and%250Adeadlock%2520in%2520complex%2520dynamic%2520environments%252C%2520leading%2520to%2520increased%2520collision%2520risks.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520present%2520TRUST-Planner%252C%2520a%2520topology-guided%250Ahierarchical%2520planning%2520framework%2520for%2520robust%2520spatial-temporal%2520obstacle%2520avoidance.%250AIn%2520the%2520frontend%252C%2520a%2520dynamic%2520enhanced%2520visible%2520probabilistic%2520roadmap%2520%2528DEV-PRM%2529%2520is%250Aproposed%2520to%2520rapidly%2520explore%2520topological%2520paths%2520for%2520global%2520guidance.%2520The%2520backend%250Autilizes%2520a%2520uniform%2520terminal-free%2520minimum%2520control%2520polynomial%2520%2528UTF-MINCO%2529%2520and%250Adynamic%2520distance%2520field%2520%2528DDF%2529%2520to%2520enable%2520efficient%2520predictive%2520obstacle%2520avoidance%250Aand%2520fast%2520parallel%2520computation.%2520Furthermore%252C%2520an%2520incremental%2520multi-branch%250Atrajectory%2520management%2520framework%2520is%2520introduced%2520to%2520enable%2520spatio-temporal%250Atopological%2520decision-making%252C%2520while%2520efficiently%2520leveraging%2520historical%250Ainformation%2520to%2520reduce%2520replanning%2520time.%2520Simulation%2520results%2520show%2520that%250ATRUST-Planner%2520outperforms%2520baseline%2520competitors%252C%2520achieving%2520a%252096%255C%2525%2520success%2520rate%250Aand%2520millisecond-level%2520computation%2520efficiency%2520in%2520tested%2520complex%2520environments.%250AReal-world%2520experiments%2520further%2520validate%2520the%2520feasibility%2520and%2520practicality%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRUST-Planner%3A%20Topology-guided%20Robust%20Trajectory%20Planner%20for%20AAVs%20with%0A%20%20Uncertain%20Obstacle%20Spatial-temporal%20Avoidance&entry.906535625=Junzhi%20Li%20and%20Teng%20Long%20and%20Jingliang%20Sun%20and%20Jianxin%20Zhong&entry.1292438233=%20%20Despite%20extensive%20developments%20in%20motion%20planning%20of%20autonomous%20aerial%0Avehicles%20%28AAVs%29%2C%20existing%20frameworks%20faces%20the%20challenges%20of%20local%20minima%20and%0Adeadlock%20in%20complex%20dynamic%20environments%2C%20leading%20to%20increased%20collision%20risks.%0ATo%20address%20these%20challenges%2C%20we%20present%20TRUST-Planner%2C%20a%20topology-guided%0Ahierarchical%20planning%20framework%20for%20robust%20spatial-temporal%20obstacle%20avoidance.%0AIn%20the%20frontend%2C%20a%20dynamic%20enhanced%20visible%20probabilistic%20roadmap%20%28DEV-PRM%29%20is%0Aproposed%20to%20rapidly%20explore%20topological%20paths%20for%20global%20guidance.%20The%20backend%0Autilizes%20a%20uniform%20terminal-free%20minimum%20control%20polynomial%20%28UTF-MINCO%29%20and%0Adynamic%20distance%20field%20%28DDF%29%20to%20enable%20efficient%20predictive%20obstacle%20avoidance%0Aand%20fast%20parallel%20computation.%20Furthermore%2C%20an%20incremental%20multi-branch%0Atrajectory%20management%20framework%20is%20introduced%20to%20enable%20spatio-temporal%0Atopological%20decision-making%2C%20while%20efficiently%20leveraging%20historical%0Ainformation%20to%20reduce%20replanning%20time.%20Simulation%20results%20show%20that%0ATRUST-Planner%20outperforms%20baseline%20competitors%2C%20achieving%20a%2096%5C%25%20success%20rate%0Aand%20millisecond-level%20computation%20efficiency%20in%20tested%20complex%20environments.%0AReal-world%20experiments%20further%20validate%20the%20feasibility%20and%20practicality%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14610v1&entry.124074799=Read"},
{"title": "DINOv3 with Test-Time Training for Medical Image Registration", "author": "Shansong Wang and Mojtaba Safari and Mingzhe Hu and Qiang Li and Chih-Wei Chang and Richard LJ Qiu and Xiaofeng Yang", "abstract": "  Prior medical image registration approaches, particularly learning-based\nmethods, often require large amounts of training data, which constrains\nclinical adoption. To overcome this limitation, we propose a training-free\npipeline that relies on a frozen DINOv3 encoder and test-time optimization of\nthe deformation field in feature space. Across two representative benchmarks,\nthe method is accurate and yields regular deformations. On Abdomen MR-CT, it\nattained the best mean Dice score (DSC) of 0.790 together with the lowest 95th\npercentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard\ndeviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it\nimproves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked\ngain over the initial alignment. The results indicate that operating in a\ncompact foundation feature space at test time offers a practical and general\nsolution for clinical registration without additional training.\n", "link": "http://arxiv.org/abs/2508.14809v1", "date": "2025-08-20", "relevancy": 2.1065, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5457}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5374}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINOv3%20with%20Test-Time%20Training%20for%20Medical%20Image%20Registration&body=Title%3A%20DINOv3%20with%20Test-Time%20Training%20for%20Medical%20Image%20Registration%0AAuthor%3A%20Shansong%20Wang%20and%20Mojtaba%20Safari%20and%20Mingzhe%20Hu%20and%20Qiang%20Li%20and%20Chih-Wei%20Chang%20and%20Richard%20LJ%20Qiu%20and%20Xiaofeng%20Yang%0AAbstract%3A%20%20%20Prior%20medical%20image%20registration%20approaches%2C%20particularly%20learning-based%0Amethods%2C%20often%20require%20large%20amounts%20of%20training%20data%2C%20which%20constrains%0Aclinical%20adoption.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20training-free%0Apipeline%20that%20relies%20on%20a%20frozen%20DINOv3%20encoder%20and%20test-time%20optimization%20of%0Athe%20deformation%20field%20in%20feature%20space.%20Across%20two%20representative%20benchmarks%2C%0Athe%20method%20is%20accurate%20and%20yields%20regular%20deformations.%20On%20Abdomen%20MR-CT%2C%20it%0Aattained%20the%20best%20mean%20Dice%20score%20%28DSC%29%20of%200.790%20together%20with%20the%20lowest%2095th%0Apercentile%20Hausdorff%20Distance%20%28HD95%29%20of%204.9%2B-5.0%20and%20the%20lowest%20standard%0Adeviation%20of%20Log-Jacobian%20%28SDLogJ%29%20of%200.08%2B-0.02.%20On%20ACDC%20cardiac%20MRI%2C%20it%0Aimproves%20mean%20DSC%20to%200.769%20and%20reduces%20SDLogJ%20to%200.11%20and%20HD95%20to%204.8%2C%20a%20marked%0Again%20over%20the%20initial%20alignment.%20The%20results%20indicate%20that%20operating%20in%20a%0Acompact%20foundation%20feature%20space%20at%20test%20time%20offers%20a%20practical%20and%20general%0Asolution%20for%20clinical%20registration%20without%20additional%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINOv3%2520with%2520Test-Time%2520Training%2520for%2520Medical%2520Image%2520Registration%26entry.906535625%3DShansong%2520Wang%2520and%2520Mojtaba%2520Safari%2520and%2520Mingzhe%2520Hu%2520and%2520Qiang%2520Li%2520and%2520Chih-Wei%2520Chang%2520and%2520Richard%2520LJ%2520Qiu%2520and%2520Xiaofeng%2520Yang%26entry.1292438233%3D%2520%2520Prior%2520medical%2520image%2520registration%2520approaches%252C%2520particularly%2520learning-based%250Amethods%252C%2520often%2520require%2520large%2520amounts%2520of%2520training%2520data%252C%2520which%2520constrains%250Aclinical%2520adoption.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520training-free%250Apipeline%2520that%2520relies%2520on%2520a%2520frozen%2520DINOv3%2520encoder%2520and%2520test-time%2520optimization%2520of%250Athe%2520deformation%2520field%2520in%2520feature%2520space.%2520Across%2520two%2520representative%2520benchmarks%252C%250Athe%2520method%2520is%2520accurate%2520and%2520yields%2520regular%2520deformations.%2520On%2520Abdomen%2520MR-CT%252C%2520it%250Aattained%2520the%2520best%2520mean%2520Dice%2520score%2520%2528DSC%2529%2520of%25200.790%2520together%2520with%2520the%2520lowest%252095th%250Apercentile%2520Hausdorff%2520Distance%2520%2528HD95%2529%2520of%25204.9%252B-5.0%2520and%2520the%2520lowest%2520standard%250Adeviation%2520of%2520Log-Jacobian%2520%2528SDLogJ%2529%2520of%25200.08%252B-0.02.%2520On%2520ACDC%2520cardiac%2520MRI%252C%2520it%250Aimproves%2520mean%2520DSC%2520to%25200.769%2520and%2520reduces%2520SDLogJ%2520to%25200.11%2520and%2520HD95%2520to%25204.8%252C%2520a%2520marked%250Again%2520over%2520the%2520initial%2520alignment.%2520The%2520results%2520indicate%2520that%2520operating%2520in%2520a%250Acompact%2520foundation%2520feature%2520space%2520at%2520test%2520time%2520offers%2520a%2520practical%2520and%2520general%250Asolution%2520for%2520clinical%2520registration%2520without%2520additional%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINOv3%20with%20Test-Time%20Training%20for%20Medical%20Image%20Registration&entry.906535625=Shansong%20Wang%20and%20Mojtaba%20Safari%20and%20Mingzhe%20Hu%20and%20Qiang%20Li%20and%20Chih-Wei%20Chang%20and%20Richard%20LJ%20Qiu%20and%20Xiaofeng%20Yang&entry.1292438233=%20%20Prior%20medical%20image%20registration%20approaches%2C%20particularly%20learning-based%0Amethods%2C%20often%20require%20large%20amounts%20of%20training%20data%2C%20which%20constrains%0Aclinical%20adoption.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20training-free%0Apipeline%20that%20relies%20on%20a%20frozen%20DINOv3%20encoder%20and%20test-time%20optimization%20of%0Athe%20deformation%20field%20in%20feature%20space.%20Across%20two%20representative%20benchmarks%2C%0Athe%20method%20is%20accurate%20and%20yields%20regular%20deformations.%20On%20Abdomen%20MR-CT%2C%20it%0Aattained%20the%20best%20mean%20Dice%20score%20%28DSC%29%20of%200.790%20together%20with%20the%20lowest%2095th%0Apercentile%20Hausdorff%20Distance%20%28HD95%29%20of%204.9%2B-5.0%20and%20the%20lowest%20standard%0Adeviation%20of%20Log-Jacobian%20%28SDLogJ%29%20of%200.08%2B-0.02.%20On%20ACDC%20cardiac%20MRI%2C%20it%0Aimproves%20mean%20DSC%20to%200.769%20and%20reduces%20SDLogJ%20to%200.11%20and%20HD95%20to%204.8%2C%20a%20marked%0Again%20over%20the%20initial%20alignment.%20The%20results%20indicate%20that%20operating%20in%20a%0Acompact%20foundation%20feature%20space%20at%20test%20time%20offers%20a%20practical%20and%20general%0Asolution%20for%20clinical%20registration%20without%20additional%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14809v1&entry.124074799=Read"},
{"title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "author": "Junying Chen and Zhenyang Cai and Zhiheng Liu and Yunjin Yang and Rongsheng Wang and Qingying Xiao and Xiangyi Feng and Zhan Su and Jing Guo and Xiang Wan and Guangjun Yu and Haizhou Li and Benyou Wang", "abstract": "  Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field.\n", "link": "http://arxiv.org/abs/2508.14706v1", "date": "2025-08-20", "relevancy": 2.1032, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5752}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShizhenGPT%3A%20Towards%20Multimodal%20LLMs%20for%20Traditional%20Chinese%20Medicine&body=Title%3A%20ShizhenGPT%3A%20Towards%20Multimodal%20LLMs%20for%20Traditional%20Chinese%20Medicine%0AAuthor%3A%20Junying%20Chen%20and%20Zhenyang%20Cai%20and%20Zhiheng%20Liu%20and%20Yunjin%20Yang%20and%20Rongsheng%20Wang%20and%20Qingying%20Xiao%20and%20Xiangyi%20Feng%20and%20Zhan%20Su%20and%20Jing%20Guo%20and%20Xiang%20Wan%20and%20Guangjun%20Yu%20and%20Haizhou%20Li%20and%20Benyou%20Wang%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20large%20language%20models%20%28LLMs%29%20in%20various%20domains%2C%20their%0Apotential%20in%20Traditional%20Chinese%20Medicine%20%28TCM%29%20remains%20largely%20underexplored%0Adue%20to%20two%20critical%20barriers%3A%20%281%29%20the%20scarcity%20of%20high-quality%20TCM%20data%20and%20%282%29%0Athe%20inherently%20multimodal%20nature%20of%20TCM%20diagnostics%2C%20which%20involve%20looking%2C%0Alistening%2C%20smelling%2C%20and%20pulse-taking.%20These%20sensory-rich%20modalities%20are%20beyond%0Athe%20scope%20of%20conventional%20LLMs.%20To%20address%20these%20challenges%2C%20we%20present%0AShizhenGPT%2C%20the%20first%20multimodal%20LLM%20tailored%20for%20TCM.%20To%20overcome%20data%0Ascarcity%2C%20we%20curate%20the%20largest%20TCM%20dataset%20to%20date%2C%20comprising%20100GB%2B%20of%20text%0Aand%20200GB%2B%20of%20multimodal%20data%2C%20including%201.2M%20images%2C%20200%20hours%20of%20audio%2C%20and%0Aphysiological%20signals.%20ShizhenGPT%20is%20pretrained%20and%20instruction-tuned%20to%0Aachieve%20deep%20TCM%20knowledge%20and%20multimodal%20reasoning.%20For%20evaluation%2C%20we%20collect%0Arecent%20national%20TCM%20qualification%20exams%20and%20build%20a%20visual%20benchmark%20for%0AMedicinal%20Recognition%20and%20Visual%20Diagnosis.%20Experiments%20demonstrate%20that%0AShizhenGPT%20outperforms%20comparable-scale%20LLMs%20and%20competes%20with%20larger%0Aproprietary%20models.%20Moreover%2C%20it%20leads%20in%20TCM%20visual%20understanding%20among%0Aexisting%20multimodal%20LLMs%20and%20demonstrates%20unified%20perception%20across%20modalities%0Alike%20sound%2C%20pulse%2C%20smell%2C%20and%20vision%2C%20paving%20the%20way%20toward%20holistic%20multimodal%0Aperception%20and%20diagnosis%20in%20TCM.%20Datasets%2C%20models%2C%20and%20code%20are%20publicly%0Aavailable.%20We%20hope%20this%20work%20will%20inspire%20further%20exploration%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShizhenGPT%253A%2520Towards%2520Multimodal%2520LLMs%2520for%2520Traditional%2520Chinese%2520Medicine%26entry.906535625%3DJunying%2520Chen%2520and%2520Zhenyang%2520Cai%2520and%2520Zhiheng%2520Liu%2520and%2520Yunjin%2520Yang%2520and%2520Rongsheng%2520Wang%2520and%2520Qingying%2520Xiao%2520and%2520Xiangyi%2520Feng%2520and%2520Zhan%2520Su%2520and%2520Jing%2520Guo%2520and%2520Xiang%2520Wan%2520and%2520Guangjun%2520Yu%2520and%2520Haizhou%2520Li%2520and%2520Benyou%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520various%2520domains%252C%2520their%250Apotential%2520in%2520Traditional%2520Chinese%2520Medicine%2520%2528TCM%2529%2520remains%2520largely%2520underexplored%250Adue%2520to%2520two%2520critical%2520barriers%253A%2520%25281%2529%2520the%2520scarcity%2520of%2520high-quality%2520TCM%2520data%2520and%2520%25282%2529%250Athe%2520inherently%2520multimodal%2520nature%2520of%2520TCM%2520diagnostics%252C%2520which%2520involve%2520looking%252C%250Alistening%252C%2520smelling%252C%2520and%2520pulse-taking.%2520These%2520sensory-rich%2520modalities%2520are%2520beyond%250Athe%2520scope%2520of%2520conventional%2520LLMs.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%250AShizhenGPT%252C%2520the%2520first%2520multimodal%2520LLM%2520tailored%2520for%2520TCM.%2520To%2520overcome%2520data%250Ascarcity%252C%2520we%2520curate%2520the%2520largest%2520TCM%2520dataset%2520to%2520date%252C%2520comprising%2520100GB%252B%2520of%2520text%250Aand%2520200GB%252B%2520of%2520multimodal%2520data%252C%2520including%25201.2M%2520images%252C%2520200%2520hours%2520of%2520audio%252C%2520and%250Aphysiological%2520signals.%2520ShizhenGPT%2520is%2520pretrained%2520and%2520instruction-tuned%2520to%250Aachieve%2520deep%2520TCM%2520knowledge%2520and%2520multimodal%2520reasoning.%2520For%2520evaluation%252C%2520we%2520collect%250Arecent%2520national%2520TCM%2520qualification%2520exams%2520and%2520build%2520a%2520visual%2520benchmark%2520for%250AMedicinal%2520Recognition%2520and%2520Visual%2520Diagnosis.%2520Experiments%2520demonstrate%2520that%250AShizhenGPT%2520outperforms%2520comparable-scale%2520LLMs%2520and%2520competes%2520with%2520larger%250Aproprietary%2520models.%2520Moreover%252C%2520it%2520leads%2520in%2520TCM%2520visual%2520understanding%2520among%250Aexisting%2520multimodal%2520LLMs%2520and%2520demonstrates%2520unified%2520perception%2520across%2520modalities%250Alike%2520sound%252C%2520pulse%252C%2520smell%252C%2520and%2520vision%252C%2520paving%2520the%2520way%2520toward%2520holistic%2520multimodal%250Aperception%2520and%2520diagnosis%2520in%2520TCM.%2520Datasets%252C%2520models%252C%2520and%2520code%2520are%2520publicly%250Aavailable.%2520We%2520hope%2520this%2520work%2520will%2520inspire%2520further%2520exploration%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShizhenGPT%3A%20Towards%20Multimodal%20LLMs%20for%20Traditional%20Chinese%20Medicine&entry.906535625=Junying%20Chen%20and%20Zhenyang%20Cai%20and%20Zhiheng%20Liu%20and%20Yunjin%20Yang%20and%20Rongsheng%20Wang%20and%20Qingying%20Xiao%20and%20Xiangyi%20Feng%20and%20Zhan%20Su%20and%20Jing%20Guo%20and%20Xiang%20Wan%20and%20Guangjun%20Yu%20and%20Haizhou%20Li%20and%20Benyou%20Wang&entry.1292438233=%20%20Despite%20the%20success%20of%20large%20language%20models%20%28LLMs%29%20in%20various%20domains%2C%20their%0Apotential%20in%20Traditional%20Chinese%20Medicine%20%28TCM%29%20remains%20largely%20underexplored%0Adue%20to%20two%20critical%20barriers%3A%20%281%29%20the%20scarcity%20of%20high-quality%20TCM%20data%20and%20%282%29%0Athe%20inherently%20multimodal%20nature%20of%20TCM%20diagnostics%2C%20which%20involve%20looking%2C%0Alistening%2C%20smelling%2C%20and%20pulse-taking.%20These%20sensory-rich%20modalities%20are%20beyond%0Athe%20scope%20of%20conventional%20LLMs.%20To%20address%20these%20challenges%2C%20we%20present%0AShizhenGPT%2C%20the%20first%20multimodal%20LLM%20tailored%20for%20TCM.%20To%20overcome%20data%0Ascarcity%2C%20we%20curate%20the%20largest%20TCM%20dataset%20to%20date%2C%20comprising%20100GB%2B%20of%20text%0Aand%20200GB%2B%20of%20multimodal%20data%2C%20including%201.2M%20images%2C%20200%20hours%20of%20audio%2C%20and%0Aphysiological%20signals.%20ShizhenGPT%20is%20pretrained%20and%20instruction-tuned%20to%0Aachieve%20deep%20TCM%20knowledge%20and%20multimodal%20reasoning.%20For%20evaluation%2C%20we%20collect%0Arecent%20national%20TCM%20qualification%20exams%20and%20build%20a%20visual%20benchmark%20for%0AMedicinal%20Recognition%20and%20Visual%20Diagnosis.%20Experiments%20demonstrate%20that%0AShizhenGPT%20outperforms%20comparable-scale%20LLMs%20and%20competes%20with%20larger%0Aproprietary%20models.%20Moreover%2C%20it%20leads%20in%20TCM%20visual%20understanding%20among%0Aexisting%20multimodal%20LLMs%20and%20demonstrates%20unified%20perception%20across%20modalities%0Alike%20sound%2C%20pulse%2C%20smell%2C%20and%20vision%2C%20paving%20the%20way%20toward%20holistic%20multimodal%0Aperception%20and%20diagnosis%20in%20TCM.%20Datasets%2C%20models%2C%20and%20code%20are%20publicly%0Aavailable.%20We%20hope%20this%20work%20will%20inspire%20further%20exploration%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14706v1&entry.124074799=Read"},
{"title": "Reliable generation of isomorphic physics problems using ChatGPT with\n  prompt-chaining and tool use", "author": "Zhongzhou Chen", "abstract": "  We present a method for generating large numbers of isomorphic physics\nproblems using ChatGPT through prompt chaining and tool use. This approach\nenables precise control over structural variations-such as numeric values and\nspatial relations-while supporting diverse contextual variations in the problem\nbody. By utilizing the Python code interpreter, the method supports automatic\nsolution validation and simple diagram generation, addressing key limitations\nin existing LLM-based methods. We generated two example isomorphic problem\nbanks and compared the outcome against simpler prompt-based approaches. Results\nshow that prompt-chaining produces significantly higher quality and more\nconsistent outputs than simpler, non-chaining prompts. This work demonstrates a\npromising method for efficient problem creation accessible to the average\ninstructor, which opens new possibilities for personalized adaptive testing and\nautomated content development.\n", "link": "http://arxiv.org/abs/2508.14755v1", "date": "2025-08-20", "relevancy": 2.0929, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5673}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4935}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20generation%20of%20isomorphic%20physics%20problems%20using%20ChatGPT%20with%0A%20%20prompt-chaining%20and%20tool%20use&body=Title%3A%20Reliable%20generation%20of%20isomorphic%20physics%20problems%20using%20ChatGPT%20with%0A%20%20prompt-chaining%20and%20tool%20use%0AAuthor%3A%20Zhongzhou%20Chen%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20generating%20large%20numbers%20of%20isomorphic%20physics%0Aproblems%20using%20ChatGPT%20through%20prompt%20chaining%20and%20tool%20use.%20This%20approach%0Aenables%20precise%20control%20over%20structural%20variations-such%20as%20numeric%20values%20and%0Aspatial%20relations-while%20supporting%20diverse%20contextual%20variations%20in%20the%20problem%0Abody.%20By%20utilizing%20the%20Python%20code%20interpreter%2C%20the%20method%20supports%20automatic%0Asolution%20validation%20and%20simple%20diagram%20generation%2C%20addressing%20key%20limitations%0Ain%20existing%20LLM-based%20methods.%20We%20generated%20two%20example%20isomorphic%20problem%0Abanks%20and%20compared%20the%20outcome%20against%20simpler%20prompt-based%20approaches.%20Results%0Ashow%20that%20prompt-chaining%20produces%20significantly%20higher%20quality%20and%20more%0Aconsistent%20outputs%20than%20simpler%2C%20non-chaining%20prompts.%20This%20work%20demonstrates%20a%0Apromising%20method%20for%20efficient%20problem%20creation%20accessible%20to%20the%20average%0Ainstructor%2C%20which%20opens%20new%20possibilities%20for%20personalized%20adaptive%20testing%20and%0Aautomated%20content%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520generation%2520of%2520isomorphic%2520physics%2520problems%2520using%2520ChatGPT%2520with%250A%2520%2520prompt-chaining%2520and%2520tool%2520use%26entry.906535625%3DZhongzhou%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520generating%2520large%2520numbers%2520of%2520isomorphic%2520physics%250Aproblems%2520using%2520ChatGPT%2520through%2520prompt%2520chaining%2520and%2520tool%2520use.%2520This%2520approach%250Aenables%2520precise%2520control%2520over%2520structural%2520variations-such%2520as%2520numeric%2520values%2520and%250Aspatial%2520relations-while%2520supporting%2520diverse%2520contextual%2520variations%2520in%2520the%2520problem%250Abody.%2520By%2520utilizing%2520the%2520Python%2520code%2520interpreter%252C%2520the%2520method%2520supports%2520automatic%250Asolution%2520validation%2520and%2520simple%2520diagram%2520generation%252C%2520addressing%2520key%2520limitations%250Ain%2520existing%2520LLM-based%2520methods.%2520We%2520generated%2520two%2520example%2520isomorphic%2520problem%250Abanks%2520and%2520compared%2520the%2520outcome%2520against%2520simpler%2520prompt-based%2520approaches.%2520Results%250Ashow%2520that%2520prompt-chaining%2520produces%2520significantly%2520higher%2520quality%2520and%2520more%250Aconsistent%2520outputs%2520than%2520simpler%252C%2520non-chaining%2520prompts.%2520This%2520work%2520demonstrates%2520a%250Apromising%2520method%2520for%2520efficient%2520problem%2520creation%2520accessible%2520to%2520the%2520average%250Ainstructor%252C%2520which%2520opens%2520new%2520possibilities%2520for%2520personalized%2520adaptive%2520testing%2520and%250Aautomated%2520content%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20generation%20of%20isomorphic%20physics%20problems%20using%20ChatGPT%20with%0A%20%20prompt-chaining%20and%20tool%20use&entry.906535625=Zhongzhou%20Chen&entry.1292438233=%20%20We%20present%20a%20method%20for%20generating%20large%20numbers%20of%20isomorphic%20physics%0Aproblems%20using%20ChatGPT%20through%20prompt%20chaining%20and%20tool%20use.%20This%20approach%0Aenables%20precise%20control%20over%20structural%20variations-such%20as%20numeric%20values%20and%0Aspatial%20relations-while%20supporting%20diverse%20contextual%20variations%20in%20the%20problem%0Abody.%20By%20utilizing%20the%20Python%20code%20interpreter%2C%20the%20method%20supports%20automatic%0Asolution%20validation%20and%20simple%20diagram%20generation%2C%20addressing%20key%20limitations%0Ain%20existing%20LLM-based%20methods.%20We%20generated%20two%20example%20isomorphic%20problem%0Abanks%20and%20compared%20the%20outcome%20against%20simpler%20prompt-based%20approaches.%20Results%0Ashow%20that%20prompt-chaining%20produces%20significantly%20higher%20quality%20and%20more%0Aconsistent%20outputs%20than%20simpler%2C%20non-chaining%20prompts.%20This%20work%20demonstrates%20a%0Apromising%20method%20for%20efficient%20problem%20creation%20accessible%20to%20the%20average%0Ainstructor%2C%20which%20opens%20new%20possibilities%20for%20personalized%20adaptive%20testing%20and%0Aautomated%20content%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14755v1&entry.124074799=Read"},
{"title": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents", "author": "Thomas Carta and Cl\u00e9ment Romac and Loris Gaven and Pierre-Yves Oudeyer and Olivier Sigaud and Sylvain Lamprier", "abstract": "  Open-ended AI agents need to be able to learn efficiently goals of increasing\ncomplexity, abstraction and heterogeneity over their lifetime. Beyond sampling\nefficiently their own goals, autotelic agents specifically need to be able to\nkeep the growing complexity of goals under control, limiting the associated\ngrowth in sample and computational complexity. To adress this challenge, recent\napproaches have leveraged hierarchical reinforcement learning (HRL) and\nlanguage, capitalizing on its compositional and combinatorial generalization\ncapabilities to acquire temporally extended reusable behaviours. Existing\napproaches use expert defined spaces of subgoals over which they instantiate a\nhierarchy, and often assume pre-trained associated low-level policies. Such\ndesigns are inadequate in open-ended scenarios, where goal spaces naturally\ndiversify across a broad spectrum of difficulties. We introduce HERAKLES, a\nframework that enables a two-level hierarchical autotelic agent to continuously\ncompile mastered goals into the low-level policy, executed by a small, fast\nneural network, dynamically expanding the set of subgoals available to the\nhigh-level policy. We train a Large Language Model (LLM) to serve as the\nhigh-level controller, exploiting its strengths in goal decomposition and\ngeneralization to operate effectively over this evolving subgoal space. We\nevaluate HERAKLES in the open-ended Crafter environment and show that it scales\neffectively with goal complexity, improves sample efficiency through skill\ncompilation, and enables the agent to adapt robustly to novel challenges over\ntime.\n", "link": "http://arxiv.org/abs/2508.14751v1", "date": "2025-08-20", "relevancy": 2.0859, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5362}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5267}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HERAKLES%3A%20Hierarchical%20Skill%20Compilation%20for%20Open-ended%20LLM%20Agents&body=Title%3A%20HERAKLES%3A%20Hierarchical%20Skill%20Compilation%20for%20Open-ended%20LLM%20Agents%0AAuthor%3A%20Thomas%20Carta%20and%20Cl%C3%A9ment%20Romac%20and%20Loris%20Gaven%20and%20Pierre-Yves%20Oudeyer%20and%20Olivier%20Sigaud%20and%20Sylvain%20Lamprier%0AAbstract%3A%20%20%20Open-ended%20AI%20agents%20need%20to%20be%20able%20to%20learn%20efficiently%20goals%20of%20increasing%0Acomplexity%2C%20abstraction%20and%20heterogeneity%20over%20their%20lifetime.%20Beyond%20sampling%0Aefficiently%20their%20own%20goals%2C%20autotelic%20agents%20specifically%20need%20to%20be%20able%20to%0Akeep%20the%20growing%20complexity%20of%20goals%20under%20control%2C%20limiting%20the%20associated%0Agrowth%20in%20sample%20and%20computational%20complexity.%20To%20adress%20this%20challenge%2C%20recent%0Aapproaches%20have%20leveraged%20hierarchical%20reinforcement%20learning%20%28HRL%29%20and%0Alanguage%2C%20capitalizing%20on%20its%20compositional%20and%20combinatorial%20generalization%0Acapabilities%20to%20acquire%20temporally%20extended%20reusable%20behaviours.%20Existing%0Aapproaches%20use%20expert%20defined%20spaces%20of%20subgoals%20over%20which%20they%20instantiate%20a%0Ahierarchy%2C%20and%20often%20assume%20pre-trained%20associated%20low-level%20policies.%20Such%0Adesigns%20are%20inadequate%20in%20open-ended%20scenarios%2C%20where%20goal%20spaces%20naturally%0Adiversify%20across%20a%20broad%20spectrum%20of%20difficulties.%20We%20introduce%20HERAKLES%2C%20a%0Aframework%20that%20enables%20a%20two-level%20hierarchical%20autotelic%20agent%20to%20continuously%0Acompile%20mastered%20goals%20into%20the%20low-level%20policy%2C%20executed%20by%20a%20small%2C%20fast%0Aneural%20network%2C%20dynamically%20expanding%20the%20set%20of%20subgoals%20available%20to%20the%0Ahigh-level%20policy.%20We%20train%20a%20Large%20Language%20Model%20%28LLM%29%20to%20serve%20as%20the%0Ahigh-level%20controller%2C%20exploiting%20its%20strengths%20in%20goal%20decomposition%20and%0Ageneralization%20to%20operate%20effectively%20over%20this%20evolving%20subgoal%20space.%20We%0Aevaluate%20HERAKLES%20in%20the%20open-ended%20Crafter%20environment%20and%20show%20that%20it%20scales%0Aeffectively%20with%20goal%20complexity%2C%20improves%20sample%20efficiency%20through%20skill%0Acompilation%2C%20and%20enables%20the%20agent%20to%20adapt%20robustly%20to%20novel%20challenges%20over%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHERAKLES%253A%2520Hierarchical%2520Skill%2520Compilation%2520for%2520Open-ended%2520LLM%2520Agents%26entry.906535625%3DThomas%2520Carta%2520and%2520Cl%25C3%25A9ment%2520Romac%2520and%2520Loris%2520Gaven%2520and%2520Pierre-Yves%2520Oudeyer%2520and%2520Olivier%2520Sigaud%2520and%2520Sylvain%2520Lamprier%26entry.1292438233%3D%2520%2520Open-ended%2520AI%2520agents%2520need%2520to%2520be%2520able%2520to%2520learn%2520efficiently%2520goals%2520of%2520increasing%250Acomplexity%252C%2520abstraction%2520and%2520heterogeneity%2520over%2520their%2520lifetime.%2520Beyond%2520sampling%250Aefficiently%2520their%2520own%2520goals%252C%2520autotelic%2520agents%2520specifically%2520need%2520to%2520be%2520able%2520to%250Akeep%2520the%2520growing%2520complexity%2520of%2520goals%2520under%2520control%252C%2520limiting%2520the%2520associated%250Agrowth%2520in%2520sample%2520and%2520computational%2520complexity.%2520To%2520adress%2520this%2520challenge%252C%2520recent%250Aapproaches%2520have%2520leveraged%2520hierarchical%2520reinforcement%2520learning%2520%2528HRL%2529%2520and%250Alanguage%252C%2520capitalizing%2520on%2520its%2520compositional%2520and%2520combinatorial%2520generalization%250Acapabilities%2520to%2520acquire%2520temporally%2520extended%2520reusable%2520behaviours.%2520Existing%250Aapproaches%2520use%2520expert%2520defined%2520spaces%2520of%2520subgoals%2520over%2520which%2520they%2520instantiate%2520a%250Ahierarchy%252C%2520and%2520often%2520assume%2520pre-trained%2520associated%2520low-level%2520policies.%2520Such%250Adesigns%2520are%2520inadequate%2520in%2520open-ended%2520scenarios%252C%2520where%2520goal%2520spaces%2520naturally%250Adiversify%2520across%2520a%2520broad%2520spectrum%2520of%2520difficulties.%2520We%2520introduce%2520HERAKLES%252C%2520a%250Aframework%2520that%2520enables%2520a%2520two-level%2520hierarchical%2520autotelic%2520agent%2520to%2520continuously%250Acompile%2520mastered%2520goals%2520into%2520the%2520low-level%2520policy%252C%2520executed%2520by%2520a%2520small%252C%2520fast%250Aneural%2520network%252C%2520dynamically%2520expanding%2520the%2520set%2520of%2520subgoals%2520available%2520to%2520the%250Ahigh-level%2520policy.%2520We%2520train%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520serve%2520as%2520the%250Ahigh-level%2520controller%252C%2520exploiting%2520its%2520strengths%2520in%2520goal%2520decomposition%2520and%250Ageneralization%2520to%2520operate%2520effectively%2520over%2520this%2520evolving%2520subgoal%2520space.%2520We%250Aevaluate%2520HERAKLES%2520in%2520the%2520open-ended%2520Crafter%2520environment%2520and%2520show%2520that%2520it%2520scales%250Aeffectively%2520with%2520goal%2520complexity%252C%2520improves%2520sample%2520efficiency%2520through%2520skill%250Acompilation%252C%2520and%2520enables%2520the%2520agent%2520to%2520adapt%2520robustly%2520to%2520novel%2520challenges%2520over%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HERAKLES%3A%20Hierarchical%20Skill%20Compilation%20for%20Open-ended%20LLM%20Agents&entry.906535625=Thomas%20Carta%20and%20Cl%C3%A9ment%20Romac%20and%20Loris%20Gaven%20and%20Pierre-Yves%20Oudeyer%20and%20Olivier%20Sigaud%20and%20Sylvain%20Lamprier&entry.1292438233=%20%20Open-ended%20AI%20agents%20need%20to%20be%20able%20to%20learn%20efficiently%20goals%20of%20increasing%0Acomplexity%2C%20abstraction%20and%20heterogeneity%20over%20their%20lifetime.%20Beyond%20sampling%0Aefficiently%20their%20own%20goals%2C%20autotelic%20agents%20specifically%20need%20to%20be%20able%20to%0Akeep%20the%20growing%20complexity%20of%20goals%20under%20control%2C%20limiting%20the%20associated%0Agrowth%20in%20sample%20and%20computational%20complexity.%20To%20adress%20this%20challenge%2C%20recent%0Aapproaches%20have%20leveraged%20hierarchical%20reinforcement%20learning%20%28HRL%29%20and%0Alanguage%2C%20capitalizing%20on%20its%20compositional%20and%20combinatorial%20generalization%0Acapabilities%20to%20acquire%20temporally%20extended%20reusable%20behaviours.%20Existing%0Aapproaches%20use%20expert%20defined%20spaces%20of%20subgoals%20over%20which%20they%20instantiate%20a%0Ahierarchy%2C%20and%20often%20assume%20pre-trained%20associated%20low-level%20policies.%20Such%0Adesigns%20are%20inadequate%20in%20open-ended%20scenarios%2C%20where%20goal%20spaces%20naturally%0Adiversify%20across%20a%20broad%20spectrum%20of%20difficulties.%20We%20introduce%20HERAKLES%2C%20a%0Aframework%20that%20enables%20a%20two-level%20hierarchical%20autotelic%20agent%20to%20continuously%0Acompile%20mastered%20goals%20into%20the%20low-level%20policy%2C%20executed%20by%20a%20small%2C%20fast%0Aneural%20network%2C%20dynamically%20expanding%20the%20set%20of%20subgoals%20available%20to%20the%0Ahigh-level%20policy.%20We%20train%20a%20Large%20Language%20Model%20%28LLM%29%20to%20serve%20as%20the%0Ahigh-level%20controller%2C%20exploiting%20its%20strengths%20in%20goal%20decomposition%20and%0Ageneralization%20to%20operate%20effectively%20over%20this%20evolving%20subgoal%20space.%20We%0Aevaluate%20HERAKLES%20in%20the%20open-ended%20Crafter%20environment%20and%20show%20that%20it%20scales%0Aeffectively%20with%20goal%20complexity%2C%20improves%20sample%20efficiency%20through%20skill%0Acompilation%2C%20and%20enables%20the%20agent%20to%20adapt%20robustly%20to%20novel%20challenges%20over%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14751v1&entry.124074799=Read"},
{"title": "Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of\n  Bipedal Navigation using Benders Decomposition", "author": "Jiming Ren and Xuan Lin and Roman Mineyev and Karen M. Feigh and Samuel Coogan and Ye Zhao", "abstract": "  Task and motion planning under Signal Temporal Logic constraints is known to\nbe NP-hard. A common class of approaches formulates these hybrid problems,\nwhich involve discrete task scheduling and continuous motion planning, as\nmixed-integer programs (MIP). However, in applications for bipedal locomotion,\nintroduction of non-convex constraints such as kinematic reachability and\nfootstep rotation exacerbates the computational complexity of MIPs. In this\nwork, we present a method based on Benders Decomposition to address scenarios\nwhere solving the entire monolithic optimization problem is prohibitively\nintractable. Benders Decomposition proposes an iterative cutting-plane\ntechnique that partitions the problem into a master problem to prototype a plan\nthat meets the task specification, and a series of subproblems for kinematics\nand dynamics feasibility checks. Our experiments demonstrate that this method\nachieves faster planning compared to alternative algorithms for solving the\nresulting optimization program with nonlinear constraints.\n", "link": "http://arxiv.org/abs/2508.13407v2", "date": "2025-08-20", "relevancy": 2.0743, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5612}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.515}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Signal-Temporal-Logic-Based%20Task%20and%20Motion%20Planning%20of%0A%20%20Bipedal%20Navigation%20using%20Benders%20Decomposition&body=Title%3A%20Accelerating%20Signal-Temporal-Logic-Based%20Task%20and%20Motion%20Planning%20of%0A%20%20Bipedal%20Navigation%20using%20Benders%20Decomposition%0AAuthor%3A%20Jiming%20Ren%20and%20Xuan%20Lin%20and%20Roman%20Mineyev%20and%20Karen%20M.%20Feigh%20and%20Samuel%20Coogan%20and%20Ye%20Zhao%0AAbstract%3A%20%20%20Task%20and%20motion%20planning%20under%20Signal%20Temporal%20Logic%20constraints%20is%20known%20to%0Abe%20NP-hard.%20A%20common%20class%20of%20approaches%20formulates%20these%20hybrid%20problems%2C%0Awhich%20involve%20discrete%20task%20scheduling%20and%20continuous%20motion%20planning%2C%20as%0Amixed-integer%20programs%20%28MIP%29.%20However%2C%20in%20applications%20for%20bipedal%20locomotion%2C%0Aintroduction%20of%20non-convex%20constraints%20such%20as%20kinematic%20reachability%20and%0Afootstep%20rotation%20exacerbates%20the%20computational%20complexity%20of%20MIPs.%20In%20this%0Awork%2C%20we%20present%20a%20method%20based%20on%20Benders%20Decomposition%20to%20address%20scenarios%0Awhere%20solving%20the%20entire%20monolithic%20optimization%20problem%20is%20prohibitively%0Aintractable.%20Benders%20Decomposition%20proposes%20an%20iterative%20cutting-plane%0Atechnique%20that%20partitions%20the%20problem%20into%20a%20master%20problem%20to%20prototype%20a%20plan%0Athat%20meets%20the%20task%20specification%2C%20and%20a%20series%20of%20subproblems%20for%20kinematics%0Aand%20dynamics%20feasibility%20checks.%20Our%20experiments%20demonstrate%20that%20this%20method%0Aachieves%20faster%20planning%20compared%20to%20alternative%20algorithms%20for%20solving%20the%0Aresulting%20optimization%20program%20with%20nonlinear%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Signal-Temporal-Logic-Based%2520Task%2520and%2520Motion%2520Planning%2520of%250A%2520%2520Bipedal%2520Navigation%2520using%2520Benders%2520Decomposition%26entry.906535625%3DJiming%2520Ren%2520and%2520Xuan%2520Lin%2520and%2520Roman%2520Mineyev%2520and%2520Karen%2520M.%2520Feigh%2520and%2520Samuel%2520Coogan%2520and%2520Ye%2520Zhao%26entry.1292438233%3D%2520%2520Task%2520and%2520motion%2520planning%2520under%2520Signal%2520Temporal%2520Logic%2520constraints%2520is%2520known%2520to%250Abe%2520NP-hard.%2520A%2520common%2520class%2520of%2520approaches%2520formulates%2520these%2520hybrid%2520problems%252C%250Awhich%2520involve%2520discrete%2520task%2520scheduling%2520and%2520continuous%2520motion%2520planning%252C%2520as%250Amixed-integer%2520programs%2520%2528MIP%2529.%2520However%252C%2520in%2520applications%2520for%2520bipedal%2520locomotion%252C%250Aintroduction%2520of%2520non-convex%2520constraints%2520such%2520as%2520kinematic%2520reachability%2520and%250Afootstep%2520rotation%2520exacerbates%2520the%2520computational%2520complexity%2520of%2520MIPs.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520method%2520based%2520on%2520Benders%2520Decomposition%2520to%2520address%2520scenarios%250Awhere%2520solving%2520the%2520entire%2520monolithic%2520optimization%2520problem%2520is%2520prohibitively%250Aintractable.%2520Benders%2520Decomposition%2520proposes%2520an%2520iterative%2520cutting-plane%250Atechnique%2520that%2520partitions%2520the%2520problem%2520into%2520a%2520master%2520problem%2520to%2520prototype%2520a%2520plan%250Athat%2520meets%2520the%2520task%2520specification%252C%2520and%2520a%2520series%2520of%2520subproblems%2520for%2520kinematics%250Aand%2520dynamics%2520feasibility%2520checks.%2520Our%2520experiments%2520demonstrate%2520that%2520this%2520method%250Aachieves%2520faster%2520planning%2520compared%2520to%2520alternative%2520algorithms%2520for%2520solving%2520the%250Aresulting%2520optimization%2520program%2520with%2520nonlinear%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Signal-Temporal-Logic-Based%20Task%20and%20Motion%20Planning%20of%0A%20%20Bipedal%20Navigation%20using%20Benders%20Decomposition&entry.906535625=Jiming%20Ren%20and%20Xuan%20Lin%20and%20Roman%20Mineyev%20and%20Karen%20M.%20Feigh%20and%20Samuel%20Coogan%20and%20Ye%20Zhao&entry.1292438233=%20%20Task%20and%20motion%20planning%20under%20Signal%20Temporal%20Logic%20constraints%20is%20known%20to%0Abe%20NP-hard.%20A%20common%20class%20of%20approaches%20formulates%20these%20hybrid%20problems%2C%0Awhich%20involve%20discrete%20task%20scheduling%20and%20continuous%20motion%20planning%2C%20as%0Amixed-integer%20programs%20%28MIP%29.%20However%2C%20in%20applications%20for%20bipedal%20locomotion%2C%0Aintroduction%20of%20non-convex%20constraints%20such%20as%20kinematic%20reachability%20and%0Afootstep%20rotation%20exacerbates%20the%20computational%20complexity%20of%20MIPs.%20In%20this%0Awork%2C%20we%20present%20a%20method%20based%20on%20Benders%20Decomposition%20to%20address%20scenarios%0Awhere%20solving%20the%20entire%20monolithic%20optimization%20problem%20is%20prohibitively%0Aintractable.%20Benders%20Decomposition%20proposes%20an%20iterative%20cutting-plane%0Atechnique%20that%20partitions%20the%20problem%20into%20a%20master%20problem%20to%20prototype%20a%20plan%0Athat%20meets%20the%20task%20specification%2C%20and%20a%20series%20of%20subproblems%20for%20kinematics%0Aand%20dynamics%20feasibility%20checks.%20Our%20experiments%20demonstrate%20that%20this%20method%0Aachieves%20faster%20planning%20compared%20to%20alternative%20algorithms%20for%20solving%20the%0Aresulting%20optimization%20program%20with%20nonlinear%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13407v2&entry.124074799=Read"},
{"title": "Graph Structure Learning with Temporal Graph Information Bottleneck for\n  Inductive Representation Learning", "author": "Jiafeng Xiong and Rizos Sakellariou", "abstract": "  Temporal graph learning is crucial for dynamic networks where nodes and edges\nevolve over time and new nodes continuously join the system. Inductive\nrepresentation learning in such settings faces two major challenges:\neffectively representing unseen nodes and mitigating noisy or redundant graph\ninformation. We propose GTGIB, a versatile framework that integrates Graph\nStructure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). We\ndesign a novel two-step GSL-based structural enhancer to enrich and optimize\nnode neighborhoods and demonstrate its effectiveness and efficiency through\ntheoretical proofs and experiments. The TGIB refines the optimized graph by\nextending the information bottleneck principle to temporal graphs, regularizing\nboth edges and features based on our derived tractable TGIB objective function\nvia variational approximation, enabling stable and efficient optimization.\nGTGIB-based models are evaluated to predict links on four real-world datasets;\nthey outperform existing methods in all datasets under the inductive setting,\nwith significant and consistent improvement in the transductive setting.\n", "link": "http://arxiv.org/abs/2508.14859v1", "date": "2025-08-20", "relevancy": 2.0699, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Structure%20Learning%20with%20Temporal%20Graph%20Information%20Bottleneck%20for%0A%20%20Inductive%20Representation%20Learning&body=Title%3A%20Graph%20Structure%20Learning%20with%20Temporal%20Graph%20Information%20Bottleneck%20for%0A%20%20Inductive%20Representation%20Learning%0AAuthor%3A%20Jiafeng%20Xiong%20and%20Rizos%20Sakellariou%0AAbstract%3A%20%20%20Temporal%20graph%20learning%20is%20crucial%20for%20dynamic%20networks%20where%20nodes%20and%20edges%0Aevolve%20over%20time%20and%20new%20nodes%20continuously%20join%20the%20system.%20Inductive%0Arepresentation%20learning%20in%20such%20settings%20faces%20two%20major%20challenges%3A%0Aeffectively%20representing%20unseen%20nodes%20and%20mitigating%20noisy%20or%20redundant%20graph%0Ainformation.%20We%20propose%20GTGIB%2C%20a%20versatile%20framework%20that%20integrates%20Graph%0AStructure%20Learning%20%28GSL%29%20with%20Temporal%20Graph%20Information%20Bottleneck%20%28TGIB%29.%20We%0Adesign%20a%20novel%20two-step%20GSL-based%20structural%20enhancer%20to%20enrich%20and%20optimize%0Anode%20neighborhoods%20and%20demonstrate%20its%20effectiveness%20and%20efficiency%20through%0Atheoretical%20proofs%20and%20experiments.%20The%20TGIB%20refines%20the%20optimized%20graph%20by%0Aextending%20the%20information%20bottleneck%20principle%20to%20temporal%20graphs%2C%20regularizing%0Aboth%20edges%20and%20features%20based%20on%20our%20derived%20tractable%20TGIB%20objective%20function%0Avia%20variational%20approximation%2C%20enabling%20stable%20and%20efficient%20optimization.%0AGTGIB-based%20models%20are%20evaluated%20to%20predict%20links%20on%20four%20real-world%20datasets%3B%0Athey%20outperform%20existing%20methods%20in%20all%20datasets%20under%20the%20inductive%20setting%2C%0Awith%20significant%20and%20consistent%20improvement%20in%20the%20transductive%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Structure%2520Learning%2520with%2520Temporal%2520Graph%2520Information%2520Bottleneck%2520for%250A%2520%2520Inductive%2520Representation%2520Learning%26entry.906535625%3DJiafeng%2520Xiong%2520and%2520Rizos%2520Sakellariou%26entry.1292438233%3D%2520%2520Temporal%2520graph%2520learning%2520is%2520crucial%2520for%2520dynamic%2520networks%2520where%2520nodes%2520and%2520edges%250Aevolve%2520over%2520time%2520and%2520new%2520nodes%2520continuously%2520join%2520the%2520system.%2520Inductive%250Arepresentation%2520learning%2520in%2520such%2520settings%2520faces%2520two%2520major%2520challenges%253A%250Aeffectively%2520representing%2520unseen%2520nodes%2520and%2520mitigating%2520noisy%2520or%2520redundant%2520graph%250Ainformation.%2520We%2520propose%2520GTGIB%252C%2520a%2520versatile%2520framework%2520that%2520integrates%2520Graph%250AStructure%2520Learning%2520%2528GSL%2529%2520with%2520Temporal%2520Graph%2520Information%2520Bottleneck%2520%2528TGIB%2529.%2520We%250Adesign%2520a%2520novel%2520two-step%2520GSL-based%2520structural%2520enhancer%2520to%2520enrich%2520and%2520optimize%250Anode%2520neighborhoods%2520and%2520demonstrate%2520its%2520effectiveness%2520and%2520efficiency%2520through%250Atheoretical%2520proofs%2520and%2520experiments.%2520The%2520TGIB%2520refines%2520the%2520optimized%2520graph%2520by%250Aextending%2520the%2520information%2520bottleneck%2520principle%2520to%2520temporal%2520graphs%252C%2520regularizing%250Aboth%2520edges%2520and%2520features%2520based%2520on%2520our%2520derived%2520tractable%2520TGIB%2520objective%2520function%250Avia%2520variational%2520approximation%252C%2520enabling%2520stable%2520and%2520efficient%2520optimization.%250AGTGIB-based%2520models%2520are%2520evaluated%2520to%2520predict%2520links%2520on%2520four%2520real-world%2520datasets%253B%250Athey%2520outperform%2520existing%2520methods%2520in%2520all%2520datasets%2520under%2520the%2520inductive%2520setting%252C%250Awith%2520significant%2520and%2520consistent%2520improvement%2520in%2520the%2520transductive%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Structure%20Learning%20with%20Temporal%20Graph%20Information%20Bottleneck%20for%0A%20%20Inductive%20Representation%20Learning&entry.906535625=Jiafeng%20Xiong%20and%20Rizos%20Sakellariou&entry.1292438233=%20%20Temporal%20graph%20learning%20is%20crucial%20for%20dynamic%20networks%20where%20nodes%20and%20edges%0Aevolve%20over%20time%20and%20new%20nodes%20continuously%20join%20the%20system.%20Inductive%0Arepresentation%20learning%20in%20such%20settings%20faces%20two%20major%20challenges%3A%0Aeffectively%20representing%20unseen%20nodes%20and%20mitigating%20noisy%20or%20redundant%20graph%0Ainformation.%20We%20propose%20GTGIB%2C%20a%20versatile%20framework%20that%20integrates%20Graph%0AStructure%20Learning%20%28GSL%29%20with%20Temporal%20Graph%20Information%20Bottleneck%20%28TGIB%29.%20We%0Adesign%20a%20novel%20two-step%20GSL-based%20structural%20enhancer%20to%20enrich%20and%20optimize%0Anode%20neighborhoods%20and%20demonstrate%20its%20effectiveness%20and%20efficiency%20through%0Atheoretical%20proofs%20and%20experiments.%20The%20TGIB%20refines%20the%20optimized%20graph%20by%0Aextending%20the%20information%20bottleneck%20principle%20to%20temporal%20graphs%2C%20regularizing%0Aboth%20edges%20and%20features%20based%20on%20our%20derived%20tractable%20TGIB%20objective%20function%0Avia%20variational%20approximation%2C%20enabling%20stable%20and%20efficient%20optimization.%0AGTGIB-based%20models%20are%20evaluated%20to%20predict%20links%20on%20four%20real-world%20datasets%3B%0Athey%20outperform%20existing%20methods%20in%20all%20datasets%20under%20the%20inductive%20setting%2C%0Awith%20significant%20and%20consistent%20improvement%20in%20the%20transductive%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14859v1&entry.124074799=Read"},
{"title": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning\n  Strategies", "author": "Julia Werner and Christoph Gerum and Jorg Nick and Maxime Le Floch and Franz Brinkmann and Jochen Hampe and Oliver Bringmann", "abstract": "  Capsule endoscopy is a method to capture images of the gastrointestinal tract\nand screen for diseases which might remain hidden if investigated with standard\nendoscopes. Due to the limited size of a video capsule, embedding AI models\ndirectly into the capsule demands careful consideration of the model size and\nthus complicates anomaly detection in this field. Furthermore, the scarcity of\navailable data in this domain poses an ongoing challenge to achieving effective\nanomaly detection. Thus, this work introduces an ensemble strategy to address\nthis challenge in anomaly detection tasks in video capsule endoscopies,\nrequiring only a small number of individual neural networks during both the\ntraining and inference phases. Ensemble learning combines the predictions of\nmultiple independently trained neural networks. This has shown to be highly\neffective in enhancing both the accuracy and robustness of machine learning\nmodels. However, this comes at the cost of higher memory usage and increased\ncomputational effort, which quickly becomes prohibitive in many real-world\napplications. Instead of applying the same training algorithm to each\nindividual network, we propose using various loss functions, drawn from the\nanomaly detection field, to train each network. The methods are validated on\nthe two largest publicly available datasets for video capsule endoscopy images,\nthe Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on\nthe Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our\napproach outperforms current baselines with significantly fewer parameters\nacross all models, which is a crucial step towards incorporating artificial\nintelligence into capsule endoscopies.\n", "link": "http://arxiv.org/abs/2504.06039v2", "date": "2025-08-20", "relevancy": 2.0619, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5487}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Anomaly%20Detection%20for%20Capsule%20Endoscopy%20Using%20Ensemble%20Learning%0A%20%20Strategies&body=Title%3A%20Enhanced%20Anomaly%20Detection%20for%20Capsule%20Endoscopy%20Using%20Ensemble%20Learning%0A%20%20Strategies%0AAuthor%3A%20Julia%20Werner%20and%20Christoph%20Gerum%20and%20Jorg%20Nick%20and%20Maxime%20Le%20Floch%20and%20Franz%20Brinkmann%20and%20Jochen%20Hampe%20and%20Oliver%20Bringmann%0AAbstract%3A%20%20%20Capsule%20endoscopy%20is%20a%20method%20to%20capture%20images%20of%20the%20gastrointestinal%20tract%0Aand%20screen%20for%20diseases%20which%20might%20remain%20hidden%20if%20investigated%20with%20standard%0Aendoscopes.%20Due%20to%20the%20limited%20size%20of%20a%20video%20capsule%2C%20embedding%20AI%20models%0Adirectly%20into%20the%20capsule%20demands%20careful%20consideration%20of%20the%20model%20size%20and%0Athus%20complicates%20anomaly%20detection%20in%20this%20field.%20Furthermore%2C%20the%20scarcity%20of%0Aavailable%20data%20in%20this%20domain%20poses%20an%20ongoing%20challenge%20to%20achieving%20effective%0Aanomaly%20detection.%20Thus%2C%20this%20work%20introduces%20an%20ensemble%20strategy%20to%20address%0Athis%20challenge%20in%20anomaly%20detection%20tasks%20in%20video%20capsule%20endoscopies%2C%0Arequiring%20only%20a%20small%20number%20of%20individual%20neural%20networks%20during%20both%20the%0Atraining%20and%20inference%20phases.%20Ensemble%20learning%20combines%20the%20predictions%20of%0Amultiple%20independently%20trained%20neural%20networks.%20This%20has%20shown%20to%20be%20highly%0Aeffective%20in%20enhancing%20both%20the%20accuracy%20and%20robustness%20of%20machine%20learning%0Amodels.%20However%2C%20this%20comes%20at%20the%20cost%20of%20higher%20memory%20usage%20and%20increased%0Acomputational%20effort%2C%20which%20quickly%20becomes%20prohibitive%20in%20many%20real-world%0Aapplications.%20Instead%20of%20applying%20the%20same%20training%20algorithm%20to%20each%0Aindividual%20network%2C%20we%20propose%20using%20various%20loss%20functions%2C%20drawn%20from%20the%0Aanomaly%20detection%20field%2C%20to%20train%20each%20network.%20The%20methods%20are%20validated%20on%0Athe%20two%20largest%20publicly%20available%20datasets%20for%20video%20capsule%20endoscopy%20images%2C%0Athe%20Galar%20and%20the%20Kvasir-Capsule%20dataset.%20We%20achieve%20an%20AUC%20score%20of%2076.86%25%20on%0Athe%20Kvasir-Capsule%20and%20an%20AUC%20score%20of%2076.98%25%20on%20the%20Galar%20dataset.%20Our%0Aapproach%20outperforms%20current%20baselines%20with%20significantly%20fewer%20parameters%0Aacross%20all%20models%2C%20which%20is%20a%20crucial%20step%20towards%20incorporating%20artificial%0Aintelligence%20into%20capsule%20endoscopies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Anomaly%2520Detection%2520for%2520Capsule%2520Endoscopy%2520Using%2520Ensemble%2520Learning%250A%2520%2520Strategies%26entry.906535625%3DJulia%2520Werner%2520and%2520Christoph%2520Gerum%2520and%2520Jorg%2520Nick%2520and%2520Maxime%2520Le%2520Floch%2520and%2520Franz%2520Brinkmann%2520and%2520Jochen%2520Hampe%2520and%2520Oliver%2520Bringmann%26entry.1292438233%3D%2520%2520Capsule%2520endoscopy%2520is%2520a%2520method%2520to%2520capture%2520images%2520of%2520the%2520gastrointestinal%2520tract%250Aand%2520screen%2520for%2520diseases%2520which%2520might%2520remain%2520hidden%2520if%2520investigated%2520with%2520standard%250Aendoscopes.%2520Due%2520to%2520the%2520limited%2520size%2520of%2520a%2520video%2520capsule%252C%2520embedding%2520AI%2520models%250Adirectly%2520into%2520the%2520capsule%2520demands%2520careful%2520consideration%2520of%2520the%2520model%2520size%2520and%250Athus%2520complicates%2520anomaly%2520detection%2520in%2520this%2520field.%2520Furthermore%252C%2520the%2520scarcity%2520of%250Aavailable%2520data%2520in%2520this%2520domain%2520poses%2520an%2520ongoing%2520challenge%2520to%2520achieving%2520effective%250Aanomaly%2520detection.%2520Thus%252C%2520this%2520work%2520introduces%2520an%2520ensemble%2520strategy%2520to%2520address%250Athis%2520challenge%2520in%2520anomaly%2520detection%2520tasks%2520in%2520video%2520capsule%2520endoscopies%252C%250Arequiring%2520only%2520a%2520small%2520number%2520of%2520individual%2520neural%2520networks%2520during%2520both%2520the%250Atraining%2520and%2520inference%2520phases.%2520Ensemble%2520learning%2520combines%2520the%2520predictions%2520of%250Amultiple%2520independently%2520trained%2520neural%2520networks.%2520This%2520has%2520shown%2520to%2520be%2520highly%250Aeffective%2520in%2520enhancing%2520both%2520the%2520accuracy%2520and%2520robustness%2520of%2520machine%2520learning%250Amodels.%2520However%252C%2520this%2520comes%2520at%2520the%2520cost%2520of%2520higher%2520memory%2520usage%2520and%2520increased%250Acomputational%2520effort%252C%2520which%2520quickly%2520becomes%2520prohibitive%2520in%2520many%2520real-world%250Aapplications.%2520Instead%2520of%2520applying%2520the%2520same%2520training%2520algorithm%2520to%2520each%250Aindividual%2520network%252C%2520we%2520propose%2520using%2520various%2520loss%2520functions%252C%2520drawn%2520from%2520the%250Aanomaly%2520detection%2520field%252C%2520to%2520train%2520each%2520network.%2520The%2520methods%2520are%2520validated%2520on%250Athe%2520two%2520largest%2520publicly%2520available%2520datasets%2520for%2520video%2520capsule%2520endoscopy%2520images%252C%250Athe%2520Galar%2520and%2520the%2520Kvasir-Capsule%2520dataset.%2520We%2520achieve%2520an%2520AUC%2520score%2520of%252076.86%2525%2520on%250Athe%2520Kvasir-Capsule%2520and%2520an%2520AUC%2520score%2520of%252076.98%2525%2520on%2520the%2520Galar%2520dataset.%2520Our%250Aapproach%2520outperforms%2520current%2520baselines%2520with%2520significantly%2520fewer%2520parameters%250Aacross%2520all%2520models%252C%2520which%2520is%2520a%2520crucial%2520step%2520towards%2520incorporating%2520artificial%250Aintelligence%2520into%2520capsule%2520endoscopies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Anomaly%20Detection%20for%20Capsule%20Endoscopy%20Using%20Ensemble%20Learning%0A%20%20Strategies&entry.906535625=Julia%20Werner%20and%20Christoph%20Gerum%20and%20Jorg%20Nick%20and%20Maxime%20Le%20Floch%20and%20Franz%20Brinkmann%20and%20Jochen%20Hampe%20and%20Oliver%20Bringmann&entry.1292438233=%20%20Capsule%20endoscopy%20is%20a%20method%20to%20capture%20images%20of%20the%20gastrointestinal%20tract%0Aand%20screen%20for%20diseases%20which%20might%20remain%20hidden%20if%20investigated%20with%20standard%0Aendoscopes.%20Due%20to%20the%20limited%20size%20of%20a%20video%20capsule%2C%20embedding%20AI%20models%0Adirectly%20into%20the%20capsule%20demands%20careful%20consideration%20of%20the%20model%20size%20and%0Athus%20complicates%20anomaly%20detection%20in%20this%20field.%20Furthermore%2C%20the%20scarcity%20of%0Aavailable%20data%20in%20this%20domain%20poses%20an%20ongoing%20challenge%20to%20achieving%20effective%0Aanomaly%20detection.%20Thus%2C%20this%20work%20introduces%20an%20ensemble%20strategy%20to%20address%0Athis%20challenge%20in%20anomaly%20detection%20tasks%20in%20video%20capsule%20endoscopies%2C%0Arequiring%20only%20a%20small%20number%20of%20individual%20neural%20networks%20during%20both%20the%0Atraining%20and%20inference%20phases.%20Ensemble%20learning%20combines%20the%20predictions%20of%0Amultiple%20independently%20trained%20neural%20networks.%20This%20has%20shown%20to%20be%20highly%0Aeffective%20in%20enhancing%20both%20the%20accuracy%20and%20robustness%20of%20machine%20learning%0Amodels.%20However%2C%20this%20comes%20at%20the%20cost%20of%20higher%20memory%20usage%20and%20increased%0Acomputational%20effort%2C%20which%20quickly%20becomes%20prohibitive%20in%20many%20real-world%0Aapplications.%20Instead%20of%20applying%20the%20same%20training%20algorithm%20to%20each%0Aindividual%20network%2C%20we%20propose%20using%20various%20loss%20functions%2C%20drawn%20from%20the%0Aanomaly%20detection%20field%2C%20to%20train%20each%20network.%20The%20methods%20are%20validated%20on%0Athe%20two%20largest%20publicly%20available%20datasets%20for%20video%20capsule%20endoscopy%20images%2C%0Athe%20Galar%20and%20the%20Kvasir-Capsule%20dataset.%20We%20achieve%20an%20AUC%20score%20of%2076.86%25%20on%0Athe%20Kvasir-Capsule%20and%20an%20AUC%20score%20of%2076.98%25%20on%20the%20Galar%20dataset.%20Our%0Aapproach%20outperforms%20current%20baselines%20with%20significantly%20fewer%20parameters%0Aacross%20all%20models%2C%20which%20is%20a%20crucial%20step%20towards%20incorporating%20artificial%0Aintelligence%20into%20capsule%20endoscopies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06039v2&entry.124074799=Read"},
{"title": "Understanding Data Influence with Differential Approximation", "author": "Haoru Tan and Sitong Wu and Xiuzhe Wu and Wang Wang and Bo Zhao and Zeke Xie and Gui-Song Xia and Xiaojuan Qi", "abstract": "  Data plays a pivotal role in the groundbreaking advancements in artificial\nintelligence. The quantitative analysis of data significantly contributes to\nmodel training, enhancing both the efficiency and quality of data utilization.\nHowever, existing data analysis tools often lag in accuracy. For instance, many\nof these tools even assume that the loss function of neural networks is convex.\nThese limitations make it challenging to implement current methods effectively.\nIn this paper, we introduce a new formulation to approximate a sample's\ninfluence by accumulating the differences in influence between consecutive\nlearning steps, which we term Diff-In. Specifically, we formulate the\nsample-wise influence as the cumulative sum of its changes/differences across\nsuccessive training iterations. By employing second-order approximations, we\napproximate these difference terms with high accuracy while eliminating the\nneed for model convexity required by existing methods. Despite being a\nsecond-order method, Diff-In maintains computational complexity comparable to\nthat of first-order methods and remains scalable. This efficiency is achieved\nby computing the product of the Hessian and gradient, which can be efficiently\napproximated using finite differences of first-order gradients. We assess the\napproximation accuracy of Diff-In both theoretically and empirically. Our\ntheoretical analysis demonstrates that Diff-In achieves significantly lower\napproximation error compared to existing influence estimators. Extensive\nexperiments further confirm its superior performance across multiple benchmark\ndatasets in three data-centric tasks: data cleaning, data deletion, and coreset\nselection. Notably, our experiments on data pruning for large-scale\nvision-language pre-training show that Diff-In can scale to millions of data\npoints and outperforms strong baselines.\n", "link": "http://arxiv.org/abs/2508.14648v1", "date": "2025-08-20", "relevancy": 2.0614, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5166}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5146}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Data%20Influence%20with%20Differential%20Approximation&body=Title%3A%20Understanding%20Data%20Influence%20with%20Differential%20Approximation%0AAuthor%3A%20Haoru%20Tan%20and%20Sitong%20Wu%20and%20Xiuzhe%20Wu%20and%20Wang%20Wang%20and%20Bo%20Zhao%20and%20Zeke%20Xie%20and%20Gui-Song%20Xia%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Data%20plays%20a%20pivotal%20role%20in%20the%20groundbreaking%20advancements%20in%20artificial%0Aintelligence.%20The%20quantitative%20analysis%20of%20data%20significantly%20contributes%20to%0Amodel%20training%2C%20enhancing%20both%20the%20efficiency%20and%20quality%20of%20data%20utilization.%0AHowever%2C%20existing%20data%20analysis%20tools%20often%20lag%20in%20accuracy.%20For%20instance%2C%20many%0Aof%20these%20tools%20even%20assume%20that%20the%20loss%20function%20of%20neural%20networks%20is%20convex.%0AThese%20limitations%20make%20it%20challenging%20to%20implement%20current%20methods%20effectively.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20formulation%20to%20approximate%20a%20sample%27s%0Ainfluence%20by%20accumulating%20the%20differences%20in%20influence%20between%20consecutive%0Alearning%20steps%2C%20which%20we%20term%20Diff-In.%20Specifically%2C%20we%20formulate%20the%0Asample-wise%20influence%20as%20the%20cumulative%20sum%20of%20its%20changes/differences%20across%0Asuccessive%20training%20iterations.%20By%20employing%20second-order%20approximations%2C%20we%0Aapproximate%20these%20difference%20terms%20with%20high%20accuracy%20while%20eliminating%20the%0Aneed%20for%20model%20convexity%20required%20by%20existing%20methods.%20Despite%20being%20a%0Asecond-order%20method%2C%20Diff-In%20maintains%20computational%20complexity%20comparable%20to%0Athat%20of%20first-order%20methods%20and%20remains%20scalable.%20This%20efficiency%20is%20achieved%0Aby%20computing%20the%20product%20of%20the%20Hessian%20and%20gradient%2C%20which%20can%20be%20efficiently%0Aapproximated%20using%20finite%20differences%20of%20first-order%20gradients.%20We%20assess%20the%0Aapproximation%20accuracy%20of%20Diff-In%20both%20theoretically%20and%20empirically.%20Our%0Atheoretical%20analysis%20demonstrates%20that%20Diff-In%20achieves%20significantly%20lower%0Aapproximation%20error%20compared%20to%20existing%20influence%20estimators.%20Extensive%0Aexperiments%20further%20confirm%20its%20superior%20performance%20across%20multiple%20benchmark%0Adatasets%20in%20three%20data-centric%20tasks%3A%20data%20cleaning%2C%20data%20deletion%2C%20and%20coreset%0Aselection.%20Notably%2C%20our%20experiments%20on%20data%20pruning%20for%20large-scale%0Avision-language%20pre-training%20show%20that%20Diff-In%20can%20scale%20to%20millions%20of%20data%0Apoints%20and%20outperforms%20strong%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Data%2520Influence%2520with%2520Differential%2520Approximation%26entry.906535625%3DHaoru%2520Tan%2520and%2520Sitong%2520Wu%2520and%2520Xiuzhe%2520Wu%2520and%2520Wang%2520Wang%2520and%2520Bo%2520Zhao%2520and%2520Zeke%2520Xie%2520and%2520Gui-Song%2520Xia%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Data%2520plays%2520a%2520pivotal%2520role%2520in%2520the%2520groundbreaking%2520advancements%2520in%2520artificial%250Aintelligence.%2520The%2520quantitative%2520analysis%2520of%2520data%2520significantly%2520contributes%2520to%250Amodel%2520training%252C%2520enhancing%2520both%2520the%2520efficiency%2520and%2520quality%2520of%2520data%2520utilization.%250AHowever%252C%2520existing%2520data%2520analysis%2520tools%2520often%2520lag%2520in%2520accuracy.%2520For%2520instance%252C%2520many%250Aof%2520these%2520tools%2520even%2520assume%2520that%2520the%2520loss%2520function%2520of%2520neural%2520networks%2520is%2520convex.%250AThese%2520limitations%2520make%2520it%2520challenging%2520to%2520implement%2520current%2520methods%2520effectively.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520formulation%2520to%2520approximate%2520a%2520sample%2527s%250Ainfluence%2520by%2520accumulating%2520the%2520differences%2520in%2520influence%2520between%2520consecutive%250Alearning%2520steps%252C%2520which%2520we%2520term%2520Diff-In.%2520Specifically%252C%2520we%2520formulate%2520the%250Asample-wise%2520influence%2520as%2520the%2520cumulative%2520sum%2520of%2520its%2520changes/differences%2520across%250Asuccessive%2520training%2520iterations.%2520By%2520employing%2520second-order%2520approximations%252C%2520we%250Aapproximate%2520these%2520difference%2520terms%2520with%2520high%2520accuracy%2520while%2520eliminating%2520the%250Aneed%2520for%2520model%2520convexity%2520required%2520by%2520existing%2520methods.%2520Despite%2520being%2520a%250Asecond-order%2520method%252C%2520Diff-In%2520maintains%2520computational%2520complexity%2520comparable%2520to%250Athat%2520of%2520first-order%2520methods%2520and%2520remains%2520scalable.%2520This%2520efficiency%2520is%2520achieved%250Aby%2520computing%2520the%2520product%2520of%2520the%2520Hessian%2520and%2520gradient%252C%2520which%2520can%2520be%2520efficiently%250Aapproximated%2520using%2520finite%2520differences%2520of%2520first-order%2520gradients.%2520We%2520assess%2520the%250Aapproximation%2520accuracy%2520of%2520Diff-In%2520both%2520theoretically%2520and%2520empirically.%2520Our%250Atheoretical%2520analysis%2520demonstrates%2520that%2520Diff-In%2520achieves%2520significantly%2520lower%250Aapproximation%2520error%2520compared%2520to%2520existing%2520influence%2520estimators.%2520Extensive%250Aexperiments%2520further%2520confirm%2520its%2520superior%2520performance%2520across%2520multiple%2520benchmark%250Adatasets%2520in%2520three%2520data-centric%2520tasks%253A%2520data%2520cleaning%252C%2520data%2520deletion%252C%2520and%2520coreset%250Aselection.%2520Notably%252C%2520our%2520experiments%2520on%2520data%2520pruning%2520for%2520large-scale%250Avision-language%2520pre-training%2520show%2520that%2520Diff-In%2520can%2520scale%2520to%2520millions%2520of%2520data%250Apoints%2520and%2520outperforms%2520strong%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Data%20Influence%20with%20Differential%20Approximation&entry.906535625=Haoru%20Tan%20and%20Sitong%20Wu%20and%20Xiuzhe%20Wu%20and%20Wang%20Wang%20and%20Bo%20Zhao%20and%20Zeke%20Xie%20and%20Gui-Song%20Xia%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Data%20plays%20a%20pivotal%20role%20in%20the%20groundbreaking%20advancements%20in%20artificial%0Aintelligence.%20The%20quantitative%20analysis%20of%20data%20significantly%20contributes%20to%0Amodel%20training%2C%20enhancing%20both%20the%20efficiency%20and%20quality%20of%20data%20utilization.%0AHowever%2C%20existing%20data%20analysis%20tools%20often%20lag%20in%20accuracy.%20For%20instance%2C%20many%0Aof%20these%20tools%20even%20assume%20that%20the%20loss%20function%20of%20neural%20networks%20is%20convex.%0AThese%20limitations%20make%20it%20challenging%20to%20implement%20current%20methods%20effectively.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20formulation%20to%20approximate%20a%20sample%27s%0Ainfluence%20by%20accumulating%20the%20differences%20in%20influence%20between%20consecutive%0Alearning%20steps%2C%20which%20we%20term%20Diff-In.%20Specifically%2C%20we%20formulate%20the%0Asample-wise%20influence%20as%20the%20cumulative%20sum%20of%20its%20changes/differences%20across%0Asuccessive%20training%20iterations.%20By%20employing%20second-order%20approximations%2C%20we%0Aapproximate%20these%20difference%20terms%20with%20high%20accuracy%20while%20eliminating%20the%0Aneed%20for%20model%20convexity%20required%20by%20existing%20methods.%20Despite%20being%20a%0Asecond-order%20method%2C%20Diff-In%20maintains%20computational%20complexity%20comparable%20to%0Athat%20of%20first-order%20methods%20and%20remains%20scalable.%20This%20efficiency%20is%20achieved%0Aby%20computing%20the%20product%20of%20the%20Hessian%20and%20gradient%2C%20which%20can%20be%20efficiently%0Aapproximated%20using%20finite%20differences%20of%20first-order%20gradients.%20We%20assess%20the%0Aapproximation%20accuracy%20of%20Diff-In%20both%20theoretically%20and%20empirically.%20Our%0Atheoretical%20analysis%20demonstrates%20that%20Diff-In%20achieves%20significantly%20lower%0Aapproximation%20error%20compared%20to%20existing%20influence%20estimators.%20Extensive%0Aexperiments%20further%20confirm%20its%20superior%20performance%20across%20multiple%20benchmark%0Adatasets%20in%20three%20data-centric%20tasks%3A%20data%20cleaning%2C%20data%20deletion%2C%20and%20coreset%0Aselection.%20Notably%2C%20our%20experiments%20on%20data%20pruning%20for%20large-scale%0Avision-language%20pre-training%20show%20that%20Diff-In%20can%20scale%20to%20millions%20of%20data%0Apoints%20and%20outperforms%20strong%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14648v1&entry.124074799=Read"},
{"title": "Redundant feature screening method for human activity recognition based\n  on attention purification mechanism", "author": "Xiaoyang Li and Yixuan Jiang and Junze Zhu and Haotian Tang and Dongchen Wu and Hanyu Liu and Chao Li", "abstract": "  In the field of sensor-based Human Activity Recognition (HAR), deep neural\nnetworks provide advanced technical support. Many studies have proven that\nrecognition accuracy can be improved by increasing the depth or width of the\nnetwork. However, for wearable devices, the balance between network performance\nand resource consumption is crucial. With minimum resource consumption as the\nbasic principle, we propose a universal attention feature purification\nmechanism, called MSAP, which is suitable for multi-scale networks. The\nmechanism effectively solves the feature redundancy caused by the superposition\nof multi-scale features by means of inter-scale attention screening and\nconnection method. In addition, we have designed a network correction module\nthat integrates seamlessly between layers of individual network modules to\nmitigate inherent problems in deep networks. We also built an embedded\ndeployment system that is in line with the current level of wearable technology\nto test the practical feasibility of the HAR model, and further prove the\nefficiency of the method. Extensive experiments on four public datasets show\nthat the proposed method model effectively reduces redundant features in\nfiltered data and provides excellent performance with little resource\nconsumption.\n", "link": "http://arxiv.org/abs/2503.23537v2", "date": "2025-08-20", "relevancy": 2.0555, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.539}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5079}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redundant%20feature%20screening%20method%20for%20human%20activity%20recognition%20based%0A%20%20on%20attention%20purification%20mechanism&body=Title%3A%20Redundant%20feature%20screening%20method%20for%20human%20activity%20recognition%20based%0A%20%20on%20attention%20purification%20mechanism%0AAuthor%3A%20Xiaoyang%20Li%20and%20Yixuan%20Jiang%20and%20Junze%20Zhu%20and%20Haotian%20Tang%20and%20Dongchen%20Wu%20and%20Hanyu%20Liu%20and%20Chao%20Li%0AAbstract%3A%20%20%20In%20the%20field%20of%20sensor-based%20Human%20Activity%20Recognition%20%28HAR%29%2C%20deep%20neural%0Anetworks%20provide%20advanced%20technical%20support.%20Many%20studies%20have%20proven%20that%0Arecognition%20accuracy%20can%20be%20improved%20by%20increasing%20the%20depth%20or%20width%20of%20the%0Anetwork.%20However%2C%20for%20wearable%20devices%2C%20the%20balance%20between%20network%20performance%0Aand%20resource%20consumption%20is%20crucial.%20With%20minimum%20resource%20consumption%20as%20the%0Abasic%20principle%2C%20we%20propose%20a%20universal%20attention%20feature%20purification%0Amechanism%2C%20called%20MSAP%2C%20which%20is%20suitable%20for%20multi-scale%20networks.%20The%0Amechanism%20effectively%20solves%20the%20feature%20redundancy%20caused%20by%20the%20superposition%0Aof%20multi-scale%20features%20by%20means%20of%20inter-scale%20attention%20screening%20and%0Aconnection%20method.%20In%20addition%2C%20we%20have%20designed%20a%20network%20correction%20module%0Athat%20integrates%20seamlessly%20between%20layers%20of%20individual%20network%20modules%20to%0Amitigate%20inherent%20problems%20in%20deep%20networks.%20We%20also%20built%20an%20embedded%0Adeployment%20system%20that%20is%20in%20line%20with%20the%20current%20level%20of%20wearable%20technology%0Ato%20test%20the%20practical%20feasibility%20of%20the%20HAR%20model%2C%20and%20further%20prove%20the%0Aefficiency%20of%20the%20method.%20Extensive%20experiments%20on%20four%20public%20datasets%20show%0Athat%20the%20proposed%20method%20model%20effectively%20reduces%20redundant%20features%20in%0Afiltered%20data%20and%20provides%20excellent%20performance%20with%20little%20resource%0Aconsumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23537v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedundant%2520feature%2520screening%2520method%2520for%2520human%2520activity%2520recognition%2520based%250A%2520%2520on%2520attention%2520purification%2520mechanism%26entry.906535625%3DXiaoyang%2520Li%2520and%2520Yixuan%2520Jiang%2520and%2520Junze%2520Zhu%2520and%2520Haotian%2520Tang%2520and%2520Dongchen%2520Wu%2520and%2520Hanyu%2520Liu%2520and%2520Chao%2520Li%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520sensor-based%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529%252C%2520deep%2520neural%250Anetworks%2520provide%2520advanced%2520technical%2520support.%2520Many%2520studies%2520have%2520proven%2520that%250Arecognition%2520accuracy%2520can%2520be%2520improved%2520by%2520increasing%2520the%2520depth%2520or%2520width%2520of%2520the%250Anetwork.%2520However%252C%2520for%2520wearable%2520devices%252C%2520the%2520balance%2520between%2520network%2520performance%250Aand%2520resource%2520consumption%2520is%2520crucial.%2520With%2520minimum%2520resource%2520consumption%2520as%2520the%250Abasic%2520principle%252C%2520we%2520propose%2520a%2520universal%2520attention%2520feature%2520purification%250Amechanism%252C%2520called%2520MSAP%252C%2520which%2520is%2520suitable%2520for%2520multi-scale%2520networks.%2520The%250Amechanism%2520effectively%2520solves%2520the%2520feature%2520redundancy%2520caused%2520by%2520the%2520superposition%250Aof%2520multi-scale%2520features%2520by%2520means%2520of%2520inter-scale%2520attention%2520screening%2520and%250Aconnection%2520method.%2520In%2520addition%252C%2520we%2520have%2520designed%2520a%2520network%2520correction%2520module%250Athat%2520integrates%2520seamlessly%2520between%2520layers%2520of%2520individual%2520network%2520modules%2520to%250Amitigate%2520inherent%2520problems%2520in%2520deep%2520networks.%2520We%2520also%2520built%2520an%2520embedded%250Adeployment%2520system%2520that%2520is%2520in%2520line%2520with%2520the%2520current%2520level%2520of%2520wearable%2520technology%250Ato%2520test%2520the%2520practical%2520feasibility%2520of%2520the%2520HAR%2520model%252C%2520and%2520further%2520prove%2520the%250Aefficiency%2520of%2520the%2520method.%2520Extensive%2520experiments%2520on%2520four%2520public%2520datasets%2520show%250Athat%2520the%2520proposed%2520method%2520model%2520effectively%2520reduces%2520redundant%2520features%2520in%250Afiltered%2520data%2520and%2520provides%2520excellent%2520performance%2520with%2520little%2520resource%250Aconsumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23537v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redundant%20feature%20screening%20method%20for%20human%20activity%20recognition%20based%0A%20%20on%20attention%20purification%20mechanism&entry.906535625=Xiaoyang%20Li%20and%20Yixuan%20Jiang%20and%20Junze%20Zhu%20and%20Haotian%20Tang%20and%20Dongchen%20Wu%20and%20Hanyu%20Liu%20and%20Chao%20Li&entry.1292438233=%20%20In%20the%20field%20of%20sensor-based%20Human%20Activity%20Recognition%20%28HAR%29%2C%20deep%20neural%0Anetworks%20provide%20advanced%20technical%20support.%20Many%20studies%20have%20proven%20that%0Arecognition%20accuracy%20can%20be%20improved%20by%20increasing%20the%20depth%20or%20width%20of%20the%0Anetwork.%20However%2C%20for%20wearable%20devices%2C%20the%20balance%20between%20network%20performance%0Aand%20resource%20consumption%20is%20crucial.%20With%20minimum%20resource%20consumption%20as%20the%0Abasic%20principle%2C%20we%20propose%20a%20universal%20attention%20feature%20purification%0Amechanism%2C%20called%20MSAP%2C%20which%20is%20suitable%20for%20multi-scale%20networks.%20The%0Amechanism%20effectively%20solves%20the%20feature%20redundancy%20caused%20by%20the%20superposition%0Aof%20multi-scale%20features%20by%20means%20of%20inter-scale%20attention%20screening%20and%0Aconnection%20method.%20In%20addition%2C%20we%20have%20designed%20a%20network%20correction%20module%0Athat%20integrates%20seamlessly%20between%20layers%20of%20individual%20network%20modules%20to%0Amitigate%20inherent%20problems%20in%20deep%20networks.%20We%20also%20built%20an%20embedded%0Adeployment%20system%20that%20is%20in%20line%20with%20the%20current%20level%20of%20wearable%20technology%0Ato%20test%20the%20practical%20feasibility%20of%20the%20HAR%20model%2C%20and%20further%20prove%20the%0Aefficiency%20of%20the%20method.%20Extensive%20experiments%20on%20four%20public%20datasets%20show%0Athat%20the%20proposed%20method%20model%20effectively%20reduces%20redundant%20features%20in%0Afiltered%20data%20and%20provides%20excellent%20performance%20with%20little%20resource%0Aconsumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23537v2&entry.124074799=Read"},
{"title": "Lifespan Pancreas Morphology for Control vs Type 2 Diabetes using AI on\n  Largescale Clinical Imaging", "author": "Lucas W. Remedios and Chloe Cho and Trent M. Schwartz and Dingjie Su and Gaurav Rudravaram and Chenyu Gao and Aravind R. Krishnan and Adam M. Saunders and Michael E. Kim and Shunxing Bao and Thomas A. Lasko and Alvin C. Powers and Bennett A. Landman and John Virostko", "abstract": "  Purpose: Understanding how the pancreas changes is critical for detecting\ndeviations in type 2 diabetes and other pancreatic disease. We measure pancreas\nsize and shape using morphological measurements from ages 0 to 90. Our goals\nare to 1) identify reliable clinical imaging modalities for AI-based pancreas\nmeasurement, 2) establish normative morphological aging trends, and 3) detect\npotential deviations in type 2 diabetes.\n  Approach: We analyzed a clinically acquired dataset of 2533 patients imaged\nwith abdominal CT or MRI. We resampled the scans to 3mm isotropic resolution,\nsegmented the pancreas using automated methods, and extracted 13 morphological\npancreas features across the lifespan. First, we assessed CT and MRI\nmeasurements to determine which modalities provide consistent lifespan trends.\nSecond, we characterized distributions of normative morphological patterns\nstratified by age group and sex. Third, we used GAMLSS regression to model\npancreas morphology trends in 1350 patients matched for age, sex, and type 2\ndiabetes status to identify any deviations from normative aging associated with\ntype 2 diabetes.\n  Results: When adjusting for confounders, the aging trends for 10 of 13\nmorphological features were significantly different between patients with type\n2 diabetes and non-diabetic controls (p < 0.05 after multiple comparisons\ncorrections). Additionally, MRI appeared to yield different pancreas\nmeasurements than CT using our AI-based method.\n  Conclusions: We provide lifespan trends demonstrating that the size and shape\nof the pancreas is altered in type 2 diabetes using 675 control patients and\n675 diabetes patients. Moreover, our findings reinforce that the pancreas is\nsmaller in type 2 diabetes. Additionally, we contribute a reference of lifespan\npancreas morphology from a large cohort of non-diabetic control patients in a\nclinical setting.\n", "link": "http://arxiv.org/abs/2508.14878v1", "date": "2025-08-20", "relevancy": 2.0406, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4073}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lifespan%20Pancreas%20Morphology%20for%20Control%20vs%20Type%202%20Diabetes%20using%20AI%20on%0A%20%20Largescale%20Clinical%20Imaging&body=Title%3A%20Lifespan%20Pancreas%20Morphology%20for%20Control%20vs%20Type%202%20Diabetes%20using%20AI%20on%0A%20%20Largescale%20Clinical%20Imaging%0AAuthor%3A%20Lucas%20W.%20Remedios%20and%20Chloe%20Cho%20and%20Trent%20M.%20Schwartz%20and%20Dingjie%20Su%20and%20Gaurav%20Rudravaram%20and%20Chenyu%20Gao%20and%20Aravind%20R.%20Krishnan%20and%20Adam%20M.%20Saunders%20and%20Michael%20E.%20Kim%20and%20Shunxing%20Bao%20and%20Thomas%20A.%20Lasko%20and%20Alvin%20C.%20Powers%20and%20Bennett%20A.%20Landman%20and%20John%20Virostko%0AAbstract%3A%20%20%20Purpose%3A%20Understanding%20how%20the%20pancreas%20changes%20is%20critical%20for%20detecting%0Adeviations%20in%20type%202%20diabetes%20and%20other%20pancreatic%20disease.%20We%20measure%20pancreas%0Asize%20and%20shape%20using%20morphological%20measurements%20from%20ages%200%20to%2090.%20Our%20goals%0Aare%20to%201%29%20identify%20reliable%20clinical%20imaging%20modalities%20for%20AI-based%20pancreas%0Ameasurement%2C%202%29%20establish%20normative%20morphological%20aging%20trends%2C%20and%203%29%20detect%0Apotential%20deviations%20in%20type%202%20diabetes.%0A%20%20Approach%3A%20We%20analyzed%20a%20clinically%20acquired%20dataset%20of%202533%20patients%20imaged%0Awith%20abdominal%20CT%20or%20MRI.%20We%20resampled%20the%20scans%20to%203mm%20isotropic%20resolution%2C%0Asegmented%20the%20pancreas%20using%20automated%20methods%2C%20and%20extracted%2013%20morphological%0Apancreas%20features%20across%20the%20lifespan.%20First%2C%20we%20assessed%20CT%20and%20MRI%0Ameasurements%20to%20determine%20which%20modalities%20provide%20consistent%20lifespan%20trends.%0ASecond%2C%20we%20characterized%20distributions%20of%20normative%20morphological%20patterns%0Astratified%20by%20age%20group%20and%20sex.%20Third%2C%20we%20used%20GAMLSS%20regression%20to%20model%0Apancreas%20morphology%20trends%20in%201350%20patients%20matched%20for%20age%2C%20sex%2C%20and%20type%202%0Adiabetes%20status%20to%20identify%20any%20deviations%20from%20normative%20aging%20associated%20with%0Atype%202%20diabetes.%0A%20%20Results%3A%20When%20adjusting%20for%20confounders%2C%20the%20aging%20trends%20for%2010%20of%2013%0Amorphological%20features%20were%20significantly%20different%20between%20patients%20with%20type%0A2%20diabetes%20and%20non-diabetic%20controls%20%28p%20%3C%200.05%20after%20multiple%20comparisons%0Acorrections%29.%20Additionally%2C%20MRI%20appeared%20to%20yield%20different%20pancreas%0Ameasurements%20than%20CT%20using%20our%20AI-based%20method.%0A%20%20Conclusions%3A%20We%20provide%20lifespan%20trends%20demonstrating%20that%20the%20size%20and%20shape%0Aof%20the%20pancreas%20is%20altered%20in%20type%202%20diabetes%20using%20675%20control%20patients%20and%0A675%20diabetes%20patients.%20Moreover%2C%20our%20findings%20reinforce%20that%20the%20pancreas%20is%0Asmaller%20in%20type%202%20diabetes.%20Additionally%2C%20we%20contribute%20a%20reference%20of%20lifespan%0Apancreas%20morphology%20from%20a%20large%20cohort%20of%20non-diabetic%20control%20patients%20in%20a%0Aclinical%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLifespan%2520Pancreas%2520Morphology%2520for%2520Control%2520vs%2520Type%25202%2520Diabetes%2520using%2520AI%2520on%250A%2520%2520Largescale%2520Clinical%2520Imaging%26entry.906535625%3DLucas%2520W.%2520Remedios%2520and%2520Chloe%2520Cho%2520and%2520Trent%2520M.%2520Schwartz%2520and%2520Dingjie%2520Su%2520and%2520Gaurav%2520Rudravaram%2520and%2520Chenyu%2520Gao%2520and%2520Aravind%2520R.%2520Krishnan%2520and%2520Adam%2520M.%2520Saunders%2520and%2520Michael%2520E.%2520Kim%2520and%2520Shunxing%2520Bao%2520and%2520Thomas%2520A.%2520Lasko%2520and%2520Alvin%2520C.%2520Powers%2520and%2520Bennett%2520A.%2520Landman%2520and%2520John%2520Virostko%26entry.1292438233%3D%2520%2520Purpose%253A%2520Understanding%2520how%2520the%2520pancreas%2520changes%2520is%2520critical%2520for%2520detecting%250Adeviations%2520in%2520type%25202%2520diabetes%2520and%2520other%2520pancreatic%2520disease.%2520We%2520measure%2520pancreas%250Asize%2520and%2520shape%2520using%2520morphological%2520measurements%2520from%2520ages%25200%2520to%252090.%2520Our%2520goals%250Aare%2520to%25201%2529%2520identify%2520reliable%2520clinical%2520imaging%2520modalities%2520for%2520AI-based%2520pancreas%250Ameasurement%252C%25202%2529%2520establish%2520normative%2520morphological%2520aging%2520trends%252C%2520and%25203%2529%2520detect%250Apotential%2520deviations%2520in%2520type%25202%2520diabetes.%250A%2520%2520Approach%253A%2520We%2520analyzed%2520a%2520clinically%2520acquired%2520dataset%2520of%25202533%2520patients%2520imaged%250Awith%2520abdominal%2520CT%2520or%2520MRI.%2520We%2520resampled%2520the%2520scans%2520to%25203mm%2520isotropic%2520resolution%252C%250Asegmented%2520the%2520pancreas%2520using%2520automated%2520methods%252C%2520and%2520extracted%252013%2520morphological%250Apancreas%2520features%2520across%2520the%2520lifespan.%2520First%252C%2520we%2520assessed%2520CT%2520and%2520MRI%250Ameasurements%2520to%2520determine%2520which%2520modalities%2520provide%2520consistent%2520lifespan%2520trends.%250ASecond%252C%2520we%2520characterized%2520distributions%2520of%2520normative%2520morphological%2520patterns%250Astratified%2520by%2520age%2520group%2520and%2520sex.%2520Third%252C%2520we%2520used%2520GAMLSS%2520regression%2520to%2520model%250Apancreas%2520morphology%2520trends%2520in%25201350%2520patients%2520matched%2520for%2520age%252C%2520sex%252C%2520and%2520type%25202%250Adiabetes%2520status%2520to%2520identify%2520any%2520deviations%2520from%2520normative%2520aging%2520associated%2520with%250Atype%25202%2520diabetes.%250A%2520%2520Results%253A%2520When%2520adjusting%2520for%2520confounders%252C%2520the%2520aging%2520trends%2520for%252010%2520of%252013%250Amorphological%2520features%2520were%2520significantly%2520different%2520between%2520patients%2520with%2520type%250A2%2520diabetes%2520and%2520non-diabetic%2520controls%2520%2528p%2520%253C%25200.05%2520after%2520multiple%2520comparisons%250Acorrections%2529.%2520Additionally%252C%2520MRI%2520appeared%2520to%2520yield%2520different%2520pancreas%250Ameasurements%2520than%2520CT%2520using%2520our%2520AI-based%2520method.%250A%2520%2520Conclusions%253A%2520We%2520provide%2520lifespan%2520trends%2520demonstrating%2520that%2520the%2520size%2520and%2520shape%250Aof%2520the%2520pancreas%2520is%2520altered%2520in%2520type%25202%2520diabetes%2520using%2520675%2520control%2520patients%2520and%250A675%2520diabetes%2520patients.%2520Moreover%252C%2520our%2520findings%2520reinforce%2520that%2520the%2520pancreas%2520is%250Asmaller%2520in%2520type%25202%2520diabetes.%2520Additionally%252C%2520we%2520contribute%2520a%2520reference%2520of%2520lifespan%250Apancreas%2520morphology%2520from%2520a%2520large%2520cohort%2520of%2520non-diabetic%2520control%2520patients%2520in%2520a%250Aclinical%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lifespan%20Pancreas%20Morphology%20for%20Control%20vs%20Type%202%20Diabetes%20using%20AI%20on%0A%20%20Largescale%20Clinical%20Imaging&entry.906535625=Lucas%20W.%20Remedios%20and%20Chloe%20Cho%20and%20Trent%20M.%20Schwartz%20and%20Dingjie%20Su%20and%20Gaurav%20Rudravaram%20and%20Chenyu%20Gao%20and%20Aravind%20R.%20Krishnan%20and%20Adam%20M.%20Saunders%20and%20Michael%20E.%20Kim%20and%20Shunxing%20Bao%20and%20Thomas%20A.%20Lasko%20and%20Alvin%20C.%20Powers%20and%20Bennett%20A.%20Landman%20and%20John%20Virostko&entry.1292438233=%20%20Purpose%3A%20Understanding%20how%20the%20pancreas%20changes%20is%20critical%20for%20detecting%0Adeviations%20in%20type%202%20diabetes%20and%20other%20pancreatic%20disease.%20We%20measure%20pancreas%0Asize%20and%20shape%20using%20morphological%20measurements%20from%20ages%200%20to%2090.%20Our%20goals%0Aare%20to%201%29%20identify%20reliable%20clinical%20imaging%20modalities%20for%20AI-based%20pancreas%0Ameasurement%2C%202%29%20establish%20normative%20morphological%20aging%20trends%2C%20and%203%29%20detect%0Apotential%20deviations%20in%20type%202%20diabetes.%0A%20%20Approach%3A%20We%20analyzed%20a%20clinically%20acquired%20dataset%20of%202533%20patients%20imaged%0Awith%20abdominal%20CT%20or%20MRI.%20We%20resampled%20the%20scans%20to%203mm%20isotropic%20resolution%2C%0Asegmented%20the%20pancreas%20using%20automated%20methods%2C%20and%20extracted%2013%20morphological%0Apancreas%20features%20across%20the%20lifespan.%20First%2C%20we%20assessed%20CT%20and%20MRI%0Ameasurements%20to%20determine%20which%20modalities%20provide%20consistent%20lifespan%20trends.%0ASecond%2C%20we%20characterized%20distributions%20of%20normative%20morphological%20patterns%0Astratified%20by%20age%20group%20and%20sex.%20Third%2C%20we%20used%20GAMLSS%20regression%20to%20model%0Apancreas%20morphology%20trends%20in%201350%20patients%20matched%20for%20age%2C%20sex%2C%20and%20type%202%0Adiabetes%20status%20to%20identify%20any%20deviations%20from%20normative%20aging%20associated%20with%0Atype%202%20diabetes.%0A%20%20Results%3A%20When%20adjusting%20for%20confounders%2C%20the%20aging%20trends%20for%2010%20of%2013%0Amorphological%20features%20were%20significantly%20different%20between%20patients%20with%20type%0A2%20diabetes%20and%20non-diabetic%20controls%20%28p%20%3C%200.05%20after%20multiple%20comparisons%0Acorrections%29.%20Additionally%2C%20MRI%20appeared%20to%20yield%20different%20pancreas%0Ameasurements%20than%20CT%20using%20our%20AI-based%20method.%0A%20%20Conclusions%3A%20We%20provide%20lifespan%20trends%20demonstrating%20that%20the%20size%20and%20shape%0Aof%20the%20pancreas%20is%20altered%20in%20type%202%20diabetes%20using%20675%20control%20patients%20and%0A675%20diabetes%20patients.%20Moreover%2C%20our%20findings%20reinforce%20that%20the%20pancreas%20is%0Asmaller%20in%20type%202%20diabetes.%20Additionally%2C%20we%20contribute%20a%20reference%20of%20lifespan%0Apancreas%20morphology%20from%20a%20large%20cohort%20of%20non-diabetic%20control%20patients%20in%20a%0Aclinical%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14878v1&entry.124074799=Read"},
{"title": "Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided\n  Refinement", "author": "Zhihan Zhang and Yixin Cao and Lizi Liao", "abstract": "  Translating chart images into executable plotting scripts-referred to as the\nchart-to-code generation task-requires Multimodal Large Language Models (MLLMs)\nto perform fine-grained visual parsing, precise code synthesis, and robust\ncross-modal reasoning. However, this task is inherently under-constrained:\nmultiple valid code implementations can produce the same visual chart, and\nevaluation must consider both code correctness and visual fidelity across\ndiverse dimensions. This makes it difficult to learn accurate and generalizable\nmappings through standard supervised fine-tuning. To address these challenges,\nwe propose a dual preference-guided refinement framework that combines a\nfeedback-driven, dual-modality reward mechanism with iterative preference\nlearning. Our approach introduces a structured variant generation strategy and\na visual reward model to efficiently produce high-quality, aspect-aware\npreference pairs-making preference collection scalable and supervision more\ntargeted. These preferences are used in an offline reinforcement learning setup\nto optimize the model toward multi-dimensional fidelity. Experimental results\nshow that our framework significantly enhances the performance of\ngeneral-purpose open-source MLLMs, enabling them to generate high-quality\nplotting code that rivals specialized chart-centric models and even some\nproprietary systems. The code and datasets are publicly available at\nhttps://github.com/Zhihan72/Chart2Code.\n", "link": "http://arxiv.org/abs/2504.02906v2", "date": "2025-08-20", "relevancy": 2.0376, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5035}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Chart-to-Code%20Generation%20in%20MLLM%20via%20Dual%20Preference-Guided%0A%20%20Refinement&body=Title%3A%20Boosting%20Chart-to-Code%20Generation%20in%20MLLM%20via%20Dual%20Preference-Guided%0A%20%20Refinement%0AAuthor%3A%20Zhihan%20Zhang%20and%20Yixin%20Cao%20and%20Lizi%20Liao%0AAbstract%3A%20%20%20Translating%20chart%20images%20into%20executable%20plotting%20scripts-referred%20to%20as%20the%0Achart-to-code%20generation%20task-requires%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Ato%20perform%20fine-grained%20visual%20parsing%2C%20precise%20code%20synthesis%2C%20and%20robust%0Across-modal%20reasoning.%20However%2C%20this%20task%20is%20inherently%20under-constrained%3A%0Amultiple%20valid%20code%20implementations%20can%20produce%20the%20same%20visual%20chart%2C%20and%0Aevaluation%20must%20consider%20both%20code%20correctness%20and%20visual%20fidelity%20across%0Adiverse%20dimensions.%20This%20makes%20it%20difficult%20to%20learn%20accurate%20and%20generalizable%0Amappings%20through%20standard%20supervised%20fine-tuning.%20To%20address%20these%20challenges%2C%0Awe%20propose%20a%20dual%20preference-guided%20refinement%20framework%20that%20combines%20a%0Afeedback-driven%2C%20dual-modality%20reward%20mechanism%20with%20iterative%20preference%0Alearning.%20Our%20approach%20introduces%20a%20structured%20variant%20generation%20strategy%20and%0Aa%20visual%20reward%20model%20to%20efficiently%20produce%20high-quality%2C%20aspect-aware%0Apreference%20pairs-making%20preference%20collection%20scalable%20and%20supervision%20more%0Atargeted.%20These%20preferences%20are%20used%20in%20an%20offline%20reinforcement%20learning%20setup%0Ato%20optimize%20the%20model%20toward%20multi-dimensional%20fidelity.%20Experimental%20results%0Ashow%20that%20our%20framework%20significantly%20enhances%20the%20performance%20of%0Ageneral-purpose%20open-source%20MLLMs%2C%20enabling%20them%20to%20generate%20high-quality%0Aplotting%20code%20that%20rivals%20specialized%20chart-centric%20models%20and%20even%20some%0Aproprietary%20systems.%20The%20code%20and%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//github.com/Zhihan72/Chart2Code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02906v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Chart-to-Code%2520Generation%2520in%2520MLLM%2520via%2520Dual%2520Preference-Guided%250A%2520%2520Refinement%26entry.906535625%3DZhihan%2520Zhang%2520and%2520Yixin%2520Cao%2520and%2520Lizi%2520Liao%26entry.1292438233%3D%2520%2520Translating%2520chart%2520images%2520into%2520executable%2520plotting%2520scripts-referred%2520to%2520as%2520the%250Achart-to-code%2520generation%2520task-requires%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%250Ato%2520perform%2520fine-grained%2520visual%2520parsing%252C%2520precise%2520code%2520synthesis%252C%2520and%2520robust%250Across-modal%2520reasoning.%2520However%252C%2520this%2520task%2520is%2520inherently%2520under-constrained%253A%250Amultiple%2520valid%2520code%2520implementations%2520can%2520produce%2520the%2520same%2520visual%2520chart%252C%2520and%250Aevaluation%2520must%2520consider%2520both%2520code%2520correctness%2520and%2520visual%2520fidelity%2520across%250Adiverse%2520dimensions.%2520This%2520makes%2520it%2520difficult%2520to%2520learn%2520accurate%2520and%2520generalizable%250Amappings%2520through%2520standard%2520supervised%2520fine-tuning.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520a%2520dual%2520preference-guided%2520refinement%2520framework%2520that%2520combines%2520a%250Afeedback-driven%252C%2520dual-modality%2520reward%2520mechanism%2520with%2520iterative%2520preference%250Alearning.%2520Our%2520approach%2520introduces%2520a%2520structured%2520variant%2520generation%2520strategy%2520and%250Aa%2520visual%2520reward%2520model%2520to%2520efficiently%2520produce%2520high-quality%252C%2520aspect-aware%250Apreference%2520pairs-making%2520preference%2520collection%2520scalable%2520and%2520supervision%2520more%250Atargeted.%2520These%2520preferences%2520are%2520used%2520in%2520an%2520offline%2520reinforcement%2520learning%2520setup%250Ato%2520optimize%2520the%2520model%2520toward%2520multi-dimensional%2520fidelity.%2520Experimental%2520results%250Ashow%2520that%2520our%2520framework%2520significantly%2520enhances%2520the%2520performance%2520of%250Ageneral-purpose%2520open-source%2520MLLMs%252C%2520enabling%2520them%2520to%2520generate%2520high-quality%250Aplotting%2520code%2520that%2520rivals%2520specialized%2520chart-centric%2520models%2520and%2520even%2520some%250Aproprietary%2520systems.%2520The%2520code%2520and%2520datasets%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Zhihan72/Chart2Code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02906v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Chart-to-Code%20Generation%20in%20MLLM%20via%20Dual%20Preference-Guided%0A%20%20Refinement&entry.906535625=Zhihan%20Zhang%20and%20Yixin%20Cao%20and%20Lizi%20Liao&entry.1292438233=%20%20Translating%20chart%20images%20into%20executable%20plotting%20scripts-referred%20to%20as%20the%0Achart-to-code%20generation%20task-requires%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Ato%20perform%20fine-grained%20visual%20parsing%2C%20precise%20code%20synthesis%2C%20and%20robust%0Across-modal%20reasoning.%20However%2C%20this%20task%20is%20inherently%20under-constrained%3A%0Amultiple%20valid%20code%20implementations%20can%20produce%20the%20same%20visual%20chart%2C%20and%0Aevaluation%20must%20consider%20both%20code%20correctness%20and%20visual%20fidelity%20across%0Adiverse%20dimensions.%20This%20makes%20it%20difficult%20to%20learn%20accurate%20and%20generalizable%0Amappings%20through%20standard%20supervised%20fine-tuning.%20To%20address%20these%20challenges%2C%0Awe%20propose%20a%20dual%20preference-guided%20refinement%20framework%20that%20combines%20a%0Afeedback-driven%2C%20dual-modality%20reward%20mechanism%20with%20iterative%20preference%0Alearning.%20Our%20approach%20introduces%20a%20structured%20variant%20generation%20strategy%20and%0Aa%20visual%20reward%20model%20to%20efficiently%20produce%20high-quality%2C%20aspect-aware%0Apreference%20pairs-making%20preference%20collection%20scalable%20and%20supervision%20more%0Atargeted.%20These%20preferences%20are%20used%20in%20an%20offline%20reinforcement%20learning%20setup%0Ato%20optimize%20the%20model%20toward%20multi-dimensional%20fidelity.%20Experimental%20results%0Ashow%20that%20our%20framework%20significantly%20enhances%20the%20performance%20of%0Ageneral-purpose%20open-source%20MLLMs%2C%20enabling%20them%20to%20generate%20high-quality%0Aplotting%20code%20that%20rivals%20specialized%20chart-centric%20models%20and%20even%20some%0Aproprietary%20systems.%20The%20code%20and%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//github.com/Zhihan72/Chart2Code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02906v2&entry.124074799=Read"},
{"title": "Source-Guided Flow Matching", "author": "Zifan Wang and Alice Harting and Matthieu Barreau and Michael M. Zavlanos and Karl H. Johansson", "abstract": "  Guidance of generative models is typically achieved by modifying the\nprobability flow vector field through the addition of a guidance field. In this\npaper, we instead propose the Source-Guided Flow Matching (SGFM) framework,\nwhich modifies the source distribution directly while keeping the pre-trained\nvector field intact. This reduces the guidance problem to a well-defined\nproblem of sampling from the source distribution. We theoretically show that\nSGFM recovers the desired target distribution exactly. Furthermore, we provide\nbounds on the Wasserstein error for the generated distribution when using an\napproximate sampler of the source distribution and an approximate vector field.\nThe key benefit of our approach is that it allows the user to flexibly choose\nthe sampling method depending on their specific problem. To illustrate this, we\nsystematically compare different sampling methods and discuss conditions for\nasymptotically exact guidance. Moreover, our framework integrates well with\noptimal flow matching models since the straight transport map generated by the\nvector field is preserved. Experimental results on synthetic 2D benchmarks,\nimage datasets, and physics-informed generative tasks demonstrate the\neffectiveness and flexibility of the proposed framework.\n", "link": "http://arxiv.org/abs/2508.14807v1", "date": "2025-08-20", "relevancy": 2.0322, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5647}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4999}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source-Guided%20Flow%20Matching&body=Title%3A%20Source-Guided%20Flow%20Matching%0AAuthor%3A%20Zifan%20Wang%20and%20Alice%20Harting%20and%20Matthieu%20Barreau%20and%20Michael%20M.%20Zavlanos%20and%20Karl%20H.%20Johansson%0AAbstract%3A%20%20%20Guidance%20of%20generative%20models%20is%20typically%20achieved%20by%20modifying%20the%0Aprobability%20flow%20vector%20field%20through%20the%20addition%20of%20a%20guidance%20field.%20In%20this%0Apaper%2C%20we%20instead%20propose%20the%20Source-Guided%20Flow%20Matching%20%28SGFM%29%20framework%2C%0Awhich%20modifies%20the%20source%20distribution%20directly%20while%20keeping%20the%20pre-trained%0Avector%20field%20intact.%20This%20reduces%20the%20guidance%20problem%20to%20a%20well-defined%0Aproblem%20of%20sampling%20from%20the%20source%20distribution.%20We%20theoretically%20show%20that%0ASGFM%20recovers%20the%20desired%20target%20distribution%20exactly.%20Furthermore%2C%20we%20provide%0Abounds%20on%20the%20Wasserstein%20error%20for%20the%20generated%20distribution%20when%20using%20an%0Aapproximate%20sampler%20of%20the%20source%20distribution%20and%20an%20approximate%20vector%20field.%0AThe%20key%20benefit%20of%20our%20approach%20is%20that%20it%20allows%20the%20user%20to%20flexibly%20choose%0Athe%20sampling%20method%20depending%20on%20their%20specific%20problem.%20To%20illustrate%20this%2C%20we%0Asystematically%20compare%20different%20sampling%20methods%20and%20discuss%20conditions%20for%0Aasymptotically%20exact%20guidance.%20Moreover%2C%20our%20framework%20integrates%20well%20with%0Aoptimal%20flow%20matching%20models%20since%20the%20straight%20transport%20map%20generated%20by%20the%0Avector%20field%20is%20preserved.%20Experimental%20results%20on%20synthetic%202D%20benchmarks%2C%0Aimage%20datasets%2C%20and%20physics-informed%20generative%20tasks%20demonstrate%20the%0Aeffectiveness%20and%20flexibility%20of%20the%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource-Guided%2520Flow%2520Matching%26entry.906535625%3DZifan%2520Wang%2520and%2520Alice%2520Harting%2520and%2520Matthieu%2520Barreau%2520and%2520Michael%2520M.%2520Zavlanos%2520and%2520Karl%2520H.%2520Johansson%26entry.1292438233%3D%2520%2520Guidance%2520of%2520generative%2520models%2520is%2520typically%2520achieved%2520by%2520modifying%2520the%250Aprobability%2520flow%2520vector%2520field%2520through%2520the%2520addition%2520of%2520a%2520guidance%2520field.%2520In%2520this%250Apaper%252C%2520we%2520instead%2520propose%2520the%2520Source-Guided%2520Flow%2520Matching%2520%2528SGFM%2529%2520framework%252C%250Awhich%2520modifies%2520the%2520source%2520distribution%2520directly%2520while%2520keeping%2520the%2520pre-trained%250Avector%2520field%2520intact.%2520This%2520reduces%2520the%2520guidance%2520problem%2520to%2520a%2520well-defined%250Aproblem%2520of%2520sampling%2520from%2520the%2520source%2520distribution.%2520We%2520theoretically%2520show%2520that%250ASGFM%2520recovers%2520the%2520desired%2520target%2520distribution%2520exactly.%2520Furthermore%252C%2520we%2520provide%250Abounds%2520on%2520the%2520Wasserstein%2520error%2520for%2520the%2520generated%2520distribution%2520when%2520using%2520an%250Aapproximate%2520sampler%2520of%2520the%2520source%2520distribution%2520and%2520an%2520approximate%2520vector%2520field.%250AThe%2520key%2520benefit%2520of%2520our%2520approach%2520is%2520that%2520it%2520allows%2520the%2520user%2520to%2520flexibly%2520choose%250Athe%2520sampling%2520method%2520depending%2520on%2520their%2520specific%2520problem.%2520To%2520illustrate%2520this%252C%2520we%250Asystematically%2520compare%2520different%2520sampling%2520methods%2520and%2520discuss%2520conditions%2520for%250Aasymptotically%2520exact%2520guidance.%2520Moreover%252C%2520our%2520framework%2520integrates%2520well%2520with%250Aoptimal%2520flow%2520matching%2520models%2520since%2520the%2520straight%2520transport%2520map%2520generated%2520by%2520the%250Avector%2520field%2520is%2520preserved.%2520Experimental%2520results%2520on%2520synthetic%25202D%2520benchmarks%252C%250Aimage%2520datasets%252C%2520and%2520physics-informed%2520generative%2520tasks%2520demonstrate%2520the%250Aeffectiveness%2520and%2520flexibility%2520of%2520the%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source-Guided%20Flow%20Matching&entry.906535625=Zifan%20Wang%20and%20Alice%20Harting%20and%20Matthieu%20Barreau%20and%20Michael%20M.%20Zavlanos%20and%20Karl%20H.%20Johansson&entry.1292438233=%20%20Guidance%20of%20generative%20models%20is%20typically%20achieved%20by%20modifying%20the%0Aprobability%20flow%20vector%20field%20through%20the%20addition%20of%20a%20guidance%20field.%20In%20this%0Apaper%2C%20we%20instead%20propose%20the%20Source-Guided%20Flow%20Matching%20%28SGFM%29%20framework%2C%0Awhich%20modifies%20the%20source%20distribution%20directly%20while%20keeping%20the%20pre-trained%0Avector%20field%20intact.%20This%20reduces%20the%20guidance%20problem%20to%20a%20well-defined%0Aproblem%20of%20sampling%20from%20the%20source%20distribution.%20We%20theoretically%20show%20that%0ASGFM%20recovers%20the%20desired%20target%20distribution%20exactly.%20Furthermore%2C%20we%20provide%0Abounds%20on%20the%20Wasserstein%20error%20for%20the%20generated%20distribution%20when%20using%20an%0Aapproximate%20sampler%20of%20the%20source%20distribution%20and%20an%20approximate%20vector%20field.%0AThe%20key%20benefit%20of%20our%20approach%20is%20that%20it%20allows%20the%20user%20to%20flexibly%20choose%0Athe%20sampling%20method%20depending%20on%20their%20specific%20problem.%20To%20illustrate%20this%2C%20we%0Asystematically%20compare%20different%20sampling%20methods%20and%20discuss%20conditions%20for%0Aasymptotically%20exact%20guidance.%20Moreover%2C%20our%20framework%20integrates%20well%20with%0Aoptimal%20flow%20matching%20models%20since%20the%20straight%20transport%20map%20generated%20by%20the%0Avector%20field%20is%20preserved.%20Experimental%20results%20on%20synthetic%202D%20benchmarks%2C%0Aimage%20datasets%2C%20and%20physics-informed%20generative%20tasks%20demonstrate%20the%0Aeffectiveness%20and%20flexibility%20of%20the%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14807v1&entry.124074799=Read"},
{"title": "MetaWild: A Multimodal Dataset for Animal Re-Identification with\n  Environmental Metadata", "author": "Yuzhuo Li and Di Zhao and Tingrui Qiao and Yihao Wu and Bo Pang and Yun Sing Koh", "abstract": "  Identifying individual animals within large wildlife populations is essential\nfor effective wildlife monitoring and conservation efforts. Recent advancements\nin computer vision have shown promise in animal re-identification (Animal ReID)\nby leveraging data from camera traps. However, existing Animal ReID datasets\nrely exclusively on visual data, overlooking environmental metadata that\necologists have identified as highly correlated with animal behavior and\nidentity, such as temperature and circadian rhythms. Moreover, the emergence of\nmultimodal models capable of jointly processing visual and textual data\npresents new opportunities for Animal ReID, but existing datasets fail to\nleverage these models' text-processing capabilities, limiting their full\npotential. Additionally, to facilitate the use of metadata in existing ReID\nmethods, we propose the Meta-Feature Adapter (MFA), a lightweight module that\ncan be incorporated into existing vision-language model (VLM)-based Animal ReID\nmethods, allowing ReID models to leverage both environmental metadata and\nvisual information to improve ReID performance. Experiments on MetaWild show\nthat combining baseline ReID models with MFA to incorporate metadata\nconsistently improves performance compared to using visual information alone,\nvalidating the effectiveness of incorporating metadata in re-identification. We\nhope that our proposed dataset can inspire further exploration of multimodal\napproaches for Animal ReID.\n", "link": "http://arxiv.org/abs/2501.13368v2", "date": "2025-08-20", "relevancy": 2.0304, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4998}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaWild%3A%20A%20Multimodal%20Dataset%20for%20Animal%20Re-Identification%20with%0A%20%20Environmental%20Metadata&body=Title%3A%20MetaWild%3A%20A%20Multimodal%20Dataset%20for%20Animal%20Re-Identification%20with%0A%20%20Environmental%20Metadata%0AAuthor%3A%20Yuzhuo%20Li%20and%20Di%20Zhao%20and%20Tingrui%20Qiao%20and%20Yihao%20Wu%20and%20Bo%20Pang%20and%20Yun%20Sing%20Koh%0AAbstract%3A%20%20%20Identifying%20individual%20animals%20within%20large%20wildlife%20populations%20is%20essential%0Afor%20effective%20wildlife%20monitoring%20and%20conservation%20efforts.%20Recent%20advancements%0Ain%20computer%20vision%20have%20shown%20promise%20in%20animal%20re-identification%20%28Animal%20ReID%29%0Aby%20leveraging%20data%20from%20camera%20traps.%20However%2C%20existing%20Animal%20ReID%20datasets%0Arely%20exclusively%20on%20visual%20data%2C%20overlooking%20environmental%20metadata%20that%0Aecologists%20have%20identified%20as%20highly%20correlated%20with%20animal%20behavior%20and%0Aidentity%2C%20such%20as%20temperature%20and%20circadian%20rhythms.%20Moreover%2C%20the%20emergence%20of%0Amultimodal%20models%20capable%20of%20jointly%20processing%20visual%20and%20textual%20data%0Apresents%20new%20opportunities%20for%20Animal%20ReID%2C%20but%20existing%20datasets%20fail%20to%0Aleverage%20these%20models%27%20text-processing%20capabilities%2C%20limiting%20their%20full%0Apotential.%20Additionally%2C%20to%20facilitate%20the%20use%20of%20metadata%20in%20existing%20ReID%0Amethods%2C%20we%20propose%20the%20Meta-Feature%20Adapter%20%28MFA%29%2C%20a%20lightweight%20module%20that%0Acan%20be%20incorporated%20into%20existing%20vision-language%20model%20%28VLM%29-based%20Animal%20ReID%0Amethods%2C%20allowing%20ReID%20models%20to%20leverage%20both%20environmental%20metadata%20and%0Avisual%20information%20to%20improve%20ReID%20performance.%20Experiments%20on%20MetaWild%20show%0Athat%20combining%20baseline%20ReID%20models%20with%20MFA%20to%20incorporate%20metadata%0Aconsistently%20improves%20performance%20compared%20to%20using%20visual%20information%20alone%2C%0Avalidating%20the%20effectiveness%20of%20incorporating%20metadata%20in%20re-identification.%20We%0Ahope%20that%20our%20proposed%20dataset%20can%20inspire%20further%20exploration%20of%20multimodal%0Aapproaches%20for%20Animal%20ReID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaWild%253A%2520A%2520Multimodal%2520Dataset%2520for%2520Animal%2520Re-Identification%2520with%250A%2520%2520Environmental%2520Metadata%26entry.906535625%3DYuzhuo%2520Li%2520and%2520Di%2520Zhao%2520and%2520Tingrui%2520Qiao%2520and%2520Yihao%2520Wu%2520and%2520Bo%2520Pang%2520and%2520Yun%2520Sing%2520Koh%26entry.1292438233%3D%2520%2520Identifying%2520individual%2520animals%2520within%2520large%2520wildlife%2520populations%2520is%2520essential%250Afor%2520effective%2520wildlife%2520monitoring%2520and%2520conservation%2520efforts.%2520Recent%2520advancements%250Ain%2520computer%2520vision%2520have%2520shown%2520promise%2520in%2520animal%2520re-identification%2520%2528Animal%2520ReID%2529%250Aby%2520leveraging%2520data%2520from%2520camera%2520traps.%2520However%252C%2520existing%2520Animal%2520ReID%2520datasets%250Arely%2520exclusively%2520on%2520visual%2520data%252C%2520overlooking%2520environmental%2520metadata%2520that%250Aecologists%2520have%2520identified%2520as%2520highly%2520correlated%2520with%2520animal%2520behavior%2520and%250Aidentity%252C%2520such%2520as%2520temperature%2520and%2520circadian%2520rhythms.%2520Moreover%252C%2520the%2520emergence%2520of%250Amultimodal%2520models%2520capable%2520of%2520jointly%2520processing%2520visual%2520and%2520textual%2520data%250Apresents%2520new%2520opportunities%2520for%2520Animal%2520ReID%252C%2520but%2520existing%2520datasets%2520fail%2520to%250Aleverage%2520these%2520models%2527%2520text-processing%2520capabilities%252C%2520limiting%2520their%2520full%250Apotential.%2520Additionally%252C%2520to%2520facilitate%2520the%2520use%2520of%2520metadata%2520in%2520existing%2520ReID%250Amethods%252C%2520we%2520propose%2520the%2520Meta-Feature%2520Adapter%2520%2528MFA%2529%252C%2520a%2520lightweight%2520module%2520that%250Acan%2520be%2520incorporated%2520into%2520existing%2520vision-language%2520model%2520%2528VLM%2529-based%2520Animal%2520ReID%250Amethods%252C%2520allowing%2520ReID%2520models%2520to%2520leverage%2520both%2520environmental%2520metadata%2520and%250Avisual%2520information%2520to%2520improve%2520ReID%2520performance.%2520Experiments%2520on%2520MetaWild%2520show%250Athat%2520combining%2520baseline%2520ReID%2520models%2520with%2520MFA%2520to%2520incorporate%2520metadata%250Aconsistently%2520improves%2520performance%2520compared%2520to%2520using%2520visual%2520information%2520alone%252C%250Avalidating%2520the%2520effectiveness%2520of%2520incorporating%2520metadata%2520in%2520re-identification.%2520We%250Ahope%2520that%2520our%2520proposed%2520dataset%2520can%2520inspire%2520further%2520exploration%2520of%2520multimodal%250Aapproaches%2520for%2520Animal%2520ReID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaWild%3A%20A%20Multimodal%20Dataset%20for%20Animal%20Re-Identification%20with%0A%20%20Environmental%20Metadata&entry.906535625=Yuzhuo%20Li%20and%20Di%20Zhao%20and%20Tingrui%20Qiao%20and%20Yihao%20Wu%20and%20Bo%20Pang%20and%20Yun%20Sing%20Koh&entry.1292438233=%20%20Identifying%20individual%20animals%20within%20large%20wildlife%20populations%20is%20essential%0Afor%20effective%20wildlife%20monitoring%20and%20conservation%20efforts.%20Recent%20advancements%0Ain%20computer%20vision%20have%20shown%20promise%20in%20animal%20re-identification%20%28Animal%20ReID%29%0Aby%20leveraging%20data%20from%20camera%20traps.%20However%2C%20existing%20Animal%20ReID%20datasets%0Arely%20exclusively%20on%20visual%20data%2C%20overlooking%20environmental%20metadata%20that%0Aecologists%20have%20identified%20as%20highly%20correlated%20with%20animal%20behavior%20and%0Aidentity%2C%20such%20as%20temperature%20and%20circadian%20rhythms.%20Moreover%2C%20the%20emergence%20of%0Amultimodal%20models%20capable%20of%20jointly%20processing%20visual%20and%20textual%20data%0Apresents%20new%20opportunities%20for%20Animal%20ReID%2C%20but%20existing%20datasets%20fail%20to%0Aleverage%20these%20models%27%20text-processing%20capabilities%2C%20limiting%20their%20full%0Apotential.%20Additionally%2C%20to%20facilitate%20the%20use%20of%20metadata%20in%20existing%20ReID%0Amethods%2C%20we%20propose%20the%20Meta-Feature%20Adapter%20%28MFA%29%2C%20a%20lightweight%20module%20that%0Acan%20be%20incorporated%20into%20existing%20vision-language%20model%20%28VLM%29-based%20Animal%20ReID%0Amethods%2C%20allowing%20ReID%20models%20to%20leverage%20both%20environmental%20metadata%20and%0Avisual%20information%20to%20improve%20ReID%20performance.%20Experiments%20on%20MetaWild%20show%0Athat%20combining%20baseline%20ReID%20models%20with%20MFA%20to%20incorporate%20metadata%0Aconsistently%20improves%20performance%20compared%20to%20using%20visual%20information%20alone%2C%0Avalidating%20the%20effectiveness%20of%20incorporating%20metadata%20in%20re-identification.%20We%0Ahope%20that%20our%20proposed%20dataset%20can%20inspire%20further%20exploration%20of%20multimodal%0Aapproaches%20for%20Animal%20ReID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13368v2&entry.124074799=Read"},
{"title": "Common Data Format (CDF): A Standardized Format for Match-Data in\n  Football (Soccer)", "author": "Gabriel Anzer and Kilian Arnsmeyer and Pascal Bauer and Joris Bekkers and Ulf Brefeld and Jesse Davis and Nicolas Evans and Matthias Kempe and Samuel J Robertson and Joshua Wyatt Smith and Jan Van Haaren", "abstract": "  During football matches, a variety of different parties (e.g., companies)\neach collect (possibly overlapping) data about the match ranging from basic\ninformation (e.g., starting players) to detailed positional data. This data is\nprovided to clubs, federations, and other organizations who are increasingly\ninterested in leveraging this data to inform their decision making.\nUnfortunately, analyzing such data pose significant barriers because each\nprovider may (1) collect different data, (2) use different specifications even\nwithin the same category of data, (3) represent the data differently, and (4)\ndelivers the data in a different manner (e.g., file format, protocol).\nConsequently, working with these data requires a significant investment of time\nand money. The goal of this work is to propose a uniform and standardized\nformat for football data called the Common Data Format (CDF). The CDF specifies\na minimal schema for five types of match data: match sheet data, video footage,\nevent data, tracking data, and match meta data. It aims to ensure that the\nprovided data is clear, sufficiently contextualized (e.g., its provenance is\nclear), and complete such that it enables common downstream analysis tasks.\nConcretely, this paper will detail the technical specifications of the CDF, the\nrepresentational choices that were made to help ensure the clarity of the\nprovided data, and a concrete approach for delivering data in the CDF. This\nrepresents Version 1.0.0 of the CDF.\n", "link": "http://arxiv.org/abs/2505.15820v4", "date": "2025-08-20", "relevancy": 2.0287, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4077}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4077}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Common%20Data%20Format%20%28CDF%29%3A%20A%20Standardized%20Format%20for%20Match-Data%20in%0A%20%20Football%20%28Soccer%29&body=Title%3A%20Common%20Data%20Format%20%28CDF%29%3A%20A%20Standardized%20Format%20for%20Match-Data%20in%0A%20%20Football%20%28Soccer%29%0AAuthor%3A%20Gabriel%20Anzer%20and%20Kilian%20Arnsmeyer%20and%20Pascal%20Bauer%20and%20Joris%20Bekkers%20and%20Ulf%20Brefeld%20and%20Jesse%20Davis%20and%20Nicolas%20Evans%20and%20Matthias%20Kempe%20and%20Samuel%20J%20Robertson%20and%20Joshua%20Wyatt%20Smith%20and%20Jan%20Van%20Haaren%0AAbstract%3A%20%20%20During%20football%20matches%2C%20a%20variety%20of%20different%20parties%20%28e.g.%2C%20companies%29%0Aeach%20collect%20%28possibly%20overlapping%29%20data%20about%20the%20match%20ranging%20from%20basic%0Ainformation%20%28e.g.%2C%20starting%20players%29%20to%20detailed%20positional%20data.%20This%20data%20is%0Aprovided%20to%20clubs%2C%20federations%2C%20and%20other%20organizations%20who%20are%20increasingly%0Ainterested%20in%20leveraging%20this%20data%20to%20inform%20their%20decision%20making.%0AUnfortunately%2C%20analyzing%20such%20data%20pose%20significant%20barriers%20because%20each%0Aprovider%20may%20%281%29%20collect%20different%20data%2C%20%282%29%20use%20different%20specifications%20even%0Awithin%20the%20same%20category%20of%20data%2C%20%283%29%20represent%20the%20data%20differently%2C%20and%20%284%29%0Adelivers%20the%20data%20in%20a%20different%20manner%20%28e.g.%2C%20file%20format%2C%20protocol%29.%0AConsequently%2C%20working%20with%20these%20data%20requires%20a%20significant%20investment%20of%20time%0Aand%20money.%20The%20goal%20of%20this%20work%20is%20to%20propose%20a%20uniform%20and%20standardized%0Aformat%20for%20football%20data%20called%20the%20Common%20Data%20Format%20%28CDF%29.%20The%20CDF%20specifies%0Aa%20minimal%20schema%20for%20five%20types%20of%20match%20data%3A%20match%20sheet%20data%2C%20video%20footage%2C%0Aevent%20data%2C%20tracking%20data%2C%20and%20match%20meta%20data.%20It%20aims%20to%20ensure%20that%20the%0Aprovided%20data%20is%20clear%2C%20sufficiently%20contextualized%20%28e.g.%2C%20its%20provenance%20is%0Aclear%29%2C%20and%20complete%20such%20that%20it%20enables%20common%20downstream%20analysis%20tasks.%0AConcretely%2C%20this%20paper%20will%20detail%20the%20technical%20specifications%20of%20the%20CDF%2C%20the%0Arepresentational%20choices%20that%20were%20made%20to%20help%20ensure%20the%20clarity%20of%20the%0Aprovided%20data%2C%20and%20a%20concrete%20approach%20for%20delivering%20data%20in%20the%20CDF.%20This%0Arepresents%20Version%201.0.0%20of%20the%20CDF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15820v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommon%2520Data%2520Format%2520%2528CDF%2529%253A%2520A%2520Standardized%2520Format%2520for%2520Match-Data%2520in%250A%2520%2520Football%2520%2528Soccer%2529%26entry.906535625%3DGabriel%2520Anzer%2520and%2520Kilian%2520Arnsmeyer%2520and%2520Pascal%2520Bauer%2520and%2520Joris%2520Bekkers%2520and%2520Ulf%2520Brefeld%2520and%2520Jesse%2520Davis%2520and%2520Nicolas%2520Evans%2520and%2520Matthias%2520Kempe%2520and%2520Samuel%2520J%2520Robertson%2520and%2520Joshua%2520Wyatt%2520Smith%2520and%2520Jan%2520Van%2520Haaren%26entry.1292438233%3D%2520%2520During%2520football%2520matches%252C%2520a%2520variety%2520of%2520different%2520parties%2520%2528e.g.%252C%2520companies%2529%250Aeach%2520collect%2520%2528possibly%2520overlapping%2529%2520data%2520about%2520the%2520match%2520ranging%2520from%2520basic%250Ainformation%2520%2528e.g.%252C%2520starting%2520players%2529%2520to%2520detailed%2520positional%2520data.%2520This%2520data%2520is%250Aprovided%2520to%2520clubs%252C%2520federations%252C%2520and%2520other%2520organizations%2520who%2520are%2520increasingly%250Ainterested%2520in%2520leveraging%2520this%2520data%2520to%2520inform%2520their%2520decision%2520making.%250AUnfortunately%252C%2520analyzing%2520such%2520data%2520pose%2520significant%2520barriers%2520because%2520each%250Aprovider%2520may%2520%25281%2529%2520collect%2520different%2520data%252C%2520%25282%2529%2520use%2520different%2520specifications%2520even%250Awithin%2520the%2520same%2520category%2520of%2520data%252C%2520%25283%2529%2520represent%2520the%2520data%2520differently%252C%2520and%2520%25284%2529%250Adelivers%2520the%2520data%2520in%2520a%2520different%2520manner%2520%2528e.g.%252C%2520file%2520format%252C%2520protocol%2529.%250AConsequently%252C%2520working%2520with%2520these%2520data%2520requires%2520a%2520significant%2520investment%2520of%2520time%250Aand%2520money.%2520The%2520goal%2520of%2520this%2520work%2520is%2520to%2520propose%2520a%2520uniform%2520and%2520standardized%250Aformat%2520for%2520football%2520data%2520called%2520the%2520Common%2520Data%2520Format%2520%2528CDF%2529.%2520The%2520CDF%2520specifies%250Aa%2520minimal%2520schema%2520for%2520five%2520types%2520of%2520match%2520data%253A%2520match%2520sheet%2520data%252C%2520video%2520footage%252C%250Aevent%2520data%252C%2520tracking%2520data%252C%2520and%2520match%2520meta%2520data.%2520It%2520aims%2520to%2520ensure%2520that%2520the%250Aprovided%2520data%2520is%2520clear%252C%2520sufficiently%2520contextualized%2520%2528e.g.%252C%2520its%2520provenance%2520is%250Aclear%2529%252C%2520and%2520complete%2520such%2520that%2520it%2520enables%2520common%2520downstream%2520analysis%2520tasks.%250AConcretely%252C%2520this%2520paper%2520will%2520detail%2520the%2520technical%2520specifications%2520of%2520the%2520CDF%252C%2520the%250Arepresentational%2520choices%2520that%2520were%2520made%2520to%2520help%2520ensure%2520the%2520clarity%2520of%2520the%250Aprovided%2520data%252C%2520and%2520a%2520concrete%2520approach%2520for%2520delivering%2520data%2520in%2520the%2520CDF.%2520This%250Arepresents%2520Version%25201.0.0%2520of%2520the%2520CDF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15820v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Common%20Data%20Format%20%28CDF%29%3A%20A%20Standardized%20Format%20for%20Match-Data%20in%0A%20%20Football%20%28Soccer%29&entry.906535625=Gabriel%20Anzer%20and%20Kilian%20Arnsmeyer%20and%20Pascal%20Bauer%20and%20Joris%20Bekkers%20and%20Ulf%20Brefeld%20and%20Jesse%20Davis%20and%20Nicolas%20Evans%20and%20Matthias%20Kempe%20and%20Samuel%20J%20Robertson%20and%20Joshua%20Wyatt%20Smith%20and%20Jan%20Van%20Haaren&entry.1292438233=%20%20During%20football%20matches%2C%20a%20variety%20of%20different%20parties%20%28e.g.%2C%20companies%29%0Aeach%20collect%20%28possibly%20overlapping%29%20data%20about%20the%20match%20ranging%20from%20basic%0Ainformation%20%28e.g.%2C%20starting%20players%29%20to%20detailed%20positional%20data.%20This%20data%20is%0Aprovided%20to%20clubs%2C%20federations%2C%20and%20other%20organizations%20who%20are%20increasingly%0Ainterested%20in%20leveraging%20this%20data%20to%20inform%20their%20decision%20making.%0AUnfortunately%2C%20analyzing%20such%20data%20pose%20significant%20barriers%20because%20each%0Aprovider%20may%20%281%29%20collect%20different%20data%2C%20%282%29%20use%20different%20specifications%20even%0Awithin%20the%20same%20category%20of%20data%2C%20%283%29%20represent%20the%20data%20differently%2C%20and%20%284%29%0Adelivers%20the%20data%20in%20a%20different%20manner%20%28e.g.%2C%20file%20format%2C%20protocol%29.%0AConsequently%2C%20working%20with%20these%20data%20requires%20a%20significant%20investment%20of%20time%0Aand%20money.%20The%20goal%20of%20this%20work%20is%20to%20propose%20a%20uniform%20and%20standardized%0Aformat%20for%20football%20data%20called%20the%20Common%20Data%20Format%20%28CDF%29.%20The%20CDF%20specifies%0Aa%20minimal%20schema%20for%20five%20types%20of%20match%20data%3A%20match%20sheet%20data%2C%20video%20footage%2C%0Aevent%20data%2C%20tracking%20data%2C%20and%20match%20meta%20data.%20It%20aims%20to%20ensure%20that%20the%0Aprovided%20data%20is%20clear%2C%20sufficiently%20contextualized%20%28e.g.%2C%20its%20provenance%20is%0Aclear%29%2C%20and%20complete%20such%20that%20it%20enables%20common%20downstream%20analysis%20tasks.%0AConcretely%2C%20this%20paper%20will%20detail%20the%20technical%20specifications%20of%20the%20CDF%2C%20the%0Arepresentational%20choices%20that%20were%20made%20to%20help%20ensure%20the%20clarity%20of%20the%0Aprovided%20data%2C%20and%20a%20concrete%20approach%20for%20delivering%20data%20in%20the%20CDF.%20This%0Arepresents%20Version%201.0.0%20of%20the%20CDF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15820v4&entry.124074799=Read"},
{"title": "Improving Fairness in Graph Neural Networks via Counterfactual Debiasing", "author": "Zengyi Wo and Chang Liu and Yumeng Wang and Minglai Shao and Wenjun Wang", "abstract": "  Graph Neural Networks (GNNs) have been successful in modeling\ngraph-structured data. However, similar to other machine learning models, GNNs\ncan exhibit bias in predictions based on attributes like race and gender.\nMoreover, bias in GNNs can be exacerbated by the graph structure and\nmessage-passing mechanisms. Recent cutting-edge methods propose mitigating bias\nby filtering out sensitive information from input or representations, like edge\ndropping or feature masking. Yet, we argue that such strategies may\nunintentionally eliminate non-sensitive features, leading to a compromised\nbalance between predictive accuracy and fairness. To tackle this challenge, we\npresent a novel approach utilizing counterfactual data augmentation for bias\nmitigation. This method involves creating diverse neighborhoods using\ncounterfactuals before message passing, facilitating unbiased node\nrepresentations learning from the augmented graph. Subsequently, an adversarial\ndiscriminator is employed to diminish bias in predictions by conventional GNN\nclassifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNs\nunder moderate conditions. Experiments on standard datasets using three GNN\nbackbones demonstrate that Fair-ICD notably enhances fairness metrics while\npreserving high predictive performance.\n", "link": "http://arxiv.org/abs/2508.14683v1", "date": "2025-08-20", "relevancy": 2.027, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5345}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4982}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Fairness%20in%20Graph%20Neural%20Networks%20via%20Counterfactual%20Debiasing&body=Title%3A%20Improving%20Fairness%20in%20Graph%20Neural%20Networks%20via%20Counterfactual%20Debiasing%0AAuthor%3A%20Zengyi%20Wo%20and%20Chang%20Liu%20and%20Yumeng%20Wang%20and%20Minglai%20Shao%20and%20Wenjun%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20been%20successful%20in%20modeling%0Agraph-structured%20data.%20However%2C%20similar%20to%20other%20machine%20learning%20models%2C%20GNNs%0Acan%20exhibit%20bias%20in%20predictions%20based%20on%20attributes%20like%20race%20and%20gender.%0AMoreover%2C%20bias%20in%20GNNs%20can%20be%20exacerbated%20by%20the%20graph%20structure%20and%0Amessage-passing%20mechanisms.%20Recent%20cutting-edge%20methods%20propose%20mitigating%20bias%0Aby%20filtering%20out%20sensitive%20information%20from%20input%20or%20representations%2C%20like%20edge%0Adropping%20or%20feature%20masking.%20Yet%2C%20we%20argue%20that%20such%20strategies%20may%0Aunintentionally%20eliminate%20non-sensitive%20features%2C%20leading%20to%20a%20compromised%0Abalance%20between%20predictive%20accuracy%20and%20fairness.%20To%20tackle%20this%20challenge%2C%20we%0Apresent%20a%20novel%20approach%20utilizing%20counterfactual%20data%20augmentation%20for%20bias%0Amitigation.%20This%20method%20involves%20creating%20diverse%20neighborhoods%20using%0Acounterfactuals%20before%20message%20passing%2C%20facilitating%20unbiased%20node%0Arepresentations%20learning%20from%20the%20augmented%20graph.%20Subsequently%2C%20an%20adversarial%0Adiscriminator%20is%20employed%20to%20diminish%20bias%20in%20predictions%20by%20conventional%20GNN%0Aclassifiers.%20Our%20proposed%20technique%2C%20Fair-ICD%2C%20ensures%20the%20fairness%20of%20GNNs%0Aunder%20moderate%20conditions.%20Experiments%20on%20standard%20datasets%20using%20three%20GNN%0Abackbones%20demonstrate%20that%20Fair-ICD%20notably%20enhances%20fairness%20metrics%20while%0Apreserving%20high%20predictive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Fairness%2520in%2520Graph%2520Neural%2520Networks%2520via%2520Counterfactual%2520Debiasing%26entry.906535625%3DZengyi%2520Wo%2520and%2520Chang%2520Liu%2520and%2520Yumeng%2520Wang%2520and%2520Minglai%2520Shao%2520and%2520Wenjun%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520been%2520successful%2520in%2520modeling%250Agraph-structured%2520data.%2520However%252C%2520similar%2520to%2520other%2520machine%2520learning%2520models%252C%2520GNNs%250Acan%2520exhibit%2520bias%2520in%2520predictions%2520based%2520on%2520attributes%2520like%2520race%2520and%2520gender.%250AMoreover%252C%2520bias%2520in%2520GNNs%2520can%2520be%2520exacerbated%2520by%2520the%2520graph%2520structure%2520and%250Amessage-passing%2520mechanisms.%2520Recent%2520cutting-edge%2520methods%2520propose%2520mitigating%2520bias%250Aby%2520filtering%2520out%2520sensitive%2520information%2520from%2520input%2520or%2520representations%252C%2520like%2520edge%250Adropping%2520or%2520feature%2520masking.%2520Yet%252C%2520we%2520argue%2520that%2520such%2520strategies%2520may%250Aunintentionally%2520eliminate%2520non-sensitive%2520features%252C%2520leading%2520to%2520a%2520compromised%250Abalance%2520between%2520predictive%2520accuracy%2520and%2520fairness.%2520To%2520tackle%2520this%2520challenge%252C%2520we%250Apresent%2520a%2520novel%2520approach%2520utilizing%2520counterfactual%2520data%2520augmentation%2520for%2520bias%250Amitigation.%2520This%2520method%2520involves%2520creating%2520diverse%2520neighborhoods%2520using%250Acounterfactuals%2520before%2520message%2520passing%252C%2520facilitating%2520unbiased%2520node%250Arepresentations%2520learning%2520from%2520the%2520augmented%2520graph.%2520Subsequently%252C%2520an%2520adversarial%250Adiscriminator%2520is%2520employed%2520to%2520diminish%2520bias%2520in%2520predictions%2520by%2520conventional%2520GNN%250Aclassifiers.%2520Our%2520proposed%2520technique%252C%2520Fair-ICD%252C%2520ensures%2520the%2520fairness%2520of%2520GNNs%250Aunder%2520moderate%2520conditions.%2520Experiments%2520on%2520standard%2520datasets%2520using%2520three%2520GNN%250Abackbones%2520demonstrate%2520that%2520Fair-ICD%2520notably%2520enhances%2520fairness%2520metrics%2520while%250Apreserving%2520high%2520predictive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Fairness%20in%20Graph%20Neural%20Networks%20via%20Counterfactual%20Debiasing&entry.906535625=Zengyi%20Wo%20and%20Chang%20Liu%20and%20Yumeng%20Wang%20and%20Minglai%20Shao%20and%20Wenjun%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20been%20successful%20in%20modeling%0Agraph-structured%20data.%20However%2C%20similar%20to%20other%20machine%20learning%20models%2C%20GNNs%0Acan%20exhibit%20bias%20in%20predictions%20based%20on%20attributes%20like%20race%20and%20gender.%0AMoreover%2C%20bias%20in%20GNNs%20can%20be%20exacerbated%20by%20the%20graph%20structure%20and%0Amessage-passing%20mechanisms.%20Recent%20cutting-edge%20methods%20propose%20mitigating%20bias%0Aby%20filtering%20out%20sensitive%20information%20from%20input%20or%20representations%2C%20like%20edge%0Adropping%20or%20feature%20masking.%20Yet%2C%20we%20argue%20that%20such%20strategies%20may%0Aunintentionally%20eliminate%20non-sensitive%20features%2C%20leading%20to%20a%20compromised%0Abalance%20between%20predictive%20accuracy%20and%20fairness.%20To%20tackle%20this%20challenge%2C%20we%0Apresent%20a%20novel%20approach%20utilizing%20counterfactual%20data%20augmentation%20for%20bias%0Amitigation.%20This%20method%20involves%20creating%20diverse%20neighborhoods%20using%0Acounterfactuals%20before%20message%20passing%2C%20facilitating%20unbiased%20node%0Arepresentations%20learning%20from%20the%20augmented%20graph.%20Subsequently%2C%20an%20adversarial%0Adiscriminator%20is%20employed%20to%20diminish%20bias%20in%20predictions%20by%20conventional%20GNN%0Aclassifiers.%20Our%20proposed%20technique%2C%20Fair-ICD%2C%20ensures%20the%20fairness%20of%20GNNs%0Aunder%20moderate%20conditions.%20Experiments%20on%20standard%20datasets%20using%20three%20GNN%0Abackbones%20demonstrate%20that%20Fair-ICD%20notably%20enhances%20fairness%20metrics%20while%0Apreserving%20high%20predictive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14683v1&entry.124074799=Read"},
{"title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production\n  via Quaternion-Based Pose Encoding and Contrastive Learning", "author": "Guilhem Faur\u00e9 and Mostafa Sadeghi and Sam Bigeard and Slim Ouni", "abstract": "  One of the main challenges in neural sign language production (SLP) lies in\nthe high intra-class variability of signs, arising from signer morphology and\nstylistic variety in the training data. To improve robustness to such\nvariations, we propose two enhancements to the standard Progressive\nTransformers (PT) architecture (Saunders et al., 2020). First, we encode poses\nusing bone rotations in quaternion space and train with a geodesic loss to\nimprove the accuracy and clarity of angular joint movements. Second, we\nintroduce a contrastive loss to structure decoder embeddings by semantic\nsimilarity, using either gloss overlap or SBERT-based sentence similarity,\naiming to filter out anatomical and stylistic features that do not convey\nrelevant semantic information. On the Phoenix14T dataset, the contrastive loss\nalone yields a 16% improvement in Probability of Correct Keypoint over the PT\nbaseline. When combined with quaternion-based pose encoding, the model achieves\na 6% reduction in Mean Bone Angle Error. These results point to the benefit of\nincorporating skeletal structure modeling and semantically guided contrastive\nobjectives on sign pose representations into the training of Transformer-based\nSLP models.\n", "link": "http://arxiv.org/abs/2508.14574v1", "date": "2025-08-20", "relevancy": 2.0252, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5196}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Skeletal%20and%20Signer%20Noise%20Reduction%20in%20Sign%20Language%20Production%0A%20%20via%20Quaternion-Based%20Pose%20Encoding%20and%20Contrastive%20Learning&body=Title%3A%20Towards%20Skeletal%20and%20Signer%20Noise%20Reduction%20in%20Sign%20Language%20Production%0A%20%20via%20Quaternion-Based%20Pose%20Encoding%20and%20Contrastive%20Learning%0AAuthor%3A%20Guilhem%20Faur%C3%A9%20and%20Mostafa%20Sadeghi%20and%20Sam%20Bigeard%20and%20Slim%20Ouni%0AAbstract%3A%20%20%20One%20of%20the%20main%20challenges%20in%20neural%20sign%20language%20production%20%28SLP%29%20lies%20in%0Athe%20high%20intra-class%20variability%20of%20signs%2C%20arising%20from%20signer%20morphology%20and%0Astylistic%20variety%20in%20the%20training%20data.%20To%20improve%20robustness%20to%20such%0Avariations%2C%20we%20propose%20two%20enhancements%20to%20the%20standard%20Progressive%0ATransformers%20%28PT%29%20architecture%20%28Saunders%20et%20al.%2C%202020%29.%20First%2C%20we%20encode%20poses%0Ausing%20bone%20rotations%20in%20quaternion%20space%20and%20train%20with%20a%20geodesic%20loss%20to%0Aimprove%20the%20accuracy%20and%20clarity%20of%20angular%20joint%20movements.%20Second%2C%20we%0Aintroduce%20a%20contrastive%20loss%20to%20structure%20decoder%20embeddings%20by%20semantic%0Asimilarity%2C%20using%20either%20gloss%20overlap%20or%20SBERT-based%20sentence%20similarity%2C%0Aaiming%20to%20filter%20out%20anatomical%20and%20stylistic%20features%20that%20do%20not%20convey%0Arelevant%20semantic%20information.%20On%20the%20Phoenix14T%20dataset%2C%20the%20contrastive%20loss%0Aalone%20yields%20a%2016%25%20improvement%20in%20Probability%20of%20Correct%20Keypoint%20over%20the%20PT%0Abaseline.%20When%20combined%20with%20quaternion-based%20pose%20encoding%2C%20the%20model%20achieves%0Aa%206%25%20reduction%20in%20Mean%20Bone%20Angle%20Error.%20These%20results%20point%20to%20the%20benefit%20of%0Aincorporating%20skeletal%20structure%20modeling%20and%20semantically%20guided%20contrastive%0Aobjectives%20on%20sign%20pose%20representations%20into%20the%20training%20of%20Transformer-based%0ASLP%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Skeletal%2520and%2520Signer%2520Noise%2520Reduction%2520in%2520Sign%2520Language%2520Production%250A%2520%2520via%2520Quaternion-Based%2520Pose%2520Encoding%2520and%2520Contrastive%2520Learning%26entry.906535625%3DGuilhem%2520Faur%25C3%25A9%2520and%2520Mostafa%2520Sadeghi%2520and%2520Sam%2520Bigeard%2520and%2520Slim%2520Ouni%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520main%2520challenges%2520in%2520neural%2520sign%2520language%2520production%2520%2528SLP%2529%2520lies%2520in%250Athe%2520high%2520intra-class%2520variability%2520of%2520signs%252C%2520arising%2520from%2520signer%2520morphology%2520and%250Astylistic%2520variety%2520in%2520the%2520training%2520data.%2520To%2520improve%2520robustness%2520to%2520such%250Avariations%252C%2520we%2520propose%2520two%2520enhancements%2520to%2520the%2520standard%2520Progressive%250ATransformers%2520%2528PT%2529%2520architecture%2520%2528Saunders%2520et%2520al.%252C%25202020%2529.%2520First%252C%2520we%2520encode%2520poses%250Ausing%2520bone%2520rotations%2520in%2520quaternion%2520space%2520and%2520train%2520with%2520a%2520geodesic%2520loss%2520to%250Aimprove%2520the%2520accuracy%2520and%2520clarity%2520of%2520angular%2520joint%2520movements.%2520Second%252C%2520we%250Aintroduce%2520a%2520contrastive%2520loss%2520to%2520structure%2520decoder%2520embeddings%2520by%2520semantic%250Asimilarity%252C%2520using%2520either%2520gloss%2520overlap%2520or%2520SBERT-based%2520sentence%2520similarity%252C%250Aaiming%2520to%2520filter%2520out%2520anatomical%2520and%2520stylistic%2520features%2520that%2520do%2520not%2520convey%250Arelevant%2520semantic%2520information.%2520On%2520the%2520Phoenix14T%2520dataset%252C%2520the%2520contrastive%2520loss%250Aalone%2520yields%2520a%252016%2525%2520improvement%2520in%2520Probability%2520of%2520Correct%2520Keypoint%2520over%2520the%2520PT%250Abaseline.%2520When%2520combined%2520with%2520quaternion-based%2520pose%2520encoding%252C%2520the%2520model%2520achieves%250Aa%25206%2525%2520reduction%2520in%2520Mean%2520Bone%2520Angle%2520Error.%2520These%2520results%2520point%2520to%2520the%2520benefit%2520of%250Aincorporating%2520skeletal%2520structure%2520modeling%2520and%2520semantically%2520guided%2520contrastive%250Aobjectives%2520on%2520sign%2520pose%2520representations%2520into%2520the%2520training%2520of%2520Transformer-based%250ASLP%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Skeletal%20and%20Signer%20Noise%20Reduction%20in%20Sign%20Language%20Production%0A%20%20via%20Quaternion-Based%20Pose%20Encoding%20and%20Contrastive%20Learning&entry.906535625=Guilhem%20Faur%C3%A9%20and%20Mostafa%20Sadeghi%20and%20Sam%20Bigeard%20and%20Slim%20Ouni&entry.1292438233=%20%20One%20of%20the%20main%20challenges%20in%20neural%20sign%20language%20production%20%28SLP%29%20lies%20in%0Athe%20high%20intra-class%20variability%20of%20signs%2C%20arising%20from%20signer%20morphology%20and%0Astylistic%20variety%20in%20the%20training%20data.%20To%20improve%20robustness%20to%20such%0Avariations%2C%20we%20propose%20two%20enhancements%20to%20the%20standard%20Progressive%0ATransformers%20%28PT%29%20architecture%20%28Saunders%20et%20al.%2C%202020%29.%20First%2C%20we%20encode%20poses%0Ausing%20bone%20rotations%20in%20quaternion%20space%20and%20train%20with%20a%20geodesic%20loss%20to%0Aimprove%20the%20accuracy%20and%20clarity%20of%20angular%20joint%20movements.%20Second%2C%20we%0Aintroduce%20a%20contrastive%20loss%20to%20structure%20decoder%20embeddings%20by%20semantic%0Asimilarity%2C%20using%20either%20gloss%20overlap%20or%20SBERT-based%20sentence%20similarity%2C%0Aaiming%20to%20filter%20out%20anatomical%20and%20stylistic%20features%20that%20do%20not%20convey%0Arelevant%20semantic%20information.%20On%20the%20Phoenix14T%20dataset%2C%20the%20contrastive%20loss%0Aalone%20yields%20a%2016%25%20improvement%20in%20Probability%20of%20Correct%20Keypoint%20over%20the%20PT%0Abaseline.%20When%20combined%20with%20quaternion-based%20pose%20encoding%2C%20the%20model%20achieves%0Aa%206%25%20reduction%20in%20Mean%20Bone%20Angle%20Error.%20These%20results%20point%20to%20the%20benefit%20of%0Aincorporating%20skeletal%20structure%20modeling%20and%20semantically%20guided%20contrastive%0Aobjectives%20on%20sign%20pose%20representations%20into%20the%20training%20of%20Transformer-based%0ASLP%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14574v1&entry.124074799=Read"},
{"title": "Data-Driven Probabilistic Evaluation of Logic Properties with\n  PAC-Confidence on Mealy Machines", "author": "Swantje Plambeck and Ali Salamati and Eyke Huellermeier and Goerschwin Fey", "abstract": "  Cyber-Physical Systems (CPS) are complex systems that require powerful models\nfor tasks like verification, diagnosis, or debugging. Often, suitable models\nare not available and manual extraction is difficult. Data-driven approaches\nthen provide a solution to, e.g., diagnosis tasks and verification problems\nbased on data collected from the system. In this paper, we consider CPS with a\ndiscrete abstraction in the form of a Mealy machine. We propose a data-driven\napproach to determine the safety probability of the system on a finite horizon\nof n time steps. The approach is based on the Probably Approximately Correct\n(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic\nand probabilistic reachability analysis of systems, especially providing an\nadditional confidence on the determined probability. The learning process\nfollows an active learning paradigm, where new learning data is sampled in a\nguided way after an initial learning set is collected. We validate the approach\nwith a case study on an automated lane-keeping system.\n", "link": "http://arxiv.org/abs/2508.14710v1", "date": "2025-08-20", "relevancy": 2.0221, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5835}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5002}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Probabilistic%20Evaluation%20of%20Logic%20Properties%20with%0A%20%20PAC-Confidence%20on%20Mealy%20Machines&body=Title%3A%20Data-Driven%20Probabilistic%20Evaluation%20of%20Logic%20Properties%20with%0A%20%20PAC-Confidence%20on%20Mealy%20Machines%0AAuthor%3A%20Swantje%20Plambeck%20and%20Ali%20Salamati%20and%20Eyke%20Huellermeier%20and%20Goerschwin%20Fey%0AAbstract%3A%20%20%20Cyber-Physical%20Systems%20%28CPS%29%20are%20complex%20systems%20that%20require%20powerful%20models%0Afor%20tasks%20like%20verification%2C%20diagnosis%2C%20or%20debugging.%20Often%2C%20suitable%20models%0Aare%20not%20available%20and%20manual%20extraction%20is%20difficult.%20Data-driven%20approaches%0Athen%20provide%20a%20solution%20to%2C%20e.g.%2C%20diagnosis%20tasks%20and%20verification%20problems%0Abased%20on%20data%20collected%20from%20the%20system.%20In%20this%20paper%2C%20we%20consider%20CPS%20with%20a%0Adiscrete%20abstraction%20in%20the%20form%20of%20a%20Mealy%20machine.%20We%20propose%20a%20data-driven%0Aapproach%20to%20determine%20the%20safety%20probability%20of%20the%20system%20on%20a%20finite%20horizon%0Aof%20n%20time%20steps.%20The%20approach%20is%20based%20on%20the%20Probably%20Approximately%20Correct%0A%28PAC%29%20learning%20paradigm.%20Thus%2C%20we%20elaborate%20a%20connection%20between%20discrete%20logic%0Aand%20probabilistic%20reachability%20analysis%20of%20systems%2C%20especially%20providing%20an%0Aadditional%20confidence%20on%20the%20determined%20probability.%20The%20learning%20process%0Afollows%20an%20active%20learning%20paradigm%2C%20where%20new%20learning%20data%20is%20sampled%20in%20a%0Aguided%20way%20after%20an%20initial%20learning%20set%20is%20collected.%20We%20validate%20the%20approach%0Awith%20a%20case%20study%20on%20an%20automated%20lane-keeping%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Probabilistic%2520Evaluation%2520of%2520Logic%2520Properties%2520with%250A%2520%2520PAC-Confidence%2520on%2520Mealy%2520Machines%26entry.906535625%3DSwantje%2520Plambeck%2520and%2520Ali%2520Salamati%2520and%2520Eyke%2520Huellermeier%2520and%2520Goerschwin%2520Fey%26entry.1292438233%3D%2520%2520Cyber-Physical%2520Systems%2520%2528CPS%2529%2520are%2520complex%2520systems%2520that%2520require%2520powerful%2520models%250Afor%2520tasks%2520like%2520verification%252C%2520diagnosis%252C%2520or%2520debugging.%2520Often%252C%2520suitable%2520models%250Aare%2520not%2520available%2520and%2520manual%2520extraction%2520is%2520difficult.%2520Data-driven%2520approaches%250Athen%2520provide%2520a%2520solution%2520to%252C%2520e.g.%252C%2520diagnosis%2520tasks%2520and%2520verification%2520problems%250Abased%2520on%2520data%2520collected%2520from%2520the%2520system.%2520In%2520this%2520paper%252C%2520we%2520consider%2520CPS%2520with%2520a%250Adiscrete%2520abstraction%2520in%2520the%2520form%2520of%2520a%2520Mealy%2520machine.%2520We%2520propose%2520a%2520data-driven%250Aapproach%2520to%2520determine%2520the%2520safety%2520probability%2520of%2520the%2520system%2520on%2520a%2520finite%2520horizon%250Aof%2520n%2520time%2520steps.%2520The%2520approach%2520is%2520based%2520on%2520the%2520Probably%2520Approximately%2520Correct%250A%2528PAC%2529%2520learning%2520paradigm.%2520Thus%252C%2520we%2520elaborate%2520a%2520connection%2520between%2520discrete%2520logic%250Aand%2520probabilistic%2520reachability%2520analysis%2520of%2520systems%252C%2520especially%2520providing%2520an%250Aadditional%2520confidence%2520on%2520the%2520determined%2520probability.%2520The%2520learning%2520process%250Afollows%2520an%2520active%2520learning%2520paradigm%252C%2520where%2520new%2520learning%2520data%2520is%2520sampled%2520in%2520a%250Aguided%2520way%2520after%2520an%2520initial%2520learning%2520set%2520is%2520collected.%2520We%2520validate%2520the%2520approach%250Awith%2520a%2520case%2520study%2520on%2520an%2520automated%2520lane-keeping%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Probabilistic%20Evaluation%20of%20Logic%20Properties%20with%0A%20%20PAC-Confidence%20on%20Mealy%20Machines&entry.906535625=Swantje%20Plambeck%20and%20Ali%20Salamati%20and%20Eyke%20Huellermeier%20and%20Goerschwin%20Fey&entry.1292438233=%20%20Cyber-Physical%20Systems%20%28CPS%29%20are%20complex%20systems%20that%20require%20powerful%20models%0Afor%20tasks%20like%20verification%2C%20diagnosis%2C%20or%20debugging.%20Often%2C%20suitable%20models%0Aare%20not%20available%20and%20manual%20extraction%20is%20difficult.%20Data-driven%20approaches%0Athen%20provide%20a%20solution%20to%2C%20e.g.%2C%20diagnosis%20tasks%20and%20verification%20problems%0Abased%20on%20data%20collected%20from%20the%20system.%20In%20this%20paper%2C%20we%20consider%20CPS%20with%20a%0Adiscrete%20abstraction%20in%20the%20form%20of%20a%20Mealy%20machine.%20We%20propose%20a%20data-driven%0Aapproach%20to%20determine%20the%20safety%20probability%20of%20the%20system%20on%20a%20finite%20horizon%0Aof%20n%20time%20steps.%20The%20approach%20is%20based%20on%20the%20Probably%20Approximately%20Correct%0A%28PAC%29%20learning%20paradigm.%20Thus%2C%20we%20elaborate%20a%20connection%20between%20discrete%20logic%0Aand%20probabilistic%20reachability%20analysis%20of%20systems%2C%20especially%20providing%20an%0Aadditional%20confidence%20on%20the%20determined%20probability.%20The%20learning%20process%0Afollows%20an%20active%20learning%20paradigm%2C%20where%20new%20learning%20data%20is%20sampled%20in%20a%0Aguided%20way%20after%20an%20initial%20learning%20set%20is%20collected.%20We%20validate%20the%20approach%0Awith%20a%20case%20study%20on%20an%20automated%20lane-keeping%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14710v1&entry.124074799=Read"},
{"title": "Unsupervised Urban Tree Biodiversity Mapping from Street-Level Imagery\n  Using Spatially-Aware Visual Clustering", "author": "Diaa Addeen Abuhani and Marco Seccaroni and Martina Mazzarello and Imran Zualkernan and Fabio Duarte and Carlo Ratti", "abstract": "  Urban tree biodiversity is critical for climate resilience, ecological\nstability, and livability in cities, yet most municipalities lack detailed\nknowledge of their canopies. Field-based inventories provide reliable estimates\nof Shannon and Simpson diversity but are costly and time-consuming, while\nsupervised AI methods require labeled data that often fail to generalize across\nregions. We introduce an unsupervised clustering framework that integrates\nvisual embeddings from street-level imagery with spatial planting patterns to\nestimate biodiversity without labels. Applied to eight North American cities,\nthe method recovers genus-level diversity patterns with high fidelity,\nachieving low Wasserstein distances to ground truth for Shannon and Simpson\nindices and preserving spatial autocorrelation. This scalable, fine-grained\napproach enables biodiversity mapping in cities lacking detailed inventories\nand offers a pathway for continuous, low-cost monitoring to support equitable\naccess to greenery and adaptive management of urban ecosystems.\n", "link": "http://arxiv.org/abs/2508.13814v2", "date": "2025-08-20", "relevancy": 2.0203, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5264}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Urban%20Tree%20Biodiversity%20Mapping%20from%20Street-Level%20Imagery%0A%20%20Using%20Spatially-Aware%20Visual%20Clustering&body=Title%3A%20Unsupervised%20Urban%20Tree%20Biodiversity%20Mapping%20from%20Street-Level%20Imagery%0A%20%20Using%20Spatially-Aware%20Visual%20Clustering%0AAuthor%3A%20Diaa%20Addeen%20Abuhani%20and%20Marco%20Seccaroni%20and%20Martina%20Mazzarello%20and%20Imran%20Zualkernan%20and%20Fabio%20Duarte%20and%20Carlo%20Ratti%0AAbstract%3A%20%20%20Urban%20tree%20biodiversity%20is%20critical%20for%20climate%20resilience%2C%20ecological%0Astability%2C%20and%20livability%20in%20cities%2C%20yet%20most%20municipalities%20lack%20detailed%0Aknowledge%20of%20their%20canopies.%20Field-based%20inventories%20provide%20reliable%20estimates%0Aof%20Shannon%20and%20Simpson%20diversity%20but%20are%20costly%20and%20time-consuming%2C%20while%0Asupervised%20AI%20methods%20require%20labeled%20data%20that%20often%20fail%20to%20generalize%20across%0Aregions.%20We%20introduce%20an%20unsupervised%20clustering%20framework%20that%20integrates%0Avisual%20embeddings%20from%20street-level%20imagery%20with%20spatial%20planting%20patterns%20to%0Aestimate%20biodiversity%20without%20labels.%20Applied%20to%20eight%20North%20American%20cities%2C%0Athe%20method%20recovers%20genus-level%20diversity%20patterns%20with%20high%20fidelity%2C%0Aachieving%20low%20Wasserstein%20distances%20to%20ground%20truth%20for%20Shannon%20and%20Simpson%0Aindices%20and%20preserving%20spatial%20autocorrelation.%20This%20scalable%2C%20fine-grained%0Aapproach%20enables%20biodiversity%20mapping%20in%20cities%20lacking%20detailed%20inventories%0Aand%20offers%20a%20pathway%20for%20continuous%2C%20low-cost%20monitoring%20to%20support%20equitable%0Aaccess%20to%20greenery%20and%20adaptive%20management%20of%20urban%20ecosystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Urban%2520Tree%2520Biodiversity%2520Mapping%2520from%2520Street-Level%2520Imagery%250A%2520%2520Using%2520Spatially-Aware%2520Visual%2520Clustering%26entry.906535625%3DDiaa%2520Addeen%2520Abuhani%2520and%2520Marco%2520Seccaroni%2520and%2520Martina%2520Mazzarello%2520and%2520Imran%2520Zualkernan%2520and%2520Fabio%2520Duarte%2520and%2520Carlo%2520Ratti%26entry.1292438233%3D%2520%2520Urban%2520tree%2520biodiversity%2520is%2520critical%2520for%2520climate%2520resilience%252C%2520ecological%250Astability%252C%2520and%2520livability%2520in%2520cities%252C%2520yet%2520most%2520municipalities%2520lack%2520detailed%250Aknowledge%2520of%2520their%2520canopies.%2520Field-based%2520inventories%2520provide%2520reliable%2520estimates%250Aof%2520Shannon%2520and%2520Simpson%2520diversity%2520but%2520are%2520costly%2520and%2520time-consuming%252C%2520while%250Asupervised%2520AI%2520methods%2520require%2520labeled%2520data%2520that%2520often%2520fail%2520to%2520generalize%2520across%250Aregions.%2520We%2520introduce%2520an%2520unsupervised%2520clustering%2520framework%2520that%2520integrates%250Avisual%2520embeddings%2520from%2520street-level%2520imagery%2520with%2520spatial%2520planting%2520patterns%2520to%250Aestimate%2520biodiversity%2520without%2520labels.%2520Applied%2520to%2520eight%2520North%2520American%2520cities%252C%250Athe%2520method%2520recovers%2520genus-level%2520diversity%2520patterns%2520with%2520high%2520fidelity%252C%250Aachieving%2520low%2520Wasserstein%2520distances%2520to%2520ground%2520truth%2520for%2520Shannon%2520and%2520Simpson%250Aindices%2520and%2520preserving%2520spatial%2520autocorrelation.%2520This%2520scalable%252C%2520fine-grained%250Aapproach%2520enables%2520biodiversity%2520mapping%2520in%2520cities%2520lacking%2520detailed%2520inventories%250Aand%2520offers%2520a%2520pathway%2520for%2520continuous%252C%2520low-cost%2520monitoring%2520to%2520support%2520equitable%250Aaccess%2520to%2520greenery%2520and%2520adaptive%2520management%2520of%2520urban%2520ecosystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Urban%20Tree%20Biodiversity%20Mapping%20from%20Street-Level%20Imagery%0A%20%20Using%20Spatially-Aware%20Visual%20Clustering&entry.906535625=Diaa%20Addeen%20Abuhani%20and%20Marco%20Seccaroni%20and%20Martina%20Mazzarello%20and%20Imran%20Zualkernan%20and%20Fabio%20Duarte%20and%20Carlo%20Ratti&entry.1292438233=%20%20Urban%20tree%20biodiversity%20is%20critical%20for%20climate%20resilience%2C%20ecological%0Astability%2C%20and%20livability%20in%20cities%2C%20yet%20most%20municipalities%20lack%20detailed%0Aknowledge%20of%20their%20canopies.%20Field-based%20inventories%20provide%20reliable%20estimates%0Aof%20Shannon%20and%20Simpson%20diversity%20but%20are%20costly%20and%20time-consuming%2C%20while%0Asupervised%20AI%20methods%20require%20labeled%20data%20that%20often%20fail%20to%20generalize%20across%0Aregions.%20We%20introduce%20an%20unsupervised%20clustering%20framework%20that%20integrates%0Avisual%20embeddings%20from%20street-level%20imagery%20with%20spatial%20planting%20patterns%20to%0Aestimate%20biodiversity%20without%20labels.%20Applied%20to%20eight%20North%20American%20cities%2C%0Athe%20method%20recovers%20genus-level%20diversity%20patterns%20with%20high%20fidelity%2C%0Aachieving%20low%20Wasserstein%20distances%20to%20ground%20truth%20for%20Shannon%20and%20Simpson%0Aindices%20and%20preserving%20spatial%20autocorrelation.%20This%20scalable%2C%20fine-grained%0Aapproach%20enables%20biodiversity%20mapping%20in%20cities%20lacking%20detailed%20inventories%0Aand%20offers%20a%20pathway%20for%20continuous%2C%20low-cost%20monitoring%20to%20support%20equitable%0Aaccess%20to%20greenery%20and%20adaptive%20management%20of%20urban%20ecosystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13814v2&entry.124074799=Read"},
{"title": "Clinical semantics for lung cancer prediction", "author": "Luis H. John and Jan A. Kors and Jenna M. Reps and Peter R. Rijnbeek and Egill A. Fridgeirsson", "abstract": "  Background: Existing clinical prediction models often represent patient data\nusing features that ignore the semantic relationships between clinical\nconcepts. This study integrates domain-specific semantic information by mapping\nthe SNOMED medical term hierarchy into a low-dimensional hyperbolic space using\nPoincar\\'e embeddings, with the aim of improving lung cancer onset prediction.\n  Methods: Using a retrospective cohort from the Optum EHR dataset, we derived\na clinical knowledge graph from the SNOMED taxonomy and generated Poincar\\'e\nembeddings via Riemannian stochastic gradient descent. These embeddings were\nthen incorporated into two deep learning architectures, a ResNet and a\nTransformer model. Models were evaluated for discrimination (area under the\nreceiver operating characteristic curve) and calibration (average absolute\ndifference between observed and predicted probabilities) performance.\n  Results: Incorporating pre-trained Poincar\\'e embeddings resulted in modest\nand consistent improvements in discrimination performance compared to baseline\nmodels using randomly initialized Euclidean embeddings. ResNet models,\nparticularly those using a 10-dimensional Poincar\\'e embedding, showed enhanced\ncalibration, whereas Transformer models maintained stable calibration across\nconfigurations.\n  Discussion: Embedding clinical knowledge graphs into hyperbolic space and\nintegrating these representations into deep learning models can improve lung\ncancer onset prediction by preserving the hierarchical structure of clinical\nterminologies used for prediction. This approach demonstrates a feasible method\nfor combining data-driven feature extraction with established clinical\nknowledge.\n", "link": "http://arxiv.org/abs/2508.14627v1", "date": "2025-08-20", "relevancy": 1.9988, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clinical%20semantics%20for%20lung%20cancer%20prediction&body=Title%3A%20Clinical%20semantics%20for%20lung%20cancer%20prediction%0AAuthor%3A%20Luis%20H.%20John%20and%20Jan%20A.%20Kors%20and%20Jenna%20M.%20Reps%20and%20Peter%20R.%20Rijnbeek%20and%20Egill%20A.%20Fridgeirsson%0AAbstract%3A%20%20%20Background%3A%20Existing%20clinical%20prediction%20models%20often%20represent%20patient%20data%0Ausing%20features%20that%20ignore%20the%20semantic%20relationships%20between%20clinical%0Aconcepts.%20This%20study%20integrates%20domain-specific%20semantic%20information%20by%20mapping%0Athe%20SNOMED%20medical%20term%20hierarchy%20into%20a%20low-dimensional%20hyperbolic%20space%20using%0APoincar%5C%27e%20embeddings%2C%20with%20the%20aim%20of%20improving%20lung%20cancer%20onset%20prediction.%0A%20%20Methods%3A%20Using%20a%20retrospective%20cohort%20from%20the%20Optum%20EHR%20dataset%2C%20we%20derived%0Aa%20clinical%20knowledge%20graph%20from%20the%20SNOMED%20taxonomy%20and%20generated%20Poincar%5C%27e%0Aembeddings%20via%20Riemannian%20stochastic%20gradient%20descent.%20These%20embeddings%20were%0Athen%20incorporated%20into%20two%20deep%20learning%20architectures%2C%20a%20ResNet%20and%20a%0ATransformer%20model.%20Models%20were%20evaluated%20for%20discrimination%20%28area%20under%20the%0Areceiver%20operating%20characteristic%20curve%29%20and%20calibration%20%28average%20absolute%0Adifference%20between%20observed%20and%20predicted%20probabilities%29%20performance.%0A%20%20Results%3A%20Incorporating%20pre-trained%20Poincar%5C%27e%20embeddings%20resulted%20in%20modest%0Aand%20consistent%20improvements%20in%20discrimination%20performance%20compared%20to%20baseline%0Amodels%20using%20randomly%20initialized%20Euclidean%20embeddings.%20ResNet%20models%2C%0Aparticularly%20those%20using%20a%2010-dimensional%20Poincar%5C%27e%20embedding%2C%20showed%20enhanced%0Acalibration%2C%20whereas%20Transformer%20models%20maintained%20stable%20calibration%20across%0Aconfigurations.%0A%20%20Discussion%3A%20Embedding%20clinical%20knowledge%20graphs%20into%20hyperbolic%20space%20and%0Aintegrating%20these%20representations%20into%20deep%20learning%20models%20can%20improve%20lung%0Acancer%20onset%20prediction%20by%20preserving%20the%20hierarchical%20structure%20of%20clinical%0Aterminologies%20used%20for%20prediction.%20This%20approach%20demonstrates%20a%20feasible%20method%0Afor%20combining%20data-driven%20feature%20extraction%20with%20established%20clinical%0Aknowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClinical%2520semantics%2520for%2520lung%2520cancer%2520prediction%26entry.906535625%3DLuis%2520H.%2520John%2520and%2520Jan%2520A.%2520Kors%2520and%2520Jenna%2520M.%2520Reps%2520and%2520Peter%2520R.%2520Rijnbeek%2520and%2520Egill%2520A.%2520Fridgeirsson%26entry.1292438233%3D%2520%2520Background%253A%2520Existing%2520clinical%2520prediction%2520models%2520often%2520represent%2520patient%2520data%250Ausing%2520features%2520that%2520ignore%2520the%2520semantic%2520relationships%2520between%2520clinical%250Aconcepts.%2520This%2520study%2520integrates%2520domain-specific%2520semantic%2520information%2520by%2520mapping%250Athe%2520SNOMED%2520medical%2520term%2520hierarchy%2520into%2520a%2520low-dimensional%2520hyperbolic%2520space%2520using%250APoincar%255C%2527e%2520embeddings%252C%2520with%2520the%2520aim%2520of%2520improving%2520lung%2520cancer%2520onset%2520prediction.%250A%2520%2520Methods%253A%2520Using%2520a%2520retrospective%2520cohort%2520from%2520the%2520Optum%2520EHR%2520dataset%252C%2520we%2520derived%250Aa%2520clinical%2520knowledge%2520graph%2520from%2520the%2520SNOMED%2520taxonomy%2520and%2520generated%2520Poincar%255C%2527e%250Aembeddings%2520via%2520Riemannian%2520stochastic%2520gradient%2520descent.%2520These%2520embeddings%2520were%250Athen%2520incorporated%2520into%2520two%2520deep%2520learning%2520architectures%252C%2520a%2520ResNet%2520and%2520a%250ATransformer%2520model.%2520Models%2520were%2520evaluated%2520for%2520discrimination%2520%2528area%2520under%2520the%250Areceiver%2520operating%2520characteristic%2520curve%2529%2520and%2520calibration%2520%2528average%2520absolute%250Adifference%2520between%2520observed%2520and%2520predicted%2520probabilities%2529%2520performance.%250A%2520%2520Results%253A%2520Incorporating%2520pre-trained%2520Poincar%255C%2527e%2520embeddings%2520resulted%2520in%2520modest%250Aand%2520consistent%2520improvements%2520in%2520discrimination%2520performance%2520compared%2520to%2520baseline%250Amodels%2520using%2520randomly%2520initialized%2520Euclidean%2520embeddings.%2520ResNet%2520models%252C%250Aparticularly%2520those%2520using%2520a%252010-dimensional%2520Poincar%255C%2527e%2520embedding%252C%2520showed%2520enhanced%250Acalibration%252C%2520whereas%2520Transformer%2520models%2520maintained%2520stable%2520calibration%2520across%250Aconfigurations.%250A%2520%2520Discussion%253A%2520Embedding%2520clinical%2520knowledge%2520graphs%2520into%2520hyperbolic%2520space%2520and%250Aintegrating%2520these%2520representations%2520into%2520deep%2520learning%2520models%2520can%2520improve%2520lung%250Acancer%2520onset%2520prediction%2520by%2520preserving%2520the%2520hierarchical%2520structure%2520of%2520clinical%250Aterminologies%2520used%2520for%2520prediction.%2520This%2520approach%2520demonstrates%2520a%2520feasible%2520method%250Afor%2520combining%2520data-driven%2520feature%2520extraction%2520with%2520established%2520clinical%250Aknowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clinical%20semantics%20for%20lung%20cancer%20prediction&entry.906535625=Luis%20H.%20John%20and%20Jan%20A.%20Kors%20and%20Jenna%20M.%20Reps%20and%20Peter%20R.%20Rijnbeek%20and%20Egill%20A.%20Fridgeirsson&entry.1292438233=%20%20Background%3A%20Existing%20clinical%20prediction%20models%20often%20represent%20patient%20data%0Ausing%20features%20that%20ignore%20the%20semantic%20relationships%20between%20clinical%0Aconcepts.%20This%20study%20integrates%20domain-specific%20semantic%20information%20by%20mapping%0Athe%20SNOMED%20medical%20term%20hierarchy%20into%20a%20low-dimensional%20hyperbolic%20space%20using%0APoincar%5C%27e%20embeddings%2C%20with%20the%20aim%20of%20improving%20lung%20cancer%20onset%20prediction.%0A%20%20Methods%3A%20Using%20a%20retrospective%20cohort%20from%20the%20Optum%20EHR%20dataset%2C%20we%20derived%0Aa%20clinical%20knowledge%20graph%20from%20the%20SNOMED%20taxonomy%20and%20generated%20Poincar%5C%27e%0Aembeddings%20via%20Riemannian%20stochastic%20gradient%20descent.%20These%20embeddings%20were%0Athen%20incorporated%20into%20two%20deep%20learning%20architectures%2C%20a%20ResNet%20and%20a%0ATransformer%20model.%20Models%20were%20evaluated%20for%20discrimination%20%28area%20under%20the%0Areceiver%20operating%20characteristic%20curve%29%20and%20calibration%20%28average%20absolute%0Adifference%20between%20observed%20and%20predicted%20probabilities%29%20performance.%0A%20%20Results%3A%20Incorporating%20pre-trained%20Poincar%5C%27e%20embeddings%20resulted%20in%20modest%0Aand%20consistent%20improvements%20in%20discrimination%20performance%20compared%20to%20baseline%0Amodels%20using%20randomly%20initialized%20Euclidean%20embeddings.%20ResNet%20models%2C%0Aparticularly%20those%20using%20a%2010-dimensional%20Poincar%5C%27e%20embedding%2C%20showed%20enhanced%0Acalibration%2C%20whereas%20Transformer%20models%20maintained%20stable%20calibration%20across%0Aconfigurations.%0A%20%20Discussion%3A%20Embedding%20clinical%20knowledge%20graphs%20into%20hyperbolic%20space%20and%0Aintegrating%20these%20representations%20into%20deep%20learning%20models%20can%20improve%20lung%0Acancer%20onset%20prediction%20by%20preserving%20the%20hierarchical%20structure%20of%20clinical%0Aterminologies%20used%20for%20prediction.%20This%20approach%20demonstrates%20a%20feasible%20method%0Afor%20combining%20data-driven%20feature%20extraction%20with%20established%20clinical%0Aknowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14627v1&entry.124074799=Read"},
{"title": "Universal and Transferable Adversarial Attack on Large Language Models\n  Using Exponentiated Gradient Descent", "author": "Sajib Biswas and Mao Nishino and Samuel Jacob Chacko and Xiuwen Liu", "abstract": "  As large language models (LLMs) are increasingly deployed in critical\napplications, ensuring their robustness and safety alignment remains a major\nchallenge. Despite the overall success of alignment techniques such as\nreinforcement learning from human feedback (RLHF) on typical prompts, LLMs\nremain vulnerable to jailbreak attacks enabled by crafted adversarial triggers\nappended to user prompts. Most existing jailbreak methods either rely on\ninefficient searches over discrete token spaces or direct optimization of\ncontinuous embeddings. While continuous embeddings can be given directly to\nselected open-source models as input, doing so is not feasible for proprietary\nmodels. On the other hand, projecting these embeddings back into valid discrete\ntokens introduces additional complexity and often reduces attack effectiveness.\nWe propose an intrinsic optimization method which directly optimizes relaxed\none-hot encodings of the adversarial suffix tokens using exponentiated gradient\ndescent coupled with Bregman projection, ensuring that the optimized one-hot\nencoding of each token always remains within the probability simplex. We\nprovide theoretical proof of convergence for our proposed method and implement\nan efficient algorithm that effectively jailbreaks several widely used LLMs.\nOur method achieves higher success rates and faster convergence compared to\nthree state-of-the-art baselines, evaluated on five open-source LLMs and four\nadversarial behavior datasets curated for evaluating jailbreak methods. In\naddition to individual prompt attacks, we also generate universal adversarial\nsuffixes effective across multiple prompts and demonstrate transferability of\noptimized suffixes to different LLMs.\n", "link": "http://arxiv.org/abs/2508.14853v1", "date": "2025-08-20", "relevancy": 1.9974, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5012}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4999}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20and%20Transferable%20Adversarial%20Attack%20on%20Large%20Language%20Models%0A%20%20Using%20Exponentiated%20Gradient%20Descent&body=Title%3A%20Universal%20and%20Transferable%20Adversarial%20Attack%20on%20Large%20Language%20Models%0A%20%20Using%20Exponentiated%20Gradient%20Descent%0AAuthor%3A%20Sajib%20Biswas%20and%20Mao%20Nishino%20and%20Samuel%20Jacob%20Chacko%20and%20Xiuwen%20Liu%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20critical%0Aapplications%2C%20ensuring%20their%20robustness%20and%20safety%20alignment%20remains%20a%20major%0Achallenge.%20Despite%20the%20overall%20success%20of%20alignment%20techniques%20such%20as%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20on%20typical%20prompts%2C%20LLMs%0Aremain%20vulnerable%20to%20jailbreak%20attacks%20enabled%20by%20crafted%20adversarial%20triggers%0Aappended%20to%20user%20prompts.%20Most%20existing%20jailbreak%20methods%20either%20rely%20on%0Ainefficient%20searches%20over%20discrete%20token%20spaces%20or%20direct%20optimization%20of%0Acontinuous%20embeddings.%20While%20continuous%20embeddings%20can%20be%20given%20directly%20to%0Aselected%20open-source%20models%20as%20input%2C%20doing%20so%20is%20not%20feasible%20for%20proprietary%0Amodels.%20On%20the%20other%20hand%2C%20projecting%20these%20embeddings%20back%20into%20valid%20discrete%0Atokens%20introduces%20additional%20complexity%20and%20often%20reduces%20attack%20effectiveness.%0AWe%20propose%20an%20intrinsic%20optimization%20method%20which%20directly%20optimizes%20relaxed%0Aone-hot%20encodings%20of%20the%20adversarial%20suffix%20tokens%20using%20exponentiated%20gradient%0Adescent%20coupled%20with%20Bregman%20projection%2C%20ensuring%20that%20the%20optimized%20one-hot%0Aencoding%20of%20each%20token%20always%20remains%20within%20the%20probability%20simplex.%20We%0Aprovide%20theoretical%20proof%20of%20convergence%20for%20our%20proposed%20method%20and%20implement%0Aan%20efficient%20algorithm%20that%20effectively%20jailbreaks%20several%20widely%20used%20LLMs.%0AOur%20method%20achieves%20higher%20success%20rates%20and%20faster%20convergence%20compared%20to%0Athree%20state-of-the-art%20baselines%2C%20evaluated%20on%20five%20open-source%20LLMs%20and%20four%0Aadversarial%20behavior%20datasets%20curated%20for%20evaluating%20jailbreak%20methods.%20In%0Aaddition%20to%20individual%20prompt%20attacks%2C%20we%20also%20generate%20universal%20adversarial%0Asuffixes%20effective%20across%20multiple%20prompts%20and%20demonstrate%20transferability%20of%0Aoptimized%20suffixes%20to%20different%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520and%2520Transferable%2520Adversarial%2520Attack%2520on%2520Large%2520Language%2520Models%250A%2520%2520Using%2520Exponentiated%2520Gradient%2520Descent%26entry.906535625%3DSajib%2520Biswas%2520and%2520Mao%2520Nishino%2520and%2520Samuel%2520Jacob%2520Chacko%2520and%2520Xiuwen%2520Liu%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520critical%250Aapplications%252C%2520ensuring%2520their%2520robustness%2520and%2520safety%2520alignment%2520remains%2520a%2520major%250Achallenge.%2520Despite%2520the%2520overall%2520success%2520of%2520alignment%2520techniques%2520such%2520as%250Areinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520on%2520typical%2520prompts%252C%2520LLMs%250Aremain%2520vulnerable%2520to%2520jailbreak%2520attacks%2520enabled%2520by%2520crafted%2520adversarial%2520triggers%250Aappended%2520to%2520user%2520prompts.%2520Most%2520existing%2520jailbreak%2520methods%2520either%2520rely%2520on%250Ainefficient%2520searches%2520over%2520discrete%2520token%2520spaces%2520or%2520direct%2520optimization%2520of%250Acontinuous%2520embeddings.%2520While%2520continuous%2520embeddings%2520can%2520be%2520given%2520directly%2520to%250Aselected%2520open-source%2520models%2520as%2520input%252C%2520doing%2520so%2520is%2520not%2520feasible%2520for%2520proprietary%250Amodels.%2520On%2520the%2520other%2520hand%252C%2520projecting%2520these%2520embeddings%2520back%2520into%2520valid%2520discrete%250Atokens%2520introduces%2520additional%2520complexity%2520and%2520often%2520reduces%2520attack%2520effectiveness.%250AWe%2520propose%2520an%2520intrinsic%2520optimization%2520method%2520which%2520directly%2520optimizes%2520relaxed%250Aone-hot%2520encodings%2520of%2520the%2520adversarial%2520suffix%2520tokens%2520using%2520exponentiated%2520gradient%250Adescent%2520coupled%2520with%2520Bregman%2520projection%252C%2520ensuring%2520that%2520the%2520optimized%2520one-hot%250Aencoding%2520of%2520each%2520token%2520always%2520remains%2520within%2520the%2520probability%2520simplex.%2520We%250Aprovide%2520theoretical%2520proof%2520of%2520convergence%2520for%2520our%2520proposed%2520method%2520and%2520implement%250Aan%2520efficient%2520algorithm%2520that%2520effectively%2520jailbreaks%2520several%2520widely%2520used%2520LLMs.%250AOur%2520method%2520achieves%2520higher%2520success%2520rates%2520and%2520faster%2520convergence%2520compared%2520to%250Athree%2520state-of-the-art%2520baselines%252C%2520evaluated%2520on%2520five%2520open-source%2520LLMs%2520and%2520four%250Aadversarial%2520behavior%2520datasets%2520curated%2520for%2520evaluating%2520jailbreak%2520methods.%2520In%250Aaddition%2520to%2520individual%2520prompt%2520attacks%252C%2520we%2520also%2520generate%2520universal%2520adversarial%250Asuffixes%2520effective%2520across%2520multiple%2520prompts%2520and%2520demonstrate%2520transferability%2520of%250Aoptimized%2520suffixes%2520to%2520different%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20and%20Transferable%20Adversarial%20Attack%20on%20Large%20Language%20Models%0A%20%20Using%20Exponentiated%20Gradient%20Descent&entry.906535625=Sajib%20Biswas%20and%20Mao%20Nishino%20and%20Samuel%20Jacob%20Chacko%20and%20Xiuwen%20Liu&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20critical%0Aapplications%2C%20ensuring%20their%20robustness%20and%20safety%20alignment%20remains%20a%20major%0Achallenge.%20Despite%20the%20overall%20success%20of%20alignment%20techniques%20such%20as%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20on%20typical%20prompts%2C%20LLMs%0Aremain%20vulnerable%20to%20jailbreak%20attacks%20enabled%20by%20crafted%20adversarial%20triggers%0Aappended%20to%20user%20prompts.%20Most%20existing%20jailbreak%20methods%20either%20rely%20on%0Ainefficient%20searches%20over%20discrete%20token%20spaces%20or%20direct%20optimization%20of%0Acontinuous%20embeddings.%20While%20continuous%20embeddings%20can%20be%20given%20directly%20to%0Aselected%20open-source%20models%20as%20input%2C%20doing%20so%20is%20not%20feasible%20for%20proprietary%0Amodels.%20On%20the%20other%20hand%2C%20projecting%20these%20embeddings%20back%20into%20valid%20discrete%0Atokens%20introduces%20additional%20complexity%20and%20often%20reduces%20attack%20effectiveness.%0AWe%20propose%20an%20intrinsic%20optimization%20method%20which%20directly%20optimizes%20relaxed%0Aone-hot%20encodings%20of%20the%20adversarial%20suffix%20tokens%20using%20exponentiated%20gradient%0Adescent%20coupled%20with%20Bregman%20projection%2C%20ensuring%20that%20the%20optimized%20one-hot%0Aencoding%20of%20each%20token%20always%20remains%20within%20the%20probability%20simplex.%20We%0Aprovide%20theoretical%20proof%20of%20convergence%20for%20our%20proposed%20method%20and%20implement%0Aan%20efficient%20algorithm%20that%20effectively%20jailbreaks%20several%20widely%20used%20LLMs.%0AOur%20method%20achieves%20higher%20success%20rates%20and%20faster%20convergence%20compared%20to%0Athree%20state-of-the-art%20baselines%2C%20evaluated%20on%20five%20open-source%20LLMs%20and%20four%0Aadversarial%20behavior%20datasets%20curated%20for%20evaluating%20jailbreak%20methods.%20In%0Aaddition%20to%20individual%20prompt%20attacks%2C%20we%20also%20generate%20universal%20adversarial%0Asuffixes%20effective%20across%20multiple%20prompts%20and%20demonstrate%20transferability%20of%0Aoptimized%20suffixes%20to%20different%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14853v1&entry.124074799=Read"},
{"title": "Low-rank bias, weight decay, and model merging in neural networks", "author": "Ilja Kuzborskij and Yasin Abbasi Yadkori", "abstract": "  We explore the low-rank structure of the weight matrices in neural networks\nat the stationary points (limiting solutions of optimization algorithms) with\n$L2$ regularization (also known as weight decay). We show several properties of\nsuch deep neural networks, induced by $L2$ regularization. In particular, for a\nstationary point we show alignment of the parameters and the gradient, norm\npreservation across layers, and low-rank bias: properties previously known in\nthe context of solution of gradient descent/flow type algorithms. Experiments\nshow that the assumptions made in the analysis only mildly affect the\nobservations.\n  In addition, we investigate a multitask learning phenomenon enabled by $L2$\nregularization and low-rank bias. In particular, we show that if two networks\nare trained, such that the inputs in the training set of one network are\napproximately orthogonal to the inputs in the training set of the other\nnetwork, the new network obtained by simply summing the weights of the two\nnetworks will perform as well on both training sets as the respective\nindividual networks. We demonstrate this for shallow ReLU neural networks\ntrained by gradient descent, as well as deep linear networks trained by\ngradient flow.\n", "link": "http://arxiv.org/abs/2502.17340v2", "date": "2025-08-20", "relevancy": 1.9613, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5107}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4808}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-rank%20bias%2C%20weight%20decay%2C%20and%20model%20merging%20in%20neural%20networks&body=Title%3A%20Low-rank%20bias%2C%20weight%20decay%2C%20and%20model%20merging%20in%20neural%20networks%0AAuthor%3A%20Ilja%20Kuzborskij%20and%20Yasin%20Abbasi%20Yadkori%0AAbstract%3A%20%20%20We%20explore%20the%20low-rank%20structure%20of%20the%20weight%20matrices%20in%20neural%20networks%0Aat%20the%20stationary%20points%20%28limiting%20solutions%20of%20optimization%20algorithms%29%20with%0A%24L2%24%20regularization%20%28also%20known%20as%20weight%20decay%29.%20We%20show%20several%20properties%20of%0Asuch%20deep%20neural%20networks%2C%20induced%20by%20%24L2%24%20regularization.%20In%20particular%2C%20for%20a%0Astationary%20point%20we%20show%20alignment%20of%20the%20parameters%20and%20the%20gradient%2C%20norm%0Apreservation%20across%20layers%2C%20and%20low-rank%20bias%3A%20properties%20previously%20known%20in%0Athe%20context%20of%20solution%20of%20gradient%20descent/flow%20type%20algorithms.%20Experiments%0Ashow%20that%20the%20assumptions%20made%20in%20the%20analysis%20only%20mildly%20affect%20the%0Aobservations.%0A%20%20In%20addition%2C%20we%20investigate%20a%20multitask%20learning%20phenomenon%20enabled%20by%20%24L2%24%0Aregularization%20and%20low-rank%20bias.%20In%20particular%2C%20we%20show%20that%20if%20two%20networks%0Aare%20trained%2C%20such%20that%20the%20inputs%20in%20the%20training%20set%20of%20one%20network%20are%0Aapproximately%20orthogonal%20to%20the%20inputs%20in%20the%20training%20set%20of%20the%20other%0Anetwork%2C%20the%20new%20network%20obtained%20by%20simply%20summing%20the%20weights%20of%20the%20two%0Anetworks%20will%20perform%20as%20well%20on%20both%20training%20sets%20as%20the%20respective%0Aindividual%20networks.%20We%20demonstrate%20this%20for%20shallow%20ReLU%20neural%20networks%0Atrained%20by%20gradient%20descent%2C%20as%20well%20as%20deep%20linear%20networks%20trained%20by%0Agradient%20flow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17340v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-rank%2520bias%252C%2520weight%2520decay%252C%2520and%2520model%2520merging%2520in%2520neural%2520networks%26entry.906535625%3DIlja%2520Kuzborskij%2520and%2520Yasin%2520Abbasi%2520Yadkori%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520low-rank%2520structure%2520of%2520the%2520weight%2520matrices%2520in%2520neural%2520networks%250Aat%2520the%2520stationary%2520points%2520%2528limiting%2520solutions%2520of%2520optimization%2520algorithms%2529%2520with%250A%2524L2%2524%2520regularization%2520%2528also%2520known%2520as%2520weight%2520decay%2529.%2520We%2520show%2520several%2520properties%2520of%250Asuch%2520deep%2520neural%2520networks%252C%2520induced%2520by%2520%2524L2%2524%2520regularization.%2520In%2520particular%252C%2520for%2520a%250Astationary%2520point%2520we%2520show%2520alignment%2520of%2520the%2520parameters%2520and%2520the%2520gradient%252C%2520norm%250Apreservation%2520across%2520layers%252C%2520and%2520low-rank%2520bias%253A%2520properties%2520previously%2520known%2520in%250Athe%2520context%2520of%2520solution%2520of%2520gradient%2520descent/flow%2520type%2520algorithms.%2520Experiments%250Ashow%2520that%2520the%2520assumptions%2520made%2520in%2520the%2520analysis%2520only%2520mildly%2520affect%2520the%250Aobservations.%250A%2520%2520In%2520addition%252C%2520we%2520investigate%2520a%2520multitask%2520learning%2520phenomenon%2520enabled%2520by%2520%2524L2%2524%250Aregularization%2520and%2520low-rank%2520bias.%2520In%2520particular%252C%2520we%2520show%2520that%2520if%2520two%2520networks%250Aare%2520trained%252C%2520such%2520that%2520the%2520inputs%2520in%2520the%2520training%2520set%2520of%2520one%2520network%2520are%250Aapproximately%2520orthogonal%2520to%2520the%2520inputs%2520in%2520the%2520training%2520set%2520of%2520the%2520other%250Anetwork%252C%2520the%2520new%2520network%2520obtained%2520by%2520simply%2520summing%2520the%2520weights%2520of%2520the%2520two%250Anetworks%2520will%2520perform%2520as%2520well%2520on%2520both%2520training%2520sets%2520as%2520the%2520respective%250Aindividual%2520networks.%2520We%2520demonstrate%2520this%2520for%2520shallow%2520ReLU%2520neural%2520networks%250Atrained%2520by%2520gradient%2520descent%252C%2520as%2520well%2520as%2520deep%2520linear%2520networks%2520trained%2520by%250Agradient%2520flow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17340v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-rank%20bias%2C%20weight%20decay%2C%20and%20model%20merging%20in%20neural%20networks&entry.906535625=Ilja%20Kuzborskij%20and%20Yasin%20Abbasi%20Yadkori&entry.1292438233=%20%20We%20explore%20the%20low-rank%20structure%20of%20the%20weight%20matrices%20in%20neural%20networks%0Aat%20the%20stationary%20points%20%28limiting%20solutions%20of%20optimization%20algorithms%29%20with%0A%24L2%24%20regularization%20%28also%20known%20as%20weight%20decay%29.%20We%20show%20several%20properties%20of%0Asuch%20deep%20neural%20networks%2C%20induced%20by%20%24L2%24%20regularization.%20In%20particular%2C%20for%20a%0Astationary%20point%20we%20show%20alignment%20of%20the%20parameters%20and%20the%20gradient%2C%20norm%0Apreservation%20across%20layers%2C%20and%20low-rank%20bias%3A%20properties%20previously%20known%20in%0Athe%20context%20of%20solution%20of%20gradient%20descent/flow%20type%20algorithms.%20Experiments%0Ashow%20that%20the%20assumptions%20made%20in%20the%20analysis%20only%20mildly%20affect%20the%0Aobservations.%0A%20%20In%20addition%2C%20we%20investigate%20a%20multitask%20learning%20phenomenon%20enabled%20by%20%24L2%24%0Aregularization%20and%20low-rank%20bias.%20In%20particular%2C%20we%20show%20that%20if%20two%20networks%0Aare%20trained%2C%20such%20that%20the%20inputs%20in%20the%20training%20set%20of%20one%20network%20are%0Aapproximately%20orthogonal%20to%20the%20inputs%20in%20the%20training%20set%20of%20the%20other%0Anetwork%2C%20the%20new%20network%20obtained%20by%20simply%20summing%20the%20weights%20of%20the%20two%0Anetworks%20will%20perform%20as%20well%20on%20both%20training%20sets%20as%20the%20respective%0Aindividual%20networks.%20We%20demonstrate%20this%20for%20shallow%20ReLU%20neural%20networks%0Atrained%20by%20gradient%20descent%2C%20as%20well%20as%20deep%20linear%20networks%20trained%20by%0Agradient%20flow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17340v2&entry.124074799=Read"},
{"title": "JudgeLRM: Large Reasoning Models as a Judge", "author": "Nuo Chen and Zhiyuan Hu and Qingyun Zou and Jiaying Wu and Qian Wang and Bryan Hooi and Bingsheng He", "abstract": "  The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.\n", "link": "http://arxiv.org/abs/2504.00050v2", "date": "2025-08-20", "relevancy": 1.948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JudgeLRM%3A%20Large%20Reasoning%20Models%20as%20a%20Judge&body=Title%3A%20JudgeLRM%3A%20Large%20Reasoning%20Models%20as%20a%20Judge%0AAuthor%3A%20Nuo%20Chen%20and%20Zhiyuan%20Hu%20and%20Qingyun%20Zou%20and%20Jiaying%20Wu%20and%20Qian%20Wang%20and%20Bryan%20Hooi%20and%20Bingsheng%20He%0AAbstract%3A%20%20%20The%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20as%20evaluators%20offers%20a%20scalable%0Aalternative%20to%20human%20annotation%2C%20yet%20existing%20Supervised%20Fine-Tuning%20%28SFT%29%20for%0Ajudges%20approaches%20often%20fall%20short%20in%20domains%20requiring%20complex%20reasoning.%20In%0Athis%20work%2C%20we%20investigate%20whether%20LLM%20judges%20truly%20benefit%20from%20enhanced%0Areasoning%20capabilities.%20Through%20a%20detailed%20analysis%20of%20reasoning%20requirements%0Aacross%20evaluation%20tasks%2C%20we%20reveal%20a%20negative%20correlation%20between%20SFT%0Aperformance%20gains%20and%20the%20proportion%20of%20reasoning-demanding%20samples%20-%0Ahighlighting%20the%20limitations%20of%20SFT%20in%20such%20scenarios.%20To%20address%20this%2C%20we%0Aintroduce%20JudgeLRM%2C%20a%20family%20of%20judgment-oriented%20LLMs%20trained%20using%0Areinforcement%20learning%20%28RL%29%20with%20judge-wise%2C%20outcome-driven%20rewards.%20JudgeLRM%0Amodels%20consistently%20outperform%20both%20SFT-tuned%20and%20state-of-the-art%20reasoning%0Amodels.%20Notably%2C%20JudgeLRM-3B%20surpasses%20GPT-4%2C%20and%20JudgeLRM-7B%20outperforms%0ADeepSeek-R1%20by%202.79%25%20in%20F1%20score%2C%20particularly%20excelling%20in%20judge%20tasks%0Arequiring%20deep%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJudgeLRM%253A%2520Large%2520Reasoning%2520Models%2520as%2520a%2520Judge%26entry.906535625%3DNuo%2520Chen%2520and%2520Zhiyuan%2520Hu%2520and%2520Qingyun%2520Zou%2520and%2520Jiaying%2520Wu%2520and%2520Qian%2520Wang%2520and%2520Bryan%2520Hooi%2520and%2520Bingsheng%2520He%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520as%2520evaluators%2520offers%2520a%2520scalable%250Aalternative%2520to%2520human%2520annotation%252C%2520yet%2520existing%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520for%250Ajudges%2520approaches%2520often%2520fall%2520short%2520in%2520domains%2520requiring%2520complex%2520reasoning.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520whether%2520LLM%2520judges%2520truly%2520benefit%2520from%2520enhanced%250Areasoning%2520capabilities.%2520Through%2520a%2520detailed%2520analysis%2520of%2520reasoning%2520requirements%250Aacross%2520evaluation%2520tasks%252C%2520we%2520reveal%2520a%2520negative%2520correlation%2520between%2520SFT%250Aperformance%2520gains%2520and%2520the%2520proportion%2520of%2520reasoning-demanding%2520samples%2520-%250Ahighlighting%2520the%2520limitations%2520of%2520SFT%2520in%2520such%2520scenarios.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520JudgeLRM%252C%2520a%2520family%2520of%2520judgment-oriented%2520LLMs%2520trained%2520using%250Areinforcement%2520learning%2520%2528RL%2529%2520with%2520judge-wise%252C%2520outcome-driven%2520rewards.%2520JudgeLRM%250Amodels%2520consistently%2520outperform%2520both%2520SFT-tuned%2520and%2520state-of-the-art%2520reasoning%250Amodels.%2520Notably%252C%2520JudgeLRM-3B%2520surpasses%2520GPT-4%252C%2520and%2520JudgeLRM-7B%2520outperforms%250ADeepSeek-R1%2520by%25202.79%2525%2520in%2520F1%2520score%252C%2520particularly%2520excelling%2520in%2520judge%2520tasks%250Arequiring%2520deep%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JudgeLRM%3A%20Large%20Reasoning%20Models%20as%20a%20Judge&entry.906535625=Nuo%20Chen%20and%20Zhiyuan%20Hu%20and%20Qingyun%20Zou%20and%20Jiaying%20Wu%20and%20Qian%20Wang%20and%20Bryan%20Hooi%20and%20Bingsheng%20He&entry.1292438233=%20%20The%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20as%20evaluators%20offers%20a%20scalable%0Aalternative%20to%20human%20annotation%2C%20yet%20existing%20Supervised%20Fine-Tuning%20%28SFT%29%20for%0Ajudges%20approaches%20often%20fall%20short%20in%20domains%20requiring%20complex%20reasoning.%20In%0Athis%20work%2C%20we%20investigate%20whether%20LLM%20judges%20truly%20benefit%20from%20enhanced%0Areasoning%20capabilities.%20Through%20a%20detailed%20analysis%20of%20reasoning%20requirements%0Aacross%20evaluation%20tasks%2C%20we%20reveal%20a%20negative%20correlation%20between%20SFT%0Aperformance%20gains%20and%20the%20proportion%20of%20reasoning-demanding%20samples%20-%0Ahighlighting%20the%20limitations%20of%20SFT%20in%20such%20scenarios.%20To%20address%20this%2C%20we%0Aintroduce%20JudgeLRM%2C%20a%20family%20of%20judgment-oriented%20LLMs%20trained%20using%0Areinforcement%20learning%20%28RL%29%20with%20judge-wise%2C%20outcome-driven%20rewards.%20JudgeLRM%0Amodels%20consistently%20outperform%20both%20SFT-tuned%20and%20state-of-the-art%20reasoning%0Amodels.%20Notably%2C%20JudgeLRM-3B%20surpasses%20GPT-4%2C%20and%20JudgeLRM-7B%20outperforms%0ADeepSeek-R1%20by%202.79%25%20in%20F1%20score%2C%20particularly%20excelling%20in%20judge%20tasks%0Arequiring%20deep%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00050v2&entry.124074799=Read"},
{"title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for\n  Clinical Reasoning over EHRs", "author": "Skatje Myers and Dmitriy Dligach and Timothy A. Miller and Samantha Barr and Yanjun Gao and Matthew Churpek and Anoop Mayampurath and Majid Afshar", "abstract": "  Electronic health records (EHRs) are long, noisy, and often redundant, posing\na major challenge for the clinicians who must navigate them. Large language\nmodels (LLMs) offer a promising solution for extracting and reasoning over this\nunstructured text, but the length of clinical notes often exceeds even\nstate-of-the-art models' extended context windows. Retrieval-augmented\ngeneration (RAG) offers an alternative by retrieving task-relevant passages\nfrom across the entire EHR, potentially reducing the amount of required input\ntokens. In this work, we propose three clinical tasks designed to be replicable\nacross health systems with minimal effort: 1) extracting imaging procedures, 2)\ngenerating timelines of antibiotic use, and 3) identifying key diagnoses. Using\nEHRs from actual hospitalized patients, we test three state-of-the-art LLMs\nwith varying amounts of provided context, using either targeted text retrieval\nor the most recent clinical notes. We find that RAG closely matches or exceeds\nthe performance of using recent notes, and approaches the performance of using\nthe models' full context while requiring drastically fewer input tokens. Our\nresults suggest that RAG remains a competitive and efficient approach even as\nnewer models become capable of handling increasingly longer amounts of text.\n", "link": "http://arxiv.org/abs/2508.14817v1", "date": "2025-08-20", "relevancy": 1.9439, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Retrieval-Augmented%20Generation%20vs.%20Long-Context%20Input%20for%0A%20%20Clinical%20Reasoning%20over%20EHRs&body=Title%3A%20Evaluating%20Retrieval-Augmented%20Generation%20vs.%20Long-Context%20Input%20for%0A%20%20Clinical%20Reasoning%20over%20EHRs%0AAuthor%3A%20Skatje%20Myers%20and%20Dmitriy%20Dligach%20and%20Timothy%20A.%20Miller%20and%20Samantha%20Barr%20and%20Yanjun%20Gao%20and%20Matthew%20Churpek%20and%20Anoop%20Mayampurath%20and%20Majid%20Afshar%0AAbstract%3A%20%20%20Electronic%20health%20records%20%28EHRs%29%20are%20long%2C%20noisy%2C%20and%20often%20redundant%2C%20posing%0Aa%20major%20challenge%20for%20the%20clinicians%20who%20must%20navigate%20them.%20Large%20language%0Amodels%20%28LLMs%29%20offer%20a%20promising%20solution%20for%20extracting%20and%20reasoning%20over%20this%0Aunstructured%20text%2C%20but%20the%20length%20of%20clinical%20notes%20often%20exceeds%20even%0Astate-of-the-art%20models%27%20extended%20context%20windows.%20Retrieval-augmented%0Ageneration%20%28RAG%29%20offers%20an%20alternative%20by%20retrieving%20task-relevant%20passages%0Afrom%20across%20the%20entire%20EHR%2C%20potentially%20reducing%20the%20amount%20of%20required%20input%0Atokens.%20In%20this%20work%2C%20we%20propose%20three%20clinical%20tasks%20designed%20to%20be%20replicable%0Aacross%20health%20systems%20with%20minimal%20effort%3A%201%29%20extracting%20imaging%20procedures%2C%202%29%0Agenerating%20timelines%20of%20antibiotic%20use%2C%20and%203%29%20identifying%20key%20diagnoses.%20Using%0AEHRs%20from%20actual%20hospitalized%20patients%2C%20we%20test%20three%20state-of-the-art%20LLMs%0Awith%20varying%20amounts%20of%20provided%20context%2C%20using%20either%20targeted%20text%20retrieval%0Aor%20the%20most%20recent%20clinical%20notes.%20We%20find%20that%20RAG%20closely%20matches%20or%20exceeds%0Athe%20performance%20of%20using%20recent%20notes%2C%20and%20approaches%20the%20performance%20of%20using%0Athe%20models%27%20full%20context%20while%20requiring%20drastically%20fewer%20input%20tokens.%20Our%0Aresults%20suggest%20that%20RAG%20remains%20a%20competitive%20and%20efficient%20approach%20even%20as%0Anewer%20models%20become%20capable%20of%20handling%20increasingly%20longer%20amounts%20of%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Retrieval-Augmented%2520Generation%2520vs.%2520Long-Context%2520Input%2520for%250A%2520%2520Clinical%2520Reasoning%2520over%2520EHRs%26entry.906535625%3DSkatje%2520Myers%2520and%2520Dmitriy%2520Dligach%2520and%2520Timothy%2520A.%2520Miller%2520and%2520Samantha%2520Barr%2520and%2520Yanjun%2520Gao%2520and%2520Matthew%2520Churpek%2520and%2520Anoop%2520Mayampurath%2520and%2520Majid%2520Afshar%26entry.1292438233%3D%2520%2520Electronic%2520health%2520records%2520%2528EHRs%2529%2520are%2520long%252C%2520noisy%252C%2520and%2520often%2520redundant%252C%2520posing%250Aa%2520major%2520challenge%2520for%2520the%2520clinicians%2520who%2520must%2520navigate%2520them.%2520Large%2520language%250Amodels%2520%2528LLMs%2529%2520offer%2520a%2520promising%2520solution%2520for%2520extracting%2520and%2520reasoning%2520over%2520this%250Aunstructured%2520text%252C%2520but%2520the%2520length%2520of%2520clinical%2520notes%2520often%2520exceeds%2520even%250Astate-of-the-art%2520models%2527%2520extended%2520context%2520windows.%2520Retrieval-augmented%250Ageneration%2520%2528RAG%2529%2520offers%2520an%2520alternative%2520by%2520retrieving%2520task-relevant%2520passages%250Afrom%2520across%2520the%2520entire%2520EHR%252C%2520potentially%2520reducing%2520the%2520amount%2520of%2520required%2520input%250Atokens.%2520In%2520this%2520work%252C%2520we%2520propose%2520three%2520clinical%2520tasks%2520designed%2520to%2520be%2520replicable%250Aacross%2520health%2520systems%2520with%2520minimal%2520effort%253A%25201%2529%2520extracting%2520imaging%2520procedures%252C%25202%2529%250Agenerating%2520timelines%2520of%2520antibiotic%2520use%252C%2520and%25203%2529%2520identifying%2520key%2520diagnoses.%2520Using%250AEHRs%2520from%2520actual%2520hospitalized%2520patients%252C%2520we%2520test%2520three%2520state-of-the-art%2520LLMs%250Awith%2520varying%2520amounts%2520of%2520provided%2520context%252C%2520using%2520either%2520targeted%2520text%2520retrieval%250Aor%2520the%2520most%2520recent%2520clinical%2520notes.%2520We%2520find%2520that%2520RAG%2520closely%2520matches%2520or%2520exceeds%250Athe%2520performance%2520of%2520using%2520recent%2520notes%252C%2520and%2520approaches%2520the%2520performance%2520of%2520using%250Athe%2520models%2527%2520full%2520context%2520while%2520requiring%2520drastically%2520fewer%2520input%2520tokens.%2520Our%250Aresults%2520suggest%2520that%2520RAG%2520remains%2520a%2520competitive%2520and%2520efficient%2520approach%2520even%2520as%250Anewer%2520models%2520become%2520capable%2520of%2520handling%2520increasingly%2520longer%2520amounts%2520of%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Retrieval-Augmented%20Generation%20vs.%20Long-Context%20Input%20for%0A%20%20Clinical%20Reasoning%20over%20EHRs&entry.906535625=Skatje%20Myers%20and%20Dmitriy%20Dligach%20and%20Timothy%20A.%20Miller%20and%20Samantha%20Barr%20and%20Yanjun%20Gao%20and%20Matthew%20Churpek%20and%20Anoop%20Mayampurath%20and%20Majid%20Afshar&entry.1292438233=%20%20Electronic%20health%20records%20%28EHRs%29%20are%20long%2C%20noisy%2C%20and%20often%20redundant%2C%20posing%0Aa%20major%20challenge%20for%20the%20clinicians%20who%20must%20navigate%20them.%20Large%20language%0Amodels%20%28LLMs%29%20offer%20a%20promising%20solution%20for%20extracting%20and%20reasoning%20over%20this%0Aunstructured%20text%2C%20but%20the%20length%20of%20clinical%20notes%20often%20exceeds%20even%0Astate-of-the-art%20models%27%20extended%20context%20windows.%20Retrieval-augmented%0Ageneration%20%28RAG%29%20offers%20an%20alternative%20by%20retrieving%20task-relevant%20passages%0Afrom%20across%20the%20entire%20EHR%2C%20potentially%20reducing%20the%20amount%20of%20required%20input%0Atokens.%20In%20this%20work%2C%20we%20propose%20three%20clinical%20tasks%20designed%20to%20be%20replicable%0Aacross%20health%20systems%20with%20minimal%20effort%3A%201%29%20extracting%20imaging%20procedures%2C%202%29%0Agenerating%20timelines%20of%20antibiotic%20use%2C%20and%203%29%20identifying%20key%20diagnoses.%20Using%0AEHRs%20from%20actual%20hospitalized%20patients%2C%20we%20test%20three%20state-of-the-art%20LLMs%0Awith%20varying%20amounts%20of%20provided%20context%2C%20using%20either%20targeted%20text%20retrieval%0Aor%20the%20most%20recent%20clinical%20notes.%20We%20find%20that%20RAG%20closely%20matches%20or%20exceeds%0Athe%20performance%20of%20using%20recent%20notes%2C%20and%20approaches%20the%20performance%20of%20using%0Athe%20models%27%20full%20context%20while%20requiring%20drastically%20fewer%20input%20tokens.%20Our%0Aresults%20suggest%20that%20RAG%20remains%20a%20competitive%20and%20efficient%20approach%20even%20as%0Anewer%20models%20become%20capable%20of%20handling%20increasingly%20longer%20amounts%20of%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14817v1&entry.124074799=Read"},
{"title": "Benchmarking graph construction by large language models for\n  coherence-driven inference", "author": "Steve Huntsman and Jewell Thomas", "abstract": "  We devise an algorithm to generate propositions that objectively instantiate\ngraphs supporting coherence-driven inference. We also benchmark the ability of\nlarge language models (LLMs) to reconstruct coherence graphs from (a simple\ntransformation of) propositions expressed in natural language, with promising\nresults from a single prompt to reasoning-optimized LLMs. For example,\no1/3/4-mini achieve perfect reconstruction half of the time on sparse graphs.\nCoherence-driven inference on consistency evaluations by LLMs may advance\nmachine cognition capabilities.\n", "link": "http://arxiv.org/abs/2502.13953v2", "date": "2025-08-20", "relevancy": 1.9093, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20graph%20construction%20by%20large%20language%20models%20for%0A%20%20coherence-driven%20inference&body=Title%3A%20Benchmarking%20graph%20construction%20by%20large%20language%20models%20for%0A%20%20coherence-driven%20inference%0AAuthor%3A%20Steve%20Huntsman%20and%20Jewell%20Thomas%0AAbstract%3A%20%20%20We%20devise%20an%20algorithm%20to%20generate%20propositions%20that%20objectively%20instantiate%0Agraphs%20supporting%20coherence-driven%20inference.%20We%20also%20benchmark%20the%20ability%20of%0Alarge%20language%20models%20%28LLMs%29%20to%20reconstruct%20coherence%20graphs%20from%20%28a%20simple%0Atransformation%20of%29%20propositions%20expressed%20in%20natural%20language%2C%20with%20promising%0Aresults%20from%20a%20single%20prompt%20to%20reasoning-optimized%20LLMs.%20For%20example%2C%0Ao1/3/4-mini%20achieve%20perfect%20reconstruction%20half%20of%20the%20time%20on%20sparse%20graphs.%0ACoherence-driven%20inference%20on%20consistency%20evaluations%20by%20LLMs%20may%20advance%0Amachine%20cognition%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520graph%2520construction%2520by%2520large%2520language%2520models%2520for%250A%2520%2520coherence-driven%2520inference%26entry.906535625%3DSteve%2520Huntsman%2520and%2520Jewell%2520Thomas%26entry.1292438233%3D%2520%2520We%2520devise%2520an%2520algorithm%2520to%2520generate%2520propositions%2520that%2520objectively%2520instantiate%250Agraphs%2520supporting%2520coherence-driven%2520inference.%2520We%2520also%2520benchmark%2520the%2520ability%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520reconstruct%2520coherence%2520graphs%2520from%2520%2528a%2520simple%250Atransformation%2520of%2529%2520propositions%2520expressed%2520in%2520natural%2520language%252C%2520with%2520promising%250Aresults%2520from%2520a%2520single%2520prompt%2520to%2520reasoning-optimized%2520LLMs.%2520For%2520example%252C%250Ao1/3/4-mini%2520achieve%2520perfect%2520reconstruction%2520half%2520of%2520the%2520time%2520on%2520sparse%2520graphs.%250ACoherence-driven%2520inference%2520on%2520consistency%2520evaluations%2520by%2520LLMs%2520may%2520advance%250Amachine%2520cognition%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20graph%20construction%20by%20large%20language%20models%20for%0A%20%20coherence-driven%20inference&entry.906535625=Steve%20Huntsman%20and%20Jewell%20Thomas&entry.1292438233=%20%20We%20devise%20an%20algorithm%20to%20generate%20propositions%20that%20objectively%20instantiate%0Agraphs%20supporting%20coherence-driven%20inference.%20We%20also%20benchmark%20the%20ability%20of%0Alarge%20language%20models%20%28LLMs%29%20to%20reconstruct%20coherence%20graphs%20from%20%28a%20simple%0Atransformation%20of%29%20propositions%20expressed%20in%20natural%20language%2C%20with%20promising%0Aresults%20from%20a%20single%20prompt%20to%20reasoning-optimized%20LLMs.%20For%20example%2C%0Ao1/3/4-mini%20achieve%20perfect%20reconstruction%20half%20of%20the%20time%20on%20sparse%20graphs.%0ACoherence-driven%20inference%20on%20consistency%20evaluations%20by%20LLMs%20may%20advance%0Amachine%20cognition%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13953v2&entry.124074799=Read"},
{"title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via\n  Synthetic Natural Language Inference", "author": "Samir Abdaljalil and Erchin Serpedin and Khalid Qaraqe and Hasan Kurban", "abstract": "  Large language models (LLMs) are increasingly applied in multilingual\ncontexts, yet their capacity for consistent, logically grounded alignment\nacross languages remains underexplored. We present a controlled evaluation\nframework for multilingual natural language inference (NLI) that generates\nsynthetic, logic-based premise-hypothesis pairs and translates them into a\ntypologically diverse set of languages. This design enables precise control\nover semantic relations and allows testing in both monolingual and\nmixed-language (code-switched) conditions. Surprisingly, code-switching does\nnot degrade, and can even improve, performance, suggesting that\ntranslation-induced lexical variation may serve as a regularization signal. We\nvalidate semantic preservation through embedding-based similarity analyses and\ncross-lingual alignment visualizations, confirming the fidelity of translated\npairs. Our findings expose both the potential and the brittleness of current\nLLM cross-lingual reasoning, and identify code-switching as a promising lever\nfor improving multilingual robustness. Code available at:\nhttps://github.com/KurbanIntelligenceLab/nli-stress-testing\n", "link": "http://arxiv.org/abs/2508.14735v1", "date": "2025-08-20", "relevancy": 1.9058, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4773}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Multilingual%20and%20Code-Switched%20Alignment%20in%20LLMs%20via%0A%20%20Synthetic%20Natural%20Language%20Inference&body=Title%3A%20Evaluating%20Multilingual%20and%20Code-Switched%20Alignment%20in%20LLMs%20via%0A%20%20Synthetic%20Natural%20Language%20Inference%0AAuthor%3A%20Samir%20Abdaljalil%20and%20Erchin%20Serpedin%20and%20Khalid%20Qaraqe%20and%20Hasan%20Kurban%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20applied%20in%20multilingual%0Acontexts%2C%20yet%20their%20capacity%20for%20consistent%2C%20logically%20grounded%20alignment%0Aacross%20languages%20remains%20underexplored.%20We%20present%20a%20controlled%20evaluation%0Aframework%20for%20multilingual%20natural%20language%20inference%20%28NLI%29%20that%20generates%0Asynthetic%2C%20logic-based%20premise-hypothesis%20pairs%20and%20translates%20them%20into%20a%0Atypologically%20diverse%20set%20of%20languages.%20This%20design%20enables%20precise%20control%0Aover%20semantic%20relations%20and%20allows%20testing%20in%20both%20monolingual%20and%0Amixed-language%20%28code-switched%29%20conditions.%20Surprisingly%2C%20code-switching%20does%0Anot%20degrade%2C%20and%20can%20even%20improve%2C%20performance%2C%20suggesting%20that%0Atranslation-induced%20lexical%20variation%20may%20serve%20as%20a%20regularization%20signal.%20We%0Avalidate%20semantic%20preservation%20through%20embedding-based%20similarity%20analyses%20and%0Across-lingual%20alignment%20visualizations%2C%20confirming%20the%20fidelity%20of%20translated%0Apairs.%20Our%20findings%20expose%20both%20the%20potential%20and%20the%20brittleness%20of%20current%0ALLM%20cross-lingual%20reasoning%2C%20and%20identify%20code-switching%20as%20a%20promising%20lever%0Afor%20improving%20multilingual%20robustness.%20Code%20available%20at%3A%0Ahttps%3A//github.com/KurbanIntelligenceLab/nli-stress-testing%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Multilingual%2520and%2520Code-Switched%2520Alignment%2520in%2520LLMs%2520via%250A%2520%2520Synthetic%2520Natural%2520Language%2520Inference%26entry.906535625%3DSamir%2520Abdaljalil%2520and%2520Erchin%2520Serpedin%2520and%2520Khalid%2520Qaraqe%2520and%2520Hasan%2520Kurban%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520applied%2520in%2520multilingual%250Acontexts%252C%2520yet%2520their%2520capacity%2520for%2520consistent%252C%2520logically%2520grounded%2520alignment%250Aacross%2520languages%2520remains%2520underexplored.%2520We%2520present%2520a%2520controlled%2520evaluation%250Aframework%2520for%2520multilingual%2520natural%2520language%2520inference%2520%2528NLI%2529%2520that%2520generates%250Asynthetic%252C%2520logic-based%2520premise-hypothesis%2520pairs%2520and%2520translates%2520them%2520into%2520a%250Atypologically%2520diverse%2520set%2520of%2520languages.%2520This%2520design%2520enables%2520precise%2520control%250Aover%2520semantic%2520relations%2520and%2520allows%2520testing%2520in%2520both%2520monolingual%2520and%250Amixed-language%2520%2528code-switched%2529%2520conditions.%2520Surprisingly%252C%2520code-switching%2520does%250Anot%2520degrade%252C%2520and%2520can%2520even%2520improve%252C%2520performance%252C%2520suggesting%2520that%250Atranslation-induced%2520lexical%2520variation%2520may%2520serve%2520as%2520a%2520regularization%2520signal.%2520We%250Avalidate%2520semantic%2520preservation%2520through%2520embedding-based%2520similarity%2520analyses%2520and%250Across-lingual%2520alignment%2520visualizations%252C%2520confirming%2520the%2520fidelity%2520of%2520translated%250Apairs.%2520Our%2520findings%2520expose%2520both%2520the%2520potential%2520and%2520the%2520brittleness%2520of%2520current%250ALLM%2520cross-lingual%2520reasoning%252C%2520and%2520identify%2520code-switching%2520as%2520a%2520promising%2520lever%250Afor%2520improving%2520multilingual%2520robustness.%2520Code%2520available%2520at%253A%250Ahttps%253A//github.com/KurbanIntelligenceLab/nli-stress-testing%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Multilingual%20and%20Code-Switched%20Alignment%20in%20LLMs%20via%0A%20%20Synthetic%20Natural%20Language%20Inference&entry.906535625=Samir%20Abdaljalil%20and%20Erchin%20Serpedin%20and%20Khalid%20Qaraqe%20and%20Hasan%20Kurban&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20applied%20in%20multilingual%0Acontexts%2C%20yet%20their%20capacity%20for%20consistent%2C%20logically%20grounded%20alignment%0Aacross%20languages%20remains%20underexplored.%20We%20present%20a%20controlled%20evaluation%0Aframework%20for%20multilingual%20natural%20language%20inference%20%28NLI%29%20that%20generates%0Asynthetic%2C%20logic-based%20premise-hypothesis%20pairs%20and%20translates%20them%20into%20a%0Atypologically%20diverse%20set%20of%20languages.%20This%20design%20enables%20precise%20control%0Aover%20semantic%20relations%20and%20allows%20testing%20in%20both%20monolingual%20and%0Amixed-language%20%28code-switched%29%20conditions.%20Surprisingly%2C%20code-switching%20does%0Anot%20degrade%2C%20and%20can%20even%20improve%2C%20performance%2C%20suggesting%20that%0Atranslation-induced%20lexical%20variation%20may%20serve%20as%20a%20regularization%20signal.%20We%0Avalidate%20semantic%20preservation%20through%20embedding-based%20similarity%20analyses%20and%0Across-lingual%20alignment%20visualizations%2C%20confirming%20the%20fidelity%20of%20translated%0Apairs.%20Our%20findings%20expose%20both%20the%20potential%20and%20the%20brittleness%20of%20current%0ALLM%20cross-lingual%20reasoning%2C%20and%20identify%20code-switching%20as%20a%20promising%20lever%0Afor%20improving%20multilingual%20robustness.%20Code%20available%20at%3A%0Ahttps%3A//github.com/KurbanIntelligenceLab/nli-stress-testing%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14735v1&entry.124074799=Read"},
{"title": "Learning to Solve Related Linear Systems", "author": "Disha Hegde and Jon Cockayne", "abstract": "  Solving multiple parametrised related systems is an essential component of\nmany numerical tasks, and learning from the already solved systems will make\nthis process faster. In this work, we propose a novel probabilistic linear\nsolver over the parameter space. This leverages information from the solved\nlinear systems in a regression setting to provide an efficient posterior mean\nand covariance. We advocate using this as companion regression model for the\npreconditioned conjugate gradient method, and discuss the favourable properties\nof the posterior mean and covariance as the initial guess and preconditioner.\nWe also provide several design choices for this companion solver. Numerical\nexperiments showcase the benefits of using our novel solver in a hyperparameter\noptimisation problem.\n", "link": "http://arxiv.org/abs/2503.17265v2", "date": "2025-08-20", "relevancy": 1.9029, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5189}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4734}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Solve%20Related%20Linear%20Systems&body=Title%3A%20Learning%20to%20Solve%20Related%20Linear%20Systems%0AAuthor%3A%20Disha%20Hegde%20and%20Jon%20Cockayne%0AAbstract%3A%20%20%20Solving%20multiple%20parametrised%20related%20systems%20is%20an%20essential%20component%20of%0Amany%20numerical%20tasks%2C%20and%20learning%20from%20the%20already%20solved%20systems%20will%20make%0Athis%20process%20faster.%20In%20this%20work%2C%20we%20propose%20a%20novel%20probabilistic%20linear%0Asolver%20over%20the%20parameter%20space.%20This%20leverages%20information%20from%20the%20solved%0Alinear%20systems%20in%20a%20regression%20setting%20to%20provide%20an%20efficient%20posterior%20mean%0Aand%20covariance.%20We%20advocate%20using%20this%20as%20companion%20regression%20model%20for%20the%0Apreconditioned%20conjugate%20gradient%20method%2C%20and%20discuss%20the%20favourable%20properties%0Aof%20the%20posterior%20mean%20and%20covariance%20as%20the%20initial%20guess%20and%20preconditioner.%0AWe%20also%20provide%20several%20design%20choices%20for%20this%20companion%20solver.%20Numerical%0Aexperiments%20showcase%20the%20benefits%20of%20using%20our%20novel%20solver%20in%20a%20hyperparameter%0Aoptimisation%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Solve%2520Related%2520Linear%2520Systems%26entry.906535625%3DDisha%2520Hegde%2520and%2520Jon%2520Cockayne%26entry.1292438233%3D%2520%2520Solving%2520multiple%2520parametrised%2520related%2520systems%2520is%2520an%2520essential%2520component%2520of%250Amany%2520numerical%2520tasks%252C%2520and%2520learning%2520from%2520the%2520already%2520solved%2520systems%2520will%2520make%250Athis%2520process%2520faster.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520probabilistic%2520linear%250Asolver%2520over%2520the%2520parameter%2520space.%2520This%2520leverages%2520information%2520from%2520the%2520solved%250Alinear%2520systems%2520in%2520a%2520regression%2520setting%2520to%2520provide%2520an%2520efficient%2520posterior%2520mean%250Aand%2520covariance.%2520We%2520advocate%2520using%2520this%2520as%2520companion%2520regression%2520model%2520for%2520the%250Apreconditioned%2520conjugate%2520gradient%2520method%252C%2520and%2520discuss%2520the%2520favourable%2520properties%250Aof%2520the%2520posterior%2520mean%2520and%2520covariance%2520as%2520the%2520initial%2520guess%2520and%2520preconditioner.%250AWe%2520also%2520provide%2520several%2520design%2520choices%2520for%2520this%2520companion%2520solver.%2520Numerical%250Aexperiments%2520showcase%2520the%2520benefits%2520of%2520using%2520our%2520novel%2520solver%2520in%2520a%2520hyperparameter%250Aoptimisation%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Solve%20Related%20Linear%20Systems&entry.906535625=Disha%20Hegde%20and%20Jon%20Cockayne&entry.1292438233=%20%20Solving%20multiple%20parametrised%20related%20systems%20is%20an%20essential%20component%20of%0Amany%20numerical%20tasks%2C%20and%20learning%20from%20the%20already%20solved%20systems%20will%20make%0Athis%20process%20faster.%20In%20this%20work%2C%20we%20propose%20a%20novel%20probabilistic%20linear%0Asolver%20over%20the%20parameter%20space.%20This%20leverages%20information%20from%20the%20solved%0Alinear%20systems%20in%20a%20regression%20setting%20to%20provide%20an%20efficient%20posterior%20mean%0Aand%20covariance.%20We%20advocate%20using%20this%20as%20companion%20regression%20model%20for%20the%0Apreconditioned%20conjugate%20gradient%20method%2C%20and%20discuss%20the%20favourable%20properties%0Aof%20the%20posterior%20mean%20and%20covariance%20as%20the%20initial%20guess%20and%20preconditioner.%0AWe%20also%20provide%20several%20design%20choices%20for%20this%20companion%20solver.%20Numerical%0Aexperiments%20showcase%20the%20benefits%20of%20using%20our%20novel%20solver%20in%20a%20hyperparameter%0Aoptimisation%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17265v2&entry.124074799=Read"},
{"title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware\n  Planning and Coordination", "author": "Jo\u00e3o Vitor de Carvalho Silva and Douglas G. Macharet", "abstract": "  The ability to coordinate actions across multiple agents is critical for\nsolving complex, real-world problems. Large Language Models (LLMs) have shown\nstrong capabilities in communication, planning, and reasoning, raising the\nquestion of whether they can also support effective collaboration in\nmulti-agent settings. In this work, we investigate the use of LLM agents to\nsolve a structured victim rescue task that requires division of labor,\nprioritization, and cooperative planning. Agents operate in a fully known\ngraph-based environment and must allocate resources to victims with varying\nneeds and urgency levels. We systematically evaluate their performance using a\nsuite of coordination-sensitive metrics, including task success rate, redundant\nactions, room conflicts, and urgency-weighted efficiency. This study offers new\ninsights into the strengths and failure modes of LLMs in physically grounded\nmulti-agent collaboration tasks, contributing to future benchmarks and\narchitectural improvements.\n", "link": "http://arxiv.org/abs/2508.14635v1", "date": "2025-08-20", "relevancy": 1.8914, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.47}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLM%20Agents%20Solve%20Collaborative%20Tasks%3F%20A%20Study%20on%20Urgency-Aware%0A%20%20Planning%20and%20Coordination&body=Title%3A%20Can%20LLM%20Agents%20Solve%20Collaborative%20Tasks%3F%20A%20Study%20on%20Urgency-Aware%0A%20%20Planning%20and%20Coordination%0AAuthor%3A%20Jo%C3%A3o%20Vitor%20de%20Carvalho%20Silva%20and%20Douglas%20G.%20Macharet%0AAbstract%3A%20%20%20The%20ability%20to%20coordinate%20actions%20across%20multiple%20agents%20is%20critical%20for%0Asolving%20complex%2C%20real-world%20problems.%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%0Astrong%20capabilities%20in%20communication%2C%20planning%2C%20and%20reasoning%2C%20raising%20the%0Aquestion%20of%20whether%20they%20can%20also%20support%20effective%20collaboration%20in%0Amulti-agent%20settings.%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20LLM%20agents%20to%0Asolve%20a%20structured%20victim%20rescue%20task%20that%20requires%20division%20of%20labor%2C%0Aprioritization%2C%20and%20cooperative%20planning.%20Agents%20operate%20in%20a%20fully%20known%0Agraph-based%20environment%20and%20must%20allocate%20resources%20to%20victims%20with%20varying%0Aneeds%20and%20urgency%20levels.%20We%20systematically%20evaluate%20their%20performance%20using%20a%0Asuite%20of%20coordination-sensitive%20metrics%2C%20including%20task%20success%20rate%2C%20redundant%0Aactions%2C%20room%20conflicts%2C%20and%20urgency-weighted%20efficiency.%20This%20study%20offers%20new%0Ainsights%20into%20the%20strengths%20and%20failure%20modes%20of%20LLMs%20in%20physically%20grounded%0Amulti-agent%20collaboration%20tasks%2C%20contributing%20to%20future%20benchmarks%20and%0Aarchitectural%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLM%2520Agents%2520Solve%2520Collaborative%2520Tasks%253F%2520A%2520Study%2520on%2520Urgency-Aware%250A%2520%2520Planning%2520and%2520Coordination%26entry.906535625%3DJo%25C3%25A3o%2520Vitor%2520de%2520Carvalho%2520Silva%2520and%2520Douglas%2520G.%2520Macharet%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520coordinate%2520actions%2520across%2520multiple%2520agents%2520is%2520critical%2520for%250Asolving%2520complex%252C%2520real-world%2520problems.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%250Astrong%2520capabilities%2520in%2520communication%252C%2520planning%252C%2520and%2520reasoning%252C%2520raising%2520the%250Aquestion%2520of%2520whether%2520they%2520can%2520also%2520support%2520effective%2520collaboration%2520in%250Amulti-agent%2520settings.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520use%2520of%2520LLM%2520agents%2520to%250Asolve%2520a%2520structured%2520victim%2520rescue%2520task%2520that%2520requires%2520division%2520of%2520labor%252C%250Aprioritization%252C%2520and%2520cooperative%2520planning.%2520Agents%2520operate%2520in%2520a%2520fully%2520known%250Agraph-based%2520environment%2520and%2520must%2520allocate%2520resources%2520to%2520victims%2520with%2520varying%250Aneeds%2520and%2520urgency%2520levels.%2520We%2520systematically%2520evaluate%2520their%2520performance%2520using%2520a%250Asuite%2520of%2520coordination-sensitive%2520metrics%252C%2520including%2520task%2520success%2520rate%252C%2520redundant%250Aactions%252C%2520room%2520conflicts%252C%2520and%2520urgency-weighted%2520efficiency.%2520This%2520study%2520offers%2520new%250Ainsights%2520into%2520the%2520strengths%2520and%2520failure%2520modes%2520of%2520LLMs%2520in%2520physically%2520grounded%250Amulti-agent%2520collaboration%2520tasks%252C%2520contributing%2520to%2520future%2520benchmarks%2520and%250Aarchitectural%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLM%20Agents%20Solve%20Collaborative%20Tasks%3F%20A%20Study%20on%20Urgency-Aware%0A%20%20Planning%20and%20Coordination&entry.906535625=Jo%C3%A3o%20Vitor%20de%20Carvalho%20Silva%20and%20Douglas%20G.%20Macharet&entry.1292438233=%20%20The%20ability%20to%20coordinate%20actions%20across%20multiple%20agents%20is%20critical%20for%0Asolving%20complex%2C%20real-world%20problems.%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%0Astrong%20capabilities%20in%20communication%2C%20planning%2C%20and%20reasoning%2C%20raising%20the%0Aquestion%20of%20whether%20they%20can%20also%20support%20effective%20collaboration%20in%0Amulti-agent%20settings.%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20LLM%20agents%20to%0Asolve%20a%20structured%20victim%20rescue%20task%20that%20requires%20division%20of%20labor%2C%0Aprioritization%2C%20and%20cooperative%20planning.%20Agents%20operate%20in%20a%20fully%20known%0Agraph-based%20environment%20and%20must%20allocate%20resources%20to%20victims%20with%20varying%0Aneeds%20and%20urgency%20levels.%20We%20systematically%20evaluate%20their%20performance%20using%20a%0Asuite%20of%20coordination-sensitive%20metrics%2C%20including%20task%20success%20rate%2C%20redundant%0Aactions%2C%20room%20conflicts%2C%20and%20urgency-weighted%20efficiency.%20This%20study%20offers%20new%0Ainsights%20into%20the%20strengths%20and%20failure%20modes%20of%20LLMs%20in%20physically%20grounded%0Amulti-agent%20collaboration%20tasks%2C%20contributing%20to%20future%20benchmarks%20and%0Aarchitectural%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14635v1&entry.124074799=Read"},
{"title": "Learning from user's behaviour of some well-known congested traffic\n  networks", "author": "Isolda Cardoso and Lucas Venturato and Jorgelina Walpen", "abstract": "  We consider the problem of predicting users' behavior of a congested traffic\nnetwork under an equilibrium condition, the traffic assignment problem. We\npropose a two-stage machine learning approach which couples a neural network\nwith a fixed point algorithm, and we evaluate its performance along several\nclassical congested traffic networks.\n", "link": "http://arxiv.org/abs/2508.14804v1", "date": "2025-08-20", "relevancy": 1.8859, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4984}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4576}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20user%27s%20behaviour%20of%20some%20well-known%20congested%20traffic%0A%20%20networks&body=Title%3A%20Learning%20from%20user%27s%20behaviour%20of%20some%20well-known%20congested%20traffic%0A%20%20networks%0AAuthor%3A%20Isolda%20Cardoso%20and%20Lucas%20Venturato%20and%20Jorgelina%20Walpen%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20predicting%20users%27%20behavior%20of%20a%20congested%20traffic%0Anetwork%20under%20an%20equilibrium%20condition%2C%20the%20traffic%20assignment%20problem.%20We%0Apropose%20a%20two-stage%20machine%20learning%20approach%20which%20couples%20a%20neural%20network%0Awith%20a%20fixed%20point%20algorithm%2C%20and%20we%20evaluate%20its%20performance%20along%20several%0Aclassical%20congested%20traffic%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520user%2527s%2520behaviour%2520of%2520some%2520well-known%2520congested%2520traffic%250A%2520%2520networks%26entry.906535625%3DIsolda%2520Cardoso%2520and%2520Lucas%2520Venturato%2520and%2520Jorgelina%2520Walpen%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520predicting%2520users%2527%2520behavior%2520of%2520a%2520congested%2520traffic%250Anetwork%2520under%2520an%2520equilibrium%2520condition%252C%2520the%2520traffic%2520assignment%2520problem.%2520We%250Apropose%2520a%2520two-stage%2520machine%2520learning%2520approach%2520which%2520couples%2520a%2520neural%2520network%250Awith%2520a%2520fixed%2520point%2520algorithm%252C%2520and%2520we%2520evaluate%2520its%2520performance%2520along%2520several%250Aclassical%2520congested%2520traffic%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20user%27s%20behaviour%20of%20some%20well-known%20congested%20traffic%0A%20%20networks&entry.906535625=Isolda%20Cardoso%20and%20Lucas%20Venturato%20and%20Jorgelina%20Walpen&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20predicting%20users%27%20behavior%20of%20a%20congested%20traffic%0Anetwork%20under%20an%20equilibrium%20condition%2C%20the%20traffic%20assignment%20problem.%20We%0Apropose%20a%20two-stage%20machine%20learning%20approach%20which%20couples%20a%20neural%20network%0Awith%20a%20fixed%20point%20algorithm%2C%20and%20we%20evaluate%20its%20performance%20along%20several%0Aclassical%20congested%20traffic%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14804v1&entry.124074799=Read"},
{"title": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT\n  SFT and Reinforcement Learning", "author": "Ruheng Wang and Hang Zhang and Trieu Nguyen and Shasha Feng and Hao-Wei Pang and Xiang Yu and Li Xiao and Peter Zhiping Zhang", "abstract": "  Designing therapeutic peptides with tailored properties is hindered by the\nvastness of sequence space, limited experimental data, and poor\ninterpretability of current generative models. To address these challenges, we\nintroduce PepThink-R1, a generative framework that integrates large language\nmodels (LLMs) with chain-of-thought (CoT) supervised fine-tuning and\nreinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly\nreasons about monomer-level modifications during sequence generation, enabling\ninterpretable design choices while optimizing for multiple pharmacological\nproperties. Guided by a tailored reward function balancing chemical validity\nand property improvements, the model autonomously explores diverse sequence\nvariants. We demonstrate that PepThink-R1 generates cyclic peptides with\nsignificantly enhanced lipophilicity, stability, and exposure, outperforming\nexisting general LLMs (e.g., GPT-5) and domain-specific baseline in both\noptimization success and interpretability. To our knowledge, this is the first\nLLM-based peptide design framework that combines explicit reasoning with\nRL-driven property control, marking a step toward reliable and transparent\npeptide optimization for therapeutic discovery.\n", "link": "http://arxiv.org/abs/2508.14765v1", "date": "2025-08-20", "relevancy": 1.8767, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4762}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4656}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PepThink-R1%3A%20LLM%20for%20Interpretable%20Cyclic%20Peptide%20Optimization%20with%20CoT%0A%20%20SFT%20and%20Reinforcement%20Learning&body=Title%3A%20PepThink-R1%3A%20LLM%20for%20Interpretable%20Cyclic%20Peptide%20Optimization%20with%20CoT%0A%20%20SFT%20and%20Reinforcement%20Learning%0AAuthor%3A%20Ruheng%20Wang%20and%20Hang%20Zhang%20and%20Trieu%20Nguyen%20and%20Shasha%20Feng%20and%20Hao-Wei%20Pang%20and%20Xiang%20Yu%20and%20Li%20Xiao%20and%20Peter%20Zhiping%20Zhang%0AAbstract%3A%20%20%20Designing%20therapeutic%20peptides%20with%20tailored%20properties%20is%20hindered%20by%20the%0Avastness%20of%20sequence%20space%2C%20limited%20experimental%20data%2C%20and%20poor%0Ainterpretability%20of%20current%20generative%20models.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20PepThink-R1%2C%20a%20generative%20framework%20that%20integrates%20large%20language%0Amodels%20%28LLMs%29%20with%20chain-of-thought%20%28CoT%29%20supervised%20fine-tuning%20and%0Areinforcement%20learning%20%28RL%29.%20Unlike%20prior%20approaches%2C%20PepThink-R1%20explicitly%0Areasons%20about%20monomer-level%20modifications%20during%20sequence%20generation%2C%20enabling%0Ainterpretable%20design%20choices%20while%20optimizing%20for%20multiple%20pharmacological%0Aproperties.%20Guided%20by%20a%20tailored%20reward%20function%20balancing%20chemical%20validity%0Aand%20property%20improvements%2C%20the%20model%20autonomously%20explores%20diverse%20sequence%0Avariants.%20We%20demonstrate%20that%20PepThink-R1%20generates%20cyclic%20peptides%20with%0Asignificantly%20enhanced%20lipophilicity%2C%20stability%2C%20and%20exposure%2C%20outperforming%0Aexisting%20general%20LLMs%20%28e.g.%2C%20GPT-5%29%20and%20domain-specific%20baseline%20in%20both%0Aoptimization%20success%20and%20interpretability.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0ALLM-based%20peptide%20design%20framework%20that%20combines%20explicit%20reasoning%20with%0ARL-driven%20property%20control%2C%20marking%20a%20step%20toward%20reliable%20and%20transparent%0Apeptide%20optimization%20for%20therapeutic%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPepThink-R1%253A%2520LLM%2520for%2520Interpretable%2520Cyclic%2520Peptide%2520Optimization%2520with%2520CoT%250A%2520%2520SFT%2520and%2520Reinforcement%2520Learning%26entry.906535625%3DRuheng%2520Wang%2520and%2520Hang%2520Zhang%2520and%2520Trieu%2520Nguyen%2520and%2520Shasha%2520Feng%2520and%2520Hao-Wei%2520Pang%2520and%2520Xiang%2520Yu%2520and%2520Li%2520Xiao%2520and%2520Peter%2520Zhiping%2520Zhang%26entry.1292438233%3D%2520%2520Designing%2520therapeutic%2520peptides%2520with%2520tailored%2520properties%2520is%2520hindered%2520by%2520the%250Avastness%2520of%2520sequence%2520space%252C%2520limited%2520experimental%2520data%252C%2520and%2520poor%250Ainterpretability%2520of%2520current%2520generative%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520PepThink-R1%252C%2520a%2520generative%2520framework%2520that%2520integrates%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520with%2520chain-of-thought%2520%2528CoT%2529%2520supervised%2520fine-tuning%2520and%250Areinforcement%2520learning%2520%2528RL%2529.%2520Unlike%2520prior%2520approaches%252C%2520PepThink-R1%2520explicitly%250Areasons%2520about%2520monomer-level%2520modifications%2520during%2520sequence%2520generation%252C%2520enabling%250Ainterpretable%2520design%2520choices%2520while%2520optimizing%2520for%2520multiple%2520pharmacological%250Aproperties.%2520Guided%2520by%2520a%2520tailored%2520reward%2520function%2520balancing%2520chemical%2520validity%250Aand%2520property%2520improvements%252C%2520the%2520model%2520autonomously%2520explores%2520diverse%2520sequence%250Avariants.%2520We%2520demonstrate%2520that%2520PepThink-R1%2520generates%2520cyclic%2520peptides%2520with%250Asignificantly%2520enhanced%2520lipophilicity%252C%2520stability%252C%2520and%2520exposure%252C%2520outperforming%250Aexisting%2520general%2520LLMs%2520%2528e.g.%252C%2520GPT-5%2529%2520and%2520domain-specific%2520baseline%2520in%2520both%250Aoptimization%2520success%2520and%2520interpretability.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250ALLM-based%2520peptide%2520design%2520framework%2520that%2520combines%2520explicit%2520reasoning%2520with%250ARL-driven%2520property%2520control%252C%2520marking%2520a%2520step%2520toward%2520reliable%2520and%2520transparent%250Apeptide%2520optimization%2520for%2520therapeutic%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PepThink-R1%3A%20LLM%20for%20Interpretable%20Cyclic%20Peptide%20Optimization%20with%20CoT%0A%20%20SFT%20and%20Reinforcement%20Learning&entry.906535625=Ruheng%20Wang%20and%20Hang%20Zhang%20and%20Trieu%20Nguyen%20and%20Shasha%20Feng%20and%20Hao-Wei%20Pang%20and%20Xiang%20Yu%20and%20Li%20Xiao%20and%20Peter%20Zhiping%20Zhang&entry.1292438233=%20%20Designing%20therapeutic%20peptides%20with%20tailored%20properties%20is%20hindered%20by%20the%0Avastness%20of%20sequence%20space%2C%20limited%20experimental%20data%2C%20and%20poor%0Ainterpretability%20of%20current%20generative%20models.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20PepThink-R1%2C%20a%20generative%20framework%20that%20integrates%20large%20language%0Amodels%20%28LLMs%29%20with%20chain-of-thought%20%28CoT%29%20supervised%20fine-tuning%20and%0Areinforcement%20learning%20%28RL%29.%20Unlike%20prior%20approaches%2C%20PepThink-R1%20explicitly%0Areasons%20about%20monomer-level%20modifications%20during%20sequence%20generation%2C%20enabling%0Ainterpretable%20design%20choices%20while%20optimizing%20for%20multiple%20pharmacological%0Aproperties.%20Guided%20by%20a%20tailored%20reward%20function%20balancing%20chemical%20validity%0Aand%20property%20improvements%2C%20the%20model%20autonomously%20explores%20diverse%20sequence%0Avariants.%20We%20demonstrate%20that%20PepThink-R1%20generates%20cyclic%20peptides%20with%0Asignificantly%20enhanced%20lipophilicity%2C%20stability%2C%20and%20exposure%2C%20outperforming%0Aexisting%20general%20LLMs%20%28e.g.%2C%20GPT-5%29%20and%20domain-specific%20baseline%20in%20both%0Aoptimization%20success%20and%20interpretability.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0ALLM-based%20peptide%20design%20framework%20that%20combines%20explicit%20reasoning%20with%0ARL-driven%20property%20control%2C%20marking%20a%20step%20toward%20reliable%20and%20transparent%0Apeptide%20optimization%20for%20therapeutic%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14765v1&entry.124074799=Read"},
{"title": "Towards the Use of Saliency Maps for Explaining Low-Quality\n  Electrocardiograms to End Users", "author": "Ana Lucic and Sheeraz Ahmad and Amanda Furtado Brinhosa and Vera Liao and Himani Agrawal and Umang Bhatt and Krishnaram Kenthapadi and Alice Xiang and Maarten de Rijke and Nicholas Drabowski", "abstract": "  When using medical images for diagnosis, either by clinicians or artificial\nintelligence (AI) systems, it is important that the images are of high quality.\nWhen an image is of low quality, the medical exam that produced the image often\nneeds to be redone. In telemedicine, a common problem is that the quality issue\nis only flagged once the patient has left the clinic, meaning they must return\nin order to have the exam redone. This can be especially difficult for people\nliving in remote regions, who make up a substantial portion of the patients at\nPortal Telemedicina, a digital healthcare organization based in Brazil. In this\npaper, we report on ongoing work regarding (i) the development of an AI system\nfor flagging and explaining low-quality medical images in real-time, (ii) an\ninterview study to understand the explanation needs of stakeholders using the\nAI system at OurCompany, and, (iii) a longitudinal user study design to examine\nthe effect of including explanations on the workflow of the technicians in our\nclinics. To the best of our knowledge, this would be the first longitudinal\nstudy on evaluating the effects of XAI methods on end-users -- stakeholders\nthat use AI systems but do not have AI-specific expertise. We welcome feedback\nand suggestions on our experimental setup.\n", "link": "http://arxiv.org/abs/2207.02726v2", "date": "2025-08-20", "relevancy": 1.8691, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4951}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4671}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20the%20Use%20of%20Saliency%20Maps%20for%20Explaining%20Low-Quality%0A%20%20Electrocardiograms%20to%20End%20Users&body=Title%3A%20Towards%20the%20Use%20of%20Saliency%20Maps%20for%20Explaining%20Low-Quality%0A%20%20Electrocardiograms%20to%20End%20Users%0AAuthor%3A%20Ana%20Lucic%20and%20Sheeraz%20Ahmad%20and%20Amanda%20Furtado%20Brinhosa%20and%20Vera%20Liao%20and%20Himani%20Agrawal%20and%20Umang%20Bhatt%20and%20Krishnaram%20Kenthapadi%20and%20Alice%20Xiang%20and%20Maarten%20de%20Rijke%20and%20Nicholas%20Drabowski%0AAbstract%3A%20%20%20When%20using%20medical%20images%20for%20diagnosis%2C%20either%20by%20clinicians%20or%20artificial%0Aintelligence%20%28AI%29%20systems%2C%20it%20is%20important%20that%20the%20images%20are%20of%20high%20quality.%0AWhen%20an%20image%20is%20of%20low%20quality%2C%20the%20medical%20exam%20that%20produced%20the%20image%20often%0Aneeds%20to%20be%20redone.%20In%20telemedicine%2C%20a%20common%20problem%20is%20that%20the%20quality%20issue%0Ais%20only%20flagged%20once%20the%20patient%20has%20left%20the%20clinic%2C%20meaning%20they%20must%20return%0Ain%20order%20to%20have%20the%20exam%20redone.%20This%20can%20be%20especially%20difficult%20for%20people%0Aliving%20in%20remote%20regions%2C%20who%20make%20up%20a%20substantial%20portion%20of%20the%20patients%20at%0APortal%20Telemedicina%2C%20a%20digital%20healthcare%20organization%20based%20in%20Brazil.%20In%20this%0Apaper%2C%20we%20report%20on%20ongoing%20work%20regarding%20%28i%29%20the%20development%20of%20an%20AI%20system%0Afor%20flagging%20and%20explaining%20low-quality%20medical%20images%20in%20real-time%2C%20%28ii%29%20an%0Ainterview%20study%20to%20understand%20the%20explanation%20needs%20of%20stakeholders%20using%20the%0AAI%20system%20at%20OurCompany%2C%20and%2C%20%28iii%29%20a%20longitudinal%20user%20study%20design%20to%20examine%0Athe%20effect%20of%20including%20explanations%20on%20the%20workflow%20of%20the%20technicians%20in%20our%0Aclinics.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20would%20be%20the%20first%20longitudinal%0Astudy%20on%20evaluating%20the%20effects%20of%20XAI%20methods%20on%20end-users%20--%20stakeholders%0Athat%20use%20AI%20systems%20but%20do%20not%20have%20AI-specific%20expertise.%20We%20welcome%20feedback%0Aand%20suggestions%20on%20our%20experimental%20setup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.02726v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520the%2520Use%2520of%2520Saliency%2520Maps%2520for%2520Explaining%2520Low-Quality%250A%2520%2520Electrocardiograms%2520to%2520End%2520Users%26entry.906535625%3DAna%2520Lucic%2520and%2520Sheeraz%2520Ahmad%2520and%2520Amanda%2520Furtado%2520Brinhosa%2520and%2520Vera%2520Liao%2520and%2520Himani%2520Agrawal%2520and%2520Umang%2520Bhatt%2520and%2520Krishnaram%2520Kenthapadi%2520and%2520Alice%2520Xiang%2520and%2520Maarten%2520de%2520Rijke%2520and%2520Nicholas%2520Drabowski%26entry.1292438233%3D%2520%2520When%2520using%2520medical%2520images%2520for%2520diagnosis%252C%2520either%2520by%2520clinicians%2520or%2520artificial%250Aintelligence%2520%2528AI%2529%2520systems%252C%2520it%2520is%2520important%2520that%2520the%2520images%2520are%2520of%2520high%2520quality.%250AWhen%2520an%2520image%2520is%2520of%2520low%2520quality%252C%2520the%2520medical%2520exam%2520that%2520produced%2520the%2520image%2520often%250Aneeds%2520to%2520be%2520redone.%2520In%2520telemedicine%252C%2520a%2520common%2520problem%2520is%2520that%2520the%2520quality%2520issue%250Ais%2520only%2520flagged%2520once%2520the%2520patient%2520has%2520left%2520the%2520clinic%252C%2520meaning%2520they%2520must%2520return%250Ain%2520order%2520to%2520have%2520the%2520exam%2520redone.%2520This%2520can%2520be%2520especially%2520difficult%2520for%2520people%250Aliving%2520in%2520remote%2520regions%252C%2520who%2520make%2520up%2520a%2520substantial%2520portion%2520of%2520the%2520patients%2520at%250APortal%2520Telemedicina%252C%2520a%2520digital%2520healthcare%2520organization%2520based%2520in%2520Brazil.%2520In%2520this%250Apaper%252C%2520we%2520report%2520on%2520ongoing%2520work%2520regarding%2520%2528i%2529%2520the%2520development%2520of%2520an%2520AI%2520system%250Afor%2520flagging%2520and%2520explaining%2520low-quality%2520medical%2520images%2520in%2520real-time%252C%2520%2528ii%2529%2520an%250Ainterview%2520study%2520to%2520understand%2520the%2520explanation%2520needs%2520of%2520stakeholders%2520using%2520the%250AAI%2520system%2520at%2520OurCompany%252C%2520and%252C%2520%2528iii%2529%2520a%2520longitudinal%2520user%2520study%2520design%2520to%2520examine%250Athe%2520effect%2520of%2520including%2520explanations%2520on%2520the%2520workflow%2520of%2520the%2520technicians%2520in%2520our%250Aclinics.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520would%2520be%2520the%2520first%2520longitudinal%250Astudy%2520on%2520evaluating%2520the%2520effects%2520of%2520XAI%2520methods%2520on%2520end-users%2520--%2520stakeholders%250Athat%2520use%2520AI%2520systems%2520but%2520do%2520not%2520have%2520AI-specific%2520expertise.%2520We%2520welcome%2520feedback%250Aand%2520suggestions%2520on%2520our%2520experimental%2520setup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.02726v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20the%20Use%20of%20Saliency%20Maps%20for%20Explaining%20Low-Quality%0A%20%20Electrocardiograms%20to%20End%20Users&entry.906535625=Ana%20Lucic%20and%20Sheeraz%20Ahmad%20and%20Amanda%20Furtado%20Brinhosa%20and%20Vera%20Liao%20and%20Himani%20Agrawal%20and%20Umang%20Bhatt%20and%20Krishnaram%20Kenthapadi%20and%20Alice%20Xiang%20and%20Maarten%20de%20Rijke%20and%20Nicholas%20Drabowski&entry.1292438233=%20%20When%20using%20medical%20images%20for%20diagnosis%2C%20either%20by%20clinicians%20or%20artificial%0Aintelligence%20%28AI%29%20systems%2C%20it%20is%20important%20that%20the%20images%20are%20of%20high%20quality.%0AWhen%20an%20image%20is%20of%20low%20quality%2C%20the%20medical%20exam%20that%20produced%20the%20image%20often%0Aneeds%20to%20be%20redone.%20In%20telemedicine%2C%20a%20common%20problem%20is%20that%20the%20quality%20issue%0Ais%20only%20flagged%20once%20the%20patient%20has%20left%20the%20clinic%2C%20meaning%20they%20must%20return%0Ain%20order%20to%20have%20the%20exam%20redone.%20This%20can%20be%20especially%20difficult%20for%20people%0Aliving%20in%20remote%20regions%2C%20who%20make%20up%20a%20substantial%20portion%20of%20the%20patients%20at%0APortal%20Telemedicina%2C%20a%20digital%20healthcare%20organization%20based%20in%20Brazil.%20In%20this%0Apaper%2C%20we%20report%20on%20ongoing%20work%20regarding%20%28i%29%20the%20development%20of%20an%20AI%20system%0Afor%20flagging%20and%20explaining%20low-quality%20medical%20images%20in%20real-time%2C%20%28ii%29%20an%0Ainterview%20study%20to%20understand%20the%20explanation%20needs%20of%20stakeholders%20using%20the%0AAI%20system%20at%20OurCompany%2C%20and%2C%20%28iii%29%20a%20longitudinal%20user%20study%20design%20to%20examine%0Athe%20effect%20of%20including%20explanations%20on%20the%20workflow%20of%20the%20technicians%20in%20our%0Aclinics.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20would%20be%20the%20first%20longitudinal%0Astudy%20on%20evaluating%20the%20effects%20of%20XAI%20methods%20on%20end-users%20--%20stakeholders%0Athat%20use%20AI%20systems%20but%20do%20not%20have%20AI-specific%20expertise.%20We%20welcome%20feedback%0Aand%20suggestions%20on%20our%20experimental%20setup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.02726v2&entry.124074799=Read"},
{"title": "AFABench: A Generic Framework for Benchmarking Active Feature\n  Acquisition", "author": "Valter Sch\u00fctz and Han Wu and Reza Rezvan and Linus Aronsson and Morteza Haghir Chehreghani", "abstract": "  In many real-world scenarios, acquiring all features of a data instance can\nbe expensive or impractical due to monetary cost, latency, or privacy concerns.\nActive Feature Acquisition (AFA) addresses this challenge by dynamically\nselecting a subset of informative features for each data instance, trading\npredictive performance against acquisition cost. While numerous methods have\nbeen proposed for AFA, ranging from greedy information-theoretic strategies to\nnon-myopic reinforcement learning approaches, fair and systematic evaluation of\nthese methods has been hindered by the lack of standardized benchmarks. In this\npaper, we introduce AFABench, the first benchmark framework for AFA. Our\nbenchmark includes a diverse set of synthetic and real-world datasets, supports\na wide range of acquisition policies, and provides a modular design that\nenables easy integration of new methods and tasks. We implement and evaluate\nrepresentative algorithms from all major categories, including static, greedy,\nand reinforcement learning-based approaches. To test the lookahead capabilities\nof AFA policies, we introduce a novel synthetic dataset, AFAContext, designed\nto expose the limitations of greedy selection. Our results highlight key\ntrade-offs between different AFA strategies and provide actionable insights for\nfuture research. The benchmark code is available at:\nhttps://github.com/Linusaronsson/AFA-Benchmark.\n", "link": "http://arxiv.org/abs/2508.14734v1", "date": "2025-08-20", "relevancy": 1.8672, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4989}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AFABench%3A%20A%20Generic%20Framework%20for%20Benchmarking%20Active%20Feature%0A%20%20Acquisition&body=Title%3A%20AFABench%3A%20A%20Generic%20Framework%20for%20Benchmarking%20Active%20Feature%0A%20%20Acquisition%0AAuthor%3A%20Valter%20Sch%C3%BCtz%20and%20Han%20Wu%20and%20Reza%20Rezvan%20and%20Linus%20Aronsson%20and%20Morteza%20Haghir%20Chehreghani%0AAbstract%3A%20%20%20In%20many%20real-world%20scenarios%2C%20acquiring%20all%20features%20of%20a%20data%20instance%20can%0Abe%20expensive%20or%20impractical%20due%20to%20monetary%20cost%2C%20latency%2C%20or%20privacy%20concerns.%0AActive%20Feature%20Acquisition%20%28AFA%29%20addresses%20this%20challenge%20by%20dynamically%0Aselecting%20a%20subset%20of%20informative%20features%20for%20each%20data%20instance%2C%20trading%0Apredictive%20performance%20against%20acquisition%20cost.%20While%20numerous%20methods%20have%0Abeen%20proposed%20for%20AFA%2C%20ranging%20from%20greedy%20information-theoretic%20strategies%20to%0Anon-myopic%20reinforcement%20learning%20approaches%2C%20fair%20and%20systematic%20evaluation%20of%0Athese%20methods%20has%20been%20hindered%20by%20the%20lack%20of%20standardized%20benchmarks.%20In%20this%0Apaper%2C%20we%20introduce%20AFABench%2C%20the%20first%20benchmark%20framework%20for%20AFA.%20Our%0Abenchmark%20includes%20a%20diverse%20set%20of%20synthetic%20and%20real-world%20datasets%2C%20supports%0Aa%20wide%20range%20of%20acquisition%20policies%2C%20and%20provides%20a%20modular%20design%20that%0Aenables%20easy%20integration%20of%20new%20methods%20and%20tasks.%20We%20implement%20and%20evaluate%0Arepresentative%20algorithms%20from%20all%20major%20categories%2C%20including%20static%2C%20greedy%2C%0Aand%20reinforcement%20learning-based%20approaches.%20To%20test%20the%20lookahead%20capabilities%0Aof%20AFA%20policies%2C%20we%20introduce%20a%20novel%20synthetic%20dataset%2C%20AFAContext%2C%20designed%0Ato%20expose%20the%20limitations%20of%20greedy%20selection.%20Our%20results%20highlight%20key%0Atrade-offs%20between%20different%20AFA%20strategies%20and%20provide%20actionable%20insights%20for%0Afuture%20research.%20The%20benchmark%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Linusaronsson/AFA-Benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAFABench%253A%2520A%2520Generic%2520Framework%2520for%2520Benchmarking%2520Active%2520Feature%250A%2520%2520Acquisition%26entry.906535625%3DValter%2520Sch%25C3%25BCtz%2520and%2520Han%2520Wu%2520and%2520Reza%2520Rezvan%2520and%2520Linus%2520Aronsson%2520and%2520Morteza%2520Haghir%2520Chehreghani%26entry.1292438233%3D%2520%2520In%2520many%2520real-world%2520scenarios%252C%2520acquiring%2520all%2520features%2520of%2520a%2520data%2520instance%2520can%250Abe%2520expensive%2520or%2520impractical%2520due%2520to%2520monetary%2520cost%252C%2520latency%252C%2520or%2520privacy%2520concerns.%250AActive%2520Feature%2520Acquisition%2520%2528AFA%2529%2520addresses%2520this%2520challenge%2520by%2520dynamically%250Aselecting%2520a%2520subset%2520of%2520informative%2520features%2520for%2520each%2520data%2520instance%252C%2520trading%250Apredictive%2520performance%2520against%2520acquisition%2520cost.%2520While%2520numerous%2520methods%2520have%250Abeen%2520proposed%2520for%2520AFA%252C%2520ranging%2520from%2520greedy%2520information-theoretic%2520strategies%2520to%250Anon-myopic%2520reinforcement%2520learning%2520approaches%252C%2520fair%2520and%2520systematic%2520evaluation%2520of%250Athese%2520methods%2520has%2520been%2520hindered%2520by%2520the%2520lack%2520of%2520standardized%2520benchmarks.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520AFABench%252C%2520the%2520first%2520benchmark%2520framework%2520for%2520AFA.%2520Our%250Abenchmark%2520includes%2520a%2520diverse%2520set%2520of%2520synthetic%2520and%2520real-world%2520datasets%252C%2520supports%250Aa%2520wide%2520range%2520of%2520acquisition%2520policies%252C%2520and%2520provides%2520a%2520modular%2520design%2520that%250Aenables%2520easy%2520integration%2520of%2520new%2520methods%2520and%2520tasks.%2520We%2520implement%2520and%2520evaluate%250Arepresentative%2520algorithms%2520from%2520all%2520major%2520categories%252C%2520including%2520static%252C%2520greedy%252C%250Aand%2520reinforcement%2520learning-based%2520approaches.%2520To%2520test%2520the%2520lookahead%2520capabilities%250Aof%2520AFA%2520policies%252C%2520we%2520introduce%2520a%2520novel%2520synthetic%2520dataset%252C%2520AFAContext%252C%2520designed%250Ato%2520expose%2520the%2520limitations%2520of%2520greedy%2520selection.%2520Our%2520results%2520highlight%2520key%250Atrade-offs%2520between%2520different%2520AFA%2520strategies%2520and%2520provide%2520actionable%2520insights%2520for%250Afuture%2520research.%2520The%2520benchmark%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Linusaronsson/AFA-Benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AFABench%3A%20A%20Generic%20Framework%20for%20Benchmarking%20Active%20Feature%0A%20%20Acquisition&entry.906535625=Valter%20Sch%C3%BCtz%20and%20Han%20Wu%20and%20Reza%20Rezvan%20and%20Linus%20Aronsson%20and%20Morteza%20Haghir%20Chehreghani&entry.1292438233=%20%20In%20many%20real-world%20scenarios%2C%20acquiring%20all%20features%20of%20a%20data%20instance%20can%0Abe%20expensive%20or%20impractical%20due%20to%20monetary%20cost%2C%20latency%2C%20or%20privacy%20concerns.%0AActive%20Feature%20Acquisition%20%28AFA%29%20addresses%20this%20challenge%20by%20dynamically%0Aselecting%20a%20subset%20of%20informative%20features%20for%20each%20data%20instance%2C%20trading%0Apredictive%20performance%20against%20acquisition%20cost.%20While%20numerous%20methods%20have%0Abeen%20proposed%20for%20AFA%2C%20ranging%20from%20greedy%20information-theoretic%20strategies%20to%0Anon-myopic%20reinforcement%20learning%20approaches%2C%20fair%20and%20systematic%20evaluation%20of%0Athese%20methods%20has%20been%20hindered%20by%20the%20lack%20of%20standardized%20benchmarks.%20In%20this%0Apaper%2C%20we%20introduce%20AFABench%2C%20the%20first%20benchmark%20framework%20for%20AFA.%20Our%0Abenchmark%20includes%20a%20diverse%20set%20of%20synthetic%20and%20real-world%20datasets%2C%20supports%0Aa%20wide%20range%20of%20acquisition%20policies%2C%20and%20provides%20a%20modular%20design%20that%0Aenables%20easy%20integration%20of%20new%20methods%20and%20tasks.%20We%20implement%20and%20evaluate%0Arepresentative%20algorithms%20from%20all%20major%20categories%2C%20including%20static%2C%20greedy%2C%0Aand%20reinforcement%20learning-based%20approaches.%20To%20test%20the%20lookahead%20capabilities%0Aof%20AFA%20policies%2C%20we%20introduce%20a%20novel%20synthetic%20dataset%2C%20AFAContext%2C%20designed%0Ato%20expose%20the%20limitations%20of%20greedy%20selection.%20Our%20results%20highlight%20key%0Atrade-offs%20between%20different%20AFA%20strategies%20and%20provide%20actionable%20insights%20for%0Afuture%20research.%20The%20benchmark%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Linusaronsson/AFA-Benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14734v1&entry.124074799=Read"},
{"title": "Federated Distillation on Edge Devices: Efficient Client-Side Filtering\n  for Non-IID Data", "author": "Ahmed Mujtaba and Gleb Radchenko and Radu Prodan and Marc Masana", "abstract": "  Federated distillation has emerged as a promising collaborative machine\nlearning approach, offering enhanced privacy protection and reduced\ncommunication compared to traditional federated learning by exchanging model\noutputs (soft logits) rather than full model parameters. However, existing\nmethods employ complex selective knowledge-sharing strategies that require\nclients to identify in-distribution proxy data through computationally\nexpensive statistical density ratio estimators. Additionally, server-side\nfiltering of ambiguous knowledge introduces latency to the process. To address\nthese challenges, we propose a robust, resource-efficient EdgeFD method that\nreduces the complexity of the client-side density ratio estimation and removes\nthe need for server-side filtering. EdgeFD introduces an efficient KMeans-based\ndensity ratio estimator for effectively filtering both in-distribution and\nout-of-distribution proxy data on clients, significantly improving the quality\nof knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,\nincluding strong non-IID, weak non-IID, and IID data distributions on clients,\nwithout requiring a pre-trained teacher model on the server for knowledge\ndistillation. Experimental results demonstrate that EdgeFD outperforms\nstate-of-the-art methods, consistently achieving accuracy levels close to IID\nscenarios even under heterogeneous and challenging conditions. The\nsignificantly reduced computational overhead of the KMeans-based estimator is\nsuitable for deployment on resource-constrained edge devices, thereby enhancing\nthe scalability and real-world applicability of federated distillation. The\ncode is available online for reproducibility.\n", "link": "http://arxiv.org/abs/2508.14769v1", "date": "2025-08-20", "relevancy": 1.464, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4916}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4841}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Distillation%20on%20Edge%20Devices%3A%20Efficient%20Client-Side%20Filtering%0A%20%20for%20Non-IID%20Data&body=Title%3A%20Federated%20Distillation%20on%20Edge%20Devices%3A%20Efficient%20Client-Side%20Filtering%0A%20%20for%20Non-IID%20Data%0AAuthor%3A%20Ahmed%20Mujtaba%20and%20Gleb%20Radchenko%20and%20Radu%20Prodan%20and%20Marc%20Masana%0AAbstract%3A%20%20%20Federated%20distillation%20has%20emerged%20as%20a%20promising%20collaborative%20machine%0Alearning%20approach%2C%20offering%20enhanced%20privacy%20protection%20and%20reduced%0Acommunication%20compared%20to%20traditional%20federated%20learning%20by%20exchanging%20model%0Aoutputs%20%28soft%20logits%29%20rather%20than%20full%20model%20parameters.%20However%2C%20existing%0Amethods%20employ%20complex%20selective%20knowledge-sharing%20strategies%20that%20require%0Aclients%20to%20identify%20in-distribution%20proxy%20data%20through%20computationally%0Aexpensive%20statistical%20density%20ratio%20estimators.%20Additionally%2C%20server-side%0Afiltering%20of%20ambiguous%20knowledge%20introduces%20latency%20to%20the%20process.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20robust%2C%20resource-efficient%20EdgeFD%20method%20that%0Areduces%20the%20complexity%20of%20the%20client-side%20density%20ratio%20estimation%20and%20removes%0Athe%20need%20for%20server-side%20filtering.%20EdgeFD%20introduces%20an%20efficient%20KMeans-based%0Adensity%20ratio%20estimator%20for%20effectively%20filtering%20both%20in-distribution%20and%0Aout-of-distribution%20proxy%20data%20on%20clients%2C%20significantly%20improving%20the%20quality%0Aof%20knowledge%20sharing.%20We%20evaluate%20EdgeFD%20across%20diverse%20practical%20scenarios%2C%0Aincluding%20strong%20non-IID%2C%20weak%20non-IID%2C%20and%20IID%20data%20distributions%20on%20clients%2C%0Awithout%20requiring%20a%20pre-trained%20teacher%20model%20on%20the%20server%20for%20knowledge%0Adistillation.%20Experimental%20results%20demonstrate%20that%20EdgeFD%20outperforms%0Astate-of-the-art%20methods%2C%20consistently%20achieving%20accuracy%20levels%20close%20to%20IID%0Ascenarios%20even%20under%20heterogeneous%20and%20challenging%20conditions.%20The%0Asignificantly%20reduced%20computational%20overhead%20of%20the%20KMeans-based%20estimator%20is%0Asuitable%20for%20deployment%20on%20resource-constrained%20edge%20devices%2C%20thereby%20enhancing%0Athe%20scalability%20and%20real-world%20applicability%20of%20federated%20distillation.%20The%0Acode%20is%20available%20online%20for%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Distillation%2520on%2520Edge%2520Devices%253A%2520Efficient%2520Client-Side%2520Filtering%250A%2520%2520for%2520Non-IID%2520Data%26entry.906535625%3DAhmed%2520Mujtaba%2520and%2520Gleb%2520Radchenko%2520and%2520Radu%2520Prodan%2520and%2520Marc%2520Masana%26entry.1292438233%3D%2520%2520Federated%2520distillation%2520has%2520emerged%2520as%2520a%2520promising%2520collaborative%2520machine%250Alearning%2520approach%252C%2520offering%2520enhanced%2520privacy%2520protection%2520and%2520reduced%250Acommunication%2520compared%2520to%2520traditional%2520federated%2520learning%2520by%2520exchanging%2520model%250Aoutputs%2520%2528soft%2520logits%2529%2520rather%2520than%2520full%2520model%2520parameters.%2520However%252C%2520existing%250Amethods%2520employ%2520complex%2520selective%2520knowledge-sharing%2520strategies%2520that%2520require%250Aclients%2520to%2520identify%2520in-distribution%2520proxy%2520data%2520through%2520computationally%250Aexpensive%2520statistical%2520density%2520ratio%2520estimators.%2520Additionally%252C%2520server-side%250Afiltering%2520of%2520ambiguous%2520knowledge%2520introduces%2520latency%2520to%2520the%2520process.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520robust%252C%2520resource-efficient%2520EdgeFD%2520method%2520that%250Areduces%2520the%2520complexity%2520of%2520the%2520client-side%2520density%2520ratio%2520estimation%2520and%2520removes%250Athe%2520need%2520for%2520server-side%2520filtering.%2520EdgeFD%2520introduces%2520an%2520efficient%2520KMeans-based%250Adensity%2520ratio%2520estimator%2520for%2520effectively%2520filtering%2520both%2520in-distribution%2520and%250Aout-of-distribution%2520proxy%2520data%2520on%2520clients%252C%2520significantly%2520improving%2520the%2520quality%250Aof%2520knowledge%2520sharing.%2520We%2520evaluate%2520EdgeFD%2520across%2520diverse%2520practical%2520scenarios%252C%250Aincluding%2520strong%2520non-IID%252C%2520weak%2520non-IID%252C%2520and%2520IID%2520data%2520distributions%2520on%2520clients%252C%250Awithout%2520requiring%2520a%2520pre-trained%2520teacher%2520model%2520on%2520the%2520server%2520for%2520knowledge%250Adistillation.%2520Experimental%2520results%2520demonstrate%2520that%2520EdgeFD%2520outperforms%250Astate-of-the-art%2520methods%252C%2520consistently%2520achieving%2520accuracy%2520levels%2520close%2520to%2520IID%250Ascenarios%2520even%2520under%2520heterogeneous%2520and%2520challenging%2520conditions.%2520The%250Asignificantly%2520reduced%2520computational%2520overhead%2520of%2520the%2520KMeans-based%2520estimator%2520is%250Asuitable%2520for%2520deployment%2520on%2520resource-constrained%2520edge%2520devices%252C%2520thereby%2520enhancing%250Athe%2520scalability%2520and%2520real-world%2520applicability%2520of%2520federated%2520distillation.%2520The%250Acode%2520is%2520available%2520online%2520for%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Distillation%20on%20Edge%20Devices%3A%20Efficient%20Client-Side%20Filtering%0A%20%20for%20Non-IID%20Data&entry.906535625=Ahmed%20Mujtaba%20and%20Gleb%20Radchenko%20and%20Radu%20Prodan%20and%20Marc%20Masana&entry.1292438233=%20%20Federated%20distillation%20has%20emerged%20as%20a%20promising%20collaborative%20machine%0Alearning%20approach%2C%20offering%20enhanced%20privacy%20protection%20and%20reduced%0Acommunication%20compared%20to%20traditional%20federated%20learning%20by%20exchanging%20model%0Aoutputs%20%28soft%20logits%29%20rather%20than%20full%20model%20parameters.%20However%2C%20existing%0Amethods%20employ%20complex%20selective%20knowledge-sharing%20strategies%20that%20require%0Aclients%20to%20identify%20in-distribution%20proxy%20data%20through%20computationally%0Aexpensive%20statistical%20density%20ratio%20estimators.%20Additionally%2C%20server-side%0Afiltering%20of%20ambiguous%20knowledge%20introduces%20latency%20to%20the%20process.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20robust%2C%20resource-efficient%20EdgeFD%20method%20that%0Areduces%20the%20complexity%20of%20the%20client-side%20density%20ratio%20estimation%20and%20removes%0Athe%20need%20for%20server-side%20filtering.%20EdgeFD%20introduces%20an%20efficient%20KMeans-based%0Adensity%20ratio%20estimator%20for%20effectively%20filtering%20both%20in-distribution%20and%0Aout-of-distribution%20proxy%20data%20on%20clients%2C%20significantly%20improving%20the%20quality%0Aof%20knowledge%20sharing.%20We%20evaluate%20EdgeFD%20across%20diverse%20practical%20scenarios%2C%0Aincluding%20strong%20non-IID%2C%20weak%20non-IID%2C%20and%20IID%20data%20distributions%20on%20clients%2C%0Awithout%20requiring%20a%20pre-trained%20teacher%20model%20on%20the%20server%20for%20knowledge%0Adistillation.%20Experimental%20results%20demonstrate%20that%20EdgeFD%20outperforms%0Astate-of-the-art%20methods%2C%20consistently%20achieving%20accuracy%20levels%20close%20to%20IID%0Ascenarios%20even%20under%20heterogeneous%20and%20challenging%20conditions.%20The%0Asignificantly%20reduced%20computational%20overhead%20of%20the%20KMeans-based%20estimator%20is%0Asuitable%20for%20deployment%20on%20resource-constrained%20edge%20devices%2C%20thereby%20enhancing%0Athe%20scalability%20and%20real-world%20applicability%20of%20federated%20distillation.%20The%0Acode%20is%20available%20online%20for%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14769v1&entry.124074799=Read"},
{"title": "Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge\n  Distillation Method", "author": "Suleyman Olcay Polat and Poli A. Nemkova and Mark V. Albert", "abstract": "  Model distillation enables the transfer of knowledge from large-scale models\nto compact student models, facilitating deployment in resource-constrained\nenvironments. However, conventional distillation approaches often suffer from\ncomputational overhead and limited generalization. We propose a novel adaptive\ndistillation framework that dynamically augments training data in regions of\nhigh student model loss. Using UMAP-based dimensionality reduction and nearest\nneighbor sampling, our method identifies underperforming regions in the\nembedding space and generates targeted synthetic examples to guide student\nlearning. To further improve efficiency, we introduce a lightweight\nteacher-student interface that bypasses the teacher's input layer, enabling\ndirect distillation on vectorized representations. Experiments across standard\nNLP benchmarks demonstrate that our 66M-parameter student model consistently\nmatches or surpasses established baselines, achieving 91.2% on QNLI and 92.3%\non SST-2, while training with fewer epochs. These results highlight the promise\nof loss-aware data augmentation and vectorized distillation for efficient and\neffective model compression.\n", "link": "http://arxiv.org/abs/2508.14783v1", "date": "2025-08-20", "relevancy": 1.5261, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5211}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5085}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Adaptive%20Guided%20Embeddings%20%28SAGE%29%3A%20A%20Novel%20Knowledge%0A%20%20Distillation%20Method&body=Title%3A%20Synthetic%20Adaptive%20Guided%20Embeddings%20%28SAGE%29%3A%20A%20Novel%20Knowledge%0A%20%20Distillation%20Method%0AAuthor%3A%20Suleyman%20Olcay%20Polat%20and%20Poli%20A.%20Nemkova%20and%20Mark%20V.%20Albert%0AAbstract%3A%20%20%20Model%20distillation%20enables%20the%20transfer%20of%20knowledge%20from%20large-scale%20models%0Ato%20compact%20student%20models%2C%20facilitating%20deployment%20in%20resource-constrained%0Aenvironments.%20However%2C%20conventional%20distillation%20approaches%20often%20suffer%20from%0Acomputational%20overhead%20and%20limited%20generalization.%20We%20propose%20a%20novel%20adaptive%0Adistillation%20framework%20that%20dynamically%20augments%20training%20data%20in%20regions%20of%0Ahigh%20student%20model%20loss.%20Using%20UMAP-based%20dimensionality%20reduction%20and%20nearest%0Aneighbor%20sampling%2C%20our%20method%20identifies%20underperforming%20regions%20in%20the%0Aembedding%20space%20and%20generates%20targeted%20synthetic%20examples%20to%20guide%20student%0Alearning.%20To%20further%20improve%20efficiency%2C%20we%20introduce%20a%20lightweight%0Ateacher-student%20interface%20that%20bypasses%20the%20teacher%27s%20input%20layer%2C%20enabling%0Adirect%20distillation%20on%20vectorized%20representations.%20Experiments%20across%20standard%0ANLP%20benchmarks%20demonstrate%20that%20our%2066M-parameter%20student%20model%20consistently%0Amatches%20or%20surpasses%20established%20baselines%2C%20achieving%2091.2%25%20on%20QNLI%20and%2092.3%25%0Aon%20SST-2%2C%20while%20training%20with%20fewer%20epochs.%20These%20results%20highlight%20the%20promise%0Aof%20loss-aware%20data%20augmentation%20and%20vectorized%20distillation%20for%20efficient%20and%0Aeffective%20model%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Adaptive%2520Guided%2520Embeddings%2520%2528SAGE%2529%253A%2520A%2520Novel%2520Knowledge%250A%2520%2520Distillation%2520Method%26entry.906535625%3DSuleyman%2520Olcay%2520Polat%2520and%2520Poli%2520A.%2520Nemkova%2520and%2520Mark%2520V.%2520Albert%26entry.1292438233%3D%2520%2520Model%2520distillation%2520enables%2520the%2520transfer%2520of%2520knowledge%2520from%2520large-scale%2520models%250Ato%2520compact%2520student%2520models%252C%2520facilitating%2520deployment%2520in%2520resource-constrained%250Aenvironments.%2520However%252C%2520conventional%2520distillation%2520approaches%2520often%2520suffer%2520from%250Acomputational%2520overhead%2520and%2520limited%2520generalization.%2520We%2520propose%2520a%2520novel%2520adaptive%250Adistillation%2520framework%2520that%2520dynamically%2520augments%2520training%2520data%2520in%2520regions%2520of%250Ahigh%2520student%2520model%2520loss.%2520Using%2520UMAP-based%2520dimensionality%2520reduction%2520and%2520nearest%250Aneighbor%2520sampling%252C%2520our%2520method%2520identifies%2520underperforming%2520regions%2520in%2520the%250Aembedding%2520space%2520and%2520generates%2520targeted%2520synthetic%2520examples%2520to%2520guide%2520student%250Alearning.%2520To%2520further%2520improve%2520efficiency%252C%2520we%2520introduce%2520a%2520lightweight%250Ateacher-student%2520interface%2520that%2520bypasses%2520the%2520teacher%2527s%2520input%2520layer%252C%2520enabling%250Adirect%2520distillation%2520on%2520vectorized%2520representations.%2520Experiments%2520across%2520standard%250ANLP%2520benchmarks%2520demonstrate%2520that%2520our%252066M-parameter%2520student%2520model%2520consistently%250Amatches%2520or%2520surpasses%2520established%2520baselines%252C%2520achieving%252091.2%2525%2520on%2520QNLI%2520and%252092.3%2525%250Aon%2520SST-2%252C%2520while%2520training%2520with%2520fewer%2520epochs.%2520These%2520results%2520highlight%2520the%2520promise%250Aof%2520loss-aware%2520data%2520augmentation%2520and%2520vectorized%2520distillation%2520for%2520efficient%2520and%250Aeffective%2520model%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Adaptive%20Guided%20Embeddings%20%28SAGE%29%3A%20A%20Novel%20Knowledge%0A%20%20Distillation%20Method&entry.906535625=Suleyman%20Olcay%20Polat%20and%20Poli%20A.%20Nemkova%20and%20Mark%20V.%20Albert&entry.1292438233=%20%20Model%20distillation%20enables%20the%20transfer%20of%20knowledge%20from%20large-scale%20models%0Ato%20compact%20student%20models%2C%20facilitating%20deployment%20in%20resource-constrained%0Aenvironments.%20However%2C%20conventional%20distillation%20approaches%20often%20suffer%20from%0Acomputational%20overhead%20and%20limited%20generalization.%20We%20propose%20a%20novel%20adaptive%0Adistillation%20framework%20that%20dynamically%20augments%20training%20data%20in%20regions%20of%0Ahigh%20student%20model%20loss.%20Using%20UMAP-based%20dimensionality%20reduction%20and%20nearest%0Aneighbor%20sampling%2C%20our%20method%20identifies%20underperforming%20regions%20in%20the%0Aembedding%20space%20and%20generates%20targeted%20synthetic%20examples%20to%20guide%20student%0Alearning.%20To%20further%20improve%20efficiency%2C%20we%20introduce%20a%20lightweight%0Ateacher-student%20interface%20that%20bypasses%20the%20teacher%27s%20input%20layer%2C%20enabling%0Adirect%20distillation%20on%20vectorized%20representations.%20Experiments%20across%20standard%0ANLP%20benchmarks%20demonstrate%20that%20our%2066M-parameter%20student%20model%20consistently%0Amatches%20or%20surpasses%20established%20baselines%2C%20achieving%2091.2%25%20on%20QNLI%20and%2092.3%25%0Aon%20SST-2%2C%20while%20training%20with%20fewer%20epochs.%20These%20results%20highlight%20the%20promise%0Aof%20loss-aware%20data%20augmentation%20and%20vectorized%20distillation%20for%20efficient%20and%0Aeffective%20model%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14783v1&entry.124074799=Read"},
{"title": "Multi-agent Auditory Scene Analysis", "author": "Caleb Rascon and Luis Gato-Diaz and Eduardo Garc\u00eda-Alarc\u00f3n", "abstract": "  Auditory scene analysis (ASA) aims to retrieve information from the acoustic\nenvironment, by carrying out three main tasks: sound source location,\nseparation, and classification. These tasks are traditionally executed with a\nlinear data flow, where the sound sources are first located; then, using their\nlocation, each source is separated into its own audio stream; from each of\nwhich, information is extracted that is relevant to the application scenario\n(audio event detection, speaker identification, emotion classification, etc.).\nHowever, running these tasks linearly increases the overall response time,\nwhile making the last tasks (separation and classification) highly sensitive to\nerrors of the first task (location). A considerable amount of effort and\ncomputational complexity has been employed in the state-of-the-art to develop\ntechniques that are the least error-prone possible. However, doing so gives\nrise to an ASA system that is non-viable in many applications that require a\nsmall computational footprint and a low response time, such as bioacoustics,\nhearing-aid design, search and rescue, human-robot interaction, etc. To this\neffect, in this work, a multi-agent approach is proposed to carry out ASA where\nthe tasks are run in parallel, with feedback loops between them to compensate\nfor local errors, such as: using the quality of the separation output to\ncorrect the location error; and using the classification result to reduce the\nlocalization's sensitivity towards interferences. The result is a multi-agent\nauditory scene analysis (MASA) system that is robust against local errors,\nwithout a considerable increase in complexity, and with a low response time.\nThe complete proposed MASA system is provided as a publicly available framework\nthat uses open-source tools for sound acquisition and reproduction (JACK) and\ninter-agent communication (ROS2), allowing users to add their own agents.\n", "link": "http://arxiv.org/abs/2507.02755v3", "date": "2025-08-20", "relevancy": 1.5121, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-agent%20Auditory%20Scene%20Analysis&body=Title%3A%20Multi-agent%20Auditory%20Scene%20Analysis%0AAuthor%3A%20Caleb%20Rascon%20and%20Luis%20Gato-Diaz%20and%20Eduardo%20Garc%C3%ADa-Alarc%C3%B3n%0AAbstract%3A%20%20%20Auditory%20scene%20analysis%20%28ASA%29%20aims%20to%20retrieve%20information%20from%20the%20acoustic%0Aenvironment%2C%20by%20carrying%20out%20three%20main%20tasks%3A%20sound%20source%20location%2C%0Aseparation%2C%20and%20classification.%20These%20tasks%20are%20traditionally%20executed%20with%20a%0Alinear%20data%20flow%2C%20where%20the%20sound%20sources%20are%20first%20located%3B%20then%2C%20using%20their%0Alocation%2C%20each%20source%20is%20separated%20into%20its%20own%20audio%20stream%3B%20from%20each%20of%0Awhich%2C%20information%20is%20extracted%20that%20is%20relevant%20to%20the%20application%20scenario%0A%28audio%20event%20detection%2C%20speaker%20identification%2C%20emotion%20classification%2C%20etc.%29.%0AHowever%2C%20running%20these%20tasks%20linearly%20increases%20the%20overall%20response%20time%2C%0Awhile%20making%20the%20last%20tasks%20%28separation%20and%20classification%29%20highly%20sensitive%20to%0Aerrors%20of%20the%20first%20task%20%28location%29.%20A%20considerable%20amount%20of%20effort%20and%0Acomputational%20complexity%20has%20been%20employed%20in%20the%20state-of-the-art%20to%20develop%0Atechniques%20that%20are%20the%20least%20error-prone%20possible.%20However%2C%20doing%20so%20gives%0Arise%20to%20an%20ASA%20system%20that%20is%20non-viable%20in%20many%20applications%20that%20require%20a%0Asmall%20computational%20footprint%20and%20a%20low%20response%20time%2C%20such%20as%20bioacoustics%2C%0Ahearing-aid%20design%2C%20search%20and%20rescue%2C%20human-robot%20interaction%2C%20etc.%20To%20this%0Aeffect%2C%20in%20this%20work%2C%20a%20multi-agent%20approach%20is%20proposed%20to%20carry%20out%20ASA%20where%0Athe%20tasks%20are%20run%20in%20parallel%2C%20with%20feedback%20loops%20between%20them%20to%20compensate%0Afor%20local%20errors%2C%20such%20as%3A%20using%20the%20quality%20of%20the%20separation%20output%20to%0Acorrect%20the%20location%20error%3B%20and%20using%20the%20classification%20result%20to%20reduce%20the%0Alocalization%27s%20sensitivity%20towards%20interferences.%20The%20result%20is%20a%20multi-agent%0Aauditory%20scene%20analysis%20%28MASA%29%20system%20that%20is%20robust%20against%20local%20errors%2C%0Awithout%20a%20considerable%20increase%20in%20complexity%2C%20and%20with%20a%20low%20response%20time.%0AThe%20complete%20proposed%20MASA%20system%20is%20provided%20as%20a%20publicly%20available%20framework%0Athat%20uses%20open-source%20tools%20for%20sound%20acquisition%20and%20reproduction%20%28JACK%29%20and%0Ainter-agent%20communication%20%28ROS2%29%2C%20allowing%20users%20to%20add%20their%20own%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02755v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-agent%2520Auditory%2520Scene%2520Analysis%26entry.906535625%3DCaleb%2520Rascon%2520and%2520Luis%2520Gato-Diaz%2520and%2520Eduardo%2520Garc%25C3%25ADa-Alarc%25C3%25B3n%26entry.1292438233%3D%2520%2520Auditory%2520scene%2520analysis%2520%2528ASA%2529%2520aims%2520to%2520retrieve%2520information%2520from%2520the%2520acoustic%250Aenvironment%252C%2520by%2520carrying%2520out%2520three%2520main%2520tasks%253A%2520sound%2520source%2520location%252C%250Aseparation%252C%2520and%2520classification.%2520These%2520tasks%2520are%2520traditionally%2520executed%2520with%2520a%250Alinear%2520data%2520flow%252C%2520where%2520the%2520sound%2520sources%2520are%2520first%2520located%253B%2520then%252C%2520using%2520their%250Alocation%252C%2520each%2520source%2520is%2520separated%2520into%2520its%2520own%2520audio%2520stream%253B%2520from%2520each%2520of%250Awhich%252C%2520information%2520is%2520extracted%2520that%2520is%2520relevant%2520to%2520the%2520application%2520scenario%250A%2528audio%2520event%2520detection%252C%2520speaker%2520identification%252C%2520emotion%2520classification%252C%2520etc.%2529.%250AHowever%252C%2520running%2520these%2520tasks%2520linearly%2520increases%2520the%2520overall%2520response%2520time%252C%250Awhile%2520making%2520the%2520last%2520tasks%2520%2528separation%2520and%2520classification%2529%2520highly%2520sensitive%2520to%250Aerrors%2520of%2520the%2520first%2520task%2520%2528location%2529.%2520A%2520considerable%2520amount%2520of%2520effort%2520and%250Acomputational%2520complexity%2520has%2520been%2520employed%2520in%2520the%2520state-of-the-art%2520to%2520develop%250Atechniques%2520that%2520are%2520the%2520least%2520error-prone%2520possible.%2520However%252C%2520doing%2520so%2520gives%250Arise%2520to%2520an%2520ASA%2520system%2520that%2520is%2520non-viable%2520in%2520many%2520applications%2520that%2520require%2520a%250Asmall%2520computational%2520footprint%2520and%2520a%2520low%2520response%2520time%252C%2520such%2520as%2520bioacoustics%252C%250Ahearing-aid%2520design%252C%2520search%2520and%2520rescue%252C%2520human-robot%2520interaction%252C%2520etc.%2520To%2520this%250Aeffect%252C%2520in%2520this%2520work%252C%2520a%2520multi-agent%2520approach%2520is%2520proposed%2520to%2520carry%2520out%2520ASA%2520where%250Athe%2520tasks%2520are%2520run%2520in%2520parallel%252C%2520with%2520feedback%2520loops%2520between%2520them%2520to%2520compensate%250Afor%2520local%2520errors%252C%2520such%2520as%253A%2520using%2520the%2520quality%2520of%2520the%2520separation%2520output%2520to%250Acorrect%2520the%2520location%2520error%253B%2520and%2520using%2520the%2520classification%2520result%2520to%2520reduce%2520the%250Alocalization%2527s%2520sensitivity%2520towards%2520interferences.%2520The%2520result%2520is%2520a%2520multi-agent%250Aauditory%2520scene%2520analysis%2520%2528MASA%2529%2520system%2520that%2520is%2520robust%2520against%2520local%2520errors%252C%250Awithout%2520a%2520considerable%2520increase%2520in%2520complexity%252C%2520and%2520with%2520a%2520low%2520response%2520time.%250AThe%2520complete%2520proposed%2520MASA%2520system%2520is%2520provided%2520as%2520a%2520publicly%2520available%2520framework%250Athat%2520uses%2520open-source%2520tools%2520for%2520sound%2520acquisition%2520and%2520reproduction%2520%2528JACK%2529%2520and%250Ainter-agent%2520communication%2520%2528ROS2%2529%252C%2520allowing%2520users%2520to%2520add%2520their%2520own%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02755v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-agent%20Auditory%20Scene%20Analysis&entry.906535625=Caleb%20Rascon%20and%20Luis%20Gato-Diaz%20and%20Eduardo%20Garc%C3%ADa-Alarc%C3%B3n&entry.1292438233=%20%20Auditory%20scene%20analysis%20%28ASA%29%20aims%20to%20retrieve%20information%20from%20the%20acoustic%0Aenvironment%2C%20by%20carrying%20out%20three%20main%20tasks%3A%20sound%20source%20location%2C%0Aseparation%2C%20and%20classification.%20These%20tasks%20are%20traditionally%20executed%20with%20a%0Alinear%20data%20flow%2C%20where%20the%20sound%20sources%20are%20first%20located%3B%20then%2C%20using%20their%0Alocation%2C%20each%20source%20is%20separated%20into%20its%20own%20audio%20stream%3B%20from%20each%20of%0Awhich%2C%20information%20is%20extracted%20that%20is%20relevant%20to%20the%20application%20scenario%0A%28audio%20event%20detection%2C%20speaker%20identification%2C%20emotion%20classification%2C%20etc.%29.%0AHowever%2C%20running%20these%20tasks%20linearly%20increases%20the%20overall%20response%20time%2C%0Awhile%20making%20the%20last%20tasks%20%28separation%20and%20classification%29%20highly%20sensitive%20to%0Aerrors%20of%20the%20first%20task%20%28location%29.%20A%20considerable%20amount%20of%20effort%20and%0Acomputational%20complexity%20has%20been%20employed%20in%20the%20state-of-the-art%20to%20develop%0Atechniques%20that%20are%20the%20least%20error-prone%20possible.%20However%2C%20doing%20so%20gives%0Arise%20to%20an%20ASA%20system%20that%20is%20non-viable%20in%20many%20applications%20that%20require%20a%0Asmall%20computational%20footprint%20and%20a%20low%20response%20time%2C%20such%20as%20bioacoustics%2C%0Ahearing-aid%20design%2C%20search%20and%20rescue%2C%20human-robot%20interaction%2C%20etc.%20To%20this%0Aeffect%2C%20in%20this%20work%2C%20a%20multi-agent%20approach%20is%20proposed%20to%20carry%20out%20ASA%20where%0Athe%20tasks%20are%20run%20in%20parallel%2C%20with%20feedback%20loops%20between%20them%20to%20compensate%0Afor%20local%20errors%2C%20such%20as%3A%20using%20the%20quality%20of%20the%20separation%20output%20to%0Acorrect%20the%20location%20error%3B%20and%20using%20the%20classification%20result%20to%20reduce%20the%0Alocalization%27s%20sensitivity%20towards%20interferences.%20The%20result%20is%20a%20multi-agent%0Aauditory%20scene%20analysis%20%28MASA%29%20system%20that%20is%20robust%20against%20local%20errors%2C%0Awithout%20a%20considerable%20increase%20in%20complexity%2C%20and%20with%20a%20low%20response%20time.%0AThe%20complete%20proposed%20MASA%20system%20is%20provided%20as%20a%20publicly%20available%20framework%0Athat%20uses%20open-source%20tools%20for%20sound%20acquisition%20and%20reproduction%20%28JACK%29%20and%0Ainter-agent%20communication%20%28ROS2%29%2C%20allowing%20users%20to%20add%20their%20own%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02755v3&entry.124074799=Read"},
{"title": "Safe and Transparent Robots for Human-in-the-Loop Meat Processing", "author": "Sagar Parekh and Casey Grothoff and Ryan Wright and Robin White and Dylan P. Losey", "abstract": "  Labor shortages have severely affected the meat processing sector. Automated\ntechnology has the potential to support the meat industry, assist workers, and\nenhance job quality. However, existing automation in meat processing is highly\nspecialized, inflexible, and cost intensive. Instead of forcing manufacturers\nto buy a separate device for each step of the process, our objective is to\ndevelop general-purpose robotic systems that work alongside humans to perform\nmultiple meat processing tasks. Through a recently conducted survey of industry\nexperts, we identified two main challenges associated with integrating these\ncollaborative robots alongside human workers. First, there must be measures to\nensure the safety of human coworkers; second, the coworkers need to understand\nwhat the robot is doing. This paper addresses both challenges by introducing a\nsafety and transparency framework for general-purpose meat processing robots.\nFor safety, we implement a hand-detection system that continuously monitors\nnearby humans. This system can halt the robot in situations where the human\ncomes into close proximity of the operating robot. We also develop an\ninstrumented knife equipped with a force sensor that can differentiate contact\nbetween objects such as meat, bone, or fixtures. For transparency, we introduce\na method that detects the robot's uncertainty about its performance and uses an\nLED interface to communicate that uncertainty to the human. Additionally, we\ndesign a graphical interface that displays the robot's plans and allows the\nhuman to provide feedback on the planned cut. Overall, our framework can ensure\nsafe operation while keeping human workers in-the-loop about the robot's\nactions which we validate through a user study.\n", "link": "http://arxiv.org/abs/2508.14763v1", "date": "2025-08-20", "relevancy": 1.5605, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5623}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.51}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20and%20Transparent%20Robots%20for%20Human-in-the-Loop%20Meat%20Processing&body=Title%3A%20Safe%20and%20Transparent%20Robots%20for%20Human-in-the-Loop%20Meat%20Processing%0AAuthor%3A%20Sagar%20Parekh%20and%20Casey%20Grothoff%20and%20Ryan%20Wright%20and%20Robin%20White%20and%20Dylan%20P.%20Losey%0AAbstract%3A%20%20%20Labor%20shortages%20have%20severely%20affected%20the%20meat%20processing%20sector.%20Automated%0Atechnology%20has%20the%20potential%20to%20support%20the%20meat%20industry%2C%20assist%20workers%2C%20and%0Aenhance%20job%20quality.%20However%2C%20existing%20automation%20in%20meat%20processing%20is%20highly%0Aspecialized%2C%20inflexible%2C%20and%20cost%20intensive.%20Instead%20of%20forcing%20manufacturers%0Ato%20buy%20a%20separate%20device%20for%20each%20step%20of%20the%20process%2C%20our%20objective%20is%20to%0Adevelop%20general-purpose%20robotic%20systems%20that%20work%20alongside%20humans%20to%20perform%0Amultiple%20meat%20processing%20tasks.%20Through%20a%20recently%20conducted%20survey%20of%20industry%0Aexperts%2C%20we%20identified%20two%20main%20challenges%20associated%20with%20integrating%20these%0Acollaborative%20robots%20alongside%20human%20workers.%20First%2C%20there%20must%20be%20measures%20to%0Aensure%20the%20safety%20of%20human%20coworkers%3B%20second%2C%20the%20coworkers%20need%20to%20understand%0Awhat%20the%20robot%20is%20doing.%20This%20paper%20addresses%20both%20challenges%20by%20introducing%20a%0Asafety%20and%20transparency%20framework%20for%20general-purpose%20meat%20processing%20robots.%0AFor%20safety%2C%20we%20implement%20a%20hand-detection%20system%20that%20continuously%20monitors%0Anearby%20humans.%20This%20system%20can%20halt%20the%20robot%20in%20situations%20where%20the%20human%0Acomes%20into%20close%20proximity%20of%20the%20operating%20robot.%20We%20also%20develop%20an%0Ainstrumented%20knife%20equipped%20with%20a%20force%20sensor%20that%20can%20differentiate%20contact%0Abetween%20objects%20such%20as%20meat%2C%20bone%2C%20or%20fixtures.%20For%20transparency%2C%20we%20introduce%0Aa%20method%20that%20detects%20the%20robot%27s%20uncertainty%20about%20its%20performance%20and%20uses%20an%0ALED%20interface%20to%20communicate%20that%20uncertainty%20to%20the%20human.%20Additionally%2C%20we%0Adesign%20a%20graphical%20interface%20that%20displays%20the%20robot%27s%20plans%20and%20allows%20the%0Ahuman%20to%20provide%20feedback%20on%20the%20planned%20cut.%20Overall%2C%20our%20framework%20can%20ensure%0Asafe%20operation%20while%20keeping%20human%20workers%20in-the-loop%20about%20the%20robot%27s%0Aactions%20which%20we%20validate%20through%20a%20user%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520and%2520Transparent%2520Robots%2520for%2520Human-in-the-Loop%2520Meat%2520Processing%26entry.906535625%3DSagar%2520Parekh%2520and%2520Casey%2520Grothoff%2520and%2520Ryan%2520Wright%2520and%2520Robin%2520White%2520and%2520Dylan%2520P.%2520Losey%26entry.1292438233%3D%2520%2520Labor%2520shortages%2520have%2520severely%2520affected%2520the%2520meat%2520processing%2520sector.%2520Automated%250Atechnology%2520has%2520the%2520potential%2520to%2520support%2520the%2520meat%2520industry%252C%2520assist%2520workers%252C%2520and%250Aenhance%2520job%2520quality.%2520However%252C%2520existing%2520automation%2520in%2520meat%2520processing%2520is%2520highly%250Aspecialized%252C%2520inflexible%252C%2520and%2520cost%2520intensive.%2520Instead%2520of%2520forcing%2520manufacturers%250Ato%2520buy%2520a%2520separate%2520device%2520for%2520each%2520step%2520of%2520the%2520process%252C%2520our%2520objective%2520is%2520to%250Adevelop%2520general-purpose%2520robotic%2520systems%2520that%2520work%2520alongside%2520humans%2520to%2520perform%250Amultiple%2520meat%2520processing%2520tasks.%2520Through%2520a%2520recently%2520conducted%2520survey%2520of%2520industry%250Aexperts%252C%2520we%2520identified%2520two%2520main%2520challenges%2520associated%2520with%2520integrating%2520these%250Acollaborative%2520robots%2520alongside%2520human%2520workers.%2520First%252C%2520there%2520must%2520be%2520measures%2520to%250Aensure%2520the%2520safety%2520of%2520human%2520coworkers%253B%2520second%252C%2520the%2520coworkers%2520need%2520to%2520understand%250Awhat%2520the%2520robot%2520is%2520doing.%2520This%2520paper%2520addresses%2520both%2520challenges%2520by%2520introducing%2520a%250Asafety%2520and%2520transparency%2520framework%2520for%2520general-purpose%2520meat%2520processing%2520robots.%250AFor%2520safety%252C%2520we%2520implement%2520a%2520hand-detection%2520system%2520that%2520continuously%2520monitors%250Anearby%2520humans.%2520This%2520system%2520can%2520halt%2520the%2520robot%2520in%2520situations%2520where%2520the%2520human%250Acomes%2520into%2520close%2520proximity%2520of%2520the%2520operating%2520robot.%2520We%2520also%2520develop%2520an%250Ainstrumented%2520knife%2520equipped%2520with%2520a%2520force%2520sensor%2520that%2520can%2520differentiate%2520contact%250Abetween%2520objects%2520such%2520as%2520meat%252C%2520bone%252C%2520or%2520fixtures.%2520For%2520transparency%252C%2520we%2520introduce%250Aa%2520method%2520that%2520detects%2520the%2520robot%2527s%2520uncertainty%2520about%2520its%2520performance%2520and%2520uses%2520an%250ALED%2520interface%2520to%2520communicate%2520that%2520uncertainty%2520to%2520the%2520human.%2520Additionally%252C%2520we%250Adesign%2520a%2520graphical%2520interface%2520that%2520displays%2520the%2520robot%2527s%2520plans%2520and%2520allows%2520the%250Ahuman%2520to%2520provide%2520feedback%2520on%2520the%2520planned%2520cut.%2520Overall%252C%2520our%2520framework%2520can%2520ensure%250Asafe%2520operation%2520while%2520keeping%2520human%2520workers%2520in-the-loop%2520about%2520the%2520robot%2527s%250Aactions%2520which%2520we%2520validate%2520through%2520a%2520user%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20and%20Transparent%20Robots%20for%20Human-in-the-Loop%20Meat%20Processing&entry.906535625=Sagar%20Parekh%20and%20Casey%20Grothoff%20and%20Ryan%20Wright%20and%20Robin%20White%20and%20Dylan%20P.%20Losey&entry.1292438233=%20%20Labor%20shortages%20have%20severely%20affected%20the%20meat%20processing%20sector.%20Automated%0Atechnology%20has%20the%20potential%20to%20support%20the%20meat%20industry%2C%20assist%20workers%2C%20and%0Aenhance%20job%20quality.%20However%2C%20existing%20automation%20in%20meat%20processing%20is%20highly%0Aspecialized%2C%20inflexible%2C%20and%20cost%20intensive.%20Instead%20of%20forcing%20manufacturers%0Ato%20buy%20a%20separate%20device%20for%20each%20step%20of%20the%20process%2C%20our%20objective%20is%20to%0Adevelop%20general-purpose%20robotic%20systems%20that%20work%20alongside%20humans%20to%20perform%0Amultiple%20meat%20processing%20tasks.%20Through%20a%20recently%20conducted%20survey%20of%20industry%0Aexperts%2C%20we%20identified%20two%20main%20challenges%20associated%20with%20integrating%20these%0Acollaborative%20robots%20alongside%20human%20workers.%20First%2C%20there%20must%20be%20measures%20to%0Aensure%20the%20safety%20of%20human%20coworkers%3B%20second%2C%20the%20coworkers%20need%20to%20understand%0Awhat%20the%20robot%20is%20doing.%20This%20paper%20addresses%20both%20challenges%20by%20introducing%20a%0Asafety%20and%20transparency%20framework%20for%20general-purpose%20meat%20processing%20robots.%0AFor%20safety%2C%20we%20implement%20a%20hand-detection%20system%20that%20continuously%20monitors%0Anearby%20humans.%20This%20system%20can%20halt%20the%20robot%20in%20situations%20where%20the%20human%0Acomes%20into%20close%20proximity%20of%20the%20operating%20robot.%20We%20also%20develop%20an%0Ainstrumented%20knife%20equipped%20with%20a%20force%20sensor%20that%20can%20differentiate%20contact%0Abetween%20objects%20such%20as%20meat%2C%20bone%2C%20or%20fixtures.%20For%20transparency%2C%20we%20introduce%0Aa%20method%20that%20detects%20the%20robot%27s%20uncertainty%20about%20its%20performance%20and%20uses%20an%0ALED%20interface%20to%20communicate%20that%20uncertainty%20to%20the%20human.%20Additionally%2C%20we%0Adesign%20a%20graphical%20interface%20that%20displays%20the%20robot%27s%20plans%20and%20allows%20the%0Ahuman%20to%20provide%20feedback%20on%20the%20planned%20cut.%20Overall%2C%20our%20framework%20can%20ensure%0Asafe%20operation%20while%20keeping%20human%20workers%20in-the-loop%20about%20the%20robot%27s%0Aactions%20which%20we%20validate%20through%20a%20user%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14763v1&entry.124074799=Read"},
{"title": "Consistent Pose Estimation of Unmanned Ground Vehicles through\n  Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds", "author": "Alexander Raab and Stephan Weiss and Alessandro Fornasier and Christian Brommer and Abdalrahman Ibrahim", "abstract": "  Aiming to enhance the consistency and thus long-term accuracy of Extended\nKalman Filters for terrestrial vehicle localization, this paper introduces the\nManifold Error State Extended Kalman Filter (M-ESEKF). By representing the\nrobot's pose in a space with reduced dimensionality, the approach ensures\nfeasible estimates on generic smooth surfaces, without introducing artificial\nconstraints or simplifications that may degrade a filter's performance. The\naccompanying measurement models are compatible with common loosely- and\ntightly-coupled sensor modalities and also implicitly account for the ground\ngeometry. We extend the formulation by introducing a novel correction scheme\nthat embeds additional domain knowledge into the sensor data, giving more\naccurate uncertainty approximations and further enhancing filter consistency.\nThe proposed estimator is seamlessly integrated into a validated modular state\nestimation framework, demonstrating compatibility with existing\nimplementations. Extensive Monte Carlo simulations across diverse scenarios and\ndynamic sensor configurations show that the M-ESEKF outperforms classical\nfilter formulations in terms of consistency and stability. Moreover, it\neliminates the need for scenario-specific parameter tuning, enabling its\napplication in a variety of real-world settings.\n", "link": "http://arxiv.org/abs/2508.14661v1", "date": "2025-08-20", "relevancy": 1.6996, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5782}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5701}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Pose%20Estimation%20of%20Unmanned%20Ground%20Vehicles%20through%0A%20%20Terrain-Aided%20Multi-Sensor%20Fusion%20on%20Geometric%20Manifolds&body=Title%3A%20Consistent%20Pose%20Estimation%20of%20Unmanned%20Ground%20Vehicles%20through%0A%20%20Terrain-Aided%20Multi-Sensor%20Fusion%20on%20Geometric%20Manifolds%0AAuthor%3A%20Alexander%20Raab%20and%20Stephan%20Weiss%20and%20Alessandro%20Fornasier%20and%20Christian%20Brommer%20and%20Abdalrahman%20Ibrahim%0AAbstract%3A%20%20%20Aiming%20to%20enhance%20the%20consistency%20and%20thus%20long-term%20accuracy%20of%20Extended%0AKalman%20Filters%20for%20terrestrial%20vehicle%20localization%2C%20this%20paper%20introduces%20the%0AManifold%20Error%20State%20Extended%20Kalman%20Filter%20%28M-ESEKF%29.%20By%20representing%20the%0Arobot%27s%20pose%20in%20a%20space%20with%20reduced%20dimensionality%2C%20the%20approach%20ensures%0Afeasible%20estimates%20on%20generic%20smooth%20surfaces%2C%20without%20introducing%20artificial%0Aconstraints%20or%20simplifications%20that%20may%20degrade%20a%20filter%27s%20performance.%20The%0Aaccompanying%20measurement%20models%20are%20compatible%20with%20common%20loosely-%20and%0Atightly-coupled%20sensor%20modalities%20and%20also%20implicitly%20account%20for%20the%20ground%0Ageometry.%20We%20extend%20the%20formulation%20by%20introducing%20a%20novel%20correction%20scheme%0Athat%20embeds%20additional%20domain%20knowledge%20into%20the%20sensor%20data%2C%20giving%20more%0Aaccurate%20uncertainty%20approximations%20and%20further%20enhancing%20filter%20consistency.%0AThe%20proposed%20estimator%20is%20seamlessly%20integrated%20into%20a%20validated%20modular%20state%0Aestimation%20framework%2C%20demonstrating%20compatibility%20with%20existing%0Aimplementations.%20Extensive%20Monte%20Carlo%20simulations%20across%20diverse%20scenarios%20and%0Adynamic%20sensor%20configurations%20show%20that%20the%20M-ESEKF%20outperforms%20classical%0Afilter%20formulations%20in%20terms%20of%20consistency%20and%20stability.%20Moreover%2C%20it%0Aeliminates%20the%20need%20for%20scenario-specific%20parameter%20tuning%2C%20enabling%20its%0Aapplication%20in%20a%20variety%20of%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Pose%2520Estimation%2520of%2520Unmanned%2520Ground%2520Vehicles%2520through%250A%2520%2520Terrain-Aided%2520Multi-Sensor%2520Fusion%2520on%2520Geometric%2520Manifolds%26entry.906535625%3DAlexander%2520Raab%2520and%2520Stephan%2520Weiss%2520and%2520Alessandro%2520Fornasier%2520and%2520Christian%2520Brommer%2520and%2520Abdalrahman%2520Ibrahim%26entry.1292438233%3D%2520%2520Aiming%2520to%2520enhance%2520the%2520consistency%2520and%2520thus%2520long-term%2520accuracy%2520of%2520Extended%250AKalman%2520Filters%2520for%2520terrestrial%2520vehicle%2520localization%252C%2520this%2520paper%2520introduces%2520the%250AManifold%2520Error%2520State%2520Extended%2520Kalman%2520Filter%2520%2528M-ESEKF%2529.%2520By%2520representing%2520the%250Arobot%2527s%2520pose%2520in%2520a%2520space%2520with%2520reduced%2520dimensionality%252C%2520the%2520approach%2520ensures%250Afeasible%2520estimates%2520on%2520generic%2520smooth%2520surfaces%252C%2520without%2520introducing%2520artificial%250Aconstraints%2520or%2520simplifications%2520that%2520may%2520degrade%2520a%2520filter%2527s%2520performance.%2520The%250Aaccompanying%2520measurement%2520models%2520are%2520compatible%2520with%2520common%2520loosely-%2520and%250Atightly-coupled%2520sensor%2520modalities%2520and%2520also%2520implicitly%2520account%2520for%2520the%2520ground%250Ageometry.%2520We%2520extend%2520the%2520formulation%2520by%2520introducing%2520a%2520novel%2520correction%2520scheme%250Athat%2520embeds%2520additional%2520domain%2520knowledge%2520into%2520the%2520sensor%2520data%252C%2520giving%2520more%250Aaccurate%2520uncertainty%2520approximations%2520and%2520further%2520enhancing%2520filter%2520consistency.%250AThe%2520proposed%2520estimator%2520is%2520seamlessly%2520integrated%2520into%2520a%2520validated%2520modular%2520state%250Aestimation%2520framework%252C%2520demonstrating%2520compatibility%2520with%2520existing%250Aimplementations.%2520Extensive%2520Monte%2520Carlo%2520simulations%2520across%2520diverse%2520scenarios%2520and%250Adynamic%2520sensor%2520configurations%2520show%2520that%2520the%2520M-ESEKF%2520outperforms%2520classical%250Afilter%2520formulations%2520in%2520terms%2520of%2520consistency%2520and%2520stability.%2520Moreover%252C%2520it%250Aeliminates%2520the%2520need%2520for%2520scenario-specific%2520parameter%2520tuning%252C%2520enabling%2520its%250Aapplication%2520in%2520a%2520variety%2520of%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Pose%20Estimation%20of%20Unmanned%20Ground%20Vehicles%20through%0A%20%20Terrain-Aided%20Multi-Sensor%20Fusion%20on%20Geometric%20Manifolds&entry.906535625=Alexander%20Raab%20and%20Stephan%20Weiss%20and%20Alessandro%20Fornasier%20and%20Christian%20Brommer%20and%20Abdalrahman%20Ibrahim&entry.1292438233=%20%20Aiming%20to%20enhance%20the%20consistency%20and%20thus%20long-term%20accuracy%20of%20Extended%0AKalman%20Filters%20for%20terrestrial%20vehicle%20localization%2C%20this%20paper%20introduces%20the%0AManifold%20Error%20State%20Extended%20Kalman%20Filter%20%28M-ESEKF%29.%20By%20representing%20the%0Arobot%27s%20pose%20in%20a%20space%20with%20reduced%20dimensionality%2C%20the%20approach%20ensures%0Afeasible%20estimates%20on%20generic%20smooth%20surfaces%2C%20without%20introducing%20artificial%0Aconstraints%20or%20simplifications%20that%20may%20degrade%20a%20filter%27s%20performance.%20The%0Aaccompanying%20measurement%20models%20are%20compatible%20with%20common%20loosely-%20and%0Atightly-coupled%20sensor%20modalities%20and%20also%20implicitly%20account%20for%20the%20ground%0Ageometry.%20We%20extend%20the%20formulation%20by%20introducing%20a%20novel%20correction%20scheme%0Athat%20embeds%20additional%20domain%20knowledge%20into%20the%20sensor%20data%2C%20giving%20more%0Aaccurate%20uncertainty%20approximations%20and%20further%20enhancing%20filter%20consistency.%0AThe%20proposed%20estimator%20is%20seamlessly%20integrated%20into%20a%20validated%20modular%20state%0Aestimation%20framework%2C%20demonstrating%20compatibility%20with%20existing%0Aimplementations.%20Extensive%20Monte%20Carlo%20simulations%20across%20diverse%20scenarios%20and%0Adynamic%20sensor%20configurations%20show%20that%20the%20M-ESEKF%20outperforms%20classical%0Afilter%20formulations%20in%20terms%20of%20consistency%20and%20stability.%20Moreover%2C%20it%0Aeliminates%20the%20need%20for%20scenario-specific%20parameter%20tuning%2C%20enabling%20its%0Aapplication%20in%20a%20variety%20of%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14661v1&entry.124074799=Read"},
{"title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic\n  Design", "author": "Fei Liu and Yilu Liu and Qingfu Zhang and Xialiang Tong and Mingxuan Yuan", "abstract": "  Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements.\n", "link": "http://arxiv.org/abs/2508.03082v2", "date": "2025-08-20", "relevancy": 1.4181, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4785}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4723}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EoH-S%3A%20Evolution%20of%20Heuristic%20Set%20using%20LLMs%20for%20Automated%20Heuristic%0A%20%20Design&body=Title%3A%20EoH-S%3A%20Evolution%20of%20Heuristic%20Set%20using%20LLMs%20for%20Automated%20Heuristic%0A%20%20Design%0AAuthor%3A%20Fei%20Liu%20and%20Yilu%20Liu%20and%20Qingfu%20Zhang%20and%20Xialiang%20Tong%20and%20Mingxuan%20Yuan%0AAbstract%3A%20%20%20Automated%20Heuristic%20Design%20%28AHD%29%20using%20Large%20Language%20Models%20%28LLMs%29%20has%0Aachieved%20notable%20success%20in%20recent%20years.%20Despite%20the%20effectiveness%20of%20existing%0Aapproaches%2C%20they%20only%20design%20a%20single%20heuristic%20to%20serve%20all%20problem%20instances%2C%0Aoften%20inducing%20poor%20generalization%20across%20different%20distributions%20or%20settings.%0ATo%20address%20this%20issue%2C%20we%20propose%20Automated%20Heuristic%20Set%20Design%20%28AHSD%29%2C%20a%20new%0Aformulation%20for%20LLM-driven%20AHD.%20The%20aim%20of%20AHSD%20is%20to%20automatically%20generate%20a%0Asmall-sized%20complementary%20heuristic%20set%20to%20serve%20diverse%20problem%20instances%2C%0Asuch%20that%20each%20problem%20instance%20could%20be%20optimized%20by%20at%20least%20one%20heuristic%20in%0Athis%20set.%20We%20show%20that%20the%20objective%20function%20of%20AHSD%20is%20monotone%20and%0Asupermodular.%20Then%2C%20we%20propose%20Evolution%20of%20Heuristic%20Set%20%28EoH-S%29%20to%20apply%20the%0AAHSD%20formulation%20for%20LLM-driven%20AHD.%20With%20two%20novel%20mechanisms%20of%20complementary%0Apopulation%20management%20and%20complementary-aware%20memetic%20search%2C%20EoH-S%20could%0Aeffectively%20generate%20a%20set%20of%20high-quality%20and%20complementary%20heuristics.%0AComprehensive%20experimental%20results%20on%20three%20AHD%20tasks%20with%20diverse%20instances%0Aspanning%20various%20sizes%20and%20distributions%20demonstrate%20that%20EoH-S%20consistently%0Aoutperforms%20existing%20state-of-the-art%20AHD%20methods%20and%20achieves%20up%20to%2060%5C%25%0Aperformance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEoH-S%253A%2520Evolution%2520of%2520Heuristic%2520Set%2520using%2520LLMs%2520for%2520Automated%2520Heuristic%250A%2520%2520Design%26entry.906535625%3DFei%2520Liu%2520and%2520Yilu%2520Liu%2520and%2520Qingfu%2520Zhang%2520and%2520Xialiang%2520Tong%2520and%2520Mingxuan%2520Yuan%26entry.1292438233%3D%2520%2520Automated%2520Heuristic%2520Design%2520%2528AHD%2529%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%250Aachieved%2520notable%2520success%2520in%2520recent%2520years.%2520Despite%2520the%2520effectiveness%2520of%2520existing%250Aapproaches%252C%2520they%2520only%2520design%2520a%2520single%2520heuristic%2520to%2520serve%2520all%2520problem%2520instances%252C%250Aoften%2520inducing%2520poor%2520generalization%2520across%2520different%2520distributions%2520or%2520settings.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520Automated%2520Heuristic%2520Set%2520Design%2520%2528AHSD%2529%252C%2520a%2520new%250Aformulation%2520for%2520LLM-driven%2520AHD.%2520The%2520aim%2520of%2520AHSD%2520is%2520to%2520automatically%2520generate%2520a%250Asmall-sized%2520complementary%2520heuristic%2520set%2520to%2520serve%2520diverse%2520problem%2520instances%252C%250Asuch%2520that%2520each%2520problem%2520instance%2520could%2520be%2520optimized%2520by%2520at%2520least%2520one%2520heuristic%2520in%250Athis%2520set.%2520We%2520show%2520that%2520the%2520objective%2520function%2520of%2520AHSD%2520is%2520monotone%2520and%250Asupermodular.%2520Then%252C%2520we%2520propose%2520Evolution%2520of%2520Heuristic%2520Set%2520%2528EoH-S%2529%2520to%2520apply%2520the%250AAHSD%2520formulation%2520for%2520LLM-driven%2520AHD.%2520With%2520two%2520novel%2520mechanisms%2520of%2520complementary%250Apopulation%2520management%2520and%2520complementary-aware%2520memetic%2520search%252C%2520EoH-S%2520could%250Aeffectively%2520generate%2520a%2520set%2520of%2520high-quality%2520and%2520complementary%2520heuristics.%250AComprehensive%2520experimental%2520results%2520on%2520three%2520AHD%2520tasks%2520with%2520diverse%2520instances%250Aspanning%2520various%2520sizes%2520and%2520distributions%2520demonstrate%2520that%2520EoH-S%2520consistently%250Aoutperforms%2520existing%2520state-of-the-art%2520AHD%2520methods%2520and%2520achieves%2520up%2520to%252060%255C%2525%250Aperformance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EoH-S%3A%20Evolution%20of%20Heuristic%20Set%20using%20LLMs%20for%20Automated%20Heuristic%0A%20%20Design&entry.906535625=Fei%20Liu%20and%20Yilu%20Liu%20and%20Qingfu%20Zhang%20and%20Xialiang%20Tong%20and%20Mingxuan%20Yuan&entry.1292438233=%20%20Automated%20Heuristic%20Design%20%28AHD%29%20using%20Large%20Language%20Models%20%28LLMs%29%20has%0Aachieved%20notable%20success%20in%20recent%20years.%20Despite%20the%20effectiveness%20of%20existing%0Aapproaches%2C%20they%20only%20design%20a%20single%20heuristic%20to%20serve%20all%20problem%20instances%2C%0Aoften%20inducing%20poor%20generalization%20across%20different%20distributions%20or%20settings.%0ATo%20address%20this%20issue%2C%20we%20propose%20Automated%20Heuristic%20Set%20Design%20%28AHSD%29%2C%20a%20new%0Aformulation%20for%20LLM-driven%20AHD.%20The%20aim%20of%20AHSD%20is%20to%20automatically%20generate%20a%0Asmall-sized%20complementary%20heuristic%20set%20to%20serve%20diverse%20problem%20instances%2C%0Asuch%20that%20each%20problem%20instance%20could%20be%20optimized%20by%20at%20least%20one%20heuristic%20in%0Athis%20set.%20We%20show%20that%20the%20objective%20function%20of%20AHSD%20is%20monotone%20and%0Asupermodular.%20Then%2C%20we%20propose%20Evolution%20of%20Heuristic%20Set%20%28EoH-S%29%20to%20apply%20the%0AAHSD%20formulation%20for%20LLM-driven%20AHD.%20With%20two%20novel%20mechanisms%20of%20complementary%0Apopulation%20management%20and%20complementary-aware%20memetic%20search%2C%20EoH-S%20could%0Aeffectively%20generate%20a%20set%20of%20high-quality%20and%20complementary%20heuristics.%0AComprehensive%20experimental%20results%20on%20three%20AHD%20tasks%20with%20diverse%20instances%0Aspanning%20various%20sizes%20and%20distributions%20demonstrate%20that%20EoH-S%20consistently%0Aoutperforms%20existing%20state-of-the-art%20AHD%20methods%20and%20achieves%20up%20to%2060%5C%25%0Aperformance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03082v2&entry.124074799=Read"},
{"title": "Action Engine: Automatic Workflow Generation in FaaS", "author": "Akiharu Esashi and Pawissanutt Lertpongrujikorn and Shinji Kato and Mohsen Amini Salehi", "abstract": "  Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge, platform dependence, and difficulty in scalability in\nbuilding functional workflows persist for cloud-native application developers.\nTo overcome these challenges and mitigate the burden of developing FaaS-based\napplications, in this paper, we propose a mechanism called Action Engine, that\nmakes use of tool-augmented large language models (LLMs) at its kernel to\ninterpret human language queries and automates FaaS workflow generation,\nthereby, reducing the need for specialized expertise and manual design. Action\nEngine includes modules to identify relevant functions from the FaaS repository\nand seamlessly manage the data dependency between them, ensuring the\ndeveloper's query is processed and resolved. Beyond that, Action Engine can\nexecute the generated workflow by injecting the user-provided arguments. On\nanother front, this work addresses a gap in tool-augmented LLM research via\nadopting an Automatic FaaS Workflow Generation perspective to systematically\nevaluate methodologies across four fundamental sub-processes. Through\nbenchmarking various parameters, this research provides critical insights into\nstreamlining workflow automation for real-world applications, specifically in\nthe FaaS continuum. Our evaluations demonstrate that the Action Engine achieves\ncomparable performance to the few-shot learning approach while maintaining\nplatform- and language-agnosticism, thereby, mitigating provider-specific\ndependencies in workflow generation. We notice that Action Engine can unlock\nFaaS workflow generation for non-cloud-savvy developers and expedite the\ndevelopment cycles of cloud-native applications.\n", "link": "http://arxiv.org/abs/2411.19485v2", "date": "2025-08-20", "relevancy": 1.7219, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4539}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4384}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action%20Engine%3A%20Automatic%20Workflow%20Generation%20in%20FaaS&body=Title%3A%20Action%20Engine%3A%20Automatic%20Workflow%20Generation%20in%20FaaS%0AAuthor%3A%20Akiharu%20Esashi%20and%20Pawissanutt%20Lertpongrujikorn%20and%20Shinji%20Kato%20and%20Mohsen%20Amini%20Salehi%0AAbstract%3A%20%20%20Function%20as%20a%20Service%20%28FaaS%29%20is%20poised%20to%20become%20the%20foundation%20of%20the%20next%0Ageneration%20of%20cloud%20systems%20due%20to%20its%20inherent%20advantages%20in%20scalability%2C%0Acost-efficiency%2C%20and%20ease%20of%20use.%20However%2C%20challenges%20such%20as%20the%20need%20for%0Aspecialized%20knowledge%2C%20platform%20dependence%2C%20and%20difficulty%20in%20scalability%20in%0Abuilding%20functional%20workflows%20persist%20for%20cloud-native%20application%20developers.%0ATo%20overcome%20these%20challenges%20and%20mitigate%20the%20burden%20of%20developing%20FaaS-based%0Aapplications%2C%20in%20this%20paper%2C%20we%20propose%20a%20mechanism%20called%20Action%20Engine%2C%20that%0Amakes%20use%20of%20tool-augmented%20large%20language%20models%20%28LLMs%29%20at%20its%20kernel%20to%0Ainterpret%20human%20language%20queries%20and%20automates%20FaaS%20workflow%20generation%2C%0Athereby%2C%20reducing%20the%20need%20for%20specialized%20expertise%20and%20manual%20design.%20Action%0AEngine%20includes%20modules%20to%20identify%20relevant%20functions%20from%20the%20FaaS%20repository%0Aand%20seamlessly%20manage%20the%20data%20dependency%20between%20them%2C%20ensuring%20the%0Adeveloper%27s%20query%20is%20processed%20and%20resolved.%20Beyond%20that%2C%20Action%20Engine%20can%0Aexecute%20the%20generated%20workflow%20by%20injecting%20the%20user-provided%20arguments.%20On%0Aanother%20front%2C%20this%20work%20addresses%20a%20gap%20in%20tool-augmented%20LLM%20research%20via%0Aadopting%20an%20Automatic%20FaaS%20Workflow%20Generation%20perspective%20to%20systematically%0Aevaluate%20methodologies%20across%20four%20fundamental%20sub-processes.%20Through%0Abenchmarking%20various%20parameters%2C%20this%20research%20provides%20critical%20insights%20into%0Astreamlining%20workflow%20automation%20for%20real-world%20applications%2C%20specifically%20in%0Athe%20FaaS%20continuum.%20Our%20evaluations%20demonstrate%20that%20the%20Action%20Engine%20achieves%0Acomparable%20performance%20to%20the%20few-shot%20learning%20approach%20while%20maintaining%0Aplatform-%20and%20language-agnosticism%2C%20thereby%2C%20mitigating%20provider-specific%0Adependencies%20in%20workflow%20generation.%20We%20notice%20that%20Action%20Engine%20can%20unlock%0AFaaS%20workflow%20generation%20for%20non-cloud-savvy%20developers%20and%20expedite%20the%0Adevelopment%20cycles%20of%20cloud-native%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction%2520Engine%253A%2520Automatic%2520Workflow%2520Generation%2520in%2520FaaS%26entry.906535625%3DAkiharu%2520Esashi%2520and%2520Pawissanutt%2520Lertpongrujikorn%2520and%2520Shinji%2520Kato%2520and%2520Mohsen%2520Amini%2520Salehi%26entry.1292438233%3D%2520%2520Function%2520as%2520a%2520Service%2520%2528FaaS%2529%2520is%2520poised%2520to%2520become%2520the%2520foundation%2520of%2520the%2520next%250Ageneration%2520of%2520cloud%2520systems%2520due%2520to%2520its%2520inherent%2520advantages%2520in%2520scalability%252C%250Acost-efficiency%252C%2520and%2520ease%2520of%2520use.%2520However%252C%2520challenges%2520such%2520as%2520the%2520need%2520for%250Aspecialized%2520knowledge%252C%2520platform%2520dependence%252C%2520and%2520difficulty%2520in%2520scalability%2520in%250Abuilding%2520functional%2520workflows%2520persist%2520for%2520cloud-native%2520application%2520developers.%250ATo%2520overcome%2520these%2520challenges%2520and%2520mitigate%2520the%2520burden%2520of%2520developing%2520FaaS-based%250Aapplications%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520mechanism%2520called%2520Action%2520Engine%252C%2520that%250Amakes%2520use%2520of%2520tool-augmented%2520large%2520language%2520models%2520%2528LLMs%2529%2520at%2520its%2520kernel%2520to%250Ainterpret%2520human%2520language%2520queries%2520and%2520automates%2520FaaS%2520workflow%2520generation%252C%250Athereby%252C%2520reducing%2520the%2520need%2520for%2520specialized%2520expertise%2520and%2520manual%2520design.%2520Action%250AEngine%2520includes%2520modules%2520to%2520identify%2520relevant%2520functions%2520from%2520the%2520FaaS%2520repository%250Aand%2520seamlessly%2520manage%2520the%2520data%2520dependency%2520between%2520them%252C%2520ensuring%2520the%250Adeveloper%2527s%2520query%2520is%2520processed%2520and%2520resolved.%2520Beyond%2520that%252C%2520Action%2520Engine%2520can%250Aexecute%2520the%2520generated%2520workflow%2520by%2520injecting%2520the%2520user-provided%2520arguments.%2520On%250Aanother%2520front%252C%2520this%2520work%2520addresses%2520a%2520gap%2520in%2520tool-augmented%2520LLM%2520research%2520via%250Aadopting%2520an%2520Automatic%2520FaaS%2520Workflow%2520Generation%2520perspective%2520to%2520systematically%250Aevaluate%2520methodologies%2520across%2520four%2520fundamental%2520sub-processes.%2520Through%250Abenchmarking%2520various%2520parameters%252C%2520this%2520research%2520provides%2520critical%2520insights%2520into%250Astreamlining%2520workflow%2520automation%2520for%2520real-world%2520applications%252C%2520specifically%2520in%250Athe%2520FaaS%2520continuum.%2520Our%2520evaluations%2520demonstrate%2520that%2520the%2520Action%2520Engine%2520achieves%250Acomparable%2520performance%2520to%2520the%2520few-shot%2520learning%2520approach%2520while%2520maintaining%250Aplatform-%2520and%2520language-agnosticism%252C%2520thereby%252C%2520mitigating%2520provider-specific%250Adependencies%2520in%2520workflow%2520generation.%2520We%2520notice%2520that%2520Action%2520Engine%2520can%2520unlock%250AFaaS%2520workflow%2520generation%2520for%2520non-cloud-savvy%2520developers%2520and%2520expedite%2520the%250Adevelopment%2520cycles%2520of%2520cloud-native%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action%20Engine%3A%20Automatic%20Workflow%20Generation%20in%20FaaS&entry.906535625=Akiharu%20Esashi%20and%20Pawissanutt%20Lertpongrujikorn%20and%20Shinji%20Kato%20and%20Mohsen%20Amini%20Salehi&entry.1292438233=%20%20Function%20as%20a%20Service%20%28FaaS%29%20is%20poised%20to%20become%20the%20foundation%20of%20the%20next%0Ageneration%20of%20cloud%20systems%20due%20to%20its%20inherent%20advantages%20in%20scalability%2C%0Acost-efficiency%2C%20and%20ease%20of%20use.%20However%2C%20challenges%20such%20as%20the%20need%20for%0Aspecialized%20knowledge%2C%20platform%20dependence%2C%20and%20difficulty%20in%20scalability%20in%0Abuilding%20functional%20workflows%20persist%20for%20cloud-native%20application%20developers.%0ATo%20overcome%20these%20challenges%20and%20mitigate%20the%20burden%20of%20developing%20FaaS-based%0Aapplications%2C%20in%20this%20paper%2C%20we%20propose%20a%20mechanism%20called%20Action%20Engine%2C%20that%0Amakes%20use%20of%20tool-augmented%20large%20language%20models%20%28LLMs%29%20at%20its%20kernel%20to%0Ainterpret%20human%20language%20queries%20and%20automates%20FaaS%20workflow%20generation%2C%0Athereby%2C%20reducing%20the%20need%20for%20specialized%20expertise%20and%20manual%20design.%20Action%0AEngine%20includes%20modules%20to%20identify%20relevant%20functions%20from%20the%20FaaS%20repository%0Aand%20seamlessly%20manage%20the%20data%20dependency%20between%20them%2C%20ensuring%20the%0Adeveloper%27s%20query%20is%20processed%20and%20resolved.%20Beyond%20that%2C%20Action%20Engine%20can%0Aexecute%20the%20generated%20workflow%20by%20injecting%20the%20user-provided%20arguments.%20On%0Aanother%20front%2C%20this%20work%20addresses%20a%20gap%20in%20tool-augmented%20LLM%20research%20via%0Aadopting%20an%20Automatic%20FaaS%20Workflow%20Generation%20perspective%20to%20systematically%0Aevaluate%20methodologies%20across%20four%20fundamental%20sub-processes.%20Through%0Abenchmarking%20various%20parameters%2C%20this%20research%20provides%20critical%20insights%20into%0Astreamlining%20workflow%20automation%20for%20real-world%20applications%2C%20specifically%20in%0Athe%20FaaS%20continuum.%20Our%20evaluations%20demonstrate%20that%20the%20Action%20Engine%20achieves%0Acomparable%20performance%20to%20the%20few-shot%20learning%20approach%20while%20maintaining%0Aplatform-%20and%20language-agnosticism%2C%20thereby%2C%20mitigating%20provider-specific%0Adependencies%20in%20workflow%20generation.%20We%20notice%20that%20Action%20Engine%20can%20unlock%0AFaaS%20workflow%20generation%20for%20non-cloud-savvy%20developers%20and%20expedite%20the%0Adevelopment%20cycles%20of%20cloud-native%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19485v2&entry.124074799=Read"},
{"title": "Compute-Optimal Scaling for Value-Based Deep RL", "author": "Preston Fu and Oleh Rybkin and Zhiyuan Zhou and Michal Nauman and Pieter Abbeel and Sergey Levine and Aviral Kumar", "abstract": "  As models grow larger and training them becomes expensive, it becomes\nincreasingly important to scale training recipes not just to larger models and\nmore data, but to do so in a compute-optimal manner that extracts maximal\nperformance per unit of compute. While such scaling has been well studied for\nlanguage modeling, reinforcement learning (RL) has received less attention in\nthis regard. In this paper, we investigate compute scaling for online,\nvalue-based deep RL. These methods present two primary axes for compute\nallocation: model capacity and the update-to-data (UTD) ratio. Given a fixed\ncompute budget, we ask: how should resources be partitioned across these axes\nto maximize sample efficiency? Our analysis reveals a nuanced interplay between\nmodel size, batch size, and UTD. In particular, we identify a phenomenon we\ncall TD-overfitting: increasing the batch quickly harms Q-function accuracy for\nsmall models, but this effect is absent in large models, enabling effective use\nof large batch size at scale. We provide a mental model for understanding this\nphenomenon and build guidelines for choosing batch size and UTD to optimize\ncompute usage. Our findings provide a grounded starting point for\ncompute-optimal scaling in deep RL, mirroring studies in supervised learning\nbut adapted to TD learning.\n", "link": "http://arxiv.org/abs/2508.14881v1", "date": "2025-08-20", "relevancy": 1.5305, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.563}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4978}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compute-Optimal%20Scaling%20for%20Value-Based%20Deep%20RL&body=Title%3A%20Compute-Optimal%20Scaling%20for%20Value-Based%20Deep%20RL%0AAuthor%3A%20Preston%20Fu%20and%20Oleh%20Rybkin%20and%20Zhiyuan%20Zhou%20and%20Michal%20Nauman%20and%20Pieter%20Abbeel%20and%20Sergey%20Levine%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20As%20models%20grow%20larger%20and%20training%20them%20becomes%20expensive%2C%20it%20becomes%0Aincreasingly%20important%20to%20scale%20training%20recipes%20not%20just%20to%20larger%20models%20and%0Amore%20data%2C%20but%20to%20do%20so%20in%20a%20compute-optimal%20manner%20that%20extracts%20maximal%0Aperformance%20per%20unit%20of%20compute.%20While%20such%20scaling%20has%20been%20well%20studied%20for%0Alanguage%20modeling%2C%20reinforcement%20learning%20%28RL%29%20has%20received%20less%20attention%20in%0Athis%20regard.%20In%20this%20paper%2C%20we%20investigate%20compute%20scaling%20for%20online%2C%0Avalue-based%20deep%20RL.%20These%20methods%20present%20two%20primary%20axes%20for%20compute%0Aallocation%3A%20model%20capacity%20and%20the%20update-to-data%20%28UTD%29%20ratio.%20Given%20a%20fixed%0Acompute%20budget%2C%20we%20ask%3A%20how%20should%20resources%20be%20partitioned%20across%20these%20axes%0Ato%20maximize%20sample%20efficiency%3F%20Our%20analysis%20reveals%20a%20nuanced%20interplay%20between%0Amodel%20size%2C%20batch%20size%2C%20and%20UTD.%20In%20particular%2C%20we%20identify%20a%20phenomenon%20we%0Acall%20TD-overfitting%3A%20increasing%20the%20batch%20quickly%20harms%20Q-function%20accuracy%20for%0Asmall%20models%2C%20but%20this%20effect%20is%20absent%20in%20large%20models%2C%20enabling%20effective%20use%0Aof%20large%20batch%20size%20at%20scale.%20We%20provide%20a%20mental%20model%20for%20understanding%20this%0Aphenomenon%20and%20build%20guidelines%20for%20choosing%20batch%20size%20and%20UTD%20to%20optimize%0Acompute%20usage.%20Our%20findings%20provide%20a%20grounded%20starting%20point%20for%0Acompute-optimal%20scaling%20in%20deep%20RL%2C%20mirroring%20studies%20in%20supervised%20learning%0Abut%20adapted%20to%20TD%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompute-Optimal%2520Scaling%2520for%2520Value-Based%2520Deep%2520RL%26entry.906535625%3DPreston%2520Fu%2520and%2520Oleh%2520Rybkin%2520and%2520Zhiyuan%2520Zhou%2520and%2520Michal%2520Nauman%2520and%2520Pieter%2520Abbeel%2520and%2520Sergey%2520Levine%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520As%2520models%2520grow%2520larger%2520and%2520training%2520them%2520becomes%2520expensive%252C%2520it%2520becomes%250Aincreasingly%2520important%2520to%2520scale%2520training%2520recipes%2520not%2520just%2520to%2520larger%2520models%2520and%250Amore%2520data%252C%2520but%2520to%2520do%2520so%2520in%2520a%2520compute-optimal%2520manner%2520that%2520extracts%2520maximal%250Aperformance%2520per%2520unit%2520of%2520compute.%2520While%2520such%2520scaling%2520has%2520been%2520well%2520studied%2520for%250Alanguage%2520modeling%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520received%2520less%2520attention%2520in%250Athis%2520regard.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520compute%2520scaling%2520for%2520online%252C%250Avalue-based%2520deep%2520RL.%2520These%2520methods%2520present%2520two%2520primary%2520axes%2520for%2520compute%250Aallocation%253A%2520model%2520capacity%2520and%2520the%2520update-to-data%2520%2528UTD%2529%2520ratio.%2520Given%2520a%2520fixed%250Acompute%2520budget%252C%2520we%2520ask%253A%2520how%2520should%2520resources%2520be%2520partitioned%2520across%2520these%2520axes%250Ato%2520maximize%2520sample%2520efficiency%253F%2520Our%2520analysis%2520reveals%2520a%2520nuanced%2520interplay%2520between%250Amodel%2520size%252C%2520batch%2520size%252C%2520and%2520UTD.%2520In%2520particular%252C%2520we%2520identify%2520a%2520phenomenon%2520we%250Acall%2520TD-overfitting%253A%2520increasing%2520the%2520batch%2520quickly%2520harms%2520Q-function%2520accuracy%2520for%250Asmall%2520models%252C%2520but%2520this%2520effect%2520is%2520absent%2520in%2520large%2520models%252C%2520enabling%2520effective%2520use%250Aof%2520large%2520batch%2520size%2520at%2520scale.%2520We%2520provide%2520a%2520mental%2520model%2520for%2520understanding%2520this%250Aphenomenon%2520and%2520build%2520guidelines%2520for%2520choosing%2520batch%2520size%2520and%2520UTD%2520to%2520optimize%250Acompute%2520usage.%2520Our%2520findings%2520provide%2520a%2520grounded%2520starting%2520point%2520for%250Acompute-optimal%2520scaling%2520in%2520deep%2520RL%252C%2520mirroring%2520studies%2520in%2520supervised%2520learning%250Abut%2520adapted%2520to%2520TD%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compute-Optimal%20Scaling%20for%20Value-Based%20Deep%20RL&entry.906535625=Preston%20Fu%20and%20Oleh%20Rybkin%20and%20Zhiyuan%20Zhou%20and%20Michal%20Nauman%20and%20Pieter%20Abbeel%20and%20Sergey%20Levine%20and%20Aviral%20Kumar&entry.1292438233=%20%20As%20models%20grow%20larger%20and%20training%20them%20becomes%20expensive%2C%20it%20becomes%0Aincreasingly%20important%20to%20scale%20training%20recipes%20not%20just%20to%20larger%20models%20and%0Amore%20data%2C%20but%20to%20do%20so%20in%20a%20compute-optimal%20manner%20that%20extracts%20maximal%0Aperformance%20per%20unit%20of%20compute.%20While%20such%20scaling%20has%20been%20well%20studied%20for%0Alanguage%20modeling%2C%20reinforcement%20learning%20%28RL%29%20has%20received%20less%20attention%20in%0Athis%20regard.%20In%20this%20paper%2C%20we%20investigate%20compute%20scaling%20for%20online%2C%0Avalue-based%20deep%20RL.%20These%20methods%20present%20two%20primary%20axes%20for%20compute%0Aallocation%3A%20model%20capacity%20and%20the%20update-to-data%20%28UTD%29%20ratio.%20Given%20a%20fixed%0Acompute%20budget%2C%20we%20ask%3A%20how%20should%20resources%20be%20partitioned%20across%20these%20axes%0Ato%20maximize%20sample%20efficiency%3F%20Our%20analysis%20reveals%20a%20nuanced%20interplay%20between%0Amodel%20size%2C%20batch%20size%2C%20and%20UTD.%20In%20particular%2C%20we%20identify%20a%20phenomenon%20we%0Acall%20TD-overfitting%3A%20increasing%20the%20batch%20quickly%20harms%20Q-function%20accuracy%20for%0Asmall%20models%2C%20but%20this%20effect%20is%20absent%20in%20large%20models%2C%20enabling%20effective%20use%0Aof%20large%20batch%20size%20at%20scale.%20We%20provide%20a%20mental%20model%20for%20understanding%20this%0Aphenomenon%20and%20build%20guidelines%20for%20choosing%20batch%20size%20and%20UTD%20to%20optimize%0Acompute%20usage.%20Our%20findings%20provide%20a%20grounded%20starting%20point%20for%0Acompute-optimal%20scaling%20in%20deep%20RL%2C%20mirroring%20studies%20in%20supervised%20learning%0Abut%20adapted%20to%20TD%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14881v1&entry.124074799=Read"},
{"title": "Foe for Fraud: Transferable Adversarial Attacks in Credit Card Fraud\n  Detection", "author": "Jan Lum Fok and Qingwen Zeng and Shiping Chen and Oscar Fawkes and Huaming Chen", "abstract": "  Credit card fraud detection (CCFD) is a critical application of Machine\nLearning (ML) in the financial sector, where accurately identifying fraudulent\ntransactions is essential for mitigating financial losses. ML models have\ndemonstrated their effectiveness in fraud detection task, in particular with\nthe tabular dataset. While adversarial attacks have been extensively studied in\ncomputer vision and deep learning, their impacts on the ML models, particularly\nthose trained on CCFD tabular datasets, remains largely unexplored. These\nlatent vulnerabilities pose significant threats to the security and stability\nof the financial industry, especially in high-value transactions where losses\ncould be substantial. To address this gap, in this paper, we present a holistic\nframework that investigate the robustness of CCFD ML model against adversarial\nperturbations under different circumstances. Specifically, the gradient-based\nattack methods are incorporated into the tabular credit card transaction data\nin both black- and white-box adversarial attacks settings. Our findings confirm\nthat tabular data is also susceptible to subtle perturbations, highlighting the\nneed for heightened awareness among financial technology practitioners\nregarding ML model security and trustworthiness. Furthermore, the experiments\nby transferring adversarial samples from gradient-based attack method to\nnon-gradient-based models also verify our findings. Our results demonstrate\nthat such attacks remain effective, emphasizing the necessity of developing\nrobust defenses for CCFD algorithms.\n", "link": "http://arxiv.org/abs/2508.14699v1", "date": "2025-08-20", "relevancy": 1.867, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4877}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4539}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foe%20for%20Fraud%3A%20Transferable%20Adversarial%20Attacks%20in%20Credit%20Card%20Fraud%0A%20%20Detection&body=Title%3A%20Foe%20for%20Fraud%3A%20Transferable%20Adversarial%20Attacks%20in%20Credit%20Card%20Fraud%0A%20%20Detection%0AAuthor%3A%20Jan%20Lum%20Fok%20and%20Qingwen%20Zeng%20and%20Shiping%20Chen%20and%20Oscar%20Fawkes%20and%20Huaming%20Chen%0AAbstract%3A%20%20%20Credit%20card%20fraud%20detection%20%28CCFD%29%20is%20a%20critical%20application%20of%20Machine%0ALearning%20%28ML%29%20in%20the%20financial%20sector%2C%20where%20accurately%20identifying%20fraudulent%0Atransactions%20is%20essential%20for%20mitigating%20financial%20losses.%20ML%20models%20have%0Ademonstrated%20their%20effectiveness%20in%20fraud%20detection%20task%2C%20in%20particular%20with%0Athe%20tabular%20dataset.%20While%20adversarial%20attacks%20have%20been%20extensively%20studied%20in%0Acomputer%20vision%20and%20deep%20learning%2C%20their%20impacts%20on%20the%20ML%20models%2C%20particularly%0Athose%20trained%20on%20CCFD%20tabular%20datasets%2C%20remains%20largely%20unexplored.%20These%0Alatent%20vulnerabilities%20pose%20significant%20threats%20to%20the%20security%20and%20stability%0Aof%20the%20financial%20industry%2C%20especially%20in%20high-value%20transactions%20where%20losses%0Acould%20be%20substantial.%20To%20address%20this%20gap%2C%20in%20this%20paper%2C%20we%20present%20a%20holistic%0Aframework%20that%20investigate%20the%20robustness%20of%20CCFD%20ML%20model%20against%20adversarial%0Aperturbations%20under%20different%20circumstances.%20Specifically%2C%20the%20gradient-based%0Aattack%20methods%20are%20incorporated%20into%20the%20tabular%20credit%20card%20transaction%20data%0Ain%20both%20black-%20and%20white-box%20adversarial%20attacks%20settings.%20Our%20findings%20confirm%0Athat%20tabular%20data%20is%20also%20susceptible%20to%20subtle%20perturbations%2C%20highlighting%20the%0Aneed%20for%20heightened%20awareness%20among%20financial%20technology%20practitioners%0Aregarding%20ML%20model%20security%20and%20trustworthiness.%20Furthermore%2C%20the%20experiments%0Aby%20transferring%20adversarial%20samples%20from%20gradient-based%20attack%20method%20to%0Anon-gradient-based%20models%20also%20verify%20our%20findings.%20Our%20results%20demonstrate%0Athat%20such%20attacks%20remain%20effective%2C%20emphasizing%20the%20necessity%20of%20developing%0Arobust%20defenses%20for%20CCFD%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoe%2520for%2520Fraud%253A%2520Transferable%2520Adversarial%2520Attacks%2520in%2520Credit%2520Card%2520Fraud%250A%2520%2520Detection%26entry.906535625%3DJan%2520Lum%2520Fok%2520and%2520Qingwen%2520Zeng%2520and%2520Shiping%2520Chen%2520and%2520Oscar%2520Fawkes%2520and%2520Huaming%2520Chen%26entry.1292438233%3D%2520%2520Credit%2520card%2520fraud%2520detection%2520%2528CCFD%2529%2520is%2520a%2520critical%2520application%2520of%2520Machine%250ALearning%2520%2528ML%2529%2520in%2520the%2520financial%2520sector%252C%2520where%2520accurately%2520identifying%2520fraudulent%250Atransactions%2520is%2520essential%2520for%2520mitigating%2520financial%2520losses.%2520ML%2520models%2520have%250Ademonstrated%2520their%2520effectiveness%2520in%2520fraud%2520detection%2520task%252C%2520in%2520particular%2520with%250Athe%2520tabular%2520dataset.%2520While%2520adversarial%2520attacks%2520have%2520been%2520extensively%2520studied%2520in%250Acomputer%2520vision%2520and%2520deep%2520learning%252C%2520their%2520impacts%2520on%2520the%2520ML%2520models%252C%2520particularly%250Athose%2520trained%2520on%2520CCFD%2520tabular%2520datasets%252C%2520remains%2520largely%2520unexplored.%2520These%250Alatent%2520vulnerabilities%2520pose%2520significant%2520threats%2520to%2520the%2520security%2520and%2520stability%250Aof%2520the%2520financial%2520industry%252C%2520especially%2520in%2520high-value%2520transactions%2520where%2520losses%250Acould%2520be%2520substantial.%2520To%2520address%2520this%2520gap%252C%2520in%2520this%2520paper%252C%2520we%2520present%2520a%2520holistic%250Aframework%2520that%2520investigate%2520the%2520robustness%2520of%2520CCFD%2520ML%2520model%2520against%2520adversarial%250Aperturbations%2520under%2520different%2520circumstances.%2520Specifically%252C%2520the%2520gradient-based%250Aattack%2520methods%2520are%2520incorporated%2520into%2520the%2520tabular%2520credit%2520card%2520transaction%2520data%250Ain%2520both%2520black-%2520and%2520white-box%2520adversarial%2520attacks%2520settings.%2520Our%2520findings%2520confirm%250Athat%2520tabular%2520data%2520is%2520also%2520susceptible%2520to%2520subtle%2520perturbations%252C%2520highlighting%2520the%250Aneed%2520for%2520heightened%2520awareness%2520among%2520financial%2520technology%2520practitioners%250Aregarding%2520ML%2520model%2520security%2520and%2520trustworthiness.%2520Furthermore%252C%2520the%2520experiments%250Aby%2520transferring%2520adversarial%2520samples%2520from%2520gradient-based%2520attack%2520method%2520to%250Anon-gradient-based%2520models%2520also%2520verify%2520our%2520findings.%2520Our%2520results%2520demonstrate%250Athat%2520such%2520attacks%2520remain%2520effective%252C%2520emphasizing%2520the%2520necessity%2520of%2520developing%250Arobust%2520defenses%2520for%2520CCFD%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foe%20for%20Fraud%3A%20Transferable%20Adversarial%20Attacks%20in%20Credit%20Card%20Fraud%0A%20%20Detection&entry.906535625=Jan%20Lum%20Fok%20and%20Qingwen%20Zeng%20and%20Shiping%20Chen%20and%20Oscar%20Fawkes%20and%20Huaming%20Chen&entry.1292438233=%20%20Credit%20card%20fraud%20detection%20%28CCFD%29%20is%20a%20critical%20application%20of%20Machine%0ALearning%20%28ML%29%20in%20the%20financial%20sector%2C%20where%20accurately%20identifying%20fraudulent%0Atransactions%20is%20essential%20for%20mitigating%20financial%20losses.%20ML%20models%20have%0Ademonstrated%20their%20effectiveness%20in%20fraud%20detection%20task%2C%20in%20particular%20with%0Athe%20tabular%20dataset.%20While%20adversarial%20attacks%20have%20been%20extensively%20studied%20in%0Acomputer%20vision%20and%20deep%20learning%2C%20their%20impacts%20on%20the%20ML%20models%2C%20particularly%0Athose%20trained%20on%20CCFD%20tabular%20datasets%2C%20remains%20largely%20unexplored.%20These%0Alatent%20vulnerabilities%20pose%20significant%20threats%20to%20the%20security%20and%20stability%0Aof%20the%20financial%20industry%2C%20especially%20in%20high-value%20transactions%20where%20losses%0Acould%20be%20substantial.%20To%20address%20this%20gap%2C%20in%20this%20paper%2C%20we%20present%20a%20holistic%0Aframework%20that%20investigate%20the%20robustness%20of%20CCFD%20ML%20model%20against%20adversarial%0Aperturbations%20under%20different%20circumstances.%20Specifically%2C%20the%20gradient-based%0Aattack%20methods%20are%20incorporated%20into%20the%20tabular%20credit%20card%20transaction%20data%0Ain%20both%20black-%20and%20white-box%20adversarial%20attacks%20settings.%20Our%20findings%20confirm%0Athat%20tabular%20data%20is%20also%20susceptible%20to%20subtle%20perturbations%2C%20highlighting%20the%0Aneed%20for%20heightened%20awareness%20among%20financial%20technology%20practitioners%0Aregarding%20ML%20model%20security%20and%20trustworthiness.%20Furthermore%2C%20the%20experiments%0Aby%20transferring%20adversarial%20samples%20from%20gradient-based%20attack%20method%20to%0Anon-gradient-based%20models%20also%20verify%20our%20findings.%20Our%20results%20demonstrate%0Athat%20such%20attacks%20remain%20effective%2C%20emphasizing%20the%20necessity%20of%20developing%0Arobust%20defenses%20for%20CCFD%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14699v1&entry.124074799=Read"},
{"title": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent\n  Framework with LLM and Knowledge Graph Integration", "author": "Peilin Ji and Xiao Xue and Simeng Wang and Wenhao Yan", "abstract": "  In recent years, the increasing frequency of extreme urban rainfall events\nhas posed significant challenges to emergency scheduling systems. Urban\nflooding often leads to severe traffic congestion and service disruptions,\nthreatening public safety and mobility. However, effective decision making\nremains hindered by three key challenges: (1) managing trade-offs among\ncompeting goals (e.g., traffic flow, task completion, and risk mitigation)\nrequires dynamic, context-aware strategies; (2) rapidly evolving environmental\nconditions render static rules inadequate; and (3) LLM-generated strategies\nfrequently suffer from semantic instability and execution inconsistency.\nExisting methods fail to align perception, global optimization, and multi-agent\ncoordination within a unified framework. To tackle these challenges, we\nintroduce H-J, a hierarchical multi-agent framework that integrates\nknowledge-guided prompting, entropy-constrained generation, and feedback-driven\noptimization. The framework establishes a closed-loop pipeline spanning from\nmulti-source perception to strategic execution and continuous refinement. We\nevaluate H-J on real-world urban topology and rainfall data under three\nrepresentative conditions: extreme rainfall, intermittent bursts, and daily\nlight rain. Experiments show that H-J outperforms rule-based and\nreinforcement-learning baselines in traffic smoothness, task success rate, and\nsystem robustness. These findings highlight the promise of uncertainty-aware,\nknowledge-constrained LLM-based approaches for enhancing resilience in urban\nflood response.\n", "link": "http://arxiv.org/abs/2508.14654v1", "date": "2025-08-20", "relevancy": 1.5286, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5092}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Constrained%20Strategy%20Optimization%20in%20Urban%20Floods%3A%20A%20Multi-Agent%0A%20%20Framework%20with%20LLM%20and%20Knowledge%20Graph%20Integration&body=Title%3A%20Entropy-Constrained%20Strategy%20Optimization%20in%20Urban%20Floods%3A%20A%20Multi-Agent%0A%20%20Framework%20with%20LLM%20and%20Knowledge%20Graph%20Integration%0AAuthor%3A%20Peilin%20Ji%20and%20Xiao%20Xue%20and%20Simeng%20Wang%20and%20Wenhao%20Yan%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20increasing%20frequency%20of%20extreme%20urban%20rainfall%20events%0Ahas%20posed%20significant%20challenges%20to%20emergency%20scheduling%20systems.%20Urban%0Aflooding%20often%20leads%20to%20severe%20traffic%20congestion%20and%20service%20disruptions%2C%0Athreatening%20public%20safety%20and%20mobility.%20However%2C%20effective%20decision%20making%0Aremains%20hindered%20by%20three%20key%20challenges%3A%20%281%29%20managing%20trade-offs%20among%0Acompeting%20goals%20%28e.g.%2C%20traffic%20flow%2C%20task%20completion%2C%20and%20risk%20mitigation%29%0Arequires%20dynamic%2C%20context-aware%20strategies%3B%20%282%29%20rapidly%20evolving%20environmental%0Aconditions%20render%20static%20rules%20inadequate%3B%20and%20%283%29%20LLM-generated%20strategies%0Afrequently%20suffer%20from%20semantic%20instability%20and%20execution%20inconsistency.%0AExisting%20methods%20fail%20to%20align%20perception%2C%20global%20optimization%2C%20and%20multi-agent%0Acoordination%20within%20a%20unified%20framework.%20To%20tackle%20these%20challenges%2C%20we%0Aintroduce%20H-J%2C%20a%20hierarchical%20multi-agent%20framework%20that%20integrates%0Aknowledge-guided%20prompting%2C%20entropy-constrained%20generation%2C%20and%20feedback-driven%0Aoptimization.%20The%20framework%20establishes%20a%20closed-loop%20pipeline%20spanning%20from%0Amulti-source%20perception%20to%20strategic%20execution%20and%20continuous%20refinement.%20We%0Aevaluate%20H-J%20on%20real-world%20urban%20topology%20and%20rainfall%20data%20under%20three%0Arepresentative%20conditions%3A%20extreme%20rainfall%2C%20intermittent%20bursts%2C%20and%20daily%0Alight%20rain.%20Experiments%20show%20that%20H-J%20outperforms%20rule-based%20and%0Areinforcement-learning%20baselines%20in%20traffic%20smoothness%2C%20task%20success%20rate%2C%20and%0Asystem%20robustness.%20These%20findings%20highlight%20the%20promise%20of%20uncertainty-aware%2C%0Aknowledge-constrained%20LLM-based%20approaches%20for%20enhancing%20resilience%20in%20urban%0Aflood%20response.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Constrained%2520Strategy%2520Optimization%2520in%2520Urban%2520Floods%253A%2520A%2520Multi-Agent%250A%2520%2520Framework%2520with%2520LLM%2520and%2520Knowledge%2520Graph%2520Integration%26entry.906535625%3DPeilin%2520Ji%2520and%2520Xiao%2520Xue%2520and%2520Simeng%2520Wang%2520and%2520Wenhao%2520Yan%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520increasing%2520frequency%2520of%2520extreme%2520urban%2520rainfall%2520events%250Ahas%2520posed%2520significant%2520challenges%2520to%2520emergency%2520scheduling%2520systems.%2520Urban%250Aflooding%2520often%2520leads%2520to%2520severe%2520traffic%2520congestion%2520and%2520service%2520disruptions%252C%250Athreatening%2520public%2520safety%2520and%2520mobility.%2520However%252C%2520effective%2520decision%2520making%250Aremains%2520hindered%2520by%2520three%2520key%2520challenges%253A%2520%25281%2529%2520managing%2520trade-offs%2520among%250Acompeting%2520goals%2520%2528e.g.%252C%2520traffic%2520flow%252C%2520task%2520completion%252C%2520and%2520risk%2520mitigation%2529%250Arequires%2520dynamic%252C%2520context-aware%2520strategies%253B%2520%25282%2529%2520rapidly%2520evolving%2520environmental%250Aconditions%2520render%2520static%2520rules%2520inadequate%253B%2520and%2520%25283%2529%2520LLM-generated%2520strategies%250Afrequently%2520suffer%2520from%2520semantic%2520instability%2520and%2520execution%2520inconsistency.%250AExisting%2520methods%2520fail%2520to%2520align%2520perception%252C%2520global%2520optimization%252C%2520and%2520multi-agent%250Acoordination%2520within%2520a%2520unified%2520framework.%2520To%2520tackle%2520these%2520challenges%252C%2520we%250Aintroduce%2520H-J%252C%2520a%2520hierarchical%2520multi-agent%2520framework%2520that%2520integrates%250Aknowledge-guided%2520prompting%252C%2520entropy-constrained%2520generation%252C%2520and%2520feedback-driven%250Aoptimization.%2520The%2520framework%2520establishes%2520a%2520closed-loop%2520pipeline%2520spanning%2520from%250Amulti-source%2520perception%2520to%2520strategic%2520execution%2520and%2520continuous%2520refinement.%2520We%250Aevaluate%2520H-J%2520on%2520real-world%2520urban%2520topology%2520and%2520rainfall%2520data%2520under%2520three%250Arepresentative%2520conditions%253A%2520extreme%2520rainfall%252C%2520intermittent%2520bursts%252C%2520and%2520daily%250Alight%2520rain.%2520Experiments%2520show%2520that%2520H-J%2520outperforms%2520rule-based%2520and%250Areinforcement-learning%2520baselines%2520in%2520traffic%2520smoothness%252C%2520task%2520success%2520rate%252C%2520and%250Asystem%2520robustness.%2520These%2520findings%2520highlight%2520the%2520promise%2520of%2520uncertainty-aware%252C%250Aknowledge-constrained%2520LLM-based%2520approaches%2520for%2520enhancing%2520resilience%2520in%2520urban%250Aflood%2520response.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Constrained%20Strategy%20Optimization%20in%20Urban%20Floods%3A%20A%20Multi-Agent%0A%20%20Framework%20with%20LLM%20and%20Knowledge%20Graph%20Integration&entry.906535625=Peilin%20Ji%20and%20Xiao%20Xue%20and%20Simeng%20Wang%20and%20Wenhao%20Yan&entry.1292438233=%20%20In%20recent%20years%2C%20the%20increasing%20frequency%20of%20extreme%20urban%20rainfall%20events%0Ahas%20posed%20significant%20challenges%20to%20emergency%20scheduling%20systems.%20Urban%0Aflooding%20often%20leads%20to%20severe%20traffic%20congestion%20and%20service%20disruptions%2C%0Athreatening%20public%20safety%20and%20mobility.%20However%2C%20effective%20decision%20making%0Aremains%20hindered%20by%20three%20key%20challenges%3A%20%281%29%20managing%20trade-offs%20among%0Acompeting%20goals%20%28e.g.%2C%20traffic%20flow%2C%20task%20completion%2C%20and%20risk%20mitigation%29%0Arequires%20dynamic%2C%20context-aware%20strategies%3B%20%282%29%20rapidly%20evolving%20environmental%0Aconditions%20render%20static%20rules%20inadequate%3B%20and%20%283%29%20LLM-generated%20strategies%0Afrequently%20suffer%20from%20semantic%20instability%20and%20execution%20inconsistency.%0AExisting%20methods%20fail%20to%20align%20perception%2C%20global%20optimization%2C%20and%20multi-agent%0Acoordination%20within%20a%20unified%20framework.%20To%20tackle%20these%20challenges%2C%20we%0Aintroduce%20H-J%2C%20a%20hierarchical%20multi-agent%20framework%20that%20integrates%0Aknowledge-guided%20prompting%2C%20entropy-constrained%20generation%2C%20and%20feedback-driven%0Aoptimization.%20The%20framework%20establishes%20a%20closed-loop%20pipeline%20spanning%20from%0Amulti-source%20perception%20to%20strategic%20execution%20and%20continuous%20refinement.%20We%0Aevaluate%20H-J%20on%20real-world%20urban%20topology%20and%20rainfall%20data%20under%20three%0Arepresentative%20conditions%3A%20extreme%20rainfall%2C%20intermittent%20bursts%2C%20and%20daily%0Alight%20rain.%20Experiments%20show%20that%20H-J%20outperforms%20rule-based%20and%0Areinforcement-learning%20baselines%20in%20traffic%20smoothness%2C%20task%20success%20rate%2C%20and%0Asystem%20robustness.%20These%20findings%20highlight%20the%20promise%20of%20uncertainty-aware%2C%0Aknowledge-constrained%20LLM-based%20approaches%20for%20enhancing%20resilience%20in%20urban%0Aflood%20response.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14654v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


