<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240807.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "IG-SLAM: Instant Gaussian SLAM", "author": "F. Aykut Sarikamis and A. Aydin Alatan", "abstract": "  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n", "link": "http://arxiv.org/abs/2408.01126v2", "date": "2024-08-07", "relevancy": 3.4601, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8596}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6353}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IG-SLAM%3A%20Instant%20Gaussian%20SLAM&body=Title%3A%20IG-SLAM%3A%20Instant%20Gaussian%20SLAM%0AAuthor%3A%20F.%20Aykut%20Sarikamis%20and%20A.%20Aydin%20Alatan%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20recently%20shown%20promising%20results%20as%20an%20alternative%0Ascene%20representation%20in%20SLAM%20systems%20to%20neural%20implicit%20representations.%0AHowever%2C%20current%20methods%20either%20lack%20dense%20depth%20maps%20to%20supervise%20the%20mapping%0Aprocess%20or%20detailed%20training%20designs%20that%20consider%20the%20scale%20of%20the%0Aenvironment.%20To%20address%20these%20drawbacks%2C%20we%20present%20IG-SLAM%2C%20a%20dense%20RGB-only%0ASLAM%20system%20that%20employs%20robust%20Dense-SLAM%20methods%20for%20tracking%20and%20combines%0Athem%20with%20Gaussian%20Splatting.%20A%203D%20map%20of%20the%20environment%20is%20constructed%20using%0Aaccurate%20pose%20and%20dense%20depth%20provided%20by%20tracking.%20Additionally%2C%20we%20utilize%0Adepth%20uncertainty%20in%20map%20optimization%20to%20improve%203D%20reconstruction.%20Our%20decay%0Astrategy%20in%20map%20optimization%20enhances%20convergence%20and%20allows%20the%20system%20to%20run%0Aat%2010%20fps%20in%20a%20single%20process.%20We%20demonstrate%20competitive%20performance%20with%0Astate-of-the-art%20RGB-only%20SLAM%20systems%20while%20achieving%20faster%20operation%20speeds.%0AWe%20present%20our%20experiments%20on%20the%20Replica%2C%20TUM-RGBD%2C%20ScanNet%2C%20and%20EuRoC%0Adatasets.%20The%20system%20achieves%20photo-realistic%203D%20reconstruction%20in%20large-scale%0Asequences%2C%20particularly%20in%20the%20EuRoC%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIG-SLAM%253A%2520Instant%2520Gaussian%2520SLAM%26entry.906535625%3DF.%2520Aykut%2520Sarikamis%2520and%2520A.%2520Aydin%2520Alatan%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520recently%2520shown%2520promising%2520results%2520as%2520an%2520alternative%250Ascene%2520representation%2520in%2520SLAM%2520systems%2520to%2520neural%2520implicit%2520representations.%250AHowever%252C%2520current%2520methods%2520either%2520lack%2520dense%2520depth%2520maps%2520to%2520supervise%2520the%2520mapping%250Aprocess%2520or%2520detailed%2520training%2520designs%2520that%2520consider%2520the%2520scale%2520of%2520the%250Aenvironment.%2520To%2520address%2520these%2520drawbacks%252C%2520we%2520present%2520IG-SLAM%252C%2520a%2520dense%2520RGB-only%250ASLAM%2520system%2520that%2520employs%2520robust%2520Dense-SLAM%2520methods%2520for%2520tracking%2520and%2520combines%250Athem%2520with%2520Gaussian%2520Splatting.%2520A%25203D%2520map%2520of%2520the%2520environment%2520is%2520constructed%2520using%250Aaccurate%2520pose%2520and%2520dense%2520depth%2520provided%2520by%2520tracking.%2520Additionally%252C%2520we%2520utilize%250Adepth%2520uncertainty%2520in%2520map%2520optimization%2520to%2520improve%25203D%2520reconstruction.%2520Our%2520decay%250Astrategy%2520in%2520map%2520optimization%2520enhances%2520convergence%2520and%2520allows%2520the%2520system%2520to%2520run%250Aat%252010%2520fps%2520in%2520a%2520single%2520process.%2520We%2520demonstrate%2520competitive%2520performance%2520with%250Astate-of-the-art%2520RGB-only%2520SLAM%2520systems%2520while%2520achieving%2520faster%2520operation%2520speeds.%250AWe%2520present%2520our%2520experiments%2520on%2520the%2520Replica%252C%2520TUM-RGBD%252C%2520ScanNet%252C%2520and%2520EuRoC%250Adatasets.%2520The%2520system%2520achieves%2520photo-realistic%25203D%2520reconstruction%2520in%2520large-scale%250Asequences%252C%2520particularly%2520in%2520the%2520EuRoC%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IG-SLAM%3A%20Instant%20Gaussian%20SLAM&entry.906535625=F.%20Aykut%20Sarikamis%20and%20A.%20Aydin%20Alatan&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20recently%20shown%20promising%20results%20as%20an%20alternative%0Ascene%20representation%20in%20SLAM%20systems%20to%20neural%20implicit%20representations.%0AHowever%2C%20current%20methods%20either%20lack%20dense%20depth%20maps%20to%20supervise%20the%20mapping%0Aprocess%20or%20detailed%20training%20designs%20that%20consider%20the%20scale%20of%20the%0Aenvironment.%20To%20address%20these%20drawbacks%2C%20we%20present%20IG-SLAM%2C%20a%20dense%20RGB-only%0ASLAM%20system%20that%20employs%20robust%20Dense-SLAM%20methods%20for%20tracking%20and%20combines%0Athem%20with%20Gaussian%20Splatting.%20A%203D%20map%20of%20the%20environment%20is%20constructed%20using%0Aaccurate%20pose%20and%20dense%20depth%20provided%20by%20tracking.%20Additionally%2C%20we%20utilize%0Adepth%20uncertainty%20in%20map%20optimization%20to%20improve%203D%20reconstruction.%20Our%20decay%0Astrategy%20in%20map%20optimization%20enhances%20convergence%20and%20allows%20the%20system%20to%20run%0Aat%2010%20fps%20in%20a%20single%20process.%20We%20demonstrate%20competitive%20performance%20with%0Astate-of-the-art%20RGB-only%20SLAM%20systems%20while%20achieving%20faster%20operation%20speeds.%0AWe%20present%20our%20experiments%20on%20the%20Replica%2C%20TUM-RGBD%2C%20ScanNet%2C%20and%20EuRoC%0Adatasets.%20The%20system%20achieves%20photo-realistic%203D%20reconstruction%20in%20large-scale%0Asequences%2C%20particularly%20in%20the%20EuRoC%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01126v2&entry.124074799=Read"},
{"title": "Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields", "author": "Joo Chan Lee and Daniel Rho and Xiangyu Sun and Jong Hwan Ko and Eunbyung Park", "abstract": "  3D Gaussian splatting (3DGS) has recently emerged as an alternative\nrepresentation that leverages a 3D Gaussian-based representation and introduces\nan approximated volumetric rendering, achieving very fast rendering speed and\npromising image quality. Furthermore, subsequent studies have successfully\nextended 3DGS to dynamic 3D scenes, demonstrating its wide range of\napplications. However, a significant drawback arises as 3DGS and its following\nmethods entail a substantial number of Gaussians to maintain the high fidelity\nof the rendered images, which requires a large amount of memory and storage. To\naddress this critical issue, we place a specific emphasis on two key\nobjectives: reducing the number of Gaussian points without sacrificing\nperformance and compressing the Gaussian attributes, such as view-dependent\ncolor and covariance. To this end, we propose a learnable mask strategy that\nsignificantly reduces the number of Gaussians while preserving high\nperformance. In addition, we propose a compact but effective representation of\nview-dependent color by employing a grid-based neural field rather than relying\non spherical harmonics. Finally, we learn codebooks to compactly represent the\ngeometric and temporal attributes by residual vector quantization. With model\ncompression techniques such as quantization and entropy coding, we consistently\nshow over 25x reduced storage and enhanced rendering speed compared to 3DGS for\nstatic scenes, while maintaining the quality of the scene representation. For\ndynamic scenes, our approach achieves more than 12x storage efficiency and\nretains a high-quality reconstruction compared to the existing state-of-the-art\nmethods. Our work provides a comprehensive framework for 3D scene\nrepresentation, achieving high performance, fast training, compactness, and\nreal-time rendering. Our project page is available at\nhttps://maincold2.github.io/c3dgs/.\n", "link": "http://arxiv.org/abs/2408.03822v1", "date": "2024-08-07", "relevancy": 3.3722, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7462}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6943}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compact%203D%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Radiance%20Fields&body=Title%3A%20Compact%203D%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Radiance%20Fields%0AAuthor%3A%20Joo%20Chan%20Lee%20and%20Daniel%20Rho%20and%20Xiangyu%20Sun%20and%20Jong%20Hwan%20Ko%20and%20Eunbyung%20Park%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20emerged%20as%20an%20alternative%0Arepresentation%20that%20leverages%20a%203D%20Gaussian-based%20representation%20and%20introduces%0Aan%20approximated%20volumetric%20rendering%2C%20achieving%20very%20fast%20rendering%20speed%20and%0Apromising%20image%20quality.%20Furthermore%2C%20subsequent%20studies%20have%20successfully%0Aextended%203DGS%20to%20dynamic%203D%20scenes%2C%20demonstrating%20its%20wide%20range%20of%0Aapplications.%20However%2C%20a%20significant%20drawback%20arises%20as%203DGS%20and%20its%20following%0Amethods%20entail%20a%20substantial%20number%20of%20Gaussians%20to%20maintain%20the%20high%20fidelity%0Aof%20the%20rendered%20images%2C%20which%20requires%20a%20large%20amount%20of%20memory%20and%20storage.%20To%0Aaddress%20this%20critical%20issue%2C%20we%20place%20a%20specific%20emphasis%20on%20two%20key%0Aobjectives%3A%20reducing%20the%20number%20of%20Gaussian%20points%20without%20sacrificing%0Aperformance%20and%20compressing%20the%20Gaussian%20attributes%2C%20such%20as%20view-dependent%0Acolor%20and%20covariance.%20To%20this%20end%2C%20we%20propose%20a%20learnable%20mask%20strategy%20that%0Asignificantly%20reduces%20the%20number%20of%20Gaussians%20while%20preserving%20high%0Aperformance.%20In%20addition%2C%20we%20propose%20a%20compact%20but%20effective%20representation%20of%0Aview-dependent%20color%20by%20employing%20a%20grid-based%20neural%20field%20rather%20than%20relying%0Aon%20spherical%20harmonics.%20Finally%2C%20we%20learn%20codebooks%20to%20compactly%20represent%20the%0Ageometric%20and%20temporal%20attributes%20by%20residual%20vector%20quantization.%20With%20model%0Acompression%20techniques%20such%20as%20quantization%20and%20entropy%20coding%2C%20we%20consistently%0Ashow%20over%2025x%20reduced%20storage%20and%20enhanced%20rendering%20speed%20compared%20to%203DGS%20for%0Astatic%20scenes%2C%20while%20maintaining%20the%20quality%20of%20the%20scene%20representation.%20For%0Adynamic%20scenes%2C%20our%20approach%20achieves%20more%20than%2012x%20storage%20efficiency%20and%0Aretains%20a%20high-quality%20reconstruction%20compared%20to%20the%20existing%20state-of-the-art%0Amethods.%20Our%20work%20provides%20a%20comprehensive%20framework%20for%203D%20scene%0Arepresentation%2C%20achieving%20high%20performance%2C%20fast%20training%2C%20compactness%2C%20and%0Areal-time%20rendering.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//maincold2.github.io/c3dgs/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompact%25203D%2520Gaussian%2520Splatting%2520for%2520Static%2520and%2520Dynamic%2520Radiance%2520Fields%26entry.906535625%3DJoo%2520Chan%2520Lee%2520and%2520Daniel%2520Rho%2520and%2520Xiangyu%2520Sun%2520and%2520Jong%2520Hwan%2520Ko%2520and%2520Eunbyung%2520Park%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520has%2520recently%2520emerged%2520as%2520an%2520alternative%250Arepresentation%2520that%2520leverages%2520a%25203D%2520Gaussian-based%2520representation%2520and%2520introduces%250Aan%2520approximated%2520volumetric%2520rendering%252C%2520achieving%2520very%2520fast%2520rendering%2520speed%2520and%250Apromising%2520image%2520quality.%2520Furthermore%252C%2520subsequent%2520studies%2520have%2520successfully%250Aextended%25203DGS%2520to%2520dynamic%25203D%2520scenes%252C%2520demonstrating%2520its%2520wide%2520range%2520of%250Aapplications.%2520However%252C%2520a%2520significant%2520drawback%2520arises%2520as%25203DGS%2520and%2520its%2520following%250Amethods%2520entail%2520a%2520substantial%2520number%2520of%2520Gaussians%2520to%2520maintain%2520the%2520high%2520fidelity%250Aof%2520the%2520rendered%2520images%252C%2520which%2520requires%2520a%2520large%2520amount%2520of%2520memory%2520and%2520storage.%2520To%250Aaddress%2520this%2520critical%2520issue%252C%2520we%2520place%2520a%2520specific%2520emphasis%2520on%2520two%2520key%250Aobjectives%253A%2520reducing%2520the%2520number%2520of%2520Gaussian%2520points%2520without%2520sacrificing%250Aperformance%2520and%2520compressing%2520the%2520Gaussian%2520attributes%252C%2520such%2520as%2520view-dependent%250Acolor%2520and%2520covariance.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520learnable%2520mask%2520strategy%2520that%250Asignificantly%2520reduces%2520the%2520number%2520of%2520Gaussians%2520while%2520preserving%2520high%250Aperformance.%2520In%2520addition%252C%2520we%2520propose%2520a%2520compact%2520but%2520effective%2520representation%2520of%250Aview-dependent%2520color%2520by%2520employing%2520a%2520grid-based%2520neural%2520field%2520rather%2520than%2520relying%250Aon%2520spherical%2520harmonics.%2520Finally%252C%2520we%2520learn%2520codebooks%2520to%2520compactly%2520represent%2520the%250Ageometric%2520and%2520temporal%2520attributes%2520by%2520residual%2520vector%2520quantization.%2520With%2520model%250Acompression%2520techniques%2520such%2520as%2520quantization%2520and%2520entropy%2520coding%252C%2520we%2520consistently%250Ashow%2520over%252025x%2520reduced%2520storage%2520and%2520enhanced%2520rendering%2520speed%2520compared%2520to%25203DGS%2520for%250Astatic%2520scenes%252C%2520while%2520maintaining%2520the%2520quality%2520of%2520the%2520scene%2520representation.%2520For%250Adynamic%2520scenes%252C%2520our%2520approach%2520achieves%2520more%2520than%252012x%2520storage%2520efficiency%2520and%250Aretains%2520a%2520high-quality%2520reconstruction%2520compared%2520to%2520the%2520existing%2520state-of-the-art%250Amethods.%2520Our%2520work%2520provides%2520a%2520comprehensive%2520framework%2520for%25203D%2520scene%250Arepresentation%252C%2520achieving%2520high%2520performance%252C%2520fast%2520training%252C%2520compactness%252C%2520and%250Areal-time%2520rendering.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//maincold2.github.io/c3dgs/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compact%203D%20Gaussian%20Splatting%20for%20Static%20and%20Dynamic%20Radiance%20Fields&entry.906535625=Joo%20Chan%20Lee%20and%20Daniel%20Rho%20and%20Xiangyu%20Sun%20and%20Jong%20Hwan%20Ko%20and%20Eunbyung%20Park&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20emerged%20as%20an%20alternative%0Arepresentation%20that%20leverages%20a%203D%20Gaussian-based%20representation%20and%20introduces%0Aan%20approximated%20volumetric%20rendering%2C%20achieving%20very%20fast%20rendering%20speed%20and%0Apromising%20image%20quality.%20Furthermore%2C%20subsequent%20studies%20have%20successfully%0Aextended%203DGS%20to%20dynamic%203D%20scenes%2C%20demonstrating%20its%20wide%20range%20of%0Aapplications.%20However%2C%20a%20significant%20drawback%20arises%20as%203DGS%20and%20its%20following%0Amethods%20entail%20a%20substantial%20number%20of%20Gaussians%20to%20maintain%20the%20high%20fidelity%0Aof%20the%20rendered%20images%2C%20which%20requires%20a%20large%20amount%20of%20memory%20and%20storage.%20To%0Aaddress%20this%20critical%20issue%2C%20we%20place%20a%20specific%20emphasis%20on%20two%20key%0Aobjectives%3A%20reducing%20the%20number%20of%20Gaussian%20points%20without%20sacrificing%0Aperformance%20and%20compressing%20the%20Gaussian%20attributes%2C%20such%20as%20view-dependent%0Acolor%20and%20covariance.%20To%20this%20end%2C%20we%20propose%20a%20learnable%20mask%20strategy%20that%0Asignificantly%20reduces%20the%20number%20of%20Gaussians%20while%20preserving%20high%0Aperformance.%20In%20addition%2C%20we%20propose%20a%20compact%20but%20effective%20representation%20of%0Aview-dependent%20color%20by%20employing%20a%20grid-based%20neural%20field%20rather%20than%20relying%0Aon%20spherical%20harmonics.%20Finally%2C%20we%20learn%20codebooks%20to%20compactly%20represent%20the%0Ageometric%20and%20temporal%20attributes%20by%20residual%20vector%20quantization.%20With%20model%0Acompression%20techniques%20such%20as%20quantization%20and%20entropy%20coding%2C%20we%20consistently%0Ashow%20over%2025x%20reduced%20storage%20and%20enhanced%20rendering%20speed%20compared%20to%203DGS%20for%0Astatic%20scenes%2C%20while%20maintaining%20the%20quality%20of%20the%20scene%20representation.%20For%0Adynamic%20scenes%2C%20our%20approach%20achieves%20more%20than%2012x%20storage%20efficiency%20and%0Aretains%20a%20high-quality%20reconstruction%20compared%20to%20the%20existing%20state-of-the-art%0Amethods.%20Our%20work%20provides%20a%20comprehensive%20framework%20for%203D%20scene%0Arepresentation%2C%20achieving%20high%20performance%2C%20fast%20training%2C%20compactness%2C%20and%0Areal-time%20rendering.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//maincold2.github.io/c3dgs/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03822v1&entry.124074799=Read"},
{"title": "Towards Real-Time Gaussian Splatting: Accelerating 3DGS through\n  Photometric SLAM", "author": "Yan Song Hu and Dayou Mao and Yuhao Chen and John Zelek", "abstract": "  Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous\nLocalization and Mapping (VSLAM) demonstrate the generation of high-quality\nvolumetric reconstructions from monocular video streams. However, despite these\npromising advancements, current 3DGS integrations have reduced tracking\nperformance and lower operating speeds compared to traditional VSLAM. To\naddress these issues, we propose integrating 3DGS with Direct Sparse Odometry,\na monocular photometric SLAM system. We have done preliminary experiments\nshowing that using Direct Sparse Odometry point cloud outputs, as opposed to\nstandard structure-from-motion methods, significantly shortens the training\ntime needed to achieve high-quality renders. Reducing 3DGS training time\nenables the development of 3DGS-integrated SLAM systems that operate in\nreal-time on mobile hardware. These promising initial findings suggest further\nexploration is warranted in combining traditional VSLAM systems with 3DGS.\n", "link": "http://arxiv.org/abs/2408.03825v1", "date": "2024-08-07", "relevancy": 3.348, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8082}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6569}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Real-Time%20Gaussian%20Splatting%3A%20Accelerating%203DGS%20through%0A%20%20Photometric%20SLAM&body=Title%3A%20Towards%20Real-Time%20Gaussian%20Splatting%3A%20Accelerating%203DGS%20through%0A%20%20Photometric%20SLAM%0AAuthor%3A%20Yan%20Song%20Hu%20and%20Dayou%20Mao%20and%20Yuhao%20Chen%20and%20John%20Zelek%0AAbstract%3A%20%20%20Initial%20applications%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20in%20Visual%20Simultaneous%0ALocalization%20and%20Mapping%20%28VSLAM%29%20demonstrate%20the%20generation%20of%20high-quality%0Avolumetric%20reconstructions%20from%20monocular%20video%20streams.%20However%2C%20despite%20these%0Apromising%20advancements%2C%20current%203DGS%20integrations%20have%20reduced%20tracking%0Aperformance%20and%20lower%20operating%20speeds%20compared%20to%20traditional%20VSLAM.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20integrating%203DGS%20with%20Direct%20Sparse%20Odometry%2C%0Aa%20monocular%20photometric%20SLAM%20system.%20We%20have%20done%20preliminary%20experiments%0Ashowing%20that%20using%20Direct%20Sparse%20Odometry%20point%20cloud%20outputs%2C%20as%20opposed%20to%0Astandard%20structure-from-motion%20methods%2C%20significantly%20shortens%20the%20training%0Atime%20needed%20to%20achieve%20high-quality%20renders.%20Reducing%203DGS%20training%20time%0Aenables%20the%20development%20of%203DGS-integrated%20SLAM%20systems%20that%20operate%20in%0Areal-time%20on%20mobile%20hardware.%20These%20promising%20initial%20findings%20suggest%20further%0Aexploration%20is%20warranted%20in%20combining%20traditional%20VSLAM%20systems%20with%203DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Real-Time%2520Gaussian%2520Splatting%253A%2520Accelerating%25203DGS%2520through%250A%2520%2520Photometric%2520SLAM%26entry.906535625%3DYan%2520Song%2520Hu%2520and%2520Dayou%2520Mao%2520and%2520Yuhao%2520Chen%2520and%2520John%2520Zelek%26entry.1292438233%3D%2520%2520Initial%2520applications%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520in%2520Visual%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528VSLAM%2529%2520demonstrate%2520the%2520generation%2520of%2520high-quality%250Avolumetric%2520reconstructions%2520from%2520monocular%2520video%2520streams.%2520However%252C%2520despite%2520these%250Apromising%2520advancements%252C%2520current%25203DGS%2520integrations%2520have%2520reduced%2520tracking%250Aperformance%2520and%2520lower%2520operating%2520speeds%2520compared%2520to%2520traditional%2520VSLAM.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520integrating%25203DGS%2520with%2520Direct%2520Sparse%2520Odometry%252C%250Aa%2520monocular%2520photometric%2520SLAM%2520system.%2520We%2520have%2520done%2520preliminary%2520experiments%250Ashowing%2520that%2520using%2520Direct%2520Sparse%2520Odometry%2520point%2520cloud%2520outputs%252C%2520as%2520opposed%2520to%250Astandard%2520structure-from-motion%2520methods%252C%2520significantly%2520shortens%2520the%2520training%250Atime%2520needed%2520to%2520achieve%2520high-quality%2520renders.%2520Reducing%25203DGS%2520training%2520time%250Aenables%2520the%2520development%2520of%25203DGS-integrated%2520SLAM%2520systems%2520that%2520operate%2520in%250Areal-time%2520on%2520mobile%2520hardware.%2520These%2520promising%2520initial%2520findings%2520suggest%2520further%250Aexploration%2520is%2520warranted%2520in%2520combining%2520traditional%2520VSLAM%2520systems%2520with%25203DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Real-Time%20Gaussian%20Splatting%3A%20Accelerating%203DGS%20through%0A%20%20Photometric%20SLAM&entry.906535625=Yan%20Song%20Hu%20and%20Dayou%20Mao%20and%20Yuhao%20Chen%20and%20John%20Zelek&entry.1292438233=%20%20Initial%20applications%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20in%20Visual%20Simultaneous%0ALocalization%20and%20Mapping%20%28VSLAM%29%20demonstrate%20the%20generation%20of%20high-quality%0Avolumetric%20reconstructions%20from%20monocular%20video%20streams.%20However%2C%20despite%20these%0Apromising%20advancements%2C%20current%203DGS%20integrations%20have%20reduced%20tracking%0Aperformance%20and%20lower%20operating%20speeds%20compared%20to%20traditional%20VSLAM.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20integrating%203DGS%20with%20Direct%20Sparse%20Odometry%2C%0Aa%20monocular%20photometric%20SLAM%20system.%20We%20have%20done%20preliminary%20experiments%0Ashowing%20that%20using%20Direct%20Sparse%20Odometry%20point%20cloud%20outputs%2C%20as%20opposed%20to%0Astandard%20structure-from-motion%20methods%2C%20significantly%20shortens%20the%20training%0Atime%20needed%20to%20achieve%20high-quality%20renders.%20Reducing%203DGS%20training%20time%0Aenables%20the%20development%20of%203DGS-integrated%20SLAM%20systems%20that%20operate%20in%0Areal-time%20on%20mobile%20hardware.%20These%20promising%20initial%20findings%20suggest%20further%0Aexploration%20is%20warranted%20in%20combining%20traditional%20VSLAM%20systems%20with%203DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03825v1&entry.124074799=Read"},
{"title": "3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting", "author": "Zhe Jun Tang and Tat-Jen Cham", "abstract": "  The use of 3D Gaussians as representation of radiance fields has enabled high\nquality novel view synthesis at real-time rendering speed. However, the choice\nof optimising the outgoing radiance of each Gaussian independently as spherical\nharmonics results in unsatisfactory view dependent effects. In response to\nthese limitations, our work, Factorised Tensorial Illumination for 3D Gaussian\nSplatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering\nquality. Instead of optimising a single outgoing radiance parameter, 3iGS\nenhances 3DGS view-dependent effects by expressing the outgoing radiance as a\nfunction of a local illumination field and Bidirectional Reflectance\nDistribution Function (BRDF) features. We optimise a continuous incident\nillumination field through a Tensorial Factorisation representation, while\nseparately fine-tuning the BRDF features of each 3D Gaussian relative to this\nillumination field. Our methodology significantly enhances the rendering\nquality of specular view-dependent effects of 3DGS, while maintaining rapid\ntraining and rendering speeds.\n", "link": "http://arxiv.org/abs/2408.03753v1", "date": "2024-08-07", "relevancy": 2.9927, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6758}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5745}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203iGS%3A%20Factorised%20Tensorial%20Illumination%20for%203D%20Gaussian%20Splatting&body=Title%3A%203iGS%3A%20Factorised%20Tensorial%20Illumination%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zhe%20Jun%20Tang%20and%20Tat-Jen%20Cham%0AAbstract%3A%20%20%20The%20use%20of%203D%20Gaussians%20as%20representation%20of%20radiance%20fields%20has%20enabled%20high%0Aquality%20novel%20view%20synthesis%20at%20real-time%20rendering%20speed.%20However%2C%20the%20choice%0Aof%20optimising%20the%20outgoing%20radiance%20of%20each%20Gaussian%20independently%20as%20spherical%0Aharmonics%20results%20in%20unsatisfactory%20view%20dependent%20effects.%20In%20response%20to%0Athese%20limitations%2C%20our%20work%2C%20Factorised%20Tensorial%20Illumination%20for%203D%20Gaussian%0ASplatting%2C%20or%203iGS%2C%20improves%20upon%203D%20Gaussian%20Splatting%20%283DGS%29%20rendering%0Aquality.%20Instead%20of%20optimising%20a%20single%20outgoing%20radiance%20parameter%2C%203iGS%0Aenhances%203DGS%20view-dependent%20effects%20by%20expressing%20the%20outgoing%20radiance%20as%20a%0Afunction%20of%20a%20local%20illumination%20field%20and%20Bidirectional%20Reflectance%0ADistribution%20Function%20%28BRDF%29%20features.%20We%20optimise%20a%20continuous%20incident%0Aillumination%20field%20through%20a%20Tensorial%20Factorisation%20representation%2C%20while%0Aseparately%20fine-tuning%20the%20BRDF%20features%20of%20each%203D%20Gaussian%20relative%20to%20this%0Aillumination%20field.%20Our%20methodology%20significantly%20enhances%20the%20rendering%0Aquality%20of%20specular%20view-dependent%20effects%20of%203DGS%2C%20while%20maintaining%20rapid%0Atraining%20and%20rendering%20speeds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3iGS%253A%2520Factorised%2520Tensorial%2520Illumination%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZhe%2520Jun%2520Tang%2520and%2520Tat-Jen%2520Cham%26entry.1292438233%3D%2520%2520The%2520use%2520of%25203D%2520Gaussians%2520as%2520representation%2520of%2520radiance%2520fields%2520has%2520enabled%2520high%250Aquality%2520novel%2520view%2520synthesis%2520at%2520real-time%2520rendering%2520speed.%2520However%252C%2520the%2520choice%250Aof%2520optimising%2520the%2520outgoing%2520radiance%2520of%2520each%2520Gaussian%2520independently%2520as%2520spherical%250Aharmonics%2520results%2520in%2520unsatisfactory%2520view%2520dependent%2520effects.%2520In%2520response%2520to%250Athese%2520limitations%252C%2520our%2520work%252C%2520Factorised%2520Tensorial%2520Illumination%2520for%25203D%2520Gaussian%250ASplatting%252C%2520or%25203iGS%252C%2520improves%2520upon%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520rendering%250Aquality.%2520Instead%2520of%2520optimising%2520a%2520single%2520outgoing%2520radiance%2520parameter%252C%25203iGS%250Aenhances%25203DGS%2520view-dependent%2520effects%2520by%2520expressing%2520the%2520outgoing%2520radiance%2520as%2520a%250Afunction%2520of%2520a%2520local%2520illumination%2520field%2520and%2520Bidirectional%2520Reflectance%250ADistribution%2520Function%2520%2528BRDF%2529%2520features.%2520We%2520optimise%2520a%2520continuous%2520incident%250Aillumination%2520field%2520through%2520a%2520Tensorial%2520Factorisation%2520representation%252C%2520while%250Aseparately%2520fine-tuning%2520the%2520BRDF%2520features%2520of%2520each%25203D%2520Gaussian%2520relative%2520to%2520this%250Aillumination%2520field.%2520Our%2520methodology%2520significantly%2520enhances%2520the%2520rendering%250Aquality%2520of%2520specular%2520view-dependent%2520effects%2520of%25203DGS%252C%2520while%2520maintaining%2520rapid%250Atraining%2520and%2520rendering%2520speeds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3iGS%3A%20Factorised%20Tensorial%20Illumination%20for%203D%20Gaussian%20Splatting&entry.906535625=Zhe%20Jun%20Tang%20and%20Tat-Jen%20Cham&entry.1292438233=%20%20The%20use%20of%203D%20Gaussians%20as%20representation%20of%20radiance%20fields%20has%20enabled%20high%0Aquality%20novel%20view%20synthesis%20at%20real-time%20rendering%20speed.%20However%2C%20the%20choice%0Aof%20optimising%20the%20outgoing%20radiance%20of%20each%20Gaussian%20independently%20as%20spherical%0Aharmonics%20results%20in%20unsatisfactory%20view%20dependent%20effects.%20In%20response%20to%0Athese%20limitations%2C%20our%20work%2C%20Factorised%20Tensorial%20Illumination%20for%203D%20Gaussian%0ASplatting%2C%20or%203iGS%2C%20improves%20upon%203D%20Gaussian%20Splatting%20%283DGS%29%20rendering%0Aquality.%20Instead%20of%20optimising%20a%20single%20outgoing%20radiance%20parameter%2C%203iGS%0Aenhances%203DGS%20view-dependent%20effects%20by%20expressing%20the%20outgoing%20radiance%20as%20a%0Afunction%20of%20a%20local%20illumination%20field%20and%20Bidirectional%20Reflectance%0ADistribution%20Function%20%28BRDF%29%20features.%20We%20optimise%20a%20continuous%20incident%0Aillumination%20field%20through%20a%20Tensorial%20Factorisation%20representation%2C%20while%0Aseparately%20fine-tuning%20the%20BRDF%20features%20of%20each%203D%20Gaussian%20relative%20to%20this%0Aillumination%20field.%20Our%20methodology%20significantly%20enhances%20the%20rendering%0Aquality%20of%20specular%20view-dependent%20effects%20of%203DGS%2C%20while%20maintaining%20rapid%0Atraining%20and%20rendering%20speeds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03753v1&entry.124074799=Read"},
{"title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning", "author": "Emanuele Frascaroli and Aniello Panariello and Pietro Buzzega and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n", "link": "http://arxiv.org/abs/2407.15793v2", "date": "2024-08-07", "relevancy": 2.7956, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5741}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5663}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning&body=Title%3A%20CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning%0AAuthor%3A%20Emanuele%20Frascaroli%20and%20Aniello%20Panariello%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20Transformers%20and%20Vision-Language%20Models%20%28VLMs%29%20such%20as%0ACLIP%2C%20large%20pre-trained%20models%20have%20become%20a%20common%20strategy%20to%20enhance%0Aperformance%20in%20Continual%20Learning%20scenarios.%20This%20led%20to%20the%20development%20of%0Anumerous%20prompting%20strategies%20to%20effectively%20fine-tune%20transformer-based%20models%0Awithout%20succumbing%20to%20catastrophic%20forgetting.%20However%2C%20these%20methods%20struggle%0Ato%20specialize%20the%20model%20on%20domains%20significantly%20deviating%20from%20the%0Apre-training%20and%20preserving%20its%20zero-shot%20capabilities.%20In%20this%20work%2C%20we%0Apropose%20Continual%20Generative%20training%20for%20Incremental%20prompt-Learning%2C%20a%20novel%0Aapproach%20to%20mitigate%20forgetting%20while%20adapting%20a%20VLM%2C%20which%20exploits%20generative%0Areplay%20to%20align%20prompts%20to%20tasks.%20We%20also%20introduce%20a%20new%20metric%20to%20evaluate%0Azero-shot%20capabilities%20within%20CL%20benchmarks.%20Through%20extensive%20experiments%20on%0Adifferent%20domains%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20framework%20in%0Aadapting%20to%20new%20tasks%20while%20improving%20zero-shot%20capabilities.%20Further%20analysis%0Areveals%20that%20our%20approach%20can%20bridge%20the%20gap%20with%20joint%20prompt%20tuning.%20The%0Acodebase%20is%20available%20at%20https%3A//github.com/aimagelab/mammoth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15793v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2520with%2520Generative%2520Latent%2520Replay%253A%2520a%2520Strong%2520Baseline%2520for%2520Incremental%250A%2520%2520Learning%26entry.906535625%3DEmanuele%2520Frascaroli%2520and%2520Aniello%2520Panariello%2520and%2520Pietro%2520Buzzega%2520and%2520Lorenzo%2520Bonicelli%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520Transformers%2520and%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520such%2520as%250ACLIP%252C%2520large%2520pre-trained%2520models%2520have%2520become%2520a%2520common%2520strategy%2520to%2520enhance%250Aperformance%2520in%2520Continual%2520Learning%2520scenarios.%2520This%2520led%2520to%2520the%2520development%2520of%250Anumerous%2520prompting%2520strategies%2520to%2520effectively%2520fine-tune%2520transformer-based%2520models%250Awithout%2520succumbing%2520to%2520catastrophic%2520forgetting.%2520However%252C%2520these%2520methods%2520struggle%250Ato%2520specialize%2520the%2520model%2520on%2520domains%2520significantly%2520deviating%2520from%2520the%250Apre-training%2520and%2520preserving%2520its%2520zero-shot%2520capabilities.%2520In%2520this%2520work%252C%2520we%250Apropose%2520Continual%2520Generative%2520training%2520for%2520Incremental%2520prompt-Learning%252C%2520a%2520novel%250Aapproach%2520to%2520mitigate%2520forgetting%2520while%2520adapting%2520a%2520VLM%252C%2520which%2520exploits%2520generative%250Areplay%2520to%2520align%2520prompts%2520to%2520tasks.%2520We%2520also%2520introduce%2520a%2520new%2520metric%2520to%2520evaluate%250Azero-shot%2520capabilities%2520within%2520CL%2520benchmarks.%2520Through%2520extensive%2520experiments%2520on%250Adifferent%2520domains%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework%2520in%250Aadapting%2520to%2520new%2520tasks%2520while%2520improving%2520zero-shot%2520capabilities.%2520Further%2520analysis%250Areveals%2520that%2520our%2520approach%2520can%2520bridge%2520the%2520gap%2520with%2520joint%2520prompt%2520tuning.%2520The%250Acodebase%2520is%2520available%2520at%2520https%253A//github.com/aimagelab/mammoth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15793v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning&entry.906535625=Emanuele%20Frascaroli%20and%20Aniello%20Panariello%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20With%20the%20emergence%20of%20Transformers%20and%20Vision-Language%20Models%20%28VLMs%29%20such%20as%0ACLIP%2C%20large%20pre-trained%20models%20have%20become%20a%20common%20strategy%20to%20enhance%0Aperformance%20in%20Continual%20Learning%20scenarios.%20This%20led%20to%20the%20development%20of%0Anumerous%20prompting%20strategies%20to%20effectively%20fine-tune%20transformer-based%20models%0Awithout%20succumbing%20to%20catastrophic%20forgetting.%20However%2C%20these%20methods%20struggle%0Ato%20specialize%20the%20model%20on%20domains%20significantly%20deviating%20from%20the%0Apre-training%20and%20preserving%20its%20zero-shot%20capabilities.%20In%20this%20work%2C%20we%0Apropose%20Continual%20Generative%20training%20for%20Incremental%20prompt-Learning%2C%20a%20novel%0Aapproach%20to%20mitigate%20forgetting%20while%20adapting%20a%20VLM%2C%20which%20exploits%20generative%0Areplay%20to%20align%20prompts%20to%20tasks.%20We%20also%20introduce%20a%20new%20metric%20to%20evaluate%0Azero-shot%20capabilities%20within%20CL%20benchmarks.%20Through%20extensive%20experiments%20on%0Adifferent%20domains%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20framework%20in%0Aadapting%20to%20new%20tasks%20while%20improving%20zero-shot%20capabilities.%20Further%20analysis%0Areveals%20that%20our%20approach%20can%20bridge%20the%20gap%20with%20joint%20prompt%20tuning.%20The%0Acodebase%20is%20available%20at%20https%3A//github.com/aimagelab/mammoth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15793v2&entry.124074799=Read"},
{"title": "Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction", "author": "Tengjie Zhu and Zhuo Chen and Jingnan Gao and Yichao Yan and Xiaokang Yang", "abstract": "  Inverse rendering methods have achieved remarkable performance in\nreconstructing high-fidelity 3D objects with disentangled geometries,\nmaterials, and environmental light. However, they still face huge challenges in\nreflective surface reconstruction. Although recent methods model the light\ntrace to learn specularity, the ignorance of indirect illumination makes it\nhard to handle inter-reflections among multiple smooth objects. In this work,\nwe propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which\ncomprehensively computes the environmental illumination and meanwhile considers\nthe reflective light from object surfaces. To address the computation challenge\nas the times of Monte Carlo sampling grow, we propose a specularity-adaptive\nsampling strategy, significantly reducing the computational complexity. Besides\nthe computational resource, higher geometry accuracy is also required because\ngeometric errors accumulate multiple times. Therefore, we further introduce a\nreflection-aware surface model to initialize the geometry and refine it during\ninverse rendering. We construct a challenging dataset containing scenes with\nmultiple objects and inter-reflections. Experiments show that our method\noutperforms other inverse rendering methods on various object groups. We also\nshow downstream applications, e.g., relighting and material editing, to\nillustrate the disentanglement ability of our method.\n", "link": "http://arxiv.org/abs/2407.05771v2", "date": "2024-08-07", "relevancy": 2.7917, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5759}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5759}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-times%20Monte%20Carlo%20Rendering%20for%20Inter-reflection%20Reconstruction&body=Title%3A%20Multi-times%20Monte%20Carlo%20Rendering%20for%20Inter-reflection%20Reconstruction%0AAuthor%3A%20Tengjie%20Zhu%20and%20Zhuo%20Chen%20and%20Jingnan%20Gao%20and%20Yichao%20Yan%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Inverse%20rendering%20methods%20have%20achieved%20remarkable%20performance%20in%0Areconstructing%20high-fidelity%203D%20objects%20with%20disentangled%20geometries%2C%0Amaterials%2C%20and%20environmental%20light.%20However%2C%20they%20still%20face%20huge%20challenges%20in%0Areflective%20surface%20reconstruction.%20Although%20recent%20methods%20model%20the%20light%0Atrace%20to%20learn%20specularity%2C%20the%20ignorance%20of%20indirect%20illumination%20makes%20it%0Ahard%20to%20handle%20inter-reflections%20among%20multiple%20smooth%20objects.%20In%20this%20work%2C%0Awe%20propose%20Ref-MC2%20that%20introduces%20the%20multi-time%20Monte%20Carlo%20sampling%20which%0Acomprehensively%20computes%20the%20environmental%20illumination%20and%20meanwhile%20considers%0Athe%20reflective%20light%20from%20object%20surfaces.%20To%20address%20the%20computation%20challenge%0Aas%20the%20times%20of%20Monte%20Carlo%20sampling%20grow%2C%20we%20propose%20a%20specularity-adaptive%0Asampling%20strategy%2C%20significantly%20reducing%20the%20computational%20complexity.%20Besides%0Athe%20computational%20resource%2C%20higher%20geometry%20accuracy%20is%20also%20required%20because%0Ageometric%20errors%20accumulate%20multiple%20times.%20Therefore%2C%20we%20further%20introduce%20a%0Areflection-aware%20surface%20model%20to%20initialize%20the%20geometry%20and%20refine%20it%20during%0Ainverse%20rendering.%20We%20construct%20a%20challenging%20dataset%20containing%20scenes%20with%0Amultiple%20objects%20and%20inter-reflections.%20Experiments%20show%20that%20our%20method%0Aoutperforms%20other%20inverse%20rendering%20methods%20on%20various%20object%20groups.%20We%20also%0Ashow%20downstream%20applications%2C%20e.g.%2C%20relighting%20and%20material%20editing%2C%20to%0Aillustrate%20the%20disentanglement%20ability%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-times%2520Monte%2520Carlo%2520Rendering%2520for%2520Inter-reflection%2520Reconstruction%26entry.906535625%3DTengjie%2520Zhu%2520and%2520Zhuo%2520Chen%2520and%2520Jingnan%2520Gao%2520and%2520Yichao%2520Yan%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Inverse%2520rendering%2520methods%2520have%2520achieved%2520remarkable%2520performance%2520in%250Areconstructing%2520high-fidelity%25203D%2520objects%2520with%2520disentangled%2520geometries%252C%250Amaterials%252C%2520and%2520environmental%2520light.%2520However%252C%2520they%2520still%2520face%2520huge%2520challenges%2520in%250Areflective%2520surface%2520reconstruction.%2520Although%2520recent%2520methods%2520model%2520the%2520light%250Atrace%2520to%2520learn%2520specularity%252C%2520the%2520ignorance%2520of%2520indirect%2520illumination%2520makes%2520it%250Ahard%2520to%2520handle%2520inter-reflections%2520among%2520multiple%2520smooth%2520objects.%2520In%2520this%2520work%252C%250Awe%2520propose%2520Ref-MC2%2520that%2520introduces%2520the%2520multi-time%2520Monte%2520Carlo%2520sampling%2520which%250Acomprehensively%2520computes%2520the%2520environmental%2520illumination%2520and%2520meanwhile%2520considers%250Athe%2520reflective%2520light%2520from%2520object%2520surfaces.%2520To%2520address%2520the%2520computation%2520challenge%250Aas%2520the%2520times%2520of%2520Monte%2520Carlo%2520sampling%2520grow%252C%2520we%2520propose%2520a%2520specularity-adaptive%250Asampling%2520strategy%252C%2520significantly%2520reducing%2520the%2520computational%2520complexity.%2520Besides%250Athe%2520computational%2520resource%252C%2520higher%2520geometry%2520accuracy%2520is%2520also%2520required%2520because%250Ageometric%2520errors%2520accumulate%2520multiple%2520times.%2520Therefore%252C%2520we%2520further%2520introduce%2520a%250Areflection-aware%2520surface%2520model%2520to%2520initialize%2520the%2520geometry%2520and%2520refine%2520it%2520during%250Ainverse%2520rendering.%2520We%2520construct%2520a%2520challenging%2520dataset%2520containing%2520scenes%2520with%250Amultiple%2520objects%2520and%2520inter-reflections.%2520Experiments%2520show%2520that%2520our%2520method%250Aoutperforms%2520other%2520inverse%2520rendering%2520methods%2520on%2520various%2520object%2520groups.%2520We%2520also%250Ashow%2520downstream%2520applications%252C%2520e.g.%252C%2520relighting%2520and%2520material%2520editing%252C%2520to%250Aillustrate%2520the%2520disentanglement%2520ability%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-times%20Monte%20Carlo%20Rendering%20for%20Inter-reflection%20Reconstruction&entry.906535625=Tengjie%20Zhu%20and%20Zhuo%20Chen%20and%20Jingnan%20Gao%20and%20Yichao%20Yan%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Inverse%20rendering%20methods%20have%20achieved%20remarkable%20performance%20in%0Areconstructing%20high-fidelity%203D%20objects%20with%20disentangled%20geometries%2C%0Amaterials%2C%20and%20environmental%20light.%20However%2C%20they%20still%20face%20huge%20challenges%20in%0Areflective%20surface%20reconstruction.%20Although%20recent%20methods%20model%20the%20light%0Atrace%20to%20learn%20specularity%2C%20the%20ignorance%20of%20indirect%20illumination%20makes%20it%0Ahard%20to%20handle%20inter-reflections%20among%20multiple%20smooth%20objects.%20In%20this%20work%2C%0Awe%20propose%20Ref-MC2%20that%20introduces%20the%20multi-time%20Monte%20Carlo%20sampling%20which%0Acomprehensively%20computes%20the%20environmental%20illumination%20and%20meanwhile%20considers%0Athe%20reflective%20light%20from%20object%20surfaces.%20To%20address%20the%20computation%20challenge%0Aas%20the%20times%20of%20Monte%20Carlo%20sampling%20grow%2C%20we%20propose%20a%20specularity-adaptive%0Asampling%20strategy%2C%20significantly%20reducing%20the%20computational%20complexity.%20Besides%0Athe%20computational%20resource%2C%20higher%20geometry%20accuracy%20is%20also%20required%20because%0Ageometric%20errors%20accumulate%20multiple%20times.%20Therefore%2C%20we%20further%20introduce%20a%0Areflection-aware%20surface%20model%20to%20initialize%20the%20geometry%20and%20refine%20it%20during%0Ainverse%20rendering.%20We%20construct%20a%20challenging%20dataset%20containing%20scenes%20with%0Amultiple%20objects%20and%20inter-reflections.%20Experiments%20show%20that%20our%20method%0Aoutperforms%20other%20inverse%20rendering%20methods%20on%20various%20object%20groups.%20We%20also%0Ashow%20downstream%20applications%2C%20e.g.%2C%20relighting%20and%20material%20editing%2C%20to%0Aillustrate%20the%20disentanglement%20ability%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05771v2&entry.124074799=Read"},
{"title": "Intuitionistic Fuzzy Cognitive Maps for Interpretable Image\n  Classification", "author": "Georgia Sovatzidi and Michael D. Vasilakakis and Dimitris K. Iakovidis", "abstract": "  The interpretability of machine learning models is critical, as users may be\nreluctant to rely on their inferences. Intuitionistic FCMs (iFCMs) have been\nproposed as an extension of FCMs offering a natural mechanism to assess the\nquality of their output through the estimation of hesitancy, a concept\nresembling to human hesitation in decision making. To address the challenge of\ninterpretable image classification, this paper introduces a novel framework,\nnamed Interpretable Intuitionistic FCM (I2FCM) which is domain-independent,\nsimple to implement, and can be applied on Convolutional Neural Network (CNN)\nmodels, rendering them interpretable. To the best of our knowledge this is the\nfirst time iFCMs are applied for image classification. Further novel\ncontributions include: a feature extraction process focusing on the most\ninformative image regions; a learning algorithm for data-driven determination\nof the intuitionistic fuzzy interconnections of the iFCM; an inherently\ninterpretable classification approach based on image contents. In the context\nof image classification, hesitancy is considered as a degree of inconfidence\nwith which an image is categorized to a class. The constructed iFCM model\ndistinguishes the most representative image semantics and analyses them\nutilizing cause-and-effect relations. The effectiveness of the introduced\nframework is evaluated on publicly available datasets, and the experimental\nresults confirm that it can provide enhanced classification performance, while\nproviding interpretable inferences.\n", "link": "http://arxiv.org/abs/2408.03745v1", "date": "2024-08-07", "relevancy": 2.7182, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5701}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5514}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intuitionistic%20Fuzzy%20Cognitive%20Maps%20for%20Interpretable%20Image%0A%20%20Classification&body=Title%3A%20Intuitionistic%20Fuzzy%20Cognitive%20Maps%20for%20Interpretable%20Image%0A%20%20Classification%0AAuthor%3A%20Georgia%20Sovatzidi%20and%20Michael%20D.%20Vasilakakis%20and%20Dimitris%20K.%20Iakovidis%0AAbstract%3A%20%20%20The%20interpretability%20of%20machine%20learning%20models%20is%20critical%2C%20as%20users%20may%20be%0Areluctant%20to%20rely%20on%20their%20inferences.%20Intuitionistic%20FCMs%20%28iFCMs%29%20have%20been%0Aproposed%20as%20an%20extension%20of%20FCMs%20offering%20a%20natural%20mechanism%20to%20assess%20the%0Aquality%20of%20their%20output%20through%20the%20estimation%20of%20hesitancy%2C%20a%20concept%0Aresembling%20to%20human%20hesitation%20in%20decision%20making.%20To%20address%20the%20challenge%20of%0Ainterpretable%20image%20classification%2C%20this%20paper%20introduces%20a%20novel%20framework%2C%0Anamed%20Interpretable%20Intuitionistic%20FCM%20%28I2FCM%29%20which%20is%20domain-independent%2C%0Asimple%20to%20implement%2C%20and%20can%20be%20applied%20on%20Convolutional%20Neural%20Network%20%28CNN%29%0Amodels%2C%20rendering%20them%20interpretable.%20To%20the%20best%20of%20our%20knowledge%20this%20is%20the%0Afirst%20time%20iFCMs%20are%20applied%20for%20image%20classification.%20Further%20novel%0Acontributions%20include%3A%20a%20feature%20extraction%20process%20focusing%20on%20the%20most%0Ainformative%20image%20regions%3B%20a%20learning%20algorithm%20for%20data-driven%20determination%0Aof%20the%20intuitionistic%20fuzzy%20interconnections%20of%20the%20iFCM%3B%20an%20inherently%0Ainterpretable%20classification%20approach%20based%20on%20image%20contents.%20In%20the%20context%0Aof%20image%20classification%2C%20hesitancy%20is%20considered%20as%20a%20degree%20of%20inconfidence%0Awith%20which%20an%20image%20is%20categorized%20to%20a%20class.%20The%20constructed%20iFCM%20model%0Adistinguishes%20the%20most%20representative%20image%20semantics%20and%20analyses%20them%0Autilizing%20cause-and-effect%20relations.%20The%20effectiveness%20of%20the%20introduced%0Aframework%20is%20evaluated%20on%20publicly%20available%20datasets%2C%20and%20the%20experimental%0Aresults%20confirm%20that%20it%20can%20provide%20enhanced%20classification%20performance%2C%20while%0Aproviding%20interpretable%20inferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntuitionistic%2520Fuzzy%2520Cognitive%2520Maps%2520for%2520Interpretable%2520Image%250A%2520%2520Classification%26entry.906535625%3DGeorgia%2520Sovatzidi%2520and%2520Michael%2520D.%2520Vasilakakis%2520and%2520Dimitris%2520K.%2520Iakovidis%26entry.1292438233%3D%2520%2520The%2520interpretability%2520of%2520machine%2520learning%2520models%2520is%2520critical%252C%2520as%2520users%2520may%2520be%250Areluctant%2520to%2520rely%2520on%2520their%2520inferences.%2520Intuitionistic%2520FCMs%2520%2528iFCMs%2529%2520have%2520been%250Aproposed%2520as%2520an%2520extension%2520of%2520FCMs%2520offering%2520a%2520natural%2520mechanism%2520to%2520assess%2520the%250Aquality%2520of%2520their%2520output%2520through%2520the%2520estimation%2520of%2520hesitancy%252C%2520a%2520concept%250Aresembling%2520to%2520human%2520hesitation%2520in%2520decision%2520making.%2520To%2520address%2520the%2520challenge%2520of%250Ainterpretable%2520image%2520classification%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520framework%252C%250Anamed%2520Interpretable%2520Intuitionistic%2520FCM%2520%2528I2FCM%2529%2520which%2520is%2520domain-independent%252C%250Asimple%2520to%2520implement%252C%2520and%2520can%2520be%2520applied%2520on%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%250Amodels%252C%2520rendering%2520them%2520interpretable.%2520To%2520the%2520best%2520of%2520our%2520knowledge%2520this%2520is%2520the%250Afirst%2520time%2520iFCMs%2520are%2520applied%2520for%2520image%2520classification.%2520Further%2520novel%250Acontributions%2520include%253A%2520a%2520feature%2520extraction%2520process%2520focusing%2520on%2520the%2520most%250Ainformative%2520image%2520regions%253B%2520a%2520learning%2520algorithm%2520for%2520data-driven%2520determination%250Aof%2520the%2520intuitionistic%2520fuzzy%2520interconnections%2520of%2520the%2520iFCM%253B%2520an%2520inherently%250Ainterpretable%2520classification%2520approach%2520based%2520on%2520image%2520contents.%2520In%2520the%2520context%250Aof%2520image%2520classification%252C%2520hesitancy%2520is%2520considered%2520as%2520a%2520degree%2520of%2520inconfidence%250Awith%2520which%2520an%2520image%2520is%2520categorized%2520to%2520a%2520class.%2520The%2520constructed%2520iFCM%2520model%250Adistinguishes%2520the%2520most%2520representative%2520image%2520semantics%2520and%2520analyses%2520them%250Autilizing%2520cause-and-effect%2520relations.%2520The%2520effectiveness%2520of%2520the%2520introduced%250Aframework%2520is%2520evaluated%2520on%2520publicly%2520available%2520datasets%252C%2520and%2520the%2520experimental%250Aresults%2520confirm%2520that%2520it%2520can%2520provide%2520enhanced%2520classification%2520performance%252C%2520while%250Aproviding%2520interpretable%2520inferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intuitionistic%20Fuzzy%20Cognitive%20Maps%20for%20Interpretable%20Image%0A%20%20Classification&entry.906535625=Georgia%20Sovatzidi%20and%20Michael%20D.%20Vasilakakis%20and%20Dimitris%20K.%20Iakovidis&entry.1292438233=%20%20The%20interpretability%20of%20machine%20learning%20models%20is%20critical%2C%20as%20users%20may%20be%0Areluctant%20to%20rely%20on%20their%20inferences.%20Intuitionistic%20FCMs%20%28iFCMs%29%20have%20been%0Aproposed%20as%20an%20extension%20of%20FCMs%20offering%20a%20natural%20mechanism%20to%20assess%20the%0Aquality%20of%20their%20output%20through%20the%20estimation%20of%20hesitancy%2C%20a%20concept%0Aresembling%20to%20human%20hesitation%20in%20decision%20making.%20To%20address%20the%20challenge%20of%0Ainterpretable%20image%20classification%2C%20this%20paper%20introduces%20a%20novel%20framework%2C%0Anamed%20Interpretable%20Intuitionistic%20FCM%20%28I2FCM%29%20which%20is%20domain-independent%2C%0Asimple%20to%20implement%2C%20and%20can%20be%20applied%20on%20Convolutional%20Neural%20Network%20%28CNN%29%0Amodels%2C%20rendering%20them%20interpretable.%20To%20the%20best%20of%20our%20knowledge%20this%20is%20the%0Afirst%20time%20iFCMs%20are%20applied%20for%20image%20classification.%20Further%20novel%0Acontributions%20include%3A%20a%20feature%20extraction%20process%20focusing%20on%20the%20most%0Ainformative%20image%20regions%3B%20a%20learning%20algorithm%20for%20data-driven%20determination%0Aof%20the%20intuitionistic%20fuzzy%20interconnections%20of%20the%20iFCM%3B%20an%20inherently%0Ainterpretable%20classification%20approach%20based%20on%20image%20contents.%20In%20the%20context%0Aof%20image%20classification%2C%20hesitancy%20is%20considered%20as%20a%20degree%20of%20inconfidence%0Awith%20which%20an%20image%20is%20categorized%20to%20a%20class.%20The%20constructed%20iFCM%20model%0Adistinguishes%20the%20most%20representative%20image%20semantics%20and%20analyses%20them%0Autilizing%20cause-and-effect%20relations.%20The%20effectiveness%20of%20the%20introduced%0Aframework%20is%20evaluated%20on%20publicly%20available%20datasets%2C%20and%20the%20experimental%0Aresults%20confirm%20that%20it%20can%20provide%20enhanced%20classification%20performance%2C%20while%0Aproviding%20interpretable%20inferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03745v1&entry.124074799=Read"},
{"title": "PHOCUS: Physics-Based Deconvolution for Ultrasound Resolution\n  Enhancement", "author": "Felix Duelmer and Walter Simson and Mohammad Farid Azampour and Magdalena Wysocki and Angelos Karlas and Nassir Navab", "abstract": "  Ultrasound is widely used in medical diagnostics allowing for accessible and\npowerful imaging but suffers from resolution limitations due to diffraction and\nthe finite aperture of the imaging system, which restricts diagnostic use. The\nimpulse function of an ultrasound imaging system is called the point spread\nfunction (PSF), which is convolved with the spatial distribution of reflectors\nin the image formation process. Recovering high-resolution reflector\ndistributions by removing image distortions induced by the convolution process\nimproves image clarity and detail. Conventionally, deconvolution techniques\nattempt to rectify the imaging system's dependent PSF, working directly on the\nradio-frequency (RF) data. However, RF data is often not readily accessible.\nTherefore, we introduce a physics-based deconvolution process using a modeled\nPSF, working directly on the more commonly available B-mode images. By\nleveraging Implicit Neural Representations (INRs), we learn a continuous\nmapping from spatial locations to their respective echogenicity values,\neffectively compensating for the discretized image space. Our contribution\nconsists of a novel methodology for retrieving a continuous echogenicity map\ndirectly from a B-mode image through a differentiable physics-based rendering\npipeline for ultrasound resolution enhancement. We qualitatively and\nquantitatively evaluate our approach on synthetic data, demonstrating\nimprovements over traditional methods in metrics such as PSNR and SSIM.\nFurthermore, we show qualitative enhancements on an ultrasound phantom and an\nin-vivo acquisition of a carotid artery.\n", "link": "http://arxiv.org/abs/2408.03657v1", "date": "2024-08-07", "relevancy": 2.5911, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5205}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5171}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PHOCUS%3A%20Physics-Based%20Deconvolution%20for%20Ultrasound%20Resolution%0A%20%20Enhancement&body=Title%3A%20PHOCUS%3A%20Physics-Based%20Deconvolution%20for%20Ultrasound%20Resolution%0A%20%20Enhancement%0AAuthor%3A%20Felix%20Duelmer%20and%20Walter%20Simson%20and%20Mohammad%20Farid%20Azampour%20and%20Magdalena%20Wysocki%20and%20Angelos%20Karlas%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20Ultrasound%20is%20widely%20used%20in%20medical%20diagnostics%20allowing%20for%20accessible%20and%0Apowerful%20imaging%20but%20suffers%20from%20resolution%20limitations%20due%20to%20diffraction%20and%0Athe%20finite%20aperture%20of%20the%20imaging%20system%2C%20which%20restricts%20diagnostic%20use.%20The%0Aimpulse%20function%20of%20an%20ultrasound%20imaging%20system%20is%20called%20the%20point%20spread%0Afunction%20%28PSF%29%2C%20which%20is%20convolved%20with%20the%20spatial%20distribution%20of%20reflectors%0Ain%20the%20image%20formation%20process.%20Recovering%20high-resolution%20reflector%0Adistributions%20by%20removing%20image%20distortions%20induced%20by%20the%20convolution%20process%0Aimproves%20image%20clarity%20and%20detail.%20Conventionally%2C%20deconvolution%20techniques%0Aattempt%20to%20rectify%20the%20imaging%20system%27s%20dependent%20PSF%2C%20working%20directly%20on%20the%0Aradio-frequency%20%28RF%29%20data.%20However%2C%20RF%20data%20is%20often%20not%20readily%20accessible.%0ATherefore%2C%20we%20introduce%20a%20physics-based%20deconvolution%20process%20using%20a%20modeled%0APSF%2C%20working%20directly%20on%20the%20more%20commonly%20available%20B-mode%20images.%20By%0Aleveraging%20Implicit%20Neural%20Representations%20%28INRs%29%2C%20we%20learn%20a%20continuous%0Amapping%20from%20spatial%20locations%20to%20their%20respective%20echogenicity%20values%2C%0Aeffectively%20compensating%20for%20the%20discretized%20image%20space.%20Our%20contribution%0Aconsists%20of%20a%20novel%20methodology%20for%20retrieving%20a%20continuous%20echogenicity%20map%0Adirectly%20from%20a%20B-mode%20image%20through%20a%20differentiable%20physics-based%20rendering%0Apipeline%20for%20ultrasound%20resolution%20enhancement.%20We%20qualitatively%20and%0Aquantitatively%20evaluate%20our%20approach%20on%20synthetic%20data%2C%20demonstrating%0Aimprovements%20over%20traditional%20methods%20in%20metrics%20such%20as%20PSNR%20and%20SSIM.%0AFurthermore%2C%20we%20show%20qualitative%20enhancements%20on%20an%20ultrasound%20phantom%20and%20an%0Ain-vivo%20acquisition%20of%20a%20carotid%20artery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPHOCUS%253A%2520Physics-Based%2520Deconvolution%2520for%2520Ultrasound%2520Resolution%250A%2520%2520Enhancement%26entry.906535625%3DFelix%2520Duelmer%2520and%2520Walter%2520Simson%2520and%2520Mohammad%2520Farid%2520Azampour%2520and%2520Magdalena%2520Wysocki%2520and%2520Angelos%2520Karlas%2520and%2520Nassir%2520Navab%26entry.1292438233%3D%2520%2520Ultrasound%2520is%2520widely%2520used%2520in%2520medical%2520diagnostics%2520allowing%2520for%2520accessible%2520and%250Apowerful%2520imaging%2520but%2520suffers%2520from%2520resolution%2520limitations%2520due%2520to%2520diffraction%2520and%250Athe%2520finite%2520aperture%2520of%2520the%2520imaging%2520system%252C%2520which%2520restricts%2520diagnostic%2520use.%2520The%250Aimpulse%2520function%2520of%2520an%2520ultrasound%2520imaging%2520system%2520is%2520called%2520the%2520point%2520spread%250Afunction%2520%2528PSF%2529%252C%2520which%2520is%2520convolved%2520with%2520the%2520spatial%2520distribution%2520of%2520reflectors%250Ain%2520the%2520image%2520formation%2520process.%2520Recovering%2520high-resolution%2520reflector%250Adistributions%2520by%2520removing%2520image%2520distortions%2520induced%2520by%2520the%2520convolution%2520process%250Aimproves%2520image%2520clarity%2520and%2520detail.%2520Conventionally%252C%2520deconvolution%2520techniques%250Aattempt%2520to%2520rectify%2520the%2520imaging%2520system%2527s%2520dependent%2520PSF%252C%2520working%2520directly%2520on%2520the%250Aradio-frequency%2520%2528RF%2529%2520data.%2520However%252C%2520RF%2520data%2520is%2520often%2520not%2520readily%2520accessible.%250ATherefore%252C%2520we%2520introduce%2520a%2520physics-based%2520deconvolution%2520process%2520using%2520a%2520modeled%250APSF%252C%2520working%2520directly%2520on%2520the%2520more%2520commonly%2520available%2520B-mode%2520images.%2520By%250Aleveraging%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%252C%2520we%2520learn%2520a%2520continuous%250Amapping%2520from%2520spatial%2520locations%2520to%2520their%2520respective%2520echogenicity%2520values%252C%250Aeffectively%2520compensating%2520for%2520the%2520discretized%2520image%2520space.%2520Our%2520contribution%250Aconsists%2520of%2520a%2520novel%2520methodology%2520for%2520retrieving%2520a%2520continuous%2520echogenicity%2520map%250Adirectly%2520from%2520a%2520B-mode%2520image%2520through%2520a%2520differentiable%2520physics-based%2520rendering%250Apipeline%2520for%2520ultrasound%2520resolution%2520enhancement.%2520We%2520qualitatively%2520and%250Aquantitatively%2520evaluate%2520our%2520approach%2520on%2520synthetic%2520data%252C%2520demonstrating%250Aimprovements%2520over%2520traditional%2520methods%2520in%2520metrics%2520such%2520as%2520PSNR%2520and%2520SSIM.%250AFurthermore%252C%2520we%2520show%2520qualitative%2520enhancements%2520on%2520an%2520ultrasound%2520phantom%2520and%2520an%250Ain-vivo%2520acquisition%2520of%2520a%2520carotid%2520artery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PHOCUS%3A%20Physics-Based%20Deconvolution%20for%20Ultrasound%20Resolution%0A%20%20Enhancement&entry.906535625=Felix%20Duelmer%20and%20Walter%20Simson%20and%20Mohammad%20Farid%20Azampour%20and%20Magdalena%20Wysocki%20and%20Angelos%20Karlas%20and%20Nassir%20Navab&entry.1292438233=%20%20Ultrasound%20is%20widely%20used%20in%20medical%20diagnostics%20allowing%20for%20accessible%20and%0Apowerful%20imaging%20but%20suffers%20from%20resolution%20limitations%20due%20to%20diffraction%20and%0Athe%20finite%20aperture%20of%20the%20imaging%20system%2C%20which%20restricts%20diagnostic%20use.%20The%0Aimpulse%20function%20of%20an%20ultrasound%20imaging%20system%20is%20called%20the%20point%20spread%0Afunction%20%28PSF%29%2C%20which%20is%20convolved%20with%20the%20spatial%20distribution%20of%20reflectors%0Ain%20the%20image%20formation%20process.%20Recovering%20high-resolution%20reflector%0Adistributions%20by%20removing%20image%20distortions%20induced%20by%20the%20convolution%20process%0Aimproves%20image%20clarity%20and%20detail.%20Conventionally%2C%20deconvolution%20techniques%0Aattempt%20to%20rectify%20the%20imaging%20system%27s%20dependent%20PSF%2C%20working%20directly%20on%20the%0Aradio-frequency%20%28RF%29%20data.%20However%2C%20RF%20data%20is%20often%20not%20readily%20accessible.%0ATherefore%2C%20we%20introduce%20a%20physics-based%20deconvolution%20process%20using%20a%20modeled%0APSF%2C%20working%20directly%20on%20the%20more%20commonly%20available%20B-mode%20images.%20By%0Aleveraging%20Implicit%20Neural%20Representations%20%28INRs%29%2C%20we%20learn%20a%20continuous%0Amapping%20from%20spatial%20locations%20to%20their%20respective%20echogenicity%20values%2C%0Aeffectively%20compensating%20for%20the%20discretized%20image%20space.%20Our%20contribution%0Aconsists%20of%20a%20novel%20methodology%20for%20retrieving%20a%20continuous%20echogenicity%20map%0Adirectly%20from%20a%20B-mode%20image%20through%20a%20differentiable%20physics-based%20rendering%0Apipeline%20for%20ultrasound%20resolution%20enhancement.%20We%20qualitatively%20and%0Aquantitatively%20evaluate%20our%20approach%20on%20synthetic%20data%2C%20demonstrating%0Aimprovements%20over%20traditional%20methods%20in%20metrics%20such%20as%20PSNR%20and%20SSIM.%0AFurthermore%2C%20we%20show%20qualitative%20enhancements%20on%20an%20ultrasound%20phantom%20and%20an%0Ain-vivo%20acquisition%20of%20a%20carotid%20artery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03657v1&entry.124074799=Read"},
{"title": "MS-Mapping: An Uncertainty-Aware Large-Scale Multi-Session LiDAR Mapping\n  System", "author": "Xiangcheng Hu and Jin Wu and Jianhao Jiao and Binqian Jiang and Wei Zhang and Wenshuo Wang and Ping Tan", "abstract": "  Large-scale multi-session LiDAR mapping is essential for a wide range of\napplications, including surveying, autonomous driving, crowdsourced mapping,\nand multi-agent navigation. However, existing approaches often struggle with\ndata redundancy, robustness, and accuracy in complex environments. To address\nthese challenges, we present MS-Mapping, an novel multi-session LiDAR mapping\nsystem that employs an incremental mapping scheme for robust and accurate map\nassembly in large-scale environments. Our approach introduces three key\ninnovations: 1) A distribution-aware keyframe selection method that captures\nthe subtle contributions of each point cloud frame to the map by analyzing the\nsimilarity of map distributions. This method effectively reduces data\nredundancy and pose graph size, while enhancing graph optimization speed; 2) An\nuncertainty model that automatically performs least-squares adjustments\naccording to the covariance matrix during graph optimization, improving mapping\nprecision, robustness, and flexibility without the need for scene-specific\nparameter tuning. This uncertainty model enables our system to monitor pose\nuncertainty and avoid ill-posed optimizations, thereby increasing adaptability\nto diverse and challenging environments. 3) To ensure fair evaluation, we\nredesign baseline comparisons and the evaluation benchmark. Direct assessment\nof map accuracy demonstrates the superiority of the proposed MS-Mapping\nalgorithm compared to state-of-the-art methods. In addition to employing public\ndatasets such as Urban-Nav, FusionPortable, and Newer College, we conducted\nextensive experiments on such a large \\SI{855}{m}$\\times$\\SI{636}{m} ground\ntruth map, collecting over \\SI{20}{km} of indoor and outdoor data across more\nthan ten sequences...\n", "link": "http://arxiv.org/abs/2408.03723v1", "date": "2024-08-07", "relevancy": 2.5527, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.7029}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5991}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-Mapping%3A%20An%20Uncertainty-Aware%20Large-Scale%20Multi-Session%20LiDAR%20Mapping%0A%20%20System&body=Title%3A%20MS-Mapping%3A%20An%20Uncertainty-Aware%20Large-Scale%20Multi-Session%20LiDAR%20Mapping%0A%20%20System%0AAuthor%3A%20Xiangcheng%20Hu%20and%20Jin%20Wu%20and%20Jianhao%20Jiao%20and%20Binqian%20Jiang%20and%20Wei%20Zhang%20and%20Wenshuo%20Wang%20and%20Ping%20Tan%0AAbstract%3A%20%20%20Large-scale%20multi-session%20LiDAR%20mapping%20is%20essential%20for%20a%20wide%20range%20of%0Aapplications%2C%20including%20surveying%2C%20autonomous%20driving%2C%20crowdsourced%20mapping%2C%0Aand%20multi-agent%20navigation.%20However%2C%20existing%20approaches%20often%20struggle%20with%0Adata%20redundancy%2C%20robustness%2C%20and%20accuracy%20in%20complex%20environments.%20To%20address%0Athese%20challenges%2C%20we%20present%20MS-Mapping%2C%20an%20novel%20multi-session%20LiDAR%20mapping%0Asystem%20that%20employs%20an%20incremental%20mapping%20scheme%20for%20robust%20and%20accurate%20map%0Aassembly%20in%20large-scale%20environments.%20Our%20approach%20introduces%20three%20key%0Ainnovations%3A%201%29%20A%20distribution-aware%20keyframe%20selection%20method%20that%20captures%0Athe%20subtle%20contributions%20of%20each%20point%20cloud%20frame%20to%20the%20map%20by%20analyzing%20the%0Asimilarity%20of%20map%20distributions.%20This%20method%20effectively%20reduces%20data%0Aredundancy%20and%20pose%20graph%20size%2C%20while%20enhancing%20graph%20optimization%20speed%3B%202%29%20An%0Auncertainty%20model%20that%20automatically%20performs%20least-squares%20adjustments%0Aaccording%20to%20the%20covariance%20matrix%20during%20graph%20optimization%2C%20improving%20mapping%0Aprecision%2C%20robustness%2C%20and%20flexibility%20without%20the%20need%20for%20scene-specific%0Aparameter%20tuning.%20This%20uncertainty%20model%20enables%20our%20system%20to%20monitor%20pose%0Auncertainty%20and%20avoid%20ill-posed%20optimizations%2C%20thereby%20increasing%20adaptability%0Ato%20diverse%20and%20challenging%20environments.%203%29%20To%20ensure%20fair%20evaluation%2C%20we%0Aredesign%20baseline%20comparisons%20and%20the%20evaluation%20benchmark.%20Direct%20assessment%0Aof%20map%20accuracy%20demonstrates%20the%20superiority%20of%20the%20proposed%20MS-Mapping%0Aalgorithm%20compared%20to%20state-of-the-art%20methods.%20In%20addition%20to%20employing%20public%0Adatasets%20such%20as%20Urban-Nav%2C%20FusionPortable%2C%20and%20Newer%20College%2C%20we%20conducted%0Aextensive%20experiments%20on%20such%20a%20large%20%5CSI%7B855%7D%7Bm%7D%24%5Ctimes%24%5CSI%7B636%7D%7Bm%7D%20ground%0Atruth%20map%2C%20collecting%20over%20%5CSI%7B20%7D%7Bkm%7D%20of%20indoor%20and%20outdoor%20data%20across%20more%0Athan%20ten%20sequences...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-Mapping%253A%2520An%2520Uncertainty-Aware%2520Large-Scale%2520Multi-Session%2520LiDAR%2520Mapping%250A%2520%2520System%26entry.906535625%3DXiangcheng%2520Hu%2520and%2520Jin%2520Wu%2520and%2520Jianhao%2520Jiao%2520and%2520Binqian%2520Jiang%2520and%2520Wei%2520Zhang%2520and%2520Wenshuo%2520Wang%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520Large-scale%2520multi-session%2520LiDAR%2520mapping%2520is%2520essential%2520for%2520a%2520wide%2520range%2520of%250Aapplications%252C%2520including%2520surveying%252C%2520autonomous%2520driving%252C%2520crowdsourced%2520mapping%252C%250Aand%2520multi-agent%2520navigation.%2520However%252C%2520existing%2520approaches%2520often%2520struggle%2520with%250Adata%2520redundancy%252C%2520robustness%252C%2520and%2520accuracy%2520in%2520complex%2520environments.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520present%2520MS-Mapping%252C%2520an%2520novel%2520multi-session%2520LiDAR%2520mapping%250Asystem%2520that%2520employs%2520an%2520incremental%2520mapping%2520scheme%2520for%2520robust%2520and%2520accurate%2520map%250Aassembly%2520in%2520large-scale%2520environments.%2520Our%2520approach%2520introduces%2520three%2520key%250Ainnovations%253A%25201%2529%2520A%2520distribution-aware%2520keyframe%2520selection%2520method%2520that%2520captures%250Athe%2520subtle%2520contributions%2520of%2520each%2520point%2520cloud%2520frame%2520to%2520the%2520map%2520by%2520analyzing%2520the%250Asimilarity%2520of%2520map%2520distributions.%2520This%2520method%2520effectively%2520reduces%2520data%250Aredundancy%2520and%2520pose%2520graph%2520size%252C%2520while%2520enhancing%2520graph%2520optimization%2520speed%253B%25202%2529%2520An%250Auncertainty%2520model%2520that%2520automatically%2520performs%2520least-squares%2520adjustments%250Aaccording%2520to%2520the%2520covariance%2520matrix%2520during%2520graph%2520optimization%252C%2520improving%2520mapping%250Aprecision%252C%2520robustness%252C%2520and%2520flexibility%2520without%2520the%2520need%2520for%2520scene-specific%250Aparameter%2520tuning.%2520This%2520uncertainty%2520model%2520enables%2520our%2520system%2520to%2520monitor%2520pose%250Auncertainty%2520and%2520avoid%2520ill-posed%2520optimizations%252C%2520thereby%2520increasing%2520adaptability%250Ato%2520diverse%2520and%2520challenging%2520environments.%25203%2529%2520To%2520ensure%2520fair%2520evaluation%252C%2520we%250Aredesign%2520baseline%2520comparisons%2520and%2520the%2520evaluation%2520benchmark.%2520Direct%2520assessment%250Aof%2520map%2520accuracy%2520demonstrates%2520the%2520superiority%2520of%2520the%2520proposed%2520MS-Mapping%250Aalgorithm%2520compared%2520to%2520state-of-the-art%2520methods.%2520In%2520addition%2520to%2520employing%2520public%250Adatasets%2520such%2520as%2520Urban-Nav%252C%2520FusionPortable%252C%2520and%2520Newer%2520College%252C%2520we%2520conducted%250Aextensive%2520experiments%2520on%2520such%2520a%2520large%2520%255CSI%257B855%257D%257Bm%257D%2524%255Ctimes%2524%255CSI%257B636%257D%257Bm%257D%2520ground%250Atruth%2520map%252C%2520collecting%2520over%2520%255CSI%257B20%257D%257Bkm%257D%2520of%2520indoor%2520and%2520outdoor%2520data%2520across%2520more%250Athan%2520ten%2520sequences...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-Mapping%3A%20An%20Uncertainty-Aware%20Large-Scale%20Multi-Session%20LiDAR%20Mapping%0A%20%20System&entry.906535625=Xiangcheng%20Hu%20and%20Jin%20Wu%20and%20Jianhao%20Jiao%20and%20Binqian%20Jiang%20and%20Wei%20Zhang%20and%20Wenshuo%20Wang%20and%20Ping%20Tan&entry.1292438233=%20%20Large-scale%20multi-session%20LiDAR%20mapping%20is%20essential%20for%20a%20wide%20range%20of%0Aapplications%2C%20including%20surveying%2C%20autonomous%20driving%2C%20crowdsourced%20mapping%2C%0Aand%20multi-agent%20navigation.%20However%2C%20existing%20approaches%20often%20struggle%20with%0Adata%20redundancy%2C%20robustness%2C%20and%20accuracy%20in%20complex%20environments.%20To%20address%0Athese%20challenges%2C%20we%20present%20MS-Mapping%2C%20an%20novel%20multi-session%20LiDAR%20mapping%0Asystem%20that%20employs%20an%20incremental%20mapping%20scheme%20for%20robust%20and%20accurate%20map%0Aassembly%20in%20large-scale%20environments.%20Our%20approach%20introduces%20three%20key%0Ainnovations%3A%201%29%20A%20distribution-aware%20keyframe%20selection%20method%20that%20captures%0Athe%20subtle%20contributions%20of%20each%20point%20cloud%20frame%20to%20the%20map%20by%20analyzing%20the%0Asimilarity%20of%20map%20distributions.%20This%20method%20effectively%20reduces%20data%0Aredundancy%20and%20pose%20graph%20size%2C%20while%20enhancing%20graph%20optimization%20speed%3B%202%29%20An%0Auncertainty%20model%20that%20automatically%20performs%20least-squares%20adjustments%0Aaccording%20to%20the%20covariance%20matrix%20during%20graph%20optimization%2C%20improving%20mapping%0Aprecision%2C%20robustness%2C%20and%20flexibility%20without%20the%20need%20for%20scene-specific%0Aparameter%20tuning.%20This%20uncertainty%20model%20enables%20our%20system%20to%20monitor%20pose%0Auncertainty%20and%20avoid%20ill-posed%20optimizations%2C%20thereby%20increasing%20adaptability%0Ato%20diverse%20and%20challenging%20environments.%203%29%20To%20ensure%20fair%20evaluation%2C%20we%0Aredesign%20baseline%20comparisons%20and%20the%20evaluation%20benchmark.%20Direct%20assessment%0Aof%20map%20accuracy%20demonstrates%20the%20superiority%20of%20the%20proposed%20MS-Mapping%0Aalgorithm%20compared%20to%20state-of-the-art%20methods.%20In%20addition%20to%20employing%20public%0Adatasets%20such%20as%20Urban-Nav%2C%20FusionPortable%2C%20and%20Newer%20College%2C%20we%20conducted%0Aextensive%20experiments%20on%20such%20a%20large%20%5CSI%7B855%7D%7Bm%7D%24%5Ctimes%24%5CSI%7B636%7D%7Bm%7D%20ground%0Atruth%20map%2C%20collecting%20over%20%5CSI%7B20%7D%7Bkm%7D%20of%20indoor%20and%20outdoor%20data%20across%20more%0Athan%20ten%20sequences...%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03723v1&entry.124074799=Read"},
{"title": "Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep\n  Graph Neural Networks", "author": "Jie Peng and Runlin Lei and Zhewei Wei", "abstract": "  The drastic performance degradation of Graph Neural Networks (GNNs) as the\ndepth of the graph propagation layers exceeds 8-10 is widely attributed to a\nphenomenon of Over-smoothing. Although recent research suggests that\nOver-smoothing may not be the dominant reason for such a performance\ndegradation, they have not provided rigorous analysis from a theoretical view,\nwhich warrants further investigation. In this paper, we systematically analyze\nthe real dominant problem in deep GNNs and identify the issues that these GNNs\ntowards addressing Over-smoothing essentially work on via empirical experiments\nand theoretical gradient analysis. We theoretically prove that the difficult\ntraining problem of deep MLPs is actually the main challenge, and various\nexisting methods that supposedly tackle Over-smoothing actually improve the\ntrainability of MLPs, which is the main reason for their performance gains. Our\nfurther investigation into trainability issues reveals that properly\nconstrained smaller upper bounds of gradient flow notably enhance the\ntrainability of GNNs. Experimental results on diverse datasets demonstrate\nconsistency between our theoretical findings and empirical evidence. Our\nanalysis provides new insights in constructing deep graph models.\n", "link": "http://arxiv.org/abs/2408.03669v1", "date": "2024-08-07", "relevancy": 2.546, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5269}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.503}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Over-smoothing%3A%20Uncovering%20the%20Trainability%20Challenges%20in%20Deep%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20Beyond%20Over-smoothing%3A%20Uncovering%20the%20Trainability%20Challenges%20in%20Deep%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20Jie%20Peng%20and%20Runlin%20Lei%20and%20Zhewei%20Wei%0AAbstract%3A%20%20%20The%20drastic%20performance%20degradation%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20as%20the%0Adepth%20of%20the%20graph%20propagation%20layers%20exceeds%208-10%20is%20widely%20attributed%20to%20a%0Aphenomenon%20of%20Over-smoothing.%20Although%20recent%20research%20suggests%20that%0AOver-smoothing%20may%20not%20be%20the%20dominant%20reason%20for%20such%20a%20performance%0Adegradation%2C%20they%20have%20not%20provided%20rigorous%20analysis%20from%20a%20theoretical%20view%2C%0Awhich%20warrants%20further%20investigation.%20In%20this%20paper%2C%20we%20systematically%20analyze%0Athe%20real%20dominant%20problem%20in%20deep%20GNNs%20and%20identify%20the%20issues%20that%20these%20GNNs%0Atowards%20addressing%20Over-smoothing%20essentially%20work%20on%20via%20empirical%20experiments%0Aand%20theoretical%20gradient%20analysis.%20We%20theoretically%20prove%20that%20the%20difficult%0Atraining%20problem%20of%20deep%20MLPs%20is%20actually%20the%20main%20challenge%2C%20and%20various%0Aexisting%20methods%20that%20supposedly%20tackle%20Over-smoothing%20actually%20improve%20the%0Atrainability%20of%20MLPs%2C%20which%20is%20the%20main%20reason%20for%20their%20performance%20gains.%20Our%0Afurther%20investigation%20into%20trainability%20issues%20reveals%20that%20properly%0Aconstrained%20smaller%20upper%20bounds%20of%20gradient%20flow%20notably%20enhance%20the%0Atrainability%20of%20GNNs.%20Experimental%20results%20on%20diverse%20datasets%20demonstrate%0Aconsistency%20between%20our%20theoretical%20findings%20and%20empirical%20evidence.%20Our%0Aanalysis%20provides%20new%20insights%20in%20constructing%20deep%20graph%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Over-smoothing%253A%2520Uncovering%2520the%2520Trainability%2520Challenges%2520in%2520Deep%250A%2520%2520Graph%2520Neural%2520Networks%26entry.906535625%3DJie%2520Peng%2520and%2520Runlin%2520Lei%2520and%2520Zhewei%2520Wei%26entry.1292438233%3D%2520%2520The%2520drastic%2520performance%2520degradation%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520as%2520the%250Adepth%2520of%2520the%2520graph%2520propagation%2520layers%2520exceeds%25208-10%2520is%2520widely%2520attributed%2520to%2520a%250Aphenomenon%2520of%2520Over-smoothing.%2520Although%2520recent%2520research%2520suggests%2520that%250AOver-smoothing%2520may%2520not%2520be%2520the%2520dominant%2520reason%2520for%2520such%2520a%2520performance%250Adegradation%252C%2520they%2520have%2520not%2520provided%2520rigorous%2520analysis%2520from%2520a%2520theoretical%2520view%252C%250Awhich%2520warrants%2520further%2520investigation.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520analyze%250Athe%2520real%2520dominant%2520problem%2520in%2520deep%2520GNNs%2520and%2520identify%2520the%2520issues%2520that%2520these%2520GNNs%250Atowards%2520addressing%2520Over-smoothing%2520essentially%2520work%2520on%2520via%2520empirical%2520experiments%250Aand%2520theoretical%2520gradient%2520analysis.%2520We%2520theoretically%2520prove%2520that%2520the%2520difficult%250Atraining%2520problem%2520of%2520deep%2520MLPs%2520is%2520actually%2520the%2520main%2520challenge%252C%2520and%2520various%250Aexisting%2520methods%2520that%2520supposedly%2520tackle%2520Over-smoothing%2520actually%2520improve%2520the%250Atrainability%2520of%2520MLPs%252C%2520which%2520is%2520the%2520main%2520reason%2520for%2520their%2520performance%2520gains.%2520Our%250Afurther%2520investigation%2520into%2520trainability%2520issues%2520reveals%2520that%2520properly%250Aconstrained%2520smaller%2520upper%2520bounds%2520of%2520gradient%2520flow%2520notably%2520enhance%2520the%250Atrainability%2520of%2520GNNs.%2520Experimental%2520results%2520on%2520diverse%2520datasets%2520demonstrate%250Aconsistency%2520between%2520our%2520theoretical%2520findings%2520and%2520empirical%2520evidence.%2520Our%250Aanalysis%2520provides%2520new%2520insights%2520in%2520constructing%2520deep%2520graph%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Over-smoothing%3A%20Uncovering%20the%20Trainability%20Challenges%20in%20Deep%0A%20%20Graph%20Neural%20Networks&entry.906535625=Jie%20Peng%20and%20Runlin%20Lei%20and%20Zhewei%20Wei&entry.1292438233=%20%20The%20drastic%20performance%20degradation%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20as%20the%0Adepth%20of%20the%20graph%20propagation%20layers%20exceeds%208-10%20is%20widely%20attributed%20to%20a%0Aphenomenon%20of%20Over-smoothing.%20Although%20recent%20research%20suggests%20that%0AOver-smoothing%20may%20not%20be%20the%20dominant%20reason%20for%20such%20a%20performance%0Adegradation%2C%20they%20have%20not%20provided%20rigorous%20analysis%20from%20a%20theoretical%20view%2C%0Awhich%20warrants%20further%20investigation.%20In%20this%20paper%2C%20we%20systematically%20analyze%0Athe%20real%20dominant%20problem%20in%20deep%20GNNs%20and%20identify%20the%20issues%20that%20these%20GNNs%0Atowards%20addressing%20Over-smoothing%20essentially%20work%20on%20via%20empirical%20experiments%0Aand%20theoretical%20gradient%20analysis.%20We%20theoretically%20prove%20that%20the%20difficult%0Atraining%20problem%20of%20deep%20MLPs%20is%20actually%20the%20main%20challenge%2C%20and%20various%0Aexisting%20methods%20that%20supposedly%20tackle%20Over-smoothing%20actually%20improve%20the%0Atrainability%20of%20MLPs%2C%20which%20is%20the%20main%20reason%20for%20their%20performance%20gains.%20Our%0Afurther%20investigation%20into%20trainability%20issues%20reveals%20that%20properly%0Aconstrained%20smaller%20upper%20bounds%20of%20gradient%20flow%20notably%20enhance%20the%0Atrainability%20of%20GNNs.%20Experimental%20results%20on%20diverse%20datasets%20demonstrate%0Aconsistency%20between%20our%20theoretical%20findings%20and%20empirical%20evidence.%20Our%0Aanalysis%20provides%20new%20insights%20in%20constructing%20deep%20graph%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03669v1&entry.124074799=Read"},
{"title": "Open-Set Multivariate Time-Series Anomaly Detection", "author": "Thomas Lai and Thi Kieu Khanh Ho and Narges Armanfard", "abstract": "  Numerous methods for time-series anomaly detection (TSAD) have emerged in\nrecent years, most of which are unsupervised and assume that only normal\nsamples are available during the training phase, due to the challenge of\nobtaining abnormal data in real-world scenarios. Still, limited samples of\nabnormal data are often available, albeit they are far from representative of\nall possible anomalies. Supervised methods can be utilized to classify normal\nand seen anomalies, but they tend to overfit to the seen anomalies present\nduring training, hence, they fail to generalize to unseen anomalies. We propose\nthe first algorithm to address the open-set TSAD problem, called Multivariate\nOpen-Set Time-Series Anomaly Detector (MOSAD), that leverages only a few shots\nof labeled anomalies during the training phase in order to achieve superior\nanomaly detection performance compared to both supervised and unsupervised TSAD\nalgorithms. MOSAD is a novel multi-head TSAD framework with a shared\nrepresentation space and specialized heads, including the Generative head, the\nDiscriminative head, and the Anomaly-Aware Contrastive head. The latter\nproduces a superior representation space for anomaly detection compared to\nconventional supervised contrastive learning. Extensive experiments on three\nreal-world datasets establish MOSAD as a new state-of-the-art in the TSAD\nfield.\n", "link": "http://arxiv.org/abs/2310.12294v3", "date": "2024-08-07", "relevancy": 2.5361, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5144}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5126}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Set%20Multivariate%20Time-Series%20Anomaly%20Detection&body=Title%3A%20Open-Set%20Multivariate%20Time-Series%20Anomaly%20Detection%0AAuthor%3A%20Thomas%20Lai%20and%20Thi%20Kieu%20Khanh%20Ho%20and%20Narges%20Armanfard%0AAbstract%3A%20%20%20Numerous%20methods%20for%20time-series%20anomaly%20detection%20%28TSAD%29%20have%20emerged%20in%0Arecent%20years%2C%20most%20of%20which%20are%20unsupervised%20and%20assume%20that%20only%20normal%0Asamples%20are%20available%20during%20the%20training%20phase%2C%20due%20to%20the%20challenge%20of%0Aobtaining%20abnormal%20data%20in%20real-world%20scenarios.%20Still%2C%20limited%20samples%20of%0Aabnormal%20data%20are%20often%20available%2C%20albeit%20they%20are%20far%20from%20representative%20of%0Aall%20possible%20anomalies.%20Supervised%20methods%20can%20be%20utilized%20to%20classify%20normal%0Aand%20seen%20anomalies%2C%20but%20they%20tend%20to%20overfit%20to%20the%20seen%20anomalies%20present%0Aduring%20training%2C%20hence%2C%20they%20fail%20to%20generalize%20to%20unseen%20anomalies.%20We%20propose%0Athe%20first%20algorithm%20to%20address%20the%20open-set%20TSAD%20problem%2C%20called%20Multivariate%0AOpen-Set%20Time-Series%20Anomaly%20Detector%20%28MOSAD%29%2C%20that%20leverages%20only%20a%20few%20shots%0Aof%20labeled%20anomalies%20during%20the%20training%20phase%20in%20order%20to%20achieve%20superior%0Aanomaly%20detection%20performance%20compared%20to%20both%20supervised%20and%20unsupervised%20TSAD%0Aalgorithms.%20MOSAD%20is%20a%20novel%20multi-head%20TSAD%20framework%20with%20a%20shared%0Arepresentation%20space%20and%20specialized%20heads%2C%20including%20the%20Generative%20head%2C%20the%0ADiscriminative%20head%2C%20and%20the%20Anomaly-Aware%20Contrastive%20head.%20The%20latter%0Aproduces%20a%20superior%20representation%20space%20for%20anomaly%20detection%20compared%20to%0Aconventional%20supervised%20contrastive%20learning.%20Extensive%20experiments%20on%20three%0Areal-world%20datasets%20establish%20MOSAD%20as%20a%20new%20state-of-the-art%20in%20the%20TSAD%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12294v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Set%2520Multivariate%2520Time-Series%2520Anomaly%2520Detection%26entry.906535625%3DThomas%2520Lai%2520and%2520Thi%2520Kieu%2520Khanh%2520Ho%2520and%2520Narges%2520Armanfard%26entry.1292438233%3D%2520%2520Numerous%2520methods%2520for%2520time-series%2520anomaly%2520detection%2520%2528TSAD%2529%2520have%2520emerged%2520in%250Arecent%2520years%252C%2520most%2520of%2520which%2520are%2520unsupervised%2520and%2520assume%2520that%2520only%2520normal%250Asamples%2520are%2520available%2520during%2520the%2520training%2520phase%252C%2520due%2520to%2520the%2520challenge%2520of%250Aobtaining%2520abnormal%2520data%2520in%2520real-world%2520scenarios.%2520Still%252C%2520limited%2520samples%2520of%250Aabnormal%2520data%2520are%2520often%2520available%252C%2520albeit%2520they%2520are%2520far%2520from%2520representative%2520of%250Aall%2520possible%2520anomalies.%2520Supervised%2520methods%2520can%2520be%2520utilized%2520to%2520classify%2520normal%250Aand%2520seen%2520anomalies%252C%2520but%2520they%2520tend%2520to%2520overfit%2520to%2520the%2520seen%2520anomalies%2520present%250Aduring%2520training%252C%2520hence%252C%2520they%2520fail%2520to%2520generalize%2520to%2520unseen%2520anomalies.%2520We%2520propose%250Athe%2520first%2520algorithm%2520to%2520address%2520the%2520open-set%2520TSAD%2520problem%252C%2520called%2520Multivariate%250AOpen-Set%2520Time-Series%2520Anomaly%2520Detector%2520%2528MOSAD%2529%252C%2520that%2520leverages%2520only%2520a%2520few%2520shots%250Aof%2520labeled%2520anomalies%2520during%2520the%2520training%2520phase%2520in%2520order%2520to%2520achieve%2520superior%250Aanomaly%2520detection%2520performance%2520compared%2520to%2520both%2520supervised%2520and%2520unsupervised%2520TSAD%250Aalgorithms.%2520MOSAD%2520is%2520a%2520novel%2520multi-head%2520TSAD%2520framework%2520with%2520a%2520shared%250Arepresentation%2520space%2520and%2520specialized%2520heads%252C%2520including%2520the%2520Generative%2520head%252C%2520the%250ADiscriminative%2520head%252C%2520and%2520the%2520Anomaly-Aware%2520Contrastive%2520head.%2520The%2520latter%250Aproduces%2520a%2520superior%2520representation%2520space%2520for%2520anomaly%2520detection%2520compared%2520to%250Aconventional%2520supervised%2520contrastive%2520learning.%2520Extensive%2520experiments%2520on%2520three%250Areal-world%2520datasets%2520establish%2520MOSAD%2520as%2520a%2520new%2520state-of-the-art%2520in%2520the%2520TSAD%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12294v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Set%20Multivariate%20Time-Series%20Anomaly%20Detection&entry.906535625=Thomas%20Lai%20and%20Thi%20Kieu%20Khanh%20Ho%20and%20Narges%20Armanfard&entry.1292438233=%20%20Numerous%20methods%20for%20time-series%20anomaly%20detection%20%28TSAD%29%20have%20emerged%20in%0Arecent%20years%2C%20most%20of%20which%20are%20unsupervised%20and%20assume%20that%20only%20normal%0Asamples%20are%20available%20during%20the%20training%20phase%2C%20due%20to%20the%20challenge%20of%0Aobtaining%20abnormal%20data%20in%20real-world%20scenarios.%20Still%2C%20limited%20samples%20of%0Aabnormal%20data%20are%20often%20available%2C%20albeit%20they%20are%20far%20from%20representative%20of%0Aall%20possible%20anomalies.%20Supervised%20methods%20can%20be%20utilized%20to%20classify%20normal%0Aand%20seen%20anomalies%2C%20but%20they%20tend%20to%20overfit%20to%20the%20seen%20anomalies%20present%0Aduring%20training%2C%20hence%2C%20they%20fail%20to%20generalize%20to%20unseen%20anomalies.%20We%20propose%0Athe%20first%20algorithm%20to%20address%20the%20open-set%20TSAD%20problem%2C%20called%20Multivariate%0AOpen-Set%20Time-Series%20Anomaly%20Detector%20%28MOSAD%29%2C%20that%20leverages%20only%20a%20few%20shots%0Aof%20labeled%20anomalies%20during%20the%20training%20phase%20in%20order%20to%20achieve%20superior%0Aanomaly%20detection%20performance%20compared%20to%20both%20supervised%20and%20unsupervised%20TSAD%0Aalgorithms.%20MOSAD%20is%20a%20novel%20multi-head%20TSAD%20framework%20with%20a%20shared%0Arepresentation%20space%20and%20specialized%20heads%2C%20including%20the%20Generative%20head%2C%20the%0ADiscriminative%20head%2C%20and%20the%20Anomaly-Aware%20Contrastive%20head.%20The%20latter%0Aproduces%20a%20superior%20representation%20space%20for%20anomaly%20detection%20compared%20to%0Aconventional%20supervised%20contrastive%20learning.%20Extensive%20experiments%20on%20three%0Areal-world%20datasets%20establish%20MOSAD%20as%20a%20new%20state-of-the-art%20in%20the%20TSAD%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12294v3&entry.124074799=Read"},
{"title": "Anomalies, Representations, and Self-Supervision", "author": "Barry M. Dillon and Luigi Favaro and Friedrich Feiden and Tanmoy Modak and Tilman Plehn", "abstract": "  We develop a self-supervised method for density-based anomaly detection using\ncontrastive learning, and test it using event-level anomaly data from CMS\nADC2021. The AnomalyCLR technique is data-driven and uses augmentations of the\nbackground data to mimic non-Standard-Model events in a model-agnostic way. It\nuses a permutation-invariant Transformer Encoder architecture to map the\nobjects measured in a collider event to the representation space, where the\ndata augmentations define a representation space which is sensitive to\npotential anomalous features. An AutoEncoder trained on background\nrepresentations then computes anomaly scores for a variety of signals in the\nrepresentation space. With AnomalyCLR we find significant improvements on\nperformance metrics for all signals when compared to the raw data baseline.\n", "link": "http://arxiv.org/abs/2301.04660v2", "date": "2024-08-07", "relevancy": 2.4501, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4952}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4878}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomalies%2C%20Representations%2C%20and%20Self-Supervision&body=Title%3A%20Anomalies%2C%20Representations%2C%20and%20Self-Supervision%0AAuthor%3A%20Barry%20M.%20Dillon%20and%20Luigi%20Favaro%20and%20Friedrich%20Feiden%20and%20Tanmoy%20Modak%20and%20Tilman%20Plehn%0AAbstract%3A%20%20%20We%20develop%20a%20self-supervised%20method%20for%20density-based%20anomaly%20detection%20using%0Acontrastive%20learning%2C%20and%20test%20it%20using%20event-level%20anomaly%20data%20from%20CMS%0AADC2021.%20The%20AnomalyCLR%20technique%20is%20data-driven%20and%20uses%20augmentations%20of%20the%0Abackground%20data%20to%20mimic%20non-Standard-Model%20events%20in%20a%20model-agnostic%20way.%20It%0Auses%20a%20permutation-invariant%20Transformer%20Encoder%20architecture%20to%20map%20the%0Aobjects%20measured%20in%20a%20collider%20event%20to%20the%20representation%20space%2C%20where%20the%0Adata%20augmentations%20define%20a%20representation%20space%20which%20is%20sensitive%20to%0Apotential%20anomalous%20features.%20An%20AutoEncoder%20trained%20on%20background%0Arepresentations%20then%20computes%20anomaly%20scores%20for%20a%20variety%20of%20signals%20in%20the%0Arepresentation%20space.%20With%20AnomalyCLR%20we%20find%20significant%20improvements%20on%0Aperformance%20metrics%20for%20all%20signals%20when%20compared%20to%20the%20raw%20data%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.04660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalies%252C%2520Representations%252C%2520and%2520Self-Supervision%26entry.906535625%3DBarry%2520M.%2520Dillon%2520and%2520Luigi%2520Favaro%2520and%2520Friedrich%2520Feiden%2520and%2520Tanmoy%2520Modak%2520and%2520Tilman%2520Plehn%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520self-supervised%2520method%2520for%2520density-based%2520anomaly%2520detection%2520using%250Acontrastive%2520learning%252C%2520and%2520test%2520it%2520using%2520event-level%2520anomaly%2520data%2520from%2520CMS%250AADC2021.%2520The%2520AnomalyCLR%2520technique%2520is%2520data-driven%2520and%2520uses%2520augmentations%2520of%2520the%250Abackground%2520data%2520to%2520mimic%2520non-Standard-Model%2520events%2520in%2520a%2520model-agnostic%2520way.%2520It%250Auses%2520a%2520permutation-invariant%2520Transformer%2520Encoder%2520architecture%2520to%2520map%2520the%250Aobjects%2520measured%2520in%2520a%2520collider%2520event%2520to%2520the%2520representation%2520space%252C%2520where%2520the%250Adata%2520augmentations%2520define%2520a%2520representation%2520space%2520which%2520is%2520sensitive%2520to%250Apotential%2520anomalous%2520features.%2520An%2520AutoEncoder%2520trained%2520on%2520background%250Arepresentations%2520then%2520computes%2520anomaly%2520scores%2520for%2520a%2520variety%2520of%2520signals%2520in%2520the%250Arepresentation%2520space.%2520With%2520AnomalyCLR%2520we%2520find%2520significant%2520improvements%2520on%250Aperformance%2520metrics%2520for%2520all%2520signals%2520when%2520compared%2520to%2520the%2520raw%2520data%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.04660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomalies%2C%20Representations%2C%20and%20Self-Supervision&entry.906535625=Barry%20M.%20Dillon%20and%20Luigi%20Favaro%20and%20Friedrich%20Feiden%20and%20Tanmoy%20Modak%20and%20Tilman%20Plehn&entry.1292438233=%20%20We%20develop%20a%20self-supervised%20method%20for%20density-based%20anomaly%20detection%20using%0Acontrastive%20learning%2C%20and%20test%20it%20using%20event-level%20anomaly%20data%20from%20CMS%0AADC2021.%20The%20AnomalyCLR%20technique%20is%20data-driven%20and%20uses%20augmentations%20of%20the%0Abackground%20data%20to%20mimic%20non-Standard-Model%20events%20in%20a%20model-agnostic%20way.%20It%0Auses%20a%20permutation-invariant%20Transformer%20Encoder%20architecture%20to%20map%20the%0Aobjects%20measured%20in%20a%20collider%20event%20to%20the%20representation%20space%2C%20where%20the%0Adata%20augmentations%20define%20a%20representation%20space%20which%20is%20sensitive%20to%0Apotential%20anomalous%20features.%20An%20AutoEncoder%20trained%20on%20background%0Arepresentations%20then%20computes%20anomaly%20scores%20for%20a%20variety%20of%20signals%20in%20the%0Arepresentation%20space.%20With%20AnomalyCLR%20we%20find%20significant%20improvements%20on%0Aperformance%20metrics%20for%20all%20signals%20when%20compared%20to%20the%20raw%20data%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.04660v2&entry.124074799=Read"},
{"title": "LiRank: Industrial Large Scale Ranking Models at LinkedIn", "author": "Fedor Borisyuk and Mingzhou Zhou and Qingquan Song and Siyu Zhu and Birjodh Tiwana and Ganesh Parameswaran and Siddharth Dangi and Lars Hertel and Qiang Xiao and Xiaochen Hou and Yunbo Ouyang and Aman Gupta and Sheallika Singh and Dan Liu and Hailing Cheng and Lei Le and Jonathan Hung and Sathiya Keerthi and Ruoyan Wang and Fengyu Zhang and Mohit Kothari and Chen Zhu and Daqi Sun and Yun Dai and Xun Luan and Sirou Zhu and Zhiwei Wang and Neil Daftary and Qianqi Shen and Chengming Jiang and Haichao Wei and Maneesh Varshney and Amol Ghoting and Souvik Ghosh", "abstract": "  We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.\n", "link": "http://arxiv.org/abs/2402.06859v2", "date": "2024-08-07", "relevancy": 2.412, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4866}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4835}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiRank%3A%20Industrial%20Large%20Scale%20Ranking%20Models%20at%20LinkedIn&body=Title%3A%20LiRank%3A%20Industrial%20Large%20Scale%20Ranking%20Models%20at%20LinkedIn%0AAuthor%3A%20Fedor%20Borisyuk%20and%20Mingzhou%20Zhou%20and%20Qingquan%20Song%20and%20Siyu%20Zhu%20and%20Birjodh%20Tiwana%20and%20Ganesh%20Parameswaran%20and%20Siddharth%20Dangi%20and%20Lars%20Hertel%20and%20Qiang%20Xiao%20and%20Xiaochen%20Hou%20and%20Yunbo%20Ouyang%20and%20Aman%20Gupta%20and%20Sheallika%20Singh%20and%20Dan%20Liu%20and%20Hailing%20Cheng%20and%20Lei%20Le%20and%20Jonathan%20Hung%20and%20Sathiya%20Keerthi%20and%20Ruoyan%20Wang%20and%20Fengyu%20Zhang%20and%20Mohit%20Kothari%20and%20Chen%20Zhu%20and%20Daqi%20Sun%20and%20Yun%20Dai%20and%20Xun%20Luan%20and%20Sirou%20Zhu%20and%20Zhiwei%20Wang%20and%20Neil%20Daftary%20and%20Qianqi%20Shen%20and%20Chengming%20Jiang%20and%20Haichao%20Wei%20and%20Maneesh%20Varshney%20and%20Amol%20Ghoting%20and%20Souvik%20Ghosh%0AAbstract%3A%20%20%20We%20present%20LiRank%2C%20a%20large-scale%20ranking%20framework%20at%20LinkedIn%20that%20brings%20to%0Aproduction%20state-of-the-art%20modeling%20architectures%20and%20optimization%20methods.%20We%0Aunveil%20several%20modeling%20improvements%2C%20including%20Residual%20DCN%2C%20which%20adds%0Aattention%20and%20residual%20connections%20to%20the%20famous%20DCNv2%20architecture.%20We%20share%0Ainsights%20into%20combining%20and%20tuning%20SOTA%20architectures%20to%20create%20a%20unified%0Amodel%2C%20including%20Dense%20Gating%2C%20Transformers%20and%20Residual%20DCN.%20We%20also%20propose%0Anovel%20techniques%20for%20calibration%20and%20describe%20how%20we%20productionalized%20deep%0Alearning%20based%20explore/exploit%20methods.%20To%20enable%20effective%2C%20production-grade%0Aserving%20of%20large%20ranking%20models%2C%20we%20detail%20how%20to%20train%20and%20compress%20models%0Ausing%20quantization%20and%20vocabulary%20compression.%20We%20provide%20details%20about%20the%0Adeployment%20setup%20for%20large-scale%20use%20cases%20of%20Feed%20ranking%2C%20Jobs%0ARecommendations%2C%20and%20Ads%20click-through%20rate%20%28CTR%29%20prediction.%20We%20summarize%20our%0Alearnings%20from%20various%20A/B%20tests%20by%20elucidating%20the%20most%20effective%20technical%0Aapproaches.%20These%20ideas%20have%20contributed%20to%20relative%20metrics%20improvements%0Aacross%20the%20board%20at%20LinkedIn%3A%20%2B0.5%25%20member%20sessions%20in%20the%20Feed%2C%20%2B1.76%25%0Aqualified%20job%20applications%20for%20Jobs%20search%20and%20recommendations%2C%20and%20%2B4.3%25%20for%0AAds%20CTR.%20We%20hope%20this%20work%20can%20provide%20practical%20insights%20and%20solutions%20for%0Apractitioners%20interested%20in%20leveraging%20large-scale%20deep%20ranking%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06859v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiRank%253A%2520Industrial%2520Large%2520Scale%2520Ranking%2520Models%2520at%2520LinkedIn%26entry.906535625%3DFedor%2520Borisyuk%2520and%2520Mingzhou%2520Zhou%2520and%2520Qingquan%2520Song%2520and%2520Siyu%2520Zhu%2520and%2520Birjodh%2520Tiwana%2520and%2520Ganesh%2520Parameswaran%2520and%2520Siddharth%2520Dangi%2520and%2520Lars%2520Hertel%2520and%2520Qiang%2520Xiao%2520and%2520Xiaochen%2520Hou%2520and%2520Yunbo%2520Ouyang%2520and%2520Aman%2520Gupta%2520and%2520Sheallika%2520Singh%2520and%2520Dan%2520Liu%2520and%2520Hailing%2520Cheng%2520and%2520Lei%2520Le%2520and%2520Jonathan%2520Hung%2520and%2520Sathiya%2520Keerthi%2520and%2520Ruoyan%2520Wang%2520and%2520Fengyu%2520Zhang%2520and%2520Mohit%2520Kothari%2520and%2520Chen%2520Zhu%2520and%2520Daqi%2520Sun%2520and%2520Yun%2520Dai%2520and%2520Xun%2520Luan%2520and%2520Sirou%2520Zhu%2520and%2520Zhiwei%2520Wang%2520and%2520Neil%2520Daftary%2520and%2520Qianqi%2520Shen%2520and%2520Chengming%2520Jiang%2520and%2520Haichao%2520Wei%2520and%2520Maneesh%2520Varshney%2520and%2520Amol%2520Ghoting%2520and%2520Souvik%2520Ghosh%26entry.1292438233%3D%2520%2520We%2520present%2520LiRank%252C%2520a%2520large-scale%2520ranking%2520framework%2520at%2520LinkedIn%2520that%2520brings%2520to%250Aproduction%2520state-of-the-art%2520modeling%2520architectures%2520and%2520optimization%2520methods.%2520We%250Aunveil%2520several%2520modeling%2520improvements%252C%2520including%2520Residual%2520DCN%252C%2520which%2520adds%250Aattention%2520and%2520residual%2520connections%2520to%2520the%2520famous%2520DCNv2%2520architecture.%2520We%2520share%250Ainsights%2520into%2520combining%2520and%2520tuning%2520SOTA%2520architectures%2520to%2520create%2520a%2520unified%250Amodel%252C%2520including%2520Dense%2520Gating%252C%2520Transformers%2520and%2520Residual%2520DCN.%2520We%2520also%2520propose%250Anovel%2520techniques%2520for%2520calibration%2520and%2520describe%2520how%2520we%2520productionalized%2520deep%250Alearning%2520based%2520explore/exploit%2520methods.%2520To%2520enable%2520effective%252C%2520production-grade%250Aserving%2520of%2520large%2520ranking%2520models%252C%2520we%2520detail%2520how%2520to%2520train%2520and%2520compress%2520models%250Ausing%2520quantization%2520and%2520vocabulary%2520compression.%2520We%2520provide%2520details%2520about%2520the%250Adeployment%2520setup%2520for%2520large-scale%2520use%2520cases%2520of%2520Feed%2520ranking%252C%2520Jobs%250ARecommendations%252C%2520and%2520Ads%2520click-through%2520rate%2520%2528CTR%2529%2520prediction.%2520We%2520summarize%2520our%250Alearnings%2520from%2520various%2520A/B%2520tests%2520by%2520elucidating%2520the%2520most%2520effective%2520technical%250Aapproaches.%2520These%2520ideas%2520have%2520contributed%2520to%2520relative%2520metrics%2520improvements%250Aacross%2520the%2520board%2520at%2520LinkedIn%253A%2520%252B0.5%2525%2520member%2520sessions%2520in%2520the%2520Feed%252C%2520%252B1.76%2525%250Aqualified%2520job%2520applications%2520for%2520Jobs%2520search%2520and%2520recommendations%252C%2520and%2520%252B4.3%2525%2520for%250AAds%2520CTR.%2520We%2520hope%2520this%2520work%2520can%2520provide%2520practical%2520insights%2520and%2520solutions%2520for%250Apractitioners%2520interested%2520in%2520leveraging%2520large-scale%2520deep%2520ranking%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06859v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiRank%3A%20Industrial%20Large%20Scale%20Ranking%20Models%20at%20LinkedIn&entry.906535625=Fedor%20Borisyuk%20and%20Mingzhou%20Zhou%20and%20Qingquan%20Song%20and%20Siyu%20Zhu%20and%20Birjodh%20Tiwana%20and%20Ganesh%20Parameswaran%20and%20Siddharth%20Dangi%20and%20Lars%20Hertel%20and%20Qiang%20Xiao%20and%20Xiaochen%20Hou%20and%20Yunbo%20Ouyang%20and%20Aman%20Gupta%20and%20Sheallika%20Singh%20and%20Dan%20Liu%20and%20Hailing%20Cheng%20and%20Lei%20Le%20and%20Jonathan%20Hung%20and%20Sathiya%20Keerthi%20and%20Ruoyan%20Wang%20and%20Fengyu%20Zhang%20and%20Mohit%20Kothari%20and%20Chen%20Zhu%20and%20Daqi%20Sun%20and%20Yun%20Dai%20and%20Xun%20Luan%20and%20Sirou%20Zhu%20and%20Zhiwei%20Wang%20and%20Neil%20Daftary%20and%20Qianqi%20Shen%20and%20Chengming%20Jiang%20and%20Haichao%20Wei%20and%20Maneesh%20Varshney%20and%20Amol%20Ghoting%20and%20Souvik%20Ghosh&entry.1292438233=%20%20We%20present%20LiRank%2C%20a%20large-scale%20ranking%20framework%20at%20LinkedIn%20that%20brings%20to%0Aproduction%20state-of-the-art%20modeling%20architectures%20and%20optimization%20methods.%20We%0Aunveil%20several%20modeling%20improvements%2C%20including%20Residual%20DCN%2C%20which%20adds%0Aattention%20and%20residual%20connections%20to%20the%20famous%20DCNv2%20architecture.%20We%20share%0Ainsights%20into%20combining%20and%20tuning%20SOTA%20architectures%20to%20create%20a%20unified%0Amodel%2C%20including%20Dense%20Gating%2C%20Transformers%20and%20Residual%20DCN.%20We%20also%20propose%0Anovel%20techniques%20for%20calibration%20and%20describe%20how%20we%20productionalized%20deep%0Alearning%20based%20explore/exploit%20methods.%20To%20enable%20effective%2C%20production-grade%0Aserving%20of%20large%20ranking%20models%2C%20we%20detail%20how%20to%20train%20and%20compress%20models%0Ausing%20quantization%20and%20vocabulary%20compression.%20We%20provide%20details%20about%20the%0Adeployment%20setup%20for%20large-scale%20use%20cases%20of%20Feed%20ranking%2C%20Jobs%0ARecommendations%2C%20and%20Ads%20click-through%20rate%20%28CTR%29%20prediction.%20We%20summarize%20our%0Alearnings%20from%20various%20A/B%20tests%20by%20elucidating%20the%20most%20effective%20technical%0Aapproaches.%20These%20ideas%20have%20contributed%20to%20relative%20metrics%20improvements%0Aacross%20the%20board%20at%20LinkedIn%3A%20%2B0.5%25%20member%20sessions%20in%20the%20Feed%2C%20%2B1.76%25%0Aqualified%20job%20applications%20for%20Jobs%20search%20and%20recommendations%2C%20and%20%2B4.3%25%20for%0AAds%20CTR.%20We%20hope%20this%20work%20can%20provide%20practical%20insights%20and%20solutions%20for%0Apractitioners%20interested%20in%20leveraging%20large-scale%20deep%20ranking%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06859v2&entry.124074799=Read"},
{"title": "Vision-Language Guidance for LiDAR-based Unsupervised 3D Object\n  Detection", "author": "Christian Fruhwirth-Reisinger and Wei Lin and Du\u0161an Mali\u0107 and Horst Bischof and Horst Possegger", "abstract": "  Accurate 3D object detection in LiDAR point clouds is crucial for autonomous\ndriving systems. To achieve state-of-the-art performance, the supervised\ntraining of detectors requires large amounts of human-annotated data, which is\nexpensive to obtain and restricted to predefined object categories. To mitigate\nmanual labeling efforts, recent unsupervised object detection approaches\ngenerate class-agnostic pseudo-labels for moving objects, subsequently serving\nas supervision signal to bootstrap a detector. Despite promising results, these\napproaches do not provide class labels or generalize well to static objects.\nFurthermore, they are mostly restricted to data containing multiple drives from\nthe same scene or images from a precisely calibrated and synchronized camera\nsetup. To overcome these limitations, we propose a vision-language-guided\nunsupervised 3D detection approach that operates exclusively on LiDAR point\nclouds. We transfer CLIP knowledge to classify point clusters of static and\nmoving objects, which we discover by exploiting the inherent spatio-temporal\ninformation of LiDAR point clouds for clustering, tracking, as well as box and\nlabel refinement. Our approach outperforms state-of-the-art unsupervised 3D\nobject detectors on the Waymo Open Dataset ($+23~\\text{AP}_{3D}$) and Argoverse\n2 ($+7.9~\\text{AP}_{3D}$) and provides class labels not solely based on object\nsize assumptions, marking a significant advancement in the field.\n", "link": "http://arxiv.org/abs/2408.03790v1", "date": "2024-08-07", "relevancy": 2.4083, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.617}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6064}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Guidance%20for%20LiDAR-based%20Unsupervised%203D%20Object%0A%20%20Detection&body=Title%3A%20Vision-Language%20Guidance%20for%20LiDAR-based%20Unsupervised%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Christian%20Fruhwirth-Reisinger%20and%20Wei%20Lin%20and%20Du%C5%A1an%20Mali%C4%87%20and%20Horst%20Bischof%20and%20Horst%20Possegger%0AAbstract%3A%20%20%20Accurate%203D%20object%20detection%20in%20LiDAR%20point%20clouds%20is%20crucial%20for%20autonomous%0Adriving%20systems.%20To%20achieve%20state-of-the-art%20performance%2C%20the%20supervised%0Atraining%20of%20detectors%20requires%20large%20amounts%20of%20human-annotated%20data%2C%20which%20is%0Aexpensive%20to%20obtain%20and%20restricted%20to%20predefined%20object%20categories.%20To%20mitigate%0Amanual%20labeling%20efforts%2C%20recent%20unsupervised%20object%20detection%20approaches%0Agenerate%20class-agnostic%20pseudo-labels%20for%20moving%20objects%2C%20subsequently%20serving%0Aas%20supervision%20signal%20to%20bootstrap%20a%20detector.%20Despite%20promising%20results%2C%20these%0Aapproaches%20do%20not%20provide%20class%20labels%20or%20generalize%20well%20to%20static%20objects.%0AFurthermore%2C%20they%20are%20mostly%20restricted%20to%20data%20containing%20multiple%20drives%20from%0Athe%20same%20scene%20or%20images%20from%20a%20precisely%20calibrated%20and%20synchronized%20camera%0Asetup.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20vision-language-guided%0Aunsupervised%203D%20detection%20approach%20that%20operates%20exclusively%20on%20LiDAR%20point%0Aclouds.%20We%20transfer%20CLIP%20knowledge%20to%20classify%20point%20clusters%20of%20static%20and%0Amoving%20objects%2C%20which%20we%20discover%20by%20exploiting%20the%20inherent%20spatio-temporal%0Ainformation%20of%20LiDAR%20point%20clouds%20for%20clustering%2C%20tracking%2C%20as%20well%20as%20box%20and%0Alabel%20refinement.%20Our%20approach%20outperforms%20state-of-the-art%20unsupervised%203D%0Aobject%20detectors%20on%20the%20Waymo%20Open%20Dataset%20%28%24%2B23~%5Ctext%7BAP%7D_%7B3D%7D%24%29%20and%20Argoverse%0A2%20%28%24%2B7.9~%5Ctext%7BAP%7D_%7B3D%7D%24%29%20and%20provides%20class%20labels%20not%20solely%20based%20on%20object%0Asize%20assumptions%2C%20marking%20a%20significant%20advancement%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Guidance%2520for%2520LiDAR-based%2520Unsupervised%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DChristian%2520Fruhwirth-Reisinger%2520and%2520Wei%2520Lin%2520and%2520Du%25C5%25A1an%2520Mali%25C4%2587%2520and%2520Horst%2520Bischof%2520and%2520Horst%2520Possegger%26entry.1292438233%3D%2520%2520Accurate%25203D%2520object%2520detection%2520in%2520LiDAR%2520point%2520clouds%2520is%2520crucial%2520for%2520autonomous%250Adriving%2520systems.%2520To%2520achieve%2520state-of-the-art%2520performance%252C%2520the%2520supervised%250Atraining%2520of%2520detectors%2520requires%2520large%2520amounts%2520of%2520human-annotated%2520data%252C%2520which%2520is%250Aexpensive%2520to%2520obtain%2520and%2520restricted%2520to%2520predefined%2520object%2520categories.%2520To%2520mitigate%250Amanual%2520labeling%2520efforts%252C%2520recent%2520unsupervised%2520object%2520detection%2520approaches%250Agenerate%2520class-agnostic%2520pseudo-labels%2520for%2520moving%2520objects%252C%2520subsequently%2520serving%250Aas%2520supervision%2520signal%2520to%2520bootstrap%2520a%2520detector.%2520Despite%2520promising%2520results%252C%2520these%250Aapproaches%2520do%2520not%2520provide%2520class%2520labels%2520or%2520generalize%2520well%2520to%2520static%2520objects.%250AFurthermore%252C%2520they%2520are%2520mostly%2520restricted%2520to%2520data%2520containing%2520multiple%2520drives%2520from%250Athe%2520same%2520scene%2520or%2520images%2520from%2520a%2520precisely%2520calibrated%2520and%2520synchronized%2520camera%250Asetup.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520vision-language-guided%250Aunsupervised%25203D%2520detection%2520approach%2520that%2520operates%2520exclusively%2520on%2520LiDAR%2520point%250Aclouds.%2520We%2520transfer%2520CLIP%2520knowledge%2520to%2520classify%2520point%2520clusters%2520of%2520static%2520and%250Amoving%2520objects%252C%2520which%2520we%2520discover%2520by%2520exploiting%2520the%2520inherent%2520spatio-temporal%250Ainformation%2520of%2520LiDAR%2520point%2520clouds%2520for%2520clustering%252C%2520tracking%252C%2520as%2520well%2520as%2520box%2520and%250Alabel%2520refinement.%2520Our%2520approach%2520outperforms%2520state-of-the-art%2520unsupervised%25203D%250Aobject%2520detectors%2520on%2520the%2520Waymo%2520Open%2520Dataset%2520%2528%2524%252B23~%255Ctext%257BAP%257D_%257B3D%257D%2524%2529%2520and%2520Argoverse%250A2%2520%2528%2524%252B7.9~%255Ctext%257BAP%257D_%257B3D%257D%2524%2529%2520and%2520provides%2520class%2520labels%2520not%2520solely%2520based%2520on%2520object%250Asize%2520assumptions%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Guidance%20for%20LiDAR-based%20Unsupervised%203D%20Object%0A%20%20Detection&entry.906535625=Christian%20Fruhwirth-Reisinger%20and%20Wei%20Lin%20and%20Du%C5%A1an%20Mali%C4%87%20and%20Horst%20Bischof%20and%20Horst%20Possegger&entry.1292438233=%20%20Accurate%203D%20object%20detection%20in%20LiDAR%20point%20clouds%20is%20crucial%20for%20autonomous%0Adriving%20systems.%20To%20achieve%20state-of-the-art%20performance%2C%20the%20supervised%0Atraining%20of%20detectors%20requires%20large%20amounts%20of%20human-annotated%20data%2C%20which%20is%0Aexpensive%20to%20obtain%20and%20restricted%20to%20predefined%20object%20categories.%20To%20mitigate%0Amanual%20labeling%20efforts%2C%20recent%20unsupervised%20object%20detection%20approaches%0Agenerate%20class-agnostic%20pseudo-labels%20for%20moving%20objects%2C%20subsequently%20serving%0Aas%20supervision%20signal%20to%20bootstrap%20a%20detector.%20Despite%20promising%20results%2C%20these%0Aapproaches%20do%20not%20provide%20class%20labels%20or%20generalize%20well%20to%20static%20objects.%0AFurthermore%2C%20they%20are%20mostly%20restricted%20to%20data%20containing%20multiple%20drives%20from%0Athe%20same%20scene%20or%20images%20from%20a%20precisely%20calibrated%20and%20synchronized%20camera%0Asetup.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20vision-language-guided%0Aunsupervised%203D%20detection%20approach%20that%20operates%20exclusively%20on%20LiDAR%20point%0Aclouds.%20We%20transfer%20CLIP%20knowledge%20to%20classify%20point%20clusters%20of%20static%20and%0Amoving%20objects%2C%20which%20we%20discover%20by%20exploiting%20the%20inherent%20spatio-temporal%0Ainformation%20of%20LiDAR%20point%20clouds%20for%20clustering%2C%20tracking%2C%20as%20well%20as%20box%20and%0Alabel%20refinement.%20Our%20approach%20outperforms%20state-of-the-art%20unsupervised%203D%0Aobject%20detectors%20on%20the%20Waymo%20Open%20Dataset%20%28%24%2B23~%5Ctext%7BAP%7D_%7B3D%7D%24%29%20and%20Argoverse%0A2%20%28%24%2B7.9~%5Ctext%7BAP%7D_%7B3D%7D%24%29%20and%20provides%20class%20labels%20not%20solely%20based%20on%20object%0Asize%20assumptions%2C%20marking%20a%20significant%20advancement%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03790v1&entry.124074799=Read"},
{"title": "BAST: Binaural Audio Spectrogram Transformer for Binaural Sound\n  Localization", "author": "Sheng Kuang and Jie Shi and Kiki van der Heijden and Siamak Mehrkanoon", "abstract": "  Accurate sound localization in a reverberation environment is essential for\nhuman auditory perception. Recently, Convolutional Neural Networks (CNNs) have\nbeen utilized to model the binaural human auditory pathway. However, CNN shows\nbarriers in capturing the global acoustic features. To address this issue, we\npropose a novel end-to-end Binaural Audio Spectrogram Transformer (BAST) model\nto predict the sound azimuth in both anechoic and reverberation environments.\nTwo modes of implementation, i.e. BAST-SP and BAST-NSP corresponding to BAST\nmodel with shared and non-shared parameters respectively, are explored. Our\nmodel with subtraction interaural integration and hybrid loss achieves an\nangular distance of 1.29 degrees and a Mean Square Error of 1e-3 at all\nazimuths, significantly surpassing CNN based model. The exploratory analysis of\nthe BAST's performance on the left-right hemifields and anechoic and\nreverberation environments shows its generalization ability as well as the\nfeasibility of binaural Transformers in sound localization. Furthermore, the\nanalysis of the attention maps is provided to give additional insights on the\ninterpretation of the localization process in a natural reverberant\nenvironment.\n", "link": "http://arxiv.org/abs/2207.03927v2", "date": "2024-08-07", "relevancy": 2.4051, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4844}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4794}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAST%3A%20Binaural%20Audio%20Spectrogram%20Transformer%20for%20Binaural%20Sound%0A%20%20Localization&body=Title%3A%20BAST%3A%20Binaural%20Audio%20Spectrogram%20Transformer%20for%20Binaural%20Sound%0A%20%20Localization%0AAuthor%3A%20Sheng%20Kuang%20and%20Jie%20Shi%20and%20Kiki%20van%20der%20Heijden%20and%20Siamak%20Mehrkanoon%0AAbstract%3A%20%20%20Accurate%20sound%20localization%20in%20a%20reverberation%20environment%20is%20essential%20for%0Ahuman%20auditory%20perception.%20Recently%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%0Abeen%20utilized%20to%20model%20the%20binaural%20human%20auditory%20pathway.%20However%2C%20CNN%20shows%0Abarriers%20in%20capturing%20the%20global%20acoustic%20features.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20novel%20end-to-end%20Binaural%20Audio%20Spectrogram%20Transformer%20%28BAST%29%20model%0Ato%20predict%20the%20sound%20azimuth%20in%20both%20anechoic%20and%20reverberation%20environments.%0ATwo%20modes%20of%20implementation%2C%20i.e.%20BAST-SP%20and%20BAST-NSP%20corresponding%20to%20BAST%0Amodel%20with%20shared%20and%20non-shared%20parameters%20respectively%2C%20are%20explored.%20Our%0Amodel%20with%20subtraction%20interaural%20integration%20and%20hybrid%20loss%20achieves%20an%0Aangular%20distance%20of%201.29%20degrees%20and%20a%20Mean%20Square%20Error%20of%201e-3%20at%20all%0Aazimuths%2C%20significantly%20surpassing%20CNN%20based%20model.%20The%20exploratory%20analysis%20of%0Athe%20BAST%27s%20performance%20on%20the%20left-right%20hemifields%20and%20anechoic%20and%0Areverberation%20environments%20shows%20its%20generalization%20ability%20as%20well%20as%20the%0Afeasibility%20of%20binaural%20Transformers%20in%20sound%20localization.%20Furthermore%2C%20the%0Aanalysis%20of%20the%20attention%20maps%20is%20provided%20to%20give%20additional%20insights%20on%20the%0Ainterpretation%20of%20the%20localization%20process%20in%20a%20natural%20reverberant%0Aenvironment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.03927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAST%253A%2520Binaural%2520Audio%2520Spectrogram%2520Transformer%2520for%2520Binaural%2520Sound%250A%2520%2520Localization%26entry.906535625%3DSheng%2520Kuang%2520and%2520Jie%2520Shi%2520and%2520Kiki%2520van%2520der%2520Heijden%2520and%2520Siamak%2520Mehrkanoon%26entry.1292438233%3D%2520%2520Accurate%2520sound%2520localization%2520in%2520a%2520reverberation%2520environment%2520is%2520essential%2520for%250Ahuman%2520auditory%2520perception.%2520Recently%252C%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%250Abeen%2520utilized%2520to%2520model%2520the%2520binaural%2520human%2520auditory%2520pathway.%2520However%252C%2520CNN%2520shows%250Abarriers%2520in%2520capturing%2520the%2520global%2520acoustic%2520features.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520novel%2520end-to-end%2520Binaural%2520Audio%2520Spectrogram%2520Transformer%2520%2528BAST%2529%2520model%250Ato%2520predict%2520the%2520sound%2520azimuth%2520in%2520both%2520anechoic%2520and%2520reverberation%2520environments.%250ATwo%2520modes%2520of%2520implementation%252C%2520i.e.%2520BAST-SP%2520and%2520BAST-NSP%2520corresponding%2520to%2520BAST%250Amodel%2520with%2520shared%2520and%2520non-shared%2520parameters%2520respectively%252C%2520are%2520explored.%2520Our%250Amodel%2520with%2520subtraction%2520interaural%2520integration%2520and%2520hybrid%2520loss%2520achieves%2520an%250Aangular%2520distance%2520of%25201.29%2520degrees%2520and%2520a%2520Mean%2520Square%2520Error%2520of%25201e-3%2520at%2520all%250Aazimuths%252C%2520significantly%2520surpassing%2520CNN%2520based%2520model.%2520The%2520exploratory%2520analysis%2520of%250Athe%2520BAST%2527s%2520performance%2520on%2520the%2520left-right%2520hemifields%2520and%2520anechoic%2520and%250Areverberation%2520environments%2520shows%2520its%2520generalization%2520ability%2520as%2520well%2520as%2520the%250Afeasibility%2520of%2520binaural%2520Transformers%2520in%2520sound%2520localization.%2520Furthermore%252C%2520the%250Aanalysis%2520of%2520the%2520attention%2520maps%2520is%2520provided%2520to%2520give%2520additional%2520insights%2520on%2520the%250Ainterpretation%2520of%2520the%2520localization%2520process%2520in%2520a%2520natural%2520reverberant%250Aenvironment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.03927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAST%3A%20Binaural%20Audio%20Spectrogram%20Transformer%20for%20Binaural%20Sound%0A%20%20Localization&entry.906535625=Sheng%20Kuang%20and%20Jie%20Shi%20and%20Kiki%20van%20der%20Heijden%20and%20Siamak%20Mehrkanoon&entry.1292438233=%20%20Accurate%20sound%20localization%20in%20a%20reverberation%20environment%20is%20essential%20for%0Ahuman%20auditory%20perception.%20Recently%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%0Abeen%20utilized%20to%20model%20the%20binaural%20human%20auditory%20pathway.%20However%2C%20CNN%20shows%0Abarriers%20in%20capturing%20the%20global%20acoustic%20features.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20novel%20end-to-end%20Binaural%20Audio%20Spectrogram%20Transformer%20%28BAST%29%20model%0Ato%20predict%20the%20sound%20azimuth%20in%20both%20anechoic%20and%20reverberation%20environments.%0ATwo%20modes%20of%20implementation%2C%20i.e.%20BAST-SP%20and%20BAST-NSP%20corresponding%20to%20BAST%0Amodel%20with%20shared%20and%20non-shared%20parameters%20respectively%2C%20are%20explored.%20Our%0Amodel%20with%20subtraction%20interaural%20integration%20and%20hybrid%20loss%20achieves%20an%0Aangular%20distance%20of%201.29%20degrees%20and%20a%20Mean%20Square%20Error%20of%201e-3%20at%20all%0Aazimuths%2C%20significantly%20surpassing%20CNN%20based%20model.%20The%20exploratory%20analysis%20of%0Athe%20BAST%27s%20performance%20on%20the%20left-right%20hemifields%20and%20anechoic%20and%0Areverberation%20environments%20shows%20its%20generalization%20ability%20as%20well%20as%20the%0Afeasibility%20of%20binaural%20Transformers%20in%20sound%20localization.%20Furthermore%2C%20the%0Aanalysis%20of%20the%20attention%20maps%20is%20provided%20to%20give%20additional%20insights%20on%20the%0Ainterpretation%20of%20the%20localization%20process%20in%20a%20natural%20reverberant%0Aenvironment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.03927v2&entry.124074799=Read"},
{"title": "Subjective-Aligned Dataset and Metric for Text-to-Video Quality\n  Assessment", "author": "Tengchuan Kou and Xiaohong Liu and Zicheng Zhang and Chunyi Li and Haoning Wu and Xiongkuo Min and Guangtao Zhai and Ning Liu", "abstract": "  With the rapid development of generative models, Artificial\nIntelligence-Generated Contents (AIGC) have exponentially increased in daily\nlives. Among them, Text-to-Video (T2V) generation has received widespread\nattention. Though many T2V models have been released for generating high\nperceptual quality videos, there is still lack of a method to evaluate the\nquality of these videos quantitatively. To solve this issue, we establish the\nlargest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The\ndataset is composed of 10,000 videos generated by 9 different T2V models. We\nalso conduct a subjective study to obtain each video's corresponding mean\nopinion score. Based on T2VQA-DB, we propose a novel transformer-based model\nfor subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model\nextracts features from text-video alignment and video fidelity perspectives,\nthen it leverages the ability of a large language model to give the prediction\nscore. Experimental results show that T2VQA outperforms existing T2V metrics\nand SOTA video quality assessment models. Quantitative analysis indicates that\nT2VQA is capable of giving subjective-align predictions, validating its\neffectiveness. The dataset and code will be released at\nhttps://github.com/QMME/T2VQA.\n", "link": "http://arxiv.org/abs/2403.11956v5", "date": "2024-08-07", "relevancy": 2.3993, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6226}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.613}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subjective-Aligned%20Dataset%20and%20Metric%20for%20Text-to-Video%20Quality%0A%20%20Assessment&body=Title%3A%20Subjective-Aligned%20Dataset%20and%20Metric%20for%20Text-to-Video%20Quality%0A%20%20Assessment%0AAuthor%3A%20Tengchuan%20Kou%20and%20Xiaohong%20Liu%20and%20Zicheng%20Zhang%20and%20Chunyi%20Li%20and%20Haoning%20Wu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Ning%20Liu%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20generative%20models%2C%20Artificial%0AIntelligence-Generated%20Contents%20%28AIGC%29%20have%20exponentially%20increased%20in%20daily%0Alives.%20Among%20them%2C%20Text-to-Video%20%28T2V%29%20generation%20has%20received%20widespread%0Aattention.%20Though%20many%20T2V%20models%20have%20been%20released%20for%20generating%20high%0Aperceptual%20quality%20videos%2C%20there%20is%20still%20lack%20of%20a%20method%20to%20evaluate%20the%0Aquality%20of%20these%20videos%20quantitatively.%20To%20solve%20this%20issue%2C%20we%20establish%20the%0Alargest-scale%20Text-to-Video%20Quality%20Assessment%20DataBase%20%28T2VQA-DB%29%20to%20date.%20The%0Adataset%20is%20composed%20of%2010%2C000%20videos%20generated%20by%209%20different%20T2V%20models.%20We%0Aalso%20conduct%20a%20subjective%20study%20to%20obtain%20each%20video%27s%20corresponding%20mean%0Aopinion%20score.%20Based%20on%20T2VQA-DB%2C%20we%20propose%20a%20novel%20transformer-based%20model%0Afor%20subjective-aligned%20Text-to-Video%20Quality%20Assessment%20%28T2VQA%29.%20The%20model%0Aextracts%20features%20from%20text-video%20alignment%20and%20video%20fidelity%20perspectives%2C%0Athen%20it%20leverages%20the%20ability%20of%20a%20large%20language%20model%20to%20give%20the%20prediction%0Ascore.%20Experimental%20results%20show%20that%20T2VQA%20outperforms%20existing%20T2V%20metrics%0Aand%20SOTA%20video%20quality%20assessment%20models.%20Quantitative%20analysis%20indicates%20that%0AT2VQA%20is%20capable%20of%20giving%20subjective-align%20predictions%2C%20validating%20its%0Aeffectiveness.%20The%20dataset%20and%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/QMME/T2VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11956v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubjective-Aligned%2520Dataset%2520and%2520Metric%2520for%2520Text-to-Video%2520Quality%250A%2520%2520Assessment%26entry.906535625%3DTengchuan%2520Kou%2520and%2520Xiaohong%2520Liu%2520and%2520Zicheng%2520Zhang%2520and%2520Chunyi%2520Li%2520and%2520Haoning%2520Wu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%2520and%2520Ning%2520Liu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520generative%2520models%252C%2520Artificial%250AIntelligence-Generated%2520Contents%2520%2528AIGC%2529%2520have%2520exponentially%2520increased%2520in%2520daily%250Alives.%2520Among%2520them%252C%2520Text-to-Video%2520%2528T2V%2529%2520generation%2520has%2520received%2520widespread%250Aattention.%2520Though%2520many%2520T2V%2520models%2520have%2520been%2520released%2520for%2520generating%2520high%250Aperceptual%2520quality%2520videos%252C%2520there%2520is%2520still%2520lack%2520of%2520a%2520method%2520to%2520evaluate%2520the%250Aquality%2520of%2520these%2520videos%2520quantitatively.%2520To%2520solve%2520this%2520issue%252C%2520we%2520establish%2520the%250Alargest-scale%2520Text-to-Video%2520Quality%2520Assessment%2520DataBase%2520%2528T2VQA-DB%2529%2520to%2520date.%2520The%250Adataset%2520is%2520composed%2520of%252010%252C000%2520videos%2520generated%2520by%25209%2520different%2520T2V%2520models.%2520We%250Aalso%2520conduct%2520a%2520subjective%2520study%2520to%2520obtain%2520each%2520video%2527s%2520corresponding%2520mean%250Aopinion%2520score.%2520Based%2520on%2520T2VQA-DB%252C%2520we%2520propose%2520a%2520novel%2520transformer-based%2520model%250Afor%2520subjective-aligned%2520Text-to-Video%2520Quality%2520Assessment%2520%2528T2VQA%2529.%2520The%2520model%250Aextracts%2520features%2520from%2520text-video%2520alignment%2520and%2520video%2520fidelity%2520perspectives%252C%250Athen%2520it%2520leverages%2520the%2520ability%2520of%2520a%2520large%2520language%2520model%2520to%2520give%2520the%2520prediction%250Ascore.%2520Experimental%2520results%2520show%2520that%2520T2VQA%2520outperforms%2520existing%2520T2V%2520metrics%250Aand%2520SOTA%2520video%2520quality%2520assessment%2520models.%2520Quantitative%2520analysis%2520indicates%2520that%250AT2VQA%2520is%2520capable%2520of%2520giving%2520subjective-align%2520predictions%252C%2520validating%2520its%250Aeffectiveness.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/QMME/T2VQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11956v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subjective-Aligned%20Dataset%20and%20Metric%20for%20Text-to-Video%20Quality%0A%20%20Assessment&entry.906535625=Tengchuan%20Kou%20and%20Xiaohong%20Liu%20and%20Zicheng%20Zhang%20and%20Chunyi%20Li%20and%20Haoning%20Wu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Ning%20Liu&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20generative%20models%2C%20Artificial%0AIntelligence-Generated%20Contents%20%28AIGC%29%20have%20exponentially%20increased%20in%20daily%0Alives.%20Among%20them%2C%20Text-to-Video%20%28T2V%29%20generation%20has%20received%20widespread%0Aattention.%20Though%20many%20T2V%20models%20have%20been%20released%20for%20generating%20high%0Aperceptual%20quality%20videos%2C%20there%20is%20still%20lack%20of%20a%20method%20to%20evaluate%20the%0Aquality%20of%20these%20videos%20quantitatively.%20To%20solve%20this%20issue%2C%20we%20establish%20the%0Alargest-scale%20Text-to-Video%20Quality%20Assessment%20DataBase%20%28T2VQA-DB%29%20to%20date.%20The%0Adataset%20is%20composed%20of%2010%2C000%20videos%20generated%20by%209%20different%20T2V%20models.%20We%0Aalso%20conduct%20a%20subjective%20study%20to%20obtain%20each%20video%27s%20corresponding%20mean%0Aopinion%20score.%20Based%20on%20T2VQA-DB%2C%20we%20propose%20a%20novel%20transformer-based%20model%0Afor%20subjective-aligned%20Text-to-Video%20Quality%20Assessment%20%28T2VQA%29.%20The%20model%0Aextracts%20features%20from%20text-video%20alignment%20and%20video%20fidelity%20perspectives%2C%0Athen%20it%20leverages%20the%20ability%20of%20a%20large%20language%20model%20to%20give%20the%20prediction%0Ascore.%20Experimental%20results%20show%20that%20T2VQA%20outperforms%20existing%20T2V%20metrics%0Aand%20SOTA%20video%20quality%20assessment%20models.%20Quantitative%20analysis%20indicates%20that%0AT2VQA%20is%20capable%20of%20giving%20subjective-align%20predictions%2C%20validating%20its%0Aeffectiveness.%20The%20dataset%20and%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/QMME/T2VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11956v5&entry.124074799=Read"},
{"title": "Every Dataset Counts: Scaling up Monocular 3D Object Detection with\n  Joint Datasets Training", "author": "Fulong Ma and Xiaoyang Yan and Guoyang Zhao and Xiaojie Xu and Yuxuan Liu and Ming Liu", "abstract": "  Monocular 3D object detection plays a crucial role in autonomous driving.\nHowever, existing monocular 3D detection algorithms depend on 3D labels derived\nfrom LiDAR measurements, which are costly to acquire for new datasets and\nchallenging to deploy in novel environments. Specifically, this study\ninvestigates the pipeline for training a monocular 3D object detection model on\na diverse collection of 3D and 2D datasets. The proposed framework comprises\nthree components: (1) a robust monocular 3D model capable of functioning across\nvarious camera settings, (2) a selective-training strategy to accommodate\ndatasets with differing class annotations, and (3) a pseudo 3D training\napproach using 2D labels to enhance detection performance in scenes containing\nonly 2D labels. With this framework, we could train models on a joint set of\nvarious open 3D/2D datasets to obtain models with significantly stronger\ngeneralization capability and enhanced performance on new dataset with only 2D\nlabels. We conduct extensive experiments on\nKITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scaling\nability of the proposed method.\n", "link": "http://arxiv.org/abs/2310.00920v3", "date": "2024-08-07", "relevancy": 2.3852, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6077}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Every%20Dataset%20Counts%3A%20Scaling%20up%20Monocular%203D%20Object%20Detection%20with%0A%20%20Joint%20Datasets%20Training&body=Title%3A%20Every%20Dataset%20Counts%3A%20Scaling%20up%20Monocular%203D%20Object%20Detection%20with%0A%20%20Joint%20Datasets%20Training%0AAuthor%3A%20Fulong%20Ma%20and%20Xiaoyang%20Yan%20and%20Guoyang%20Zhao%20and%20Xiaojie%20Xu%20and%20Yuxuan%20Liu%20and%20Ming%20Liu%0AAbstract%3A%20%20%20Monocular%203D%20object%20detection%20plays%20a%20crucial%20role%20in%20autonomous%20driving.%0AHowever%2C%20existing%20monocular%203D%20detection%20algorithms%20depend%20on%203D%20labels%20derived%0Afrom%20LiDAR%20measurements%2C%20which%20are%20costly%20to%20acquire%20for%20new%20datasets%20and%0Achallenging%20to%20deploy%20in%20novel%20environments.%20Specifically%2C%20this%20study%0Ainvestigates%20the%20pipeline%20for%20training%20a%20monocular%203D%20object%20detection%20model%20on%0Aa%20diverse%20collection%20of%203D%20and%202D%20datasets.%20The%20proposed%20framework%20comprises%0Athree%20components%3A%20%281%29%20a%20robust%20monocular%203D%20model%20capable%20of%20functioning%20across%0Avarious%20camera%20settings%2C%20%282%29%20a%20selective-training%20strategy%20to%20accommodate%0Adatasets%20with%20differing%20class%20annotations%2C%20and%20%283%29%20a%20pseudo%203D%20training%0Aapproach%20using%202D%20labels%20to%20enhance%20detection%20performance%20in%20scenes%20containing%0Aonly%202D%20labels.%20With%20this%20framework%2C%20we%20could%20train%20models%20on%20a%20joint%20set%20of%0Avarious%20open%203D/2D%20datasets%20to%20obtain%20models%20with%20significantly%20stronger%0Ageneralization%20capability%20and%20enhanced%20performance%20on%20new%20dataset%20with%20only%202D%0Alabels.%20We%20conduct%20extensive%20experiments%20on%0AKITTI/nuScenes/ONCE/Cityscapes/BDD100K%20datasets%20to%20demonstrate%20the%20scaling%0Aability%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00920v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvery%2520Dataset%2520Counts%253A%2520Scaling%2520up%2520Monocular%25203D%2520Object%2520Detection%2520with%250A%2520%2520Joint%2520Datasets%2520Training%26entry.906535625%3DFulong%2520Ma%2520and%2520Xiaoyang%2520Yan%2520and%2520Guoyang%2520Zhao%2520and%2520Xiaojie%2520Xu%2520and%2520Yuxuan%2520Liu%2520and%2520Ming%2520Liu%26entry.1292438233%3D%2520%2520Monocular%25203D%2520object%2520detection%2520plays%2520a%2520crucial%2520role%2520in%2520autonomous%2520driving.%250AHowever%252C%2520existing%2520monocular%25203D%2520detection%2520algorithms%2520depend%2520on%25203D%2520labels%2520derived%250Afrom%2520LiDAR%2520measurements%252C%2520which%2520are%2520costly%2520to%2520acquire%2520for%2520new%2520datasets%2520and%250Achallenging%2520to%2520deploy%2520in%2520novel%2520environments.%2520Specifically%252C%2520this%2520study%250Ainvestigates%2520the%2520pipeline%2520for%2520training%2520a%2520monocular%25203D%2520object%2520detection%2520model%2520on%250Aa%2520diverse%2520collection%2520of%25203D%2520and%25202D%2520datasets.%2520The%2520proposed%2520framework%2520comprises%250Athree%2520components%253A%2520%25281%2529%2520a%2520robust%2520monocular%25203D%2520model%2520capable%2520of%2520functioning%2520across%250Avarious%2520camera%2520settings%252C%2520%25282%2529%2520a%2520selective-training%2520strategy%2520to%2520accommodate%250Adatasets%2520with%2520differing%2520class%2520annotations%252C%2520and%2520%25283%2529%2520a%2520pseudo%25203D%2520training%250Aapproach%2520using%25202D%2520labels%2520to%2520enhance%2520detection%2520performance%2520in%2520scenes%2520containing%250Aonly%25202D%2520labels.%2520With%2520this%2520framework%252C%2520we%2520could%2520train%2520models%2520on%2520a%2520joint%2520set%2520of%250Avarious%2520open%25203D/2D%2520datasets%2520to%2520obtain%2520models%2520with%2520significantly%2520stronger%250Ageneralization%2520capability%2520and%2520enhanced%2520performance%2520on%2520new%2520dataset%2520with%2520only%25202D%250Alabels.%2520We%2520conduct%2520extensive%2520experiments%2520on%250AKITTI/nuScenes/ONCE/Cityscapes/BDD100K%2520datasets%2520to%2520demonstrate%2520the%2520scaling%250Aability%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00920v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Every%20Dataset%20Counts%3A%20Scaling%20up%20Monocular%203D%20Object%20Detection%20with%0A%20%20Joint%20Datasets%20Training&entry.906535625=Fulong%20Ma%20and%20Xiaoyang%20Yan%20and%20Guoyang%20Zhao%20and%20Xiaojie%20Xu%20and%20Yuxuan%20Liu%20and%20Ming%20Liu&entry.1292438233=%20%20Monocular%203D%20object%20detection%20plays%20a%20crucial%20role%20in%20autonomous%20driving.%0AHowever%2C%20existing%20monocular%203D%20detection%20algorithms%20depend%20on%203D%20labels%20derived%0Afrom%20LiDAR%20measurements%2C%20which%20are%20costly%20to%20acquire%20for%20new%20datasets%20and%0Achallenging%20to%20deploy%20in%20novel%20environments.%20Specifically%2C%20this%20study%0Ainvestigates%20the%20pipeline%20for%20training%20a%20monocular%203D%20object%20detection%20model%20on%0Aa%20diverse%20collection%20of%203D%20and%202D%20datasets.%20The%20proposed%20framework%20comprises%0Athree%20components%3A%20%281%29%20a%20robust%20monocular%203D%20model%20capable%20of%20functioning%20across%0Avarious%20camera%20settings%2C%20%282%29%20a%20selective-training%20strategy%20to%20accommodate%0Adatasets%20with%20differing%20class%20annotations%2C%20and%20%283%29%20a%20pseudo%203D%20training%0Aapproach%20using%202D%20labels%20to%20enhance%20detection%20performance%20in%20scenes%20containing%0Aonly%202D%20labels.%20With%20this%20framework%2C%20we%20could%20train%20models%20on%20a%20joint%20set%20of%0Avarious%20open%203D/2D%20datasets%20to%20obtain%20models%20with%20significantly%20stronger%0Ageneralization%20capability%20and%20enhanced%20performance%20on%20new%20dataset%20with%20only%202D%0Alabels.%20We%20conduct%20extensive%20experiments%20on%0AKITTI/nuScenes/ONCE/Cityscapes/BDD100K%20datasets%20to%20demonstrate%20the%20scaling%0Aability%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00920v3&entry.124074799=Read"},
{"title": "MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video", "author": "Xiaoqing Guo and Qianhui Men and J. Alison Noble", "abstract": "  We present the first automated multimodal summary generation system,\nMMSummary, for medical imaging video, particularly with a focus on fetal\nultrasound analysis. Imitating the examination process performed by a human\nsonographer, MMSummary is designed as a three-stage pipeline, progressing from\nkeyframe detection to keyframe captioning and finally anatomy segmentation and\nmeasurement. In the keyframe detection stage, an innovative automated workflow\nis proposed to progressively select a concise set of keyframes, preserving\nsufficient video information without redundancy. Subsequently, we adapt a large\nlanguage model to generate meaningful captions for fetal ultrasound keyframes\nin the keyframe captioning stage. If a keyframe is captioned as fetal biometry,\nthe segmentation and measurement stage estimates biometric parameters by\nsegmenting the region of interest according to the textual prior. The MMSummary\nsystem provides comprehensive summaries for fetal ultrasound examinations and\nbased on reported experiments is estimated to reduce scanning time by\napproximately 31.5%, thereby suggesting the potential to enhance clinical\nworkflow efficiency.\n", "link": "http://arxiv.org/abs/2408.03761v1", "date": "2024-08-07", "relevancy": 2.372, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4768}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4733}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMSummary%3A%20Multimodal%20Summary%20Generation%20for%20Fetal%20Ultrasound%20Video&body=Title%3A%20MMSummary%3A%20Multimodal%20Summary%20Generation%20for%20Fetal%20Ultrasound%20Video%0AAuthor%3A%20Xiaoqing%20Guo%20and%20Qianhui%20Men%20and%20J.%20Alison%20Noble%0AAbstract%3A%20%20%20We%20present%20the%20first%20automated%20multimodal%20summary%20generation%20system%2C%0AMMSummary%2C%20for%20medical%20imaging%20video%2C%20particularly%20with%20a%20focus%20on%20fetal%0Aultrasound%20analysis.%20Imitating%20the%20examination%20process%20performed%20by%20a%20human%0Asonographer%2C%20MMSummary%20is%20designed%20as%20a%20three-stage%20pipeline%2C%20progressing%20from%0Akeyframe%20detection%20to%20keyframe%20captioning%20and%20finally%20anatomy%20segmentation%20and%0Ameasurement.%20In%20the%20keyframe%20detection%20stage%2C%20an%20innovative%20automated%20workflow%0Ais%20proposed%20to%20progressively%20select%20a%20concise%20set%20of%20keyframes%2C%20preserving%0Asufficient%20video%20information%20without%20redundancy.%20Subsequently%2C%20we%20adapt%20a%20large%0Alanguage%20model%20to%20generate%20meaningful%20captions%20for%20fetal%20ultrasound%20keyframes%0Ain%20the%20keyframe%20captioning%20stage.%20If%20a%20keyframe%20is%20captioned%20as%20fetal%20biometry%2C%0Athe%20segmentation%20and%20measurement%20stage%20estimates%20biometric%20parameters%20by%0Asegmenting%20the%20region%20of%20interest%20according%20to%20the%20textual%20prior.%20The%20MMSummary%0Asystem%20provides%20comprehensive%20summaries%20for%20fetal%20ultrasound%20examinations%20and%0Abased%20on%20reported%20experiments%20is%20estimated%20to%20reduce%20scanning%20time%20by%0Aapproximately%2031.5%25%2C%20thereby%20suggesting%20the%20potential%20to%20enhance%20clinical%0Aworkflow%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMSummary%253A%2520Multimodal%2520Summary%2520Generation%2520for%2520Fetal%2520Ultrasound%2520Video%26entry.906535625%3DXiaoqing%2520Guo%2520and%2520Qianhui%2520Men%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520automated%2520multimodal%2520summary%2520generation%2520system%252C%250AMMSummary%252C%2520for%2520medical%2520imaging%2520video%252C%2520particularly%2520with%2520a%2520focus%2520on%2520fetal%250Aultrasound%2520analysis.%2520Imitating%2520the%2520examination%2520process%2520performed%2520by%2520a%2520human%250Asonographer%252C%2520MMSummary%2520is%2520designed%2520as%2520a%2520three-stage%2520pipeline%252C%2520progressing%2520from%250Akeyframe%2520detection%2520to%2520keyframe%2520captioning%2520and%2520finally%2520anatomy%2520segmentation%2520and%250Ameasurement.%2520In%2520the%2520keyframe%2520detection%2520stage%252C%2520an%2520innovative%2520automated%2520workflow%250Ais%2520proposed%2520to%2520progressively%2520select%2520a%2520concise%2520set%2520of%2520keyframes%252C%2520preserving%250Asufficient%2520video%2520information%2520without%2520redundancy.%2520Subsequently%252C%2520we%2520adapt%2520a%2520large%250Alanguage%2520model%2520to%2520generate%2520meaningful%2520captions%2520for%2520fetal%2520ultrasound%2520keyframes%250Ain%2520the%2520keyframe%2520captioning%2520stage.%2520If%2520a%2520keyframe%2520is%2520captioned%2520as%2520fetal%2520biometry%252C%250Athe%2520segmentation%2520and%2520measurement%2520stage%2520estimates%2520biometric%2520parameters%2520by%250Asegmenting%2520the%2520region%2520of%2520interest%2520according%2520to%2520the%2520textual%2520prior.%2520The%2520MMSummary%250Asystem%2520provides%2520comprehensive%2520summaries%2520for%2520fetal%2520ultrasound%2520examinations%2520and%250Abased%2520on%2520reported%2520experiments%2520is%2520estimated%2520to%2520reduce%2520scanning%2520time%2520by%250Aapproximately%252031.5%2525%252C%2520thereby%2520suggesting%2520the%2520potential%2520to%2520enhance%2520clinical%250Aworkflow%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMSummary%3A%20Multimodal%20Summary%20Generation%20for%20Fetal%20Ultrasound%20Video&entry.906535625=Xiaoqing%20Guo%20and%20Qianhui%20Men%20and%20J.%20Alison%20Noble&entry.1292438233=%20%20We%20present%20the%20first%20automated%20multimodal%20summary%20generation%20system%2C%0AMMSummary%2C%20for%20medical%20imaging%20video%2C%20particularly%20with%20a%20focus%20on%20fetal%0Aultrasound%20analysis.%20Imitating%20the%20examination%20process%20performed%20by%20a%20human%0Asonographer%2C%20MMSummary%20is%20designed%20as%20a%20three-stage%20pipeline%2C%20progressing%20from%0Akeyframe%20detection%20to%20keyframe%20captioning%20and%20finally%20anatomy%20segmentation%20and%0Ameasurement.%20In%20the%20keyframe%20detection%20stage%2C%20an%20innovative%20automated%20workflow%0Ais%20proposed%20to%20progressively%20select%20a%20concise%20set%20of%20keyframes%2C%20preserving%0Asufficient%20video%20information%20without%20redundancy.%20Subsequently%2C%20we%20adapt%20a%20large%0Alanguage%20model%20to%20generate%20meaningful%20captions%20for%20fetal%20ultrasound%20keyframes%0Ain%20the%20keyframe%20captioning%20stage.%20If%20a%20keyframe%20is%20captioned%20as%20fetal%20biometry%2C%0Athe%20segmentation%20and%20measurement%20stage%20estimates%20biometric%20parameters%20by%0Asegmenting%20the%20region%20of%20interest%20according%20to%20the%20textual%20prior.%20The%20MMSummary%0Asystem%20provides%20comprehensive%20summaries%20for%20fetal%20ultrasound%20examinations%20and%0Abased%20on%20reported%20experiments%20is%20estimated%20to%20reduce%20scanning%20time%20by%0Aapproximately%2031.5%25%2C%20thereby%20suggesting%20the%20potential%20to%20enhance%20clinical%0Aworkflow%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03761v1&entry.124074799=Read"},
{"title": "Reliable Node Similarity Matrix Guided Contrastive Graph Clustering", "author": "Yunhui Liu and Xinyi Gao and Tieke He and Tao Zheng and Jianhua Zhao and Hongzhi Yin", "abstract": "  Graph clustering, which involves the partitioning of nodes within a graph\ninto disjoint clusters, holds significant importance for numerous subsequent\napplications. Recently, contrastive learning, known for utilizing supervisory\ninformation, has demonstrated encouraging results in deep graph clustering.\nThis methodology facilitates the learning of favorable node representations for\nclustering by attracting positively correlated node pairs and distancing\nnegatively correlated pairs within the representation space. Nevertheless, a\nsignificant limitation of existing methods is their inadequacy in thoroughly\nexploring node-wise similarity. For instance, some hypothesize that the node\nsimilarity matrix within the representation space is identical, ignoring the\ninherent semantic relationships among nodes. Given the fundamental role of\ninstance similarity in clustering, our research investigates contrastive graph\nclustering from the perspective of the node similarity matrix. We argue that an\nideal node similarity matrix within the representation space should accurately\nreflect the inherent semantic relationships among nodes, ensuring the\npreservation of semantic similarities in the learned representations. In\nresponse to this, we introduce a new framework, Reliable Node Similarity Matrix\nGuided Contrastive Graph Clustering (NS4GC), which estimates an approximately\nideal node similarity matrix within the representation space to guide\nrepresentation learning. Our method introduces node-neighbor alignment and\nsemantic-aware sparsification, ensuring the node similarity matrix is both\naccurate and efficiently sparse. Comprehensive experiments conducted on $8$\nreal-world datasets affirm the efficacy of learning the node similarity matrix\nand the superior performance of NS4GC.\n", "link": "http://arxiv.org/abs/2408.03765v1", "date": "2024-08-07", "relevancy": 2.37, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5071}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Node%20Similarity%20Matrix%20Guided%20Contrastive%20Graph%20Clustering&body=Title%3A%20Reliable%20Node%20Similarity%20Matrix%20Guided%20Contrastive%20Graph%20Clustering%0AAuthor%3A%20Yunhui%20Liu%20and%20Xinyi%20Gao%20and%20Tieke%20He%20and%20Tao%20Zheng%20and%20Jianhua%20Zhao%20and%20Hongzhi%20Yin%0AAbstract%3A%20%20%20Graph%20clustering%2C%20which%20involves%20the%20partitioning%20of%20nodes%20within%20a%20graph%0Ainto%20disjoint%20clusters%2C%20holds%20significant%20importance%20for%20numerous%20subsequent%0Aapplications.%20Recently%2C%20contrastive%20learning%2C%20known%20for%20utilizing%20supervisory%0Ainformation%2C%20has%20demonstrated%20encouraging%20results%20in%20deep%20graph%20clustering.%0AThis%20methodology%20facilitates%20the%20learning%20of%20favorable%20node%20representations%20for%0Aclustering%20by%20attracting%20positively%20correlated%20node%20pairs%20and%20distancing%0Anegatively%20correlated%20pairs%20within%20the%20representation%20space.%20Nevertheless%2C%20a%0Asignificant%20limitation%20of%20existing%20methods%20is%20their%20inadequacy%20in%20thoroughly%0Aexploring%20node-wise%20similarity.%20For%20instance%2C%20some%20hypothesize%20that%20the%20node%0Asimilarity%20matrix%20within%20the%20representation%20space%20is%20identical%2C%20ignoring%20the%0Ainherent%20semantic%20relationships%20among%20nodes.%20Given%20the%20fundamental%20role%20of%0Ainstance%20similarity%20in%20clustering%2C%20our%20research%20investigates%20contrastive%20graph%0Aclustering%20from%20the%20perspective%20of%20the%20node%20similarity%20matrix.%20We%20argue%20that%20an%0Aideal%20node%20similarity%20matrix%20within%20the%20representation%20space%20should%20accurately%0Areflect%20the%20inherent%20semantic%20relationships%20among%20nodes%2C%20ensuring%20the%0Apreservation%20of%20semantic%20similarities%20in%20the%20learned%20representations.%20In%0Aresponse%20to%20this%2C%20we%20introduce%20a%20new%20framework%2C%20Reliable%20Node%20Similarity%20Matrix%0AGuided%20Contrastive%20Graph%20Clustering%20%28NS4GC%29%2C%20which%20estimates%20an%20approximately%0Aideal%20node%20similarity%20matrix%20within%20the%20representation%20space%20to%20guide%0Arepresentation%20learning.%20Our%20method%20introduces%20node-neighbor%20alignment%20and%0Asemantic-aware%20sparsification%2C%20ensuring%20the%20node%20similarity%20matrix%20is%20both%0Aaccurate%20and%20efficiently%20sparse.%20Comprehensive%20experiments%20conducted%20on%20%248%24%0Areal-world%20datasets%20affirm%20the%20efficacy%20of%20learning%20the%20node%20similarity%20matrix%0Aand%20the%20superior%20performance%20of%20NS4GC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Node%2520Similarity%2520Matrix%2520Guided%2520Contrastive%2520Graph%2520Clustering%26entry.906535625%3DYunhui%2520Liu%2520and%2520Xinyi%2520Gao%2520and%2520Tieke%2520He%2520and%2520Tao%2520Zheng%2520and%2520Jianhua%2520Zhao%2520and%2520Hongzhi%2520Yin%26entry.1292438233%3D%2520%2520Graph%2520clustering%252C%2520which%2520involves%2520the%2520partitioning%2520of%2520nodes%2520within%2520a%2520graph%250Ainto%2520disjoint%2520clusters%252C%2520holds%2520significant%2520importance%2520for%2520numerous%2520subsequent%250Aapplications.%2520Recently%252C%2520contrastive%2520learning%252C%2520known%2520for%2520utilizing%2520supervisory%250Ainformation%252C%2520has%2520demonstrated%2520encouraging%2520results%2520in%2520deep%2520graph%2520clustering.%250AThis%2520methodology%2520facilitates%2520the%2520learning%2520of%2520favorable%2520node%2520representations%2520for%250Aclustering%2520by%2520attracting%2520positively%2520correlated%2520node%2520pairs%2520and%2520distancing%250Anegatively%2520correlated%2520pairs%2520within%2520the%2520representation%2520space.%2520Nevertheless%252C%2520a%250Asignificant%2520limitation%2520of%2520existing%2520methods%2520is%2520their%2520inadequacy%2520in%2520thoroughly%250Aexploring%2520node-wise%2520similarity.%2520For%2520instance%252C%2520some%2520hypothesize%2520that%2520the%2520node%250Asimilarity%2520matrix%2520within%2520the%2520representation%2520space%2520is%2520identical%252C%2520ignoring%2520the%250Ainherent%2520semantic%2520relationships%2520among%2520nodes.%2520Given%2520the%2520fundamental%2520role%2520of%250Ainstance%2520similarity%2520in%2520clustering%252C%2520our%2520research%2520investigates%2520contrastive%2520graph%250Aclustering%2520from%2520the%2520perspective%2520of%2520the%2520node%2520similarity%2520matrix.%2520We%2520argue%2520that%2520an%250Aideal%2520node%2520similarity%2520matrix%2520within%2520the%2520representation%2520space%2520should%2520accurately%250Areflect%2520the%2520inherent%2520semantic%2520relationships%2520among%2520nodes%252C%2520ensuring%2520the%250Apreservation%2520of%2520semantic%2520similarities%2520in%2520the%2520learned%2520representations.%2520In%250Aresponse%2520to%2520this%252C%2520we%2520introduce%2520a%2520new%2520framework%252C%2520Reliable%2520Node%2520Similarity%2520Matrix%250AGuided%2520Contrastive%2520Graph%2520Clustering%2520%2528NS4GC%2529%252C%2520which%2520estimates%2520an%2520approximately%250Aideal%2520node%2520similarity%2520matrix%2520within%2520the%2520representation%2520space%2520to%2520guide%250Arepresentation%2520learning.%2520Our%2520method%2520introduces%2520node-neighbor%2520alignment%2520and%250Asemantic-aware%2520sparsification%252C%2520ensuring%2520the%2520node%2520similarity%2520matrix%2520is%2520both%250Aaccurate%2520and%2520efficiently%2520sparse.%2520Comprehensive%2520experiments%2520conducted%2520on%2520%25248%2524%250Areal-world%2520datasets%2520affirm%2520the%2520efficacy%2520of%2520learning%2520the%2520node%2520similarity%2520matrix%250Aand%2520the%2520superior%2520performance%2520of%2520NS4GC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Node%20Similarity%20Matrix%20Guided%20Contrastive%20Graph%20Clustering&entry.906535625=Yunhui%20Liu%20and%20Xinyi%20Gao%20and%20Tieke%20He%20and%20Tao%20Zheng%20and%20Jianhua%20Zhao%20and%20Hongzhi%20Yin&entry.1292438233=%20%20Graph%20clustering%2C%20which%20involves%20the%20partitioning%20of%20nodes%20within%20a%20graph%0Ainto%20disjoint%20clusters%2C%20holds%20significant%20importance%20for%20numerous%20subsequent%0Aapplications.%20Recently%2C%20contrastive%20learning%2C%20known%20for%20utilizing%20supervisory%0Ainformation%2C%20has%20demonstrated%20encouraging%20results%20in%20deep%20graph%20clustering.%0AThis%20methodology%20facilitates%20the%20learning%20of%20favorable%20node%20representations%20for%0Aclustering%20by%20attracting%20positively%20correlated%20node%20pairs%20and%20distancing%0Anegatively%20correlated%20pairs%20within%20the%20representation%20space.%20Nevertheless%2C%20a%0Asignificant%20limitation%20of%20existing%20methods%20is%20their%20inadequacy%20in%20thoroughly%0Aexploring%20node-wise%20similarity.%20For%20instance%2C%20some%20hypothesize%20that%20the%20node%0Asimilarity%20matrix%20within%20the%20representation%20space%20is%20identical%2C%20ignoring%20the%0Ainherent%20semantic%20relationships%20among%20nodes.%20Given%20the%20fundamental%20role%20of%0Ainstance%20similarity%20in%20clustering%2C%20our%20research%20investigates%20contrastive%20graph%0Aclustering%20from%20the%20perspective%20of%20the%20node%20similarity%20matrix.%20We%20argue%20that%20an%0Aideal%20node%20similarity%20matrix%20within%20the%20representation%20space%20should%20accurately%0Areflect%20the%20inherent%20semantic%20relationships%20among%20nodes%2C%20ensuring%20the%0Apreservation%20of%20semantic%20similarities%20in%20the%20learned%20representations.%20In%0Aresponse%20to%20this%2C%20we%20introduce%20a%20new%20framework%2C%20Reliable%20Node%20Similarity%20Matrix%0AGuided%20Contrastive%20Graph%20Clustering%20%28NS4GC%29%2C%20which%20estimates%20an%20approximately%0Aideal%20node%20similarity%20matrix%20within%20the%20representation%20space%20to%20guide%0Arepresentation%20learning.%20Our%20method%20introduces%20node-neighbor%20alignment%20and%0Asemantic-aware%20sparsification%2C%20ensuring%20the%20node%20similarity%20matrix%20is%20both%0Aaccurate%20and%20efficiently%20sparse.%20Comprehensive%20experiments%20conducted%20on%20%248%24%0Areal-world%20datasets%20affirm%20the%20efficacy%20of%20learning%20the%20node%20similarity%20matrix%0Aand%20the%20superior%20performance%20of%20NS4GC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03765v1&entry.124074799=Read"},
{"title": "Digital Twins and Civil Engineering Phases: Reorienting Adoption\n  Strategies", "author": "Taiwo A. Adebiyi and Nafeezat A. Ajenifuja and Ruda Zhang", "abstract": "  Digital twin (DT) technology has received immense attention over the years\ndue to the promises it presents to various stakeholders in science and\nengineering. As a result, different thematic areas of DT have been explored.\nThis is no different in specific fields such as manufacturing, automation, oil\nand gas, and civil engineering, leading to fragmented approaches for\nfield-specific applications. The civil engineering industry is further\ndisadvantaged in this regard as it relies on external techniques by other\nengineering fields for its DT adoption. A rising consequence of these\nextensions is a concentrated application of DT to the operations and\nmaintenance phase. On another spectrum, Building Information Modeling (BIM) is\npervasively utilized in the planning/design phase, and the transient nature of\nthe construction phase remains a challenge for its DT adoption. In this paper,\nwe present a phase-based development of DT in the Architecture, Engineering,\nand Construction industry. We commence by presenting succinct expositions on DT\nas a concept and as a service, and establish a five-level scale system.\nFurthermore, we present separately a systematic literature review of the\nconventional techniques employed at each civil engineering phase. In this\nregard, we identified enabling technologies such as computer vision for\nextended sensing and the Internet of Things for reliable integration.\nUltimately, we attempt to reveal DT as an important tool across the entire life\ncycle of civil engineering projects, and nudge researchers to think more\nholistically in their quest for the integration of DT for civil engineering\napplications.\n", "link": "http://arxiv.org/abs/2403.02426v2", "date": "2024-08-07", "relevancy": 2.316, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4806}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4806}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20Twins%20and%20Civil%20Engineering%20Phases%3A%20Reorienting%20Adoption%0A%20%20Strategies&body=Title%3A%20Digital%20Twins%20and%20Civil%20Engineering%20Phases%3A%20Reorienting%20Adoption%0A%20%20Strategies%0AAuthor%3A%20Taiwo%20A.%20Adebiyi%20and%20Nafeezat%20A.%20Ajenifuja%20and%20Ruda%20Zhang%0AAbstract%3A%20%20%20Digital%20twin%20%28DT%29%20technology%20has%20received%20immense%20attention%20over%20the%20years%0Adue%20to%20the%20promises%20it%20presents%20to%20various%20stakeholders%20in%20science%20and%0Aengineering.%20As%20a%20result%2C%20different%20thematic%20areas%20of%20DT%20have%20been%20explored.%0AThis%20is%20no%20different%20in%20specific%20fields%20such%20as%20manufacturing%2C%20automation%2C%20oil%0Aand%20gas%2C%20and%20civil%20engineering%2C%20leading%20to%20fragmented%20approaches%20for%0Afield-specific%20applications.%20The%20civil%20engineering%20industry%20is%20further%0Adisadvantaged%20in%20this%20regard%20as%20it%20relies%20on%20external%20techniques%20by%20other%0Aengineering%20fields%20for%20its%20DT%20adoption.%20A%20rising%20consequence%20of%20these%0Aextensions%20is%20a%20concentrated%20application%20of%20DT%20to%20the%20operations%20and%0Amaintenance%20phase.%20On%20another%20spectrum%2C%20Building%20Information%20Modeling%20%28BIM%29%20is%0Apervasively%20utilized%20in%20the%20planning/design%20phase%2C%20and%20the%20transient%20nature%20of%0Athe%20construction%20phase%20remains%20a%20challenge%20for%20its%20DT%20adoption.%20In%20this%20paper%2C%0Awe%20present%20a%20phase-based%20development%20of%20DT%20in%20the%20Architecture%2C%20Engineering%2C%0Aand%20Construction%20industry.%20We%20commence%20by%20presenting%20succinct%20expositions%20on%20DT%0Aas%20a%20concept%20and%20as%20a%20service%2C%20and%20establish%20a%20five-level%20scale%20system.%0AFurthermore%2C%20we%20present%20separately%20a%20systematic%20literature%20review%20of%20the%0Aconventional%20techniques%20employed%20at%20each%20civil%20engineering%20phase.%20In%20this%0Aregard%2C%20we%20identified%20enabling%20technologies%20such%20as%20computer%20vision%20for%0Aextended%20sensing%20and%20the%20Internet%20of%20Things%20for%20reliable%20integration.%0AUltimately%2C%20we%20attempt%20to%20reveal%20DT%20as%20an%20important%20tool%20across%20the%20entire%20life%0Acycle%20of%20civil%20engineering%20projects%2C%20and%20nudge%20researchers%20to%20think%20more%0Aholistically%20in%20their%20quest%20for%20the%20integration%20of%20DT%20for%20civil%20engineering%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520Twins%2520and%2520Civil%2520Engineering%2520Phases%253A%2520Reorienting%2520Adoption%250A%2520%2520Strategies%26entry.906535625%3DTaiwo%2520A.%2520Adebiyi%2520and%2520Nafeezat%2520A.%2520Ajenifuja%2520and%2520Ruda%2520Zhang%26entry.1292438233%3D%2520%2520Digital%2520twin%2520%2528DT%2529%2520technology%2520has%2520received%2520immense%2520attention%2520over%2520the%2520years%250Adue%2520to%2520the%2520promises%2520it%2520presents%2520to%2520various%2520stakeholders%2520in%2520science%2520and%250Aengineering.%2520As%2520a%2520result%252C%2520different%2520thematic%2520areas%2520of%2520DT%2520have%2520been%2520explored.%250AThis%2520is%2520no%2520different%2520in%2520specific%2520fields%2520such%2520as%2520manufacturing%252C%2520automation%252C%2520oil%250Aand%2520gas%252C%2520and%2520civil%2520engineering%252C%2520leading%2520to%2520fragmented%2520approaches%2520for%250Afield-specific%2520applications.%2520The%2520civil%2520engineering%2520industry%2520is%2520further%250Adisadvantaged%2520in%2520this%2520regard%2520as%2520it%2520relies%2520on%2520external%2520techniques%2520by%2520other%250Aengineering%2520fields%2520for%2520its%2520DT%2520adoption.%2520A%2520rising%2520consequence%2520of%2520these%250Aextensions%2520is%2520a%2520concentrated%2520application%2520of%2520DT%2520to%2520the%2520operations%2520and%250Amaintenance%2520phase.%2520On%2520another%2520spectrum%252C%2520Building%2520Information%2520Modeling%2520%2528BIM%2529%2520is%250Apervasively%2520utilized%2520in%2520the%2520planning/design%2520phase%252C%2520and%2520the%2520transient%2520nature%2520of%250Athe%2520construction%2520phase%2520remains%2520a%2520challenge%2520for%2520its%2520DT%2520adoption.%2520In%2520this%2520paper%252C%250Awe%2520present%2520a%2520phase-based%2520development%2520of%2520DT%2520in%2520the%2520Architecture%252C%2520Engineering%252C%250Aand%2520Construction%2520industry.%2520We%2520commence%2520by%2520presenting%2520succinct%2520expositions%2520on%2520DT%250Aas%2520a%2520concept%2520and%2520as%2520a%2520service%252C%2520and%2520establish%2520a%2520five-level%2520scale%2520system.%250AFurthermore%252C%2520we%2520present%2520separately%2520a%2520systematic%2520literature%2520review%2520of%2520the%250Aconventional%2520techniques%2520employed%2520at%2520each%2520civil%2520engineering%2520phase.%2520In%2520this%250Aregard%252C%2520we%2520identified%2520enabling%2520technologies%2520such%2520as%2520computer%2520vision%2520for%250Aextended%2520sensing%2520and%2520the%2520Internet%2520of%2520Things%2520for%2520reliable%2520integration.%250AUltimately%252C%2520we%2520attempt%2520to%2520reveal%2520DT%2520as%2520an%2520important%2520tool%2520across%2520the%2520entire%2520life%250Acycle%2520of%2520civil%2520engineering%2520projects%252C%2520and%2520nudge%2520researchers%2520to%2520think%2520more%250Aholistically%2520in%2520their%2520quest%2520for%2520the%2520integration%2520of%2520DT%2520for%2520civil%2520engineering%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Twins%20and%20Civil%20Engineering%20Phases%3A%20Reorienting%20Adoption%0A%20%20Strategies&entry.906535625=Taiwo%20A.%20Adebiyi%20and%20Nafeezat%20A.%20Ajenifuja%20and%20Ruda%20Zhang&entry.1292438233=%20%20Digital%20twin%20%28DT%29%20technology%20has%20received%20immense%20attention%20over%20the%20years%0Adue%20to%20the%20promises%20it%20presents%20to%20various%20stakeholders%20in%20science%20and%0Aengineering.%20As%20a%20result%2C%20different%20thematic%20areas%20of%20DT%20have%20been%20explored.%0AThis%20is%20no%20different%20in%20specific%20fields%20such%20as%20manufacturing%2C%20automation%2C%20oil%0Aand%20gas%2C%20and%20civil%20engineering%2C%20leading%20to%20fragmented%20approaches%20for%0Afield-specific%20applications.%20The%20civil%20engineering%20industry%20is%20further%0Adisadvantaged%20in%20this%20regard%20as%20it%20relies%20on%20external%20techniques%20by%20other%0Aengineering%20fields%20for%20its%20DT%20adoption.%20A%20rising%20consequence%20of%20these%0Aextensions%20is%20a%20concentrated%20application%20of%20DT%20to%20the%20operations%20and%0Amaintenance%20phase.%20On%20another%20spectrum%2C%20Building%20Information%20Modeling%20%28BIM%29%20is%0Apervasively%20utilized%20in%20the%20planning/design%20phase%2C%20and%20the%20transient%20nature%20of%0Athe%20construction%20phase%20remains%20a%20challenge%20for%20its%20DT%20adoption.%20In%20this%20paper%2C%0Awe%20present%20a%20phase-based%20development%20of%20DT%20in%20the%20Architecture%2C%20Engineering%2C%0Aand%20Construction%20industry.%20We%20commence%20by%20presenting%20succinct%20expositions%20on%20DT%0Aas%20a%20concept%20and%20as%20a%20service%2C%20and%20establish%20a%20five-level%20scale%20system.%0AFurthermore%2C%20we%20present%20separately%20a%20systematic%20literature%20review%20of%20the%0Aconventional%20techniques%20employed%20at%20each%20civil%20engineering%20phase.%20In%20this%0Aregard%2C%20we%20identified%20enabling%20technologies%20such%20as%20computer%20vision%20for%0Aextended%20sensing%20and%20the%20Internet%20of%20Things%20for%20reliable%20integration.%0AUltimately%2C%20we%20attempt%20to%20reveal%20DT%20as%20an%20important%20tool%20across%20the%20entire%20life%0Acycle%20of%20civil%20engineering%20projects%2C%20and%20nudge%20researchers%20to%20think%20more%0Aholistically%20in%20their%20quest%20for%20the%20integration%20of%20DT%20for%20civil%20engineering%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02426v2&entry.124074799=Read"},
{"title": "Text-Region Matching for Multi-Label Image Recognition with Missing\n  Labels", "author": "Leilei Ma and Hongxing Xie and Lei Wang and Yanping Fu and Dengdi Sun and Haifeng Zhao", "abstract": "  Recently, large-scale visual language pre-trained (VLP) models have\ndemonstrated impressive performance across various downstream tasks. Motivated\nby these advancements, pioneering efforts have emerged in multi-label image\nrecognition with missing labels, leveraging VLP prompt-tuning technology.\nHowever, they usually cannot match text and vision features well, due to\ncomplicated semantics gaps and missing labels in a multi-label image. To tackle\nthis challenge, we propose \\textbf{T}ext-\\textbf{R}egion \\textbf{M}atching for\noptimizing \\textbf{M}ulti-\\textbf{L}abel prompt tuning, namely TRM-ML, a novel\nmethod for enhancing meaningful cross-modal matching. Compared to existing\nmethods, we advocate exploring the information of category-aware regions rather\nthan the entire image or pixels, which contributes to bridging the semantic gap\nbetween textual and visual representations in a one-to-one matching manner.\nConcurrently, we further introduce multimodal contrastive learning to narrow\nthe semantic gap between textual and visual modalities and establish\nintra-class and inter-class relationships. Additionally, to deal with missing\nlabels, we propose a multimodal category prototype that leverages intra- and\ninter-category semantic relationships to estimate unknown labels, facilitating\npseudo-label generation. Extensive experiments on the MS-COCO, PASCAL VOC,\nVisual Genome, NUS-WIDE, and CUB-200-211 benchmark datasets demonstrate that\nour proposed framework outperforms the state-of-the-art methods by a\nsignificant margin. Our code is available\nhere\\href{https://github.com/yu-gi-oh-leilei/TRM-ML}{\\raisebox{-1pt}{\\faGithub}}.\n", "link": "http://arxiv.org/abs/2407.18520v2", "date": "2024-08-07", "relevancy": 2.3142, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6328}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5421}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Region%20Matching%20for%20Multi-Label%20Image%20Recognition%20with%20Missing%0A%20%20Labels&body=Title%3A%20Text-Region%20Matching%20for%20Multi-Label%20Image%20Recognition%20with%20Missing%0A%20%20Labels%0AAuthor%3A%20Leilei%20Ma%20and%20Hongxing%20Xie%20and%20Lei%20Wang%20and%20Yanping%20Fu%20and%20Dengdi%20Sun%20and%20Haifeng%20Zhao%0AAbstract%3A%20%20%20Recently%2C%20large-scale%20visual%20language%20pre-trained%20%28VLP%29%20models%20have%0Ademonstrated%20impressive%20performance%20across%20various%20downstream%20tasks.%20Motivated%0Aby%20these%20advancements%2C%20pioneering%20efforts%20have%20emerged%20in%20multi-label%20image%0Arecognition%20with%20missing%20labels%2C%20leveraging%20VLP%20prompt-tuning%20technology.%0AHowever%2C%20they%20usually%20cannot%20match%20text%20and%20vision%20features%20well%2C%20due%20to%0Acomplicated%20semantics%20gaps%20and%20missing%20labels%20in%20a%20multi-label%20image.%20To%20tackle%0Athis%20challenge%2C%20we%20propose%20%5Ctextbf%7BT%7Dext-%5Ctextbf%7BR%7Degion%20%5Ctextbf%7BM%7Datching%20for%0Aoptimizing%20%5Ctextbf%7BM%7Dulti-%5Ctextbf%7BL%7Dabel%20prompt%20tuning%2C%20namely%20TRM-ML%2C%20a%20novel%0Amethod%20for%20enhancing%20meaningful%20cross-modal%20matching.%20Compared%20to%20existing%0Amethods%2C%20we%20advocate%20exploring%20the%20information%20of%20category-aware%20regions%20rather%0Athan%20the%20entire%20image%20or%20pixels%2C%20which%20contributes%20to%20bridging%20the%20semantic%20gap%0Abetween%20textual%20and%20visual%20representations%20in%20a%20one-to-one%20matching%20manner.%0AConcurrently%2C%20we%20further%20introduce%20multimodal%20contrastive%20learning%20to%20narrow%0Athe%20semantic%20gap%20between%20textual%20and%20visual%20modalities%20and%20establish%0Aintra-class%20and%20inter-class%20relationships.%20Additionally%2C%20to%20deal%20with%20missing%0Alabels%2C%20we%20propose%20a%20multimodal%20category%20prototype%20that%20leverages%20intra-%20and%0Ainter-category%20semantic%20relationships%20to%20estimate%20unknown%20labels%2C%20facilitating%0Apseudo-label%20generation.%20Extensive%20experiments%20on%20the%20MS-COCO%2C%20PASCAL%20VOC%2C%0AVisual%20Genome%2C%20NUS-WIDE%2C%20and%20CUB-200-211%20benchmark%20datasets%20demonstrate%20that%0Aour%20proposed%20framework%20outperforms%20the%20state-of-the-art%20methods%20by%20a%0Asignificant%20margin.%20Our%20code%20is%20available%0Ahere%5Chref%7Bhttps%3A//github.com/yu-gi-oh-leilei/TRM-ML%7D%7B%5Craisebox%7B-1pt%7D%7B%5CfaGithub%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18520v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Region%2520Matching%2520for%2520Multi-Label%2520Image%2520Recognition%2520with%2520Missing%250A%2520%2520Labels%26entry.906535625%3DLeilei%2520Ma%2520and%2520Hongxing%2520Xie%2520and%2520Lei%2520Wang%2520and%2520Yanping%2520Fu%2520and%2520Dengdi%2520Sun%2520and%2520Haifeng%2520Zhao%26entry.1292438233%3D%2520%2520Recently%252C%2520large-scale%2520visual%2520language%2520pre-trained%2520%2528VLP%2529%2520models%2520have%250Ademonstrated%2520impressive%2520performance%2520across%2520various%2520downstream%2520tasks.%2520Motivated%250Aby%2520these%2520advancements%252C%2520pioneering%2520efforts%2520have%2520emerged%2520in%2520multi-label%2520image%250Arecognition%2520with%2520missing%2520labels%252C%2520leveraging%2520VLP%2520prompt-tuning%2520technology.%250AHowever%252C%2520they%2520usually%2520cannot%2520match%2520text%2520and%2520vision%2520features%2520well%252C%2520due%2520to%250Acomplicated%2520semantics%2520gaps%2520and%2520missing%2520labels%2520in%2520a%2520multi-label%2520image.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520propose%2520%255Ctextbf%257BT%257Dext-%255Ctextbf%257BR%257Degion%2520%255Ctextbf%257BM%257Datching%2520for%250Aoptimizing%2520%255Ctextbf%257BM%257Dulti-%255Ctextbf%257BL%257Dabel%2520prompt%2520tuning%252C%2520namely%2520TRM-ML%252C%2520a%2520novel%250Amethod%2520for%2520enhancing%2520meaningful%2520cross-modal%2520matching.%2520Compared%2520to%2520existing%250Amethods%252C%2520we%2520advocate%2520exploring%2520the%2520information%2520of%2520category-aware%2520regions%2520rather%250Athan%2520the%2520entire%2520image%2520or%2520pixels%252C%2520which%2520contributes%2520to%2520bridging%2520the%2520semantic%2520gap%250Abetween%2520textual%2520and%2520visual%2520representations%2520in%2520a%2520one-to-one%2520matching%2520manner.%250AConcurrently%252C%2520we%2520further%2520introduce%2520multimodal%2520contrastive%2520learning%2520to%2520narrow%250Athe%2520semantic%2520gap%2520between%2520textual%2520and%2520visual%2520modalities%2520and%2520establish%250Aintra-class%2520and%2520inter-class%2520relationships.%2520Additionally%252C%2520to%2520deal%2520with%2520missing%250Alabels%252C%2520we%2520propose%2520a%2520multimodal%2520category%2520prototype%2520that%2520leverages%2520intra-%2520and%250Ainter-category%2520semantic%2520relationships%2520to%2520estimate%2520unknown%2520labels%252C%2520facilitating%250Apseudo-label%2520generation.%2520Extensive%2520experiments%2520on%2520the%2520MS-COCO%252C%2520PASCAL%2520VOC%252C%250AVisual%2520Genome%252C%2520NUS-WIDE%252C%2520and%2520CUB-200-211%2520benchmark%2520datasets%2520demonstrate%2520that%250Aour%2520proposed%2520framework%2520outperforms%2520the%2520state-of-the-art%2520methods%2520by%2520a%250Asignificant%2520margin.%2520Our%2520code%2520is%2520available%250Ahere%255Chref%257Bhttps%253A//github.com/yu-gi-oh-leilei/TRM-ML%257D%257B%255Craisebox%257B-1pt%257D%257B%255CfaGithub%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18520v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Region%20Matching%20for%20Multi-Label%20Image%20Recognition%20with%20Missing%0A%20%20Labels&entry.906535625=Leilei%20Ma%20and%20Hongxing%20Xie%20and%20Lei%20Wang%20and%20Yanping%20Fu%20and%20Dengdi%20Sun%20and%20Haifeng%20Zhao&entry.1292438233=%20%20Recently%2C%20large-scale%20visual%20language%20pre-trained%20%28VLP%29%20models%20have%0Ademonstrated%20impressive%20performance%20across%20various%20downstream%20tasks.%20Motivated%0Aby%20these%20advancements%2C%20pioneering%20efforts%20have%20emerged%20in%20multi-label%20image%0Arecognition%20with%20missing%20labels%2C%20leveraging%20VLP%20prompt-tuning%20technology.%0AHowever%2C%20they%20usually%20cannot%20match%20text%20and%20vision%20features%20well%2C%20due%20to%0Acomplicated%20semantics%20gaps%20and%20missing%20labels%20in%20a%20multi-label%20image.%20To%20tackle%0Athis%20challenge%2C%20we%20propose%20%5Ctextbf%7BT%7Dext-%5Ctextbf%7BR%7Degion%20%5Ctextbf%7BM%7Datching%20for%0Aoptimizing%20%5Ctextbf%7BM%7Dulti-%5Ctextbf%7BL%7Dabel%20prompt%20tuning%2C%20namely%20TRM-ML%2C%20a%20novel%0Amethod%20for%20enhancing%20meaningful%20cross-modal%20matching.%20Compared%20to%20existing%0Amethods%2C%20we%20advocate%20exploring%20the%20information%20of%20category-aware%20regions%20rather%0Athan%20the%20entire%20image%20or%20pixels%2C%20which%20contributes%20to%20bridging%20the%20semantic%20gap%0Abetween%20textual%20and%20visual%20representations%20in%20a%20one-to-one%20matching%20manner.%0AConcurrently%2C%20we%20further%20introduce%20multimodal%20contrastive%20learning%20to%20narrow%0Athe%20semantic%20gap%20between%20textual%20and%20visual%20modalities%20and%20establish%0Aintra-class%20and%20inter-class%20relationships.%20Additionally%2C%20to%20deal%20with%20missing%0Alabels%2C%20we%20propose%20a%20multimodal%20category%20prototype%20that%20leverages%20intra-%20and%0Ainter-category%20semantic%20relationships%20to%20estimate%20unknown%20labels%2C%20facilitating%0Apseudo-label%20generation.%20Extensive%20experiments%20on%20the%20MS-COCO%2C%20PASCAL%20VOC%2C%0AVisual%20Genome%2C%20NUS-WIDE%2C%20and%20CUB-200-211%20benchmark%20datasets%20demonstrate%20that%0Aour%20proposed%20framework%20outperforms%20the%20state-of-the-art%20methods%20by%20a%0Asignificant%20margin.%20Our%20code%20is%20available%0Ahere%5Chref%7Bhttps%3A//github.com/yu-gi-oh-leilei/TRM-ML%7D%7B%5Craisebox%7B-1pt%7D%7B%5CfaGithub%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18520v2&entry.124074799=Read"},
{"title": "Bi-Level Spatial and Channel-aware Transformer for Learned Image\n  Compression", "author": "Hamidreza Soltani and Erfan Ghasemi", "abstract": "  Recent advancements in learned image compression (LIC) methods have\ndemonstrated superior performance over traditional hand-crafted codecs. These\nlearning-based methods often employ convolutional neural networks (CNNs) or\nTransformer-based architectures. However, these nonlinear approaches frequently\noverlook the frequency characteristics of images, which limits their\ncompression efficiency. To address this issue, we propose a novel\nTransformer-based image compression method that enhances the transformation\nstage by considering frequency components within the feature map. Our method\nintegrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),\nwhere a spatial-based branch independently handles high and low frequencies at\nthe attention layer, and a Channel-aware Self-Attention (CaSA) module captures\ninformation across channels, significantly improving compression performance.\nAdditionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)\nwithin the Transformer block to enhance the extraction of diverse and rich\ninformation, which is crucial for effective compression. These innovations\ncollectively improve the transformation's ability to project data into a more\ndecorrelated latent space, thereby boosting overall compression efficiency.\nExperimental results demonstrate that our framework surpasses state-of-the-art\nLIC methods in rate-distortion performance.\n", "link": "http://arxiv.org/abs/2408.03842v1", "date": "2024-08-07", "relevancy": 2.3093, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5968}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bi-Level%20Spatial%20and%20Channel-aware%20Transformer%20for%20Learned%20Image%0A%20%20Compression&body=Title%3A%20Bi-Level%20Spatial%20and%20Channel-aware%20Transformer%20for%20Learned%20Image%0A%20%20Compression%0AAuthor%3A%20Hamidreza%20Soltani%20and%20Erfan%20Ghasemi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20learned%20image%20compression%20%28LIC%29%20methods%20have%0Ademonstrated%20superior%20performance%20over%20traditional%20hand-crafted%20codecs.%20These%0Alearning-based%20methods%20often%20employ%20convolutional%20neural%20networks%20%28CNNs%29%20or%0ATransformer-based%20architectures.%20However%2C%20these%20nonlinear%20approaches%20frequently%0Aoverlook%20the%20frequency%20characteristics%20of%20images%2C%20which%20limits%20their%0Acompression%20efficiency.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0ATransformer-based%20image%20compression%20method%20that%20enhances%20the%20transformation%0Astage%20by%20considering%20frequency%20components%20within%20the%20feature%20map.%20Our%20method%0Aintegrates%20a%20novel%20Hybrid%20Spatial-Channel%20Attention%20Transformer%20Block%20%28HSCATB%29%2C%0Awhere%20a%20spatial-based%20branch%20independently%20handles%20high%20and%20low%20frequencies%20at%0Athe%20attention%20layer%2C%20and%20a%20Channel-aware%20Self-Attention%20%28CaSA%29%20module%20captures%0Ainformation%20across%20channels%2C%20significantly%20improving%20compression%20performance.%0AAdditionally%2C%20we%20introduce%20a%20Mixed%20Local-Global%20Feed%20Forward%20Network%20%28MLGFFN%29%0Awithin%20the%20Transformer%20block%20to%20enhance%20the%20extraction%20of%20diverse%20and%20rich%0Ainformation%2C%20which%20is%20crucial%20for%20effective%20compression.%20These%20innovations%0Acollectively%20improve%20the%20transformation%27s%20ability%20to%20project%20data%20into%20a%20more%0Adecorrelated%20latent%20space%2C%20thereby%20boosting%20overall%20compression%20efficiency.%0AExperimental%20results%20demonstrate%20that%20our%20framework%20surpasses%20state-of-the-art%0ALIC%20methods%20in%20rate-distortion%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBi-Level%2520Spatial%2520and%2520Channel-aware%2520Transformer%2520for%2520Learned%2520Image%250A%2520%2520Compression%26entry.906535625%3DHamidreza%2520Soltani%2520and%2520Erfan%2520Ghasemi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520learned%2520image%2520compression%2520%2528LIC%2529%2520methods%2520have%250Ademonstrated%2520superior%2520performance%2520over%2520traditional%2520hand-crafted%2520codecs.%2520These%250Alearning-based%2520methods%2520often%2520employ%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520or%250ATransformer-based%2520architectures.%2520However%252C%2520these%2520nonlinear%2520approaches%2520frequently%250Aoverlook%2520the%2520frequency%2520characteristics%2520of%2520images%252C%2520which%2520limits%2520their%250Acompression%2520efficiency.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%250ATransformer-based%2520image%2520compression%2520method%2520that%2520enhances%2520the%2520transformation%250Astage%2520by%2520considering%2520frequency%2520components%2520within%2520the%2520feature%2520map.%2520Our%2520method%250Aintegrates%2520a%2520novel%2520Hybrid%2520Spatial-Channel%2520Attention%2520Transformer%2520Block%2520%2528HSCATB%2529%252C%250Awhere%2520a%2520spatial-based%2520branch%2520independently%2520handles%2520high%2520and%2520low%2520frequencies%2520at%250Athe%2520attention%2520layer%252C%2520and%2520a%2520Channel-aware%2520Self-Attention%2520%2528CaSA%2529%2520module%2520captures%250Ainformation%2520across%2520channels%252C%2520significantly%2520improving%2520compression%2520performance.%250AAdditionally%252C%2520we%2520introduce%2520a%2520Mixed%2520Local-Global%2520Feed%2520Forward%2520Network%2520%2528MLGFFN%2529%250Awithin%2520the%2520Transformer%2520block%2520to%2520enhance%2520the%2520extraction%2520of%2520diverse%2520and%2520rich%250Ainformation%252C%2520which%2520is%2520crucial%2520for%2520effective%2520compression.%2520These%2520innovations%250Acollectively%2520improve%2520the%2520transformation%2527s%2520ability%2520to%2520project%2520data%2520into%2520a%2520more%250Adecorrelated%2520latent%2520space%252C%2520thereby%2520boosting%2520overall%2520compression%2520efficiency.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520framework%2520surpasses%2520state-of-the-art%250ALIC%2520methods%2520in%2520rate-distortion%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-Level%20Spatial%20and%20Channel-aware%20Transformer%20for%20Learned%20Image%0A%20%20Compression&entry.906535625=Hamidreza%20Soltani%20and%20Erfan%20Ghasemi&entry.1292438233=%20%20Recent%20advancements%20in%20learned%20image%20compression%20%28LIC%29%20methods%20have%0Ademonstrated%20superior%20performance%20over%20traditional%20hand-crafted%20codecs.%20These%0Alearning-based%20methods%20often%20employ%20convolutional%20neural%20networks%20%28CNNs%29%20or%0ATransformer-based%20architectures.%20However%2C%20these%20nonlinear%20approaches%20frequently%0Aoverlook%20the%20frequency%20characteristics%20of%20images%2C%20which%20limits%20their%0Acompression%20efficiency.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0ATransformer-based%20image%20compression%20method%20that%20enhances%20the%20transformation%0Astage%20by%20considering%20frequency%20components%20within%20the%20feature%20map.%20Our%20method%0Aintegrates%20a%20novel%20Hybrid%20Spatial-Channel%20Attention%20Transformer%20Block%20%28HSCATB%29%2C%0Awhere%20a%20spatial-based%20branch%20independently%20handles%20high%20and%20low%20frequencies%20at%0Athe%20attention%20layer%2C%20and%20a%20Channel-aware%20Self-Attention%20%28CaSA%29%20module%20captures%0Ainformation%20across%20channels%2C%20significantly%20improving%20compression%20performance.%0AAdditionally%2C%20we%20introduce%20a%20Mixed%20Local-Global%20Feed%20Forward%20Network%20%28MLGFFN%29%0Awithin%20the%20Transformer%20block%20to%20enhance%20the%20extraction%20of%20diverse%20and%20rich%0Ainformation%2C%20which%20is%20crucial%20for%20effective%20compression.%20These%20innovations%0Acollectively%20improve%20the%20transformation%27s%20ability%20to%20project%20data%20into%20a%20more%0Adecorrelated%20latent%20space%2C%20thereby%20boosting%20overall%20compression%20efficiency.%0AExperimental%20results%20demonstrate%20that%20our%20framework%20surpasses%20state-of-the-art%0ALIC%20methods%20in%20rate-distortion%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03842v1&entry.124074799=Read"},
{"title": "Compression-Realized Deep Structural Network for Video Quality\n  Enhancement", "author": "Hanchi Sun and Xiaohong Liu and Xinyang Jiang and Yifei Shen and Dongsheng Li and Xiongkuo Min and Guangtao Zhai", "abstract": "  This paper focuses on the task of quality enhancement for compressed videos.\nAlthough deep network-based video restorers achieve impressive progress, most\nof the existing methods lack a structured design to optimally leverage the\npriors within compression codecs. Since the quality degradation of the video is\nprimarily induced by the compression algorithm, a new paradigm is urgently\nneeded for a more ``conscious'' process of quality enhancement. As a result, we\npropose the Compression-Realized Deep Structural Network (CRDS), introducing\nthree inductive biases aligned with the three primary processes in the classic\ncompression codec, merging the strengths of classical encoder architecture with\ndeep network capabilities. Inspired by the residual extraction and domain\ntransformation process in the codec, a pre-trained Latent Degradation Residual\nAuto-Encoder is proposed to transform video frames into a latent feature space,\nand the mutual neighborhood attention mechanism is integrated for precise\nmotion estimation and residual extraction. Furthermore, drawing inspiration\nfrom the quantization noise distribution of the codec, CRDS proposes a novel\nProgressive Denoising framework with intermediate supervision that decomposes\nthe quality enhancement into a series of simpler denoising sub-tasks.\nExperimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our\napproach surpasses state-of-the-art models. Codes are available at\nhttps://github.com/shc15522/CRDS.\n", "link": "http://arxiv.org/abs/2405.06342v2", "date": "2024-08-07", "relevancy": 2.3017, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6138}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5511}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compression-Realized%20Deep%20Structural%20Network%20for%20Video%20Quality%0A%20%20Enhancement&body=Title%3A%20Compression-Realized%20Deep%20Structural%20Network%20for%20Video%20Quality%0A%20%20Enhancement%0AAuthor%3A%20Hanchi%20Sun%20and%20Xiaohong%20Liu%20and%20Xinyang%20Jiang%20and%20Yifei%20Shen%20and%20Dongsheng%20Li%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20the%20task%20of%20quality%20enhancement%20for%20compressed%20videos.%0AAlthough%20deep%20network-based%20video%20restorers%20achieve%20impressive%20progress%2C%20most%0Aof%20the%20existing%20methods%20lack%20a%20structured%20design%20to%20optimally%20leverage%20the%0Apriors%20within%20compression%20codecs.%20Since%20the%20quality%20degradation%20of%20the%20video%20is%0Aprimarily%20induced%20by%20the%20compression%20algorithm%2C%20a%20new%20paradigm%20is%20urgently%0Aneeded%20for%20a%20more%20%60%60conscious%27%27%20process%20of%20quality%20enhancement.%20As%20a%20result%2C%20we%0Apropose%20the%20Compression-Realized%20Deep%20Structural%20Network%20%28CRDS%29%2C%20introducing%0Athree%20inductive%20biases%20aligned%20with%20the%20three%20primary%20processes%20in%20the%20classic%0Acompression%20codec%2C%20merging%20the%20strengths%20of%20classical%20encoder%20architecture%20with%0Adeep%20network%20capabilities.%20Inspired%20by%20the%20residual%20extraction%20and%20domain%0Atransformation%20process%20in%20the%20codec%2C%20a%20pre-trained%20Latent%20Degradation%20Residual%0AAuto-Encoder%20is%20proposed%20to%20transform%20video%20frames%20into%20a%20latent%20feature%20space%2C%0Aand%20the%20mutual%20neighborhood%20attention%20mechanism%20is%20integrated%20for%20precise%0Amotion%20estimation%20and%20residual%20extraction.%20Furthermore%2C%20drawing%20inspiration%0Afrom%20the%20quantization%20noise%20distribution%20of%20the%20codec%2C%20CRDS%20proposes%20a%20novel%0AProgressive%20Denoising%20framework%20with%20intermediate%20supervision%20that%20decomposes%0Athe%20quality%20enhancement%20into%20a%20series%20of%20simpler%20denoising%20sub-tasks.%0AExperimental%20results%20on%20datasets%20like%20LDV%202.0%20and%20MFQE%202.0%20indicate%20our%0Aapproach%20surpasses%20state-of-the-art%20models.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/shc15522/CRDS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompression-Realized%2520Deep%2520Structural%2520Network%2520for%2520Video%2520Quality%250A%2520%2520Enhancement%26entry.906535625%3DHanchi%2520Sun%2520and%2520Xiaohong%2520Liu%2520and%2520Xinyang%2520Jiang%2520and%2520Yifei%2520Shen%2520and%2520Dongsheng%2520Li%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520the%2520task%2520of%2520quality%2520enhancement%2520for%2520compressed%2520videos.%250AAlthough%2520deep%2520network-based%2520video%2520restorers%2520achieve%2520impressive%2520progress%252C%2520most%250Aof%2520the%2520existing%2520methods%2520lack%2520a%2520structured%2520design%2520to%2520optimally%2520leverage%2520the%250Apriors%2520within%2520compression%2520codecs.%2520Since%2520the%2520quality%2520degradation%2520of%2520the%2520video%2520is%250Aprimarily%2520induced%2520by%2520the%2520compression%2520algorithm%252C%2520a%2520new%2520paradigm%2520is%2520urgently%250Aneeded%2520for%2520a%2520more%2520%2560%2560conscious%2527%2527%2520process%2520of%2520quality%2520enhancement.%2520As%2520a%2520result%252C%2520we%250Apropose%2520the%2520Compression-Realized%2520Deep%2520Structural%2520Network%2520%2528CRDS%2529%252C%2520introducing%250Athree%2520inductive%2520biases%2520aligned%2520with%2520the%2520three%2520primary%2520processes%2520in%2520the%2520classic%250Acompression%2520codec%252C%2520merging%2520the%2520strengths%2520of%2520classical%2520encoder%2520architecture%2520with%250Adeep%2520network%2520capabilities.%2520Inspired%2520by%2520the%2520residual%2520extraction%2520and%2520domain%250Atransformation%2520process%2520in%2520the%2520codec%252C%2520a%2520pre-trained%2520Latent%2520Degradation%2520Residual%250AAuto-Encoder%2520is%2520proposed%2520to%2520transform%2520video%2520frames%2520into%2520a%2520latent%2520feature%2520space%252C%250Aand%2520the%2520mutual%2520neighborhood%2520attention%2520mechanism%2520is%2520integrated%2520for%2520precise%250Amotion%2520estimation%2520and%2520residual%2520extraction.%2520Furthermore%252C%2520drawing%2520inspiration%250Afrom%2520the%2520quantization%2520noise%2520distribution%2520of%2520the%2520codec%252C%2520CRDS%2520proposes%2520a%2520novel%250AProgressive%2520Denoising%2520framework%2520with%2520intermediate%2520supervision%2520that%2520decomposes%250Athe%2520quality%2520enhancement%2520into%2520a%2520series%2520of%2520simpler%2520denoising%2520sub-tasks.%250AExperimental%2520results%2520on%2520datasets%2520like%2520LDV%25202.0%2520and%2520MFQE%25202.0%2520indicate%2520our%250Aapproach%2520surpasses%2520state-of-the-art%2520models.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/shc15522/CRDS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compression-Realized%20Deep%20Structural%20Network%20for%20Video%20Quality%0A%20%20Enhancement&entry.906535625=Hanchi%20Sun%20and%20Xiaohong%20Liu%20and%20Xinyang%20Jiang%20and%20Yifei%20Shen%20and%20Dongsheng%20Li%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20This%20paper%20focuses%20on%20the%20task%20of%20quality%20enhancement%20for%20compressed%20videos.%0AAlthough%20deep%20network-based%20video%20restorers%20achieve%20impressive%20progress%2C%20most%0Aof%20the%20existing%20methods%20lack%20a%20structured%20design%20to%20optimally%20leverage%20the%0Apriors%20within%20compression%20codecs.%20Since%20the%20quality%20degradation%20of%20the%20video%20is%0Aprimarily%20induced%20by%20the%20compression%20algorithm%2C%20a%20new%20paradigm%20is%20urgently%0Aneeded%20for%20a%20more%20%60%60conscious%27%27%20process%20of%20quality%20enhancement.%20As%20a%20result%2C%20we%0Apropose%20the%20Compression-Realized%20Deep%20Structural%20Network%20%28CRDS%29%2C%20introducing%0Athree%20inductive%20biases%20aligned%20with%20the%20three%20primary%20processes%20in%20the%20classic%0Acompression%20codec%2C%20merging%20the%20strengths%20of%20classical%20encoder%20architecture%20with%0Adeep%20network%20capabilities.%20Inspired%20by%20the%20residual%20extraction%20and%20domain%0Atransformation%20process%20in%20the%20codec%2C%20a%20pre-trained%20Latent%20Degradation%20Residual%0AAuto-Encoder%20is%20proposed%20to%20transform%20video%20frames%20into%20a%20latent%20feature%20space%2C%0Aand%20the%20mutual%20neighborhood%20attention%20mechanism%20is%20integrated%20for%20precise%0Amotion%20estimation%20and%20residual%20extraction.%20Furthermore%2C%20drawing%20inspiration%0Afrom%20the%20quantization%20noise%20distribution%20of%20the%20codec%2C%20CRDS%20proposes%20a%20novel%0AProgressive%20Denoising%20framework%20with%20intermediate%20supervision%20that%20decomposes%0Athe%20quality%20enhancement%20into%20a%20series%20of%20simpler%20denoising%20sub-tasks.%0AExperimental%20results%20on%20datasets%20like%20LDV%202.0%20and%20MFQE%202.0%20indicate%20our%0Aapproach%20surpasses%20state-of-the-art%20models.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/shc15522/CRDS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06342v2&entry.124074799=Read"},
{"title": "Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding\n  Remote Smartphone-based Consultation", "author": "Dhruv Srikanth and Jayang Gurung and N Satya Deepika and Vineet Joshi and Lopamudra Giri and Pravin Vaddavalli and Soumya Jana", "abstract": "  Blindness and other eye diseases are a global health concern, particularly in\nlow- and middle-income countries like India. In this regard, during the\nCOVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi\nattachment for smartphone-based eye imaging gained in use. However, quality of\nuser-captured image often remained inadequate, requiring clinician vetting and\ndelays. In this backdrop, we propose an AI-based quality assessment system with\ninstant feedback mimicking clinicians' judgments and tested on patient-captured\nimages. Dividing the complex problem hierarchically, here we tackle a\nnontrivial part, and demonstrate a proof of the concept.\n", "link": "http://arxiv.org/abs/2402.07118v2", "date": "2024-08-07", "relevancy": 2.2886, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4731}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4507}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Generation%20Teleophthalmology%3A%20AI-enabled%20Quality%20Assessment%20Aiding%0A%20%20Remote%20Smartphone-based%20Consultation&body=Title%3A%20Next-Generation%20Teleophthalmology%3A%20AI-enabled%20Quality%20Assessment%20Aiding%0A%20%20Remote%20Smartphone-based%20Consultation%0AAuthor%3A%20Dhruv%20Srikanth%20and%20Jayang%20Gurung%20and%20N%20Satya%20Deepika%20and%20Vineet%20Joshi%20and%20Lopamudra%20Giri%20and%20Pravin%20Vaddavalli%20and%20Soumya%20Jana%0AAbstract%3A%20%20%20Blindness%20and%20other%20eye%20diseases%20are%20a%20global%20health%20concern%2C%20particularly%20in%0Alow-%20and%20middle-income%20countries%20like%20India.%20In%20this%20regard%2C%20during%20the%0ACOVID-19%20pandemic%2C%20teleophthalmology%20became%20a%20lifeline%2C%20and%20the%20Grabi%0Aattachment%20for%20smartphone-based%20eye%20imaging%20gained%20in%20use.%20However%2C%20quality%20of%0Auser-captured%20image%20often%20remained%20inadequate%2C%20requiring%20clinician%20vetting%20and%0Adelays.%20In%20this%20backdrop%2C%20we%20propose%20an%20AI-based%20quality%20assessment%20system%20with%0Ainstant%20feedback%20mimicking%20clinicians%27%20judgments%20and%20tested%20on%20patient-captured%0Aimages.%20Dividing%20the%20complex%20problem%20hierarchically%2C%20here%20we%20tackle%20a%0Anontrivial%20part%2C%20and%20demonstrate%20a%20proof%20of%20the%20concept.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Generation%2520Teleophthalmology%253A%2520AI-enabled%2520Quality%2520Assessment%2520Aiding%250A%2520%2520Remote%2520Smartphone-based%2520Consultation%26entry.906535625%3DDhruv%2520Srikanth%2520and%2520Jayang%2520Gurung%2520and%2520N%2520Satya%2520Deepika%2520and%2520Vineet%2520Joshi%2520and%2520Lopamudra%2520Giri%2520and%2520Pravin%2520Vaddavalli%2520and%2520Soumya%2520Jana%26entry.1292438233%3D%2520%2520Blindness%2520and%2520other%2520eye%2520diseases%2520are%2520a%2520global%2520health%2520concern%252C%2520particularly%2520in%250Alow-%2520and%2520middle-income%2520countries%2520like%2520India.%2520In%2520this%2520regard%252C%2520during%2520the%250ACOVID-19%2520pandemic%252C%2520teleophthalmology%2520became%2520a%2520lifeline%252C%2520and%2520the%2520Grabi%250Aattachment%2520for%2520smartphone-based%2520eye%2520imaging%2520gained%2520in%2520use.%2520However%252C%2520quality%2520of%250Auser-captured%2520image%2520often%2520remained%2520inadequate%252C%2520requiring%2520clinician%2520vetting%2520and%250Adelays.%2520In%2520this%2520backdrop%252C%2520we%2520propose%2520an%2520AI-based%2520quality%2520assessment%2520system%2520with%250Ainstant%2520feedback%2520mimicking%2520clinicians%2527%2520judgments%2520and%2520tested%2520on%2520patient-captured%250Aimages.%2520Dividing%2520the%2520complex%2520problem%2520hierarchically%252C%2520here%2520we%2520tackle%2520a%250Anontrivial%2520part%252C%2520and%2520demonstrate%2520a%2520proof%2520of%2520the%2520concept.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Generation%20Teleophthalmology%3A%20AI-enabled%20Quality%20Assessment%20Aiding%0A%20%20Remote%20Smartphone-based%20Consultation&entry.906535625=Dhruv%20Srikanth%20and%20Jayang%20Gurung%20and%20N%20Satya%20Deepika%20and%20Vineet%20Joshi%20and%20Lopamudra%20Giri%20and%20Pravin%20Vaddavalli%20and%20Soumya%20Jana&entry.1292438233=%20%20Blindness%20and%20other%20eye%20diseases%20are%20a%20global%20health%20concern%2C%20particularly%20in%0Alow-%20and%20middle-income%20countries%20like%20India.%20In%20this%20regard%2C%20during%20the%0ACOVID-19%20pandemic%2C%20teleophthalmology%20became%20a%20lifeline%2C%20and%20the%20Grabi%0Aattachment%20for%20smartphone-based%20eye%20imaging%20gained%20in%20use.%20However%2C%20quality%20of%0Auser-captured%20image%20often%20remained%20inadequate%2C%20requiring%20clinician%20vetting%20and%0Adelays.%20In%20this%20backdrop%2C%20we%20propose%20an%20AI-based%20quality%20assessment%20system%20with%0Ainstant%20feedback%20mimicking%20clinicians%27%20judgments%20and%20tested%20on%20patient-captured%0Aimages.%20Dividing%20the%20complex%20problem%20hierarchically%2C%20here%20we%20tackle%20a%0Anontrivial%20part%2C%20and%20demonstrate%20a%20proof%20of%20the%20concept.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07118v2&entry.124074799=Read"},
{"title": "Driving Animatronic Robot Facial Expression From Speech", "author": "Boren Li and Hang Li and Hangxin Liu", "abstract": "  Animatronic robots hold the promise of enabling natural human-robot\ninteraction through lifelike facial expressions. However, generating realistic,\nspeech-synchronized robot expressions poses significant challenges due to the\ncomplexities of facial biomechanics and the need for responsive motion\nsynthesis. This paper introduces a novel, skinning-centric approach to drive\nanimatronic robot facial expressions from speech input. At its core, the\nproposed approach employs linear blend skinning (LBS) as a unifying\nrepresentation, guiding innovations in both embodiment design and motion\nsynthesis. LBS informs the actuation topology, facilitates human expression\nretargeting, and enables efficient speech-driven facial motion generation. This\napproach demonstrates the capability to produce highly realistic facial\nexpressions on an animatronic face in real-time at over 4000 fps on a single\nNvidia RTX 4090, significantly advancing robots' ability to replicate nuanced\nhuman expressions for natural interaction. To foster further research and\ndevelopment in this field, the code has been made publicly available at:\n\\url{https://github.com/library87/OpenRoboExp}.\n", "link": "http://arxiv.org/abs/2403.12670v3", "date": "2024-08-07", "relevancy": 2.2672, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.568}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.568}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Driving%20Animatronic%20Robot%20Facial%20Expression%20From%20Speech&body=Title%3A%20Driving%20Animatronic%20Robot%20Facial%20Expression%20From%20Speech%0AAuthor%3A%20Boren%20Li%20and%20Hang%20Li%20and%20Hangxin%20Liu%0AAbstract%3A%20%20%20Animatronic%20robots%20hold%20the%20promise%20of%20enabling%20natural%20human-robot%0Ainteraction%20through%20lifelike%20facial%20expressions.%20However%2C%20generating%20realistic%2C%0Aspeech-synchronized%20robot%20expressions%20poses%20significant%20challenges%20due%20to%20the%0Acomplexities%20of%20facial%20biomechanics%20and%20the%20need%20for%20responsive%20motion%0Asynthesis.%20This%20paper%20introduces%20a%20novel%2C%20skinning-centric%20approach%20to%20drive%0Aanimatronic%20robot%20facial%20expressions%20from%20speech%20input.%20At%20its%20core%2C%20the%0Aproposed%20approach%20employs%20linear%20blend%20skinning%20%28LBS%29%20as%20a%20unifying%0Arepresentation%2C%20guiding%20innovations%20in%20both%20embodiment%20design%20and%20motion%0Asynthesis.%20LBS%20informs%20the%20actuation%20topology%2C%20facilitates%20human%20expression%0Aretargeting%2C%20and%20enables%20efficient%20speech-driven%20facial%20motion%20generation.%20This%0Aapproach%20demonstrates%20the%20capability%20to%20produce%20highly%20realistic%20facial%0Aexpressions%20on%20an%20animatronic%20face%20in%20real-time%20at%20over%204000%20fps%20on%20a%20single%0ANvidia%20RTX%204090%2C%20significantly%20advancing%20robots%27%20ability%20to%20replicate%20nuanced%0Ahuman%20expressions%20for%20natural%20interaction.%20To%20foster%20further%20research%20and%0Adevelopment%20in%20this%20field%2C%20the%20code%20has%20been%20made%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/library87/OpenRoboExp%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12670v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriving%2520Animatronic%2520Robot%2520Facial%2520Expression%2520From%2520Speech%26entry.906535625%3DBoren%2520Li%2520and%2520Hang%2520Li%2520and%2520Hangxin%2520Liu%26entry.1292438233%3D%2520%2520Animatronic%2520robots%2520hold%2520the%2520promise%2520of%2520enabling%2520natural%2520human-robot%250Ainteraction%2520through%2520lifelike%2520facial%2520expressions.%2520However%252C%2520generating%2520realistic%252C%250Aspeech-synchronized%2520robot%2520expressions%2520poses%2520significant%2520challenges%2520due%2520to%2520the%250Acomplexities%2520of%2520facial%2520biomechanics%2520and%2520the%2520need%2520for%2520responsive%2520motion%250Asynthesis.%2520This%2520paper%2520introduces%2520a%2520novel%252C%2520skinning-centric%2520approach%2520to%2520drive%250Aanimatronic%2520robot%2520facial%2520expressions%2520from%2520speech%2520input.%2520At%2520its%2520core%252C%2520the%250Aproposed%2520approach%2520employs%2520linear%2520blend%2520skinning%2520%2528LBS%2529%2520as%2520a%2520unifying%250Arepresentation%252C%2520guiding%2520innovations%2520in%2520both%2520embodiment%2520design%2520and%2520motion%250Asynthesis.%2520LBS%2520informs%2520the%2520actuation%2520topology%252C%2520facilitates%2520human%2520expression%250Aretargeting%252C%2520and%2520enables%2520efficient%2520speech-driven%2520facial%2520motion%2520generation.%2520This%250Aapproach%2520demonstrates%2520the%2520capability%2520to%2520produce%2520highly%2520realistic%2520facial%250Aexpressions%2520on%2520an%2520animatronic%2520face%2520in%2520real-time%2520at%2520over%25204000%2520fps%2520on%2520a%2520single%250ANvidia%2520RTX%25204090%252C%2520significantly%2520advancing%2520robots%2527%2520ability%2520to%2520replicate%2520nuanced%250Ahuman%2520expressions%2520for%2520natural%2520interaction.%2520To%2520foster%2520further%2520research%2520and%250Adevelopment%2520in%2520this%2520field%252C%2520the%2520code%2520has%2520been%2520made%2520publicly%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/library87/OpenRoboExp%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12670v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driving%20Animatronic%20Robot%20Facial%20Expression%20From%20Speech&entry.906535625=Boren%20Li%20and%20Hang%20Li%20and%20Hangxin%20Liu&entry.1292438233=%20%20Animatronic%20robots%20hold%20the%20promise%20of%20enabling%20natural%20human-robot%0Ainteraction%20through%20lifelike%20facial%20expressions.%20However%2C%20generating%20realistic%2C%0Aspeech-synchronized%20robot%20expressions%20poses%20significant%20challenges%20due%20to%20the%0Acomplexities%20of%20facial%20biomechanics%20and%20the%20need%20for%20responsive%20motion%0Asynthesis.%20This%20paper%20introduces%20a%20novel%2C%20skinning-centric%20approach%20to%20drive%0Aanimatronic%20robot%20facial%20expressions%20from%20speech%20input.%20At%20its%20core%2C%20the%0Aproposed%20approach%20employs%20linear%20blend%20skinning%20%28LBS%29%20as%20a%20unifying%0Arepresentation%2C%20guiding%20innovations%20in%20both%20embodiment%20design%20and%20motion%0Asynthesis.%20LBS%20informs%20the%20actuation%20topology%2C%20facilitates%20human%20expression%0Aretargeting%2C%20and%20enables%20efficient%20speech-driven%20facial%20motion%20generation.%20This%0Aapproach%20demonstrates%20the%20capability%20to%20produce%20highly%20realistic%20facial%0Aexpressions%20on%20an%20animatronic%20face%20in%20real-time%20at%20over%204000%20fps%20on%20a%20single%0ANvidia%20RTX%204090%2C%20significantly%20advancing%20robots%27%20ability%20to%20replicate%20nuanced%0Ahuman%20expressions%20for%20natural%20interaction.%20To%20foster%20further%20research%20and%0Adevelopment%20in%20this%20field%2C%20the%20code%20has%20been%20made%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/library87/OpenRoboExp%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12670v3&entry.124074799=Read"},
{"title": "CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications", "author": "Tianfang Zhang and Lei Li and Yang Zhou and Wentao Liu and Chen Qian and Xiangyang Ji", "abstract": "  Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we construct a novel additive\nsimilarity function following this paradigm and present an efficient\nimplementation named Convolutional Additive Token Mixer (CATM). This\nsimplification leads to a significant reduction in computational overhead. We\nevaluate CAS-ViT across a variety of vision tasks, including image\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Our experiments, conducted on GPUs, ONNX, and iPhones,\ndemonstrate that CAS-ViT achieves a competitive performance when compared to\nother state-of-the-art backbones, establishing it as a viable option for\nefficient mobile vision applications. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}\n", "link": "http://arxiv.org/abs/2408.03703v1", "date": "2024-08-07", "relevancy": 2.2658, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.587}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5589}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAS-ViT%3A%20Convolutional%20Additive%20Self-attention%20Vision%20Transformers%20for%0A%20%20Efficient%20Mobile%20Applications&body=Title%3A%20CAS-ViT%3A%20Convolutional%20Additive%20Self-attention%20Vision%20Transformers%20for%0A%20%20Efficient%20Mobile%20Applications%0AAuthor%3A%20Tianfang%20Zhang%20and%20Lei%20Li%20and%20Yang%20Zhou%20and%20Wentao%20Liu%20and%20Chen%20Qian%20and%20Xiangyang%20Ji%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20mark%20a%20revolutionary%20advance%20in%20neural%20networks%0Awith%20their%20token%20mixer%27s%20powerful%20global%20context%20capability.%20However%2C%20the%0Apairwise%20token%20affinity%20and%20complex%20matrix%20operations%20limit%20its%20deployment%20on%0Aresource-constrained%20scenarios%20and%20real-time%20applications%2C%20such%20as%20mobile%0Adevices%2C%20although%20considerable%20efforts%20have%20been%20made%20in%20previous%20works.%20In%0Athis%20paper%2C%20we%20introduce%20CAS-ViT%3A%20Convolutional%20Additive%20Self-attention%20Vision%0ATransformers%2C%20to%20achieve%20a%20balance%20between%20efficiency%20and%20performance%20in%20mobile%0Aapplications.%20Firstly%2C%20we%20argue%20that%20the%20capability%20of%20token%20mixers%20to%20obtain%0Aglobal%20contextual%20information%20hinges%20on%20multiple%20information%20interactions%2C%20such%0Aas%20spatial%20and%20channel%20domains.%20Subsequently%2C%20we%20construct%20a%20novel%20additive%0Asimilarity%20function%20following%20this%20paradigm%20and%20present%20an%20efficient%0Aimplementation%20named%20Convolutional%20Additive%20Token%20Mixer%20%28CATM%29.%20This%0Asimplification%20leads%20to%20a%20significant%20reduction%20in%20computational%20overhead.%20We%0Aevaluate%20CAS-ViT%20across%20a%20variety%20of%20vision%20tasks%2C%20including%20image%0Aclassification%2C%20object%20detection%2C%20instance%20segmentation%2C%20and%20semantic%0Asegmentation.%20Our%20experiments%2C%20conducted%20on%20GPUs%2C%20ONNX%2C%20and%20iPhones%2C%0Ademonstrate%20that%20CAS-ViT%20achieves%20a%20competitive%20performance%20when%20compared%20to%0Aother%20state-of-the-art%20backbones%2C%20establishing%20it%20as%20a%20viable%20option%20for%0Aefficient%20mobile%20vision%20applications.%20Our%20code%20and%20model%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Tianfang-Zhang/CAS-ViT%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAS-ViT%253A%2520Convolutional%2520Additive%2520Self-attention%2520Vision%2520Transformers%2520for%250A%2520%2520Efficient%2520Mobile%2520Applications%26entry.906535625%3DTianfang%2520Zhang%2520and%2520Lei%2520Li%2520and%2520Yang%2520Zhou%2520and%2520Wentao%2520Liu%2520and%2520Chen%2520Qian%2520and%2520Xiangyang%2520Ji%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520mark%2520a%2520revolutionary%2520advance%2520in%2520neural%2520networks%250Awith%2520their%2520token%2520mixer%2527s%2520powerful%2520global%2520context%2520capability.%2520However%252C%2520the%250Apairwise%2520token%2520affinity%2520and%2520complex%2520matrix%2520operations%2520limit%2520its%2520deployment%2520on%250Aresource-constrained%2520scenarios%2520and%2520real-time%2520applications%252C%2520such%2520as%2520mobile%250Adevices%252C%2520although%2520considerable%2520efforts%2520have%2520been%2520made%2520in%2520previous%2520works.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520CAS-ViT%253A%2520Convolutional%2520Additive%2520Self-attention%2520Vision%250ATransformers%252C%2520to%2520achieve%2520a%2520balance%2520between%2520efficiency%2520and%2520performance%2520in%2520mobile%250Aapplications.%2520Firstly%252C%2520we%2520argue%2520that%2520the%2520capability%2520of%2520token%2520mixers%2520to%2520obtain%250Aglobal%2520contextual%2520information%2520hinges%2520on%2520multiple%2520information%2520interactions%252C%2520such%250Aas%2520spatial%2520and%2520channel%2520domains.%2520Subsequently%252C%2520we%2520construct%2520a%2520novel%2520additive%250Asimilarity%2520function%2520following%2520this%2520paradigm%2520and%2520present%2520an%2520efficient%250Aimplementation%2520named%2520Convolutional%2520Additive%2520Token%2520Mixer%2520%2528CATM%2529.%2520This%250Asimplification%2520leads%2520to%2520a%2520significant%2520reduction%2520in%2520computational%2520overhead.%2520We%250Aevaluate%2520CAS-ViT%2520across%2520a%2520variety%2520of%2520vision%2520tasks%252C%2520including%2520image%250Aclassification%252C%2520object%2520detection%252C%2520instance%2520segmentation%252C%2520and%2520semantic%250Asegmentation.%2520Our%2520experiments%252C%2520conducted%2520on%2520GPUs%252C%2520ONNX%252C%2520and%2520iPhones%252C%250Ademonstrate%2520that%2520CAS-ViT%2520achieves%2520a%2520competitive%2520performance%2520when%2520compared%2520to%250Aother%2520state-of-the-art%2520backbones%252C%2520establishing%2520it%2520as%2520a%2520viable%2520option%2520for%250Aefficient%2520mobile%2520vision%2520applications.%2520Our%2520code%2520and%2520model%2520are%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/Tianfang-Zhang/CAS-ViT%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAS-ViT%3A%20Convolutional%20Additive%20Self-attention%20Vision%20Transformers%20for%0A%20%20Efficient%20Mobile%20Applications&entry.906535625=Tianfang%20Zhang%20and%20Lei%20Li%20and%20Yang%20Zhou%20and%20Wentao%20Liu%20and%20Chen%20Qian%20and%20Xiangyang%20Ji&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20mark%20a%20revolutionary%20advance%20in%20neural%20networks%0Awith%20their%20token%20mixer%27s%20powerful%20global%20context%20capability.%20However%2C%20the%0Apairwise%20token%20affinity%20and%20complex%20matrix%20operations%20limit%20its%20deployment%20on%0Aresource-constrained%20scenarios%20and%20real-time%20applications%2C%20such%20as%20mobile%0Adevices%2C%20although%20considerable%20efforts%20have%20been%20made%20in%20previous%20works.%20In%0Athis%20paper%2C%20we%20introduce%20CAS-ViT%3A%20Convolutional%20Additive%20Self-attention%20Vision%0ATransformers%2C%20to%20achieve%20a%20balance%20between%20efficiency%20and%20performance%20in%20mobile%0Aapplications.%20Firstly%2C%20we%20argue%20that%20the%20capability%20of%20token%20mixers%20to%20obtain%0Aglobal%20contextual%20information%20hinges%20on%20multiple%20information%20interactions%2C%20such%0Aas%20spatial%20and%20channel%20domains.%20Subsequently%2C%20we%20construct%20a%20novel%20additive%0Asimilarity%20function%20following%20this%20paradigm%20and%20present%20an%20efficient%0Aimplementation%20named%20Convolutional%20Additive%20Token%20Mixer%20%28CATM%29.%20This%0Asimplification%20leads%20to%20a%20significant%20reduction%20in%20computational%20overhead.%20We%0Aevaluate%20CAS-ViT%20across%20a%20variety%20of%20vision%20tasks%2C%20including%20image%0Aclassification%2C%20object%20detection%2C%20instance%20segmentation%2C%20and%20semantic%0Asegmentation.%20Our%20experiments%2C%20conducted%20on%20GPUs%2C%20ONNX%2C%20and%20iPhones%2C%0Ademonstrate%20that%20CAS-ViT%20achieves%20a%20competitive%20performance%20when%20compared%20to%0Aother%20state-of-the-art%20backbones%2C%20establishing%20it%20as%20a%20viable%20option%20for%0Aefficient%20mobile%20vision%20applications.%20Our%20code%20and%20model%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Tianfang-Zhang/CAS-ViT%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03703v1&entry.124074799=Read"},
{"title": "New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models", "author": "Wenxuan Wang and Haonan Bai and Jen-tse Huang and Yuxuan Wan and Youliang Yuan and Haoyi Qiu and Nanyun Peng and Michael R. Lyu", "abstract": "  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.\n", "link": "http://arxiv.org/abs/2401.00763v2", "date": "2024-08-07", "relevancy": 2.2373, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5853}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5729}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20New%20Job%2C%20New%20Gender%3F%20Measuring%20the%20Social%20Bias%20in%20Image%20Generation%0A%20%20Models&body=Title%3A%20New%20Job%2C%20New%20Gender%3F%20Measuring%20the%20Social%20Bias%20in%20Image%20Generation%0A%20%20Models%0AAuthor%3A%20Wenxuan%20Wang%20and%20Haonan%20Bai%20and%20Jen-tse%20Huang%20and%20Yuxuan%20Wan%20and%20Youliang%20Yuan%20and%20Haoyi%20Qiu%20and%20Nanyun%20Peng%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Image%20generation%20models%20can%20generate%20or%20edit%20images%20from%20a%20given%20text.%20Recent%0Aadvancements%20in%20image%20generation%20technology%2C%20exemplified%20by%20DALL-E%20and%0AMidjourney%2C%20have%20been%20groundbreaking.%20These%20advanced%20models%2C%20despite%20their%0Aimpressive%20capabilities%2C%20are%20often%20trained%20on%20massive%20Internet%20datasets%2C%20making%0Athem%20susceptible%20to%20generating%20content%20that%20perpetuates%20social%20stereotypes%20and%0Abiases%2C%20which%20can%20lead%20to%20severe%20consequences.%20Prior%20research%20on%20assessing%20bias%0Awithin%20image%20generation%20models%20suffers%20from%20several%20shortcomings%2C%20including%0Alimited%20accuracy%2C%20reliance%20on%20extensive%20human%20labor%2C%20and%20lack%20of%20comprehensive%0Aanalysis.%20In%20this%20paper%2C%20we%20propose%20BiasPainter%2C%20a%20novel%20evaluation%20framework%0Athat%20can%20accurately%2C%20automatically%20and%20comprehensively%20trigger%20social%20bias%20in%0Aimage%20generation%20models.%20BiasPainter%20uses%20a%20diverse%20range%20of%20seed%20images%20of%0Aindividuals%20and%20prompts%20the%20image%20generation%20models%20to%20edit%20these%20images%20using%0Agender%2C%20race%2C%20and%20age-neutral%20queries.%20These%20queries%20span%2062%20professions%2C%2039%0Aactivities%2C%2057%20types%20of%20objects%2C%20and%2070%20personality%20traits.%20The%20framework%20then%0Acompares%20the%20edited%20images%20to%20the%20original%20seed%20images%2C%20focusing%20on%20the%0Asignificant%20changes%20related%20to%20gender%2C%20race%2C%20and%20age.%20BiasPainter%20adopts%20a%20key%0Ainsight%20that%20these%20characteristics%20should%20not%20be%20modified%20when%20subjected%20to%0Aneutral%20prompts.%20Built%20upon%20this%20design%2C%20BiasPainter%20can%20trigger%20the%20social%0Abias%20and%20evaluate%20the%20fairness%20of%20image%20generation%20models.%20We%20use%20BiasPainter%0Ato%20evaluate%20six%20widely-used%20image%20generation%20models%2C%20such%20as%20stable%20diffusion%0Aand%20Midjourney.%20Experimental%20results%20show%20that%20BiasPainter%20can%20successfully%0Atrigger%20social%20bias%20in%20image%20generation%20models.%20According%20to%20our%20human%0Aevaluation%2C%20BiasPainter%20can%20achieve%2090.8%25%20accuracy%20on%20automatic%20bias%20detection%2C%0Awhich%20is%20significantly%20higher%20than%20the%20results%20reported%20in%20previous%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00763v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNew%2520Job%252C%2520New%2520Gender%253F%2520Measuring%2520the%2520Social%2520Bias%2520in%2520Image%2520Generation%250A%2520%2520Models%26entry.906535625%3DWenxuan%2520Wang%2520and%2520Haonan%2520Bai%2520and%2520Jen-tse%2520Huang%2520and%2520Yuxuan%2520Wan%2520and%2520Youliang%2520Yuan%2520and%2520Haoyi%2520Qiu%2520and%2520Nanyun%2520Peng%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Image%2520generation%2520models%2520can%2520generate%2520or%2520edit%2520images%2520from%2520a%2520given%2520text.%2520Recent%250Aadvancements%2520in%2520image%2520generation%2520technology%252C%2520exemplified%2520by%2520DALL-E%2520and%250AMidjourney%252C%2520have%2520been%2520groundbreaking.%2520These%2520advanced%2520models%252C%2520despite%2520their%250Aimpressive%2520capabilities%252C%2520are%2520often%2520trained%2520on%2520massive%2520Internet%2520datasets%252C%2520making%250Athem%2520susceptible%2520to%2520generating%2520content%2520that%2520perpetuates%2520social%2520stereotypes%2520and%250Abiases%252C%2520which%2520can%2520lead%2520to%2520severe%2520consequences.%2520Prior%2520research%2520on%2520assessing%2520bias%250Awithin%2520image%2520generation%2520models%2520suffers%2520from%2520several%2520shortcomings%252C%2520including%250Alimited%2520accuracy%252C%2520reliance%2520on%2520extensive%2520human%2520labor%252C%2520and%2520lack%2520of%2520comprehensive%250Aanalysis.%2520In%2520this%2520paper%252C%2520we%2520propose%2520BiasPainter%252C%2520a%2520novel%2520evaluation%2520framework%250Athat%2520can%2520accurately%252C%2520automatically%2520and%2520comprehensively%2520trigger%2520social%2520bias%2520in%250Aimage%2520generation%2520models.%2520BiasPainter%2520uses%2520a%2520diverse%2520range%2520of%2520seed%2520images%2520of%250Aindividuals%2520and%2520prompts%2520the%2520image%2520generation%2520models%2520to%2520edit%2520these%2520images%2520using%250Agender%252C%2520race%252C%2520and%2520age-neutral%2520queries.%2520These%2520queries%2520span%252062%2520professions%252C%252039%250Aactivities%252C%252057%2520types%2520of%2520objects%252C%2520and%252070%2520personality%2520traits.%2520The%2520framework%2520then%250Acompares%2520the%2520edited%2520images%2520to%2520the%2520original%2520seed%2520images%252C%2520focusing%2520on%2520the%250Asignificant%2520changes%2520related%2520to%2520gender%252C%2520race%252C%2520and%2520age.%2520BiasPainter%2520adopts%2520a%2520key%250Ainsight%2520that%2520these%2520characteristics%2520should%2520not%2520be%2520modified%2520when%2520subjected%2520to%250Aneutral%2520prompts.%2520Built%2520upon%2520this%2520design%252C%2520BiasPainter%2520can%2520trigger%2520the%2520social%250Abias%2520and%2520evaluate%2520the%2520fairness%2520of%2520image%2520generation%2520models.%2520We%2520use%2520BiasPainter%250Ato%2520evaluate%2520six%2520widely-used%2520image%2520generation%2520models%252C%2520such%2520as%2520stable%2520diffusion%250Aand%2520Midjourney.%2520Experimental%2520results%2520show%2520that%2520BiasPainter%2520can%2520successfully%250Atrigger%2520social%2520bias%2520in%2520image%2520generation%2520models.%2520According%2520to%2520our%2520human%250Aevaluation%252C%2520BiasPainter%2520can%2520achieve%252090.8%2525%2520accuracy%2520on%2520automatic%2520bias%2520detection%252C%250Awhich%2520is%2520significantly%2520higher%2520than%2520the%2520results%2520reported%2520in%2520previous%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00763v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Job%2C%20New%20Gender%3F%20Measuring%20the%20Social%20Bias%20in%20Image%20Generation%0A%20%20Models&entry.906535625=Wenxuan%20Wang%20and%20Haonan%20Bai%20and%20Jen-tse%20Huang%20and%20Yuxuan%20Wan%20and%20Youliang%20Yuan%20and%20Haoyi%20Qiu%20and%20Nanyun%20Peng%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Image%20generation%20models%20can%20generate%20or%20edit%20images%20from%20a%20given%20text.%20Recent%0Aadvancements%20in%20image%20generation%20technology%2C%20exemplified%20by%20DALL-E%20and%0AMidjourney%2C%20have%20been%20groundbreaking.%20These%20advanced%20models%2C%20despite%20their%0Aimpressive%20capabilities%2C%20are%20often%20trained%20on%20massive%20Internet%20datasets%2C%20making%0Athem%20susceptible%20to%20generating%20content%20that%20perpetuates%20social%20stereotypes%20and%0Abiases%2C%20which%20can%20lead%20to%20severe%20consequences.%20Prior%20research%20on%20assessing%20bias%0Awithin%20image%20generation%20models%20suffers%20from%20several%20shortcomings%2C%20including%0Alimited%20accuracy%2C%20reliance%20on%20extensive%20human%20labor%2C%20and%20lack%20of%20comprehensive%0Aanalysis.%20In%20this%20paper%2C%20we%20propose%20BiasPainter%2C%20a%20novel%20evaluation%20framework%0Athat%20can%20accurately%2C%20automatically%20and%20comprehensively%20trigger%20social%20bias%20in%0Aimage%20generation%20models.%20BiasPainter%20uses%20a%20diverse%20range%20of%20seed%20images%20of%0Aindividuals%20and%20prompts%20the%20image%20generation%20models%20to%20edit%20these%20images%20using%0Agender%2C%20race%2C%20and%20age-neutral%20queries.%20These%20queries%20span%2062%20professions%2C%2039%0Aactivities%2C%2057%20types%20of%20objects%2C%20and%2070%20personality%20traits.%20The%20framework%20then%0Acompares%20the%20edited%20images%20to%20the%20original%20seed%20images%2C%20focusing%20on%20the%0Asignificant%20changes%20related%20to%20gender%2C%20race%2C%20and%20age.%20BiasPainter%20adopts%20a%20key%0Ainsight%20that%20these%20characteristics%20should%20not%20be%20modified%20when%20subjected%20to%0Aneutral%20prompts.%20Built%20upon%20this%20design%2C%20BiasPainter%20can%20trigger%20the%20social%0Abias%20and%20evaluate%20the%20fairness%20of%20image%20generation%20models.%20We%20use%20BiasPainter%0Ato%20evaluate%20six%20widely-used%20image%20generation%20models%2C%20such%20as%20stable%20diffusion%0Aand%20Midjourney.%20Experimental%20results%20show%20that%20BiasPainter%20can%20successfully%0Atrigger%20social%20bias%20in%20image%20generation%20models.%20According%20to%20our%20human%0Aevaluation%2C%20BiasPainter%20can%20achieve%2090.8%25%20accuracy%20on%20automatic%20bias%20detection%2C%0Awhich%20is%20significantly%20higher%20than%20the%20results%20reported%20in%20previous%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00763v2&entry.124074799=Read"},
{"title": "Improving Composed Image Retrieval via Contrastive Learning with Scaling\n  Positives and Negatives", "author": "Zhangchi Feng and Richong Zhang and Zhijie Nie", "abstract": "  The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethod also performs well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario. Our code and data are released\nat https://github.com/BUAADreamer/SPN4CIR.\n", "link": "http://arxiv.org/abs/2404.11317v2", "date": "2024-08-07", "relevancy": 2.2312, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5576}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Composed%20Image%20Retrieval%20via%20Contrastive%20Learning%20with%20Scaling%0A%20%20Positives%20and%20Negatives&body=Title%3A%20Improving%20Composed%20Image%20Retrieval%20via%20Contrastive%20Learning%20with%20Scaling%0A%20%20Positives%20and%20Negatives%0AAuthor%3A%20Zhangchi%20Feng%20and%20Richong%20Zhang%20and%20Zhijie%20Nie%0AAbstract%3A%20%20%20The%20Composed%20Image%20Retrieval%20%28CIR%29%20task%20aims%20to%20retrieve%20target%20images%20using%0Aa%20composed%20query%20consisting%20of%20a%20reference%20image%20and%20a%20modified%20text.%20Advanced%0Amethods%20often%20utilize%20contrastive%20learning%20as%20the%20optimization%20objective%2C%20which%0Abenefits%20from%20adequate%20positive%20and%20negative%20examples.%20However%2C%20the%20triplet%20for%0ACIR%20incurs%20high%20manual%20annotation%20costs%2C%20resulting%20in%20limited%20positive%0Aexamples.%20Furthermore%2C%20existing%20methods%20commonly%20use%20in-batch%20negative%0Asampling%2C%20which%20reduces%20the%20negative%20number%20available%20for%20the%20model.%20To%20address%0Athe%20problem%20of%20lack%20of%20positives%2C%20we%20propose%20a%20data%20generation%20method%20by%0Aleveraging%20a%20multi-modal%20large%20language%20model%20to%20construct%20triplets%20for%20CIR.%20To%0Aintroduce%20more%20negatives%20during%20fine-tuning%2C%20we%20design%20a%20two-stage%20fine-tuning%0Aframework%20for%20CIR%2C%20whose%20second%20stage%20introduces%20plenty%20of%20static%0Arepresentations%20of%20negatives%20to%20optimize%20the%20representation%20space%20rapidly.%20The%0Aabove%20two%20improvements%20can%20be%20effectively%20stacked%20and%20designed%20to%20be%0Aplug-and-play%2C%20easily%20applied%20to%20existing%20CIR%20models%20without%20changing%20their%0Aoriginal%20architectures.%20Extensive%20experiments%20and%20ablation%20analysis%20demonstrate%0Athat%20our%20method%20effectively%20scales%20positives%20and%20negatives%20and%20achieves%0Astate-of-the-art%20results%20on%20both%20FashionIQ%20and%20CIRR%20datasets.%20In%20addition%2C%20our%0Amethod%20also%20performs%20well%20in%20zero-shot%20composed%20image%20retrieval%2C%20providing%20a%0Anew%20CIR%20solution%20for%20the%20low-resources%20scenario.%20Our%20code%20and%20data%20are%20released%0Aat%20https%3A//github.com/BUAADreamer/SPN4CIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Composed%2520Image%2520Retrieval%2520via%2520Contrastive%2520Learning%2520with%2520Scaling%250A%2520%2520Positives%2520and%2520Negatives%26entry.906535625%3DZhangchi%2520Feng%2520and%2520Richong%2520Zhang%2520and%2520Zhijie%2520Nie%26entry.1292438233%3D%2520%2520The%2520Composed%2520Image%2520Retrieval%2520%2528CIR%2529%2520task%2520aims%2520to%2520retrieve%2520target%2520images%2520using%250Aa%2520composed%2520query%2520consisting%2520of%2520a%2520reference%2520image%2520and%2520a%2520modified%2520text.%2520Advanced%250Amethods%2520often%2520utilize%2520contrastive%2520learning%2520as%2520the%2520optimization%2520objective%252C%2520which%250Abenefits%2520from%2520adequate%2520positive%2520and%2520negative%2520examples.%2520However%252C%2520the%2520triplet%2520for%250ACIR%2520incurs%2520high%2520manual%2520annotation%2520costs%252C%2520resulting%2520in%2520limited%2520positive%250Aexamples.%2520Furthermore%252C%2520existing%2520methods%2520commonly%2520use%2520in-batch%2520negative%250Asampling%252C%2520which%2520reduces%2520the%2520negative%2520number%2520available%2520for%2520the%2520model.%2520To%2520address%250Athe%2520problem%2520of%2520lack%2520of%2520positives%252C%2520we%2520propose%2520a%2520data%2520generation%2520method%2520by%250Aleveraging%2520a%2520multi-modal%2520large%2520language%2520model%2520to%2520construct%2520triplets%2520for%2520CIR.%2520To%250Aintroduce%2520more%2520negatives%2520during%2520fine-tuning%252C%2520we%2520design%2520a%2520two-stage%2520fine-tuning%250Aframework%2520for%2520CIR%252C%2520whose%2520second%2520stage%2520introduces%2520plenty%2520of%2520static%250Arepresentations%2520of%2520negatives%2520to%2520optimize%2520the%2520representation%2520space%2520rapidly.%2520The%250Aabove%2520two%2520improvements%2520can%2520be%2520effectively%2520stacked%2520and%2520designed%2520to%2520be%250Aplug-and-play%252C%2520easily%2520applied%2520to%2520existing%2520CIR%2520models%2520without%2520changing%2520their%250Aoriginal%2520architectures.%2520Extensive%2520experiments%2520and%2520ablation%2520analysis%2520demonstrate%250Athat%2520our%2520method%2520effectively%2520scales%2520positives%2520and%2520negatives%2520and%2520achieves%250Astate-of-the-art%2520results%2520on%2520both%2520FashionIQ%2520and%2520CIRR%2520datasets.%2520In%2520addition%252C%2520our%250Amethod%2520also%2520performs%2520well%2520in%2520zero-shot%2520composed%2520image%2520retrieval%252C%2520providing%2520a%250Anew%2520CIR%2520solution%2520for%2520the%2520low-resources%2520scenario.%2520Our%2520code%2520and%2520data%2520are%2520released%250Aat%2520https%253A//github.com/BUAADreamer/SPN4CIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Composed%20Image%20Retrieval%20via%20Contrastive%20Learning%20with%20Scaling%0A%20%20Positives%20and%20Negatives&entry.906535625=Zhangchi%20Feng%20and%20Richong%20Zhang%20and%20Zhijie%20Nie&entry.1292438233=%20%20The%20Composed%20Image%20Retrieval%20%28CIR%29%20task%20aims%20to%20retrieve%20target%20images%20using%0Aa%20composed%20query%20consisting%20of%20a%20reference%20image%20and%20a%20modified%20text.%20Advanced%0Amethods%20often%20utilize%20contrastive%20learning%20as%20the%20optimization%20objective%2C%20which%0Abenefits%20from%20adequate%20positive%20and%20negative%20examples.%20However%2C%20the%20triplet%20for%0ACIR%20incurs%20high%20manual%20annotation%20costs%2C%20resulting%20in%20limited%20positive%0Aexamples.%20Furthermore%2C%20existing%20methods%20commonly%20use%20in-batch%20negative%0Asampling%2C%20which%20reduces%20the%20negative%20number%20available%20for%20the%20model.%20To%20address%0Athe%20problem%20of%20lack%20of%20positives%2C%20we%20propose%20a%20data%20generation%20method%20by%0Aleveraging%20a%20multi-modal%20large%20language%20model%20to%20construct%20triplets%20for%20CIR.%20To%0Aintroduce%20more%20negatives%20during%20fine-tuning%2C%20we%20design%20a%20two-stage%20fine-tuning%0Aframework%20for%20CIR%2C%20whose%20second%20stage%20introduces%20plenty%20of%20static%0Arepresentations%20of%20negatives%20to%20optimize%20the%20representation%20space%20rapidly.%20The%0Aabove%20two%20improvements%20can%20be%20effectively%20stacked%20and%20designed%20to%20be%0Aplug-and-play%2C%20easily%20applied%20to%20existing%20CIR%20models%20without%20changing%20their%0Aoriginal%20architectures.%20Extensive%20experiments%20and%20ablation%20analysis%20demonstrate%0Athat%20our%20method%20effectively%20scales%20positives%20and%20negatives%20and%20achieves%0Astate-of-the-art%20results%20on%20both%20FashionIQ%20and%20CIRR%20datasets.%20In%20addition%2C%20our%0Amethod%20also%20performs%20well%20in%20zero-shot%20composed%20image%20retrieval%2C%20providing%20a%0Anew%20CIR%20solution%20for%20the%20low-resources%20scenario.%20Our%20code%20and%20data%20are%20released%0Aat%20https%3A//github.com/BUAADreamer/SPN4CIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11317v2&entry.124074799=Read"},
{"title": "AdapMTL: Adaptive Pruning Framework for Multitask Learning Model", "author": "Mingcan Xiang and Steven Jiaxun Tang and Qizheng Yang and Hui Guan and Tongping Liu", "abstract": "  In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.\n", "link": "http://arxiv.org/abs/2408.03913v1", "date": "2024-08-07", "relevancy": 2.195, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.587}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5267}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdapMTL%3A%20Adaptive%20Pruning%20Framework%20for%20Multitask%20Learning%20Model&body=Title%3A%20AdapMTL%3A%20Adaptive%20Pruning%20Framework%20for%20Multitask%20Learning%20Model%0AAuthor%3A%20Mingcan%20Xiang%20and%20Steven%20Jiaxun%20Tang%20and%20Qizheng%20Yang%20and%20Hui%20Guan%20and%20Tongping%20Liu%0AAbstract%3A%20%20%20In%20the%20domain%20of%20multimedia%20and%20multimodal%20processing%2C%20the%20efficient%20handling%0Aof%20diverse%20data%20streams%20such%20as%20images%2C%20video%2C%20and%20sensor%20data%20is%20paramount.%0AModel%20compression%20and%20multitask%20learning%20%28MTL%29%20are%20crucial%20in%20this%20field%2C%0Aoffering%20the%20potential%20to%20address%20the%20resource-intensive%20demands%20of%20processing%0Aand%20interpreting%20multiple%20forms%20of%20media%20simultaneously.%20However%2C%20effectively%0Acompressing%20a%20multitask%20model%20presents%20significant%20challenges%20due%20to%20the%0Acomplexities%20of%20balancing%20sparsity%20allocation%20and%20accuracy%20performance%20across%0Amultiple%20tasks.%20To%20tackle%20these%20challenges%2C%20we%20propose%20AdapMTL%2C%20an%20adaptive%0Apruning%20framework%20for%20MTL%20models.%20AdapMTL%20leverages%20multiple%20learnable%20soft%0Athresholds%20independently%20assigned%20to%20the%20shared%20backbone%20and%20the%20task-specific%0Aheads%20to%20capture%20the%20nuances%20in%20different%20components%27%20sensitivity%20to%20pruning.%0ADuring%20training%2C%20it%20co-optimizes%20the%20soft%20thresholds%20and%20MTL%20model%20weights%20to%0Aautomatically%20determine%20the%20suitable%20sparsity%20level%20at%20each%20component%20to%0Aachieve%20both%20high%20task%20accuracy%20and%20high%20overall%20sparsity.%20It%20further%0Aincorporates%20an%20adaptive%20weighting%20mechanism%20that%20dynamically%20adjusts%20the%0Aimportance%20of%20task-specific%20losses%20based%20on%20each%20task%27s%20robustness%20to%20pruning.%0AWe%20demonstrate%20the%20effectiveness%20of%20AdapMTL%20through%20comprehensive%20experiments%0Aon%20popular%20multitask%20datasets%2C%20namely%20NYU-v2%20and%20Tiny-Taskonomy%2C%20with%20different%0Aarchitectures%2C%20showcasing%20superior%20performance%20compared%20to%20state-of-the-art%0Apruning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapMTL%253A%2520Adaptive%2520Pruning%2520Framework%2520for%2520Multitask%2520Learning%2520Model%26entry.906535625%3DMingcan%2520Xiang%2520and%2520Steven%2520Jiaxun%2520Tang%2520and%2520Qizheng%2520Yang%2520and%2520Hui%2520Guan%2520and%2520Tongping%2520Liu%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%2520multimedia%2520and%2520multimodal%2520processing%252C%2520the%2520efficient%2520handling%250Aof%2520diverse%2520data%2520streams%2520such%2520as%2520images%252C%2520video%252C%2520and%2520sensor%2520data%2520is%2520paramount.%250AModel%2520compression%2520and%2520multitask%2520learning%2520%2528MTL%2529%2520are%2520crucial%2520in%2520this%2520field%252C%250Aoffering%2520the%2520potential%2520to%2520address%2520the%2520resource-intensive%2520demands%2520of%2520processing%250Aand%2520interpreting%2520multiple%2520forms%2520of%2520media%2520simultaneously.%2520However%252C%2520effectively%250Acompressing%2520a%2520multitask%2520model%2520presents%2520significant%2520challenges%2520due%2520to%2520the%250Acomplexities%2520of%2520balancing%2520sparsity%2520allocation%2520and%2520accuracy%2520performance%2520across%250Amultiple%2520tasks.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520AdapMTL%252C%2520an%2520adaptive%250Apruning%2520framework%2520for%2520MTL%2520models.%2520AdapMTL%2520leverages%2520multiple%2520learnable%2520soft%250Athresholds%2520independently%2520assigned%2520to%2520the%2520shared%2520backbone%2520and%2520the%2520task-specific%250Aheads%2520to%2520capture%2520the%2520nuances%2520in%2520different%2520components%2527%2520sensitivity%2520to%2520pruning.%250ADuring%2520training%252C%2520it%2520co-optimizes%2520the%2520soft%2520thresholds%2520and%2520MTL%2520model%2520weights%2520to%250Aautomatically%2520determine%2520the%2520suitable%2520sparsity%2520level%2520at%2520each%2520component%2520to%250Aachieve%2520both%2520high%2520task%2520accuracy%2520and%2520high%2520overall%2520sparsity.%2520It%2520further%250Aincorporates%2520an%2520adaptive%2520weighting%2520mechanism%2520that%2520dynamically%2520adjusts%2520the%250Aimportance%2520of%2520task-specific%2520losses%2520based%2520on%2520each%2520task%2527s%2520robustness%2520to%2520pruning.%250AWe%2520demonstrate%2520the%2520effectiveness%2520of%2520AdapMTL%2520through%2520comprehensive%2520experiments%250Aon%2520popular%2520multitask%2520datasets%252C%2520namely%2520NYU-v2%2520and%2520Tiny-Taskonomy%252C%2520with%2520different%250Aarchitectures%252C%2520showcasing%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%250Apruning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdapMTL%3A%20Adaptive%20Pruning%20Framework%20for%20Multitask%20Learning%20Model&entry.906535625=Mingcan%20Xiang%20and%20Steven%20Jiaxun%20Tang%20and%20Qizheng%20Yang%20and%20Hui%20Guan%20and%20Tongping%20Liu&entry.1292438233=%20%20In%20the%20domain%20of%20multimedia%20and%20multimodal%20processing%2C%20the%20efficient%20handling%0Aof%20diverse%20data%20streams%20such%20as%20images%2C%20video%2C%20and%20sensor%20data%20is%20paramount.%0AModel%20compression%20and%20multitask%20learning%20%28MTL%29%20are%20crucial%20in%20this%20field%2C%0Aoffering%20the%20potential%20to%20address%20the%20resource-intensive%20demands%20of%20processing%0Aand%20interpreting%20multiple%20forms%20of%20media%20simultaneously.%20However%2C%20effectively%0Acompressing%20a%20multitask%20model%20presents%20significant%20challenges%20due%20to%20the%0Acomplexities%20of%20balancing%20sparsity%20allocation%20and%20accuracy%20performance%20across%0Amultiple%20tasks.%20To%20tackle%20these%20challenges%2C%20we%20propose%20AdapMTL%2C%20an%20adaptive%0Apruning%20framework%20for%20MTL%20models.%20AdapMTL%20leverages%20multiple%20learnable%20soft%0Athresholds%20independently%20assigned%20to%20the%20shared%20backbone%20and%20the%20task-specific%0Aheads%20to%20capture%20the%20nuances%20in%20different%20components%27%20sensitivity%20to%20pruning.%0ADuring%20training%2C%20it%20co-optimizes%20the%20soft%20thresholds%20and%20MTL%20model%20weights%20to%0Aautomatically%20determine%20the%20suitable%20sparsity%20level%20at%20each%20component%20to%0Aachieve%20both%20high%20task%20accuracy%20and%20high%20overall%20sparsity.%20It%20further%0Aincorporates%20an%20adaptive%20weighting%20mechanism%20that%20dynamically%20adjusts%20the%0Aimportance%20of%20task-specific%20losses%20based%20on%20each%20task%27s%20robustness%20to%20pruning.%0AWe%20demonstrate%20the%20effectiveness%20of%20AdapMTL%20through%20comprehensive%20experiments%0Aon%20popular%20multitask%20datasets%2C%20namely%20NYU-v2%20and%20Tiny-Taskonomy%2C%20with%20different%0Aarchitectures%2C%20showcasing%20superior%20performance%20compared%20to%20state-of-the-art%0Apruning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03913v1&entry.124074799=Read"},
{"title": "Global-Local Progressive Integration Network for Blind Image Quality\n  Assessment", "author": "Xiaoqi Wang and Yun Zhang", "abstract": "  Vision transformers (ViTs) excel in computer vision for modeling long-term\ndependencies, yet face two key challenges for image quality assessment (IQA):\ndiscarding fine details during patch embedding, and requiring extensive\ntraining data due to lack of inductive biases. In this study, we propose a\nGlobal-Local progressive INTegration network for IQA, called GlintIQA, to\naddress these issues through three key components: 1) Hybrid feature extraction\ncombines ViT-based global feature extractor (VGFE) and convolutional neural\nnetworks (CNNs)-based local feature extractor (CLFE) to capture global\ncoarse-grained features and local fine-grained features, respectively. The\nincorporation of CNNs mitigates the patch-level information loss and inductive\nbias constraints inherent to ViT architectures. 2) Progressive feature\nintegration leverages diverse kernel sizes in embedding to spatially align\ncoarse- and fine-grained features, and progressively aggregate these features\nby interactively stacking channel-wise attention and spatial enhancement\nmodules to build effective quality-aware representations. 3) Content\nsimilarity-based labeling approach is proposed that automatically assigns\nquality labels to images with diverse content based on subjective quality\nscores. This addresses the scarcity of labeled training data in synthetic\ndatasets and bolsters model generalization. The experimental results\ndemonstrate the efficacy of our approach, yielding 5.04% average SROCC gains on\ncross-authentic dataset evaluations. Moreover, our model and its counterpart\npre-trained on the proposed dataset respectively exhibited 5.40% and 13.23%\nimprovements on across-synthetic datasets evaluation. The codes and proposed\ndataset will be released at https://github.com/XiaoqiWang/GlintIQA.\n", "link": "http://arxiv.org/abs/2408.03885v1", "date": "2024-08-07", "relevancy": 2.1616, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5587}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5277}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global-Local%20Progressive%20Integration%20Network%20for%20Blind%20Image%20Quality%0A%20%20Assessment&body=Title%3A%20Global-Local%20Progressive%20Integration%20Network%20for%20Blind%20Image%20Quality%0A%20%20Assessment%0AAuthor%3A%20Xiaoqi%20Wang%20and%20Yun%20Zhang%0AAbstract%3A%20%20%20Vision%20transformers%20%28ViTs%29%20excel%20in%20computer%20vision%20for%20modeling%20long-term%0Adependencies%2C%20yet%20face%20two%20key%20challenges%20for%20image%20quality%20assessment%20%28IQA%29%3A%0Adiscarding%20fine%20details%20during%20patch%20embedding%2C%20and%20requiring%20extensive%0Atraining%20data%20due%20to%20lack%20of%20inductive%20biases.%20In%20this%20study%2C%20we%20propose%20a%0AGlobal-Local%20progressive%20INTegration%20network%20for%20IQA%2C%20called%20GlintIQA%2C%20to%0Aaddress%20these%20issues%20through%20three%20key%20components%3A%201%29%20Hybrid%20feature%20extraction%0Acombines%20ViT-based%20global%20feature%20extractor%20%28VGFE%29%20and%20convolutional%20neural%0Anetworks%20%28CNNs%29-based%20local%20feature%20extractor%20%28CLFE%29%20to%20capture%20global%0Acoarse-grained%20features%20and%20local%20fine-grained%20features%2C%20respectively.%20The%0Aincorporation%20of%20CNNs%20mitigates%20the%20patch-level%20information%20loss%20and%20inductive%0Abias%20constraints%20inherent%20to%20ViT%20architectures.%202%29%20Progressive%20feature%0Aintegration%20leverages%20diverse%20kernel%20sizes%20in%20embedding%20to%20spatially%20align%0Acoarse-%20and%20fine-grained%20features%2C%20and%20progressively%20aggregate%20these%20features%0Aby%20interactively%20stacking%20channel-wise%20attention%20and%20spatial%20enhancement%0Amodules%20to%20build%20effective%20quality-aware%20representations.%203%29%20Content%0Asimilarity-based%20labeling%20approach%20is%20proposed%20that%20automatically%20assigns%0Aquality%20labels%20to%20images%20with%20diverse%20content%20based%20on%20subjective%20quality%0Ascores.%20This%20addresses%20the%20scarcity%20of%20labeled%20training%20data%20in%20synthetic%0Adatasets%20and%20bolsters%20model%20generalization.%20The%20experimental%20results%0Ademonstrate%20the%20efficacy%20of%20our%20approach%2C%20yielding%205.04%25%20average%20SROCC%20gains%20on%0Across-authentic%20dataset%20evaluations.%20Moreover%2C%20our%20model%20and%20its%20counterpart%0Apre-trained%20on%20the%20proposed%20dataset%20respectively%20exhibited%205.40%25%20and%2013.23%25%0Aimprovements%20on%20across-synthetic%20datasets%20evaluation.%20The%20codes%20and%20proposed%0Adataset%20will%20be%20released%20at%20https%3A//github.com/XiaoqiWang/GlintIQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal-Local%2520Progressive%2520Integration%2520Network%2520for%2520Blind%2520Image%2520Quality%250A%2520%2520Assessment%26entry.906535625%3DXiaoqi%2520Wang%2520and%2520Yun%2520Zhang%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520%2528ViTs%2529%2520excel%2520in%2520computer%2520vision%2520for%2520modeling%2520long-term%250Adependencies%252C%2520yet%2520face%2520two%2520key%2520challenges%2520for%2520image%2520quality%2520assessment%2520%2528IQA%2529%253A%250Adiscarding%2520fine%2520details%2520during%2520patch%2520embedding%252C%2520and%2520requiring%2520extensive%250Atraining%2520data%2520due%2520to%2520lack%2520of%2520inductive%2520biases.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250AGlobal-Local%2520progressive%2520INTegration%2520network%2520for%2520IQA%252C%2520called%2520GlintIQA%252C%2520to%250Aaddress%2520these%2520issues%2520through%2520three%2520key%2520components%253A%25201%2529%2520Hybrid%2520feature%2520extraction%250Acombines%2520ViT-based%2520global%2520feature%2520extractor%2520%2528VGFE%2529%2520and%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529-based%2520local%2520feature%2520extractor%2520%2528CLFE%2529%2520to%2520capture%2520global%250Acoarse-grained%2520features%2520and%2520local%2520fine-grained%2520features%252C%2520respectively.%2520The%250Aincorporation%2520of%2520CNNs%2520mitigates%2520the%2520patch-level%2520information%2520loss%2520and%2520inductive%250Abias%2520constraints%2520inherent%2520to%2520ViT%2520architectures.%25202%2529%2520Progressive%2520feature%250Aintegration%2520leverages%2520diverse%2520kernel%2520sizes%2520in%2520embedding%2520to%2520spatially%2520align%250Acoarse-%2520and%2520fine-grained%2520features%252C%2520and%2520progressively%2520aggregate%2520these%2520features%250Aby%2520interactively%2520stacking%2520channel-wise%2520attention%2520and%2520spatial%2520enhancement%250Amodules%2520to%2520build%2520effective%2520quality-aware%2520representations.%25203%2529%2520Content%250Asimilarity-based%2520labeling%2520approach%2520is%2520proposed%2520that%2520automatically%2520assigns%250Aquality%2520labels%2520to%2520images%2520with%2520diverse%2520content%2520based%2520on%2520subjective%2520quality%250Ascores.%2520This%2520addresses%2520the%2520scarcity%2520of%2520labeled%2520training%2520data%2520in%2520synthetic%250Adatasets%2520and%2520bolsters%2520model%2520generalization.%2520The%2520experimental%2520results%250Ademonstrate%2520the%2520efficacy%2520of%2520our%2520approach%252C%2520yielding%25205.04%2525%2520average%2520SROCC%2520gains%2520on%250Across-authentic%2520dataset%2520evaluations.%2520Moreover%252C%2520our%2520model%2520and%2520its%2520counterpart%250Apre-trained%2520on%2520the%2520proposed%2520dataset%2520respectively%2520exhibited%25205.40%2525%2520and%252013.23%2525%250Aimprovements%2520on%2520across-synthetic%2520datasets%2520evaluation.%2520The%2520codes%2520and%2520proposed%250Adataset%2520will%2520be%2520released%2520at%2520https%253A//github.com/XiaoqiWang/GlintIQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global-Local%20Progressive%20Integration%20Network%20for%20Blind%20Image%20Quality%0A%20%20Assessment&entry.906535625=Xiaoqi%20Wang%20and%20Yun%20Zhang&entry.1292438233=%20%20Vision%20transformers%20%28ViTs%29%20excel%20in%20computer%20vision%20for%20modeling%20long-term%0Adependencies%2C%20yet%20face%20two%20key%20challenges%20for%20image%20quality%20assessment%20%28IQA%29%3A%0Adiscarding%20fine%20details%20during%20patch%20embedding%2C%20and%20requiring%20extensive%0Atraining%20data%20due%20to%20lack%20of%20inductive%20biases.%20In%20this%20study%2C%20we%20propose%20a%0AGlobal-Local%20progressive%20INTegration%20network%20for%20IQA%2C%20called%20GlintIQA%2C%20to%0Aaddress%20these%20issues%20through%20three%20key%20components%3A%201%29%20Hybrid%20feature%20extraction%0Acombines%20ViT-based%20global%20feature%20extractor%20%28VGFE%29%20and%20convolutional%20neural%0Anetworks%20%28CNNs%29-based%20local%20feature%20extractor%20%28CLFE%29%20to%20capture%20global%0Acoarse-grained%20features%20and%20local%20fine-grained%20features%2C%20respectively.%20The%0Aincorporation%20of%20CNNs%20mitigates%20the%20patch-level%20information%20loss%20and%20inductive%0Abias%20constraints%20inherent%20to%20ViT%20architectures.%202%29%20Progressive%20feature%0Aintegration%20leverages%20diverse%20kernel%20sizes%20in%20embedding%20to%20spatially%20align%0Acoarse-%20and%20fine-grained%20features%2C%20and%20progressively%20aggregate%20these%20features%0Aby%20interactively%20stacking%20channel-wise%20attention%20and%20spatial%20enhancement%0Amodules%20to%20build%20effective%20quality-aware%20representations.%203%29%20Content%0Asimilarity-based%20labeling%20approach%20is%20proposed%20that%20automatically%20assigns%0Aquality%20labels%20to%20images%20with%20diverse%20content%20based%20on%20subjective%20quality%0Ascores.%20This%20addresses%20the%20scarcity%20of%20labeled%20training%20data%20in%20synthetic%0Adatasets%20and%20bolsters%20model%20generalization.%20The%20experimental%20results%0Ademonstrate%20the%20efficacy%20of%20our%20approach%2C%20yielding%205.04%25%20average%20SROCC%20gains%20on%0Across-authentic%20dataset%20evaluations.%20Moreover%2C%20our%20model%20and%20its%20counterpart%0Apre-trained%20on%20the%20proposed%20dataset%20respectively%20exhibited%205.40%25%20and%2013.23%25%0Aimprovements%20on%20across-synthetic%20datasets%20evaluation.%20The%20codes%20and%20proposed%0Adataset%20will%20be%20released%20at%20https%3A//github.com/XiaoqiWang/GlintIQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03885v1&entry.124074799=Read"},
{"title": "Hate Speech Detection and Classification in Amharic Text with Deep\n  Learning", "author": "Samuel Minale Gashe and Seid Muhie Yimam and Yaregal Assabie", "abstract": "  Hate speech is a growing problem on social media. It can seriously impact\nsociety, especially in countries like Ethiopia, where it can trigger conflicts\namong diverse ethnic and religious groups. While hate speech detection in\nresource rich languages are progressing, for low resource languages such as\nAmharic are lacking. To address this gap, we develop Amharic hate speech data\nand SBi-LSTM deep learning model that can detect and classify text into four\ncategories of hate speech: racial, religious, gender, and non-hate speech. We\nhave annotated 5k Amharic social media post and comment data into four\ncategories. The data is annotated using a custom annotation tool by a total of\n100 native Amharic speakers. The model achieves a 94.8 F1-score performance.\nFuture improvements will include expanding the dataset and develop state-of-the\nart models.\n  Keywords: Amharic hate speech detection, classification, Amharic dataset,\nDeep Learning, SBi-LSTM\n", "link": "http://arxiv.org/abs/2408.03849v1", "date": "2024-08-07", "relevancy": 2.1524, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4398}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4299}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hate%20Speech%20Detection%20and%20Classification%20in%20Amharic%20Text%20with%20Deep%0A%20%20Learning&body=Title%3A%20Hate%20Speech%20Detection%20and%20Classification%20in%20Amharic%20Text%20with%20Deep%0A%20%20Learning%0AAuthor%3A%20Samuel%20Minale%20Gashe%20and%20Seid%20Muhie%20Yimam%20and%20Yaregal%20Assabie%0AAbstract%3A%20%20%20Hate%20speech%20is%20a%20growing%20problem%20on%20social%20media.%20It%20can%20seriously%20impact%0Asociety%2C%20especially%20in%20countries%20like%20Ethiopia%2C%20where%20it%20can%20trigger%20conflicts%0Aamong%20diverse%20ethnic%20and%20religious%20groups.%20While%20hate%20speech%20detection%20in%0Aresource%20rich%20languages%20are%20progressing%2C%20for%20low%20resource%20languages%20such%20as%0AAmharic%20are%20lacking.%20To%20address%20this%20gap%2C%20we%20develop%20Amharic%20hate%20speech%20data%0Aand%20SBi-LSTM%20deep%20learning%20model%20that%20can%20detect%20and%20classify%20text%20into%20four%0Acategories%20of%20hate%20speech%3A%20racial%2C%20religious%2C%20gender%2C%20and%20non-hate%20speech.%20We%0Ahave%20annotated%205k%20Amharic%20social%20media%20post%20and%20comment%20data%20into%20four%0Acategories.%20The%20data%20is%20annotated%20using%20a%20custom%20annotation%20tool%20by%20a%20total%20of%0A100%20native%20Amharic%20speakers.%20The%20model%20achieves%20a%2094.8%20F1-score%20performance.%0AFuture%20improvements%20will%20include%20expanding%20the%20dataset%20and%20develop%20state-of-the%0Aart%20models.%0A%20%20Keywords%3A%20Amharic%20hate%20speech%20detection%2C%20classification%2C%20Amharic%20dataset%2C%0ADeep%20Learning%2C%20SBi-LSTM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHate%2520Speech%2520Detection%2520and%2520Classification%2520in%2520Amharic%2520Text%2520with%2520Deep%250A%2520%2520Learning%26entry.906535625%3DSamuel%2520Minale%2520Gashe%2520and%2520Seid%2520Muhie%2520Yimam%2520and%2520Yaregal%2520Assabie%26entry.1292438233%3D%2520%2520Hate%2520speech%2520is%2520a%2520growing%2520problem%2520on%2520social%2520media.%2520It%2520can%2520seriously%2520impact%250Asociety%252C%2520especially%2520in%2520countries%2520like%2520Ethiopia%252C%2520where%2520it%2520can%2520trigger%2520conflicts%250Aamong%2520diverse%2520ethnic%2520and%2520religious%2520groups.%2520While%2520hate%2520speech%2520detection%2520in%250Aresource%2520rich%2520languages%2520are%2520progressing%252C%2520for%2520low%2520resource%2520languages%2520such%2520as%250AAmharic%2520are%2520lacking.%2520To%2520address%2520this%2520gap%252C%2520we%2520develop%2520Amharic%2520hate%2520speech%2520data%250Aand%2520SBi-LSTM%2520deep%2520learning%2520model%2520that%2520can%2520detect%2520and%2520classify%2520text%2520into%2520four%250Acategories%2520of%2520hate%2520speech%253A%2520racial%252C%2520religious%252C%2520gender%252C%2520and%2520non-hate%2520speech.%2520We%250Ahave%2520annotated%25205k%2520Amharic%2520social%2520media%2520post%2520and%2520comment%2520data%2520into%2520four%250Acategories.%2520The%2520data%2520is%2520annotated%2520using%2520a%2520custom%2520annotation%2520tool%2520by%2520a%2520total%2520of%250A100%2520native%2520Amharic%2520speakers.%2520The%2520model%2520achieves%2520a%252094.8%2520F1-score%2520performance.%250AFuture%2520improvements%2520will%2520include%2520expanding%2520the%2520dataset%2520and%2520develop%2520state-of-the%250Aart%2520models.%250A%2520%2520Keywords%253A%2520Amharic%2520hate%2520speech%2520detection%252C%2520classification%252C%2520Amharic%2520dataset%252C%250ADeep%2520Learning%252C%2520SBi-LSTM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hate%20Speech%20Detection%20and%20Classification%20in%20Amharic%20Text%20with%20Deep%0A%20%20Learning&entry.906535625=Samuel%20Minale%20Gashe%20and%20Seid%20Muhie%20Yimam%20and%20Yaregal%20Assabie&entry.1292438233=%20%20Hate%20speech%20is%20a%20growing%20problem%20on%20social%20media.%20It%20can%20seriously%20impact%0Asociety%2C%20especially%20in%20countries%20like%20Ethiopia%2C%20where%20it%20can%20trigger%20conflicts%0Aamong%20diverse%20ethnic%20and%20religious%20groups.%20While%20hate%20speech%20detection%20in%0Aresource%20rich%20languages%20are%20progressing%2C%20for%20low%20resource%20languages%20such%20as%0AAmharic%20are%20lacking.%20To%20address%20this%20gap%2C%20we%20develop%20Amharic%20hate%20speech%20data%0Aand%20SBi-LSTM%20deep%20learning%20model%20that%20can%20detect%20and%20classify%20text%20into%20four%0Acategories%20of%20hate%20speech%3A%20racial%2C%20religious%2C%20gender%2C%20and%20non-hate%20speech.%20We%0Ahave%20annotated%205k%20Amharic%20social%20media%20post%20and%20comment%20data%20into%20four%0Acategories.%20The%20data%20is%20annotated%20using%20a%20custom%20annotation%20tool%20by%20a%20total%20of%0A100%20native%20Amharic%20speakers.%20The%20model%20achieves%20a%2094.8%20F1-score%20performance.%0AFuture%20improvements%20will%20include%20expanding%20the%20dataset%20and%20develop%20state-of-the%0Aart%20models.%0A%20%20Keywords%3A%20Amharic%20hate%20speech%20detection%2C%20classification%2C%20Amharic%20dataset%2C%0ADeep%20Learning%2C%20SBi-LSTM%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03849v1&entry.124074799=Read"},
{"title": "FMiFood: Multi-modal Contrastive Learning for Food Image Classification", "author": "Xinyue Pan and Jiangpeng He and Fengqing Zhu", "abstract": "  Food image classification is the fundamental step in image-based dietary\nassessment, which aims to estimate participants' nutrient intake from eating\noccasion images. A common challenge of food images is the intra-class diversity\nand inter-class similarity, which can significantly hinder classification\nperformance. To address this issue, we introduce a novel multi-modal\ncontrastive learning framework called FMiFood, which learns more discriminative\nfeatures by integrating additional contextual information, such as food\ncategory text descriptions, to enhance classification accuracy. Specifically,\nwe propose a flexible matching technique that improves the similarity matching\nbetween text and image embeddings to focus on multiple key information.\nFurthermore, we incorporate the classification objectives into the framework\nand explore the use of GPT-4 to enrich the text descriptions and provide more\ndetailed context. Our method demonstrates improved performance on both the\nUPMC-101 and VFN datasets compared to existing methods.\n", "link": "http://arxiv.org/abs/2408.03922v1", "date": "2024-08-07", "relevancy": 2.1384, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5681}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5468}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FMiFood%3A%20Multi-modal%20Contrastive%20Learning%20for%20Food%20Image%20Classification&body=Title%3A%20FMiFood%3A%20Multi-modal%20Contrastive%20Learning%20for%20Food%20Image%20Classification%0AAuthor%3A%20Xinyue%20Pan%20and%20Jiangpeng%20He%20and%20Fengqing%20Zhu%0AAbstract%3A%20%20%20Food%20image%20classification%20is%20the%20fundamental%20step%20in%20image-based%20dietary%0Aassessment%2C%20which%20aims%20to%20estimate%20participants%27%20nutrient%20intake%20from%20eating%0Aoccasion%20images.%20A%20common%20challenge%20of%20food%20images%20is%20the%20intra-class%20diversity%0Aand%20inter-class%20similarity%2C%20which%20can%20significantly%20hinder%20classification%0Aperformance.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20multi-modal%0Acontrastive%20learning%20framework%20called%20FMiFood%2C%20which%20learns%20more%20discriminative%0Afeatures%20by%20integrating%20additional%20contextual%20information%2C%20such%20as%20food%0Acategory%20text%20descriptions%2C%20to%20enhance%20classification%20accuracy.%20Specifically%2C%0Awe%20propose%20a%20flexible%20matching%20technique%20that%20improves%20the%20similarity%20matching%0Abetween%20text%20and%20image%20embeddings%20to%20focus%20on%20multiple%20key%20information.%0AFurthermore%2C%20we%20incorporate%20the%20classification%20objectives%20into%20the%20framework%0Aand%20explore%20the%20use%20of%20GPT-4%20to%20enrich%20the%20text%20descriptions%20and%20provide%20more%0Adetailed%20context.%20Our%20method%20demonstrates%20improved%20performance%20on%20both%20the%0AUPMC-101%20and%20VFN%20datasets%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFMiFood%253A%2520Multi-modal%2520Contrastive%2520Learning%2520for%2520Food%2520Image%2520Classification%26entry.906535625%3DXinyue%2520Pan%2520and%2520Jiangpeng%2520He%2520and%2520Fengqing%2520Zhu%26entry.1292438233%3D%2520%2520Food%2520image%2520classification%2520is%2520the%2520fundamental%2520step%2520in%2520image-based%2520dietary%250Aassessment%252C%2520which%2520aims%2520to%2520estimate%2520participants%2527%2520nutrient%2520intake%2520from%2520eating%250Aoccasion%2520images.%2520A%2520common%2520challenge%2520of%2520food%2520images%2520is%2520the%2520intra-class%2520diversity%250Aand%2520inter-class%2520similarity%252C%2520which%2520can%2520significantly%2520hinder%2520classification%250Aperformance.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520multi-modal%250Acontrastive%2520learning%2520framework%2520called%2520FMiFood%252C%2520which%2520learns%2520more%2520discriminative%250Afeatures%2520by%2520integrating%2520additional%2520contextual%2520information%252C%2520such%2520as%2520food%250Acategory%2520text%2520descriptions%252C%2520to%2520enhance%2520classification%2520accuracy.%2520Specifically%252C%250Awe%2520propose%2520a%2520flexible%2520matching%2520technique%2520that%2520improves%2520the%2520similarity%2520matching%250Abetween%2520text%2520and%2520image%2520embeddings%2520to%2520focus%2520on%2520multiple%2520key%2520information.%250AFurthermore%252C%2520we%2520incorporate%2520the%2520classification%2520objectives%2520into%2520the%2520framework%250Aand%2520explore%2520the%2520use%2520of%2520GPT-4%2520to%2520enrich%2520the%2520text%2520descriptions%2520and%2520provide%2520more%250Adetailed%2520context.%2520Our%2520method%2520demonstrates%2520improved%2520performance%2520on%2520both%2520the%250AUPMC-101%2520and%2520VFN%2520datasets%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FMiFood%3A%20Multi-modal%20Contrastive%20Learning%20for%20Food%20Image%20Classification&entry.906535625=Xinyue%20Pan%20and%20Jiangpeng%20He%20and%20Fengqing%20Zhu&entry.1292438233=%20%20Food%20image%20classification%20is%20the%20fundamental%20step%20in%20image-based%20dietary%0Aassessment%2C%20which%20aims%20to%20estimate%20participants%27%20nutrient%20intake%20from%20eating%0Aoccasion%20images.%20A%20common%20challenge%20of%20food%20images%20is%20the%20intra-class%20diversity%0Aand%20inter-class%20similarity%2C%20which%20can%20significantly%20hinder%20classification%0Aperformance.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20multi-modal%0Acontrastive%20learning%20framework%20called%20FMiFood%2C%20which%20learns%20more%20discriminative%0Afeatures%20by%20integrating%20additional%20contextual%20information%2C%20such%20as%20food%0Acategory%20text%20descriptions%2C%20to%20enhance%20classification%20accuracy.%20Specifically%2C%0Awe%20propose%20a%20flexible%20matching%20technique%20that%20improves%20the%20similarity%20matching%0Abetween%20text%20and%20image%20embeddings%20to%20focus%20on%20multiple%20key%20information.%0AFurthermore%2C%20we%20incorporate%20the%20classification%20objectives%20into%20the%20framework%0Aand%20explore%20the%20use%20of%20GPT-4%20to%20enrich%20the%20text%20descriptions%20and%20provide%20more%0Adetailed%20context.%20Our%20method%20demonstrates%20improved%20performance%20on%20both%20the%0AUPMC-101%20and%20VFN%20datasets%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03922v1&entry.124074799=Read"},
{"title": "Trustworthy Image Semantic Communication with GenAI: Explainablity,\n  Controllability, and Efficiency", "author": "Xijun Wang and Dongshan Ye and Chenyuan Feng and Howard H. Yang and Xiang Chen and Tony Q. S. Quek", "abstract": "  Image semantic communication (ISC) has garnered significant attention for its\npotential to achieve high efficiency in visual content transmission. However,\nexisting ISC systems based on joint source-channel coding face challenges in\ninterpretability, operability, and compatibility. To address these limitations,\nwe propose a novel trustworthy ISC framework. This approach leverages text\nextraction and segmentation mapping techniques to convert images into\nexplainable semantics, while employing Generative Artificial Intelligence\n(GenAI) for multiple downstream inference tasks. We also introduce a multi-rate\nISC transmission protocol that dynamically adapts to both the received\nexplainable semantic content and specific task requirements at the receiver.\nSimulation results demonstrate that our framework achieves explainable\nlearning, decoupled training, and compatible transmission in various\napplication scenarios. Finally, some intriguing research directions and\napplication scenarios are identified.\n", "link": "http://arxiv.org/abs/2408.03806v1", "date": "2024-08-07", "relevancy": 2.1246, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5419}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5257}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trustworthy%20Image%20Semantic%20Communication%20with%20GenAI%3A%20Explainablity%2C%0A%20%20Controllability%2C%20and%20Efficiency&body=Title%3A%20Trustworthy%20Image%20Semantic%20Communication%20with%20GenAI%3A%20Explainablity%2C%0A%20%20Controllability%2C%20and%20Efficiency%0AAuthor%3A%20Xijun%20Wang%20and%20Dongshan%20Ye%20and%20Chenyuan%20Feng%20and%20Howard%20H.%20Yang%20and%20Xiang%20Chen%20and%20Tony%20Q.%20S.%20Quek%0AAbstract%3A%20%20%20Image%20semantic%20communication%20%28ISC%29%20has%20garnered%20significant%20attention%20for%20its%0Apotential%20to%20achieve%20high%20efficiency%20in%20visual%20content%20transmission.%20However%2C%0Aexisting%20ISC%20systems%20based%20on%20joint%20source-channel%20coding%20face%20challenges%20in%0Ainterpretability%2C%20operability%2C%20and%20compatibility.%20To%20address%20these%20limitations%2C%0Awe%20propose%20a%20novel%20trustworthy%20ISC%20framework.%20This%20approach%20leverages%20text%0Aextraction%20and%20segmentation%20mapping%20techniques%20to%20convert%20images%20into%0Aexplainable%20semantics%2C%20while%20employing%20Generative%20Artificial%20Intelligence%0A%28GenAI%29%20for%20multiple%20downstream%20inference%20tasks.%20We%20also%20introduce%20a%20multi-rate%0AISC%20transmission%20protocol%20that%20dynamically%20adapts%20to%20both%20the%20received%0Aexplainable%20semantic%20content%20and%20specific%20task%20requirements%20at%20the%20receiver.%0ASimulation%20results%20demonstrate%20that%20our%20framework%20achieves%20explainable%0Alearning%2C%20decoupled%20training%2C%20and%20compatible%20transmission%20in%20various%0Aapplication%20scenarios.%20Finally%2C%20some%20intriguing%20research%20directions%20and%0Aapplication%20scenarios%20are%20identified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustworthy%2520Image%2520Semantic%2520Communication%2520with%2520GenAI%253A%2520Explainablity%252C%250A%2520%2520Controllability%252C%2520and%2520Efficiency%26entry.906535625%3DXijun%2520Wang%2520and%2520Dongshan%2520Ye%2520and%2520Chenyuan%2520Feng%2520and%2520Howard%2520H.%2520Yang%2520and%2520Xiang%2520Chen%2520and%2520Tony%2520Q.%2520S.%2520Quek%26entry.1292438233%3D%2520%2520Image%2520semantic%2520communication%2520%2528ISC%2529%2520has%2520garnered%2520significant%2520attention%2520for%2520its%250Apotential%2520to%2520achieve%2520high%2520efficiency%2520in%2520visual%2520content%2520transmission.%2520However%252C%250Aexisting%2520ISC%2520systems%2520based%2520on%2520joint%2520source-channel%2520coding%2520face%2520challenges%2520in%250Ainterpretability%252C%2520operability%252C%2520and%2520compatibility.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520a%2520novel%2520trustworthy%2520ISC%2520framework.%2520This%2520approach%2520leverages%2520text%250Aextraction%2520and%2520segmentation%2520mapping%2520techniques%2520to%2520convert%2520images%2520into%250Aexplainable%2520semantics%252C%2520while%2520employing%2520Generative%2520Artificial%2520Intelligence%250A%2528GenAI%2529%2520for%2520multiple%2520downstream%2520inference%2520tasks.%2520We%2520also%2520introduce%2520a%2520multi-rate%250AISC%2520transmission%2520protocol%2520that%2520dynamically%2520adapts%2520to%2520both%2520the%2520received%250Aexplainable%2520semantic%2520content%2520and%2520specific%2520task%2520requirements%2520at%2520the%2520receiver.%250ASimulation%2520results%2520demonstrate%2520that%2520our%2520framework%2520achieves%2520explainable%250Alearning%252C%2520decoupled%2520training%252C%2520and%2520compatible%2520transmission%2520in%2520various%250Aapplication%2520scenarios.%2520Finally%252C%2520some%2520intriguing%2520research%2520directions%2520and%250Aapplication%2520scenarios%2520are%2520identified.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%20Image%20Semantic%20Communication%20with%20GenAI%3A%20Explainablity%2C%0A%20%20Controllability%2C%20and%20Efficiency&entry.906535625=Xijun%20Wang%20and%20Dongshan%20Ye%20and%20Chenyuan%20Feng%20and%20Howard%20H.%20Yang%20and%20Xiang%20Chen%20and%20Tony%20Q.%20S.%20Quek&entry.1292438233=%20%20Image%20semantic%20communication%20%28ISC%29%20has%20garnered%20significant%20attention%20for%20its%0Apotential%20to%20achieve%20high%20efficiency%20in%20visual%20content%20transmission.%20However%2C%0Aexisting%20ISC%20systems%20based%20on%20joint%20source-channel%20coding%20face%20challenges%20in%0Ainterpretability%2C%20operability%2C%20and%20compatibility.%20To%20address%20these%20limitations%2C%0Awe%20propose%20a%20novel%20trustworthy%20ISC%20framework.%20This%20approach%20leverages%20text%0Aextraction%20and%20segmentation%20mapping%20techniques%20to%20convert%20images%20into%0Aexplainable%20semantics%2C%20while%20employing%20Generative%20Artificial%20Intelligence%0A%28GenAI%29%20for%20multiple%20downstream%20inference%20tasks.%20We%20also%20introduce%20a%20multi-rate%0AISC%20transmission%20protocol%20that%20dynamically%20adapts%20to%20both%20the%20received%0Aexplainable%20semantic%20content%20and%20specific%20task%20requirements%20at%20the%20receiver.%0ASimulation%20results%20demonstrate%20that%20our%20framework%20achieves%20explainable%0Alearning%2C%20decoupled%20training%2C%20and%20compatible%20transmission%20in%20various%0Aapplication%20scenarios.%20Finally%2C%20some%20intriguing%20research%20directions%20and%0Aapplication%20scenarios%20are%20identified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03806v1&entry.124074799=Read"},
{"title": "UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction", "author": "Lan Feng and Mohammadhossein Bahari and Kaouther Messaoud Ben Amor and \u00c9loi Zablocki and Matthieu Cord and Alexandre Alahi", "abstract": "  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n", "link": "http://arxiv.org/abs/2403.15098v3", "date": "2024-08-07", "relevancy": 2.1175, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5415}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5319}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction&body=Title%3A%20UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction%0AAuthor%3A%20Lan%20Feng%20and%20Mohammadhossein%20Bahari%20and%20Kaouther%20Messaoud%20Ben%20Amor%20and%20%C3%89loi%20Zablocki%20and%20Matthieu%20Cord%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Vehicle%20trajectory%20prediction%20has%20increasingly%20relied%20on%20data-driven%0Asolutions%2C%20but%20their%20ability%20to%20scale%20to%20different%20data%20domains%20and%20the%20impact%0Aof%20larger%20dataset%20sizes%20on%20their%20generalization%20remain%20under-explored.%20While%0Athese%20questions%20can%20be%20studied%20by%20employing%20multiple%20datasets%2C%20it%20is%0Achallenging%20due%20to%20several%20discrepancies%2C%20e.g.%2C%20in%20data%20formats%2C%20map%0Aresolution%2C%20and%20semantic%20annotation%20types.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20UniTraj%2C%20a%20comprehensive%20framework%20that%20unifies%20various%20datasets%2C%0Amodels%2C%20and%20evaluation%20criteria%2C%20presenting%20new%20opportunities%20for%20the%20vehicle%0Atrajectory%20prediction%20field.%20In%20particular%2C%20using%20UniTraj%2C%20we%20conduct%20extensive%0Aexperiments%20and%20find%20that%20model%20performance%20significantly%20drops%20when%0Atransferred%20to%20other%20datasets.%20However%2C%20enlarging%20data%20size%20and%20diversity%20can%0Asubstantially%20improve%20performance%2C%20leading%20to%20a%20new%20state-of-the-art%20result%20for%0Athe%20nuScenes%20dataset.%20We%20provide%20insights%20into%20dataset%20characteristics%20to%0Aexplain%20these%20findings.%20The%20code%20can%20be%20found%20here%3A%0Ahttps%3A//github.com/vita-epfl/UniTraj%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15098v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTraj%253A%2520A%2520Unified%2520Framework%2520for%2520Scalable%2520Vehicle%2520Trajectory%2520Prediction%26entry.906535625%3DLan%2520Feng%2520and%2520Mohammadhossein%2520Bahari%2520and%2520Kaouther%2520Messaoud%2520Ben%2520Amor%2520and%2520%25C3%2589loi%2520Zablocki%2520and%2520Matthieu%2520Cord%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520Vehicle%2520trajectory%2520prediction%2520has%2520increasingly%2520relied%2520on%2520data-driven%250Asolutions%252C%2520but%2520their%2520ability%2520to%2520scale%2520to%2520different%2520data%2520domains%2520and%2520the%2520impact%250Aof%2520larger%2520dataset%2520sizes%2520on%2520their%2520generalization%2520remain%2520under-explored.%2520While%250Athese%2520questions%2520can%2520be%2520studied%2520by%2520employing%2520multiple%2520datasets%252C%2520it%2520is%250Achallenging%2520due%2520to%2520several%2520discrepancies%252C%2520e.g.%252C%2520in%2520data%2520formats%252C%2520map%250Aresolution%252C%2520and%2520semantic%2520annotation%2520types.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520UniTraj%252C%2520a%2520comprehensive%2520framework%2520that%2520unifies%2520various%2520datasets%252C%250Amodels%252C%2520and%2520evaluation%2520criteria%252C%2520presenting%2520new%2520opportunities%2520for%2520the%2520vehicle%250Atrajectory%2520prediction%2520field.%2520In%2520particular%252C%2520using%2520UniTraj%252C%2520we%2520conduct%2520extensive%250Aexperiments%2520and%2520find%2520that%2520model%2520performance%2520significantly%2520drops%2520when%250Atransferred%2520to%2520other%2520datasets.%2520However%252C%2520enlarging%2520data%2520size%2520and%2520diversity%2520can%250Asubstantially%2520improve%2520performance%252C%2520leading%2520to%2520a%2520new%2520state-of-the-art%2520result%2520for%250Athe%2520nuScenes%2520dataset.%2520We%2520provide%2520insights%2520into%2520dataset%2520characteristics%2520to%250Aexplain%2520these%2520findings.%2520The%2520code%2520can%2520be%2520found%2520here%253A%250Ahttps%253A//github.com/vita-epfl/UniTraj%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15098v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction&entry.906535625=Lan%20Feng%20and%20Mohammadhossein%20Bahari%20and%20Kaouther%20Messaoud%20Ben%20Amor%20and%20%C3%89loi%20Zablocki%20and%20Matthieu%20Cord%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Vehicle%20trajectory%20prediction%20has%20increasingly%20relied%20on%20data-driven%0Asolutions%2C%20but%20their%20ability%20to%20scale%20to%20different%20data%20domains%20and%20the%20impact%0Aof%20larger%20dataset%20sizes%20on%20their%20generalization%20remain%20under-explored.%20While%0Athese%20questions%20can%20be%20studied%20by%20employing%20multiple%20datasets%2C%20it%20is%0Achallenging%20due%20to%20several%20discrepancies%2C%20e.g.%2C%20in%20data%20formats%2C%20map%0Aresolution%2C%20and%20semantic%20annotation%20types.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20UniTraj%2C%20a%20comprehensive%20framework%20that%20unifies%20various%20datasets%2C%0Amodels%2C%20and%20evaluation%20criteria%2C%20presenting%20new%20opportunities%20for%20the%20vehicle%0Atrajectory%20prediction%20field.%20In%20particular%2C%20using%20UniTraj%2C%20we%20conduct%20extensive%0Aexperiments%20and%20find%20that%20model%20performance%20significantly%20drops%20when%0Atransferred%20to%20other%20datasets.%20However%2C%20enlarging%20data%20size%20and%20diversity%20can%0Asubstantially%20improve%20performance%2C%20leading%20to%20a%20new%20state-of-the-art%20result%20for%0Athe%20nuScenes%20dataset.%20We%20provide%20insights%20into%20dataset%20characteristics%20to%0Aexplain%20these%20findings.%20The%20code%20can%20be%20found%20here%3A%0Ahttps%3A//github.com/vita-epfl/UniTraj%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15098v3&entry.124074799=Read"},
{"title": "Advancing Prompt Learning through an External Layer", "author": "Fangming Cui and Xun Yang and Chao Wu and Liang Xiao and Xinmei Tian", "abstract": "  Prompt learning represents a promising method for adapting pre-trained\nvision-language models (VLMs) to various downstream tasks by learning a set of\ntext embeddings. One challenge inherent to these methods is the poor\ngeneralization performance due to the invalidity of the learned text embeddings\nfor unseen tasks. A straightforward approach to bridge this gap is to freeze\nthe text embeddings in prompts, which results in a lack of capacity to adapt\nVLMs for downstream tasks. To address this dilemma, we propose a paradigm\ncalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose a\ntextual external layer and learnable visual embeddings for adapting VLMs to\ndownstream tasks. The learnable external layer is built upon valid embeddings\nof pre-trained CLIP. This design considers the balance of learning capabilities\nbetween the two branches. To align the textual and visual features, we propose\na novel two-pronged approach: i) we introduce the optimal transport as the\ndiscrepancy metric to align the vision and text modalities, and ii) we\nintroduce a novel strengthening feature to enhance the interaction between\nthese two modalities. Four representative experiments (i.e., base-to-novel\ngeneralization, few-shot learning, cross-dataset generalization, domain shifts\ngeneralization) across 15 datasets demonstrate that our method outperforms the\nexisting prompt learning method.\n", "link": "http://arxiv.org/abs/2407.19674v3", "date": "2024-08-07", "relevancy": 2.1092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.525}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Prompt%20Learning%20through%20an%20External%20Layer&body=Title%3A%20Advancing%20Prompt%20Learning%20through%20an%20External%20Layer%0AAuthor%3A%20Fangming%20Cui%20and%20Xun%20Yang%20and%20Chao%20Wu%20and%20Liang%20Xiao%20and%20Xinmei%20Tian%0AAbstract%3A%20%20%20Prompt%20learning%20represents%20a%20promising%20method%20for%20adapting%20pre-trained%0Avision-language%20models%20%28VLMs%29%20to%20various%20downstream%20tasks%20by%20learning%20a%20set%20of%0Atext%20embeddings.%20One%20challenge%20inherent%20to%20these%20methods%20is%20the%20poor%0Ageneralization%20performance%20due%20to%20the%20invalidity%20of%20the%20learned%20text%20embeddings%0Afor%20unseen%20tasks.%20A%20straightforward%20approach%20to%20bridge%20this%20gap%20is%20to%20freeze%0Athe%20text%20embeddings%20in%20prompts%2C%20which%20results%20in%20a%20lack%20of%20capacity%20to%20adapt%0AVLMs%20for%20downstream%20tasks.%20To%20address%20this%20dilemma%2C%20we%20propose%20a%20paradigm%0Acalled%20EnPrompt%20with%20a%20novel%20External%20Layer%20%28EnLa%29.%20Specifically%2C%20we%20propose%20a%0Atextual%20external%20layer%20and%20learnable%20visual%20embeddings%20for%20adapting%20VLMs%20to%0Adownstream%20tasks.%20The%20learnable%20external%20layer%20is%20built%20upon%20valid%20embeddings%0Aof%20pre-trained%20CLIP.%20This%20design%20considers%20the%20balance%20of%20learning%20capabilities%0Abetween%20the%20two%20branches.%20To%20align%20the%20textual%20and%20visual%20features%2C%20we%20propose%0Aa%20novel%20two-pronged%20approach%3A%20i%29%20we%20introduce%20the%20optimal%20transport%20as%20the%0Adiscrepancy%20metric%20to%20align%20the%20vision%20and%20text%20modalities%2C%20and%20ii%29%20we%0Aintroduce%20a%20novel%20strengthening%20feature%20to%20enhance%20the%20interaction%20between%0Athese%20two%20modalities.%20Four%20representative%20experiments%20%28i.e.%2C%20base-to-novel%0Ageneralization%2C%20few-shot%20learning%2C%20cross-dataset%20generalization%2C%20domain%20shifts%0Ageneralization%29%20across%2015%20datasets%20demonstrate%20that%20our%20method%20outperforms%20the%0Aexisting%20prompt%20learning%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19674v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Prompt%2520Learning%2520through%2520an%2520External%2520Layer%26entry.906535625%3DFangming%2520Cui%2520and%2520Xun%2520Yang%2520and%2520Chao%2520Wu%2520and%2520Liang%2520Xiao%2520and%2520Xinmei%2520Tian%26entry.1292438233%3D%2520%2520Prompt%2520learning%2520represents%2520a%2520promising%2520method%2520for%2520adapting%2520pre-trained%250Avision-language%2520models%2520%2528VLMs%2529%2520to%2520various%2520downstream%2520tasks%2520by%2520learning%2520a%2520set%2520of%250Atext%2520embeddings.%2520One%2520challenge%2520inherent%2520to%2520these%2520methods%2520is%2520the%2520poor%250Ageneralization%2520performance%2520due%2520to%2520the%2520invalidity%2520of%2520the%2520learned%2520text%2520embeddings%250Afor%2520unseen%2520tasks.%2520A%2520straightforward%2520approach%2520to%2520bridge%2520this%2520gap%2520is%2520to%2520freeze%250Athe%2520text%2520embeddings%2520in%2520prompts%252C%2520which%2520results%2520in%2520a%2520lack%2520of%2520capacity%2520to%2520adapt%250AVLMs%2520for%2520downstream%2520tasks.%2520To%2520address%2520this%2520dilemma%252C%2520we%2520propose%2520a%2520paradigm%250Acalled%2520EnPrompt%2520with%2520a%2520novel%2520External%2520Layer%2520%2528EnLa%2529.%2520Specifically%252C%2520we%2520propose%2520a%250Atextual%2520external%2520layer%2520and%2520learnable%2520visual%2520embeddings%2520for%2520adapting%2520VLMs%2520to%250Adownstream%2520tasks.%2520The%2520learnable%2520external%2520layer%2520is%2520built%2520upon%2520valid%2520embeddings%250Aof%2520pre-trained%2520CLIP.%2520This%2520design%2520considers%2520the%2520balance%2520of%2520learning%2520capabilities%250Abetween%2520the%2520two%2520branches.%2520To%2520align%2520the%2520textual%2520and%2520visual%2520features%252C%2520we%2520propose%250Aa%2520novel%2520two-pronged%2520approach%253A%2520i%2529%2520we%2520introduce%2520the%2520optimal%2520transport%2520as%2520the%250Adiscrepancy%2520metric%2520to%2520align%2520the%2520vision%2520and%2520text%2520modalities%252C%2520and%2520ii%2529%2520we%250Aintroduce%2520a%2520novel%2520strengthening%2520feature%2520to%2520enhance%2520the%2520interaction%2520between%250Athese%2520two%2520modalities.%2520Four%2520representative%2520experiments%2520%2528i.e.%252C%2520base-to-novel%250Ageneralization%252C%2520few-shot%2520learning%252C%2520cross-dataset%2520generalization%252C%2520domain%2520shifts%250Ageneralization%2529%2520across%252015%2520datasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520the%250Aexisting%2520prompt%2520learning%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19674v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Prompt%20Learning%20through%20an%20External%20Layer&entry.906535625=Fangming%20Cui%20and%20Xun%20Yang%20and%20Chao%20Wu%20and%20Liang%20Xiao%20and%20Xinmei%20Tian&entry.1292438233=%20%20Prompt%20learning%20represents%20a%20promising%20method%20for%20adapting%20pre-trained%0Avision-language%20models%20%28VLMs%29%20to%20various%20downstream%20tasks%20by%20learning%20a%20set%20of%0Atext%20embeddings.%20One%20challenge%20inherent%20to%20these%20methods%20is%20the%20poor%0Ageneralization%20performance%20due%20to%20the%20invalidity%20of%20the%20learned%20text%20embeddings%0Afor%20unseen%20tasks.%20A%20straightforward%20approach%20to%20bridge%20this%20gap%20is%20to%20freeze%0Athe%20text%20embeddings%20in%20prompts%2C%20which%20results%20in%20a%20lack%20of%20capacity%20to%20adapt%0AVLMs%20for%20downstream%20tasks.%20To%20address%20this%20dilemma%2C%20we%20propose%20a%20paradigm%0Acalled%20EnPrompt%20with%20a%20novel%20External%20Layer%20%28EnLa%29.%20Specifically%2C%20we%20propose%20a%0Atextual%20external%20layer%20and%20learnable%20visual%20embeddings%20for%20adapting%20VLMs%20to%0Adownstream%20tasks.%20The%20learnable%20external%20layer%20is%20built%20upon%20valid%20embeddings%0Aof%20pre-trained%20CLIP.%20This%20design%20considers%20the%20balance%20of%20learning%20capabilities%0Abetween%20the%20two%20branches.%20To%20align%20the%20textual%20and%20visual%20features%2C%20we%20propose%0Aa%20novel%20two-pronged%20approach%3A%20i%29%20we%20introduce%20the%20optimal%20transport%20as%20the%0Adiscrepancy%20metric%20to%20align%20the%20vision%20and%20text%20modalities%2C%20and%20ii%29%20we%0Aintroduce%20a%20novel%20strengthening%20feature%20to%20enhance%20the%20interaction%20between%0Athese%20two%20modalities.%20Four%20representative%20experiments%20%28i.e.%2C%20base-to-novel%0Ageneralization%2C%20few-shot%20learning%2C%20cross-dataset%20generalization%2C%20domain%20shifts%0Ageneralization%29%20across%2015%20datasets%20demonstrate%20that%20our%20method%20outperforms%20the%0Aexisting%20prompt%20learning%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19674v3&entry.124074799=Read"},
{"title": "Surgformer: Surgical Transformer with Hierarchical Temporal Attention\n  for Surgical Phase Recognition", "author": "Shu Yang and Luyang Luo and Qiong Wang and Hao Chen", "abstract": "  Existing state-of-the-art methods for surgical phase recognition either rely\non the extraction of spatial-temporal features at a short-range temporal\nresolution or adopt the sequential extraction of the spatial and temporal\nfeatures across the entire temporal resolution. However, these methods have\nlimitations in modeling spatial-temporal dependency and addressing\nspatial-temporal redundancy: 1) These methods fail to effectively model\nspatial-temporal dependency, due to the lack of long-range information or joint\nspatial-temporal modeling. 2) These methods utilize dense spatial features\nacross the entire temporal resolution, resulting in significant\nspatial-temporal redundancy. In this paper, we propose the Surgical Transformer\n(Surgformer) to address the issues of spatial-temporal modeling and redundancy\nin an end-to-end manner, which employs divided spatial-temporal attention and\ntakes a limited set of sparse frames as input. Moreover, we propose a novel\nHierarchical Temporal Attention (HTA) to capture both global and local\ninformation within varied temporal resolutions from a target frame-centric\nperspective. Distinct from conventional temporal attention that primarily\nemphasizes dense long-range similarity, HTA not only captures long-term\ninformation but also considers local latent consistency among informative\nframes. HTA then employs pyramid feature aggregation to effectively utilize\ntemporal information across diverse temporal resolutions, thereby enhancing the\noverall temporal representation. Extensive experiments on two challenging\nbenchmark datasets verify that our proposed Surgformer performs favorably\nagainst the state-of-the-art methods. The code is released at\nhttps://github.com/isyangshu/Surgformer.\n", "link": "http://arxiv.org/abs/2408.03867v1", "date": "2024-08-07", "relevancy": 2.106, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5463}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5135}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surgformer%3A%20Surgical%20Transformer%20with%20Hierarchical%20Temporal%20Attention%0A%20%20for%20Surgical%20Phase%20Recognition&body=Title%3A%20Surgformer%3A%20Surgical%20Transformer%20with%20Hierarchical%20Temporal%20Attention%0A%20%20for%20Surgical%20Phase%20Recognition%0AAuthor%3A%20Shu%20Yang%20and%20Luyang%20Luo%20and%20Qiong%20Wang%20and%20Hao%20Chen%0AAbstract%3A%20%20%20Existing%20state-of-the-art%20methods%20for%20surgical%20phase%20recognition%20either%20rely%0Aon%20the%20extraction%20of%20spatial-temporal%20features%20at%20a%20short-range%20temporal%0Aresolution%20or%20adopt%20the%20sequential%20extraction%20of%20the%20spatial%20and%20temporal%0Afeatures%20across%20the%20entire%20temporal%20resolution.%20However%2C%20these%20methods%20have%0Alimitations%20in%20modeling%20spatial-temporal%20dependency%20and%20addressing%0Aspatial-temporal%20redundancy%3A%201%29%20These%20methods%20fail%20to%20effectively%20model%0Aspatial-temporal%20dependency%2C%20due%20to%20the%20lack%20of%20long-range%20information%20or%20joint%0Aspatial-temporal%20modeling.%202%29%20These%20methods%20utilize%20dense%20spatial%20features%0Aacross%20the%20entire%20temporal%20resolution%2C%20resulting%20in%20significant%0Aspatial-temporal%20redundancy.%20In%20this%20paper%2C%20we%20propose%20the%20Surgical%20Transformer%0A%28Surgformer%29%20to%20address%20the%20issues%20of%20spatial-temporal%20modeling%20and%20redundancy%0Ain%20an%20end-to-end%20manner%2C%20which%20employs%20divided%20spatial-temporal%20attention%20and%0Atakes%20a%20limited%20set%20of%20sparse%20frames%20as%20input.%20Moreover%2C%20we%20propose%20a%20novel%0AHierarchical%20Temporal%20Attention%20%28HTA%29%20to%20capture%20both%20global%20and%20local%0Ainformation%20within%20varied%20temporal%20resolutions%20from%20a%20target%20frame-centric%0Aperspective.%20Distinct%20from%20conventional%20temporal%20attention%20that%20primarily%0Aemphasizes%20dense%20long-range%20similarity%2C%20HTA%20not%20only%20captures%20long-term%0Ainformation%20but%20also%20considers%20local%20latent%20consistency%20among%20informative%0Aframes.%20HTA%20then%20employs%20pyramid%20feature%20aggregation%20to%20effectively%20utilize%0Atemporal%20information%20across%20diverse%20temporal%20resolutions%2C%20thereby%20enhancing%20the%0Aoverall%20temporal%20representation.%20Extensive%20experiments%20on%20two%20challenging%0Abenchmark%20datasets%20verify%20that%20our%20proposed%20Surgformer%20performs%20favorably%0Aagainst%20the%20state-of-the-art%20methods.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/isyangshu/Surgformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgformer%253A%2520Surgical%2520Transformer%2520with%2520Hierarchical%2520Temporal%2520Attention%250A%2520%2520for%2520Surgical%2520Phase%2520Recognition%26entry.906535625%3DShu%2520Yang%2520and%2520Luyang%2520Luo%2520and%2520Qiong%2520Wang%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520Existing%2520state-of-the-art%2520methods%2520for%2520surgical%2520phase%2520recognition%2520either%2520rely%250Aon%2520the%2520extraction%2520of%2520spatial-temporal%2520features%2520at%2520a%2520short-range%2520temporal%250Aresolution%2520or%2520adopt%2520the%2520sequential%2520extraction%2520of%2520the%2520spatial%2520and%2520temporal%250Afeatures%2520across%2520the%2520entire%2520temporal%2520resolution.%2520However%252C%2520these%2520methods%2520have%250Alimitations%2520in%2520modeling%2520spatial-temporal%2520dependency%2520and%2520addressing%250Aspatial-temporal%2520redundancy%253A%25201%2529%2520These%2520methods%2520fail%2520to%2520effectively%2520model%250Aspatial-temporal%2520dependency%252C%2520due%2520to%2520the%2520lack%2520of%2520long-range%2520information%2520or%2520joint%250Aspatial-temporal%2520modeling.%25202%2529%2520These%2520methods%2520utilize%2520dense%2520spatial%2520features%250Aacross%2520the%2520entire%2520temporal%2520resolution%252C%2520resulting%2520in%2520significant%250Aspatial-temporal%2520redundancy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Surgical%2520Transformer%250A%2528Surgformer%2529%2520to%2520address%2520the%2520issues%2520of%2520spatial-temporal%2520modeling%2520and%2520redundancy%250Ain%2520an%2520end-to-end%2520manner%252C%2520which%2520employs%2520divided%2520spatial-temporal%2520attention%2520and%250Atakes%2520a%2520limited%2520set%2520of%2520sparse%2520frames%2520as%2520input.%2520Moreover%252C%2520we%2520propose%2520a%2520novel%250AHierarchical%2520Temporal%2520Attention%2520%2528HTA%2529%2520to%2520capture%2520both%2520global%2520and%2520local%250Ainformation%2520within%2520varied%2520temporal%2520resolutions%2520from%2520a%2520target%2520frame-centric%250Aperspective.%2520Distinct%2520from%2520conventional%2520temporal%2520attention%2520that%2520primarily%250Aemphasizes%2520dense%2520long-range%2520similarity%252C%2520HTA%2520not%2520only%2520captures%2520long-term%250Ainformation%2520but%2520also%2520considers%2520local%2520latent%2520consistency%2520among%2520informative%250Aframes.%2520HTA%2520then%2520employs%2520pyramid%2520feature%2520aggregation%2520to%2520effectively%2520utilize%250Atemporal%2520information%2520across%2520diverse%2520temporal%2520resolutions%252C%2520thereby%2520enhancing%2520the%250Aoverall%2520temporal%2520representation.%2520Extensive%2520experiments%2520on%2520two%2520challenging%250Abenchmark%2520datasets%2520verify%2520that%2520our%2520proposed%2520Surgformer%2520performs%2520favorably%250Aagainst%2520the%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/isyangshu/Surgformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surgformer%3A%20Surgical%20Transformer%20with%20Hierarchical%20Temporal%20Attention%0A%20%20for%20Surgical%20Phase%20Recognition&entry.906535625=Shu%20Yang%20and%20Luyang%20Luo%20and%20Qiong%20Wang%20and%20Hao%20Chen&entry.1292438233=%20%20Existing%20state-of-the-art%20methods%20for%20surgical%20phase%20recognition%20either%20rely%0Aon%20the%20extraction%20of%20spatial-temporal%20features%20at%20a%20short-range%20temporal%0Aresolution%20or%20adopt%20the%20sequential%20extraction%20of%20the%20spatial%20and%20temporal%0Afeatures%20across%20the%20entire%20temporal%20resolution.%20However%2C%20these%20methods%20have%0Alimitations%20in%20modeling%20spatial-temporal%20dependency%20and%20addressing%0Aspatial-temporal%20redundancy%3A%201%29%20These%20methods%20fail%20to%20effectively%20model%0Aspatial-temporal%20dependency%2C%20due%20to%20the%20lack%20of%20long-range%20information%20or%20joint%0Aspatial-temporal%20modeling.%202%29%20These%20methods%20utilize%20dense%20spatial%20features%0Aacross%20the%20entire%20temporal%20resolution%2C%20resulting%20in%20significant%0Aspatial-temporal%20redundancy.%20In%20this%20paper%2C%20we%20propose%20the%20Surgical%20Transformer%0A%28Surgformer%29%20to%20address%20the%20issues%20of%20spatial-temporal%20modeling%20and%20redundancy%0Ain%20an%20end-to-end%20manner%2C%20which%20employs%20divided%20spatial-temporal%20attention%20and%0Atakes%20a%20limited%20set%20of%20sparse%20frames%20as%20input.%20Moreover%2C%20we%20propose%20a%20novel%0AHierarchical%20Temporal%20Attention%20%28HTA%29%20to%20capture%20both%20global%20and%20local%0Ainformation%20within%20varied%20temporal%20resolutions%20from%20a%20target%20frame-centric%0Aperspective.%20Distinct%20from%20conventional%20temporal%20attention%20that%20primarily%0Aemphasizes%20dense%20long-range%20similarity%2C%20HTA%20not%20only%20captures%20long-term%0Ainformation%20but%20also%20considers%20local%20latent%20consistency%20among%20informative%0Aframes.%20HTA%20then%20employs%20pyramid%20feature%20aggregation%20to%20effectively%20utilize%0Atemporal%20information%20across%20diverse%20temporal%20resolutions%2C%20thereby%20enhancing%20the%0Aoverall%20temporal%20representation.%20Extensive%20experiments%20on%20two%20challenging%0Abenchmark%20datasets%20verify%20that%20our%20proposed%20Surgformer%20performs%20favorably%0Aagainst%20the%20state-of-the-art%20methods.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/isyangshu/Surgformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03867v1&entry.124074799=Read"},
{"title": "L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection", "author": "Xun Huang and Ziyu Xu and Hai Wu and Jinlong Wang and Qiming Xia and Yan Xia and Jonathan Li and Kyle Gao and Chenglu Wen and Cheng Wang", "abstract": "  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 18.17% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n", "link": "http://arxiv.org/abs/2408.03677v1", "date": "2024-08-07", "relevancy": 2.0993, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5272}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5262}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection&body=Title%3A%20L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection%0AAuthor%3A%20Xun%20Huang%20and%20Ziyu%20Xu%20and%20Hai%20Wu%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Yan%20Xia%20and%20Jonathan%20Li%20and%20Kyle%20Gao%20and%20Chenglu%20Wen%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20LiDAR-based%20vision%20systems%20are%20integral%20for%203D%20object%20detection%2C%20which%20is%0Acrucial%20for%20autonomous%20navigation.%20However%2C%20they%20suffer%20from%20performance%0Adegradation%20in%20adverse%20weather%20conditions%20due%20to%20the%20quality%20deterioration%20of%0ALiDAR%20point%20clouds.%20Fusing%20LiDAR%20with%20the%20weather-robust%204D%20radar%20sensor%20is%0Aexpected%20to%20solve%20this%20problem.%20However%2C%20the%20fusion%20of%20LiDAR%20and%204D%20radar%20is%0Achallenging%20because%20they%20differ%20significantly%20in%20terms%20of%20data%20quality%20and%20the%0Adegree%20of%20degradation%20in%20adverse%20weather.%20To%20address%20these%20issues%2C%20we%20introduce%0AL4DR%2C%20a%20weather-robust%203D%20object%20detection%20method%20that%20effectively%20achieves%0ALiDAR%20and%204D%20Radar%20fusion.%20Our%20L4DR%20includes%20Multi-Modal%20Encoding%20%28MME%29%20and%0AForeground-Aware%20Denoising%20%28FAD%29%20technique%20to%20reconcile%20sensor%20gaps%2C%20which%20is%0Athe%20first%20exploration%20of%20the%20complementarity%20of%20early%20fusion%20between%20LiDAR%20and%0A4D%20radar.%20Additionally%2C%20we%20design%20an%20Inter-Modal%20and%20Intra-Modal%20%28%7BIM%7D2%20%29%0Aparallel%20feature%20extraction%20backbone%20coupled%20with%20a%20Multi-Scale%20Gated%20Fusion%0A%28MSGF%29%20module%20to%20counteract%20the%20varying%20degrees%20of%20sensor%20degradation%20under%0Aadverse%20weather%20conditions.%20Experimental%20evaluation%20on%20a%20VoD%20dataset%20with%0Asimulated%20fog%20proves%20that%20L4DR%20is%20more%20adaptable%20to%20changing%20weather%0Aconditions.%20It%20delivers%20a%20significant%20performance%20increase%20under%20different%20fog%0Alevels%2C%20improving%20the%203D%20mAP%20by%20up%20to%2018.17%25%20over%20the%20traditional%20LiDAR-only%0Aapproach.%20Moreover%2C%20the%20results%20on%20the%20K-Radar%20dataset%20validate%20the%20consistent%0Aperformance%20improvement%20of%20L4DR%20in%20real-world%20adverse%20weather%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL4DR%253A%2520LiDAR-4DRadar%2520Fusion%2520for%2520Weather-Robust%25203D%2520Object%2520Detection%26entry.906535625%3DXun%2520Huang%2520and%2520Ziyu%2520Xu%2520and%2520Hai%2520Wu%2520and%2520Jinlong%2520Wang%2520and%2520Qiming%2520Xia%2520and%2520Yan%2520Xia%2520and%2520Jonathan%2520Li%2520and%2520Kyle%2520Gao%2520and%2520Chenglu%2520Wen%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520LiDAR-based%2520vision%2520systems%2520are%2520integral%2520for%25203D%2520object%2520detection%252C%2520which%2520is%250Acrucial%2520for%2520autonomous%2520navigation.%2520However%252C%2520they%2520suffer%2520from%2520performance%250Adegradation%2520in%2520adverse%2520weather%2520conditions%2520due%2520to%2520the%2520quality%2520deterioration%2520of%250ALiDAR%2520point%2520clouds.%2520Fusing%2520LiDAR%2520with%2520the%2520weather-robust%25204D%2520radar%2520sensor%2520is%250Aexpected%2520to%2520solve%2520this%2520problem.%2520However%252C%2520the%2520fusion%2520of%2520LiDAR%2520and%25204D%2520radar%2520is%250Achallenging%2520because%2520they%2520differ%2520significantly%2520in%2520terms%2520of%2520data%2520quality%2520and%2520the%250Adegree%2520of%2520degradation%2520in%2520adverse%2520weather.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250AL4DR%252C%2520a%2520weather-robust%25203D%2520object%2520detection%2520method%2520that%2520effectively%2520achieves%250ALiDAR%2520and%25204D%2520Radar%2520fusion.%2520Our%2520L4DR%2520includes%2520Multi-Modal%2520Encoding%2520%2528MME%2529%2520and%250AForeground-Aware%2520Denoising%2520%2528FAD%2529%2520technique%2520to%2520reconcile%2520sensor%2520gaps%252C%2520which%2520is%250Athe%2520first%2520exploration%2520of%2520the%2520complementarity%2520of%2520early%2520fusion%2520between%2520LiDAR%2520and%250A4D%2520radar.%2520Additionally%252C%2520we%2520design%2520an%2520Inter-Modal%2520and%2520Intra-Modal%2520%2528%257BIM%257D2%2520%2529%250Aparallel%2520feature%2520extraction%2520backbone%2520coupled%2520with%2520a%2520Multi-Scale%2520Gated%2520Fusion%250A%2528MSGF%2529%2520module%2520to%2520counteract%2520the%2520varying%2520degrees%2520of%2520sensor%2520degradation%2520under%250Aadverse%2520weather%2520conditions.%2520Experimental%2520evaluation%2520on%2520a%2520VoD%2520dataset%2520with%250Asimulated%2520fog%2520proves%2520that%2520L4DR%2520is%2520more%2520adaptable%2520to%2520changing%2520weather%250Aconditions.%2520It%2520delivers%2520a%2520significant%2520performance%2520increase%2520under%2520different%2520fog%250Alevels%252C%2520improving%2520the%25203D%2520mAP%2520by%2520up%2520to%252018.17%2525%2520over%2520the%2520traditional%2520LiDAR-only%250Aapproach.%2520Moreover%252C%2520the%2520results%2520on%2520the%2520K-Radar%2520dataset%2520validate%2520the%2520consistent%250Aperformance%2520improvement%2520of%2520L4DR%2520in%2520real-world%2520adverse%2520weather%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection&entry.906535625=Xun%20Huang%20and%20Ziyu%20Xu%20and%20Hai%20Wu%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Yan%20Xia%20and%20Jonathan%20Li%20and%20Kyle%20Gao%20and%20Chenglu%20Wen%20and%20Cheng%20Wang&entry.1292438233=%20%20LiDAR-based%20vision%20systems%20are%20integral%20for%203D%20object%20detection%2C%20which%20is%0Acrucial%20for%20autonomous%20navigation.%20However%2C%20they%20suffer%20from%20performance%0Adegradation%20in%20adverse%20weather%20conditions%20due%20to%20the%20quality%20deterioration%20of%0ALiDAR%20point%20clouds.%20Fusing%20LiDAR%20with%20the%20weather-robust%204D%20radar%20sensor%20is%0Aexpected%20to%20solve%20this%20problem.%20However%2C%20the%20fusion%20of%20LiDAR%20and%204D%20radar%20is%0Achallenging%20because%20they%20differ%20significantly%20in%20terms%20of%20data%20quality%20and%20the%0Adegree%20of%20degradation%20in%20adverse%20weather.%20To%20address%20these%20issues%2C%20we%20introduce%0AL4DR%2C%20a%20weather-robust%203D%20object%20detection%20method%20that%20effectively%20achieves%0ALiDAR%20and%204D%20Radar%20fusion.%20Our%20L4DR%20includes%20Multi-Modal%20Encoding%20%28MME%29%20and%0AForeground-Aware%20Denoising%20%28FAD%29%20technique%20to%20reconcile%20sensor%20gaps%2C%20which%20is%0Athe%20first%20exploration%20of%20the%20complementarity%20of%20early%20fusion%20between%20LiDAR%20and%0A4D%20radar.%20Additionally%2C%20we%20design%20an%20Inter-Modal%20and%20Intra-Modal%20%28%7BIM%7D2%20%29%0Aparallel%20feature%20extraction%20backbone%20coupled%20with%20a%20Multi-Scale%20Gated%20Fusion%0A%28MSGF%29%20module%20to%20counteract%20the%20varying%20degrees%20of%20sensor%20degradation%20under%0Aadverse%20weather%20conditions.%20Experimental%20evaluation%20on%20a%20VoD%20dataset%20with%0Asimulated%20fog%20proves%20that%20L4DR%20is%20more%20adaptable%20to%20changing%20weather%0Aconditions.%20It%20delivers%20a%20significant%20performance%20increase%20under%20different%20fog%0Alevels%2C%20improving%20the%203D%20mAP%20by%20up%20to%2018.17%25%20over%20the%20traditional%20LiDAR-only%0Aapproach.%20Moreover%2C%20the%20results%20on%20the%20K-Radar%20dataset%20validate%20the%20consistent%0Aperformance%20improvement%20of%20L4DR%20in%20real-world%20adverse%20weather%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03677v1&entry.124074799=Read"},
{"title": "Counterfactuals and Uncertainty-Based Explainable Paradigm for the\n  Automated Detection and Segmentation of Renal Cysts in Computed Tomography\n  Images: A Multi-Center Study", "author": "Zohaib Salahuddin and Abdalla Ibrahim and Sheng Kuang and Yousif Widaatalla and Razvan L. Miclea and Oliver Morin and Spencer Behr and Marnix P. M. Kop and Tom Marcelissen and Patricia Zondervan and Auke Jager and Philippe Lambin and Henry C Woodruff", "abstract": "  Routine computed tomography (CT) scans often detect a wide range of renal\ncysts, some of which may be malignant. Early and precise localization of these\ncysts can significantly aid quantitative image analysis. Current segmentation\nmethods, however, do not offer sufficient interpretability at the feature and\npixel levels, emphasizing the necessity for an explainable framework that can\ndetect and rectify model inaccuracies. We developed an interpretable\nsegmentation framework and validated it on a multi-centric dataset. A\nVariational Autoencoder Generative Adversarial Network (VAE-GAN) was employed\nto learn the latent representation of 3D input patches and reconstruct input\nimages. Modifications in the latent representation using the gradient of the\nsegmentation model generated counterfactual explanations for varying dice\nsimilarity coefficients (DSC). Radiomics features extracted from these\ncounterfactual images, using a ground truth cyst mask, were analyzed to\ndetermine their correlation with segmentation performance. The DSCs for the\noriginal and VAE-GAN reconstructed images for counterfactual image generation\nshowed no significant differences. Counterfactual explanations highlighted how\nvariations in cyst image features influence segmentation outcomes and showed\nmodel discrepancies. Radiomics features correlating positively and negatively\nwith dice scores were identified. The uncertainty of the predicted segmentation\nmasks was estimated using posterior sampling of the weight space. The\ncombination of counterfactual explanations and uncertainty maps provided a\ndeeper understanding of the image features within the segmented renal cysts\nthat lead to high uncertainty. The proposed segmentation framework not only\nachieved high segmentation accuracy but also increased interpretability\nregarding how image features impact segmentation performance.\n", "link": "http://arxiv.org/abs/2408.03789v1", "date": "2024-08-07", "relevancy": 2.0834, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5682}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5267}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactuals%20and%20Uncertainty-Based%20Explainable%20Paradigm%20for%20the%0A%20%20Automated%20Detection%20and%20Segmentation%20of%20Renal%20Cysts%20in%20Computed%20Tomography%0A%20%20Images%3A%20A%20Multi-Center%20Study&body=Title%3A%20Counterfactuals%20and%20Uncertainty-Based%20Explainable%20Paradigm%20for%20the%0A%20%20Automated%20Detection%20and%20Segmentation%20of%20Renal%20Cysts%20in%20Computed%20Tomography%0A%20%20Images%3A%20A%20Multi-Center%20Study%0AAuthor%3A%20Zohaib%20Salahuddin%20and%20Abdalla%20Ibrahim%20and%20Sheng%20Kuang%20and%20Yousif%20Widaatalla%20and%20Razvan%20L.%20Miclea%20and%20Oliver%20Morin%20and%20Spencer%20Behr%20and%20Marnix%20P.%20M.%20Kop%20and%20Tom%20Marcelissen%20and%20Patricia%20Zondervan%20and%20Auke%20Jager%20and%20Philippe%20Lambin%20and%20Henry%20C%20Woodruff%0AAbstract%3A%20%20%20Routine%20computed%20tomography%20%28CT%29%20scans%20often%20detect%20a%20wide%20range%20of%20renal%0Acysts%2C%20some%20of%20which%20may%20be%20malignant.%20Early%20and%20precise%20localization%20of%20these%0Acysts%20can%20significantly%20aid%20quantitative%20image%20analysis.%20Current%20segmentation%0Amethods%2C%20however%2C%20do%20not%20offer%20sufficient%20interpretability%20at%20the%20feature%20and%0Apixel%20levels%2C%20emphasizing%20the%20necessity%20for%20an%20explainable%20framework%20that%20can%0Adetect%20and%20rectify%20model%20inaccuracies.%20We%20developed%20an%20interpretable%0Asegmentation%20framework%20and%20validated%20it%20on%20a%20multi-centric%20dataset.%20A%0AVariational%20Autoencoder%20Generative%20Adversarial%20Network%20%28VAE-GAN%29%20was%20employed%0Ato%20learn%20the%20latent%20representation%20of%203D%20input%20patches%20and%20reconstruct%20input%0Aimages.%20Modifications%20in%20the%20latent%20representation%20using%20the%20gradient%20of%20the%0Asegmentation%20model%20generated%20counterfactual%20explanations%20for%20varying%20dice%0Asimilarity%20coefficients%20%28DSC%29.%20Radiomics%20features%20extracted%20from%20these%0Acounterfactual%20images%2C%20using%20a%20ground%20truth%20cyst%20mask%2C%20were%20analyzed%20to%0Adetermine%20their%20correlation%20with%20segmentation%20performance.%20The%20DSCs%20for%20the%0Aoriginal%20and%20VAE-GAN%20reconstructed%20images%20for%20counterfactual%20image%20generation%0Ashowed%20no%20significant%20differences.%20Counterfactual%20explanations%20highlighted%20how%0Avariations%20in%20cyst%20image%20features%20influence%20segmentation%20outcomes%20and%20showed%0Amodel%20discrepancies.%20Radiomics%20features%20correlating%20positively%20and%20negatively%0Awith%20dice%20scores%20were%20identified.%20The%20uncertainty%20of%20the%20predicted%20segmentation%0Amasks%20was%20estimated%20using%20posterior%20sampling%20of%20the%20weight%20space.%20The%0Acombination%20of%20counterfactual%20explanations%20and%20uncertainty%20maps%20provided%20a%0Adeeper%20understanding%20of%20the%20image%20features%20within%20the%20segmented%20renal%20cysts%0Athat%20lead%20to%20high%20uncertainty.%20The%20proposed%20segmentation%20framework%20not%20only%0Aachieved%20high%20segmentation%20accuracy%20but%20also%20increased%20interpretability%0Aregarding%20how%20image%20features%20impact%20segmentation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactuals%2520and%2520Uncertainty-Based%2520Explainable%2520Paradigm%2520for%2520the%250A%2520%2520Automated%2520Detection%2520and%2520Segmentation%2520of%2520Renal%2520Cysts%2520in%2520Computed%2520Tomography%250A%2520%2520Images%253A%2520A%2520Multi-Center%2520Study%26entry.906535625%3DZohaib%2520Salahuddin%2520and%2520Abdalla%2520Ibrahim%2520and%2520Sheng%2520Kuang%2520and%2520Yousif%2520Widaatalla%2520and%2520Razvan%2520L.%2520Miclea%2520and%2520Oliver%2520Morin%2520and%2520Spencer%2520Behr%2520and%2520Marnix%2520P.%2520M.%2520Kop%2520and%2520Tom%2520Marcelissen%2520and%2520Patricia%2520Zondervan%2520and%2520Auke%2520Jager%2520and%2520Philippe%2520Lambin%2520and%2520Henry%2520C%2520Woodruff%26entry.1292438233%3D%2520%2520Routine%2520computed%2520tomography%2520%2528CT%2529%2520scans%2520often%2520detect%2520a%2520wide%2520range%2520of%2520renal%250Acysts%252C%2520some%2520of%2520which%2520may%2520be%2520malignant.%2520Early%2520and%2520precise%2520localization%2520of%2520these%250Acysts%2520can%2520significantly%2520aid%2520quantitative%2520image%2520analysis.%2520Current%2520segmentation%250Amethods%252C%2520however%252C%2520do%2520not%2520offer%2520sufficient%2520interpretability%2520at%2520the%2520feature%2520and%250Apixel%2520levels%252C%2520emphasizing%2520the%2520necessity%2520for%2520an%2520explainable%2520framework%2520that%2520can%250Adetect%2520and%2520rectify%2520model%2520inaccuracies.%2520We%2520developed%2520an%2520interpretable%250Asegmentation%2520framework%2520and%2520validated%2520it%2520on%2520a%2520multi-centric%2520dataset.%2520A%250AVariational%2520Autoencoder%2520Generative%2520Adversarial%2520Network%2520%2528VAE-GAN%2529%2520was%2520employed%250Ato%2520learn%2520the%2520latent%2520representation%2520of%25203D%2520input%2520patches%2520and%2520reconstruct%2520input%250Aimages.%2520Modifications%2520in%2520the%2520latent%2520representation%2520using%2520the%2520gradient%2520of%2520the%250Asegmentation%2520model%2520generated%2520counterfactual%2520explanations%2520for%2520varying%2520dice%250Asimilarity%2520coefficients%2520%2528DSC%2529.%2520Radiomics%2520features%2520extracted%2520from%2520these%250Acounterfactual%2520images%252C%2520using%2520a%2520ground%2520truth%2520cyst%2520mask%252C%2520were%2520analyzed%2520to%250Adetermine%2520their%2520correlation%2520with%2520segmentation%2520performance.%2520The%2520DSCs%2520for%2520the%250Aoriginal%2520and%2520VAE-GAN%2520reconstructed%2520images%2520for%2520counterfactual%2520image%2520generation%250Ashowed%2520no%2520significant%2520differences.%2520Counterfactual%2520explanations%2520highlighted%2520how%250Avariations%2520in%2520cyst%2520image%2520features%2520influence%2520segmentation%2520outcomes%2520and%2520showed%250Amodel%2520discrepancies.%2520Radiomics%2520features%2520correlating%2520positively%2520and%2520negatively%250Awith%2520dice%2520scores%2520were%2520identified.%2520The%2520uncertainty%2520of%2520the%2520predicted%2520segmentation%250Amasks%2520was%2520estimated%2520using%2520posterior%2520sampling%2520of%2520the%2520weight%2520space.%2520The%250Acombination%2520of%2520counterfactual%2520explanations%2520and%2520uncertainty%2520maps%2520provided%2520a%250Adeeper%2520understanding%2520of%2520the%2520image%2520features%2520within%2520the%2520segmented%2520renal%2520cysts%250Athat%2520lead%2520to%2520high%2520uncertainty.%2520The%2520proposed%2520segmentation%2520framework%2520not%2520only%250Aachieved%2520high%2520segmentation%2520accuracy%2520but%2520also%2520increased%2520interpretability%250Aregarding%2520how%2520image%2520features%2520impact%2520segmentation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactuals%20and%20Uncertainty-Based%20Explainable%20Paradigm%20for%20the%0A%20%20Automated%20Detection%20and%20Segmentation%20of%20Renal%20Cysts%20in%20Computed%20Tomography%0A%20%20Images%3A%20A%20Multi-Center%20Study&entry.906535625=Zohaib%20Salahuddin%20and%20Abdalla%20Ibrahim%20and%20Sheng%20Kuang%20and%20Yousif%20Widaatalla%20and%20Razvan%20L.%20Miclea%20and%20Oliver%20Morin%20and%20Spencer%20Behr%20and%20Marnix%20P.%20M.%20Kop%20and%20Tom%20Marcelissen%20and%20Patricia%20Zondervan%20and%20Auke%20Jager%20and%20Philippe%20Lambin%20and%20Henry%20C%20Woodruff&entry.1292438233=%20%20Routine%20computed%20tomography%20%28CT%29%20scans%20often%20detect%20a%20wide%20range%20of%20renal%0Acysts%2C%20some%20of%20which%20may%20be%20malignant.%20Early%20and%20precise%20localization%20of%20these%0Acysts%20can%20significantly%20aid%20quantitative%20image%20analysis.%20Current%20segmentation%0Amethods%2C%20however%2C%20do%20not%20offer%20sufficient%20interpretability%20at%20the%20feature%20and%0Apixel%20levels%2C%20emphasizing%20the%20necessity%20for%20an%20explainable%20framework%20that%20can%0Adetect%20and%20rectify%20model%20inaccuracies.%20We%20developed%20an%20interpretable%0Asegmentation%20framework%20and%20validated%20it%20on%20a%20multi-centric%20dataset.%20A%0AVariational%20Autoencoder%20Generative%20Adversarial%20Network%20%28VAE-GAN%29%20was%20employed%0Ato%20learn%20the%20latent%20representation%20of%203D%20input%20patches%20and%20reconstruct%20input%0Aimages.%20Modifications%20in%20the%20latent%20representation%20using%20the%20gradient%20of%20the%0Asegmentation%20model%20generated%20counterfactual%20explanations%20for%20varying%20dice%0Asimilarity%20coefficients%20%28DSC%29.%20Radiomics%20features%20extracted%20from%20these%0Acounterfactual%20images%2C%20using%20a%20ground%20truth%20cyst%20mask%2C%20were%20analyzed%20to%0Adetermine%20their%20correlation%20with%20segmentation%20performance.%20The%20DSCs%20for%20the%0Aoriginal%20and%20VAE-GAN%20reconstructed%20images%20for%20counterfactual%20image%20generation%0Ashowed%20no%20significant%20differences.%20Counterfactual%20explanations%20highlighted%20how%0Avariations%20in%20cyst%20image%20features%20influence%20segmentation%20outcomes%20and%20showed%0Amodel%20discrepancies.%20Radiomics%20features%20correlating%20positively%20and%20negatively%0Awith%20dice%20scores%20were%20identified.%20The%20uncertainty%20of%20the%20predicted%20segmentation%0Amasks%20was%20estimated%20using%20posterior%20sampling%20of%20the%20weight%20space.%20The%0Acombination%20of%20counterfactual%20explanations%20and%20uncertainty%20maps%20provided%20a%0Adeeper%20understanding%20of%20the%20image%20features%20within%20the%20segmented%20renal%20cysts%0Athat%20lead%20to%20high%20uncertainty.%20The%20proposed%20segmentation%20framework%20not%20only%0Aachieved%20high%20segmentation%20accuracy%20but%20also%20increased%20interpretability%0Aregarding%20how%20image%20features%20impact%20segmentation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03789v1&entry.124074799=Read"},
{"title": "Pick of the Bunch: Detecting Infrared Small Targets Beyond Hit-Miss\n  Trade-Offs via Selective Rank-Aware Attention", "author": "Yimian Dai and Peiwen Pan and Yulei Qian and Yuxuan Li and Xiang Li and Jian Yang and Huan Wan", "abstract": "  Infrared small target detection faces the inherent challenge of precisely\nlocalizing dim targets amidst complex background clutter. Traditional\napproaches struggle to balance detection precision and false alarm rates. To\nbreak this dilemma, we propose SeRankDet, a deep network that achieves high\naccuracy beyond the conventional hit-miss trade-off, by following the ``Pick of\nthe Bunch'' principle. At its core lies our Selective Rank-Aware Attention\n(SeRank) module, employing a non-linear Top-K selection process that preserves\nthe most salient responses, preventing target signal dilution while maintaining\nconstant complexity. Furthermore, we replace the static concatenation typical\nin U-Net structures with our Large Selective Feature Fusion (LSFF) module, a\ndynamic fusion strategy that empowers SeRankDet with adaptive feature\nintegration, enhancing its ability to discriminate true targets from false\nalarms. The network's discernment is further refined by our Dilated Difference\nConvolution (DDC) module, which merges differential convolution aimed at\namplifying subtle target characteristics with dilated convolution to expand the\nreceptive field, thereby substantially improving target-background separation.\nDespite its lightweight architecture, the proposed SeRankDet sets new\nbenchmarks in state-of-the-art performance across multiple public datasets. The\ncode is available at https://github.com/GrokCV/SeRankDet.\n", "link": "http://arxiv.org/abs/2408.03717v1", "date": "2024-08-07", "relevancy": 2.0828, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5368}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5122}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pick%20of%20the%20Bunch%3A%20Detecting%20Infrared%20Small%20Targets%20Beyond%20Hit-Miss%0A%20%20Trade-Offs%20via%20Selective%20Rank-Aware%20Attention&body=Title%3A%20Pick%20of%20the%20Bunch%3A%20Detecting%20Infrared%20Small%20Targets%20Beyond%20Hit-Miss%0A%20%20Trade-Offs%20via%20Selective%20Rank-Aware%20Attention%0AAuthor%3A%20Yimian%20Dai%20and%20Peiwen%20Pan%20and%20Yulei%20Qian%20and%20Yuxuan%20Li%20and%20Xiang%20Li%20and%20Jian%20Yang%20and%20Huan%20Wan%0AAbstract%3A%20%20%20Infrared%20small%20target%20detection%20faces%20the%20inherent%20challenge%20of%20precisely%0Alocalizing%20dim%20targets%20amidst%20complex%20background%20clutter.%20Traditional%0Aapproaches%20struggle%20to%20balance%20detection%20precision%20and%20false%20alarm%20rates.%20To%0Abreak%20this%20dilemma%2C%20we%20propose%20SeRankDet%2C%20a%20deep%20network%20that%20achieves%20high%0Aaccuracy%20beyond%20the%20conventional%20hit-miss%20trade-off%2C%20by%20following%20the%20%60%60Pick%20of%0Athe%20Bunch%27%27%20principle.%20At%20its%20core%20lies%20our%20Selective%20Rank-Aware%20Attention%0A%28SeRank%29%20module%2C%20employing%20a%20non-linear%20Top-K%20selection%20process%20that%20preserves%0Athe%20most%20salient%20responses%2C%20preventing%20target%20signal%20dilution%20while%20maintaining%0Aconstant%20complexity.%20Furthermore%2C%20we%20replace%20the%20static%20concatenation%20typical%0Ain%20U-Net%20structures%20with%20our%20Large%20Selective%20Feature%20Fusion%20%28LSFF%29%20module%2C%20a%0Adynamic%20fusion%20strategy%20that%20empowers%20SeRankDet%20with%20adaptive%20feature%0Aintegration%2C%20enhancing%20its%20ability%20to%20discriminate%20true%20targets%20from%20false%0Aalarms.%20The%20network%27s%20discernment%20is%20further%20refined%20by%20our%20Dilated%20Difference%0AConvolution%20%28DDC%29%20module%2C%20which%20merges%20differential%20convolution%20aimed%20at%0Aamplifying%20subtle%20target%20characteristics%20with%20dilated%20convolution%20to%20expand%20the%0Areceptive%20field%2C%20thereby%20substantially%20improving%20target-background%20separation.%0ADespite%20its%20lightweight%20architecture%2C%20the%20proposed%20SeRankDet%20sets%20new%0Abenchmarks%20in%20state-of-the-art%20performance%20across%20multiple%20public%20datasets.%20The%0Acode%20is%20available%20at%20https%3A//github.com/GrokCV/SeRankDet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPick%2520of%2520the%2520Bunch%253A%2520Detecting%2520Infrared%2520Small%2520Targets%2520Beyond%2520Hit-Miss%250A%2520%2520Trade-Offs%2520via%2520Selective%2520Rank-Aware%2520Attention%26entry.906535625%3DYimian%2520Dai%2520and%2520Peiwen%2520Pan%2520and%2520Yulei%2520Qian%2520and%2520Yuxuan%2520Li%2520and%2520Xiang%2520Li%2520and%2520Jian%2520Yang%2520and%2520Huan%2520Wan%26entry.1292438233%3D%2520%2520Infrared%2520small%2520target%2520detection%2520faces%2520the%2520inherent%2520challenge%2520of%2520precisely%250Alocalizing%2520dim%2520targets%2520amidst%2520complex%2520background%2520clutter.%2520Traditional%250Aapproaches%2520struggle%2520to%2520balance%2520detection%2520precision%2520and%2520false%2520alarm%2520rates.%2520To%250Abreak%2520this%2520dilemma%252C%2520we%2520propose%2520SeRankDet%252C%2520a%2520deep%2520network%2520that%2520achieves%2520high%250Aaccuracy%2520beyond%2520the%2520conventional%2520hit-miss%2520trade-off%252C%2520by%2520following%2520the%2520%2560%2560Pick%2520of%250Athe%2520Bunch%2527%2527%2520principle.%2520At%2520its%2520core%2520lies%2520our%2520Selective%2520Rank-Aware%2520Attention%250A%2528SeRank%2529%2520module%252C%2520employing%2520a%2520non-linear%2520Top-K%2520selection%2520process%2520that%2520preserves%250Athe%2520most%2520salient%2520responses%252C%2520preventing%2520target%2520signal%2520dilution%2520while%2520maintaining%250Aconstant%2520complexity.%2520Furthermore%252C%2520we%2520replace%2520the%2520static%2520concatenation%2520typical%250Ain%2520U-Net%2520structures%2520with%2520our%2520Large%2520Selective%2520Feature%2520Fusion%2520%2528LSFF%2529%2520module%252C%2520a%250Adynamic%2520fusion%2520strategy%2520that%2520empowers%2520SeRankDet%2520with%2520adaptive%2520feature%250Aintegration%252C%2520enhancing%2520its%2520ability%2520to%2520discriminate%2520true%2520targets%2520from%2520false%250Aalarms.%2520The%2520network%2527s%2520discernment%2520is%2520further%2520refined%2520by%2520our%2520Dilated%2520Difference%250AConvolution%2520%2528DDC%2529%2520module%252C%2520which%2520merges%2520differential%2520convolution%2520aimed%2520at%250Aamplifying%2520subtle%2520target%2520characteristics%2520with%2520dilated%2520convolution%2520to%2520expand%2520the%250Areceptive%2520field%252C%2520thereby%2520substantially%2520improving%2520target-background%2520separation.%250ADespite%2520its%2520lightweight%2520architecture%252C%2520the%2520proposed%2520SeRankDet%2520sets%2520new%250Abenchmarks%2520in%2520state-of-the-art%2520performance%2520across%2520multiple%2520public%2520datasets.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/GrokCV/SeRankDet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pick%20of%20the%20Bunch%3A%20Detecting%20Infrared%20Small%20Targets%20Beyond%20Hit-Miss%0A%20%20Trade-Offs%20via%20Selective%20Rank-Aware%20Attention&entry.906535625=Yimian%20Dai%20and%20Peiwen%20Pan%20and%20Yulei%20Qian%20and%20Yuxuan%20Li%20and%20Xiang%20Li%20and%20Jian%20Yang%20and%20Huan%20Wan&entry.1292438233=%20%20Infrared%20small%20target%20detection%20faces%20the%20inherent%20challenge%20of%20precisely%0Alocalizing%20dim%20targets%20amidst%20complex%20background%20clutter.%20Traditional%0Aapproaches%20struggle%20to%20balance%20detection%20precision%20and%20false%20alarm%20rates.%20To%0Abreak%20this%20dilemma%2C%20we%20propose%20SeRankDet%2C%20a%20deep%20network%20that%20achieves%20high%0Aaccuracy%20beyond%20the%20conventional%20hit-miss%20trade-off%2C%20by%20following%20the%20%60%60Pick%20of%0Athe%20Bunch%27%27%20principle.%20At%20its%20core%20lies%20our%20Selective%20Rank-Aware%20Attention%0A%28SeRank%29%20module%2C%20employing%20a%20non-linear%20Top-K%20selection%20process%20that%20preserves%0Athe%20most%20salient%20responses%2C%20preventing%20target%20signal%20dilution%20while%20maintaining%0Aconstant%20complexity.%20Furthermore%2C%20we%20replace%20the%20static%20concatenation%20typical%0Ain%20U-Net%20structures%20with%20our%20Large%20Selective%20Feature%20Fusion%20%28LSFF%29%20module%2C%20a%0Adynamic%20fusion%20strategy%20that%20empowers%20SeRankDet%20with%20adaptive%20feature%0Aintegration%2C%20enhancing%20its%20ability%20to%20discriminate%20true%20targets%20from%20false%0Aalarms.%20The%20network%27s%20discernment%20is%20further%20refined%20by%20our%20Dilated%20Difference%0AConvolution%20%28DDC%29%20module%2C%20which%20merges%20differential%20convolution%20aimed%20at%0Aamplifying%20subtle%20target%20characteristics%20with%20dilated%20convolution%20to%20expand%20the%0Areceptive%20field%2C%20thereby%20substantially%20improving%20target-background%20separation.%0ADespite%20its%20lightweight%20architecture%2C%20the%20proposed%20SeRankDet%20sets%20new%0Abenchmarks%20in%20state-of-the-art%20performance%20across%20multiple%20public%20datasets.%20The%0Acode%20is%20available%20at%20https%3A//github.com/GrokCV/SeRankDet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03717v1&entry.124074799=Read"},
{"title": "Local Topology Measures of Contextual Language Model Latent Spaces With\n  Applications to Dialogue Term Extraction", "author": "Benjamin Matthias Ruppik and Michael Heck and Carel van Niekerk and Renato Vukovic and Hsien-chin Lin and Shutong Feng and Marcus Zibrowius and Milica Ga\u0161i\u0107", "abstract": "  A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.\n", "link": "http://arxiv.org/abs/2408.03706v1", "date": "2024-08-07", "relevancy": 2.0768, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5363}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5135}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Topology%20Measures%20of%20Contextual%20Language%20Model%20Latent%20Spaces%20With%0A%20%20Applications%20to%20Dialogue%20Term%20Extraction&body=Title%3A%20Local%20Topology%20Measures%20of%20Contextual%20Language%20Model%20Latent%20Spaces%20With%0A%20%20Applications%20to%20Dialogue%20Term%20Extraction%0AAuthor%3A%20Benjamin%20Matthias%20Ruppik%20and%20Michael%20Heck%20and%20Carel%20van%20Niekerk%20and%20Renato%20Vukovic%20and%20Hsien-chin%20Lin%20and%20Shutong%20Feng%20and%20Marcus%20Zibrowius%20and%20Milica%20Ga%C5%A1i%C4%87%0AAbstract%3A%20%20%20A%20common%20approach%20for%20sequence%20tagging%20tasks%20based%20on%20contextual%20word%0Arepresentations%20is%20to%20train%20a%20machine%20learning%20classifier%20directly%20on%20these%0Aembedding%20vectors.%20This%20approach%20has%20two%20shortcomings.%20First%2C%20such%20methods%0Aconsider%20single%20input%20sequences%20in%20isolation%20and%20are%20unable%20to%20put%20an%0Aindividual%20embedding%20vector%20in%20relation%20to%20vectors%20outside%20the%20current%20local%0Acontext%20of%20use.%20Second%2C%20the%20high%20performance%20of%20these%20models%20relies%20on%0Afine-tuning%20the%20embedding%20model%20in%20conjunction%20with%20the%20classifier%2C%20which%20may%0Anot%20always%20be%20feasible%20due%20to%20the%20size%20or%20inaccessibility%20of%20the%20underlying%0Afeature-generation%20model.%20It%20is%20thus%20desirable%2C%20given%20a%20collection%20of%20embedding%0Avectors%20of%20a%20corpus%2C%20i.e.%2C%20a%20datastore%2C%20to%20find%20features%20of%20each%20vector%20that%0Adescribe%20its%20relation%20to%20other%2C%20similar%20vectors%20in%20the%20datastore.%20With%20this%20in%0Amind%2C%20we%20introduce%20complexity%20measures%20of%20the%20local%20topology%20of%20the%20latent%0Aspace%20of%20a%20contextual%20language%20model%20with%20respect%20to%20a%20given%20datastore.%20The%0Aeffectiveness%20of%20our%20features%20is%20demonstrated%20through%20their%20application%20to%0Adialogue%20term%20extraction.%20Our%20work%20continues%20a%20line%20of%20research%20that%20explores%0Athe%20manifold%20hypothesis%20for%20word%20embeddings%2C%20demonstrating%20that%20local%20structure%0Ain%20the%20space%20carved%20out%20by%20word%20embeddings%20can%20be%20exploited%20to%20infer%20semantic%0Aproperties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Topology%2520Measures%2520of%2520Contextual%2520Language%2520Model%2520Latent%2520Spaces%2520With%250A%2520%2520Applications%2520to%2520Dialogue%2520Term%2520Extraction%26entry.906535625%3DBenjamin%2520Matthias%2520Ruppik%2520and%2520Michael%2520Heck%2520and%2520Carel%2520van%2520Niekerk%2520and%2520Renato%2520Vukovic%2520and%2520Hsien-chin%2520Lin%2520and%2520Shutong%2520Feng%2520and%2520Marcus%2520Zibrowius%2520and%2520Milica%2520Ga%25C5%25A1i%25C4%2587%26entry.1292438233%3D%2520%2520A%2520common%2520approach%2520for%2520sequence%2520tagging%2520tasks%2520based%2520on%2520contextual%2520word%250Arepresentations%2520is%2520to%2520train%2520a%2520machine%2520learning%2520classifier%2520directly%2520on%2520these%250Aembedding%2520vectors.%2520This%2520approach%2520has%2520two%2520shortcomings.%2520First%252C%2520such%2520methods%250Aconsider%2520single%2520input%2520sequences%2520in%2520isolation%2520and%2520are%2520unable%2520to%2520put%2520an%250Aindividual%2520embedding%2520vector%2520in%2520relation%2520to%2520vectors%2520outside%2520the%2520current%2520local%250Acontext%2520of%2520use.%2520Second%252C%2520the%2520high%2520performance%2520of%2520these%2520models%2520relies%2520on%250Afine-tuning%2520the%2520embedding%2520model%2520in%2520conjunction%2520with%2520the%2520classifier%252C%2520which%2520may%250Anot%2520always%2520be%2520feasible%2520due%2520to%2520the%2520size%2520or%2520inaccessibility%2520of%2520the%2520underlying%250Afeature-generation%2520model.%2520It%2520is%2520thus%2520desirable%252C%2520given%2520a%2520collection%2520of%2520embedding%250Avectors%2520of%2520a%2520corpus%252C%2520i.e.%252C%2520a%2520datastore%252C%2520to%2520find%2520features%2520of%2520each%2520vector%2520that%250Adescribe%2520its%2520relation%2520to%2520other%252C%2520similar%2520vectors%2520in%2520the%2520datastore.%2520With%2520this%2520in%250Amind%252C%2520we%2520introduce%2520complexity%2520measures%2520of%2520the%2520local%2520topology%2520of%2520the%2520latent%250Aspace%2520of%2520a%2520contextual%2520language%2520model%2520with%2520respect%2520to%2520a%2520given%2520datastore.%2520The%250Aeffectiveness%2520of%2520our%2520features%2520is%2520demonstrated%2520through%2520their%2520application%2520to%250Adialogue%2520term%2520extraction.%2520Our%2520work%2520continues%2520a%2520line%2520of%2520research%2520that%2520explores%250Athe%2520manifold%2520hypothesis%2520for%2520word%2520embeddings%252C%2520demonstrating%2520that%2520local%2520structure%250Ain%2520the%2520space%2520carved%2520out%2520by%2520word%2520embeddings%2520can%2520be%2520exploited%2520to%2520infer%2520semantic%250Aproperties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Topology%20Measures%20of%20Contextual%20Language%20Model%20Latent%20Spaces%20With%0A%20%20Applications%20to%20Dialogue%20Term%20Extraction&entry.906535625=Benjamin%20Matthias%20Ruppik%20and%20Michael%20Heck%20and%20Carel%20van%20Niekerk%20and%20Renato%20Vukovic%20and%20Hsien-chin%20Lin%20and%20Shutong%20Feng%20and%20Marcus%20Zibrowius%20and%20Milica%20Ga%C5%A1i%C4%87&entry.1292438233=%20%20A%20common%20approach%20for%20sequence%20tagging%20tasks%20based%20on%20contextual%20word%0Arepresentations%20is%20to%20train%20a%20machine%20learning%20classifier%20directly%20on%20these%0Aembedding%20vectors.%20This%20approach%20has%20two%20shortcomings.%20First%2C%20such%20methods%0Aconsider%20single%20input%20sequences%20in%20isolation%20and%20are%20unable%20to%20put%20an%0Aindividual%20embedding%20vector%20in%20relation%20to%20vectors%20outside%20the%20current%20local%0Acontext%20of%20use.%20Second%2C%20the%20high%20performance%20of%20these%20models%20relies%20on%0Afine-tuning%20the%20embedding%20model%20in%20conjunction%20with%20the%20classifier%2C%20which%20may%0Anot%20always%20be%20feasible%20due%20to%20the%20size%20or%20inaccessibility%20of%20the%20underlying%0Afeature-generation%20model.%20It%20is%20thus%20desirable%2C%20given%20a%20collection%20of%20embedding%0Avectors%20of%20a%20corpus%2C%20i.e.%2C%20a%20datastore%2C%20to%20find%20features%20of%20each%20vector%20that%0Adescribe%20its%20relation%20to%20other%2C%20similar%20vectors%20in%20the%20datastore.%20With%20this%20in%0Amind%2C%20we%20introduce%20complexity%20measures%20of%20the%20local%20topology%20of%20the%20latent%0Aspace%20of%20a%20contextual%20language%20model%20with%20respect%20to%20a%20given%20datastore.%20The%0Aeffectiveness%20of%20our%20features%20is%20demonstrated%20through%20their%20application%20to%0Adialogue%20term%20extraction.%20Our%20work%20continues%20a%20line%20of%20research%20that%20explores%0Athe%20manifold%20hypothesis%20for%20word%20embeddings%2C%20demonstrating%20that%20local%20structure%0Ain%20the%20space%20carved%20out%20by%20word%20embeddings%20can%20be%20exploited%20to%20infer%20semantic%0Aproperties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03706v1&entry.124074799=Read"},
{"title": "Using a Distance Sensor to Detect Deviations in a Planar Surface", "author": "Carter Sifferman and William Sun and Mohit Gupta and Michael Gleicher", "abstract": "  We investigate methods for determining if a planar surface contains geometric\ndeviations (e.g., protrusions, objects, divots, or cliffs) using only an\ninstantaneous measurement from a miniature optical time-of-flight sensor. The\nkey to our method is to utilize the entirety of information encoded in raw\ntime-of-flight data captured by off-the-shelf distance sensors. We provide an\nanalysis of the problem in which we identify the key ambiguity between geometry\nand surface photometrics. To overcome this challenging ambiguity, we fit a\nGaussian mixture model to a small dataset of planar surface measurements. This\nmodel implicitly captures the expected geometry and distribution of\nphotometrics of the planar surface and is used to identify measurements that\nare likely to contain deviations. We characterize our method on a variety of\nsurfaces and planar deviations across a range of scenarios. We find that our\nmethod utilizing raw time-of-flight data outperforms baselines which use only\nderived distance estimates. We build an example application in which our method\nenables mobile robot obstacle and cliff avoidance over a wide field-of-view.\n", "link": "http://arxiv.org/abs/2408.03838v1", "date": "2024-08-07", "relevancy": 2.0574, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.545}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5097}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20a%20Distance%20Sensor%20to%20Detect%20Deviations%20in%20a%20Planar%20Surface&body=Title%3A%20Using%20a%20Distance%20Sensor%20to%20Detect%20Deviations%20in%20a%20Planar%20Surface%0AAuthor%3A%20Carter%20Sifferman%20and%20William%20Sun%20and%20Mohit%20Gupta%20and%20Michael%20Gleicher%0AAbstract%3A%20%20%20We%20investigate%20methods%20for%20determining%20if%20a%20planar%20surface%20contains%20geometric%0Adeviations%20%28e.g.%2C%20protrusions%2C%20objects%2C%20divots%2C%20or%20cliffs%29%20using%20only%20an%0Ainstantaneous%20measurement%20from%20a%20miniature%20optical%20time-of-flight%20sensor.%20The%0Akey%20to%20our%20method%20is%20to%20utilize%20the%20entirety%20of%20information%20encoded%20in%20raw%0Atime-of-flight%20data%20captured%20by%20off-the-shelf%20distance%20sensors.%20We%20provide%20an%0Aanalysis%20of%20the%20problem%20in%20which%20we%20identify%20the%20key%20ambiguity%20between%20geometry%0Aand%20surface%20photometrics.%20To%20overcome%20this%20challenging%20ambiguity%2C%20we%20fit%20a%0AGaussian%20mixture%20model%20to%20a%20small%20dataset%20of%20planar%20surface%20measurements.%20This%0Amodel%20implicitly%20captures%20the%20expected%20geometry%20and%20distribution%20of%0Aphotometrics%20of%20the%20planar%20surface%20and%20is%20used%20to%20identify%20measurements%20that%0Aare%20likely%20to%20contain%20deviations.%20We%20characterize%20our%20method%20on%20a%20variety%20of%0Asurfaces%20and%20planar%20deviations%20across%20a%20range%20of%20scenarios.%20We%20find%20that%20our%0Amethod%20utilizing%20raw%20time-of-flight%20data%20outperforms%20baselines%20which%20use%20only%0Aderived%20distance%20estimates.%20We%20build%20an%20example%20application%20in%20which%20our%20method%0Aenables%20mobile%20robot%20obstacle%20and%20cliff%20avoidance%20over%20a%20wide%20field-of-view.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520a%2520Distance%2520Sensor%2520to%2520Detect%2520Deviations%2520in%2520a%2520Planar%2520Surface%26entry.906535625%3DCarter%2520Sifferman%2520and%2520William%2520Sun%2520and%2520Mohit%2520Gupta%2520and%2520Michael%2520Gleicher%26entry.1292438233%3D%2520%2520We%2520investigate%2520methods%2520for%2520determining%2520if%2520a%2520planar%2520surface%2520contains%2520geometric%250Adeviations%2520%2528e.g.%252C%2520protrusions%252C%2520objects%252C%2520divots%252C%2520or%2520cliffs%2529%2520using%2520only%2520an%250Ainstantaneous%2520measurement%2520from%2520a%2520miniature%2520optical%2520time-of-flight%2520sensor.%2520The%250Akey%2520to%2520our%2520method%2520is%2520to%2520utilize%2520the%2520entirety%2520of%2520information%2520encoded%2520in%2520raw%250Atime-of-flight%2520data%2520captured%2520by%2520off-the-shelf%2520distance%2520sensors.%2520We%2520provide%2520an%250Aanalysis%2520of%2520the%2520problem%2520in%2520which%2520we%2520identify%2520the%2520key%2520ambiguity%2520between%2520geometry%250Aand%2520surface%2520photometrics.%2520To%2520overcome%2520this%2520challenging%2520ambiguity%252C%2520we%2520fit%2520a%250AGaussian%2520mixture%2520model%2520to%2520a%2520small%2520dataset%2520of%2520planar%2520surface%2520measurements.%2520This%250Amodel%2520implicitly%2520captures%2520the%2520expected%2520geometry%2520and%2520distribution%2520of%250Aphotometrics%2520of%2520the%2520planar%2520surface%2520and%2520is%2520used%2520to%2520identify%2520measurements%2520that%250Aare%2520likely%2520to%2520contain%2520deviations.%2520We%2520characterize%2520our%2520method%2520on%2520a%2520variety%2520of%250Asurfaces%2520and%2520planar%2520deviations%2520across%2520a%2520range%2520of%2520scenarios.%2520We%2520find%2520that%2520our%250Amethod%2520utilizing%2520raw%2520time-of-flight%2520data%2520outperforms%2520baselines%2520which%2520use%2520only%250Aderived%2520distance%2520estimates.%2520We%2520build%2520an%2520example%2520application%2520in%2520which%2520our%2520method%250Aenables%2520mobile%2520robot%2520obstacle%2520and%2520cliff%2520avoidance%2520over%2520a%2520wide%2520field-of-view.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20a%20Distance%20Sensor%20to%20Detect%20Deviations%20in%20a%20Planar%20Surface&entry.906535625=Carter%20Sifferman%20and%20William%20Sun%20and%20Mohit%20Gupta%20and%20Michael%20Gleicher&entry.1292438233=%20%20We%20investigate%20methods%20for%20determining%20if%20a%20planar%20surface%20contains%20geometric%0Adeviations%20%28e.g.%2C%20protrusions%2C%20objects%2C%20divots%2C%20or%20cliffs%29%20using%20only%20an%0Ainstantaneous%20measurement%20from%20a%20miniature%20optical%20time-of-flight%20sensor.%20The%0Akey%20to%20our%20method%20is%20to%20utilize%20the%20entirety%20of%20information%20encoded%20in%20raw%0Atime-of-flight%20data%20captured%20by%20off-the-shelf%20distance%20sensors.%20We%20provide%20an%0Aanalysis%20of%20the%20problem%20in%20which%20we%20identify%20the%20key%20ambiguity%20between%20geometry%0Aand%20surface%20photometrics.%20To%20overcome%20this%20challenging%20ambiguity%2C%20we%20fit%20a%0AGaussian%20mixture%20model%20to%20a%20small%20dataset%20of%20planar%20surface%20measurements.%20This%0Amodel%20implicitly%20captures%20the%20expected%20geometry%20and%20distribution%20of%0Aphotometrics%20of%20the%20planar%20surface%20and%20is%20used%20to%20identify%20measurements%20that%0Aare%20likely%20to%20contain%20deviations.%20We%20characterize%20our%20method%20on%20a%20variety%20of%0Asurfaces%20and%20planar%20deviations%20across%20a%20range%20of%20scenarios.%20We%20find%20that%20our%0Amethod%20utilizing%20raw%20time-of-flight%20data%20outperforms%20baselines%20which%20use%20only%0Aderived%20distance%20estimates.%20We%20build%20an%20example%20application%20in%20which%20our%20method%0Aenables%20mobile%20robot%20obstacle%20and%20cliff%20avoidance%20over%20a%20wide%20field-of-view.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03838v1&entry.124074799=Read"},
{"title": "LiNR: Model Based Neural Retrieval on GPUs at LinkedIn", "author": "Fedor Borisyuk and Qingquan Song and Mingzhou Zhou and Ganesh Parameswaran and Madhu Arun and Siva Popuri and Tugrul Bingol and Zhuotao Pei and Kuang-Hsuan Lee and Lu Zheng and Qizhan Shao and Ali Naqvi and Sen Zhou and Aman Gupta", "abstract": "  This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrieval\nsystem. LiNR supports a billion-sized index on GPU models. We discuss our\nexperiences and challenges in creating scalable, differentiable search indexes\nusing TensorFlow and PyTorch at production scale. In LiNR, both items and model\nweights are integrated into the model binary. Viewing index construction as a\nform of model training, we describe scaling our system for large indexes,\nincorporating full scans and efficient filtering. A key focus is on enabling\nattribute-based pre-filtering for exhaustive GPU searches, addressing the\ncommon challenge of post-filtering in KNN searches that often reduces system\nquality. We further provide multi-embedding retrieval algorithms and strategies\nfor tackling cold start issues in retrieval. Our advancements in supporting\nlarger indexes through quantization are also discussed. We believe LiNR\nrepresents one of the industry's first Live-updated model-based retrieval\nindexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNR\nhas contributed to a 3% relative increase in professional daily active users.\nWe envisage LiNR as a step towards integrating retrieval and ranking into a\nsingle GPU model, simplifying complex infrastructures and enabling end-to-end\noptimization of the entire differentiable infrastructure through gradient\ndescent.\n", "link": "http://arxiv.org/abs/2407.13218v3", "date": "2024-08-07", "relevancy": 2.0321, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5203}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5019}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiNR%3A%20Model%20Based%20Neural%20Retrieval%20on%20GPUs%20at%20LinkedIn&body=Title%3A%20LiNR%3A%20Model%20Based%20Neural%20Retrieval%20on%20GPUs%20at%20LinkedIn%0AAuthor%3A%20Fedor%20Borisyuk%20and%20Qingquan%20Song%20and%20Mingzhou%20Zhou%20and%20Ganesh%20Parameswaran%20and%20Madhu%20Arun%20and%20Siva%20Popuri%20and%20Tugrul%20Bingol%20and%20Zhuotao%20Pei%20and%20Kuang-Hsuan%20Lee%20and%20Lu%20Zheng%20and%20Qizhan%20Shao%20and%20Ali%20Naqvi%20and%20Sen%20Zhou%20and%20Aman%20Gupta%0AAbstract%3A%20%20%20This%20paper%20introduces%20LiNR%2C%20LinkedIn%27s%20large-scale%2C%20GPU-based%20retrieval%0Asystem.%20LiNR%20supports%20a%20billion-sized%20index%20on%20GPU%20models.%20We%20discuss%20our%0Aexperiences%20and%20challenges%20in%20creating%20scalable%2C%20differentiable%20search%20indexes%0Ausing%20TensorFlow%20and%20PyTorch%20at%20production%20scale.%20In%20LiNR%2C%20both%20items%20and%20model%0Aweights%20are%20integrated%20into%20the%20model%20binary.%20Viewing%20index%20construction%20as%20a%0Aform%20of%20model%20training%2C%20we%20describe%20scaling%20our%20system%20for%20large%20indexes%2C%0Aincorporating%20full%20scans%20and%20efficient%20filtering.%20A%20key%20focus%20is%20on%20enabling%0Aattribute-based%20pre-filtering%20for%20exhaustive%20GPU%20searches%2C%20addressing%20the%0Acommon%20challenge%20of%20post-filtering%20in%20KNN%20searches%20that%20often%20reduces%20system%0Aquality.%20We%20further%20provide%20multi-embedding%20retrieval%20algorithms%20and%20strategies%0Afor%20tackling%20cold%20start%20issues%20in%20retrieval.%20Our%20advancements%20in%20supporting%0Alarger%20indexes%20through%20quantization%20are%20also%20discussed.%20We%20believe%20LiNR%0Arepresents%20one%20of%20the%20industry%27s%20first%20Live-updated%20model-based%20retrieval%0Aindexes.%20Applied%20to%20out-of-network%20post%20recommendations%20on%20LinkedIn%20Feed%2C%20LiNR%0Ahas%20contributed%20to%20a%203%25%20relative%20increase%20in%20professional%20daily%20active%20users.%0AWe%20envisage%20LiNR%20as%20a%20step%20towards%20integrating%20retrieval%20and%20ranking%20into%20a%0Asingle%20GPU%20model%2C%20simplifying%20complex%20infrastructures%20and%20enabling%20end-to-end%0Aoptimization%20of%20the%20entire%20differentiable%20infrastructure%20through%20gradient%0Adescent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13218v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiNR%253A%2520Model%2520Based%2520Neural%2520Retrieval%2520on%2520GPUs%2520at%2520LinkedIn%26entry.906535625%3DFedor%2520Borisyuk%2520and%2520Qingquan%2520Song%2520and%2520Mingzhou%2520Zhou%2520and%2520Ganesh%2520Parameswaran%2520and%2520Madhu%2520Arun%2520and%2520Siva%2520Popuri%2520and%2520Tugrul%2520Bingol%2520and%2520Zhuotao%2520Pei%2520and%2520Kuang-Hsuan%2520Lee%2520and%2520Lu%2520Zheng%2520and%2520Qizhan%2520Shao%2520and%2520Ali%2520Naqvi%2520and%2520Sen%2520Zhou%2520and%2520Aman%2520Gupta%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520LiNR%252C%2520LinkedIn%2527s%2520large-scale%252C%2520GPU-based%2520retrieval%250Asystem.%2520LiNR%2520supports%2520a%2520billion-sized%2520index%2520on%2520GPU%2520models.%2520We%2520discuss%2520our%250Aexperiences%2520and%2520challenges%2520in%2520creating%2520scalable%252C%2520differentiable%2520search%2520indexes%250Ausing%2520TensorFlow%2520and%2520PyTorch%2520at%2520production%2520scale.%2520In%2520LiNR%252C%2520both%2520items%2520and%2520model%250Aweights%2520are%2520integrated%2520into%2520the%2520model%2520binary.%2520Viewing%2520index%2520construction%2520as%2520a%250Aform%2520of%2520model%2520training%252C%2520we%2520describe%2520scaling%2520our%2520system%2520for%2520large%2520indexes%252C%250Aincorporating%2520full%2520scans%2520and%2520efficient%2520filtering.%2520A%2520key%2520focus%2520is%2520on%2520enabling%250Aattribute-based%2520pre-filtering%2520for%2520exhaustive%2520GPU%2520searches%252C%2520addressing%2520the%250Acommon%2520challenge%2520of%2520post-filtering%2520in%2520KNN%2520searches%2520that%2520often%2520reduces%2520system%250Aquality.%2520We%2520further%2520provide%2520multi-embedding%2520retrieval%2520algorithms%2520and%2520strategies%250Afor%2520tackling%2520cold%2520start%2520issues%2520in%2520retrieval.%2520Our%2520advancements%2520in%2520supporting%250Alarger%2520indexes%2520through%2520quantization%2520are%2520also%2520discussed.%2520We%2520believe%2520LiNR%250Arepresents%2520one%2520of%2520the%2520industry%2527s%2520first%2520Live-updated%2520model-based%2520retrieval%250Aindexes.%2520Applied%2520to%2520out-of-network%2520post%2520recommendations%2520on%2520LinkedIn%2520Feed%252C%2520LiNR%250Ahas%2520contributed%2520to%2520a%25203%2525%2520relative%2520increase%2520in%2520professional%2520daily%2520active%2520users.%250AWe%2520envisage%2520LiNR%2520as%2520a%2520step%2520towards%2520integrating%2520retrieval%2520and%2520ranking%2520into%2520a%250Asingle%2520GPU%2520model%252C%2520simplifying%2520complex%2520infrastructures%2520and%2520enabling%2520end-to-end%250Aoptimization%2520of%2520the%2520entire%2520differentiable%2520infrastructure%2520through%2520gradient%250Adescent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13218v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiNR%3A%20Model%20Based%20Neural%20Retrieval%20on%20GPUs%20at%20LinkedIn&entry.906535625=Fedor%20Borisyuk%20and%20Qingquan%20Song%20and%20Mingzhou%20Zhou%20and%20Ganesh%20Parameswaran%20and%20Madhu%20Arun%20and%20Siva%20Popuri%20and%20Tugrul%20Bingol%20and%20Zhuotao%20Pei%20and%20Kuang-Hsuan%20Lee%20and%20Lu%20Zheng%20and%20Qizhan%20Shao%20and%20Ali%20Naqvi%20and%20Sen%20Zhou%20and%20Aman%20Gupta&entry.1292438233=%20%20This%20paper%20introduces%20LiNR%2C%20LinkedIn%27s%20large-scale%2C%20GPU-based%20retrieval%0Asystem.%20LiNR%20supports%20a%20billion-sized%20index%20on%20GPU%20models.%20We%20discuss%20our%0Aexperiences%20and%20challenges%20in%20creating%20scalable%2C%20differentiable%20search%20indexes%0Ausing%20TensorFlow%20and%20PyTorch%20at%20production%20scale.%20In%20LiNR%2C%20both%20items%20and%20model%0Aweights%20are%20integrated%20into%20the%20model%20binary.%20Viewing%20index%20construction%20as%20a%0Aform%20of%20model%20training%2C%20we%20describe%20scaling%20our%20system%20for%20large%20indexes%2C%0Aincorporating%20full%20scans%20and%20efficient%20filtering.%20A%20key%20focus%20is%20on%20enabling%0Aattribute-based%20pre-filtering%20for%20exhaustive%20GPU%20searches%2C%20addressing%20the%0Acommon%20challenge%20of%20post-filtering%20in%20KNN%20searches%20that%20often%20reduces%20system%0Aquality.%20We%20further%20provide%20multi-embedding%20retrieval%20algorithms%20and%20strategies%0Afor%20tackling%20cold%20start%20issues%20in%20retrieval.%20Our%20advancements%20in%20supporting%0Alarger%20indexes%20through%20quantization%20are%20also%20discussed.%20We%20believe%20LiNR%0Arepresents%20one%20of%20the%20industry%27s%20first%20Live-updated%20model-based%20retrieval%0Aindexes.%20Applied%20to%20out-of-network%20post%20recommendations%20on%20LinkedIn%20Feed%2C%20LiNR%0Ahas%20contributed%20to%20a%203%25%20relative%20increase%20in%20professional%20daily%20active%20users.%0AWe%20envisage%20LiNR%20as%20a%20step%20towards%20integrating%20retrieval%20and%20ranking%20into%20a%0Asingle%20GPU%20model%2C%20simplifying%20complex%20infrastructures%20and%20enabling%20end-to-end%0Aoptimization%20of%20the%20entire%20differentiable%20infrastructure%20through%20gradient%0Adescent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13218v3&entry.124074799=Read"},
{"title": "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation", "author": "Yue Xu and Yong-Lu Li and Kaitong Cui and Ziyu Wang and Cewu Lu and Yu-Wing Tai and Chi-Keung Tang", "abstract": "  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n", "link": "http://arxiv.org/abs/2305.18381v4", "date": "2024-08-07", "relevancy": 2.0271, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5265}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4985}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distill%20Gold%20from%20Massive%20Ores%3A%20Bi-level%20Data%20Pruning%20towards%20Efficient%0A%20%20Dataset%20Distillation&body=Title%3A%20Distill%20Gold%20from%20Massive%20Ores%3A%20Bi-level%20Data%20Pruning%20towards%20Efficient%0A%20%20Dataset%20Distillation%0AAuthor%3A%20Yue%20Xu%20and%20Yong-Lu%20Li%20and%20Kaitong%20Cui%20and%20Ziyu%20Wang%20and%20Cewu%20Lu%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang%0AAbstract%3A%20%20%20Data-efficient%20learning%20has%20garnered%20significant%20attention%2C%20especially%20given%0Athe%20current%20trend%20of%20large%20multi-modal%20models.%20Recently%2C%20dataset%20distillation%0Ahas%20become%20an%20effective%20approach%20by%20synthesizing%20data%20samples%20that%20are%0Aessential%20for%20network%20training.%20However%2C%20it%20remains%20to%20be%20explored%20which%0Asamples%20are%20essential%20for%20the%20dataset%20distillation%20process%20itself.%20In%20this%0Awork%2C%20we%20study%20the%20data%20efficiency%20and%20selection%20for%20the%20dataset%20distillation%0Atask.%20By%20re-formulating%20the%20dynamics%20of%20distillation%2C%20we%20provide%20insight%20into%0Athe%20inherent%20redundancy%20in%20the%20real%20dataset%2C%20both%20theoretically%20and%0Aempirically.%20We%20propose%20to%20use%20the%20empirical%20loss%20value%20as%20a%20static%20data%0Apruning%20criterion.%20To%20further%20compensate%20for%20the%20variation%20of%20the%20data%20value%20in%0Atraining%2C%20we%20find%20the%20most%20contributing%20samples%20based%20on%20their%20causal%20effects%0Aon%20the%20distillation.%20The%20proposed%20selection%20strategy%20can%20efficiently%20exploit%0Athe%20training%20dataset%2C%20outperform%20the%20previous%20SOTA%20distillation%20algorithms%2C%20and%0Aconsistently%20enhance%20the%20distillation%20algorithms%2C%20even%20on%20much%20larger-scale%20and%0Amore%20heterogeneous%20datasets%2C%20e.g.%2C%20full%20ImageNet-1K%20and%20Kinetics-400.%20We%0Abelieve%20this%20paradigm%20will%20open%20up%20new%20avenues%20in%20the%20dynamics%20of%20distillation%0Aand%20pave%20the%20way%20for%20efficient%20dataset%20distillation.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/silicx/GoldFromOres-BiLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.18381v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistill%2520Gold%2520from%2520Massive%2520Ores%253A%2520Bi-level%2520Data%2520Pruning%2520towards%2520Efficient%250A%2520%2520Dataset%2520Distillation%26entry.906535625%3DYue%2520Xu%2520and%2520Yong-Lu%2520Li%2520and%2520Kaitong%2520Cui%2520and%2520Ziyu%2520Wang%2520and%2520Cewu%2520Lu%2520and%2520Yu-Wing%2520Tai%2520and%2520Chi-Keung%2520Tang%26entry.1292438233%3D%2520%2520Data-efficient%2520learning%2520has%2520garnered%2520significant%2520attention%252C%2520especially%2520given%250Athe%2520current%2520trend%2520of%2520large%2520multi-modal%2520models.%2520Recently%252C%2520dataset%2520distillation%250Ahas%2520become%2520an%2520effective%2520approach%2520by%2520synthesizing%2520data%2520samples%2520that%2520are%250Aessential%2520for%2520network%2520training.%2520However%252C%2520it%2520remains%2520to%2520be%2520explored%2520which%250Asamples%2520are%2520essential%2520for%2520the%2520dataset%2520distillation%2520process%2520itself.%2520In%2520this%250Awork%252C%2520we%2520study%2520the%2520data%2520efficiency%2520and%2520selection%2520for%2520the%2520dataset%2520distillation%250Atask.%2520By%2520re-formulating%2520the%2520dynamics%2520of%2520distillation%252C%2520we%2520provide%2520insight%2520into%250Athe%2520inherent%2520redundancy%2520in%2520the%2520real%2520dataset%252C%2520both%2520theoretically%2520and%250Aempirically.%2520We%2520propose%2520to%2520use%2520the%2520empirical%2520loss%2520value%2520as%2520a%2520static%2520data%250Apruning%2520criterion.%2520To%2520further%2520compensate%2520for%2520the%2520variation%2520of%2520the%2520data%2520value%2520in%250Atraining%252C%2520we%2520find%2520the%2520most%2520contributing%2520samples%2520based%2520on%2520their%2520causal%2520effects%250Aon%2520the%2520distillation.%2520The%2520proposed%2520selection%2520strategy%2520can%2520efficiently%2520exploit%250Athe%2520training%2520dataset%252C%2520outperform%2520the%2520previous%2520SOTA%2520distillation%2520algorithms%252C%2520and%250Aconsistently%2520enhance%2520the%2520distillation%2520algorithms%252C%2520even%2520on%2520much%2520larger-scale%2520and%250Amore%2520heterogeneous%2520datasets%252C%2520e.g.%252C%2520full%2520ImageNet-1K%2520and%2520Kinetics-400.%2520We%250Abelieve%2520this%2520paradigm%2520will%2520open%2520up%2520new%2520avenues%2520in%2520the%2520dynamics%2520of%2520distillation%250Aand%2520pave%2520the%2520way%2520for%2520efficient%2520dataset%2520distillation.%2520Our%2520code%2520is%2520available%2520on%250Ahttps%253A//github.com/silicx/GoldFromOres-BiLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.18381v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distill%20Gold%20from%20Massive%20Ores%3A%20Bi-level%20Data%20Pruning%20towards%20Efficient%0A%20%20Dataset%20Distillation&entry.906535625=Yue%20Xu%20and%20Yong-Lu%20Li%20and%20Kaitong%20Cui%20and%20Ziyu%20Wang%20and%20Cewu%20Lu%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang&entry.1292438233=%20%20Data-efficient%20learning%20has%20garnered%20significant%20attention%2C%20especially%20given%0Athe%20current%20trend%20of%20large%20multi-modal%20models.%20Recently%2C%20dataset%20distillation%0Ahas%20become%20an%20effective%20approach%20by%20synthesizing%20data%20samples%20that%20are%0Aessential%20for%20network%20training.%20However%2C%20it%20remains%20to%20be%20explored%20which%0Asamples%20are%20essential%20for%20the%20dataset%20distillation%20process%20itself.%20In%20this%0Awork%2C%20we%20study%20the%20data%20efficiency%20and%20selection%20for%20the%20dataset%20distillation%0Atask.%20By%20re-formulating%20the%20dynamics%20of%20distillation%2C%20we%20provide%20insight%20into%0Athe%20inherent%20redundancy%20in%20the%20real%20dataset%2C%20both%20theoretically%20and%0Aempirically.%20We%20propose%20to%20use%20the%20empirical%20loss%20value%20as%20a%20static%20data%0Apruning%20criterion.%20To%20further%20compensate%20for%20the%20variation%20of%20the%20data%20value%20in%0Atraining%2C%20we%20find%20the%20most%20contributing%20samples%20based%20on%20their%20causal%20effects%0Aon%20the%20distillation.%20The%20proposed%20selection%20strategy%20can%20efficiently%20exploit%0Athe%20training%20dataset%2C%20outperform%20the%20previous%20SOTA%20distillation%20algorithms%2C%20and%0Aconsistently%20enhance%20the%20distillation%20algorithms%2C%20even%20on%20much%20larger-scale%20and%0Amore%20heterogeneous%20datasets%2C%20e.g.%2C%20full%20ImageNet-1K%20and%20Kinetics-400.%20We%0Abelieve%20this%20paradigm%20will%20open%20up%20new%20avenues%20in%20the%20dynamics%20of%20distillation%0Aand%20pave%20the%20way%20for%20efficient%20dataset%20distillation.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/silicx/GoldFromOres-BiLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.18381v4&entry.124074799=Read"},
{"title": "Designing Extremely Memory-Efficient CNNs for On-device Vision Tasks", "author": "Jaewook Lee and Yoel Park and Seulki Lee", "abstract": "  In this paper, we introduce a memory-efficient CNN (convolutional neural\nnetwork), which enables resource-constrained low-end embedded and IoT devices\nto perform on-device vision tasks, such as image classification and object\ndetection, using extremely low memory, i.e., only 63 KB on ImageNet\nclassification. Based on the bottleneck block of MobileNet, we propose three\ndesign principles that significantly curtail the peak memory usage of a CNN so\nthat it can fit the limited KB memory of the low-end device. First, 'input\nsegmentation' divides an input image into a set of patches, including the\ncentral patch overlapped with the others, reducing the size (and memory\nrequirement) of a large input image. Second, 'patch tunneling' builds\nindependent tunnel-like paths consisting of multiple bottleneck blocks per\npatch, penetrating through the entire model from an input patch to the last\nlayer of the network, maintaining lightweight memory usage throughout the whole\nnetwork. Lastly, 'bottleneck reordering' rearranges the execution order of\nconvolution operations inside the bottleneck block such that the memory usage\nremains constant regardless of the size of the convolution output channels. The\nexperiment result shows that the proposed network classifies ImageNet with\nextremely low memory (i.e., 63 KB) while achieving competitive top-1 accuracy\n(i.e., 61.58\\%). To the best of our knowledge, the memory usage of the proposed\nnetwork is far smaller than state-of-the-art memory-efficient networks, i.e.,\nup to 89x and 3.1x smaller than MobileNet (i.e., 5.6 MB) and MCUNet (i.e., 196\nKB), respectively.\n", "link": "http://arxiv.org/abs/2408.03663v1", "date": "2024-08-07", "relevancy": 2.0224, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5218}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5127}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20Extremely%20Memory-Efficient%20CNNs%20for%20On-device%20Vision%20Tasks&body=Title%3A%20Designing%20Extremely%20Memory-Efficient%20CNNs%20for%20On-device%20Vision%20Tasks%0AAuthor%3A%20Jaewook%20Lee%20and%20Yoel%20Park%20and%20Seulki%20Lee%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20memory-efficient%20CNN%20%28convolutional%20neural%0Anetwork%29%2C%20which%20enables%20resource-constrained%20low-end%20embedded%20and%20IoT%20devices%0Ato%20perform%20on-device%20vision%20tasks%2C%20such%20as%20image%20classification%20and%20object%0Adetection%2C%20using%20extremely%20low%20memory%2C%20i.e.%2C%20only%2063%20KB%20on%20ImageNet%0Aclassification.%20Based%20on%20the%20bottleneck%20block%20of%20MobileNet%2C%20we%20propose%20three%0Adesign%20principles%20that%20significantly%20curtail%20the%20peak%20memory%20usage%20of%20a%20CNN%20so%0Athat%20it%20can%20fit%20the%20limited%20KB%20memory%20of%20the%20low-end%20device.%20First%2C%20%27input%0Asegmentation%27%20divides%20an%20input%20image%20into%20a%20set%20of%20patches%2C%20including%20the%0Acentral%20patch%20overlapped%20with%20the%20others%2C%20reducing%20the%20size%20%28and%20memory%0Arequirement%29%20of%20a%20large%20input%20image.%20Second%2C%20%27patch%20tunneling%27%20builds%0Aindependent%20tunnel-like%20paths%20consisting%20of%20multiple%20bottleneck%20blocks%20per%0Apatch%2C%20penetrating%20through%20the%20entire%20model%20from%20an%20input%20patch%20to%20the%20last%0Alayer%20of%20the%20network%2C%20maintaining%20lightweight%20memory%20usage%20throughout%20the%20whole%0Anetwork.%20Lastly%2C%20%27bottleneck%20reordering%27%20rearranges%20the%20execution%20order%20of%0Aconvolution%20operations%20inside%20the%20bottleneck%20block%20such%20that%20the%20memory%20usage%0Aremains%20constant%20regardless%20of%20the%20size%20of%20the%20convolution%20output%20channels.%20The%0Aexperiment%20result%20shows%20that%20the%20proposed%20network%20classifies%20ImageNet%20with%0Aextremely%20low%20memory%20%28i.e.%2C%2063%20KB%29%20while%20achieving%20competitive%20top-1%20accuracy%0A%28i.e.%2C%2061.58%5C%25%29.%20To%20the%20best%20of%20our%20knowledge%2C%20the%20memory%20usage%20of%20the%20proposed%0Anetwork%20is%20far%20smaller%20than%20state-of-the-art%20memory-efficient%20networks%2C%20i.e.%2C%0Aup%20to%2089x%20and%203.1x%20smaller%20than%20MobileNet%20%28i.e.%2C%205.6%20MB%29%20and%20MCUNet%20%28i.e.%2C%20196%0AKB%29%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520Extremely%2520Memory-Efficient%2520CNNs%2520for%2520On-device%2520Vision%2520Tasks%26entry.906535625%3DJaewook%2520Lee%2520and%2520Yoel%2520Park%2520and%2520Seulki%2520Lee%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520memory-efficient%2520CNN%2520%2528convolutional%2520neural%250Anetwork%2529%252C%2520which%2520enables%2520resource-constrained%2520low-end%2520embedded%2520and%2520IoT%2520devices%250Ato%2520perform%2520on-device%2520vision%2520tasks%252C%2520such%2520as%2520image%2520classification%2520and%2520object%250Adetection%252C%2520using%2520extremely%2520low%2520memory%252C%2520i.e.%252C%2520only%252063%2520KB%2520on%2520ImageNet%250Aclassification.%2520Based%2520on%2520the%2520bottleneck%2520block%2520of%2520MobileNet%252C%2520we%2520propose%2520three%250Adesign%2520principles%2520that%2520significantly%2520curtail%2520the%2520peak%2520memory%2520usage%2520of%2520a%2520CNN%2520so%250Athat%2520it%2520can%2520fit%2520the%2520limited%2520KB%2520memory%2520of%2520the%2520low-end%2520device.%2520First%252C%2520%2527input%250Asegmentation%2527%2520divides%2520an%2520input%2520image%2520into%2520a%2520set%2520of%2520patches%252C%2520including%2520the%250Acentral%2520patch%2520overlapped%2520with%2520the%2520others%252C%2520reducing%2520the%2520size%2520%2528and%2520memory%250Arequirement%2529%2520of%2520a%2520large%2520input%2520image.%2520Second%252C%2520%2527patch%2520tunneling%2527%2520builds%250Aindependent%2520tunnel-like%2520paths%2520consisting%2520of%2520multiple%2520bottleneck%2520blocks%2520per%250Apatch%252C%2520penetrating%2520through%2520the%2520entire%2520model%2520from%2520an%2520input%2520patch%2520to%2520the%2520last%250Alayer%2520of%2520the%2520network%252C%2520maintaining%2520lightweight%2520memory%2520usage%2520throughout%2520the%2520whole%250Anetwork.%2520Lastly%252C%2520%2527bottleneck%2520reordering%2527%2520rearranges%2520the%2520execution%2520order%2520of%250Aconvolution%2520operations%2520inside%2520the%2520bottleneck%2520block%2520such%2520that%2520the%2520memory%2520usage%250Aremains%2520constant%2520regardless%2520of%2520the%2520size%2520of%2520the%2520convolution%2520output%2520channels.%2520The%250Aexperiment%2520result%2520shows%2520that%2520the%2520proposed%2520network%2520classifies%2520ImageNet%2520with%250Aextremely%2520low%2520memory%2520%2528i.e.%252C%252063%2520KB%2529%2520while%2520achieving%2520competitive%2520top-1%2520accuracy%250A%2528i.e.%252C%252061.58%255C%2525%2529.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520memory%2520usage%2520of%2520the%2520proposed%250Anetwork%2520is%2520far%2520smaller%2520than%2520state-of-the-art%2520memory-efficient%2520networks%252C%2520i.e.%252C%250Aup%2520to%252089x%2520and%25203.1x%2520smaller%2520than%2520MobileNet%2520%2528i.e.%252C%25205.6%2520MB%2529%2520and%2520MCUNet%2520%2528i.e.%252C%2520196%250AKB%2529%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20Extremely%20Memory-Efficient%20CNNs%20for%20On-device%20Vision%20Tasks&entry.906535625=Jaewook%20Lee%20and%20Yoel%20Park%20and%20Seulki%20Lee&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20memory-efficient%20CNN%20%28convolutional%20neural%0Anetwork%29%2C%20which%20enables%20resource-constrained%20low-end%20embedded%20and%20IoT%20devices%0Ato%20perform%20on-device%20vision%20tasks%2C%20such%20as%20image%20classification%20and%20object%0Adetection%2C%20using%20extremely%20low%20memory%2C%20i.e.%2C%20only%2063%20KB%20on%20ImageNet%0Aclassification.%20Based%20on%20the%20bottleneck%20block%20of%20MobileNet%2C%20we%20propose%20three%0Adesign%20principles%20that%20significantly%20curtail%20the%20peak%20memory%20usage%20of%20a%20CNN%20so%0Athat%20it%20can%20fit%20the%20limited%20KB%20memory%20of%20the%20low-end%20device.%20First%2C%20%27input%0Asegmentation%27%20divides%20an%20input%20image%20into%20a%20set%20of%20patches%2C%20including%20the%0Acentral%20patch%20overlapped%20with%20the%20others%2C%20reducing%20the%20size%20%28and%20memory%0Arequirement%29%20of%20a%20large%20input%20image.%20Second%2C%20%27patch%20tunneling%27%20builds%0Aindependent%20tunnel-like%20paths%20consisting%20of%20multiple%20bottleneck%20blocks%20per%0Apatch%2C%20penetrating%20through%20the%20entire%20model%20from%20an%20input%20patch%20to%20the%20last%0Alayer%20of%20the%20network%2C%20maintaining%20lightweight%20memory%20usage%20throughout%20the%20whole%0Anetwork.%20Lastly%2C%20%27bottleneck%20reordering%27%20rearranges%20the%20execution%20order%20of%0Aconvolution%20operations%20inside%20the%20bottleneck%20block%20such%20that%20the%20memory%20usage%0Aremains%20constant%20regardless%20of%20the%20size%20of%20the%20convolution%20output%20channels.%20The%0Aexperiment%20result%20shows%20that%20the%20proposed%20network%20classifies%20ImageNet%20with%0Aextremely%20low%20memory%20%28i.e.%2C%2063%20KB%29%20while%20achieving%20competitive%20top-1%20accuracy%0A%28i.e.%2C%2061.58%5C%25%29.%20To%20the%20best%20of%20our%20knowledge%2C%20the%20memory%20usage%20of%20the%20proposed%0Anetwork%20is%20far%20smaller%20than%20state-of-the-art%20memory-efficient%20networks%2C%20i.e.%2C%0Aup%20to%2089x%20and%203.1x%20smaller%20than%20MobileNet%20%28i.e.%2C%205.6%20MB%29%20and%20MCUNet%20%28i.e.%2C%20196%0AKB%29%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03663v1&entry.124074799=Read"},
{"title": "Fast Sprite Decomposition from Animated Graphics", "author": "Tomoyuki Suzuki and Kotaro Kikuchi and Kota Yamaguchi", "abstract": "  This paper presents an approach to decomposing animated graphics into\nsprites, a set of basic elements or layers. Our approach builds on the\noptimization of sprite parameters to fit the raster video. For efficiency, we\nassume static textures for sprites to reduce the search space while preventing\nartifacts using a texture prior model. To further speed up the optimization, we\nintroduce the initialization of the sprite parameters utilizing a pre-trained\nvideo object segmentation model and user input of single frame annotations. For\nour study, we construct the Crello Animation dataset from an online design\nservice and define quantitative metrics to measure the quality of the extracted\nsprites. Experiments show that our method significantly outperforms baselines\nfor similar decomposition tasks in terms of the quality/efficiency tradeoff.\n", "link": "http://arxiv.org/abs/2408.03923v1", "date": "2024-08-07", "relevancy": 2.0177, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5125}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5077}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Sprite%20Decomposition%20from%20Animated%20Graphics&body=Title%3A%20Fast%20Sprite%20Decomposition%20from%20Animated%20Graphics%0AAuthor%3A%20Tomoyuki%20Suzuki%20and%20Kotaro%20Kikuchi%20and%20Kota%20Yamaguchi%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20approach%20to%20decomposing%20animated%20graphics%20into%0Asprites%2C%20a%20set%20of%20basic%20elements%20or%20layers.%20Our%20approach%20builds%20on%20the%0Aoptimization%20of%20sprite%20parameters%20to%20fit%20the%20raster%20video.%20For%20efficiency%2C%20we%0Aassume%20static%20textures%20for%20sprites%20to%20reduce%20the%20search%20space%20while%20preventing%0Aartifacts%20using%20a%20texture%20prior%20model.%20To%20further%20speed%20up%20the%20optimization%2C%20we%0Aintroduce%20the%20initialization%20of%20the%20sprite%20parameters%20utilizing%20a%20pre-trained%0Avideo%20object%20segmentation%20model%20and%20user%20input%20of%20single%20frame%20annotations.%20For%0Aour%20study%2C%20we%20construct%20the%20Crello%20Animation%20dataset%20from%20an%20online%20design%0Aservice%20and%20define%20quantitative%20metrics%20to%20measure%20the%20quality%20of%20the%20extracted%0Asprites.%20Experiments%20show%20that%20our%20method%20significantly%20outperforms%20baselines%0Afor%20similar%20decomposition%20tasks%20in%20terms%20of%20the%20quality/efficiency%20tradeoff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Sprite%2520Decomposition%2520from%2520Animated%2520Graphics%26entry.906535625%3DTomoyuki%2520Suzuki%2520and%2520Kotaro%2520Kikuchi%2520and%2520Kota%2520Yamaguchi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520approach%2520to%2520decomposing%2520animated%2520graphics%2520into%250Asprites%252C%2520a%2520set%2520of%2520basic%2520elements%2520or%2520layers.%2520Our%2520approach%2520builds%2520on%2520the%250Aoptimization%2520of%2520sprite%2520parameters%2520to%2520fit%2520the%2520raster%2520video.%2520For%2520efficiency%252C%2520we%250Aassume%2520static%2520textures%2520for%2520sprites%2520to%2520reduce%2520the%2520search%2520space%2520while%2520preventing%250Aartifacts%2520using%2520a%2520texture%2520prior%2520model.%2520To%2520further%2520speed%2520up%2520the%2520optimization%252C%2520we%250Aintroduce%2520the%2520initialization%2520of%2520the%2520sprite%2520parameters%2520utilizing%2520a%2520pre-trained%250Avideo%2520object%2520segmentation%2520model%2520and%2520user%2520input%2520of%2520single%2520frame%2520annotations.%2520For%250Aour%2520study%252C%2520we%2520construct%2520the%2520Crello%2520Animation%2520dataset%2520from%2520an%2520online%2520design%250Aservice%2520and%2520define%2520quantitative%2520metrics%2520to%2520measure%2520the%2520quality%2520of%2520the%2520extracted%250Asprites.%2520Experiments%2520show%2520that%2520our%2520method%2520significantly%2520outperforms%2520baselines%250Afor%2520similar%2520decomposition%2520tasks%2520in%2520terms%2520of%2520the%2520quality/efficiency%2520tradeoff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Sprite%20Decomposition%20from%20Animated%20Graphics&entry.906535625=Tomoyuki%20Suzuki%20and%20Kotaro%20Kikuchi%20and%20Kota%20Yamaguchi&entry.1292438233=%20%20This%20paper%20presents%20an%20approach%20to%20decomposing%20animated%20graphics%20into%0Asprites%2C%20a%20set%20of%20basic%20elements%20or%20layers.%20Our%20approach%20builds%20on%20the%0Aoptimization%20of%20sprite%20parameters%20to%20fit%20the%20raster%20video.%20For%20efficiency%2C%20we%0Aassume%20static%20textures%20for%20sprites%20to%20reduce%20the%20search%20space%20while%20preventing%0Aartifacts%20using%20a%20texture%20prior%20model.%20To%20further%20speed%20up%20the%20optimization%2C%20we%0Aintroduce%20the%20initialization%20of%20the%20sprite%20parameters%20utilizing%20a%20pre-trained%0Avideo%20object%20segmentation%20model%20and%20user%20input%20of%20single%20frame%20annotations.%20For%0Aour%20study%2C%20we%20construct%20the%20Crello%20Animation%20dataset%20from%20an%20online%20design%0Aservice%20and%20define%20quantitative%20metrics%20to%20measure%20the%20quality%20of%20the%20extracted%0Asprites.%20Experiments%20show%20that%20our%20method%20significantly%20outperforms%20baselines%0Afor%20similar%20decomposition%20tasks%20in%20terms%20of%20the%20quality/efficiency%20tradeoff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03923v1&entry.124074799=Read"},
{"title": "MAO: A Framework for Process Model Generation with Multi-Agent\n  Orchestration", "author": "Leilei Lin and Yumeng Jin and Yingming Zhou and Wenlong Chen and Chen Qian", "abstract": "  Process models are frequently used in software engineering to describe\nbusiness requirements, guide software testing and control system improvement.\nHowever, traditional process modeling methods often require the participation\nof numerous experts, which is expensive and time-consuming. Therefore, the\nexploration of a more efficient and cost-effective automated modeling method\nhas emerged as a focal point in current research. This article explores a\nframework for automatically generating process models with multi-agent\norchestration (MAO), aiming to enhance the efficiency of process modeling and\noffer valuable insights for domain experts. Our framework MAO leverages large\nlanguage models as the cornerstone for multi-agent, employing an innovative\nprompt strategy to ensure efficient collaboration among multi-agent.\nSpecifically, 1) generation. The first phase of MAO is to generate a slightly\nrough process model from the text description; 2) refinement. The agents would\ncontinuously refine the initial process model through multiple rounds of\ndialogue; 3) reviewing. Large language models are prone to hallucination\nphenomena among multi-turn dialogues, so the agents need to review and repair\nsemantic hallucinations in process models; 4) testing. The representation of\nprocess models is diverse. Consequently, the agents utilize external tools to\ntest whether the generated process model contains format errors, namely format\nhallucinations, and then adjust the process model to conform to the output\nparadigm. The experiments demonstrate that the process models generated by our\nframework outperform existing methods and surpass manual modeling by 89%, 61%,\n52%, and 75% on four different datasets, respectively.\n", "link": "http://arxiv.org/abs/2408.01916v2", "date": "2024-08-07", "relevancy": 2.0171, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.53}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5176}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAO%3A%20A%20Framework%20for%20Process%20Model%20Generation%20with%20Multi-Agent%0A%20%20Orchestration&body=Title%3A%20MAO%3A%20A%20Framework%20for%20Process%20Model%20Generation%20with%20Multi-Agent%0A%20%20Orchestration%0AAuthor%3A%20Leilei%20Lin%20and%20Yumeng%20Jin%20and%20Yingming%20Zhou%20and%20Wenlong%20Chen%20and%20Chen%20Qian%0AAbstract%3A%20%20%20Process%20models%20are%20frequently%20used%20in%20software%20engineering%20to%20describe%0Abusiness%20requirements%2C%20guide%20software%20testing%20and%20control%20system%20improvement.%0AHowever%2C%20traditional%20process%20modeling%20methods%20often%20require%20the%20participation%0Aof%20numerous%20experts%2C%20which%20is%20expensive%20and%20time-consuming.%20Therefore%2C%20the%0Aexploration%20of%20a%20more%20efficient%20and%20cost-effective%20automated%20modeling%20method%0Ahas%20emerged%20as%20a%20focal%20point%20in%20current%20research.%20This%20article%20explores%20a%0Aframework%20for%20automatically%20generating%20process%20models%20with%20multi-agent%0Aorchestration%20%28MAO%29%2C%20aiming%20to%20enhance%20the%20efficiency%20of%20process%20modeling%20and%0Aoffer%20valuable%20insights%20for%20domain%20experts.%20Our%20framework%20MAO%20leverages%20large%0Alanguage%20models%20as%20the%20cornerstone%20for%20multi-agent%2C%20employing%20an%20innovative%0Aprompt%20strategy%20to%20ensure%20efficient%20collaboration%20among%20multi-agent.%0ASpecifically%2C%201%29%20generation.%20The%20first%20phase%20of%20MAO%20is%20to%20generate%20a%20slightly%0Arough%20process%20model%20from%20the%20text%20description%3B%202%29%20refinement.%20The%20agents%20would%0Acontinuously%20refine%20the%20initial%20process%20model%20through%20multiple%20rounds%20of%0Adialogue%3B%203%29%20reviewing.%20Large%20language%20models%20are%20prone%20to%20hallucination%0Aphenomena%20among%20multi-turn%20dialogues%2C%20so%20the%20agents%20need%20to%20review%20and%20repair%0Asemantic%20hallucinations%20in%20process%20models%3B%204%29%20testing.%20The%20representation%20of%0Aprocess%20models%20is%20diverse.%20Consequently%2C%20the%20agents%20utilize%20external%20tools%20to%0Atest%20whether%20the%20generated%20process%20model%20contains%20format%20errors%2C%20namely%20format%0Ahallucinations%2C%20and%20then%20adjust%20the%20process%20model%20to%20conform%20to%20the%20output%0Aparadigm.%20The%20experiments%20demonstrate%20that%20the%20process%20models%20generated%20by%20our%0Aframework%20outperform%20existing%20methods%20and%20surpass%20manual%20modeling%20by%2089%25%2C%2061%25%2C%0A52%25%2C%20and%2075%25%20on%20four%20different%20datasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01916v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAO%253A%2520A%2520Framework%2520for%2520Process%2520Model%2520Generation%2520with%2520Multi-Agent%250A%2520%2520Orchestration%26entry.906535625%3DLeilei%2520Lin%2520and%2520Yumeng%2520Jin%2520and%2520Yingming%2520Zhou%2520and%2520Wenlong%2520Chen%2520and%2520Chen%2520Qian%26entry.1292438233%3D%2520%2520Process%2520models%2520are%2520frequently%2520used%2520in%2520software%2520engineering%2520to%2520describe%250Abusiness%2520requirements%252C%2520guide%2520software%2520testing%2520and%2520control%2520system%2520improvement.%250AHowever%252C%2520traditional%2520process%2520modeling%2520methods%2520often%2520require%2520the%2520participation%250Aof%2520numerous%2520experts%252C%2520which%2520is%2520expensive%2520and%2520time-consuming.%2520Therefore%252C%2520the%250Aexploration%2520of%2520a%2520more%2520efficient%2520and%2520cost-effective%2520automated%2520modeling%2520method%250Ahas%2520emerged%2520as%2520a%2520focal%2520point%2520in%2520current%2520research.%2520This%2520article%2520explores%2520a%250Aframework%2520for%2520automatically%2520generating%2520process%2520models%2520with%2520multi-agent%250Aorchestration%2520%2528MAO%2529%252C%2520aiming%2520to%2520enhance%2520the%2520efficiency%2520of%2520process%2520modeling%2520and%250Aoffer%2520valuable%2520insights%2520for%2520domain%2520experts.%2520Our%2520framework%2520MAO%2520leverages%2520large%250Alanguage%2520models%2520as%2520the%2520cornerstone%2520for%2520multi-agent%252C%2520employing%2520an%2520innovative%250Aprompt%2520strategy%2520to%2520ensure%2520efficient%2520collaboration%2520among%2520multi-agent.%250ASpecifically%252C%25201%2529%2520generation.%2520The%2520first%2520phase%2520of%2520MAO%2520is%2520to%2520generate%2520a%2520slightly%250Arough%2520process%2520model%2520from%2520the%2520text%2520description%253B%25202%2529%2520refinement.%2520The%2520agents%2520would%250Acontinuously%2520refine%2520the%2520initial%2520process%2520model%2520through%2520multiple%2520rounds%2520of%250Adialogue%253B%25203%2529%2520reviewing.%2520Large%2520language%2520models%2520are%2520prone%2520to%2520hallucination%250Aphenomena%2520among%2520multi-turn%2520dialogues%252C%2520so%2520the%2520agents%2520need%2520to%2520review%2520and%2520repair%250Asemantic%2520hallucinations%2520in%2520process%2520models%253B%25204%2529%2520testing.%2520The%2520representation%2520of%250Aprocess%2520models%2520is%2520diverse.%2520Consequently%252C%2520the%2520agents%2520utilize%2520external%2520tools%2520to%250Atest%2520whether%2520the%2520generated%2520process%2520model%2520contains%2520format%2520errors%252C%2520namely%2520format%250Ahallucinations%252C%2520and%2520then%2520adjust%2520the%2520process%2520model%2520to%2520conform%2520to%2520the%2520output%250Aparadigm.%2520The%2520experiments%2520demonstrate%2520that%2520the%2520process%2520models%2520generated%2520by%2520our%250Aframework%2520outperform%2520existing%2520methods%2520and%2520surpass%2520manual%2520modeling%2520by%252089%2525%252C%252061%2525%252C%250A52%2525%252C%2520and%252075%2525%2520on%2520four%2520different%2520datasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01916v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAO%3A%20A%20Framework%20for%20Process%20Model%20Generation%20with%20Multi-Agent%0A%20%20Orchestration&entry.906535625=Leilei%20Lin%20and%20Yumeng%20Jin%20and%20Yingming%20Zhou%20and%20Wenlong%20Chen%20and%20Chen%20Qian&entry.1292438233=%20%20Process%20models%20are%20frequently%20used%20in%20software%20engineering%20to%20describe%0Abusiness%20requirements%2C%20guide%20software%20testing%20and%20control%20system%20improvement.%0AHowever%2C%20traditional%20process%20modeling%20methods%20often%20require%20the%20participation%0Aof%20numerous%20experts%2C%20which%20is%20expensive%20and%20time-consuming.%20Therefore%2C%20the%0Aexploration%20of%20a%20more%20efficient%20and%20cost-effective%20automated%20modeling%20method%0Ahas%20emerged%20as%20a%20focal%20point%20in%20current%20research.%20This%20article%20explores%20a%0Aframework%20for%20automatically%20generating%20process%20models%20with%20multi-agent%0Aorchestration%20%28MAO%29%2C%20aiming%20to%20enhance%20the%20efficiency%20of%20process%20modeling%20and%0Aoffer%20valuable%20insights%20for%20domain%20experts.%20Our%20framework%20MAO%20leverages%20large%0Alanguage%20models%20as%20the%20cornerstone%20for%20multi-agent%2C%20employing%20an%20innovative%0Aprompt%20strategy%20to%20ensure%20efficient%20collaboration%20among%20multi-agent.%0ASpecifically%2C%201%29%20generation.%20The%20first%20phase%20of%20MAO%20is%20to%20generate%20a%20slightly%0Arough%20process%20model%20from%20the%20text%20description%3B%202%29%20refinement.%20The%20agents%20would%0Acontinuously%20refine%20the%20initial%20process%20model%20through%20multiple%20rounds%20of%0Adialogue%3B%203%29%20reviewing.%20Large%20language%20models%20are%20prone%20to%20hallucination%0Aphenomena%20among%20multi-turn%20dialogues%2C%20so%20the%20agents%20need%20to%20review%20and%20repair%0Asemantic%20hallucinations%20in%20process%20models%3B%204%29%20testing.%20The%20representation%20of%0Aprocess%20models%20is%20diverse.%20Consequently%2C%20the%20agents%20utilize%20external%20tools%20to%0Atest%20whether%20the%20generated%20process%20model%20contains%20format%20errors%2C%20namely%20format%0Ahallucinations%2C%20and%20then%20adjust%20the%20process%20model%20to%20conform%20to%20the%20output%0Aparadigm.%20The%20experiments%20demonstrate%20that%20the%20process%20models%20generated%20by%20our%0Aframework%20outperform%20existing%20methods%20and%20surpass%20manual%20modeling%20by%2089%25%2C%2061%25%2C%0A52%25%2C%20and%2075%25%20on%20four%20different%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01916v2&entry.124074799=Read"},
{"title": "G-invariant diffusion maps", "author": "Eitan Rosen and Xiuyuan Cheng and Yoel Shkolnisky", "abstract": "  The diffusion maps embedding of data lying on a manifold has shown success in\ntasks such as dimensionality reduction, clustering, and data visualization. In\nthis work, we consider embedding data sets that were sampled from a manifold\nwhich is closed under the action of a continuous matrix group. An example of\nsuch a data set is images whose planar rotations are arbitrary. The G-invariant\ngraph Laplacian, introduced in Part I of this work, admits eigenfunctions in\nthe form of tensor products between the elements of the irreducible unitary\nrepresentations of the group and eigenvectors of certain matrices. We employ\nthese eigenfunctions to derive diffusion maps that intrinsically account for\nthe group action on the data. In particular, we construct both equivariant and\ninvariant embeddings, which can be used to cluster and align the data points.\nWe demonstrate the utility of our construction in the problem of random\ncomputerized tomography.\n", "link": "http://arxiv.org/abs/2306.07350v3", "date": "2024-08-07", "relevancy": 1.9999, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5117}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-invariant%20diffusion%20maps&body=Title%3A%20G-invariant%20diffusion%20maps%0AAuthor%3A%20Eitan%20Rosen%20and%20Xiuyuan%20Cheng%20and%20Yoel%20Shkolnisky%0AAbstract%3A%20%20%20The%20diffusion%20maps%20embedding%20of%20data%20lying%20on%20a%20manifold%20has%20shown%20success%20in%0Atasks%20such%20as%20dimensionality%20reduction%2C%20clustering%2C%20and%20data%20visualization.%20In%0Athis%20work%2C%20we%20consider%20embedding%20data%20sets%20that%20were%20sampled%20from%20a%20manifold%0Awhich%20is%20closed%20under%20the%20action%20of%20a%20continuous%20matrix%20group.%20An%20example%20of%0Asuch%20a%20data%20set%20is%20images%20whose%20planar%20rotations%20are%20arbitrary.%20The%20G-invariant%0Agraph%20Laplacian%2C%20introduced%20in%20Part%20I%20of%20this%20work%2C%20admits%20eigenfunctions%20in%0Athe%20form%20of%20tensor%20products%20between%20the%20elements%20of%20the%20irreducible%20unitary%0Arepresentations%20of%20the%20group%20and%20eigenvectors%20of%20certain%20matrices.%20We%20employ%0Athese%20eigenfunctions%20to%20derive%20diffusion%20maps%20that%20intrinsically%20account%20for%0Athe%20group%20action%20on%20the%20data.%20In%20particular%2C%20we%20construct%20both%20equivariant%20and%0Ainvariant%20embeddings%2C%20which%20can%20be%20used%20to%20cluster%20and%20align%20the%20data%20points.%0AWe%20demonstrate%20the%20utility%20of%20our%20construction%20in%20the%20problem%20of%20random%0Acomputerized%20tomography.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07350v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-invariant%2520diffusion%2520maps%26entry.906535625%3DEitan%2520Rosen%2520and%2520Xiuyuan%2520Cheng%2520and%2520Yoel%2520Shkolnisky%26entry.1292438233%3D%2520%2520The%2520diffusion%2520maps%2520embedding%2520of%2520data%2520lying%2520on%2520a%2520manifold%2520has%2520shown%2520success%2520in%250Atasks%2520such%2520as%2520dimensionality%2520reduction%252C%2520clustering%252C%2520and%2520data%2520visualization.%2520In%250Athis%2520work%252C%2520we%2520consider%2520embedding%2520data%2520sets%2520that%2520were%2520sampled%2520from%2520a%2520manifold%250Awhich%2520is%2520closed%2520under%2520the%2520action%2520of%2520a%2520continuous%2520matrix%2520group.%2520An%2520example%2520of%250Asuch%2520a%2520data%2520set%2520is%2520images%2520whose%2520planar%2520rotations%2520are%2520arbitrary.%2520The%2520G-invariant%250Agraph%2520Laplacian%252C%2520introduced%2520in%2520Part%2520I%2520of%2520this%2520work%252C%2520admits%2520eigenfunctions%2520in%250Athe%2520form%2520of%2520tensor%2520products%2520between%2520the%2520elements%2520of%2520the%2520irreducible%2520unitary%250Arepresentations%2520of%2520the%2520group%2520and%2520eigenvectors%2520of%2520certain%2520matrices.%2520We%2520employ%250Athese%2520eigenfunctions%2520to%2520derive%2520diffusion%2520maps%2520that%2520intrinsically%2520account%2520for%250Athe%2520group%2520action%2520on%2520the%2520data.%2520In%2520particular%252C%2520we%2520construct%2520both%2520equivariant%2520and%250Ainvariant%2520embeddings%252C%2520which%2520can%2520be%2520used%2520to%2520cluster%2520and%2520align%2520the%2520data%2520points.%250AWe%2520demonstrate%2520the%2520utility%2520of%2520our%2520construction%2520in%2520the%2520problem%2520of%2520random%250Acomputerized%2520tomography.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07350v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-invariant%20diffusion%20maps&entry.906535625=Eitan%20Rosen%20and%20Xiuyuan%20Cheng%20and%20Yoel%20Shkolnisky&entry.1292438233=%20%20The%20diffusion%20maps%20embedding%20of%20data%20lying%20on%20a%20manifold%20has%20shown%20success%20in%0Atasks%20such%20as%20dimensionality%20reduction%2C%20clustering%2C%20and%20data%20visualization.%20In%0Athis%20work%2C%20we%20consider%20embedding%20data%20sets%20that%20were%20sampled%20from%20a%20manifold%0Awhich%20is%20closed%20under%20the%20action%20of%20a%20continuous%20matrix%20group.%20An%20example%20of%0Asuch%20a%20data%20set%20is%20images%20whose%20planar%20rotations%20are%20arbitrary.%20The%20G-invariant%0Agraph%20Laplacian%2C%20introduced%20in%20Part%20I%20of%20this%20work%2C%20admits%20eigenfunctions%20in%0Athe%20form%20of%20tensor%20products%20between%20the%20elements%20of%20the%20irreducible%20unitary%0Arepresentations%20of%20the%20group%20and%20eigenvectors%20of%20certain%20matrices.%20We%20employ%0Athese%20eigenfunctions%20to%20derive%20diffusion%20maps%20that%20intrinsically%20account%20for%0Athe%20group%20action%20on%20the%20data.%20In%20particular%2C%20we%20construct%20both%20equivariant%20and%0Ainvariant%20embeddings%2C%20which%20can%20be%20used%20to%20cluster%20and%20align%20the%20data%20points.%0AWe%20demonstrate%20the%20utility%20of%20our%20construction%20in%20the%20problem%20of%20random%0Acomputerized%20tomography.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07350v3&entry.124074799=Read"},
{"title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue\n  System with Transfer Capabilities", "author": "Ming Zhang and Caishuang Huang and Yilong Wu and Shichun Liu and Huiyuan Zheng and Yurui Dong and Yujiong Shen and Shihan Dou and Jun Zhao and Junjie Ye and Qi Zhang and Tao Gui and Xuanjing Huang", "abstract": "  Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented\nconversations, including information collection. How to utilize TOD accurately,\nefficiently and effectively for information collection has always been a\ncritical and challenging task. Recent studies have demonstrated that Large\nLanguage Models (LLMs) excel in dialogue, instruction generation, and\nreasoning, and can significantly enhance the performance of TOD through\nfine-tuning. However, current datasets primarily cater to user-led systems and\nare limited to predefined specific scenarios and slots, thereby necessitating\nimprovements in the proactiveness, diversity, and capabilities of TOD. In this\nstudy, we present a detailed multi-domain task-oriented data construction\nprocess for conversations, and a Chinese dialogue dataset generated based on\nthis process, TransferTOD, which authentically simulates human-computer\ndialogues in 30 popular life service scenarios. Leveraging this dataset, we\ntrained a model called TransferTOD-7B using full-parameter fine-tuning,\nshowcasing notable abilities in slot filling and questioning. Our work has\ndemonstrated its strong generalization capabilities in various downstream\nscenarios, significantly enhancing both data utilization efficiency and system\nperformance. The data is released in\nhttps://github.com/KongLongGeFDU/TransferTOD.\n", "link": "http://arxiv.org/abs/2407.21693v2", "date": "2024-08-07", "relevancy": 1.9953, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4797}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransferTOD%3A%20A%20Generalizable%20Chinese%20Multi-Domain%20Task-Oriented%20Dialogue%0A%20%20System%20with%20Transfer%20Capabilities&body=Title%3A%20TransferTOD%3A%20A%20Generalizable%20Chinese%20Multi-Domain%20Task-Oriented%20Dialogue%0A%20%20System%20with%20Transfer%20Capabilities%0AAuthor%3A%20Ming%20Zhang%20and%20Caishuang%20Huang%20and%20Yilong%20Wu%20and%20Shichun%20Liu%20and%20Huiyuan%20Zheng%20and%20Yurui%20Dong%20and%20Yujiong%20Shen%20and%20Shihan%20Dou%20and%20Jun%20Zhao%20and%20Junjie%20Ye%20and%20Qi%20Zhang%20and%20Tao%20Gui%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20Task-oriented%20dialogue%20%28TOD%29%20systems%20aim%20to%20efficiently%20handle%20task-oriented%0Aconversations%2C%20including%20information%20collection.%20How%20to%20utilize%20TOD%20accurately%2C%0Aefficiently%20and%20effectively%20for%20information%20collection%20has%20always%20been%20a%0Acritical%20and%20challenging%20task.%20Recent%20studies%20have%20demonstrated%20that%20Large%0ALanguage%20Models%20%28LLMs%29%20excel%20in%20dialogue%2C%20instruction%20generation%2C%20and%0Areasoning%2C%20and%20can%20significantly%20enhance%20the%20performance%20of%20TOD%20through%0Afine-tuning.%20However%2C%20current%20datasets%20primarily%20cater%20to%20user-led%20systems%20and%0Aare%20limited%20to%20predefined%20specific%20scenarios%20and%20slots%2C%20thereby%20necessitating%0Aimprovements%20in%20the%20proactiveness%2C%20diversity%2C%20and%20capabilities%20of%20TOD.%20In%20this%0Astudy%2C%20we%20present%20a%20detailed%20multi-domain%20task-oriented%20data%20construction%0Aprocess%20for%20conversations%2C%20and%20a%20Chinese%20dialogue%20dataset%20generated%20based%20on%0Athis%20process%2C%20TransferTOD%2C%20which%20authentically%20simulates%20human-computer%0Adialogues%20in%2030%20popular%20life%20service%20scenarios.%20Leveraging%20this%20dataset%2C%20we%0Atrained%20a%20model%20called%20TransferTOD-7B%20using%20full-parameter%20fine-tuning%2C%0Ashowcasing%20notable%20abilities%20in%20slot%20filling%20and%20questioning.%20Our%20work%20has%0Ademonstrated%20its%20strong%20generalization%20capabilities%20in%20various%20downstream%0Ascenarios%2C%20significantly%20enhancing%20both%20data%20utilization%20efficiency%20and%20system%0Aperformance.%20The%20data%20is%20released%20in%0Ahttps%3A//github.com/KongLongGeFDU/TransferTOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferTOD%253A%2520A%2520Generalizable%2520Chinese%2520Multi-Domain%2520Task-Oriented%2520Dialogue%250A%2520%2520System%2520with%2520Transfer%2520Capabilities%26entry.906535625%3DMing%2520Zhang%2520and%2520Caishuang%2520Huang%2520and%2520Yilong%2520Wu%2520and%2520Shichun%2520Liu%2520and%2520Huiyuan%2520Zheng%2520and%2520Yurui%2520Dong%2520and%2520Yujiong%2520Shen%2520and%2520Shihan%2520Dou%2520and%2520Jun%2520Zhao%2520and%2520Junjie%2520Ye%2520and%2520Qi%2520Zhang%2520and%2520Tao%2520Gui%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520Task-oriented%2520dialogue%2520%2528TOD%2529%2520systems%2520aim%2520to%2520efficiently%2520handle%2520task-oriented%250Aconversations%252C%2520including%2520information%2520collection.%2520How%2520to%2520utilize%2520TOD%2520accurately%252C%250Aefficiently%2520and%2520effectively%2520for%2520information%2520collection%2520has%2520always%2520been%2520a%250Acritical%2520and%2520challenging%2520task.%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520dialogue%252C%2520instruction%2520generation%252C%2520and%250Areasoning%252C%2520and%2520can%2520significantly%2520enhance%2520the%2520performance%2520of%2520TOD%2520through%250Afine-tuning.%2520However%252C%2520current%2520datasets%2520primarily%2520cater%2520to%2520user-led%2520systems%2520and%250Aare%2520limited%2520to%2520predefined%2520specific%2520scenarios%2520and%2520slots%252C%2520thereby%2520necessitating%250Aimprovements%2520in%2520the%2520proactiveness%252C%2520diversity%252C%2520and%2520capabilities%2520of%2520TOD.%2520In%2520this%250Astudy%252C%2520we%2520present%2520a%2520detailed%2520multi-domain%2520task-oriented%2520data%2520construction%250Aprocess%2520for%2520conversations%252C%2520and%2520a%2520Chinese%2520dialogue%2520dataset%2520generated%2520based%2520on%250Athis%2520process%252C%2520TransferTOD%252C%2520which%2520authentically%2520simulates%2520human-computer%250Adialogues%2520in%252030%2520popular%2520life%2520service%2520scenarios.%2520Leveraging%2520this%2520dataset%252C%2520we%250Atrained%2520a%2520model%2520called%2520TransferTOD-7B%2520using%2520full-parameter%2520fine-tuning%252C%250Ashowcasing%2520notable%2520abilities%2520in%2520slot%2520filling%2520and%2520questioning.%2520Our%2520work%2520has%250Ademonstrated%2520its%2520strong%2520generalization%2520capabilities%2520in%2520various%2520downstream%250Ascenarios%252C%2520significantly%2520enhancing%2520both%2520data%2520utilization%2520efficiency%2520and%2520system%250Aperformance.%2520The%2520data%2520is%2520released%2520in%250Ahttps%253A//github.com/KongLongGeFDU/TransferTOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransferTOD%3A%20A%20Generalizable%20Chinese%20Multi-Domain%20Task-Oriented%20Dialogue%0A%20%20System%20with%20Transfer%20Capabilities&entry.906535625=Ming%20Zhang%20and%20Caishuang%20Huang%20and%20Yilong%20Wu%20and%20Shichun%20Liu%20and%20Huiyuan%20Zheng%20and%20Yurui%20Dong%20and%20Yujiong%20Shen%20and%20Shihan%20Dou%20and%20Jun%20Zhao%20and%20Junjie%20Ye%20and%20Qi%20Zhang%20and%20Tao%20Gui%20and%20Xuanjing%20Huang&entry.1292438233=%20%20Task-oriented%20dialogue%20%28TOD%29%20systems%20aim%20to%20efficiently%20handle%20task-oriented%0Aconversations%2C%20including%20information%20collection.%20How%20to%20utilize%20TOD%20accurately%2C%0Aefficiently%20and%20effectively%20for%20information%20collection%20has%20always%20been%20a%0Acritical%20and%20challenging%20task.%20Recent%20studies%20have%20demonstrated%20that%20Large%0ALanguage%20Models%20%28LLMs%29%20excel%20in%20dialogue%2C%20instruction%20generation%2C%20and%0Areasoning%2C%20and%20can%20significantly%20enhance%20the%20performance%20of%20TOD%20through%0Afine-tuning.%20However%2C%20current%20datasets%20primarily%20cater%20to%20user-led%20systems%20and%0Aare%20limited%20to%20predefined%20specific%20scenarios%20and%20slots%2C%20thereby%20necessitating%0Aimprovements%20in%20the%20proactiveness%2C%20diversity%2C%20and%20capabilities%20of%20TOD.%20In%20this%0Astudy%2C%20we%20present%20a%20detailed%20multi-domain%20task-oriented%20data%20construction%0Aprocess%20for%20conversations%2C%20and%20a%20Chinese%20dialogue%20dataset%20generated%20based%20on%0Athis%20process%2C%20TransferTOD%2C%20which%20authentically%20simulates%20human-computer%0Adialogues%20in%2030%20popular%20life%20service%20scenarios.%20Leveraging%20this%20dataset%2C%20we%0Atrained%20a%20model%20called%20TransferTOD-7B%20using%20full-parameter%20fine-tuning%2C%0Ashowcasing%20notable%20abilities%20in%20slot%20filling%20and%20questioning.%20Our%20work%20has%0Ademonstrated%20its%20strong%20generalization%20capabilities%20in%20various%20downstream%0Ascenarios%2C%20significantly%20enhancing%20both%20data%20utilization%20efficiency%20and%20system%0Aperformance.%20The%20data%20is%20released%20in%0Ahttps%3A//github.com/KongLongGeFDU/TransferTOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21693v2&entry.124074799=Read"},
{"title": "Generative Language Models with Retrieval Augmented Generation for\n  Automated Short Answer Scoring", "author": "Zifan Wang and Christopher Ormerod", "abstract": "  Automated Short Answer Scoring (ASAS) is a critical component in educational\nassessment. While traditional ASAS systems relied on rule-based algorithms or\ncomplex deep learning methods, recent advancements in Generative Language\nModels (GLMs) offer new opportunities for improvement. This study explores the\napplication of GLMs to ASAS, leveraging their off-the-shelf capabilities and\nperformance in various domains. We propose a novel pipeline that combines\nvector databases, transformer-based encoders, and GLMs to enhance short answer\nscoring accuracy. Our approach stores training responses in a vector database,\nretrieves semantically similar responses during inference, and employs a GLM to\nanalyze these responses and determine appropriate scores. We further optimize\nthe system through fine-tuned retrieval processes and prompt engineering.\nEvaluation on the SemEval 2013 dataset demonstrates a significant improvement\non the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,\nhighlighting the potential of GLMs in advancing ASAS technology.\n", "link": "http://arxiv.org/abs/2408.03811v1", "date": "2024-08-07", "relevancy": 1.9846, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5078}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4939}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Language%20Models%20with%20Retrieval%20Augmented%20Generation%20for%0A%20%20Automated%20Short%20Answer%20Scoring&body=Title%3A%20Generative%20Language%20Models%20with%20Retrieval%20Augmented%20Generation%20for%0A%20%20Automated%20Short%20Answer%20Scoring%0AAuthor%3A%20Zifan%20Wang%20and%20Christopher%20Ormerod%0AAbstract%3A%20%20%20Automated%20Short%20Answer%20Scoring%20%28ASAS%29%20is%20a%20critical%20component%20in%20educational%0Aassessment.%20While%20traditional%20ASAS%20systems%20relied%20on%20rule-based%20algorithms%20or%0Acomplex%20deep%20learning%20methods%2C%20recent%20advancements%20in%20Generative%20Language%0AModels%20%28GLMs%29%20offer%20new%20opportunities%20for%20improvement.%20This%20study%20explores%20the%0Aapplication%20of%20GLMs%20to%20ASAS%2C%20leveraging%20their%20off-the-shelf%20capabilities%20and%0Aperformance%20in%20various%20domains.%20We%20propose%20a%20novel%20pipeline%20that%20combines%0Avector%20databases%2C%20transformer-based%20encoders%2C%20and%20GLMs%20to%20enhance%20short%20answer%0Ascoring%20accuracy.%20Our%20approach%20stores%20training%20responses%20in%20a%20vector%20database%2C%0Aretrieves%20semantically%20similar%20responses%20during%20inference%2C%20and%20employs%20a%20GLM%20to%0Aanalyze%20these%20responses%20and%20determine%20appropriate%20scores.%20We%20further%20optimize%0Athe%20system%20through%20fine-tuned%20retrieval%20processes%20and%20prompt%20engineering.%0AEvaluation%20on%20the%20SemEval%202013%20dataset%20demonstrates%20a%20significant%20improvement%0Aon%20the%20SCIENTSBANK%203-way%20and%202-way%20tasks%20compared%20to%20existing%20methods%2C%0Ahighlighting%20the%20potential%20of%20GLMs%20in%20advancing%20ASAS%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Language%2520Models%2520with%2520Retrieval%2520Augmented%2520Generation%2520for%250A%2520%2520Automated%2520Short%2520Answer%2520Scoring%26entry.906535625%3DZifan%2520Wang%2520and%2520Christopher%2520Ormerod%26entry.1292438233%3D%2520%2520Automated%2520Short%2520Answer%2520Scoring%2520%2528ASAS%2529%2520is%2520a%2520critical%2520component%2520in%2520educational%250Aassessment.%2520While%2520traditional%2520ASAS%2520systems%2520relied%2520on%2520rule-based%2520algorithms%2520or%250Acomplex%2520deep%2520learning%2520methods%252C%2520recent%2520advancements%2520in%2520Generative%2520Language%250AModels%2520%2528GLMs%2529%2520offer%2520new%2520opportunities%2520for%2520improvement.%2520This%2520study%2520explores%2520the%250Aapplication%2520of%2520GLMs%2520to%2520ASAS%252C%2520leveraging%2520their%2520off-the-shelf%2520capabilities%2520and%250Aperformance%2520in%2520various%2520domains.%2520We%2520propose%2520a%2520novel%2520pipeline%2520that%2520combines%250Avector%2520databases%252C%2520transformer-based%2520encoders%252C%2520and%2520GLMs%2520to%2520enhance%2520short%2520answer%250Ascoring%2520accuracy.%2520Our%2520approach%2520stores%2520training%2520responses%2520in%2520a%2520vector%2520database%252C%250Aretrieves%2520semantically%2520similar%2520responses%2520during%2520inference%252C%2520and%2520employs%2520a%2520GLM%2520to%250Aanalyze%2520these%2520responses%2520and%2520determine%2520appropriate%2520scores.%2520We%2520further%2520optimize%250Athe%2520system%2520through%2520fine-tuned%2520retrieval%2520processes%2520and%2520prompt%2520engineering.%250AEvaluation%2520on%2520the%2520SemEval%25202013%2520dataset%2520demonstrates%2520a%2520significant%2520improvement%250Aon%2520the%2520SCIENTSBANK%25203-way%2520and%25202-way%2520tasks%2520compared%2520to%2520existing%2520methods%252C%250Ahighlighting%2520the%2520potential%2520of%2520GLMs%2520in%2520advancing%2520ASAS%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Language%20Models%20with%20Retrieval%20Augmented%20Generation%20for%0A%20%20Automated%20Short%20Answer%20Scoring&entry.906535625=Zifan%20Wang%20and%20Christopher%20Ormerod&entry.1292438233=%20%20Automated%20Short%20Answer%20Scoring%20%28ASAS%29%20is%20a%20critical%20component%20in%20educational%0Aassessment.%20While%20traditional%20ASAS%20systems%20relied%20on%20rule-based%20algorithms%20or%0Acomplex%20deep%20learning%20methods%2C%20recent%20advancements%20in%20Generative%20Language%0AModels%20%28GLMs%29%20offer%20new%20opportunities%20for%20improvement.%20This%20study%20explores%20the%0Aapplication%20of%20GLMs%20to%20ASAS%2C%20leveraging%20their%20off-the-shelf%20capabilities%20and%0Aperformance%20in%20various%20domains.%20We%20propose%20a%20novel%20pipeline%20that%20combines%0Avector%20databases%2C%20transformer-based%20encoders%2C%20and%20GLMs%20to%20enhance%20short%20answer%0Ascoring%20accuracy.%20Our%20approach%20stores%20training%20responses%20in%20a%20vector%20database%2C%0Aretrieves%20semantically%20similar%20responses%20during%20inference%2C%20and%20employs%20a%20GLM%20to%0Aanalyze%20these%20responses%20and%20determine%20appropriate%20scores.%20We%20further%20optimize%0Athe%20system%20through%20fine-tuned%20retrieval%20processes%20and%20prompt%20engineering.%0AEvaluation%20on%20the%20SemEval%202013%20dataset%20demonstrates%20a%20significant%20improvement%0Aon%20the%20SCIENTSBANK%203-way%20and%202-way%20tasks%20compared%20to%20existing%20methods%2C%0Ahighlighting%20the%20potential%20of%20GLMs%20in%20advancing%20ASAS%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03811v1&entry.124074799=Read"},
{"title": "Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning", "author": "Simret Araya Gebreegziabher and Kuangshi Ai and Zheng Zhang and Elena L. Glassman and Toby Jia-Jun Li", "abstract": "  Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL.\n", "link": "http://arxiv.org/abs/2408.03819v1", "date": "2024-08-07", "relevancy": 1.9796, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5038}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Variation%20Theory%20in%20Counterfactual%20Data%20Augmentation%20for%0A%20%20Optimized%20Active%20Learning&body=Title%3A%20Leveraging%20Variation%20Theory%20in%20Counterfactual%20Data%20Augmentation%20for%0A%20%20Optimized%20Active%20Learning%0AAuthor%3A%20Simret%20Araya%20Gebreegziabher%20and%20Kuangshi%20Ai%20and%20Zheng%20Zhang%20and%20Elena%20L.%20Glassman%20and%20Toby%20Jia-Jun%20Li%0AAbstract%3A%20%20%20Active%20Learning%20%28AL%29%20allows%20models%20to%20learn%20interactively%20from%20user%20feedback.%0AThis%20paper%20introduces%20a%20counterfactual%20data%20augmentation%20approach%20to%20AL%2C%0Aparticularly%20addressing%20the%20selection%20of%20datapoints%20for%20user%20querying%2C%20a%0Apivotal%20concern%20in%20enhancing%20data%20efficiency.%20Our%20approach%20is%20inspired%20by%0AVariation%20Theory%2C%20a%20theory%20of%20human%20concept%20learning%20that%20emphasizes%20the%0Aessential%20features%20of%20a%20concept%20by%20focusing%20on%20what%20stays%20the%20same%20and%20what%0Achanges.%20Instead%20of%20just%20querying%20with%20existing%20datapoints%2C%20our%20approach%0Asynthesizes%20artificial%20datapoints%20that%20highlight%20potential%20key%20similarities%20and%0Adifferences%20among%20labels%20using%20a%20neuro-symbolic%20pipeline%20combining%20large%0Alanguage%20models%20%28LLMs%29%20and%20rule-based%20models.%20Through%20an%20experiment%20in%20the%0Aexample%20domain%20of%20text%20classification%2C%20we%20show%20that%20our%20approach%20achieves%0Asignificantly%20higher%20performance%20when%20there%20are%20fewer%20annotated%20data.%20As%20the%0Aannotated%20training%20data%20gets%20larger%20the%20impact%20of%20the%20generated%20data%20starts%20to%0Adiminish%20showing%20its%20capability%20to%20address%20the%20cold%20start%20problem%20in%20AL.%20This%0Aresearch%20sheds%20light%20on%20integrating%20theories%20of%20human%20learning%20into%20the%0Aoptimization%20of%20AL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Variation%2520Theory%2520in%2520Counterfactual%2520Data%2520Augmentation%2520for%250A%2520%2520Optimized%2520Active%2520Learning%26entry.906535625%3DSimret%2520Araya%2520Gebreegziabher%2520and%2520Kuangshi%2520Ai%2520and%2520Zheng%2520Zhang%2520and%2520Elena%2520L.%2520Glassman%2520and%2520Toby%2520Jia-Jun%2520Li%26entry.1292438233%3D%2520%2520Active%2520Learning%2520%2528AL%2529%2520allows%2520models%2520to%2520learn%2520interactively%2520from%2520user%2520feedback.%250AThis%2520paper%2520introduces%2520a%2520counterfactual%2520data%2520augmentation%2520approach%2520to%2520AL%252C%250Aparticularly%2520addressing%2520the%2520selection%2520of%2520datapoints%2520for%2520user%2520querying%252C%2520a%250Apivotal%2520concern%2520in%2520enhancing%2520data%2520efficiency.%2520Our%2520approach%2520is%2520inspired%2520by%250AVariation%2520Theory%252C%2520a%2520theory%2520of%2520human%2520concept%2520learning%2520that%2520emphasizes%2520the%250Aessential%2520features%2520of%2520a%2520concept%2520by%2520focusing%2520on%2520what%2520stays%2520the%2520same%2520and%2520what%250Achanges.%2520Instead%2520of%2520just%2520querying%2520with%2520existing%2520datapoints%252C%2520our%2520approach%250Asynthesizes%2520artificial%2520datapoints%2520that%2520highlight%2520potential%2520key%2520similarities%2520and%250Adifferences%2520among%2520labels%2520using%2520a%2520neuro-symbolic%2520pipeline%2520combining%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520and%2520rule-based%2520models.%2520Through%2520an%2520experiment%2520in%2520the%250Aexample%2520domain%2520of%2520text%2520classification%252C%2520we%2520show%2520that%2520our%2520approach%2520achieves%250Asignificantly%2520higher%2520performance%2520when%2520there%2520are%2520fewer%2520annotated%2520data.%2520As%2520the%250Aannotated%2520training%2520data%2520gets%2520larger%2520the%2520impact%2520of%2520the%2520generated%2520data%2520starts%2520to%250Adiminish%2520showing%2520its%2520capability%2520to%2520address%2520the%2520cold%2520start%2520problem%2520in%2520AL.%2520This%250Aresearch%2520sheds%2520light%2520on%2520integrating%2520theories%2520of%2520human%2520learning%2520into%2520the%250Aoptimization%2520of%2520AL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Variation%20Theory%20in%20Counterfactual%20Data%20Augmentation%20for%0A%20%20Optimized%20Active%20Learning&entry.906535625=Simret%20Araya%20Gebreegziabher%20and%20Kuangshi%20Ai%20and%20Zheng%20Zhang%20and%20Elena%20L.%20Glassman%20and%20Toby%20Jia-Jun%20Li&entry.1292438233=%20%20Active%20Learning%20%28AL%29%20allows%20models%20to%20learn%20interactively%20from%20user%20feedback.%0AThis%20paper%20introduces%20a%20counterfactual%20data%20augmentation%20approach%20to%20AL%2C%0Aparticularly%20addressing%20the%20selection%20of%20datapoints%20for%20user%20querying%2C%20a%0Apivotal%20concern%20in%20enhancing%20data%20efficiency.%20Our%20approach%20is%20inspired%20by%0AVariation%20Theory%2C%20a%20theory%20of%20human%20concept%20learning%20that%20emphasizes%20the%0Aessential%20features%20of%20a%20concept%20by%20focusing%20on%20what%20stays%20the%20same%20and%20what%0Achanges.%20Instead%20of%20just%20querying%20with%20existing%20datapoints%2C%20our%20approach%0Asynthesizes%20artificial%20datapoints%20that%20highlight%20potential%20key%20similarities%20and%0Adifferences%20among%20labels%20using%20a%20neuro-symbolic%20pipeline%20combining%20large%0Alanguage%20models%20%28LLMs%29%20and%20rule-based%20models.%20Through%20an%20experiment%20in%20the%0Aexample%20domain%20of%20text%20classification%2C%20we%20show%20that%20our%20approach%20achieves%0Asignificantly%20higher%20performance%20when%20there%20are%20fewer%20annotated%20data.%20As%20the%0Aannotated%20training%20data%20gets%20larger%20the%20impact%20of%20the%20generated%20data%20starts%20to%0Adiminish%20showing%20its%20capability%20to%20address%20the%20cold%20start%20problem%20in%20AL.%20This%0Aresearch%20sheds%20light%20on%20integrating%20theories%20of%20human%20learning%20into%20the%0Aoptimization%20of%20AL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03819v1&entry.124074799=Read"},
{"title": "Nonparametric Linear Feature Learning in Regression Through\n  Regularisation", "author": "Bertille Follain and Francis Bach", "abstract": "  Representation learning plays a crucial role in automated feature selection,\nparticularly in the context of high-dimensional data, where non-parametric\nmethods often struggle. In this study, we focus on supervised learning\nscenarios where the pertinent information resides within a lower-dimensional\nlinear subspace of the data, namely the multi-index model. If this subspace\nwere known, it would greatly enhance prediction, computation, and\ninterpretation. To address this challenge, we propose a novel method for joint\nlinear feature learning and non-parametric function estimation, aimed at more\neffectively leveraging hidden features for learning. Our approach employs\nempirical risk minimisation, augmented with a penalty on function derivatives,\nensuring versatility. Leveraging the orthogonality and rotation invariance\nproperties of Hermite polynomials, we introduce our estimator, named RegFeaL.\nBy using alternative minimisation, we iteratively rotate the data to improve\nalignment with leading directions. We establish that the expected risk of our\nmethod converges in high-probability to the minimal risk under minimal\nassumptions and with explicit rates. Additionally, we provide empirical results\ndemonstrating the performance of RegFeaL in various experiments.\n", "link": "http://arxiv.org/abs/2307.12754v4", "date": "2024-08-07", "relevancy": 1.9661, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.492}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonparametric%20Linear%20Feature%20Learning%20in%20Regression%20Through%0A%20%20Regularisation&body=Title%3A%20Nonparametric%20Linear%20Feature%20Learning%20in%20Regression%20Through%0A%20%20Regularisation%0AAuthor%3A%20Bertille%20Follain%20and%20Francis%20Bach%0AAbstract%3A%20%20%20Representation%20learning%20plays%20a%20crucial%20role%20in%20automated%20feature%20selection%2C%0Aparticularly%20in%20the%20context%20of%20high-dimensional%20data%2C%20where%20non-parametric%0Amethods%20often%20struggle.%20In%20this%20study%2C%20we%20focus%20on%20supervised%20learning%0Ascenarios%20where%20the%20pertinent%20information%20resides%20within%20a%20lower-dimensional%0Alinear%20subspace%20of%20the%20data%2C%20namely%20the%20multi-index%20model.%20If%20this%20subspace%0Awere%20known%2C%20it%20would%20greatly%20enhance%20prediction%2C%20computation%2C%20and%0Ainterpretation.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20method%20for%20joint%0Alinear%20feature%20learning%20and%20non-parametric%20function%20estimation%2C%20aimed%20at%20more%0Aeffectively%20leveraging%20hidden%20features%20for%20learning.%20Our%20approach%20employs%0Aempirical%20risk%20minimisation%2C%20augmented%20with%20a%20penalty%20on%20function%20derivatives%2C%0Aensuring%20versatility.%20Leveraging%20the%20orthogonality%20and%20rotation%20invariance%0Aproperties%20of%20Hermite%20polynomials%2C%20we%20introduce%20our%20estimator%2C%20named%20RegFeaL.%0ABy%20using%20alternative%20minimisation%2C%20we%20iteratively%20rotate%20the%20data%20to%20improve%0Aalignment%20with%20leading%20directions.%20We%20establish%20that%20the%20expected%20risk%20of%20our%0Amethod%20converges%20in%20high-probability%20to%20the%20minimal%20risk%20under%20minimal%0Aassumptions%20and%20with%20explicit%20rates.%20Additionally%2C%20we%20provide%20empirical%20results%0Ademonstrating%20the%20performance%20of%20RegFeaL%20in%20various%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.12754v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonparametric%2520Linear%2520Feature%2520Learning%2520in%2520Regression%2520Through%250A%2520%2520Regularisation%26entry.906535625%3DBertille%2520Follain%2520and%2520Francis%2520Bach%26entry.1292438233%3D%2520%2520Representation%2520learning%2520plays%2520a%2520crucial%2520role%2520in%2520automated%2520feature%2520selection%252C%250Aparticularly%2520in%2520the%2520context%2520of%2520high-dimensional%2520data%252C%2520where%2520non-parametric%250Amethods%2520often%2520struggle.%2520In%2520this%2520study%252C%2520we%2520focus%2520on%2520supervised%2520learning%250Ascenarios%2520where%2520the%2520pertinent%2520information%2520resides%2520within%2520a%2520lower-dimensional%250Alinear%2520subspace%2520of%2520the%2520data%252C%2520namely%2520the%2520multi-index%2520model.%2520If%2520this%2520subspace%250Awere%2520known%252C%2520it%2520would%2520greatly%2520enhance%2520prediction%252C%2520computation%252C%2520and%250Ainterpretation.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520joint%250Alinear%2520feature%2520learning%2520and%2520non-parametric%2520function%2520estimation%252C%2520aimed%2520at%2520more%250Aeffectively%2520leveraging%2520hidden%2520features%2520for%2520learning.%2520Our%2520approach%2520employs%250Aempirical%2520risk%2520minimisation%252C%2520augmented%2520with%2520a%2520penalty%2520on%2520function%2520derivatives%252C%250Aensuring%2520versatility.%2520Leveraging%2520the%2520orthogonality%2520and%2520rotation%2520invariance%250Aproperties%2520of%2520Hermite%2520polynomials%252C%2520we%2520introduce%2520our%2520estimator%252C%2520named%2520RegFeaL.%250ABy%2520using%2520alternative%2520minimisation%252C%2520we%2520iteratively%2520rotate%2520the%2520data%2520to%2520improve%250Aalignment%2520with%2520leading%2520directions.%2520We%2520establish%2520that%2520the%2520expected%2520risk%2520of%2520our%250Amethod%2520converges%2520in%2520high-probability%2520to%2520the%2520minimal%2520risk%2520under%2520minimal%250Aassumptions%2520and%2520with%2520explicit%2520rates.%2520Additionally%252C%2520we%2520provide%2520empirical%2520results%250Ademonstrating%2520the%2520performance%2520of%2520RegFeaL%2520in%2520various%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.12754v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonparametric%20Linear%20Feature%20Learning%20in%20Regression%20Through%0A%20%20Regularisation&entry.906535625=Bertille%20Follain%20and%20Francis%20Bach&entry.1292438233=%20%20Representation%20learning%20plays%20a%20crucial%20role%20in%20automated%20feature%20selection%2C%0Aparticularly%20in%20the%20context%20of%20high-dimensional%20data%2C%20where%20non-parametric%0Amethods%20often%20struggle.%20In%20this%20study%2C%20we%20focus%20on%20supervised%20learning%0Ascenarios%20where%20the%20pertinent%20information%20resides%20within%20a%20lower-dimensional%0Alinear%20subspace%20of%20the%20data%2C%20namely%20the%20multi-index%20model.%20If%20this%20subspace%0Awere%20known%2C%20it%20would%20greatly%20enhance%20prediction%2C%20computation%2C%20and%0Ainterpretation.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20method%20for%20joint%0Alinear%20feature%20learning%20and%20non-parametric%20function%20estimation%2C%20aimed%20at%20more%0Aeffectively%20leveraging%20hidden%20features%20for%20learning.%20Our%20approach%20employs%0Aempirical%20risk%20minimisation%2C%20augmented%20with%20a%20penalty%20on%20function%20derivatives%2C%0Aensuring%20versatility.%20Leveraging%20the%20orthogonality%20and%20rotation%20invariance%0Aproperties%20of%20Hermite%20polynomials%2C%20we%20introduce%20our%20estimator%2C%20named%20RegFeaL.%0ABy%20using%20alternative%20minimisation%2C%20we%20iteratively%20rotate%20the%20data%20to%20improve%0Aalignment%20with%20leading%20directions.%20We%20establish%20that%20the%20expected%20risk%20of%20our%0Amethod%20converges%20in%20high-probability%20to%20the%20minimal%20risk%20under%20minimal%0Aassumptions%20and%20with%20explicit%20rates.%20Additionally%2C%20we%20provide%20empirical%20results%0Ademonstrating%20the%20performance%20of%20RegFeaL%20in%20various%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12754v4&entry.124074799=Read"},
{"title": "Bayes-optimal learning of an extensive-width neural network from\n  quadratically many samples", "author": "Antoine Maillard and Emanuele Troiani and Simon Martin and Florent Krzakala and Lenka Zdeborov\u00e1", "abstract": "  We consider the problem of learning a target function corresponding to a\nsingle hidden layer neural network, with a quadratic activation function after\nthe first layer, and random weights. We consider the asymptotic limit where the\ninput dimension and the network width are proportionally large. Recent work\n[Cui & al '23] established that linear regression provides Bayes-optimal test\nerror to learn such a function when the number of available samples is only\nlinear in the dimension. That work stressed the open challenge of theoretically\nanalyzing the optimal test error in the more interesting regime where the\nnumber of samples is quadratic in the dimension. In this paper, we solve this\nchallenge for quadratic activations and derive a closed-form expression for the\nBayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,\nwhich combines approximate message passing with rotationally invariant matrix\ndenoising, and that asymptotically achieves the optimal performance.\nTechnically, our result is enabled by establishing a link with recent works on\noptimal denoising of extensive-rank matrices and on the ellipsoid fitting\nproblem. We further show empirically that, in the absence of noise,\nrandomly-initialized gradient descent seems to sample the space of weights,\nleading to zero training loss, and averaging over initialization leads to a\ntest error equal to the Bayes-optimal one.\n", "link": "http://arxiv.org/abs/2408.03733v1", "date": "2024-08-07", "relevancy": 1.9562, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5104}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayes-optimal%20learning%20of%20an%20extensive-width%20neural%20network%20from%0A%20%20quadratically%20many%20samples&body=Title%3A%20Bayes-optimal%20learning%20of%20an%20extensive-width%20neural%20network%20from%0A%20%20quadratically%20many%20samples%0AAuthor%3A%20Antoine%20Maillard%20and%20Emanuele%20Troiani%20and%20Simon%20Martin%20and%20Florent%20Krzakala%20and%20Lenka%20Zdeborov%C3%A1%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20learning%20a%20target%20function%20corresponding%20to%20a%0Asingle%20hidden%20layer%20neural%20network%2C%20with%20a%20quadratic%20activation%20function%20after%0Athe%20first%20layer%2C%20and%20random%20weights.%20We%20consider%20the%20asymptotic%20limit%20where%20the%0Ainput%20dimension%20and%20the%20network%20width%20are%20proportionally%20large.%20Recent%20work%0A%5BCui%20%26%20al%20%2723%5D%20established%20that%20linear%20regression%20provides%20Bayes-optimal%20test%0Aerror%20to%20learn%20such%20a%20function%20when%20the%20number%20of%20available%20samples%20is%20only%0Alinear%20in%20the%20dimension.%20That%20work%20stressed%20the%20open%20challenge%20of%20theoretically%0Aanalyzing%20the%20optimal%20test%20error%20in%20the%20more%20interesting%20regime%20where%20the%0Anumber%20of%20samples%20is%20quadratic%20in%20the%20dimension.%20In%20this%20paper%2C%20we%20solve%20this%0Achallenge%20for%20quadratic%20activations%20and%20derive%20a%20closed-form%20expression%20for%20the%0ABayes-optimal%20test%20error.%20We%20also%20provide%20an%20algorithm%2C%20that%20we%20call%20GAMP-RIE%2C%0Awhich%20combines%20approximate%20message%20passing%20with%20rotationally%20invariant%20matrix%0Adenoising%2C%20and%20that%20asymptotically%20achieves%20the%20optimal%20performance.%0ATechnically%2C%20our%20result%20is%20enabled%20by%20establishing%20a%20link%20with%20recent%20works%20on%0Aoptimal%20denoising%20of%20extensive-rank%20matrices%20and%20on%20the%20ellipsoid%20fitting%0Aproblem.%20We%20further%20show%20empirically%20that%2C%20in%20the%20absence%20of%20noise%2C%0Arandomly-initialized%20gradient%20descent%20seems%20to%20sample%20the%20space%20of%20weights%2C%0Aleading%20to%20zero%20training%20loss%2C%20and%20averaging%20over%20initialization%20leads%20to%20a%0Atest%20error%20equal%20to%20the%20Bayes-optimal%20one.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayes-optimal%2520learning%2520of%2520an%2520extensive-width%2520neural%2520network%2520from%250A%2520%2520quadratically%2520many%2520samples%26entry.906535625%3DAntoine%2520Maillard%2520and%2520Emanuele%2520Troiani%2520and%2520Simon%2520Martin%2520and%2520Florent%2520Krzakala%2520and%2520Lenka%2520Zdeborov%25C3%25A1%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520learning%2520a%2520target%2520function%2520corresponding%2520to%2520a%250Asingle%2520hidden%2520layer%2520neural%2520network%252C%2520with%2520a%2520quadratic%2520activation%2520function%2520after%250Athe%2520first%2520layer%252C%2520and%2520random%2520weights.%2520We%2520consider%2520the%2520asymptotic%2520limit%2520where%2520the%250Ainput%2520dimension%2520and%2520the%2520network%2520width%2520are%2520proportionally%2520large.%2520Recent%2520work%250A%255BCui%2520%2526%2520al%2520%252723%255D%2520established%2520that%2520linear%2520regression%2520provides%2520Bayes-optimal%2520test%250Aerror%2520to%2520learn%2520such%2520a%2520function%2520when%2520the%2520number%2520of%2520available%2520samples%2520is%2520only%250Alinear%2520in%2520the%2520dimension.%2520That%2520work%2520stressed%2520the%2520open%2520challenge%2520of%2520theoretically%250Aanalyzing%2520the%2520optimal%2520test%2520error%2520in%2520the%2520more%2520interesting%2520regime%2520where%2520the%250Anumber%2520of%2520samples%2520is%2520quadratic%2520in%2520the%2520dimension.%2520In%2520this%2520paper%252C%2520we%2520solve%2520this%250Achallenge%2520for%2520quadratic%2520activations%2520and%2520derive%2520a%2520closed-form%2520expression%2520for%2520the%250ABayes-optimal%2520test%2520error.%2520We%2520also%2520provide%2520an%2520algorithm%252C%2520that%2520we%2520call%2520GAMP-RIE%252C%250Awhich%2520combines%2520approximate%2520message%2520passing%2520with%2520rotationally%2520invariant%2520matrix%250Adenoising%252C%2520and%2520that%2520asymptotically%2520achieves%2520the%2520optimal%2520performance.%250ATechnically%252C%2520our%2520result%2520is%2520enabled%2520by%2520establishing%2520a%2520link%2520with%2520recent%2520works%2520on%250Aoptimal%2520denoising%2520of%2520extensive-rank%2520matrices%2520and%2520on%2520the%2520ellipsoid%2520fitting%250Aproblem.%2520We%2520further%2520show%2520empirically%2520that%252C%2520in%2520the%2520absence%2520of%2520noise%252C%250Arandomly-initialized%2520gradient%2520descent%2520seems%2520to%2520sample%2520the%2520space%2520of%2520weights%252C%250Aleading%2520to%2520zero%2520training%2520loss%252C%2520and%2520averaging%2520over%2520initialization%2520leads%2520to%2520a%250Atest%2520error%2520equal%2520to%2520the%2520Bayes-optimal%2520one.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayes-optimal%20learning%20of%20an%20extensive-width%20neural%20network%20from%0A%20%20quadratically%20many%20samples&entry.906535625=Antoine%20Maillard%20and%20Emanuele%20Troiani%20and%20Simon%20Martin%20and%20Florent%20Krzakala%20and%20Lenka%20Zdeborov%C3%A1&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20learning%20a%20target%20function%20corresponding%20to%20a%0Asingle%20hidden%20layer%20neural%20network%2C%20with%20a%20quadratic%20activation%20function%20after%0Athe%20first%20layer%2C%20and%20random%20weights.%20We%20consider%20the%20asymptotic%20limit%20where%20the%0Ainput%20dimension%20and%20the%20network%20width%20are%20proportionally%20large.%20Recent%20work%0A%5BCui%20%26%20al%20%2723%5D%20established%20that%20linear%20regression%20provides%20Bayes-optimal%20test%0Aerror%20to%20learn%20such%20a%20function%20when%20the%20number%20of%20available%20samples%20is%20only%0Alinear%20in%20the%20dimension.%20That%20work%20stressed%20the%20open%20challenge%20of%20theoretically%0Aanalyzing%20the%20optimal%20test%20error%20in%20the%20more%20interesting%20regime%20where%20the%0Anumber%20of%20samples%20is%20quadratic%20in%20the%20dimension.%20In%20this%20paper%2C%20we%20solve%20this%0Achallenge%20for%20quadratic%20activations%20and%20derive%20a%20closed-form%20expression%20for%20the%0ABayes-optimal%20test%20error.%20We%20also%20provide%20an%20algorithm%2C%20that%20we%20call%20GAMP-RIE%2C%0Awhich%20combines%20approximate%20message%20passing%20with%20rotationally%20invariant%20matrix%0Adenoising%2C%20and%20that%20asymptotically%20achieves%20the%20optimal%20performance.%0ATechnically%2C%20our%20result%20is%20enabled%20by%20establishing%20a%20link%20with%20recent%20works%20on%0Aoptimal%20denoising%20of%20extensive-rank%20matrices%20and%20on%20the%20ellipsoid%20fitting%0Aproblem.%20We%20further%20show%20empirically%20that%2C%20in%20the%20absence%20of%20noise%2C%0Arandomly-initialized%20gradient%20descent%20seems%20to%20sample%20the%20space%20of%20weights%2C%0Aleading%20to%20zero%20training%20loss%2C%20and%20averaging%20over%20initialization%20leads%20to%20a%0Atest%20error%20equal%20to%20the%20Bayes-optimal%20one.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03733v1&entry.124074799=Read"},
{"title": "Visualize and Paint GAN Activations", "author": "Rudolf Herdt and Peter Maass", "abstract": "  We investigate how generated structures of GANs correlate with their\nactivations in hidden layers, with the purpose of better understanding the\ninner workings of those models and being able to paint structures with\nunconditionally trained GANs. This gives us more control over the generated\nimages, allowing to generate them from a semantic segmentation map while not\nrequiring such a segmentation in the training data. To this end we introduce\nthe concept of tileable features, allowing us to identify activations that work\nwell for painting.\n", "link": "http://arxiv.org/abs/2405.15636v3", "date": "2024-08-07", "relevancy": 1.949, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5106}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4729}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualize%20and%20Paint%20GAN%20Activations&body=Title%3A%20Visualize%20and%20Paint%20GAN%20Activations%0AAuthor%3A%20Rudolf%20Herdt%20and%20Peter%20Maass%0AAbstract%3A%20%20%20We%20investigate%20how%20generated%20structures%20of%20GANs%20correlate%20with%20their%0Aactivations%20in%20hidden%20layers%2C%20with%20the%20purpose%20of%20better%20understanding%20the%0Ainner%20workings%20of%20those%20models%20and%20being%20able%20to%20paint%20structures%20with%0Aunconditionally%20trained%20GANs.%20This%20gives%20us%20more%20control%20over%20the%20generated%0Aimages%2C%20allowing%20to%20generate%20them%20from%20a%20semantic%20segmentation%20map%20while%20not%0Arequiring%20such%20a%20segmentation%20in%20the%20training%20data.%20To%20this%20end%20we%20introduce%0Athe%20concept%20of%20tileable%20features%2C%20allowing%20us%20to%20identify%20activations%20that%20work%0Awell%20for%20painting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15636v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualize%2520and%2520Paint%2520GAN%2520Activations%26entry.906535625%3DRudolf%2520Herdt%2520and%2520Peter%2520Maass%26entry.1292438233%3D%2520%2520We%2520investigate%2520how%2520generated%2520structures%2520of%2520GANs%2520correlate%2520with%2520their%250Aactivations%2520in%2520hidden%2520layers%252C%2520with%2520the%2520purpose%2520of%2520better%2520understanding%2520the%250Ainner%2520workings%2520of%2520those%2520models%2520and%2520being%2520able%2520to%2520paint%2520structures%2520with%250Aunconditionally%2520trained%2520GANs.%2520This%2520gives%2520us%2520more%2520control%2520over%2520the%2520generated%250Aimages%252C%2520allowing%2520to%2520generate%2520them%2520from%2520a%2520semantic%2520segmentation%2520map%2520while%2520not%250Arequiring%2520such%2520a%2520segmentation%2520in%2520the%2520training%2520data.%2520To%2520this%2520end%2520we%2520introduce%250Athe%2520concept%2520of%2520tileable%2520features%252C%2520allowing%2520us%2520to%2520identify%2520activations%2520that%2520work%250Awell%2520for%2520painting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15636v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualize%20and%20Paint%20GAN%20Activations&entry.906535625=Rudolf%20Herdt%20and%20Peter%20Maass&entry.1292438233=%20%20We%20investigate%20how%20generated%20structures%20of%20GANs%20correlate%20with%20their%0Aactivations%20in%20hidden%20layers%2C%20with%20the%20purpose%20of%20better%20understanding%20the%0Ainner%20workings%20of%20those%20models%20and%20being%20able%20to%20paint%20structures%20with%0Aunconditionally%20trained%20GANs.%20This%20gives%20us%20more%20control%20over%20the%20generated%0Aimages%2C%20allowing%20to%20generate%20them%20from%20a%20semantic%20segmentation%20map%20while%20not%0Arequiring%20such%20a%20segmentation%20in%20the%20training%20data.%20To%20this%20end%20we%20introduce%0Athe%20concept%20of%20tileable%20features%2C%20allowing%20us%20to%20identify%20activations%20that%20work%0Awell%20for%20painting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15636v3&entry.124074799=Read"},
{"title": "Knowledge Probing for Graph Representation Learning", "author": "Mingyu Zhao and Xingyu Huang and Ziyu Lyu and Yanlin Wang and Lixin Cui and Lu Bai", "abstract": "  Graph learning methods have been extensively applied in diverse application\nareas. However, what kind of inherent graph properties e.g. graph proximity,\ngraph structural information has been encoded into graph representation\nlearning for downstream tasks is still under-explored. In this paper, we\npropose a novel graph probing framework (GraphProbe) to investigate and\ninterpret whether the family of graph learning methods has encoded different\nlevels of knowledge in graph representation learning. Based on the intrinsic\nproperties of graphs, we design three probes to systematically investigate the\ngraph representation learning process from different perspectives, respectively\nthe node-wise level, the path-wise level, and the structural level. We\nconstruct a thorough evaluation benchmark with nine representative graph\nlearning methods from random walk based approaches, basic graph neural networks\nand self-supervised graph methods, and probe them on six benchmark datasets for\nnode classification, link prediction and graph classification. The experimental\nevaluation verify that GraphProbe can estimate the capability of graph\nrepresentation learning. Remaking results have been concluded: GCN and\nWeightedGCN methods are relatively versatile methods achieving better results\nwith respect to different tasks.\n", "link": "http://arxiv.org/abs/2408.03877v1", "date": "2024-08-07", "relevancy": 1.9376, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5131}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4902}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Probing%20for%20Graph%20Representation%20Learning&body=Title%3A%20Knowledge%20Probing%20for%20Graph%20Representation%20Learning%0AAuthor%3A%20Mingyu%20Zhao%20and%20Xingyu%20Huang%20and%20Ziyu%20Lyu%20and%20Yanlin%20Wang%20and%20Lixin%20Cui%20and%20Lu%20Bai%0AAbstract%3A%20%20%20Graph%20learning%20methods%20have%20been%20extensively%20applied%20in%20diverse%20application%0Aareas.%20However%2C%20what%20kind%20of%20inherent%20graph%20properties%20e.g.%20graph%20proximity%2C%0Agraph%20structural%20information%20has%20been%20encoded%20into%20graph%20representation%0Alearning%20for%20downstream%20tasks%20is%20still%20under-explored.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20graph%20probing%20framework%20%28GraphProbe%29%20to%20investigate%20and%0Ainterpret%20whether%20the%20family%20of%20graph%20learning%20methods%20has%20encoded%20different%0Alevels%20of%20knowledge%20in%20graph%20representation%20learning.%20Based%20on%20the%20intrinsic%0Aproperties%20of%20graphs%2C%20we%20design%20three%20probes%20to%20systematically%20investigate%20the%0Agraph%20representation%20learning%20process%20from%20different%20perspectives%2C%20respectively%0Athe%20node-wise%20level%2C%20the%20path-wise%20level%2C%20and%20the%20structural%20level.%20We%0Aconstruct%20a%20thorough%20evaluation%20benchmark%20with%20nine%20representative%20graph%0Alearning%20methods%20from%20random%20walk%20based%20approaches%2C%20basic%20graph%20neural%20networks%0Aand%20self-supervised%20graph%20methods%2C%20and%20probe%20them%20on%20six%20benchmark%20datasets%20for%0Anode%20classification%2C%20link%20prediction%20and%20graph%20classification.%20The%20experimental%0Aevaluation%20verify%20that%20GraphProbe%20can%20estimate%20the%20capability%20of%20graph%0Arepresentation%20learning.%20Remaking%20results%20have%20been%20concluded%3A%20GCN%20and%0AWeightedGCN%20methods%20are%20relatively%20versatile%20methods%20achieving%20better%20results%0Awith%20respect%20to%20different%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Probing%2520for%2520Graph%2520Representation%2520Learning%26entry.906535625%3DMingyu%2520Zhao%2520and%2520Xingyu%2520Huang%2520and%2520Ziyu%2520Lyu%2520and%2520Yanlin%2520Wang%2520and%2520Lixin%2520Cui%2520and%2520Lu%2520Bai%26entry.1292438233%3D%2520%2520Graph%2520learning%2520methods%2520have%2520been%2520extensively%2520applied%2520in%2520diverse%2520application%250Aareas.%2520However%252C%2520what%2520kind%2520of%2520inherent%2520graph%2520properties%2520e.g.%2520graph%2520proximity%252C%250Agraph%2520structural%2520information%2520has%2520been%2520encoded%2520into%2520graph%2520representation%250Alearning%2520for%2520downstream%2520tasks%2520is%2520still%2520under-explored.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520graph%2520probing%2520framework%2520%2528GraphProbe%2529%2520to%2520investigate%2520and%250Ainterpret%2520whether%2520the%2520family%2520of%2520graph%2520learning%2520methods%2520has%2520encoded%2520different%250Alevels%2520of%2520knowledge%2520in%2520graph%2520representation%2520learning.%2520Based%2520on%2520the%2520intrinsic%250Aproperties%2520of%2520graphs%252C%2520we%2520design%2520three%2520probes%2520to%2520systematically%2520investigate%2520the%250Agraph%2520representation%2520learning%2520process%2520from%2520different%2520perspectives%252C%2520respectively%250Athe%2520node-wise%2520level%252C%2520the%2520path-wise%2520level%252C%2520and%2520the%2520structural%2520level.%2520We%250Aconstruct%2520a%2520thorough%2520evaluation%2520benchmark%2520with%2520nine%2520representative%2520graph%250Alearning%2520methods%2520from%2520random%2520walk%2520based%2520approaches%252C%2520basic%2520graph%2520neural%2520networks%250Aand%2520self-supervised%2520graph%2520methods%252C%2520and%2520probe%2520them%2520on%2520six%2520benchmark%2520datasets%2520for%250Anode%2520classification%252C%2520link%2520prediction%2520and%2520graph%2520classification.%2520The%2520experimental%250Aevaluation%2520verify%2520that%2520GraphProbe%2520can%2520estimate%2520the%2520capability%2520of%2520graph%250Arepresentation%2520learning.%2520Remaking%2520results%2520have%2520been%2520concluded%253A%2520GCN%2520and%250AWeightedGCN%2520methods%2520are%2520relatively%2520versatile%2520methods%2520achieving%2520better%2520results%250Awith%2520respect%2520to%2520different%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Probing%20for%20Graph%20Representation%20Learning&entry.906535625=Mingyu%20Zhao%20and%20Xingyu%20Huang%20and%20Ziyu%20Lyu%20and%20Yanlin%20Wang%20and%20Lixin%20Cui%20and%20Lu%20Bai&entry.1292438233=%20%20Graph%20learning%20methods%20have%20been%20extensively%20applied%20in%20diverse%20application%0Aareas.%20However%2C%20what%20kind%20of%20inherent%20graph%20properties%20e.g.%20graph%20proximity%2C%0Agraph%20structural%20information%20has%20been%20encoded%20into%20graph%20representation%0Alearning%20for%20downstream%20tasks%20is%20still%20under-explored.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20graph%20probing%20framework%20%28GraphProbe%29%20to%20investigate%20and%0Ainterpret%20whether%20the%20family%20of%20graph%20learning%20methods%20has%20encoded%20different%0Alevels%20of%20knowledge%20in%20graph%20representation%20learning.%20Based%20on%20the%20intrinsic%0Aproperties%20of%20graphs%2C%20we%20design%20three%20probes%20to%20systematically%20investigate%20the%0Agraph%20representation%20learning%20process%20from%20different%20perspectives%2C%20respectively%0Athe%20node-wise%20level%2C%20the%20path-wise%20level%2C%20and%20the%20structural%20level.%20We%0Aconstruct%20a%20thorough%20evaluation%20benchmark%20with%20nine%20representative%20graph%0Alearning%20methods%20from%20random%20walk%20based%20approaches%2C%20basic%20graph%20neural%20networks%0Aand%20self-supervised%20graph%20methods%2C%20and%20probe%20them%20on%20six%20benchmark%20datasets%20for%0Anode%20classification%2C%20link%20prediction%20and%20graph%20classification.%20The%20experimental%0Aevaluation%20verify%20that%20GraphProbe%20can%20estimate%20the%20capability%20of%20graph%0Arepresentation%20learning.%20Remaking%20results%20have%20been%20concluded%3A%20GCN%20and%0AWeightedGCN%20methods%20are%20relatively%20versatile%20methods%20achieving%20better%20results%0Awith%20respect%20to%20different%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03877v1&entry.124074799=Read"},
{"title": "AI-Driven approach for sustainable extraction of earth's subsurface\n  renewable energy while minimizing seismic activity", "author": "Diego Gutierrez-Oribio and Alexandros Stathas and Ioannis Stefanou", "abstract": "  Deep Geothermal Energy, Carbon Capture and Storage, and Hydrogen Storage hold\nconsiderable promise for meeting the energy sector's large-scale requirements\nand reducing CO$_2$ emissions. However, the injection of fluids into the\nEarth's crust, essential for these activities, can induce or trigger\nearthquakes. In this paper, we highlight a new approach based on Reinforcement\nLearning for the control of human-induced seismicity in the highly complex\nenvironment of an underground reservoir. This complex system poses significant\nchallenges in the control design due to parameter uncertainties and unmodeled\ndynamics. We show that the reinforcement learning algorithm can interact\nefficiently with a robust controller, by choosing the controller parameters in\nreal-time, reducing human-induced seismicity and allowing the consideration of\nfurther production objectives, \\textit{e.g.}, minimal control power.\nSimulations are presented for a simplified underground reservoir under various\nenergy demand scenarios, demonstrating the reliability and effectiveness of the\nproposed control-reinforcement learning approach.\n", "link": "http://arxiv.org/abs/2408.03664v1", "date": "2024-08-07", "relevancy": 1.9251, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4909}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Driven%20approach%20for%20sustainable%20extraction%20of%20earth%27s%20subsurface%0A%20%20renewable%20energy%20while%20minimizing%20seismic%20activity&body=Title%3A%20AI-Driven%20approach%20for%20sustainable%20extraction%20of%20earth%27s%20subsurface%0A%20%20renewable%20energy%20while%20minimizing%20seismic%20activity%0AAuthor%3A%20Diego%20Gutierrez-Oribio%20and%20Alexandros%20Stathas%20and%20Ioannis%20Stefanou%0AAbstract%3A%20%20%20Deep%20Geothermal%20Energy%2C%20Carbon%20Capture%20and%20Storage%2C%20and%20Hydrogen%20Storage%20hold%0Aconsiderable%20promise%20for%20meeting%20the%20energy%20sector%27s%20large-scale%20requirements%0Aand%20reducing%20CO%24_2%24%20emissions.%20However%2C%20the%20injection%20of%20fluids%20into%20the%0AEarth%27s%20crust%2C%20essential%20for%20these%20activities%2C%20can%20induce%20or%20trigger%0Aearthquakes.%20In%20this%20paper%2C%20we%20highlight%20a%20new%20approach%20based%20on%20Reinforcement%0ALearning%20for%20the%20control%20of%20human-induced%20seismicity%20in%20the%20highly%20complex%0Aenvironment%20of%20an%20underground%20reservoir.%20This%20complex%20system%20poses%20significant%0Achallenges%20in%20the%20control%20design%20due%20to%20parameter%20uncertainties%20and%20unmodeled%0Adynamics.%20We%20show%20that%20the%20reinforcement%20learning%20algorithm%20can%20interact%0Aefficiently%20with%20a%20robust%20controller%2C%20by%20choosing%20the%20controller%20parameters%20in%0Areal-time%2C%20reducing%20human-induced%20seismicity%20and%20allowing%20the%20consideration%20of%0Afurther%20production%20objectives%2C%20%5Ctextit%7Be.g.%7D%2C%20minimal%20control%20power.%0ASimulations%20are%20presented%20for%20a%20simplified%20underground%20reservoir%20under%20various%0Aenergy%20demand%20scenarios%2C%20demonstrating%20the%20reliability%20and%20effectiveness%20of%20the%0Aproposed%20control-reinforcement%20learning%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Driven%2520approach%2520for%2520sustainable%2520extraction%2520of%2520earth%2527s%2520subsurface%250A%2520%2520renewable%2520energy%2520while%2520minimizing%2520seismic%2520activity%26entry.906535625%3DDiego%2520Gutierrez-Oribio%2520and%2520Alexandros%2520Stathas%2520and%2520Ioannis%2520Stefanou%26entry.1292438233%3D%2520%2520Deep%2520Geothermal%2520Energy%252C%2520Carbon%2520Capture%2520and%2520Storage%252C%2520and%2520Hydrogen%2520Storage%2520hold%250Aconsiderable%2520promise%2520for%2520meeting%2520the%2520energy%2520sector%2527s%2520large-scale%2520requirements%250Aand%2520reducing%2520CO%2524_2%2524%2520emissions.%2520However%252C%2520the%2520injection%2520of%2520fluids%2520into%2520the%250AEarth%2527s%2520crust%252C%2520essential%2520for%2520these%2520activities%252C%2520can%2520induce%2520or%2520trigger%250Aearthquakes.%2520In%2520this%2520paper%252C%2520we%2520highlight%2520a%2520new%2520approach%2520based%2520on%2520Reinforcement%250ALearning%2520for%2520the%2520control%2520of%2520human-induced%2520seismicity%2520in%2520the%2520highly%2520complex%250Aenvironment%2520of%2520an%2520underground%2520reservoir.%2520This%2520complex%2520system%2520poses%2520significant%250Achallenges%2520in%2520the%2520control%2520design%2520due%2520to%2520parameter%2520uncertainties%2520and%2520unmodeled%250Adynamics.%2520We%2520show%2520that%2520the%2520reinforcement%2520learning%2520algorithm%2520can%2520interact%250Aefficiently%2520with%2520a%2520robust%2520controller%252C%2520by%2520choosing%2520the%2520controller%2520parameters%2520in%250Areal-time%252C%2520reducing%2520human-induced%2520seismicity%2520and%2520allowing%2520the%2520consideration%2520of%250Afurther%2520production%2520objectives%252C%2520%255Ctextit%257Be.g.%257D%252C%2520minimal%2520control%2520power.%250ASimulations%2520are%2520presented%2520for%2520a%2520simplified%2520underground%2520reservoir%2520under%2520various%250Aenergy%2520demand%2520scenarios%252C%2520demonstrating%2520the%2520reliability%2520and%2520effectiveness%2520of%2520the%250Aproposed%2520control-reinforcement%2520learning%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Driven%20approach%20for%20sustainable%20extraction%20of%20earth%27s%20subsurface%0A%20%20renewable%20energy%20while%20minimizing%20seismic%20activity&entry.906535625=Diego%20Gutierrez-Oribio%20and%20Alexandros%20Stathas%20and%20Ioannis%20Stefanou&entry.1292438233=%20%20Deep%20Geothermal%20Energy%2C%20Carbon%20Capture%20and%20Storage%2C%20and%20Hydrogen%20Storage%20hold%0Aconsiderable%20promise%20for%20meeting%20the%20energy%20sector%27s%20large-scale%20requirements%0Aand%20reducing%20CO%24_2%24%20emissions.%20However%2C%20the%20injection%20of%20fluids%20into%20the%0AEarth%27s%20crust%2C%20essential%20for%20these%20activities%2C%20can%20induce%20or%20trigger%0Aearthquakes.%20In%20this%20paper%2C%20we%20highlight%20a%20new%20approach%20based%20on%20Reinforcement%0ALearning%20for%20the%20control%20of%20human-induced%20seismicity%20in%20the%20highly%20complex%0Aenvironment%20of%20an%20underground%20reservoir.%20This%20complex%20system%20poses%20significant%0Achallenges%20in%20the%20control%20design%20due%20to%20parameter%20uncertainties%20and%20unmodeled%0Adynamics.%20We%20show%20that%20the%20reinforcement%20learning%20algorithm%20can%20interact%0Aefficiently%20with%20a%20robust%20controller%2C%20by%20choosing%20the%20controller%20parameters%20in%0Areal-time%2C%20reducing%20human-induced%20seismicity%20and%20allowing%20the%20consideration%20of%0Afurther%20production%20objectives%2C%20%5Ctextit%7Be.g.%7D%2C%20minimal%20control%20power.%0ASimulations%20are%20presented%20for%20a%20simplified%20underground%20reservoir%20under%20various%0Aenergy%20demand%20scenarios%2C%20demonstrating%20the%20reliability%20and%20effectiveness%20of%20the%0Aproposed%20control-reinforcement%20learning%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03664v1&entry.124074799=Read"},
{"title": "CodexGraph: Bridging Large Language Models and Code Repositories via\n  Code Graph Databases", "author": "Xiangyan Liu and Bo Lan and Zhiyuan Hu and Yang Liu and Zhicheng Zhang and Wenmeng Zhou and Fei Wang and Michael Shieh", "abstract": "  Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce \\framework, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, \\framework enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess \\framework using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, \\framework demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.\n", "link": "http://arxiv.org/abs/2408.03910v1", "date": "2024-08-07", "relevancy": 1.9134, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5486}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodexGraph%3A%20Bridging%20Large%20Language%20Models%20and%20Code%20Repositories%20via%0A%20%20Code%20Graph%20Databases&body=Title%3A%20CodexGraph%3A%20Bridging%20Large%20Language%20Models%20and%20Code%20Repositories%20via%0A%20%20Code%20Graph%20Databases%0AAuthor%3A%20Xiangyan%20Liu%20and%20Bo%20Lan%20and%20Zhiyuan%20Hu%20and%20Yang%20Liu%20and%20Zhicheng%20Zhang%20and%20Wenmeng%20Zhou%20and%20Fei%20Wang%20and%20Michael%20Shieh%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20stand-alone%20code%20tasks%20like%20HumanEval%0Aand%20MBPP%2C%20but%20struggle%20with%20handling%20entire%20code%20repositories.%20This%20challenge%0Ahas%20prompted%20research%20on%20enhancing%20LLM-codebase%20interaction%20at%20a%20repository%0Ascale.%20Current%20solutions%20rely%20on%20similarity-based%20retrieval%20or%20manual%20tools%20and%0AAPIs%2C%20each%20with%20notable%20drawbacks.%20Similarity-based%20retrieval%20often%20has%20low%0Arecall%20in%20complex%20tasks%2C%20while%20manual%20tools%20and%20APIs%20are%20typically%0Atask-specific%20and%20require%20expert%20knowledge%2C%20reducing%20their%20generalizability%0Aacross%20diverse%20code%20tasks%20and%20real-world%20applications.%20To%20mitigate%20these%0Alimitations%2C%20we%20introduce%20%5Cframework%2C%20a%20system%20that%20integrates%20LLM%20agents%20with%0Agraph%20database%20interfaces%20extracted%20from%20code%20repositories.%20By%20leveraging%20the%0Astructural%20properties%20of%20graph%20databases%20and%20the%20flexibility%20of%20the%20graph%20query%0Alanguage%2C%20%5Cframework%20enables%20the%20LLM%20agent%20to%20construct%20and%20execute%20queries%2C%0Aallowing%20for%20precise%2C%20code%20structure-aware%20context%20retrieval%20and%20code%0Anavigation.%20We%20assess%20%5Cframework%20using%20three%20benchmarks%3A%20CrossCodeEval%2C%0ASWE-bench%2C%20and%20EvoCodeBench.%20Additionally%2C%20we%20develop%20five%20real-world%20coding%0Aapplications.%20With%20a%20unified%20graph%20database%20schema%2C%20%5Cframework%20demonstrates%0Acompetitive%20performance%20and%20potential%20in%20both%20academic%20and%20real-world%0Aenvironments%2C%20showcasing%20its%20versatility%20and%20efficacy%20in%20software%20engineering.%0AOur%20application%20demo%3A%0Ahttps%3A//github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodexGraph%253A%2520Bridging%2520Large%2520Language%2520Models%2520and%2520Code%2520Repositories%2520via%250A%2520%2520Code%2520Graph%2520Databases%26entry.906535625%3DXiangyan%2520Liu%2520and%2520Bo%2520Lan%2520and%2520Zhiyuan%2520Hu%2520and%2520Yang%2520Liu%2520and%2520Zhicheng%2520Zhang%2520and%2520Wenmeng%2520Zhou%2520and%2520Fei%2520Wang%2520and%2520Michael%2520Shieh%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520stand-alone%2520code%2520tasks%2520like%2520HumanEval%250Aand%2520MBPP%252C%2520but%2520struggle%2520with%2520handling%2520entire%2520code%2520repositories.%2520This%2520challenge%250Ahas%2520prompted%2520research%2520on%2520enhancing%2520LLM-codebase%2520interaction%2520at%2520a%2520repository%250Ascale.%2520Current%2520solutions%2520rely%2520on%2520similarity-based%2520retrieval%2520or%2520manual%2520tools%2520and%250AAPIs%252C%2520each%2520with%2520notable%2520drawbacks.%2520Similarity-based%2520retrieval%2520often%2520has%2520low%250Arecall%2520in%2520complex%2520tasks%252C%2520while%2520manual%2520tools%2520and%2520APIs%2520are%2520typically%250Atask-specific%2520and%2520require%2520expert%2520knowledge%252C%2520reducing%2520their%2520generalizability%250Aacross%2520diverse%2520code%2520tasks%2520and%2520real-world%2520applications.%2520To%2520mitigate%2520these%250Alimitations%252C%2520we%2520introduce%2520%255Cframework%252C%2520a%2520system%2520that%2520integrates%2520LLM%2520agents%2520with%250Agraph%2520database%2520interfaces%2520extracted%2520from%2520code%2520repositories.%2520By%2520leveraging%2520the%250Astructural%2520properties%2520of%2520graph%2520databases%2520and%2520the%2520flexibility%2520of%2520the%2520graph%2520query%250Alanguage%252C%2520%255Cframework%2520enables%2520the%2520LLM%2520agent%2520to%2520construct%2520and%2520execute%2520queries%252C%250Aallowing%2520for%2520precise%252C%2520code%2520structure-aware%2520context%2520retrieval%2520and%2520code%250Anavigation.%2520We%2520assess%2520%255Cframework%2520using%2520three%2520benchmarks%253A%2520CrossCodeEval%252C%250ASWE-bench%252C%2520and%2520EvoCodeBench.%2520Additionally%252C%2520we%2520develop%2520five%2520real-world%2520coding%250Aapplications.%2520With%2520a%2520unified%2520graph%2520database%2520schema%252C%2520%255Cframework%2520demonstrates%250Acompetitive%2520performance%2520and%2520potential%2520in%2520both%2520academic%2520and%2520real-world%250Aenvironments%252C%2520showcasing%2520its%2520versatility%2520and%2520efficacy%2520in%2520software%2520engineering.%250AOur%2520application%2520demo%253A%250Ahttps%253A//github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodexGraph%3A%20Bridging%20Large%20Language%20Models%20and%20Code%20Repositories%20via%0A%20%20Code%20Graph%20Databases&entry.906535625=Xiangyan%20Liu%20and%20Bo%20Lan%20and%20Zhiyuan%20Hu%20and%20Yang%20Liu%20and%20Zhicheng%20Zhang%20and%20Wenmeng%20Zhou%20and%20Fei%20Wang%20and%20Michael%20Shieh&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20stand-alone%20code%20tasks%20like%20HumanEval%0Aand%20MBPP%2C%20but%20struggle%20with%20handling%20entire%20code%20repositories.%20This%20challenge%0Ahas%20prompted%20research%20on%20enhancing%20LLM-codebase%20interaction%20at%20a%20repository%0Ascale.%20Current%20solutions%20rely%20on%20similarity-based%20retrieval%20or%20manual%20tools%20and%0AAPIs%2C%20each%20with%20notable%20drawbacks.%20Similarity-based%20retrieval%20often%20has%20low%0Arecall%20in%20complex%20tasks%2C%20while%20manual%20tools%20and%20APIs%20are%20typically%0Atask-specific%20and%20require%20expert%20knowledge%2C%20reducing%20their%20generalizability%0Aacross%20diverse%20code%20tasks%20and%20real-world%20applications.%20To%20mitigate%20these%0Alimitations%2C%20we%20introduce%20%5Cframework%2C%20a%20system%20that%20integrates%20LLM%20agents%20with%0Agraph%20database%20interfaces%20extracted%20from%20code%20repositories.%20By%20leveraging%20the%0Astructural%20properties%20of%20graph%20databases%20and%20the%20flexibility%20of%20the%20graph%20query%0Alanguage%2C%20%5Cframework%20enables%20the%20LLM%20agent%20to%20construct%20and%20execute%20queries%2C%0Aallowing%20for%20precise%2C%20code%20structure-aware%20context%20retrieval%20and%20code%0Anavigation.%20We%20assess%20%5Cframework%20using%20three%20benchmarks%3A%20CrossCodeEval%2C%0ASWE-bench%2C%20and%20EvoCodeBench.%20Additionally%2C%20we%20develop%20five%20real-world%20coding%0Aapplications.%20With%20a%20unified%20graph%20database%20schema%2C%20%5Cframework%20demonstrates%0Acompetitive%20performance%20and%20potential%20in%20both%20academic%20and%20real-world%0Aenvironments%2C%20showcasing%20its%20versatility%20and%20efficacy%20in%20software%20engineering.%0AOur%20application%20demo%3A%0Ahttps%3A//github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03910v1&entry.124074799=Read"},
{"title": "Improving the Intelligent Driver Model by Incorporating Vehicle\n  Dynamics: Microscopic Calibration and Macroscopic Validation", "author": "Dominik Salles and Steve Oswald and Hans-Christian Reuss", "abstract": "  Microscopic traffic simulations are used to evaluate the impact of\ninfrastructure modifications and evolving vehicle technologies, such as\nconnected and automated driving. Simulated vehicles are controlled via\ncar-following, lane-changing and junction models, which are designed to imitate\nhuman driving behavior. However, physics-based car-following models (CFMs)\ncannot fully replicate measured vehicle trajectories. Therefore, we present\nmodel extensions for the Intelligent Driver Model (IDM), of which some are\nalready included in the Extended Intelligent Driver Model (EIDM), to improve\ncalibration and validation results. They consist of equations based on vehicle\ndynamics and drive off procedures. In addition, parameter selection plays a\ndecisive role. Thus, we introduce a framework to calibrate CFMs using drone\ndata captured at a signalized intersection in Stuttgart, Germany. We compare\nthe calibration error of the Krauss Model with the IDM and EIDM. In this setup,\nthe EIDM achieves a 17.78 % lower mean error than the IDM, based on the\ndistance difference between real world and simulated vehicles. Adding vehicle\ndynamics equations to the EIDM further improves the results by an additional\n18.97 %. The calibrated vehicle-driver combinations are then investigated by\nsimulating the traffic in three different scenarios: at the original\nintersection, in a closed loop and in a stop-and-go wave. The data shows that\nthe improved calibration process of individual vehicles, openly available at\nhttps://www.github.com/stepeos/pycarmodel_calibration, also provides more\naccurate macroscopic results.\n", "link": "http://arxiv.org/abs/2408.03722v1", "date": "2024-08-07", "relevancy": 1.9067, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4887}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4755}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Intelligent%20Driver%20Model%20by%20Incorporating%20Vehicle%0A%20%20Dynamics%3A%20Microscopic%20Calibration%20and%20Macroscopic%20Validation&body=Title%3A%20Improving%20the%20Intelligent%20Driver%20Model%20by%20Incorporating%20Vehicle%0A%20%20Dynamics%3A%20Microscopic%20Calibration%20and%20Macroscopic%20Validation%0AAuthor%3A%20Dominik%20Salles%20and%20Steve%20Oswald%20and%20Hans-Christian%20Reuss%0AAbstract%3A%20%20%20Microscopic%20traffic%20simulations%20are%20used%20to%20evaluate%20the%20impact%20of%0Ainfrastructure%20modifications%20and%20evolving%20vehicle%20technologies%2C%20such%20as%0Aconnected%20and%20automated%20driving.%20Simulated%20vehicles%20are%20controlled%20via%0Acar-following%2C%20lane-changing%20and%20junction%20models%2C%20which%20are%20designed%20to%20imitate%0Ahuman%20driving%20behavior.%20However%2C%20physics-based%20car-following%20models%20%28CFMs%29%0Acannot%20fully%20replicate%20measured%20vehicle%20trajectories.%20Therefore%2C%20we%20present%0Amodel%20extensions%20for%20the%20Intelligent%20Driver%20Model%20%28IDM%29%2C%20of%20which%20some%20are%0Aalready%20included%20in%20the%20Extended%20Intelligent%20Driver%20Model%20%28EIDM%29%2C%20to%20improve%0Acalibration%20and%20validation%20results.%20They%20consist%20of%20equations%20based%20on%20vehicle%0Adynamics%20and%20drive%20off%20procedures.%20In%20addition%2C%20parameter%20selection%20plays%20a%0Adecisive%20role.%20Thus%2C%20we%20introduce%20a%20framework%20to%20calibrate%20CFMs%20using%20drone%0Adata%20captured%20at%20a%20signalized%20intersection%20in%20Stuttgart%2C%20Germany.%20We%20compare%0Athe%20calibration%20error%20of%20the%20Krauss%20Model%20with%20the%20IDM%20and%20EIDM.%20In%20this%20setup%2C%0Athe%20EIDM%20achieves%20a%2017.78%20%25%20lower%20mean%20error%20than%20the%20IDM%2C%20based%20on%20the%0Adistance%20difference%20between%20real%20world%20and%20simulated%20vehicles.%20Adding%20vehicle%0Adynamics%20equations%20to%20the%20EIDM%20further%20improves%20the%20results%20by%20an%20additional%0A18.97%20%25.%20The%20calibrated%20vehicle-driver%20combinations%20are%20then%20investigated%20by%0Asimulating%20the%20traffic%20in%20three%20different%20scenarios%3A%20at%20the%20original%0Aintersection%2C%20in%20a%20closed%20loop%20and%20in%20a%20stop-and-go%20wave.%20The%20data%20shows%20that%0Athe%20improved%20calibration%20process%20of%20individual%20vehicles%2C%20openly%20available%20at%0Ahttps%3A//www.github.com/stepeos/pycarmodel_calibration%2C%20also%20provides%20more%0Aaccurate%20macroscopic%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Intelligent%2520Driver%2520Model%2520by%2520Incorporating%2520Vehicle%250A%2520%2520Dynamics%253A%2520Microscopic%2520Calibration%2520and%2520Macroscopic%2520Validation%26entry.906535625%3DDominik%2520Salles%2520and%2520Steve%2520Oswald%2520and%2520Hans-Christian%2520Reuss%26entry.1292438233%3D%2520%2520Microscopic%2520traffic%2520simulations%2520are%2520used%2520to%2520evaluate%2520the%2520impact%2520of%250Ainfrastructure%2520modifications%2520and%2520evolving%2520vehicle%2520technologies%252C%2520such%2520as%250Aconnected%2520and%2520automated%2520driving.%2520Simulated%2520vehicles%2520are%2520controlled%2520via%250Acar-following%252C%2520lane-changing%2520and%2520junction%2520models%252C%2520which%2520are%2520designed%2520to%2520imitate%250Ahuman%2520driving%2520behavior.%2520However%252C%2520physics-based%2520car-following%2520models%2520%2528CFMs%2529%250Acannot%2520fully%2520replicate%2520measured%2520vehicle%2520trajectories.%2520Therefore%252C%2520we%2520present%250Amodel%2520extensions%2520for%2520the%2520Intelligent%2520Driver%2520Model%2520%2528IDM%2529%252C%2520of%2520which%2520some%2520are%250Aalready%2520included%2520in%2520the%2520Extended%2520Intelligent%2520Driver%2520Model%2520%2528EIDM%2529%252C%2520to%2520improve%250Acalibration%2520and%2520validation%2520results.%2520They%2520consist%2520of%2520equations%2520based%2520on%2520vehicle%250Adynamics%2520and%2520drive%2520off%2520procedures.%2520In%2520addition%252C%2520parameter%2520selection%2520plays%2520a%250Adecisive%2520role.%2520Thus%252C%2520we%2520introduce%2520a%2520framework%2520to%2520calibrate%2520CFMs%2520using%2520drone%250Adata%2520captured%2520at%2520a%2520signalized%2520intersection%2520in%2520Stuttgart%252C%2520Germany.%2520We%2520compare%250Athe%2520calibration%2520error%2520of%2520the%2520Krauss%2520Model%2520with%2520the%2520IDM%2520and%2520EIDM.%2520In%2520this%2520setup%252C%250Athe%2520EIDM%2520achieves%2520a%252017.78%2520%2525%2520lower%2520mean%2520error%2520than%2520the%2520IDM%252C%2520based%2520on%2520the%250Adistance%2520difference%2520between%2520real%2520world%2520and%2520simulated%2520vehicles.%2520Adding%2520vehicle%250Adynamics%2520equations%2520to%2520the%2520EIDM%2520further%2520improves%2520the%2520results%2520by%2520an%2520additional%250A18.97%2520%2525.%2520The%2520calibrated%2520vehicle-driver%2520combinations%2520are%2520then%2520investigated%2520by%250Asimulating%2520the%2520traffic%2520in%2520three%2520different%2520scenarios%253A%2520at%2520the%2520original%250Aintersection%252C%2520in%2520a%2520closed%2520loop%2520and%2520in%2520a%2520stop-and-go%2520wave.%2520The%2520data%2520shows%2520that%250Athe%2520improved%2520calibration%2520process%2520of%2520individual%2520vehicles%252C%2520openly%2520available%2520at%250Ahttps%253A//www.github.com/stepeos/pycarmodel_calibration%252C%2520also%2520provides%2520more%250Aaccurate%2520macroscopic%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Intelligent%20Driver%20Model%20by%20Incorporating%20Vehicle%0A%20%20Dynamics%3A%20Microscopic%20Calibration%20and%20Macroscopic%20Validation&entry.906535625=Dominik%20Salles%20and%20Steve%20Oswald%20and%20Hans-Christian%20Reuss&entry.1292438233=%20%20Microscopic%20traffic%20simulations%20are%20used%20to%20evaluate%20the%20impact%20of%0Ainfrastructure%20modifications%20and%20evolving%20vehicle%20technologies%2C%20such%20as%0Aconnected%20and%20automated%20driving.%20Simulated%20vehicles%20are%20controlled%20via%0Acar-following%2C%20lane-changing%20and%20junction%20models%2C%20which%20are%20designed%20to%20imitate%0Ahuman%20driving%20behavior.%20However%2C%20physics-based%20car-following%20models%20%28CFMs%29%0Acannot%20fully%20replicate%20measured%20vehicle%20trajectories.%20Therefore%2C%20we%20present%0Amodel%20extensions%20for%20the%20Intelligent%20Driver%20Model%20%28IDM%29%2C%20of%20which%20some%20are%0Aalready%20included%20in%20the%20Extended%20Intelligent%20Driver%20Model%20%28EIDM%29%2C%20to%20improve%0Acalibration%20and%20validation%20results.%20They%20consist%20of%20equations%20based%20on%20vehicle%0Adynamics%20and%20drive%20off%20procedures.%20In%20addition%2C%20parameter%20selection%20plays%20a%0Adecisive%20role.%20Thus%2C%20we%20introduce%20a%20framework%20to%20calibrate%20CFMs%20using%20drone%0Adata%20captured%20at%20a%20signalized%20intersection%20in%20Stuttgart%2C%20Germany.%20We%20compare%0Athe%20calibration%20error%20of%20the%20Krauss%20Model%20with%20the%20IDM%20and%20EIDM.%20In%20this%20setup%2C%0Athe%20EIDM%20achieves%20a%2017.78%20%25%20lower%20mean%20error%20than%20the%20IDM%2C%20based%20on%20the%0Adistance%20difference%20between%20real%20world%20and%20simulated%20vehicles.%20Adding%20vehicle%0Adynamics%20equations%20to%20the%20EIDM%20further%20improves%20the%20results%20by%20an%20additional%0A18.97%20%25.%20The%20calibrated%20vehicle-driver%20combinations%20are%20then%20investigated%20by%0Asimulating%20the%20traffic%20in%20three%20different%20scenarios%3A%20at%20the%20original%0Aintersection%2C%20in%20a%20closed%20loop%20and%20in%20a%20stop-and-go%20wave.%20The%20data%20shows%20that%0Athe%20improved%20calibration%20process%20of%20individual%20vehicles%2C%20openly%20available%20at%0Ahttps%3A//www.github.com/stepeos/pycarmodel_calibration%2C%20also%20provides%20more%0Aaccurate%20macroscopic%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03722v1&entry.124074799=Read"},
{"title": "MaxMind: A Memory Loop Network to Enhance Software Productivity based on\n  Large Language Models", "author": "Yuchen Dong and XiaoXiang Fang and Yuchen Hu and Renshuang Jiang and Zhe Jiang", "abstract": "  The application of large language models to facilitate automated software\noperations and tool generation (SOTG), thus augmenting software productivity,\nmirrors the early stages of human evolution when the ability to create and use\ntools accelerated the progress of civilization. These complex tasks require AI\nto continuously summarize and improve. Current research often overlooks the\nimportance of converting real-time task experiences into system memory and\ndifferentiating the value of existing knowledge for future reference. This\npaper addresses these issues by evolving external memory models into\nMemory-Loop Networks for timely memorization and experience referencing. We\nalso enhance a RAG mechanism with knowledge precision segmentation to utilize\nmemory based on value differentiation, and design the MaxMind model for SOTG\naccordingly.To demonstrate our approach, we developed MaxMind4Sheet, an\nelectronic spreadsheet processing system aligned with the MaxMind philosophy.\nComparative experiments with SheetCopilot have demonstrated that the\naccumulation and recycling of task memories lead to a steady enhancement in\ntask success rate, with an improvement rate of approximately 3%-6% per round in\nthis implementation example. Note that as the memories continue to grow, this\ncumulative improvement may be substantial. The inclusion of memory recycling\ncan also boost the system's task execution efficiency by up to 25%, and it can\naddress the retraining issue faced by LLMs when handling specialized tasks\nthrough memories transfer.These suggest that MaxMind has significant potential\nto enhance the capabilities and productivity of LLM systems in SOTG.\n", "link": "http://arxiv.org/abs/2408.03841v1", "date": "2024-08-07", "relevancy": 1.9028, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4849}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaxMind%3A%20A%20Memory%20Loop%20Network%20to%20Enhance%20Software%20Productivity%20based%20on%0A%20%20Large%20Language%20Models&body=Title%3A%20MaxMind%3A%20A%20Memory%20Loop%20Network%20to%20Enhance%20Software%20Productivity%20based%20on%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yuchen%20Dong%20and%20XiaoXiang%20Fang%20and%20Yuchen%20Hu%20and%20Renshuang%20Jiang%20and%20Zhe%20Jiang%0AAbstract%3A%20%20%20The%20application%20of%20large%20language%20models%20to%20facilitate%20automated%20software%0Aoperations%20and%20tool%20generation%20%28SOTG%29%2C%20thus%20augmenting%20software%20productivity%2C%0Amirrors%20the%20early%20stages%20of%20human%20evolution%20when%20the%20ability%20to%20create%20and%20use%0Atools%20accelerated%20the%20progress%20of%20civilization.%20These%20complex%20tasks%20require%20AI%0Ato%20continuously%20summarize%20and%20improve.%20Current%20research%20often%20overlooks%20the%0Aimportance%20of%20converting%20real-time%20task%20experiences%20into%20system%20memory%20and%0Adifferentiating%20the%20value%20of%20existing%20knowledge%20for%20future%20reference.%20This%0Apaper%20addresses%20these%20issues%20by%20evolving%20external%20memory%20models%20into%0AMemory-Loop%20Networks%20for%20timely%20memorization%20and%20experience%20referencing.%20We%0Aalso%20enhance%20a%20RAG%20mechanism%20with%20knowledge%20precision%20segmentation%20to%20utilize%0Amemory%20based%20on%20value%20differentiation%2C%20and%20design%20the%20MaxMind%20model%20for%20SOTG%0Aaccordingly.To%20demonstrate%20our%20approach%2C%20we%20developed%20MaxMind4Sheet%2C%20an%0Aelectronic%20spreadsheet%20processing%20system%20aligned%20with%20the%20MaxMind%20philosophy.%0AComparative%20experiments%20with%20SheetCopilot%20have%20demonstrated%20that%20the%0Aaccumulation%20and%20recycling%20of%20task%20memories%20lead%20to%20a%20steady%20enhancement%20in%0Atask%20success%20rate%2C%20with%20an%20improvement%20rate%20of%20approximately%203%25-6%25%20per%20round%20in%0Athis%20implementation%20example.%20Note%20that%20as%20the%20memories%20continue%20to%20grow%2C%20this%0Acumulative%20improvement%20may%20be%20substantial.%20The%20inclusion%20of%20memory%20recycling%0Acan%20also%20boost%20the%20system%27s%20task%20execution%20efficiency%20by%20up%20to%2025%25%2C%20and%20it%20can%0Aaddress%20the%20retraining%20issue%20faced%20by%20LLMs%20when%20handling%20specialized%20tasks%0Athrough%20memories%20transfer.These%20suggest%20that%20MaxMind%20has%20significant%20potential%0Ato%20enhance%20the%20capabilities%20and%20productivity%20of%20LLM%20systems%20in%20SOTG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaxMind%253A%2520A%2520Memory%2520Loop%2520Network%2520to%2520Enhance%2520Software%2520Productivity%2520based%2520on%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYuchen%2520Dong%2520and%2520XiaoXiang%2520Fang%2520and%2520Yuchen%2520Hu%2520and%2520Renshuang%2520Jiang%2520and%2520Zhe%2520Jiang%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520large%2520language%2520models%2520to%2520facilitate%2520automated%2520software%250Aoperations%2520and%2520tool%2520generation%2520%2528SOTG%2529%252C%2520thus%2520augmenting%2520software%2520productivity%252C%250Amirrors%2520the%2520early%2520stages%2520of%2520human%2520evolution%2520when%2520the%2520ability%2520to%2520create%2520and%2520use%250Atools%2520accelerated%2520the%2520progress%2520of%2520civilization.%2520These%2520complex%2520tasks%2520require%2520AI%250Ato%2520continuously%2520summarize%2520and%2520improve.%2520Current%2520research%2520often%2520overlooks%2520the%250Aimportance%2520of%2520converting%2520real-time%2520task%2520experiences%2520into%2520system%2520memory%2520and%250Adifferentiating%2520the%2520value%2520of%2520existing%2520knowledge%2520for%2520future%2520reference.%2520This%250Apaper%2520addresses%2520these%2520issues%2520by%2520evolving%2520external%2520memory%2520models%2520into%250AMemory-Loop%2520Networks%2520for%2520timely%2520memorization%2520and%2520experience%2520referencing.%2520We%250Aalso%2520enhance%2520a%2520RAG%2520mechanism%2520with%2520knowledge%2520precision%2520segmentation%2520to%2520utilize%250Amemory%2520based%2520on%2520value%2520differentiation%252C%2520and%2520design%2520the%2520MaxMind%2520model%2520for%2520SOTG%250Aaccordingly.To%2520demonstrate%2520our%2520approach%252C%2520we%2520developed%2520MaxMind4Sheet%252C%2520an%250Aelectronic%2520spreadsheet%2520processing%2520system%2520aligned%2520with%2520the%2520MaxMind%2520philosophy.%250AComparative%2520experiments%2520with%2520SheetCopilot%2520have%2520demonstrated%2520that%2520the%250Aaccumulation%2520and%2520recycling%2520of%2520task%2520memories%2520lead%2520to%2520a%2520steady%2520enhancement%2520in%250Atask%2520success%2520rate%252C%2520with%2520an%2520improvement%2520rate%2520of%2520approximately%25203%2525-6%2525%2520per%2520round%2520in%250Athis%2520implementation%2520example.%2520Note%2520that%2520as%2520the%2520memories%2520continue%2520to%2520grow%252C%2520this%250Acumulative%2520improvement%2520may%2520be%2520substantial.%2520The%2520inclusion%2520of%2520memory%2520recycling%250Acan%2520also%2520boost%2520the%2520system%2527s%2520task%2520execution%2520efficiency%2520by%2520up%2520to%252025%2525%252C%2520and%2520it%2520can%250Aaddress%2520the%2520retraining%2520issue%2520faced%2520by%2520LLMs%2520when%2520handling%2520specialized%2520tasks%250Athrough%2520memories%2520transfer.These%2520suggest%2520that%2520MaxMind%2520has%2520significant%2520potential%250Ato%2520enhance%2520the%2520capabilities%2520and%2520productivity%2520of%2520LLM%2520systems%2520in%2520SOTG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaxMind%3A%20A%20Memory%20Loop%20Network%20to%20Enhance%20Software%20Productivity%20based%20on%0A%20%20Large%20Language%20Models&entry.906535625=Yuchen%20Dong%20and%20XiaoXiang%20Fang%20and%20Yuchen%20Hu%20and%20Renshuang%20Jiang%20and%20Zhe%20Jiang&entry.1292438233=%20%20The%20application%20of%20large%20language%20models%20to%20facilitate%20automated%20software%0Aoperations%20and%20tool%20generation%20%28SOTG%29%2C%20thus%20augmenting%20software%20productivity%2C%0Amirrors%20the%20early%20stages%20of%20human%20evolution%20when%20the%20ability%20to%20create%20and%20use%0Atools%20accelerated%20the%20progress%20of%20civilization.%20These%20complex%20tasks%20require%20AI%0Ato%20continuously%20summarize%20and%20improve.%20Current%20research%20often%20overlooks%20the%0Aimportance%20of%20converting%20real-time%20task%20experiences%20into%20system%20memory%20and%0Adifferentiating%20the%20value%20of%20existing%20knowledge%20for%20future%20reference.%20This%0Apaper%20addresses%20these%20issues%20by%20evolving%20external%20memory%20models%20into%0AMemory-Loop%20Networks%20for%20timely%20memorization%20and%20experience%20referencing.%20We%0Aalso%20enhance%20a%20RAG%20mechanism%20with%20knowledge%20precision%20segmentation%20to%20utilize%0Amemory%20based%20on%20value%20differentiation%2C%20and%20design%20the%20MaxMind%20model%20for%20SOTG%0Aaccordingly.To%20demonstrate%20our%20approach%2C%20we%20developed%20MaxMind4Sheet%2C%20an%0Aelectronic%20spreadsheet%20processing%20system%20aligned%20with%20the%20MaxMind%20philosophy.%0AComparative%20experiments%20with%20SheetCopilot%20have%20demonstrated%20that%20the%0Aaccumulation%20and%20recycling%20of%20task%20memories%20lead%20to%20a%20steady%20enhancement%20in%0Atask%20success%20rate%2C%20with%20an%20improvement%20rate%20of%20approximately%203%25-6%25%20per%20round%20in%0Athis%20implementation%20example.%20Note%20that%20as%20the%20memories%20continue%20to%20grow%2C%20this%0Acumulative%20improvement%20may%20be%20substantial.%20The%20inclusion%20of%20memory%20recycling%0Acan%20also%20boost%20the%20system%27s%20task%20execution%20efficiency%20by%20up%20to%2025%25%2C%20and%20it%20can%0Aaddress%20the%20retraining%20issue%20faced%20by%20LLMs%20when%20handling%20specialized%20tasks%0Athrough%20memories%20transfer.These%20suggest%20that%20MaxMind%20has%20significant%20potential%0Ato%20enhance%20the%20capabilities%20and%20productivity%20of%20LLM%20systems%20in%20SOTG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03841v1&entry.124074799=Read"},
{"title": "Sampling for Model Predictive Trajectory Planning in Autonomous Driving\n  using Normalizing Flows", "author": "Georg Rabenstein and Lars Ullrich and Knut Graichen", "abstract": "  Alongside optimization-based planners, sampling-based approaches are often\nused in trajectory planning for autonomous driving due to their simplicity.\nModel predictive path integral control is a framework that builds upon\noptimization principles while incorporating stochastic sampling of input\ntrajectories. This paper investigates several sampling approaches for\ntrajectory generation. In this context, normalizing flows originating from the\nfield of variational inference are considered for the generation of sampling\ndistributions, as they model transformations of simple to more complex\ndistributions. Accordingly, learning-based normalizing flow models are trained\nfor a more efficient exploration of the input domain for the task at hand. The\ndeveloped algorithm and the proposed sampling distributions are evaluated in\ntwo simulation scenarios.\n", "link": "http://arxiv.org/abs/2404.09657v3", "date": "2024-08-07", "relevancy": 1.901, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4779}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows&body=Title%3A%20Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows%0AAuthor%3A%20Georg%20Rabenstein%20and%20Lars%20Ullrich%20and%20Knut%20Graichen%0AAbstract%3A%20%20%20Alongside%20optimization-based%20planners%2C%20sampling-based%20approaches%20are%20often%0Aused%20in%20trajectory%20planning%20for%20autonomous%20driving%20due%20to%20their%20simplicity.%0AModel%20predictive%20path%20integral%20control%20is%20a%20framework%20that%20builds%20upon%0Aoptimization%20principles%20while%20incorporating%20stochastic%20sampling%20of%20input%0Atrajectories.%20This%20paper%20investigates%20several%20sampling%20approaches%20for%0Atrajectory%20generation.%20In%20this%20context%2C%20normalizing%20flows%20originating%20from%20the%0Afield%20of%20variational%20inference%20are%20considered%20for%20the%20generation%20of%20sampling%0Adistributions%2C%20as%20they%20model%20transformations%20of%20simple%20to%20more%20complex%0Adistributions.%20Accordingly%2C%20learning-based%20normalizing%20flow%20models%20are%20trained%0Afor%20a%20more%20efficient%20exploration%20of%20the%20input%20domain%20for%20the%20task%20at%20hand.%20The%0Adeveloped%20algorithm%20and%20the%20proposed%20sampling%20distributions%20are%20evaluated%20in%0Atwo%20simulation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09657v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%2520for%2520Model%2520Predictive%2520Trajectory%2520Planning%2520in%2520Autonomous%2520Driving%250A%2520%2520using%2520Normalizing%2520Flows%26entry.906535625%3DGeorg%2520Rabenstein%2520and%2520Lars%2520Ullrich%2520and%2520Knut%2520Graichen%26entry.1292438233%3D%2520%2520Alongside%2520optimization-based%2520planners%252C%2520sampling-based%2520approaches%2520are%2520often%250Aused%2520in%2520trajectory%2520planning%2520for%2520autonomous%2520driving%2520due%2520to%2520their%2520simplicity.%250AModel%2520predictive%2520path%2520integral%2520control%2520is%2520a%2520framework%2520that%2520builds%2520upon%250Aoptimization%2520principles%2520while%2520incorporating%2520stochastic%2520sampling%2520of%2520input%250Atrajectories.%2520This%2520paper%2520investigates%2520several%2520sampling%2520approaches%2520for%250Atrajectory%2520generation.%2520In%2520this%2520context%252C%2520normalizing%2520flows%2520originating%2520from%2520the%250Afield%2520of%2520variational%2520inference%2520are%2520considered%2520for%2520the%2520generation%2520of%2520sampling%250Adistributions%252C%2520as%2520they%2520model%2520transformations%2520of%2520simple%2520to%2520more%2520complex%250Adistributions.%2520Accordingly%252C%2520learning-based%2520normalizing%2520flow%2520models%2520are%2520trained%250Afor%2520a%2520more%2520efficient%2520exploration%2520of%2520the%2520input%2520domain%2520for%2520the%2520task%2520at%2520hand.%2520The%250Adeveloped%2520algorithm%2520and%2520the%2520proposed%2520sampling%2520distributions%2520are%2520evaluated%2520in%250Atwo%2520simulation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09657v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20for%20Model%20Predictive%20Trajectory%20Planning%20in%20Autonomous%20Driving%0A%20%20using%20Normalizing%20Flows&entry.906535625=Georg%20Rabenstein%20and%20Lars%20Ullrich%20and%20Knut%20Graichen&entry.1292438233=%20%20Alongside%20optimization-based%20planners%2C%20sampling-based%20approaches%20are%20often%0Aused%20in%20trajectory%20planning%20for%20autonomous%20driving%20due%20to%20their%20simplicity.%0AModel%20predictive%20path%20integral%20control%20is%20a%20framework%20that%20builds%20upon%0Aoptimization%20principles%20while%20incorporating%20stochastic%20sampling%20of%20input%0Atrajectories.%20This%20paper%20investigates%20several%20sampling%20approaches%20for%0Atrajectory%20generation.%20In%20this%20context%2C%20normalizing%20flows%20originating%20from%20the%0Afield%20of%20variational%20inference%20are%20considered%20for%20the%20generation%20of%20sampling%0Adistributions%2C%20as%20they%20model%20transformations%20of%20simple%20to%20more%20complex%0Adistributions.%20Accordingly%2C%20learning-based%20normalizing%20flow%20models%20are%20trained%0Afor%20a%20more%20efficient%20exploration%20of%20the%20input%20domain%20for%20the%20task%20at%20hand.%20The%0Adeveloped%20algorithm%20and%20the%20proposed%20sampling%20distributions%20are%20evaluated%20in%0Atwo%20simulation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09657v3&entry.124074799=Read"},
{"title": "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic\n  Performance for Mercosur Common Nomenclature", "author": "Vin\u00edcius Di Oliveira and Yuri Fa\u00e7anha Bezerra and Li Weigang and Pedro Carvalho Brom and Victor Rafael R. Celestino", "abstract": "  Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.\n", "link": "http://arxiv.org/abs/2408.03936v1", "date": "2024-08-07", "relevancy": 1.871, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4992}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLIM-RAFT%3A%20A%20Novel%20Fine-Tuning%20Approach%20to%20Improve%20Cross-Linguistic%0A%20%20Performance%20for%20Mercosur%20Common%20Nomenclature&body=Title%3A%20SLIM-RAFT%3A%20A%20Novel%20Fine-Tuning%20Approach%20to%20Improve%20Cross-Linguistic%0A%20%20Performance%20for%20Mercosur%20Common%20Nomenclature%0AAuthor%3A%20Vin%C3%ADcius%20Di%20Oliveira%20and%20Yuri%20Fa%C3%A7anha%20Bezerra%20and%20Li%20Weigang%20and%20Pedro%20Carvalho%20Brom%20and%20Victor%20Rafael%20R.%20Celestino%0AAbstract%3A%20%20%20Natural%20language%20processing%20%28NLP%29%20has%20seen%20significant%20advancements%20with%20the%0Aadvent%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20substantial%20improvements%20are%0Astill%20needed%20for%20languages%20other%20than%20English%2C%20especially%20for%20specific%20domains%0Alike%20the%20applications%20of%20Mercosur%20Common%20Nomenclature%20%28NCM%29%2C%20a%20Brazilian%0AHarmonized%20System%20%28HS%29.%20To%20address%20this%20gap%2C%20this%20study%20uses%20TeenyTineLLaMA%2C%20a%0Afoundational%20Portuguese%20LLM%2C%20as%20an%20LLM%20source%20to%20implement%20the%20NCM%20application%0Aprocessing.%20Additionally%2C%20a%20simplified%20Retrieval-Augmented%20Fine-Tuning%20%28RAFT%29%0Atechnique%2C%20termed%20SLIM-RAFT%2C%20is%20proposed%20for%20task-specific%20fine-tuning%20of%20LLMs.%0AThis%20approach%20retains%20the%20chain-of-thought%20%28CoT%29%20methodology%20for%20prompt%0Adevelopment%20in%20a%20more%20concise%20and%20streamlined%20manner%2C%20utilizing%20brief%20and%0Afocused%20documents%20for%20training.%20The%20proposed%20model%20demonstrates%20an%20efficient%0Aand%20cost-effective%20alternative%20for%20fine-tuning%20smaller%20LLMs%2C%20significantly%0Aoutperforming%20TeenyTineLLaMA%20and%20ChatGPT-4%20in%20the%20same%20task.%20Although%20the%0Aresearch%20focuses%20on%20NCM%20applications%2C%20the%20methodology%20can%20be%20easily%20adapted%20for%0AHS%20applications%20worldwide.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLIM-RAFT%253A%2520A%2520Novel%2520Fine-Tuning%2520Approach%2520to%2520Improve%2520Cross-Linguistic%250A%2520%2520Performance%2520for%2520Mercosur%2520Common%2520Nomenclature%26entry.906535625%3DVin%25C3%25ADcius%2520Di%2520Oliveira%2520and%2520Yuri%2520Fa%25C3%25A7anha%2520Bezerra%2520and%2520Li%2520Weigang%2520and%2520Pedro%2520Carvalho%2520Brom%2520and%2520Victor%2520Rafael%2520R.%2520Celestino%26entry.1292438233%3D%2520%2520Natural%2520language%2520processing%2520%2528NLP%2529%2520has%2520seen%2520significant%2520advancements%2520with%2520the%250Aadvent%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520substantial%2520improvements%2520are%250Astill%2520needed%2520for%2520languages%2520other%2520than%2520English%252C%2520especially%2520for%2520specific%2520domains%250Alike%2520the%2520applications%2520of%2520Mercosur%2520Common%2520Nomenclature%2520%2528NCM%2529%252C%2520a%2520Brazilian%250AHarmonized%2520System%2520%2528HS%2529.%2520To%2520address%2520this%2520gap%252C%2520this%2520study%2520uses%2520TeenyTineLLaMA%252C%2520a%250Afoundational%2520Portuguese%2520LLM%252C%2520as%2520an%2520LLM%2520source%2520to%2520implement%2520the%2520NCM%2520application%250Aprocessing.%2520Additionally%252C%2520a%2520simplified%2520Retrieval-Augmented%2520Fine-Tuning%2520%2528RAFT%2529%250Atechnique%252C%2520termed%2520SLIM-RAFT%252C%2520is%2520proposed%2520for%2520task-specific%2520fine-tuning%2520of%2520LLMs.%250AThis%2520approach%2520retains%2520the%2520chain-of-thought%2520%2528CoT%2529%2520methodology%2520for%2520prompt%250Adevelopment%2520in%2520a%2520more%2520concise%2520and%2520streamlined%2520manner%252C%2520utilizing%2520brief%2520and%250Afocused%2520documents%2520for%2520training.%2520The%2520proposed%2520model%2520demonstrates%2520an%2520efficient%250Aand%2520cost-effective%2520alternative%2520for%2520fine-tuning%2520smaller%2520LLMs%252C%2520significantly%250Aoutperforming%2520TeenyTineLLaMA%2520and%2520ChatGPT-4%2520in%2520the%2520same%2520task.%2520Although%2520the%250Aresearch%2520focuses%2520on%2520NCM%2520applications%252C%2520the%2520methodology%2520can%2520be%2520easily%2520adapted%2520for%250AHS%2520applications%2520worldwide.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLIM-RAFT%3A%20A%20Novel%20Fine-Tuning%20Approach%20to%20Improve%20Cross-Linguistic%0A%20%20Performance%20for%20Mercosur%20Common%20Nomenclature&entry.906535625=Vin%C3%ADcius%20Di%20Oliveira%20and%20Yuri%20Fa%C3%A7anha%20Bezerra%20and%20Li%20Weigang%20and%20Pedro%20Carvalho%20Brom%20and%20Victor%20Rafael%20R.%20Celestino&entry.1292438233=%20%20Natural%20language%20processing%20%28NLP%29%20has%20seen%20significant%20advancements%20with%20the%0Aadvent%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20substantial%20improvements%20are%0Astill%20needed%20for%20languages%20other%20than%20English%2C%20especially%20for%20specific%20domains%0Alike%20the%20applications%20of%20Mercosur%20Common%20Nomenclature%20%28NCM%29%2C%20a%20Brazilian%0AHarmonized%20System%20%28HS%29.%20To%20address%20this%20gap%2C%20this%20study%20uses%20TeenyTineLLaMA%2C%20a%0Afoundational%20Portuguese%20LLM%2C%20as%20an%20LLM%20source%20to%20implement%20the%20NCM%20application%0Aprocessing.%20Additionally%2C%20a%20simplified%20Retrieval-Augmented%20Fine-Tuning%20%28RAFT%29%0Atechnique%2C%20termed%20SLIM-RAFT%2C%20is%20proposed%20for%20task-specific%20fine-tuning%20of%20LLMs.%0AThis%20approach%20retains%20the%20chain-of-thought%20%28CoT%29%20methodology%20for%20prompt%0Adevelopment%20in%20a%20more%20concise%20and%20streamlined%20manner%2C%20utilizing%20brief%20and%0Afocused%20documents%20for%20training.%20The%20proposed%20model%20demonstrates%20an%20efficient%0Aand%20cost-effective%20alternative%20for%20fine-tuning%20smaller%20LLMs%2C%20significantly%0Aoutperforming%20TeenyTineLLaMA%20and%20ChatGPT-4%20in%20the%20same%20task.%20Although%20the%0Aresearch%20focuses%20on%20NCM%20applications%2C%20the%20methodology%20can%20be%20easily%20adapted%20for%0AHS%20applications%20worldwide.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03936v1&entry.124074799=Read"},
{"title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large\n  Language Models", "author": "Pengxiang Zhao and Hanyu Hu and Ping Li and Yi Zheng and Zhefeng Wang and Xiaoming Yuan", "abstract": "  Pruning is a critical strategy for compressing trained large language models\n(LLMs), aiming at substantial memory conservation and computational\nacceleration without compromising performance. However, existing pruning\nmethods often necessitate inefficient retraining for billion-scale LLMs or rely\non heuristic methods such as the optimal brain surgeon framework, which degrade\nperformance. In this paper, we introduce FISTAPruner, the first post-training\npruner based on convex optimization models and algorithms. Specifically, we\npropose a convex optimization model incorporating $\\ell_1$ norm to induce\nsparsity and utilize the FISTA solver for optimization. FISTAPruner\nincorporates an intra-layer cumulative error correction mechanism and supports\nparallel pruning. We comprehensively evaluate FISTAPruner on models such as\nOPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructured\nand 2:4 semi-structured sparsity, demonstrating superior performance over\nexisting state-of-the-art methods across various language benchmarks.\n", "link": "http://arxiv.org/abs/2408.03728v1", "date": "2024-08-07", "relevancy": 1.862, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.477}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4581}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Convex-optimization-based%20Layer-wise%20Post-training%20Pruner%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20A%20Convex-optimization-based%20Layer-wise%20Post-training%20Pruner%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Pengxiang%20Zhao%20and%20Hanyu%20Hu%20and%20Ping%20Li%20and%20Yi%20Zheng%20and%20Zhefeng%20Wang%20and%20Xiaoming%20Yuan%0AAbstract%3A%20%20%20Pruning%20is%20a%20critical%20strategy%20for%20compressing%20trained%20large%20language%20models%0A%28LLMs%29%2C%20aiming%20at%20substantial%20memory%20conservation%20and%20computational%0Aacceleration%20without%20compromising%20performance.%20However%2C%20existing%20pruning%0Amethods%20often%20necessitate%20inefficient%20retraining%20for%20billion-scale%20LLMs%20or%20rely%0Aon%20heuristic%20methods%20such%20as%20the%20optimal%20brain%20surgeon%20framework%2C%20which%20degrade%0Aperformance.%20In%20this%20paper%2C%20we%20introduce%20FISTAPruner%2C%20the%20first%20post-training%0Apruner%20based%20on%20convex%20optimization%20models%20and%20algorithms.%20Specifically%2C%20we%0Apropose%20a%20convex%20optimization%20model%20incorporating%20%24%5Cell_1%24%20norm%20to%20induce%0Asparsity%20and%20utilize%20the%20FISTA%20solver%20for%20optimization.%20FISTAPruner%0Aincorporates%20an%20intra-layer%20cumulative%20error%20correction%20mechanism%20and%20supports%0Aparallel%20pruning.%20We%20comprehensively%20evaluate%20FISTAPruner%20on%20models%20such%20as%0AOPT%2C%20LLaMA%2C%20LLaMA-2%2C%20and%20LLaMA-3%20with%20125M%20to%2070B%20parameters%20under%20unstructured%0Aand%202%3A4%20semi-structured%20sparsity%2C%20demonstrating%20superior%20performance%20over%0Aexisting%20state-of-the-art%20methods%20across%20various%20language%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Convex-optimization-based%2520Layer-wise%2520Post-training%2520Pruner%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DPengxiang%2520Zhao%2520and%2520Hanyu%2520Hu%2520and%2520Ping%2520Li%2520and%2520Yi%2520Zheng%2520and%2520Zhefeng%2520Wang%2520and%2520Xiaoming%2520Yuan%26entry.1292438233%3D%2520%2520Pruning%2520is%2520a%2520critical%2520strategy%2520for%2520compressing%2520trained%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520aiming%2520at%2520substantial%2520memory%2520conservation%2520and%2520computational%250Aacceleration%2520without%2520compromising%2520performance.%2520However%252C%2520existing%2520pruning%250Amethods%2520often%2520necessitate%2520inefficient%2520retraining%2520for%2520billion-scale%2520LLMs%2520or%2520rely%250Aon%2520heuristic%2520methods%2520such%2520as%2520the%2520optimal%2520brain%2520surgeon%2520framework%252C%2520which%2520degrade%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520FISTAPruner%252C%2520the%2520first%2520post-training%250Apruner%2520based%2520on%2520convex%2520optimization%2520models%2520and%2520algorithms.%2520Specifically%252C%2520we%250Apropose%2520a%2520convex%2520optimization%2520model%2520incorporating%2520%2524%255Cell_1%2524%2520norm%2520to%2520induce%250Asparsity%2520and%2520utilize%2520the%2520FISTA%2520solver%2520for%2520optimization.%2520FISTAPruner%250Aincorporates%2520an%2520intra-layer%2520cumulative%2520error%2520correction%2520mechanism%2520and%2520supports%250Aparallel%2520pruning.%2520We%2520comprehensively%2520evaluate%2520FISTAPruner%2520on%2520models%2520such%2520as%250AOPT%252C%2520LLaMA%252C%2520LLaMA-2%252C%2520and%2520LLaMA-3%2520with%2520125M%2520to%252070B%2520parameters%2520under%2520unstructured%250Aand%25202%253A4%2520semi-structured%2520sparsity%252C%2520demonstrating%2520superior%2520performance%2520over%250Aexisting%2520state-of-the-art%2520methods%2520across%2520various%2520language%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Convex-optimization-based%20Layer-wise%20Post-training%20Pruner%20for%20Large%0A%20%20Language%20Models&entry.906535625=Pengxiang%20Zhao%20and%20Hanyu%20Hu%20and%20Ping%20Li%20and%20Yi%20Zheng%20and%20Zhefeng%20Wang%20and%20Xiaoming%20Yuan&entry.1292438233=%20%20Pruning%20is%20a%20critical%20strategy%20for%20compressing%20trained%20large%20language%20models%0A%28LLMs%29%2C%20aiming%20at%20substantial%20memory%20conservation%20and%20computational%0Aacceleration%20without%20compromising%20performance.%20However%2C%20existing%20pruning%0Amethods%20often%20necessitate%20inefficient%20retraining%20for%20billion-scale%20LLMs%20or%20rely%0Aon%20heuristic%20methods%20such%20as%20the%20optimal%20brain%20surgeon%20framework%2C%20which%20degrade%0Aperformance.%20In%20this%20paper%2C%20we%20introduce%20FISTAPruner%2C%20the%20first%20post-training%0Apruner%20based%20on%20convex%20optimization%20models%20and%20algorithms.%20Specifically%2C%20we%0Apropose%20a%20convex%20optimization%20model%20incorporating%20%24%5Cell_1%24%20norm%20to%20induce%0Asparsity%20and%20utilize%20the%20FISTA%20solver%20for%20optimization.%20FISTAPruner%0Aincorporates%20an%20intra-layer%20cumulative%20error%20correction%20mechanism%20and%20supports%0Aparallel%20pruning.%20We%20comprehensively%20evaluate%20FISTAPruner%20on%20models%20such%20as%0AOPT%2C%20LLaMA%2C%20LLaMA-2%2C%20and%20LLaMA-3%20with%20125M%20to%2070B%20parameters%20under%20unstructured%0Aand%202%3A4%20semi-structured%20sparsity%2C%20demonstrating%20superior%20performance%20over%0Aexisting%20state-of-the-art%20methods%20across%20various%20language%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03728v1&entry.124074799=Read"},
{"title": "LaFA: Latent Feature Attacks on Non-negative Matrix Factorization", "author": "Minh Vu and Ben Nebgen and Erik Skau and Geigh Zollicoffer and Juan Castorena and Kim Rasmussen and Boian Alexandrov and Manish Bhattarai", "abstract": "  As Machine Learning (ML) applications rapidly grow, concerns about\nadversarial attacks compromising their reliability have gained significant\nattention. One unsupervised ML method known for its resilience to such attacks\nis Non-negative Matrix Factorization (NMF), an algorithm that decomposes input\ndata into lower-dimensional latent features. However, the introduction of\npowerful computational tools such as Pytorch enables the computation of\ngradients of the latent features with respect to the original data, raising\nconcerns about NMF's reliability. Interestingly, naively deriving the\nadversarial loss for NMF as in the case of ML would result in the\nreconstruction loss, which can be shown theoretically to be an ineffective\nattacking objective. In this work, we introduce a novel class of attacks in NMF\ntermed Latent Feature Attacks (LaFA), which aim to manipulate the latent\nfeatures produced by the NMF process. Our method utilizes the Feature Error\n(FE) loss directly on the latent features. By employing FE loss, we generate\nperturbations in the original data that significantly affect the extracted\nlatent features, revealing vulnerabilities akin to those found in other ML\ntechniques. To handle large peak-memory overhead from gradient back-propagation\nin FE attacks, we develop a method based on implicit differentiation which\nenables their scaling to larger datasets. We validate NMF vulnerabilities and\nFE attacks effectiveness through extensive experiments on synthetic and\nreal-world data.\n", "link": "http://arxiv.org/abs/2408.03909v1", "date": "2024-08-07", "relevancy": 1.8266, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4467}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaFA%3A%20Latent%20Feature%20Attacks%20on%20Non-negative%20Matrix%20Factorization&body=Title%3A%20LaFA%3A%20Latent%20Feature%20Attacks%20on%20Non-negative%20Matrix%20Factorization%0AAuthor%3A%20Minh%20Vu%20and%20Ben%20Nebgen%20and%20Erik%20Skau%20and%20Geigh%20Zollicoffer%20and%20Juan%20Castorena%20and%20Kim%20Rasmussen%20and%20Boian%20Alexandrov%20and%20Manish%20Bhattarai%0AAbstract%3A%20%20%20As%20Machine%20Learning%20%28ML%29%20applications%20rapidly%20grow%2C%20concerns%20about%0Aadversarial%20attacks%20compromising%20their%20reliability%20have%20gained%20significant%0Aattention.%20One%20unsupervised%20ML%20method%20known%20for%20its%20resilience%20to%20such%20attacks%0Ais%20Non-negative%20Matrix%20Factorization%20%28NMF%29%2C%20an%20algorithm%20that%20decomposes%20input%0Adata%20into%20lower-dimensional%20latent%20features.%20However%2C%20the%20introduction%20of%0Apowerful%20computational%20tools%20such%20as%20Pytorch%20enables%20the%20computation%20of%0Agradients%20of%20the%20latent%20features%20with%20respect%20to%20the%20original%20data%2C%20raising%0Aconcerns%20about%20NMF%27s%20reliability.%20Interestingly%2C%20naively%20deriving%20the%0Aadversarial%20loss%20for%20NMF%20as%20in%20the%20case%20of%20ML%20would%20result%20in%20the%0Areconstruction%20loss%2C%20which%20can%20be%20shown%20theoretically%20to%20be%20an%20ineffective%0Aattacking%20objective.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20class%20of%20attacks%20in%20NMF%0Atermed%20Latent%20Feature%20Attacks%20%28LaFA%29%2C%20which%20aim%20to%20manipulate%20the%20latent%0Afeatures%20produced%20by%20the%20NMF%20process.%20Our%20method%20utilizes%20the%20Feature%20Error%0A%28FE%29%20loss%20directly%20on%20the%20latent%20features.%20By%20employing%20FE%20loss%2C%20we%20generate%0Aperturbations%20in%20the%20original%20data%20that%20significantly%20affect%20the%20extracted%0Alatent%20features%2C%20revealing%20vulnerabilities%20akin%20to%20those%20found%20in%20other%20ML%0Atechniques.%20To%20handle%20large%20peak-memory%20overhead%20from%20gradient%20back-propagation%0Ain%20FE%20attacks%2C%20we%20develop%20a%20method%20based%20on%20implicit%20differentiation%20which%0Aenables%20their%20scaling%20to%20larger%20datasets.%20We%20validate%20NMF%20vulnerabilities%20and%0AFE%20attacks%20effectiveness%20through%20extensive%20experiments%20on%20synthetic%20and%0Areal-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaFA%253A%2520Latent%2520Feature%2520Attacks%2520on%2520Non-negative%2520Matrix%2520Factorization%26entry.906535625%3DMinh%2520Vu%2520and%2520Ben%2520Nebgen%2520and%2520Erik%2520Skau%2520and%2520Geigh%2520Zollicoffer%2520and%2520Juan%2520Castorena%2520and%2520Kim%2520Rasmussen%2520and%2520Boian%2520Alexandrov%2520and%2520Manish%2520Bhattarai%26entry.1292438233%3D%2520%2520As%2520Machine%2520Learning%2520%2528ML%2529%2520applications%2520rapidly%2520grow%252C%2520concerns%2520about%250Aadversarial%2520attacks%2520compromising%2520their%2520reliability%2520have%2520gained%2520significant%250Aattention.%2520One%2520unsupervised%2520ML%2520method%2520known%2520for%2520its%2520resilience%2520to%2520such%2520attacks%250Ais%2520Non-negative%2520Matrix%2520Factorization%2520%2528NMF%2529%252C%2520an%2520algorithm%2520that%2520decomposes%2520input%250Adata%2520into%2520lower-dimensional%2520latent%2520features.%2520However%252C%2520the%2520introduction%2520of%250Apowerful%2520computational%2520tools%2520such%2520as%2520Pytorch%2520enables%2520the%2520computation%2520of%250Agradients%2520of%2520the%2520latent%2520features%2520with%2520respect%2520to%2520the%2520original%2520data%252C%2520raising%250Aconcerns%2520about%2520NMF%2527s%2520reliability.%2520Interestingly%252C%2520naively%2520deriving%2520the%250Aadversarial%2520loss%2520for%2520NMF%2520as%2520in%2520the%2520case%2520of%2520ML%2520would%2520result%2520in%2520the%250Areconstruction%2520loss%252C%2520which%2520can%2520be%2520shown%2520theoretically%2520to%2520be%2520an%2520ineffective%250Aattacking%2520objective.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520class%2520of%2520attacks%2520in%2520NMF%250Atermed%2520Latent%2520Feature%2520Attacks%2520%2528LaFA%2529%252C%2520which%2520aim%2520to%2520manipulate%2520the%2520latent%250Afeatures%2520produced%2520by%2520the%2520NMF%2520process.%2520Our%2520method%2520utilizes%2520the%2520Feature%2520Error%250A%2528FE%2529%2520loss%2520directly%2520on%2520the%2520latent%2520features.%2520By%2520employing%2520FE%2520loss%252C%2520we%2520generate%250Aperturbations%2520in%2520the%2520original%2520data%2520that%2520significantly%2520affect%2520the%2520extracted%250Alatent%2520features%252C%2520revealing%2520vulnerabilities%2520akin%2520to%2520those%2520found%2520in%2520other%2520ML%250Atechniques.%2520To%2520handle%2520large%2520peak-memory%2520overhead%2520from%2520gradient%2520back-propagation%250Ain%2520FE%2520attacks%252C%2520we%2520develop%2520a%2520method%2520based%2520on%2520implicit%2520differentiation%2520which%250Aenables%2520their%2520scaling%2520to%2520larger%2520datasets.%2520We%2520validate%2520NMF%2520vulnerabilities%2520and%250AFE%2520attacks%2520effectiveness%2520through%2520extensive%2520experiments%2520on%2520synthetic%2520and%250Areal-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaFA%3A%20Latent%20Feature%20Attacks%20on%20Non-negative%20Matrix%20Factorization&entry.906535625=Minh%20Vu%20and%20Ben%20Nebgen%20and%20Erik%20Skau%20and%20Geigh%20Zollicoffer%20and%20Juan%20Castorena%20and%20Kim%20Rasmussen%20and%20Boian%20Alexandrov%20and%20Manish%20Bhattarai&entry.1292438233=%20%20As%20Machine%20Learning%20%28ML%29%20applications%20rapidly%20grow%2C%20concerns%20about%0Aadversarial%20attacks%20compromising%20their%20reliability%20have%20gained%20significant%0Aattention.%20One%20unsupervised%20ML%20method%20known%20for%20its%20resilience%20to%20such%20attacks%0Ais%20Non-negative%20Matrix%20Factorization%20%28NMF%29%2C%20an%20algorithm%20that%20decomposes%20input%0Adata%20into%20lower-dimensional%20latent%20features.%20However%2C%20the%20introduction%20of%0Apowerful%20computational%20tools%20such%20as%20Pytorch%20enables%20the%20computation%20of%0Agradients%20of%20the%20latent%20features%20with%20respect%20to%20the%20original%20data%2C%20raising%0Aconcerns%20about%20NMF%27s%20reliability.%20Interestingly%2C%20naively%20deriving%20the%0Aadversarial%20loss%20for%20NMF%20as%20in%20the%20case%20of%20ML%20would%20result%20in%20the%0Areconstruction%20loss%2C%20which%20can%20be%20shown%20theoretically%20to%20be%20an%20ineffective%0Aattacking%20objective.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20class%20of%20attacks%20in%20NMF%0Atermed%20Latent%20Feature%20Attacks%20%28LaFA%29%2C%20which%20aim%20to%20manipulate%20the%20latent%0Afeatures%20produced%20by%20the%20NMF%20process.%20Our%20method%20utilizes%20the%20Feature%20Error%0A%28FE%29%20loss%20directly%20on%20the%20latent%20features.%20By%20employing%20FE%20loss%2C%20we%20generate%0Aperturbations%20in%20the%20original%20data%20that%20significantly%20affect%20the%20extracted%0Alatent%20features%2C%20revealing%20vulnerabilities%20akin%20to%20those%20found%20in%20other%20ML%0Atechniques.%20To%20handle%20large%20peak-memory%20overhead%20from%20gradient%20back-propagation%0Ain%20FE%20attacks%2C%20we%20develop%20a%20method%20based%20on%20implicit%20differentiation%20which%0Aenables%20their%20scaling%20to%20larger%20datasets.%20We%20validate%20NMF%20vulnerabilities%20and%0AFE%20attacks%20effectiveness%20through%20extensive%20experiments%20on%20synthetic%20and%0Areal-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03909v1&entry.124074799=Read"},
{"title": "Algorithmic Collective Action in Machine Learning", "author": "Moritz Hardt and Eric Mazumdar and Celestine Mendler-D\u00fcnner and Tijana Zrnic", "abstract": "  We initiate a principled study of algorithmic collective action on digital\nplatforms that deploy machine learning algorithms. We propose a simple\ntheoretical model of a collective interacting with a firm's learning algorithm.\nThe collective pools the data of participating individuals and executes an\nalgorithmic strategy by instructing participants how to modify their own data\nto achieve a collective goal. We investigate the consequences of this model in\nthree fundamental learning-theoretic settings: the case of a nonparametric\noptimal learning algorithm, a parametric risk minimizer, and gradient-based\noptimization. In each setting, we come up with coordinated algorithmic\nstrategies and characterize natural success criteria as a function of the\ncollective's size. Complementing our theory, we conduct systematic experiments\non a skill classification task involving tens of thousands of resumes from a\ngig platform for freelancers. Through more than two thousand model training\nruns of a BERT-like language model, we see a striking correspondence emerge\nbetween our empirical observations and the predictions made by our theory.\nTaken together, our theory and experiments broadly support the conclusion that\nalgorithmic collectives of exceedingly small fractional size can exert\nsignificant control over a platform's learning algorithm.\n", "link": "http://arxiv.org/abs/2302.04262v3", "date": "2024-08-07", "relevancy": 1.8182, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4603}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Algorithmic%20Collective%20Action%20in%20Machine%20Learning&body=Title%3A%20Algorithmic%20Collective%20Action%20in%20Machine%20Learning%0AAuthor%3A%20Moritz%20Hardt%20and%20Eric%20Mazumdar%20and%20Celestine%20Mendler-D%C3%BCnner%20and%20Tijana%20Zrnic%0AAbstract%3A%20%20%20We%20initiate%20a%20principled%20study%20of%20algorithmic%20collective%20action%20on%20digital%0Aplatforms%20that%20deploy%20machine%20learning%20algorithms.%20We%20propose%20a%20simple%0Atheoretical%20model%20of%20a%20collective%20interacting%20with%20a%20firm%27s%20learning%20algorithm.%0AThe%20collective%20pools%20the%20data%20of%20participating%20individuals%20and%20executes%20an%0Aalgorithmic%20strategy%20by%20instructing%20participants%20how%20to%20modify%20their%20own%20data%0Ato%20achieve%20a%20collective%20goal.%20We%20investigate%20the%20consequences%20of%20this%20model%20in%0Athree%20fundamental%20learning-theoretic%20settings%3A%20the%20case%20of%20a%20nonparametric%0Aoptimal%20learning%20algorithm%2C%20a%20parametric%20risk%20minimizer%2C%20and%20gradient-based%0Aoptimization.%20In%20each%20setting%2C%20we%20come%20up%20with%20coordinated%20algorithmic%0Astrategies%20and%20characterize%20natural%20success%20criteria%20as%20a%20function%20of%20the%0Acollective%27s%20size.%20Complementing%20our%20theory%2C%20we%20conduct%20systematic%20experiments%0Aon%20a%20skill%20classification%20task%20involving%20tens%20of%20thousands%20of%20resumes%20from%20a%0Agig%20platform%20for%20freelancers.%20Through%20more%20than%20two%20thousand%20model%20training%0Aruns%20of%20a%20BERT-like%20language%20model%2C%20we%20see%20a%20striking%20correspondence%20emerge%0Abetween%20our%20empirical%20observations%20and%20the%20predictions%20made%20by%20our%20theory.%0ATaken%20together%2C%20our%20theory%20and%20experiments%20broadly%20support%20the%20conclusion%20that%0Aalgorithmic%20collectives%20of%20exceedingly%20small%20fractional%20size%20can%20exert%0Asignificant%20control%20over%20a%20platform%27s%20learning%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.04262v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgorithmic%2520Collective%2520Action%2520in%2520Machine%2520Learning%26entry.906535625%3DMoritz%2520Hardt%2520and%2520Eric%2520Mazumdar%2520and%2520Celestine%2520Mendler-D%25C3%25BCnner%2520and%2520Tijana%2520Zrnic%26entry.1292438233%3D%2520%2520We%2520initiate%2520a%2520principled%2520study%2520of%2520algorithmic%2520collective%2520action%2520on%2520digital%250Aplatforms%2520that%2520deploy%2520machine%2520learning%2520algorithms.%2520We%2520propose%2520a%2520simple%250Atheoretical%2520model%2520of%2520a%2520collective%2520interacting%2520with%2520a%2520firm%2527s%2520learning%2520algorithm.%250AThe%2520collective%2520pools%2520the%2520data%2520of%2520participating%2520individuals%2520and%2520executes%2520an%250Aalgorithmic%2520strategy%2520by%2520instructing%2520participants%2520how%2520to%2520modify%2520their%2520own%2520data%250Ato%2520achieve%2520a%2520collective%2520goal.%2520We%2520investigate%2520the%2520consequences%2520of%2520this%2520model%2520in%250Athree%2520fundamental%2520learning-theoretic%2520settings%253A%2520the%2520case%2520of%2520a%2520nonparametric%250Aoptimal%2520learning%2520algorithm%252C%2520a%2520parametric%2520risk%2520minimizer%252C%2520and%2520gradient-based%250Aoptimization.%2520In%2520each%2520setting%252C%2520we%2520come%2520up%2520with%2520coordinated%2520algorithmic%250Astrategies%2520and%2520characterize%2520natural%2520success%2520criteria%2520as%2520a%2520function%2520of%2520the%250Acollective%2527s%2520size.%2520Complementing%2520our%2520theory%252C%2520we%2520conduct%2520systematic%2520experiments%250Aon%2520a%2520skill%2520classification%2520task%2520involving%2520tens%2520of%2520thousands%2520of%2520resumes%2520from%2520a%250Agig%2520platform%2520for%2520freelancers.%2520Through%2520more%2520than%2520two%2520thousand%2520model%2520training%250Aruns%2520of%2520a%2520BERT-like%2520language%2520model%252C%2520we%2520see%2520a%2520striking%2520correspondence%2520emerge%250Abetween%2520our%2520empirical%2520observations%2520and%2520the%2520predictions%2520made%2520by%2520our%2520theory.%250ATaken%2520together%252C%2520our%2520theory%2520and%2520experiments%2520broadly%2520support%2520the%2520conclusion%2520that%250Aalgorithmic%2520collectives%2520of%2520exceedingly%2520small%2520fractional%2520size%2520can%2520exert%250Asignificant%2520control%2520over%2520a%2520platform%2527s%2520learning%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.04262v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithmic%20Collective%20Action%20in%20Machine%20Learning&entry.906535625=Moritz%20Hardt%20and%20Eric%20Mazumdar%20and%20Celestine%20Mendler-D%C3%BCnner%20and%20Tijana%20Zrnic&entry.1292438233=%20%20We%20initiate%20a%20principled%20study%20of%20algorithmic%20collective%20action%20on%20digital%0Aplatforms%20that%20deploy%20machine%20learning%20algorithms.%20We%20propose%20a%20simple%0Atheoretical%20model%20of%20a%20collective%20interacting%20with%20a%20firm%27s%20learning%20algorithm.%0AThe%20collective%20pools%20the%20data%20of%20participating%20individuals%20and%20executes%20an%0Aalgorithmic%20strategy%20by%20instructing%20participants%20how%20to%20modify%20their%20own%20data%0Ato%20achieve%20a%20collective%20goal.%20We%20investigate%20the%20consequences%20of%20this%20model%20in%0Athree%20fundamental%20learning-theoretic%20settings%3A%20the%20case%20of%20a%20nonparametric%0Aoptimal%20learning%20algorithm%2C%20a%20parametric%20risk%20minimizer%2C%20and%20gradient-based%0Aoptimization.%20In%20each%20setting%2C%20we%20come%20up%20with%20coordinated%20algorithmic%0Astrategies%20and%20characterize%20natural%20success%20criteria%20as%20a%20function%20of%20the%0Acollective%27s%20size.%20Complementing%20our%20theory%2C%20we%20conduct%20systematic%20experiments%0Aon%20a%20skill%20classification%20task%20involving%20tens%20of%20thousands%20of%20resumes%20from%20a%0Agig%20platform%20for%20freelancers.%20Through%20more%20than%20two%20thousand%20model%20training%0Aruns%20of%20a%20BERT-like%20language%20model%2C%20we%20see%20a%20striking%20correspondence%20emerge%0Abetween%20our%20empirical%20observations%20and%20the%20predictions%20made%20by%20our%20theory.%0ATaken%20together%2C%20our%20theory%20and%20experiments%20broadly%20support%20the%20conclusion%20that%0Aalgorithmic%20collectives%20of%20exceedingly%20small%20fractional%20size%20can%20exert%0Asignificant%20control%20over%20a%20platform%27s%20learning%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.04262v3&entry.124074799=Read"},
{"title": "Investigating and Defending Shortcut Learning in Personalized Diffusion\n  Models", "author": "Yixin Liu and Ruoxi Chen and Lichao Sun", "abstract": "  Personalized diffusion models have gained popularity for adapting pre-trained\ntext-to-image models to generate images of specific topics with minimal\ntraining data. However, these models are vulnerable to minor adversarial\nperturbations, leading to degraded performance on corrupted datasets. Such\nvulnerabilities are further exploited to craft protective perturbations on\nsensitive images like portraits that prevent unauthorized generation. In\nresponse, diffusion-based purification methods have been proposed to remove\nthese perturbations and retain generation performance. However, existing works\nturn to over-purifying the images, which causes information loss. In this\npaper, we take a closer look at the fine-tuning process of personalized\ndiffusion models through the lens of shortcut learning. And we propose a\nhypothesis explaining the manipulation mechanisms of existing perturbation\nmethods, demonstrating that perturbed images significantly deviate from their\noriginal prompts in the CLIP-based latent space. This misalignment during\nfine-tuning causes models to associate noisy patterns with identifiers,\nresulting in performance degradation. Based on these insights, we introduce a\nsystematic approach to maintain training performance through purification. Our\nmethod first purifies the images to realign them with their original semantic\nmeanings in latent space. Then, we introduce contrastive learning with negative\ntokens to decouple the learning of clean identities from noisy patterns, which\nshows a strong potential capacity against adaptive perturbation. Our study\nuncovers shortcut learning vulnerabilities in personalized diffusion models and\nprovides a firm evaluation framework for future protective perturbation\nresearch. Code is available at https://github.com/liuyixin-louis/DiffShortcut.\n", "link": "http://arxiv.org/abs/2406.18944v3", "date": "2024-08-07", "relevancy": 1.7986, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6168}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5977}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20and%20Defending%20Shortcut%20Learning%20in%20Personalized%20Diffusion%0A%20%20Models&body=Title%3A%20Investigating%20and%20Defending%20Shortcut%20Learning%20in%20Personalized%20Diffusion%0A%20%20Models%0AAuthor%3A%20Yixin%20Liu%20and%20Ruoxi%20Chen%20and%20Lichao%20Sun%0AAbstract%3A%20%20%20Personalized%20diffusion%20models%20have%20gained%20popularity%20for%20adapting%20pre-trained%0Atext-to-image%20models%20to%20generate%20images%20of%20specific%20topics%20with%20minimal%0Atraining%20data.%20However%2C%20these%20models%20are%20vulnerable%20to%20minor%20adversarial%0Aperturbations%2C%20leading%20to%20degraded%20performance%20on%20corrupted%20datasets.%20Such%0Avulnerabilities%20are%20further%20exploited%20to%20craft%20protective%20perturbations%20on%0Asensitive%20images%20like%20portraits%20that%20prevent%20unauthorized%20generation.%20In%0Aresponse%2C%20diffusion-based%20purification%20methods%20have%20been%20proposed%20to%20remove%0Athese%20perturbations%20and%20retain%20generation%20performance.%20However%2C%20existing%20works%0Aturn%20to%20over-purifying%20the%20images%2C%20which%20causes%20information%20loss.%20In%20this%0Apaper%2C%20we%20take%20a%20closer%20look%20at%20the%20fine-tuning%20process%20of%20personalized%0Adiffusion%20models%20through%20the%20lens%20of%20shortcut%20learning.%20And%20we%20propose%20a%0Ahypothesis%20explaining%20the%20manipulation%20mechanisms%20of%20existing%20perturbation%0Amethods%2C%20demonstrating%20that%20perturbed%20images%20significantly%20deviate%20from%20their%0Aoriginal%20prompts%20in%20the%20CLIP-based%20latent%20space.%20This%20misalignment%20during%0Afine-tuning%20causes%20models%20to%20associate%20noisy%20patterns%20with%20identifiers%2C%0Aresulting%20in%20performance%20degradation.%20Based%20on%20these%20insights%2C%20we%20introduce%20a%0Asystematic%20approach%20to%20maintain%20training%20performance%20through%20purification.%20Our%0Amethod%20first%20purifies%20the%20images%20to%20realign%20them%20with%20their%20original%20semantic%0Ameanings%20in%20latent%20space.%20Then%2C%20we%20introduce%20contrastive%20learning%20with%20negative%0Atokens%20to%20decouple%20the%20learning%20of%20clean%20identities%20from%20noisy%20patterns%2C%20which%0Ashows%20a%20strong%20potential%20capacity%20against%20adaptive%20perturbation.%20Our%20study%0Auncovers%20shortcut%20learning%20vulnerabilities%20in%20personalized%20diffusion%20models%20and%0Aprovides%20a%20firm%20evaluation%20framework%20for%20future%20protective%20perturbation%0Aresearch.%20Code%20is%20available%20at%20https%3A//github.com/liuyixin-louis/DiffShortcut.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18944v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520and%2520Defending%2520Shortcut%2520Learning%2520in%2520Personalized%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DYixin%2520Liu%2520and%2520Ruoxi%2520Chen%2520and%2520Lichao%2520Sun%26entry.1292438233%3D%2520%2520Personalized%2520diffusion%2520models%2520have%2520gained%2520popularity%2520for%2520adapting%2520pre-trained%250Atext-to-image%2520models%2520to%2520generate%2520images%2520of%2520specific%2520topics%2520with%2520minimal%250Atraining%2520data.%2520However%252C%2520these%2520models%2520are%2520vulnerable%2520to%2520minor%2520adversarial%250Aperturbations%252C%2520leading%2520to%2520degraded%2520performance%2520on%2520corrupted%2520datasets.%2520Such%250Avulnerabilities%2520are%2520further%2520exploited%2520to%2520craft%2520protective%2520perturbations%2520on%250Asensitive%2520images%2520like%2520portraits%2520that%2520prevent%2520unauthorized%2520generation.%2520In%250Aresponse%252C%2520diffusion-based%2520purification%2520methods%2520have%2520been%2520proposed%2520to%2520remove%250Athese%2520perturbations%2520and%2520retain%2520generation%2520performance.%2520However%252C%2520existing%2520works%250Aturn%2520to%2520over-purifying%2520the%2520images%252C%2520which%2520causes%2520information%2520loss.%2520In%2520this%250Apaper%252C%2520we%2520take%2520a%2520closer%2520look%2520at%2520the%2520fine-tuning%2520process%2520of%2520personalized%250Adiffusion%2520models%2520through%2520the%2520lens%2520of%2520shortcut%2520learning.%2520And%2520we%2520propose%2520a%250Ahypothesis%2520explaining%2520the%2520manipulation%2520mechanisms%2520of%2520existing%2520perturbation%250Amethods%252C%2520demonstrating%2520that%2520perturbed%2520images%2520significantly%2520deviate%2520from%2520their%250Aoriginal%2520prompts%2520in%2520the%2520CLIP-based%2520latent%2520space.%2520This%2520misalignment%2520during%250Afine-tuning%2520causes%2520models%2520to%2520associate%2520noisy%2520patterns%2520with%2520identifiers%252C%250Aresulting%2520in%2520performance%2520degradation.%2520Based%2520on%2520these%2520insights%252C%2520we%2520introduce%2520a%250Asystematic%2520approach%2520to%2520maintain%2520training%2520performance%2520through%2520purification.%2520Our%250Amethod%2520first%2520purifies%2520the%2520images%2520to%2520realign%2520them%2520with%2520their%2520original%2520semantic%250Ameanings%2520in%2520latent%2520space.%2520Then%252C%2520we%2520introduce%2520contrastive%2520learning%2520with%2520negative%250Atokens%2520to%2520decouple%2520the%2520learning%2520of%2520clean%2520identities%2520from%2520noisy%2520patterns%252C%2520which%250Ashows%2520a%2520strong%2520potential%2520capacity%2520against%2520adaptive%2520perturbation.%2520Our%2520study%250Auncovers%2520shortcut%2520learning%2520vulnerabilities%2520in%2520personalized%2520diffusion%2520models%2520and%250Aprovides%2520a%2520firm%2520evaluation%2520framework%2520for%2520future%2520protective%2520perturbation%250Aresearch.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/liuyixin-louis/DiffShortcut.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18944v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20and%20Defending%20Shortcut%20Learning%20in%20Personalized%20Diffusion%0A%20%20Models&entry.906535625=Yixin%20Liu%20and%20Ruoxi%20Chen%20and%20Lichao%20Sun&entry.1292438233=%20%20Personalized%20diffusion%20models%20have%20gained%20popularity%20for%20adapting%20pre-trained%0Atext-to-image%20models%20to%20generate%20images%20of%20specific%20topics%20with%20minimal%0Atraining%20data.%20However%2C%20these%20models%20are%20vulnerable%20to%20minor%20adversarial%0Aperturbations%2C%20leading%20to%20degraded%20performance%20on%20corrupted%20datasets.%20Such%0Avulnerabilities%20are%20further%20exploited%20to%20craft%20protective%20perturbations%20on%0Asensitive%20images%20like%20portraits%20that%20prevent%20unauthorized%20generation.%20In%0Aresponse%2C%20diffusion-based%20purification%20methods%20have%20been%20proposed%20to%20remove%0Athese%20perturbations%20and%20retain%20generation%20performance.%20However%2C%20existing%20works%0Aturn%20to%20over-purifying%20the%20images%2C%20which%20causes%20information%20loss.%20In%20this%0Apaper%2C%20we%20take%20a%20closer%20look%20at%20the%20fine-tuning%20process%20of%20personalized%0Adiffusion%20models%20through%20the%20lens%20of%20shortcut%20learning.%20And%20we%20propose%20a%0Ahypothesis%20explaining%20the%20manipulation%20mechanisms%20of%20existing%20perturbation%0Amethods%2C%20demonstrating%20that%20perturbed%20images%20significantly%20deviate%20from%20their%0Aoriginal%20prompts%20in%20the%20CLIP-based%20latent%20space.%20This%20misalignment%20during%0Afine-tuning%20causes%20models%20to%20associate%20noisy%20patterns%20with%20identifiers%2C%0Aresulting%20in%20performance%20degradation.%20Based%20on%20these%20insights%2C%20we%20introduce%20a%0Asystematic%20approach%20to%20maintain%20training%20performance%20through%20purification.%20Our%0Amethod%20first%20purifies%20the%20images%20to%20realign%20them%20with%20their%20original%20semantic%0Ameanings%20in%20latent%20space.%20Then%2C%20we%20introduce%20contrastive%20learning%20with%20negative%0Atokens%20to%20decouple%20the%20learning%20of%20clean%20identities%20from%20noisy%20patterns%2C%20which%0Ashows%20a%20strong%20potential%20capacity%20against%20adaptive%20perturbation.%20Our%20study%0Auncovers%20shortcut%20learning%20vulnerabilities%20in%20personalized%20diffusion%20models%20and%0Aprovides%20a%20firm%20evaluation%20framework%20for%20future%20protective%20perturbation%0Aresearch.%20Code%20is%20available%20at%20https%3A//github.com/liuyixin-louis/DiffShortcut.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18944v3&entry.124074799=Read"},
{"title": "Out-of-Distribution-Aware Electric Vehicle Charging", "author": "Tongxin Li and Chenxi Sun", "abstract": "  We tackle the challenge of learning to charge Electric Vehicles (EVs) with\nOut-of-Distribution (OOD) data. Traditional scheduling algorithms typically\nfail to balance near-optimal average performance with worst-case guarantees,\nparticularly with OOD data. Model Predictive Control (MPC) is often too\nconservative and data-independent, whereas Reinforcement Learning (RL) tends to\nbe overly aggressive and fully trusts the data, hindering their ability to\nconsistently achieve the best-of-both-worlds. To bridge this gap, we introduce\na novel OOD-aware scheduling algorithm, denoted OOD-Charging. This algorithm\nemploys a dynamic \"awareness radius\", which updates in real-time based on the\nTemporal Difference (TD)-error that reflects the severity of OOD. The\nOOD-Charging algorithm allows for a more effective balance between consistency\nand robustness in EV charging schedules, thereby significantly enhancing\nadaptability and efficiency in real-world charging environments. Our results\ndemonstrate that this approach improves the scheduling reward reliably under\nreal OOD scenarios with remarkable shifts of EV charging behaviors caused by\nCOVID-19 in the Caltech ACN-Data.\n", "link": "http://arxiv.org/abs/2311.05941v3", "date": "2024-08-07", "relevancy": 1.7929, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.471}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.432}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out-of-Distribution-Aware%20Electric%20Vehicle%20Charging&body=Title%3A%20Out-of-Distribution-Aware%20Electric%20Vehicle%20Charging%0AAuthor%3A%20Tongxin%20Li%20and%20Chenxi%20Sun%0AAbstract%3A%20%20%20We%20tackle%20the%20challenge%20of%20learning%20to%20charge%20Electric%20Vehicles%20%28EVs%29%20with%0AOut-of-Distribution%20%28OOD%29%20data.%20Traditional%20scheduling%20algorithms%20typically%0Afail%20to%20balance%20near-optimal%20average%20performance%20with%20worst-case%20guarantees%2C%0Aparticularly%20with%20OOD%20data.%20Model%20Predictive%20Control%20%28MPC%29%20is%20often%20too%0Aconservative%20and%20data-independent%2C%20whereas%20Reinforcement%20Learning%20%28RL%29%20tends%20to%0Abe%20overly%20aggressive%20and%20fully%20trusts%20the%20data%2C%20hindering%20their%20ability%20to%0Aconsistently%20achieve%20the%20best-of-both-worlds.%20To%20bridge%20this%20gap%2C%20we%20introduce%0Aa%20novel%20OOD-aware%20scheduling%20algorithm%2C%20denoted%20OOD-Charging.%20This%20algorithm%0Aemploys%20a%20dynamic%20%22awareness%20radius%22%2C%20which%20updates%20in%20real-time%20based%20on%20the%0ATemporal%20Difference%20%28TD%29-error%20that%20reflects%20the%20severity%20of%20OOD.%20The%0AOOD-Charging%20algorithm%20allows%20for%20a%20more%20effective%20balance%20between%20consistency%0Aand%20robustness%20in%20EV%20charging%20schedules%2C%20thereby%20significantly%20enhancing%0Aadaptability%20and%20efficiency%20in%20real-world%20charging%20environments.%20Our%20results%0Ademonstrate%20that%20this%20approach%20improves%20the%20scheduling%20reward%20reliably%20under%0Areal%20OOD%20scenarios%20with%20remarkable%20shifts%20of%20EV%20charging%20behaviors%20caused%20by%0ACOVID-19%20in%20the%20Caltech%20ACN-Data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05941v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut-of-Distribution-Aware%2520Electric%2520Vehicle%2520Charging%26entry.906535625%3DTongxin%2520Li%2520and%2520Chenxi%2520Sun%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520challenge%2520of%2520learning%2520to%2520charge%2520Electric%2520Vehicles%2520%2528EVs%2529%2520with%250AOut-of-Distribution%2520%2528OOD%2529%2520data.%2520Traditional%2520scheduling%2520algorithms%2520typically%250Afail%2520to%2520balance%2520near-optimal%2520average%2520performance%2520with%2520worst-case%2520guarantees%252C%250Aparticularly%2520with%2520OOD%2520data.%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520is%2520often%2520too%250Aconservative%2520and%2520data-independent%252C%2520whereas%2520Reinforcement%2520Learning%2520%2528RL%2529%2520tends%2520to%250Abe%2520overly%2520aggressive%2520and%2520fully%2520trusts%2520the%2520data%252C%2520hindering%2520their%2520ability%2520to%250Aconsistently%2520achieve%2520the%2520best-of-both-worlds.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%250Aa%2520novel%2520OOD-aware%2520scheduling%2520algorithm%252C%2520denoted%2520OOD-Charging.%2520This%2520algorithm%250Aemploys%2520a%2520dynamic%2520%2522awareness%2520radius%2522%252C%2520which%2520updates%2520in%2520real-time%2520based%2520on%2520the%250ATemporal%2520Difference%2520%2528TD%2529-error%2520that%2520reflects%2520the%2520severity%2520of%2520OOD.%2520The%250AOOD-Charging%2520algorithm%2520allows%2520for%2520a%2520more%2520effective%2520balance%2520between%2520consistency%250Aand%2520robustness%2520in%2520EV%2520charging%2520schedules%252C%2520thereby%2520significantly%2520enhancing%250Aadaptability%2520and%2520efficiency%2520in%2520real-world%2520charging%2520environments.%2520Our%2520results%250Ademonstrate%2520that%2520this%2520approach%2520improves%2520the%2520scheduling%2520reward%2520reliably%2520under%250Areal%2520OOD%2520scenarios%2520with%2520remarkable%2520shifts%2520of%2520EV%2520charging%2520behaviors%2520caused%2520by%250ACOVID-19%2520in%2520the%2520Caltech%2520ACN-Data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05941v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-Distribution-Aware%20Electric%20Vehicle%20Charging&entry.906535625=Tongxin%20Li%20and%20Chenxi%20Sun&entry.1292438233=%20%20We%20tackle%20the%20challenge%20of%20learning%20to%20charge%20Electric%20Vehicles%20%28EVs%29%20with%0AOut-of-Distribution%20%28OOD%29%20data.%20Traditional%20scheduling%20algorithms%20typically%0Afail%20to%20balance%20near-optimal%20average%20performance%20with%20worst-case%20guarantees%2C%0Aparticularly%20with%20OOD%20data.%20Model%20Predictive%20Control%20%28MPC%29%20is%20often%20too%0Aconservative%20and%20data-independent%2C%20whereas%20Reinforcement%20Learning%20%28RL%29%20tends%20to%0Abe%20overly%20aggressive%20and%20fully%20trusts%20the%20data%2C%20hindering%20their%20ability%20to%0Aconsistently%20achieve%20the%20best-of-both-worlds.%20To%20bridge%20this%20gap%2C%20we%20introduce%0Aa%20novel%20OOD-aware%20scheduling%20algorithm%2C%20denoted%20OOD-Charging.%20This%20algorithm%0Aemploys%20a%20dynamic%20%22awareness%20radius%22%2C%20which%20updates%20in%20real-time%20based%20on%20the%0ATemporal%20Difference%20%28TD%29-error%20that%20reflects%20the%20severity%20of%20OOD.%20The%0AOOD-Charging%20algorithm%20allows%20for%20a%20more%20effective%20balance%20between%20consistency%0Aand%20robustness%20in%20EV%20charging%20schedules%2C%20thereby%20significantly%20enhancing%0Aadaptability%20and%20efficiency%20in%20real-world%20charging%20environments.%20Our%20results%0Ademonstrate%20that%20this%20approach%20improves%20the%20scheduling%20reward%20reliably%20under%0Areal%20OOD%20scenarios%20with%20remarkable%20shifts%20of%20EV%20charging%20behaviors%20caused%20by%0ACOVID-19%20in%20the%20Caltech%20ACN-Data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05941v3&entry.124074799=Read"},
{"title": "TGS: Trajectory Generation and Selection using Vision Language Models in\n  Mapless Outdoor Environments", "author": "Daeun Song and Jing Liang and Xuesu Xiao and Dinesh Manocha", "abstract": "  We present a multi-modal trajectory generation and selection algorithm for\nreal-world mapless outdoor navigation in challenging scenarios with\nunstructured off-road features like buildings, grass, and curbs. Our goal is to\ncompute suitable trajectories that (1) satisfy the environment-specific\ntraversability constraints and (2) generate human-like paths while navigating\nin crosswalks, sidewalks, etc. Our formulation uses a Conditional Variational\nAutoencoder (CVAE) generative model enhanced with traversability constraints to\ngenerate multiple candidate trajectories for global navigation. We use VLMs and\na visual prompting approach with their zero-shot ability of semantic\nunderstanding and logical reasoning to choose the best trajectory given the\ncontextual information about the task. We evaluate our methods in various\noutdoor scenes with wheeled robots and compare the performance with other\nglobal navigation algorithms. In practice, we observe at least 3.35%\nimprovement in traversability and 20.61% improvement in terms of human-like\nnavigation in generated trajectories in challenging outdoor navigation\nscenarios.\n", "link": "http://arxiv.org/abs/2408.02454v2", "date": "2024-08-07", "relevancy": 1.7896, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.613}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6008}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TGS%3A%20Trajectory%20Generation%20and%20Selection%20using%20Vision%20Language%20Models%20in%0A%20%20Mapless%20Outdoor%20Environments&body=Title%3A%20TGS%3A%20Trajectory%20Generation%20and%20Selection%20using%20Vision%20Language%20Models%20in%0A%20%20Mapless%20Outdoor%20Environments%0AAuthor%3A%20Daeun%20Song%20and%20Jing%20Liang%20and%20Xuesu%20Xiao%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20a%20multi-modal%20trajectory%20generation%20and%20selection%20algorithm%20for%0Areal-world%20mapless%20outdoor%20navigation%20in%20challenging%20scenarios%20with%0Aunstructured%20off-road%20features%20like%20buildings%2C%20grass%2C%20and%20curbs.%20Our%20goal%20is%20to%0Acompute%20suitable%20trajectories%20that%20%281%29%20satisfy%20the%20environment-specific%0Atraversability%20constraints%20and%20%282%29%20generate%20human-like%20paths%20while%20navigating%0Ain%20crosswalks%2C%20sidewalks%2C%20etc.%20Our%20formulation%20uses%20a%20Conditional%20Variational%0AAutoencoder%20%28CVAE%29%20generative%20model%20enhanced%20with%20traversability%20constraints%20to%0Agenerate%20multiple%20candidate%20trajectories%20for%20global%20navigation.%20We%20use%20VLMs%20and%0Aa%20visual%20prompting%20approach%20with%20their%20zero-shot%20ability%20of%20semantic%0Aunderstanding%20and%20logical%20reasoning%20to%20choose%20the%20best%20trajectory%20given%20the%0Acontextual%20information%20about%20the%20task.%20We%20evaluate%20our%20methods%20in%20various%0Aoutdoor%20scenes%20with%20wheeled%20robots%20and%20compare%20the%20performance%20with%20other%0Aglobal%20navigation%20algorithms.%20In%20practice%2C%20we%20observe%20at%20least%203.35%25%0Aimprovement%20in%20traversability%20and%2020.61%25%20improvement%20in%20terms%20of%20human-like%0Anavigation%20in%20generated%20trajectories%20in%20challenging%20outdoor%20navigation%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02454v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTGS%253A%2520Trajectory%2520Generation%2520and%2520Selection%2520using%2520Vision%2520Language%2520Models%2520in%250A%2520%2520Mapless%2520Outdoor%2520Environments%26entry.906535625%3DDaeun%2520Song%2520and%2520Jing%2520Liang%2520and%2520Xuesu%2520Xiao%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520multi-modal%2520trajectory%2520generation%2520and%2520selection%2520algorithm%2520for%250Areal-world%2520mapless%2520outdoor%2520navigation%2520in%2520challenging%2520scenarios%2520with%250Aunstructured%2520off-road%2520features%2520like%2520buildings%252C%2520grass%252C%2520and%2520curbs.%2520Our%2520goal%2520is%2520to%250Acompute%2520suitable%2520trajectories%2520that%2520%25281%2529%2520satisfy%2520the%2520environment-specific%250Atraversability%2520constraints%2520and%2520%25282%2529%2520generate%2520human-like%2520paths%2520while%2520navigating%250Ain%2520crosswalks%252C%2520sidewalks%252C%2520etc.%2520Our%2520formulation%2520uses%2520a%2520Conditional%2520Variational%250AAutoencoder%2520%2528CVAE%2529%2520generative%2520model%2520enhanced%2520with%2520traversability%2520constraints%2520to%250Agenerate%2520multiple%2520candidate%2520trajectories%2520for%2520global%2520navigation.%2520We%2520use%2520VLMs%2520and%250Aa%2520visual%2520prompting%2520approach%2520with%2520their%2520zero-shot%2520ability%2520of%2520semantic%250Aunderstanding%2520and%2520logical%2520reasoning%2520to%2520choose%2520the%2520best%2520trajectory%2520given%2520the%250Acontextual%2520information%2520about%2520the%2520task.%2520We%2520evaluate%2520our%2520methods%2520in%2520various%250Aoutdoor%2520scenes%2520with%2520wheeled%2520robots%2520and%2520compare%2520the%2520performance%2520with%2520other%250Aglobal%2520navigation%2520algorithms.%2520In%2520practice%252C%2520we%2520observe%2520at%2520least%25203.35%2525%250Aimprovement%2520in%2520traversability%2520and%252020.61%2525%2520improvement%2520in%2520terms%2520of%2520human-like%250Anavigation%2520in%2520generated%2520trajectories%2520in%2520challenging%2520outdoor%2520navigation%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02454v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TGS%3A%20Trajectory%20Generation%20and%20Selection%20using%20Vision%20Language%20Models%20in%0A%20%20Mapless%20Outdoor%20Environments&entry.906535625=Daeun%20Song%20and%20Jing%20Liang%20and%20Xuesu%20Xiao%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20a%20multi-modal%20trajectory%20generation%20and%20selection%20algorithm%20for%0Areal-world%20mapless%20outdoor%20navigation%20in%20challenging%20scenarios%20with%0Aunstructured%20off-road%20features%20like%20buildings%2C%20grass%2C%20and%20curbs.%20Our%20goal%20is%20to%0Acompute%20suitable%20trajectories%20that%20%281%29%20satisfy%20the%20environment-specific%0Atraversability%20constraints%20and%20%282%29%20generate%20human-like%20paths%20while%20navigating%0Ain%20crosswalks%2C%20sidewalks%2C%20etc.%20Our%20formulation%20uses%20a%20Conditional%20Variational%0AAutoencoder%20%28CVAE%29%20generative%20model%20enhanced%20with%20traversability%20constraints%20to%0Agenerate%20multiple%20candidate%20trajectories%20for%20global%20navigation.%20We%20use%20VLMs%20and%0Aa%20visual%20prompting%20approach%20with%20their%20zero-shot%20ability%20of%20semantic%0Aunderstanding%20and%20logical%20reasoning%20to%20choose%20the%20best%20trajectory%20given%20the%0Acontextual%20information%20about%20the%20task.%20We%20evaluate%20our%20methods%20in%20various%0Aoutdoor%20scenes%20with%20wheeled%20robots%20and%20compare%20the%20performance%20with%20other%0Aglobal%20navigation%20algorithms.%20In%20practice%2C%20we%20observe%20at%20least%203.35%25%0Aimprovement%20in%20traversability%20and%2020.61%25%20improvement%20in%20terms%20of%20human-like%0Anavigation%20in%20generated%20trajectories%20in%20challenging%20outdoor%20navigation%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02454v2&entry.124074799=Read"},
{"title": "PraFFL: A Preference-Aware Scheme in Fair Federated Learning", "author": "Rongguang Ye and Wei-Bin Kou and Ming Tang", "abstract": "  Fairness in federated learning has emerged as a critical concern, aiming to\ndevelop an unbiased model for any special group (e.g., male or female) of\nsensitive features. However, there is a trade-off between model performance and\nfairness, i.e., improving model fairness will decrease model performance.\nExisting approaches have characterized such a trade-off by introducing\nhyperparameters to quantify client's preferences for model fairness and model\nperformance. Nevertheless, these approaches are limited to scenarios where each\nclient has only a single pre-defined preference, and fail to work in practical\nsystems where each client generally have multiple preferences. The key\nchallenge is to design a method that allows the model to adapt to diverse\npreferences of each client in real time. To this end, we propose a\nPreference-aware scheme in Fair Federated Learning paradigm (called PraFFL) to\ngenerate preference-wise model in real time. PraFFL can adaptively adjust the\nmodel based on each client's preferences to meet their needs. We theoretically\nprove that PraFFL can offer the optimal model tailored to an arbitrary\npreference of each client, and show its linear convergence. Experimental\nresults show that our proposed PraFFL outperforms five fair federated learning\nalgorithms in terms of the model's capability of adapting to clients' different\npreferences.\n", "link": "http://arxiv.org/abs/2404.08973v2", "date": "2024-08-07", "relevancy": 1.7822, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4568}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4431}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PraFFL%3A%20A%20Preference-Aware%20Scheme%20in%20Fair%20Federated%20Learning&body=Title%3A%20PraFFL%3A%20A%20Preference-Aware%20Scheme%20in%20Fair%20Federated%20Learning%0AAuthor%3A%20Rongguang%20Ye%20and%20Wei-Bin%20Kou%20and%20Ming%20Tang%0AAbstract%3A%20%20%20Fairness%20in%20federated%20learning%20has%20emerged%20as%20a%20critical%20concern%2C%20aiming%20to%0Adevelop%20an%20unbiased%20model%20for%20any%20special%20group%20%28e.g.%2C%20male%20or%20female%29%20of%0Asensitive%20features.%20However%2C%20there%20is%20a%20trade-off%20between%20model%20performance%20and%0Afairness%2C%20i.e.%2C%20improving%20model%20fairness%20will%20decrease%20model%20performance.%0AExisting%20approaches%20have%20characterized%20such%20a%20trade-off%20by%20introducing%0Ahyperparameters%20to%20quantify%20client%27s%20preferences%20for%20model%20fairness%20and%20model%0Aperformance.%20Nevertheless%2C%20these%20approaches%20are%20limited%20to%20scenarios%20where%20each%0Aclient%20has%20only%20a%20single%20pre-defined%20preference%2C%20and%20fail%20to%20work%20in%20practical%0Asystems%20where%20each%20client%20generally%20have%20multiple%20preferences.%20The%20key%0Achallenge%20is%20to%20design%20a%20method%20that%20allows%20the%20model%20to%20adapt%20to%20diverse%0Apreferences%20of%20each%20client%20in%20real%20time.%20To%20this%20end%2C%20we%20propose%20a%0APreference-aware%20scheme%20in%20Fair%20Federated%20Learning%20paradigm%20%28called%20PraFFL%29%20to%0Agenerate%20preference-wise%20model%20in%20real%20time.%20PraFFL%20can%20adaptively%20adjust%20the%0Amodel%20based%20on%20each%20client%27s%20preferences%20to%20meet%20their%20needs.%20We%20theoretically%0Aprove%20that%20PraFFL%20can%20offer%20the%20optimal%20model%20tailored%20to%20an%20arbitrary%0Apreference%20of%20each%20client%2C%20and%20show%20its%20linear%20convergence.%20Experimental%0Aresults%20show%20that%20our%20proposed%20PraFFL%20outperforms%20five%20fair%20federated%20learning%0Aalgorithms%20in%20terms%20of%20the%20model%27s%20capability%20of%20adapting%20to%20clients%27%20different%0Apreferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08973v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPraFFL%253A%2520A%2520Preference-Aware%2520Scheme%2520in%2520Fair%2520Federated%2520Learning%26entry.906535625%3DRongguang%2520Ye%2520and%2520Wei-Bin%2520Kou%2520and%2520Ming%2520Tang%26entry.1292438233%3D%2520%2520Fairness%2520in%2520federated%2520learning%2520has%2520emerged%2520as%2520a%2520critical%2520concern%252C%2520aiming%2520to%250Adevelop%2520an%2520unbiased%2520model%2520for%2520any%2520special%2520group%2520%2528e.g.%252C%2520male%2520or%2520female%2529%2520of%250Asensitive%2520features.%2520However%252C%2520there%2520is%2520a%2520trade-off%2520between%2520model%2520performance%2520and%250Afairness%252C%2520i.e.%252C%2520improving%2520model%2520fairness%2520will%2520decrease%2520model%2520performance.%250AExisting%2520approaches%2520have%2520characterized%2520such%2520a%2520trade-off%2520by%2520introducing%250Ahyperparameters%2520to%2520quantify%2520client%2527s%2520preferences%2520for%2520model%2520fairness%2520and%2520model%250Aperformance.%2520Nevertheless%252C%2520these%2520approaches%2520are%2520limited%2520to%2520scenarios%2520where%2520each%250Aclient%2520has%2520only%2520a%2520single%2520pre-defined%2520preference%252C%2520and%2520fail%2520to%2520work%2520in%2520practical%250Asystems%2520where%2520each%2520client%2520generally%2520have%2520multiple%2520preferences.%2520The%2520key%250Achallenge%2520is%2520to%2520design%2520a%2520method%2520that%2520allows%2520the%2520model%2520to%2520adapt%2520to%2520diverse%250Apreferences%2520of%2520each%2520client%2520in%2520real%2520time.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250APreference-aware%2520scheme%2520in%2520Fair%2520Federated%2520Learning%2520paradigm%2520%2528called%2520PraFFL%2529%2520to%250Agenerate%2520preference-wise%2520model%2520in%2520real%2520time.%2520PraFFL%2520can%2520adaptively%2520adjust%2520the%250Amodel%2520based%2520on%2520each%2520client%2527s%2520preferences%2520to%2520meet%2520their%2520needs.%2520We%2520theoretically%250Aprove%2520that%2520PraFFL%2520can%2520offer%2520the%2520optimal%2520model%2520tailored%2520to%2520an%2520arbitrary%250Apreference%2520of%2520each%2520client%252C%2520and%2520show%2520its%2520linear%2520convergence.%2520Experimental%250Aresults%2520show%2520that%2520our%2520proposed%2520PraFFL%2520outperforms%2520five%2520fair%2520federated%2520learning%250Aalgorithms%2520in%2520terms%2520of%2520the%2520model%2527s%2520capability%2520of%2520adapting%2520to%2520clients%2527%2520different%250Apreferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08973v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PraFFL%3A%20A%20Preference-Aware%20Scheme%20in%20Fair%20Federated%20Learning&entry.906535625=Rongguang%20Ye%20and%20Wei-Bin%20Kou%20and%20Ming%20Tang&entry.1292438233=%20%20Fairness%20in%20federated%20learning%20has%20emerged%20as%20a%20critical%20concern%2C%20aiming%20to%0Adevelop%20an%20unbiased%20model%20for%20any%20special%20group%20%28e.g.%2C%20male%20or%20female%29%20of%0Asensitive%20features.%20However%2C%20there%20is%20a%20trade-off%20between%20model%20performance%20and%0Afairness%2C%20i.e.%2C%20improving%20model%20fairness%20will%20decrease%20model%20performance.%0AExisting%20approaches%20have%20characterized%20such%20a%20trade-off%20by%20introducing%0Ahyperparameters%20to%20quantify%20client%27s%20preferences%20for%20model%20fairness%20and%20model%0Aperformance.%20Nevertheless%2C%20these%20approaches%20are%20limited%20to%20scenarios%20where%20each%0Aclient%20has%20only%20a%20single%20pre-defined%20preference%2C%20and%20fail%20to%20work%20in%20practical%0Asystems%20where%20each%20client%20generally%20have%20multiple%20preferences.%20The%20key%0Achallenge%20is%20to%20design%20a%20method%20that%20allows%20the%20model%20to%20adapt%20to%20diverse%0Apreferences%20of%20each%20client%20in%20real%20time.%20To%20this%20end%2C%20we%20propose%20a%0APreference-aware%20scheme%20in%20Fair%20Federated%20Learning%20paradigm%20%28called%20PraFFL%29%20to%0Agenerate%20preference-wise%20model%20in%20real%20time.%20PraFFL%20can%20adaptively%20adjust%20the%0Amodel%20based%20on%20each%20client%27s%20preferences%20to%20meet%20their%20needs.%20We%20theoretically%0Aprove%20that%20PraFFL%20can%20offer%20the%20optimal%20model%20tailored%20to%20an%20arbitrary%0Apreference%20of%20each%20client%2C%20and%20show%20its%20linear%20convergence.%20Experimental%0Aresults%20show%20that%20our%20proposed%20PraFFL%20outperforms%20five%20fair%20federated%20learning%0Aalgorithms%20in%20terms%20of%20the%20model%27s%20capability%20of%20adapting%20to%20clients%27%20different%0Apreferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08973v2&entry.124074799=Read"},
{"title": "Data Generation Scheme for Thermal Modality with Edge-Guided Adversarial\n  Conditional Diffusion Model", "author": "Guoqing Zhu and Honghu Pan and Qiang Wang and Chao Tian and Chao Yang and Zhenyu He", "abstract": "  In challenging low light and adverse weather conditions,thermal vision\nalgorithms,especially object detection,have exhibited remarkable\npotential,contrasting with the frequent struggles encountered by visible vision\nalgorithms. Nevertheless,the efficacy of thermal vision algorithms driven by\ndeep learning models remains constrained by the paucity of available training\ndata samples. To this end,this paper introduces a novel approach termed the\nedge guided conditional diffusion model. This framework aims to produce\nmeticulously aligned pseudo thermal images at the pixel level,leveraging edge\ninformation extracted from visible images. By utilizing edges as contextual\ncues from the visible domain,the diffusion model achieves meticulous control\nover the delineation of objects within the generated images. To alleviate the\nimpacts of those visible-specific edge information that should not appear in\nthe thermal domain,a two-stage modality adversarial training strategy is\nproposed to filter them out from the generated images by differentiating the\nvisible and thermal modality. Extensive experiments on LLVIP demonstrate ECDM s\nsuperiority over existing state-of-the-art approaches in terms of image\ngeneration quality.\n", "link": "http://arxiv.org/abs/2408.03748v1", "date": "2024-08-07", "relevancy": 1.7604, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6287}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5927}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Generation%20Scheme%20for%20Thermal%20Modality%20with%20Edge-Guided%20Adversarial%0A%20%20Conditional%20Diffusion%20Model&body=Title%3A%20Data%20Generation%20Scheme%20for%20Thermal%20Modality%20with%20Edge-Guided%20Adversarial%0A%20%20Conditional%20Diffusion%20Model%0AAuthor%3A%20Guoqing%20Zhu%20and%20Honghu%20Pan%20and%20Qiang%20Wang%20and%20Chao%20Tian%20and%20Chao%20Yang%20and%20Zhenyu%20He%0AAbstract%3A%20%20%20In%20challenging%20low%20light%20and%20adverse%20weather%20conditions%2Cthermal%20vision%0Aalgorithms%2Cespecially%20object%20detection%2Chave%20exhibited%20remarkable%0Apotential%2Ccontrasting%20with%20the%20frequent%20struggles%20encountered%20by%20visible%20vision%0Aalgorithms.%20Nevertheless%2Cthe%20efficacy%20of%20thermal%20vision%20algorithms%20driven%20by%0Adeep%20learning%20models%20remains%20constrained%20by%20the%20paucity%20of%20available%20training%0Adata%20samples.%20To%20this%20end%2Cthis%20paper%20introduces%20a%20novel%20approach%20termed%20the%0Aedge%20guided%20conditional%20diffusion%20model.%20This%20framework%20aims%20to%20produce%0Ameticulously%20aligned%20pseudo%20thermal%20images%20at%20the%20pixel%20level%2Cleveraging%20edge%0Ainformation%20extracted%20from%20visible%20images.%20By%20utilizing%20edges%20as%20contextual%0Acues%20from%20the%20visible%20domain%2Cthe%20diffusion%20model%20achieves%20meticulous%20control%0Aover%20the%20delineation%20of%20objects%20within%20the%20generated%20images.%20To%20alleviate%20the%0Aimpacts%20of%20those%20visible-specific%20edge%20information%20that%20should%20not%20appear%20in%0Athe%20thermal%20domain%2Ca%20two-stage%20modality%20adversarial%20training%20strategy%20is%0Aproposed%20to%20filter%20them%20out%20from%20the%20generated%20images%20by%20differentiating%20the%0Avisible%20and%20thermal%20modality.%20Extensive%20experiments%20on%20LLVIP%20demonstrate%20ECDM%20s%0Asuperiority%20over%20existing%20state-of-the-art%20approaches%20in%20terms%20of%20image%0Ageneration%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Generation%2520Scheme%2520for%2520Thermal%2520Modality%2520with%2520Edge-Guided%2520Adversarial%250A%2520%2520Conditional%2520Diffusion%2520Model%26entry.906535625%3DGuoqing%2520Zhu%2520and%2520Honghu%2520Pan%2520and%2520Qiang%2520Wang%2520and%2520Chao%2520Tian%2520and%2520Chao%2520Yang%2520and%2520Zhenyu%2520He%26entry.1292438233%3D%2520%2520In%2520challenging%2520low%2520light%2520and%2520adverse%2520weather%2520conditions%252Cthermal%2520vision%250Aalgorithms%252Cespecially%2520object%2520detection%252Chave%2520exhibited%2520remarkable%250Apotential%252Ccontrasting%2520with%2520the%2520frequent%2520struggles%2520encountered%2520by%2520visible%2520vision%250Aalgorithms.%2520Nevertheless%252Cthe%2520efficacy%2520of%2520thermal%2520vision%2520algorithms%2520driven%2520by%250Adeep%2520learning%2520models%2520remains%2520constrained%2520by%2520the%2520paucity%2520of%2520available%2520training%250Adata%2520samples.%2520To%2520this%2520end%252Cthis%2520paper%2520introduces%2520a%2520novel%2520approach%2520termed%2520the%250Aedge%2520guided%2520conditional%2520diffusion%2520model.%2520This%2520framework%2520aims%2520to%2520produce%250Ameticulously%2520aligned%2520pseudo%2520thermal%2520images%2520at%2520the%2520pixel%2520level%252Cleveraging%2520edge%250Ainformation%2520extracted%2520from%2520visible%2520images.%2520By%2520utilizing%2520edges%2520as%2520contextual%250Acues%2520from%2520the%2520visible%2520domain%252Cthe%2520diffusion%2520model%2520achieves%2520meticulous%2520control%250Aover%2520the%2520delineation%2520of%2520objects%2520within%2520the%2520generated%2520images.%2520To%2520alleviate%2520the%250Aimpacts%2520of%2520those%2520visible-specific%2520edge%2520information%2520that%2520should%2520not%2520appear%2520in%250Athe%2520thermal%2520domain%252Ca%2520two-stage%2520modality%2520adversarial%2520training%2520strategy%2520is%250Aproposed%2520to%2520filter%2520them%2520out%2520from%2520the%2520generated%2520images%2520by%2520differentiating%2520the%250Avisible%2520and%2520thermal%2520modality.%2520Extensive%2520experiments%2520on%2520LLVIP%2520demonstrate%2520ECDM%2520s%250Asuperiority%2520over%2520existing%2520state-of-the-art%2520approaches%2520in%2520terms%2520of%2520image%250Ageneration%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Generation%20Scheme%20for%20Thermal%20Modality%20with%20Edge-Guided%20Adversarial%0A%20%20Conditional%20Diffusion%20Model&entry.906535625=Guoqing%20Zhu%20and%20Honghu%20Pan%20and%20Qiang%20Wang%20and%20Chao%20Tian%20and%20Chao%20Yang%20and%20Zhenyu%20He&entry.1292438233=%20%20In%20challenging%20low%20light%20and%20adverse%20weather%20conditions%2Cthermal%20vision%0Aalgorithms%2Cespecially%20object%20detection%2Chave%20exhibited%20remarkable%0Apotential%2Ccontrasting%20with%20the%20frequent%20struggles%20encountered%20by%20visible%20vision%0Aalgorithms.%20Nevertheless%2Cthe%20efficacy%20of%20thermal%20vision%20algorithms%20driven%20by%0Adeep%20learning%20models%20remains%20constrained%20by%20the%20paucity%20of%20available%20training%0Adata%20samples.%20To%20this%20end%2Cthis%20paper%20introduces%20a%20novel%20approach%20termed%20the%0Aedge%20guided%20conditional%20diffusion%20model.%20This%20framework%20aims%20to%20produce%0Ameticulously%20aligned%20pseudo%20thermal%20images%20at%20the%20pixel%20level%2Cleveraging%20edge%0Ainformation%20extracted%20from%20visible%20images.%20By%20utilizing%20edges%20as%20contextual%0Acues%20from%20the%20visible%20domain%2Cthe%20diffusion%20model%20achieves%20meticulous%20control%0Aover%20the%20delineation%20of%20objects%20within%20the%20generated%20images.%20To%20alleviate%20the%0Aimpacts%20of%20those%20visible-specific%20edge%20information%20that%20should%20not%20appear%20in%0Athe%20thermal%20domain%2Ca%20two-stage%20modality%20adversarial%20training%20strategy%20is%0Aproposed%20to%20filter%20them%20out%20from%20the%20generated%20images%20by%20differentiating%20the%0Avisible%20and%20thermal%20modality.%20Extensive%20experiments%20on%20LLVIP%20demonstrate%20ECDM%20s%0Asuperiority%20over%20existing%20state-of-the-art%20approaches%20in%20terms%20of%20image%0Ageneration%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03748v1&entry.124074799=Read"},
{"title": "Navigating the Human Maze: Real-Time Robot Pathfinding with Generative\n  Imitation Learning", "author": "Martin Moder and Stephen Adhisaputra and Josef Pauli", "abstract": "  This paper addresses navigation in crowded environments by integrating\ngoal-conditioned generative models with Sampling-based Model Predictive Control\n(SMPC). We introduce goal-conditioned autoregressive models to generate crowd\nbehaviors, capturing intricate interactions among individuals. The model\nprocesses potential robot trajectory samples and predicts the reactions of\nsurrounding individuals, enabling proactive robotic navigation in complex\nscenarios. Extensive experiments show that this algorithm enables real-time\nnavigation, significantly reducing collision rates and path lengths, and\noutperforming selected baseline methods. The practical effectiveness of this\nalgorithm is validated on an actual robotic platform, demonstrating its\ncapability in dynamic settings.\n", "link": "http://arxiv.org/abs/2408.03807v1", "date": "2024-08-07", "relevancy": 1.75, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6132}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5944}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20the%20Human%20Maze%3A%20Real-Time%20Robot%20Pathfinding%20with%20Generative%0A%20%20Imitation%20Learning&body=Title%3A%20Navigating%20the%20Human%20Maze%3A%20Real-Time%20Robot%20Pathfinding%20with%20Generative%0A%20%20Imitation%20Learning%0AAuthor%3A%20Martin%20Moder%20and%20Stephen%20Adhisaputra%20and%20Josef%20Pauli%0AAbstract%3A%20%20%20This%20paper%20addresses%20navigation%20in%20crowded%20environments%20by%20integrating%0Agoal-conditioned%20generative%20models%20with%20Sampling-based%20Model%20Predictive%20Control%0A%28SMPC%29.%20We%20introduce%20goal-conditioned%20autoregressive%20models%20to%20generate%20crowd%0Abehaviors%2C%20capturing%20intricate%20interactions%20among%20individuals.%20The%20model%0Aprocesses%20potential%20robot%20trajectory%20samples%20and%20predicts%20the%20reactions%20of%0Asurrounding%20individuals%2C%20enabling%20proactive%20robotic%20navigation%20in%20complex%0Ascenarios.%20Extensive%20experiments%20show%20that%20this%20algorithm%20enables%20real-time%0Anavigation%2C%20significantly%20reducing%20collision%20rates%20and%20path%20lengths%2C%20and%0Aoutperforming%20selected%20baseline%20methods.%20The%20practical%20effectiveness%20of%20this%0Aalgorithm%20is%20validated%20on%20an%20actual%20robotic%20platform%2C%20demonstrating%20its%0Acapability%20in%20dynamic%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520the%2520Human%2520Maze%253A%2520Real-Time%2520Robot%2520Pathfinding%2520with%2520Generative%250A%2520%2520Imitation%2520Learning%26entry.906535625%3DMartin%2520Moder%2520and%2520Stephen%2520Adhisaputra%2520and%2520Josef%2520Pauli%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520navigation%2520in%2520crowded%2520environments%2520by%2520integrating%250Agoal-conditioned%2520generative%2520models%2520with%2520Sampling-based%2520Model%2520Predictive%2520Control%250A%2528SMPC%2529.%2520We%2520introduce%2520goal-conditioned%2520autoregressive%2520models%2520to%2520generate%2520crowd%250Abehaviors%252C%2520capturing%2520intricate%2520interactions%2520among%2520individuals.%2520The%2520model%250Aprocesses%2520potential%2520robot%2520trajectory%2520samples%2520and%2520predicts%2520the%2520reactions%2520of%250Asurrounding%2520individuals%252C%2520enabling%2520proactive%2520robotic%2520navigation%2520in%2520complex%250Ascenarios.%2520Extensive%2520experiments%2520show%2520that%2520this%2520algorithm%2520enables%2520real-time%250Anavigation%252C%2520significantly%2520reducing%2520collision%2520rates%2520and%2520path%2520lengths%252C%2520and%250Aoutperforming%2520selected%2520baseline%2520methods.%2520The%2520practical%2520effectiveness%2520of%2520this%250Aalgorithm%2520is%2520validated%2520on%2520an%2520actual%2520robotic%2520platform%252C%2520demonstrating%2520its%250Acapability%2520in%2520dynamic%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20the%20Human%20Maze%3A%20Real-Time%20Robot%20Pathfinding%20with%20Generative%0A%20%20Imitation%20Learning&entry.906535625=Martin%20Moder%20and%20Stephen%20Adhisaputra%20and%20Josef%20Pauli&entry.1292438233=%20%20This%20paper%20addresses%20navigation%20in%20crowded%20environments%20by%20integrating%0Agoal-conditioned%20generative%20models%20with%20Sampling-based%20Model%20Predictive%20Control%0A%28SMPC%29.%20We%20introduce%20goal-conditioned%20autoregressive%20models%20to%20generate%20crowd%0Abehaviors%2C%20capturing%20intricate%20interactions%20among%20individuals.%20The%20model%0Aprocesses%20potential%20robot%20trajectory%20samples%20and%20predicts%20the%20reactions%20of%0Asurrounding%20individuals%2C%20enabling%20proactive%20robotic%20navigation%20in%20complex%0Ascenarios.%20Extensive%20experiments%20show%20that%20this%20algorithm%20enables%20real-time%0Anavigation%2C%20significantly%20reducing%20collision%20rates%20and%20path%20lengths%2C%20and%0Aoutperforming%20selected%20baseline%20methods.%20The%20practical%20effectiveness%20of%20this%0Aalgorithm%20is%20validated%20on%20an%20actual%20robotic%20platform%2C%20demonstrating%20its%0Acapability%20in%20dynamic%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03807v1&entry.124074799=Read"},
{"title": "Simplifying Scholarly Abstracts for Accessible Digital Libraries", "author": "Haining Wang and Jason Clark", "abstract": "  Standing at the forefront of knowledge dissemination, digital libraries\ncurate vast collections of scientific literature. However, these scholarly\nwritings are often laden with jargon and tailored for domain experts rather\nthan the general public. As librarians, we strive to offer services to a\ndiverse audience, including those with lower reading levels. To extend our\nservices beyond mere access, we propose fine-tuning a language model to rewrite\nscholarly abstracts into more comprehensible versions, thereby making scholarly\nliterature more accessible when requested. We began by introducing a corpus\nspecifically designed for training models to simplify scholarly abstracts. This\ncorpus consists of over three thousand pairs of abstracts and significance\nstatements from diverse disciplines. We then fine-tuned four language models\nusing this corpus. The outputs from the models were subsequently examined both\nquantitatively for accessibility and semantic coherence, and qualitatively for\nlanguage quality, faithfulness, and completeness. Our findings show that the\nresulting models can improve readability by over three grade levels, while\nmaintaining fidelity to the original content. Although commercial\nstate-of-the-art models still hold an edge, our models are much more compact,\ncan be deployed locally in an affordable manner, and alleviate the privacy\nconcerns associated with using commercial models. We envision this work as a\nstep toward more inclusive and accessible libraries, improving our services for\nyoung readers and those without a college degree.\n", "link": "http://arxiv.org/abs/2408.03899v1", "date": "2024-08-07", "relevancy": 1.7452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4418}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4398}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplifying%20Scholarly%20Abstracts%20for%20Accessible%20Digital%20Libraries&body=Title%3A%20Simplifying%20Scholarly%20Abstracts%20for%20Accessible%20Digital%20Libraries%0AAuthor%3A%20Haining%20Wang%20and%20Jason%20Clark%0AAbstract%3A%20%20%20Standing%20at%20the%20forefront%20of%20knowledge%20dissemination%2C%20digital%20libraries%0Acurate%20vast%20collections%20of%20scientific%20literature.%20However%2C%20these%20scholarly%0Awritings%20are%20often%20laden%20with%20jargon%20and%20tailored%20for%20domain%20experts%20rather%0Athan%20the%20general%20public.%20As%20librarians%2C%20we%20strive%20to%20offer%20services%20to%20a%0Adiverse%20audience%2C%20including%20those%20with%20lower%20reading%20levels.%20To%20extend%20our%0Aservices%20beyond%20mere%20access%2C%20we%20propose%20fine-tuning%20a%20language%20model%20to%20rewrite%0Ascholarly%20abstracts%20into%20more%20comprehensible%20versions%2C%20thereby%20making%20scholarly%0Aliterature%20more%20accessible%20when%20requested.%20We%20began%20by%20introducing%20a%20corpus%0Aspecifically%20designed%20for%20training%20models%20to%20simplify%20scholarly%20abstracts.%20This%0Acorpus%20consists%20of%20over%20three%20thousand%20pairs%20of%20abstracts%20and%20significance%0Astatements%20from%20diverse%20disciplines.%20We%20then%20fine-tuned%20four%20language%20models%0Ausing%20this%20corpus.%20The%20outputs%20from%20the%20models%20were%20subsequently%20examined%20both%0Aquantitatively%20for%20accessibility%20and%20semantic%20coherence%2C%20and%20qualitatively%20for%0Alanguage%20quality%2C%20faithfulness%2C%20and%20completeness.%20Our%20findings%20show%20that%20the%0Aresulting%20models%20can%20improve%20readability%20by%20over%20three%20grade%20levels%2C%20while%0Amaintaining%20fidelity%20to%20the%20original%20content.%20Although%20commercial%0Astate-of-the-art%20models%20still%20hold%20an%20edge%2C%20our%20models%20are%20much%20more%20compact%2C%0Acan%20be%20deployed%20locally%20in%20an%20affordable%20manner%2C%20and%20alleviate%20the%20privacy%0Aconcerns%20associated%20with%20using%20commercial%20models.%20We%20envision%20this%20work%20as%20a%0Astep%20toward%20more%20inclusive%20and%20accessible%20libraries%2C%20improving%20our%20services%20for%0Ayoung%20readers%20and%20those%20without%20a%20college%20degree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplifying%2520Scholarly%2520Abstracts%2520for%2520Accessible%2520Digital%2520Libraries%26entry.906535625%3DHaining%2520Wang%2520and%2520Jason%2520Clark%26entry.1292438233%3D%2520%2520Standing%2520at%2520the%2520forefront%2520of%2520knowledge%2520dissemination%252C%2520digital%2520libraries%250Acurate%2520vast%2520collections%2520of%2520scientific%2520literature.%2520However%252C%2520these%2520scholarly%250Awritings%2520are%2520often%2520laden%2520with%2520jargon%2520and%2520tailored%2520for%2520domain%2520experts%2520rather%250Athan%2520the%2520general%2520public.%2520As%2520librarians%252C%2520we%2520strive%2520to%2520offer%2520services%2520to%2520a%250Adiverse%2520audience%252C%2520including%2520those%2520with%2520lower%2520reading%2520levels.%2520To%2520extend%2520our%250Aservices%2520beyond%2520mere%2520access%252C%2520we%2520propose%2520fine-tuning%2520a%2520language%2520model%2520to%2520rewrite%250Ascholarly%2520abstracts%2520into%2520more%2520comprehensible%2520versions%252C%2520thereby%2520making%2520scholarly%250Aliterature%2520more%2520accessible%2520when%2520requested.%2520We%2520began%2520by%2520introducing%2520a%2520corpus%250Aspecifically%2520designed%2520for%2520training%2520models%2520to%2520simplify%2520scholarly%2520abstracts.%2520This%250Acorpus%2520consists%2520of%2520over%2520three%2520thousand%2520pairs%2520of%2520abstracts%2520and%2520significance%250Astatements%2520from%2520diverse%2520disciplines.%2520We%2520then%2520fine-tuned%2520four%2520language%2520models%250Ausing%2520this%2520corpus.%2520The%2520outputs%2520from%2520the%2520models%2520were%2520subsequently%2520examined%2520both%250Aquantitatively%2520for%2520accessibility%2520and%2520semantic%2520coherence%252C%2520and%2520qualitatively%2520for%250Alanguage%2520quality%252C%2520faithfulness%252C%2520and%2520completeness.%2520Our%2520findings%2520show%2520that%2520the%250Aresulting%2520models%2520can%2520improve%2520readability%2520by%2520over%2520three%2520grade%2520levels%252C%2520while%250Amaintaining%2520fidelity%2520to%2520the%2520original%2520content.%2520Although%2520commercial%250Astate-of-the-art%2520models%2520still%2520hold%2520an%2520edge%252C%2520our%2520models%2520are%2520much%2520more%2520compact%252C%250Acan%2520be%2520deployed%2520locally%2520in%2520an%2520affordable%2520manner%252C%2520and%2520alleviate%2520the%2520privacy%250Aconcerns%2520associated%2520with%2520using%2520commercial%2520models.%2520We%2520envision%2520this%2520work%2520as%2520a%250Astep%2520toward%2520more%2520inclusive%2520and%2520accessible%2520libraries%252C%2520improving%2520our%2520services%2520for%250Ayoung%2520readers%2520and%2520those%2520without%2520a%2520college%2520degree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplifying%20Scholarly%20Abstracts%20for%20Accessible%20Digital%20Libraries&entry.906535625=Haining%20Wang%20and%20Jason%20Clark&entry.1292438233=%20%20Standing%20at%20the%20forefront%20of%20knowledge%20dissemination%2C%20digital%20libraries%0Acurate%20vast%20collections%20of%20scientific%20literature.%20However%2C%20these%20scholarly%0Awritings%20are%20often%20laden%20with%20jargon%20and%20tailored%20for%20domain%20experts%20rather%0Athan%20the%20general%20public.%20As%20librarians%2C%20we%20strive%20to%20offer%20services%20to%20a%0Adiverse%20audience%2C%20including%20those%20with%20lower%20reading%20levels.%20To%20extend%20our%0Aservices%20beyond%20mere%20access%2C%20we%20propose%20fine-tuning%20a%20language%20model%20to%20rewrite%0Ascholarly%20abstracts%20into%20more%20comprehensible%20versions%2C%20thereby%20making%20scholarly%0Aliterature%20more%20accessible%20when%20requested.%20We%20began%20by%20introducing%20a%20corpus%0Aspecifically%20designed%20for%20training%20models%20to%20simplify%20scholarly%20abstracts.%20This%0Acorpus%20consists%20of%20over%20three%20thousand%20pairs%20of%20abstracts%20and%20significance%0Astatements%20from%20diverse%20disciplines.%20We%20then%20fine-tuned%20four%20language%20models%0Ausing%20this%20corpus.%20The%20outputs%20from%20the%20models%20were%20subsequently%20examined%20both%0Aquantitatively%20for%20accessibility%20and%20semantic%20coherence%2C%20and%20qualitatively%20for%0Alanguage%20quality%2C%20faithfulness%2C%20and%20completeness.%20Our%20findings%20show%20that%20the%0Aresulting%20models%20can%20improve%20readability%20by%20over%20three%20grade%20levels%2C%20while%0Amaintaining%20fidelity%20to%20the%20original%20content.%20Although%20commercial%0Astate-of-the-art%20models%20still%20hold%20an%20edge%2C%20our%20models%20are%20much%20more%20compact%2C%0Acan%20be%20deployed%20locally%20in%20an%20affordable%20manner%2C%20and%20alleviate%20the%20privacy%0Aconcerns%20associated%20with%20using%20commercial%20models.%20We%20envision%20this%20work%20as%20a%0Astep%20toward%20more%20inclusive%20and%20accessible%20libraries%2C%20improving%20our%20services%20for%0Ayoung%20readers%20and%20those%20without%20a%20college%20degree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03899v1&entry.124074799=Read"},
{"title": "Diffusion-based Human Motion Style Transfer with Semantic Guidance", "author": "Lei Hu and Zihao Zhang and Yongjing Ye and Yiwen Xu and Shihong Xia", "abstract": "  3D Human motion style transfer is a fundamental problem in computer graphic\nand animation processing. Existing AdaIN- based methods necessitate datasets\nwith balanced style distribution and content/style labels to train the\nclustered latent space. However, we may encounter a single unseen style example\nin practical scenarios, but not in sufficient quantity to constitute a style\ncluster for AdaIN-based methods. Therefore, in this paper, we propose a novel\ntwo-stage framework for few-shot style transfer learning based on the diffusion\nmodel. Specifically, in the first stage, we pre-train a diffusion-based\ntext-to-motion model as a generative prior so that it can cope with various\ncontent motion inputs. In the second stage, based on the single style example,\nwe fine-tune the pre-trained diffusion model in a few-shot manner to make it\ncapable of style transfer. The key idea is regarding the reverse process of\ndiffusion as a motion-style translation process since the motion styles can be\nviewed as special motion variations. During the fine-tuning for style transfer,\na simple yet effective semantic-guided style transfer loss coordinated with\nstyle example reconstruction loss is introduced to supervise the style transfer\nin CLIP semantic space. The qualitative and quantitative evaluations\ndemonstrate that our method can achieve state-of-the-art performance and has\npractical applications.\n", "link": "http://arxiv.org/abs/2405.06646v2", "date": "2024-08-07", "relevancy": 1.7157, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6123}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5853}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Human%20Motion%20Style%20Transfer%20with%20Semantic%20Guidance&body=Title%3A%20Diffusion-based%20Human%20Motion%20Style%20Transfer%20with%20Semantic%20Guidance%0AAuthor%3A%20Lei%20Hu%20and%20Zihao%20Zhang%20and%20Yongjing%20Ye%20and%20Yiwen%20Xu%20and%20Shihong%20Xia%0AAbstract%3A%20%20%203D%20Human%20motion%20style%20transfer%20is%20a%20fundamental%20problem%20in%20computer%20graphic%0Aand%20animation%20processing.%20Existing%20AdaIN-%20based%20methods%20necessitate%20datasets%0Awith%20balanced%20style%20distribution%20and%20content/style%20labels%20to%20train%20the%0Aclustered%20latent%20space.%20However%2C%20we%20may%20encounter%20a%20single%20unseen%20style%20example%0Ain%20practical%20scenarios%2C%20but%20not%20in%20sufficient%20quantity%20to%20constitute%20a%20style%0Acluster%20for%20AdaIN-based%20methods.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Atwo-stage%20framework%20for%20few-shot%20style%20transfer%20learning%20based%20on%20the%20diffusion%0Amodel.%20Specifically%2C%20in%20the%20first%20stage%2C%20we%20pre-train%20a%20diffusion-based%0Atext-to-motion%20model%20as%20a%20generative%20prior%20so%20that%20it%20can%20cope%20with%20various%0Acontent%20motion%20inputs.%20In%20the%20second%20stage%2C%20based%20on%20the%20single%20style%20example%2C%0Awe%20fine-tune%20the%20pre-trained%20diffusion%20model%20in%20a%20few-shot%20manner%20to%20make%20it%0Acapable%20of%20style%20transfer.%20The%20key%20idea%20is%20regarding%20the%20reverse%20process%20of%0Adiffusion%20as%20a%20motion-style%20translation%20process%20since%20the%20motion%20styles%20can%20be%0Aviewed%20as%20special%20motion%20variations.%20During%20the%20fine-tuning%20for%20style%20transfer%2C%0Aa%20simple%20yet%20effective%20semantic-guided%20style%20transfer%20loss%20coordinated%20with%0Astyle%20example%20reconstruction%20loss%20is%20introduced%20to%20supervise%20the%20style%20transfer%0Ain%20CLIP%20semantic%20space.%20The%20qualitative%20and%20quantitative%20evaluations%0Ademonstrate%20that%20our%20method%20can%20achieve%20state-of-the-art%20performance%20and%20has%0Apractical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06646v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Human%2520Motion%2520Style%2520Transfer%2520with%2520Semantic%2520Guidance%26entry.906535625%3DLei%2520Hu%2520and%2520Zihao%2520Zhang%2520and%2520Yongjing%2520Ye%2520and%2520Yiwen%2520Xu%2520and%2520Shihong%2520Xia%26entry.1292438233%3D%2520%25203D%2520Human%2520motion%2520style%2520transfer%2520is%2520a%2520fundamental%2520problem%2520in%2520computer%2520graphic%250Aand%2520animation%2520processing.%2520Existing%2520AdaIN-%2520based%2520methods%2520necessitate%2520datasets%250Awith%2520balanced%2520style%2520distribution%2520and%2520content/style%2520labels%2520to%2520train%2520the%250Aclustered%2520latent%2520space.%2520However%252C%2520we%2520may%2520encounter%2520a%2520single%2520unseen%2520style%2520example%250Ain%2520practical%2520scenarios%252C%2520but%2520not%2520in%2520sufficient%2520quantity%2520to%2520constitute%2520a%2520style%250Acluster%2520for%2520AdaIN-based%2520methods.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Atwo-stage%2520framework%2520for%2520few-shot%2520style%2520transfer%2520learning%2520based%2520on%2520the%2520diffusion%250Amodel.%2520Specifically%252C%2520in%2520the%2520first%2520stage%252C%2520we%2520pre-train%2520a%2520diffusion-based%250Atext-to-motion%2520model%2520as%2520a%2520generative%2520prior%2520so%2520that%2520it%2520can%2520cope%2520with%2520various%250Acontent%2520motion%2520inputs.%2520In%2520the%2520second%2520stage%252C%2520based%2520on%2520the%2520single%2520style%2520example%252C%250Awe%2520fine-tune%2520the%2520pre-trained%2520diffusion%2520model%2520in%2520a%2520few-shot%2520manner%2520to%2520make%2520it%250Acapable%2520of%2520style%2520transfer.%2520The%2520key%2520idea%2520is%2520regarding%2520the%2520reverse%2520process%2520of%250Adiffusion%2520as%2520a%2520motion-style%2520translation%2520process%2520since%2520the%2520motion%2520styles%2520can%2520be%250Aviewed%2520as%2520special%2520motion%2520variations.%2520During%2520the%2520fine-tuning%2520for%2520style%2520transfer%252C%250Aa%2520simple%2520yet%2520effective%2520semantic-guided%2520style%2520transfer%2520loss%2520coordinated%2520with%250Astyle%2520example%2520reconstruction%2520loss%2520is%2520introduced%2520to%2520supervise%2520the%2520style%2520transfer%250Ain%2520CLIP%2520semantic%2520space.%2520The%2520qualitative%2520and%2520quantitative%2520evaluations%250Ademonstrate%2520that%2520our%2520method%2520can%2520achieve%2520state-of-the-art%2520performance%2520and%2520has%250Apractical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06646v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Human%20Motion%20Style%20Transfer%20with%20Semantic%20Guidance&entry.906535625=Lei%20Hu%20and%20Zihao%20Zhang%20and%20Yongjing%20Ye%20and%20Yiwen%20Xu%20and%20Shihong%20Xia&entry.1292438233=%20%203D%20Human%20motion%20style%20transfer%20is%20a%20fundamental%20problem%20in%20computer%20graphic%0Aand%20animation%20processing.%20Existing%20AdaIN-%20based%20methods%20necessitate%20datasets%0Awith%20balanced%20style%20distribution%20and%20content/style%20labels%20to%20train%20the%0Aclustered%20latent%20space.%20However%2C%20we%20may%20encounter%20a%20single%20unseen%20style%20example%0Ain%20practical%20scenarios%2C%20but%20not%20in%20sufficient%20quantity%20to%20constitute%20a%20style%0Acluster%20for%20AdaIN-based%20methods.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Atwo-stage%20framework%20for%20few-shot%20style%20transfer%20learning%20based%20on%20the%20diffusion%0Amodel.%20Specifically%2C%20in%20the%20first%20stage%2C%20we%20pre-train%20a%20diffusion-based%0Atext-to-motion%20model%20as%20a%20generative%20prior%20so%20that%20it%20can%20cope%20with%20various%0Acontent%20motion%20inputs.%20In%20the%20second%20stage%2C%20based%20on%20the%20single%20style%20example%2C%0Awe%20fine-tune%20the%20pre-trained%20diffusion%20model%20in%20a%20few-shot%20manner%20to%20make%20it%0Acapable%20of%20style%20transfer.%20The%20key%20idea%20is%20regarding%20the%20reverse%20process%20of%0Adiffusion%20as%20a%20motion-style%20translation%20process%20since%20the%20motion%20styles%20can%20be%0Aviewed%20as%20special%20motion%20variations.%20During%20the%20fine-tuning%20for%20style%20transfer%2C%0Aa%20simple%20yet%20effective%20semantic-guided%20style%20transfer%20loss%20coordinated%20with%0Astyle%20example%20reconstruction%20loss%20is%20introduced%20to%20supervise%20the%20style%20transfer%0Ain%20CLIP%20semantic%20space.%20The%20qualitative%20and%20quantitative%20evaluations%0Ademonstrate%20that%20our%20method%20can%20achieve%20state-of-the-art%20performance%20and%20has%0Apractical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06646v2&entry.124074799=Read"},
{"title": "HDPlanner: Advancing Autonomous Deployments in Unknown Environments\n  through Hierarchical Decision Networks", "author": "Jingsong Liang and Yuhong Cao and Yixiao Ma and Hanqi Zhao and Guillaume Sartoretti", "abstract": "  In this paper, we introduce HDPlanner, a deep reinforcement learning (DRL)\nbased framework designed to tackle two core and challenging tasks for mobile\nrobots: autonomous exploration and navigation, where the robot must optimize\nits trajectory adaptively to achieve the task objective through continuous\ninteractions in unknown environments. Specifically, HDPlanner relies on novel\nhierarchical attention networks to empower the robot to reason about its belief\nacross multiple spatial scales and sequence collaborative decisions, where our\nnetworks decompose long-term objectives into short-term informative task\nassignments and informative path plannings. We further propose a contrastive\nlearning-based joint optimization to enhance the robustness of HDPlanner. We\nempirically demonstrate that HDPlanner significantly outperforms\nstate-of-the-art conventional and learning-based baselines on an extensive set\nof simulations, including hundreds of test maps and large-scale, complex Gazebo\nenvironments. Notably, HDPlanner achieves real-time planning with travel\ndistances reduced by up to 35.7% compared to exploration benchmarks and by up\nto 16.5% than navigation benchmarks. Furthermore, we validate our approach on\nhardware, where it generates high-quality, adaptive trajectories in both indoor\nand outdoor environments, highlighting its real-world applicability without\nadditional training.\n", "link": "http://arxiv.org/abs/2408.03768v1", "date": "2024-08-07", "relevancy": 1.7151, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HDPlanner%3A%20Advancing%20Autonomous%20Deployments%20in%20Unknown%20Environments%0A%20%20through%20Hierarchical%20Decision%20Networks&body=Title%3A%20HDPlanner%3A%20Advancing%20Autonomous%20Deployments%20in%20Unknown%20Environments%0A%20%20through%20Hierarchical%20Decision%20Networks%0AAuthor%3A%20Jingsong%20Liang%20and%20Yuhong%20Cao%20and%20Yixiao%20Ma%20and%20Hanqi%20Zhao%20and%20Guillaume%20Sartoretti%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20HDPlanner%2C%20a%20deep%20reinforcement%20learning%20%28DRL%29%0Abased%20framework%20designed%20to%20tackle%20two%20core%20and%20challenging%20tasks%20for%20mobile%0Arobots%3A%20autonomous%20exploration%20and%20navigation%2C%20where%20the%20robot%20must%20optimize%0Aits%20trajectory%20adaptively%20to%20achieve%20the%20task%20objective%20through%20continuous%0Ainteractions%20in%20unknown%20environments.%20Specifically%2C%20HDPlanner%20relies%20on%20novel%0Ahierarchical%20attention%20networks%20to%20empower%20the%20robot%20to%20reason%20about%20its%20belief%0Aacross%20multiple%20spatial%20scales%20and%20sequence%20collaborative%20decisions%2C%20where%20our%0Anetworks%20decompose%20long-term%20objectives%20into%20short-term%20informative%20task%0Aassignments%20and%20informative%20path%20plannings.%20We%20further%20propose%20a%20contrastive%0Alearning-based%20joint%20optimization%20to%20enhance%20the%20robustness%20of%20HDPlanner.%20We%0Aempirically%20demonstrate%20that%20HDPlanner%20significantly%20outperforms%0Astate-of-the-art%20conventional%20and%20learning-based%20baselines%20on%20an%20extensive%20set%0Aof%20simulations%2C%20including%20hundreds%20of%20test%20maps%20and%20large-scale%2C%20complex%20Gazebo%0Aenvironments.%20Notably%2C%20HDPlanner%20achieves%20real-time%20planning%20with%20travel%0Adistances%20reduced%20by%20up%20to%2035.7%25%20compared%20to%20exploration%20benchmarks%20and%20by%20up%0Ato%2016.5%25%20than%20navigation%20benchmarks.%20Furthermore%2C%20we%20validate%20our%20approach%20on%0Ahardware%2C%20where%20it%20generates%20high-quality%2C%20adaptive%20trajectories%20in%20both%20indoor%0Aand%20outdoor%20environments%2C%20highlighting%20its%20real-world%20applicability%20without%0Aadditional%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHDPlanner%253A%2520Advancing%2520Autonomous%2520Deployments%2520in%2520Unknown%2520Environments%250A%2520%2520through%2520Hierarchical%2520Decision%2520Networks%26entry.906535625%3DJingsong%2520Liang%2520and%2520Yuhong%2520Cao%2520and%2520Yixiao%2520Ma%2520and%2520Hanqi%2520Zhao%2520and%2520Guillaume%2520Sartoretti%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520HDPlanner%252C%2520a%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%250Abased%2520framework%2520designed%2520to%2520tackle%2520two%2520core%2520and%2520challenging%2520tasks%2520for%2520mobile%250Arobots%253A%2520autonomous%2520exploration%2520and%2520navigation%252C%2520where%2520the%2520robot%2520must%2520optimize%250Aits%2520trajectory%2520adaptively%2520to%2520achieve%2520the%2520task%2520objective%2520through%2520continuous%250Ainteractions%2520in%2520unknown%2520environments.%2520Specifically%252C%2520HDPlanner%2520relies%2520on%2520novel%250Ahierarchical%2520attention%2520networks%2520to%2520empower%2520the%2520robot%2520to%2520reason%2520about%2520its%2520belief%250Aacross%2520multiple%2520spatial%2520scales%2520and%2520sequence%2520collaborative%2520decisions%252C%2520where%2520our%250Anetworks%2520decompose%2520long-term%2520objectives%2520into%2520short-term%2520informative%2520task%250Aassignments%2520and%2520informative%2520path%2520plannings.%2520We%2520further%2520propose%2520a%2520contrastive%250Alearning-based%2520joint%2520optimization%2520to%2520enhance%2520the%2520robustness%2520of%2520HDPlanner.%2520We%250Aempirically%2520demonstrate%2520that%2520HDPlanner%2520significantly%2520outperforms%250Astate-of-the-art%2520conventional%2520and%2520learning-based%2520baselines%2520on%2520an%2520extensive%2520set%250Aof%2520simulations%252C%2520including%2520hundreds%2520of%2520test%2520maps%2520and%2520large-scale%252C%2520complex%2520Gazebo%250Aenvironments.%2520Notably%252C%2520HDPlanner%2520achieves%2520real-time%2520planning%2520with%2520travel%250Adistances%2520reduced%2520by%2520up%2520to%252035.7%2525%2520compared%2520to%2520exploration%2520benchmarks%2520and%2520by%2520up%250Ato%252016.5%2525%2520than%2520navigation%2520benchmarks.%2520Furthermore%252C%2520we%2520validate%2520our%2520approach%2520on%250Ahardware%252C%2520where%2520it%2520generates%2520high-quality%252C%2520adaptive%2520trajectories%2520in%2520both%2520indoor%250Aand%2520outdoor%2520environments%252C%2520highlighting%2520its%2520real-world%2520applicability%2520without%250Aadditional%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HDPlanner%3A%20Advancing%20Autonomous%20Deployments%20in%20Unknown%20Environments%0A%20%20through%20Hierarchical%20Decision%20Networks&entry.906535625=Jingsong%20Liang%20and%20Yuhong%20Cao%20and%20Yixiao%20Ma%20and%20Hanqi%20Zhao%20and%20Guillaume%20Sartoretti&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20HDPlanner%2C%20a%20deep%20reinforcement%20learning%20%28DRL%29%0Abased%20framework%20designed%20to%20tackle%20two%20core%20and%20challenging%20tasks%20for%20mobile%0Arobots%3A%20autonomous%20exploration%20and%20navigation%2C%20where%20the%20robot%20must%20optimize%0Aits%20trajectory%20adaptively%20to%20achieve%20the%20task%20objective%20through%20continuous%0Ainteractions%20in%20unknown%20environments.%20Specifically%2C%20HDPlanner%20relies%20on%20novel%0Ahierarchical%20attention%20networks%20to%20empower%20the%20robot%20to%20reason%20about%20its%20belief%0Aacross%20multiple%20spatial%20scales%20and%20sequence%20collaborative%20decisions%2C%20where%20our%0Anetworks%20decompose%20long-term%20objectives%20into%20short-term%20informative%20task%0Aassignments%20and%20informative%20path%20plannings.%20We%20further%20propose%20a%20contrastive%0Alearning-based%20joint%20optimization%20to%20enhance%20the%20robustness%20of%20HDPlanner.%20We%0Aempirically%20demonstrate%20that%20HDPlanner%20significantly%20outperforms%0Astate-of-the-art%20conventional%20and%20learning-based%20baselines%20on%20an%20extensive%20set%0Aof%20simulations%2C%20including%20hundreds%20of%20test%20maps%20and%20large-scale%2C%20complex%20Gazebo%0Aenvironments.%20Notably%2C%20HDPlanner%20achieves%20real-time%20planning%20with%20travel%0Adistances%20reduced%20by%20up%20to%2035.7%25%20compared%20to%20exploration%20benchmarks%20and%20by%20up%0Ato%2016.5%25%20than%20navigation%20benchmarks.%20Furthermore%2C%20we%20validate%20our%20approach%20on%0Ahardware%2C%20where%20it%20generates%20high-quality%2C%20adaptive%20trajectories%20in%20both%20indoor%0Aand%20outdoor%20environments%2C%20highlighting%20its%20real-world%20applicability%20without%0Aadditional%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03768v1&entry.124074799=Read"},
{"title": "Generative Adversarial Models for Extreme Geospatial Downscaling", "author": "Guiye Li and Guofeng Cao", "abstract": "  Addressing the challenges of climate change requires accurate and\nhigh-resolution mapping of geospatial data, especially climate and weather\nvariables. However, many existing geospatial datasets, such as the gridded\noutputs of the state-of-the-art numerical climate models (e.g., general\ncirculation models), are only available at very coarse spatial resolutions due\nto the model complexity and extremely high computational demand.\nDeep-learning-based methods, particularly generative adversarial networks\n(GANs) and their variants, have proved effective for refining natural images\nand have shown great promise in improving geospatial datasets. This paper\ndescribes a conditional GAN-based stochastic geospatial downscaling method that\ncan accommodates very high scaling factors. Compared to most existing methods,\nthe method can generate high-resolution accurate climate datasets from very\nlow-resolution inputs. More importantly, the method explicitly considers the\nuncertainty inherent to the downscaling process that tends to be ignored in\nexisting methods. Given an input, the method can produce a multitude of\nplausible high-resolution samples instead of one single deterministic result.\nThese samples allow for an empirical exploration and inferences of model\nuncertainty and robustness. With a case study of gridded climate datasets (wind\nvelocity and solar irradiance), we demonstrate the performances of the\nframework in downscaling tasks with large scaling factors (up to $64\\times$)\nand highlight the advantages of the framework with a comprehensive comparison\nwith commonly used and most recent downscaling methods, including area-to-point\n(ATP) kriging, deep image prior (DIP), enhanced super-resolution generative\nadversarial networks (ESRGAN), physics-informed resolution-enhancing GAN (PhIRE\nGAN), and an efficient diffusion model for remote sensing image\nsuper-resolution (EDiffSR).\n", "link": "http://arxiv.org/abs/2402.14049v2", "date": "2024-08-07", "relevancy": 1.6959, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.568}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5662}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Adversarial%20Models%20for%20Extreme%20Geospatial%20Downscaling&body=Title%3A%20Generative%20Adversarial%20Models%20for%20Extreme%20Geospatial%20Downscaling%0AAuthor%3A%20Guiye%20Li%20and%20Guofeng%20Cao%0AAbstract%3A%20%20%20Addressing%20the%20challenges%20of%20climate%20change%20requires%20accurate%20and%0Ahigh-resolution%20mapping%20of%20geospatial%20data%2C%20especially%20climate%20and%20weather%0Avariables.%20However%2C%20many%20existing%20geospatial%20datasets%2C%20such%20as%20the%20gridded%0Aoutputs%20of%20the%20state-of-the-art%20numerical%20climate%20models%20%28e.g.%2C%20general%0Acirculation%20models%29%2C%20are%20only%20available%20at%20very%20coarse%20spatial%20resolutions%20due%0Ato%20the%20model%20complexity%20and%20extremely%20high%20computational%20demand.%0ADeep-learning-based%20methods%2C%20particularly%20generative%20adversarial%20networks%0A%28GANs%29%20and%20their%20variants%2C%20have%20proved%20effective%20for%20refining%20natural%20images%0Aand%20have%20shown%20great%20promise%20in%20improving%20geospatial%20datasets.%20This%20paper%0Adescribes%20a%20conditional%20GAN-based%20stochastic%20geospatial%20downscaling%20method%20that%0Acan%20accommodates%20very%20high%20scaling%20factors.%20Compared%20to%20most%20existing%20methods%2C%0Athe%20method%20can%20generate%20high-resolution%20accurate%20climate%20datasets%20from%20very%0Alow-resolution%20inputs.%20More%20importantly%2C%20the%20method%20explicitly%20considers%20the%0Auncertainty%20inherent%20to%20the%20downscaling%20process%20that%20tends%20to%20be%20ignored%20in%0Aexisting%20methods.%20Given%20an%20input%2C%20the%20method%20can%20produce%20a%20multitude%20of%0Aplausible%20high-resolution%20samples%20instead%20of%20one%20single%20deterministic%20result.%0AThese%20samples%20allow%20for%20an%20empirical%20exploration%20and%20inferences%20of%20model%0Auncertainty%20and%20robustness.%20With%20a%20case%20study%20of%20gridded%20climate%20datasets%20%28wind%0Avelocity%20and%20solar%20irradiance%29%2C%20we%20demonstrate%20the%20performances%20of%20the%0Aframework%20in%20downscaling%20tasks%20with%20large%20scaling%20factors%20%28up%20to%20%2464%5Ctimes%24%29%0Aand%20highlight%20the%20advantages%20of%20the%20framework%20with%20a%20comprehensive%20comparison%0Awith%20commonly%20used%20and%20most%20recent%20downscaling%20methods%2C%20including%20area-to-point%0A%28ATP%29%20kriging%2C%20deep%20image%20prior%20%28DIP%29%2C%20enhanced%20super-resolution%20generative%0Aadversarial%20networks%20%28ESRGAN%29%2C%20physics-informed%20resolution-enhancing%20GAN%20%28PhIRE%0AGAN%29%2C%20and%20an%20efficient%20diffusion%20model%20for%20remote%20sensing%20image%0Asuper-resolution%20%28EDiffSR%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Adversarial%2520Models%2520for%2520Extreme%2520Geospatial%2520Downscaling%26entry.906535625%3DGuiye%2520Li%2520and%2520Guofeng%2520Cao%26entry.1292438233%3D%2520%2520Addressing%2520the%2520challenges%2520of%2520climate%2520change%2520requires%2520accurate%2520and%250Ahigh-resolution%2520mapping%2520of%2520geospatial%2520data%252C%2520especially%2520climate%2520and%2520weather%250Avariables.%2520However%252C%2520many%2520existing%2520geospatial%2520datasets%252C%2520such%2520as%2520the%2520gridded%250Aoutputs%2520of%2520the%2520state-of-the-art%2520numerical%2520climate%2520models%2520%2528e.g.%252C%2520general%250Acirculation%2520models%2529%252C%2520are%2520only%2520available%2520at%2520very%2520coarse%2520spatial%2520resolutions%2520due%250Ato%2520the%2520model%2520complexity%2520and%2520extremely%2520high%2520computational%2520demand.%250ADeep-learning-based%2520methods%252C%2520particularly%2520generative%2520adversarial%2520networks%250A%2528GANs%2529%2520and%2520their%2520variants%252C%2520have%2520proved%2520effective%2520for%2520refining%2520natural%2520images%250Aand%2520have%2520shown%2520great%2520promise%2520in%2520improving%2520geospatial%2520datasets.%2520This%2520paper%250Adescribes%2520a%2520conditional%2520GAN-based%2520stochastic%2520geospatial%2520downscaling%2520method%2520that%250Acan%2520accommodates%2520very%2520high%2520scaling%2520factors.%2520Compared%2520to%2520most%2520existing%2520methods%252C%250Athe%2520method%2520can%2520generate%2520high-resolution%2520accurate%2520climate%2520datasets%2520from%2520very%250Alow-resolution%2520inputs.%2520More%2520importantly%252C%2520the%2520method%2520explicitly%2520considers%2520the%250Auncertainty%2520inherent%2520to%2520the%2520downscaling%2520process%2520that%2520tends%2520to%2520be%2520ignored%2520in%250Aexisting%2520methods.%2520Given%2520an%2520input%252C%2520the%2520method%2520can%2520produce%2520a%2520multitude%2520of%250Aplausible%2520high-resolution%2520samples%2520instead%2520of%2520one%2520single%2520deterministic%2520result.%250AThese%2520samples%2520allow%2520for%2520an%2520empirical%2520exploration%2520and%2520inferences%2520of%2520model%250Auncertainty%2520and%2520robustness.%2520With%2520a%2520case%2520study%2520of%2520gridded%2520climate%2520datasets%2520%2528wind%250Avelocity%2520and%2520solar%2520irradiance%2529%252C%2520we%2520demonstrate%2520the%2520performances%2520of%2520the%250Aframework%2520in%2520downscaling%2520tasks%2520with%2520large%2520scaling%2520factors%2520%2528up%2520to%2520%252464%255Ctimes%2524%2529%250Aand%2520highlight%2520the%2520advantages%2520of%2520the%2520framework%2520with%2520a%2520comprehensive%2520comparison%250Awith%2520commonly%2520used%2520and%2520most%2520recent%2520downscaling%2520methods%252C%2520including%2520area-to-point%250A%2528ATP%2529%2520kriging%252C%2520deep%2520image%2520prior%2520%2528DIP%2529%252C%2520enhanced%2520super-resolution%2520generative%250Aadversarial%2520networks%2520%2528ESRGAN%2529%252C%2520physics-informed%2520resolution-enhancing%2520GAN%2520%2528PhIRE%250AGAN%2529%252C%2520and%2520an%2520efficient%2520diffusion%2520model%2520for%2520remote%2520sensing%2520image%250Asuper-resolution%2520%2528EDiffSR%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Adversarial%20Models%20for%20Extreme%20Geospatial%20Downscaling&entry.906535625=Guiye%20Li%20and%20Guofeng%20Cao&entry.1292438233=%20%20Addressing%20the%20challenges%20of%20climate%20change%20requires%20accurate%20and%0Ahigh-resolution%20mapping%20of%20geospatial%20data%2C%20especially%20climate%20and%20weather%0Avariables.%20However%2C%20many%20existing%20geospatial%20datasets%2C%20such%20as%20the%20gridded%0Aoutputs%20of%20the%20state-of-the-art%20numerical%20climate%20models%20%28e.g.%2C%20general%0Acirculation%20models%29%2C%20are%20only%20available%20at%20very%20coarse%20spatial%20resolutions%20due%0Ato%20the%20model%20complexity%20and%20extremely%20high%20computational%20demand.%0ADeep-learning-based%20methods%2C%20particularly%20generative%20adversarial%20networks%0A%28GANs%29%20and%20their%20variants%2C%20have%20proved%20effective%20for%20refining%20natural%20images%0Aand%20have%20shown%20great%20promise%20in%20improving%20geospatial%20datasets.%20This%20paper%0Adescribes%20a%20conditional%20GAN-based%20stochastic%20geospatial%20downscaling%20method%20that%0Acan%20accommodates%20very%20high%20scaling%20factors.%20Compared%20to%20most%20existing%20methods%2C%0Athe%20method%20can%20generate%20high-resolution%20accurate%20climate%20datasets%20from%20very%0Alow-resolution%20inputs.%20More%20importantly%2C%20the%20method%20explicitly%20considers%20the%0Auncertainty%20inherent%20to%20the%20downscaling%20process%20that%20tends%20to%20be%20ignored%20in%0Aexisting%20methods.%20Given%20an%20input%2C%20the%20method%20can%20produce%20a%20multitude%20of%0Aplausible%20high-resolution%20samples%20instead%20of%20one%20single%20deterministic%20result.%0AThese%20samples%20allow%20for%20an%20empirical%20exploration%20and%20inferences%20of%20model%0Auncertainty%20and%20robustness.%20With%20a%20case%20study%20of%20gridded%20climate%20datasets%20%28wind%0Avelocity%20and%20solar%20irradiance%29%2C%20we%20demonstrate%20the%20performances%20of%20the%0Aframework%20in%20downscaling%20tasks%20with%20large%20scaling%20factors%20%28up%20to%20%2464%5Ctimes%24%29%0Aand%20highlight%20the%20advantages%20of%20the%20framework%20with%20a%20comprehensive%20comparison%0Awith%20commonly%20used%20and%20most%20recent%20downscaling%20methods%2C%20including%20area-to-point%0A%28ATP%29%20kriging%2C%20deep%20image%20prior%20%28DIP%29%2C%20enhanced%20super-resolution%20generative%0Aadversarial%20networks%20%28ESRGAN%29%2C%20physics-informed%20resolution-enhancing%20GAN%20%28PhIRE%0AGAN%29%2C%20and%20an%20efficient%20diffusion%20model%20for%20remote%20sensing%20image%0Asuper-resolution%20%28EDiffSR%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14049v2&entry.124074799=Read"},
{"title": "Lightweight Video Denoising Using a Classic Bayesian Backbone", "author": "Cl\u00e9ment Bled and Fran\u00e7ois Piti\u00e9", "abstract": "  In recent years, state-of-the-art image and video denoising networks have\nbecome increasingly large, requiring millions of trainable parameters to\nachieve best-in-class performance. Improved denoising quality has come at the\ncost of denoising speed, where modern transformer networks are far slower to\nrun than smaller denoising networks such as FastDVDnet and classic Bayesian\ndenoisers such as the Wiener filter.\n  In this paper, we implement a hybrid Wiener filter which leverages small\nancillary networks to increase the original denoiser performance, while\nretaining fast denoising speeds. These networks are used to refine the Wiener\ncoring estimate, optimise windowing functions and estimate the unknown noise\nprofile. Using these methods, we outperform several popular denoisers and\nremain within 0.2 dB, on average, of the popular VRT transformer. Our method\nwas found to be over x10 faster than the transformer method, with a far lower\nparameter cost.\n", "link": "http://arxiv.org/abs/2408.03904v1", "date": "2024-08-07", "relevancy": 1.6829, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5894}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5268}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Video%20Denoising%20Using%20a%20Classic%20Bayesian%20Backbone&body=Title%3A%20Lightweight%20Video%20Denoising%20Using%20a%20Classic%20Bayesian%20Backbone%0AAuthor%3A%20Cl%C3%A9ment%20Bled%20and%20Fran%C3%A7ois%20Piti%C3%A9%0AAbstract%3A%20%20%20In%20recent%20years%2C%20state-of-the-art%20image%20and%20video%20denoising%20networks%20have%0Abecome%20increasingly%20large%2C%20requiring%20millions%20of%20trainable%20parameters%20to%0Aachieve%20best-in-class%20performance.%20Improved%20denoising%20quality%20has%20come%20at%20the%0Acost%20of%20denoising%20speed%2C%20where%20modern%20transformer%20networks%20are%20far%20slower%20to%0Arun%20than%20smaller%20denoising%20networks%20such%20as%20FastDVDnet%20and%20classic%20Bayesian%0Adenoisers%20such%20as%20the%20Wiener%20filter.%0A%20%20In%20this%20paper%2C%20we%20implement%20a%20hybrid%20Wiener%20filter%20which%20leverages%20small%0Aancillary%20networks%20to%20increase%20the%20original%20denoiser%20performance%2C%20while%0Aretaining%20fast%20denoising%20speeds.%20These%20networks%20are%20used%20to%20refine%20the%20Wiener%0Acoring%20estimate%2C%20optimise%20windowing%20functions%20and%20estimate%20the%20unknown%20noise%0Aprofile.%20Using%20these%20methods%2C%20we%20outperform%20several%20popular%20denoisers%20and%0Aremain%20within%200.2%20dB%2C%20on%20average%2C%20of%20the%20popular%20VRT%20transformer.%20Our%20method%0Awas%20found%20to%20be%20over%20x10%20faster%20than%20the%20transformer%20method%2C%20with%20a%20far%20lower%0Aparameter%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Video%2520Denoising%2520Using%2520a%2520Classic%2520Bayesian%2520Backbone%26entry.906535625%3DCl%25C3%25A9ment%2520Bled%2520and%2520Fran%25C3%25A7ois%2520Piti%25C3%25A9%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520state-of-the-art%2520image%2520and%2520video%2520denoising%2520networks%2520have%250Abecome%2520increasingly%2520large%252C%2520requiring%2520millions%2520of%2520trainable%2520parameters%2520to%250Aachieve%2520best-in-class%2520performance.%2520Improved%2520denoising%2520quality%2520has%2520come%2520at%2520the%250Acost%2520of%2520denoising%2520speed%252C%2520where%2520modern%2520transformer%2520networks%2520are%2520far%2520slower%2520to%250Arun%2520than%2520smaller%2520denoising%2520networks%2520such%2520as%2520FastDVDnet%2520and%2520classic%2520Bayesian%250Adenoisers%2520such%2520as%2520the%2520Wiener%2520filter.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520implement%2520a%2520hybrid%2520Wiener%2520filter%2520which%2520leverages%2520small%250Aancillary%2520networks%2520to%2520increase%2520the%2520original%2520denoiser%2520performance%252C%2520while%250Aretaining%2520fast%2520denoising%2520speeds.%2520These%2520networks%2520are%2520used%2520to%2520refine%2520the%2520Wiener%250Acoring%2520estimate%252C%2520optimise%2520windowing%2520functions%2520and%2520estimate%2520the%2520unknown%2520noise%250Aprofile.%2520Using%2520these%2520methods%252C%2520we%2520outperform%2520several%2520popular%2520denoisers%2520and%250Aremain%2520within%25200.2%2520dB%252C%2520on%2520average%252C%2520of%2520the%2520popular%2520VRT%2520transformer.%2520Our%2520method%250Awas%2520found%2520to%2520be%2520over%2520x10%2520faster%2520than%2520the%2520transformer%2520method%252C%2520with%2520a%2520far%2520lower%250Aparameter%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Video%20Denoising%20Using%20a%20Classic%20Bayesian%20Backbone&entry.906535625=Cl%C3%A9ment%20Bled%20and%20Fran%C3%A7ois%20Piti%C3%A9&entry.1292438233=%20%20In%20recent%20years%2C%20state-of-the-art%20image%20and%20video%20denoising%20networks%20have%0Abecome%20increasingly%20large%2C%20requiring%20millions%20of%20trainable%20parameters%20to%0Aachieve%20best-in-class%20performance.%20Improved%20denoising%20quality%20has%20come%20at%20the%0Acost%20of%20denoising%20speed%2C%20where%20modern%20transformer%20networks%20are%20far%20slower%20to%0Arun%20than%20smaller%20denoising%20networks%20such%20as%20FastDVDnet%20and%20classic%20Bayesian%0Adenoisers%20such%20as%20the%20Wiener%20filter.%0A%20%20In%20this%20paper%2C%20we%20implement%20a%20hybrid%20Wiener%20filter%20which%20leverages%20small%0Aancillary%20networks%20to%20increase%20the%20original%20denoiser%20performance%2C%20while%0Aretaining%20fast%20denoising%20speeds.%20These%20networks%20are%20used%20to%20refine%20the%20Wiener%0Acoring%20estimate%2C%20optimise%20windowing%20functions%20and%20estimate%20the%20unknown%20noise%0Aprofile.%20Using%20these%20methods%2C%20we%20outperform%20several%20popular%20denoisers%20and%0Aremain%20within%200.2%20dB%2C%20on%20average%2C%20of%20the%20popular%20VRT%20transformer.%20Our%20method%0Awas%20found%20to%20be%20over%20x10%20faster%20than%20the%20transformer%20method%2C%20with%20a%20far%20lower%0Aparameter%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03904v1&entry.124074799=Read"},
{"title": "A Soft Robotic System Automatically Learns Precise Agile Motions Without\n  Model Information", "author": "Simon Bachhuber and Alexander Pawluchin and Arka Pal and Ivo Boblan and Thomas Seel", "abstract": "  Many application domains, e.g., in medicine and manufacturing, can greatly\nbenefit from pneumatic Soft Robots (SRs). However, the accurate control of SRs\nhas remained a significant challenge to date, mainly due to their nonlinear\ndynamics and viscoelastic material properties. Conventional control design\nmethods often rely on either complex system modeling or time-intensive manual\ntuning, both of which require significant amounts of human expertise and thus\nlimit their practicality. In recent works, the data-driven method, Automatic\nNeural ODE Control (ANODEC) has been successfully used to -- fully\nautomatically and utilizing only input-output data -- design controllers for\nvarious nonlinear systems in silico, and without requiring prior model\nknowledge or extensive manual tuning. In this work, we successfully apply\nANODEC to automatically learn to perform agile, non-repetitive reference\ntracking motion tasks in a real-world SR and within a finite time horizon. To\nthe best of the authors' knowledge, ANODEC achieves, for the first time,\nperformant control of a SR with hysteresis effects from only 30 seconds of\ninput-output data and without any prior model knowledge. We show that for\nmultiple, qualitatively different and even out-of-training-distribution\nreference signals, a single feedback controller designed by ANODEC outperforms\na manually tuned PID baseline consistently. Overall, this contribution not only\nfurther strengthens the validity of ANODEC, but it marks an important step\ntowards more practical, easy-to-use SRs that can automatically learn to perform\nagile motions from minimal experimental interaction time.\n", "link": "http://arxiv.org/abs/2408.03754v1", "date": "2024-08-07", "relevancy": 1.6726, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5882}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Soft%20Robotic%20System%20Automatically%20Learns%20Precise%20Agile%20Motions%20Without%0A%20%20Model%20Information&body=Title%3A%20A%20Soft%20Robotic%20System%20Automatically%20Learns%20Precise%20Agile%20Motions%20Without%0A%20%20Model%20Information%0AAuthor%3A%20Simon%20Bachhuber%20and%20Alexander%20Pawluchin%20and%20Arka%20Pal%20and%20Ivo%20Boblan%20and%20Thomas%20Seel%0AAbstract%3A%20%20%20Many%20application%20domains%2C%20e.g.%2C%20in%20medicine%20and%20manufacturing%2C%20can%20greatly%0Abenefit%20from%20pneumatic%20Soft%20Robots%20%28SRs%29.%20However%2C%20the%20accurate%20control%20of%20SRs%0Ahas%20remained%20a%20significant%20challenge%20to%20date%2C%20mainly%20due%20to%20their%20nonlinear%0Adynamics%20and%20viscoelastic%20material%20properties.%20Conventional%20control%20design%0Amethods%20often%20rely%20on%20either%20complex%20system%20modeling%20or%20time-intensive%20manual%0Atuning%2C%20both%20of%20which%20require%20significant%20amounts%20of%20human%20expertise%20and%20thus%0Alimit%20their%20practicality.%20In%20recent%20works%2C%20the%20data-driven%20method%2C%20Automatic%0ANeural%20ODE%20Control%20%28ANODEC%29%20has%20been%20successfully%20used%20to%20--%20fully%0Aautomatically%20and%20utilizing%20only%20input-output%20data%20--%20design%20controllers%20for%0Avarious%20nonlinear%20systems%20in%20silico%2C%20and%20without%20requiring%20prior%20model%0Aknowledge%20or%20extensive%20manual%20tuning.%20In%20this%20work%2C%20we%20successfully%20apply%0AANODEC%20to%20automatically%20learn%20to%20perform%20agile%2C%20non-repetitive%20reference%0Atracking%20motion%20tasks%20in%20a%20real-world%20SR%20and%20within%20a%20finite%20time%20horizon.%20To%0Athe%20best%20of%20the%20authors%27%20knowledge%2C%20ANODEC%20achieves%2C%20for%20the%20first%20time%2C%0Aperformant%20control%20of%20a%20SR%20with%20hysteresis%20effects%20from%20only%2030%20seconds%20of%0Ainput-output%20data%20and%20without%20any%20prior%20model%20knowledge.%20We%20show%20that%20for%0Amultiple%2C%20qualitatively%20different%20and%20even%20out-of-training-distribution%0Areference%20signals%2C%20a%20single%20feedback%20controller%20designed%20by%20ANODEC%20outperforms%0Aa%20manually%20tuned%20PID%20baseline%20consistently.%20Overall%2C%20this%20contribution%20not%20only%0Afurther%20strengthens%20the%20validity%20of%20ANODEC%2C%20but%20it%20marks%20an%20important%20step%0Atowards%20more%20practical%2C%20easy-to-use%20SRs%20that%20can%20automatically%20learn%20to%20perform%0Aagile%20motions%20from%20minimal%20experimental%20interaction%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Soft%2520Robotic%2520System%2520Automatically%2520Learns%2520Precise%2520Agile%2520Motions%2520Without%250A%2520%2520Model%2520Information%26entry.906535625%3DSimon%2520Bachhuber%2520and%2520Alexander%2520Pawluchin%2520and%2520Arka%2520Pal%2520and%2520Ivo%2520Boblan%2520and%2520Thomas%2520Seel%26entry.1292438233%3D%2520%2520Many%2520application%2520domains%252C%2520e.g.%252C%2520in%2520medicine%2520and%2520manufacturing%252C%2520can%2520greatly%250Abenefit%2520from%2520pneumatic%2520Soft%2520Robots%2520%2528SRs%2529.%2520However%252C%2520the%2520accurate%2520control%2520of%2520SRs%250Ahas%2520remained%2520a%2520significant%2520challenge%2520to%2520date%252C%2520mainly%2520due%2520to%2520their%2520nonlinear%250Adynamics%2520and%2520viscoelastic%2520material%2520properties.%2520Conventional%2520control%2520design%250Amethods%2520often%2520rely%2520on%2520either%2520complex%2520system%2520modeling%2520or%2520time-intensive%2520manual%250Atuning%252C%2520both%2520of%2520which%2520require%2520significant%2520amounts%2520of%2520human%2520expertise%2520and%2520thus%250Alimit%2520their%2520practicality.%2520In%2520recent%2520works%252C%2520the%2520data-driven%2520method%252C%2520Automatic%250ANeural%2520ODE%2520Control%2520%2528ANODEC%2529%2520has%2520been%2520successfully%2520used%2520to%2520--%2520fully%250Aautomatically%2520and%2520utilizing%2520only%2520input-output%2520data%2520--%2520design%2520controllers%2520for%250Avarious%2520nonlinear%2520systems%2520in%2520silico%252C%2520and%2520without%2520requiring%2520prior%2520model%250Aknowledge%2520or%2520extensive%2520manual%2520tuning.%2520In%2520this%2520work%252C%2520we%2520successfully%2520apply%250AANODEC%2520to%2520automatically%2520learn%2520to%2520perform%2520agile%252C%2520non-repetitive%2520reference%250Atracking%2520motion%2520tasks%2520in%2520a%2520real-world%2520SR%2520and%2520within%2520a%2520finite%2520time%2520horizon.%2520To%250Athe%2520best%2520of%2520the%2520authors%2527%2520knowledge%252C%2520ANODEC%2520achieves%252C%2520for%2520the%2520first%2520time%252C%250Aperformant%2520control%2520of%2520a%2520SR%2520with%2520hysteresis%2520effects%2520from%2520only%252030%2520seconds%2520of%250Ainput-output%2520data%2520and%2520without%2520any%2520prior%2520model%2520knowledge.%2520We%2520show%2520that%2520for%250Amultiple%252C%2520qualitatively%2520different%2520and%2520even%2520out-of-training-distribution%250Areference%2520signals%252C%2520a%2520single%2520feedback%2520controller%2520designed%2520by%2520ANODEC%2520outperforms%250Aa%2520manually%2520tuned%2520PID%2520baseline%2520consistently.%2520Overall%252C%2520this%2520contribution%2520not%2520only%250Afurther%2520strengthens%2520the%2520validity%2520of%2520ANODEC%252C%2520but%2520it%2520marks%2520an%2520important%2520step%250Atowards%2520more%2520practical%252C%2520easy-to-use%2520SRs%2520that%2520can%2520automatically%2520learn%2520to%2520perform%250Aagile%2520motions%2520from%2520minimal%2520experimental%2520interaction%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Soft%20Robotic%20System%20Automatically%20Learns%20Precise%20Agile%20Motions%20Without%0A%20%20Model%20Information&entry.906535625=Simon%20Bachhuber%20and%20Alexander%20Pawluchin%20and%20Arka%20Pal%20and%20Ivo%20Boblan%20and%20Thomas%20Seel&entry.1292438233=%20%20Many%20application%20domains%2C%20e.g.%2C%20in%20medicine%20and%20manufacturing%2C%20can%20greatly%0Abenefit%20from%20pneumatic%20Soft%20Robots%20%28SRs%29.%20However%2C%20the%20accurate%20control%20of%20SRs%0Ahas%20remained%20a%20significant%20challenge%20to%20date%2C%20mainly%20due%20to%20their%20nonlinear%0Adynamics%20and%20viscoelastic%20material%20properties.%20Conventional%20control%20design%0Amethods%20often%20rely%20on%20either%20complex%20system%20modeling%20or%20time-intensive%20manual%0Atuning%2C%20both%20of%20which%20require%20significant%20amounts%20of%20human%20expertise%20and%20thus%0Alimit%20their%20practicality.%20In%20recent%20works%2C%20the%20data-driven%20method%2C%20Automatic%0ANeural%20ODE%20Control%20%28ANODEC%29%20has%20been%20successfully%20used%20to%20--%20fully%0Aautomatically%20and%20utilizing%20only%20input-output%20data%20--%20design%20controllers%20for%0Avarious%20nonlinear%20systems%20in%20silico%2C%20and%20without%20requiring%20prior%20model%0Aknowledge%20or%20extensive%20manual%20tuning.%20In%20this%20work%2C%20we%20successfully%20apply%0AANODEC%20to%20automatically%20learn%20to%20perform%20agile%2C%20non-repetitive%20reference%0Atracking%20motion%20tasks%20in%20a%20real-world%20SR%20and%20within%20a%20finite%20time%20horizon.%20To%0Athe%20best%20of%20the%20authors%27%20knowledge%2C%20ANODEC%20achieves%2C%20for%20the%20first%20time%2C%0Aperformant%20control%20of%20a%20SR%20with%20hysteresis%20effects%20from%20only%2030%20seconds%20of%0Ainput-output%20data%20and%20without%20any%20prior%20model%20knowledge.%20We%20show%20that%20for%0Amultiple%2C%20qualitatively%20different%20and%20even%20out-of-training-distribution%0Areference%20signals%2C%20a%20single%20feedback%20controller%20designed%20by%20ANODEC%20outperforms%0Aa%20manually%20tuned%20PID%20baseline%20consistently.%20Overall%2C%20this%20contribution%20not%20only%0Afurther%20strengthens%20the%20validity%20of%20ANODEC%2C%20but%20it%20marks%20an%20important%20step%0Atowards%20more%20practical%2C%20easy-to-use%20SRs%20that%20can%20automatically%20learn%20to%20perform%0Aagile%20motions%20from%20minimal%20experimental%20interaction%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03754v1&entry.124074799=Read"},
{"title": "Semantic-guided modeling of spatial relation and object co-occurrence\n  for indoor scene recognition", "author": "Chuanxin Song and Hanbo Wu and Xin Ma", "abstract": "  Exploring the semantic context in scene images is essential for indoor scene\nrecognition. However, due to the diverse intra-class spatial layouts and the\ncoexisting inter-class objects, modeling contextual relationships to adapt\nvarious image characteristics is a great challenge. Existing contextual\nmodeling methods for scene recognition exhibit two limitations: 1) They\ntypically model only one type of spatial relationship (order or metric) among\nobjects within scenes, with limited exploration of diverse spatial layouts. 2)\nThey often overlook the differences in coexisting objects across different\nscenes, suppressing scene recognition performance. To overcome these\nlimitations, we propose SpaCoNet, which simultaneously models Spatial relation\nand Co-occurrence of objects guided by semantic segmentation. Firstly, the\nSemantic Spatial Relation Module (SSRM) is constructed to model scene spatial\nfeatures. With the help of semantic segmentation, this module decouples spatial\ninformation from the scene image and thoroughly explores all spatial\nrelationships among objects in an end-to-end manner, thereby obtaining\nsemantic-based spatial features. Secondly, both spatial features from the SSRM\nand deep features from the Image Feature Extraction Module are allocated to\neach object, so as to distinguish the coexisting object across different\nscenes. Finally, utilizing the discriminative features above, we design a\nGlobal-Local Dependency Module to explore the long-range co-occurrence among\nobjects, and further generate a semantic-guided feature representation for\nindoor scene recognition. Experimental results on three widely used scene\ndatasets demonstrate the effectiveness and generality of the proposed method.\n", "link": "http://arxiv.org/abs/2305.12661v4", "date": "2024-08-07", "relevancy": 1.6713, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5686}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5549}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-guided%20modeling%20of%20spatial%20relation%20and%20object%20co-occurrence%0A%20%20for%20indoor%20scene%20recognition&body=Title%3A%20Semantic-guided%20modeling%20of%20spatial%20relation%20and%20object%20co-occurrence%0A%20%20for%20indoor%20scene%20recognition%0AAuthor%3A%20Chuanxin%20Song%20and%20Hanbo%20Wu%20and%20Xin%20Ma%0AAbstract%3A%20%20%20Exploring%20the%20semantic%20context%20in%20scene%20images%20is%20essential%20for%20indoor%20scene%0Arecognition.%20However%2C%20due%20to%20the%20diverse%20intra-class%20spatial%20layouts%20and%20the%0Acoexisting%20inter-class%20objects%2C%20modeling%20contextual%20relationships%20to%20adapt%0Avarious%20image%20characteristics%20is%20a%20great%20challenge.%20Existing%20contextual%0Amodeling%20methods%20for%20scene%20recognition%20exhibit%20two%20limitations%3A%201%29%20They%0Atypically%20model%20only%20one%20type%20of%20spatial%20relationship%20%28order%20or%20metric%29%20among%0Aobjects%20within%20scenes%2C%20with%20limited%20exploration%20of%20diverse%20spatial%20layouts.%202%29%0AThey%20often%20overlook%20the%20differences%20in%20coexisting%20objects%20across%20different%0Ascenes%2C%20suppressing%20scene%20recognition%20performance.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20SpaCoNet%2C%20which%20simultaneously%20models%20Spatial%20relation%0Aand%20Co-occurrence%20of%20objects%20guided%20by%20semantic%20segmentation.%20Firstly%2C%20the%0ASemantic%20Spatial%20Relation%20Module%20%28SSRM%29%20is%20constructed%20to%20model%20scene%20spatial%0Afeatures.%20With%20the%20help%20of%20semantic%20segmentation%2C%20this%20module%20decouples%20spatial%0Ainformation%20from%20the%20scene%20image%20and%20thoroughly%20explores%20all%20spatial%0Arelationships%20among%20objects%20in%20an%20end-to-end%20manner%2C%20thereby%20obtaining%0Asemantic-based%20spatial%20features.%20Secondly%2C%20both%20spatial%20features%20from%20the%20SSRM%0Aand%20deep%20features%20from%20the%20Image%20Feature%20Extraction%20Module%20are%20allocated%20to%0Aeach%20object%2C%20so%20as%20to%20distinguish%20the%20coexisting%20object%20across%20different%0Ascenes.%20Finally%2C%20utilizing%20the%20discriminative%20features%20above%2C%20we%20design%20a%0AGlobal-Local%20Dependency%20Module%20to%20explore%20the%20long-range%20co-occurrence%20among%0Aobjects%2C%20and%20further%20generate%20a%20semantic-guided%20feature%20representation%20for%0Aindoor%20scene%20recognition.%20Experimental%20results%20on%20three%20widely%20used%20scene%0Adatasets%20demonstrate%20the%20effectiveness%20and%20generality%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12661v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-guided%2520modeling%2520of%2520spatial%2520relation%2520and%2520object%2520co-occurrence%250A%2520%2520for%2520indoor%2520scene%2520recognition%26entry.906535625%3DChuanxin%2520Song%2520and%2520Hanbo%2520Wu%2520and%2520Xin%2520Ma%26entry.1292438233%3D%2520%2520Exploring%2520the%2520semantic%2520context%2520in%2520scene%2520images%2520is%2520essential%2520for%2520indoor%2520scene%250Arecognition.%2520However%252C%2520due%2520to%2520the%2520diverse%2520intra-class%2520spatial%2520layouts%2520and%2520the%250Acoexisting%2520inter-class%2520objects%252C%2520modeling%2520contextual%2520relationships%2520to%2520adapt%250Avarious%2520image%2520characteristics%2520is%2520a%2520great%2520challenge.%2520Existing%2520contextual%250Amodeling%2520methods%2520for%2520scene%2520recognition%2520exhibit%2520two%2520limitations%253A%25201%2529%2520They%250Atypically%2520model%2520only%2520one%2520type%2520of%2520spatial%2520relationship%2520%2528order%2520or%2520metric%2529%2520among%250Aobjects%2520within%2520scenes%252C%2520with%2520limited%2520exploration%2520of%2520diverse%2520spatial%2520layouts.%25202%2529%250AThey%2520often%2520overlook%2520the%2520differences%2520in%2520coexisting%2520objects%2520across%2520different%250Ascenes%252C%2520suppressing%2520scene%2520recognition%2520performance.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520SpaCoNet%252C%2520which%2520simultaneously%2520models%2520Spatial%2520relation%250Aand%2520Co-occurrence%2520of%2520objects%2520guided%2520by%2520semantic%2520segmentation.%2520Firstly%252C%2520the%250ASemantic%2520Spatial%2520Relation%2520Module%2520%2528SSRM%2529%2520is%2520constructed%2520to%2520model%2520scene%2520spatial%250Afeatures.%2520With%2520the%2520help%2520of%2520semantic%2520segmentation%252C%2520this%2520module%2520decouples%2520spatial%250Ainformation%2520from%2520the%2520scene%2520image%2520and%2520thoroughly%2520explores%2520all%2520spatial%250Arelationships%2520among%2520objects%2520in%2520an%2520end-to-end%2520manner%252C%2520thereby%2520obtaining%250Asemantic-based%2520spatial%2520features.%2520Secondly%252C%2520both%2520spatial%2520features%2520from%2520the%2520SSRM%250Aand%2520deep%2520features%2520from%2520the%2520Image%2520Feature%2520Extraction%2520Module%2520are%2520allocated%2520to%250Aeach%2520object%252C%2520so%2520as%2520to%2520distinguish%2520the%2520coexisting%2520object%2520across%2520different%250Ascenes.%2520Finally%252C%2520utilizing%2520the%2520discriminative%2520features%2520above%252C%2520we%2520design%2520a%250AGlobal-Local%2520Dependency%2520Module%2520to%2520explore%2520the%2520long-range%2520co-occurrence%2520among%250Aobjects%252C%2520and%2520further%2520generate%2520a%2520semantic-guided%2520feature%2520representation%2520for%250Aindoor%2520scene%2520recognition.%2520Experimental%2520results%2520on%2520three%2520widely%2520used%2520scene%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520and%2520generality%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12661v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-guided%20modeling%20of%20spatial%20relation%20and%20object%20co-occurrence%0A%20%20for%20indoor%20scene%20recognition&entry.906535625=Chuanxin%20Song%20and%20Hanbo%20Wu%20and%20Xin%20Ma&entry.1292438233=%20%20Exploring%20the%20semantic%20context%20in%20scene%20images%20is%20essential%20for%20indoor%20scene%0Arecognition.%20However%2C%20due%20to%20the%20diverse%20intra-class%20spatial%20layouts%20and%20the%0Acoexisting%20inter-class%20objects%2C%20modeling%20contextual%20relationships%20to%20adapt%0Avarious%20image%20characteristics%20is%20a%20great%20challenge.%20Existing%20contextual%0Amodeling%20methods%20for%20scene%20recognition%20exhibit%20two%20limitations%3A%201%29%20They%0Atypically%20model%20only%20one%20type%20of%20spatial%20relationship%20%28order%20or%20metric%29%20among%0Aobjects%20within%20scenes%2C%20with%20limited%20exploration%20of%20diverse%20spatial%20layouts.%202%29%0AThey%20often%20overlook%20the%20differences%20in%20coexisting%20objects%20across%20different%0Ascenes%2C%20suppressing%20scene%20recognition%20performance.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20SpaCoNet%2C%20which%20simultaneously%20models%20Spatial%20relation%0Aand%20Co-occurrence%20of%20objects%20guided%20by%20semantic%20segmentation.%20Firstly%2C%20the%0ASemantic%20Spatial%20Relation%20Module%20%28SSRM%29%20is%20constructed%20to%20model%20scene%20spatial%0Afeatures.%20With%20the%20help%20of%20semantic%20segmentation%2C%20this%20module%20decouples%20spatial%0Ainformation%20from%20the%20scene%20image%20and%20thoroughly%20explores%20all%20spatial%0Arelationships%20among%20objects%20in%20an%20end-to-end%20manner%2C%20thereby%20obtaining%0Asemantic-based%20spatial%20features.%20Secondly%2C%20both%20spatial%20features%20from%20the%20SSRM%0Aand%20deep%20features%20from%20the%20Image%20Feature%20Extraction%20Module%20are%20allocated%20to%0Aeach%20object%2C%20so%20as%20to%20distinguish%20the%20coexisting%20object%20across%20different%0Ascenes.%20Finally%2C%20utilizing%20the%20discriminative%20features%20above%2C%20we%20design%20a%0AGlobal-Local%20Dependency%20Module%20to%20explore%20the%20long-range%20co-occurrence%20among%0Aobjects%2C%20and%20further%20generate%20a%20semantic-guided%20feature%20representation%20for%0Aindoor%20scene%20recognition.%20Experimental%20results%20on%20three%20widely%20used%20scene%0Adatasets%20demonstrate%20the%20effectiveness%20and%20generality%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12661v4&entry.124074799=Read"},
{"title": "Achieving Human Level Competitive Robot Table Tennis", "author": "David B. D'Ambrosio and Saminda Abeyruwan and Laura Graesser and Atil Iscen and Heni Ben Amor and Alex Bewley and Barney J. Reed and Krista Reymann and Leila Takayama and Yuval Tassa and Krzysztof Choromanski and Erwin Coumans and Deepali Jain and Navdeep Jaitly and Natasha Jaques and Satoshi Kataoka and Yuheng Kuang and Nevena Lazic and Reza Mahjourian and Sherry Moore and Kenneth Oslund and Anish Shankar and Vikas Sindhwani and Vincent Vanhoucke and Grace Vesom and Peng Xu and Pannag R. Sanketi", "abstract": "  Achieving human-level speed and performance on real world tasks is a north\nstar for the robotics research community. This work takes a step towards that\ngoal and presents the first learned robot agent that reaches amateur\nhuman-level performance in competitive table tennis. Table tennis is a\nphysically demanding sport which requires human players to undergo years of\ntraining to achieve an advanced level of proficiency. In this paper, we\ncontribute (1) a hierarchical and modular policy architecture consisting of (i)\nlow level controllers with their detailed skill descriptors which model the\nagent's capabilities and help to bridge the sim-to-real gap and (ii) a high\nlevel controller that chooses the low level skills, (2) techniques for enabling\nzero-shot sim-to-real including an iterative approach to defining the task\ndistribution that is grounded in the real-world and defines an automatic\ncurriculum, and (3) real time adaptation to unseen opponents. Policy\nperformance was assessed through 29 robot vs. human matches of which the robot\nwon 45% (13/29). All humans were unseen players and their skill level varied\nfrom beginner to tournament level. Whilst the robot lost all matches vs. the\nmost advanced players it won 100% matches vs. beginners and 55% matches vs.\nintermediate players, demonstrating solidly amateur human-level performance.\nVideos of the matches can be viewed at\nhttps://sites.google.com/view/competitive-robot-table-tennis\n", "link": "http://arxiv.org/abs/2408.03906v1", "date": "2024-08-07", "relevancy": 1.6664, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5638}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5572}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Achieving%20Human%20Level%20Competitive%20Robot%20Table%20Tennis&body=Title%3A%20Achieving%20Human%20Level%20Competitive%20Robot%20Table%20Tennis%0AAuthor%3A%20David%20B.%20D%27Ambrosio%20and%20Saminda%20Abeyruwan%20and%20Laura%20Graesser%20and%20Atil%20Iscen%20and%20Heni%20Ben%20Amor%20and%20Alex%20Bewley%20and%20Barney%20J.%20Reed%20and%20Krista%20Reymann%20and%20Leila%20Takayama%20and%20Yuval%20Tassa%20and%20Krzysztof%20Choromanski%20and%20Erwin%20Coumans%20and%20Deepali%20Jain%20and%20Navdeep%20Jaitly%20and%20Natasha%20Jaques%20and%20Satoshi%20Kataoka%20and%20Yuheng%20Kuang%20and%20Nevena%20Lazic%20and%20Reza%20Mahjourian%20and%20Sherry%20Moore%20and%20Kenneth%20Oslund%20and%20Anish%20Shankar%20and%20Vikas%20Sindhwani%20and%20Vincent%20Vanhoucke%20and%20Grace%20Vesom%20and%20Peng%20Xu%20and%20Pannag%20R.%20Sanketi%0AAbstract%3A%20%20%20Achieving%20human-level%20speed%20and%20performance%20on%20real%20world%20tasks%20is%20a%20north%0Astar%20for%20the%20robotics%20research%20community.%20This%20work%20takes%20a%20step%20towards%20that%0Agoal%20and%20presents%20the%20first%20learned%20robot%20agent%20that%20reaches%20amateur%0Ahuman-level%20performance%20in%20competitive%20table%20tennis.%20Table%20tennis%20is%20a%0Aphysically%20demanding%20sport%20which%20requires%20human%20players%20to%20undergo%20years%20of%0Atraining%20to%20achieve%20an%20advanced%20level%20of%20proficiency.%20In%20this%20paper%2C%20we%0Acontribute%20%281%29%20a%20hierarchical%20and%20modular%20policy%20architecture%20consisting%20of%20%28i%29%0Alow%20level%20controllers%20with%20their%20detailed%20skill%20descriptors%20which%20model%20the%0Aagent%27s%20capabilities%20and%20help%20to%20bridge%20the%20sim-to-real%20gap%20and%20%28ii%29%20a%20high%0Alevel%20controller%20that%20chooses%20the%20low%20level%20skills%2C%20%282%29%20techniques%20for%20enabling%0Azero-shot%20sim-to-real%20including%20an%20iterative%20approach%20to%20defining%20the%20task%0Adistribution%20that%20is%20grounded%20in%20the%20real-world%20and%20defines%20an%20automatic%0Acurriculum%2C%20and%20%283%29%20real%20time%20adaptation%20to%20unseen%20opponents.%20Policy%0Aperformance%20was%20assessed%20through%2029%20robot%20vs.%20human%20matches%20of%20which%20the%20robot%0Awon%2045%25%20%2813/29%29.%20All%20humans%20were%20unseen%20players%20and%20their%20skill%20level%20varied%0Afrom%20beginner%20to%20tournament%20level.%20Whilst%20the%20robot%20lost%20all%20matches%20vs.%20the%0Amost%20advanced%20players%20it%20won%20100%25%20matches%20vs.%20beginners%20and%2055%25%20matches%20vs.%0Aintermediate%20players%2C%20demonstrating%20solidly%20amateur%20human-level%20performance.%0AVideos%20of%20the%20matches%20can%20be%20viewed%20at%0Ahttps%3A//sites.google.com/view/competitive-robot-table-tennis%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAchieving%2520Human%2520Level%2520Competitive%2520Robot%2520Table%2520Tennis%26entry.906535625%3DDavid%2520B.%2520D%2527Ambrosio%2520and%2520Saminda%2520Abeyruwan%2520and%2520Laura%2520Graesser%2520and%2520Atil%2520Iscen%2520and%2520Heni%2520Ben%2520Amor%2520and%2520Alex%2520Bewley%2520and%2520Barney%2520J.%2520Reed%2520and%2520Krista%2520Reymann%2520and%2520Leila%2520Takayama%2520and%2520Yuval%2520Tassa%2520and%2520Krzysztof%2520Choromanski%2520and%2520Erwin%2520Coumans%2520and%2520Deepali%2520Jain%2520and%2520Navdeep%2520Jaitly%2520and%2520Natasha%2520Jaques%2520and%2520Satoshi%2520Kataoka%2520and%2520Yuheng%2520Kuang%2520and%2520Nevena%2520Lazic%2520and%2520Reza%2520Mahjourian%2520and%2520Sherry%2520Moore%2520and%2520Kenneth%2520Oslund%2520and%2520Anish%2520Shankar%2520and%2520Vikas%2520Sindhwani%2520and%2520Vincent%2520Vanhoucke%2520and%2520Grace%2520Vesom%2520and%2520Peng%2520Xu%2520and%2520Pannag%2520R.%2520Sanketi%26entry.1292438233%3D%2520%2520Achieving%2520human-level%2520speed%2520and%2520performance%2520on%2520real%2520world%2520tasks%2520is%2520a%2520north%250Astar%2520for%2520the%2520robotics%2520research%2520community.%2520This%2520work%2520takes%2520a%2520step%2520towards%2520that%250Agoal%2520and%2520presents%2520the%2520first%2520learned%2520robot%2520agent%2520that%2520reaches%2520amateur%250Ahuman-level%2520performance%2520in%2520competitive%2520table%2520tennis.%2520Table%2520tennis%2520is%2520a%250Aphysically%2520demanding%2520sport%2520which%2520requires%2520human%2520players%2520to%2520undergo%2520years%2520of%250Atraining%2520to%2520achieve%2520an%2520advanced%2520level%2520of%2520proficiency.%2520In%2520this%2520paper%252C%2520we%250Acontribute%2520%25281%2529%2520a%2520hierarchical%2520and%2520modular%2520policy%2520architecture%2520consisting%2520of%2520%2528i%2529%250Alow%2520level%2520controllers%2520with%2520their%2520detailed%2520skill%2520descriptors%2520which%2520model%2520the%250Aagent%2527s%2520capabilities%2520and%2520help%2520to%2520bridge%2520the%2520sim-to-real%2520gap%2520and%2520%2528ii%2529%2520a%2520high%250Alevel%2520controller%2520that%2520chooses%2520the%2520low%2520level%2520skills%252C%2520%25282%2529%2520techniques%2520for%2520enabling%250Azero-shot%2520sim-to-real%2520including%2520an%2520iterative%2520approach%2520to%2520defining%2520the%2520task%250Adistribution%2520that%2520is%2520grounded%2520in%2520the%2520real-world%2520and%2520defines%2520an%2520automatic%250Acurriculum%252C%2520and%2520%25283%2529%2520real%2520time%2520adaptation%2520to%2520unseen%2520opponents.%2520Policy%250Aperformance%2520was%2520assessed%2520through%252029%2520robot%2520vs.%2520human%2520matches%2520of%2520which%2520the%2520robot%250Awon%252045%2525%2520%252813/29%2529.%2520All%2520humans%2520were%2520unseen%2520players%2520and%2520their%2520skill%2520level%2520varied%250Afrom%2520beginner%2520to%2520tournament%2520level.%2520Whilst%2520the%2520robot%2520lost%2520all%2520matches%2520vs.%2520the%250Amost%2520advanced%2520players%2520it%2520won%2520100%2525%2520matches%2520vs.%2520beginners%2520and%252055%2525%2520matches%2520vs.%250Aintermediate%2520players%252C%2520demonstrating%2520solidly%2520amateur%2520human-level%2520performance.%250AVideos%2520of%2520the%2520matches%2520can%2520be%2520viewed%2520at%250Ahttps%253A//sites.google.com/view/competitive-robot-table-tennis%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%20Human%20Level%20Competitive%20Robot%20Table%20Tennis&entry.906535625=David%20B.%20D%27Ambrosio%20and%20Saminda%20Abeyruwan%20and%20Laura%20Graesser%20and%20Atil%20Iscen%20and%20Heni%20Ben%20Amor%20and%20Alex%20Bewley%20and%20Barney%20J.%20Reed%20and%20Krista%20Reymann%20and%20Leila%20Takayama%20and%20Yuval%20Tassa%20and%20Krzysztof%20Choromanski%20and%20Erwin%20Coumans%20and%20Deepali%20Jain%20and%20Navdeep%20Jaitly%20and%20Natasha%20Jaques%20and%20Satoshi%20Kataoka%20and%20Yuheng%20Kuang%20and%20Nevena%20Lazic%20and%20Reza%20Mahjourian%20and%20Sherry%20Moore%20and%20Kenneth%20Oslund%20and%20Anish%20Shankar%20and%20Vikas%20Sindhwani%20and%20Vincent%20Vanhoucke%20and%20Grace%20Vesom%20and%20Peng%20Xu%20and%20Pannag%20R.%20Sanketi&entry.1292438233=%20%20Achieving%20human-level%20speed%20and%20performance%20on%20real%20world%20tasks%20is%20a%20north%0Astar%20for%20the%20robotics%20research%20community.%20This%20work%20takes%20a%20step%20towards%20that%0Agoal%20and%20presents%20the%20first%20learned%20robot%20agent%20that%20reaches%20amateur%0Ahuman-level%20performance%20in%20competitive%20table%20tennis.%20Table%20tennis%20is%20a%0Aphysically%20demanding%20sport%20which%20requires%20human%20players%20to%20undergo%20years%20of%0Atraining%20to%20achieve%20an%20advanced%20level%20of%20proficiency.%20In%20this%20paper%2C%20we%0Acontribute%20%281%29%20a%20hierarchical%20and%20modular%20policy%20architecture%20consisting%20of%20%28i%29%0Alow%20level%20controllers%20with%20their%20detailed%20skill%20descriptors%20which%20model%20the%0Aagent%27s%20capabilities%20and%20help%20to%20bridge%20the%20sim-to-real%20gap%20and%20%28ii%29%20a%20high%0Alevel%20controller%20that%20chooses%20the%20low%20level%20skills%2C%20%282%29%20techniques%20for%20enabling%0Azero-shot%20sim-to-real%20including%20an%20iterative%20approach%20to%20defining%20the%20task%0Adistribution%20that%20is%20grounded%20in%20the%20real-world%20and%20defines%20an%20automatic%0Acurriculum%2C%20and%20%283%29%20real%20time%20adaptation%20to%20unseen%20opponents.%20Policy%0Aperformance%20was%20assessed%20through%2029%20robot%20vs.%20human%20matches%20of%20which%20the%20robot%0Awon%2045%25%20%2813/29%29.%20All%20humans%20were%20unseen%20players%20and%20their%20skill%20level%20varied%0Afrom%20beginner%20to%20tournament%20level.%20Whilst%20the%20robot%20lost%20all%20matches%20vs.%20the%0Amost%20advanced%20players%20it%20won%20100%25%20matches%20vs.%20beginners%20and%2055%25%20matches%20vs.%0Aintermediate%20players%2C%20demonstrating%20solidly%20amateur%20human-level%20performance.%0AVideos%20of%20the%20matches%20can%20be%20viewed%20at%0Ahttps%3A//sites.google.com/view/competitive-robot-table-tennis%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03906v1&entry.124074799=Read"},
{"title": "A Backbone for Long-Horizon Robot Task Understanding", "author": "Xiaoshuai Chen and Wei Chen and Dongmyoung Lee and Yukun Ge and Nicolas Rojas and Petar Kormushev", "abstract": "  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n", "link": "http://arxiv.org/abs/2408.01334v2", "date": "2024-08-07", "relevancy": 1.6644, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5816}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5668}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Backbone%20for%20Long-Horizon%20Robot%20Task%20Understanding&body=Title%3A%20A%20Backbone%20for%20Long-Horizon%20Robot%20Task%20Understanding%0AAuthor%3A%20Xiaoshuai%20Chen%20and%20Wei%20Chen%20and%20Dongmyoung%20Lee%20and%20Yukun%20Ge%20and%20Nicolas%20Rojas%20and%20Petar%20Kormushev%0AAbstract%3A%20%20%20End-to-end%20robot%20learning%2C%20particularly%20for%20long-horizon%20tasks%2C%20often%20results%0Ain%20unpredictable%20outcomes%20and%20poor%20generalization.%20To%20address%20these%20challenges%2C%0Awe%20propose%20a%20novel%20Therblig-based%20Backbone%20Framework%20%28TBBF%29%20to%20enhance%20robot%0Atask%20understanding%20and%20transferability.%20This%20framework%20uses%20therbligs%20%28basic%0Aaction%20elements%29%20as%20the%20backbone%20to%20decompose%20high-level%20robot%20tasks%20into%0Aelemental%20robot%20configurations%2C%20which%20are%20then%20integrated%20with%20current%0Afoundation%20models%20to%20improve%20task%20understanding.%20The%20approach%20consists%20of%20two%0Astages%3A%20offline%20training%20and%20online%20testing.%20During%20the%20offline%20training%20stage%2C%0Awe%20developed%20the%20Meta-RGate%20SynerFusion%20%28MGSF%29%20network%20for%20accurate%20therblig%0Asegmentation%20across%20various%20tasks.%20In%20the%20online%20testing%20stage%2C%20after%20a%0Aone-shot%20demonstration%20of%20a%20new%20task%20is%20collected%2C%20our%20MGSF%20network%20extracts%0Ahigh-level%20knowledge%2C%20which%20is%20then%20encoded%20into%20the%20image%20using%20Action%0ARegistration%20%28ActionREG%29.%20Additionally%2C%20the%20Large%20Language%20Model%0A%28LLM%29-Alignment%20Policy%20for%20Visual%20Correction%20%28LAP-VC%29%20is%20employed%20to%20ensure%0Aprecise%20action%20execution%2C%20facilitating%20trajectory%20transfer%20in%20novel%20robot%0Ascenarios.%20Experimental%20results%20validate%20these%20methods%2C%20achieving%2094.37%25%20recall%0Ain%20therblig%20segmentation%20and%20success%20rates%20of%2094.4%25%20and%2080%25%20in%20real-world%0Aonline%20robot%20testing%20for%20simple%20and%20complex%20scenarios%2C%20respectively.%0ASupplementary%20material%20is%20available%20at%3A%0Ahttps%3A//sites.google.com/view/therbligsbasedbackbone/home%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Backbone%2520for%2520Long-Horizon%2520Robot%2520Task%2520Understanding%26entry.906535625%3DXiaoshuai%2520Chen%2520and%2520Wei%2520Chen%2520and%2520Dongmyoung%2520Lee%2520and%2520Yukun%2520Ge%2520and%2520Nicolas%2520Rojas%2520and%2520Petar%2520Kormushev%26entry.1292438233%3D%2520%2520End-to-end%2520robot%2520learning%252C%2520particularly%2520for%2520long-horizon%2520tasks%252C%2520often%2520results%250Ain%2520unpredictable%2520outcomes%2520and%2520poor%2520generalization.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520a%2520novel%2520Therblig-based%2520Backbone%2520Framework%2520%2528TBBF%2529%2520to%2520enhance%2520robot%250Atask%2520understanding%2520and%2520transferability.%2520This%2520framework%2520uses%2520therbligs%2520%2528basic%250Aaction%2520elements%2529%2520as%2520the%2520backbone%2520to%2520decompose%2520high-level%2520robot%2520tasks%2520into%250Aelemental%2520robot%2520configurations%252C%2520which%2520are%2520then%2520integrated%2520with%2520current%250Afoundation%2520models%2520to%2520improve%2520task%2520understanding.%2520The%2520approach%2520consists%2520of%2520two%250Astages%253A%2520offline%2520training%2520and%2520online%2520testing.%2520During%2520the%2520offline%2520training%2520stage%252C%250Awe%2520developed%2520the%2520Meta-RGate%2520SynerFusion%2520%2528MGSF%2529%2520network%2520for%2520accurate%2520therblig%250Asegmentation%2520across%2520various%2520tasks.%2520In%2520the%2520online%2520testing%2520stage%252C%2520after%2520a%250Aone-shot%2520demonstration%2520of%2520a%2520new%2520task%2520is%2520collected%252C%2520our%2520MGSF%2520network%2520extracts%250Ahigh-level%2520knowledge%252C%2520which%2520is%2520then%2520encoded%2520into%2520the%2520image%2520using%2520Action%250ARegistration%2520%2528ActionREG%2529.%2520Additionally%252C%2520the%2520Large%2520Language%2520Model%250A%2528LLM%2529-Alignment%2520Policy%2520for%2520Visual%2520Correction%2520%2528LAP-VC%2529%2520is%2520employed%2520to%2520ensure%250Aprecise%2520action%2520execution%252C%2520facilitating%2520trajectory%2520transfer%2520in%2520novel%2520robot%250Ascenarios.%2520Experimental%2520results%2520validate%2520these%2520methods%252C%2520achieving%252094.37%2525%2520recall%250Ain%2520therblig%2520segmentation%2520and%2520success%2520rates%2520of%252094.4%2525%2520and%252080%2525%2520in%2520real-world%250Aonline%2520robot%2520testing%2520for%2520simple%2520and%2520complex%2520scenarios%252C%2520respectively.%250ASupplementary%2520material%2520is%2520available%2520at%253A%250Ahttps%253A//sites.google.com/view/therbligsbasedbackbone/home%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Backbone%20for%20Long-Horizon%20Robot%20Task%20Understanding&entry.906535625=Xiaoshuai%20Chen%20and%20Wei%20Chen%20and%20Dongmyoung%20Lee%20and%20Yukun%20Ge%20and%20Nicolas%20Rojas%20and%20Petar%20Kormushev&entry.1292438233=%20%20End-to-end%20robot%20learning%2C%20particularly%20for%20long-horizon%20tasks%2C%20often%20results%0Ain%20unpredictable%20outcomes%20and%20poor%20generalization.%20To%20address%20these%20challenges%2C%0Awe%20propose%20a%20novel%20Therblig-based%20Backbone%20Framework%20%28TBBF%29%20to%20enhance%20robot%0Atask%20understanding%20and%20transferability.%20This%20framework%20uses%20therbligs%20%28basic%0Aaction%20elements%29%20as%20the%20backbone%20to%20decompose%20high-level%20robot%20tasks%20into%0Aelemental%20robot%20configurations%2C%20which%20are%20then%20integrated%20with%20current%0Afoundation%20models%20to%20improve%20task%20understanding.%20The%20approach%20consists%20of%20two%0Astages%3A%20offline%20training%20and%20online%20testing.%20During%20the%20offline%20training%20stage%2C%0Awe%20developed%20the%20Meta-RGate%20SynerFusion%20%28MGSF%29%20network%20for%20accurate%20therblig%0Asegmentation%20across%20various%20tasks.%20In%20the%20online%20testing%20stage%2C%20after%20a%0Aone-shot%20demonstration%20of%20a%20new%20task%20is%20collected%2C%20our%20MGSF%20network%20extracts%0Ahigh-level%20knowledge%2C%20which%20is%20then%20encoded%20into%20the%20image%20using%20Action%0ARegistration%20%28ActionREG%29.%20Additionally%2C%20the%20Large%20Language%20Model%0A%28LLM%29-Alignment%20Policy%20for%20Visual%20Correction%20%28LAP-VC%29%20is%20employed%20to%20ensure%0Aprecise%20action%20execution%2C%20facilitating%20trajectory%20transfer%20in%20novel%20robot%0Ascenarios.%20Experimental%20results%20validate%20these%20methods%2C%20achieving%2094.37%25%20recall%0Ain%20therblig%20segmentation%20and%20success%20rates%20of%2094.4%25%20and%2080%25%20in%20real-world%0Aonline%20robot%20testing%20for%20simple%20and%20complex%20scenarios%2C%20respectively.%0ASupplementary%20material%20is%20available%20at%3A%0Ahttps%3A//sites.google.com/view/therbligsbasedbackbone/home%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01334v2&entry.124074799=Read"},
{"title": "Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation", "author": "Jingjing Xie and Yuxin Zhang and Mingbao Lin and Liujuan Cao and Rongrong Ji", "abstract": "  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n", "link": "http://arxiv.org/abs/2408.03735v1", "date": "2024-08-07", "relevancy": 1.653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5708}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Multimodal%20Large%20Language%20Models%20with%20Quantization-Aware%20Scale%0A%20%20Learning%20for%20Efficient%20Adaptation&body=Title%3A%20Advancing%20Multimodal%20Large%20Language%20Models%20with%20Quantization-Aware%20Scale%0A%20%20Learning%20for%20Efficient%20Adaptation%0AAuthor%3A%20Jingjing%20Xie%20and%20Yuxin%20Zhang%20and%20Mingbao%20Lin%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20first%20study%20to%20explore%20the%20potential%20of%20parameter%0Aquantization%20for%20multimodal%20large%20language%20models%20to%20alleviate%20the%20significant%0Aresource%20constraint%20encountered%20during%20vision-language%20instruction%20tuning.%20We%0Aintroduce%20a%20Quantization-aware%20Scale%20LeArning%20method%20based%20on%20multimodal%0AWarmup%2C%20termed%20QSLAW.%20This%20method%20is%20grounded%20in%20two%20key%20innovations%3A%20%281%29%20The%0Alearning%20of%20group-wise%20scale%20factors%20for%20quantized%20LLM%20weights%20to%20mitigate%20the%0Aquantization%20error%20arising%20from%20activation%20outliers%20and%20achieve%20more%20effective%0Avision-language%20instruction%20tuning%3B%20%282%29%20The%20implementation%20of%20a%20multimodal%0Awarmup%20that%20progressively%20integrates%20linguistic%20and%20multimodal%20training%0Asamples%2C%20thereby%20preventing%20overfitting%20of%20the%20quantized%20model%20to%20multimodal%0Adata%20while%20ensuring%20stable%20adaptation%20of%20multimodal%20large%20language%20models%20to%0Adownstream%20vision-language%20tasks.%20Extensive%20experiments%20demonstrate%20that%20models%0Aquantized%20by%20QSLAW%20perform%20on%20par%20with%2C%20or%20even%20surpass%2C%20their%20full-precision%0Acounterparts%2C%20while%20facilitating%20up%20to%201.4%20times%20reduction%20in%20VL%20tuning%20time%0Aand%20GPU%20consumption.%20Our%20code%20is%20released%20at%20https%3A//github.com/xjjxmu/QSLAW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Multimodal%2520Large%2520Language%2520Models%2520with%2520Quantization-Aware%2520Scale%250A%2520%2520Learning%2520for%2520Efficient%2520Adaptation%26entry.906535625%3DJingjing%2520Xie%2520and%2520Yuxin%2520Zhang%2520and%2520Mingbao%2520Lin%2520and%2520Liujuan%2520Cao%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520first%2520study%2520to%2520explore%2520the%2520potential%2520of%2520parameter%250Aquantization%2520for%2520multimodal%2520large%2520language%2520models%2520to%2520alleviate%2520the%2520significant%250Aresource%2520constraint%2520encountered%2520during%2520vision-language%2520instruction%2520tuning.%2520We%250Aintroduce%2520a%2520Quantization-aware%2520Scale%2520LeArning%2520method%2520based%2520on%2520multimodal%250AWarmup%252C%2520termed%2520QSLAW.%2520This%2520method%2520is%2520grounded%2520in%2520two%2520key%2520innovations%253A%2520%25281%2529%2520The%250Alearning%2520of%2520group-wise%2520scale%2520factors%2520for%2520quantized%2520LLM%2520weights%2520to%2520mitigate%2520the%250Aquantization%2520error%2520arising%2520from%2520activation%2520outliers%2520and%2520achieve%2520more%2520effective%250Avision-language%2520instruction%2520tuning%253B%2520%25282%2529%2520The%2520implementation%2520of%2520a%2520multimodal%250Awarmup%2520that%2520progressively%2520integrates%2520linguistic%2520and%2520multimodal%2520training%250Asamples%252C%2520thereby%2520preventing%2520overfitting%2520of%2520the%2520quantized%2520model%2520to%2520multimodal%250Adata%2520while%2520ensuring%2520stable%2520adaptation%2520of%2520multimodal%2520large%2520language%2520models%2520to%250Adownstream%2520vision-language%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520models%250Aquantized%2520by%2520QSLAW%2520perform%2520on%2520par%2520with%252C%2520or%2520even%2520surpass%252C%2520their%2520full-precision%250Acounterparts%252C%2520while%2520facilitating%2520up%2520to%25201.4%2520times%2520reduction%2520in%2520VL%2520tuning%2520time%250Aand%2520GPU%2520consumption.%2520Our%2520code%2520is%2520released%2520at%2520https%253A//github.com/xjjxmu/QSLAW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Multimodal%20Large%20Language%20Models%20with%20Quantization-Aware%20Scale%0A%20%20Learning%20for%20Efficient%20Adaptation&entry.906535625=Jingjing%20Xie%20and%20Yuxin%20Zhang%20and%20Mingbao%20Lin%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji&entry.1292438233=%20%20This%20paper%20presents%20the%20first%20study%20to%20explore%20the%20potential%20of%20parameter%0Aquantization%20for%20multimodal%20large%20language%20models%20to%20alleviate%20the%20significant%0Aresource%20constraint%20encountered%20during%20vision-language%20instruction%20tuning.%20We%0Aintroduce%20a%20Quantization-aware%20Scale%20LeArning%20method%20based%20on%20multimodal%0AWarmup%2C%20termed%20QSLAW.%20This%20method%20is%20grounded%20in%20two%20key%20innovations%3A%20%281%29%20The%0Alearning%20of%20group-wise%20scale%20factors%20for%20quantized%20LLM%20weights%20to%20mitigate%20the%0Aquantization%20error%20arising%20from%20activation%20outliers%20and%20achieve%20more%20effective%0Avision-language%20instruction%20tuning%3B%20%282%29%20The%20implementation%20of%20a%20multimodal%0Awarmup%20that%20progressively%20integrates%20linguistic%20and%20multimodal%20training%0Asamples%2C%20thereby%20preventing%20overfitting%20of%20the%20quantized%20model%20to%20multimodal%0Adata%20while%20ensuring%20stable%20adaptation%20of%20multimodal%20large%20language%20models%20to%0Adownstream%20vision-language%20tasks.%20Extensive%20experiments%20demonstrate%20that%20models%0Aquantized%20by%20QSLAW%20perform%20on%20par%20with%2C%20or%20even%20surpass%2C%20their%20full-precision%0Acounterparts%2C%20while%20facilitating%20up%20to%201.4%20times%20reduction%20in%20VL%20tuning%20time%0Aand%20GPU%20consumption.%20Our%20code%20is%20released%20at%20https%3A//github.com/xjjxmu/QSLAW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03735v1&entry.124074799=Read"},
{"title": "DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs", "author": "Donghyun Kim and Byeongho Heo and Dongyoon Han", "abstract": "  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n", "link": "http://arxiv.org/abs/2403.19588v2", "date": "2024-08-07", "relevancy": 1.6499, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5717}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.528}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenseNets%20Reloaded%3A%20Paradigm%20Shift%20Beyond%20ResNets%20and%20ViTs&body=Title%3A%20DenseNets%20Reloaded%3A%20Paradigm%20Shift%20Beyond%20ResNets%20and%20ViTs%0AAuthor%3A%20Donghyun%20Kim%20and%20Byeongho%20Heo%20and%20Dongyoon%20Han%0AAbstract%3A%20%20%20This%20paper%20revives%20Densely%20Connected%20Convolutional%20Networks%20%28DenseNets%29%20and%0Areveals%20the%20underrated%20effectiveness%20over%20predominant%20ResNet-style%0Aarchitectures.%20We%20believe%20DenseNets%27%20potential%20was%20overlooked%20due%20to%20untouched%0Atraining%20methods%20and%20traditional%20design%20elements%20not%20fully%20revealing%20their%0Acapabilities.%20Our%20pilot%20study%20shows%20dense%20connections%20through%20concatenation%20are%0Astrong%2C%20demonstrating%20that%20DenseNets%20can%20be%20revitalized%20to%20compete%20with%20modern%0Aarchitectures.%20We%20methodically%20refine%20suboptimal%20components%20-%20architectural%0Aadjustments%2C%20block%20redesign%2C%20and%20improved%20training%20recipes%20towards%20widening%0ADenseNets%20and%20boosting%20memory%20efficiency%20while%20keeping%20concatenation%20shortcuts.%0AOur%20models%2C%20employing%20simple%20architectural%20elements%2C%20ultimately%20surpass%20Swin%0ATransformer%2C%20ConvNeXt%2C%20and%20DeiT-III%20-%20key%20architectures%20in%20the%20residual%0Alearning%20lineage.%20Furthermore%2C%20our%20models%20exhibit%20near%20state-of-the-art%0Aperformance%20on%20ImageNet-1K%2C%20competing%20with%20the%20very%20recent%20models%20and%0Adownstream%20tasks%2C%20ADE20k%20semantic%20segmentation%2C%20and%20COCO%20object%0Adetection/instance%20segmentation.%20Finally%2C%20we%20provide%20empirical%20analyses%20that%0Auncover%20the%20merits%20of%20the%20concatenation%20over%20additive%20shortcuts%2C%20steering%20a%0Arenewed%20preference%20towards%20DenseNet-style%20designs.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/naver-ai/rdnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19588v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenseNets%2520Reloaded%253A%2520Paradigm%2520Shift%2520Beyond%2520ResNets%2520and%2520ViTs%26entry.906535625%3DDonghyun%2520Kim%2520and%2520Byeongho%2520Heo%2520and%2520Dongyoon%2520Han%26entry.1292438233%3D%2520%2520This%2520paper%2520revives%2520Densely%2520Connected%2520Convolutional%2520Networks%2520%2528DenseNets%2529%2520and%250Areveals%2520the%2520underrated%2520effectiveness%2520over%2520predominant%2520ResNet-style%250Aarchitectures.%2520We%2520believe%2520DenseNets%2527%2520potential%2520was%2520overlooked%2520due%2520to%2520untouched%250Atraining%2520methods%2520and%2520traditional%2520design%2520elements%2520not%2520fully%2520revealing%2520their%250Acapabilities.%2520Our%2520pilot%2520study%2520shows%2520dense%2520connections%2520through%2520concatenation%2520are%250Astrong%252C%2520demonstrating%2520that%2520DenseNets%2520can%2520be%2520revitalized%2520to%2520compete%2520with%2520modern%250Aarchitectures.%2520We%2520methodically%2520refine%2520suboptimal%2520components%2520-%2520architectural%250Aadjustments%252C%2520block%2520redesign%252C%2520and%2520improved%2520training%2520recipes%2520towards%2520widening%250ADenseNets%2520and%2520boosting%2520memory%2520efficiency%2520while%2520keeping%2520concatenation%2520shortcuts.%250AOur%2520models%252C%2520employing%2520simple%2520architectural%2520elements%252C%2520ultimately%2520surpass%2520Swin%250ATransformer%252C%2520ConvNeXt%252C%2520and%2520DeiT-III%2520-%2520key%2520architectures%2520in%2520the%2520residual%250Alearning%2520lineage.%2520Furthermore%252C%2520our%2520models%2520exhibit%2520near%2520state-of-the-art%250Aperformance%2520on%2520ImageNet-1K%252C%2520competing%2520with%2520the%2520very%2520recent%2520models%2520and%250Adownstream%2520tasks%252C%2520ADE20k%2520semantic%2520segmentation%252C%2520and%2520COCO%2520object%250Adetection/instance%2520segmentation.%2520Finally%252C%2520we%2520provide%2520empirical%2520analyses%2520that%250Auncover%2520the%2520merits%2520of%2520the%2520concatenation%2520over%2520additive%2520shortcuts%252C%2520steering%2520a%250Arenewed%2520preference%2520towards%2520DenseNet-style%2520designs.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/naver-ai/rdnet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19588v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenseNets%20Reloaded%3A%20Paradigm%20Shift%20Beyond%20ResNets%20and%20ViTs&entry.906535625=Donghyun%20Kim%20and%20Byeongho%20Heo%20and%20Dongyoon%20Han&entry.1292438233=%20%20This%20paper%20revives%20Densely%20Connected%20Convolutional%20Networks%20%28DenseNets%29%20and%0Areveals%20the%20underrated%20effectiveness%20over%20predominant%20ResNet-style%0Aarchitectures.%20We%20believe%20DenseNets%27%20potential%20was%20overlooked%20due%20to%20untouched%0Atraining%20methods%20and%20traditional%20design%20elements%20not%20fully%20revealing%20their%0Acapabilities.%20Our%20pilot%20study%20shows%20dense%20connections%20through%20concatenation%20are%0Astrong%2C%20demonstrating%20that%20DenseNets%20can%20be%20revitalized%20to%20compete%20with%20modern%0Aarchitectures.%20We%20methodically%20refine%20suboptimal%20components%20-%20architectural%0Aadjustments%2C%20block%20redesign%2C%20and%20improved%20training%20recipes%20towards%20widening%0ADenseNets%20and%20boosting%20memory%20efficiency%20while%20keeping%20concatenation%20shortcuts.%0AOur%20models%2C%20employing%20simple%20architectural%20elements%2C%20ultimately%20surpass%20Swin%0ATransformer%2C%20ConvNeXt%2C%20and%20DeiT-III%20-%20key%20architectures%20in%20the%20residual%0Alearning%20lineage.%20Furthermore%2C%20our%20models%20exhibit%20near%20state-of-the-art%0Aperformance%20on%20ImageNet-1K%2C%20competing%20with%20the%20very%20recent%20models%20and%0Adownstream%20tasks%2C%20ADE20k%20semantic%20segmentation%2C%20and%20COCO%20object%0Adetection/instance%20segmentation.%20Finally%2C%20we%20provide%20empirical%20analyses%20that%0Auncover%20the%20merits%20of%20the%20concatenation%20over%20additive%20shortcuts%2C%20steering%20a%0Arenewed%20preference%20towards%20DenseNet-style%20designs.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/naver-ai/rdnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19588v2&entry.124074799=Read"},
{"title": "ESP-MedSAM: Efficient Self-Prompting SAM for Universal Image\n  Segmentation", "author": "Qing Xu and Jiaxuan Li and Xiangjian He and Ziyu Liu and Zhen Chen and Wenting Duan and Chenxin Li and Maggie M. He and Fiseha B. Tesema and Wooi P. Cheah and Yi Wang and Rong Qu and Jonathan M. Garibaldi", "abstract": "  The Segment Anything Model (SAM) has demonstrated outstanding adaptation to\nmedical image segmentation but still faces three major challenges. Firstly, the\nhuge computational costs of SAM limit its real-world applicability. Secondly,\nSAM depends on manual annotations (e.g., points, boxes) as prompts, which are\nlaborious and impractical in clinical scenarios. Thirdly, SAM handles all\nsegmentation targets equally, which is suboptimal for diverse medical\nmodalities with inherent heterogeneity. To address these issues, we propose an\nEfficient Self-Prompting SAM for universal medical image segmentation, named\nESP-MedSAM. We devise a Multi-Modal Decoupled Knowledge Distillation (MMDKD)\nstrategy to distil common image knowledge and domain-specific medical knowledge\nfrom the foundation model to train a lightweight image encoder and a modality\ncontroller. Further, they combine with the additionally introduced Self-Patch\nPrompt Generator (SPPG) and Query-Decoupled Modality Decoder (QDMD) to\nconstruct ESP-MedSAM. Specifically, SPPG aims to generate a set of patch\nprompts automatically and QDMD leverages a one-to-one strategy to provide an\nindependent decoding channel for every modality. Extensive experiments indicate\nthat ESP-MedSAM outperforms state-of-the-arts in diverse medical imaging\nsegmentation takes, displaying superior zero-shot learning and modality\ntransfer ability. Especially, our framework uses only 31.4% parameters compared\nto SAM-Base.\n", "link": "http://arxiv.org/abs/2407.14153v2", "date": "2024-08-07", "relevancy": 1.6465, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5524}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5486}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESP-MedSAM%3A%20Efficient%20Self-Prompting%20SAM%20for%20Universal%20Image%0A%20%20Segmentation&body=Title%3A%20ESP-MedSAM%3A%20Efficient%20Self-Prompting%20SAM%20for%20Universal%20Image%0A%20%20Segmentation%0AAuthor%3A%20Qing%20Xu%20and%20Jiaxuan%20Li%20and%20Xiangjian%20He%20and%20Ziyu%20Liu%20and%20Zhen%20Chen%20and%20Wenting%20Duan%20and%20Chenxin%20Li%20and%20Maggie%20M.%20He%20and%20Fiseha%20B.%20Tesema%20and%20Wooi%20P.%20Cheah%20and%20Yi%20Wang%20and%20Rong%20Qu%20and%20Jonathan%20M.%20Garibaldi%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20demonstrated%20outstanding%20adaptation%20to%0Amedical%20image%20segmentation%20but%20still%20faces%20three%20major%20challenges.%20Firstly%2C%20the%0Ahuge%20computational%20costs%20of%20SAM%20limit%20its%20real-world%20applicability.%20Secondly%2C%0ASAM%20depends%20on%20manual%20annotations%20%28e.g.%2C%20points%2C%20boxes%29%20as%20prompts%2C%20which%20are%0Alaborious%20and%20impractical%20in%20clinical%20scenarios.%20Thirdly%2C%20SAM%20handles%20all%0Asegmentation%20targets%20equally%2C%20which%20is%20suboptimal%20for%20diverse%20medical%0Amodalities%20with%20inherent%20heterogeneity.%20To%20address%20these%20issues%2C%20we%20propose%20an%0AEfficient%20Self-Prompting%20SAM%20for%20universal%20medical%20image%20segmentation%2C%20named%0AESP-MedSAM.%20We%20devise%20a%20Multi-Modal%20Decoupled%20Knowledge%20Distillation%20%28MMDKD%29%0Astrategy%20to%20distil%20common%20image%20knowledge%20and%20domain-specific%20medical%20knowledge%0Afrom%20the%20foundation%20model%20to%20train%20a%20lightweight%20image%20encoder%20and%20a%20modality%0Acontroller.%20Further%2C%20they%20combine%20with%20the%20additionally%20introduced%20Self-Patch%0APrompt%20Generator%20%28SPPG%29%20and%20Query-Decoupled%20Modality%20Decoder%20%28QDMD%29%20to%0Aconstruct%20ESP-MedSAM.%20Specifically%2C%20SPPG%20aims%20to%20generate%20a%20set%20of%20patch%0Aprompts%20automatically%20and%20QDMD%20leverages%20a%20one-to-one%20strategy%20to%20provide%20an%0Aindependent%20decoding%20channel%20for%20every%20modality.%20Extensive%20experiments%20indicate%0Athat%20ESP-MedSAM%20outperforms%20state-of-the-arts%20in%20diverse%20medical%20imaging%0Asegmentation%20takes%2C%20displaying%20superior%20zero-shot%20learning%20and%20modality%0Atransfer%20ability.%20Especially%2C%20our%20framework%20uses%20only%2031.4%25%20parameters%20compared%0Ato%20SAM-Base.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESP-MedSAM%253A%2520Efficient%2520Self-Prompting%2520SAM%2520for%2520Universal%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DQing%2520Xu%2520and%2520Jiaxuan%2520Li%2520and%2520Xiangjian%2520He%2520and%2520Ziyu%2520Liu%2520and%2520Zhen%2520Chen%2520and%2520Wenting%2520Duan%2520and%2520Chenxin%2520Li%2520and%2520Maggie%2520M.%2520He%2520and%2520Fiseha%2520B.%2520Tesema%2520and%2520Wooi%2520P.%2520Cheah%2520and%2520Yi%2520Wang%2520and%2520Rong%2520Qu%2520and%2520Jonathan%2520M.%2520Garibaldi%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520demonstrated%2520outstanding%2520adaptation%2520to%250Amedical%2520image%2520segmentation%2520but%2520still%2520faces%2520three%2520major%2520challenges.%2520Firstly%252C%2520the%250Ahuge%2520computational%2520costs%2520of%2520SAM%2520limit%2520its%2520real-world%2520applicability.%2520Secondly%252C%250ASAM%2520depends%2520on%2520manual%2520annotations%2520%2528e.g.%252C%2520points%252C%2520boxes%2529%2520as%2520prompts%252C%2520which%2520are%250Alaborious%2520and%2520impractical%2520in%2520clinical%2520scenarios.%2520Thirdly%252C%2520SAM%2520handles%2520all%250Asegmentation%2520targets%2520equally%252C%2520which%2520is%2520suboptimal%2520for%2520diverse%2520medical%250Amodalities%2520with%2520inherent%2520heterogeneity.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520an%250AEfficient%2520Self-Prompting%2520SAM%2520for%2520universal%2520medical%2520image%2520segmentation%252C%2520named%250AESP-MedSAM.%2520We%2520devise%2520a%2520Multi-Modal%2520Decoupled%2520Knowledge%2520Distillation%2520%2528MMDKD%2529%250Astrategy%2520to%2520distil%2520common%2520image%2520knowledge%2520and%2520domain-specific%2520medical%2520knowledge%250Afrom%2520the%2520foundation%2520model%2520to%2520train%2520a%2520lightweight%2520image%2520encoder%2520and%2520a%2520modality%250Acontroller.%2520Further%252C%2520they%2520combine%2520with%2520the%2520additionally%2520introduced%2520Self-Patch%250APrompt%2520Generator%2520%2528SPPG%2529%2520and%2520Query-Decoupled%2520Modality%2520Decoder%2520%2528QDMD%2529%2520to%250Aconstruct%2520ESP-MedSAM.%2520Specifically%252C%2520SPPG%2520aims%2520to%2520generate%2520a%2520set%2520of%2520patch%250Aprompts%2520automatically%2520and%2520QDMD%2520leverages%2520a%2520one-to-one%2520strategy%2520to%2520provide%2520an%250Aindependent%2520decoding%2520channel%2520for%2520every%2520modality.%2520Extensive%2520experiments%2520indicate%250Athat%2520ESP-MedSAM%2520outperforms%2520state-of-the-arts%2520in%2520diverse%2520medical%2520imaging%250Asegmentation%2520takes%252C%2520displaying%2520superior%2520zero-shot%2520learning%2520and%2520modality%250Atransfer%2520ability.%2520Especially%252C%2520our%2520framework%2520uses%2520only%252031.4%2525%2520parameters%2520compared%250Ato%2520SAM-Base.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESP-MedSAM%3A%20Efficient%20Self-Prompting%20SAM%20for%20Universal%20Image%0A%20%20Segmentation&entry.906535625=Qing%20Xu%20and%20Jiaxuan%20Li%20and%20Xiangjian%20He%20and%20Ziyu%20Liu%20and%20Zhen%20Chen%20and%20Wenting%20Duan%20and%20Chenxin%20Li%20and%20Maggie%20M.%20He%20and%20Fiseha%20B.%20Tesema%20and%20Wooi%20P.%20Cheah%20and%20Yi%20Wang%20and%20Rong%20Qu%20and%20Jonathan%20M.%20Garibaldi&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20demonstrated%20outstanding%20adaptation%20to%0Amedical%20image%20segmentation%20but%20still%20faces%20three%20major%20challenges.%20Firstly%2C%20the%0Ahuge%20computational%20costs%20of%20SAM%20limit%20its%20real-world%20applicability.%20Secondly%2C%0ASAM%20depends%20on%20manual%20annotations%20%28e.g.%2C%20points%2C%20boxes%29%20as%20prompts%2C%20which%20are%0Alaborious%20and%20impractical%20in%20clinical%20scenarios.%20Thirdly%2C%20SAM%20handles%20all%0Asegmentation%20targets%20equally%2C%20which%20is%20suboptimal%20for%20diverse%20medical%0Amodalities%20with%20inherent%20heterogeneity.%20To%20address%20these%20issues%2C%20we%20propose%20an%0AEfficient%20Self-Prompting%20SAM%20for%20universal%20medical%20image%20segmentation%2C%20named%0AESP-MedSAM.%20We%20devise%20a%20Multi-Modal%20Decoupled%20Knowledge%20Distillation%20%28MMDKD%29%0Astrategy%20to%20distil%20common%20image%20knowledge%20and%20domain-specific%20medical%20knowledge%0Afrom%20the%20foundation%20model%20to%20train%20a%20lightweight%20image%20encoder%20and%20a%20modality%0Acontroller.%20Further%2C%20they%20combine%20with%20the%20additionally%20introduced%20Self-Patch%0APrompt%20Generator%20%28SPPG%29%20and%20Query-Decoupled%20Modality%20Decoder%20%28QDMD%29%20to%0Aconstruct%20ESP-MedSAM.%20Specifically%2C%20SPPG%20aims%20to%20generate%20a%20set%20of%20patch%0Aprompts%20automatically%20and%20QDMD%20leverages%20a%20one-to-one%20strategy%20to%20provide%20an%0Aindependent%20decoding%20channel%20for%20every%20modality.%20Extensive%20experiments%20indicate%0Athat%20ESP-MedSAM%20outperforms%20state-of-the-arts%20in%20diverse%20medical%20imaging%0Asegmentation%20takes%2C%20displaying%20superior%20zero-shot%20learning%20and%20modality%0Atransfer%20ability.%20Especially%2C%20our%20framework%20uses%20only%2031.4%25%20parameters%20compared%0Ato%20SAM-Base.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14153v2&entry.124074799=Read"},
{"title": "RCA: Region Conditioned Adaptation for Visual Abductive Reasoning", "author": "Hao Zhang and Yeo Keat Ee and Basura Fernando", "abstract": "  Visual abductive reasoning aims to make likely explanations for visual\nobservations. We propose a simple yet effective Region Conditioned Adaptation,\na hybrid parameter-efficient fine-tuning method that equips the frozen CLIP\nwith the ability to infer explanations from local visual cues. We encode\n``local hints'' and ``global contexts'' into visual prompts of the CLIP model\nseparately at fine and coarse-grained levels. Adapters are used for fine-tuning\nCLIP models for downstream tasks and we design a new attention adapter, that\ndirectly steers the focus of the attention map with trainable query and key\nprojections of a frozen CLIP model. Finally, we train our new model with a\nmodified contrastive loss to regress the visual feature simultaneously toward\nfeatures of literal description and plausible explanations. The loss enables\nCLIP to maintain both perception and reasoning abilities. Experiments on the\nSherlock visual abductive reasoning benchmark show that the RCA significantly\noutstands previous SOTAs, ranking the \\nth{1} on the leaderboards (e.g., Human\nAcc: RCA 31.74 \\textit{vs} CPT-CLIP 29.58, higher =better). We also validate\nthe RCA is generalizable to local perception benchmarks like RefCOCO. We\nopen-source our project at\n\\textit{\\color{magenta}{\\url{https://github.com/LUNAProject22/RPA}}}.\n", "link": "http://arxiv.org/abs/2303.10428v5", "date": "2024-08-07", "relevancy": 1.6428, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RCA%3A%20Region%20Conditioned%20Adaptation%20for%20Visual%20Abductive%20Reasoning&body=Title%3A%20RCA%3A%20Region%20Conditioned%20Adaptation%20for%20Visual%20Abductive%20Reasoning%0AAuthor%3A%20Hao%20Zhang%20and%20Yeo%20Keat%20Ee%20and%20Basura%20Fernando%0AAbstract%3A%20%20%20Visual%20abductive%20reasoning%20aims%20to%20make%20likely%20explanations%20for%20visual%0Aobservations.%20We%20propose%20a%20simple%20yet%20effective%20Region%20Conditioned%20Adaptation%2C%0Aa%20hybrid%20parameter-efficient%20fine-tuning%20method%20that%20equips%20the%20frozen%20CLIP%0Awith%20the%20ability%20to%20infer%20explanations%20from%20local%20visual%20cues.%20We%20encode%0A%60%60local%20hints%27%27%20and%20%60%60global%20contexts%27%27%20into%20visual%20prompts%20of%20the%20CLIP%20model%0Aseparately%20at%20fine%20and%20coarse-grained%20levels.%20Adapters%20are%20used%20for%20fine-tuning%0ACLIP%20models%20for%20downstream%20tasks%20and%20we%20design%20a%20new%20attention%20adapter%2C%20that%0Adirectly%20steers%20the%20focus%20of%20the%20attention%20map%20with%20trainable%20query%20and%20key%0Aprojections%20of%20a%20frozen%20CLIP%20model.%20Finally%2C%20we%20train%20our%20new%20model%20with%20a%0Amodified%20contrastive%20loss%20to%20regress%20the%20visual%20feature%20simultaneously%20toward%0Afeatures%20of%20literal%20description%20and%20plausible%20explanations.%20The%20loss%20enables%0ACLIP%20to%20maintain%20both%20perception%20and%20reasoning%20abilities.%20Experiments%20on%20the%0ASherlock%20visual%20abductive%20reasoning%20benchmark%20show%20that%20the%20RCA%20significantly%0Aoutstands%20previous%20SOTAs%2C%20ranking%20the%20%5Cnth%7B1%7D%20on%20the%20leaderboards%20%28e.g.%2C%20Human%0AAcc%3A%20RCA%2031.74%20%5Ctextit%7Bvs%7D%20CPT-CLIP%2029.58%2C%20higher%20%3Dbetter%29.%20We%20also%20validate%0Athe%20RCA%20is%20generalizable%20to%20local%20perception%20benchmarks%20like%20RefCOCO.%20We%0Aopen-source%20our%20project%20at%0A%5Ctextit%7B%5Ccolor%7Bmagenta%7D%7B%5Curl%7Bhttps%3A//github.com/LUNAProject22/RPA%7D%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.10428v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRCA%253A%2520Region%2520Conditioned%2520Adaptation%2520for%2520Visual%2520Abductive%2520Reasoning%26entry.906535625%3DHao%2520Zhang%2520and%2520Yeo%2520Keat%2520Ee%2520and%2520Basura%2520Fernando%26entry.1292438233%3D%2520%2520Visual%2520abductive%2520reasoning%2520aims%2520to%2520make%2520likely%2520explanations%2520for%2520visual%250Aobservations.%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520Region%2520Conditioned%2520Adaptation%252C%250Aa%2520hybrid%2520parameter-efficient%2520fine-tuning%2520method%2520that%2520equips%2520the%2520frozen%2520CLIP%250Awith%2520the%2520ability%2520to%2520infer%2520explanations%2520from%2520local%2520visual%2520cues.%2520We%2520encode%250A%2560%2560local%2520hints%2527%2527%2520and%2520%2560%2560global%2520contexts%2527%2527%2520into%2520visual%2520prompts%2520of%2520the%2520CLIP%2520model%250Aseparately%2520at%2520fine%2520and%2520coarse-grained%2520levels.%2520Adapters%2520are%2520used%2520for%2520fine-tuning%250ACLIP%2520models%2520for%2520downstream%2520tasks%2520and%2520we%2520design%2520a%2520new%2520attention%2520adapter%252C%2520that%250Adirectly%2520steers%2520the%2520focus%2520of%2520the%2520attention%2520map%2520with%2520trainable%2520query%2520and%2520key%250Aprojections%2520of%2520a%2520frozen%2520CLIP%2520model.%2520Finally%252C%2520we%2520train%2520our%2520new%2520model%2520with%2520a%250Amodified%2520contrastive%2520loss%2520to%2520regress%2520the%2520visual%2520feature%2520simultaneously%2520toward%250Afeatures%2520of%2520literal%2520description%2520and%2520plausible%2520explanations.%2520The%2520loss%2520enables%250ACLIP%2520to%2520maintain%2520both%2520perception%2520and%2520reasoning%2520abilities.%2520Experiments%2520on%2520the%250ASherlock%2520visual%2520abductive%2520reasoning%2520benchmark%2520show%2520that%2520the%2520RCA%2520significantly%250Aoutstands%2520previous%2520SOTAs%252C%2520ranking%2520the%2520%255Cnth%257B1%257D%2520on%2520the%2520leaderboards%2520%2528e.g.%252C%2520Human%250AAcc%253A%2520RCA%252031.74%2520%255Ctextit%257Bvs%257D%2520CPT-CLIP%252029.58%252C%2520higher%2520%253Dbetter%2529.%2520We%2520also%2520validate%250Athe%2520RCA%2520is%2520generalizable%2520to%2520local%2520perception%2520benchmarks%2520like%2520RefCOCO.%2520We%250Aopen-source%2520our%2520project%2520at%250A%255Ctextit%257B%255Ccolor%257Bmagenta%257D%257B%255Curl%257Bhttps%253A//github.com/LUNAProject22/RPA%257D%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.10428v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RCA%3A%20Region%20Conditioned%20Adaptation%20for%20Visual%20Abductive%20Reasoning&entry.906535625=Hao%20Zhang%20and%20Yeo%20Keat%20Ee%20and%20Basura%20Fernando&entry.1292438233=%20%20Visual%20abductive%20reasoning%20aims%20to%20make%20likely%20explanations%20for%20visual%0Aobservations.%20We%20propose%20a%20simple%20yet%20effective%20Region%20Conditioned%20Adaptation%2C%0Aa%20hybrid%20parameter-efficient%20fine-tuning%20method%20that%20equips%20the%20frozen%20CLIP%0Awith%20the%20ability%20to%20infer%20explanations%20from%20local%20visual%20cues.%20We%20encode%0A%60%60local%20hints%27%27%20and%20%60%60global%20contexts%27%27%20into%20visual%20prompts%20of%20the%20CLIP%20model%0Aseparately%20at%20fine%20and%20coarse-grained%20levels.%20Adapters%20are%20used%20for%20fine-tuning%0ACLIP%20models%20for%20downstream%20tasks%20and%20we%20design%20a%20new%20attention%20adapter%2C%20that%0Adirectly%20steers%20the%20focus%20of%20the%20attention%20map%20with%20trainable%20query%20and%20key%0Aprojections%20of%20a%20frozen%20CLIP%20model.%20Finally%2C%20we%20train%20our%20new%20model%20with%20a%0Amodified%20contrastive%20loss%20to%20regress%20the%20visual%20feature%20simultaneously%20toward%0Afeatures%20of%20literal%20description%20and%20plausible%20explanations.%20The%20loss%20enables%0ACLIP%20to%20maintain%20both%20perception%20and%20reasoning%20abilities.%20Experiments%20on%20the%0ASherlock%20visual%20abductive%20reasoning%20benchmark%20show%20that%20the%20RCA%20significantly%0Aoutstands%20previous%20SOTAs%2C%20ranking%20the%20%5Cnth%7B1%7D%20on%20the%20leaderboards%20%28e.g.%2C%20Human%0AAcc%3A%20RCA%2031.74%20%5Ctextit%7Bvs%7D%20CPT-CLIP%2029.58%2C%20higher%20%3Dbetter%29.%20We%20also%20validate%0Athe%20RCA%20is%20generalizable%20to%20local%20perception%20benchmarks%20like%20RefCOCO.%20We%0Aopen-source%20our%20project%20at%0A%5Ctextit%7B%5Ccolor%7Bmagenta%7D%7B%5Curl%7Bhttps%3A//github.com/LUNAProject22/RPA%7D%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10428v5&entry.124074799=Read"},
{"title": "Soft-Hard Attention U-Net Model and Benchmark Dataset for Multiscale\n  Image Shadow Removal", "author": "Eirini Cholopoulou and Dimitrios E. Diamantis and Dimitra-Christina C. Koutsiou and Dimitris K. Iakovidis", "abstract": "  Effective shadow removal is pivotal in enhancing the visual quality of images\nin various applications, ranging from computer vision to digital photography.\nDuring the last decades physics and machine learning -based methodologies have\nbeen proposed; however, most of them have limited capacity in capturing complex\nshadow patterns due to restrictive model assumptions, neglecting the fact that\nshadows usually appear at different scales. Also, current datasets used for\nbenchmarking shadow removal are composed of a limited number of images with\nsimple scenes containing mainly uniform shadows cast by single objects, whereas\nonly a few of them include both manual shadow annotations and paired\nshadow-free images. Aiming to address all these limitations in the context of\nnatural scene imaging, including urban environments with complex scenes, the\ncontribution of this study is twofold: a) it proposes a novel deep learning\narchitecture, named Soft-Hard Attention U-net (SHAU), focusing on multiscale\nshadow removal; b) it provides a novel synthetic dataset, named Multiscale\nShadow Removal Dataset (MSRD), containing complex shadow patterns of multiple\nscales, aiming to serve as a privacy-preserving dataset for a more\ncomprehensive benchmarking of future shadow removal methodologies. Key\narchitectural components of SHAU are the soft and hard attention modules, which\nalong with multiscale feature extraction blocks enable effective shadow removal\nof different scales and intensities. The results demonstrate the effectiveness\nof SHAU over the relevant state-of-the-art shadow removal methods across\nvarious benchmark datasets, improving the Peak Signal-to-Noise Ratio and Root\nMean Square Error for the shadow area by 25.1% and 61.3%, respectively.\n", "link": "http://arxiv.org/abs/2408.03734v1", "date": "2024-08-07", "relevancy": 1.6223, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5511}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5467}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft-Hard%20Attention%20U-Net%20Model%20and%20Benchmark%20Dataset%20for%20Multiscale%0A%20%20Image%20Shadow%20Removal&body=Title%3A%20Soft-Hard%20Attention%20U-Net%20Model%20and%20Benchmark%20Dataset%20for%20Multiscale%0A%20%20Image%20Shadow%20Removal%0AAuthor%3A%20Eirini%20Cholopoulou%20and%20Dimitrios%20E.%20Diamantis%20and%20Dimitra-Christina%20C.%20Koutsiou%20and%20Dimitris%20K.%20Iakovidis%0AAbstract%3A%20%20%20Effective%20shadow%20removal%20is%20pivotal%20in%20enhancing%20the%20visual%20quality%20of%20images%0Ain%20various%20applications%2C%20ranging%20from%20computer%20vision%20to%20digital%20photography.%0ADuring%20the%20last%20decades%20physics%20and%20machine%20learning%20-based%20methodologies%20have%0Abeen%20proposed%3B%20however%2C%20most%20of%20them%20have%20limited%20capacity%20in%20capturing%20complex%0Ashadow%20patterns%20due%20to%20restrictive%20model%20assumptions%2C%20neglecting%20the%20fact%20that%0Ashadows%20usually%20appear%20at%20different%20scales.%20Also%2C%20current%20datasets%20used%20for%0Abenchmarking%20shadow%20removal%20are%20composed%20of%20a%20limited%20number%20of%20images%20with%0Asimple%20scenes%20containing%20mainly%20uniform%20shadows%20cast%20by%20single%20objects%2C%20whereas%0Aonly%20a%20few%20of%20them%20include%20both%20manual%20shadow%20annotations%20and%20paired%0Ashadow-free%20images.%20Aiming%20to%20address%20all%20these%20limitations%20in%20the%20context%20of%0Anatural%20scene%20imaging%2C%20including%20urban%20environments%20with%20complex%20scenes%2C%20the%0Acontribution%20of%20this%20study%20is%20twofold%3A%20a%29%20it%20proposes%20a%20novel%20deep%20learning%0Aarchitecture%2C%20named%20Soft-Hard%20Attention%20U-net%20%28SHAU%29%2C%20focusing%20on%20multiscale%0Ashadow%20removal%3B%20b%29%20it%20provides%20a%20novel%20synthetic%20dataset%2C%20named%20Multiscale%0AShadow%20Removal%20Dataset%20%28MSRD%29%2C%20containing%20complex%20shadow%20patterns%20of%20multiple%0Ascales%2C%20aiming%20to%20serve%20as%20a%20privacy-preserving%20dataset%20for%20a%20more%0Acomprehensive%20benchmarking%20of%20future%20shadow%20removal%20methodologies.%20Key%0Aarchitectural%20components%20of%20SHAU%20are%20the%20soft%20and%20hard%20attention%20modules%2C%20which%0Aalong%20with%20multiscale%20feature%20extraction%20blocks%20enable%20effective%20shadow%20removal%0Aof%20different%20scales%20and%20intensities.%20The%20results%20demonstrate%20the%20effectiveness%0Aof%20SHAU%20over%20the%20relevant%20state-of-the-art%20shadow%20removal%20methods%20across%0Avarious%20benchmark%20datasets%2C%20improving%20the%20Peak%20Signal-to-Noise%20Ratio%20and%20Root%0AMean%20Square%20Error%20for%20the%20shadow%20area%20by%2025.1%25%20and%2061.3%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft-Hard%2520Attention%2520U-Net%2520Model%2520and%2520Benchmark%2520Dataset%2520for%2520Multiscale%250A%2520%2520Image%2520Shadow%2520Removal%26entry.906535625%3DEirini%2520Cholopoulou%2520and%2520Dimitrios%2520E.%2520Diamantis%2520and%2520Dimitra-Christina%2520C.%2520Koutsiou%2520and%2520Dimitris%2520K.%2520Iakovidis%26entry.1292438233%3D%2520%2520Effective%2520shadow%2520removal%2520is%2520pivotal%2520in%2520enhancing%2520the%2520visual%2520quality%2520of%2520images%250Ain%2520various%2520applications%252C%2520ranging%2520from%2520computer%2520vision%2520to%2520digital%2520photography.%250ADuring%2520the%2520last%2520decades%2520physics%2520and%2520machine%2520learning%2520-based%2520methodologies%2520have%250Abeen%2520proposed%253B%2520however%252C%2520most%2520of%2520them%2520have%2520limited%2520capacity%2520in%2520capturing%2520complex%250Ashadow%2520patterns%2520due%2520to%2520restrictive%2520model%2520assumptions%252C%2520neglecting%2520the%2520fact%2520that%250Ashadows%2520usually%2520appear%2520at%2520different%2520scales.%2520Also%252C%2520current%2520datasets%2520used%2520for%250Abenchmarking%2520shadow%2520removal%2520are%2520composed%2520of%2520a%2520limited%2520number%2520of%2520images%2520with%250Asimple%2520scenes%2520containing%2520mainly%2520uniform%2520shadows%2520cast%2520by%2520single%2520objects%252C%2520whereas%250Aonly%2520a%2520few%2520of%2520them%2520include%2520both%2520manual%2520shadow%2520annotations%2520and%2520paired%250Ashadow-free%2520images.%2520Aiming%2520to%2520address%2520all%2520these%2520limitations%2520in%2520the%2520context%2520of%250Anatural%2520scene%2520imaging%252C%2520including%2520urban%2520environments%2520with%2520complex%2520scenes%252C%2520the%250Acontribution%2520of%2520this%2520study%2520is%2520twofold%253A%2520a%2529%2520it%2520proposes%2520a%2520novel%2520deep%2520learning%250Aarchitecture%252C%2520named%2520Soft-Hard%2520Attention%2520U-net%2520%2528SHAU%2529%252C%2520focusing%2520on%2520multiscale%250Ashadow%2520removal%253B%2520b%2529%2520it%2520provides%2520a%2520novel%2520synthetic%2520dataset%252C%2520named%2520Multiscale%250AShadow%2520Removal%2520Dataset%2520%2528MSRD%2529%252C%2520containing%2520complex%2520shadow%2520patterns%2520of%2520multiple%250Ascales%252C%2520aiming%2520to%2520serve%2520as%2520a%2520privacy-preserving%2520dataset%2520for%2520a%2520more%250Acomprehensive%2520benchmarking%2520of%2520future%2520shadow%2520removal%2520methodologies.%2520Key%250Aarchitectural%2520components%2520of%2520SHAU%2520are%2520the%2520soft%2520and%2520hard%2520attention%2520modules%252C%2520which%250Aalong%2520with%2520multiscale%2520feature%2520extraction%2520blocks%2520enable%2520effective%2520shadow%2520removal%250Aof%2520different%2520scales%2520and%2520intensities.%2520The%2520results%2520demonstrate%2520the%2520effectiveness%250Aof%2520SHAU%2520over%2520the%2520relevant%2520state-of-the-art%2520shadow%2520removal%2520methods%2520across%250Avarious%2520benchmark%2520datasets%252C%2520improving%2520the%2520Peak%2520Signal-to-Noise%2520Ratio%2520and%2520Root%250AMean%2520Square%2520Error%2520for%2520the%2520shadow%2520area%2520by%252025.1%2525%2520and%252061.3%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft-Hard%20Attention%20U-Net%20Model%20and%20Benchmark%20Dataset%20for%20Multiscale%0A%20%20Image%20Shadow%20Removal&entry.906535625=Eirini%20Cholopoulou%20and%20Dimitrios%20E.%20Diamantis%20and%20Dimitra-Christina%20C.%20Koutsiou%20and%20Dimitris%20K.%20Iakovidis&entry.1292438233=%20%20Effective%20shadow%20removal%20is%20pivotal%20in%20enhancing%20the%20visual%20quality%20of%20images%0Ain%20various%20applications%2C%20ranging%20from%20computer%20vision%20to%20digital%20photography.%0ADuring%20the%20last%20decades%20physics%20and%20machine%20learning%20-based%20methodologies%20have%0Abeen%20proposed%3B%20however%2C%20most%20of%20them%20have%20limited%20capacity%20in%20capturing%20complex%0Ashadow%20patterns%20due%20to%20restrictive%20model%20assumptions%2C%20neglecting%20the%20fact%20that%0Ashadows%20usually%20appear%20at%20different%20scales.%20Also%2C%20current%20datasets%20used%20for%0Abenchmarking%20shadow%20removal%20are%20composed%20of%20a%20limited%20number%20of%20images%20with%0Asimple%20scenes%20containing%20mainly%20uniform%20shadows%20cast%20by%20single%20objects%2C%20whereas%0Aonly%20a%20few%20of%20them%20include%20both%20manual%20shadow%20annotations%20and%20paired%0Ashadow-free%20images.%20Aiming%20to%20address%20all%20these%20limitations%20in%20the%20context%20of%0Anatural%20scene%20imaging%2C%20including%20urban%20environments%20with%20complex%20scenes%2C%20the%0Acontribution%20of%20this%20study%20is%20twofold%3A%20a%29%20it%20proposes%20a%20novel%20deep%20learning%0Aarchitecture%2C%20named%20Soft-Hard%20Attention%20U-net%20%28SHAU%29%2C%20focusing%20on%20multiscale%0Ashadow%20removal%3B%20b%29%20it%20provides%20a%20novel%20synthetic%20dataset%2C%20named%20Multiscale%0AShadow%20Removal%20Dataset%20%28MSRD%29%2C%20containing%20complex%20shadow%20patterns%20of%20multiple%0Ascales%2C%20aiming%20to%20serve%20as%20a%20privacy-preserving%20dataset%20for%20a%20more%0Acomprehensive%20benchmarking%20of%20future%20shadow%20removal%20methodologies.%20Key%0Aarchitectural%20components%20of%20SHAU%20are%20the%20soft%20and%20hard%20attention%20modules%2C%20which%0Aalong%20with%20multiscale%20feature%20extraction%20blocks%20enable%20effective%20shadow%20removal%0Aof%20different%20scales%20and%20intensities.%20The%20results%20demonstrate%20the%20effectiveness%0Aof%20SHAU%20over%20the%20relevant%20state-of-the-art%20shadow%20removal%20methods%20across%0Avarious%20benchmark%20datasets%2C%20improving%20the%20Peak%20Signal-to-Noise%20Ratio%20and%20Root%0AMean%20Square%20Error%20for%20the%20shadow%20area%20by%2025.1%25%20and%2061.3%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03734v1&entry.124074799=Read"},
{"title": "Dual-Modeling Decouple Distillation for Unsupervised Anomaly Detection", "author": "Xinyue Liu and Jianyuan Wang and Biao Leng and Shuo Zhang", "abstract": "  Knowledge distillation based on student-teacher network is one of the\nmainstream solution paradigms for the challenging unsupervised Anomaly\nDetection task, utilizing the difference in representation capabilities of the\nteacher and student networks to implement anomaly localization. However,\nover-generalization of the student network to the teacher network may lead to\nnegligible differences in representation capabilities of anomaly, thus\naffecting the detection effectiveness. Existing methods address the possible\nover-generalization by using differentiated students and teachers from the\nstructural perspective or explicitly expanding distilled information from the\ncontent perspective, which inevitably result in an increased likelihood of\nunderfitting of the student network and poor anomaly detection capabilities in\nanomaly center or edge. In this paper, we propose Dual-Modeling Decouple\nDistillation (DMDD) for the unsupervised anomaly detection. In DMDD, a Decouple\nStudent-Teacher Network is proposed to decouple the initial student features\ninto normality and abnormality features. We further introduce Dual-Modeling\nDistillation based on normal-anomaly image pairs, fitting normality features of\nanomalous image and the teacher features of the corresponding normal image,\nwidening the distance between abnormality features and the teacher features in\nanomalous regions. Synthesizing these two distillation ideas, we achieve\nanomaly detection which focuses on both edge and center of anomaly. Finally, a\nMulti-perception Segmentation Network is proposed to achieve focused anomaly\nmap fusion based on multiple attention. Experimental results on MVTec AD show\nthat DMDD surpasses SOTA localization performance of previous knowledge\ndistillation-based methods, reaching 98.85% on pixel-level AUC and 96.13% on\nPRO.\n", "link": "http://arxiv.org/abs/2408.03888v1", "date": "2024-08-07", "relevancy": 1.6021, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5372}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5309}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Modeling%20Decouple%20Distillation%20for%20Unsupervised%20Anomaly%20Detection&body=Title%3A%20Dual-Modeling%20Decouple%20Distillation%20for%20Unsupervised%20Anomaly%20Detection%0AAuthor%3A%20Xinyue%20Liu%20and%20Jianyuan%20Wang%20and%20Biao%20Leng%20and%20Shuo%20Zhang%0AAbstract%3A%20%20%20Knowledge%20distillation%20based%20on%20student-teacher%20network%20is%20one%20of%20the%0Amainstream%20solution%20paradigms%20for%20the%20challenging%20unsupervised%20Anomaly%0ADetection%20task%2C%20utilizing%20the%20difference%20in%20representation%20capabilities%20of%20the%0Ateacher%20and%20student%20networks%20to%20implement%20anomaly%20localization.%20However%2C%0Aover-generalization%20of%20the%20student%20network%20to%20the%20teacher%20network%20may%20lead%20to%0Anegligible%20differences%20in%20representation%20capabilities%20of%20anomaly%2C%20thus%0Aaffecting%20the%20detection%20effectiveness.%20Existing%20methods%20address%20the%20possible%0Aover-generalization%20by%20using%20differentiated%20students%20and%20teachers%20from%20the%0Astructural%20perspective%20or%20explicitly%20expanding%20distilled%20information%20from%20the%0Acontent%20perspective%2C%20which%20inevitably%20result%20in%20an%20increased%20likelihood%20of%0Aunderfitting%20of%20the%20student%20network%20and%20poor%20anomaly%20detection%20capabilities%20in%0Aanomaly%20center%20or%20edge.%20In%20this%20paper%2C%20we%20propose%20Dual-Modeling%20Decouple%0ADistillation%20%28DMDD%29%20for%20the%20unsupervised%20anomaly%20detection.%20In%20DMDD%2C%20a%20Decouple%0AStudent-Teacher%20Network%20is%20proposed%20to%20decouple%20the%20initial%20student%20features%0Ainto%20normality%20and%20abnormality%20features.%20We%20further%20introduce%20Dual-Modeling%0ADistillation%20based%20on%20normal-anomaly%20image%20pairs%2C%20fitting%20normality%20features%20of%0Aanomalous%20image%20and%20the%20teacher%20features%20of%20the%20corresponding%20normal%20image%2C%0Awidening%20the%20distance%20between%20abnormality%20features%20and%20the%20teacher%20features%20in%0Aanomalous%20regions.%20Synthesizing%20these%20two%20distillation%20ideas%2C%20we%20achieve%0Aanomaly%20detection%20which%20focuses%20on%20both%20edge%20and%20center%20of%20anomaly.%20Finally%2C%20a%0AMulti-perception%20Segmentation%20Network%20is%20proposed%20to%20achieve%20focused%20anomaly%0Amap%20fusion%20based%20on%20multiple%20attention.%20Experimental%20results%20on%20MVTec%20AD%20show%0Athat%20DMDD%20surpasses%20SOTA%20localization%20performance%20of%20previous%20knowledge%0Adistillation-based%20methods%2C%20reaching%2098.85%25%20on%20pixel-level%20AUC%20and%2096.13%25%20on%0APRO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Modeling%2520Decouple%2520Distillation%2520for%2520Unsupervised%2520Anomaly%2520Detection%26entry.906535625%3DXinyue%2520Liu%2520and%2520Jianyuan%2520Wang%2520and%2520Biao%2520Leng%2520and%2520Shuo%2520Zhang%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520based%2520on%2520student-teacher%2520network%2520is%2520one%2520of%2520the%250Amainstream%2520solution%2520paradigms%2520for%2520the%2520challenging%2520unsupervised%2520Anomaly%250ADetection%2520task%252C%2520utilizing%2520the%2520difference%2520in%2520representation%2520capabilities%2520of%2520the%250Ateacher%2520and%2520student%2520networks%2520to%2520implement%2520anomaly%2520localization.%2520However%252C%250Aover-generalization%2520of%2520the%2520student%2520network%2520to%2520the%2520teacher%2520network%2520may%2520lead%2520to%250Anegligible%2520differences%2520in%2520representation%2520capabilities%2520of%2520anomaly%252C%2520thus%250Aaffecting%2520the%2520detection%2520effectiveness.%2520Existing%2520methods%2520address%2520the%2520possible%250Aover-generalization%2520by%2520using%2520differentiated%2520students%2520and%2520teachers%2520from%2520the%250Astructural%2520perspective%2520or%2520explicitly%2520expanding%2520distilled%2520information%2520from%2520the%250Acontent%2520perspective%252C%2520which%2520inevitably%2520result%2520in%2520an%2520increased%2520likelihood%2520of%250Aunderfitting%2520of%2520the%2520student%2520network%2520and%2520poor%2520anomaly%2520detection%2520capabilities%2520in%250Aanomaly%2520center%2520or%2520edge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Dual-Modeling%2520Decouple%250ADistillation%2520%2528DMDD%2529%2520for%2520the%2520unsupervised%2520anomaly%2520detection.%2520In%2520DMDD%252C%2520a%2520Decouple%250AStudent-Teacher%2520Network%2520is%2520proposed%2520to%2520decouple%2520the%2520initial%2520student%2520features%250Ainto%2520normality%2520and%2520abnormality%2520features.%2520We%2520further%2520introduce%2520Dual-Modeling%250ADistillation%2520based%2520on%2520normal-anomaly%2520image%2520pairs%252C%2520fitting%2520normality%2520features%2520of%250Aanomalous%2520image%2520and%2520the%2520teacher%2520features%2520of%2520the%2520corresponding%2520normal%2520image%252C%250Awidening%2520the%2520distance%2520between%2520abnormality%2520features%2520and%2520the%2520teacher%2520features%2520in%250Aanomalous%2520regions.%2520Synthesizing%2520these%2520two%2520distillation%2520ideas%252C%2520we%2520achieve%250Aanomaly%2520detection%2520which%2520focuses%2520on%2520both%2520edge%2520and%2520center%2520of%2520anomaly.%2520Finally%252C%2520a%250AMulti-perception%2520Segmentation%2520Network%2520is%2520proposed%2520to%2520achieve%2520focused%2520anomaly%250Amap%2520fusion%2520based%2520on%2520multiple%2520attention.%2520Experimental%2520results%2520on%2520MVTec%2520AD%2520show%250Athat%2520DMDD%2520surpasses%2520SOTA%2520localization%2520performance%2520of%2520previous%2520knowledge%250Adistillation-based%2520methods%252C%2520reaching%252098.85%2525%2520on%2520pixel-level%2520AUC%2520and%252096.13%2525%2520on%250APRO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Modeling%20Decouple%20Distillation%20for%20Unsupervised%20Anomaly%20Detection&entry.906535625=Xinyue%20Liu%20and%20Jianyuan%20Wang%20and%20Biao%20Leng%20and%20Shuo%20Zhang&entry.1292438233=%20%20Knowledge%20distillation%20based%20on%20student-teacher%20network%20is%20one%20of%20the%0Amainstream%20solution%20paradigms%20for%20the%20challenging%20unsupervised%20Anomaly%0ADetection%20task%2C%20utilizing%20the%20difference%20in%20representation%20capabilities%20of%20the%0Ateacher%20and%20student%20networks%20to%20implement%20anomaly%20localization.%20However%2C%0Aover-generalization%20of%20the%20student%20network%20to%20the%20teacher%20network%20may%20lead%20to%0Anegligible%20differences%20in%20representation%20capabilities%20of%20anomaly%2C%20thus%0Aaffecting%20the%20detection%20effectiveness.%20Existing%20methods%20address%20the%20possible%0Aover-generalization%20by%20using%20differentiated%20students%20and%20teachers%20from%20the%0Astructural%20perspective%20or%20explicitly%20expanding%20distilled%20information%20from%20the%0Acontent%20perspective%2C%20which%20inevitably%20result%20in%20an%20increased%20likelihood%20of%0Aunderfitting%20of%20the%20student%20network%20and%20poor%20anomaly%20detection%20capabilities%20in%0Aanomaly%20center%20or%20edge.%20In%20this%20paper%2C%20we%20propose%20Dual-Modeling%20Decouple%0ADistillation%20%28DMDD%29%20for%20the%20unsupervised%20anomaly%20detection.%20In%20DMDD%2C%20a%20Decouple%0AStudent-Teacher%20Network%20is%20proposed%20to%20decouple%20the%20initial%20student%20features%0Ainto%20normality%20and%20abnormality%20features.%20We%20further%20introduce%20Dual-Modeling%0ADistillation%20based%20on%20normal-anomaly%20image%20pairs%2C%20fitting%20normality%20features%20of%0Aanomalous%20image%20and%20the%20teacher%20features%20of%20the%20corresponding%20normal%20image%2C%0Awidening%20the%20distance%20between%20abnormality%20features%20and%20the%20teacher%20features%20in%0Aanomalous%20regions.%20Synthesizing%20these%20two%20distillation%20ideas%2C%20we%20achieve%0Aanomaly%20detection%20which%20focuses%20on%20both%20edge%20and%20center%20of%20anomaly.%20Finally%2C%20a%0AMulti-perception%20Segmentation%20Network%20is%20proposed%20to%20achieve%20focused%20anomaly%0Amap%20fusion%20based%20on%20multiple%20attention.%20Experimental%20results%20on%20MVTec%20AD%20show%0Athat%20DMDD%20surpasses%20SOTA%20localization%20performance%20of%20previous%20knowledge%0Adistillation-based%20methods%2C%20reaching%2098.85%25%20on%20pixel-level%20AUC%20and%2096.13%25%20on%0APRO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03888v1&entry.124074799=Read"},
{"title": "Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse\n  Training Data", "author": "Kang Lin and Reinhard Heckel", "abstract": "  Deep learning based methods for image reconstruction are state-of-the-art for\na variety of imaging tasks. However, neural networks often perform worse if the\ntraining data differs significantly from the data they are applied to. For\nexample, a model trained for accelerated magnetic resonance imaging (MRI) on\none scanner performs worse on another scanner. In this work, we investigate the\nimpact of the training data on a model's performance and robustness for\naccelerated MRI. We find that models trained on the combination of various data\ndistributions, such as those obtained from different MRI scanners and\nanatomies, exhibit robustness equal or superior to models trained on the best\nsingle distribution for a specific target distribution. Thus training on such\ndiverse data tends to improve robustness. Furthermore, training on such a\ndiverse dataset does not compromise in-distribution performance, i.e., a model\ntrained on diverse data yields in-distribution performance at least as good as\nmodels trained on the more narrow individual distributions. Our results suggest\nthat training a model for imaging on a variety of distributions tends to yield\na more effective and robust model than maintaining separate models for\nindividual distributions.\n", "link": "http://arxiv.org/abs/2312.10271v2", "date": "2024-08-07", "relevancy": 1.572, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5273}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.521}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20of%20Deep%20Learning%20for%20Accelerated%20MRI%3A%20Benefits%20of%20Diverse%0A%20%20Training%20Data&body=Title%3A%20Robustness%20of%20Deep%20Learning%20for%20Accelerated%20MRI%3A%20Benefits%20of%20Diverse%0A%20%20Training%20Data%0AAuthor%3A%20Kang%20Lin%20and%20Reinhard%20Heckel%0AAbstract%3A%20%20%20Deep%20learning%20based%20methods%20for%20image%20reconstruction%20are%20state-of-the-art%20for%0Aa%20variety%20of%20imaging%20tasks.%20However%2C%20neural%20networks%20often%20perform%20worse%20if%20the%0Atraining%20data%20differs%20significantly%20from%20the%20data%20they%20are%20applied%20to.%20For%0Aexample%2C%20a%20model%20trained%20for%20accelerated%20magnetic%20resonance%20imaging%20%28MRI%29%20on%0Aone%20scanner%20performs%20worse%20on%20another%20scanner.%20In%20this%20work%2C%20we%20investigate%20the%0Aimpact%20of%20the%20training%20data%20on%20a%20model%27s%20performance%20and%20robustness%20for%0Aaccelerated%20MRI.%20We%20find%20that%20models%20trained%20on%20the%20combination%20of%20various%20data%0Adistributions%2C%20such%20as%20those%20obtained%20from%20different%20MRI%20scanners%20and%0Aanatomies%2C%20exhibit%20robustness%20equal%20or%20superior%20to%20models%20trained%20on%20the%20best%0Asingle%20distribution%20for%20a%20specific%20target%20distribution.%20Thus%20training%20on%20such%0Adiverse%20data%20tends%20to%20improve%20robustness.%20Furthermore%2C%20training%20on%20such%20a%0Adiverse%20dataset%20does%20not%20compromise%20in-distribution%20performance%2C%20i.e.%2C%20a%20model%0Atrained%20on%20diverse%20data%20yields%20in-distribution%20performance%20at%20least%20as%20good%20as%0Amodels%20trained%20on%20the%20more%20narrow%20individual%20distributions.%20Our%20results%20suggest%0Athat%20training%20a%20model%20for%20imaging%20on%20a%20variety%20of%20distributions%20tends%20to%20yield%0Aa%20more%20effective%20and%20robust%20model%20than%20maintaining%20separate%20models%20for%0Aindividual%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520of%2520Deep%2520Learning%2520for%2520Accelerated%2520MRI%253A%2520Benefits%2520of%2520Diverse%250A%2520%2520Training%2520Data%26entry.906535625%3DKang%2520Lin%2520and%2520Reinhard%2520Heckel%26entry.1292438233%3D%2520%2520Deep%2520learning%2520based%2520methods%2520for%2520image%2520reconstruction%2520are%2520state-of-the-art%2520for%250Aa%2520variety%2520of%2520imaging%2520tasks.%2520However%252C%2520neural%2520networks%2520often%2520perform%2520worse%2520if%2520the%250Atraining%2520data%2520differs%2520significantly%2520from%2520the%2520data%2520they%2520are%2520applied%2520to.%2520For%250Aexample%252C%2520a%2520model%2520trained%2520for%2520accelerated%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520on%250Aone%2520scanner%2520performs%2520worse%2520on%2520another%2520scanner.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520the%2520training%2520data%2520on%2520a%2520model%2527s%2520performance%2520and%2520robustness%2520for%250Aaccelerated%2520MRI.%2520We%2520find%2520that%2520models%2520trained%2520on%2520the%2520combination%2520of%2520various%2520data%250Adistributions%252C%2520such%2520as%2520those%2520obtained%2520from%2520different%2520MRI%2520scanners%2520and%250Aanatomies%252C%2520exhibit%2520robustness%2520equal%2520or%2520superior%2520to%2520models%2520trained%2520on%2520the%2520best%250Asingle%2520distribution%2520for%2520a%2520specific%2520target%2520distribution.%2520Thus%2520training%2520on%2520such%250Adiverse%2520data%2520tends%2520to%2520improve%2520robustness.%2520Furthermore%252C%2520training%2520on%2520such%2520a%250Adiverse%2520dataset%2520does%2520not%2520compromise%2520in-distribution%2520performance%252C%2520i.e.%252C%2520a%2520model%250Atrained%2520on%2520diverse%2520data%2520yields%2520in-distribution%2520performance%2520at%2520least%2520as%2520good%2520as%250Amodels%2520trained%2520on%2520the%2520more%2520narrow%2520individual%2520distributions.%2520Our%2520results%2520suggest%250Athat%2520training%2520a%2520model%2520for%2520imaging%2520on%2520a%2520variety%2520of%2520distributions%2520tends%2520to%2520yield%250Aa%2520more%2520effective%2520and%2520robust%2520model%2520than%2520maintaining%2520separate%2520models%2520for%250Aindividual%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20of%20Deep%20Learning%20for%20Accelerated%20MRI%3A%20Benefits%20of%20Diverse%0A%20%20Training%20Data&entry.906535625=Kang%20Lin%20and%20Reinhard%20Heckel&entry.1292438233=%20%20Deep%20learning%20based%20methods%20for%20image%20reconstruction%20are%20state-of-the-art%20for%0Aa%20variety%20of%20imaging%20tasks.%20However%2C%20neural%20networks%20often%20perform%20worse%20if%20the%0Atraining%20data%20differs%20significantly%20from%20the%20data%20they%20are%20applied%20to.%20For%0Aexample%2C%20a%20model%20trained%20for%20accelerated%20magnetic%20resonance%20imaging%20%28MRI%29%20on%0Aone%20scanner%20performs%20worse%20on%20another%20scanner.%20In%20this%20work%2C%20we%20investigate%20the%0Aimpact%20of%20the%20training%20data%20on%20a%20model%27s%20performance%20and%20robustness%20for%0Aaccelerated%20MRI.%20We%20find%20that%20models%20trained%20on%20the%20combination%20of%20various%20data%0Adistributions%2C%20such%20as%20those%20obtained%20from%20different%20MRI%20scanners%20and%0Aanatomies%2C%20exhibit%20robustness%20equal%20or%20superior%20to%20models%20trained%20on%20the%20best%0Asingle%20distribution%20for%20a%20specific%20target%20distribution.%20Thus%20training%20on%20such%0Adiverse%20data%20tends%20to%20improve%20robustness.%20Furthermore%2C%20training%20on%20such%20a%0Adiverse%20dataset%20does%20not%20compromise%20in-distribution%20performance%2C%20i.e.%2C%20a%20model%0Atrained%20on%20diverse%20data%20yields%20in-distribution%20performance%20at%20least%20as%20good%20as%0Amodels%20trained%20on%20the%20more%20narrow%20individual%20distributions.%20Our%20results%20suggest%0Athat%20training%20a%20model%20for%20imaging%20on%20a%20variety%20of%20distributions%20tends%20to%20yield%0Aa%20more%20effective%20and%20robust%20model%20than%20maintaining%20separate%20models%20for%0Aindividual%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10271v2&entry.124074799=Read"},
{"title": "How Well Can Vision Language Models See Image Details?", "author": "Chenhui Gou and Abdulwahab Felemban and Faizan Farooq Khan and Deyao Zhu and Jianfei Cai and Hamid Rezatofighi and Mohamed Elhoseiny", "abstract": "  Large Language Model-based Vision-Language Models (LLM-based VLMs) have\ndemonstrated impressive results in various vision-language understanding tasks.\nHowever, how well these VLMs can see image detail beyond the semantic level\nremains unclear. In our study, we introduce a pixel value prediction task (PVP)\nto explore \"How Well Can Vision Language Models See Image Details?\" and to\nassist VLMs in perceiving more details. Typically, these models comprise a\nfrozen CLIP visual encoder, a large language model, and a connecting module.\nAfter fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to\npredict precise pixel values by only fine-tuning the connection module and LLM;\nand 2) prediction precision is significantly improved when the vision encoder\nis also adapted. Additionally, our research reveals that incorporating pixel\nvalue prediction as one of the VLM pre-training tasks and vision encoder\nadaptation markedly boosts VLM performance on downstream image-language\nunderstanding tasks requiring detailed image perception, such as referring\nimage segmentation (with an average +10.19 cIoU improvement) and video game\ndecision making (with average score improvements of +80.34 and +70.54 on two\ngames, respectively).\n", "link": "http://arxiv.org/abs/2408.03940v1", "date": "2024-08-07", "relevancy": 1.5717, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5123}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Well%20Can%20Vision%20Language%20Models%20See%20Image%20Details%3F&body=Title%3A%20How%20Well%20Can%20Vision%20Language%20Models%20See%20Image%20Details%3F%0AAuthor%3A%20Chenhui%20Gou%20and%20Abdulwahab%20Felemban%20and%20Faizan%20Farooq%20Khan%20and%20Deyao%20Zhu%20and%20Jianfei%20Cai%20and%20Hamid%20Rezatofighi%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20Large%20Language%20Model-based%20Vision-Language%20Models%20%28LLM-based%20VLMs%29%20have%0Ademonstrated%20impressive%20results%20in%20various%20vision-language%20understanding%20tasks.%0AHowever%2C%20how%20well%20these%20VLMs%20can%20see%20image%20detail%20beyond%20the%20semantic%20level%0Aremains%20unclear.%20In%20our%20study%2C%20we%20introduce%20a%20pixel%20value%20prediction%20task%20%28PVP%29%0Ato%20explore%20%22How%20Well%20Can%20Vision%20Language%20Models%20See%20Image%20Details%3F%22%20and%20to%0Aassist%20VLMs%20in%20perceiving%20more%20details.%20Typically%2C%20these%20models%20comprise%20a%0Afrozen%20CLIP%20visual%20encoder%2C%20a%20large%20language%20model%2C%20and%20a%20connecting%20module.%0AAfter%20fine-tuning%20VLMs%20on%20the%20PVP%20task%2C%20we%20find%3A%201%29%20existing%20VLMs%20struggle%20to%0Apredict%20precise%20pixel%20values%20by%20only%20fine-tuning%20the%20connection%20module%20and%20LLM%3B%0Aand%202%29%20prediction%20precision%20is%20significantly%20improved%20when%20the%20vision%20encoder%0Ais%20also%20adapted.%20Additionally%2C%20our%20research%20reveals%20that%20incorporating%20pixel%0Avalue%20prediction%20as%20one%20of%20the%20VLM%20pre-training%20tasks%20and%20vision%20encoder%0Aadaptation%20markedly%20boosts%20VLM%20performance%20on%20downstream%20image-language%0Aunderstanding%20tasks%20requiring%20detailed%20image%20perception%2C%20such%20as%20referring%0Aimage%20segmentation%20%28with%20an%20average%20%2B10.19%20cIoU%20improvement%29%20and%20video%20game%0Adecision%20making%20%28with%20average%20score%20improvements%20of%20%2B80.34%20and%20%2B70.54%20on%20two%0Agames%2C%20respectively%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Well%2520Can%2520Vision%2520Language%2520Models%2520See%2520Image%2520Details%253F%26entry.906535625%3DChenhui%2520Gou%2520and%2520Abdulwahab%2520Felemban%2520and%2520Faizan%2520Farooq%2520Khan%2520and%2520Deyao%2520Zhu%2520and%2520Jianfei%2520Cai%2520and%2520Hamid%2520Rezatofighi%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model-based%2520Vision-Language%2520Models%2520%2528LLM-based%2520VLMs%2529%2520have%250Ademonstrated%2520impressive%2520results%2520in%2520various%2520vision-language%2520understanding%2520tasks.%250AHowever%252C%2520how%2520well%2520these%2520VLMs%2520can%2520see%2520image%2520detail%2520beyond%2520the%2520semantic%2520level%250Aremains%2520unclear.%2520In%2520our%2520study%252C%2520we%2520introduce%2520a%2520pixel%2520value%2520prediction%2520task%2520%2528PVP%2529%250Ato%2520explore%2520%2522How%2520Well%2520Can%2520Vision%2520Language%2520Models%2520See%2520Image%2520Details%253F%2522%2520and%2520to%250Aassist%2520VLMs%2520in%2520perceiving%2520more%2520details.%2520Typically%252C%2520these%2520models%2520comprise%2520a%250Afrozen%2520CLIP%2520visual%2520encoder%252C%2520a%2520large%2520language%2520model%252C%2520and%2520a%2520connecting%2520module.%250AAfter%2520fine-tuning%2520VLMs%2520on%2520the%2520PVP%2520task%252C%2520we%2520find%253A%25201%2529%2520existing%2520VLMs%2520struggle%2520to%250Apredict%2520precise%2520pixel%2520values%2520by%2520only%2520fine-tuning%2520the%2520connection%2520module%2520and%2520LLM%253B%250Aand%25202%2529%2520prediction%2520precision%2520is%2520significantly%2520improved%2520when%2520the%2520vision%2520encoder%250Ais%2520also%2520adapted.%2520Additionally%252C%2520our%2520research%2520reveals%2520that%2520incorporating%2520pixel%250Avalue%2520prediction%2520as%2520one%2520of%2520the%2520VLM%2520pre-training%2520tasks%2520and%2520vision%2520encoder%250Aadaptation%2520markedly%2520boosts%2520VLM%2520performance%2520on%2520downstream%2520image-language%250Aunderstanding%2520tasks%2520requiring%2520detailed%2520image%2520perception%252C%2520such%2520as%2520referring%250Aimage%2520segmentation%2520%2528with%2520an%2520average%2520%252B10.19%2520cIoU%2520improvement%2529%2520and%2520video%2520game%250Adecision%2520making%2520%2528with%2520average%2520score%2520improvements%2520of%2520%252B80.34%2520and%2520%252B70.54%2520on%2520two%250Agames%252C%2520respectively%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Well%20Can%20Vision%20Language%20Models%20See%20Image%20Details%3F&entry.906535625=Chenhui%20Gou%20and%20Abdulwahab%20Felemban%20and%20Faizan%20Farooq%20Khan%20and%20Deyao%20Zhu%20and%20Jianfei%20Cai%20and%20Hamid%20Rezatofighi%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20Large%20Language%20Model-based%20Vision-Language%20Models%20%28LLM-based%20VLMs%29%20have%0Ademonstrated%20impressive%20results%20in%20various%20vision-language%20understanding%20tasks.%0AHowever%2C%20how%20well%20these%20VLMs%20can%20see%20image%20detail%20beyond%20the%20semantic%20level%0Aremains%20unclear.%20In%20our%20study%2C%20we%20introduce%20a%20pixel%20value%20prediction%20task%20%28PVP%29%0Ato%20explore%20%22How%20Well%20Can%20Vision%20Language%20Models%20See%20Image%20Details%3F%22%20and%20to%0Aassist%20VLMs%20in%20perceiving%20more%20details.%20Typically%2C%20these%20models%20comprise%20a%0Afrozen%20CLIP%20visual%20encoder%2C%20a%20large%20language%20model%2C%20and%20a%20connecting%20module.%0AAfter%20fine-tuning%20VLMs%20on%20the%20PVP%20task%2C%20we%20find%3A%201%29%20existing%20VLMs%20struggle%20to%0Apredict%20precise%20pixel%20values%20by%20only%20fine-tuning%20the%20connection%20module%20and%20LLM%3B%0Aand%202%29%20prediction%20precision%20is%20significantly%20improved%20when%20the%20vision%20encoder%0Ais%20also%20adapted.%20Additionally%2C%20our%20research%20reveals%20that%20incorporating%20pixel%0Avalue%20prediction%20as%20one%20of%20the%20VLM%20pre-training%20tasks%20and%20vision%20encoder%0Aadaptation%20markedly%20boosts%20VLM%20performance%20on%20downstream%20image-language%0Aunderstanding%20tasks%20requiring%20detailed%20image%20perception%2C%20such%20as%20referring%0Aimage%20segmentation%20%28with%20an%20average%20%2B10.19%20cIoU%20improvement%29%20and%20video%20game%0Adecision%20making%20%28with%20average%20score%20improvements%20of%20%2B80.34%20and%20%2B70.54%20on%20two%0Agames%2C%20respectively%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03940v1&entry.124074799=Read"},
{"title": "EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning", "author": "Yue Chen and Chenrui Tie and Ruihai Wu and Hao Dong", "abstract": "  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n", "link": "http://arxiv.org/abs/2408.01953v2", "date": "2024-08-07", "relevancy": 1.5488, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5634}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5194}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EqvAfford%3A%20SE%283%29%20Equivariance%20for%20Point-Level%20Affordance%20Learning&body=Title%3A%20EqvAfford%3A%20SE%283%29%20Equivariance%20for%20Point-Level%20Affordance%20Learning%0AAuthor%3A%20Yue%20Chen%20and%20Chenrui%20Tie%20and%20Ruihai%20Wu%20and%20Hao%20Dong%0AAbstract%3A%20%20%20Humans%20perceive%20and%20interact%20with%20the%20world%20with%20the%20awareness%20of%0Aequivariance%2C%20facilitating%20us%20in%20manipulating%20different%20objects%20in%20diverse%0Aposes.%20For%20robotic%20manipulation%2C%20such%20equivariance%20also%20exists%20in%20many%0Ascenarios.%20For%20example%2C%20no%20matter%20what%20the%20pose%20of%20a%20drawer%20is%20%28translation%2C%0Arotation%20and%20tilt%29%2C%20the%20manipulation%20strategy%20is%20consistent%20%28grasp%20the%20handle%0Aand%20pull%20in%20a%20line%29.%20While%20traditional%20models%20usually%20do%20not%20have%20the%20awareness%0Aof%20equivariance%20for%20robotic%20manipulation%2C%20which%20might%20result%20in%20more%20data%20for%0Atraining%20and%20poor%20performance%20in%20novel%20object%20poses%2C%20we%20propose%20our%20EqvAfford%0Aframework%2C%20with%20novel%20designs%20to%20guarantee%20the%20equivariance%20in%20point-level%0Aaffordance%20learning%20for%20downstream%20robotic%20manipulation%2C%20with%20great%20performance%0Aand%20generalization%20ability%20on%20representative%20tasks%20on%20objects%20in%20diverse%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEqvAfford%253A%2520SE%25283%2529%2520Equivariance%2520for%2520Point-Level%2520Affordance%2520Learning%26entry.906535625%3DYue%2520Chen%2520and%2520Chenrui%2520Tie%2520and%2520Ruihai%2520Wu%2520and%2520Hao%2520Dong%26entry.1292438233%3D%2520%2520Humans%2520perceive%2520and%2520interact%2520with%2520the%2520world%2520with%2520the%2520awareness%2520of%250Aequivariance%252C%2520facilitating%2520us%2520in%2520manipulating%2520different%2520objects%2520in%2520diverse%250Aposes.%2520For%2520robotic%2520manipulation%252C%2520such%2520equivariance%2520also%2520exists%2520in%2520many%250Ascenarios.%2520For%2520example%252C%2520no%2520matter%2520what%2520the%2520pose%2520of%2520a%2520drawer%2520is%2520%2528translation%252C%250Arotation%2520and%2520tilt%2529%252C%2520the%2520manipulation%2520strategy%2520is%2520consistent%2520%2528grasp%2520the%2520handle%250Aand%2520pull%2520in%2520a%2520line%2529.%2520While%2520traditional%2520models%2520usually%2520do%2520not%2520have%2520the%2520awareness%250Aof%2520equivariance%2520for%2520robotic%2520manipulation%252C%2520which%2520might%2520result%2520in%2520more%2520data%2520for%250Atraining%2520and%2520poor%2520performance%2520in%2520novel%2520object%2520poses%252C%2520we%2520propose%2520our%2520EqvAfford%250Aframework%252C%2520with%2520novel%2520designs%2520to%2520guarantee%2520the%2520equivariance%2520in%2520point-level%250Aaffordance%2520learning%2520for%2520downstream%2520robotic%2520manipulation%252C%2520with%2520great%2520performance%250Aand%2520generalization%2520ability%2520on%2520representative%2520tasks%2520on%2520objects%2520in%2520diverse%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EqvAfford%3A%20SE%283%29%20Equivariance%20for%20Point-Level%20Affordance%20Learning&entry.906535625=Yue%20Chen%20and%20Chenrui%20Tie%20and%20Ruihai%20Wu%20and%20Hao%20Dong&entry.1292438233=%20%20Humans%20perceive%20and%20interact%20with%20the%20world%20with%20the%20awareness%20of%0Aequivariance%2C%20facilitating%20us%20in%20manipulating%20different%20objects%20in%20diverse%0Aposes.%20For%20robotic%20manipulation%2C%20such%20equivariance%20also%20exists%20in%20many%0Ascenarios.%20For%20example%2C%20no%20matter%20what%20the%20pose%20of%20a%20drawer%20is%20%28translation%2C%0Arotation%20and%20tilt%29%2C%20the%20manipulation%20strategy%20is%20consistent%20%28grasp%20the%20handle%0Aand%20pull%20in%20a%20line%29.%20While%20traditional%20models%20usually%20do%20not%20have%20the%20awareness%0Aof%20equivariance%20for%20robotic%20manipulation%2C%20which%20might%20result%20in%20more%20data%20for%0Atraining%20and%20poor%20performance%20in%20novel%20object%20poses%2C%20we%20propose%20our%20EqvAfford%0Aframework%2C%20with%20novel%20designs%20to%20guarantee%20the%20equivariance%20in%20point-level%0Aaffordance%20learning%20for%20downstream%20robotic%20manipulation%2C%20with%20great%20performance%0Aand%20generalization%20ability%20on%20representative%20tasks%20on%20objects%20in%20diverse%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01953v2&entry.124074799=Read"},
{"title": "Automated Code Fix Suggestions for Accessibility Issues in Mobile Apps", "author": "Forough Mehralian and Titus Barik and Jeff Nichols and Amanda Swearngin", "abstract": "  Accessibility is crucial for inclusive app usability, yet developers often\nstruggle to identify and fix app accessibility issues due to a lack of\nawareness, expertise, and inadequate tools. Current accessibility testing tools\ncan identify accessibility issues but may not always provide guidance on how to\naddress them. We introduce FixAlly, an automated tool designed to suggest\nsource code fixes for accessibility issues detected by automated accessibility\nscanners. FixAlly employs a multi-agent LLM architecture to generate fix\nstrategies, localize issues within the source code, and propose code\nmodification suggestions to fix the accessibility issue. Our empirical study\ndemonstrates FixAlly's capability in suggesting fixes that resolve issues found\nby accessibility scanners -- with an effectiveness of 77% in generating\nplausible fix suggestions -- and our survey of 12 iOS developers finds they\nwould be willing to accept 69.4% of evaluated fix suggestions.\n", "link": "http://arxiv.org/abs/2408.03827v1", "date": "2024-08-07", "relevancy": 1.5475, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3921}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3884}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Code%20Fix%20Suggestions%20for%20Accessibility%20Issues%20in%20Mobile%20Apps&body=Title%3A%20Automated%20Code%20Fix%20Suggestions%20for%20Accessibility%20Issues%20in%20Mobile%20Apps%0AAuthor%3A%20Forough%20Mehralian%20and%20Titus%20Barik%20and%20Jeff%20Nichols%20and%20Amanda%20Swearngin%0AAbstract%3A%20%20%20Accessibility%20is%20crucial%20for%20inclusive%20app%20usability%2C%20yet%20developers%20often%0Astruggle%20to%20identify%20and%20fix%20app%20accessibility%20issues%20due%20to%20a%20lack%20of%0Aawareness%2C%20expertise%2C%20and%20inadequate%20tools.%20Current%20accessibility%20testing%20tools%0Acan%20identify%20accessibility%20issues%20but%20may%20not%20always%20provide%20guidance%20on%20how%20to%0Aaddress%20them.%20We%20introduce%20FixAlly%2C%20an%20automated%20tool%20designed%20to%20suggest%0Asource%20code%20fixes%20for%20accessibility%20issues%20detected%20by%20automated%20accessibility%0Ascanners.%20FixAlly%20employs%20a%20multi-agent%20LLM%20architecture%20to%20generate%20fix%0Astrategies%2C%20localize%20issues%20within%20the%20source%20code%2C%20and%20propose%20code%0Amodification%20suggestions%20to%20fix%20the%20accessibility%20issue.%20Our%20empirical%20study%0Ademonstrates%20FixAlly%27s%20capability%20in%20suggesting%20fixes%20that%20resolve%20issues%20found%0Aby%20accessibility%20scanners%20--%20with%20an%20effectiveness%20of%2077%25%20in%20generating%0Aplausible%20fix%20suggestions%20--%20and%20our%20survey%20of%2012%20iOS%20developers%20finds%20they%0Awould%20be%20willing%20to%20accept%2069.4%25%20of%20evaluated%20fix%20suggestions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Code%2520Fix%2520Suggestions%2520for%2520Accessibility%2520Issues%2520in%2520Mobile%2520Apps%26entry.906535625%3DForough%2520Mehralian%2520and%2520Titus%2520Barik%2520and%2520Jeff%2520Nichols%2520and%2520Amanda%2520Swearngin%26entry.1292438233%3D%2520%2520Accessibility%2520is%2520crucial%2520for%2520inclusive%2520app%2520usability%252C%2520yet%2520developers%2520often%250Astruggle%2520to%2520identify%2520and%2520fix%2520app%2520accessibility%2520issues%2520due%2520to%2520a%2520lack%2520of%250Aawareness%252C%2520expertise%252C%2520and%2520inadequate%2520tools.%2520Current%2520accessibility%2520testing%2520tools%250Acan%2520identify%2520accessibility%2520issues%2520but%2520may%2520not%2520always%2520provide%2520guidance%2520on%2520how%2520to%250Aaddress%2520them.%2520We%2520introduce%2520FixAlly%252C%2520an%2520automated%2520tool%2520designed%2520to%2520suggest%250Asource%2520code%2520fixes%2520for%2520accessibility%2520issues%2520detected%2520by%2520automated%2520accessibility%250Ascanners.%2520FixAlly%2520employs%2520a%2520multi-agent%2520LLM%2520architecture%2520to%2520generate%2520fix%250Astrategies%252C%2520localize%2520issues%2520within%2520the%2520source%2520code%252C%2520and%2520propose%2520code%250Amodification%2520suggestions%2520to%2520fix%2520the%2520accessibility%2520issue.%2520Our%2520empirical%2520study%250Ademonstrates%2520FixAlly%2527s%2520capability%2520in%2520suggesting%2520fixes%2520that%2520resolve%2520issues%2520found%250Aby%2520accessibility%2520scanners%2520--%2520with%2520an%2520effectiveness%2520of%252077%2525%2520in%2520generating%250Aplausible%2520fix%2520suggestions%2520--%2520and%2520our%2520survey%2520of%252012%2520iOS%2520developers%2520finds%2520they%250Awould%2520be%2520willing%2520to%2520accept%252069.4%2525%2520of%2520evaluated%2520fix%2520suggestions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Code%20Fix%20Suggestions%20for%20Accessibility%20Issues%20in%20Mobile%20Apps&entry.906535625=Forough%20Mehralian%20and%20Titus%20Barik%20and%20Jeff%20Nichols%20and%20Amanda%20Swearngin&entry.1292438233=%20%20Accessibility%20is%20crucial%20for%20inclusive%20app%20usability%2C%20yet%20developers%20often%0Astruggle%20to%20identify%20and%20fix%20app%20accessibility%20issues%20due%20to%20a%20lack%20of%0Aawareness%2C%20expertise%2C%20and%20inadequate%20tools.%20Current%20accessibility%20testing%20tools%0Acan%20identify%20accessibility%20issues%20but%20may%20not%20always%20provide%20guidance%20on%20how%20to%0Aaddress%20them.%20We%20introduce%20FixAlly%2C%20an%20automated%20tool%20designed%20to%20suggest%0Asource%20code%20fixes%20for%20accessibility%20issues%20detected%20by%20automated%20accessibility%0Ascanners.%20FixAlly%20employs%20a%20multi-agent%20LLM%20architecture%20to%20generate%20fix%0Astrategies%2C%20localize%20issues%20within%20the%20source%20code%2C%20and%20propose%20code%0Amodification%20suggestions%20to%20fix%20the%20accessibility%20issue.%20Our%20empirical%20study%0Ademonstrates%20FixAlly%27s%20capability%20in%20suggesting%20fixes%20that%20resolve%20issues%20found%0Aby%20accessibility%20scanners%20--%20with%20an%20effectiveness%20of%2077%25%20in%20generating%0Aplausible%20fix%20suggestions%20--%20and%20our%20survey%20of%2012%20iOS%20developers%20finds%20they%0Awould%20be%20willing%20to%20accept%2069.4%25%20of%20evaluated%20fix%20suggestions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03827v1&entry.124074799=Read"},
{"title": "HyperKAN: Kolmogorov-Arnold Networks make Hyperspectral Image\n  Classificators Smarter", "author": "Valeriy Lobanov and Nikita Firsov and Evgeny Myasnikov and Roman Khabibullin and Artem Nikonorov", "abstract": "  In traditional neural network architectures, a multilayer perceptron (MLP) is\ntypically employed as a classification block following the feature extraction\nstage. However, the Kolmogorov-Arnold Network (KAN) presents a promising\nalternative to MLP, offering the potential to enhance prediction accuracy. In\nthis paper, we propose the replacement of linear and convolutional layers of\ntraditional networks with KAN-based counterparts. These modifications allowed\nus to significantly increase the per-pixel classification accuracy for\nhyperspectral remote-sensing images. We modified seven different neural network\narchitectures for hyperspectral image classification and observed a substantial\nimprovement in the classification accuracy across all the networks. The\narchitectures considered in the paper include baseline MLP, state-of-the-art 1D\n(1DCNN) and 3D convolutional (two different 3DCNN, NM3DCNN), and transformer\n(SSFTT) architectures, as well as newly proposed M1DCNN. The greatest effect\nwas achieved for convolutional networks working exclusively on spectral data,\nand the best classification quality was achieved using a KAN-based transformer\narchitecture. All the experiments were conducted using seven openly available\nhyperspectral datasets. Our code is available at\nhttps://github.com/f-neumann77/HyperKAN.\n", "link": "http://arxiv.org/abs/2407.05278v2", "date": "2024-08-07", "relevancy": 1.5348, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperKAN%3A%20Kolmogorov-Arnold%20Networks%20make%20Hyperspectral%20Image%0A%20%20Classificators%20Smarter&body=Title%3A%20HyperKAN%3A%20Kolmogorov-Arnold%20Networks%20make%20Hyperspectral%20Image%0A%20%20Classificators%20Smarter%0AAuthor%3A%20Valeriy%20Lobanov%20and%20Nikita%20Firsov%20and%20Evgeny%20Myasnikov%20and%20Roman%20Khabibullin%20and%20Artem%20Nikonorov%0AAbstract%3A%20%20%20In%20traditional%20neural%20network%20architectures%2C%20a%20multilayer%20perceptron%20%28MLP%29%20is%0Atypically%20employed%20as%20a%20classification%20block%20following%20the%20feature%20extraction%0Astage.%20However%2C%20the%20Kolmogorov-Arnold%20Network%20%28KAN%29%20presents%20a%20promising%0Aalternative%20to%20MLP%2C%20offering%20the%20potential%20to%20enhance%20prediction%20accuracy.%20In%0Athis%20paper%2C%20we%20propose%20the%20replacement%20of%20linear%20and%20convolutional%20layers%20of%0Atraditional%20networks%20with%20KAN-based%20counterparts.%20These%20modifications%20allowed%0Aus%20to%20significantly%20increase%20the%20per-pixel%20classification%20accuracy%20for%0Ahyperspectral%20remote-sensing%20images.%20We%20modified%20seven%20different%20neural%20network%0Aarchitectures%20for%20hyperspectral%20image%20classification%20and%20observed%20a%20substantial%0Aimprovement%20in%20the%20classification%20accuracy%20across%20all%20the%20networks.%20The%0Aarchitectures%20considered%20in%20the%20paper%20include%20baseline%20MLP%2C%20state-of-the-art%201D%0A%281DCNN%29%20and%203D%20convolutional%20%28two%20different%203DCNN%2C%20NM3DCNN%29%2C%20and%20transformer%0A%28SSFTT%29%20architectures%2C%20as%20well%20as%20newly%20proposed%20M1DCNN.%20The%20greatest%20effect%0Awas%20achieved%20for%20convolutional%20networks%20working%20exclusively%20on%20spectral%20data%2C%0Aand%20the%20best%20classification%20quality%20was%20achieved%20using%20a%20KAN-based%20transformer%0Aarchitecture.%20All%20the%20experiments%20were%20conducted%20using%20seven%20openly%20available%0Ahyperspectral%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/f-neumann77/HyperKAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperKAN%253A%2520Kolmogorov-Arnold%2520Networks%2520make%2520Hyperspectral%2520Image%250A%2520%2520Classificators%2520Smarter%26entry.906535625%3DValeriy%2520Lobanov%2520and%2520Nikita%2520Firsov%2520and%2520Evgeny%2520Myasnikov%2520and%2520Roman%2520Khabibullin%2520and%2520Artem%2520Nikonorov%26entry.1292438233%3D%2520%2520In%2520traditional%2520neural%2520network%2520architectures%252C%2520a%2520multilayer%2520perceptron%2520%2528MLP%2529%2520is%250Atypically%2520employed%2520as%2520a%2520classification%2520block%2520following%2520the%2520feature%2520extraction%250Astage.%2520However%252C%2520the%2520Kolmogorov-Arnold%2520Network%2520%2528KAN%2529%2520presents%2520a%2520promising%250Aalternative%2520to%2520MLP%252C%2520offering%2520the%2520potential%2520to%2520enhance%2520prediction%2520accuracy.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520the%2520replacement%2520of%2520linear%2520and%2520convolutional%2520layers%2520of%250Atraditional%2520networks%2520with%2520KAN-based%2520counterparts.%2520These%2520modifications%2520allowed%250Aus%2520to%2520significantly%2520increase%2520the%2520per-pixel%2520classification%2520accuracy%2520for%250Ahyperspectral%2520remote-sensing%2520images.%2520We%2520modified%2520seven%2520different%2520neural%2520network%250Aarchitectures%2520for%2520hyperspectral%2520image%2520classification%2520and%2520observed%2520a%2520substantial%250Aimprovement%2520in%2520the%2520classification%2520accuracy%2520across%2520all%2520the%2520networks.%2520The%250Aarchitectures%2520considered%2520in%2520the%2520paper%2520include%2520baseline%2520MLP%252C%2520state-of-the-art%25201D%250A%25281DCNN%2529%2520and%25203D%2520convolutional%2520%2528two%2520different%25203DCNN%252C%2520NM3DCNN%2529%252C%2520and%2520transformer%250A%2528SSFTT%2529%2520architectures%252C%2520as%2520well%2520as%2520newly%2520proposed%2520M1DCNN.%2520The%2520greatest%2520effect%250Awas%2520achieved%2520for%2520convolutional%2520networks%2520working%2520exclusively%2520on%2520spectral%2520data%252C%250Aand%2520the%2520best%2520classification%2520quality%2520was%2520achieved%2520using%2520a%2520KAN-based%2520transformer%250Aarchitecture.%2520All%2520the%2520experiments%2520were%2520conducted%2520using%2520seven%2520openly%2520available%250Ahyperspectral%2520datasets.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/f-neumann77/HyperKAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperKAN%3A%20Kolmogorov-Arnold%20Networks%20make%20Hyperspectral%20Image%0A%20%20Classificators%20Smarter&entry.906535625=Valeriy%20Lobanov%20and%20Nikita%20Firsov%20and%20Evgeny%20Myasnikov%20and%20Roman%20Khabibullin%20and%20Artem%20Nikonorov&entry.1292438233=%20%20In%20traditional%20neural%20network%20architectures%2C%20a%20multilayer%20perceptron%20%28MLP%29%20is%0Atypically%20employed%20as%20a%20classification%20block%20following%20the%20feature%20extraction%0Astage.%20However%2C%20the%20Kolmogorov-Arnold%20Network%20%28KAN%29%20presents%20a%20promising%0Aalternative%20to%20MLP%2C%20offering%20the%20potential%20to%20enhance%20prediction%20accuracy.%20In%0Athis%20paper%2C%20we%20propose%20the%20replacement%20of%20linear%20and%20convolutional%20layers%20of%0Atraditional%20networks%20with%20KAN-based%20counterparts.%20These%20modifications%20allowed%0Aus%20to%20significantly%20increase%20the%20per-pixel%20classification%20accuracy%20for%0Ahyperspectral%20remote-sensing%20images.%20We%20modified%20seven%20different%20neural%20network%0Aarchitectures%20for%20hyperspectral%20image%20classification%20and%20observed%20a%20substantial%0Aimprovement%20in%20the%20classification%20accuracy%20across%20all%20the%20networks.%20The%0Aarchitectures%20considered%20in%20the%20paper%20include%20baseline%20MLP%2C%20state-of-the-art%201D%0A%281DCNN%29%20and%203D%20convolutional%20%28two%20different%203DCNN%2C%20NM3DCNN%29%2C%20and%20transformer%0A%28SSFTT%29%20architectures%2C%20as%20well%20as%20newly%20proposed%20M1DCNN.%20The%20greatest%20effect%0Awas%20achieved%20for%20convolutional%20networks%20working%20exclusively%20on%20spectral%20data%2C%0Aand%20the%20best%20classification%20quality%20was%20achieved%20using%20a%20KAN-based%20transformer%0Aarchitecture.%20All%20the%20experiments%20were%20conducted%20using%20seven%20openly%20available%0Ahyperspectral%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/f-neumann77/HyperKAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05278v2&entry.124074799=Read"},
{"title": "DrPlanner: Diagnosis and Repair of Motion Planners for Automated\n  Vehicles Using Large Language Models", "author": "Yuanfei Lin and Chenran Li and Mingyu Ding and Masayoshi Tomizuka and Wei Zhan and Matthias Althoff", "abstract": "  Motion planners are essential for the safe operation of automated vehicles\nacross various scenarios. However, no motion planning algorithm has achieved\nperfection in the literature, and improving its performance is often\ntime-consuming and labor-intensive. To tackle the aforementioned issues, we\npresent DrPlanner, the first framework designed to automatically diagnose and\nrepair motion planners using large language models. Initially, we generate a\nstructured description of the planner and its planned trajectories from both\nnatural and programming languages. Leveraging the profound capabilities of\nlarge language models, our framework returns repaired planners with detailed\ndiagnostic descriptions. Furthermore, our framework advances iteratively with\ncontinuous feedback from the evaluation of the repaired outcomes. Our approach\nis validated using both search- and sampling-based motion planners for\nautomated vehicles; experimental results highlight the need for demonstrations\nin the prompt and show the ability of our framework to effectively identify and\nrectify elusive issues.\n", "link": "http://arxiv.org/abs/2403.07470v2", "date": "2024-08-07", "relevancy": 1.5346, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5051}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DrPlanner%3A%20Diagnosis%20and%20Repair%20of%20Motion%20Planners%20for%20Automated%0A%20%20Vehicles%20Using%20Large%20Language%20Models&body=Title%3A%20DrPlanner%3A%20Diagnosis%20and%20Repair%20of%20Motion%20Planners%20for%20Automated%0A%20%20Vehicles%20Using%20Large%20Language%20Models%0AAuthor%3A%20Yuanfei%20Lin%20and%20Chenran%20Li%20and%20Mingyu%20Ding%20and%20Masayoshi%20Tomizuka%20and%20Wei%20Zhan%20and%20Matthias%20Althoff%0AAbstract%3A%20%20%20Motion%20planners%20are%20essential%20for%20the%20safe%20operation%20of%20automated%20vehicles%0Aacross%20various%20scenarios.%20However%2C%20no%20motion%20planning%20algorithm%20has%20achieved%0Aperfection%20in%20the%20literature%2C%20and%20improving%20its%20performance%20is%20often%0Atime-consuming%20and%20labor-intensive.%20To%20tackle%20the%20aforementioned%20issues%2C%20we%0Apresent%20DrPlanner%2C%20the%20first%20framework%20designed%20to%20automatically%20diagnose%20and%0Arepair%20motion%20planners%20using%20large%20language%20models.%20Initially%2C%20we%20generate%20a%0Astructured%20description%20of%20the%20planner%20and%20its%20planned%20trajectories%20from%20both%0Anatural%20and%20programming%20languages.%20Leveraging%20the%20profound%20capabilities%20of%0Alarge%20language%20models%2C%20our%20framework%20returns%20repaired%20planners%20with%20detailed%0Adiagnostic%20descriptions.%20Furthermore%2C%20our%20framework%20advances%20iteratively%20with%0Acontinuous%20feedback%20from%20the%20evaluation%20of%20the%20repaired%20outcomes.%20Our%20approach%0Ais%20validated%20using%20both%20search-%20and%20sampling-based%20motion%20planners%20for%0Aautomated%20vehicles%3B%20experimental%20results%20highlight%20the%20need%20for%20demonstrations%0Ain%20the%20prompt%20and%20show%20the%20ability%20of%20our%20framework%20to%20effectively%20identify%20and%0Arectify%20elusive%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrPlanner%253A%2520Diagnosis%2520and%2520Repair%2520of%2520Motion%2520Planners%2520for%2520Automated%250A%2520%2520Vehicles%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DYuanfei%2520Lin%2520and%2520Chenran%2520Li%2520and%2520Mingyu%2520Ding%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Wei%2520Zhan%2520and%2520Matthias%2520Althoff%26entry.1292438233%3D%2520%2520Motion%2520planners%2520are%2520essential%2520for%2520the%2520safe%2520operation%2520of%2520automated%2520vehicles%250Aacross%2520various%2520scenarios.%2520However%252C%2520no%2520motion%2520planning%2520algorithm%2520has%2520achieved%250Aperfection%2520in%2520the%2520literature%252C%2520and%2520improving%2520its%2520performance%2520is%2520often%250Atime-consuming%2520and%2520labor-intensive.%2520To%2520tackle%2520the%2520aforementioned%2520issues%252C%2520we%250Apresent%2520DrPlanner%252C%2520the%2520first%2520framework%2520designed%2520to%2520automatically%2520diagnose%2520and%250Arepair%2520motion%2520planners%2520using%2520large%2520language%2520models.%2520Initially%252C%2520we%2520generate%2520a%250Astructured%2520description%2520of%2520the%2520planner%2520and%2520its%2520planned%2520trajectories%2520from%2520both%250Anatural%2520and%2520programming%2520languages.%2520Leveraging%2520the%2520profound%2520capabilities%2520of%250Alarge%2520language%2520models%252C%2520our%2520framework%2520returns%2520repaired%2520planners%2520with%2520detailed%250Adiagnostic%2520descriptions.%2520Furthermore%252C%2520our%2520framework%2520advances%2520iteratively%2520with%250Acontinuous%2520feedback%2520from%2520the%2520evaluation%2520of%2520the%2520repaired%2520outcomes.%2520Our%2520approach%250Ais%2520validated%2520using%2520both%2520search-%2520and%2520sampling-based%2520motion%2520planners%2520for%250Aautomated%2520vehicles%253B%2520experimental%2520results%2520highlight%2520the%2520need%2520for%2520demonstrations%250Ain%2520the%2520prompt%2520and%2520show%2520the%2520ability%2520of%2520our%2520framework%2520to%2520effectively%2520identify%2520and%250Arectify%2520elusive%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DrPlanner%3A%20Diagnosis%20and%20Repair%20of%20Motion%20Planners%20for%20Automated%0A%20%20Vehicles%20Using%20Large%20Language%20Models&entry.906535625=Yuanfei%20Lin%20and%20Chenran%20Li%20and%20Mingyu%20Ding%20and%20Masayoshi%20Tomizuka%20and%20Wei%20Zhan%20and%20Matthias%20Althoff&entry.1292438233=%20%20Motion%20planners%20are%20essential%20for%20the%20safe%20operation%20of%20automated%20vehicles%0Aacross%20various%20scenarios.%20However%2C%20no%20motion%20planning%20algorithm%20has%20achieved%0Aperfection%20in%20the%20literature%2C%20and%20improving%20its%20performance%20is%20often%0Atime-consuming%20and%20labor-intensive.%20To%20tackle%20the%20aforementioned%20issues%2C%20we%0Apresent%20DrPlanner%2C%20the%20first%20framework%20designed%20to%20automatically%20diagnose%20and%0Arepair%20motion%20planners%20using%20large%20language%20models.%20Initially%2C%20we%20generate%20a%0Astructured%20description%20of%20the%20planner%20and%20its%20planned%20trajectories%20from%20both%0Anatural%20and%20programming%20languages.%20Leveraging%20the%20profound%20capabilities%20of%0Alarge%20language%20models%2C%20our%20framework%20returns%20repaired%20planners%20with%20detailed%0Adiagnostic%20descriptions.%20Furthermore%2C%20our%20framework%20advances%20iteratively%20with%0Acontinuous%20feedback%20from%20the%20evaluation%20of%20the%20repaired%20outcomes.%20Our%20approach%0Ais%20validated%20using%20both%20search-%20and%20sampling-based%20motion%20planners%20for%0Aautomated%20vehicles%3B%20experimental%20results%20highlight%20the%20need%20for%20demonstrations%0Ain%20the%20prompt%20and%20show%20the%20ability%20of%20our%20framework%20to%20effectively%20identify%20and%0Arectify%20elusive%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07470v2&entry.124074799=Read"},
{"title": "Generative Design of Periodic Orbits in the Restricted Three-Body\n  Problem", "author": "Alvaro Francisco Gil and Walther Litteri and Victor Rodriguez-Fernandez and David Camacho and Massimiliano Vasile", "abstract": "  The Three-Body Problem has fascinated scientists for centuries and it has\nbeen crucial in the design of modern space missions. Recent developments in\nGenerative Artificial Intelligence hold transformative promise for addressing\nthis longstanding problem. This work investigates the use of Variational\nAutoencoder (VAE) and its internal representation to generate periodic orbits.\nWe utilize a comprehensive dataset of periodic orbits in the Circular\nRestricted Three-Body Problem (CR3BP) to train deep-learning architectures that\ncapture key orbital characteristics, and we set up physical evaluation metrics\nfor the generated trajectories. Through this investigation, we seek to enhance\nthe understanding of how Generative AI can improve space mission planning and\nastrodynamics research, leading to novel, data-driven approaches in the field.\n", "link": "http://arxiv.org/abs/2408.03691v1", "date": "2024-08-07", "relevancy": 1.4423, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5014}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.457}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Design%20of%20Periodic%20Orbits%20in%20the%20Restricted%20Three-Body%0A%20%20Problem&body=Title%3A%20Generative%20Design%20of%20Periodic%20Orbits%20in%20the%20Restricted%20Three-Body%0A%20%20Problem%0AAuthor%3A%20Alvaro%20Francisco%20Gil%20and%20Walther%20Litteri%20and%20Victor%20Rodriguez-Fernandez%20and%20David%20Camacho%20and%20Massimiliano%20Vasile%0AAbstract%3A%20%20%20The%20Three-Body%20Problem%20has%20fascinated%20scientists%20for%20centuries%20and%20it%20has%0Abeen%20crucial%20in%20the%20design%20of%20modern%20space%20missions.%20Recent%20developments%20in%0AGenerative%20Artificial%20Intelligence%20hold%20transformative%20promise%20for%20addressing%0Athis%20longstanding%20problem.%20This%20work%20investigates%20the%20use%20of%20Variational%0AAutoencoder%20%28VAE%29%20and%20its%20internal%20representation%20to%20generate%20periodic%20orbits.%0AWe%20utilize%20a%20comprehensive%20dataset%20of%20periodic%20orbits%20in%20the%20Circular%0ARestricted%20Three-Body%20Problem%20%28CR3BP%29%20to%20train%20deep-learning%20architectures%20that%0Acapture%20key%20orbital%20characteristics%2C%20and%20we%20set%20up%20physical%20evaluation%20metrics%0Afor%20the%20generated%20trajectories.%20Through%20this%20investigation%2C%20we%20seek%20to%20enhance%0Athe%20understanding%20of%20how%20Generative%20AI%20can%20improve%20space%20mission%20planning%20and%0Aastrodynamics%20research%2C%20leading%20to%20novel%2C%20data-driven%20approaches%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Design%2520of%2520Periodic%2520Orbits%2520in%2520the%2520Restricted%2520Three-Body%250A%2520%2520Problem%26entry.906535625%3DAlvaro%2520Francisco%2520Gil%2520and%2520Walther%2520Litteri%2520and%2520Victor%2520Rodriguez-Fernandez%2520and%2520David%2520Camacho%2520and%2520Massimiliano%2520Vasile%26entry.1292438233%3D%2520%2520The%2520Three-Body%2520Problem%2520has%2520fascinated%2520scientists%2520for%2520centuries%2520and%2520it%2520has%250Abeen%2520crucial%2520in%2520the%2520design%2520of%2520modern%2520space%2520missions.%2520Recent%2520developments%2520in%250AGenerative%2520Artificial%2520Intelligence%2520hold%2520transformative%2520promise%2520for%2520addressing%250Athis%2520longstanding%2520problem.%2520This%2520work%2520investigates%2520the%2520use%2520of%2520Variational%250AAutoencoder%2520%2528VAE%2529%2520and%2520its%2520internal%2520representation%2520to%2520generate%2520periodic%2520orbits.%250AWe%2520utilize%2520a%2520comprehensive%2520dataset%2520of%2520periodic%2520orbits%2520in%2520the%2520Circular%250ARestricted%2520Three-Body%2520Problem%2520%2528CR3BP%2529%2520to%2520train%2520deep-learning%2520architectures%2520that%250Acapture%2520key%2520orbital%2520characteristics%252C%2520and%2520we%2520set%2520up%2520physical%2520evaluation%2520metrics%250Afor%2520the%2520generated%2520trajectories.%2520Through%2520this%2520investigation%252C%2520we%2520seek%2520to%2520enhance%250Athe%2520understanding%2520of%2520how%2520Generative%2520AI%2520can%2520improve%2520space%2520mission%2520planning%2520and%250Aastrodynamics%2520research%252C%2520leading%2520to%2520novel%252C%2520data-driven%2520approaches%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Design%20of%20Periodic%20Orbits%20in%20the%20Restricted%20Three-Body%0A%20%20Problem&entry.906535625=Alvaro%20Francisco%20Gil%20and%20Walther%20Litteri%20and%20Victor%20Rodriguez-Fernandez%20and%20David%20Camacho%20and%20Massimiliano%20Vasile&entry.1292438233=%20%20The%20Three-Body%20Problem%20has%20fascinated%20scientists%20for%20centuries%20and%20it%20has%0Abeen%20crucial%20in%20the%20design%20of%20modern%20space%20missions.%20Recent%20developments%20in%0AGenerative%20Artificial%20Intelligence%20hold%20transformative%20promise%20for%20addressing%0Athis%20longstanding%20problem.%20This%20work%20investigates%20the%20use%20of%20Variational%0AAutoencoder%20%28VAE%29%20and%20its%20internal%20representation%20to%20generate%20periodic%20orbits.%0AWe%20utilize%20a%20comprehensive%20dataset%20of%20periodic%20orbits%20in%20the%20Circular%0ARestricted%20Three-Body%20Problem%20%28CR3BP%29%20to%20train%20deep-learning%20architectures%20that%0Acapture%20key%20orbital%20characteristics%2C%20and%20we%20set%20up%20physical%20evaluation%20metrics%0Afor%20the%20generated%20trajectories.%20Through%20this%20investigation%2C%20we%20seek%20to%20enhance%0Athe%20understanding%20of%20how%20Generative%20AI%20can%20improve%20space%20mission%20planning%20and%0Aastrodynamics%20research%2C%20leading%20to%20novel%2C%20data-driven%20approaches%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03691v1&entry.124074799=Read"},
{"title": "PackMamba: Efficient Processing of Variable-Length Sequences in Mamba\n  training", "author": "Haoran Xu and Ziqian Liu and Rong Fu and Zhongling Su and Zerui Wang and Zheng Cai and Zhilin Pei and Xingcheng Zhang", "abstract": "  With the evolution of large language models, traditional Transformer models\nbecome computationally demanding for lengthy sequences due to the quadratic\ngrowth in computation with respect to the sequence length. Mamba, emerging as a\ngroundbreaking architecture in the field of generative AI, demonstrates\nremarkable proficiency in handling elongated sequences with reduced\ncomputational and memory complexity. Nevertheless, the existing training\nframework of Mamba presents inefficiency with variable-length sequence inputs.\nEither single-sequence training results in low GPU utilization, or batched\nprocessing of variable-length sequences to a maximum length incurs considerable\nmemory and computational overhead. To address this problem, we analyze the\nperformance of bottleneck operators in Mamba under diverse tensor shapes and\nproposed PackMamba, a high-throughput Mamba that efficiently handles\nvariable-length sequences. Diving deep into state-space models (SSMs), we\nmodify the parallel operators to avoid passing information between individual\nsequences while maintaining high performance. Experimental results on an NVIDIA\nA100 GPU demonstrate throughput exceeding the baseline single-sequence\nprocessing scheme: 3.06x speedup on the 1.4B model and 2.62x on the 2.8B model.\n", "link": "http://arxiv.org/abs/2408.03865v1", "date": "2024-08-07", "relevancy": 1.4415, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4813}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PackMamba%3A%20Efficient%20Processing%20of%20Variable-Length%20Sequences%20in%20Mamba%0A%20%20training&body=Title%3A%20PackMamba%3A%20Efficient%20Processing%20of%20Variable-Length%20Sequences%20in%20Mamba%0A%20%20training%0AAuthor%3A%20Haoran%20Xu%20and%20Ziqian%20Liu%20and%20Rong%20Fu%20and%20Zhongling%20Su%20and%20Zerui%20Wang%20and%20Zheng%20Cai%20and%20Zhilin%20Pei%20and%20Xingcheng%20Zhang%0AAbstract%3A%20%20%20With%20the%20evolution%20of%20large%20language%20models%2C%20traditional%20Transformer%20models%0Abecome%20computationally%20demanding%20for%20lengthy%20sequences%20due%20to%20the%20quadratic%0Agrowth%20in%20computation%20with%20respect%20to%20the%20sequence%20length.%20Mamba%2C%20emerging%20as%20a%0Agroundbreaking%20architecture%20in%20the%20field%20of%20generative%20AI%2C%20demonstrates%0Aremarkable%20proficiency%20in%20handling%20elongated%20sequences%20with%20reduced%0Acomputational%20and%20memory%20complexity.%20Nevertheless%2C%20the%20existing%20training%0Aframework%20of%20Mamba%20presents%20inefficiency%20with%20variable-length%20sequence%20inputs.%0AEither%20single-sequence%20training%20results%20in%20low%20GPU%20utilization%2C%20or%20batched%0Aprocessing%20of%20variable-length%20sequences%20to%20a%20maximum%20length%20incurs%20considerable%0Amemory%20and%20computational%20overhead.%20To%20address%20this%20problem%2C%20we%20analyze%20the%0Aperformance%20of%20bottleneck%20operators%20in%20Mamba%20under%20diverse%20tensor%20shapes%20and%0Aproposed%20PackMamba%2C%20a%20high-throughput%20Mamba%20that%20efficiently%20handles%0Avariable-length%20sequences.%20Diving%20deep%20into%20state-space%20models%20%28SSMs%29%2C%20we%0Amodify%20the%20parallel%20operators%20to%20avoid%20passing%20information%20between%20individual%0Asequences%20while%20maintaining%20high%20performance.%20Experimental%20results%20on%20an%20NVIDIA%0AA100%20GPU%20demonstrate%20throughput%20exceeding%20the%20baseline%20single-sequence%0Aprocessing%20scheme%3A%203.06x%20speedup%20on%20the%201.4B%20model%20and%202.62x%20on%20the%202.8B%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPackMamba%253A%2520Efficient%2520Processing%2520of%2520Variable-Length%2520Sequences%2520in%2520Mamba%250A%2520%2520training%26entry.906535625%3DHaoran%2520Xu%2520and%2520Ziqian%2520Liu%2520and%2520Rong%2520Fu%2520and%2520Zhongling%2520Su%2520and%2520Zerui%2520Wang%2520and%2520Zheng%2520Cai%2520and%2520Zhilin%2520Pei%2520and%2520Xingcheng%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520evolution%2520of%2520large%2520language%2520models%252C%2520traditional%2520Transformer%2520models%250Abecome%2520computationally%2520demanding%2520for%2520lengthy%2520sequences%2520due%2520to%2520the%2520quadratic%250Agrowth%2520in%2520computation%2520with%2520respect%2520to%2520the%2520sequence%2520length.%2520Mamba%252C%2520emerging%2520as%2520a%250Agroundbreaking%2520architecture%2520in%2520the%2520field%2520of%2520generative%2520AI%252C%2520demonstrates%250Aremarkable%2520proficiency%2520in%2520handling%2520elongated%2520sequences%2520with%2520reduced%250Acomputational%2520and%2520memory%2520complexity.%2520Nevertheless%252C%2520the%2520existing%2520training%250Aframework%2520of%2520Mamba%2520presents%2520inefficiency%2520with%2520variable-length%2520sequence%2520inputs.%250AEither%2520single-sequence%2520training%2520results%2520in%2520low%2520GPU%2520utilization%252C%2520or%2520batched%250Aprocessing%2520of%2520variable-length%2520sequences%2520to%2520a%2520maximum%2520length%2520incurs%2520considerable%250Amemory%2520and%2520computational%2520overhead.%2520To%2520address%2520this%2520problem%252C%2520we%2520analyze%2520the%250Aperformance%2520of%2520bottleneck%2520operators%2520in%2520Mamba%2520under%2520diverse%2520tensor%2520shapes%2520and%250Aproposed%2520PackMamba%252C%2520a%2520high-throughput%2520Mamba%2520that%2520efficiently%2520handles%250Avariable-length%2520sequences.%2520Diving%2520deep%2520into%2520state-space%2520models%2520%2528SSMs%2529%252C%2520we%250Amodify%2520the%2520parallel%2520operators%2520to%2520avoid%2520passing%2520information%2520between%2520individual%250Asequences%2520while%2520maintaining%2520high%2520performance.%2520Experimental%2520results%2520on%2520an%2520NVIDIA%250AA100%2520GPU%2520demonstrate%2520throughput%2520exceeding%2520the%2520baseline%2520single-sequence%250Aprocessing%2520scheme%253A%25203.06x%2520speedup%2520on%2520the%25201.4B%2520model%2520and%25202.62x%2520on%2520the%25202.8B%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PackMamba%3A%20Efficient%20Processing%20of%20Variable-Length%20Sequences%20in%20Mamba%0A%20%20training&entry.906535625=Haoran%20Xu%20and%20Ziqian%20Liu%20and%20Rong%20Fu%20and%20Zhongling%20Su%20and%20Zerui%20Wang%20and%20Zheng%20Cai%20and%20Zhilin%20Pei%20and%20Xingcheng%20Zhang&entry.1292438233=%20%20With%20the%20evolution%20of%20large%20language%20models%2C%20traditional%20Transformer%20models%0Abecome%20computationally%20demanding%20for%20lengthy%20sequences%20due%20to%20the%20quadratic%0Agrowth%20in%20computation%20with%20respect%20to%20the%20sequence%20length.%20Mamba%2C%20emerging%20as%20a%0Agroundbreaking%20architecture%20in%20the%20field%20of%20generative%20AI%2C%20demonstrates%0Aremarkable%20proficiency%20in%20handling%20elongated%20sequences%20with%20reduced%0Acomputational%20and%20memory%20complexity.%20Nevertheless%2C%20the%20existing%20training%0Aframework%20of%20Mamba%20presents%20inefficiency%20with%20variable-length%20sequence%20inputs.%0AEither%20single-sequence%20training%20results%20in%20low%20GPU%20utilization%2C%20or%20batched%0Aprocessing%20of%20variable-length%20sequences%20to%20a%20maximum%20length%20incurs%20considerable%0Amemory%20and%20computational%20overhead.%20To%20address%20this%20problem%2C%20we%20analyze%20the%0Aperformance%20of%20bottleneck%20operators%20in%20Mamba%20under%20diverse%20tensor%20shapes%20and%0Aproposed%20PackMamba%2C%20a%20high-throughput%20Mamba%20that%20efficiently%20handles%0Avariable-length%20sequences.%20Diving%20deep%20into%20state-space%20models%20%28SSMs%29%2C%20we%0Amodify%20the%20parallel%20operators%20to%20avoid%20passing%20information%20between%20individual%0Asequences%20while%20maintaining%20high%20performance.%20Experimental%20results%20on%20an%20NVIDIA%0AA100%20GPU%20demonstrate%20throughput%20exceeding%20the%20baseline%20single-sequence%0Aprocessing%20scheme%3A%203.06x%20speedup%20on%20the%201.4B%20model%20and%202.62x%20on%20the%202.8B%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03865v1&entry.124074799=Read"},
{"title": "Inter-Series Transformer: Attending to Products in Time Series\n  Forecasting", "author": "Rares Cristian and Pavithra Harsha and Clemente Ocejo and Georgia Perakis and Brian Quanz and Ioannis Spantidakis and Hamza Zerhouni", "abstract": "  Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.\n", "link": "http://arxiv.org/abs/2408.03872v1", "date": "2024-08-07", "relevancy": 1.3147, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4871}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4286}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inter-Series%20Transformer%3A%20Attending%20to%20Products%20in%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Inter-Series%20Transformer%3A%20Attending%20to%20Products%20in%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Rares%20Cristian%20and%20Pavithra%20Harsha%20and%20Clemente%20Ocejo%20and%20Georgia%20Perakis%20and%20Brian%20Quanz%20and%20Ioannis%20Spantidakis%20and%20Hamza%20Zerhouni%0AAbstract%3A%20%20%20Time%20series%20forecasting%20is%20an%20important%20task%20in%20many%20fields%20ranging%20from%0Asupply%20chain%20management%20to%20weather%20forecasting.%20Recently%2C%20Transformer%20neural%0Anetwork%20architectures%20have%20shown%20promising%20results%20in%20forecasting%20on%20common%0Atime%20series%20benchmark%20datasets.%20However%2C%20application%20to%20supply%20chain%20demand%0Aforecasting%2C%20which%20can%20have%20challenging%20characteristics%20such%20as%20sparsity%20and%0Across-series%20effects%2C%20has%20been%20limited.%0A%20%20In%20this%20work%2C%20we%20explore%20the%20application%20of%20Transformer-based%20models%20to%0Asupply%20chain%20demand%20forecasting.%20In%20particular%2C%20we%20develop%20a%20new%0ATransformer-based%20forecasting%20approach%20using%20a%20shared%2C%20multi-task%20per-time%0Aseries%20network%20with%20an%20initial%20component%20applying%20attention%20across%20time%20series%2C%0Ato%20capture%20interactions%20and%20help%20address%20sparsity.%20We%20provide%20a%20case%20study%0Aapplying%20our%20approach%20to%20successfully%20improve%20demand%20prediction%20for%20a%20medical%0Adevice%20manufacturing%20company.%20To%20further%20validate%20our%20approach%2C%20we%20also%20apply%0Ait%20to%20public%20demand%20forecasting%20datasets%20as%20well%20and%20demonstrate%20competitive%20to%0Asuperior%20performance%20compared%20to%20a%20variety%20of%20baseline%20and%20state-of-the-art%0Aforecast%20methods%20across%20the%20private%20and%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInter-Series%2520Transformer%253A%2520Attending%2520to%2520Products%2520in%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DRares%2520Cristian%2520and%2520Pavithra%2520Harsha%2520and%2520Clemente%2520Ocejo%2520and%2520Georgia%2520Perakis%2520and%2520Brian%2520Quanz%2520and%2520Ioannis%2520Spantidakis%2520and%2520Hamza%2520Zerhouni%26entry.1292438233%3D%2520%2520Time%2520series%2520forecasting%2520is%2520an%2520important%2520task%2520in%2520many%2520fields%2520ranging%2520from%250Asupply%2520chain%2520management%2520to%2520weather%2520forecasting.%2520Recently%252C%2520Transformer%2520neural%250Anetwork%2520architectures%2520have%2520shown%2520promising%2520results%2520in%2520forecasting%2520on%2520common%250Atime%2520series%2520benchmark%2520datasets.%2520However%252C%2520application%2520to%2520supply%2520chain%2520demand%250Aforecasting%252C%2520which%2520can%2520have%2520challenging%2520characteristics%2520such%2520as%2520sparsity%2520and%250Across-series%2520effects%252C%2520has%2520been%2520limited.%250A%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520application%2520of%2520Transformer-based%2520models%2520to%250Asupply%2520chain%2520demand%2520forecasting.%2520In%2520particular%252C%2520we%2520develop%2520a%2520new%250ATransformer-based%2520forecasting%2520approach%2520using%2520a%2520shared%252C%2520multi-task%2520per-time%250Aseries%2520network%2520with%2520an%2520initial%2520component%2520applying%2520attention%2520across%2520time%2520series%252C%250Ato%2520capture%2520interactions%2520and%2520help%2520address%2520sparsity.%2520We%2520provide%2520a%2520case%2520study%250Aapplying%2520our%2520approach%2520to%2520successfully%2520improve%2520demand%2520prediction%2520for%2520a%2520medical%250Adevice%2520manufacturing%2520company.%2520To%2520further%2520validate%2520our%2520approach%252C%2520we%2520also%2520apply%250Ait%2520to%2520public%2520demand%2520forecasting%2520datasets%2520as%2520well%2520and%2520demonstrate%2520competitive%2520to%250Asuperior%2520performance%2520compared%2520to%2520a%2520variety%2520of%2520baseline%2520and%2520state-of-the-art%250Aforecast%2520methods%2520across%2520the%2520private%2520and%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inter-Series%20Transformer%3A%20Attending%20to%20Products%20in%20Time%20Series%0A%20%20Forecasting&entry.906535625=Rares%20Cristian%20and%20Pavithra%20Harsha%20and%20Clemente%20Ocejo%20and%20Georgia%20Perakis%20and%20Brian%20Quanz%20and%20Ioannis%20Spantidakis%20and%20Hamza%20Zerhouni&entry.1292438233=%20%20Time%20series%20forecasting%20is%20an%20important%20task%20in%20many%20fields%20ranging%20from%0Asupply%20chain%20management%20to%20weather%20forecasting.%20Recently%2C%20Transformer%20neural%0Anetwork%20architectures%20have%20shown%20promising%20results%20in%20forecasting%20on%20common%0Atime%20series%20benchmark%20datasets.%20However%2C%20application%20to%20supply%20chain%20demand%0Aforecasting%2C%20which%20can%20have%20challenging%20characteristics%20such%20as%20sparsity%20and%0Across-series%20effects%2C%20has%20been%20limited.%0A%20%20In%20this%20work%2C%20we%20explore%20the%20application%20of%20Transformer-based%20models%20to%0Asupply%20chain%20demand%20forecasting.%20In%20particular%2C%20we%20develop%20a%20new%0ATransformer-based%20forecasting%20approach%20using%20a%20shared%2C%20multi-task%20per-time%0Aseries%20network%20with%20an%20initial%20component%20applying%20attention%20across%20time%20series%2C%0Ato%20capture%20interactions%20and%20help%20address%20sparsity.%20We%20provide%20a%20case%20study%0Aapplying%20our%20approach%20to%20successfully%20improve%20demand%20prediction%20for%20a%20medical%0Adevice%20manufacturing%20company.%20To%20further%20validate%20our%20approach%2C%20we%20also%20apply%0Ait%20to%20public%20demand%20forecasting%20datasets%20as%20well%20and%20demonstrate%20competitive%20to%0Asuperior%20performance%20compared%20to%20a%20variety%20of%20baseline%20and%20state-of-the-art%0Aforecast%20methods%20across%20the%20private%20and%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03872v1&entry.124074799=Read"},
{"title": "An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases", "author": "Dylan Bouchard", "abstract": "  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. This paper aims to provide a technical guide for\npractitioners to assess bias and fairness risks in LLM use cases. The main\ncontribution of this work is a decision framework that allows practitioners to\ndetermine which metrics to use for a specific LLM use case. To achieve this,\nthis study categorizes LLM bias and fairness risks, maps those risks to a\ntaxonomy of LLM use cases, and then formally defines various metrics to assess\neach type of risk. As part of this work, several new bias and fairness metrics\nare introduced, including innovative counterfactual metrics as well as metrics\nbased on stereotype classifiers. Instead of focusing solely on the model\nitself, the sensitivity of both prompt-risk and model-risk are taken into\naccount by defining evaluations at the level of an LLM use case, characterized\nby a model and a population of prompts. Furthermore, because all of the\nevaluation metrics are calculated solely using the LLM output, the proposed\nframework is highly practical and easily actionable for practitioners.\n", "link": "http://arxiv.org/abs/2407.10853v2", "date": "2024-08-07", "relevancy": 1.3745, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4692}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Actionable%20Framework%20for%20Assessing%20Bias%20and%20Fairness%20in%20Large%0A%20%20Language%20Model%20Use%20Cases&body=Title%3A%20An%20Actionable%20Framework%20for%20Assessing%20Bias%20and%20Fairness%20in%20Large%0A%20%20Language%20Model%20Use%20Cases%0AAuthor%3A%20Dylan%20Bouchard%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20exhibit%20bias%20in%20a%20variety%20of%20ways.%20Such%0Abiases%20can%20create%20or%20exacerbate%20unfair%20outcomes%20for%20certain%20groups%20within%20a%0Aprotected%20attribute%2C%20including%2C%20but%20not%20limited%20to%20sex%2C%20race%2C%20sexual%0Aorientation%2C%20or%20age.%20This%20paper%20aims%20to%20provide%20a%20technical%20guide%20for%0Apractitioners%20to%20assess%20bias%20and%20fairness%20risks%20in%20LLM%20use%20cases.%20The%20main%0Acontribution%20of%20this%20work%20is%20a%20decision%20framework%20that%20allows%20practitioners%20to%0Adetermine%20which%20metrics%20to%20use%20for%20a%20specific%20LLM%20use%20case.%20To%20achieve%20this%2C%0Athis%20study%20categorizes%20LLM%20bias%20and%20fairness%20risks%2C%20maps%20those%20risks%20to%20a%0Ataxonomy%20of%20LLM%20use%20cases%2C%20and%20then%20formally%20defines%20various%20metrics%20to%20assess%0Aeach%20type%20of%20risk.%20As%20part%20of%20this%20work%2C%20several%20new%20bias%20and%20fairness%20metrics%0Aare%20introduced%2C%20including%20innovative%20counterfactual%20metrics%20as%20well%20as%20metrics%0Abased%20on%20stereotype%20classifiers.%20Instead%20of%20focusing%20solely%20on%20the%20model%0Aitself%2C%20the%20sensitivity%20of%20both%20prompt-risk%20and%20model-risk%20are%20taken%20into%0Aaccount%20by%20defining%20evaluations%20at%20the%20level%20of%20an%20LLM%20use%20case%2C%20characterized%0Aby%20a%20model%20and%20a%20population%20of%20prompts.%20Furthermore%2C%20because%20all%20of%20the%0Aevaluation%20metrics%20are%20calculated%20solely%20using%20the%20LLM%20output%2C%20the%20proposed%0Aframework%20is%20highly%20practical%20and%20easily%20actionable%20for%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Actionable%2520Framework%2520for%2520Assessing%2520Bias%2520and%2520Fairness%2520in%2520Large%250A%2520%2520Language%2520Model%2520Use%2520Cases%26entry.906535625%3DDylan%2520Bouchard%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520exhibit%2520bias%2520in%2520a%2520variety%2520of%2520ways.%2520Such%250Abiases%2520can%2520create%2520or%2520exacerbate%2520unfair%2520outcomes%2520for%2520certain%2520groups%2520within%2520a%250Aprotected%2520attribute%252C%2520including%252C%2520but%2520not%2520limited%2520to%2520sex%252C%2520race%252C%2520sexual%250Aorientation%252C%2520or%2520age.%2520This%2520paper%2520aims%2520to%2520provide%2520a%2520technical%2520guide%2520for%250Apractitioners%2520to%2520assess%2520bias%2520and%2520fairness%2520risks%2520in%2520LLM%2520use%2520cases.%2520The%2520main%250Acontribution%2520of%2520this%2520work%2520is%2520a%2520decision%2520framework%2520that%2520allows%2520practitioners%2520to%250Adetermine%2520which%2520metrics%2520to%2520use%2520for%2520a%2520specific%2520LLM%2520use%2520case.%2520To%2520achieve%2520this%252C%250Athis%2520study%2520categorizes%2520LLM%2520bias%2520and%2520fairness%2520risks%252C%2520maps%2520those%2520risks%2520to%2520a%250Ataxonomy%2520of%2520LLM%2520use%2520cases%252C%2520and%2520then%2520formally%2520defines%2520various%2520metrics%2520to%2520assess%250Aeach%2520type%2520of%2520risk.%2520As%2520part%2520of%2520this%2520work%252C%2520several%2520new%2520bias%2520and%2520fairness%2520metrics%250Aare%2520introduced%252C%2520including%2520innovative%2520counterfactual%2520metrics%2520as%2520well%2520as%2520metrics%250Abased%2520on%2520stereotype%2520classifiers.%2520Instead%2520of%2520focusing%2520solely%2520on%2520the%2520model%250Aitself%252C%2520the%2520sensitivity%2520of%2520both%2520prompt-risk%2520and%2520model-risk%2520are%2520taken%2520into%250Aaccount%2520by%2520defining%2520evaluations%2520at%2520the%2520level%2520of%2520an%2520LLM%2520use%2520case%252C%2520characterized%250Aby%2520a%2520model%2520and%2520a%2520population%2520of%2520prompts.%2520Furthermore%252C%2520because%2520all%2520of%2520the%250Aevaluation%2520metrics%2520are%2520calculated%2520solely%2520using%2520the%2520LLM%2520output%252C%2520the%2520proposed%250Aframework%2520is%2520highly%2520practical%2520and%2520easily%2520actionable%2520for%2520practitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Actionable%20Framework%20for%20Assessing%20Bias%20and%20Fairness%20in%20Large%0A%20%20Language%20Model%20Use%20Cases&entry.906535625=Dylan%20Bouchard&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20exhibit%20bias%20in%20a%20variety%20of%20ways.%20Such%0Abiases%20can%20create%20or%20exacerbate%20unfair%20outcomes%20for%20certain%20groups%20within%20a%0Aprotected%20attribute%2C%20including%2C%20but%20not%20limited%20to%20sex%2C%20race%2C%20sexual%0Aorientation%2C%20or%20age.%20This%20paper%20aims%20to%20provide%20a%20technical%20guide%20for%0Apractitioners%20to%20assess%20bias%20and%20fairness%20risks%20in%20LLM%20use%20cases.%20The%20main%0Acontribution%20of%20this%20work%20is%20a%20decision%20framework%20that%20allows%20practitioners%20to%0Adetermine%20which%20metrics%20to%20use%20for%20a%20specific%20LLM%20use%20case.%20To%20achieve%20this%2C%0Athis%20study%20categorizes%20LLM%20bias%20and%20fairness%20risks%2C%20maps%20those%20risks%20to%20a%0Ataxonomy%20of%20LLM%20use%20cases%2C%20and%20then%20formally%20defines%20various%20metrics%20to%20assess%0Aeach%20type%20of%20risk.%20As%20part%20of%20this%20work%2C%20several%20new%20bias%20and%20fairness%20metrics%0Aare%20introduced%2C%20including%20innovative%20counterfactual%20metrics%20as%20well%20as%20metrics%0Abased%20on%20stereotype%20classifiers.%20Instead%20of%20focusing%20solely%20on%20the%20model%0Aitself%2C%20the%20sensitivity%20of%20both%20prompt-risk%20and%20model-risk%20are%20taken%20into%0Aaccount%20by%20defining%20evaluations%20at%20the%20level%20of%20an%20LLM%20use%20case%2C%20characterized%0Aby%20a%20model%20and%20a%20population%20of%20prompts.%20Furthermore%2C%20because%20all%20of%20the%0Aevaluation%20metrics%20are%20calculated%20solely%20using%20the%20LLM%20output%2C%20the%20proposed%0Aframework%20is%20highly%20practical%20and%20easily%20actionable%20for%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10853v2&entry.124074799=Read"},
{"title": "Target Prompting for Information Extraction with Vision Language Model", "author": "Dipankar Medhi", "abstract": "  The recent trend in the Large Vision and Language model has brought a new\nchange in how information extraction systems are built. VLMs have set a new\nbenchmark with their State-of-the-art techniques in understanding documents and\nbuilding question-answering systems across various industries. They are\nsignificantly better at generating text from document images and providing\naccurate answers to questions. However, there are still some challenges in\neffectively utilizing these models to build a precise conversational system.\nGeneral prompting techniques used with large language models are often not\nsuitable for these specially designed vision language models. The output\ngenerated by such generic input prompts is ordinary and may contain information\ngaps when compared with the actual content of the document. To obtain more\naccurate and specific answers, a well-targeted prompt is required by the vision\nlanguage model, along with the document image. In this paper, a technique is\ndiscussed called Target prompting, which focuses on explicitly targeting parts\nof document images and generating related answers from those specific regions\nonly. The paper also covers the evaluation of response for each prompting\ntechnique using different user queries and input prompts.\n", "link": "http://arxiv.org/abs/2408.03834v1", "date": "2024-08-07", "relevancy": 1.4553, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4962}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4884}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Target%20Prompting%20for%20Information%20Extraction%20with%20Vision%20Language%20Model&body=Title%3A%20Target%20Prompting%20for%20Information%20Extraction%20with%20Vision%20Language%20Model%0AAuthor%3A%20Dipankar%20Medhi%0AAbstract%3A%20%20%20The%20recent%20trend%20in%20the%20Large%20Vision%20and%20Language%20model%20has%20brought%20a%20new%0Achange%20in%20how%20information%20extraction%20systems%20are%20built.%20VLMs%20have%20set%20a%20new%0Abenchmark%20with%20their%20State-of-the-art%20techniques%20in%20understanding%20documents%20and%0Abuilding%20question-answering%20systems%20across%20various%20industries.%20They%20are%0Asignificantly%20better%20at%20generating%20text%20from%20document%20images%20and%20providing%0Aaccurate%20answers%20to%20questions.%20However%2C%20there%20are%20still%20some%20challenges%20in%0Aeffectively%20utilizing%20these%20models%20to%20build%20a%20precise%20conversational%20system.%0AGeneral%20prompting%20techniques%20used%20with%20large%20language%20models%20are%20often%20not%0Asuitable%20for%20these%20specially%20designed%20vision%20language%20models.%20The%20output%0Agenerated%20by%20such%20generic%20input%20prompts%20is%20ordinary%20and%20may%20contain%20information%0Agaps%20when%20compared%20with%20the%20actual%20content%20of%20the%20document.%20To%20obtain%20more%0Aaccurate%20and%20specific%20answers%2C%20a%20well-targeted%20prompt%20is%20required%20by%20the%20vision%0Alanguage%20model%2C%20along%20with%20the%20document%20image.%20In%20this%20paper%2C%20a%20technique%20is%0Adiscussed%20called%20Target%20prompting%2C%20which%20focuses%20on%20explicitly%20targeting%20parts%0Aof%20document%20images%20and%20generating%20related%20answers%20from%20those%20specific%20regions%0Aonly.%20The%20paper%20also%20covers%20the%20evaluation%20of%20response%20for%20each%20prompting%0Atechnique%20using%20different%20user%20queries%20and%20input%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTarget%2520Prompting%2520for%2520Information%2520Extraction%2520with%2520Vision%2520Language%2520Model%26entry.906535625%3DDipankar%2520Medhi%26entry.1292438233%3D%2520%2520The%2520recent%2520trend%2520in%2520the%2520Large%2520Vision%2520and%2520Language%2520model%2520has%2520brought%2520a%2520new%250Achange%2520in%2520how%2520information%2520extraction%2520systems%2520are%2520built.%2520VLMs%2520have%2520set%2520a%2520new%250Abenchmark%2520with%2520their%2520State-of-the-art%2520techniques%2520in%2520understanding%2520documents%2520and%250Abuilding%2520question-answering%2520systems%2520across%2520various%2520industries.%2520They%2520are%250Asignificantly%2520better%2520at%2520generating%2520text%2520from%2520document%2520images%2520and%2520providing%250Aaccurate%2520answers%2520to%2520questions.%2520However%252C%2520there%2520are%2520still%2520some%2520challenges%2520in%250Aeffectively%2520utilizing%2520these%2520models%2520to%2520build%2520a%2520precise%2520conversational%2520system.%250AGeneral%2520prompting%2520techniques%2520used%2520with%2520large%2520language%2520models%2520are%2520often%2520not%250Asuitable%2520for%2520these%2520specially%2520designed%2520vision%2520language%2520models.%2520The%2520output%250Agenerated%2520by%2520such%2520generic%2520input%2520prompts%2520is%2520ordinary%2520and%2520may%2520contain%2520information%250Agaps%2520when%2520compared%2520with%2520the%2520actual%2520content%2520of%2520the%2520document.%2520To%2520obtain%2520more%250Aaccurate%2520and%2520specific%2520answers%252C%2520a%2520well-targeted%2520prompt%2520is%2520required%2520by%2520the%2520vision%250Alanguage%2520model%252C%2520along%2520with%2520the%2520document%2520image.%2520In%2520this%2520paper%252C%2520a%2520technique%2520is%250Adiscussed%2520called%2520Target%2520prompting%252C%2520which%2520focuses%2520on%2520explicitly%2520targeting%2520parts%250Aof%2520document%2520images%2520and%2520generating%2520related%2520answers%2520from%2520those%2520specific%2520regions%250Aonly.%2520The%2520paper%2520also%2520covers%2520the%2520evaluation%2520of%2520response%2520for%2520each%2520prompting%250Atechnique%2520using%2520different%2520user%2520queries%2520and%2520input%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Target%20Prompting%20for%20Information%20Extraction%20with%20Vision%20Language%20Model&entry.906535625=Dipankar%20Medhi&entry.1292438233=%20%20The%20recent%20trend%20in%20the%20Large%20Vision%20and%20Language%20model%20has%20brought%20a%20new%0Achange%20in%20how%20information%20extraction%20systems%20are%20built.%20VLMs%20have%20set%20a%20new%0Abenchmark%20with%20their%20State-of-the-art%20techniques%20in%20understanding%20documents%20and%0Abuilding%20question-answering%20systems%20across%20various%20industries.%20They%20are%0Asignificantly%20better%20at%20generating%20text%20from%20document%20images%20and%20providing%0Aaccurate%20answers%20to%20questions.%20However%2C%20there%20are%20still%20some%20challenges%20in%0Aeffectively%20utilizing%20these%20models%20to%20build%20a%20precise%20conversational%20system.%0AGeneral%20prompting%20techniques%20used%20with%20large%20language%20models%20are%20often%20not%0Asuitable%20for%20these%20specially%20designed%20vision%20language%20models.%20The%20output%0Agenerated%20by%20such%20generic%20input%20prompts%20is%20ordinary%20and%20may%20contain%20information%0Agaps%20when%20compared%20with%20the%20actual%20content%20of%20the%20document.%20To%20obtain%20more%0Aaccurate%20and%20specific%20answers%2C%20a%20well-targeted%20prompt%20is%20required%20by%20the%20vision%0Alanguage%20model%2C%20along%20with%20the%20document%20image.%20In%20this%20paper%2C%20a%20technique%20is%0Adiscussed%20called%20Target%20prompting%2C%20which%20focuses%20on%20explicitly%20targeting%20parts%0Aof%20document%20images%20and%20generating%20related%20answers%20from%20those%20specific%20regions%0Aonly.%20The%20paper%20also%20covers%20the%20evaluation%20of%20response%20for%20each%20prompting%0Atechnique%20using%20different%20user%20queries%20and%20input%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03834v1&entry.124074799=Read"},
{"title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias\n  Detection in Language Models", "author": "Shachi H Kumar and Saurav Sahay and Sahisnu Mazumder and Eda Okur and Ramesh Manuvinakurike and Nicole Beckage and Hsuan Su and Hung-yi Lee and Lama Nachman", "abstract": "  Large Language Models (LLMs) have excelled at language understanding and\ngenerating human-level text. However, even with supervised training and human\nalignment, these LLMs are susceptible to adversarial attacks where malicious\nusers can prompt the model to generate undesirable text. LLMs also inherently\nencode potential biases that can cause various harmful effects during\ninteractions. Bias evaluation metrics lack standards as well as consensus and\nexisting methods often rely on human-generated templates and annotations which\nare expensive and labor intensive. In this work, we train models to\nautomatically create adversarial prompts to elicit biased responses from target\nLLMs. We present LLM- based bias evaluation metrics and also analyze several\nexisting automatic evaluation methods and metrics. We analyze the various\nnuances of model responses, identify the strengths and weaknesses of model\nfamilies, and assess where evaluation methods fall short. We compare these\nmetrics to human evaluation and validate that the LLM-as-a-Judge metric aligns\nwith human judgement on bias in response generation.\n", "link": "http://arxiv.org/abs/2408.03907v1", "date": "2024-08-07", "relevancy": 1.4385, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.473}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Biases%3A%20Automated%20Methods%20and%20LLM%20Judges%20for%20Gender%20Bias%0A%20%20Detection%20in%20Language%20Models&body=Title%3A%20Decoding%20Biases%3A%20Automated%20Methods%20and%20LLM%20Judges%20for%20Gender%20Bias%0A%20%20Detection%20in%20Language%20Models%0AAuthor%3A%20Shachi%20H%20Kumar%20and%20Saurav%20Sahay%20and%20Sahisnu%20Mazumder%20and%20Eda%20Okur%20and%20Ramesh%20Manuvinakurike%20and%20Nicole%20Beckage%20and%20Hsuan%20Su%20and%20Hung-yi%20Lee%20and%20Lama%20Nachman%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20excelled%20at%20language%20understanding%20and%0Agenerating%20human-level%20text.%20However%2C%20even%20with%20supervised%20training%20and%20human%0Aalignment%2C%20these%20LLMs%20are%20susceptible%20to%20adversarial%20attacks%20where%20malicious%0Ausers%20can%20prompt%20the%20model%20to%20generate%20undesirable%20text.%20LLMs%20also%20inherently%0Aencode%20potential%20biases%20that%20can%20cause%20various%20harmful%20effects%20during%0Ainteractions.%20Bias%20evaluation%20metrics%20lack%20standards%20as%20well%20as%20consensus%20and%0Aexisting%20methods%20often%20rely%20on%20human-generated%20templates%20and%20annotations%20which%0Aare%20expensive%20and%20labor%20intensive.%20In%20this%20work%2C%20we%20train%20models%20to%0Aautomatically%20create%20adversarial%20prompts%20to%20elicit%20biased%20responses%20from%20target%0ALLMs.%20We%20present%20LLM-%20based%20bias%20evaluation%20metrics%20and%20also%20analyze%20several%0Aexisting%20automatic%20evaluation%20methods%20and%20metrics.%20We%20analyze%20the%20various%0Anuances%20of%20model%20responses%2C%20identify%20the%20strengths%20and%20weaknesses%20of%20model%0Afamilies%2C%20and%20assess%20where%20evaluation%20methods%20fall%20short.%20We%20compare%20these%0Ametrics%20to%20human%20evaluation%20and%20validate%20that%20the%20LLM-as-a-Judge%20metric%20aligns%0Awith%20human%20judgement%20on%20bias%20in%20response%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Biases%253A%2520Automated%2520Methods%2520and%2520LLM%2520Judges%2520for%2520Gender%2520Bias%250A%2520%2520Detection%2520in%2520Language%2520Models%26entry.906535625%3DShachi%2520H%2520Kumar%2520and%2520Saurav%2520Sahay%2520and%2520Sahisnu%2520Mazumder%2520and%2520Eda%2520Okur%2520and%2520Ramesh%2520Manuvinakurike%2520and%2520Nicole%2520Beckage%2520and%2520Hsuan%2520Su%2520and%2520Hung-yi%2520Lee%2520and%2520Lama%2520Nachman%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520excelled%2520at%2520language%2520understanding%2520and%250Agenerating%2520human-level%2520text.%2520However%252C%2520even%2520with%2520supervised%2520training%2520and%2520human%250Aalignment%252C%2520these%2520LLMs%2520are%2520susceptible%2520to%2520adversarial%2520attacks%2520where%2520malicious%250Ausers%2520can%2520prompt%2520the%2520model%2520to%2520generate%2520undesirable%2520text.%2520LLMs%2520also%2520inherently%250Aencode%2520potential%2520biases%2520that%2520can%2520cause%2520various%2520harmful%2520effects%2520during%250Ainteractions.%2520Bias%2520evaluation%2520metrics%2520lack%2520standards%2520as%2520well%2520as%2520consensus%2520and%250Aexisting%2520methods%2520often%2520rely%2520on%2520human-generated%2520templates%2520and%2520annotations%2520which%250Aare%2520expensive%2520and%2520labor%2520intensive.%2520In%2520this%2520work%252C%2520we%2520train%2520models%2520to%250Aautomatically%2520create%2520adversarial%2520prompts%2520to%2520elicit%2520biased%2520responses%2520from%2520target%250ALLMs.%2520We%2520present%2520LLM-%2520based%2520bias%2520evaluation%2520metrics%2520and%2520also%2520analyze%2520several%250Aexisting%2520automatic%2520evaluation%2520methods%2520and%2520metrics.%2520We%2520analyze%2520the%2520various%250Anuances%2520of%2520model%2520responses%252C%2520identify%2520the%2520strengths%2520and%2520weaknesses%2520of%2520model%250Afamilies%252C%2520and%2520assess%2520where%2520evaluation%2520methods%2520fall%2520short.%2520We%2520compare%2520these%250Ametrics%2520to%2520human%2520evaluation%2520and%2520validate%2520that%2520the%2520LLM-as-a-Judge%2520metric%2520aligns%250Awith%2520human%2520judgement%2520on%2520bias%2520in%2520response%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Biases%3A%20Automated%20Methods%20and%20LLM%20Judges%20for%20Gender%20Bias%0A%20%20Detection%20in%20Language%20Models&entry.906535625=Shachi%20H%20Kumar%20and%20Saurav%20Sahay%20and%20Sahisnu%20Mazumder%20and%20Eda%20Okur%20and%20Ramesh%20Manuvinakurike%20and%20Nicole%20Beckage%20and%20Hsuan%20Su%20and%20Hung-yi%20Lee%20and%20Lama%20Nachman&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20excelled%20at%20language%20understanding%20and%0Agenerating%20human-level%20text.%20However%2C%20even%20with%20supervised%20training%20and%20human%0Aalignment%2C%20these%20LLMs%20are%20susceptible%20to%20adversarial%20attacks%20where%20malicious%0Ausers%20can%20prompt%20the%20model%20to%20generate%20undesirable%20text.%20LLMs%20also%20inherently%0Aencode%20potential%20biases%20that%20can%20cause%20various%20harmful%20effects%20during%0Ainteractions.%20Bias%20evaluation%20metrics%20lack%20standards%20as%20well%20as%20consensus%20and%0Aexisting%20methods%20often%20rely%20on%20human-generated%20templates%20and%20annotations%20which%0Aare%20expensive%20and%20labor%20intensive.%20In%20this%20work%2C%20we%20train%20models%20to%0Aautomatically%20create%20adversarial%20prompts%20to%20elicit%20biased%20responses%20from%20target%0ALLMs.%20We%20present%20LLM-%20based%20bias%20evaluation%20metrics%20and%20also%20analyze%20several%0Aexisting%20automatic%20evaluation%20methods%20and%20metrics.%20We%20analyze%20the%20various%0Anuances%20of%20model%20responses%2C%20identify%20the%20strengths%20and%20weaknesses%20of%20model%0Afamilies%2C%20and%20assess%20where%20evaluation%20methods%20fall%20short.%20We%20compare%20these%0Ametrics%20to%20human%20evaluation%20and%20validate%20that%20the%20LLM-as-a-Judge%20metric%20aligns%0Awith%20human%20judgement%20on%20bias%20in%20response%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03907v1&entry.124074799=Read"},
{"title": "Methodological Explainability Evaluation of an Interpretable Deep\n  Learning Model for Post-Hepatectomy Liver Failure Prediction Incorporating\n  Counterfactual Explanations and Layerwise Relevance Propagation: A\n  Prospective In Silico Trial", "author": "Xian Zhong and Zohaib Salahuddin and Yi Chen and Henry C Woodruff and Haiyi Long and Jianyun Peng and Nuwan Udawatte and Roberto Casale and Ayoub Mokhtari and Xiaoer Zhang and Jiayao Huang and Qingyu Wu and Li Tan and Lili Chen and Dongming Li and Xiaoyan Xie and Manxia Lin and Philippe Lambin", "abstract": "  Artificial intelligence (AI)-based decision support systems have demonstrated\nvalue in predicting post-hepatectomy liver failure (PHLF) in hepatocellular\ncarcinoma (HCC). However, they often lack transparency, and the impact of model\nexplanations on clinicians' decisions has not been thoroughly evaluated.\nBuilding on prior research, we developed a variational autoencoder-multilayer\nperceptron (VAE-MLP) model for preoperative PHLF prediction. This model\nintegrated counterfactuals and layerwise relevance propagation (LRP) to provide\ninsights into its decision-making mechanism. Additionally, we proposed a\nmethodological framework for evaluating the explainability of AI systems. This\nframework includes qualitative and quantitative assessments of explanations\nagainst recognized biomarkers, usability evaluations, and an in silico clinical\ntrial. Our evaluations demonstrated that the model's explanation correlated\nwith established biomarkers and exhibited high usability at both the case and\nsystem levels. Furthermore, results from the three-track in silico clinical\ntrial showed that clinicians' prediction accuracy and confidence increased when\nAI explanations were provided.\n", "link": "http://arxiv.org/abs/2408.03771v1", "date": "2024-08-07", "relevancy": 1.3895, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5072}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Methodological%20Explainability%20Evaluation%20of%20an%20Interpretable%20Deep%0A%20%20Learning%20Model%20for%20Post-Hepatectomy%20Liver%20Failure%20Prediction%20Incorporating%0A%20%20Counterfactual%20Explanations%20and%20Layerwise%20Relevance%20Propagation%3A%20A%0A%20%20Prospective%20In%20Silico%20Trial&body=Title%3A%20Methodological%20Explainability%20Evaluation%20of%20an%20Interpretable%20Deep%0A%20%20Learning%20Model%20for%20Post-Hepatectomy%20Liver%20Failure%20Prediction%20Incorporating%0A%20%20Counterfactual%20Explanations%20and%20Layerwise%20Relevance%20Propagation%3A%20A%0A%20%20Prospective%20In%20Silico%20Trial%0AAuthor%3A%20Xian%20Zhong%20and%20Zohaib%20Salahuddin%20and%20Yi%20Chen%20and%20Henry%20C%20Woodruff%20and%20Haiyi%20Long%20and%20Jianyun%20Peng%20and%20Nuwan%20Udawatte%20and%20Roberto%20Casale%20and%20Ayoub%20Mokhtari%20and%20Xiaoer%20Zhang%20and%20Jiayao%20Huang%20and%20Qingyu%20Wu%20and%20Li%20Tan%20and%20Lili%20Chen%20and%20Dongming%20Li%20and%20Xiaoyan%20Xie%20and%20Manxia%20Lin%20and%20Philippe%20Lambin%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29-based%20decision%20support%20systems%20have%20demonstrated%0Avalue%20in%20predicting%20post-hepatectomy%20liver%20failure%20%28PHLF%29%20in%20hepatocellular%0Acarcinoma%20%28HCC%29.%20However%2C%20they%20often%20lack%20transparency%2C%20and%20the%20impact%20of%20model%0Aexplanations%20on%20clinicians%27%20decisions%20has%20not%20been%20thoroughly%20evaluated.%0ABuilding%20on%20prior%20research%2C%20we%20developed%20a%20variational%20autoencoder-multilayer%0Aperceptron%20%28VAE-MLP%29%20model%20for%20preoperative%20PHLF%20prediction.%20This%20model%0Aintegrated%20counterfactuals%20and%20layerwise%20relevance%20propagation%20%28LRP%29%20to%20provide%0Ainsights%20into%20its%20decision-making%20mechanism.%20Additionally%2C%20we%20proposed%20a%0Amethodological%20framework%20for%20evaluating%20the%20explainability%20of%20AI%20systems.%20This%0Aframework%20includes%20qualitative%20and%20quantitative%20assessments%20of%20explanations%0Aagainst%20recognized%20biomarkers%2C%20usability%20evaluations%2C%20and%20an%20in%20silico%20clinical%0Atrial.%20Our%20evaluations%20demonstrated%20that%20the%20model%27s%20explanation%20correlated%0Awith%20established%20biomarkers%20and%20exhibited%20high%20usability%20at%20both%20the%20case%20and%0Asystem%20levels.%20Furthermore%2C%20results%20from%20the%20three-track%20in%20silico%20clinical%0Atrial%20showed%20that%20clinicians%27%20prediction%20accuracy%20and%20confidence%20increased%20when%0AAI%20explanations%20were%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMethodological%2520Explainability%2520Evaluation%2520of%2520an%2520Interpretable%2520Deep%250A%2520%2520Learning%2520Model%2520for%2520Post-Hepatectomy%2520Liver%2520Failure%2520Prediction%2520Incorporating%250A%2520%2520Counterfactual%2520Explanations%2520and%2520Layerwise%2520Relevance%2520Propagation%253A%2520A%250A%2520%2520Prospective%2520In%2520Silico%2520Trial%26entry.906535625%3DXian%2520Zhong%2520and%2520Zohaib%2520Salahuddin%2520and%2520Yi%2520Chen%2520and%2520Henry%2520C%2520Woodruff%2520and%2520Haiyi%2520Long%2520and%2520Jianyun%2520Peng%2520and%2520Nuwan%2520Udawatte%2520and%2520Roberto%2520Casale%2520and%2520Ayoub%2520Mokhtari%2520and%2520Xiaoer%2520Zhang%2520and%2520Jiayao%2520Huang%2520and%2520Qingyu%2520Wu%2520and%2520Li%2520Tan%2520and%2520Lili%2520Chen%2520and%2520Dongming%2520Li%2520and%2520Xiaoyan%2520Xie%2520and%2520Manxia%2520Lin%2520and%2520Philippe%2520Lambin%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529-based%2520decision%2520support%2520systems%2520have%2520demonstrated%250Avalue%2520in%2520predicting%2520post-hepatectomy%2520liver%2520failure%2520%2528PHLF%2529%2520in%2520hepatocellular%250Acarcinoma%2520%2528HCC%2529.%2520However%252C%2520they%2520often%2520lack%2520transparency%252C%2520and%2520the%2520impact%2520of%2520model%250Aexplanations%2520on%2520clinicians%2527%2520decisions%2520has%2520not%2520been%2520thoroughly%2520evaluated.%250ABuilding%2520on%2520prior%2520research%252C%2520we%2520developed%2520a%2520variational%2520autoencoder-multilayer%250Aperceptron%2520%2528VAE-MLP%2529%2520model%2520for%2520preoperative%2520PHLF%2520prediction.%2520This%2520model%250Aintegrated%2520counterfactuals%2520and%2520layerwise%2520relevance%2520propagation%2520%2528LRP%2529%2520to%2520provide%250Ainsights%2520into%2520its%2520decision-making%2520mechanism.%2520Additionally%252C%2520we%2520proposed%2520a%250Amethodological%2520framework%2520for%2520evaluating%2520the%2520explainability%2520of%2520AI%2520systems.%2520This%250Aframework%2520includes%2520qualitative%2520and%2520quantitative%2520assessments%2520of%2520explanations%250Aagainst%2520recognized%2520biomarkers%252C%2520usability%2520evaluations%252C%2520and%2520an%2520in%2520silico%2520clinical%250Atrial.%2520Our%2520evaluations%2520demonstrated%2520that%2520the%2520model%2527s%2520explanation%2520correlated%250Awith%2520established%2520biomarkers%2520and%2520exhibited%2520high%2520usability%2520at%2520both%2520the%2520case%2520and%250Asystem%2520levels.%2520Furthermore%252C%2520results%2520from%2520the%2520three-track%2520in%2520silico%2520clinical%250Atrial%2520showed%2520that%2520clinicians%2527%2520prediction%2520accuracy%2520and%2520confidence%2520increased%2520when%250AAI%2520explanations%2520were%2520provided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Methodological%20Explainability%20Evaluation%20of%20an%20Interpretable%20Deep%0A%20%20Learning%20Model%20for%20Post-Hepatectomy%20Liver%20Failure%20Prediction%20Incorporating%0A%20%20Counterfactual%20Explanations%20and%20Layerwise%20Relevance%20Propagation%3A%20A%0A%20%20Prospective%20In%20Silico%20Trial&entry.906535625=Xian%20Zhong%20and%20Zohaib%20Salahuddin%20and%20Yi%20Chen%20and%20Henry%20C%20Woodruff%20and%20Haiyi%20Long%20and%20Jianyun%20Peng%20and%20Nuwan%20Udawatte%20and%20Roberto%20Casale%20and%20Ayoub%20Mokhtari%20and%20Xiaoer%20Zhang%20and%20Jiayao%20Huang%20and%20Qingyu%20Wu%20and%20Li%20Tan%20and%20Lili%20Chen%20and%20Dongming%20Li%20and%20Xiaoyan%20Xie%20and%20Manxia%20Lin%20and%20Philippe%20Lambin&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29-based%20decision%20support%20systems%20have%20demonstrated%0Avalue%20in%20predicting%20post-hepatectomy%20liver%20failure%20%28PHLF%29%20in%20hepatocellular%0Acarcinoma%20%28HCC%29.%20However%2C%20they%20often%20lack%20transparency%2C%20and%20the%20impact%20of%20model%0Aexplanations%20on%20clinicians%27%20decisions%20has%20not%20been%20thoroughly%20evaluated.%0ABuilding%20on%20prior%20research%2C%20we%20developed%20a%20variational%20autoencoder-multilayer%0Aperceptron%20%28VAE-MLP%29%20model%20for%20preoperative%20PHLF%20prediction.%20This%20model%0Aintegrated%20counterfactuals%20and%20layerwise%20relevance%20propagation%20%28LRP%29%20to%20provide%0Ainsights%20into%20its%20decision-making%20mechanism.%20Additionally%2C%20we%20proposed%20a%0Amethodological%20framework%20for%20evaluating%20the%20explainability%20of%20AI%20systems.%20This%0Aframework%20includes%20qualitative%20and%20quantitative%20assessments%20of%20explanations%0Aagainst%20recognized%20biomarkers%2C%20usability%20evaluations%2C%20and%20an%20in%20silico%20clinical%0Atrial.%20Our%20evaluations%20demonstrated%20that%20the%20model%27s%20explanation%20correlated%0Awith%20established%20biomarkers%20and%20exhibited%20high%20usability%20at%20both%20the%20case%20and%0Asystem%20levels.%20Furthermore%2C%20results%20from%20the%20three-track%20in%20silico%20clinical%0Atrial%20showed%20that%20clinicians%27%20prediction%20accuracy%20and%20confidence%20increased%20when%0AAI%20explanations%20were%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03771v1&entry.124074799=Read"},
{"title": "RL-ADN: A High-Performance Deep Reinforcement Learning Environment for\n  Optimal Energy Storage Systems Dispatch in Active Distribution Networks", "author": "Shengren Hou and Shuyi Gao and Weijie Xia and Edgar Mauricio Salazar Duque and Peter Palensky and Pedro P. Vergara", "abstract": "  Deep Reinforcement Learning (DRL) presents a promising avenue for optimizing\nEnergy Storage Systems (ESSs) dispatch in distribution networks. This paper\nintroduces RL-ADN, an innovative open-source library specifically designed for\nsolving the optimal ESSs dispatch in active distribution networks. RL-ADN\noffers unparalleled flexibility in modeling distribution networks, and ESSs,\naccommodating a wide range of research goals. A standout feature of RL-ADN is\nits data augmentation module, based on Gaussian Mixture Model and Copula (GMC)\nfunctions, which elevates the performance ceiling of DRL agents. Additionally,\nRL-ADN incorporates the Laurent power flow solver, significantly reducing the\ncomputational burden of power flow calculations during training without\nsacrificing accuracy. The effectiveness of RL-ADN is demonstrated using in\ndifferent sizes of distribution networks, showing marked performance\nimprovements in the adaptability of DRL algorithms for ESS dispatch tasks. This\nenhancement is particularly beneficial from the increased diversity of training\nscenarios. Furthermore, RL-ADN achieves a tenfold increase in computational\nefficiency during training, making it highly suitable for large-scale network\napplications. The library sets a new benchmark in DRL-based ESSs dispatch in\ndistribution networks and it is poised to advance DRL applications in\ndistribution network operations significantly. RL-ADN is available at:\nhttps://github.com/ShengrenHou/RL-ADN.\n", "link": "http://arxiv.org/abs/2408.03685v1", "date": "2024-08-07", "relevancy": 1.504, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5296}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4666}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-ADN%3A%20A%20High-Performance%20Deep%20Reinforcement%20Learning%20Environment%20for%0A%20%20Optimal%20Energy%20Storage%20Systems%20Dispatch%20in%20Active%20Distribution%20Networks&body=Title%3A%20RL-ADN%3A%20A%20High-Performance%20Deep%20Reinforcement%20Learning%20Environment%20for%0A%20%20Optimal%20Energy%20Storage%20Systems%20Dispatch%20in%20Active%20Distribution%20Networks%0AAuthor%3A%20Shengren%20Hou%20and%20Shuyi%20Gao%20and%20Weijie%20Xia%20and%20Edgar%20Mauricio%20Salazar%20Duque%20and%20Peter%20Palensky%20and%20Pedro%20P.%20Vergara%0AAbstract%3A%20%20%20Deep%20Reinforcement%20Learning%20%28DRL%29%20presents%20a%20promising%20avenue%20for%20optimizing%0AEnergy%20Storage%20Systems%20%28ESSs%29%20dispatch%20in%20distribution%20networks.%20This%20paper%0Aintroduces%20RL-ADN%2C%20an%20innovative%20open-source%20library%20specifically%20designed%20for%0Asolving%20the%20optimal%20ESSs%20dispatch%20in%20active%20distribution%20networks.%20RL-ADN%0Aoffers%20unparalleled%20flexibility%20in%20modeling%20distribution%20networks%2C%20and%20ESSs%2C%0Aaccommodating%20a%20wide%20range%20of%20research%20goals.%20A%20standout%20feature%20of%20RL-ADN%20is%0Aits%20data%20augmentation%20module%2C%20based%20on%20Gaussian%20Mixture%20Model%20and%20Copula%20%28GMC%29%0Afunctions%2C%20which%20elevates%20the%20performance%20ceiling%20of%20DRL%20agents.%20Additionally%2C%0ARL-ADN%20incorporates%20the%20Laurent%20power%20flow%20solver%2C%20significantly%20reducing%20the%0Acomputational%20burden%20of%20power%20flow%20calculations%20during%20training%20without%0Asacrificing%20accuracy.%20The%20effectiveness%20of%20RL-ADN%20is%20demonstrated%20using%20in%0Adifferent%20sizes%20of%20distribution%20networks%2C%20showing%20marked%20performance%0Aimprovements%20in%20the%20adaptability%20of%20DRL%20algorithms%20for%20ESS%20dispatch%20tasks.%20This%0Aenhancement%20is%20particularly%20beneficial%20from%20the%20increased%20diversity%20of%20training%0Ascenarios.%20Furthermore%2C%20RL-ADN%20achieves%20a%20tenfold%20increase%20in%20computational%0Aefficiency%20during%20training%2C%20making%20it%20highly%20suitable%20for%20large-scale%20network%0Aapplications.%20The%20library%20sets%20a%20new%20benchmark%20in%20DRL-based%20ESSs%20dispatch%20in%0Adistribution%20networks%20and%20it%20is%20poised%20to%20advance%20DRL%20applications%20in%0Adistribution%20network%20operations%20significantly.%20RL-ADN%20is%20available%20at%3A%0Ahttps%3A//github.com/ShengrenHou/RL-ADN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-ADN%253A%2520A%2520High-Performance%2520Deep%2520Reinforcement%2520Learning%2520Environment%2520for%250A%2520%2520Optimal%2520Energy%2520Storage%2520Systems%2520Dispatch%2520in%2520Active%2520Distribution%2520Networks%26entry.906535625%3DShengren%2520Hou%2520and%2520Shuyi%2520Gao%2520and%2520Weijie%2520Xia%2520and%2520Edgar%2520Mauricio%2520Salazar%2520Duque%2520and%2520Peter%2520Palensky%2520and%2520Pedro%2520P.%2520Vergara%26entry.1292438233%3D%2520%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520presents%2520a%2520promising%2520avenue%2520for%2520optimizing%250AEnergy%2520Storage%2520Systems%2520%2528ESSs%2529%2520dispatch%2520in%2520distribution%2520networks.%2520This%2520paper%250Aintroduces%2520RL-ADN%252C%2520an%2520innovative%2520open-source%2520library%2520specifically%2520designed%2520for%250Asolving%2520the%2520optimal%2520ESSs%2520dispatch%2520in%2520active%2520distribution%2520networks.%2520RL-ADN%250Aoffers%2520unparalleled%2520flexibility%2520in%2520modeling%2520distribution%2520networks%252C%2520and%2520ESSs%252C%250Aaccommodating%2520a%2520wide%2520range%2520of%2520research%2520goals.%2520A%2520standout%2520feature%2520of%2520RL-ADN%2520is%250Aits%2520data%2520augmentation%2520module%252C%2520based%2520on%2520Gaussian%2520Mixture%2520Model%2520and%2520Copula%2520%2528GMC%2529%250Afunctions%252C%2520which%2520elevates%2520the%2520performance%2520ceiling%2520of%2520DRL%2520agents.%2520Additionally%252C%250ARL-ADN%2520incorporates%2520the%2520Laurent%2520power%2520flow%2520solver%252C%2520significantly%2520reducing%2520the%250Acomputational%2520burden%2520of%2520power%2520flow%2520calculations%2520during%2520training%2520without%250Asacrificing%2520accuracy.%2520The%2520effectiveness%2520of%2520RL-ADN%2520is%2520demonstrated%2520using%2520in%250Adifferent%2520sizes%2520of%2520distribution%2520networks%252C%2520showing%2520marked%2520performance%250Aimprovements%2520in%2520the%2520adaptability%2520of%2520DRL%2520algorithms%2520for%2520ESS%2520dispatch%2520tasks.%2520This%250Aenhancement%2520is%2520particularly%2520beneficial%2520from%2520the%2520increased%2520diversity%2520of%2520training%250Ascenarios.%2520Furthermore%252C%2520RL-ADN%2520achieves%2520a%2520tenfold%2520increase%2520in%2520computational%250Aefficiency%2520during%2520training%252C%2520making%2520it%2520highly%2520suitable%2520for%2520large-scale%2520network%250Aapplications.%2520The%2520library%2520sets%2520a%2520new%2520benchmark%2520in%2520DRL-based%2520ESSs%2520dispatch%2520in%250Adistribution%2520networks%2520and%2520it%2520is%2520poised%2520to%2520advance%2520DRL%2520applications%2520in%250Adistribution%2520network%2520operations%2520significantly.%2520RL-ADN%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ShengrenHou/RL-ADN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-ADN%3A%20A%20High-Performance%20Deep%20Reinforcement%20Learning%20Environment%20for%0A%20%20Optimal%20Energy%20Storage%20Systems%20Dispatch%20in%20Active%20Distribution%20Networks&entry.906535625=Shengren%20Hou%20and%20Shuyi%20Gao%20and%20Weijie%20Xia%20and%20Edgar%20Mauricio%20Salazar%20Duque%20and%20Peter%20Palensky%20and%20Pedro%20P.%20Vergara&entry.1292438233=%20%20Deep%20Reinforcement%20Learning%20%28DRL%29%20presents%20a%20promising%20avenue%20for%20optimizing%0AEnergy%20Storage%20Systems%20%28ESSs%29%20dispatch%20in%20distribution%20networks.%20This%20paper%0Aintroduces%20RL-ADN%2C%20an%20innovative%20open-source%20library%20specifically%20designed%20for%0Asolving%20the%20optimal%20ESSs%20dispatch%20in%20active%20distribution%20networks.%20RL-ADN%0Aoffers%20unparalleled%20flexibility%20in%20modeling%20distribution%20networks%2C%20and%20ESSs%2C%0Aaccommodating%20a%20wide%20range%20of%20research%20goals.%20A%20standout%20feature%20of%20RL-ADN%20is%0Aits%20data%20augmentation%20module%2C%20based%20on%20Gaussian%20Mixture%20Model%20and%20Copula%20%28GMC%29%0Afunctions%2C%20which%20elevates%20the%20performance%20ceiling%20of%20DRL%20agents.%20Additionally%2C%0ARL-ADN%20incorporates%20the%20Laurent%20power%20flow%20solver%2C%20significantly%20reducing%20the%0Acomputational%20burden%20of%20power%20flow%20calculations%20during%20training%20without%0Asacrificing%20accuracy.%20The%20effectiveness%20of%20RL-ADN%20is%20demonstrated%20using%20in%0Adifferent%20sizes%20of%20distribution%20networks%2C%20showing%20marked%20performance%0Aimprovements%20in%20the%20adaptability%20of%20DRL%20algorithms%20for%20ESS%20dispatch%20tasks.%20This%0Aenhancement%20is%20particularly%20beneficial%20from%20the%20increased%20diversity%20of%20training%0Ascenarios.%20Furthermore%2C%20RL-ADN%20achieves%20a%20tenfold%20increase%20in%20computational%0Aefficiency%20during%20training%2C%20making%20it%20highly%20suitable%20for%20large-scale%20network%0Aapplications.%20The%20library%20sets%20a%20new%20benchmark%20in%20DRL-based%20ESSs%20dispatch%20in%0Adistribution%20networks%20and%20it%20is%20poised%20to%20advance%20DRL%20applications%20in%0Adistribution%20network%20operations%20significantly.%20RL-ADN%20is%20available%20at%3A%0Ahttps%3A//github.com/ShengrenHou/RL-ADN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03685v1&entry.124074799=Read"},
{"title": "WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language\n  Models", "author": "Prannaya Gupta and Le Qi Yau and Hao Han Low and I-Shiang Lee and Hugo Maximus Lim and Yu Xin Teoh and Jia Hng Koh and Dar Win Liew and Rishabh Bhardwaj and Rajat Bhardwaj and Soujanya Poria", "abstract": "  WalledEval is a comprehensive AI safety testing toolkit designed to evaluate\nlarge language models (LLMs). It accommodates a diverse range of models,\nincluding both open-weight and API-based ones, and features over 35 safety\nbenchmarks covering areas such as multilingual safety, exaggerated safety, and\nprompt injections. The framework supports both LLM and judge benchmarking, and\nincorporates custom mutators to test safety against various text-style\nmutations such as future tense and paraphrasing. Additionally, WalledEval\nintroduces WalledGuard, a new, small and performant content moderation tool,\nand SGXSTest, a benchmark for assessing exaggerated safety in cultural\ncontexts. We make WalledEval publicly available at\nhttps://github.com/walledai/walledevalA.\n", "link": "http://arxiv.org/abs/2408.03837v1", "date": "2024-08-07", "relevancy": 1.3123, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4401}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WalledEval%3A%20A%20Comprehensive%20Safety%20Evaluation%20Toolkit%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20WalledEval%3A%20A%20Comprehensive%20Safety%20Evaluation%20Toolkit%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Prannaya%20Gupta%20and%20Le%20Qi%20Yau%20and%20Hao%20Han%20Low%20and%20I-Shiang%20Lee%20and%20Hugo%20Maximus%20Lim%20and%20Yu%20Xin%20Teoh%20and%20Jia%20Hng%20Koh%20and%20Dar%20Win%20Liew%20and%20Rishabh%20Bhardwaj%20and%20Rajat%20Bhardwaj%20and%20Soujanya%20Poria%0AAbstract%3A%20%20%20WalledEval%20is%20a%20comprehensive%20AI%20safety%20testing%20toolkit%20designed%20to%20evaluate%0Alarge%20language%20models%20%28LLMs%29.%20It%20accommodates%20a%20diverse%20range%20of%20models%2C%0Aincluding%20both%20open-weight%20and%20API-based%20ones%2C%20and%20features%20over%2035%20safety%0Abenchmarks%20covering%20areas%20such%20as%20multilingual%20safety%2C%20exaggerated%20safety%2C%20and%0Aprompt%20injections.%20The%20framework%20supports%20both%20LLM%20and%20judge%20benchmarking%2C%20and%0Aincorporates%20custom%20mutators%20to%20test%20safety%20against%20various%20text-style%0Amutations%20such%20as%20future%20tense%20and%20paraphrasing.%20Additionally%2C%20WalledEval%0Aintroduces%20WalledGuard%2C%20a%20new%2C%20small%20and%20performant%20content%20moderation%20tool%2C%0Aand%20SGXSTest%2C%20a%20benchmark%20for%20assessing%20exaggerated%20safety%20in%20cultural%0Acontexts.%20We%20make%20WalledEval%20publicly%20available%20at%0Ahttps%3A//github.com/walledai/walledevalA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWalledEval%253A%2520A%2520Comprehensive%2520Safety%2520Evaluation%2520Toolkit%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DPrannaya%2520Gupta%2520and%2520Le%2520Qi%2520Yau%2520and%2520Hao%2520Han%2520Low%2520and%2520I-Shiang%2520Lee%2520and%2520Hugo%2520Maximus%2520Lim%2520and%2520Yu%2520Xin%2520Teoh%2520and%2520Jia%2520Hng%2520Koh%2520and%2520Dar%2520Win%2520Liew%2520and%2520Rishabh%2520Bhardwaj%2520and%2520Rajat%2520Bhardwaj%2520and%2520Soujanya%2520Poria%26entry.1292438233%3D%2520%2520WalledEval%2520is%2520a%2520comprehensive%2520AI%2520safety%2520testing%2520toolkit%2520designed%2520to%2520evaluate%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520It%2520accommodates%2520a%2520diverse%2520range%2520of%2520models%252C%250Aincluding%2520both%2520open-weight%2520and%2520API-based%2520ones%252C%2520and%2520features%2520over%252035%2520safety%250Abenchmarks%2520covering%2520areas%2520such%2520as%2520multilingual%2520safety%252C%2520exaggerated%2520safety%252C%2520and%250Aprompt%2520injections.%2520The%2520framework%2520supports%2520both%2520LLM%2520and%2520judge%2520benchmarking%252C%2520and%250Aincorporates%2520custom%2520mutators%2520to%2520test%2520safety%2520against%2520various%2520text-style%250Amutations%2520such%2520as%2520future%2520tense%2520and%2520paraphrasing.%2520Additionally%252C%2520WalledEval%250Aintroduces%2520WalledGuard%252C%2520a%2520new%252C%2520small%2520and%2520performant%2520content%2520moderation%2520tool%252C%250Aand%2520SGXSTest%252C%2520a%2520benchmark%2520for%2520assessing%2520exaggerated%2520safety%2520in%2520cultural%250Acontexts.%2520We%2520make%2520WalledEval%2520publicly%2520available%2520at%250Ahttps%253A//github.com/walledai/walledevalA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WalledEval%3A%20A%20Comprehensive%20Safety%20Evaluation%20Toolkit%20for%20Large%20Language%0A%20%20Models&entry.906535625=Prannaya%20Gupta%20and%20Le%20Qi%20Yau%20and%20Hao%20Han%20Low%20and%20I-Shiang%20Lee%20and%20Hugo%20Maximus%20Lim%20and%20Yu%20Xin%20Teoh%20and%20Jia%20Hng%20Koh%20and%20Dar%20Win%20Liew%20and%20Rishabh%20Bhardwaj%20and%20Rajat%20Bhardwaj%20and%20Soujanya%20Poria&entry.1292438233=%20%20WalledEval%20is%20a%20comprehensive%20AI%20safety%20testing%20toolkit%20designed%20to%20evaluate%0Alarge%20language%20models%20%28LLMs%29.%20It%20accommodates%20a%20diverse%20range%20of%20models%2C%0Aincluding%20both%20open-weight%20and%20API-based%20ones%2C%20and%20features%20over%2035%20safety%0Abenchmarks%20covering%20areas%20such%20as%20multilingual%20safety%2C%20exaggerated%20safety%2C%20and%0Aprompt%20injections.%20The%20framework%20supports%20both%20LLM%20and%20judge%20benchmarking%2C%20and%0Aincorporates%20custom%20mutators%20to%20test%20safety%20against%20various%20text-style%0Amutations%20such%20as%20future%20tense%20and%20paraphrasing.%20Additionally%2C%20WalledEval%0Aintroduces%20WalledGuard%2C%20a%20new%2C%20small%20and%20performant%20content%20moderation%20tool%2C%0Aand%20SGXSTest%2C%20a%20benchmark%20for%20assessing%20exaggerated%20safety%20in%20cultural%0Acontexts.%20We%20make%20WalledEval%20publicly%20available%20at%0Ahttps%3A//github.com/walledai/walledevalA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03837v1&entry.124074799=Read"},
{"title": "Frank's triangular norms in Piaget's logical proportions", "author": "Henri Prade and Gilles Richard", "abstract": "  Starting from the Boolean notion of logical proportion in Piaget's sense,\nwhich turns out to be equivalent to analogical proportion, this note proposes a\ndefinition of analogical proportion between numerical values based on\ntriangular norms (and dual co-norms). Frank's family of triangular norms is\nparticularly interesting from this perspective. The article concludes with a\ncomparative discussion with another very recent proposal for defining\nanalogical proportions between numerical values based on the family of\ngeneralized means.\n", "link": "http://arxiv.org/abs/2408.03795v1", "date": "2024-08-07", "relevancy": 1.1142, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3661}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frank%27s%20triangular%20norms%20in%20Piaget%27s%20logical%20proportions&body=Title%3A%20Frank%27s%20triangular%20norms%20in%20Piaget%27s%20logical%20proportions%0AAuthor%3A%20Henri%20Prade%20and%20Gilles%20Richard%0AAbstract%3A%20%20%20Starting%20from%20the%20Boolean%20notion%20of%20logical%20proportion%20in%20Piaget%27s%20sense%2C%0Awhich%20turns%20out%20to%20be%20equivalent%20to%20analogical%20proportion%2C%20this%20note%20proposes%20a%0Adefinition%20of%20analogical%20proportion%20between%20numerical%20values%20based%20on%0Atriangular%20norms%20%28and%20dual%20co-norms%29.%20Frank%27s%20family%20of%20triangular%20norms%20is%0Aparticularly%20interesting%20from%20this%20perspective.%20The%20article%20concludes%20with%20a%0Acomparative%20discussion%20with%20another%20very%20recent%20proposal%20for%20defining%0Aanalogical%20proportions%20between%20numerical%20values%20based%20on%20the%20family%20of%0Ageneralized%20means.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrank%2527s%2520triangular%2520norms%2520in%2520Piaget%2527s%2520logical%2520proportions%26entry.906535625%3DHenri%2520Prade%2520and%2520Gilles%2520Richard%26entry.1292438233%3D%2520%2520Starting%2520from%2520the%2520Boolean%2520notion%2520of%2520logical%2520proportion%2520in%2520Piaget%2527s%2520sense%252C%250Awhich%2520turns%2520out%2520to%2520be%2520equivalent%2520to%2520analogical%2520proportion%252C%2520this%2520note%2520proposes%2520a%250Adefinition%2520of%2520analogical%2520proportion%2520between%2520numerical%2520values%2520based%2520on%250Atriangular%2520norms%2520%2528and%2520dual%2520co-norms%2529.%2520Frank%2527s%2520family%2520of%2520triangular%2520norms%2520is%250Aparticularly%2520interesting%2520from%2520this%2520perspective.%2520The%2520article%2520concludes%2520with%2520a%250Acomparative%2520discussion%2520with%2520another%2520very%2520recent%2520proposal%2520for%2520defining%250Aanalogical%2520proportions%2520between%2520numerical%2520values%2520based%2520on%2520the%2520family%2520of%250Ageneralized%2520means.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frank%27s%20triangular%20norms%20in%20Piaget%27s%20logical%20proportions&entry.906535625=Henri%20Prade%20and%20Gilles%20Richard&entry.1292438233=%20%20Starting%20from%20the%20Boolean%20notion%20of%20logical%20proportion%20in%20Piaget%27s%20sense%2C%0Awhich%20turns%20out%20to%20be%20equivalent%20to%20analogical%20proportion%2C%20this%20note%20proposes%20a%0Adefinition%20of%20analogical%20proportion%20between%20numerical%20values%20based%20on%0Atriangular%20norms%20%28and%20dual%20co-norms%29.%20Frank%27s%20family%20of%20triangular%20norms%20is%0Aparticularly%20interesting%20from%20this%20perspective.%20The%20article%20concludes%20with%20a%0Acomparative%20discussion%20with%20another%20very%20recent%20proposal%20for%20defining%0Aanalogical%20proportions%20between%20numerical%20values%20based%20on%20the%20family%20of%0Ageneralized%20means.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03795v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


