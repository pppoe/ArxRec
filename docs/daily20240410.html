<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PAT: Pixel-wise Adaptive Training for Long-tailed Segmentation", "author": "Khoi Do and Duong Nguyen and Nguyen H. Tran and Viet Dung Nguyen", "abstract": "  Beyond class frequency, we recognize the impact of class-wise relationships\namong various class-specific predictions and the imbalance in label masks on\nlong-tailed segmentation learning. To address these challenges, we propose an\ninnovative Pixel-wise Adaptive Training (PAT) technique tailored for\nlong-tailed segmentation. PAT has two key features: 1) class-wise gradient\nmagnitude homogenization, and 2) pixel-wise class-specific loss adaptation\n(PCLA). First, the class-wise gradient magnitude homogenization helps alleviate\nthe imbalance among label masks by ensuring equal consideration of the\nclass-wise impact on model updates. Second, PCLA tackles the detrimental impact\nof both rare classes within the long-tailed distribution and inaccurate\npredictions from previous training stages by encouraging learning classes with\nlow prediction confidence and guarding against forgetting classes with high\nconfidence. This combined approach fosters robust learning while preventing the\nmodel from forgetting previously learned knowledge. PAT exhibits significant\nperformance improvements, surpassing the current state-of-the-art by 2.2% in\nthe NyU dataset. Moreover, it enhances overall pixel-wise accuracy by 2.85% and\nintersection over union value by 2.07%, with a particularly notable declination\nof 0.39% in detecting rare classes compared to Balance Logits Variation, as\ndemonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and\nNYU.\n", "link": "http://arxiv.org/abs/2404.05393v2", "date": "2024-04-09", "relevancy": 2.8985, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6206}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PAT%3A%20Pixel-wise%20Adaptive%20Training%20for%20Long-tailed%20Segmentation&body=Title%3A%20PAT%3A%20Pixel-wise%20Adaptive%20Training%20for%20Long-tailed%20Segmentation%0AAuthor%3A%20Khoi%20Do%20and%20Duong%20Nguyen%20and%20Nguyen%20H.%20Tran%20and%20Viet%20Dung%20Nguyen%0AAbstract%3A%20%20%20Beyond%20class%20frequency%2C%20we%20recognize%20the%20impact%20of%20class-wise%20relationships%0Aamong%20various%20class-specific%20predictions%20and%20the%20imbalance%20in%20label%20masks%20on%0Along-tailed%20segmentation%20learning.%20To%20address%20these%20challenges%2C%20we%20propose%20an%0Ainnovative%20Pixel-wise%20Adaptive%20Training%20%28PAT%29%20technique%20tailored%20for%0Along-tailed%20segmentation.%20PAT%20has%20two%20key%20features%3A%201%29%20class-wise%20gradient%0Amagnitude%20homogenization%2C%20and%202%29%20pixel-wise%20class-specific%20loss%20adaptation%0A%28PCLA%29.%20First%2C%20the%20class-wise%20gradient%20magnitude%20homogenization%20helps%20alleviate%0Athe%20imbalance%20among%20label%20masks%20by%20ensuring%20equal%20consideration%20of%20the%0Aclass-wise%20impact%20on%20model%20updates.%20Second%2C%20PCLA%20tackles%20the%20detrimental%20impact%0Aof%20both%20rare%20classes%20within%20the%20long-tailed%20distribution%20and%20inaccurate%0Apredictions%20from%20previous%20training%20stages%20by%20encouraging%20learning%20classes%20with%0Alow%20prediction%20confidence%20and%20guarding%20against%20forgetting%20classes%20with%20high%0Aconfidence.%20This%20combined%20approach%20fosters%20robust%20learning%20while%20preventing%20the%0Amodel%20from%20forgetting%20previously%20learned%20knowledge.%20PAT%20exhibits%20significant%0Aperformance%20improvements%2C%20surpassing%20the%20current%20state-of-the-art%20by%202.2%25%20in%0Athe%20NyU%20dataset.%20Moreover%2C%20it%20enhances%20overall%20pixel-wise%20accuracy%20by%202.85%25%20and%0Aintersection%20over%20union%20value%20by%202.07%25%2C%20with%20a%20particularly%20notable%20declination%0Aof%200.39%25%20in%20detecting%20rare%20classes%20compared%20to%20Balance%20Logits%20Variation%2C%20as%0Ademonstrated%20on%20the%20three%20popular%20datasets%2C%20i.e.%2C%20OxfordPetIII%2C%20CityScape%2C%20and%0ANYU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05393v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAT%3A%20Pixel-wise%20Adaptive%20Training%20for%20Long-tailed%20Segmentation&entry.906535625=Khoi%20Do%20and%20Duong%20Nguyen%20and%20Nguyen%20H.%20Tran%20and%20Viet%20Dung%20Nguyen&entry.1292438233=%20%20Beyond%20class%20frequency%2C%20we%20recognize%20the%20impact%20of%20class-wise%20relationships%0Aamong%20various%20class-specific%20predictions%20and%20the%20imbalance%20in%20label%20masks%20on%0Along-tailed%20segmentation%20learning.%20To%20address%20these%20challenges%2C%20we%20propose%20an%0Ainnovative%20Pixel-wise%20Adaptive%20Training%20%28PAT%29%20technique%20tailored%20for%0Along-tailed%20segmentation.%20PAT%20has%20two%20key%20features%3A%201%29%20class-wise%20gradient%0Amagnitude%20homogenization%2C%20and%202%29%20pixel-wise%20class-specific%20loss%20adaptation%0A%28PCLA%29.%20First%2C%20the%20class-wise%20gradient%20magnitude%20homogenization%20helps%20alleviate%0Athe%20imbalance%20among%20label%20masks%20by%20ensuring%20equal%20consideration%20of%20the%0Aclass-wise%20impact%20on%20model%20updates.%20Second%2C%20PCLA%20tackles%20the%20detrimental%20impact%0Aof%20both%20rare%20classes%20within%20the%20long-tailed%20distribution%20and%20inaccurate%0Apredictions%20from%20previous%20training%20stages%20by%20encouraging%20learning%20classes%20with%0Alow%20prediction%20confidence%20and%20guarding%20against%20forgetting%20classes%20with%20high%0Aconfidence.%20This%20combined%20approach%20fosters%20robust%20learning%20while%20preventing%20the%0Amodel%20from%20forgetting%20previously%20learned%20knowledge.%20PAT%20exhibits%20significant%0Aperformance%20improvements%2C%20surpassing%20the%20current%20state-of-the-art%20by%202.2%25%20in%0Athe%20NyU%20dataset.%20Moreover%2C%20it%20enhances%20overall%20pixel-wise%20accuracy%20by%202.85%25%20and%0Aintersection%20over%20union%20value%20by%202.07%25%2C%20with%20a%20particularly%20notable%20declination%0Aof%200.39%25%20in%20detecting%20rare%20classes%20compared%20to%20Balance%20Logits%20Variation%2C%20as%0Ademonstrated%20on%20the%20three%20popular%20datasets%2C%20i.e.%2C%20OxfordPetIII%2C%20CityScape%2C%20and%0ANYU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05393v2&entry.124074799=Read"},
{"title": "ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised\n  Action Recognition in Videos", "author": "Sharana Dharshikgan Suresh Dass and Hrishav Bakul Barua and Ganesh Krishnasamy and Raveendran Paramesran and Raphael C. -W. Phan", "abstract": "  Human action or activity recognition in videos is a fundamental task in\ncomputer vision with applications in surveillance and monitoring, self-driving\ncars, sports analytics, human-robot interaction and many more. Traditional\nsupervised methods require large annotated datasets for training, which are\nexpensive and time-consuming to acquire. This work proposes a novel approach\nusing Cross-Architecture Pseudo-Labeling with contrastive learning for\nsemi-supervised action recognition. Our framework leverages both labeled and\nunlabelled data to robustly learn action representations in videos, combining\npseudo-labeling with contrastive learning for effective learning from both\ntypes of samples. We introduce a novel cross-architecture approach where 3D\nConvolutional Neural Networks (3D CNNs) and video transformers (VIT) are\nutilised to capture different aspects of action representations; hence we call\nit ActNetFormer. The 3D CNNs excel at capturing spatial features and local\ndependencies in the temporal domain, while VIT excels at capturing long-range\ndependencies across frames. By integrating these complementary architectures\nwithin the ActNetFormer framework, our approach can effectively capture both\nlocal and global contextual information of an action. This comprehensive\nrepresentation learning enables the model to achieve better performance in\nsemi-supervised action recognition tasks by leveraging the strengths of each of\nthese architectures. Experimental results on standard action recognition\ndatasets demonstrate that our approach performs better than the existing\nmethods, achieving state-of-the-art performance with only a fraction of labeled\ndata. The official website of this work is available at:\nhttps://github.com/rana2149/ActNetFormer.\n", "link": "http://arxiv.org/abs/2404.06243v1", "date": "2024-04-09", "relevancy": 2.8831, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6023}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5769}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5507}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ActNetFormer%3A%20Transformer-ResNet%20Hybrid%20Method%20for%20Semi-Supervised%0A%20%20Action%20Recognition%20in%20Videos&body=Title%3A%20ActNetFormer%3A%20Transformer-ResNet%20Hybrid%20Method%20for%20Semi-Supervised%0A%20%20Action%20Recognition%20in%20Videos%0AAuthor%3A%20Sharana%20Dharshikgan%20Suresh%20Dass%20and%20Hrishav%20Bakul%20Barua%20and%20Ganesh%20Krishnasamy%20and%20Raveendran%20Paramesran%20and%20Raphael%20C.%20-W.%20Phan%0AAbstract%3A%20%20%20Human%20action%20or%20activity%20recognition%20in%20videos%20is%20a%20fundamental%20task%20in%0Acomputer%20vision%20with%20applications%20in%20surveillance%20and%20monitoring%2C%20self-driving%0Acars%2C%20sports%20analytics%2C%20human-robot%20interaction%20and%20many%20more.%20Traditional%0Asupervised%20methods%20require%20large%20annotated%20datasets%20for%20training%2C%20which%20are%0Aexpensive%20and%20time-consuming%20to%20acquire.%20This%20work%20proposes%20a%20novel%20approach%0Ausing%20Cross-Architecture%20Pseudo-Labeling%20with%20contrastive%20learning%20for%0Asemi-supervised%20action%20recognition.%20Our%20framework%20leverages%20both%20labeled%20and%0Aunlabelled%20data%20to%20robustly%20learn%20action%20representations%20in%20videos%2C%20combining%0Apseudo-labeling%20with%20contrastive%20learning%20for%20effective%20learning%20from%20both%0Atypes%20of%20samples.%20We%20introduce%20a%20novel%20cross-architecture%20approach%20where%203D%0AConvolutional%20Neural%20Networks%20%283D%20CNNs%29%20and%20video%20transformers%20%28VIT%29%20are%0Autilised%20to%20capture%20different%20aspects%20of%20action%20representations%3B%20hence%20we%20call%0Ait%20ActNetFormer.%20The%203D%20CNNs%20excel%20at%20capturing%20spatial%20features%20and%20local%0Adependencies%20in%20the%20temporal%20domain%2C%20while%20VIT%20excels%20at%20capturing%20long-range%0Adependencies%20across%20frames.%20By%20integrating%20these%20complementary%20architectures%0Awithin%20the%20ActNetFormer%20framework%2C%20our%20approach%20can%20effectively%20capture%20both%0Alocal%20and%20global%20contextual%20information%20of%20an%20action.%20This%20comprehensive%0Arepresentation%20learning%20enables%20the%20model%20to%20achieve%20better%20performance%20in%0Asemi-supervised%20action%20recognition%20tasks%20by%20leveraging%20the%20strengths%20of%20each%20of%0Athese%20architectures.%20Experimental%20results%20on%20standard%20action%20recognition%0Adatasets%20demonstrate%20that%20our%20approach%20performs%20better%20than%20the%20existing%0Amethods%2C%20achieving%20state-of-the-art%20performance%20with%20only%20a%20fraction%20of%20labeled%0Adata.%20The%20official%20website%20of%20this%20work%20is%20available%20at%3A%0Ahttps%3A//github.com/rana2149/ActNetFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06243v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActNetFormer%3A%20Transformer-ResNet%20Hybrid%20Method%20for%20Semi-Supervised%0A%20%20Action%20Recognition%20in%20Videos&entry.906535625=Sharana%20Dharshikgan%20Suresh%20Dass%20and%20Hrishav%20Bakul%20Barua%20and%20Ganesh%20Krishnasamy%20and%20Raveendran%20Paramesran%20and%20Raphael%20C.%20-W.%20Phan&entry.1292438233=%20%20Human%20action%20or%20activity%20recognition%20in%20videos%20is%20a%20fundamental%20task%20in%0Acomputer%20vision%20with%20applications%20in%20surveillance%20and%20monitoring%2C%20self-driving%0Acars%2C%20sports%20analytics%2C%20human-robot%20interaction%20and%20many%20more.%20Traditional%0Asupervised%20methods%20require%20large%20annotated%20datasets%20for%20training%2C%20which%20are%0Aexpensive%20and%20time-consuming%20to%20acquire.%20This%20work%20proposes%20a%20novel%20approach%0Ausing%20Cross-Architecture%20Pseudo-Labeling%20with%20contrastive%20learning%20for%0Asemi-supervised%20action%20recognition.%20Our%20framework%20leverages%20both%20labeled%20and%0Aunlabelled%20data%20to%20robustly%20learn%20action%20representations%20in%20videos%2C%20combining%0Apseudo-labeling%20with%20contrastive%20learning%20for%20effective%20learning%20from%20both%0Atypes%20of%20samples.%20We%20introduce%20a%20novel%20cross-architecture%20approach%20where%203D%0AConvolutional%20Neural%20Networks%20%283D%20CNNs%29%20and%20video%20transformers%20%28VIT%29%20are%0Autilised%20to%20capture%20different%20aspects%20of%20action%20representations%3B%20hence%20we%20call%0Ait%20ActNetFormer.%20The%203D%20CNNs%20excel%20at%20capturing%20spatial%20features%20and%20local%0Adependencies%20in%20the%20temporal%20domain%2C%20while%20VIT%20excels%20at%20capturing%20long-range%0Adependencies%20across%20frames.%20By%20integrating%20these%20complementary%20architectures%0Awithin%20the%20ActNetFormer%20framework%2C%20our%20approach%20can%20effectively%20capture%20both%0Alocal%20and%20global%20contextual%20information%20of%20an%20action.%20This%20comprehensive%0Arepresentation%20learning%20enables%20the%20model%20to%20achieve%20better%20performance%20in%0Asemi-supervised%20action%20recognition%20tasks%20by%20leveraging%20the%20strengths%20of%20each%20of%0Athese%20architectures.%20Experimental%20results%20on%20standard%20action%20recognition%0Adatasets%20demonstrate%20that%20our%20approach%20performs%20better%20than%20the%20existing%0Amethods%2C%20achieving%20state-of-the-art%20performance%20with%20only%20a%20fraction%20of%20labeled%0Adata.%20The%20official%20website%20of%20this%20work%20is%20available%20at%3A%0Ahttps%3A//github.com/rana2149/ActNetFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06243v1&entry.124074799=Read"},
{"title": "Playing to Vision Foundation Model's Strengths in Stereo Matching", "author": "Chuang-Wei Liu and Qijun Chen and Rui Fan", "abstract": "  Stereo matching has become a key technique for 3D environment perception in\nintelligent vehicles. For a considerable time, convolutional neural networks\n(CNNs) have remained the mainstream choice for feature extraction in this\ndomain. Nonetheless, there is a growing consensus that the existing paradigm\nshould evolve towards vision foundation models (VFM), particularly those\ndeveloped based on vision Transformers (ViTs) and pre-trained through\nself-supervision on extensive, unlabeled datasets. While VFMs are adept at\nextracting informative, general-purpose visual features, specifically for dense\nprediction tasks, their performance often lacks in geometric vision tasks. This\nstudy serves as the first exploration of a viable approach for adapting VFMs to\nstereo matching. Our ViT adapter, referred to as ViTAS, is constructed upon\nthree types of modules: spatial differentiation, patch attention fusion, and\ncross-attention. The first module initializes feature pyramids, while the\nlatter two aggregate stereo and multi-scale contextual information into\nfine-grained features, respectively. ViTAStereo, which combines ViTAS with cost\nvolume-based stereo matching back-end processes, achieves the top rank on the\nKITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by\napproximately 7.9% in terms of the percentage of error pixels, with a tolerance\nof 3 pixels. Additional experiments across diverse scenarios further\ndemonstrate its superior generalizability compared to all other\nstate-of-the-art approaches. We believe this new paradigm will pave the way for\nthe next generation of stereo matching networks.\n", "link": "http://arxiv.org/abs/2404.06261v1", "date": "2024-04-09", "relevancy": 2.8166, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5581}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5532}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Playing%20to%20Vision%20Foundation%20Model%27s%20Strengths%20in%20Stereo%20Matching&body=Title%3A%20Playing%20to%20Vision%20Foundation%20Model%27s%20Strengths%20in%20Stereo%20Matching%0AAuthor%3A%20Chuang-Wei%20Liu%20and%20Qijun%20Chen%20and%20Rui%20Fan%0AAbstract%3A%20%20%20Stereo%20matching%20has%20become%20a%20key%20technique%20for%203D%20environment%20perception%20in%0Aintelligent%20vehicles.%20For%20a%20considerable%20time%2C%20convolutional%20neural%20networks%0A%28CNNs%29%20have%20remained%20the%20mainstream%20choice%20for%20feature%20extraction%20in%20this%0Adomain.%20Nonetheless%2C%20there%20is%20a%20growing%20consensus%20that%20the%20existing%20paradigm%0Ashould%20evolve%20towards%20vision%20foundation%20models%20%28VFM%29%2C%20particularly%20those%0Adeveloped%20based%20on%20vision%20Transformers%20%28ViTs%29%20and%20pre-trained%20through%0Aself-supervision%20on%20extensive%2C%20unlabeled%20datasets.%20While%20VFMs%20are%20adept%20at%0Aextracting%20informative%2C%20general-purpose%20visual%20features%2C%20specifically%20for%20dense%0Aprediction%20tasks%2C%20their%20performance%20often%20lacks%20in%20geometric%20vision%20tasks.%20This%0Astudy%20serves%20as%20the%20first%20exploration%20of%20a%20viable%20approach%20for%20adapting%20VFMs%20to%0Astereo%20matching.%20Our%20ViT%20adapter%2C%20referred%20to%20as%20ViTAS%2C%20is%20constructed%20upon%0Athree%20types%20of%20modules%3A%20spatial%20differentiation%2C%20patch%20attention%20fusion%2C%20and%0Across-attention.%20The%20first%20module%20initializes%20feature%20pyramids%2C%20while%20the%0Alatter%20two%20aggregate%20stereo%20and%20multi-scale%20contextual%20information%20into%0Afine-grained%20features%2C%20respectively.%20ViTAStereo%2C%20which%20combines%20ViTAS%20with%20cost%0Avolume-based%20stereo%20matching%20back-end%20processes%2C%20achieves%20the%20top%20rank%20on%20the%0AKITTI%20Stereo%202012%20dataset%20and%20outperforms%20the%20second-best%20network%20StereoBase%20by%0Aapproximately%207.9%25%20in%20terms%20of%20the%20percentage%20of%20error%20pixels%2C%20with%20a%20tolerance%0Aof%203%20pixels.%20Additional%20experiments%20across%20diverse%20scenarios%20further%0Ademonstrate%20its%20superior%20generalizability%20compared%20to%20all%20other%0Astate-of-the-art%20approaches.%20We%20believe%20this%20new%20paradigm%20will%20pave%20the%20way%20for%0Athe%20next%20generation%20of%20stereo%20matching%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06261v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Playing%20to%20Vision%20Foundation%20Model%27s%20Strengths%20in%20Stereo%20Matching&entry.906535625=Chuang-Wei%20Liu%20and%20Qijun%20Chen%20and%20Rui%20Fan&entry.1292438233=%20%20Stereo%20matching%20has%20become%20a%20key%20technique%20for%203D%20environment%20perception%20in%0Aintelligent%20vehicles.%20For%20a%20considerable%20time%2C%20convolutional%20neural%20networks%0A%28CNNs%29%20have%20remained%20the%20mainstream%20choice%20for%20feature%20extraction%20in%20this%0Adomain.%20Nonetheless%2C%20there%20is%20a%20growing%20consensus%20that%20the%20existing%20paradigm%0Ashould%20evolve%20towards%20vision%20foundation%20models%20%28VFM%29%2C%20particularly%20those%0Adeveloped%20based%20on%20vision%20Transformers%20%28ViTs%29%20and%20pre-trained%20through%0Aself-supervision%20on%20extensive%2C%20unlabeled%20datasets.%20While%20VFMs%20are%20adept%20at%0Aextracting%20informative%2C%20general-purpose%20visual%20features%2C%20specifically%20for%20dense%0Aprediction%20tasks%2C%20their%20performance%20often%20lacks%20in%20geometric%20vision%20tasks.%20This%0Astudy%20serves%20as%20the%20first%20exploration%20of%20a%20viable%20approach%20for%20adapting%20VFMs%20to%0Astereo%20matching.%20Our%20ViT%20adapter%2C%20referred%20to%20as%20ViTAS%2C%20is%20constructed%20upon%0Athree%20types%20of%20modules%3A%20spatial%20differentiation%2C%20patch%20attention%20fusion%2C%20and%0Across-attention.%20The%20first%20module%20initializes%20feature%20pyramids%2C%20while%20the%0Alatter%20two%20aggregate%20stereo%20and%20multi-scale%20contextual%20information%20into%0Afine-grained%20features%2C%20respectively.%20ViTAStereo%2C%20which%20combines%20ViTAS%20with%20cost%0Avolume-based%20stereo%20matching%20back-end%20processes%2C%20achieves%20the%20top%20rank%20on%20the%0AKITTI%20Stereo%202012%20dataset%20and%20outperforms%20the%20second-best%20network%20StereoBase%20by%0Aapproximately%207.9%25%20in%20terms%20of%20the%20percentage%20of%20error%20pixels%2C%20with%20a%20tolerance%0Aof%203%20pixels.%20Additional%20experiments%20across%20diverse%20scenarios%20further%0Ademonstrate%20its%20superior%20generalizability%20compared%20to%20all%20other%0Astate-of-the-art%20approaches.%20We%20believe%20this%20new%20paradigm%20will%20pave%20the%20way%20for%0Athe%20next%20generation%20of%20stereo%20matching%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06261v1&entry.124074799=Read"},
{"title": "Learning Embeddings with Centroid Triplet Loss for Object Identification\n  in Robotic Grasping", "author": "Anas Gouda and Max Schwarz and Christopher Reining and Sven Behnke and Alice Kirchheim", "abstract": "  Foundation models are a strong trend in deep learning and computer vision.\nThese models serve as a base for applications as they require minor or no\nfurther fine-tuning by developers to integrate into their applications.\nFoundation models for zero-shot object segmentation such as Segment Anything\n(SAM) output segmentation masks from images without any further object\ninformation. When they are followed in a pipeline by an object identification\nmodel, they can perform object detection without training. Here, we focus on\ntraining such an object identification model. A crucial practical aspect for an\nobject identification model is to be flexible in input size. As object\nidentification is an image retrieval problem, a suitable method should handle\nmulti-query multi-gallery situations without constraining the number of input\nimages (e.g. by having fixed-size aggregation layers). The key solution to\ntrain such a model is the centroid triplet loss (CTL), which aggregates image\nfeatures to their centroids. CTL yields high accuracy, avoids misleading\ntraining signals and keeps the model input size flexible. In our experiments,\nwe establish a new state of the art on the ArmBench object identification task,\nwhich shows general applicability of our model. We furthermore demonstrate an\nintegrated unseen object detection pipeline on the challenging HOPE dataset,\nwhich requires fine-grained detection. There, our pipeline matches and\nsurpasses related methods which have been trained on dataset-specific data.\n", "link": "http://arxiv.org/abs/2404.06277v1", "date": "2024-04-09", "relevancy": 2.8031, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.573}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5635}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5453}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Embeddings%20with%20Centroid%20Triplet%20Loss%20for%20Object%20Identification%0A%20%20in%20Robotic%20Grasping&body=Title%3A%20Learning%20Embeddings%20with%20Centroid%20Triplet%20Loss%20for%20Object%20Identification%0A%20%20in%20Robotic%20Grasping%0AAuthor%3A%20Anas%20Gouda%20and%20Max%20Schwarz%20and%20Christopher%20Reining%20and%20Sven%20Behnke%20and%20Alice%20Kirchheim%0AAbstract%3A%20%20%20Foundation%20models%20are%20a%20strong%20trend%20in%20deep%20learning%20and%20computer%20vision.%0AThese%20models%20serve%20as%20a%20base%20for%20applications%20as%20they%20require%20minor%20or%20no%0Afurther%20fine-tuning%20by%20developers%20to%20integrate%20into%20their%20applications.%0AFoundation%20models%20for%20zero-shot%20object%20segmentation%20such%20as%20Segment%20Anything%0A%28SAM%29%20output%20segmentation%20masks%20from%20images%20without%20any%20further%20object%0Ainformation.%20When%20they%20are%20followed%20in%20a%20pipeline%20by%20an%20object%20identification%0Amodel%2C%20they%20can%20perform%20object%20detection%20without%20training.%20Here%2C%20we%20focus%20on%0Atraining%20such%20an%20object%20identification%20model.%20A%20crucial%20practical%20aspect%20for%20an%0Aobject%20identification%20model%20is%20to%20be%20flexible%20in%20input%20size.%20As%20object%0Aidentification%20is%20an%20image%20retrieval%20problem%2C%20a%20suitable%20method%20should%20handle%0Amulti-query%20multi-gallery%20situations%20without%20constraining%20the%20number%20of%20input%0Aimages%20%28e.g.%20by%20having%20fixed-size%20aggregation%20layers%29.%20The%20key%20solution%20to%0Atrain%20such%20a%20model%20is%20the%20centroid%20triplet%20loss%20%28CTL%29%2C%20which%20aggregates%20image%0Afeatures%20to%20their%20centroids.%20CTL%20yields%20high%20accuracy%2C%20avoids%20misleading%0Atraining%20signals%20and%20keeps%20the%20model%20input%20size%20flexible.%20In%20our%20experiments%2C%0Awe%20establish%20a%20new%20state%20of%20the%20art%20on%20the%20ArmBench%20object%20identification%20task%2C%0Awhich%20shows%20general%20applicability%20of%20our%20model.%20We%20furthermore%20demonstrate%20an%0Aintegrated%20unseen%20object%20detection%20pipeline%20on%20the%20challenging%20HOPE%20dataset%2C%0Awhich%20requires%20fine-grained%20detection.%20There%2C%20our%20pipeline%20matches%20and%0Asurpasses%20related%20methods%20which%20have%20been%20trained%20on%20dataset-specific%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06277v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Embeddings%20with%20Centroid%20Triplet%20Loss%20for%20Object%20Identification%0A%20%20in%20Robotic%20Grasping&entry.906535625=Anas%20Gouda%20and%20Max%20Schwarz%20and%20Christopher%20Reining%20and%20Sven%20Behnke%20and%20Alice%20Kirchheim&entry.1292438233=%20%20Foundation%20models%20are%20a%20strong%20trend%20in%20deep%20learning%20and%20computer%20vision.%0AThese%20models%20serve%20as%20a%20base%20for%20applications%20as%20they%20require%20minor%20or%20no%0Afurther%20fine-tuning%20by%20developers%20to%20integrate%20into%20their%20applications.%0AFoundation%20models%20for%20zero-shot%20object%20segmentation%20such%20as%20Segment%20Anything%0A%28SAM%29%20output%20segmentation%20masks%20from%20images%20without%20any%20further%20object%0Ainformation.%20When%20they%20are%20followed%20in%20a%20pipeline%20by%20an%20object%20identification%0Amodel%2C%20they%20can%20perform%20object%20detection%20without%20training.%20Here%2C%20we%20focus%20on%0Atraining%20such%20an%20object%20identification%20model.%20A%20crucial%20practical%20aspect%20for%20an%0Aobject%20identification%20model%20is%20to%20be%20flexible%20in%20input%20size.%20As%20object%0Aidentification%20is%20an%20image%20retrieval%20problem%2C%20a%20suitable%20method%20should%20handle%0Amulti-query%20multi-gallery%20situations%20without%20constraining%20the%20number%20of%20input%0Aimages%20%28e.g.%20by%20having%20fixed-size%20aggregation%20layers%29.%20The%20key%20solution%20to%0Atrain%20such%20a%20model%20is%20the%20centroid%20triplet%20loss%20%28CTL%29%2C%20which%20aggregates%20image%0Afeatures%20to%20their%20centroids.%20CTL%20yields%20high%20accuracy%2C%20avoids%20misleading%0Atraining%20signals%20and%20keeps%20the%20model%20input%20size%20flexible.%20In%20our%20experiments%2C%0Awe%20establish%20a%20new%20state%20of%20the%20art%20on%20the%20ArmBench%20object%20identification%20task%2C%0Awhich%20shows%20general%20applicability%20of%20our%20model.%20We%20furthermore%20demonstrate%20an%0Aintegrated%20unseen%20object%20detection%20pipeline%20on%20the%20challenging%20HOPE%20dataset%2C%0Awhich%20requires%20fine-grained%20detection.%20There%2C%20our%20pipeline%20matches%20and%0Asurpasses%20related%20methods%20which%20have%20been%20trained%20on%20dataset-specific%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06277v1&entry.124074799=Read"},
{"title": "LRR: Language-Driven Resamplable Continuous Representation against\n  Adversarial Tracking Attacks", "author": "Jianlang Chen and Xuhong Ren and Qing Guo and Felix Juefei-Xu and Di Lin and Wei Feng and Lei Ma and Jianjun Zhao", "abstract": "  Visual object tracking plays a critical role in visual-based autonomous\nsystems, as it aims to estimate the position and size of the object of interest\nwithin a live video. Despite significant progress made in this field,\nstate-of-the-art (SOTA) trackers often fail when faced with adversarial\nperturbations in the incoming frames. This can lead to significant robustness\nand security issues when these trackers are deployed in the real world. To\nachieve high accuracy on both clean and adversarial data, we propose building a\nspatial-temporal continuous representation using the semantic text guidance of\nthe object of interest. This novel continuous representation enables us to\nreconstruct incoming frames to maintain semantic and appearance consistency\nwith the object of interest and its clean counterparts. As a result, our\nproposed method successfully defends against different SOTA adversarial\ntracking attacks while maintaining high accuracy on clean data. In particular,\nour method significantly increases tracking accuracy under adversarial attacks\nwith around 90% relative improvement on UAV123, which is even higher than the\naccuracy on clean data.\n", "link": "http://arxiv.org/abs/2404.06247v1", "date": "2024-04-09", "relevancy": 2.7588, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.546}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LRR%3A%20Language-Driven%20Resamplable%20Continuous%20Representation%20against%0A%20%20Adversarial%20Tracking%20Attacks&body=Title%3A%20LRR%3A%20Language-Driven%20Resamplable%20Continuous%20Representation%20against%0A%20%20Adversarial%20Tracking%20Attacks%0AAuthor%3A%20Jianlang%20Chen%20and%20Xuhong%20Ren%20and%20Qing%20Guo%20and%20Felix%20Juefei-Xu%20and%20Di%20Lin%20and%20Wei%20Feng%20and%20Lei%20Ma%20and%20Jianjun%20Zhao%0AAbstract%3A%20%20%20Visual%20object%20tracking%20plays%20a%20critical%20role%20in%20visual-based%20autonomous%0Asystems%2C%20as%20it%20aims%20to%20estimate%20the%20position%20and%20size%20of%20the%20object%20of%20interest%0Awithin%20a%20live%20video.%20Despite%20significant%20progress%20made%20in%20this%20field%2C%0Astate-of-the-art%20%28SOTA%29%20trackers%20often%20fail%20when%20faced%20with%20adversarial%0Aperturbations%20in%20the%20incoming%20frames.%20This%20can%20lead%20to%20significant%20robustness%0Aand%20security%20issues%20when%20these%20trackers%20are%20deployed%20in%20the%20real%20world.%20To%0Aachieve%20high%20accuracy%20on%20both%20clean%20and%20adversarial%20data%2C%20we%20propose%20building%20a%0Aspatial-temporal%20continuous%20representation%20using%20the%20semantic%20text%20guidance%20of%0Athe%20object%20of%20interest.%20This%20novel%20continuous%20representation%20enables%20us%20to%0Areconstruct%20incoming%20frames%20to%20maintain%20semantic%20and%20appearance%20consistency%0Awith%20the%20object%20of%20interest%20and%20its%20clean%20counterparts.%20As%20a%20result%2C%20our%0Aproposed%20method%20successfully%20defends%20against%20different%20SOTA%20adversarial%0Atracking%20attacks%20while%20maintaining%20high%20accuracy%20on%20clean%20data.%20In%20particular%2C%0Aour%20method%20significantly%20increases%20tracking%20accuracy%20under%20adversarial%20attacks%0Awith%20around%2090%25%20relative%20improvement%20on%20UAV123%2C%20which%20is%20even%20higher%20than%20the%0Aaccuracy%20on%20clean%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06247v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LRR%3A%20Language-Driven%20Resamplable%20Continuous%20Representation%20against%0A%20%20Adversarial%20Tracking%20Attacks&entry.906535625=Jianlang%20Chen%20and%20Xuhong%20Ren%20and%20Qing%20Guo%20and%20Felix%20Juefei-Xu%20and%20Di%20Lin%20and%20Wei%20Feng%20and%20Lei%20Ma%20and%20Jianjun%20Zhao&entry.1292438233=%20%20Visual%20object%20tracking%20plays%20a%20critical%20role%20in%20visual-based%20autonomous%0Asystems%2C%20as%20it%20aims%20to%20estimate%20the%20position%20and%20size%20of%20the%20object%20of%20interest%0Awithin%20a%20live%20video.%20Despite%20significant%20progress%20made%20in%20this%20field%2C%0Astate-of-the-art%20%28SOTA%29%20trackers%20often%20fail%20when%20faced%20with%20adversarial%0Aperturbations%20in%20the%20incoming%20frames.%20This%20can%20lead%20to%20significant%20robustness%0Aand%20security%20issues%20when%20these%20trackers%20are%20deployed%20in%20the%20real%20world.%20To%0Aachieve%20high%20accuracy%20on%20both%20clean%20and%20adversarial%20data%2C%20we%20propose%20building%20a%0Aspatial-temporal%20continuous%20representation%20using%20the%20semantic%20text%20guidance%20of%0Athe%20object%20of%20interest.%20This%20novel%20continuous%20representation%20enables%20us%20to%0Areconstruct%20incoming%20frames%20to%20maintain%20semantic%20and%20appearance%20consistency%0Awith%20the%20object%20of%20interest%20and%20its%20clean%20counterparts.%20As%20a%20result%2C%20our%0Aproposed%20method%20successfully%20defends%20against%20different%20SOTA%20adversarial%0Atracking%20attacks%20while%20maintaining%20high%20accuracy%20on%20clean%20data.%20In%20particular%2C%0Aour%20method%20significantly%20increases%20tracking%20accuracy%20under%20adversarial%20attacks%0Awith%20around%2090%25%20relative%20improvement%20on%20UAV123%2C%20which%20is%20even%20higher%20than%20the%0Aaccuracy%20on%20clean%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06247v1&entry.124074799=Read"},
{"title": "Zero-shot Referring Expression Comprehension via Structural Similarity\n  Between Images and Captions", "author": "Zeyu Han and Fangrui Zhu and Qianru Lao and Huaizu Jiang", "abstract": "  Zero-shot referring expression comprehension aims at localizing bounding\nboxes in an image corresponding to provided textual prompts, which requires:\n(i) a fine-grained disentanglement of complex visual scene and textual context,\nand (ii) a capacity to understand relationships among disentangled entities.\nUnfortunately, existing large vision-language alignment (VLA) models, e.g.,\nCLIP, struggle with both aspects so cannot be directly used for this task. To\nmitigate this gap, we leverage large foundation models to disentangle both\nimages and texts into triplets in the format of (subject, predicate, object).\nAfter that, grounding is accomplished by calculating the structural similarity\nmatrix between visual and textual triplets with a VLA model, and subsequently\npropagate it to an instance-level similarity matrix. Furthermore, to equip VLA\nmodels with the ability of relationship understanding, we design a\ntriplet-matching objective to fine-tune the VLA models on a collection of\ncurated dataset containing abundant entity relationships. Experiments\ndemonstrate that our visual grounding performance increase of up to 19.5% over\nthe SOTA zero-shot model on RefCOCO/+/g. On the more challenging Who's Waldo\ndataset, our zero-shot approach achieves comparable accuracy to the fully\nsupervised model. Code is available at\nhttps://github.com/Show-han/Zeroshot_REC.\n", "link": "http://arxiv.org/abs/2311.17048v3", "date": "2024-04-09", "relevancy": 2.7334, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5857}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Referring%20Expression%20Comprehension%20via%20Structural%20Similarity%0A%20%20Between%20Images%20and%20Captions&body=Title%3A%20Zero-shot%20Referring%20Expression%20Comprehension%20via%20Structural%20Similarity%0A%20%20Between%20Images%20and%20Captions%0AAuthor%3A%20Zeyu%20Han%20and%20Fangrui%20Zhu%20and%20Qianru%20Lao%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Zero-shot%20referring%20expression%20comprehension%20aims%20at%20localizing%20bounding%0Aboxes%20in%20an%20image%20corresponding%20to%20provided%20textual%20prompts%2C%20which%20requires%3A%0A%28i%29%20a%20fine-grained%20disentanglement%20of%20complex%20visual%20scene%20and%20textual%20context%2C%0Aand%20%28ii%29%20a%20capacity%20to%20understand%20relationships%20among%20disentangled%20entities.%0AUnfortunately%2C%20existing%20large%20vision-language%20alignment%20%28VLA%29%20models%2C%20e.g.%2C%0ACLIP%2C%20struggle%20with%20both%20aspects%20so%20cannot%20be%20directly%20used%20for%20this%20task.%20To%0Amitigate%20this%20gap%2C%20we%20leverage%20large%20foundation%20models%20to%20disentangle%20both%0Aimages%20and%20texts%20into%20triplets%20in%20the%20format%20of%20%28subject%2C%20predicate%2C%20object%29.%0AAfter%20that%2C%20grounding%20is%20accomplished%20by%20calculating%20the%20structural%20similarity%0Amatrix%20between%20visual%20and%20textual%20triplets%20with%20a%20VLA%20model%2C%20and%20subsequently%0Apropagate%20it%20to%20an%20instance-level%20similarity%20matrix.%20Furthermore%2C%20to%20equip%20VLA%0Amodels%20with%20the%20ability%20of%20relationship%20understanding%2C%20we%20design%20a%0Atriplet-matching%20objective%20to%20fine-tune%20the%20VLA%20models%20on%20a%20collection%20of%0Acurated%20dataset%20containing%20abundant%20entity%20relationships.%20Experiments%0Ademonstrate%20that%20our%20visual%20grounding%20performance%20increase%20of%20up%20to%2019.5%25%20over%0Athe%20SOTA%20zero-shot%20model%20on%20RefCOCO/%2B/g.%20On%20the%20more%20challenging%20Who%27s%20Waldo%0Adataset%2C%20our%20zero-shot%20approach%20achieves%20comparable%20accuracy%20to%20the%20fully%0Asupervised%20model.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Show-han/Zeroshot_REC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17048v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Referring%20Expression%20Comprehension%20via%20Structural%20Similarity%0A%20%20Between%20Images%20and%20Captions&entry.906535625=Zeyu%20Han%20and%20Fangrui%20Zhu%20and%20Qianru%20Lao%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Zero-shot%20referring%20expression%20comprehension%20aims%20at%20localizing%20bounding%0Aboxes%20in%20an%20image%20corresponding%20to%20provided%20textual%20prompts%2C%20which%20requires%3A%0A%28i%29%20a%20fine-grained%20disentanglement%20of%20complex%20visual%20scene%20and%20textual%20context%2C%0Aand%20%28ii%29%20a%20capacity%20to%20understand%20relationships%20among%20disentangled%20entities.%0AUnfortunately%2C%20existing%20large%20vision-language%20alignment%20%28VLA%29%20models%2C%20e.g.%2C%0ACLIP%2C%20struggle%20with%20both%20aspects%20so%20cannot%20be%20directly%20used%20for%20this%20task.%20To%0Amitigate%20this%20gap%2C%20we%20leverage%20large%20foundation%20models%20to%20disentangle%20both%0Aimages%20and%20texts%20into%20triplets%20in%20the%20format%20of%20%28subject%2C%20predicate%2C%20object%29.%0AAfter%20that%2C%20grounding%20is%20accomplished%20by%20calculating%20the%20structural%20similarity%0Amatrix%20between%20visual%20and%20textual%20triplets%20with%20a%20VLA%20model%2C%20and%20subsequently%0Apropagate%20it%20to%20an%20instance-level%20similarity%20matrix.%20Furthermore%2C%20to%20equip%20VLA%0Amodels%20with%20the%20ability%20of%20relationship%20understanding%2C%20we%20design%20a%0Atriplet-matching%20objective%20to%20fine-tune%20the%20VLA%20models%20on%20a%20collection%20of%0Acurated%20dataset%20containing%20abundant%20entity%20relationships.%20Experiments%0Ademonstrate%20that%20our%20visual%20grounding%20performance%20increase%20of%20up%20to%2019.5%25%20over%0Athe%20SOTA%20zero-shot%20model%20on%20RefCOCO/%2B/g.%20On%20the%20more%20challenging%20Who%27s%20Waldo%0Adataset%2C%20our%20zero-shot%20approach%20achieves%20comparable%20accuracy%20to%20the%20fully%0Asupervised%20model.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Show-han/Zeroshot_REC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17048v3&entry.124074799=Read"},
{"title": "Automated National Urban Map Extraction", "author": "Hasan Nasrallah and Abed Ellatif Samhat and Cristiano Nattero and Ali J. Ghandour", "abstract": "  Developing countries usually lack the proper governance means to generate and\nregularly update a national rooftop map. Using traditional photogrammetry and\nsurveying methods to produce a building map at the federal level is costly and\ntime consuming. Using earth observation and deep learning methods, we can\nbridge this gap and propose an automated pipeline to fetch such national urban\nmaps. This paper aims to exploit the power of fully convolutional neural\nnetworks for multi-class buildings' instance segmentation to leverage high\nobject-wise accuracy results. Buildings' instance segmentation from sub-meter\nhigh-resolution satellite images can be achieved with relatively high\npixel-wise metric scores. We detail all engineering steps to replicate this\nwork and ensure highly accurate results in dense and slum areas witnessed in\nregions that lack proper urban planning in the Global South. We applied a case\nstudy of the proposed pipeline to Lebanon and successfully produced the first\ncomprehensive national building footprint map with approximately 1 Million\nunits with an 84% accuracy. The proposed architecture relies on advanced\naugmentation techniques to overcome dataset scarcity, which is often the case\nin developing countries.\n", "link": "http://arxiv.org/abs/2404.06202v1", "date": "2024-04-09", "relevancy": 2.7304, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6047}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5243}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5093}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automated%20National%20Urban%20Map%20Extraction&body=Title%3A%20Automated%20National%20Urban%20Map%20Extraction%0AAuthor%3A%20Hasan%20Nasrallah%20and%20Abed%20Ellatif%20Samhat%20and%20Cristiano%20Nattero%20and%20Ali%20J.%20Ghandour%0AAbstract%3A%20%20%20Developing%20countries%20usually%20lack%20the%20proper%20governance%20means%20to%20generate%20and%0Aregularly%20update%20a%20national%20rooftop%20map.%20Using%20traditional%20photogrammetry%20and%0Asurveying%20methods%20to%20produce%20a%20building%20map%20at%20the%20federal%20level%20is%20costly%20and%0Atime%20consuming.%20Using%20earth%20observation%20and%20deep%20learning%20methods%2C%20we%20can%0Abridge%20this%20gap%20and%20propose%20an%20automated%20pipeline%20to%20fetch%20such%20national%20urban%0Amaps.%20This%20paper%20aims%20to%20exploit%20the%20power%20of%20fully%20convolutional%20neural%0Anetworks%20for%20multi-class%20buildings%27%20instance%20segmentation%20to%20leverage%20high%0Aobject-wise%20accuracy%20results.%20Buildings%27%20instance%20segmentation%20from%20sub-meter%0Ahigh-resolution%20satellite%20images%20can%20be%20achieved%20with%20relatively%20high%0Apixel-wise%20metric%20scores.%20We%20detail%20all%20engineering%20steps%20to%20replicate%20this%0Awork%20and%20ensure%20highly%20accurate%20results%20in%20dense%20and%20slum%20areas%20witnessed%20in%0Aregions%20that%20lack%20proper%20urban%20planning%20in%20the%20Global%20South.%20We%20applied%20a%20case%0Astudy%20of%20the%20proposed%20pipeline%20to%20Lebanon%20and%20successfully%20produced%20the%20first%0Acomprehensive%20national%20building%20footprint%20map%20with%20approximately%201%20Million%0Aunits%20with%20an%2084%25%20accuracy.%20The%20proposed%20architecture%20relies%20on%20advanced%0Aaugmentation%20techniques%20to%20overcome%20dataset%20scarcity%2C%20which%20is%20often%20the%20case%0Ain%20developing%20countries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06202v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20National%20Urban%20Map%20Extraction&entry.906535625=Hasan%20Nasrallah%20and%20Abed%20Ellatif%20Samhat%20and%20Cristiano%20Nattero%20and%20Ali%20J.%20Ghandour&entry.1292438233=%20%20Developing%20countries%20usually%20lack%20the%20proper%20governance%20means%20to%20generate%20and%0Aregularly%20update%20a%20national%20rooftop%20map.%20Using%20traditional%20photogrammetry%20and%0Asurveying%20methods%20to%20produce%20a%20building%20map%20at%20the%20federal%20level%20is%20costly%20and%0Atime%20consuming.%20Using%20earth%20observation%20and%20deep%20learning%20methods%2C%20we%20can%0Abridge%20this%20gap%20and%20propose%20an%20automated%20pipeline%20to%20fetch%20such%20national%20urban%0Amaps.%20This%20paper%20aims%20to%20exploit%20the%20power%20of%20fully%20convolutional%20neural%0Anetworks%20for%20multi-class%20buildings%27%20instance%20segmentation%20to%20leverage%20high%0Aobject-wise%20accuracy%20results.%20Buildings%27%20instance%20segmentation%20from%20sub-meter%0Ahigh-resolution%20satellite%20images%20can%20be%20achieved%20with%20relatively%20high%0Apixel-wise%20metric%20scores.%20We%20detail%20all%20engineering%20steps%20to%20replicate%20this%0Awork%20and%20ensure%20highly%20accurate%20results%20in%20dense%20and%20slum%20areas%20witnessed%20in%0Aregions%20that%20lack%20proper%20urban%20planning%20in%20the%20Global%20South.%20We%20applied%20a%20case%0Astudy%20of%20the%20proposed%20pipeline%20to%20Lebanon%20and%20successfully%20produced%20the%20first%0Acomprehensive%20national%20building%20footprint%20map%20with%20approximately%201%20Million%0Aunits%20with%20an%2084%25%20accuracy.%20The%20proposed%20architecture%20relies%20on%20advanced%0Aaugmentation%20techniques%20to%20overcome%20dataset%20scarcity%2C%20which%20is%20often%20the%20case%0Ain%20developing%20countries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06202v1&entry.124074799=Read"},
{"title": "Text-Based Reasoning About Vector Graphics", "author": "Zhenhailong Wang and Joy Hsu and Xingyao Wang and Kuan-Hao Huang and Manling Li and Jiajun Wu and Heng Ji", "abstract": "  While large multimodal models excel in broad vision-language benchmarks, they\noften struggle with tasks requiring precise perception of low-level visual\ndetails, such as comparing line lengths or solving simple mazes. In particular,\nthis failure mode persists in question-answering tasks about vector graphics --\nimages composed purely of 2D objects and shapes. To address this challenge, we\npropose the Visually Descriptive Language Model (VDLM), which performs\ntext-based reasoning about vector graphics. VDLM leverages Scalable Vector\nGraphics (SVG) for a more precise visual description and first uses an\noff-the-shelf raster-to-SVG algorithm for encoding. Since existing language\nmodels cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG\nwith pretrained language models through a newly introduced intermediate\nsymbolic representation, Primal Visual Description (PVD), comprising primitive\nattributes (e.g., shape, position, measurement) with their corresponding\npredicted values. PVD is task-agnostic and represents visual primitives that\nare universal across all vector graphics. It can be learned with procedurally\ngenerated (SVG, PVD) pairs and also enables the direct use of LLMs for\ngeneralization to complex reasoning tasks. By casting an image to a text-based\nrepresentation, we can leverage the power of language models to learn alignment\nfrom SVG to visual primitives and generalize to unseen question-answering\ntasks. Empirical results show that VDLM achieves stronger zero-shot performance\ncompared to state-of-the-art LMMs, such as GPT-4V, in various low-level\nmultimodal perception and reasoning tasks on vector graphics. We additionally\npresent extensive analyses on VDLM's performance, demonstrating that our\nframework offers better interpretability due to its disentangled perception and\nreasoning processes. Project page: https://mikewangwzhl.github.io/VDLM/\n", "link": "http://arxiv.org/abs/2404.06479v1", "date": "2024-04-09", "relevancy": 2.7267, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5621}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5438}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5302}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Text-Based%20Reasoning%20About%20Vector%20Graphics&body=Title%3A%20Text-Based%20Reasoning%20About%20Vector%20Graphics%0AAuthor%3A%20Zhenhailong%20Wang%20and%20Joy%20Hsu%20and%20Xingyao%20Wang%20and%20Kuan-Hao%20Huang%20and%20Manling%20Li%20and%20Jiajun%20Wu%20and%20Heng%20Ji%0AAbstract%3A%20%20%20While%20large%20multimodal%20models%20excel%20in%20broad%20vision-language%20benchmarks%2C%20they%0Aoften%20struggle%20with%20tasks%20requiring%20precise%20perception%20of%20low-level%20visual%0Adetails%2C%20such%20as%20comparing%20line%20lengths%20or%20solving%20simple%20mazes.%20In%20particular%2C%0Athis%20failure%20mode%20persists%20in%20question-answering%20tasks%20about%20vector%20graphics%20--%0Aimages%20composed%20purely%20of%202D%20objects%20and%20shapes.%20To%20address%20this%20challenge%2C%20we%0Apropose%20the%20Visually%20Descriptive%20Language%20Model%20%28VDLM%29%2C%20which%20performs%0Atext-based%20reasoning%20about%20vector%20graphics.%20VDLM%20leverages%20Scalable%20Vector%0AGraphics%20%28SVG%29%20for%20a%20more%20precise%20visual%20description%20and%20first%20uses%20an%0Aoff-the-shelf%20raster-to-SVG%20algorithm%20for%20encoding.%20Since%20existing%20language%0Amodels%20cannot%20understand%20raw%20SVGs%20in%20a%20zero-shot%20setting%2C%20VDLM%20then%20bridges%20SVG%0Awith%20pretrained%20language%20models%20through%20a%20newly%20introduced%20intermediate%0Asymbolic%20representation%2C%20Primal%20Visual%20Description%20%28PVD%29%2C%20comprising%20primitive%0Aattributes%20%28e.g.%2C%20shape%2C%20position%2C%20measurement%29%20with%20their%20corresponding%0Apredicted%20values.%20PVD%20is%20task-agnostic%20and%20represents%20visual%20primitives%20that%0Aare%20universal%20across%20all%20vector%20graphics.%20It%20can%20be%20learned%20with%20procedurally%0Agenerated%20%28SVG%2C%20PVD%29%20pairs%20and%20also%20enables%20the%20direct%20use%20of%20LLMs%20for%0Ageneralization%20to%20complex%20reasoning%20tasks.%20By%20casting%20an%20image%20to%20a%20text-based%0Arepresentation%2C%20we%20can%20leverage%20the%20power%20of%20language%20models%20to%20learn%20alignment%0Afrom%20SVG%20to%20visual%20primitives%20and%20generalize%20to%20unseen%20question-answering%0Atasks.%20Empirical%20results%20show%20that%20VDLM%20achieves%20stronger%20zero-shot%20performance%0Acompared%20to%20state-of-the-art%20LMMs%2C%20such%20as%20GPT-4V%2C%20in%20various%20low-level%0Amultimodal%20perception%20and%20reasoning%20tasks%20on%20vector%20graphics.%20We%20additionally%0Apresent%20extensive%20analyses%20on%20VDLM%27s%20performance%2C%20demonstrating%20that%20our%0Aframework%20offers%20better%20interpretability%20due%20to%20its%20disentangled%20perception%20and%0Areasoning%20processes.%20Project%20page%3A%20https%3A//mikewangwzhl.github.io/VDLM/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06479v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Based%20Reasoning%20About%20Vector%20Graphics&entry.906535625=Zhenhailong%20Wang%20and%20Joy%20Hsu%20and%20Xingyao%20Wang%20and%20Kuan-Hao%20Huang%20and%20Manling%20Li%20and%20Jiajun%20Wu%20and%20Heng%20Ji&entry.1292438233=%20%20While%20large%20multimodal%20models%20excel%20in%20broad%20vision-language%20benchmarks%2C%20they%0Aoften%20struggle%20with%20tasks%20requiring%20precise%20perception%20of%20low-level%20visual%0Adetails%2C%20such%20as%20comparing%20line%20lengths%20or%20solving%20simple%20mazes.%20In%20particular%2C%0Athis%20failure%20mode%20persists%20in%20question-answering%20tasks%20about%20vector%20graphics%20--%0Aimages%20composed%20purely%20of%202D%20objects%20and%20shapes.%20To%20address%20this%20challenge%2C%20we%0Apropose%20the%20Visually%20Descriptive%20Language%20Model%20%28VDLM%29%2C%20which%20performs%0Atext-based%20reasoning%20about%20vector%20graphics.%20VDLM%20leverages%20Scalable%20Vector%0AGraphics%20%28SVG%29%20for%20a%20more%20precise%20visual%20description%20and%20first%20uses%20an%0Aoff-the-shelf%20raster-to-SVG%20algorithm%20for%20encoding.%20Since%20existing%20language%0Amodels%20cannot%20understand%20raw%20SVGs%20in%20a%20zero-shot%20setting%2C%20VDLM%20then%20bridges%20SVG%0Awith%20pretrained%20language%20models%20through%20a%20newly%20introduced%20intermediate%0Asymbolic%20representation%2C%20Primal%20Visual%20Description%20%28PVD%29%2C%20comprising%20primitive%0Aattributes%20%28e.g.%2C%20shape%2C%20position%2C%20measurement%29%20with%20their%20corresponding%0Apredicted%20values.%20PVD%20is%20task-agnostic%20and%20represents%20visual%20primitives%20that%0Aare%20universal%20across%20all%20vector%20graphics.%20It%20can%20be%20learned%20with%20procedurally%0Agenerated%20%28SVG%2C%20PVD%29%20pairs%20and%20also%20enables%20the%20direct%20use%20of%20LLMs%20for%0Ageneralization%20to%20complex%20reasoning%20tasks.%20By%20casting%20an%20image%20to%20a%20text-based%0Arepresentation%2C%20we%20can%20leverage%20the%20power%20of%20language%20models%20to%20learn%20alignment%0Afrom%20SVG%20to%20visual%20primitives%20and%20generalize%20to%20unseen%20question-answering%0Atasks.%20Empirical%20results%20show%20that%20VDLM%20achieves%20stronger%20zero-shot%20performance%0Acompared%20to%20state-of-the-art%20LMMs%2C%20such%20as%20GPT-4V%2C%20in%20various%20low-level%0Amultimodal%20perception%20and%20reasoning%20tasks%20on%20vector%20graphics.%20We%20additionally%0Apresent%20extensive%20analyses%20on%20VDLM%27s%20performance%2C%20demonstrating%20that%20our%0Aframework%20offers%20better%20interpretability%20due%20to%20its%20disentangled%20perception%20and%0Areasoning%20processes.%20Project%20page%3A%20https%3A//mikewangwzhl.github.io/VDLM/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06479v1&entry.124074799=Read"},
{"title": "MultIOD: Rehearsal-free Multihead Incremental Object Detector", "author": "Eden Belouadah and Arnaud Dapogny and Kevin Bailly", "abstract": "  Class-Incremental learning (CIL) refers to the ability of artificial agents\nto integrate new classes as they appear in a stream. It is particularly\ninteresting in evolving environments where agents have limited access to memory\nand computational resources. The main challenge of incremental learning is\ncatastrophic forgetting, the inability of neural networks to retain past\nknowledge when learning a new one. Unfortunately, most existing\nclass-incremental methods for object detection are applied to two-stage\nalgorithms such as Faster-RCNN, and rely on rehearsal memory to retain past\nknowledge. We argue that those are not suitable in resource-limited\nenvironments, and more effort should be dedicated to anchor-free and\nrehearsal-free object detection. In this paper, we propose MultIOD, a\nclass-incremental object detector based on CenterNet. Our contributions are:\n(1) we propose a multihead feature pyramid and multihead detection architecture\nto efficiently separate class representations, (2) we employ transfer learning\nbetween classes learned initially and those learned incrementally to tackle\ncatastrophic forgetting, and (3) we use a class-wise non-max-suppression as a\npost-processing technique to remove redundant boxes. Results show that our\nmethod outperforms state-of-the-art methods on two Pascal VOC datasets, while\nonly saving the model in its current state, contrary to other\ndistillation-based counterparts.\n", "link": "http://arxiv.org/abs/2309.05334v3", "date": "2024-04-09", "relevancy": 2.696, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5458}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5366}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MultIOD%3A%20Rehearsal-free%20Multihead%20Incremental%20Object%20Detector&body=Title%3A%20MultIOD%3A%20Rehearsal-free%20Multihead%20Incremental%20Object%20Detector%0AAuthor%3A%20Eden%20Belouadah%20and%20Arnaud%20Dapogny%20and%20Kevin%20Bailly%0AAbstract%3A%20%20%20Class-Incremental%20learning%20%28CIL%29%20refers%20to%20the%20ability%20of%20artificial%20agents%0Ato%20integrate%20new%20classes%20as%20they%20appear%20in%20a%20stream.%20It%20is%20particularly%0Ainteresting%20in%20evolving%20environments%20where%20agents%20have%20limited%20access%20to%20memory%0Aand%20computational%20resources.%20The%20main%20challenge%20of%20incremental%20learning%20is%0Acatastrophic%20forgetting%2C%20the%20inability%20of%20neural%20networks%20to%20retain%20past%0Aknowledge%20when%20learning%20a%20new%20one.%20Unfortunately%2C%20most%20existing%0Aclass-incremental%20methods%20for%20object%20detection%20are%20applied%20to%20two-stage%0Aalgorithms%20such%20as%20Faster-RCNN%2C%20and%20rely%20on%20rehearsal%20memory%20to%20retain%20past%0Aknowledge.%20We%20argue%20that%20those%20are%20not%20suitable%20in%20resource-limited%0Aenvironments%2C%20and%20more%20effort%20should%20be%20dedicated%20to%20anchor-free%20and%0Arehearsal-free%20object%20detection.%20In%20this%20paper%2C%20we%20propose%20MultIOD%2C%20a%0Aclass-incremental%20object%20detector%20based%20on%20CenterNet.%20Our%20contributions%20are%3A%0A%281%29%20we%20propose%20a%20multihead%20feature%20pyramid%20and%20multihead%20detection%20architecture%0Ato%20efficiently%20separate%20class%20representations%2C%20%282%29%20we%20employ%20transfer%20learning%0Abetween%20classes%20learned%20initially%20and%20those%20learned%20incrementally%20to%20tackle%0Acatastrophic%20forgetting%2C%20and%20%283%29%20we%20use%20a%20class-wise%20non-max-suppression%20as%20a%0Apost-processing%20technique%20to%20remove%20redundant%20boxes.%20Results%20show%20that%20our%0Amethod%20outperforms%20state-of-the-art%20methods%20on%20two%20Pascal%20VOC%20datasets%2C%20while%0Aonly%20saving%20the%20model%20in%20its%20current%20state%2C%20contrary%20to%20other%0Adistillation-based%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05334v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultIOD%3A%20Rehearsal-free%20Multihead%20Incremental%20Object%20Detector&entry.906535625=Eden%20Belouadah%20and%20Arnaud%20Dapogny%20and%20Kevin%20Bailly&entry.1292438233=%20%20Class-Incremental%20learning%20%28CIL%29%20refers%20to%20the%20ability%20of%20artificial%20agents%0Ato%20integrate%20new%20classes%20as%20they%20appear%20in%20a%20stream.%20It%20is%20particularly%0Ainteresting%20in%20evolving%20environments%20where%20agents%20have%20limited%20access%20to%20memory%0Aand%20computational%20resources.%20The%20main%20challenge%20of%20incremental%20learning%20is%0Acatastrophic%20forgetting%2C%20the%20inability%20of%20neural%20networks%20to%20retain%20past%0Aknowledge%20when%20learning%20a%20new%20one.%20Unfortunately%2C%20most%20existing%0Aclass-incremental%20methods%20for%20object%20detection%20are%20applied%20to%20two-stage%0Aalgorithms%20such%20as%20Faster-RCNN%2C%20and%20rely%20on%20rehearsal%20memory%20to%20retain%20past%0Aknowledge.%20We%20argue%20that%20those%20are%20not%20suitable%20in%20resource-limited%0Aenvironments%2C%20and%20more%20effort%20should%20be%20dedicated%20to%20anchor-free%20and%0Arehearsal-free%20object%20detection.%20In%20this%20paper%2C%20we%20propose%20MultIOD%2C%20a%0Aclass-incremental%20object%20detector%20based%20on%20CenterNet.%20Our%20contributions%20are%3A%0A%281%29%20we%20propose%20a%20multihead%20feature%20pyramid%20and%20multihead%20detection%20architecture%0Ato%20efficiently%20separate%20class%20representations%2C%20%282%29%20we%20employ%20transfer%20learning%0Abetween%20classes%20learned%20initially%20and%20those%20learned%20incrementally%20to%20tackle%0Acatastrophic%20forgetting%2C%20and%20%283%29%20we%20use%20a%20class-wise%20non-max-suppression%20as%20a%0Apost-processing%20technique%20to%20remove%20redundant%20boxes.%20Results%20show%20that%20our%0Amethod%20outperforms%20state-of-the-art%20methods%20on%20two%20Pascal%20VOC%20datasets%2C%20while%0Aonly%20saving%20the%20model%20in%20its%20current%20state%2C%20contrary%20to%20other%0Adistillation-based%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05334v3&entry.124074799=Read"},
{"title": "Cross-Silo Federated Learning Across Divergent Domains with Iterative\n  Parameter Alignment", "author": "Matt Gorbett and Hossein Shirazi and Indrakshi Ray", "abstract": "  Learning from the collective knowledge of data dispersed across private\nsources can provide neural networks with enhanced generalization capabilities.\nFederated learning, a method for collaboratively training a machine learning\nmodel across remote clients, achieves this by combining client models via the\norchestration of a central server. However, current approaches face two\ncritical limitations: i) they struggle to converge when client domains are\nsufficiently different, and ii) current aggregation techniques produce an\nidentical global model for each client. In this work, we address these issues\nby reformulating the typical federated learning setup: rather than learning a\nsingle global model, we learn N models each optimized for a common objective.\nTo achieve this, we apply a weighted distance minimization to model parameters\nshared in a peer-to-peer topology. The resulting framework, Iterative Parameter\nAlignment, applies naturally to the cross-silo setting, and has the following\nproperties: (i) a unique solution for each participant, with the option to\nglobally converge each model in the federation, and (ii) an optional\nearly-stopping mechanism to elicit fairness among peers in collaborative\nlearning settings. These characteristics jointly provide a flexible new\nframework for iteratively learning from peer models trained on disparate\ndatasets. We find that the technique achieves competitive results on a variety\nof data partitions compared to state-of-the-art approaches. Further, we show\nthat the method is robust to divergent domains (i.e. disjoint classes across\npeers) where existing approaches struggle.\n", "link": "http://arxiv.org/abs/2311.04818v4", "date": "2024-04-09", "relevancy": 2.6934, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5437}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5228}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-Silo%20Federated%20Learning%20Across%20Divergent%20Domains%20with%20Iterative%0A%20%20Parameter%20Alignment&body=Title%3A%20Cross-Silo%20Federated%20Learning%20Across%20Divergent%20Domains%20with%20Iterative%0A%20%20Parameter%20Alignment%0AAuthor%3A%20Matt%20Gorbett%20and%20Hossein%20Shirazi%20and%20Indrakshi%20Ray%0AAbstract%3A%20%20%20Learning%20from%20the%20collective%20knowledge%20of%20data%20dispersed%20across%20private%0Asources%20can%20provide%20neural%20networks%20with%20enhanced%20generalization%20capabilities.%0AFederated%20learning%2C%20a%20method%20for%20collaboratively%20training%20a%20machine%20learning%0Amodel%20across%20remote%20clients%2C%20achieves%20this%20by%20combining%20client%20models%20via%20the%0Aorchestration%20of%20a%20central%20server.%20However%2C%20current%20approaches%20face%20two%0Acritical%20limitations%3A%20i%29%20they%20struggle%20to%20converge%20when%20client%20domains%20are%0Asufficiently%20different%2C%20and%20ii%29%20current%20aggregation%20techniques%20produce%20an%0Aidentical%20global%20model%20for%20each%20client.%20In%20this%20work%2C%20we%20address%20these%20issues%0Aby%20reformulating%20the%20typical%20federated%20learning%20setup%3A%20rather%20than%20learning%20a%0Asingle%20global%20model%2C%20we%20learn%20N%20models%20each%20optimized%20for%20a%20common%20objective.%0ATo%20achieve%20this%2C%20we%20apply%20a%20weighted%20distance%20minimization%20to%20model%20parameters%0Ashared%20in%20a%20peer-to-peer%20topology.%20The%20resulting%20framework%2C%20Iterative%20Parameter%0AAlignment%2C%20applies%20naturally%20to%20the%20cross-silo%20setting%2C%20and%20has%20the%20following%0Aproperties%3A%20%28i%29%20a%20unique%20solution%20for%20each%20participant%2C%20with%20the%20option%20to%0Aglobally%20converge%20each%20model%20in%20the%20federation%2C%20and%20%28ii%29%20an%20optional%0Aearly-stopping%20mechanism%20to%20elicit%20fairness%20among%20peers%20in%20collaborative%0Alearning%20settings.%20These%20characteristics%20jointly%20provide%20a%20flexible%20new%0Aframework%20for%20iteratively%20learning%20from%20peer%20models%20trained%20on%20disparate%0Adatasets.%20We%20find%20that%20the%20technique%20achieves%20competitive%20results%20on%20a%20variety%0Aof%20data%20partitions%20compared%20to%20state-of-the-art%20approaches.%20Further%2C%20we%20show%0Athat%20the%20method%20is%20robust%20to%20divergent%20domains%20%28i.e.%20disjoint%20classes%20across%0Apeers%29%20where%20existing%20approaches%20struggle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04818v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Silo%20Federated%20Learning%20Across%20Divergent%20Domains%20with%20Iterative%0A%20%20Parameter%20Alignment&entry.906535625=Matt%20Gorbett%20and%20Hossein%20Shirazi%20and%20Indrakshi%20Ray&entry.1292438233=%20%20Learning%20from%20the%20collective%20knowledge%20of%20data%20dispersed%20across%20private%0Asources%20can%20provide%20neural%20networks%20with%20enhanced%20generalization%20capabilities.%0AFederated%20learning%2C%20a%20method%20for%20collaboratively%20training%20a%20machine%20learning%0Amodel%20across%20remote%20clients%2C%20achieves%20this%20by%20combining%20client%20models%20via%20the%0Aorchestration%20of%20a%20central%20server.%20However%2C%20current%20approaches%20face%20two%0Acritical%20limitations%3A%20i%29%20they%20struggle%20to%20converge%20when%20client%20domains%20are%0Asufficiently%20different%2C%20and%20ii%29%20current%20aggregation%20techniques%20produce%20an%0Aidentical%20global%20model%20for%20each%20client.%20In%20this%20work%2C%20we%20address%20these%20issues%0Aby%20reformulating%20the%20typical%20federated%20learning%20setup%3A%20rather%20than%20learning%20a%0Asingle%20global%20model%2C%20we%20learn%20N%20models%20each%20optimized%20for%20a%20common%20objective.%0ATo%20achieve%20this%2C%20we%20apply%20a%20weighted%20distance%20minimization%20to%20model%20parameters%0Ashared%20in%20a%20peer-to-peer%20topology.%20The%20resulting%20framework%2C%20Iterative%20Parameter%0AAlignment%2C%20applies%20naturally%20to%20the%20cross-silo%20setting%2C%20and%20has%20the%20following%0Aproperties%3A%20%28i%29%20a%20unique%20solution%20for%20each%20participant%2C%20with%20the%20option%20to%0Aglobally%20converge%20each%20model%20in%20the%20federation%2C%20and%20%28ii%29%20an%20optional%0Aearly-stopping%20mechanism%20to%20elicit%20fairness%20among%20peers%20in%20collaborative%0Alearning%20settings.%20These%20characteristics%20jointly%20provide%20a%20flexible%20new%0Aframework%20for%20iteratively%20learning%20from%20peer%20models%20trained%20on%20disparate%0Adatasets.%20We%20find%20that%20the%20technique%20achieves%20competitive%20results%20on%20a%20variety%0Aof%20data%20partitions%20compared%20to%20state-of-the-art%20approaches.%20Further%2C%20we%20show%0Athat%20the%20method%20is%20robust%20to%20divergent%20domains%20%28i.e.%20disjoint%20classes%20across%0Apeers%29%20where%20existing%20approaches%20struggle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04818v4&entry.124074799=Read"},
{"title": "From Barlow Twins to Triplet Training: Differentiating Dementia with\n  Limited Data", "author": "Yitong Li and Tom Nuno Wolf and Sebastian P\u00f6lsterl and Igor Yakushev and Dennis M. Hedderich and Christian Wachinger", "abstract": "  Differential diagnosis of dementia is challenging due to overlapping\nsymptoms, with structural magnetic resonance imaging (MRI) being the primary\nmethod for diagnosis. Despite the clinical value of computer-aided differential\ndiagnosis, research has been limited, mainly due to the absence of public\ndatasets that contain diverse types of dementia. This leaves researchers with\nsmall in-house datasets that are insufficient for training deep neural networks\n(DNNs). Self-supervised learning shows promise for utilizing unlabeled MRI\nscans in training, but small batch sizes for volumetric brain scans make its\napplication challenging. To address these issues, we propose Triplet Training\nfor differential diagnosis with limited target data. It consists of three key\nstages: (i) self-supervised pre-training on unlabeled data with Barlow Twins,\n(ii) self-distillation on task-related data, and (iii) fine-tuning on the\ntarget dataset. Our approach significantly outperforms traditional training\nstrategies, achieving a balanced accuracy of 75.6%. We further provide insights\ninto the training process by visualizing changes in the latent space after each\nstep. Finally, we validate the robustness of Triplet Training in terms of its\nindividual components in a comprehensive ablation study. Our code is available\nat https://github.com/ai-med/TripletTraining.\n", "link": "http://arxiv.org/abs/2404.06253v1", "date": "2024-04-09", "relevancy": 2.6814, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5695}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5233}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20Barlow%20Twins%20to%20Triplet%20Training%3A%20Differentiating%20Dementia%20with%0A%20%20Limited%20Data&body=Title%3A%20From%20Barlow%20Twins%20to%20Triplet%20Training%3A%20Differentiating%20Dementia%20with%0A%20%20Limited%20Data%0AAuthor%3A%20Yitong%20Li%20and%20Tom%20Nuno%20Wolf%20and%20Sebastian%20P%C3%B6lsterl%20and%20Igor%20Yakushev%20and%20Dennis%20M.%20Hedderich%20and%20Christian%20Wachinger%0AAbstract%3A%20%20%20Differential%20diagnosis%20of%20dementia%20is%20challenging%20due%20to%20overlapping%0Asymptoms%2C%20with%20structural%20magnetic%20resonance%20imaging%20%28MRI%29%20being%20the%20primary%0Amethod%20for%20diagnosis.%20Despite%20the%20clinical%20value%20of%20computer-aided%20differential%0Adiagnosis%2C%20research%20has%20been%20limited%2C%20mainly%20due%20to%20the%20absence%20of%20public%0Adatasets%20that%20contain%20diverse%20types%20of%20dementia.%20This%20leaves%20researchers%20with%0Asmall%20in-house%20datasets%20that%20are%20insufficient%20for%20training%20deep%20neural%20networks%0A%28DNNs%29.%20Self-supervised%20learning%20shows%20promise%20for%20utilizing%20unlabeled%20MRI%0Ascans%20in%20training%2C%20but%20small%20batch%20sizes%20for%20volumetric%20brain%20scans%20make%20its%0Aapplication%20challenging.%20To%20address%20these%20issues%2C%20we%20propose%20Triplet%20Training%0Afor%20differential%20diagnosis%20with%20limited%20target%20data.%20It%20consists%20of%20three%20key%0Astages%3A%20%28i%29%20self-supervised%20pre-training%20on%20unlabeled%20data%20with%20Barlow%20Twins%2C%0A%28ii%29%20self-distillation%20on%20task-related%20data%2C%20and%20%28iii%29%20fine-tuning%20on%20the%0Atarget%20dataset.%20Our%20approach%20significantly%20outperforms%20traditional%20training%0Astrategies%2C%20achieving%20a%20balanced%20accuracy%20of%2075.6%25.%20We%20further%20provide%20insights%0Ainto%20the%20training%20process%20by%20visualizing%20changes%20in%20the%20latent%20space%20after%20each%0Astep.%20Finally%2C%20we%20validate%20the%20robustness%20of%20Triplet%20Training%20in%20terms%20of%20its%0Aindividual%20components%20in%20a%20comprehensive%20ablation%20study.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/ai-med/TripletTraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06253v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Barlow%20Twins%20to%20Triplet%20Training%3A%20Differentiating%20Dementia%20with%0A%20%20Limited%20Data&entry.906535625=Yitong%20Li%20and%20Tom%20Nuno%20Wolf%20and%20Sebastian%20P%C3%B6lsterl%20and%20Igor%20Yakushev%20and%20Dennis%20M.%20Hedderich%20and%20Christian%20Wachinger&entry.1292438233=%20%20Differential%20diagnosis%20of%20dementia%20is%20challenging%20due%20to%20overlapping%0Asymptoms%2C%20with%20structural%20magnetic%20resonance%20imaging%20%28MRI%29%20being%20the%20primary%0Amethod%20for%20diagnosis.%20Despite%20the%20clinical%20value%20of%20computer-aided%20differential%0Adiagnosis%2C%20research%20has%20been%20limited%2C%20mainly%20due%20to%20the%20absence%20of%20public%0Adatasets%20that%20contain%20diverse%20types%20of%20dementia.%20This%20leaves%20researchers%20with%0Asmall%20in-house%20datasets%20that%20are%20insufficient%20for%20training%20deep%20neural%20networks%0A%28DNNs%29.%20Self-supervised%20learning%20shows%20promise%20for%20utilizing%20unlabeled%20MRI%0Ascans%20in%20training%2C%20but%20small%20batch%20sizes%20for%20volumetric%20brain%20scans%20make%20its%0Aapplication%20challenging.%20To%20address%20these%20issues%2C%20we%20propose%20Triplet%20Training%0Afor%20differential%20diagnosis%20with%20limited%20target%20data.%20It%20consists%20of%20three%20key%0Astages%3A%20%28i%29%20self-supervised%20pre-training%20on%20unlabeled%20data%20with%20Barlow%20Twins%2C%0A%28ii%29%20self-distillation%20on%20task-related%20data%2C%20and%20%28iii%29%20fine-tuning%20on%20the%0Atarget%20dataset.%20Our%20approach%20significantly%20outperforms%20traditional%20training%0Astrategies%2C%20achieving%20a%20balanced%20accuracy%20of%2075.6%25.%20We%20further%20provide%20insights%0Ainto%20the%20training%20process%20by%20visualizing%20changes%20in%20the%20latent%20space%20after%20each%0Astep.%20Finally%2C%20we%20validate%20the%20robustness%20of%20Triplet%20Training%20in%20terms%20of%20its%0Aindividual%20components%20in%20a%20comprehensive%20ablation%20study.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/ai-med/TripletTraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06253v1&entry.124074799=Read"},
{"title": "YOLC: You Only Look Clusters for Tiny Object Detection in Aerial Images", "author": "Chenguang Liu and Guangshuai Gao and Ziyue Huang and Zhenghui Hu and Qingjie Liu and Yunhong Wang", "abstract": "  Detecting objects from aerial images poses significant challenges due to the\nfollowing factors: 1) Aerial images typically have very large sizes, generally\nwith millions or even hundreds of millions of pixels, while computational\nresources are limited. 2) Small object size leads to insufficient information\nfor effective detection. 3) Non-uniform object distribution leads to\ncomputational resource wastage. To address these issues, we propose YOLC (You\nOnly Look Clusters), an efficient and effective framework that builds on an\nanchor-free object detector, CenterNet. To overcome the challenges posed by\nlarge-scale images and non-uniform object distribution, we introduce a Local\nScale Module (LSM) that adaptively searches cluster regions for zooming in for\naccurate detection. Additionally, we modify the regression loss using Gaussian\nWasserstein distance (GWD) to obtain high-quality bounding boxes. Deformable\nconvolution and refinement methods are employed in the detection head to\nenhance the detection of small objects. We perform extensive experiments on two\naerial image datasets, including Visdrone2019 and UAVDT, to demonstrate the\neffectiveness and superiority of our proposed approach.\n", "link": "http://arxiv.org/abs/2404.06180v1", "date": "2024-04-09", "relevancy": 2.6237, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5333}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5247}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5162}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20YOLC%3A%20You%20Only%20Look%20Clusters%20for%20Tiny%20Object%20Detection%20in%20Aerial%20Images&body=Title%3A%20YOLC%3A%20You%20Only%20Look%20Clusters%20for%20Tiny%20Object%20Detection%20in%20Aerial%20Images%0AAuthor%3A%20Chenguang%20Liu%20and%20Guangshuai%20Gao%20and%20Ziyue%20Huang%20and%20Zhenghui%20Hu%20and%20Qingjie%20Liu%20and%20Yunhong%20Wang%0AAbstract%3A%20%20%20Detecting%20objects%20from%20aerial%20images%20poses%20significant%20challenges%20due%20to%20the%0Afollowing%20factors%3A%201%29%20Aerial%20images%20typically%20have%20very%20large%20sizes%2C%20generally%0Awith%20millions%20or%20even%20hundreds%20of%20millions%20of%20pixels%2C%20while%20computational%0Aresources%20are%20limited.%202%29%20Small%20object%20size%20leads%20to%20insufficient%20information%0Afor%20effective%20detection.%203%29%20Non-uniform%20object%20distribution%20leads%20to%0Acomputational%20resource%20wastage.%20To%20address%20these%20issues%2C%20we%20propose%20YOLC%20%28You%0AOnly%20Look%20Clusters%29%2C%20an%20efficient%20and%20effective%20framework%20that%20builds%20on%20an%0Aanchor-free%20object%20detector%2C%20CenterNet.%20To%20overcome%20the%20challenges%20posed%20by%0Alarge-scale%20images%20and%20non-uniform%20object%20distribution%2C%20we%20introduce%20a%20Local%0AScale%20Module%20%28LSM%29%20that%20adaptively%20searches%20cluster%20regions%20for%20zooming%20in%20for%0Aaccurate%20detection.%20Additionally%2C%20we%20modify%20the%20regression%20loss%20using%20Gaussian%0AWasserstein%20distance%20%28GWD%29%20to%20obtain%20high-quality%20bounding%20boxes.%20Deformable%0Aconvolution%20and%20refinement%20methods%20are%20employed%20in%20the%20detection%20head%20to%0Aenhance%20the%20detection%20of%20small%20objects.%20We%20perform%20extensive%20experiments%20on%20two%0Aaerial%20image%20datasets%2C%20including%20Visdrone2019%20and%20UAVDT%2C%20to%20demonstrate%20the%0Aeffectiveness%20and%20superiority%20of%20our%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06180v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLC%3A%20You%20Only%20Look%20Clusters%20for%20Tiny%20Object%20Detection%20in%20Aerial%20Images&entry.906535625=Chenguang%20Liu%20and%20Guangshuai%20Gao%20and%20Ziyue%20Huang%20and%20Zhenghui%20Hu%20and%20Qingjie%20Liu%20and%20Yunhong%20Wang&entry.1292438233=%20%20Detecting%20objects%20from%20aerial%20images%20poses%20significant%20challenges%20due%20to%20the%0Afollowing%20factors%3A%201%29%20Aerial%20images%20typically%20have%20very%20large%20sizes%2C%20generally%0Awith%20millions%20or%20even%20hundreds%20of%20millions%20of%20pixels%2C%20while%20computational%0Aresources%20are%20limited.%202%29%20Small%20object%20size%20leads%20to%20insufficient%20information%0Afor%20effective%20detection.%203%29%20Non-uniform%20object%20distribution%20leads%20to%0Acomputational%20resource%20wastage.%20To%20address%20these%20issues%2C%20we%20propose%20YOLC%20%28You%0AOnly%20Look%20Clusters%29%2C%20an%20efficient%20and%20effective%20framework%20that%20builds%20on%20an%0Aanchor-free%20object%20detector%2C%20CenterNet.%20To%20overcome%20the%20challenges%20posed%20by%0Alarge-scale%20images%20and%20non-uniform%20object%20distribution%2C%20we%20introduce%20a%20Local%0AScale%20Module%20%28LSM%29%20that%20adaptively%20searches%20cluster%20regions%20for%20zooming%20in%20for%0Aaccurate%20detection.%20Additionally%2C%20we%20modify%20the%20regression%20loss%20using%20Gaussian%0AWasserstein%20distance%20%28GWD%29%20to%20obtain%20high-quality%20bounding%20boxes.%20Deformable%0Aconvolution%20and%20refinement%20methods%20are%20employed%20in%20the%20detection%20head%20to%0Aenhance%20the%20detection%20of%20small%20objects.%20We%20perform%20extensive%20experiments%20on%20two%0Aaerial%20image%20datasets%2C%20including%20Visdrone2019%20and%20UAVDT%2C%20to%20demonstrate%20the%0Aeffectiveness%20and%20superiority%20of%20our%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06180v1&entry.124074799=Read"},
{"title": "3D Geometry-aware Deformable Gaussian Splatting for Dynamic View\n  Synthesis", "author": "Zhicheng Lu and Xiang Guo and Le Hui and Tianrui Chen and Min Yang and Xiao Tang and Feng Zhu and Yuchao Dai", "abstract": "  In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting\nmethod for dynamic view synthesis. Existing neural radiance fields (NeRF) based\nsolutions learn the deformation in an implicit manner, which cannot incorporate\n3D scene geometry. Therefore, the learned deformation is not necessarily\ngeometrically coherent, which results in unsatisfactory dynamic view synthesis\nand 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new\nrepresentation of the 3D scene, building upon which the 3D geometry could be\nexploited in learning the complex 3D deformation. Specifically, the scenes are\nrepresented as a collection of 3D Gaussian, where each 3D Gaussian is optimized\nto move and rotate over time to model the deformation. To enforce the 3D scene\ngeometry constraint during deformation, we explicitly extract 3D geometry\nfeatures and integrate them in learning the 3D deformation. In this way, our\nsolution achieves 3D geometry-aware deformation modeling, which enables\nimproved dynamic view synthesis and 3D dynamic reconstruction. Extensive\nexperimental results on both synthetic and real datasets prove the superiority\nof our solution, which achieves new state-of-the-art performance.\n  The project is available at https://npucvr.github.io/GaGS/\n", "link": "http://arxiv.org/abs/2404.06270v1", "date": "2024-04-09", "relevancy": 2.5389, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.51}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5077}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5057}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Geometry-aware%20Deformable%20Gaussian%20Splatting%20for%20Dynamic%20View%0A%20%20Synthesis&body=Title%3A%203D%20Geometry-aware%20Deformable%20Gaussian%20Splatting%20for%20Dynamic%20View%0A%20%20Synthesis%0AAuthor%3A%20Zhicheng%20Lu%20and%20Xiang%20Guo%20and%20Le%20Hui%20and%20Tianrui%20Chen%20and%20Min%20Yang%20and%20Xiao%20Tang%20and%20Feng%20Zhu%20and%20Yuchao%20Dai%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%203D%20geometry-aware%20deformable%20Gaussian%20Splatting%0Amethod%20for%20dynamic%20view%20synthesis.%20Existing%20neural%20radiance%20fields%20%28NeRF%29%20based%0Asolutions%20learn%20the%20deformation%20in%20an%20implicit%20manner%2C%20which%20cannot%20incorporate%0A3D%20scene%20geometry.%20Therefore%2C%20the%20learned%20deformation%20is%20not%20necessarily%0Ageometrically%20coherent%2C%20which%20results%20in%20unsatisfactory%20dynamic%20view%20synthesis%0Aand%203D%20dynamic%20reconstruction.%20Recently%2C%203D%20Gaussian%20Splatting%20provides%20a%20new%0Arepresentation%20of%20the%203D%20scene%2C%20building%20upon%20which%20the%203D%20geometry%20could%20be%0Aexploited%20in%20learning%20the%20complex%203D%20deformation.%20Specifically%2C%20the%20scenes%20are%0Arepresented%20as%20a%20collection%20of%203D%20Gaussian%2C%20where%20each%203D%20Gaussian%20is%20optimized%0Ato%20move%20and%20rotate%20over%20time%20to%20model%20the%20deformation.%20To%20enforce%20the%203D%20scene%0Ageometry%20constraint%20during%20deformation%2C%20we%20explicitly%20extract%203D%20geometry%0Afeatures%20and%20integrate%20them%20in%20learning%20the%203D%20deformation.%20In%20this%20way%2C%20our%0Asolution%20achieves%203D%20geometry-aware%20deformation%20modeling%2C%20which%20enables%0Aimproved%20dynamic%20view%20synthesis%20and%203D%20dynamic%20reconstruction.%20Extensive%0Aexperimental%20results%20on%20both%20synthetic%20and%20real%20datasets%20prove%20the%20superiority%0Aof%20our%20solution%2C%20which%20achieves%20new%20state-of-the-art%20performance.%0A%20%20The%20project%20is%20available%20at%20https%3A//npucvr.github.io/GaGS/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06270v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Geometry-aware%20Deformable%20Gaussian%20Splatting%20for%20Dynamic%20View%0A%20%20Synthesis&entry.906535625=Zhicheng%20Lu%20and%20Xiang%20Guo%20and%20Le%20Hui%20and%20Tianrui%20Chen%20and%20Min%20Yang%20and%20Xiao%20Tang%20and%20Feng%20Zhu%20and%20Yuchao%20Dai&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%203D%20geometry-aware%20deformable%20Gaussian%20Splatting%0Amethod%20for%20dynamic%20view%20synthesis.%20Existing%20neural%20radiance%20fields%20%28NeRF%29%20based%0Asolutions%20learn%20the%20deformation%20in%20an%20implicit%20manner%2C%20which%20cannot%20incorporate%0A3D%20scene%20geometry.%20Therefore%2C%20the%20learned%20deformation%20is%20not%20necessarily%0Ageometrically%20coherent%2C%20which%20results%20in%20unsatisfactory%20dynamic%20view%20synthesis%0Aand%203D%20dynamic%20reconstruction.%20Recently%2C%203D%20Gaussian%20Splatting%20provides%20a%20new%0Arepresentation%20of%20the%203D%20scene%2C%20building%20upon%20which%20the%203D%20geometry%20could%20be%0Aexploited%20in%20learning%20the%20complex%203D%20deformation.%20Specifically%2C%20the%20scenes%20are%0Arepresented%20as%20a%20collection%20of%203D%20Gaussian%2C%20where%20each%203D%20Gaussian%20is%20optimized%0Ato%20move%20and%20rotate%20over%20time%20to%20model%20the%20deformation.%20To%20enforce%20the%203D%20scene%0Ageometry%20constraint%20during%20deformation%2C%20we%20explicitly%20extract%203D%20geometry%0Afeatures%20and%20integrate%20them%20in%20learning%20the%203D%20deformation.%20In%20this%20way%2C%20our%0Asolution%20achieves%203D%20geometry-aware%20deformation%20modeling%2C%20which%20enables%0Aimproved%20dynamic%20view%20synthesis%20and%203D%20dynamic%20reconstruction.%20Extensive%0Aexperimental%20results%20on%20both%20synthetic%20and%20real%20datasets%20prove%20the%20superiority%0Aof%20our%20solution%2C%20which%20achieves%20new%20state-of-the-art%20performance.%0A%20%20The%20project%20is%20available%20at%20https%3A//npucvr.github.io/GaGS/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06270v1&entry.124074799=Read"},
{"title": "Zero-Shot Relational Learning for Multimodal Knowledge Graphs", "author": "Rui Cai and Shichao Pei and Xiangliang Zhang", "abstract": "  Relational learning is an essential task in the domain of knowledge\nrepresentation, particularly in knowledge graph completion (KGC).While\nrelational learning in traditional single-modal settings has been extensively\nstudied, exploring it within a multimodal KGC context presents distinct\nchallenges and opportunities. One of the major challenges is inference on newly\ndiscovered relations without any associated training data. This zero-shot\nrelational learning scenario poses unique requirements for multimodal KGC,\ni.e., utilizing multimodality to facilitate relational learning. However,\nexisting works fail to support the leverage of multimodal information and leave\nthe problem unexplored. In this paper, we propose a novel end-to-end framework,\nconsisting of three components, i.e., multimodal learner, structure\nconsolidator, and relation embedding generator, to integrate diverse multimodal\ninformation and knowledge graph structures to facilitate the zero-shot\nrelational learning. Evaluation results on two multimodal knowledge graphs\ndemonstrate the superior performance of our proposed method.\n", "link": "http://arxiv.org/abs/2404.06220v1", "date": "2024-04-09", "relevancy": 2.5026, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5105}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4958}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4953}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Relational%20Learning%20for%20Multimodal%20Knowledge%20Graphs&body=Title%3A%20Zero-Shot%20Relational%20Learning%20for%20Multimodal%20Knowledge%20Graphs%0AAuthor%3A%20Rui%20Cai%20and%20Shichao%20Pei%20and%20Xiangliang%20Zhang%0AAbstract%3A%20%20%20Relational%20learning%20is%20an%20essential%20task%20in%20the%20domain%20of%20knowledge%0Arepresentation%2C%20particularly%20in%20knowledge%20graph%20completion%20%28KGC%29.While%0Arelational%20learning%20in%20traditional%20single-modal%20settings%20has%20been%20extensively%0Astudied%2C%20exploring%20it%20within%20a%20multimodal%20KGC%20context%20presents%20distinct%0Achallenges%20and%20opportunities.%20One%20of%20the%20major%20challenges%20is%20inference%20on%20newly%0Adiscovered%20relations%20without%20any%20associated%20training%20data.%20This%20zero-shot%0Arelational%20learning%20scenario%20poses%20unique%20requirements%20for%20multimodal%20KGC%2C%0Ai.e.%2C%20utilizing%20multimodality%20to%20facilitate%20relational%20learning.%20However%2C%0Aexisting%20works%20fail%20to%20support%20the%20leverage%20of%20multimodal%20information%20and%20leave%0Athe%20problem%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20end-to-end%20framework%2C%0Aconsisting%20of%20three%20components%2C%20i.e.%2C%20multimodal%20learner%2C%20structure%0Aconsolidator%2C%20and%20relation%20embedding%20generator%2C%20to%20integrate%20diverse%20multimodal%0Ainformation%20and%20knowledge%20graph%20structures%20to%20facilitate%20the%20zero-shot%0Arelational%20learning.%20Evaluation%20results%20on%20two%20multimodal%20knowledge%20graphs%0Ademonstrate%20the%20superior%20performance%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06220v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Relational%20Learning%20for%20Multimodal%20Knowledge%20Graphs&entry.906535625=Rui%20Cai%20and%20Shichao%20Pei%20and%20Xiangliang%20Zhang&entry.1292438233=%20%20Relational%20learning%20is%20an%20essential%20task%20in%20the%20domain%20of%20knowledge%0Arepresentation%2C%20particularly%20in%20knowledge%20graph%20completion%20%28KGC%29.While%0Arelational%20learning%20in%20traditional%20single-modal%20settings%20has%20been%20extensively%0Astudied%2C%20exploring%20it%20within%20a%20multimodal%20KGC%20context%20presents%20distinct%0Achallenges%20and%20opportunities.%20One%20of%20the%20major%20challenges%20is%20inference%20on%20newly%0Adiscovered%20relations%20without%20any%20associated%20training%20data.%20This%20zero-shot%0Arelational%20learning%20scenario%20poses%20unique%20requirements%20for%20multimodal%20KGC%2C%0Ai.e.%2C%20utilizing%20multimodality%20to%20facilitate%20relational%20learning.%20However%2C%0Aexisting%20works%20fail%20to%20support%20the%20leverage%20of%20multimodal%20information%20and%20leave%0Athe%20problem%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20end-to-end%20framework%2C%0Aconsisting%20of%20three%20components%2C%20i.e.%2C%20multimodal%20learner%2C%20structure%0Aconsolidator%2C%20and%20relation%20embedding%20generator%2C%20to%20integrate%20diverse%20multimodal%0Ainformation%20and%20knowledge%20graph%20structures%20to%20facilitate%20the%20zero-shot%0Arelational%20learning.%20Evaluation%20results%20on%20two%20multimodal%20knowledge%20graphs%0Ademonstrate%20the%20superior%20performance%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06220v1&entry.124074799=Read"},
{"title": "Bias Amplification Enhances Minority Group Performance", "author": "Gaotang Li and Jiarui Liu and Wei Hu", "abstract": "  Neural networks produced by standard training are known to suffer from poor\naccuracy on rare subgroups despite achieving high accuracy on average, due to\nthe correlations between certain spurious features and labels. Previous\napproaches based on worst-group loss minimization (e.g. Group-DRO) are\neffective in improving worse-group accuracy but require expensive group\nannotations for all the training samples. In this paper, we focus on the more\nchallenging and realistic setting where group annotations are only available on\na small validation set or are not available at all. We propose BAM, a novel\ntwo-stage training algorithm: in the first stage, the model is trained using a\nbias amplification scheme via introducing a learnable auxiliary variable for\neach training sample; in the second stage, we upweight the samples that the\nbias-amplified model misclassifies, and then continue training the same model\non the reweighted dataset. Empirically, BAM achieves competitive performance\ncompared with existing methods evaluated on spurious correlation benchmarks in\ncomputer vision and natural language processing. Moreover, we find a simple\nstopping criterion based on minimum class accuracy difference that can remove\nthe need for group annotations, with little or no loss in worst-group accuracy.\nWe perform extensive analyses and ablations to verify the effectiveness and\nrobustness of our algorithm in varying class and group imbalance ratios.\n", "link": "http://arxiv.org/abs/2309.06717v2", "date": "2024-04-09", "relevancy": 2.4951, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5241}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4918}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4812}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bias%20Amplification%20Enhances%20Minority%20Group%20Performance&body=Title%3A%20Bias%20Amplification%20Enhances%20Minority%20Group%20Performance%0AAuthor%3A%20Gaotang%20Li%20and%20Jiarui%20Liu%20and%20Wei%20Hu%0AAbstract%3A%20%20%20Neural%20networks%20produced%20by%20standard%20training%20are%20known%20to%20suffer%20from%20poor%0Aaccuracy%20on%20rare%20subgroups%20despite%20achieving%20high%20accuracy%20on%20average%2C%20due%20to%0Athe%20correlations%20between%20certain%20spurious%20features%20and%20labels.%20Previous%0Aapproaches%20based%20on%20worst-group%20loss%20minimization%20%28e.g.%20Group-DRO%29%20are%0Aeffective%20in%20improving%20worse-group%20accuracy%20but%20require%20expensive%20group%0Aannotations%20for%20all%20the%20training%20samples.%20In%20this%20paper%2C%20we%20focus%20on%20the%20more%0Achallenging%20and%20realistic%20setting%20where%20group%20annotations%20are%20only%20available%20on%0Aa%20small%20validation%20set%20or%20are%20not%20available%20at%20all.%20We%20propose%20BAM%2C%20a%20novel%0Atwo-stage%20training%20algorithm%3A%20in%20the%20first%20stage%2C%20the%20model%20is%20trained%20using%20a%0Abias%20amplification%20scheme%20via%20introducing%20a%20learnable%20auxiliary%20variable%20for%0Aeach%20training%20sample%3B%20in%20the%20second%20stage%2C%20we%20upweight%20the%20samples%20that%20the%0Abias-amplified%20model%20misclassifies%2C%20and%20then%20continue%20training%20the%20same%20model%0Aon%20the%20reweighted%20dataset.%20Empirically%2C%20BAM%20achieves%20competitive%20performance%0Acompared%20with%20existing%20methods%20evaluated%20on%20spurious%20correlation%20benchmarks%20in%0Acomputer%20vision%20and%20natural%20language%20processing.%20Moreover%2C%20we%20find%20a%20simple%0Astopping%20criterion%20based%20on%20minimum%20class%20accuracy%20difference%20that%20can%20remove%0Athe%20need%20for%20group%20annotations%2C%20with%20little%20or%20no%20loss%20in%20worst-group%20accuracy.%0AWe%20perform%20extensive%20analyses%20and%20ablations%20to%20verify%20the%20effectiveness%20and%0Arobustness%20of%20our%20algorithm%20in%20varying%20class%20and%20group%20imbalance%20ratios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06717v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias%20Amplification%20Enhances%20Minority%20Group%20Performance&entry.906535625=Gaotang%20Li%20and%20Jiarui%20Liu%20and%20Wei%20Hu&entry.1292438233=%20%20Neural%20networks%20produced%20by%20standard%20training%20are%20known%20to%20suffer%20from%20poor%0Aaccuracy%20on%20rare%20subgroups%20despite%20achieving%20high%20accuracy%20on%20average%2C%20due%20to%0Athe%20correlations%20between%20certain%20spurious%20features%20and%20labels.%20Previous%0Aapproaches%20based%20on%20worst-group%20loss%20minimization%20%28e.g.%20Group-DRO%29%20are%0Aeffective%20in%20improving%20worse-group%20accuracy%20but%20require%20expensive%20group%0Aannotations%20for%20all%20the%20training%20samples.%20In%20this%20paper%2C%20we%20focus%20on%20the%20more%0Achallenging%20and%20realistic%20setting%20where%20group%20annotations%20are%20only%20available%20on%0Aa%20small%20validation%20set%20or%20are%20not%20available%20at%20all.%20We%20propose%20BAM%2C%20a%20novel%0Atwo-stage%20training%20algorithm%3A%20in%20the%20first%20stage%2C%20the%20model%20is%20trained%20using%20a%0Abias%20amplification%20scheme%20via%20introducing%20a%20learnable%20auxiliary%20variable%20for%0Aeach%20training%20sample%3B%20in%20the%20second%20stage%2C%20we%20upweight%20the%20samples%20that%20the%0Abias-amplified%20model%20misclassifies%2C%20and%20then%20continue%20training%20the%20same%20model%0Aon%20the%20reweighted%20dataset.%20Empirically%2C%20BAM%20achieves%20competitive%20performance%0Acompared%20with%20existing%20methods%20evaluated%20on%20spurious%20correlation%20benchmarks%20in%0Acomputer%20vision%20and%20natural%20language%20processing.%20Moreover%2C%20we%20find%20a%20simple%0Astopping%20criterion%20based%20on%20minimum%20class%20accuracy%20difference%20that%20can%20remove%0Athe%20need%20for%20group%20annotations%2C%20with%20little%20or%20no%20loss%20in%20worst-group%20accuracy.%0AWe%20perform%20extensive%20analyses%20and%20ablations%20to%20verify%20the%20effectiveness%20and%0Arobustness%20of%20our%20algorithm%20in%20varying%20class%20and%20group%20imbalance%20ratios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06717v2&entry.124074799=Read"},
{"title": "Complete Neural Networks for Complete Euclidean Graphs", "author": "Snir Hordan and Tal Amir and Steven J. Gortler and Nadav Dym", "abstract": "  Neural networks for point clouds, which respect their natural invariance to\npermutation and rigid motion, have enjoyed recent success in modeling geometric\nphenomena, from molecular dynamics to recommender systems. Yet, to date, no\nmodel with polynomial complexity is known to be complete, that is, able to\ndistinguish between any pair of non-isomorphic point clouds. We fill this\ntheoretical gap by showing that point clouds can be completely determined, up\nto permutation and rigid motion, by applying the 3-WL graph isomorphism test to\nthe point cloud's centralized Gram matrix. Moreover, we formulate an Euclidean\nvariant of the 2-WL test and show that it is also sufficient to achieve\ncompleteness. We then show how our complete Euclidean WL tests can be simulated\nby an Euclidean graph neural network of moderate size and demonstrate their\nseparation capability on highly symmetrical point clouds.\n", "link": "http://arxiv.org/abs/2301.13821v4", "date": "2024-04-09", "relevancy": 2.4476, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5116}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4844}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4726}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Complete%20Neural%20Networks%20for%20Complete%20Euclidean%20Graphs&body=Title%3A%20Complete%20Neural%20Networks%20for%20Complete%20Euclidean%20Graphs%0AAuthor%3A%20Snir%20Hordan%20and%20Tal%20Amir%20and%20Steven%20J.%20Gortler%20and%20Nadav%20Dym%0AAbstract%3A%20%20%20Neural%20networks%20for%20point%20clouds%2C%20which%20respect%20their%20natural%20invariance%20to%0Apermutation%20and%20rigid%20motion%2C%20have%20enjoyed%20recent%20success%20in%20modeling%20geometric%0Aphenomena%2C%20from%20molecular%20dynamics%20to%20recommender%20systems.%20Yet%2C%20to%20date%2C%20no%0Amodel%20with%20polynomial%20complexity%20is%20known%20to%20be%20complete%2C%20that%20is%2C%20able%20to%0Adistinguish%20between%20any%20pair%20of%20non-isomorphic%20point%20clouds.%20We%20fill%20this%0Atheoretical%20gap%20by%20showing%20that%20point%20clouds%20can%20be%20completely%20determined%2C%20up%0Ato%20permutation%20and%20rigid%20motion%2C%20by%20applying%20the%203-WL%20graph%20isomorphism%20test%20to%0Athe%20point%20cloud%27s%20centralized%20Gram%20matrix.%20Moreover%2C%20we%20formulate%20an%20Euclidean%0Avariant%20of%20the%202-WL%20test%20and%20show%20that%20it%20is%20also%20sufficient%20to%20achieve%0Acompleteness.%20We%20then%20show%20how%20our%20complete%20Euclidean%20WL%20tests%20can%20be%20simulated%0Aby%20an%20Euclidean%20graph%20neural%20network%20of%20moderate%20size%20and%20demonstrate%20their%0Aseparation%20capability%20on%20highly%20symmetrical%20point%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13821v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complete%20Neural%20Networks%20for%20Complete%20Euclidean%20Graphs&entry.906535625=Snir%20Hordan%20and%20Tal%20Amir%20and%20Steven%20J.%20Gortler%20and%20Nadav%20Dym&entry.1292438233=%20%20Neural%20networks%20for%20point%20clouds%2C%20which%20respect%20their%20natural%20invariance%20to%0Apermutation%20and%20rigid%20motion%2C%20have%20enjoyed%20recent%20success%20in%20modeling%20geometric%0Aphenomena%2C%20from%20molecular%20dynamics%20to%20recommender%20systems.%20Yet%2C%20to%20date%2C%20no%0Amodel%20with%20polynomial%20complexity%20is%20known%20to%20be%20complete%2C%20that%20is%2C%20able%20to%0Adistinguish%20between%20any%20pair%20of%20non-isomorphic%20point%20clouds.%20We%20fill%20this%0Atheoretical%20gap%20by%20showing%20that%20point%20clouds%20can%20be%20completely%20determined%2C%20up%0Ato%20permutation%20and%20rigid%20motion%2C%20by%20applying%20the%203-WL%20graph%20isomorphism%20test%20to%0Athe%20point%20cloud%27s%20centralized%20Gram%20matrix.%20Moreover%2C%20we%20formulate%20an%20Euclidean%0Avariant%20of%20the%202-WL%20test%20and%20show%20that%20it%20is%20also%20sufficient%20to%20achieve%0Acompleteness.%20We%20then%20show%20how%20our%20complete%20Euclidean%20WL%20tests%20can%20be%20simulated%0Aby%20an%20Euclidean%20graph%20neural%20network%20of%20moderate%20size%20and%20demonstrate%20their%0Aseparation%20capability%20on%20highly%20symmetrical%20point%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13821v4&entry.124074799=Read"},
{"title": "One-Step Late Fusion Multi-view Clustering with Compressed Subspace", "author": "Qiyuan Ou and Pei Zhang and Sihang Zhou and En Zhu", "abstract": "  Late fusion multi-view clustering (LFMVC) has become a rapidly growing class\nof methods in the multi-view clustering (MVC) field, owing to its excellent\ncomputational speed and clustering performance. One bottleneck faced by\nexisting late fusion methods is that they are usually aligned to the average\nkernel function, which makes the clustering performance highly dependent on the\nquality of datasets. Another problem is that they require subsequent k-means\nclustering after obtaining the consensus partition matrix to get the final\ndiscrete labels, and the resulting separation of the label learning and cluster\nstructure optimization processes limits the integrity of these models. To\naddress the above issues, we propose an integrated framework named One-Step\nLate Fusion Multi-view Clustering with Compressed Subspace (OS-LFMVC-CS).\nSpecifically, we use the consensus subspace to align the partition matrix while\noptimizing the partition fusion, and utilize the fused partition matrix to\nguide the learning of discrete labels. A six-step iterative optimization\napproach with verified convergence is proposed. Sufficient experiments on\nmultiple datasets validate the effectiveness and efficiency of our proposed\nmethod.\n", "link": "http://arxiv.org/abs/2401.01558v2", "date": "2024-04-09", "relevancy": 2.445, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4983}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4881}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One-Step%20Late%20Fusion%20Multi-view%20Clustering%20with%20Compressed%20Subspace&body=Title%3A%20One-Step%20Late%20Fusion%20Multi-view%20Clustering%20with%20Compressed%20Subspace%0AAuthor%3A%20Qiyuan%20Ou%20and%20Pei%20Zhang%20and%20Sihang%20Zhou%20and%20En%20Zhu%0AAbstract%3A%20%20%20Late%20fusion%20multi-view%20clustering%20%28LFMVC%29%20has%20become%20a%20rapidly%20growing%20class%0Aof%20methods%20in%20the%20multi-view%20clustering%20%28MVC%29%20field%2C%20owing%20to%20its%20excellent%0Acomputational%20speed%20and%20clustering%20performance.%20One%20bottleneck%20faced%20by%0Aexisting%20late%20fusion%20methods%20is%20that%20they%20are%20usually%20aligned%20to%20the%20average%0Akernel%20function%2C%20which%20makes%20the%20clustering%20performance%20highly%20dependent%20on%20the%0Aquality%20of%20datasets.%20Another%20problem%20is%20that%20they%20require%20subsequent%20k-means%0Aclustering%20after%20obtaining%20the%20consensus%20partition%20matrix%20to%20get%20the%20final%0Adiscrete%20labels%2C%20and%20the%20resulting%20separation%20of%20the%20label%20learning%20and%20cluster%0Astructure%20optimization%20processes%20limits%20the%20integrity%20of%20these%20models.%20To%0Aaddress%20the%20above%20issues%2C%20we%20propose%20an%20integrated%20framework%20named%20One-Step%0ALate%20Fusion%20Multi-view%20Clustering%20with%20Compressed%20Subspace%20%28OS-LFMVC-CS%29.%0ASpecifically%2C%20we%20use%20the%20consensus%20subspace%20to%20align%20the%20partition%20matrix%20while%0Aoptimizing%20the%20partition%20fusion%2C%20and%20utilize%20the%20fused%20partition%20matrix%20to%0Aguide%20the%20learning%20of%20discrete%20labels.%20A%20six-step%20iterative%20optimization%0Aapproach%20with%20verified%20convergence%20is%20proposed.%20Sufficient%20experiments%20on%0Amultiple%20datasets%20validate%20the%20effectiveness%20and%20efficiency%20of%20our%20proposed%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01558v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Step%20Late%20Fusion%20Multi-view%20Clustering%20with%20Compressed%20Subspace&entry.906535625=Qiyuan%20Ou%20and%20Pei%20Zhang%20and%20Sihang%20Zhou%20and%20En%20Zhu&entry.1292438233=%20%20Late%20fusion%20multi-view%20clustering%20%28LFMVC%29%20has%20become%20a%20rapidly%20growing%20class%0Aof%20methods%20in%20the%20multi-view%20clustering%20%28MVC%29%20field%2C%20owing%20to%20its%20excellent%0Acomputational%20speed%20and%20clustering%20performance.%20One%20bottleneck%20faced%20by%0Aexisting%20late%20fusion%20methods%20is%20that%20they%20are%20usually%20aligned%20to%20the%20average%0Akernel%20function%2C%20which%20makes%20the%20clustering%20performance%20highly%20dependent%20on%20the%0Aquality%20of%20datasets.%20Another%20problem%20is%20that%20they%20require%20subsequent%20k-means%0Aclustering%20after%20obtaining%20the%20consensus%20partition%20matrix%20to%20get%20the%20final%0Adiscrete%20labels%2C%20and%20the%20resulting%20separation%20of%20the%20label%20learning%20and%20cluster%0Astructure%20optimization%20processes%20limits%20the%20integrity%20of%20these%20models.%20To%0Aaddress%20the%20above%20issues%2C%20we%20propose%20an%20integrated%20framework%20named%20One-Step%0ALate%20Fusion%20Multi-view%20Clustering%20with%20Compressed%20Subspace%20%28OS-LFMVC-CS%29.%0ASpecifically%2C%20we%20use%20the%20consensus%20subspace%20to%20align%20the%20partition%20matrix%20while%0Aoptimizing%20the%20partition%20fusion%2C%20and%20utilize%20the%20fused%20partition%20matrix%20to%0Aguide%20the%20learning%20of%20discrete%20labels.%20A%20six-step%20iterative%20optimization%0Aapproach%20with%20verified%20convergence%20is%20proposed.%20Sufficient%20experiments%20on%0Amultiple%20datasets%20validate%20the%20effectiveness%20and%20efficiency%20of%20our%20proposed%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01558v2&entry.124074799=Read"},
{"title": "Dynamic Resolution Guidance for Facial Expression Recognition", "author": "Jie Ou and Xu Li and Tianxiang Jiang and Yuanlun Xie", "abstract": "  Facial expression recognition (FER) is vital for human-computer interaction\nand emotion analysis, yet recognizing expressions in low-resolution images\nremains challenging. This paper introduces a practical method called Dynamic\nResolution Guidance for Facial Expression Recognition (DRGFER) to effectively\nrecognize facial expressions in images with varying resolutions without\ncompromising FER model accuracy. Our framework comprises two main components:\nthe Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation\nFacial Expression Recognition Network (MRAFER). The RRN determines image\nresolution, outputs a binary vector, and the MRAFER assigns images to suitable\nfacial expression recognition networks based on resolution. We evaluated DRGFER\non widely-used datasets RAFDB and FERPlus, demonstrating that our method\nretains optimal model performance at each resolution and outperforms\nalternative resolution approaches. The proposed framework exhibits robustness\nagainst resolution variations and facial expressions, offering a promising\nsolution for real-world applications.\n", "link": "http://arxiv.org/abs/2404.06365v1", "date": "2024-04-09", "relevancy": 2.4357, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5022}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4861}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4731}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Resolution%20Guidance%20for%20Facial%20Expression%20Recognition&body=Title%3A%20Dynamic%20Resolution%20Guidance%20for%20Facial%20Expression%20Recognition%0AAuthor%3A%20Jie%20Ou%20and%20Xu%20Li%20and%20Tianxiang%20Jiang%20and%20Yuanlun%20Xie%0AAbstract%3A%20%20%20Facial%20expression%20recognition%20%28FER%29%20is%20vital%20for%20human-computer%20interaction%0Aand%20emotion%20analysis%2C%20yet%20recognizing%20expressions%20in%20low-resolution%20images%0Aremains%20challenging.%20This%20paper%20introduces%20a%20practical%20method%20called%20Dynamic%0AResolution%20Guidance%20for%20Facial%20Expression%20Recognition%20%28DRGFER%29%20to%20effectively%0Arecognize%20facial%20expressions%20in%20images%20with%20varying%20resolutions%20without%0Acompromising%20FER%20model%20accuracy.%20Our%20framework%20comprises%20two%20main%20components%3A%0Athe%20Resolution%20Recognition%20Network%20%28RRN%29%20and%20the%20Multi-Resolution%20Adaptation%0AFacial%20Expression%20Recognition%20Network%20%28MRAFER%29.%20The%20RRN%20determines%20image%0Aresolution%2C%20outputs%20a%20binary%20vector%2C%20and%20the%20MRAFER%20assigns%20images%20to%20suitable%0Afacial%20expression%20recognition%20networks%20based%20on%20resolution.%20We%20evaluated%20DRGFER%0Aon%20widely-used%20datasets%20RAFDB%20and%20FERPlus%2C%20demonstrating%20that%20our%20method%0Aretains%20optimal%20model%20performance%20at%20each%20resolution%20and%20outperforms%0Aalternative%20resolution%20approaches.%20The%20proposed%20framework%20exhibits%20robustness%0Aagainst%20resolution%20variations%20and%20facial%20expressions%2C%20offering%20a%20promising%0Asolution%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06365v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Resolution%20Guidance%20for%20Facial%20Expression%20Recognition&entry.906535625=Jie%20Ou%20and%20Xu%20Li%20and%20Tianxiang%20Jiang%20and%20Yuanlun%20Xie&entry.1292438233=%20%20Facial%20expression%20recognition%20%28FER%29%20is%20vital%20for%20human-computer%20interaction%0Aand%20emotion%20analysis%2C%20yet%20recognizing%20expressions%20in%20low-resolution%20images%0Aremains%20challenging.%20This%20paper%20introduces%20a%20practical%20method%20called%20Dynamic%0AResolution%20Guidance%20for%20Facial%20Expression%20Recognition%20%28DRGFER%29%20to%20effectively%0Arecognize%20facial%20expressions%20in%20images%20with%20varying%20resolutions%20without%0Acompromising%20FER%20model%20accuracy.%20Our%20framework%20comprises%20two%20main%20components%3A%0Athe%20Resolution%20Recognition%20Network%20%28RRN%29%20and%20the%20Multi-Resolution%20Adaptation%0AFacial%20Expression%20Recognition%20Network%20%28MRAFER%29.%20The%20RRN%20determines%20image%0Aresolution%2C%20outputs%20a%20binary%20vector%2C%20and%20the%20MRAFER%20assigns%20images%20to%20suitable%0Afacial%20expression%20recognition%20networks%20based%20on%20resolution.%20We%20evaluated%20DRGFER%0Aon%20widely-used%20datasets%20RAFDB%20and%20FERPlus%2C%20demonstrating%20that%20our%20method%0Aretains%20optimal%20model%20performance%20at%20each%20resolution%20and%20outperforms%0Aalternative%20resolution%20approaches.%20The%20proposed%20framework%20exhibits%20robustness%0Aagainst%20resolution%20variations%20and%20facial%20expressions%2C%20offering%20a%20promising%0Asolution%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06365v1&entry.124074799=Read"},
{"title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model", "author": "Xinrun Du and Zhouliang Yu and Songyang Gao and Ding Pan and Yuyang Cheng and Ziyang Ma and Ruibin Yuan and Xingwei Qu and Jiaheng Liu and Tianyu Zheng and Xinchen Luo and Guorui Zhou and Binhang Yuan and Wenhu Chen and Jie Fu and Ge Zhang", "abstract": "  In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.\n", "link": "http://arxiv.org/abs/2404.04167v3", "date": "2024-04-09", "relevancy": 2.4341, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.47}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model&body=Title%3A%20Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model%0AAuthor%3A%20Xinrun%20Du%20and%20Zhouliang%20Yu%20and%20Songyang%20Gao%20and%20Ding%20Pan%20and%20Yuyang%20Cheng%20and%20Ziyang%20Ma%20and%20Ruibin%20Yuan%20and%20Xingwei%20Qu%20and%20Jiaheng%20Liu%20and%20Tianyu%20Zheng%20and%20Xinchen%20Luo%20and%20Guorui%20Zhou%20and%20Binhang%20Yuan%20and%20Wenhu%20Chen%20and%20Jie%20Fu%20and%20Ge%20Zhang%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20CT-LLM%2C%20a%202B%20large%20language%20model%20%28LLM%29%20that%0Aillustrates%20a%20pivotal%20shift%20towards%20prioritizing%20the%20Chinese%20language%20in%0Adeveloping%20LLMs.%20Uniquely%20initiated%20from%20scratch%2C%20CT-LLM%20diverges%20from%20the%0Aconventional%20methodology%20by%20primarily%20incorporating%20Chinese%20textual%20data%2C%0Autilizing%20an%20extensive%20corpus%20of%201%2C200%20billion%20tokens%2C%20including%20800%20billion%0AChinese%20tokens%2C%20300%20billion%20English%20tokens%2C%20and%20100%20billion%20code%20tokens.%20This%0Astrategic%20composition%20facilitates%20the%20model%27s%20exceptional%20proficiency%20in%0Aunderstanding%20and%20processing%20Chinese%2C%20a%20capability%20further%20enhanced%20through%0Aalignment%20techniques.%20Demonstrating%20remarkable%20performance%20on%20the%20CHC-Bench%2C%0ACT-LLM%20excels%20in%20Chinese%20language%20tasks%2C%20and%20showcases%20its%20adeptness%20in%20English%0Athrough%20SFT.%20This%20research%20challenges%20the%20prevailing%20paradigm%20of%20training%20LLMs%0Apredominantly%20on%20English%20corpora%20and%20then%20adapting%20them%20to%20other%20languages%2C%0Abroadening%20the%20horizons%20for%20LLM%20training%20methodologies.%20By%20open-sourcing%20the%0Afull%20process%20of%20training%20a%20Chinese%20LLM%2C%20including%20a%20detailed%20data%20processing%0Aprocedure%20with%20the%20obtained%20Massive%20Appropriate%20Pretraining%20Chinese%20Corpus%0A%28MAP-CC%29%2C%20a%20well-chosen%20multidisciplinary%20Chinese%20Hard%20Case%20Benchmark%0A%28CHC-Bench%29%2C%20and%20the%202B-size%20Chinese%20Tiny%20LLM%20%28CT-LLM%29%2C%20we%20aim%20to%20foster%0Afurther%20exploration%20and%20innovation%20in%20both%20academia%20and%20industry%2C%20paving%20the%0Away%20for%20more%20inclusive%20and%20versatile%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04167v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chinese%20Tiny%20LLM%3A%20Pretraining%20a%20Chinese-Centric%20Large%20Language%20Model&entry.906535625=Xinrun%20Du%20and%20Zhouliang%20Yu%20and%20Songyang%20Gao%20and%20Ding%20Pan%20and%20Yuyang%20Cheng%20and%20Ziyang%20Ma%20and%20Ruibin%20Yuan%20and%20Xingwei%20Qu%20and%20Jiaheng%20Liu%20and%20Tianyu%20Zheng%20and%20Xinchen%20Luo%20and%20Guorui%20Zhou%20and%20Binhang%20Yuan%20and%20Wenhu%20Chen%20and%20Jie%20Fu%20and%20Ge%20Zhang&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20CT-LLM%2C%20a%202B%20large%20language%20model%20%28LLM%29%20that%0Aillustrates%20a%20pivotal%20shift%20towards%20prioritizing%20the%20Chinese%20language%20in%0Adeveloping%20LLMs.%20Uniquely%20initiated%20from%20scratch%2C%20CT-LLM%20diverges%20from%20the%0Aconventional%20methodology%20by%20primarily%20incorporating%20Chinese%20textual%20data%2C%0Autilizing%20an%20extensive%20corpus%20of%201%2C200%20billion%20tokens%2C%20including%20800%20billion%0AChinese%20tokens%2C%20300%20billion%20English%20tokens%2C%20and%20100%20billion%20code%20tokens.%20This%0Astrategic%20composition%20facilitates%20the%20model%27s%20exceptional%20proficiency%20in%0Aunderstanding%20and%20processing%20Chinese%2C%20a%20capability%20further%20enhanced%20through%0Aalignment%20techniques.%20Demonstrating%20remarkable%20performance%20on%20the%20CHC-Bench%2C%0ACT-LLM%20excels%20in%20Chinese%20language%20tasks%2C%20and%20showcases%20its%20adeptness%20in%20English%0Athrough%20SFT.%20This%20research%20challenges%20the%20prevailing%20paradigm%20of%20training%20LLMs%0Apredominantly%20on%20English%20corpora%20and%20then%20adapting%20them%20to%20other%20languages%2C%0Abroadening%20the%20horizons%20for%20LLM%20training%20methodologies.%20By%20open-sourcing%20the%0Afull%20process%20of%20training%20a%20Chinese%20LLM%2C%20including%20a%20detailed%20data%20processing%0Aprocedure%20with%20the%20obtained%20Massive%20Appropriate%20Pretraining%20Chinese%20Corpus%0A%28MAP-CC%29%2C%20a%20well-chosen%20multidisciplinary%20Chinese%20Hard%20Case%20Benchmark%0A%28CHC-Bench%29%2C%20and%20the%202B-size%20Chinese%20Tiny%20LLM%20%28CT-LLM%29%2C%20we%20aim%20to%20foster%0Afurther%20exploration%20and%20innovation%20in%20both%20academia%20and%20industry%2C%20paving%20the%0Away%20for%20more%20inclusive%20and%20versatile%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04167v3&entry.124074799=Read"},
{"title": "EPR-Net: Constructing non-equilibrium potential landscape via a\n  variational force projection formulation", "author": "Yue Zhao and Wei Zhang and Tiejun Li", "abstract": "  We present EPR-Net, a novel and effective deep learning approach that tackles\na crucial challenge in biophysics: constructing potential landscapes for\nhigh-dimensional non-equilibrium steady-state (NESS) systems. EPR-Net leverages\na nice mathematical fact that the desired negative potential gradient is simply\nthe orthogonal projection of the driving force of the underlying dynamics in a\nweighted inner-product space. Remarkably, our loss function has an intimate\nconnection with the steady entropy production rate (EPR), enabling simultaneous\nlandscape construction and EPR estimation. We introduce an enhanced learning\nstrategy for systems with small noise, and extend our framework to include\ndimensionality reduction and state-dependent diffusion coefficient case in a\nunified fashion. Comparative evaluations on benchmark problems demonstrate the\nsuperior accuracy, effectiveness, and robustness of EPR-Net compared to\nexisting methods. We apply our approach to challenging biophysical problems,\nsuch as an 8D limit cycle and a 52D multi-stability problem, which provide\naccurate solutions and interesting insights on constructed landscapes. With its\nversatility and power, EPR-Net offers a promising solution for diverse\nlandscape construction problems in biophysics.\n", "link": "http://arxiv.org/abs/2301.01946v3", "date": "2024-04-09", "relevancy": 2.4254, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4833}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4812}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EPR-Net%3A%20Constructing%20non-equilibrium%20potential%20landscape%20via%20a%0A%20%20variational%20force%20projection%20formulation&body=Title%3A%20EPR-Net%3A%20Constructing%20non-equilibrium%20potential%20landscape%20via%20a%0A%20%20variational%20force%20projection%20formulation%0AAuthor%3A%20Yue%20Zhao%20and%20Wei%20Zhang%20and%20Tiejun%20Li%0AAbstract%3A%20%20%20We%20present%20EPR-Net%2C%20a%20novel%20and%20effective%20deep%20learning%20approach%20that%20tackles%0Aa%20crucial%20challenge%20in%20biophysics%3A%20constructing%20potential%20landscapes%20for%0Ahigh-dimensional%20non-equilibrium%20steady-state%20%28NESS%29%20systems.%20EPR-Net%20leverages%0Aa%20nice%20mathematical%20fact%20that%20the%20desired%20negative%20potential%20gradient%20is%20simply%0Athe%20orthogonal%20projection%20of%20the%20driving%20force%20of%20the%20underlying%20dynamics%20in%20a%0Aweighted%20inner-product%20space.%20Remarkably%2C%20our%20loss%20function%20has%20an%20intimate%0Aconnection%20with%20the%20steady%20entropy%20production%20rate%20%28EPR%29%2C%20enabling%20simultaneous%0Alandscape%20construction%20and%20EPR%20estimation.%20We%20introduce%20an%20enhanced%20learning%0Astrategy%20for%20systems%20with%20small%20noise%2C%20and%20extend%20our%20framework%20to%20include%0Adimensionality%20reduction%20and%20state-dependent%20diffusion%20coefficient%20case%20in%20a%0Aunified%20fashion.%20Comparative%20evaluations%20on%20benchmark%20problems%20demonstrate%20the%0Asuperior%20accuracy%2C%20effectiveness%2C%20and%20robustness%20of%20EPR-Net%20compared%20to%0Aexisting%20methods.%20We%20apply%20our%20approach%20to%20challenging%20biophysical%20problems%2C%0Asuch%20as%20an%208D%20limit%20cycle%20and%20a%2052D%20multi-stability%20problem%2C%20which%20provide%0Aaccurate%20solutions%20and%20interesting%20insights%20on%20constructed%20landscapes.%20With%20its%0Aversatility%20and%20power%2C%20EPR-Net%20offers%20a%20promising%20solution%20for%20diverse%0Alandscape%20construction%20problems%20in%20biophysics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.01946v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPR-Net%3A%20Constructing%20non-equilibrium%20potential%20landscape%20via%20a%0A%20%20variational%20force%20projection%20formulation&entry.906535625=Yue%20Zhao%20and%20Wei%20Zhang%20and%20Tiejun%20Li&entry.1292438233=%20%20We%20present%20EPR-Net%2C%20a%20novel%20and%20effective%20deep%20learning%20approach%20that%20tackles%0Aa%20crucial%20challenge%20in%20biophysics%3A%20constructing%20potential%20landscapes%20for%0Ahigh-dimensional%20non-equilibrium%20steady-state%20%28NESS%29%20systems.%20EPR-Net%20leverages%0Aa%20nice%20mathematical%20fact%20that%20the%20desired%20negative%20potential%20gradient%20is%20simply%0Athe%20orthogonal%20projection%20of%20the%20driving%20force%20of%20the%20underlying%20dynamics%20in%20a%0Aweighted%20inner-product%20space.%20Remarkably%2C%20our%20loss%20function%20has%20an%20intimate%0Aconnection%20with%20the%20steady%20entropy%20production%20rate%20%28EPR%29%2C%20enabling%20simultaneous%0Alandscape%20construction%20and%20EPR%20estimation.%20We%20introduce%20an%20enhanced%20learning%0Astrategy%20for%20systems%20with%20small%20noise%2C%20and%20extend%20our%20framework%20to%20include%0Adimensionality%20reduction%20and%20state-dependent%20diffusion%20coefficient%20case%20in%20a%0Aunified%20fashion.%20Comparative%20evaluations%20on%20benchmark%20problems%20demonstrate%20the%0Asuperior%20accuracy%2C%20effectiveness%2C%20and%20robustness%20of%20EPR-Net%20compared%20to%0Aexisting%20methods.%20We%20apply%20our%20approach%20to%20challenging%20biophysical%20problems%2C%0Asuch%20as%20an%208D%20limit%20cycle%20and%20a%2052D%20multi-stability%20problem%2C%20which%20provide%0Aaccurate%20solutions%20and%20interesting%20insights%20on%20constructed%20landscapes.%20With%20its%0Aversatility%20and%20power%2C%20EPR-Net%20offers%20a%20promising%20solution%20for%20diverse%0Alandscape%20construction%20problems%20in%20biophysics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.01946v3&entry.124074799=Read"},
{"title": "Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised\n  Medical Image Segmentation", "author": "Yuanpeng He and Lijian Li", "abstract": "  Although the existing uncertainty-based semi-supervised medical segmentation\nmethods have achieved excellent performance, they usually only consider a\nsingle uncertainty evaluation, which often fails to solve the problem related\nto credibility completely. Therefore, based on the framework of evidential deep\nlearning, this paper integrates the evidential predictive results in the\ncross-region of mixed and original samples to reallocate the confidence degree\nand uncertainty measure of each voxel, which is realized by emphasizing\nuncertain information of probability assignments fusion rule of traditional\nevidence theory. Furthermore, we design a voxel-level asymptotic learning\nstrategy by introducing information entropy to combine with the fused\nuncertainty measure to estimate voxel prediction more precisely. The model will\ngradually pay attention to the prediction results with high uncertainty in the\nlearning process, to learn the features that are difficult to master. The\nexperimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate the\nsuperior performance of our proposed method in comparison with the existing\nstate of the arts.\n", "link": "http://arxiv.org/abs/2404.06177v1", "date": "2024-04-09", "relevancy": 2.4027, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6942}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.597}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5669}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Uncertainty-aware%20Evidential%20Fusion-based%20Learning%20for%20Semi-supervised%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20Uncertainty-aware%20Evidential%20Fusion-based%20Learning%20for%20Semi-supervised%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yuanpeng%20He%20and%20Lijian%20Li%0AAbstract%3A%20%20%20Although%20the%20existing%20uncertainty-based%20semi-supervised%20medical%20segmentation%0Amethods%20have%20achieved%20excellent%20performance%2C%20they%20usually%20only%20consider%20a%0Asingle%20uncertainty%20evaluation%2C%20which%20often%20fails%20to%20solve%20the%20problem%20related%0Ato%20credibility%20completely.%20Therefore%2C%20based%20on%20the%20framework%20of%20evidential%20deep%0Alearning%2C%20this%20paper%20integrates%20the%20evidential%20predictive%20results%20in%20the%0Across-region%20of%20mixed%20and%20original%20samples%20to%20reallocate%20the%20confidence%20degree%0Aand%20uncertainty%20measure%20of%20each%20voxel%2C%20which%20is%20realized%20by%20emphasizing%0Auncertain%20information%20of%20probability%20assignments%20fusion%20rule%20of%20traditional%0Aevidence%20theory.%20Furthermore%2C%20we%20design%20a%20voxel-level%20asymptotic%20learning%0Astrategy%20by%20introducing%20information%20entropy%20to%20combine%20with%20the%20fused%0Auncertainty%20measure%20to%20estimate%20voxel%20prediction%20more%20precisely.%20The%20model%20will%0Agradually%20pay%20attention%20to%20the%20prediction%20results%20with%20high%20uncertainty%20in%20the%0Alearning%20process%2C%20to%20learn%20the%20features%20that%20are%20difficult%20to%20master.%20The%0Aexperimental%20results%20on%20LA%2C%20Pancreas-CT%2C%20ACDC%20and%20TBAD%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20our%20proposed%20method%20in%20comparison%20with%20the%20existing%0Astate%20of%20the%20arts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06177v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-aware%20Evidential%20Fusion-based%20Learning%20for%20Semi-supervised%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Yuanpeng%20He%20and%20Lijian%20Li&entry.1292438233=%20%20Although%20the%20existing%20uncertainty-based%20semi-supervised%20medical%20segmentation%0Amethods%20have%20achieved%20excellent%20performance%2C%20they%20usually%20only%20consider%20a%0Asingle%20uncertainty%20evaluation%2C%20which%20often%20fails%20to%20solve%20the%20problem%20related%0Ato%20credibility%20completely.%20Therefore%2C%20based%20on%20the%20framework%20of%20evidential%20deep%0Alearning%2C%20this%20paper%20integrates%20the%20evidential%20predictive%20results%20in%20the%0Across-region%20of%20mixed%20and%20original%20samples%20to%20reallocate%20the%20confidence%20degree%0Aand%20uncertainty%20measure%20of%20each%20voxel%2C%20which%20is%20realized%20by%20emphasizing%0Auncertain%20information%20of%20probability%20assignments%20fusion%20rule%20of%20traditional%0Aevidence%20theory.%20Furthermore%2C%20we%20design%20a%20voxel-level%20asymptotic%20learning%0Astrategy%20by%20introducing%20information%20entropy%20to%20combine%20with%20the%20fused%0Auncertainty%20measure%20to%20estimate%20voxel%20prediction%20more%20precisely.%20The%20model%20will%0Agradually%20pay%20attention%20to%20the%20prediction%20results%20with%20high%20uncertainty%20in%20the%0Alearning%20process%2C%20to%20learn%20the%20features%20that%20are%20difficult%20to%20master.%20The%0Aexperimental%20results%20on%20LA%2C%20Pancreas-CT%2C%20ACDC%20and%20TBAD%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20our%20proposed%20method%20in%20comparison%20with%20the%20existing%0Astate%20of%20the%20arts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06177v1&entry.124074799=Read"},
{"title": "Leveraging Speculative Sampling and KV-Cache Optimizations Together for\n  Generative AI using OpenVINO", "author": "Haim Barad and Ekaterina Aidova and Yury Gorbachev", "abstract": "  Inference optimizations are critical for improving user experience and\nreducing infrastructure costs and power consumption. In this article, we\nillustrate a form of dynamic execution known as speculative sampling to reduce\nthe overall latency of text generation and compare it with standard\nautoregressive sampling. This can be used together with model-based\noptimizations (e.g. quantization) to provide an optimized solution. Both\nsampling methods make use of KV caching. A Jupyter notebook and some sample\nexecutions are provided.\n", "link": "http://arxiv.org/abs/2311.04951v2", "date": "2024-04-09", "relevancy": 2.4001, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5037}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4751}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4612}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Speculative%20Sampling%20and%20KV-Cache%20Optimizations%20Together%20for%0A%20%20Generative%20AI%20using%20OpenVINO&body=Title%3A%20Leveraging%20Speculative%20Sampling%20and%20KV-Cache%20Optimizations%20Together%20for%0A%20%20Generative%20AI%20using%20OpenVINO%0AAuthor%3A%20Haim%20Barad%20and%20Ekaterina%20Aidova%20and%20Yury%20Gorbachev%0AAbstract%3A%20%20%20Inference%20optimizations%20are%20critical%20for%20improving%20user%20experience%20and%0Areducing%20infrastructure%20costs%20and%20power%20consumption.%20In%20this%20article%2C%20we%0Aillustrate%20a%20form%20of%20dynamic%20execution%20known%20as%20speculative%20sampling%20to%20reduce%0Athe%20overall%20latency%20of%20text%20generation%20and%20compare%20it%20with%20standard%0Aautoregressive%20sampling.%20This%20can%20be%20used%20together%20with%20model-based%0Aoptimizations%20%28e.g.%20quantization%29%20to%20provide%20an%20optimized%20solution.%20Both%0Asampling%20methods%20make%20use%20of%20KV%20caching.%20A%20Jupyter%20notebook%20and%20some%20sample%0Aexecutions%20are%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04951v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Speculative%20Sampling%20and%20KV-Cache%20Optimizations%20Together%20for%0A%20%20Generative%20AI%20using%20OpenVINO&entry.906535625=Haim%20Barad%20and%20Ekaterina%20Aidova%20and%20Yury%20Gorbachev&entry.1292438233=%20%20Inference%20optimizations%20are%20critical%20for%20improving%20user%20experience%20and%0Areducing%20infrastructure%20costs%20and%20power%20consumption.%20In%20this%20article%2C%20we%0Aillustrate%20a%20form%20of%20dynamic%20execution%20known%20as%20speculative%20sampling%20to%20reduce%0Athe%20overall%20latency%20of%20text%20generation%20and%20compare%20it%20with%20standard%0Aautoregressive%20sampling.%20This%20can%20be%20used%20together%20with%20model-based%0Aoptimizations%20%28e.g.%20quantization%29%20to%20provide%20an%20optimized%20solution.%20Both%0Asampling%20methods%20make%20use%20of%20KV%20caching.%20A%20Jupyter%20notebook%20and%20some%20sample%0Aexecutions%20are%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04951v2&entry.124074799=Read"},
{"title": "Online Learning of Decision Trees with Thompson Sampling", "author": "Ayman Chaouki and Jesse Read and Albert Bifet", "abstract": "  Decision Trees are prominent prediction models for interpretable Machine\nLearning. They have been thoroughly researched, mostly in the batch setting\nwith a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3\nand CART. Unfortunately, these methods are of heuristic nature, they rely on\ngreedy splits offering no guarantees of global optimality and often leading to\nunnecessarily complex and hard-to-interpret Decision Trees. Recent\nbreakthroughs addressed this suboptimality issue in the batch setting, but no\nsuch work has considered the online setting with data arriving in a stream. To\nthis end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling\nDecision Trees (TSDT), able to produce optimal Decision Trees in an online\nsetting. We analyse our algorithm and prove its almost sure convergence to the\noptimal tree. Furthermore, we conduct extensive experiments to validate our\nfindings empirically. The proposed TSDT outperforms existing algorithms on\nseveral benchmarks, all while presenting the practical advantage of being\ntailored to the online setting.\n", "link": "http://arxiv.org/abs/2404.06403v1", "date": "2024-04-09", "relevancy": 2.3989, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5093}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4724}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4577}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Online%20Learning%20of%20Decision%20Trees%20with%20Thompson%20Sampling&body=Title%3A%20Online%20Learning%20of%20Decision%20Trees%20with%20Thompson%20Sampling%0AAuthor%3A%20Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet%0AAbstract%3A%20%20%20Decision%20Trees%20are%20prominent%20prediction%20models%20for%20interpretable%20Machine%0ALearning.%20They%20have%20been%20thoroughly%20researched%2C%20mostly%20in%20the%20batch%20setting%0Awith%20a%20fixed%20labelled%20dataset%2C%20leading%20to%20popular%20algorithms%20such%20as%20C4.5%2C%20ID3%0Aand%20CART.%20Unfortunately%2C%20these%20methods%20are%20of%20heuristic%20nature%2C%20they%20rely%20on%0Agreedy%20splits%20offering%20no%20guarantees%20of%20global%20optimality%20and%20often%20leading%20to%0Aunnecessarily%20complex%20and%20hard-to-interpret%20Decision%20Trees.%20Recent%0Abreakthroughs%20addressed%20this%20suboptimality%20issue%20in%20the%20batch%20setting%2C%20but%20no%0Asuch%20work%20has%20considered%20the%20online%20setting%20with%20data%20arriving%20in%20a%20stream.%20To%0Athis%20end%2C%20we%20devise%20a%20new%20Monte%20Carlo%20Tree%20Search%20algorithm%2C%20Thompson%20Sampling%0ADecision%20Trees%20%28TSDT%29%2C%20able%20to%20produce%20optimal%20Decision%20Trees%20in%20an%20online%0Asetting.%20We%20analyse%20our%20algorithm%20and%20prove%20its%20almost%20sure%20convergence%20to%20the%0Aoptimal%20tree.%20Furthermore%2C%20we%20conduct%20extensive%20experiments%20to%20validate%20our%0Afindings%20empirically.%20The%20proposed%20TSDT%20outperforms%20existing%20algorithms%20on%0Aseveral%20benchmarks%2C%20all%20while%20presenting%20the%20practical%20advantage%20of%20being%0Atailored%20to%20the%20online%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06403v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Learning%20of%20Decision%20Trees%20with%20Thompson%20Sampling&entry.906535625=Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet&entry.1292438233=%20%20Decision%20Trees%20are%20prominent%20prediction%20models%20for%20interpretable%20Machine%0ALearning.%20They%20have%20been%20thoroughly%20researched%2C%20mostly%20in%20the%20batch%20setting%0Awith%20a%20fixed%20labelled%20dataset%2C%20leading%20to%20popular%20algorithms%20such%20as%20C4.5%2C%20ID3%0Aand%20CART.%20Unfortunately%2C%20these%20methods%20are%20of%20heuristic%20nature%2C%20they%20rely%20on%0Agreedy%20splits%20offering%20no%20guarantees%20of%20global%20optimality%20and%20often%20leading%20to%0Aunnecessarily%20complex%20and%20hard-to-interpret%20Decision%20Trees.%20Recent%0Abreakthroughs%20addressed%20this%20suboptimality%20issue%20in%20the%20batch%20setting%2C%20but%20no%0Asuch%20work%20has%20considered%20the%20online%20setting%20with%20data%20arriving%20in%20a%20stream.%20To%0Athis%20end%2C%20we%20devise%20a%20new%20Monte%20Carlo%20Tree%20Search%20algorithm%2C%20Thompson%20Sampling%0ADecision%20Trees%20%28TSDT%29%2C%20able%20to%20produce%20optimal%20Decision%20Trees%20in%20an%20online%0Asetting.%20We%20analyse%20our%20algorithm%20and%20prove%20its%20almost%20sure%20convergence%20to%20the%0Aoptimal%20tree.%20Furthermore%2C%20we%20conduct%20extensive%20experiments%20to%20validate%20our%0Afindings%20empirically.%20The%20proposed%20TSDT%20outperforms%20existing%20algorithms%20on%0Aseveral%20benchmarks%2C%20all%20while%20presenting%20the%20practical%20advantage%20of%20being%0Atailored%20to%20the%20online%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06403v1&entry.124074799=Read"},
{"title": "Multi-person 3D pose estimation from unlabelled data", "author": "Daniel Rodriguez-Criado and Pilar Bachiller and George Vogiatzis and Luis J. Manso", "abstract": "  Its numerous applications make multi-human 3D pose estimation a remarkably\nimpactful area of research. Nevertheless, assuming a multiple-view system\ncomposed of several regular RGB cameras, 3D multi-pose estimation presents\nseveral challenges. First of all, each person must be uniquely identified in\nthe different views to separate the 2D information provided by the cameras.\nSecondly, the 3D pose estimation process from the multi-view 2D information of\neach person must be robust against noise and potential occlusions in the\nscenario. In this work, we address these two challenges with the help of deep\nlearning. Specifically, we present a model based on Graph Neural Networks\ncapable of predicting the cross-view correspondence of the people in the\nscenario along with a Multilayer Perceptron that takes the 2D points to yield\nthe 3D poses of each person. These two models are trained in a self-supervised\nmanner, thus avoiding the need for large datasets with 3D annotations.\n", "link": "http://arxiv.org/abs/2212.08731v3", "date": "2024-04-09", "relevancy": 2.3984, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6134}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6082}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5855}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-person%203D%20pose%20estimation%20from%20unlabelled%20data&body=Title%3A%20Multi-person%203D%20pose%20estimation%20from%20unlabelled%20data%0AAuthor%3A%20Daniel%20Rodriguez-Criado%20and%20Pilar%20Bachiller%20and%20George%20Vogiatzis%20and%20Luis%20J.%20Manso%0AAbstract%3A%20%20%20Its%20numerous%20applications%20make%20multi-human%203D%20pose%20estimation%20a%20remarkably%0Aimpactful%20area%20of%20research.%20Nevertheless%2C%20assuming%20a%20multiple-view%20system%0Acomposed%20of%20several%20regular%20RGB%20cameras%2C%203D%20multi-pose%20estimation%20presents%0Aseveral%20challenges.%20First%20of%20all%2C%20each%20person%20must%20be%20uniquely%20identified%20in%0Athe%20different%20views%20to%20separate%20the%202D%20information%20provided%20by%20the%20cameras.%0ASecondly%2C%20the%203D%20pose%20estimation%20process%20from%20the%20multi-view%202D%20information%20of%0Aeach%20person%20must%20be%20robust%20against%20noise%20and%20potential%20occlusions%20in%20the%0Ascenario.%20In%20this%20work%2C%20we%20address%20these%20two%20challenges%20with%20the%20help%20of%20deep%0Alearning.%20Specifically%2C%20we%20present%20a%20model%20based%20on%20Graph%20Neural%20Networks%0Acapable%20of%20predicting%20the%20cross-view%20correspondence%20of%20the%20people%20in%20the%0Ascenario%20along%20with%20a%20Multilayer%20Perceptron%20that%20takes%20the%202D%20points%20to%20yield%0Athe%203D%20poses%20of%20each%20person.%20These%20two%20models%20are%20trained%20in%20a%20self-supervised%0Amanner%2C%20thus%20avoiding%20the%20need%20for%20large%20datasets%20with%203D%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.08731v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-person%203D%20pose%20estimation%20from%20unlabelled%20data&entry.906535625=Daniel%20Rodriguez-Criado%20and%20Pilar%20Bachiller%20and%20George%20Vogiatzis%20and%20Luis%20J.%20Manso&entry.1292438233=%20%20Its%20numerous%20applications%20make%20multi-human%203D%20pose%20estimation%20a%20remarkably%0Aimpactful%20area%20of%20research.%20Nevertheless%2C%20assuming%20a%20multiple-view%20system%0Acomposed%20of%20several%20regular%20RGB%20cameras%2C%203D%20multi-pose%20estimation%20presents%0Aseveral%20challenges.%20First%20of%20all%2C%20each%20person%20must%20be%20uniquely%20identified%20in%0Athe%20different%20views%20to%20separate%20the%202D%20information%20provided%20by%20the%20cameras.%0ASecondly%2C%20the%203D%20pose%20estimation%20process%20from%20the%20multi-view%202D%20information%20of%0Aeach%20person%20must%20be%20robust%20against%20noise%20and%20potential%20occlusions%20in%20the%0Ascenario.%20In%20this%20work%2C%20we%20address%20these%20two%20challenges%20with%20the%20help%20of%20deep%0Alearning.%20Specifically%2C%20we%20present%20a%20model%20based%20on%20Graph%20Neural%20Networks%0Acapable%20of%20predicting%20the%20cross-view%20correspondence%20of%20the%20people%20in%20the%0Ascenario%20along%20with%20a%20Multilayer%20Perceptron%20that%20takes%20the%202D%20points%20to%20yield%0Athe%203D%20poses%20of%20each%20person.%20These%20two%20models%20are%20trained%20in%20a%20self-supervised%0Amanner%2C%20thus%20avoiding%20the%20need%20for%20large%20datasets%20with%203D%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.08731v3&entry.124074799=Read"},
{"title": "Towards Large-Scale Incremental Dense Mapping using Robot-centric\n  Implicit Neural Representation", "author": "Jianheng Liu and Haoyao Chen", "abstract": "  Large-scale dense mapping is vital in robotics, digital twins, and virtual\nreality. Recently, implicit neural mapping has shown remarkable reconstruction\nquality. However, incremental large-scale mapping with implicit neural\nrepresentations remains problematic due to low efficiency, limited video\nmemory, and the catastrophic forgetting phenomenon. To counter these\nchallenges, we introduce the Robot-centric Implicit Mapping (RIM) technique for\nlarge-scale incremental dense mapping. This method employs a hybrid\nrepresentation, encoding shapes with implicit features via a multi-resolution\nvoxel map and decoding signed distance fields through a shallow MLP. We\nadvocate for a robot-centric local map to boost model training efficiency and\ncurb the catastrophic forgetting issue. A decoupled scalable global map is\nfurther developed to archive learned features for reuse and maintain constant\nvideo memory consumption. Validation experiments demonstrate our method's\nexceptional quality, efficiency, and adaptability across diverse scales and\nscenes over advanced dense mapping methods using range sensors. Our system's\ncode will be accessible at https://github.com/HITSZ-NRSL/RIM.git.\n", "link": "http://arxiv.org/abs/2306.10472v3", "date": "2024-04-09", "relevancy": 2.3869, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6239}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5992}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5686}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Large-Scale%20Incremental%20Dense%20Mapping%20using%20Robot-centric%0A%20%20Implicit%20Neural%20Representation&body=Title%3A%20Towards%20Large-Scale%20Incremental%20Dense%20Mapping%20using%20Robot-centric%0A%20%20Implicit%20Neural%20Representation%0AAuthor%3A%20Jianheng%20Liu%20and%20Haoyao%20Chen%0AAbstract%3A%20%20%20Large-scale%20dense%20mapping%20is%20vital%20in%20robotics%2C%20digital%20twins%2C%20and%20virtual%0Areality.%20Recently%2C%20implicit%20neural%20mapping%20has%20shown%20remarkable%20reconstruction%0Aquality.%20However%2C%20incremental%20large-scale%20mapping%20with%20implicit%20neural%0Arepresentations%20remains%20problematic%20due%20to%20low%20efficiency%2C%20limited%20video%0Amemory%2C%20and%20the%20catastrophic%20forgetting%20phenomenon.%20To%20counter%20these%0Achallenges%2C%20we%20introduce%20the%20Robot-centric%20Implicit%20Mapping%20%28RIM%29%20technique%20for%0Alarge-scale%20incremental%20dense%20mapping.%20This%20method%20employs%20a%20hybrid%0Arepresentation%2C%20encoding%20shapes%20with%20implicit%20features%20via%20a%20multi-resolution%0Avoxel%20map%20and%20decoding%20signed%20distance%20fields%20through%20a%20shallow%20MLP.%20We%0Aadvocate%20for%20a%20robot-centric%20local%20map%20to%20boost%20model%20training%20efficiency%20and%0Acurb%20the%20catastrophic%20forgetting%20issue.%20A%20decoupled%20scalable%20global%20map%20is%0Afurther%20developed%20to%20archive%20learned%20features%20for%20reuse%20and%20maintain%20constant%0Avideo%20memory%20consumption.%20Validation%20experiments%20demonstrate%20our%20method%27s%0Aexceptional%20quality%2C%20efficiency%2C%20and%20adaptability%20across%20diverse%20scales%20and%0Ascenes%20over%20advanced%20dense%20mapping%20methods%20using%20range%20sensors.%20Our%20system%27s%0Acode%20will%20be%20accessible%20at%20https%3A//github.com/HITSZ-NRSL/RIM.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10472v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Large-Scale%20Incremental%20Dense%20Mapping%20using%20Robot-centric%0A%20%20Implicit%20Neural%20Representation&entry.906535625=Jianheng%20Liu%20and%20Haoyao%20Chen&entry.1292438233=%20%20Large-scale%20dense%20mapping%20is%20vital%20in%20robotics%2C%20digital%20twins%2C%20and%20virtual%0Areality.%20Recently%2C%20implicit%20neural%20mapping%20has%20shown%20remarkable%20reconstruction%0Aquality.%20However%2C%20incremental%20large-scale%20mapping%20with%20implicit%20neural%0Arepresentations%20remains%20problematic%20due%20to%20low%20efficiency%2C%20limited%20video%0Amemory%2C%20and%20the%20catastrophic%20forgetting%20phenomenon.%20To%20counter%20these%0Achallenges%2C%20we%20introduce%20the%20Robot-centric%20Implicit%20Mapping%20%28RIM%29%20technique%20for%0Alarge-scale%20incremental%20dense%20mapping.%20This%20method%20employs%20a%20hybrid%0Arepresentation%2C%20encoding%20shapes%20with%20implicit%20features%20via%20a%20multi-resolution%0Avoxel%20map%20and%20decoding%20signed%20distance%20fields%20through%20a%20shallow%20MLP.%20We%0Aadvocate%20for%20a%20robot-centric%20local%20map%20to%20boost%20model%20training%20efficiency%20and%0Acurb%20the%20catastrophic%20forgetting%20issue.%20A%20decoupled%20scalable%20global%20map%20is%0Afurther%20developed%20to%20archive%20learned%20features%20for%20reuse%20and%20maintain%20constant%0Avideo%20memory%20consumption.%20Validation%20experiments%20demonstrate%20our%20method%27s%0Aexceptional%20quality%2C%20efficiency%2C%20and%20adaptability%20across%20diverse%20scales%20and%0Ascenes%20over%20advanced%20dense%20mapping%20methods%20using%20range%20sensors.%20Our%20system%27s%0Acode%20will%20be%20accessible%20at%20https%3A//github.com/HITSZ-NRSL/RIM.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10472v3&entry.124074799=Read"},
{"title": "Aggressive or Imperceptible, or Both: Network Pruning Assisted Hybrid\n  Byzantines in Federated Learning", "author": "Emre Ozfatura and Kerem Ozfatura and Alptekin Kupcu and Deniz Gunduz", "abstract": "  Federated learning (FL) has been introduced to enable a large number of\nclients, possibly mobile devices, to collaborate on generating a generalized\nmachine learning model thanks to utilizing a larger number of local samples\nwithout sharing to offer certain privacy to collaborating clients. However, due\nto the participation of a large number of clients, it is often difficult to\nprofile and verify each client, which leads to a security threat that malicious\nparticipants may hamper the accuracy of the trained model by conveying poisoned\nmodels during the training. Hence, the aggregation framework at the parameter\nserver also needs to minimize the detrimental effects of these malicious\nclients. A plethora of attack and defence strategies have been analyzed in the\nliterature. However, often the Byzantine problem is analyzed solely from the\noutlier detection perspective, being oblivious to the topology of neural\nnetworks (NNs).\n  In the scope of this work, we argue that by extracting certain side\ninformation specific to the NN topology, one can design stronger attacks.\nHence, inspired by the sparse neural networks, we introduce a hybrid sparse\nByzantine attack that is composed of two parts: one exhibiting a sparse nature\nand attacking only certain NN locations with higher sensitivity, and the other\nbeing more silent but accumulating over time, where each ideally targets a\ndifferent type of defence mechanism, and together they form a strong but\nimperceptible attack. Finally, we show through extensive simulations that the\nproposed hybrid Byzantine attack is effective against 8 different defence\nmethods.\n", "link": "http://arxiv.org/abs/2404.06230v1", "date": "2024-04-09", "relevancy": 2.3778, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5053}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4715}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4499}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Aggressive%20or%20Imperceptible%2C%20or%20Both%3A%20Network%20Pruning%20Assisted%20Hybrid%0A%20%20Byzantines%20in%20Federated%20Learning&body=Title%3A%20Aggressive%20or%20Imperceptible%2C%20or%20Both%3A%20Network%20Pruning%20Assisted%20Hybrid%0A%20%20Byzantines%20in%20Federated%20Learning%0AAuthor%3A%20Emre%20Ozfatura%20and%20Kerem%20Ozfatura%20and%20Alptekin%20Kupcu%20and%20Deniz%20Gunduz%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20been%20introduced%20to%20enable%20a%20large%20number%20of%0Aclients%2C%20possibly%20mobile%20devices%2C%20to%20collaborate%20on%20generating%20a%20generalized%0Amachine%20learning%20model%20thanks%20to%20utilizing%20a%20larger%20number%20of%20local%20samples%0Awithout%20sharing%20to%20offer%20certain%20privacy%20to%20collaborating%20clients.%20However%2C%20due%0Ato%20the%20participation%20of%20a%20large%20number%20of%20clients%2C%20it%20is%20often%20difficult%20to%0Aprofile%20and%20verify%20each%20client%2C%20which%20leads%20to%20a%20security%20threat%20that%20malicious%0Aparticipants%20may%20hamper%20the%20accuracy%20of%20the%20trained%20model%20by%20conveying%20poisoned%0Amodels%20during%20the%20training.%20Hence%2C%20the%20aggregation%20framework%20at%20the%20parameter%0Aserver%20also%20needs%20to%20minimize%20the%20detrimental%20effects%20of%20these%20malicious%0Aclients.%20A%20plethora%20of%20attack%20and%20defence%20strategies%20have%20been%20analyzed%20in%20the%0Aliterature.%20However%2C%20often%20the%20Byzantine%20problem%20is%20analyzed%20solely%20from%20the%0Aoutlier%20detection%20perspective%2C%20being%20oblivious%20to%20the%20topology%20of%20neural%0Anetworks%20%28NNs%29.%0A%20%20In%20the%20scope%20of%20this%20work%2C%20we%20argue%20that%20by%20extracting%20certain%20side%0Ainformation%20specific%20to%20the%20NN%20topology%2C%20one%20can%20design%20stronger%20attacks.%0AHence%2C%20inspired%20by%20the%20sparse%20neural%20networks%2C%20we%20introduce%20a%20hybrid%20sparse%0AByzantine%20attack%20that%20is%20composed%20of%20two%20parts%3A%20one%20exhibiting%20a%20sparse%20nature%0Aand%20attacking%20only%20certain%20NN%20locations%20with%20higher%20sensitivity%2C%20and%20the%20other%0Abeing%20more%20silent%20but%20accumulating%20over%20time%2C%20where%20each%20ideally%20targets%20a%0Adifferent%20type%20of%20defence%20mechanism%2C%20and%20together%20they%20form%20a%20strong%20but%0Aimperceptible%20attack.%20Finally%2C%20we%20show%20through%20extensive%20simulations%20that%20the%0Aproposed%20hybrid%20Byzantine%20attack%20is%20effective%20against%208%20different%20defence%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06230v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aggressive%20or%20Imperceptible%2C%20or%20Both%3A%20Network%20Pruning%20Assisted%20Hybrid%0A%20%20Byzantines%20in%20Federated%20Learning&entry.906535625=Emre%20Ozfatura%20and%20Kerem%20Ozfatura%20and%20Alptekin%20Kupcu%20and%20Deniz%20Gunduz&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20been%20introduced%20to%20enable%20a%20large%20number%20of%0Aclients%2C%20possibly%20mobile%20devices%2C%20to%20collaborate%20on%20generating%20a%20generalized%0Amachine%20learning%20model%20thanks%20to%20utilizing%20a%20larger%20number%20of%20local%20samples%0Awithout%20sharing%20to%20offer%20certain%20privacy%20to%20collaborating%20clients.%20However%2C%20due%0Ato%20the%20participation%20of%20a%20large%20number%20of%20clients%2C%20it%20is%20often%20difficult%20to%0Aprofile%20and%20verify%20each%20client%2C%20which%20leads%20to%20a%20security%20threat%20that%20malicious%0Aparticipants%20may%20hamper%20the%20accuracy%20of%20the%20trained%20model%20by%20conveying%20poisoned%0Amodels%20during%20the%20training.%20Hence%2C%20the%20aggregation%20framework%20at%20the%20parameter%0Aserver%20also%20needs%20to%20minimize%20the%20detrimental%20effects%20of%20these%20malicious%0Aclients.%20A%20plethora%20of%20attack%20and%20defence%20strategies%20have%20been%20analyzed%20in%20the%0Aliterature.%20However%2C%20often%20the%20Byzantine%20problem%20is%20analyzed%20solely%20from%20the%0Aoutlier%20detection%20perspective%2C%20being%20oblivious%20to%20the%20topology%20of%20neural%0Anetworks%20%28NNs%29.%0A%20%20In%20the%20scope%20of%20this%20work%2C%20we%20argue%20that%20by%20extracting%20certain%20side%0Ainformation%20specific%20to%20the%20NN%20topology%2C%20one%20can%20design%20stronger%20attacks.%0AHence%2C%20inspired%20by%20the%20sparse%20neural%20networks%2C%20we%20introduce%20a%20hybrid%20sparse%0AByzantine%20attack%20that%20is%20composed%20of%20two%20parts%3A%20one%20exhibiting%20a%20sparse%20nature%0Aand%20attacking%20only%20certain%20NN%20locations%20with%20higher%20sensitivity%2C%20and%20the%20other%0Abeing%20more%20silent%20but%20accumulating%20over%20time%2C%20where%20each%20ideally%20targets%20a%0Adifferent%20type%20of%20defence%20mechanism%2C%20and%20together%20they%20form%20a%20strong%20but%0Aimperceptible%20attack.%20Finally%2C%20we%20show%20through%20extensive%20simulations%20that%20the%0Aproposed%20hybrid%20Byzantine%20attack%20is%20effective%20against%208%20different%20defence%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06230v1&entry.124074799=Read"},
{"title": "Co-Occ: Coupling Explicit Feature Fusion with Volume Rendering\n  Regularization for Multi-Modal 3D Semantic Occupancy Prediction", "author": "Jingyi Pan and Zipeng Wang and Lin Wang", "abstract": "  3D semantic occupancy prediction is a pivotal task in the field of autonomous\ndriving. Recent approaches have made great advances in 3D semantic occupancy\npredictions on a single modality. However, multi-modal semantic occupancy\nprediction approaches have encountered difficulties in dealing with the\nmodality heterogeneity, modality misalignment, and insufficient modality\ninteractions that arise during the fusion of different modalities data, which\nmay result in the loss of important geometric and semantic information. This\nletter presents a novel multi-modal, i.e., LiDAR-camera 3D semantic occupancy\nprediction framework, dubbed Co-Occ, which couples explicit LiDAR-camera\nfeature fusion with implicit volume rendering regularization. The key insight\nis that volume rendering in the feature space can proficiently bridge the gap\nbetween 3D LiDAR sweeps and 2D images while serving as a physical\nregularization to enhance LiDAR-camera fused volumetric representation.\nSpecifically, we first propose a Geometric- and Semantic-aware Fusion\n(GSFusion) module to explicitly enhance LiDAR features by incorporating\nneighboring camera features through a K-nearest neighbors (KNN) search. Then,\nwe employ volume rendering to project the fused feature back to the image\nplanes for reconstructing color and depth maps. These maps are then supervised\nby input images from the camera and depth estimations derived from LiDAR,\nrespectively. Extensive experiments on the popular nuScenes and SemanticKITTI\nbenchmarks verify the effectiveness of our Co-Occ for 3D semantic occupancy\nprediction. The project page is available at\nhttps://rorisis.github.io/Co-Occ_project-page/.\n", "link": "http://arxiv.org/abs/2404.04561v2", "date": "2024-04-09", "relevancy": 2.3766, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.644}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5904}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5779}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Co-Occ%3A%20Coupling%20Explicit%20Feature%20Fusion%20with%20Volume%20Rendering%0A%20%20Regularization%20for%20Multi-Modal%203D%20Semantic%20Occupancy%20Prediction&body=Title%3A%20Co-Occ%3A%20Coupling%20Explicit%20Feature%20Fusion%20with%20Volume%20Rendering%0A%20%20Regularization%20for%20Multi-Modal%203D%20Semantic%20Occupancy%20Prediction%0AAuthor%3A%20Jingyi%20Pan%20and%20Zipeng%20Wang%20and%20Lin%20Wang%0AAbstract%3A%20%20%203D%20semantic%20occupancy%20prediction%20is%20a%20pivotal%20task%20in%20the%20field%20of%20autonomous%0Adriving.%20Recent%20approaches%20have%20made%20great%20advances%20in%203D%20semantic%20occupancy%0Apredictions%20on%20a%20single%20modality.%20However%2C%20multi-modal%20semantic%20occupancy%0Aprediction%20approaches%20have%20encountered%20difficulties%20in%20dealing%20with%20the%0Amodality%20heterogeneity%2C%20modality%20misalignment%2C%20and%20insufficient%20modality%0Ainteractions%20that%20arise%20during%20the%20fusion%20of%20different%20modalities%20data%2C%20which%0Amay%20result%20in%20the%20loss%20of%20important%20geometric%20and%20semantic%20information.%20This%0Aletter%20presents%20a%20novel%20multi-modal%2C%20i.e.%2C%20LiDAR-camera%203D%20semantic%20occupancy%0Aprediction%20framework%2C%20dubbed%20Co-Occ%2C%20which%20couples%20explicit%20LiDAR-camera%0Afeature%20fusion%20with%20implicit%20volume%20rendering%20regularization.%20The%20key%20insight%0Ais%20that%20volume%20rendering%20in%20the%20feature%20space%20can%20proficiently%20bridge%20the%20gap%0Abetween%203D%20LiDAR%20sweeps%20and%202D%20images%20while%20serving%20as%20a%20physical%0Aregularization%20to%20enhance%20LiDAR-camera%20fused%20volumetric%20representation.%0ASpecifically%2C%20we%20first%20propose%20a%20Geometric-%20and%20Semantic-aware%20Fusion%0A%28GSFusion%29%20module%20to%20explicitly%20enhance%20LiDAR%20features%20by%20incorporating%0Aneighboring%20camera%20features%20through%20a%20K-nearest%20neighbors%20%28KNN%29%20search.%20Then%2C%0Awe%20employ%20volume%20rendering%20to%20project%20the%20fused%20feature%20back%20to%20the%20image%0Aplanes%20for%20reconstructing%20color%20and%20depth%20maps.%20These%20maps%20are%20then%20supervised%0Aby%20input%20images%20from%20the%20camera%20and%20depth%20estimations%20derived%20from%20LiDAR%2C%0Arespectively.%20Extensive%20experiments%20on%20the%20popular%20nuScenes%20and%20SemanticKITTI%0Abenchmarks%20verify%20the%20effectiveness%20of%20our%20Co-Occ%20for%203D%20semantic%20occupancy%0Aprediction.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//rorisis.github.io/Co-Occ_project-page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04561v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Occ%3A%20Coupling%20Explicit%20Feature%20Fusion%20with%20Volume%20Rendering%0A%20%20Regularization%20for%20Multi-Modal%203D%20Semantic%20Occupancy%20Prediction&entry.906535625=Jingyi%20Pan%20and%20Zipeng%20Wang%20and%20Lin%20Wang&entry.1292438233=%20%203D%20semantic%20occupancy%20prediction%20is%20a%20pivotal%20task%20in%20the%20field%20of%20autonomous%0Adriving.%20Recent%20approaches%20have%20made%20great%20advances%20in%203D%20semantic%20occupancy%0Apredictions%20on%20a%20single%20modality.%20However%2C%20multi-modal%20semantic%20occupancy%0Aprediction%20approaches%20have%20encountered%20difficulties%20in%20dealing%20with%20the%0Amodality%20heterogeneity%2C%20modality%20misalignment%2C%20and%20insufficient%20modality%0Ainteractions%20that%20arise%20during%20the%20fusion%20of%20different%20modalities%20data%2C%20which%0Amay%20result%20in%20the%20loss%20of%20important%20geometric%20and%20semantic%20information.%20This%0Aletter%20presents%20a%20novel%20multi-modal%2C%20i.e.%2C%20LiDAR-camera%203D%20semantic%20occupancy%0Aprediction%20framework%2C%20dubbed%20Co-Occ%2C%20which%20couples%20explicit%20LiDAR-camera%0Afeature%20fusion%20with%20implicit%20volume%20rendering%20regularization.%20The%20key%20insight%0Ais%20that%20volume%20rendering%20in%20the%20feature%20space%20can%20proficiently%20bridge%20the%20gap%0Abetween%203D%20LiDAR%20sweeps%20and%202D%20images%20while%20serving%20as%20a%20physical%0Aregularization%20to%20enhance%20LiDAR-camera%20fused%20volumetric%20representation.%0ASpecifically%2C%20we%20first%20propose%20a%20Geometric-%20and%20Semantic-aware%20Fusion%0A%28GSFusion%29%20module%20to%20explicitly%20enhance%20LiDAR%20features%20by%20incorporating%0Aneighboring%20camera%20features%20through%20a%20K-nearest%20neighbors%20%28KNN%29%20search.%20Then%2C%0Awe%20employ%20volume%20rendering%20to%20project%20the%20fused%20feature%20back%20to%20the%20image%0Aplanes%20for%20reconstructing%20color%20and%20depth%20maps.%20These%20maps%20are%20then%20supervised%0Aby%20input%20images%20from%20the%20camera%20and%20depth%20estimations%20derived%20from%20LiDAR%2C%0Arespectively.%20Extensive%20experiments%20on%20the%20popular%20nuScenes%20and%20SemanticKITTI%0Abenchmarks%20verify%20the%20effectiveness%20of%20our%20Co-Occ%20for%203D%20semantic%20occupancy%0Aprediction.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//rorisis.github.io/Co-Occ_project-page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04561v2&entry.124074799=Read"},
{"title": "Spatial-Temporal Multi-level Association for Video Object Segmentation", "author": "Deshui Miao and Xin Li and Zhenyu He and Huchuan Lu and Ming-Hsuan Yang", "abstract": "  Existing semi-supervised video object segmentation methods either focus on\ntemporal feature matching or spatial-temporal feature modeling. However, they\ndo not address the issues of sufficient target interaction and efficient\nparallel processing simultaneously, thereby constraining the learning of\ndynamic, target-aware features. To tackle these limitations, this paper\nproposes a spatial-temporal multi-level association framework, which jointly\nassociates reference frame, test frame, and object features to achieve\nsufficient interaction and parallel target ID association with a\nspatial-temporal memory bank for efficient video object segmentation.\nSpecifically, we construct a spatial-temporal multi-level feature association\nmodule to learn better target-aware features, which formulates feature\nextraction and interaction as the efficient operations of object\nself-attention, reference object enhancement, and test reference correlation.\nIn addition, we propose a spatial-temporal memory to assist feature association\nand temporal ID assignment and correlation. We evaluate the proposed method by\nconducting extensive experiments on numerous video object segmentation\ndatasets, including DAVIS 2016/2017 val, DAVIS 2017 test-dev, and YouTube-VOS\n2018/2019 val. The favorable performance against the state-of-the-art methods\ndemonstrates the effectiveness of our approach. All source code and trained\nmodels will be made publicly available.\n", "link": "http://arxiv.org/abs/2404.06265v1", "date": "2024-04-09", "relevancy": 2.361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5974}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5902}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5724}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spatial-Temporal%20Multi-level%20Association%20for%20Video%20Object%20Segmentation&body=Title%3A%20Spatial-Temporal%20Multi-level%20Association%20for%20Video%20Object%20Segmentation%0AAuthor%3A%20Deshui%20Miao%20and%20Xin%20Li%20and%20Zhenyu%20He%20and%20Huchuan%20Lu%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Existing%20semi-supervised%20video%20object%20segmentation%20methods%20either%20focus%20on%0Atemporal%20feature%20matching%20or%20spatial-temporal%20feature%20modeling.%20However%2C%20they%0Ado%20not%20address%20the%20issues%20of%20sufficient%20target%20interaction%20and%20efficient%0Aparallel%20processing%20simultaneously%2C%20thereby%20constraining%20the%20learning%20of%0Adynamic%2C%20target-aware%20features.%20To%20tackle%20these%20limitations%2C%20this%20paper%0Aproposes%20a%20spatial-temporal%20multi-level%20association%20framework%2C%20which%20jointly%0Aassociates%20reference%20frame%2C%20test%20frame%2C%20and%20object%20features%20to%20achieve%0Asufficient%20interaction%20and%20parallel%20target%20ID%20association%20with%20a%0Aspatial-temporal%20memory%20bank%20for%20efficient%20video%20object%20segmentation.%0ASpecifically%2C%20we%20construct%20a%20spatial-temporal%20multi-level%20feature%20association%0Amodule%20to%20learn%20better%20target-aware%20features%2C%20which%20formulates%20feature%0Aextraction%20and%20interaction%20as%20the%20efficient%20operations%20of%20object%0Aself-attention%2C%20reference%20object%20enhancement%2C%20and%20test%20reference%20correlation.%0AIn%20addition%2C%20we%20propose%20a%20spatial-temporal%20memory%20to%20assist%20feature%20association%0Aand%20temporal%20ID%20assignment%20and%20correlation.%20We%20evaluate%20the%20proposed%20method%20by%0Aconducting%20extensive%20experiments%20on%20numerous%20video%20object%20segmentation%0Adatasets%2C%20including%20DAVIS%202016/2017%20val%2C%20DAVIS%202017%20test-dev%2C%20and%20YouTube-VOS%0A2018/2019%20val.%20The%20favorable%20performance%20against%20the%20state-of-the-art%20methods%0Ademonstrates%20the%20effectiveness%20of%20our%20approach.%20All%20source%20code%20and%20trained%0Amodels%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06265v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Temporal%20Multi-level%20Association%20for%20Video%20Object%20Segmentation&entry.906535625=Deshui%20Miao%20and%20Xin%20Li%20and%20Zhenyu%20He%20and%20Huchuan%20Lu%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Existing%20semi-supervised%20video%20object%20segmentation%20methods%20either%20focus%20on%0Atemporal%20feature%20matching%20or%20spatial-temporal%20feature%20modeling.%20However%2C%20they%0Ado%20not%20address%20the%20issues%20of%20sufficient%20target%20interaction%20and%20efficient%0Aparallel%20processing%20simultaneously%2C%20thereby%20constraining%20the%20learning%20of%0Adynamic%2C%20target-aware%20features.%20To%20tackle%20these%20limitations%2C%20this%20paper%0Aproposes%20a%20spatial-temporal%20multi-level%20association%20framework%2C%20which%20jointly%0Aassociates%20reference%20frame%2C%20test%20frame%2C%20and%20object%20features%20to%20achieve%0Asufficient%20interaction%20and%20parallel%20target%20ID%20association%20with%20a%0Aspatial-temporal%20memory%20bank%20for%20efficient%20video%20object%20segmentation.%0ASpecifically%2C%20we%20construct%20a%20spatial-temporal%20multi-level%20feature%20association%0Amodule%20to%20learn%20better%20target-aware%20features%2C%20which%20formulates%20feature%0Aextraction%20and%20interaction%20as%20the%20efficient%20operations%20of%20object%0Aself-attention%2C%20reference%20object%20enhancement%2C%20and%20test%20reference%20correlation.%0AIn%20addition%2C%20we%20propose%20a%20spatial-temporal%20memory%20to%20assist%20feature%20association%0Aand%20temporal%20ID%20assignment%20and%20correlation.%20We%20evaluate%20the%20proposed%20method%20by%0Aconducting%20extensive%20experiments%20on%20numerous%20video%20object%20segmentation%0Adatasets%2C%20including%20DAVIS%202016/2017%20val%2C%20DAVIS%202017%20test-dev%2C%20and%20YouTube-VOS%0A2018/2019%20val.%20The%20favorable%20performance%20against%20the%20state-of-the-art%20methods%0Ademonstrates%20the%20effectiveness%20of%20our%20approach.%20All%20source%20code%20and%20trained%0Amodels%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06265v1&entry.124074799=Read"},
{"title": "Leveraging edge detection and neural networks for better UAV\n  localization", "author": "Theo Di Piazza and Enric Meinhardt-Llopis and Gabriele Facciolo and Benedicte Bascle and Corentin Abgrall and Jean-Clement Devaux", "abstract": "  We propose a novel method for geolocalizing Unmanned Aerial Vehicles (UAVs)\nin environments lacking Global Navigation Satellite Systems (GNSS). Current\nstate-of-the-art techniques employ an offline-trained encoder to generate a\nvector representation (embedding) of the UAV's current view, which is then\ncompared with pre-computed embeddings of geo-referenced images to determine the\nUAV's position. Here, we demonstrate that the performance of these methods can\nbe significantly enhanced by preprocessing the images to extract their edges,\nwhich exhibit robustness to seasonal and illumination variations. Furthermore,\nwe establish that utilizing edges enhances resilience to orientation and\naltitude inaccuracies. Additionally, we introduce a confidence criterion for\nlocalization. Our findings are substantiated through synthetic experiments.\n", "link": "http://arxiv.org/abs/2404.06207v1", "date": "2024-04-09", "relevancy": 2.3448, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6313}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5692}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5157}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20edge%20detection%20and%20neural%20networks%20for%20better%20UAV%0A%20%20localization&body=Title%3A%20Leveraging%20edge%20detection%20and%20neural%20networks%20for%20better%20UAV%0A%20%20localization%0AAuthor%3A%20Theo%20Di%20Piazza%20and%20Enric%20Meinhardt-Llopis%20and%20Gabriele%20Facciolo%20and%20Benedicte%20Bascle%20and%20Corentin%20Abgrall%20and%20Jean-Clement%20Devaux%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%20for%20geolocalizing%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%0Ain%20environments%20lacking%20Global%20Navigation%20Satellite%20Systems%20%28GNSS%29.%20Current%0Astate-of-the-art%20techniques%20employ%20an%20offline-trained%20encoder%20to%20generate%20a%0Avector%20representation%20%28embedding%29%20of%20the%20UAV%27s%20current%20view%2C%20which%20is%20then%0Acompared%20with%20pre-computed%20embeddings%20of%20geo-referenced%20images%20to%20determine%20the%0AUAV%27s%20position.%20Here%2C%20we%20demonstrate%20that%20the%20performance%20of%20these%20methods%20can%0Abe%20significantly%20enhanced%20by%20preprocessing%20the%20images%20to%20extract%20their%20edges%2C%0Awhich%20exhibit%20robustness%20to%20seasonal%20and%20illumination%20variations.%20Furthermore%2C%0Awe%20establish%20that%20utilizing%20edges%20enhances%20resilience%20to%20orientation%20and%0Aaltitude%20inaccuracies.%20Additionally%2C%20we%20introduce%20a%20confidence%20criterion%20for%0Alocalization.%20Our%20findings%20are%20substantiated%20through%20synthetic%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06207v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20edge%20detection%20and%20neural%20networks%20for%20better%20UAV%0A%20%20localization&entry.906535625=Theo%20Di%20Piazza%20and%20Enric%20Meinhardt-Llopis%20and%20Gabriele%20Facciolo%20and%20Benedicte%20Bascle%20and%20Corentin%20Abgrall%20and%20Jean-Clement%20Devaux&entry.1292438233=%20%20We%20propose%20a%20novel%20method%20for%20geolocalizing%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%0Ain%20environments%20lacking%20Global%20Navigation%20Satellite%20Systems%20%28GNSS%29.%20Current%0Astate-of-the-art%20techniques%20employ%20an%20offline-trained%20encoder%20to%20generate%20a%0Avector%20representation%20%28embedding%29%20of%20the%20UAV%27s%20current%20view%2C%20which%20is%20then%0Acompared%20with%20pre-computed%20embeddings%20of%20geo-referenced%20images%20to%20determine%20the%0AUAV%27s%20position.%20Here%2C%20we%20demonstrate%20that%20the%20performance%20of%20these%20methods%20can%0Abe%20significantly%20enhanced%20by%20preprocessing%20the%20images%20to%20extract%20their%20edges%2C%0Awhich%20exhibit%20robustness%20to%20seasonal%20and%20illumination%20variations.%20Furthermore%2C%0Awe%20establish%20that%20utilizing%20edges%20enhances%20resilience%20to%20orientation%20and%0Aaltitude%20inaccuracies.%20Additionally%2C%20we%20introduce%20a%20confidence%20criterion%20for%0Alocalization.%20Our%20findings%20are%20substantiated%20through%20synthetic%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06207v1&entry.124074799=Read"},
{"title": "Label-Efficient 3D Object Detection For Road-Side Units", "author": "Minh-Quan Dao and Holger Caesar and Julie Stephany Berrio and Mao Shan and Stewart Worrall and Vincent Fr\u00e9mont and Ezio Malis", "abstract": "  Occlusion presents a significant challenge for safety-critical applications\nsuch as autonomous driving. Collaborative perception has recently attracted a\nlarge research interest thanks to the ability to enhance the perception of\nautonomous vehicles via deep information fusion with intelligent roadside units\n(RSU), thus minimizing the impact of occlusion. While significant advancement\nhas been made, the data-hungry nature of these methods creates a major hurdle\nfor their real-world deployment, particularly due to the need for annotated RSU\ndata. Manually annotating the vast amount of RSU data required for training is\nprohibitively expensive, given the sheer number of intersections and the effort\ninvolved in annotating point clouds. We address this challenge by devising a\nlabel-efficient object detection method for RSU based on unsupervised object\ndiscovery. Our paper introduces two new modules: one for object discovery based\non a spatial-temporal aggregation of point clouds, and another for refinement.\nFurthermore, we demonstrate that fine-tuning on a small portion of annotated\ndata allows our object discovery models to narrow the performance gap with, or\neven surpass, fully supervised models. Extensive experiments are carried out in\nsimulated and real-world datasets to evaluate our method.\n", "link": "http://arxiv.org/abs/2404.06256v1", "date": "2024-04-09", "relevancy": 2.3348, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6355}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5864}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5602}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Label-Efficient%203D%20Object%20Detection%20For%20Road-Side%20Units&body=Title%3A%20Label-Efficient%203D%20Object%20Detection%20For%20Road-Side%20Units%0AAuthor%3A%20Minh-Quan%20Dao%20and%20Holger%20Caesar%20and%20Julie%20Stephany%20Berrio%20and%20Mao%20Shan%20and%20Stewart%20Worrall%20and%20Vincent%20Fr%C3%A9mont%20and%20Ezio%20Malis%0AAbstract%3A%20%20%20Occlusion%20presents%20a%20significant%20challenge%20for%20safety-critical%20applications%0Asuch%20as%20autonomous%20driving.%20Collaborative%20perception%20has%20recently%20attracted%20a%0Alarge%20research%20interest%20thanks%20to%20the%20ability%20to%20enhance%20the%20perception%20of%0Aautonomous%20vehicles%20via%20deep%20information%20fusion%20with%20intelligent%20roadside%20units%0A%28RSU%29%2C%20thus%20minimizing%20the%20impact%20of%20occlusion.%20While%20significant%20advancement%0Ahas%20been%20made%2C%20the%20data-hungry%20nature%20of%20these%20methods%20creates%20a%20major%20hurdle%0Afor%20their%20real-world%20deployment%2C%20particularly%20due%20to%20the%20need%20for%20annotated%20RSU%0Adata.%20Manually%20annotating%20the%20vast%20amount%20of%20RSU%20data%20required%20for%20training%20is%0Aprohibitively%20expensive%2C%20given%20the%20sheer%20number%20of%20intersections%20and%20the%20effort%0Ainvolved%20in%20annotating%20point%20clouds.%20We%20address%20this%20challenge%20by%20devising%20a%0Alabel-efficient%20object%20detection%20method%20for%20RSU%20based%20on%20unsupervised%20object%0Adiscovery.%20Our%20paper%20introduces%20two%20new%20modules%3A%20one%20for%20object%20discovery%20based%0Aon%20a%20spatial-temporal%20aggregation%20of%20point%20clouds%2C%20and%20another%20for%20refinement.%0AFurthermore%2C%20we%20demonstrate%20that%20fine-tuning%20on%20a%20small%20portion%20of%20annotated%0Adata%20allows%20our%20object%20discovery%20models%20to%20narrow%20the%20performance%20gap%20with%2C%20or%0Aeven%20surpass%2C%20fully%20supervised%20models.%20Extensive%20experiments%20are%20carried%20out%20in%0Asimulated%20and%20real-world%20datasets%20to%20evaluate%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06256v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-Efficient%203D%20Object%20Detection%20For%20Road-Side%20Units&entry.906535625=Minh-Quan%20Dao%20and%20Holger%20Caesar%20and%20Julie%20Stephany%20Berrio%20and%20Mao%20Shan%20and%20Stewart%20Worrall%20and%20Vincent%20Fr%C3%A9mont%20and%20Ezio%20Malis&entry.1292438233=%20%20Occlusion%20presents%20a%20significant%20challenge%20for%20safety-critical%20applications%0Asuch%20as%20autonomous%20driving.%20Collaborative%20perception%20has%20recently%20attracted%20a%0Alarge%20research%20interest%20thanks%20to%20the%20ability%20to%20enhance%20the%20perception%20of%0Aautonomous%20vehicles%20via%20deep%20information%20fusion%20with%20intelligent%20roadside%20units%0A%28RSU%29%2C%20thus%20minimizing%20the%20impact%20of%20occlusion.%20While%20significant%20advancement%0Ahas%20been%20made%2C%20the%20data-hungry%20nature%20of%20these%20methods%20creates%20a%20major%20hurdle%0Afor%20their%20real-world%20deployment%2C%20particularly%20due%20to%20the%20need%20for%20annotated%20RSU%0Adata.%20Manually%20annotating%20the%20vast%20amount%20of%20RSU%20data%20required%20for%20training%20is%0Aprohibitively%20expensive%2C%20given%20the%20sheer%20number%20of%20intersections%20and%20the%20effort%0Ainvolved%20in%20annotating%20point%20clouds.%20We%20address%20this%20challenge%20by%20devising%20a%0Alabel-efficient%20object%20detection%20method%20for%20RSU%20based%20on%20unsupervised%20object%0Adiscovery.%20Our%20paper%20introduces%20two%20new%20modules%3A%20one%20for%20object%20discovery%20based%0Aon%20a%20spatial-temporal%20aggregation%20of%20point%20clouds%2C%20and%20another%20for%20refinement.%0AFurthermore%2C%20we%20demonstrate%20that%20fine-tuning%20on%20a%20small%20portion%20of%20annotated%0Adata%20allows%20our%20object%20discovery%20models%20to%20narrow%20the%20performance%20gap%20with%2C%20or%0Aeven%20surpass%2C%20fully%20supervised%20models.%20Extensive%20experiments%20are%20carried%20out%20in%0Asimulated%20and%20real-world%20datasets%20to%20evaluate%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06256v1&entry.124074799=Read"},
{"title": "EPL: Evidential Prototype Learning for Semi-supervised Medical Image\n  Segmentation", "author": "Yuanpeng He", "abstract": "  Although current semi-supervised medical segmentation methods can achieve\ndecent performance, they are still affected by the uncertainty in unlabeled\ndata and model predictions, and there is currently a lack of effective\nstrategies that can explore the uncertain aspects of both simultaneously. To\naddress the aforementioned issues, we propose Evidential Prototype Learning\n(EPL), which utilizes an extended probabilistic framework to effectively fuse\nvoxel probability predictions from different sources and achieves prototype\nfusion utilization of labeled and unlabeled data under a generalized evidential\nframework, leveraging voxel-level dual uncertainty masking. The uncertainty not\nonly enables the model to self-correct predictions but also improves the guided\nlearning process with pseudo-labels and is able to feed back into the\nconstruction of hidden features. The method proposed in this paper has been\nexperimented on LA, Pancreas-CT and TBAD datasets, achieving the\nstate-of-the-art performance in three different labeled ratios, which strongly\ndemonstrates the effectiveness of our strategy.\n", "link": "http://arxiv.org/abs/2404.06181v1", "date": "2024-04-09", "relevancy": 2.2968, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6611}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5672}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EPL%3A%20Evidential%20Prototype%20Learning%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20EPL%3A%20Evidential%20Prototype%20Learning%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Yuanpeng%20He%0AAbstract%3A%20%20%20Although%20current%20semi-supervised%20medical%20segmentation%20methods%20can%20achieve%0Adecent%20performance%2C%20they%20are%20still%20affected%20by%20the%20uncertainty%20in%20unlabeled%0Adata%20and%20model%20predictions%2C%20and%20there%20is%20currently%20a%20lack%20of%20effective%0Astrategies%20that%20can%20explore%20the%20uncertain%20aspects%20of%20both%20simultaneously.%20To%0Aaddress%20the%20aforementioned%20issues%2C%20we%20propose%20Evidential%20Prototype%20Learning%0A%28EPL%29%2C%20which%20utilizes%20an%20extended%20probabilistic%20framework%20to%20effectively%20fuse%0Avoxel%20probability%20predictions%20from%20different%20sources%20and%20achieves%20prototype%0Afusion%20utilization%20of%20labeled%20and%20unlabeled%20data%20under%20a%20generalized%20evidential%0Aframework%2C%20leveraging%20voxel-level%20dual%20uncertainty%20masking.%20The%20uncertainty%20not%0Aonly%20enables%20the%20model%20to%20self-correct%20predictions%20but%20also%20improves%20the%20guided%0Alearning%20process%20with%20pseudo-labels%20and%20is%20able%20to%20feed%20back%20into%20the%0Aconstruction%20of%20hidden%20features.%20The%20method%20proposed%20in%20this%20paper%20has%20been%0Aexperimented%20on%20LA%2C%20Pancreas-CT%20and%20TBAD%20datasets%2C%20achieving%20the%0Astate-of-the-art%20performance%20in%20three%20different%20labeled%20ratios%2C%20which%20strongly%0Ademonstrates%20the%20effectiveness%20of%20our%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06181v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPL%3A%20Evidential%20Prototype%20Learning%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Yuanpeng%20He&entry.1292438233=%20%20Although%20current%20semi-supervised%20medical%20segmentation%20methods%20can%20achieve%0Adecent%20performance%2C%20they%20are%20still%20affected%20by%20the%20uncertainty%20in%20unlabeled%0Adata%20and%20model%20predictions%2C%20and%20there%20is%20currently%20a%20lack%20of%20effective%0Astrategies%20that%20can%20explore%20the%20uncertain%20aspects%20of%20both%20simultaneously.%20To%0Aaddress%20the%20aforementioned%20issues%2C%20we%20propose%20Evidential%20Prototype%20Learning%0A%28EPL%29%2C%20which%20utilizes%20an%20extended%20probabilistic%20framework%20to%20effectively%20fuse%0Avoxel%20probability%20predictions%20from%20different%20sources%20and%20achieves%20prototype%0Afusion%20utilization%20of%20labeled%20and%20unlabeled%20data%20under%20a%20generalized%20evidential%0Aframework%2C%20leveraging%20voxel-level%20dual%20uncertainty%20masking.%20The%20uncertainty%20not%0Aonly%20enables%20the%20model%20to%20self-correct%20predictions%20but%20also%20improves%20the%20guided%0Alearning%20process%20with%20pseudo-labels%20and%20is%20able%20to%20feed%20back%20into%20the%0Aconstruction%20of%20hidden%20features.%20The%20method%20proposed%20in%20this%20paper%20has%20been%0Aexperimented%20on%20LA%2C%20Pancreas-CT%20and%20TBAD%20datasets%2C%20achieving%20the%0Astate-of-the-art%20performance%20in%20three%20different%20labeled%20ratios%2C%20which%20strongly%0Ademonstrates%20the%20effectiveness%20of%20our%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06181v1&entry.124074799=Read"},
{"title": "CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoors\n  Object Detection from Multi-view Images", "author": "Guanlin Shen and Jingwei Huang and Zhihua Hu and Bin Wang", "abstract": "  This paper introduces CN-RMA, a novel approach for 3D indoor object detection\nfrom multi-view images. We observe the key challenge as the ambiguity of image\nand 3D correspondence without explicit geometry to provide occlusion\ninformation. To address this issue, CN-RMA leverages the synergy of 3D\nreconstruction networks and 3D object detection networks, where the\nreconstruction network provides a rough Truncated Signed Distance Function\n(TSDF) and guides image features to vote to 3D space correctly in an end-to-end\nmanner. Specifically, we associate weights to sampled points of each ray\nthrough ray marching, representing the contribution of a pixel in an image to\ncorresponding 3D locations. Such weights are determined by the predicted signed\ndistances so that image features vote only to regions near the reconstructed\nsurface. Our method achieves state-of-the-art performance in 3D object\ndetection from multi-view images, as measured by mAP@0.25 and mAP@0.5 on the\nScanNet and ARKitScenes datasets. The code and models are released at\nhttps://github.com/SerCharles/CN-RMA.\n", "link": "http://arxiv.org/abs/2403.04198v2", "date": "2024-04-09", "relevancy": 2.2897, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6019}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5595}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5308}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CN-RMA%3A%20Combined%20Network%20with%20Ray%20Marching%20Aggregation%20for%203D%20Indoors%0A%20%20Object%20Detection%20from%20Multi-view%20Images&body=Title%3A%20CN-RMA%3A%20Combined%20Network%20with%20Ray%20Marching%20Aggregation%20for%203D%20Indoors%0A%20%20Object%20Detection%20from%20Multi-view%20Images%0AAuthor%3A%20Guanlin%20Shen%20and%20Jingwei%20Huang%20and%20Zhihua%20Hu%20and%20Bin%20Wang%0AAbstract%3A%20%20%20This%20paper%20introduces%20CN-RMA%2C%20a%20novel%20approach%20for%203D%20indoor%20object%20detection%0Afrom%20multi-view%20images.%20We%20observe%20the%20key%20challenge%20as%20the%20ambiguity%20of%20image%0Aand%203D%20correspondence%20without%20explicit%20geometry%20to%20provide%20occlusion%0Ainformation.%20To%20address%20this%20issue%2C%20CN-RMA%20leverages%20the%20synergy%20of%203D%0Areconstruction%20networks%20and%203D%20object%20detection%20networks%2C%20where%20the%0Areconstruction%20network%20provides%20a%20rough%20Truncated%20Signed%20Distance%20Function%0A%28TSDF%29%20and%20guides%20image%20features%20to%20vote%20to%203D%20space%20correctly%20in%20an%20end-to-end%0Amanner.%20Specifically%2C%20we%20associate%20weights%20to%20sampled%20points%20of%20each%20ray%0Athrough%20ray%20marching%2C%20representing%20the%20contribution%20of%20a%20pixel%20in%20an%20image%20to%0Acorresponding%203D%20locations.%20Such%20weights%20are%20determined%20by%20the%20predicted%20signed%0Adistances%20so%20that%20image%20features%20vote%20only%20to%20regions%20near%20the%20reconstructed%0Asurface.%20Our%20method%20achieves%20state-of-the-art%20performance%20in%203D%20object%0Adetection%20from%20multi-view%20images%2C%20as%20measured%20by%20mAP%400.25%20and%20mAP%400.5%20on%20the%0AScanNet%20and%20ARKitScenes%20datasets.%20The%20code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/SerCharles/CN-RMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04198v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CN-RMA%3A%20Combined%20Network%20with%20Ray%20Marching%20Aggregation%20for%203D%20Indoors%0A%20%20Object%20Detection%20from%20Multi-view%20Images&entry.906535625=Guanlin%20Shen%20and%20Jingwei%20Huang%20and%20Zhihua%20Hu%20and%20Bin%20Wang&entry.1292438233=%20%20This%20paper%20introduces%20CN-RMA%2C%20a%20novel%20approach%20for%203D%20indoor%20object%20detection%0Afrom%20multi-view%20images.%20We%20observe%20the%20key%20challenge%20as%20the%20ambiguity%20of%20image%0Aand%203D%20correspondence%20without%20explicit%20geometry%20to%20provide%20occlusion%0Ainformation.%20To%20address%20this%20issue%2C%20CN-RMA%20leverages%20the%20synergy%20of%203D%0Areconstruction%20networks%20and%203D%20object%20detection%20networks%2C%20where%20the%0Areconstruction%20network%20provides%20a%20rough%20Truncated%20Signed%20Distance%20Function%0A%28TSDF%29%20and%20guides%20image%20features%20to%20vote%20to%203D%20space%20correctly%20in%20an%20end-to-end%0Amanner.%20Specifically%2C%20we%20associate%20weights%20to%20sampled%20points%20of%20each%20ray%0Athrough%20ray%20marching%2C%20representing%20the%20contribution%20of%20a%20pixel%20in%20an%20image%20to%0Acorresponding%203D%20locations.%20Such%20weights%20are%20determined%20by%20the%20predicted%20signed%0Adistances%20so%20that%20image%20features%20vote%20only%20to%20regions%20near%20the%20reconstructed%0Asurface.%20Our%20method%20achieves%20state-of-the-art%20performance%20in%203D%20object%0Adetection%20from%20multi-view%20images%2C%20as%20measured%20by%20mAP%400.25%20and%20mAP%400.5%20on%20the%0AScanNet%20and%20ARKitScenes%20datasets.%20The%20code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/SerCharles/CN-RMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04198v2&entry.124074799=Read"},
{"title": "Towards Robust Domain Generation Algorithm Classification", "author": "Arthur Drichel and Marc Meyer and Ulrike Meyer", "abstract": "  In this work, we conduct a comprehensive study on the robustness of domain\ngeneration algorithm (DGA) classifiers. We implement 32 white-box attacks, 19\nof which are very effective and induce a false-negative rate (FNR) of $\\approx$\n100\\% on unhardened classifiers. To defend the classifiers, we evaluate\ndifferent hardening approaches and propose a novel training scheme that\nleverages adversarial latent space vectors and discretized adversarial domains\nto significantly improve robustness. In our study, we highlight a pitfall to\navoid when hardening classifiers and uncover training biases that can be easily\nexploited by attackers to bypass detection, but which can be mitigated by\nadversarial training (AT). In our study, we do not observe any trade-off\nbetween robustness and performance, on the contrary, hardening improves a\nclassifier's detection performance for known and unknown DGAs. We implement all\nattacks and defenses discussed in this paper as a standalone library, which we\nmake publicly available to facilitate hardening of DGA classifiers:\nhttps://gitlab.com/rwth-itsec/robust-dga-detection\n", "link": "http://arxiv.org/abs/2404.06236v1", "date": "2024-04-09", "relevancy": 2.2795, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4681}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4549}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Domain%20Generation%20Algorithm%20Classification&body=Title%3A%20Towards%20Robust%20Domain%20Generation%20Algorithm%20Classification%0AAuthor%3A%20Arthur%20Drichel%20and%20Marc%20Meyer%20and%20Ulrike%20Meyer%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20conduct%20a%20comprehensive%20study%20on%20the%20robustness%20of%20domain%0Ageneration%20algorithm%20%28DGA%29%20classifiers.%20We%20implement%2032%20white-box%20attacks%2C%2019%0Aof%20which%20are%20very%20effective%20and%20induce%20a%20false-negative%20rate%20%28FNR%29%20of%20%24%5Capprox%24%0A100%5C%25%20on%20unhardened%20classifiers.%20To%20defend%20the%20classifiers%2C%20we%20evaluate%0Adifferent%20hardening%20approaches%20and%20propose%20a%20novel%20training%20scheme%20that%0Aleverages%20adversarial%20latent%20space%20vectors%20and%20discretized%20adversarial%20domains%0Ato%20significantly%20improve%20robustness.%20In%20our%20study%2C%20we%20highlight%20a%20pitfall%20to%0Aavoid%20when%20hardening%20classifiers%20and%20uncover%20training%20biases%20that%20can%20be%20easily%0Aexploited%20by%20attackers%20to%20bypass%20detection%2C%20but%20which%20can%20be%20mitigated%20by%0Aadversarial%20training%20%28AT%29.%20In%20our%20study%2C%20we%20do%20not%20observe%20any%20trade-off%0Abetween%20robustness%20and%20performance%2C%20on%20the%20contrary%2C%20hardening%20improves%20a%0Aclassifier%27s%20detection%20performance%20for%20known%20and%20unknown%20DGAs.%20We%20implement%20all%0Aattacks%20and%20defenses%20discussed%20in%20this%20paper%20as%20a%20standalone%20library%2C%20which%20we%0Amake%20publicly%20available%20to%20facilitate%20hardening%20of%20DGA%20classifiers%3A%0Ahttps%3A//gitlab.com/rwth-itsec/robust-dga-detection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06236v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Domain%20Generation%20Algorithm%20Classification&entry.906535625=Arthur%20Drichel%20and%20Marc%20Meyer%20and%20Ulrike%20Meyer&entry.1292438233=%20%20In%20this%20work%2C%20we%20conduct%20a%20comprehensive%20study%20on%20the%20robustness%20of%20domain%0Ageneration%20algorithm%20%28DGA%29%20classifiers.%20We%20implement%2032%20white-box%20attacks%2C%2019%0Aof%20which%20are%20very%20effective%20and%20induce%20a%20false-negative%20rate%20%28FNR%29%20of%20%24%5Capprox%24%0A100%5C%25%20on%20unhardened%20classifiers.%20To%20defend%20the%20classifiers%2C%20we%20evaluate%0Adifferent%20hardening%20approaches%20and%20propose%20a%20novel%20training%20scheme%20that%0Aleverages%20adversarial%20latent%20space%20vectors%20and%20discretized%20adversarial%20domains%0Ato%20significantly%20improve%20robustness.%20In%20our%20study%2C%20we%20highlight%20a%20pitfall%20to%0Aavoid%20when%20hardening%20classifiers%20and%20uncover%20training%20biases%20that%20can%20be%20easily%0Aexploited%20by%20attackers%20to%20bypass%20detection%2C%20but%20which%20can%20be%20mitigated%20by%0Aadversarial%20training%20%28AT%29.%20In%20our%20study%2C%20we%20do%20not%20observe%20any%20trade-off%0Abetween%20robustness%20and%20performance%2C%20on%20the%20contrary%2C%20hardening%20improves%20a%0Aclassifier%27s%20detection%20performance%20for%20known%20and%20unknown%20DGAs.%20We%20implement%20all%0Aattacks%20and%20defenses%20discussed%20in%20this%20paper%20as%20a%20standalone%20library%2C%20which%20we%0Amake%20publicly%20available%20to%20facilitate%20hardening%20of%20DGA%20classifiers%3A%0Ahttps%3A//gitlab.com/rwth-itsec/robust-dga-detection%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06236v1&entry.124074799=Read"},
{"title": "Exploring Neural Network Landscapes: Star-Shaped and Geodesic\n  Connectivity", "author": "Zhanran Lin and Puheng Li and Lei Wu", "abstract": "  One of the most intriguing findings in the structure of neural network\nlandscape is the phenomenon of mode connectivity: For two typical global\nminima, there exists a path connecting them without barrier. This concept of\nmode connectivity has played a crucial role in understanding important\nphenomena in deep learning.\n  In this paper, we conduct a fine-grained analysis of this connectivity\nphenomenon. First, we demonstrate that in the overparameterized case, the\nconnecting path can be as simple as a two-piece linear path, and the path\nlength can be nearly equal to the Euclidean distance. This finding suggests\nthat the landscape should be nearly convex in a certain sense. Second, we\nuncover a surprising star-shaped connectivity: For a finite number of typical\nminima, there exists a center on minima manifold that connects all of them\nsimultaneously via linear paths. These results are provably valid for linear\nnetworks and two-layer ReLU networks under a teacher-student setup, and are\nempirically supported by models trained on MNIST and CIFAR-10.\n", "link": "http://arxiv.org/abs/2404.06391v1", "date": "2024-04-09", "relevancy": 2.2788, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4681}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.457}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4423}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Neural%20Network%20Landscapes%3A%20Star-Shaped%20and%20Geodesic%0A%20%20Connectivity&body=Title%3A%20Exploring%20Neural%20Network%20Landscapes%3A%20Star-Shaped%20and%20Geodesic%0A%20%20Connectivity%0AAuthor%3A%20Zhanran%20Lin%20and%20Puheng%20Li%20and%20Lei%20Wu%0AAbstract%3A%20%20%20One%20of%20the%20most%20intriguing%20findings%20in%20the%20structure%20of%20neural%20network%0Alandscape%20is%20the%20phenomenon%20of%20mode%20connectivity%3A%20For%20two%20typical%20global%0Aminima%2C%20there%20exists%20a%20path%20connecting%20them%20without%20barrier.%20This%20concept%20of%0Amode%20connectivity%20has%20played%20a%20crucial%20role%20in%20understanding%20important%0Aphenomena%20in%20deep%20learning.%0A%20%20In%20this%20paper%2C%20we%20conduct%20a%20fine-grained%20analysis%20of%20this%20connectivity%0Aphenomenon.%20First%2C%20we%20demonstrate%20that%20in%20the%20overparameterized%20case%2C%20the%0Aconnecting%20path%20can%20be%20as%20simple%20as%20a%20two-piece%20linear%20path%2C%20and%20the%20path%0Alength%20can%20be%20nearly%20equal%20to%20the%20Euclidean%20distance.%20This%20finding%20suggests%0Athat%20the%20landscape%20should%20be%20nearly%20convex%20in%20a%20certain%20sense.%20Second%2C%20we%0Auncover%20a%20surprising%20star-shaped%20connectivity%3A%20For%20a%20finite%20number%20of%20typical%0Aminima%2C%20there%20exists%20a%20center%20on%20minima%20manifold%20that%20connects%20all%20of%20them%0Asimultaneously%20via%20linear%20paths.%20These%20results%20are%20provably%20valid%20for%20linear%0Anetworks%20and%20two-layer%20ReLU%20networks%20under%20a%20teacher-student%20setup%2C%20and%20are%0Aempirically%20supported%20by%20models%20trained%20on%20MNIST%20and%20CIFAR-10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06391v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Neural%20Network%20Landscapes%3A%20Star-Shaped%20and%20Geodesic%0A%20%20Connectivity&entry.906535625=Zhanran%20Lin%20and%20Puheng%20Li%20and%20Lei%20Wu&entry.1292438233=%20%20One%20of%20the%20most%20intriguing%20findings%20in%20the%20structure%20of%20neural%20network%0Alandscape%20is%20the%20phenomenon%20of%20mode%20connectivity%3A%20For%20two%20typical%20global%0Aminima%2C%20there%20exists%20a%20path%20connecting%20them%20without%20barrier.%20This%20concept%20of%0Amode%20connectivity%20has%20played%20a%20crucial%20role%20in%20understanding%20important%0Aphenomena%20in%20deep%20learning.%0A%20%20In%20this%20paper%2C%20we%20conduct%20a%20fine-grained%20analysis%20of%20this%20connectivity%0Aphenomenon.%20First%2C%20we%20demonstrate%20that%20in%20the%20overparameterized%20case%2C%20the%0Aconnecting%20path%20can%20be%20as%20simple%20as%20a%20two-piece%20linear%20path%2C%20and%20the%20path%0Alength%20can%20be%20nearly%20equal%20to%20the%20Euclidean%20distance.%20This%20finding%20suggests%0Athat%20the%20landscape%20should%20be%20nearly%20convex%20in%20a%20certain%20sense.%20Second%2C%20we%0Auncover%20a%20surprising%20star-shaped%20connectivity%3A%20For%20a%20finite%20number%20of%20typical%0Aminima%2C%20there%20exists%20a%20center%20on%20minima%20manifold%20that%20connects%20all%20of%20them%0Asimultaneously%20via%20linear%20paths.%20These%20results%20are%20provably%20valid%20for%20linear%0Anetworks%20and%20two-layer%20ReLU%20networks%20under%20a%20teacher-student%20setup%2C%20and%20are%0Aempirically%20supported%20by%20models%20trained%20on%20MNIST%20and%20CIFAR-10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06391v1&entry.124074799=Read"},
{"title": "Matching 2D Images in 3D: Metric Relative Pose from Metric\n  Correspondences", "author": "Axel Barroso-Laguna and Sowmya Munukutla and Victor Adrian Prisacariu and Eric Brachmann", "abstract": "  Given two images, we can estimate the relative camera pose between them by\nestablishing image-to-image correspondences. Usually, correspondences are\n2D-to-2D and the pose we estimate is defined only up to scale. Some\napplications, aiming at instant augmented reality anywhere, require\nscale-metric pose estimates, and hence, they rely on external depth estimators\nto recover the scale. We present MicKey, a keypoint matching pipeline that is\nable to predict metric correspondences in 3D camera space. By learning to match\n3D coordinates across images, we are able to infer the metric relative pose\nwithout depth measurements. Depth measurements are also not required for\ntraining, nor are scene reconstructions or image overlap information. MicKey is\nsupervised only by pairs of images and their relative poses. MicKey achieves\nstate-of-the-art performance on the Map-Free Relocalisation benchmark while\nrequiring less supervision than competing approaches.\n", "link": "http://arxiv.org/abs/2404.06337v1", "date": "2024-04-09", "relevancy": 2.273, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6246}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Matching%202D%20Images%20in%203D%3A%20Metric%20Relative%20Pose%20from%20Metric%0A%20%20Correspondences&body=Title%3A%20Matching%202D%20Images%20in%203D%3A%20Metric%20Relative%20Pose%20from%20Metric%0A%20%20Correspondences%0AAuthor%3A%20Axel%20Barroso-Laguna%20and%20Sowmya%20Munukutla%20and%20Victor%20Adrian%20Prisacariu%20and%20Eric%20Brachmann%0AAbstract%3A%20%20%20Given%20two%20images%2C%20we%20can%20estimate%20the%20relative%20camera%20pose%20between%20them%20by%0Aestablishing%20image-to-image%20correspondences.%20Usually%2C%20correspondences%20are%0A2D-to-2D%20and%20the%20pose%20we%20estimate%20is%20defined%20only%20up%20to%20scale.%20Some%0Aapplications%2C%20aiming%20at%20instant%20augmented%20reality%20anywhere%2C%20require%0Ascale-metric%20pose%20estimates%2C%20and%20hence%2C%20they%20rely%20on%20external%20depth%20estimators%0Ato%20recover%20the%20scale.%20We%20present%20MicKey%2C%20a%20keypoint%20matching%20pipeline%20that%20is%0Aable%20to%20predict%20metric%20correspondences%20in%203D%20camera%20space.%20By%20learning%20to%20match%0A3D%20coordinates%20across%20images%2C%20we%20are%20able%20to%20infer%20the%20metric%20relative%20pose%0Awithout%20depth%20measurements.%20Depth%20measurements%20are%20also%20not%20required%20for%0Atraining%2C%20nor%20are%20scene%20reconstructions%20or%20image%20overlap%20information.%20MicKey%20is%0Asupervised%20only%20by%20pairs%20of%20images%20and%20their%20relative%20poses.%20MicKey%20achieves%0Astate-of-the-art%20performance%20on%20the%20Map-Free%20Relocalisation%20benchmark%20while%0Arequiring%20less%20supervision%20than%20competing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06337v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%202D%20Images%20in%203D%3A%20Metric%20Relative%20Pose%20from%20Metric%0A%20%20Correspondences&entry.906535625=Axel%20Barroso-Laguna%20and%20Sowmya%20Munukutla%20and%20Victor%20Adrian%20Prisacariu%20and%20Eric%20Brachmann&entry.1292438233=%20%20Given%20two%20images%2C%20we%20can%20estimate%20the%20relative%20camera%20pose%20between%20them%20by%0Aestablishing%20image-to-image%20correspondences.%20Usually%2C%20correspondences%20are%0A2D-to-2D%20and%20the%20pose%20we%20estimate%20is%20defined%20only%20up%20to%20scale.%20Some%0Aapplications%2C%20aiming%20at%20instant%20augmented%20reality%20anywhere%2C%20require%0Ascale-metric%20pose%20estimates%2C%20and%20hence%2C%20they%20rely%20on%20external%20depth%20estimators%0Ato%20recover%20the%20scale.%20We%20present%20MicKey%2C%20a%20keypoint%20matching%20pipeline%20that%20is%0Aable%20to%20predict%20metric%20correspondences%20in%203D%20camera%20space.%20By%20learning%20to%20match%0A3D%20coordinates%20across%20images%2C%20we%20are%20able%20to%20infer%20the%20metric%20relative%20pose%0Awithout%20depth%20measurements.%20Depth%20measurements%20are%20also%20not%20required%20for%0Atraining%2C%20nor%20are%20scene%20reconstructions%20or%20image%20overlap%20information.%20MicKey%20is%0Asupervised%20only%20by%20pairs%20of%20images%20and%20their%20relative%20poses.%20MicKey%20achieves%0Astate-of-the-art%20performance%20on%20the%20Map-Free%20Relocalisation%20benchmark%20while%0Arequiring%20less%20supervision%20than%20competing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06337v1&entry.124074799=Read"},
{"title": "ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation\n  Network for Video Colorization", "author": "Yixin Yang and Jiangxin Dong and Jinhui Tang and Jinshan Pan", "abstract": "  How to effectively explore spatial-temporal features is important for video\ncolorization. Instead of stacking multiple frames along the temporal dimension\nor recurrently propagating estimated features that will accumulate errors or\ncannot explore information from far-apart frames, we develop a memory-based\nfeature propagation module that can establish reliable connections with\nfeatures from far-apart frames and alleviate the influence of inaccurately\nestimated features. To extract better features from each frame for the\nabove-mentioned feature propagation, we explore the features from\nlarge-pretrained visual models to guide the feature estimation of each frame so\nthat the estimated features can model complex scenarios. In addition, we note\nthat adjacent frames usually contain similar contents. To explore this property\nfor better spatial and temporal feature utilization, we develop a local\nattention module to aggregate the features from adjacent frames in a\nspatial-temporal neighborhood. We formulate our memory-based feature\npropagation module, large-pretrained visual model guided feature estimation\nmodule, and local attention module into an end-to-end trainable network (named\nColorMNet) and show that it performs favorably against state-of-the-art methods\non both the benchmark datasets and real-world scenarios. The source code and\npre-trained models will be available at\n\\url{https://github.com/yyang181/colormnet}.\n", "link": "http://arxiv.org/abs/2404.06251v1", "date": "2024-04-09", "relevancy": 2.2719, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5779}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.562}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5581}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ColorMNet%3A%20A%20Memory-based%20Deep%20Spatial-Temporal%20Feature%20Propagation%0A%20%20Network%20for%20Video%20Colorization&body=Title%3A%20ColorMNet%3A%20A%20Memory-based%20Deep%20Spatial-Temporal%20Feature%20Propagation%0A%20%20Network%20for%20Video%20Colorization%0AAuthor%3A%20Yixin%20Yang%20and%20Jiangxin%20Dong%20and%20Jinhui%20Tang%20and%20Jinshan%20Pan%0AAbstract%3A%20%20%20How%20to%20effectively%20explore%20spatial-temporal%20features%20is%20important%20for%20video%0Acolorization.%20Instead%20of%20stacking%20multiple%20frames%20along%20the%20temporal%20dimension%0Aor%20recurrently%20propagating%20estimated%20features%20that%20will%20accumulate%20errors%20or%0Acannot%20explore%20information%20from%20far-apart%20frames%2C%20we%20develop%20a%20memory-based%0Afeature%20propagation%20module%20that%20can%20establish%20reliable%20connections%20with%0Afeatures%20from%20far-apart%20frames%20and%20alleviate%20the%20influence%20of%20inaccurately%0Aestimated%20features.%20To%20extract%20better%20features%20from%20each%20frame%20for%20the%0Aabove-mentioned%20feature%20propagation%2C%20we%20explore%20the%20features%20from%0Alarge-pretrained%20visual%20models%20to%20guide%20the%20feature%20estimation%20of%20each%20frame%20so%0Athat%20the%20estimated%20features%20can%20model%20complex%20scenarios.%20In%20addition%2C%20we%20note%0Athat%20adjacent%20frames%20usually%20contain%20similar%20contents.%20To%20explore%20this%20property%0Afor%20better%20spatial%20and%20temporal%20feature%20utilization%2C%20we%20develop%20a%20local%0Aattention%20module%20to%20aggregate%20the%20features%20from%20adjacent%20frames%20in%20a%0Aspatial-temporal%20neighborhood.%20We%20formulate%20our%20memory-based%20feature%0Apropagation%20module%2C%20large-pretrained%20visual%20model%20guided%20feature%20estimation%0Amodule%2C%20and%20local%20attention%20module%20into%20an%20end-to-end%20trainable%20network%20%28named%0AColorMNet%29%20and%20show%20that%20it%20performs%20favorably%20against%20state-of-the-art%20methods%0Aon%20both%20the%20benchmark%20datasets%20and%20real-world%20scenarios.%20The%20source%20code%20and%0Apre-trained%20models%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yyang181/colormnet%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06251v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColorMNet%3A%20A%20Memory-based%20Deep%20Spatial-Temporal%20Feature%20Propagation%0A%20%20Network%20for%20Video%20Colorization&entry.906535625=Yixin%20Yang%20and%20Jiangxin%20Dong%20and%20Jinhui%20Tang%20and%20Jinshan%20Pan&entry.1292438233=%20%20How%20to%20effectively%20explore%20spatial-temporal%20features%20is%20important%20for%20video%0Acolorization.%20Instead%20of%20stacking%20multiple%20frames%20along%20the%20temporal%20dimension%0Aor%20recurrently%20propagating%20estimated%20features%20that%20will%20accumulate%20errors%20or%0Acannot%20explore%20information%20from%20far-apart%20frames%2C%20we%20develop%20a%20memory-based%0Afeature%20propagation%20module%20that%20can%20establish%20reliable%20connections%20with%0Afeatures%20from%20far-apart%20frames%20and%20alleviate%20the%20influence%20of%20inaccurately%0Aestimated%20features.%20To%20extract%20better%20features%20from%20each%20frame%20for%20the%0Aabove-mentioned%20feature%20propagation%2C%20we%20explore%20the%20features%20from%0Alarge-pretrained%20visual%20models%20to%20guide%20the%20feature%20estimation%20of%20each%20frame%20so%0Athat%20the%20estimated%20features%20can%20model%20complex%20scenarios.%20In%20addition%2C%20we%20note%0Athat%20adjacent%20frames%20usually%20contain%20similar%20contents.%20To%20explore%20this%20property%0Afor%20better%20spatial%20and%20temporal%20feature%20utilization%2C%20we%20develop%20a%20local%0Aattention%20module%20to%20aggregate%20the%20features%20from%20adjacent%20frames%20in%20a%0Aspatial-temporal%20neighborhood.%20We%20formulate%20our%20memory-based%20feature%0Apropagation%20module%2C%20large-pretrained%20visual%20model%20guided%20feature%20estimation%0Amodule%2C%20and%20local%20attention%20module%20into%20an%20end-to-end%20trainable%20network%20%28named%0AColorMNet%29%20and%20show%20that%20it%20performs%20favorably%20against%20state-of-the-art%20methods%0Aon%20both%20the%20benchmark%20datasets%20and%20real-world%20scenarios.%20The%20source%20code%20and%0Apre-trained%20models%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yyang181/colormnet%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06251v1&entry.124074799=Read"},
{"title": "Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large\n  Multi-Modal Models", "author": "David Kurzend\u00f6rfer and Otniel-Bogdan Mercea and A. Sophia Koepke and Zeynep Akata", "abstract": "  Audio-visual zero-shot learning methods commonly build on features extracted\nfrom pre-trained models, e.g. video or audio classification models. However,\nexisting benchmarks predate the popularization of large multi-modal models,\nsuch as CLIP and CLAP. In this work, we explore such large pre-trained models\nto obtain features, i.e. CLIP for visual features, and CLAP for audio features.\nFurthermore, the CLIP and CLAP text encoders provide class label embeddings\nwhich are combined to boost the performance of the system. We propose a simple\nyet effective model that only relies on feed-forward neural networks,\nexploiting the strong generalization capabilities of the new audio, visual and\ntextual features. Our framework achieves state-of-the-art performance on\nVGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL with our new features. Code and\ndata available at: https://github.com/dkurzend/ClipClap-GZSL.\n", "link": "http://arxiv.org/abs/2404.06309v1", "date": "2024-04-09", "relevancy": 2.2684, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.614}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5423}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.512}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Audio-Visual%20Generalized%20Zero-Shot%20Learning%20using%20Pre-Trained%20Large%0A%20%20Multi-Modal%20Models&body=Title%3A%20Audio-Visual%20Generalized%20Zero-Shot%20Learning%20using%20Pre-Trained%20Large%0A%20%20Multi-Modal%20Models%0AAuthor%3A%20David%20Kurzend%C3%B6rfer%20and%20Otniel-Bogdan%20Mercea%20and%20A.%20Sophia%20Koepke%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20Audio-visual%20zero-shot%20learning%20methods%20commonly%20build%20on%20features%20extracted%0Afrom%20pre-trained%20models%2C%20e.g.%20video%20or%20audio%20classification%20models.%20However%2C%0Aexisting%20benchmarks%20predate%20the%20popularization%20of%20large%20multi-modal%20models%2C%0Asuch%20as%20CLIP%20and%20CLAP.%20In%20this%20work%2C%20we%20explore%20such%20large%20pre-trained%20models%0Ato%20obtain%20features%2C%20i.e.%20CLIP%20for%20visual%20features%2C%20and%20CLAP%20for%20audio%20features.%0AFurthermore%2C%20the%20CLIP%20and%20CLAP%20text%20encoders%20provide%20class%20label%20embeddings%0Awhich%20are%20combined%20to%20boost%20the%20performance%20of%20the%20system.%20We%20propose%20a%20simple%0Ayet%20effective%20model%20that%20only%20relies%20on%20feed-forward%20neural%20networks%2C%0Aexploiting%20the%20strong%20generalization%20capabilities%20of%20the%20new%20audio%2C%20visual%20and%0Atextual%20features.%20Our%20framework%20achieves%20state-of-the-art%20performance%20on%0AVGGSound-GZSL%2C%20UCF-GZSL%2C%20and%20ActivityNet-GZSL%20with%20our%20new%20features.%20Code%20and%0Adata%20available%20at%3A%20https%3A//github.com/dkurzend/ClipClap-GZSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06309v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Visual%20Generalized%20Zero-Shot%20Learning%20using%20Pre-Trained%20Large%0A%20%20Multi-Modal%20Models&entry.906535625=David%20Kurzend%C3%B6rfer%20and%20Otniel-Bogdan%20Mercea%20and%20A.%20Sophia%20Koepke%20and%20Zeynep%20Akata&entry.1292438233=%20%20Audio-visual%20zero-shot%20learning%20methods%20commonly%20build%20on%20features%20extracted%0Afrom%20pre-trained%20models%2C%20e.g.%20video%20or%20audio%20classification%20models.%20However%2C%0Aexisting%20benchmarks%20predate%20the%20popularization%20of%20large%20multi-modal%20models%2C%0Asuch%20as%20CLIP%20and%20CLAP.%20In%20this%20work%2C%20we%20explore%20such%20large%20pre-trained%20models%0Ato%20obtain%20features%2C%20i.e.%20CLIP%20for%20visual%20features%2C%20and%20CLAP%20for%20audio%20features.%0AFurthermore%2C%20the%20CLIP%20and%20CLAP%20text%20encoders%20provide%20class%20label%20embeddings%0Awhich%20are%20combined%20to%20boost%20the%20performance%20of%20the%20system.%20We%20propose%20a%20simple%0Ayet%20effective%20model%20that%20only%20relies%20on%20feed-forward%20neural%20networks%2C%0Aexploiting%20the%20strong%20generalization%20capabilities%20of%20the%20new%20audio%2C%20visual%20and%0Atextual%20features.%20Our%20framework%20achieves%20state-of-the-art%20performance%20on%0AVGGSound-GZSL%2C%20UCF-GZSL%2C%20and%20ActivityNet-GZSL%20with%20our%20new%20features.%20Code%20and%0Adata%20available%20at%3A%20https%3A//github.com/dkurzend/ClipClap-GZSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06309v1&entry.124074799=Read"},
{"title": "Improving Interpretable Embeddings for Ad-hoc Video Search with\n  Generative Captions and Multi-word Concept Bank", "author": "Jiaxin Wu and Chong-Wah Ngo and Wing-Kwong Chan", "abstract": "  Aligning a user query and video clips in cross-modal latent space and that\nwith semantic concepts are two mainstream approaches for ad-hoc video search\n(AVS). However, the effectiveness of existing approaches is bottlenecked by the\nsmall sizes of available video-text datasets and the low quality of concept\nbanks, which results in the failures of unseen queries and the\nout-of-vocabulary problem. This paper addresses these two problems by\nconstructing a new dataset and developing a multi-word concept bank.\nSpecifically, capitalizing on a generative model, we construct a new dataset\nconsisting of 7 million generated text and video pairs for pre-training. To\ntackle the out-of-vocabulary problem, we develop a multi-word concept bank\nbased on syntax analysis to enhance the capability of a state-of-the-art\ninterpretable AVS method in modeling relationships between query words. We also\nstudy the impact of current advanced features on the method. Experimental\nresults show that the integration of the above-proposed elements doubles the\nR@1 performance of the AVS method on the MSRVTT dataset and improves the xinfAP\non the TRECVid AVS query sets for 2016-2023 (eight years) by a margin from 2%\nto 77%, with an average about 20%.\n", "link": "http://arxiv.org/abs/2404.06173v1", "date": "2024-04-09", "relevancy": 2.2397, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5721}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5715}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Interpretable%20Embeddings%20for%20Ad-hoc%20Video%20Search%20with%0A%20%20Generative%20Captions%20and%20Multi-word%20Concept%20Bank&body=Title%3A%20Improving%20Interpretable%20Embeddings%20for%20Ad-hoc%20Video%20Search%20with%0A%20%20Generative%20Captions%20and%20Multi-word%20Concept%20Bank%0AAuthor%3A%20Jiaxin%20Wu%20and%20Chong-Wah%20Ngo%20and%20Wing-Kwong%20Chan%0AAbstract%3A%20%20%20Aligning%20a%20user%20query%20and%20video%20clips%20in%20cross-modal%20latent%20space%20and%20that%0Awith%20semantic%20concepts%20are%20two%20mainstream%20approaches%20for%20ad-hoc%20video%20search%0A%28AVS%29.%20However%2C%20the%20effectiveness%20of%20existing%20approaches%20is%20bottlenecked%20by%20the%0Asmall%20sizes%20of%20available%20video-text%20datasets%20and%20the%20low%20quality%20of%20concept%0Abanks%2C%20which%20results%20in%20the%20failures%20of%20unseen%20queries%20and%20the%0Aout-of-vocabulary%20problem.%20This%20paper%20addresses%20these%20two%20problems%20by%0Aconstructing%20a%20new%20dataset%20and%20developing%20a%20multi-word%20concept%20bank.%0ASpecifically%2C%20capitalizing%20on%20a%20generative%20model%2C%20we%20construct%20a%20new%20dataset%0Aconsisting%20of%207%20million%20generated%20text%20and%20video%20pairs%20for%20pre-training.%20To%0Atackle%20the%20out-of-vocabulary%20problem%2C%20we%20develop%20a%20multi-word%20concept%20bank%0Abased%20on%20syntax%20analysis%20to%20enhance%20the%20capability%20of%20a%20state-of-the-art%0Ainterpretable%20AVS%20method%20in%20modeling%20relationships%20between%20query%20words.%20We%20also%0Astudy%20the%20impact%20of%20current%20advanced%20features%20on%20the%20method.%20Experimental%0Aresults%20show%20that%20the%20integration%20of%20the%20above-proposed%20elements%20doubles%20the%0AR%401%20performance%20of%20the%20AVS%20method%20on%20the%20MSRVTT%20dataset%20and%20improves%20the%20xinfAP%0Aon%20the%20TRECVid%20AVS%20query%20sets%20for%202016-2023%20%28eight%20years%29%20by%20a%20margin%20from%202%25%0Ato%2077%25%2C%20with%20an%20average%20about%2020%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06173v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Interpretable%20Embeddings%20for%20Ad-hoc%20Video%20Search%20with%0A%20%20Generative%20Captions%20and%20Multi-word%20Concept%20Bank&entry.906535625=Jiaxin%20Wu%20and%20Chong-Wah%20Ngo%20and%20Wing-Kwong%20Chan&entry.1292438233=%20%20Aligning%20a%20user%20query%20and%20video%20clips%20in%20cross-modal%20latent%20space%20and%20that%0Awith%20semantic%20concepts%20are%20two%20mainstream%20approaches%20for%20ad-hoc%20video%20search%0A%28AVS%29.%20However%2C%20the%20effectiveness%20of%20existing%20approaches%20is%20bottlenecked%20by%20the%0Asmall%20sizes%20of%20available%20video-text%20datasets%20and%20the%20low%20quality%20of%20concept%0Abanks%2C%20which%20results%20in%20the%20failures%20of%20unseen%20queries%20and%20the%0Aout-of-vocabulary%20problem.%20This%20paper%20addresses%20these%20two%20problems%20by%0Aconstructing%20a%20new%20dataset%20and%20developing%20a%20multi-word%20concept%20bank.%0ASpecifically%2C%20capitalizing%20on%20a%20generative%20model%2C%20we%20construct%20a%20new%20dataset%0Aconsisting%20of%207%20million%20generated%20text%20and%20video%20pairs%20for%20pre-training.%20To%0Atackle%20the%20out-of-vocabulary%20problem%2C%20we%20develop%20a%20multi-word%20concept%20bank%0Abased%20on%20syntax%20analysis%20to%20enhance%20the%20capability%20of%20a%20state-of-the-art%0Ainterpretable%20AVS%20method%20in%20modeling%20relationships%20between%20query%20words.%20We%20also%0Astudy%20the%20impact%20of%20current%20advanced%20features%20on%20the%20method.%20Experimental%0Aresults%20show%20that%20the%20integration%20of%20the%20above-proposed%20elements%20doubles%20the%0AR%401%20performance%20of%20the%20AVS%20method%20on%20the%20MSRVTT%20dataset%20and%20improves%20the%20xinfAP%0Aon%20the%20TRECVid%20AVS%20query%20sets%20for%202016-2023%20%28eight%20years%29%20by%20a%20margin%20from%202%25%0Ato%2077%25%2C%20with%20an%20average%20about%2020%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06173v1&entry.124074799=Read"},
{"title": "QueSTMaps: Queryable Semantic Topological Maps for 3D Scene\n  Understanding", "author": "Yash Mehan and Kumaraditya Gupta and Rohit Jayanti and Anirudh Govil and Sourav Garg and Madhava Krishna", "abstract": "  Understanding the structural organisation of 3D indoor scenes in terms of\nrooms is often accomplished via floorplan extraction. Robotic tasks such as\nplanning and navigation require a semantic understanding of the scene as well.\nThis is typically achieved via object-level semantic segmentation. However,\nsuch methods struggle to segment out topological regions like \"kitchen\" in the\nscene. In this work, we introduce a two-step pipeline. First, we extract a\ntopological map, i.e., floorplan of the indoor scene using a novel\nmulti-channel occupancy representation. Then, we generate CLIP-aligned features\nand semantic labels for every room instance based on the objects it contains\nusing a self-attention transformer. Our language-topology alignment supports\nnatural language querying, e.g., a \"place to cook\" locates the \"kitchen\". We\noutperform the current state-of-the-art on room segmentation by ~20% and room\nclassification by ~12%. Our detailed qualitative analysis and ablation studies\nprovide insights into the problem of joint structural and semantic 3D scene\nunderstanding.\n", "link": "http://arxiv.org/abs/2404.06442v1", "date": "2024-04-09", "relevancy": 2.2331, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5691}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5511}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.549}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20QueSTMaps%3A%20Queryable%20Semantic%20Topological%20Maps%20for%203D%20Scene%0A%20%20Understanding&body=Title%3A%20QueSTMaps%3A%20Queryable%20Semantic%20Topological%20Maps%20for%203D%20Scene%0A%20%20Understanding%0AAuthor%3A%20Yash%20Mehan%20and%20Kumaraditya%20Gupta%20and%20Rohit%20Jayanti%20and%20Anirudh%20Govil%20and%20Sourav%20Garg%20and%20Madhava%20Krishna%0AAbstract%3A%20%20%20Understanding%20the%20structural%20organisation%20of%203D%20indoor%20scenes%20in%20terms%20of%0Arooms%20is%20often%20accomplished%20via%20floorplan%20extraction.%20Robotic%20tasks%20such%20as%0Aplanning%20and%20navigation%20require%20a%20semantic%20understanding%20of%20the%20scene%20as%20well.%0AThis%20is%20typically%20achieved%20via%20object-level%20semantic%20segmentation.%20However%2C%0Asuch%20methods%20struggle%20to%20segment%20out%20topological%20regions%20like%20%22kitchen%22%20in%20the%0Ascene.%20In%20this%20work%2C%20we%20introduce%20a%20two-step%20pipeline.%20First%2C%20we%20extract%20a%0Atopological%20map%2C%20i.e.%2C%20floorplan%20of%20the%20indoor%20scene%20using%20a%20novel%0Amulti-channel%20occupancy%20representation.%20Then%2C%20we%20generate%20CLIP-aligned%20features%0Aand%20semantic%20labels%20for%20every%20room%20instance%20based%20on%20the%20objects%20it%20contains%0Ausing%20a%20self-attention%20transformer.%20Our%20language-topology%20alignment%20supports%0Anatural%20language%20querying%2C%20e.g.%2C%20a%20%22place%20to%20cook%22%20locates%20the%20%22kitchen%22.%20We%0Aoutperform%20the%20current%20state-of-the-art%20on%20room%20segmentation%20by%20~20%25%20and%20room%0Aclassification%20by%20~12%25.%20Our%20detailed%20qualitative%20analysis%20and%20ablation%20studies%0Aprovide%20insights%20into%20the%20problem%20of%20joint%20structural%20and%20semantic%203D%20scene%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06442v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QueSTMaps%3A%20Queryable%20Semantic%20Topological%20Maps%20for%203D%20Scene%0A%20%20Understanding&entry.906535625=Yash%20Mehan%20and%20Kumaraditya%20Gupta%20and%20Rohit%20Jayanti%20and%20Anirudh%20Govil%20and%20Sourav%20Garg%20and%20Madhava%20Krishna&entry.1292438233=%20%20Understanding%20the%20structural%20organisation%20of%203D%20indoor%20scenes%20in%20terms%20of%0Arooms%20is%20often%20accomplished%20via%20floorplan%20extraction.%20Robotic%20tasks%20such%20as%0Aplanning%20and%20navigation%20require%20a%20semantic%20understanding%20of%20the%20scene%20as%20well.%0AThis%20is%20typically%20achieved%20via%20object-level%20semantic%20segmentation.%20However%2C%0Asuch%20methods%20struggle%20to%20segment%20out%20topological%20regions%20like%20%22kitchen%22%20in%20the%0Ascene.%20In%20this%20work%2C%20we%20introduce%20a%20two-step%20pipeline.%20First%2C%20we%20extract%20a%0Atopological%20map%2C%20i.e.%2C%20floorplan%20of%20the%20indoor%20scene%20using%20a%20novel%0Amulti-channel%20occupancy%20representation.%20Then%2C%20we%20generate%20CLIP-aligned%20features%0Aand%20semantic%20labels%20for%20every%20room%20instance%20based%20on%20the%20objects%20it%20contains%0Ausing%20a%20self-attention%20transformer.%20Our%20language-topology%20alignment%20supports%0Anatural%20language%20querying%2C%20e.g.%2C%20a%20%22place%20to%20cook%22%20locates%20the%20%22kitchen%22.%20We%0Aoutperform%20the%20current%20state-of-the-art%20on%20room%20segmentation%20by%20~20%25%20and%20room%0Aclassification%20by%20~12%25.%20Our%20detailed%20qualitative%20analysis%20and%20ablation%20studies%0Aprovide%20insights%20into%20the%20problem%20of%20joint%20structural%20and%20semantic%203D%20scene%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06442v1&entry.124074799=Read"},
{"title": "Improved Probabilistic Image-Text Representations", "author": "Sanghyuk Chun", "abstract": "  Image-Text Matching (ITM) task, a fundamental vision-language (VL) task,\nsuffers from the inherent ambiguity arising from multiplicity and imperfect\nannotations. Deterministic functions are not sufficiently powerful to capture\nambiguity, prompting the exploration of probabilistic embeddings to tackle the\nchallenge. However, the existing probabilistic ITM approach encounters two key\nshortcomings; the burden of heavy computations due to the Monte Carlo\napproximation, and the loss saturation issue in the face of abundant false\nnegatives. To overcome the issues, this paper presents an improved\nProbabilistic Cross-Modal Embeddings (named PCME++) by introducing a new\nprobabilistic distance with a closed-form solution. In addition, two\noptimization techniques are proposed to enhance PCME++ further: first, the\nincorporation of pseudo-positives to prevent the negative effect under massive\nfalse negatives; second, mixed sample data augmentation for probabilistic\nmatching. Experimental results on MS-COCO Caption and two extended benchmarks,\nCxC and ECCV Caption, demonstrate the effectiveness of PCME++ compared to\nstate-of-the-art ITM methods. The robustness of PCME++ is also evaluated under\nnoisy image-text correspondences. In addition, the potential applicability of\nPCME++ in automatic prompt-filtering for zero-shot classification is shown. The\ncode is available at https://github.com/naver-ai/pcmepp\n", "link": "http://arxiv.org/abs/2305.18171v5", "date": "2024-04-09", "relevancy": 2.2304, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5644}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.554}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improved%20Probabilistic%20Image-Text%20Representations&body=Title%3A%20Improved%20Probabilistic%20Image-Text%20Representations%0AAuthor%3A%20Sanghyuk%20Chun%0AAbstract%3A%20%20%20Image-Text%20Matching%20%28ITM%29%20task%2C%20a%20fundamental%20vision-language%20%28VL%29%20task%2C%0Asuffers%20from%20the%20inherent%20ambiguity%20arising%20from%20multiplicity%20and%20imperfect%0Aannotations.%20Deterministic%20functions%20are%20not%20sufficiently%20powerful%20to%20capture%0Aambiguity%2C%20prompting%20the%20exploration%20of%20probabilistic%20embeddings%20to%20tackle%20the%0Achallenge.%20However%2C%20the%20existing%20probabilistic%20ITM%20approach%20encounters%20two%20key%0Ashortcomings%3B%20the%20burden%20of%20heavy%20computations%20due%20to%20the%20Monte%20Carlo%0Aapproximation%2C%20and%20the%20loss%20saturation%20issue%20in%20the%20face%20of%20abundant%20false%0Anegatives.%20To%20overcome%20the%20issues%2C%20this%20paper%20presents%20an%20improved%0AProbabilistic%20Cross-Modal%20Embeddings%20%28named%20PCME%2B%2B%29%20by%20introducing%20a%20new%0Aprobabilistic%20distance%20with%20a%20closed-form%20solution.%20In%20addition%2C%20two%0Aoptimization%20techniques%20are%20proposed%20to%20enhance%20PCME%2B%2B%20further%3A%20first%2C%20the%0Aincorporation%20of%20pseudo-positives%20to%20prevent%20the%20negative%20effect%20under%20massive%0Afalse%20negatives%3B%20second%2C%20mixed%20sample%20data%20augmentation%20for%20probabilistic%0Amatching.%20Experimental%20results%20on%20MS-COCO%20Caption%20and%20two%20extended%20benchmarks%2C%0ACxC%20and%20ECCV%20Caption%2C%20demonstrate%20the%20effectiveness%20of%20PCME%2B%2B%20compared%20to%0Astate-of-the-art%20ITM%20methods.%20The%20robustness%20of%20PCME%2B%2B%20is%20also%20evaluated%20under%0Anoisy%20image-text%20correspondences.%20In%20addition%2C%20the%20potential%20applicability%20of%0APCME%2B%2B%20in%20automatic%20prompt-filtering%20for%20zero-shot%20classification%20is%20shown.%20The%0Acode%20is%20available%20at%20https%3A//github.com/naver-ai/pcmepp%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.18171v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Probabilistic%20Image-Text%20Representations&entry.906535625=Sanghyuk%20Chun&entry.1292438233=%20%20Image-Text%20Matching%20%28ITM%29%20task%2C%20a%20fundamental%20vision-language%20%28VL%29%20task%2C%0Asuffers%20from%20the%20inherent%20ambiguity%20arising%20from%20multiplicity%20and%20imperfect%0Aannotations.%20Deterministic%20functions%20are%20not%20sufficiently%20powerful%20to%20capture%0Aambiguity%2C%20prompting%20the%20exploration%20of%20probabilistic%20embeddings%20to%20tackle%20the%0Achallenge.%20However%2C%20the%20existing%20probabilistic%20ITM%20approach%20encounters%20two%20key%0Ashortcomings%3B%20the%20burden%20of%20heavy%20computations%20due%20to%20the%20Monte%20Carlo%0Aapproximation%2C%20and%20the%20loss%20saturation%20issue%20in%20the%20face%20of%20abundant%20false%0Anegatives.%20To%20overcome%20the%20issues%2C%20this%20paper%20presents%20an%20improved%0AProbabilistic%20Cross-Modal%20Embeddings%20%28named%20PCME%2B%2B%29%20by%20introducing%20a%20new%0Aprobabilistic%20distance%20with%20a%20closed-form%20solution.%20In%20addition%2C%20two%0Aoptimization%20techniques%20are%20proposed%20to%20enhance%20PCME%2B%2B%20further%3A%20first%2C%20the%0Aincorporation%20of%20pseudo-positives%20to%20prevent%20the%20negative%20effect%20under%20massive%0Afalse%20negatives%3B%20second%2C%20mixed%20sample%20data%20augmentation%20for%20probabilistic%0Amatching.%20Experimental%20results%20on%20MS-COCO%20Caption%20and%20two%20extended%20benchmarks%2C%0ACxC%20and%20ECCV%20Caption%2C%20demonstrate%20the%20effectiveness%20of%20PCME%2B%2B%20compared%20to%0Astate-of-the-art%20ITM%20methods.%20The%20robustness%20of%20PCME%2B%2B%20is%20also%20evaluated%20under%0Anoisy%20image-text%20correspondences.%20In%20addition%2C%20the%20potential%20applicability%20of%0APCME%2B%2B%20in%20automatic%20prompt-filtering%20for%20zero-shot%20classification%20is%20shown.%20The%0Acode%20is%20available%20at%20https%3A//github.com/naver-ai/pcmepp%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.18171v5&entry.124074799=Read"},
{"title": "Anchor-based Robust Finetuning of Vision-Language Models", "author": "Jinwei Han and Zhiwen Lin and Zhongyisun Sun and Yingguo Gao and Ke Yan and Shouhong Ding and Yuan Gao and Gui-Song Xia", "abstract": "  We aim at finetuning a vision-language model without hurting its\nout-of-distribution (OOD) generalization. We address two types of OOD\ngeneralization, i.e., i) domain shift such as natural to sketch images, and ii)\nzero-shot capability to recognize the category that was not contained in the\nfinetune data. Arguably, the diminished OOD generalization after finetuning\nstems from the excessively simplified finetuning target, which only provides\nthe class information, such as ``a photo of a [CLASS]''. This is distinct from\nthe process in that CLIP was pretrained, where there is abundant text\nsupervision with rich semantic information. Therefore, we propose to compensate\nfor the finetune process using auxiliary supervision with rich semantic\ninformation, which acts as anchors to preserve the OOD generalization.\nSpecifically, two types of anchors are elaborated in our method, including i)\ntext-compensated anchor which uses the images from the finetune set but\nenriches the text supervision from a pretrained captioner, ii) image-text-pair\nanchor which is retrieved from the dataset similar to pretraining data of CLIP\naccording to the downstream task, associating with the original CLIP text with\nrich semantics. Those anchors are utilized as auxiliary semantic information to\nmaintain the original feature space of CLIP, thereby preserving the OOD\ngeneralization capabilities. Comprehensive experiments demonstrate that our\nmethod achieves in-distribution performance akin to conventional finetuning\nwhile attaining new state-of-the-art results on domain shift and zero-shot\nlearning benchmarks.\n", "link": "http://arxiv.org/abs/2404.06244v1", "date": "2024-04-09", "relevancy": 2.216, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5589}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5588}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5297}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anchor-based%20Robust%20Finetuning%20of%20Vision-Language%20Models&body=Title%3A%20Anchor-based%20Robust%20Finetuning%20of%20Vision-Language%20Models%0AAuthor%3A%20Jinwei%20Han%20and%20Zhiwen%20Lin%20and%20Zhongyisun%20Sun%20and%20Yingguo%20Gao%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Yuan%20Gao%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20We%20aim%20at%20finetuning%20a%20vision-language%20model%20without%20hurting%20its%0Aout-of-distribution%20%28OOD%29%20generalization.%20We%20address%20two%20types%20of%20OOD%0Ageneralization%2C%20i.e.%2C%20i%29%20domain%20shift%20such%20as%20natural%20to%20sketch%20images%2C%20and%20ii%29%0Azero-shot%20capability%20to%20recognize%20the%20category%20that%20was%20not%20contained%20in%20the%0Afinetune%20data.%20Arguably%2C%20the%20diminished%20OOD%20generalization%20after%20finetuning%0Astems%20from%20the%20excessively%20simplified%20finetuning%20target%2C%20which%20only%20provides%0Athe%20class%20information%2C%20such%20as%20%60%60a%20photo%20of%20a%20%5BCLASS%5D%27%27.%20This%20is%20distinct%20from%0Athe%20process%20in%20that%20CLIP%20was%20pretrained%2C%20where%20there%20is%20abundant%20text%0Asupervision%20with%20rich%20semantic%20information.%20Therefore%2C%20we%20propose%20to%20compensate%0Afor%20the%20finetune%20process%20using%20auxiliary%20supervision%20with%20rich%20semantic%0Ainformation%2C%20which%20acts%20as%20anchors%20to%20preserve%20the%20OOD%20generalization.%0ASpecifically%2C%20two%20types%20of%20anchors%20are%20elaborated%20in%20our%20method%2C%20including%20i%29%0Atext-compensated%20anchor%20which%20uses%20the%20images%20from%20the%20finetune%20set%20but%0Aenriches%20the%20text%20supervision%20from%20a%20pretrained%20captioner%2C%20ii%29%20image-text-pair%0Aanchor%20which%20is%20retrieved%20from%20the%20dataset%20similar%20to%20pretraining%20data%20of%20CLIP%0Aaccording%20to%20the%20downstream%20task%2C%20associating%20with%20the%20original%20CLIP%20text%20with%0Arich%20semantics.%20Those%20anchors%20are%20utilized%20as%20auxiliary%20semantic%20information%20to%0Amaintain%20the%20original%20feature%20space%20of%20CLIP%2C%20thereby%20preserving%20the%20OOD%0Ageneralization%20capabilities.%20Comprehensive%20experiments%20demonstrate%20that%20our%0Amethod%20achieves%20in-distribution%20performance%20akin%20to%20conventional%20finetuning%0Awhile%20attaining%20new%20state-of-the-art%20results%20on%20domain%20shift%20and%20zero-shot%0Alearning%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06244v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anchor-based%20Robust%20Finetuning%20of%20Vision-Language%20Models&entry.906535625=Jinwei%20Han%20and%20Zhiwen%20Lin%20and%20Zhongyisun%20Sun%20and%20Yingguo%20Gao%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Yuan%20Gao%20and%20Gui-Song%20Xia&entry.1292438233=%20%20We%20aim%20at%20finetuning%20a%20vision-language%20model%20without%20hurting%20its%0Aout-of-distribution%20%28OOD%29%20generalization.%20We%20address%20two%20types%20of%20OOD%0Ageneralization%2C%20i.e.%2C%20i%29%20domain%20shift%20such%20as%20natural%20to%20sketch%20images%2C%20and%20ii%29%0Azero-shot%20capability%20to%20recognize%20the%20category%20that%20was%20not%20contained%20in%20the%0Afinetune%20data.%20Arguably%2C%20the%20diminished%20OOD%20generalization%20after%20finetuning%0Astems%20from%20the%20excessively%20simplified%20finetuning%20target%2C%20which%20only%20provides%0Athe%20class%20information%2C%20such%20as%20%60%60a%20photo%20of%20a%20%5BCLASS%5D%27%27.%20This%20is%20distinct%20from%0Athe%20process%20in%20that%20CLIP%20was%20pretrained%2C%20where%20there%20is%20abundant%20text%0Asupervision%20with%20rich%20semantic%20information.%20Therefore%2C%20we%20propose%20to%20compensate%0Afor%20the%20finetune%20process%20using%20auxiliary%20supervision%20with%20rich%20semantic%0Ainformation%2C%20which%20acts%20as%20anchors%20to%20preserve%20the%20OOD%20generalization.%0ASpecifically%2C%20two%20types%20of%20anchors%20are%20elaborated%20in%20our%20method%2C%20including%20i%29%0Atext-compensated%20anchor%20which%20uses%20the%20images%20from%20the%20finetune%20set%20but%0Aenriches%20the%20text%20supervision%20from%20a%20pretrained%20captioner%2C%20ii%29%20image-text-pair%0Aanchor%20which%20is%20retrieved%20from%20the%20dataset%20similar%20to%20pretraining%20data%20of%20CLIP%0Aaccording%20to%20the%20downstream%20task%2C%20associating%20with%20the%20original%20CLIP%20text%20with%0Arich%20semantics.%20Those%20anchors%20are%20utilized%20as%20auxiliary%20semantic%20information%20to%0Amaintain%20the%20original%20feature%20space%20of%20CLIP%2C%20thereby%20preserving%20the%20OOD%0Ageneralization%20capabilities.%20Comprehensive%20experiments%20demonstrate%20that%20our%0Amethod%20achieves%20in-distribution%20performance%20akin%20to%20conventional%20finetuning%0Awhile%20attaining%20new%20state-of-the-art%20results%20on%20domain%20shift%20and%20zero-shot%0Alearning%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06244v1&entry.124074799=Read"},
{"title": "VISION2UI: A Real-World Dataset with Layout for Code Generation from UI\n  Designs", "author": "Yi Gui and Zhen Li and Yao Wan and Yemin Shi and Hongyu Zhang and Yi Su and Shaoling Dong and Xing Zhou and Wenbin Jiang", "abstract": "  Automatically generating UI code from webpage design visions can\nsignificantly alleviate the burden of developers, enabling beginner developers\nor designers to directly generate Web pages from design diagrams. Currently,\nprior research has accomplished the objective of generating UI code from\nrudimentary design visions or sketches through designing deep neural networks.\nInspired by the groundbreaking advancements achieved by Multimodal Large\nLanguage Models (MLLMs), the automatic generation of UI code from high-fidelity\ndesign images is now emerging as a viable possibility. Nevertheless, our\ninvestigation reveals that existing MLLMs are hampered by the scarcity of\nauthentic, high-quality, and large-scale datasets, leading to unsatisfactory\nperformance in automated UI code generation. To mitigate this gap, we present a\nnovel dataset, termed VISION2UI, extracted from real-world scenarios, augmented\nwith comprehensive layout information, tailored specifically for finetuning\nMLLMs in UI code generation. Specifically, this dataset is derived through a\nseries of operations, encompassing collecting, cleaning, and filtering of the\nopen-source Common Crawl dataset. In order to uphold its quality, a neural\nscorer trained on labeled samples is utilized to refine the data, retaining\nhigher-quality instances. Ultimately, this process yields a dataset comprising\n2,000 (Much more is coming soon) parallel samples encompassing design visions\nand UI code. The dataset is available at\nhttps://huggingface.co/datasets/xcodemind/vision2ui.\n", "link": "http://arxiv.org/abs/2404.06369v1", "date": "2024-04-09", "relevancy": 2.1905, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5739}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5398}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5244}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VISION2UI%3A%20A%20Real-World%20Dataset%20with%20Layout%20for%20Code%20Generation%20from%20UI%0A%20%20Designs&body=Title%3A%20VISION2UI%3A%20A%20Real-World%20Dataset%20with%20Layout%20for%20Code%20Generation%20from%20UI%0A%20%20Designs%0AAuthor%3A%20Yi%20Gui%20and%20Zhen%20Li%20and%20Yao%20Wan%20and%20Yemin%20Shi%20and%20Hongyu%20Zhang%20and%20Yi%20Su%20and%20Shaoling%20Dong%20and%20Xing%20Zhou%20and%20Wenbin%20Jiang%0AAbstract%3A%20%20%20Automatically%20generating%20UI%20code%20from%20webpage%20design%20visions%20can%0Asignificantly%20alleviate%20the%20burden%20of%20developers%2C%20enabling%20beginner%20developers%0Aor%20designers%20to%20directly%20generate%20Web%20pages%20from%20design%20diagrams.%20Currently%2C%0Aprior%20research%20has%20accomplished%20the%20objective%20of%20generating%20UI%20code%20from%0Arudimentary%20design%20visions%20or%20sketches%20through%20designing%20deep%20neural%20networks.%0AInspired%20by%20the%20groundbreaking%20advancements%20achieved%20by%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%2C%20the%20automatic%20generation%20of%20UI%20code%20from%20high-fidelity%0Adesign%20images%20is%20now%20emerging%20as%20a%20viable%20possibility.%20Nevertheless%2C%20our%0Ainvestigation%20reveals%20that%20existing%20MLLMs%20are%20hampered%20by%20the%20scarcity%20of%0Aauthentic%2C%20high-quality%2C%20and%20large-scale%20datasets%2C%20leading%20to%20unsatisfactory%0Aperformance%20in%20automated%20UI%20code%20generation.%20To%20mitigate%20this%20gap%2C%20we%20present%20a%0Anovel%20dataset%2C%20termed%20VISION2UI%2C%20extracted%20from%20real-world%20scenarios%2C%20augmented%0Awith%20comprehensive%20layout%20information%2C%20tailored%20specifically%20for%20finetuning%0AMLLMs%20in%20UI%20code%20generation.%20Specifically%2C%20this%20dataset%20is%20derived%20through%20a%0Aseries%20of%20operations%2C%20encompassing%20collecting%2C%20cleaning%2C%20and%20filtering%20of%20the%0Aopen-source%20Common%20Crawl%20dataset.%20In%20order%20to%20uphold%20its%20quality%2C%20a%20neural%0Ascorer%20trained%20on%20labeled%20samples%20is%20utilized%20to%20refine%20the%20data%2C%20retaining%0Ahigher-quality%20instances.%20Ultimately%2C%20this%20process%20yields%20a%20dataset%20comprising%0A2%2C000%20%28Much%20more%20is%20coming%20soon%29%20parallel%20samples%20encompassing%20design%20visions%0Aand%20UI%20code.%20The%20dataset%20is%20available%20at%0Ahttps%3A//huggingface.co/datasets/xcodemind/vision2ui.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06369v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VISION2UI%3A%20A%20Real-World%20Dataset%20with%20Layout%20for%20Code%20Generation%20from%20UI%0A%20%20Designs&entry.906535625=Yi%20Gui%20and%20Zhen%20Li%20and%20Yao%20Wan%20and%20Yemin%20Shi%20and%20Hongyu%20Zhang%20and%20Yi%20Su%20and%20Shaoling%20Dong%20and%20Xing%20Zhou%20and%20Wenbin%20Jiang&entry.1292438233=%20%20Automatically%20generating%20UI%20code%20from%20webpage%20design%20visions%20can%0Asignificantly%20alleviate%20the%20burden%20of%20developers%2C%20enabling%20beginner%20developers%0Aor%20designers%20to%20directly%20generate%20Web%20pages%20from%20design%20diagrams.%20Currently%2C%0Aprior%20research%20has%20accomplished%20the%20objective%20of%20generating%20UI%20code%20from%0Arudimentary%20design%20visions%20or%20sketches%20through%20designing%20deep%20neural%20networks.%0AInspired%20by%20the%20groundbreaking%20advancements%20achieved%20by%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%2C%20the%20automatic%20generation%20of%20UI%20code%20from%20high-fidelity%0Adesign%20images%20is%20now%20emerging%20as%20a%20viable%20possibility.%20Nevertheless%2C%20our%0Ainvestigation%20reveals%20that%20existing%20MLLMs%20are%20hampered%20by%20the%20scarcity%20of%0Aauthentic%2C%20high-quality%2C%20and%20large-scale%20datasets%2C%20leading%20to%20unsatisfactory%0Aperformance%20in%20automated%20UI%20code%20generation.%20To%20mitigate%20this%20gap%2C%20we%20present%20a%0Anovel%20dataset%2C%20termed%20VISION2UI%2C%20extracted%20from%20real-world%20scenarios%2C%20augmented%0Awith%20comprehensive%20layout%20information%2C%20tailored%20specifically%20for%20finetuning%0AMLLMs%20in%20UI%20code%20generation.%20Specifically%2C%20this%20dataset%20is%20derived%20through%20a%0Aseries%20of%20operations%2C%20encompassing%20collecting%2C%20cleaning%2C%20and%20filtering%20of%20the%0Aopen-source%20Common%20Crawl%20dataset.%20In%20order%20to%20uphold%20its%20quality%2C%20a%20neural%0Ascorer%20trained%20on%20labeled%20samples%20is%20utilized%20to%20refine%20the%20data%2C%20retaining%0Ahigher-quality%20instances.%20Ultimately%2C%20this%20process%20yields%20a%20dataset%20comprising%0A2%2C000%20%28Much%20more%20is%20coming%20soon%29%20parallel%20samples%20encompassing%20design%20visions%0Aand%20UI%20code.%20The%20dataset%20is%20available%20at%0Ahttps%3A//huggingface.co/datasets/xcodemind/vision2ui.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06369v1&entry.124074799=Read"},
{"title": "Hyperparameter Selection in Continual Learning", "author": "Thomas L. Lee and Sigrid Passano Hellan and Linus Ericsson and Elliot J. Crowley and Amos Storkey", "abstract": "  In continual learning (CL) -- where a learner trains on a stream of data --\nstandard hyperparameter optimisation (HPO) cannot be applied, as a learner does\nnot have access to all of the data at the same time. This has prompted the\ndevelopment of CL-specific HPO frameworks. The most popular way to tune\nhyperparameters in CL is to repeatedly train over the whole data stream with\ndifferent hyperparameter settings. However, this end-of-training HPO is\nunrealistic as in practice a learner can only see the stream once. Hence, there\nis an open question: what HPO framework should a practitioner use for a CL\nproblem in reality? This paper answers this question by evaluating several\nrealistic HPO frameworks. We find that all the HPO frameworks considered,\nincluding end-of-training HPO, perform similarly. We therefore advocate using\nthe realistic and most computationally efficient method: fitting the\nhyperparameters on the first task and then fixing them throughout training.\n", "link": "http://arxiv.org/abs/2404.06466v1", "date": "2024-04-09", "relevancy": 2.1758, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4668}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4244}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4143}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hyperparameter%20Selection%20in%20Continual%20Learning&body=Title%3A%20Hyperparameter%20Selection%20in%20Continual%20Learning%0AAuthor%3A%20Thomas%20L.%20Lee%20and%20Sigrid%20Passano%20Hellan%20and%20Linus%20Ericsson%20and%20Elliot%20J.%20Crowley%20and%20Amos%20Storkey%0AAbstract%3A%20%20%20In%20continual%20learning%20%28CL%29%20--%20where%20a%20learner%20trains%20on%20a%20stream%20of%20data%20--%0Astandard%20hyperparameter%20optimisation%20%28HPO%29%20cannot%20be%20applied%2C%20as%20a%20learner%20does%0Anot%20have%20access%20to%20all%20of%20the%20data%20at%20the%20same%20time.%20This%20has%20prompted%20the%0Adevelopment%20of%20CL-specific%20HPO%20frameworks.%20The%20most%20popular%20way%20to%20tune%0Ahyperparameters%20in%20CL%20is%20to%20repeatedly%20train%20over%20the%20whole%20data%20stream%20with%0Adifferent%20hyperparameter%20settings.%20However%2C%20this%20end-of-training%20HPO%20is%0Aunrealistic%20as%20in%20practice%20a%20learner%20can%20only%20see%20the%20stream%20once.%20Hence%2C%20there%0Ais%20an%20open%20question%3A%20what%20HPO%20framework%20should%20a%20practitioner%20use%20for%20a%20CL%0Aproblem%20in%20reality%3F%20This%20paper%20answers%20this%20question%20by%20evaluating%20several%0Arealistic%20HPO%20frameworks.%20We%20find%20that%20all%20the%20HPO%20frameworks%20considered%2C%0Aincluding%20end-of-training%20HPO%2C%20perform%20similarly.%20We%20therefore%20advocate%20using%0Athe%20realistic%20and%20most%20computationally%20efficient%20method%3A%20fitting%20the%0Ahyperparameters%20on%20the%20first%20task%20and%20then%20fixing%20them%20throughout%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06466v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperparameter%20Selection%20in%20Continual%20Learning&entry.906535625=Thomas%20L.%20Lee%20and%20Sigrid%20Passano%20Hellan%20and%20Linus%20Ericsson%20and%20Elliot%20J.%20Crowley%20and%20Amos%20Storkey&entry.1292438233=%20%20In%20continual%20learning%20%28CL%29%20--%20where%20a%20learner%20trains%20on%20a%20stream%20of%20data%20--%0Astandard%20hyperparameter%20optimisation%20%28HPO%29%20cannot%20be%20applied%2C%20as%20a%20learner%20does%0Anot%20have%20access%20to%20all%20of%20the%20data%20at%20the%20same%20time.%20This%20has%20prompted%20the%0Adevelopment%20of%20CL-specific%20HPO%20frameworks.%20The%20most%20popular%20way%20to%20tune%0Ahyperparameters%20in%20CL%20is%20to%20repeatedly%20train%20over%20the%20whole%20data%20stream%20with%0Adifferent%20hyperparameter%20settings.%20However%2C%20this%20end-of-training%20HPO%20is%0Aunrealistic%20as%20in%20practice%20a%20learner%20can%20only%20see%20the%20stream%20once.%20Hence%2C%20there%0Ais%20an%20open%20question%3A%20what%20HPO%20framework%20should%20a%20practitioner%20use%20for%20a%20CL%0Aproblem%20in%20reality%3F%20This%20paper%20answers%20this%20question%20by%20evaluating%20several%0Arealistic%20HPO%20frameworks.%20We%20find%20that%20all%20the%20HPO%20frameworks%20considered%2C%0Aincluding%20end-of-training%20HPO%2C%20perform%20similarly.%20We%20therefore%20advocate%20using%0Athe%20realistic%20and%20most%20computationally%20efficient%20method%3A%20fitting%20the%0Ahyperparameters%20on%20the%20first%20task%20and%20then%20fixing%20them%20throughout%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06466v1&entry.124074799=Read"},
{"title": "Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot\n  Medical Image Segmentation", "author": "Sidra Aleem and Fangyijie Wang and Mayug Maniparambil and Eric Arazo and Julia Dietlmeier and Kathleen Curran and Noel E. O'Connor and Suzanne Little", "abstract": "  The Segment Anything Model (SAM) and CLIP are remarkable vision foundation\nmodels (VFMs). SAM, a prompt driven segmentation model, excels in segmentation\ntasks across diverse domains, while CLIP is renowned for its zero shot\nrecognition capabilities. However, their unified potential has not yet been\nexplored in medical image segmentation. To adapt SAM to medical imaging,\nexisting methods primarily rely on tuning strategies that require extensive\ndata or prior prompts tailored to the specific task, making it particularly\nchallenging when only a limited number of data samples are available. This work\npresents an in depth exploration of integrating SAM and CLIP into a unified\nframework for medical image segmentation. Specifically, we propose a simple\nunified framework, SaLIP, for organ segmentation. Initially, SAM is used for\npart based segmentation within the image, followed by CLIP to retrieve the mask\ncorresponding to the region of interest (ROI) from the pool of SAM generated\nmasks. Finally, SAM is prompted by the retrieved ROI to segment a specific\norgan. Thus, SaLIP is training and fine tuning free and does not rely on domain\nexpertise or labeled data for prompt engineering. Our method shows substantial\nenhancements in zero shot segmentation, showcasing notable improvements in DICE\nscores across diverse segmentation tasks like brain (63.46%), lung (50.11%),\nand fetal head (30.82%), when compared to un prompted SAM. Code and text\nprompts will be available online.\n", "link": "http://arxiv.org/abs/2404.06362v1", "date": "2024-04-09", "relevancy": 2.1755, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.593}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5124}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5073}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Adaptation%20with%20SaLIP%3A%20A%20Cascade%20of%20SAM%20and%20CLIP%20for%20Zero%20shot%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20Test-Time%20Adaptation%20with%20SaLIP%3A%20A%20Cascade%20of%20SAM%20and%20CLIP%20for%20Zero%20shot%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Sidra%20Aleem%20and%20Fangyijie%20Wang%20and%20Mayug%20Maniparambil%20and%20Eric%20Arazo%20and%20Julia%20Dietlmeier%20and%20Kathleen%20Curran%20and%20Noel%20E.%20O%27Connor%20and%20Suzanne%20Little%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20and%20CLIP%20are%20remarkable%20vision%20foundation%0Amodels%20%28VFMs%29.%20SAM%2C%20a%20prompt%20driven%20segmentation%20model%2C%20excels%20in%20segmentation%0Atasks%20across%20diverse%20domains%2C%20while%20CLIP%20is%20renowned%20for%20its%20zero%20shot%0Arecognition%20capabilities.%20However%2C%20their%20unified%20potential%20has%20not%20yet%20been%0Aexplored%20in%20medical%20image%20segmentation.%20To%20adapt%20SAM%20to%20medical%20imaging%2C%0Aexisting%20methods%20primarily%20rely%20on%20tuning%20strategies%20that%20require%20extensive%0Adata%20or%20prior%20prompts%20tailored%20to%20the%20specific%20task%2C%20making%20it%20particularly%0Achallenging%20when%20only%20a%20limited%20number%20of%20data%20samples%20are%20available.%20This%20work%0Apresents%20an%20in%20depth%20exploration%20of%20integrating%20SAM%20and%20CLIP%20into%20a%20unified%0Aframework%20for%20medical%20image%20segmentation.%20Specifically%2C%20we%20propose%20a%20simple%0Aunified%20framework%2C%20SaLIP%2C%20for%20organ%20segmentation.%20Initially%2C%20SAM%20is%20used%20for%0Apart%20based%20segmentation%20within%20the%20image%2C%20followed%20by%20CLIP%20to%20retrieve%20the%20mask%0Acorresponding%20to%20the%20region%20of%20interest%20%28ROI%29%20from%20the%20pool%20of%20SAM%20generated%0Amasks.%20Finally%2C%20SAM%20is%20prompted%20by%20the%20retrieved%20ROI%20to%20segment%20a%20specific%0Aorgan.%20Thus%2C%20SaLIP%20is%20training%20and%20fine%20tuning%20free%20and%20does%20not%20rely%20on%20domain%0Aexpertise%20or%20labeled%20data%20for%20prompt%20engineering.%20Our%20method%20shows%20substantial%0Aenhancements%20in%20zero%20shot%20segmentation%2C%20showcasing%20notable%20improvements%20in%20DICE%0Ascores%20across%20diverse%20segmentation%20tasks%20like%20brain%20%2863.46%25%29%2C%20lung%20%2850.11%25%29%2C%0Aand%20fetal%20head%20%2830.82%25%29%2C%20when%20compared%20to%20un%20prompted%20SAM.%20Code%20and%20text%0Aprompts%20will%20be%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06362v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Adaptation%20with%20SaLIP%3A%20A%20Cascade%20of%20SAM%20and%20CLIP%20for%20Zero%20shot%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Sidra%20Aleem%20and%20Fangyijie%20Wang%20and%20Mayug%20Maniparambil%20and%20Eric%20Arazo%20and%20Julia%20Dietlmeier%20and%20Kathleen%20Curran%20and%20Noel%20E.%20O%27Connor%20and%20Suzanne%20Little&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20and%20CLIP%20are%20remarkable%20vision%20foundation%0Amodels%20%28VFMs%29.%20SAM%2C%20a%20prompt%20driven%20segmentation%20model%2C%20excels%20in%20segmentation%0Atasks%20across%20diverse%20domains%2C%20while%20CLIP%20is%20renowned%20for%20its%20zero%20shot%0Arecognition%20capabilities.%20However%2C%20their%20unified%20potential%20has%20not%20yet%20been%0Aexplored%20in%20medical%20image%20segmentation.%20To%20adapt%20SAM%20to%20medical%20imaging%2C%0Aexisting%20methods%20primarily%20rely%20on%20tuning%20strategies%20that%20require%20extensive%0Adata%20or%20prior%20prompts%20tailored%20to%20the%20specific%20task%2C%20making%20it%20particularly%0Achallenging%20when%20only%20a%20limited%20number%20of%20data%20samples%20are%20available.%20This%20work%0Apresents%20an%20in%20depth%20exploration%20of%20integrating%20SAM%20and%20CLIP%20into%20a%20unified%0Aframework%20for%20medical%20image%20segmentation.%20Specifically%2C%20we%20propose%20a%20simple%0Aunified%20framework%2C%20SaLIP%2C%20for%20organ%20segmentation.%20Initially%2C%20SAM%20is%20used%20for%0Apart%20based%20segmentation%20within%20the%20image%2C%20followed%20by%20CLIP%20to%20retrieve%20the%20mask%0Acorresponding%20to%20the%20region%20of%20interest%20%28ROI%29%20from%20the%20pool%20of%20SAM%20generated%0Amasks.%20Finally%2C%20SAM%20is%20prompted%20by%20the%20retrieved%20ROI%20to%20segment%20a%20specific%0Aorgan.%20Thus%2C%20SaLIP%20is%20training%20and%20fine%20tuning%20free%20and%20does%20not%20rely%20on%20domain%0Aexpertise%20or%20labeled%20data%20for%20prompt%20engineering.%20Our%20method%20shows%20substantial%0Aenhancements%20in%20zero%20shot%20segmentation%2C%20showcasing%20notable%20improvements%20in%20DICE%0Ascores%20across%20diverse%20segmentation%20tasks%20like%20brain%20%2863.46%25%29%2C%20lung%20%2850.11%25%29%2C%0Aand%20fetal%20head%20%2830.82%25%29%2C%20when%20compared%20to%20un%20prompted%20SAM.%20Code%20and%20text%0Aprompts%20will%20be%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06362v1&entry.124074799=Read"},
{"title": "DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird's Eye View\n  Segmentation with Occlusion Reasoning", "author": "Senthil Yogamani and David Unger and Venkatraman Narayanan and Varun Ravi Kumar", "abstract": "  Semantic segmentation is an effective way to perform scene understanding.\nRecently, segmentation in 3D Bird's Eye View (BEV) space has become popular as\nits directly used by drive policy. However, there is limited work on BEV\nsegmentation for surround-view fisheye cameras, commonly used in commercial\nvehicles. As this task has no real-world public dataset and existing synthetic\ndatasets do not handle amodal regions due to occlusion, we create a synthetic\ndataset using the Cognata simulator comprising diverse road types, weather, and\nlighting conditions. We generalize the BEV segmentation to work with any camera\nmodel; this is useful for mixing diverse cameras. We implement a baseline by\napplying cylindrical rectification on the fisheye images and using a standard\nLSS-based BEV segmentation model. We demonstrate that we can achieve better\nperformance without undistortion, which has the adverse effects of increased\nruntime due to pre-processing, reduced field-of-view, and resampling artifacts.\nFurther, we introduce a distortion-aware learnable BEV pooling strategy that is\nmore effective for the fisheye cameras. We extend the model with an occlusion\nreasoning module, which is critical for estimating in BEV space. Qualitative\nperformance of DaF-BEVSeg is showcased in the video at\nhttps://streamable.com/ge4v51.\n", "link": "http://arxiv.org/abs/2404.06352v1", "date": "2024-04-09", "relevancy": 2.1718, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5547}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5445}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5367}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DaF-BEVSeg%3A%20Distortion-aware%20Fisheye%20Camera%20based%20Bird%27s%20Eye%20View%0A%20%20Segmentation%20with%20Occlusion%20Reasoning&body=Title%3A%20DaF-BEVSeg%3A%20Distortion-aware%20Fisheye%20Camera%20based%20Bird%27s%20Eye%20View%0A%20%20Segmentation%20with%20Occlusion%20Reasoning%0AAuthor%3A%20Senthil%20Yogamani%20and%20David%20Unger%20and%20Venkatraman%20Narayanan%20and%20Varun%20Ravi%20Kumar%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20an%20effective%20way%20to%20perform%20scene%20understanding.%0ARecently%2C%20segmentation%20in%203D%20Bird%27s%20Eye%20View%20%28BEV%29%20space%20has%20become%20popular%20as%0Aits%20directly%20used%20by%20drive%20policy.%20However%2C%20there%20is%20limited%20work%20on%20BEV%0Asegmentation%20for%20surround-view%20fisheye%20cameras%2C%20commonly%20used%20in%20commercial%0Avehicles.%20As%20this%20task%20has%20no%20real-world%20public%20dataset%20and%20existing%20synthetic%0Adatasets%20do%20not%20handle%20amodal%20regions%20due%20to%20occlusion%2C%20we%20create%20a%20synthetic%0Adataset%20using%20the%20Cognata%20simulator%20comprising%20diverse%20road%20types%2C%20weather%2C%20and%0Alighting%20conditions.%20We%20generalize%20the%20BEV%20segmentation%20to%20work%20with%20any%20camera%0Amodel%3B%20this%20is%20useful%20for%20mixing%20diverse%20cameras.%20We%20implement%20a%20baseline%20by%0Aapplying%20cylindrical%20rectification%20on%20the%20fisheye%20images%20and%20using%20a%20standard%0ALSS-based%20BEV%20segmentation%20model.%20We%20demonstrate%20that%20we%20can%20achieve%20better%0Aperformance%20without%20undistortion%2C%20which%20has%20the%20adverse%20effects%20of%20increased%0Aruntime%20due%20to%20pre-processing%2C%20reduced%20field-of-view%2C%20and%20resampling%20artifacts.%0AFurther%2C%20we%20introduce%20a%20distortion-aware%20learnable%20BEV%20pooling%20strategy%20that%20is%0Amore%20effective%20for%20the%20fisheye%20cameras.%20We%20extend%20the%20model%20with%20an%20occlusion%0Areasoning%20module%2C%20which%20is%20critical%20for%20estimating%20in%20BEV%20space.%20Qualitative%0Aperformance%20of%20DaF-BEVSeg%20is%20showcased%20in%20the%20video%20at%0Ahttps%3A//streamable.com/ge4v51.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06352v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DaF-BEVSeg%3A%20Distortion-aware%20Fisheye%20Camera%20based%20Bird%27s%20Eye%20View%0A%20%20Segmentation%20with%20Occlusion%20Reasoning&entry.906535625=Senthil%20Yogamani%20and%20David%20Unger%20and%20Venkatraman%20Narayanan%20and%20Varun%20Ravi%20Kumar&entry.1292438233=%20%20Semantic%20segmentation%20is%20an%20effective%20way%20to%20perform%20scene%20understanding.%0ARecently%2C%20segmentation%20in%203D%20Bird%27s%20Eye%20View%20%28BEV%29%20space%20has%20become%20popular%20as%0Aits%20directly%20used%20by%20drive%20policy.%20However%2C%20there%20is%20limited%20work%20on%20BEV%0Asegmentation%20for%20surround-view%20fisheye%20cameras%2C%20commonly%20used%20in%20commercial%0Avehicles.%20As%20this%20task%20has%20no%20real-world%20public%20dataset%20and%20existing%20synthetic%0Adatasets%20do%20not%20handle%20amodal%20regions%20due%20to%20occlusion%2C%20we%20create%20a%20synthetic%0Adataset%20using%20the%20Cognata%20simulator%20comprising%20diverse%20road%20types%2C%20weather%2C%20and%0Alighting%20conditions.%20We%20generalize%20the%20BEV%20segmentation%20to%20work%20with%20any%20camera%0Amodel%3B%20this%20is%20useful%20for%20mixing%20diverse%20cameras.%20We%20implement%20a%20baseline%20by%0Aapplying%20cylindrical%20rectification%20on%20the%20fisheye%20images%20and%20using%20a%20standard%0ALSS-based%20BEV%20segmentation%20model.%20We%20demonstrate%20that%20we%20can%20achieve%20better%0Aperformance%20without%20undistortion%2C%20which%20has%20the%20adverse%20effects%20of%20increased%0Aruntime%20due%20to%20pre-processing%2C%20reduced%20field-of-view%2C%20and%20resampling%20artifacts.%0AFurther%2C%20we%20introduce%20a%20distortion-aware%20learnable%20BEV%20pooling%20strategy%20that%20is%0Amore%20effective%20for%20the%20fisheye%20cameras.%20We%20extend%20the%20model%20with%20an%20occlusion%0Areasoning%20module%2C%20which%20is%20critical%20for%20estimating%20in%20BEV%20space.%20Qualitative%0Aperformance%20of%20DaF-BEVSeg%20is%20showcased%20in%20the%20video%20at%0Ahttps%3A//streamable.com/ge4v51.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06352v1&entry.124074799=Read"},
{"title": "Offline Supervised Learning V.S. Online Direct Policy Optimization: A\n  Comparative Study and A Unified Training Paradigm for Neural Network-Based\n  Optimal Feedback Control", "author": "Yue Zhao and Jiequn Han", "abstract": "  This work is concerned with solving neural network-based feedback controllers\nefficiently for optimal control problems. We first conduct a comparative study\nof two prevalent approaches: offline supervised learning and online direct\npolicy optimization. Albeit the training part of the supervised learning\napproach is relatively easy, the success of the method heavily depends on the\noptimal control dataset generated by open-loop optimal control solvers. In\ncontrast, direct policy optimization turns the optimal control problem into an\noptimization problem directly without any requirement of pre-computing, but the\ndynamics-related objective can be hard to optimize when the problem is\ncomplicated. Our results underscore the superiority of offline supervised\nlearning in terms of both optimality and training time. To overcome the main\nchallenges, dataset and optimization, in the two approaches respectively, we\ncomplement them and propose the Pre-train and Fine-tune strategy as a unified\ntraining paradigm for optimal feedback control, which further improves the\nperformance and robustness significantly. Our code is accessible at\nhttps://github.com/yzhao98/DeepOptimalControl.\n", "link": "http://arxiv.org/abs/2211.15930v3", "date": "2024-04-09", "relevancy": 2.1713, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5882}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5209}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4843}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Offline%20Supervised%20Learning%20V.S.%20Online%20Direct%20Policy%20Optimization%3A%20A%0A%20%20Comparative%20Study%20and%20A%20Unified%20Training%20Paradigm%20for%20Neural%20Network-Based%0A%20%20Optimal%20Feedback%20Control&body=Title%3A%20Offline%20Supervised%20Learning%20V.S.%20Online%20Direct%20Policy%20Optimization%3A%20A%0A%20%20Comparative%20Study%20and%20A%20Unified%20Training%20Paradigm%20for%20Neural%20Network-Based%0A%20%20Optimal%20Feedback%20Control%0AAuthor%3A%20Yue%20Zhao%20and%20Jiequn%20Han%0AAbstract%3A%20%20%20This%20work%20is%20concerned%20with%20solving%20neural%20network-based%20feedback%20controllers%0Aefficiently%20for%20optimal%20control%20problems.%20We%20first%20conduct%20a%20comparative%20study%0Aof%20two%20prevalent%20approaches%3A%20offline%20supervised%20learning%20and%20online%20direct%0Apolicy%20optimization.%20Albeit%20the%20training%20part%20of%20the%20supervised%20learning%0Aapproach%20is%20relatively%20easy%2C%20the%20success%20of%20the%20method%20heavily%20depends%20on%20the%0Aoptimal%20control%20dataset%20generated%20by%20open-loop%20optimal%20control%20solvers.%20In%0Acontrast%2C%20direct%20policy%20optimization%20turns%20the%20optimal%20control%20problem%20into%20an%0Aoptimization%20problem%20directly%20without%20any%20requirement%20of%20pre-computing%2C%20but%20the%0Adynamics-related%20objective%20can%20be%20hard%20to%20optimize%20when%20the%20problem%20is%0Acomplicated.%20Our%20results%20underscore%20the%20superiority%20of%20offline%20supervised%0Alearning%20in%20terms%20of%20both%20optimality%20and%20training%20time.%20To%20overcome%20the%20main%0Achallenges%2C%20dataset%20and%20optimization%2C%20in%20the%20two%20approaches%20respectively%2C%20we%0Acomplement%20them%20and%20propose%20the%20Pre-train%20and%20Fine-tune%20strategy%20as%20a%20unified%0Atraining%20paradigm%20for%20optimal%20feedback%20control%2C%20which%20further%20improves%20the%0Aperformance%20and%20robustness%20significantly.%20Our%20code%20is%20accessible%20at%0Ahttps%3A//github.com/yzhao98/DeepOptimalControl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.15930v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Offline%20Supervised%20Learning%20V.S.%20Online%20Direct%20Policy%20Optimization%3A%20A%0A%20%20Comparative%20Study%20and%20A%20Unified%20Training%20Paradigm%20for%20Neural%20Network-Based%0A%20%20Optimal%20Feedback%20Control&entry.906535625=Yue%20Zhao%20and%20Jiequn%20Han&entry.1292438233=%20%20This%20work%20is%20concerned%20with%20solving%20neural%20network-based%20feedback%20controllers%0Aefficiently%20for%20optimal%20control%20problems.%20We%20first%20conduct%20a%20comparative%20study%0Aof%20two%20prevalent%20approaches%3A%20offline%20supervised%20learning%20and%20online%20direct%0Apolicy%20optimization.%20Albeit%20the%20training%20part%20of%20the%20supervised%20learning%0Aapproach%20is%20relatively%20easy%2C%20the%20success%20of%20the%20method%20heavily%20depends%20on%20the%0Aoptimal%20control%20dataset%20generated%20by%20open-loop%20optimal%20control%20solvers.%20In%0Acontrast%2C%20direct%20policy%20optimization%20turns%20the%20optimal%20control%20problem%20into%20an%0Aoptimization%20problem%20directly%20without%20any%20requirement%20of%20pre-computing%2C%20but%20the%0Adynamics-related%20objective%20can%20be%20hard%20to%20optimize%20when%20the%20problem%20is%0Acomplicated.%20Our%20results%20underscore%20the%20superiority%20of%20offline%20supervised%0Alearning%20in%20terms%20of%20both%20optimality%20and%20training%20time.%20To%20overcome%20the%20main%0Achallenges%2C%20dataset%20and%20optimization%2C%20in%20the%20two%20approaches%20respectively%2C%20we%0Acomplement%20them%20and%20propose%20the%20Pre-train%20and%20Fine-tune%20strategy%20as%20a%20unified%0Atraining%20paradigm%20for%20optimal%20feedback%20control%2C%20which%20further%20improves%20the%0Aperformance%20and%20robustness%20significantly.%20Our%20code%20is%20accessible%20at%0Ahttps%3A//github.com/yzhao98/DeepOptimalControl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.15930v3&entry.124074799=Read"},
{"title": "Reconstructing Hand-Held Objects in 3D", "author": "Jane Wu and Georgios Pavlakos and Georgia Gkioxari and Jitendra Malik", "abstract": "  Objects manipulated by the hand (i.e., manipulanda) are particularly\nchallenging to reconstruct from in-the-wild RGB images or videos. Not only does\nthe hand occlude much of the object, but also the object is often only visible\nin a small number of image pixels. At the same time, two strong anchors emerge\nin this setting: (1) estimated 3D hands help disambiguate the location and\nscale of the object, and (2) the set of manipulanda is small relative to all\npossible objects. With these insights in mind, we present a scalable paradigm\nfor handheld object reconstruction that builds on recent breakthroughs in large\nlanguage/vision models and 3D object datasets. Our model, MCC-Hand-Object\n(MCC-HO), jointly reconstructs hand and object geometry given a single RGB\nimage and inferred 3D hand as inputs. Subsequently, we use GPT-4(V) to retrieve\na 3D object model that matches the object in the image and rigidly align the\nmodel to the network-inferred geometry; we call this alignment\nRetrieval-Augmented Reconstruction (RAR). Experiments demonstrate that MCC-HO\nachieves state-of-the-art performance on lab and Internet datasets, and we show\nhow RAR can be used to automatically obtain 3D labels for in-the-wild images of\nhand-object interactions.\n", "link": "http://arxiv.org/abs/2404.06507v1", "date": "2024-04-09", "relevancy": 2.1677, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5563}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5351}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20Hand-Held%20Objects%20in%203D&body=Title%3A%20Reconstructing%20Hand-Held%20Objects%20in%203D%0AAuthor%3A%20Jane%20Wu%20and%20Georgios%20Pavlakos%20and%20Georgia%20Gkioxari%20and%20Jitendra%20Malik%0AAbstract%3A%20%20%20Objects%20manipulated%20by%20the%20hand%20%28i.e.%2C%20manipulanda%29%20are%20particularly%0Achallenging%20to%20reconstruct%20from%20in-the-wild%20RGB%20images%20or%20videos.%20Not%20only%20does%0Athe%20hand%20occlude%20much%20of%20the%20object%2C%20but%20also%20the%20object%20is%20often%20only%20visible%0Ain%20a%20small%20number%20of%20image%20pixels.%20At%20the%20same%20time%2C%20two%20strong%20anchors%20emerge%0Ain%20this%20setting%3A%20%281%29%20estimated%203D%20hands%20help%20disambiguate%20the%20location%20and%0Ascale%20of%20the%20object%2C%20and%20%282%29%20the%20set%20of%20manipulanda%20is%20small%20relative%20to%20all%0Apossible%20objects.%20With%20these%20insights%20in%20mind%2C%20we%20present%20a%20scalable%20paradigm%0Afor%20handheld%20object%20reconstruction%20that%20builds%20on%20recent%20breakthroughs%20in%20large%0Alanguage/vision%20models%20and%203D%20object%20datasets.%20Our%20model%2C%20MCC-Hand-Object%0A%28MCC-HO%29%2C%20jointly%20reconstructs%20hand%20and%20object%20geometry%20given%20a%20single%20RGB%0Aimage%20and%20inferred%203D%20hand%20as%20inputs.%20Subsequently%2C%20we%20use%20GPT-4%28V%29%20to%20retrieve%0Aa%203D%20object%20model%20that%20matches%20the%20object%20in%20the%20image%20and%20rigidly%20align%20the%0Amodel%20to%20the%20network-inferred%20geometry%3B%20we%20call%20this%20alignment%0ARetrieval-Augmented%20Reconstruction%20%28RAR%29.%20Experiments%20demonstrate%20that%20MCC-HO%0Aachieves%20state-of-the-art%20performance%20on%20lab%20and%20Internet%20datasets%2C%20and%20we%20show%0Ahow%20RAR%20can%20be%20used%20to%20automatically%20obtain%203D%20labels%20for%20in-the-wild%20images%20of%0Ahand-object%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06507v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20Hand-Held%20Objects%20in%203D&entry.906535625=Jane%20Wu%20and%20Georgios%20Pavlakos%20and%20Georgia%20Gkioxari%20and%20Jitendra%20Malik&entry.1292438233=%20%20Objects%20manipulated%20by%20the%20hand%20%28i.e.%2C%20manipulanda%29%20are%20particularly%0Achallenging%20to%20reconstruct%20from%20in-the-wild%20RGB%20images%20or%20videos.%20Not%20only%20does%0Athe%20hand%20occlude%20much%20of%20the%20object%2C%20but%20also%20the%20object%20is%20often%20only%20visible%0Ain%20a%20small%20number%20of%20image%20pixels.%20At%20the%20same%20time%2C%20two%20strong%20anchors%20emerge%0Ain%20this%20setting%3A%20%281%29%20estimated%203D%20hands%20help%20disambiguate%20the%20location%20and%0Ascale%20of%20the%20object%2C%20and%20%282%29%20the%20set%20of%20manipulanda%20is%20small%20relative%20to%20all%0Apossible%20objects.%20With%20these%20insights%20in%20mind%2C%20we%20present%20a%20scalable%20paradigm%0Afor%20handheld%20object%20reconstruction%20that%20builds%20on%20recent%20breakthroughs%20in%20large%0Alanguage/vision%20models%20and%203D%20object%20datasets.%20Our%20model%2C%20MCC-Hand-Object%0A%28MCC-HO%29%2C%20jointly%20reconstructs%20hand%20and%20object%20geometry%20given%20a%20single%20RGB%0Aimage%20and%20inferred%203D%20hand%20as%20inputs.%20Subsequently%2C%20we%20use%20GPT-4%28V%29%20to%20retrieve%0Aa%203D%20object%20model%20that%20matches%20the%20object%20in%20the%20image%20and%20rigidly%20align%20the%0Amodel%20to%20the%20network-inferred%20geometry%3B%20we%20call%20this%20alignment%0ARetrieval-Augmented%20Reconstruction%20%28RAR%29.%20Experiments%20demonstrate%20that%20MCC-HO%0Aachieves%20state-of-the-art%20performance%20on%20lab%20and%20Internet%20datasets%2C%20and%20we%20show%0Ahow%20RAR%20can%20be%20used%20to%20automatically%20obtain%203D%20labels%20for%20in-the-wild%20images%20of%0Ahand-object%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06507v1&entry.124074799=Read"},
{"title": "Fortifying Fully Convolutional Generative Adversarial Networks for Image\n  Super-Resolution Using Divergence Measures", "author": "Arkaprabha Basu and Kushal Bose and Sankha Subhra Mullick and Anish Chakrabarty and Swagatam Das", "abstract": "  Super-Resolution (SR) is a time-hallowed image processing problem that aims\nto improve the quality of a Low-Resolution (LR) sample up to the standard of\nits High-Resolution (HR) counterpart. We aim to address this by introducing\nSuper-Resolution Generator (SuRGe), a fully-convolutional Generative\nAdversarial Network (GAN)-based architecture for SR. We show that distinct\nconvolutional features obtained at increasing depths of a GAN generator can be\noptimally combined by a set of learnable convex weights to improve the quality\nof generated SR samples. In the process, we employ the Jensen-Shannon and the\nGromov-Wasserstein losses respectively between the SR-HR and LR-SR pairs of\ndistributions to further aid the generator of SuRGe to better exploit the\navailable information in an attempt to improve SR. Moreover, we train the\ndiscriminator of SuRGe with the Wasserstein loss with gradient penalty, to\nprimarily prevent mode collapse. The proposed SuRGe, as an end-to-end GAN\nworkflow tailor-made for super-resolution, offers improved performance while\nmaintaining low inference time. The efficacy of SuRGe is substantiated by its\nsuperior performance compared to 18 state-of-the-art contenders on 10 benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2404.06294v1", "date": "2024-04-09", "relevancy": 2.1669, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5624}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5528}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5224}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fortifying%20Fully%20Convolutional%20Generative%20Adversarial%20Networks%20for%20Image%0A%20%20Super-Resolution%20Using%20Divergence%20Measures&body=Title%3A%20Fortifying%20Fully%20Convolutional%20Generative%20Adversarial%20Networks%20for%20Image%0A%20%20Super-Resolution%20Using%20Divergence%20Measures%0AAuthor%3A%20Arkaprabha%20Basu%20and%20Kushal%20Bose%20and%20Sankha%20Subhra%20Mullick%20and%20Anish%20Chakrabarty%20and%20Swagatam%20Das%0AAbstract%3A%20%20%20Super-Resolution%20%28SR%29%20is%20a%20time-hallowed%20image%20processing%20problem%20that%20aims%0Ato%20improve%20the%20quality%20of%20a%20Low-Resolution%20%28LR%29%20sample%20up%20to%20the%20standard%20of%0Aits%20High-Resolution%20%28HR%29%20counterpart.%20We%20aim%20to%20address%20this%20by%20introducing%0ASuper-Resolution%20Generator%20%28SuRGe%29%2C%20a%20fully-convolutional%20Generative%0AAdversarial%20Network%20%28GAN%29-based%20architecture%20for%20SR.%20We%20show%20that%20distinct%0Aconvolutional%20features%20obtained%20at%20increasing%20depths%20of%20a%20GAN%20generator%20can%20be%0Aoptimally%20combined%20by%20a%20set%20of%20learnable%20convex%20weights%20to%20improve%20the%20quality%0Aof%20generated%20SR%20samples.%20In%20the%20process%2C%20we%20employ%20the%20Jensen-Shannon%20and%20the%0AGromov-Wasserstein%20losses%20respectively%20between%20the%20SR-HR%20and%20LR-SR%20pairs%20of%0Adistributions%20to%20further%20aid%20the%20generator%20of%20SuRGe%20to%20better%20exploit%20the%0Aavailable%20information%20in%20an%20attempt%20to%20improve%20SR.%20Moreover%2C%20we%20train%20the%0Adiscriminator%20of%20SuRGe%20with%20the%20Wasserstein%20loss%20with%20gradient%20penalty%2C%20to%0Aprimarily%20prevent%20mode%20collapse.%20The%20proposed%20SuRGe%2C%20as%20an%20end-to-end%20GAN%0Aworkflow%20tailor-made%20for%20super-resolution%2C%20offers%20improved%20performance%20while%0Amaintaining%20low%20inference%20time.%20The%20efficacy%20of%20SuRGe%20is%20substantiated%20by%20its%0Asuperior%20performance%20compared%20to%2018%20state-of-the-art%20contenders%20on%2010%20benchmark%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06294v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fortifying%20Fully%20Convolutional%20Generative%20Adversarial%20Networks%20for%20Image%0A%20%20Super-Resolution%20Using%20Divergence%20Measures&entry.906535625=Arkaprabha%20Basu%20and%20Kushal%20Bose%20and%20Sankha%20Subhra%20Mullick%20and%20Anish%20Chakrabarty%20and%20Swagatam%20Das&entry.1292438233=%20%20Super-Resolution%20%28SR%29%20is%20a%20time-hallowed%20image%20processing%20problem%20that%20aims%0Ato%20improve%20the%20quality%20of%20a%20Low-Resolution%20%28LR%29%20sample%20up%20to%20the%20standard%20of%0Aits%20High-Resolution%20%28HR%29%20counterpart.%20We%20aim%20to%20address%20this%20by%20introducing%0ASuper-Resolution%20Generator%20%28SuRGe%29%2C%20a%20fully-convolutional%20Generative%0AAdversarial%20Network%20%28GAN%29-based%20architecture%20for%20SR.%20We%20show%20that%20distinct%0Aconvolutional%20features%20obtained%20at%20increasing%20depths%20of%20a%20GAN%20generator%20can%20be%0Aoptimally%20combined%20by%20a%20set%20of%20learnable%20convex%20weights%20to%20improve%20the%20quality%0Aof%20generated%20SR%20samples.%20In%20the%20process%2C%20we%20employ%20the%20Jensen-Shannon%20and%20the%0AGromov-Wasserstein%20losses%20respectively%20between%20the%20SR-HR%20and%20LR-SR%20pairs%20of%0Adistributions%20to%20further%20aid%20the%20generator%20of%20SuRGe%20to%20better%20exploit%20the%0Aavailable%20information%20in%20an%20attempt%20to%20improve%20SR.%20Moreover%2C%20we%20train%20the%0Adiscriminator%20of%20SuRGe%20with%20the%20Wasserstein%20loss%20with%20gradient%20penalty%2C%20to%0Aprimarily%20prevent%20mode%20collapse.%20The%20proposed%20SuRGe%2C%20as%20an%20end-to-end%20GAN%0Aworkflow%20tailor-made%20for%20super-resolution%2C%20offers%20improved%20performance%20while%0Amaintaining%20low%20inference%20time.%20The%20efficacy%20of%20SuRGe%20is%20substantiated%20by%20its%0Asuperior%20performance%20compared%20to%2018%20state-of-the-art%20contenders%20on%2010%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06294v1&entry.124074799=Read"},
{"title": "Are We on the Right Way for Evaluating Large Vision-Language Models?", "author": "Lin Chen and Jinsong Li and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Zehui Chen and Haodong Duan and Jiaqi Wang and Yu Qiao and Dahua Lin and Feng Zhao", "abstract": "  Large vision-language models (LVLMs) have recently achieved rapid progress,\nsparking numerous studies to evaluate their multi-modal capabilities. However,\nwe dig into current evaluation works and identify two primary issues: 1) Visual\ncontent is unnecessary for many samples. The answers can be directly inferred\nfrom the questions and options, or the world knowledge embedded in LLMs. This\nphenomenon is prevalent across current benchmarks. For instance, GeminiPro\nachieves 42.9% on the MMMU benchmark without any visual input, and outperforms\nthe random choice baseline across six benchmarks over 24% on average. 2)\nUnintentional data leakage exists in LLM and LVLM training. LLM and LVLM could\nstill answer some visual-necessary questions without visual content, indicating\nthe memorizing of these samples within large-scale training data. For example,\nSphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM\nbackbone with 17.9%. Both problems lead to misjudgments of actual multi-modal\ngains and potentially misguide the study of LVLM. To this end, we present\nMMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500\nsamples meticulously selected by humans. MMStar benchmarks 6 core capabilities\nand 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with\ncarefully balanced and purified samples. These samples are first roughly\nselected from current benchmarks with an automated pipeline, human review is\nthen involved to ensure each curated sample exhibits visual dependency, minimal\ndata leakage, and requires advanced multi-modal capabilities. Moreover, two\nmetrics are developed to measure data leakage and actual performance gain in\nmulti-modal training. We evaluate 16 leading LVLMs on MMStar to assess their\nmulti-modal capabilities, and on 7 benchmarks with the proposed metrics to\ninvestigate their data leakage and actual multi-modal gain.\n", "link": "http://arxiv.org/abs/2403.20330v2", "date": "2024-04-09", "relevancy": 2.1454, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5595}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5279}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5165}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Are%20We%20on%20the%20Right%20Way%20for%20Evaluating%20Large%20Vision-Language%20Models%3F&body=Title%3A%20Are%20We%20on%20the%20Right%20Way%20for%20Evaluating%20Large%20Vision-Language%20Models%3F%0AAuthor%3A%20Lin%20Chen%20and%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Zehui%20Chen%20and%20Haodong%20Duan%20and%20Jiaqi%20Wang%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Feng%20Zhao%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20recently%20achieved%20rapid%20progress%2C%0Asparking%20numerous%20studies%20to%20evaluate%20their%20multi-modal%20capabilities.%20However%2C%0Awe%20dig%20into%20current%20evaluation%20works%20and%20identify%20two%20primary%20issues%3A%201%29%20Visual%0Acontent%20is%20unnecessary%20for%20many%20samples.%20The%20answers%20can%20be%20directly%20inferred%0Afrom%20the%20questions%20and%20options%2C%20or%20the%20world%20knowledge%20embedded%20in%20LLMs.%20This%0Aphenomenon%20is%20prevalent%20across%20current%20benchmarks.%20For%20instance%2C%20GeminiPro%0Aachieves%2042.9%25%20on%20the%20MMMU%20benchmark%20without%20any%20visual%20input%2C%20and%20outperforms%0Athe%20random%20choice%20baseline%20across%20six%20benchmarks%20over%2024%25%20on%20average.%202%29%0AUnintentional%20data%20leakage%20exists%20in%20LLM%20and%20LVLM%20training.%20LLM%20and%20LVLM%20could%0Astill%20answer%20some%20visual-necessary%20questions%20without%20visual%20content%2C%20indicating%0Athe%20memorizing%20of%20these%20samples%20within%20large-scale%20training%20data.%20For%20example%2C%0ASphinx-X-MoE%20gets%2043.6%25%20on%20MMMU%20without%20accessing%20images%2C%20surpassing%20its%20LLM%0Abackbone%20with%2017.9%25.%20Both%20problems%20lead%20to%20misjudgments%20of%20actual%20multi-modal%0Agains%20and%20potentially%20misguide%20the%20study%20of%20LVLM.%20To%20this%20end%2C%20we%20present%0AMMStar%2C%20an%20elite%20vision-indispensable%20multi-modal%20benchmark%20comprising%201%2C500%0Asamples%20meticulously%20selected%20by%20humans.%20MMStar%20benchmarks%206%20core%20capabilities%0Aand%2018%20detailed%20axes%2C%20aiming%20to%20evaluate%20LVLMs%27%20multi-modal%20capacities%20with%0Acarefully%20balanced%20and%20purified%20samples.%20These%20samples%20are%20first%20roughly%0Aselected%20from%20current%20benchmarks%20with%20an%20automated%20pipeline%2C%20human%20review%20is%0Athen%20involved%20to%20ensure%20each%20curated%20sample%20exhibits%20visual%20dependency%2C%20minimal%0Adata%20leakage%2C%20and%20requires%20advanced%20multi-modal%20capabilities.%20Moreover%2C%20two%0Ametrics%20are%20developed%20to%20measure%20data%20leakage%20and%20actual%20performance%20gain%20in%0Amulti-modal%20training.%20We%20evaluate%2016%20leading%20LVLMs%20on%20MMStar%20to%20assess%20their%0Amulti-modal%20capabilities%2C%20and%20on%207%20benchmarks%20with%20the%20proposed%20metrics%20to%0Ainvestigate%20their%20data%20leakage%20and%20actual%20multi-modal%20gain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20330v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20We%20on%20the%20Right%20Way%20for%20Evaluating%20Large%20Vision-Language%20Models%3F&entry.906535625=Lin%20Chen%20and%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Zehui%20Chen%20and%20Haodong%20Duan%20and%20Jiaqi%20Wang%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Feng%20Zhao&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20recently%20achieved%20rapid%20progress%2C%0Asparking%20numerous%20studies%20to%20evaluate%20their%20multi-modal%20capabilities.%20However%2C%0Awe%20dig%20into%20current%20evaluation%20works%20and%20identify%20two%20primary%20issues%3A%201%29%20Visual%0Acontent%20is%20unnecessary%20for%20many%20samples.%20The%20answers%20can%20be%20directly%20inferred%0Afrom%20the%20questions%20and%20options%2C%20or%20the%20world%20knowledge%20embedded%20in%20LLMs.%20This%0Aphenomenon%20is%20prevalent%20across%20current%20benchmarks.%20For%20instance%2C%20GeminiPro%0Aachieves%2042.9%25%20on%20the%20MMMU%20benchmark%20without%20any%20visual%20input%2C%20and%20outperforms%0Athe%20random%20choice%20baseline%20across%20six%20benchmarks%20over%2024%25%20on%20average.%202%29%0AUnintentional%20data%20leakage%20exists%20in%20LLM%20and%20LVLM%20training.%20LLM%20and%20LVLM%20could%0Astill%20answer%20some%20visual-necessary%20questions%20without%20visual%20content%2C%20indicating%0Athe%20memorizing%20of%20these%20samples%20within%20large-scale%20training%20data.%20For%20example%2C%0ASphinx-X-MoE%20gets%2043.6%25%20on%20MMMU%20without%20accessing%20images%2C%20surpassing%20its%20LLM%0Abackbone%20with%2017.9%25.%20Both%20problems%20lead%20to%20misjudgments%20of%20actual%20multi-modal%0Agains%20and%20potentially%20misguide%20the%20study%20of%20LVLM.%20To%20this%20end%2C%20we%20present%0AMMStar%2C%20an%20elite%20vision-indispensable%20multi-modal%20benchmark%20comprising%201%2C500%0Asamples%20meticulously%20selected%20by%20humans.%20MMStar%20benchmarks%206%20core%20capabilities%0Aand%2018%20detailed%20axes%2C%20aiming%20to%20evaluate%20LVLMs%27%20multi-modal%20capacities%20with%0Acarefully%20balanced%20and%20purified%20samples.%20These%20samples%20are%20first%20roughly%0Aselected%20from%20current%20benchmarks%20with%20an%20automated%20pipeline%2C%20human%20review%20is%0Athen%20involved%20to%20ensure%20each%20curated%20sample%20exhibits%20visual%20dependency%2C%20minimal%0Adata%20leakage%2C%20and%20requires%20advanced%20multi-modal%20capabilities.%20Moreover%2C%20two%0Ametrics%20are%20developed%20to%20measure%20data%20leakage%20and%20actual%20performance%20gain%20in%0Amulti-modal%20training.%20We%20evaluate%2016%20leading%20LVLMs%20on%20MMStar%20to%20assess%20their%0Amulti-modal%20capabilities%2C%20and%20on%207%20benchmarks%20with%20the%20proposed%20metrics%20to%0Ainvestigate%20their%20data%20leakage%20and%20actual%20multi-modal%20gain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20330v2&entry.124074799=Read"},
{"title": "Learning Local and Global Temporal Contexts for Video Semantic\n  Segmentation", "author": "Guolei Sun and Yun Liu and Henghui Ding and Min Wu and Luc Van Gool", "abstract": "  Contextual information plays a core role for video semantic segmentation\n(VSS). This paper summarizes contexts for VSS in two-fold: local temporal\ncontexts (LTC) which define the contexts from neighboring frames, and global\ntemporal contexts (GTC) which represent the contexts from the whole video. As\nfor LTC, it includes static and motional contexts, corresponding to static and\nmoving content in neighboring frames, respectively. Previously, both static and\nmotional contexts have been studied. However, there is no research about\nsimultaneously learning static and motional contexts (highly complementary).\nHence, we propose a Coarse-to-Fine Feature Mining (CFFM) technique to learn a\nunified presentation of LTC. CFFM contains two parts: Coarse-to-Fine Feature\nAssembling (CFFA) and Cross-frame Feature Mining (CFM). CFFA abstracts static\nand motional contexts, and CFM mines useful information from nearby frames to\nenhance target features. To further exploit more temporal contexts, we propose\nCFFM++ by additionally learning GTC from the whole video. Specifically, we\nuniformly sample certain frames from the video and extract global contextual\nprototypes by k-means. The information within those prototypes is mined by CFM\nto refine target features. Experimental results on popular benchmarks\ndemonstrate that CFFM and CFFM++ perform favorably against state-of-the-art\nmethods. Our code is available at https://github.com/GuoleiSun/VSS-CFFM\n", "link": "http://arxiv.org/abs/2204.03330v2", "date": "2024-04-09", "relevancy": 2.1274, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5356}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5315}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5283}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Local%20and%20Global%20Temporal%20Contexts%20for%20Video%20Semantic%0A%20%20Segmentation&body=Title%3A%20Learning%20Local%20and%20Global%20Temporal%20Contexts%20for%20Video%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Guolei%20Sun%20and%20Yun%20Liu%20and%20Henghui%20Ding%20and%20Min%20Wu%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Contextual%20information%20plays%20a%20core%20role%20for%20video%20semantic%20segmentation%0A%28VSS%29.%20This%20paper%20summarizes%20contexts%20for%20VSS%20in%20two-fold%3A%20local%20temporal%0Acontexts%20%28LTC%29%20which%20define%20the%20contexts%20from%20neighboring%20frames%2C%20and%20global%0Atemporal%20contexts%20%28GTC%29%20which%20represent%20the%20contexts%20from%20the%20whole%20video.%20As%0Afor%20LTC%2C%20it%20includes%20static%20and%20motional%20contexts%2C%20corresponding%20to%20static%20and%0Amoving%20content%20in%20neighboring%20frames%2C%20respectively.%20Previously%2C%20both%20static%20and%0Amotional%20contexts%20have%20been%20studied.%20However%2C%20there%20is%20no%20research%20about%0Asimultaneously%20learning%20static%20and%20motional%20contexts%20%28highly%20complementary%29.%0AHence%2C%20we%20propose%20a%20Coarse-to-Fine%20Feature%20Mining%20%28CFFM%29%20technique%20to%20learn%20a%0Aunified%20presentation%20of%20LTC.%20CFFM%20contains%20two%20parts%3A%20Coarse-to-Fine%20Feature%0AAssembling%20%28CFFA%29%20and%20Cross-frame%20Feature%20Mining%20%28CFM%29.%20CFFA%20abstracts%20static%0Aand%20motional%20contexts%2C%20and%20CFM%20mines%20useful%20information%20from%20nearby%20frames%20to%0Aenhance%20target%20features.%20To%20further%20exploit%20more%20temporal%20contexts%2C%20we%20propose%0ACFFM%2B%2B%20by%20additionally%20learning%20GTC%20from%20the%20whole%20video.%20Specifically%2C%20we%0Auniformly%20sample%20certain%20frames%20from%20the%20video%20and%20extract%20global%20contextual%0Aprototypes%20by%20k-means.%20The%20information%20within%20those%20prototypes%20is%20mined%20by%20CFM%0Ato%20refine%20target%20features.%20Experimental%20results%20on%20popular%20benchmarks%0Ademonstrate%20that%20CFFM%20and%20CFFM%2B%2B%20perform%20favorably%20against%20state-of-the-art%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/GuoleiSun/VSS-CFFM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.03330v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Local%20and%20Global%20Temporal%20Contexts%20for%20Video%20Semantic%0A%20%20Segmentation&entry.906535625=Guolei%20Sun%20and%20Yun%20Liu%20and%20Henghui%20Ding%20and%20Min%20Wu%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Contextual%20information%20plays%20a%20core%20role%20for%20video%20semantic%20segmentation%0A%28VSS%29.%20This%20paper%20summarizes%20contexts%20for%20VSS%20in%20two-fold%3A%20local%20temporal%0Acontexts%20%28LTC%29%20which%20define%20the%20contexts%20from%20neighboring%20frames%2C%20and%20global%0Atemporal%20contexts%20%28GTC%29%20which%20represent%20the%20contexts%20from%20the%20whole%20video.%20As%0Afor%20LTC%2C%20it%20includes%20static%20and%20motional%20contexts%2C%20corresponding%20to%20static%20and%0Amoving%20content%20in%20neighboring%20frames%2C%20respectively.%20Previously%2C%20both%20static%20and%0Amotional%20contexts%20have%20been%20studied.%20However%2C%20there%20is%20no%20research%20about%0Asimultaneously%20learning%20static%20and%20motional%20contexts%20%28highly%20complementary%29.%0AHence%2C%20we%20propose%20a%20Coarse-to-Fine%20Feature%20Mining%20%28CFFM%29%20technique%20to%20learn%20a%0Aunified%20presentation%20of%20LTC.%20CFFM%20contains%20two%20parts%3A%20Coarse-to-Fine%20Feature%0AAssembling%20%28CFFA%29%20and%20Cross-frame%20Feature%20Mining%20%28CFM%29.%20CFFA%20abstracts%20static%0Aand%20motional%20contexts%2C%20and%20CFM%20mines%20useful%20information%20from%20nearby%20frames%20to%0Aenhance%20target%20features.%20To%20further%20exploit%20more%20temporal%20contexts%2C%20we%20propose%0ACFFM%2B%2B%20by%20additionally%20learning%20GTC%20from%20the%20whole%20video.%20Specifically%2C%20we%0Auniformly%20sample%20certain%20frames%20from%20the%20video%20and%20extract%20global%20contextual%0Aprototypes%20by%20k-means.%20The%20information%20within%20those%20prototypes%20is%20mined%20by%20CFM%0Ato%20refine%20target%20features.%20Experimental%20results%20on%20popular%20benchmarks%0Ademonstrate%20that%20CFFM%20and%20CFFM%2B%2B%20perform%20favorably%20against%20state-of-the-art%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/GuoleiSun/VSS-CFFM%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.03330v2&entry.124074799=Read"},
{"title": "Self-training via Metric Learning for Source-Free Domain Adaptation of\n  Semantic Segmentation", "author": "Ibrahim Batuhan Akkaya and Ugur Halici", "abstract": "  Unsupervised source-free domain adaptation methods aim to train a model for\nthe target domain utilizing a pretrained source-domain model and unlabeled\ntarget-domain data, particularly when accessibility to source data is\nrestricted due to intellectual property or privacy concerns. Traditional\nmethods usually use self-training with pseudo-labeling, which is often\nsubjected to thresholding based on prediction confidence. However, such\nthresholding limits the effectiveness of self-training due to insufficient\nsupervision. This issue becomes more severe in a source-free setting, where\nsupervision comes solely from the predictions of the pre-trained source model.\nIn this study, we propose a novel approach by incorporating a mean-teacher\nmodel, wherein the student network is trained using all predictions from the\nteacher network. Instead of employing thresholding on predictions, we introduce\na method to weight the gradients calculated from pseudo-labels based on the\nreliability of the teacher's predictions. To assess reliability, we introduce a\nnovel approach using proxy-based metric learning. Our method is evaluated in\nsynthetic-to-real and cross-city scenarios, demonstrating superior performance\ncompared to existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2212.04227v2", "date": "2024-04-09", "relevancy": 2.1222, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5358}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5292}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-training%20via%20Metric%20Learning%20for%20Source-Free%20Domain%20Adaptation%20of%0A%20%20Semantic%20Segmentation&body=Title%3A%20Self-training%20via%20Metric%20Learning%20for%20Source-Free%20Domain%20Adaptation%20of%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Ibrahim%20Batuhan%20Akkaya%20and%20Ugur%20Halici%0AAbstract%3A%20%20%20Unsupervised%20source-free%20domain%20adaptation%20methods%20aim%20to%20train%20a%20model%20for%0Athe%20target%20domain%20utilizing%20a%20pretrained%20source-domain%20model%20and%20unlabeled%0Atarget-domain%20data%2C%20particularly%20when%20accessibility%20to%20source%20data%20is%0Arestricted%20due%20to%20intellectual%20property%20or%20privacy%20concerns.%20Traditional%0Amethods%20usually%20use%20self-training%20with%20pseudo-labeling%2C%20which%20is%20often%0Asubjected%20to%20thresholding%20based%20on%20prediction%20confidence.%20However%2C%20such%0Athresholding%20limits%20the%20effectiveness%20of%20self-training%20due%20to%20insufficient%0Asupervision.%20This%20issue%20becomes%20more%20severe%20in%20a%20source-free%20setting%2C%20where%0Asupervision%20comes%20solely%20from%20the%20predictions%20of%20the%20pre-trained%20source%20model.%0AIn%20this%20study%2C%20we%20propose%20a%20novel%20approach%20by%20incorporating%20a%20mean-teacher%0Amodel%2C%20wherein%20the%20student%20network%20is%20trained%20using%20all%20predictions%20from%20the%0Ateacher%20network.%20Instead%20of%20employing%20thresholding%20on%20predictions%2C%20we%20introduce%0Aa%20method%20to%20weight%20the%20gradients%20calculated%20from%20pseudo-labels%20based%20on%20the%0Areliability%20of%20the%20teacher%27s%20predictions.%20To%20assess%20reliability%2C%20we%20introduce%20a%0Anovel%20approach%20using%20proxy-based%20metric%20learning.%20Our%20method%20is%20evaluated%20in%0Asynthetic-to-real%20and%20cross-city%20scenarios%2C%20demonstrating%20superior%20performance%0Acompared%20to%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.04227v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-training%20via%20Metric%20Learning%20for%20Source-Free%20Domain%20Adaptation%20of%0A%20%20Semantic%20Segmentation&entry.906535625=Ibrahim%20Batuhan%20Akkaya%20and%20Ugur%20Halici&entry.1292438233=%20%20Unsupervised%20source-free%20domain%20adaptation%20methods%20aim%20to%20train%20a%20model%20for%0Athe%20target%20domain%20utilizing%20a%20pretrained%20source-domain%20model%20and%20unlabeled%0Atarget-domain%20data%2C%20particularly%20when%20accessibility%20to%20source%20data%20is%0Arestricted%20due%20to%20intellectual%20property%20or%20privacy%20concerns.%20Traditional%0Amethods%20usually%20use%20self-training%20with%20pseudo-labeling%2C%20which%20is%20often%0Asubjected%20to%20thresholding%20based%20on%20prediction%20confidence.%20However%2C%20such%0Athresholding%20limits%20the%20effectiveness%20of%20self-training%20due%20to%20insufficient%0Asupervision.%20This%20issue%20becomes%20more%20severe%20in%20a%20source-free%20setting%2C%20where%0Asupervision%20comes%20solely%20from%20the%20predictions%20of%20the%20pre-trained%20source%20model.%0AIn%20this%20study%2C%20we%20propose%20a%20novel%20approach%20by%20incorporating%20a%20mean-teacher%0Amodel%2C%20wherein%20the%20student%20network%20is%20trained%20using%20all%20predictions%20from%20the%0Ateacher%20network.%20Instead%20of%20employing%20thresholding%20on%20predictions%2C%20we%20introduce%0Aa%20method%20to%20weight%20the%20gradients%20calculated%20from%20pseudo-labels%20based%20on%20the%0Areliability%20of%20the%20teacher%27s%20predictions.%20To%20assess%20reliability%2C%20we%20introduce%20a%0Anovel%20approach%20using%20proxy-based%20metric%20learning.%20Our%20method%20is%20evaluated%20in%0Asynthetic-to-real%20and%20cross-city%20scenarios%2C%20demonstrating%20superior%20performance%0Acompared%20to%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.04227v2&entry.124074799=Read"},
{"title": "Learning State-Invariant Representations of Objects from Image\n  Collections with State, Pose, and Viewpoint Changes", "author": "Rohan Sarkar and Avinash Kak", "abstract": "  We add one more invariance - state invariance - to the more commonly used\nother invariances for learning object representations for recognition and\nretrieval. By state invariance, we mean robust with respect to changes in the\nstructural form of the object, such as when an umbrella is folded, or when an\nitem of clothing is tossed on the floor. Since humans generally have no\ndifficulty in recognizing objects despite such state changes, we are naturally\nfaced with the question of whether it is possible to devise a neural\narchitecture with similar abilities. To that end, we present a novel dataset,\nObjectsWithStateChange, that captures state and pose variations in the object\nimages recorded from arbitrary viewpoints. We believe that this dataset will\nfacilitate research in fine-grained object recognition and retrieval of objects\nthat are capable of state changes. The goal of such research would be to train\nmodels capable of generating object embeddings that remain invariant to state\nchanges while also staying invariant to transformations induced by changes in\nviewpoint, pose, illumination, etc. To demonstrate the usefulness of the\nObjectsWithStateChange dataset, we also propose a curriculum learning strategy\nthat uses the similarity relationships in the learned embedding space after\neach epoch to guide the training process. The model learns discriminative\nfeatures by comparing visually similar objects within and across different\ncategories, encouraging it to differentiate between objects that may be\nchallenging to distinguish due to changes in their state. We believe that this\nstrategy enhances the model's ability to capture discriminative features for\nfine-grained tasks that may involve objects with state changes, leading to\nperformance improvements on object-level tasks not only on our new dataset, but\nalso on two other challenging multi-view datasets such as ModelNet40 and\nObjectPI.\n", "link": "http://arxiv.org/abs/2404.06470v1", "date": "2024-04-09", "relevancy": 2.1075, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5149}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5136}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20State-Invariant%20Representations%20of%20Objects%20from%20Image%0A%20%20Collections%20with%20State%2C%20Pose%2C%20and%20Viewpoint%20Changes&body=Title%3A%20Learning%20State-Invariant%20Representations%20of%20Objects%20from%20Image%0A%20%20Collections%20with%20State%2C%20Pose%2C%20and%20Viewpoint%20Changes%0AAuthor%3A%20Rohan%20Sarkar%20and%20Avinash%20Kak%0AAbstract%3A%20%20%20We%20add%20one%20more%20invariance%20-%20state%20invariance%20-%20to%20the%20more%20commonly%20used%0Aother%20invariances%20for%20learning%20object%20representations%20for%20recognition%20and%0Aretrieval.%20By%20state%20invariance%2C%20we%20mean%20robust%20with%20respect%20to%20changes%20in%20the%0Astructural%20form%20of%20the%20object%2C%20such%20as%20when%20an%20umbrella%20is%20folded%2C%20or%20when%20an%0Aitem%20of%20clothing%20is%20tossed%20on%20the%20floor.%20Since%20humans%20generally%20have%20no%0Adifficulty%20in%20recognizing%20objects%20despite%20such%20state%20changes%2C%20we%20are%20naturally%0Afaced%20with%20the%20question%20of%20whether%20it%20is%20possible%20to%20devise%20a%20neural%0Aarchitecture%20with%20similar%20abilities.%20To%20that%20end%2C%20we%20present%20a%20novel%20dataset%2C%0AObjectsWithStateChange%2C%20that%20captures%20state%20and%20pose%20variations%20in%20the%20object%0Aimages%20recorded%20from%20arbitrary%20viewpoints.%20We%20believe%20that%20this%20dataset%20will%0Afacilitate%20research%20in%20fine-grained%20object%20recognition%20and%20retrieval%20of%20objects%0Athat%20are%20capable%20of%20state%20changes.%20The%20goal%20of%20such%20research%20would%20be%20to%20train%0Amodels%20capable%20of%20generating%20object%20embeddings%20that%20remain%20invariant%20to%20state%0Achanges%20while%20also%20staying%20invariant%20to%20transformations%20induced%20by%20changes%20in%0Aviewpoint%2C%20pose%2C%20illumination%2C%20etc.%20To%20demonstrate%20the%20usefulness%20of%20the%0AObjectsWithStateChange%20dataset%2C%20we%20also%20propose%20a%20curriculum%20learning%20strategy%0Athat%20uses%20the%20similarity%20relationships%20in%20the%20learned%20embedding%20space%20after%0Aeach%20epoch%20to%20guide%20the%20training%20process.%20The%20model%20learns%20discriminative%0Afeatures%20by%20comparing%20visually%20similar%20objects%20within%20and%20across%20different%0Acategories%2C%20encouraging%20it%20to%20differentiate%20between%20objects%20that%20may%20be%0Achallenging%20to%20distinguish%20due%20to%20changes%20in%20their%20state.%20We%20believe%20that%20this%0Astrategy%20enhances%20the%20model%27s%20ability%20to%20capture%20discriminative%20features%20for%0Afine-grained%20tasks%20that%20may%20involve%20objects%20with%20state%20changes%2C%20leading%20to%0Aperformance%20improvements%20on%20object-level%20tasks%20not%20only%20on%20our%20new%20dataset%2C%20but%0Aalso%20on%20two%20other%20challenging%20multi-view%20datasets%20such%20as%20ModelNet40%20and%0AObjectPI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06470v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20State-Invariant%20Representations%20of%20Objects%20from%20Image%0A%20%20Collections%20with%20State%2C%20Pose%2C%20and%20Viewpoint%20Changes&entry.906535625=Rohan%20Sarkar%20and%20Avinash%20Kak&entry.1292438233=%20%20We%20add%20one%20more%20invariance%20-%20state%20invariance%20-%20to%20the%20more%20commonly%20used%0Aother%20invariances%20for%20learning%20object%20representations%20for%20recognition%20and%0Aretrieval.%20By%20state%20invariance%2C%20we%20mean%20robust%20with%20respect%20to%20changes%20in%20the%0Astructural%20form%20of%20the%20object%2C%20such%20as%20when%20an%20umbrella%20is%20folded%2C%20or%20when%20an%0Aitem%20of%20clothing%20is%20tossed%20on%20the%20floor.%20Since%20humans%20generally%20have%20no%0Adifficulty%20in%20recognizing%20objects%20despite%20such%20state%20changes%2C%20we%20are%20naturally%0Afaced%20with%20the%20question%20of%20whether%20it%20is%20possible%20to%20devise%20a%20neural%0Aarchitecture%20with%20similar%20abilities.%20To%20that%20end%2C%20we%20present%20a%20novel%20dataset%2C%0AObjectsWithStateChange%2C%20that%20captures%20state%20and%20pose%20variations%20in%20the%20object%0Aimages%20recorded%20from%20arbitrary%20viewpoints.%20We%20believe%20that%20this%20dataset%20will%0Afacilitate%20research%20in%20fine-grained%20object%20recognition%20and%20retrieval%20of%20objects%0Athat%20are%20capable%20of%20state%20changes.%20The%20goal%20of%20such%20research%20would%20be%20to%20train%0Amodels%20capable%20of%20generating%20object%20embeddings%20that%20remain%20invariant%20to%20state%0Achanges%20while%20also%20staying%20invariant%20to%20transformations%20induced%20by%20changes%20in%0Aviewpoint%2C%20pose%2C%20illumination%2C%20etc.%20To%20demonstrate%20the%20usefulness%20of%20the%0AObjectsWithStateChange%20dataset%2C%20we%20also%20propose%20a%20curriculum%20learning%20strategy%0Athat%20uses%20the%20similarity%20relationships%20in%20the%20learned%20embedding%20space%20after%0Aeach%20epoch%20to%20guide%20the%20training%20process.%20The%20model%20learns%20discriminative%0Afeatures%20by%20comparing%20visually%20similar%20objects%20within%20and%20across%20different%0Acategories%2C%20encouraging%20it%20to%20differentiate%20between%20objects%20that%20may%20be%0Achallenging%20to%20distinguish%20due%20to%20changes%20in%20their%20state.%20We%20believe%20that%20this%0Astrategy%20enhances%20the%20model%27s%20ability%20to%20capture%20discriminative%20features%20for%0Afine-grained%20tasks%20that%20may%20involve%20objects%20with%20state%20changes%2C%20leading%20to%0Aperformance%20improvements%20on%20object-level%20tasks%20not%20only%20on%20our%20new%20dataset%2C%20but%0Aalso%20on%20two%20other%20challenging%20multi-view%20datasets%20such%20as%20ModelNet40%20and%0AObjectPI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06470v1&entry.124074799=Read"},
{"title": "GHNeRF: Learning Generalizable Human Features with Efficient Neural\n  Radiance Fields", "author": "Arnab Dey and Di Yang and Rohith Agaram and Antitza Dantcheva and Andrew I. Comport and Srinath Sridhar and Jean Martinet", "abstract": "  Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising\nresults in 3D scene representations, including 3D human representations.\nHowever, these representations often lack crucial information on the underlying\nhuman pose and structure, which is crucial for AR/VR applications and games. In\nthis paper, we introduce a novel approach, termed GHNeRF, designed to address\nthese limitations by learning 2D/3D joint locations of human subjects with NeRF\nrepresentation. GHNeRF uses a pre-trained 2D encoder streamlined to extract\nessential human features from 2D images, which are then incorporated into the\nNeRF framework in order to encode human biomechanic features. This allows our\nnetwork to simultaneously learn biomechanic features, such as joint locations,\nalong with human geometry and texture. To assess the effectiveness of our\nmethod, we conduct a comprehensive comparison with state-of-the-art human NeRF\ntechniques and joint estimation algorithms. Our results show that GHNeRF can\nachieve state-of-the-art results in near real-time.\n", "link": "http://arxiv.org/abs/2404.06246v1", "date": "2024-04-09", "relevancy": 2.0968, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5435}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4953}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GHNeRF%3A%20Learning%20Generalizable%20Human%20Features%20with%20Efficient%20Neural%0A%20%20Radiance%20Fields&body=Title%3A%20GHNeRF%3A%20Learning%20Generalizable%20Human%20Features%20with%20Efficient%20Neural%0A%20%20Radiance%20Fields%0AAuthor%3A%20Arnab%20Dey%20and%20Di%20Yang%20and%20Rohith%20Agaram%20and%20Antitza%20Dantcheva%20and%20Andrew%20I.%20Comport%20and%20Srinath%20Sridhar%20and%20Jean%20Martinet%0AAbstract%3A%20%20%20Recent%20advances%20in%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20demonstrated%20promising%0Aresults%20in%203D%20scene%20representations%2C%20including%203D%20human%20representations.%0AHowever%2C%20these%20representations%20often%20lack%20crucial%20information%20on%20the%20underlying%0Ahuman%20pose%20and%20structure%2C%20which%20is%20crucial%20for%20AR/VR%20applications%20and%20games.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20approach%2C%20termed%20GHNeRF%2C%20designed%20to%20address%0Athese%20limitations%20by%20learning%202D/3D%20joint%20locations%20of%20human%20subjects%20with%20NeRF%0Arepresentation.%20GHNeRF%20uses%20a%20pre-trained%202D%20encoder%20streamlined%20to%20extract%0Aessential%20human%20features%20from%202D%20images%2C%20which%20are%20then%20incorporated%20into%20the%0ANeRF%20framework%20in%20order%20to%20encode%20human%20biomechanic%20features.%20This%20allows%20our%0Anetwork%20to%20simultaneously%20learn%20biomechanic%20features%2C%20such%20as%20joint%20locations%2C%0Aalong%20with%20human%20geometry%20and%20texture.%20To%20assess%20the%20effectiveness%20of%20our%0Amethod%2C%20we%20conduct%20a%20comprehensive%20comparison%20with%20state-of-the-art%20human%20NeRF%0Atechniques%20and%20joint%20estimation%20algorithms.%20Our%20results%20show%20that%20GHNeRF%20can%0Aachieve%20state-of-the-art%20results%20in%20near%20real-time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06246v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GHNeRF%3A%20Learning%20Generalizable%20Human%20Features%20with%20Efficient%20Neural%0A%20%20Radiance%20Fields&entry.906535625=Arnab%20Dey%20and%20Di%20Yang%20and%20Rohith%20Agaram%20and%20Antitza%20Dantcheva%20and%20Andrew%20I.%20Comport%20and%20Srinath%20Sridhar%20and%20Jean%20Martinet&entry.1292438233=%20%20Recent%20advances%20in%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20demonstrated%20promising%0Aresults%20in%203D%20scene%20representations%2C%20including%203D%20human%20representations.%0AHowever%2C%20these%20representations%20often%20lack%20crucial%20information%20on%20the%20underlying%0Ahuman%20pose%20and%20structure%2C%20which%20is%20crucial%20for%20AR/VR%20applications%20and%20games.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20approach%2C%20termed%20GHNeRF%2C%20designed%20to%20address%0Athese%20limitations%20by%20learning%202D/3D%20joint%20locations%20of%20human%20subjects%20with%20NeRF%0Arepresentation.%20GHNeRF%20uses%20a%20pre-trained%202D%20encoder%20streamlined%20to%20extract%0Aessential%20human%20features%20from%202D%20images%2C%20which%20are%20then%20incorporated%20into%20the%0ANeRF%20framework%20in%20order%20to%20encode%20human%20biomechanic%20features.%20This%20allows%20our%0Anetwork%20to%20simultaneously%20learn%20biomechanic%20features%2C%20such%20as%20joint%20locations%2C%0Aalong%20with%20human%20geometry%20and%20texture.%20To%20assess%20the%20effectiveness%20of%20our%0Amethod%2C%20we%20conduct%20a%20comprehensive%20comparison%20with%20state-of-the-art%20human%20NeRF%0Atechniques%20and%20joint%20estimation%20algorithms.%20Our%20results%20show%20that%20GHNeRF%20can%0Aachieve%20state-of-the-art%20results%20in%20near%20real-time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06246v1&entry.124074799=Read"},
{"title": "Deep Multi-Threshold Spiking-UNet for Image Processing", "author": "Hebei Li and Yueyi Zhang and Zhiwei Xiong and Zheng-jun Zha and Xiaoyan Sun", "abstract": "  U-Net, known for its simple yet efficient architecture, is widely utilized\nfor image processing tasks and is particularly suitable for deployment on\nneuromorphic chips. This paper introduces the novel concept of Spiking-UNet for\nimage processing, which combines the power of Spiking Neural Networks (SNNs)\nwith the U-Net architecture. To achieve an efficient Spiking-UNet, we face two\nprimary challenges: ensuring high-fidelity information propagation through the\nnetwork via spikes and formulating an effective training strategy. To address\nthe issue of information loss, we introduce multi-threshold spiking neurons,\nwhich improve the efficiency of information transmission within the\nSpiking-UNet. For the training strategy, we adopt a conversion and fine-tuning\npipeline that leverage pre-trained U-Net models. During the conversion process,\nsignificant variability in data distribution across different parts is observed\nwhen utilizing skip connections. Therefore, we propose a connection-wise\nnormalization method to prevent inaccurate firing rates. Furthermore, we adopt\na flow-based training method to fine-tune the converted models, reducing time\nsteps while preserving performance. Experimental results show that, on image\nsegmentation and denoising, our Spiking-UNet achieves comparable performance to\nits non-spiking counterpart, surpassing existing SNN methods. Compared with the\nconverted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference\ntime by approximately 90\\%. This research broadens the application scope of\nSNNs in image processing and is expected to inspire further exploration in the\nfield of neuromorphic engineering. The code for our Spiking-UNet implementation\nis available at https://github.com/SNNresearch/Spiking-UNet.\n", "link": "http://arxiv.org/abs/2307.10974v3", "date": "2024-04-09", "relevancy": 2.0928, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5565}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5328}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5002}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Multi-Threshold%20Spiking-UNet%20for%20Image%20Processing&body=Title%3A%20Deep%20Multi-Threshold%20Spiking-UNet%20for%20Image%20Processing%0AAuthor%3A%20Hebei%20Li%20and%20Yueyi%20Zhang%20and%20Zhiwei%20Xiong%20and%20Zheng-jun%20Zha%20and%20Xiaoyan%20Sun%0AAbstract%3A%20%20%20U-Net%2C%20known%20for%20its%20simple%20yet%20efficient%20architecture%2C%20is%20widely%20utilized%0Afor%20image%20processing%20tasks%20and%20is%20particularly%20suitable%20for%20deployment%20on%0Aneuromorphic%20chips.%20This%20paper%20introduces%20the%20novel%20concept%20of%20Spiking-UNet%20for%0Aimage%20processing%2C%20which%20combines%20the%20power%20of%20Spiking%20Neural%20Networks%20%28SNNs%29%0Awith%20the%20U-Net%20architecture.%20To%20achieve%20an%20efficient%20Spiking-UNet%2C%20we%20face%20two%0Aprimary%20challenges%3A%20ensuring%20high-fidelity%20information%20propagation%20through%20the%0Anetwork%20via%20spikes%20and%20formulating%20an%20effective%20training%20strategy.%20To%20address%0Athe%20issue%20of%20information%20loss%2C%20we%20introduce%20multi-threshold%20spiking%20neurons%2C%0Awhich%20improve%20the%20efficiency%20of%20information%20transmission%20within%20the%0ASpiking-UNet.%20For%20the%20training%20strategy%2C%20we%20adopt%20a%20conversion%20and%20fine-tuning%0Apipeline%20that%20leverage%20pre-trained%20U-Net%20models.%20During%20the%20conversion%20process%2C%0Asignificant%20variability%20in%20data%20distribution%20across%20different%20parts%20is%20observed%0Awhen%20utilizing%20skip%20connections.%20Therefore%2C%20we%20propose%20a%20connection-wise%0Anormalization%20method%20to%20prevent%20inaccurate%20firing%20rates.%20Furthermore%2C%20we%20adopt%0Aa%20flow-based%20training%20method%20to%20fine-tune%20the%20converted%20models%2C%20reducing%20time%0Asteps%20while%20preserving%20performance.%20Experimental%20results%20show%20that%2C%20on%20image%0Asegmentation%20and%20denoising%2C%20our%20Spiking-UNet%20achieves%20comparable%20performance%20to%0Aits%20non-spiking%20counterpart%2C%20surpassing%20existing%20SNN%20methods.%20Compared%20with%20the%0Aconverted%20Spiking-UNet%20without%20fine-tuning%2C%20our%20Spiking-UNet%20reduces%20inference%0Atime%20by%20approximately%2090%5C%25.%20This%20research%20broadens%20the%20application%20scope%20of%0ASNNs%20in%20image%20processing%20and%20is%20expected%20to%20inspire%20further%20exploration%20in%20the%0Afield%20of%20neuromorphic%20engineering.%20The%20code%20for%20our%20Spiking-UNet%20implementation%0Ais%20available%20at%20https%3A//github.com/SNNresearch/Spiking-UNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.10974v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Multi-Threshold%20Spiking-UNet%20for%20Image%20Processing&entry.906535625=Hebei%20Li%20and%20Yueyi%20Zhang%20and%20Zhiwei%20Xiong%20and%20Zheng-jun%20Zha%20and%20Xiaoyan%20Sun&entry.1292438233=%20%20U-Net%2C%20known%20for%20its%20simple%20yet%20efficient%20architecture%2C%20is%20widely%20utilized%0Afor%20image%20processing%20tasks%20and%20is%20particularly%20suitable%20for%20deployment%20on%0Aneuromorphic%20chips.%20This%20paper%20introduces%20the%20novel%20concept%20of%20Spiking-UNet%20for%0Aimage%20processing%2C%20which%20combines%20the%20power%20of%20Spiking%20Neural%20Networks%20%28SNNs%29%0Awith%20the%20U-Net%20architecture.%20To%20achieve%20an%20efficient%20Spiking-UNet%2C%20we%20face%20two%0Aprimary%20challenges%3A%20ensuring%20high-fidelity%20information%20propagation%20through%20the%0Anetwork%20via%20spikes%20and%20formulating%20an%20effective%20training%20strategy.%20To%20address%0Athe%20issue%20of%20information%20loss%2C%20we%20introduce%20multi-threshold%20spiking%20neurons%2C%0Awhich%20improve%20the%20efficiency%20of%20information%20transmission%20within%20the%0ASpiking-UNet.%20For%20the%20training%20strategy%2C%20we%20adopt%20a%20conversion%20and%20fine-tuning%0Apipeline%20that%20leverage%20pre-trained%20U-Net%20models.%20During%20the%20conversion%20process%2C%0Asignificant%20variability%20in%20data%20distribution%20across%20different%20parts%20is%20observed%0Awhen%20utilizing%20skip%20connections.%20Therefore%2C%20we%20propose%20a%20connection-wise%0Anormalization%20method%20to%20prevent%20inaccurate%20firing%20rates.%20Furthermore%2C%20we%20adopt%0Aa%20flow-based%20training%20method%20to%20fine-tune%20the%20converted%20models%2C%20reducing%20time%0Asteps%20while%20preserving%20performance.%20Experimental%20results%20show%20that%2C%20on%20image%0Asegmentation%20and%20denoising%2C%20our%20Spiking-UNet%20achieves%20comparable%20performance%20to%0Aits%20non-spiking%20counterpart%2C%20surpassing%20existing%20SNN%20methods.%20Compared%20with%20the%0Aconverted%20Spiking-UNet%20without%20fine-tuning%2C%20our%20Spiking-UNet%20reduces%20inference%0Atime%20by%20approximately%2090%5C%25.%20This%20research%20broadens%20the%20application%20scope%20of%0ASNNs%20in%20image%20processing%20and%20is%20expected%20to%20inspire%20further%20exploration%20in%20the%0Afield%20of%20neuromorphic%20engineering.%20The%20code%20for%20our%20Spiking-UNet%20implementation%0Ais%20available%20at%20https%3A//github.com/SNNresearch/Spiking-UNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.10974v3&entry.124074799=Read"},
{"title": "HPNet: Dynamic Trajectory Forecasting with Historical Prediction\n  Attention", "author": "Xiaolong Tang and Meina Kan and Shiguang Shan and Zhilong Ji and Jinfeng Bai and Xilin Chen", "abstract": "  Predicting the trajectories of road agents is essential for autonomous\ndriving systems. The recent mainstream methods follow a static paradigm, which\npredicts the future trajectory by using a fixed duration of historical frames.\nThese methods make the predictions independently even at adjacent time steps,\nwhich leads to potential instability and temporal inconsistency. As successive\ntime steps have largely overlapping historical frames, their forecasting should\nhave intrinsic correlation, such as overlapping predicted trajectories should\nbe consistent, or be different but share the same motion goal depending on the\nroad situation. Motivated by this, in this work, we introduce HPNet, a novel\ndynamic trajectory forecasting method. Aiming for stable and accurate\ntrajectory forecasting, our method leverages not only historical frames\nincluding maps and agent states, but also historical predictions. Specifically,\nwe newly design a Historical Prediction Attention module to automatically\nencode the dynamic relationship between successive predictions. Besides, it\nalso extends the attention range beyond the currently visible window\nbenefitting from the use of historical predictions. The proposed Historical\nPrediction Attention together with the Agent Attention and Mode Attention is\nfurther formulated as the Triple Factorized Attention module, serving as the\ncore design of HPNet.Experiments on the Argoverse and INTERACTION datasets show\nthat HPNet achieves state-of-the-art performance, and generates accurate and\nstable future trajectories. Our code are available at\nhttps://github.com/XiaolongTang23/HPNet.\n", "link": "http://arxiv.org/abs/2404.06351v1", "date": "2024-04-09", "relevancy": 2.0925, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5388}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5376}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5017}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HPNet%3A%20Dynamic%20Trajectory%20Forecasting%20with%20Historical%20Prediction%0A%20%20Attention&body=Title%3A%20HPNet%3A%20Dynamic%20Trajectory%20Forecasting%20with%20Historical%20Prediction%0A%20%20Attention%0AAuthor%3A%20Xiaolong%20Tang%20and%20Meina%20Kan%20and%20Shiguang%20Shan%20and%20Zhilong%20Ji%20and%20Jinfeng%20Bai%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20Predicting%20the%20trajectories%20of%20road%20agents%20is%20essential%20for%20autonomous%0Adriving%20systems.%20The%20recent%20mainstream%20methods%20follow%20a%20static%20paradigm%2C%20which%0Apredicts%20the%20future%20trajectory%20by%20using%20a%20fixed%20duration%20of%20historical%20frames.%0AThese%20methods%20make%20the%20predictions%20independently%20even%20at%20adjacent%20time%20steps%2C%0Awhich%20leads%20to%20potential%20instability%20and%20temporal%20inconsistency.%20As%20successive%0Atime%20steps%20have%20largely%20overlapping%20historical%20frames%2C%20their%20forecasting%20should%0Ahave%20intrinsic%20correlation%2C%20such%20as%20overlapping%20predicted%20trajectories%20should%0Abe%20consistent%2C%20or%20be%20different%20but%20share%20the%20same%20motion%20goal%20depending%20on%20the%0Aroad%20situation.%20Motivated%20by%20this%2C%20in%20this%20work%2C%20we%20introduce%20HPNet%2C%20a%20novel%0Adynamic%20trajectory%20forecasting%20method.%20Aiming%20for%20stable%20and%20accurate%0Atrajectory%20forecasting%2C%20our%20method%20leverages%20not%20only%20historical%20frames%0Aincluding%20maps%20and%20agent%20states%2C%20but%20also%20historical%20predictions.%20Specifically%2C%0Awe%20newly%20design%20a%20Historical%20Prediction%20Attention%20module%20to%20automatically%0Aencode%20the%20dynamic%20relationship%20between%20successive%20predictions.%20Besides%2C%20it%0Aalso%20extends%20the%20attention%20range%20beyond%20the%20currently%20visible%20window%0Abenefitting%20from%20the%20use%20of%20historical%20predictions.%20The%20proposed%20Historical%0APrediction%20Attention%20together%20with%20the%20Agent%20Attention%20and%20Mode%20Attention%20is%0Afurther%20formulated%20as%20the%20Triple%20Factorized%20Attention%20module%2C%20serving%20as%20the%0Acore%20design%20of%20HPNet.Experiments%20on%20the%20Argoverse%20and%20INTERACTION%20datasets%20show%0Athat%20HPNet%20achieves%20state-of-the-art%20performance%2C%20and%20generates%20accurate%20and%0Astable%20future%20trajectories.%20Our%20code%20are%20available%20at%0Ahttps%3A//github.com/XiaolongTang23/HPNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06351v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HPNet%3A%20Dynamic%20Trajectory%20Forecasting%20with%20Historical%20Prediction%0A%20%20Attention&entry.906535625=Xiaolong%20Tang%20and%20Meina%20Kan%20and%20Shiguang%20Shan%20and%20Zhilong%20Ji%20and%20Jinfeng%20Bai%20and%20Xilin%20Chen&entry.1292438233=%20%20Predicting%20the%20trajectories%20of%20road%20agents%20is%20essential%20for%20autonomous%0Adriving%20systems.%20The%20recent%20mainstream%20methods%20follow%20a%20static%20paradigm%2C%20which%0Apredicts%20the%20future%20trajectory%20by%20using%20a%20fixed%20duration%20of%20historical%20frames.%0AThese%20methods%20make%20the%20predictions%20independently%20even%20at%20adjacent%20time%20steps%2C%0Awhich%20leads%20to%20potential%20instability%20and%20temporal%20inconsistency.%20As%20successive%0Atime%20steps%20have%20largely%20overlapping%20historical%20frames%2C%20their%20forecasting%20should%0Ahave%20intrinsic%20correlation%2C%20such%20as%20overlapping%20predicted%20trajectories%20should%0Abe%20consistent%2C%20or%20be%20different%20but%20share%20the%20same%20motion%20goal%20depending%20on%20the%0Aroad%20situation.%20Motivated%20by%20this%2C%20in%20this%20work%2C%20we%20introduce%20HPNet%2C%20a%20novel%0Adynamic%20trajectory%20forecasting%20method.%20Aiming%20for%20stable%20and%20accurate%0Atrajectory%20forecasting%2C%20our%20method%20leverages%20not%20only%20historical%20frames%0Aincluding%20maps%20and%20agent%20states%2C%20but%20also%20historical%20predictions.%20Specifically%2C%0Awe%20newly%20design%20a%20Historical%20Prediction%20Attention%20module%20to%20automatically%0Aencode%20the%20dynamic%20relationship%20between%20successive%20predictions.%20Besides%2C%20it%0Aalso%20extends%20the%20attention%20range%20beyond%20the%20currently%20visible%20window%0Abenefitting%20from%20the%20use%20of%20historical%20predictions.%20The%20proposed%20Historical%0APrediction%20Attention%20together%20with%20the%20Agent%20Attention%20and%20Mode%20Attention%20is%0Afurther%20formulated%20as%20the%20Triple%20Factorized%20Attention%20module%2C%20serving%20as%20the%0Acore%20design%20of%20HPNet.Experiments%20on%20the%20Argoverse%20and%20INTERACTION%20datasets%20show%0Athat%20HPNet%20achieves%20state-of-the-art%20performance%2C%20and%20generates%20accurate%20and%0Astable%20future%20trajectories.%20Our%20code%20are%20available%20at%0Ahttps%3A//github.com/XiaolongTang23/HPNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06351v1&entry.124074799=Read"},
{"title": "Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial\n  Action Units Recognition", "author": "Zihan Wang and Siyang Song and Cheng Luo and Songhe Deng and Weicheng Xie and Linlin Shen", "abstract": "  Human facial action units (AUs) are mutually related in a hierarchical\nmanner, as not only they are associated with each other in both spatial and\ntemporal domains but also AUs located in the same/close facial regions show\nstronger relationships than those of different facial regions. While none of\nexisting approach thoroughly model such hierarchical inter-dependencies among\nAUs, this paper proposes to comprehensively model multi-scale AU-related\ndynamic and hierarchical spatio-temporal relationship among AUs for their\noccurrences recognition. Specifically, we first propose a novel multi-scale\ntemporal differencing network with an adaptive weighting block to explicitly\ncapture facial dynamics across frames at different spatial scales, which\nspecifically considers the heterogeneity of range and magnitude in different\nAUs' activation. Then, a two-stage strategy is introduced to hierarchically\nmodel the relationship among AUs based on their spatial distribution (i.e.,\nlocal and cross-region AU relationship modelling). Experimental results\nachieved on BP4D and DISFA show that our approach is the new state-of-the-art\nin the field of AU occurrence recognition. Our code is publicly available at\nhttps://github.com/CVI-SZU/MDHR.\n", "link": "http://arxiv.org/abs/2404.06443v1", "date": "2024-04-09", "relevancy": 2.09, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5296}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5218}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5065}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20Dynamic%20and%20Hierarchical%20Relationship%20Modeling%20for%20Facial%0A%20%20Action%20Units%20Recognition&body=Title%3A%20Multi-scale%20Dynamic%20and%20Hierarchical%20Relationship%20Modeling%20for%20Facial%0A%20%20Action%20Units%20Recognition%0AAuthor%3A%20Zihan%20Wang%20and%20Siyang%20Song%20and%20Cheng%20Luo%20and%20Songhe%20Deng%20and%20Weicheng%20Xie%20and%20Linlin%20Shen%0AAbstract%3A%20%20%20Human%20facial%20action%20units%20%28AUs%29%20are%20mutually%20related%20in%20a%20hierarchical%0Amanner%2C%20as%20not%20only%20they%20are%20associated%20with%20each%20other%20in%20both%20spatial%20and%0Atemporal%20domains%20but%20also%20AUs%20located%20in%20the%20same/close%20facial%20regions%20show%0Astronger%20relationships%20than%20those%20of%20different%20facial%20regions.%20While%20none%20of%0Aexisting%20approach%20thoroughly%20model%20such%20hierarchical%20inter-dependencies%20among%0AAUs%2C%20this%20paper%20proposes%20to%20comprehensively%20model%20multi-scale%20AU-related%0Adynamic%20and%20hierarchical%20spatio-temporal%20relationship%20among%20AUs%20for%20their%0Aoccurrences%20recognition.%20Specifically%2C%20we%20first%20propose%20a%20novel%20multi-scale%0Atemporal%20differencing%20network%20with%20an%20adaptive%20weighting%20block%20to%20explicitly%0Acapture%20facial%20dynamics%20across%20frames%20at%20different%20spatial%20scales%2C%20which%0Aspecifically%20considers%20the%20heterogeneity%20of%20range%20and%20magnitude%20in%20different%0AAUs%27%20activation.%20Then%2C%20a%20two-stage%20strategy%20is%20introduced%20to%20hierarchically%0Amodel%20the%20relationship%20among%20AUs%20based%20on%20their%20spatial%20distribution%20%28i.e.%2C%0Alocal%20and%20cross-region%20AU%20relationship%20modelling%29.%20Experimental%20results%0Aachieved%20on%20BP4D%20and%20DISFA%20show%20that%20our%20approach%20is%20the%20new%20state-of-the-art%0Ain%20the%20field%20of%20AU%20occurrence%20recognition.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/CVI-SZU/MDHR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06443v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20Dynamic%20and%20Hierarchical%20Relationship%20Modeling%20for%20Facial%0A%20%20Action%20Units%20Recognition&entry.906535625=Zihan%20Wang%20and%20Siyang%20Song%20and%20Cheng%20Luo%20and%20Songhe%20Deng%20and%20Weicheng%20Xie%20and%20Linlin%20Shen&entry.1292438233=%20%20Human%20facial%20action%20units%20%28AUs%29%20are%20mutually%20related%20in%20a%20hierarchical%0Amanner%2C%20as%20not%20only%20they%20are%20associated%20with%20each%20other%20in%20both%20spatial%20and%0Atemporal%20domains%20but%20also%20AUs%20located%20in%20the%20same/close%20facial%20regions%20show%0Astronger%20relationships%20than%20those%20of%20different%20facial%20regions.%20While%20none%20of%0Aexisting%20approach%20thoroughly%20model%20such%20hierarchical%20inter-dependencies%20among%0AAUs%2C%20this%20paper%20proposes%20to%20comprehensively%20model%20multi-scale%20AU-related%0Adynamic%20and%20hierarchical%20spatio-temporal%20relationship%20among%20AUs%20for%20their%0Aoccurrences%20recognition.%20Specifically%2C%20we%20first%20propose%20a%20novel%20multi-scale%0Atemporal%20differencing%20network%20with%20an%20adaptive%20weighting%20block%20to%20explicitly%0Acapture%20facial%20dynamics%20across%20frames%20at%20different%20spatial%20scales%2C%20which%0Aspecifically%20considers%20the%20heterogeneity%20of%20range%20and%20magnitude%20in%20different%0AAUs%27%20activation.%20Then%2C%20a%20two-stage%20strategy%20is%20introduced%20to%20hierarchically%0Amodel%20the%20relationship%20among%20AUs%20based%20on%20their%20spatial%20distribution%20%28i.e.%2C%0Alocal%20and%20cross-region%20AU%20relationship%20modelling%29.%20Experimental%20results%0Aachieved%20on%20BP4D%20and%20DISFA%20show%20that%20our%20approach%20is%20the%20new%20state-of-the-art%0Ain%20the%20field%20of%20AU%20occurrence%20recognition.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/CVI-SZU/MDHR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06443v1&entry.124074799=Read"},
{"title": "Simple Semantic-Aided Few-Shot Learning", "author": "Hai Zhang and Junzhe Xu and Shanlin Jiang and Zhenan He", "abstract": "  Learning from a limited amount of data, namely Few-Shot Learning, stands out\nas a challenging computer vision task. Several works exploit semantics and\ndesign complicated semantic fusion mechanisms to compensate for rare\nrepresentative features within restricted data. However, relying on naive\nsemantics such as class names introduces biases due to their brevity, while\nacquiring extensive semantics from external knowledge takes a huge time and\neffort. This limitation severely constrains the potential of semantics in\nFew-Shot Learning. In this paper, we design an automatic way called Semantic\nEvolution to generate high-quality semantics. The incorporation of high-quality\nsemantics alleviates the need for complex network structures and learning\nalgorithms used in previous works. Hence, we employ a simple two-layer network\ntermed Semantic Alignment Network to transform semantics and visual features\ninto robust class prototypes with rich discriminative features for few-shot\nclassification. The experimental results show our framework outperforms all\nprevious methods on six benchmarks, demonstrating a simple network with\nhigh-quality semantics can beat intricate multi-modal modules on few-shot\nclassification tasks. Code is available at\nhttps://github.com/zhangdoudou123/SemFew.\n", "link": "http://arxiv.org/abs/2311.18649v3", "date": "2024-04-09", "relevancy": 2.0749, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5374}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5054}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5053}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simple%20Semantic-Aided%20Few-Shot%20Learning&body=Title%3A%20Simple%20Semantic-Aided%20Few-Shot%20Learning%0AAuthor%3A%20Hai%20Zhang%20and%20Junzhe%20Xu%20and%20Shanlin%20Jiang%20and%20Zhenan%20He%0AAbstract%3A%20%20%20Learning%20from%20a%20limited%20amount%20of%20data%2C%20namely%20Few-Shot%20Learning%2C%20stands%20out%0Aas%20a%20challenging%20computer%20vision%20task.%20Several%20works%20exploit%20semantics%20and%0Adesign%20complicated%20semantic%20fusion%20mechanisms%20to%20compensate%20for%20rare%0Arepresentative%20features%20within%20restricted%20data.%20However%2C%20relying%20on%20naive%0Asemantics%20such%20as%20class%20names%20introduces%20biases%20due%20to%20their%20brevity%2C%20while%0Aacquiring%20extensive%20semantics%20from%20external%20knowledge%20takes%20a%20huge%20time%20and%0Aeffort.%20This%20limitation%20severely%20constrains%20the%20potential%20of%20semantics%20in%0AFew-Shot%20Learning.%20In%20this%20paper%2C%20we%20design%20an%20automatic%20way%20called%20Semantic%0AEvolution%20to%20generate%20high-quality%20semantics.%20The%20incorporation%20of%20high-quality%0Asemantics%20alleviates%20the%20need%20for%20complex%20network%20structures%20and%20learning%0Aalgorithms%20used%20in%20previous%20works.%20Hence%2C%20we%20employ%20a%20simple%20two-layer%20network%0Atermed%20Semantic%20Alignment%20Network%20to%20transform%20semantics%20and%20visual%20features%0Ainto%20robust%20class%20prototypes%20with%20rich%20discriminative%20features%20for%20few-shot%0Aclassification.%20The%20experimental%20results%20show%20our%20framework%20outperforms%20all%0Aprevious%20methods%20on%20six%20benchmarks%2C%20demonstrating%20a%20simple%20network%20with%0Ahigh-quality%20semantics%20can%20beat%20intricate%20multi-modal%20modules%20on%20few-shot%0Aclassification%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zhangdoudou123/SemFew.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18649v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Semantic-Aided%20Few-Shot%20Learning&entry.906535625=Hai%20Zhang%20and%20Junzhe%20Xu%20and%20Shanlin%20Jiang%20and%20Zhenan%20He&entry.1292438233=%20%20Learning%20from%20a%20limited%20amount%20of%20data%2C%20namely%20Few-Shot%20Learning%2C%20stands%20out%0Aas%20a%20challenging%20computer%20vision%20task.%20Several%20works%20exploit%20semantics%20and%0Adesign%20complicated%20semantic%20fusion%20mechanisms%20to%20compensate%20for%20rare%0Arepresentative%20features%20within%20restricted%20data.%20However%2C%20relying%20on%20naive%0Asemantics%20such%20as%20class%20names%20introduces%20biases%20due%20to%20their%20brevity%2C%20while%0Aacquiring%20extensive%20semantics%20from%20external%20knowledge%20takes%20a%20huge%20time%20and%0Aeffort.%20This%20limitation%20severely%20constrains%20the%20potential%20of%20semantics%20in%0AFew-Shot%20Learning.%20In%20this%20paper%2C%20we%20design%20an%20automatic%20way%20called%20Semantic%0AEvolution%20to%20generate%20high-quality%20semantics.%20The%20incorporation%20of%20high-quality%0Asemantics%20alleviates%20the%20need%20for%20complex%20network%20structures%20and%20learning%0Aalgorithms%20used%20in%20previous%20works.%20Hence%2C%20we%20employ%20a%20simple%20two-layer%20network%0Atermed%20Semantic%20Alignment%20Network%20to%20transform%20semantics%20and%20visual%20features%0Ainto%20robust%20class%20prototypes%20with%20rich%20discriminative%20features%20for%20few-shot%0Aclassification.%20The%20experimental%20results%20show%20our%20framework%20outperforms%20all%0Aprevious%20methods%20on%20six%20benchmarks%2C%20demonstrating%20a%20simple%20network%20with%0Ahigh-quality%20semantics%20can%20beat%20intricate%20multi-modal%20modules%20on%20few-shot%0Aclassification%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zhangdoudou123/SemFew.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18649v3&entry.124074799=Read"},
{"title": "Dynamic Backtracking in GFlowNets: Enhancing Decision Steps with\n  Reward-Dependent Adjustment Mechanisms", "author": "Shuai Guo and Jielei Chu and Lei Zhu and Tianrui Li", "abstract": "  Generative Flow Networks (GFlowNets) are probabilistic models predicated on\nMarkov flows, employing specific amortization algorithms to learn stochastic\npolicies that generate compositional substances including biomolecules,\nchemical materials, and more. Demonstrating formidable prowess in generating\nhigh-performance biochemical molecules, GFlowNets accelerate the discovery of\nscientific substances, effectively circumventing the time-consuming,\nlabor-intensive, and costly shortcomings intrinsic to conventional material\ndiscovery. However, previous work often struggles to accumulate exploratory\nexperience and is prone to becoming disoriented within expansive sampling\nspaces. Attempts to address this issue, such as LS-GFN, are limited to local\ngreedy searches and lack broader global adjustments. This paper introduces a\nnovel GFlowNets variant, the Dynamic Backtracking GFN (DB-GFN), which enhances\nthe adaptability of decision-making steps through a reward-based dynamic\nbacktracking mechanism. DB-GFN permits backtracking during the network\nconstruction process according to the current state's reward value, thus\ncorrecting disadvantageous decisions and exploring alternative pathways during\nthe exploration process. Applied to generative tasks of biochemical molecules\nand genetic material sequences, DB-GFN surpasses existing GFlowNets models and\ntraditional reinforcement learning methods in terms of sample quality,\nexploration sample quantity, and training convergence speed. Furthermore, the\northogonal nature of DB-GFN suggests its potential as a powerful tool for\nfuture improvements in GFlowNets, with the promise of integrating with other\nstrategies to achieve more efficient search performance.\n", "link": "http://arxiv.org/abs/2404.05576v2", "date": "2024-04-09", "relevancy": 2.0717, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5494}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5064}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.491}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Backtracking%20in%20GFlowNets%3A%20Enhancing%20Decision%20Steps%20with%0A%20%20Reward-Dependent%20Adjustment%20Mechanisms&body=Title%3A%20Dynamic%20Backtracking%20in%20GFlowNets%3A%20Enhancing%20Decision%20Steps%20with%0A%20%20Reward-Dependent%20Adjustment%20Mechanisms%0AAuthor%3A%20Shuai%20Guo%20and%20Jielei%20Chu%20and%20Lei%20Zhu%20and%20Tianrui%20Li%0AAbstract%3A%20%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20are%20probabilistic%20models%20predicated%20on%0AMarkov%20flows%2C%20employing%20specific%20amortization%20algorithms%20to%20learn%20stochastic%0Apolicies%20that%20generate%20compositional%20substances%20including%20biomolecules%2C%0Achemical%20materials%2C%20and%20more.%20Demonstrating%20formidable%20prowess%20in%20generating%0Ahigh-performance%20biochemical%20molecules%2C%20GFlowNets%20accelerate%20the%20discovery%20of%0Ascientific%20substances%2C%20effectively%20circumventing%20the%20time-consuming%2C%0Alabor-intensive%2C%20and%20costly%20shortcomings%20intrinsic%20to%20conventional%20material%0Adiscovery.%20However%2C%20previous%20work%20often%20struggles%20to%20accumulate%20exploratory%0Aexperience%20and%20is%20prone%20to%20becoming%20disoriented%20within%20expansive%20sampling%0Aspaces.%20Attempts%20to%20address%20this%20issue%2C%20such%20as%20LS-GFN%2C%20are%20limited%20to%20local%0Agreedy%20searches%20and%20lack%20broader%20global%20adjustments.%20This%20paper%20introduces%20a%0Anovel%20GFlowNets%20variant%2C%20the%20Dynamic%20Backtracking%20GFN%20%28DB-GFN%29%2C%20which%20enhances%0Athe%20adaptability%20of%20decision-making%20steps%20through%20a%20reward-based%20dynamic%0Abacktracking%20mechanism.%20DB-GFN%20permits%20backtracking%20during%20the%20network%0Aconstruction%20process%20according%20to%20the%20current%20state%27s%20reward%20value%2C%20thus%0Acorrecting%20disadvantageous%20decisions%20and%20exploring%20alternative%20pathways%20during%0Athe%20exploration%20process.%20Applied%20to%20generative%20tasks%20of%20biochemical%20molecules%0Aand%20genetic%20material%20sequences%2C%20DB-GFN%20surpasses%20existing%20GFlowNets%20models%20and%0Atraditional%20reinforcement%20learning%20methods%20in%20terms%20of%20sample%20quality%2C%0Aexploration%20sample%20quantity%2C%20and%20training%20convergence%20speed.%20Furthermore%2C%20the%0Aorthogonal%20nature%20of%20DB-GFN%20suggests%20its%20potential%20as%20a%20powerful%20tool%20for%0Afuture%20improvements%20in%20GFlowNets%2C%20with%20the%20promise%20of%20integrating%20with%20other%0Astrategies%20to%20achieve%20more%20efficient%20search%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05576v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Backtracking%20in%20GFlowNets%3A%20Enhancing%20Decision%20Steps%20with%0A%20%20Reward-Dependent%20Adjustment%20Mechanisms&entry.906535625=Shuai%20Guo%20and%20Jielei%20Chu%20and%20Lei%20Zhu%20and%20Tianrui%20Li&entry.1292438233=%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20are%20probabilistic%20models%20predicated%20on%0AMarkov%20flows%2C%20employing%20specific%20amortization%20algorithms%20to%20learn%20stochastic%0Apolicies%20that%20generate%20compositional%20substances%20including%20biomolecules%2C%0Achemical%20materials%2C%20and%20more.%20Demonstrating%20formidable%20prowess%20in%20generating%0Ahigh-performance%20biochemical%20molecules%2C%20GFlowNets%20accelerate%20the%20discovery%20of%0Ascientific%20substances%2C%20effectively%20circumventing%20the%20time-consuming%2C%0Alabor-intensive%2C%20and%20costly%20shortcomings%20intrinsic%20to%20conventional%20material%0Adiscovery.%20However%2C%20previous%20work%20often%20struggles%20to%20accumulate%20exploratory%0Aexperience%20and%20is%20prone%20to%20becoming%20disoriented%20within%20expansive%20sampling%0Aspaces.%20Attempts%20to%20address%20this%20issue%2C%20such%20as%20LS-GFN%2C%20are%20limited%20to%20local%0Agreedy%20searches%20and%20lack%20broader%20global%20adjustments.%20This%20paper%20introduces%20a%0Anovel%20GFlowNets%20variant%2C%20the%20Dynamic%20Backtracking%20GFN%20%28DB-GFN%29%2C%20which%20enhances%0Athe%20adaptability%20of%20decision-making%20steps%20through%20a%20reward-based%20dynamic%0Abacktracking%20mechanism.%20DB-GFN%20permits%20backtracking%20during%20the%20network%0Aconstruction%20process%20according%20to%20the%20current%20state%27s%20reward%20value%2C%20thus%0Acorrecting%20disadvantageous%20decisions%20and%20exploring%20alternative%20pathways%20during%0Athe%20exploration%20process.%20Applied%20to%20generative%20tasks%20of%20biochemical%20molecules%0Aand%20genetic%20material%20sequences%2C%20DB-GFN%20surpasses%20existing%20GFlowNets%20models%20and%0Atraditional%20reinforcement%20learning%20methods%20in%20terms%20of%20sample%20quality%2C%0Aexploration%20sample%20quantity%2C%20and%20training%20convergence%20speed.%20Furthermore%2C%20the%0Aorthogonal%20nature%20of%20DB-GFN%20suggests%20its%20potential%20as%20a%20powerful%20tool%20for%0Afuture%20improvements%20in%20GFlowNets%2C%20with%20the%20promise%20of%20integrating%20with%20other%0Astrategies%20to%20achieve%20more%20efficient%20search%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05576v2&entry.124074799=Read"},
{"title": "Multi-AGV Path Planning Method via Reinforcement Learning and Particle\n  Filters", "author": "Shao Shuo", "abstract": "  The Reinforcement Learning (RL) algorithm, renowned for its robust learning\ncapability and search stability, has garnered significant attention and found\nextensive application in Automated Guided Vehicle (AGV) path planning. However,\nRL planning algorithms encounter challenges stemming from the substantial\nvariance of neural networks caused by environmental instability and significant\nfluctuations in system structure. These challenges manifest in slow convergence\nspeed and low learning efficiency. To tackle this issue, this paper presents\nthe Particle Filter-Double Deep Q-Network (PF-DDQN) approach, which\nincorporates the Particle Filter (PF) into multi-AGV reinforcement learning\npath planning. The PF-DDQN method leverages the imprecise weight values of the\nnetwork as state values to formulate the state space equation. Through the\niterative fusion process of neural networks and particle filters, the DDQN\nmodel is optimized to acquire the optimal true weight values, thus enhancing\nthe algorithm's efficiency. The proposed method's effectiveness and superiority\nare validated through numerical simulations. Overall, the simulation results\ndemonstrate that the proposed algorithm surpasses the traditional DDQN\nalgorithm in terms of path planning superiority and training time indicators by\n92.62% and 76.88%, respectively. In conclusion, the PF-DDQN method addresses\nthe challenges encountered by RL planning algorithms in AGV path planning. By\nintegrating the Particle Filter and optimizing the DDQN model, the proposed\nmethod achieves enhanced efficiency and outperforms the traditional DDQN\nalgorithm in terms of path planning superiority and training time indicators.\n", "link": "http://arxiv.org/abs/2403.18236v3", "date": "2024-04-09", "relevancy": 2.0507, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5374}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5183}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4972}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-AGV%20Path%20Planning%20Method%20via%20Reinforcement%20Learning%20and%20Particle%0A%20%20Filters&body=Title%3A%20Multi-AGV%20Path%20Planning%20Method%20via%20Reinforcement%20Learning%20and%20Particle%0A%20%20Filters%0AAuthor%3A%20Shao%20Shuo%0AAbstract%3A%20%20%20The%20Reinforcement%20Learning%20%28RL%29%20algorithm%2C%20renowned%20for%20its%20robust%20learning%0Acapability%20and%20search%20stability%2C%20has%20garnered%20significant%20attention%20and%20found%0Aextensive%20application%20in%20Automated%20Guided%20Vehicle%20%28AGV%29%20path%20planning.%20However%2C%0ARL%20planning%20algorithms%20encounter%20challenges%20stemming%20from%20the%20substantial%0Avariance%20of%20neural%20networks%20caused%20by%20environmental%20instability%20and%20significant%0Afluctuations%20in%20system%20structure.%20These%20challenges%20manifest%20in%20slow%20convergence%0Aspeed%20and%20low%20learning%20efficiency.%20To%20tackle%20this%20issue%2C%20this%20paper%20presents%0Athe%20Particle%20Filter-Double%20Deep%20Q-Network%20%28PF-DDQN%29%20approach%2C%20which%0Aincorporates%20the%20Particle%20Filter%20%28PF%29%20into%20multi-AGV%20reinforcement%20learning%0Apath%20planning.%20The%20PF-DDQN%20method%20leverages%20the%20imprecise%20weight%20values%20of%20the%0Anetwork%20as%20state%20values%20to%20formulate%20the%20state%20space%20equation.%20Through%20the%0Aiterative%20fusion%20process%20of%20neural%20networks%20and%20particle%20filters%2C%20the%20DDQN%0Amodel%20is%20optimized%20to%20acquire%20the%20optimal%20true%20weight%20values%2C%20thus%20enhancing%0Athe%20algorithm%27s%20efficiency.%20The%20proposed%20method%27s%20effectiveness%20and%20superiority%0Aare%20validated%20through%20numerical%20simulations.%20Overall%2C%20the%20simulation%20results%0Ademonstrate%20that%20the%20proposed%20algorithm%20surpasses%20the%20traditional%20DDQN%0Aalgorithm%20in%20terms%20of%20path%20planning%20superiority%20and%20training%20time%20indicators%20by%0A92.62%25%20and%2076.88%25%2C%20respectively.%20In%20conclusion%2C%20the%20PF-DDQN%20method%20addresses%0Athe%20challenges%20encountered%20by%20RL%20planning%20algorithms%20in%20AGV%20path%20planning.%20By%0Aintegrating%20the%20Particle%20Filter%20and%20optimizing%20the%20DDQN%20model%2C%20the%20proposed%0Amethod%20achieves%20enhanced%20efficiency%20and%20outperforms%20the%20traditional%20DDQN%0Aalgorithm%20in%20terms%20of%20path%20planning%20superiority%20and%20training%20time%20indicators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18236v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-AGV%20Path%20Planning%20Method%20via%20Reinforcement%20Learning%20and%20Particle%0A%20%20Filters&entry.906535625=Shao%20Shuo&entry.1292438233=%20%20The%20Reinforcement%20Learning%20%28RL%29%20algorithm%2C%20renowned%20for%20its%20robust%20learning%0Acapability%20and%20search%20stability%2C%20has%20garnered%20significant%20attention%20and%20found%0Aextensive%20application%20in%20Automated%20Guided%20Vehicle%20%28AGV%29%20path%20planning.%20However%2C%0ARL%20planning%20algorithms%20encounter%20challenges%20stemming%20from%20the%20substantial%0Avariance%20of%20neural%20networks%20caused%20by%20environmental%20instability%20and%20significant%0Afluctuations%20in%20system%20structure.%20These%20challenges%20manifest%20in%20slow%20convergence%0Aspeed%20and%20low%20learning%20efficiency.%20To%20tackle%20this%20issue%2C%20this%20paper%20presents%0Athe%20Particle%20Filter-Double%20Deep%20Q-Network%20%28PF-DDQN%29%20approach%2C%20which%0Aincorporates%20the%20Particle%20Filter%20%28PF%29%20into%20multi-AGV%20reinforcement%20learning%0Apath%20planning.%20The%20PF-DDQN%20method%20leverages%20the%20imprecise%20weight%20values%20of%20the%0Anetwork%20as%20state%20values%20to%20formulate%20the%20state%20space%20equation.%20Through%20the%0Aiterative%20fusion%20process%20of%20neural%20networks%20and%20particle%20filters%2C%20the%20DDQN%0Amodel%20is%20optimized%20to%20acquire%20the%20optimal%20true%20weight%20values%2C%20thus%20enhancing%0Athe%20algorithm%27s%20efficiency.%20The%20proposed%20method%27s%20effectiveness%20and%20superiority%0Aare%20validated%20through%20numerical%20simulations.%20Overall%2C%20the%20simulation%20results%0Ademonstrate%20that%20the%20proposed%20algorithm%20surpasses%20the%20traditional%20DDQN%0Aalgorithm%20in%20terms%20of%20path%20planning%20superiority%20and%20training%20time%20indicators%20by%0A92.62%25%20and%2076.88%25%2C%20respectively.%20In%20conclusion%2C%20the%20PF-DDQN%20method%20addresses%0Athe%20challenges%20encountered%20by%20RL%20planning%20algorithms%20in%20AGV%20path%20planning.%20By%0Aintegrating%20the%20Particle%20Filter%20and%20optimizing%20the%20DDQN%20model%2C%20the%20proposed%0Amethod%20achieves%20enhanced%20efficiency%20and%20outperforms%20the%20traditional%20DDQN%0Aalgorithm%20in%20terms%20of%20path%20planning%20superiority%20and%20training%20time%20indicators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18236v3&entry.124074799=Read"},
{"title": "Towards generalizing deep-audio fake detection networks", "author": "Konstantin Gasenzer and Moritz Wolter", "abstract": "  Today's generative neural networks allow the creation of high-quality\nsynthetic speech at scale. While we welcome the creative use of this new\ntechnology, we must also recognize the risks. As synthetic speech is abused for\nmonetary and identity theft, we require a broad set of deepfake identification\ntools. Furthermore, previous work reported a limited ability of deep\nclassifiers to generalize to unseen audio generators. We study the frequency\ndomain fingerprints of current audio generators. Building on top of the\ndiscovered frequency footprints, we train excellent lightweight detectors that\ngeneralize. We report improved results on the WaveFake dataset and an extended\nversion. To account for the rapid progress in the field, we extend the WaveFake\ndataset by additionally considering samples drawn from the novel Avocodo and\nBigVGAN networks. For illustration purposes, the supplementary material\ncontains audio samples of generator artifacts.\n", "link": "http://arxiv.org/abs/2305.13033v3", "date": "2024-04-09", "relevancy": 2.0421, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5212}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5136}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4986}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20generalizing%20deep-audio%20fake%20detection%20networks&body=Title%3A%20Towards%20generalizing%20deep-audio%20fake%20detection%20networks%0AAuthor%3A%20Konstantin%20Gasenzer%20and%20Moritz%20Wolter%0AAbstract%3A%20%20%20Today%27s%20generative%20neural%20networks%20allow%20the%20creation%20of%20high-quality%0Asynthetic%20speech%20at%20scale.%20While%20we%20welcome%20the%20creative%20use%20of%20this%20new%0Atechnology%2C%20we%20must%20also%20recognize%20the%20risks.%20As%20synthetic%20speech%20is%20abused%20for%0Amonetary%20and%20identity%20theft%2C%20we%20require%20a%20broad%20set%20of%20deepfake%20identification%0Atools.%20Furthermore%2C%20previous%20work%20reported%20a%20limited%20ability%20of%20deep%0Aclassifiers%20to%20generalize%20to%20unseen%20audio%20generators.%20We%20study%20the%20frequency%0Adomain%20fingerprints%20of%20current%20audio%20generators.%20Building%20on%20top%20of%20the%0Adiscovered%20frequency%20footprints%2C%20we%20train%20excellent%20lightweight%20detectors%20that%0Ageneralize.%20We%20report%20improved%20results%20on%20the%20WaveFake%20dataset%20and%20an%20extended%0Aversion.%20To%20account%20for%20the%20rapid%20progress%20in%20the%20field%2C%20we%20extend%20the%20WaveFake%0Adataset%20by%20additionally%20considering%20samples%20drawn%20from%20the%20novel%20Avocodo%20and%0ABigVGAN%20networks.%20For%20illustration%20purposes%2C%20the%20supplementary%20material%0Acontains%20audio%20samples%20of%20generator%20artifacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.13033v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20generalizing%20deep-audio%20fake%20detection%20networks&entry.906535625=Konstantin%20Gasenzer%20and%20Moritz%20Wolter&entry.1292438233=%20%20Today%27s%20generative%20neural%20networks%20allow%20the%20creation%20of%20high-quality%0Asynthetic%20speech%20at%20scale.%20While%20we%20welcome%20the%20creative%20use%20of%20this%20new%0Atechnology%2C%20we%20must%20also%20recognize%20the%20risks.%20As%20synthetic%20speech%20is%20abused%20for%0Amonetary%20and%20identity%20theft%2C%20we%20require%20a%20broad%20set%20of%20deepfake%20identification%0Atools.%20Furthermore%2C%20previous%20work%20reported%20a%20limited%20ability%20of%20deep%0Aclassifiers%20to%20generalize%20to%20unseen%20audio%20generators.%20We%20study%20the%20frequency%0Adomain%20fingerprints%20of%20current%20audio%20generators.%20Building%20on%20top%20of%20the%0Adiscovered%20frequency%20footprints%2C%20we%20train%20excellent%20lightweight%20detectors%20that%0Ageneralize.%20We%20report%20improved%20results%20on%20the%20WaveFake%20dataset%20and%20an%20extended%0Aversion.%20To%20account%20for%20the%20rapid%20progress%20in%20the%20field%2C%20we%20extend%20the%20WaveFake%0Adataset%20by%20additionally%20considering%20samples%20drawn%20from%20the%20novel%20Avocodo%20and%0ABigVGAN%20networks.%20For%20illustration%20purposes%2C%20the%20supplementary%20material%0Acontains%20audio%20samples%20of%20generator%20artifacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.13033v3&entry.124074799=Read"},
{"title": "Bayesian Survival Analysis by Approximate Inference of Neural Networks", "author": "Christian Marius Lillelund and Martin Magris and Christian Fischer Pedersen", "abstract": "  Predicting future events always comes with uncertainty, but traditional\nnon-Bayesian methods cannot distinguish certain from uncertain predictions or\nexplain the confidence in their predictions. In survival analysis, Bayesian\nmethods applied to state-of-the-art solutions in the healthcare and biomedical\nfield are still novel, and their implications have not been fully evaluated. In\nthis paper, we study the benefits of modeling uncertainty in deep neural\nnetworks for survival analysis with a focus on prediction and calibration\nperformance. For this, we present a Bayesian deep learning framework that\nconsists of three Bayesian network architectures, which we train by optimizing\nthe Cox partial likelihood and combining input-dependent aleatoric uncertainty\nwith model-specific epistemic uncertainty. This enables us to provide\nuncertainty estimates as credible intervals when predicting the survival curve\nor as a probability density function over the predicted median survival times.\nFor our empirical analyses, we evaluated our proposed method on four benchmark\ndatasets and found that our method demonstrates prediction performance\ncomparable to the state-of-the-art based on the concordance index and\noutperforms all other Cox-based approaches in terms of the mean absolute error.\nOur work explicitly compares the extent to which different Bayesian\napproximation techniques differ from each other and improves the prediction\nover traditional non-Bayesian alternatives.\n", "link": "http://arxiv.org/abs/2404.06421v1", "date": "2024-04-09", "relevancy": 2.0328, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5501}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5268}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4728}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Survival%20Analysis%20by%20Approximate%20Inference%20of%20Neural%20Networks&body=Title%3A%20Bayesian%20Survival%20Analysis%20by%20Approximate%20Inference%20of%20Neural%20Networks%0AAuthor%3A%20Christian%20Marius%20Lillelund%20and%20Martin%20Magris%20and%20Christian%20Fischer%20Pedersen%0AAbstract%3A%20%20%20Predicting%20future%20events%20always%20comes%20with%20uncertainty%2C%20but%20traditional%0Anon-Bayesian%20methods%20cannot%20distinguish%20certain%20from%20uncertain%20predictions%20or%0Aexplain%20the%20confidence%20in%20their%20predictions.%20In%20survival%20analysis%2C%20Bayesian%0Amethods%20applied%20to%20state-of-the-art%20solutions%20in%20the%20healthcare%20and%20biomedical%0Afield%20are%20still%20novel%2C%20and%20their%20implications%20have%20not%20been%20fully%20evaluated.%20In%0Athis%20paper%2C%20we%20study%20the%20benefits%20of%20modeling%20uncertainty%20in%20deep%20neural%0Anetworks%20for%20survival%20analysis%20with%20a%20focus%20on%20prediction%20and%20calibration%0Aperformance.%20For%20this%2C%20we%20present%20a%20Bayesian%20deep%20learning%20framework%20that%0Aconsists%20of%20three%20Bayesian%20network%20architectures%2C%20which%20we%20train%20by%20optimizing%0Athe%20Cox%20partial%20likelihood%20and%20combining%20input-dependent%20aleatoric%20uncertainty%0Awith%20model-specific%20epistemic%20uncertainty.%20This%20enables%20us%20to%20provide%0Auncertainty%20estimates%20as%20credible%20intervals%20when%20predicting%20the%20survival%20curve%0Aor%20as%20a%20probability%20density%20function%20over%20the%20predicted%20median%20survival%20times.%0AFor%20our%20empirical%20analyses%2C%20we%20evaluated%20our%20proposed%20method%20on%20four%20benchmark%0Adatasets%20and%20found%20that%20our%20method%20demonstrates%20prediction%20performance%0Acomparable%20to%20the%20state-of-the-art%20based%20on%20the%20concordance%20index%20and%0Aoutperforms%20all%20other%20Cox-based%20approaches%20in%20terms%20of%20the%20mean%20absolute%20error.%0AOur%20work%20explicitly%20compares%20the%20extent%20to%20which%20different%20Bayesian%0Aapproximation%20techniques%20differ%20from%20each%20other%20and%20improves%20the%20prediction%0Aover%20traditional%20non-Bayesian%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06421v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Survival%20Analysis%20by%20Approximate%20Inference%20of%20Neural%20Networks&entry.906535625=Christian%20Marius%20Lillelund%20and%20Martin%20Magris%20and%20Christian%20Fischer%20Pedersen&entry.1292438233=%20%20Predicting%20future%20events%20always%20comes%20with%20uncertainty%2C%20but%20traditional%0Anon-Bayesian%20methods%20cannot%20distinguish%20certain%20from%20uncertain%20predictions%20or%0Aexplain%20the%20confidence%20in%20their%20predictions.%20In%20survival%20analysis%2C%20Bayesian%0Amethods%20applied%20to%20state-of-the-art%20solutions%20in%20the%20healthcare%20and%20biomedical%0Afield%20are%20still%20novel%2C%20and%20their%20implications%20have%20not%20been%20fully%20evaluated.%20In%0Athis%20paper%2C%20we%20study%20the%20benefits%20of%20modeling%20uncertainty%20in%20deep%20neural%0Anetworks%20for%20survival%20analysis%20with%20a%20focus%20on%20prediction%20and%20calibration%0Aperformance.%20For%20this%2C%20we%20present%20a%20Bayesian%20deep%20learning%20framework%20that%0Aconsists%20of%20three%20Bayesian%20network%20architectures%2C%20which%20we%20train%20by%20optimizing%0Athe%20Cox%20partial%20likelihood%20and%20combining%20input-dependent%20aleatoric%20uncertainty%0Awith%20model-specific%20epistemic%20uncertainty.%20This%20enables%20us%20to%20provide%0Auncertainty%20estimates%20as%20credible%20intervals%20when%20predicting%20the%20survival%20curve%0Aor%20as%20a%20probability%20density%20function%20over%20the%20predicted%20median%20survival%20times.%0AFor%20our%20empirical%20analyses%2C%20we%20evaluated%20our%20proposed%20method%20on%20four%20benchmark%0Adatasets%20and%20found%20that%20our%20method%20demonstrates%20prediction%20performance%0Acomparable%20to%20the%20state-of-the-art%20based%20on%20the%20concordance%20index%20and%0Aoutperforms%20all%20other%20Cox-based%20approaches%20in%20terms%20of%20the%20mean%20absolute%20error.%0AOur%20work%20explicitly%20compares%20the%20extent%20to%20which%20different%20Bayesian%0Aapproximation%20techniques%20differ%20from%20each%20other%20and%20improves%20the%20prediction%0Aover%20traditional%20non-Bayesian%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06421v1&entry.124074799=Read"},
{"title": "Model Generation from Requirements with LLMs: an Exploratory Study", "author": "Alessio Ferrari and Sallam Abualhaija and Chetan Arora", "abstract": "  Complementing natural language (NL) requirements with graphical models can\nimprove stakeholders' communication and provide directions for system design.\nHowever, creating models from requirements involves manual effort. The advent\nof generative large language models (LLMs), ChatGPT being a notable example,\noffers promising avenues for automated assistance in model generation. This\npaper investigates the capability of ChatGPT to generate a specific type of\nmodel, i.e., UML sequence diagrams, from NL requirements. We conduct a\nqualitative study in which we examine the sequence diagrams generated by\nChatGPT for 28 requirements documents of various types and from different\ndomains. Observations from the analysis of the generated diagrams have\nsystematically been captured through evaluation logs, and categorized through\nthematic analysis. Our results indicate that, although the models generally\nconform to the standard and exhibit a reasonable level of understandability,\ntheir completeness and correctness with respect to the specified requirements\noften present challenges. This issue is particularly pronounced in the presence\nof requirements smells, such as ambiguity and inconsistency. The insights\nderived from this study can influence the practical utilization of LLMs in the\nRE process, and open the door to novel RE-specific prompting strategies\ntargeting effective model generation.\n", "link": "http://arxiv.org/abs/2404.06371v1", "date": "2024-04-09", "relevancy": 2.0234, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5099}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5045}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4992}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Model%20Generation%20from%20Requirements%20with%20LLMs%3A%20an%20Exploratory%20Study&body=Title%3A%20Model%20Generation%20from%20Requirements%20with%20LLMs%3A%20an%20Exploratory%20Study%0AAuthor%3A%20Alessio%20Ferrari%20and%20Sallam%20Abualhaija%20and%20Chetan%20Arora%0AAbstract%3A%20%20%20Complementing%20natural%20language%20%28NL%29%20requirements%20with%20graphical%20models%20can%0Aimprove%20stakeholders%27%20communication%20and%20provide%20directions%20for%20system%20design.%0AHowever%2C%20creating%20models%20from%20requirements%20involves%20manual%20effort.%20The%20advent%0Aof%20generative%20large%20language%20models%20%28LLMs%29%2C%20ChatGPT%20being%20a%20notable%20example%2C%0Aoffers%20promising%20avenues%20for%20automated%20assistance%20in%20model%20generation.%20This%0Apaper%20investigates%20the%20capability%20of%20ChatGPT%20to%20generate%20a%20specific%20type%20of%0Amodel%2C%20i.e.%2C%20UML%20sequence%20diagrams%2C%20from%20NL%20requirements.%20We%20conduct%20a%0Aqualitative%20study%20in%20which%20we%20examine%20the%20sequence%20diagrams%20generated%20by%0AChatGPT%20for%2028%20requirements%20documents%20of%20various%20types%20and%20from%20different%0Adomains.%20Observations%20from%20the%20analysis%20of%20the%20generated%20diagrams%20have%0Asystematically%20been%20captured%20through%20evaluation%20logs%2C%20and%20categorized%20through%0Athematic%20analysis.%20Our%20results%20indicate%20that%2C%20although%20the%20models%20generally%0Aconform%20to%20the%20standard%20and%20exhibit%20a%20reasonable%20level%20of%20understandability%2C%0Atheir%20completeness%20and%20correctness%20with%20respect%20to%20the%20specified%20requirements%0Aoften%20present%20challenges.%20This%20issue%20is%20particularly%20pronounced%20in%20the%20presence%0Aof%20requirements%20smells%2C%20such%20as%20ambiguity%20and%20inconsistency.%20The%20insights%0Aderived%20from%20this%20study%20can%20influence%20the%20practical%20utilization%20of%20LLMs%20in%20the%0ARE%20process%2C%20and%20open%20the%20door%20to%20novel%20RE-specific%20prompting%20strategies%0Atargeting%20effective%20model%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06371v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Generation%20from%20Requirements%20with%20LLMs%3A%20an%20Exploratory%20Study&entry.906535625=Alessio%20Ferrari%20and%20Sallam%20Abualhaija%20and%20Chetan%20Arora&entry.1292438233=%20%20Complementing%20natural%20language%20%28NL%29%20requirements%20with%20graphical%20models%20can%0Aimprove%20stakeholders%27%20communication%20and%20provide%20directions%20for%20system%20design.%0AHowever%2C%20creating%20models%20from%20requirements%20involves%20manual%20effort.%20The%20advent%0Aof%20generative%20large%20language%20models%20%28LLMs%29%2C%20ChatGPT%20being%20a%20notable%20example%2C%0Aoffers%20promising%20avenues%20for%20automated%20assistance%20in%20model%20generation.%20This%0Apaper%20investigates%20the%20capability%20of%20ChatGPT%20to%20generate%20a%20specific%20type%20of%0Amodel%2C%20i.e.%2C%20UML%20sequence%20diagrams%2C%20from%20NL%20requirements.%20We%20conduct%20a%0Aqualitative%20study%20in%20which%20we%20examine%20the%20sequence%20diagrams%20generated%20by%0AChatGPT%20for%2028%20requirements%20documents%20of%20various%20types%20and%20from%20different%0Adomains.%20Observations%20from%20the%20analysis%20of%20the%20generated%20diagrams%20have%0Asystematically%20been%20captured%20through%20evaluation%20logs%2C%20and%20categorized%20through%0Athematic%20analysis.%20Our%20results%20indicate%20that%2C%20although%20the%20models%20generally%0Aconform%20to%20the%20standard%20and%20exhibit%20a%20reasonable%20level%20of%20understandability%2C%0Atheir%20completeness%20and%20correctness%20with%20respect%20to%20the%20specified%20requirements%0Aoften%20present%20challenges.%20This%20issue%20is%20particularly%20pronounced%20in%20the%20presence%0Aof%20requirements%20smells%2C%20such%20as%20ambiguity%20and%20inconsistency.%20The%20insights%0Aderived%20from%20this%20study%20can%20influence%20the%20practical%20utilization%20of%20LLMs%20in%20the%0ARE%20process%2C%20and%20open%20the%20door%20to%20novel%20RE-specific%20prompting%20strategies%0Atargeting%20effective%20model%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06371v1&entry.124074799=Read"},
{"title": "Generative Pre-Trained Transformer for Symbolic Regression Base\n  In-Context Reinforcement Learning", "author": "Yanjie Li and Weijun Li and Lina Yu and Min Wu and Jingyi Liu and Wenqiang Li and Meilan Hao and Shu Wei and Yusong Deng", "abstract": "  The mathematical formula is the human language to describe nature and is the\nessence of scientific research. Finding mathematical formulas from\nobservational data is a major demand of scientific research and a major\nchallenge of artificial intelligence. This area is called symbolic regression.\nOriginally symbolic regression was often formulated as a combinatorial\noptimization problem and solved using GP or reinforcement learning algorithms.\nThese two kinds of algorithms have strong noise robustness ability and good\nVersatility. However, inference time usually takes a long time, so the search\nefficiency is relatively low. Later, based on large-scale pre-training data\nproposed, such methods use a large number of synthetic data points and\nexpression pairs to train a Generative Pre-Trained Transformer(GPT). Then this\nGPT can only need to perform one forward propagation to obtain the results, the\nadvantage is that the inference speed is very fast. However, its performance is\nvery dependent on the training data and performs poorly on data outside the\ntraining set, which leads to poor noise robustness and Versatility of such\nmethods. So, can we combine the advantages of the above two categories of SR\nalgorithms? In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT\nusing massive sparse reward learning histories of reinforcement learning-based\nSR algorithms as training data. After training, the SR algorithm based on\nreinforcement learning is distilled into a Transformer. When new test data\ncomes, FormulaGPT can directly generate a \"reinforcement learning process\" and\nautomatically update the learning policy in context. Tested on more than ten\ndatasets including SRBench, formulaGPT achieves the state-of-the-art\nperformance in fitting ability compared with four baselines. In addition, it\nachieves satisfactory results in noise robustness, versatility, and inference\nefficiency.\n", "link": "http://arxiv.org/abs/2404.06330v1", "date": "2024-04-09", "relevancy": 2.0199, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5588}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5273}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4611}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generative%20Pre-Trained%20Transformer%20for%20Symbolic%20Regression%20Base%0A%20%20In-Context%20Reinforcement%20Learning&body=Title%3A%20Generative%20Pre-Trained%20Transformer%20for%20Symbolic%20Regression%20Base%0A%20%20In-Context%20Reinforcement%20Learning%0AAuthor%3A%20Yanjie%20Li%20and%20Weijun%20Li%20and%20Lina%20Yu%20and%20Min%20Wu%20and%20Jingyi%20Liu%20and%20Wenqiang%20Li%20and%20Meilan%20Hao%20and%20Shu%20Wei%20and%20Yusong%20Deng%0AAbstract%3A%20%20%20The%20mathematical%20formula%20is%20the%20human%20language%20to%20describe%20nature%20and%20is%20the%0Aessence%20of%20scientific%20research.%20Finding%20mathematical%20formulas%20from%0Aobservational%20data%20is%20a%20major%20demand%20of%20scientific%20research%20and%20a%20major%0Achallenge%20of%20artificial%20intelligence.%20This%20area%20is%20called%20symbolic%20regression.%0AOriginally%20symbolic%20regression%20was%20often%20formulated%20as%20a%20combinatorial%0Aoptimization%20problem%20and%20solved%20using%20GP%20or%20reinforcement%20learning%20algorithms.%0AThese%20two%20kinds%20of%20algorithms%20have%20strong%20noise%20robustness%20ability%20and%20good%0AVersatility.%20However%2C%20inference%20time%20usually%20takes%20a%20long%20time%2C%20so%20the%20search%0Aefficiency%20is%20relatively%20low.%20Later%2C%20based%20on%20large-scale%20pre-training%20data%0Aproposed%2C%20such%20methods%20use%20a%20large%20number%20of%20synthetic%20data%20points%20and%0Aexpression%20pairs%20to%20train%20a%20Generative%20Pre-Trained%20Transformer%28GPT%29.%20Then%20this%0AGPT%20can%20only%20need%20to%20perform%20one%20forward%20propagation%20to%20obtain%20the%20results%2C%20the%0Aadvantage%20is%20that%20the%20inference%20speed%20is%20very%20fast.%20However%2C%20its%20performance%20is%0Avery%20dependent%20on%20the%20training%20data%20and%20performs%20poorly%20on%20data%20outside%20the%0Atraining%20set%2C%20which%20leads%20to%20poor%20noise%20robustness%20and%20Versatility%20of%20such%0Amethods.%20So%2C%20can%20we%20combine%20the%20advantages%20of%20the%20above%20two%20categories%20of%20SR%0Aalgorithms%3F%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BFormulaGPT%7D%2C%20which%20trains%20a%20GPT%0Ausing%20massive%20sparse%20reward%20learning%20histories%20of%20reinforcement%20learning-based%0ASR%20algorithms%20as%20training%20data.%20After%20training%2C%20the%20SR%20algorithm%20based%20on%0Areinforcement%20learning%20is%20distilled%20into%20a%20Transformer.%20When%20new%20test%20data%0Acomes%2C%20FormulaGPT%20can%20directly%20generate%20a%20%22reinforcement%20learning%20process%22%20and%0Aautomatically%20update%20the%20learning%20policy%20in%20context.%20Tested%20on%20more%20than%20ten%0Adatasets%20including%20SRBench%2C%20formulaGPT%20achieves%20the%20state-of-the-art%0Aperformance%20in%20fitting%20ability%20compared%20with%20four%20baselines.%20In%20addition%2C%20it%0Aachieves%20satisfactory%20results%20in%20noise%20robustness%2C%20versatility%2C%20and%20inference%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06330v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Pre-Trained%20Transformer%20for%20Symbolic%20Regression%20Base%0A%20%20In-Context%20Reinforcement%20Learning&entry.906535625=Yanjie%20Li%20and%20Weijun%20Li%20and%20Lina%20Yu%20and%20Min%20Wu%20and%20Jingyi%20Liu%20and%20Wenqiang%20Li%20and%20Meilan%20Hao%20and%20Shu%20Wei%20and%20Yusong%20Deng&entry.1292438233=%20%20The%20mathematical%20formula%20is%20the%20human%20language%20to%20describe%20nature%20and%20is%20the%0Aessence%20of%20scientific%20research.%20Finding%20mathematical%20formulas%20from%0Aobservational%20data%20is%20a%20major%20demand%20of%20scientific%20research%20and%20a%20major%0Achallenge%20of%20artificial%20intelligence.%20This%20area%20is%20called%20symbolic%20regression.%0AOriginally%20symbolic%20regression%20was%20often%20formulated%20as%20a%20combinatorial%0Aoptimization%20problem%20and%20solved%20using%20GP%20or%20reinforcement%20learning%20algorithms.%0AThese%20two%20kinds%20of%20algorithms%20have%20strong%20noise%20robustness%20ability%20and%20good%0AVersatility.%20However%2C%20inference%20time%20usually%20takes%20a%20long%20time%2C%20so%20the%20search%0Aefficiency%20is%20relatively%20low.%20Later%2C%20based%20on%20large-scale%20pre-training%20data%0Aproposed%2C%20such%20methods%20use%20a%20large%20number%20of%20synthetic%20data%20points%20and%0Aexpression%20pairs%20to%20train%20a%20Generative%20Pre-Trained%20Transformer%28GPT%29.%20Then%20this%0AGPT%20can%20only%20need%20to%20perform%20one%20forward%20propagation%20to%20obtain%20the%20results%2C%20the%0Aadvantage%20is%20that%20the%20inference%20speed%20is%20very%20fast.%20However%2C%20its%20performance%20is%0Avery%20dependent%20on%20the%20training%20data%20and%20performs%20poorly%20on%20data%20outside%20the%0Atraining%20set%2C%20which%20leads%20to%20poor%20noise%20robustness%20and%20Versatility%20of%20such%0Amethods.%20So%2C%20can%20we%20combine%20the%20advantages%20of%20the%20above%20two%20categories%20of%20SR%0Aalgorithms%3F%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BFormulaGPT%7D%2C%20which%20trains%20a%20GPT%0Ausing%20massive%20sparse%20reward%20learning%20histories%20of%20reinforcement%20learning-based%0ASR%20algorithms%20as%20training%20data.%20After%20training%2C%20the%20SR%20algorithm%20based%20on%0Areinforcement%20learning%20is%20distilled%20into%20a%20Transformer.%20When%20new%20test%20data%0Acomes%2C%20FormulaGPT%20can%20directly%20generate%20a%20%22reinforcement%20learning%20process%22%20and%0Aautomatically%20update%20the%20learning%20policy%20in%20context.%20Tested%20on%20more%20than%20ten%0Adatasets%20including%20SRBench%2C%20formulaGPT%20achieves%20the%20state-of-the-art%0Aperformance%20in%20fitting%20ability%20compared%20with%20four%20baselines.%20In%20addition%2C%20it%0Aachieves%20satisfactory%20results%20in%20noise%20robustness%2C%20versatility%2C%20and%20inference%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06330v1&entry.124074799=Read"},
{"title": "Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion", "author": "Fan Yang and Jianfeng Zhang and Yichun Shi and Bowen Chen and Chenxu Zhang and Huichao Zhang and Xiaofeng Yang and Jiashi Feng and Guosheng Lin", "abstract": "  Benefiting from the rapid development of 2D diffusion models, 3D content\ncreation has made significant progress recently. One promising solution\ninvolves the fine-tuning of pre-trained 2D diffusion models to harness their\ncapacity for producing multi-view images, which are then lifted into accurate\n3D models via methods like fast-NeRFs or large reconstruction models. However,\nas inconsistency still exists and limited generated resolution, the generation\nresults of such methods still lack intricate textures and complex geometries.\nTo solve this problem, we propose Magic-Boost, a multi-view conditioned\ndiffusion model that significantly refines coarse generative results through a\nbrief period of SDS optimization ($\\sim15$min). Compared to the previous text\nor single image based diffusion models, Magic-Boost exhibits a robust\ncapability to generate images with high consistency from pseudo synthesized\nmulti-view images. It provides precise SDS guidance that well aligns with the\nidentity of the input images, enriching the local detail in both geometry and\ntexture of the initial generative results. Extensive experiments show\nMagic-Boost greatly enhances the coarse inputs and generates high-quality 3D\nassets with rich geometric and textural details. (Project Page:\nhttps://magic-research.github.io/magic-boost/)\n", "link": "http://arxiv.org/abs/2404.06429v1", "date": "2024-04-09", "relevancy": 1.9988, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6794}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6753}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6307}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Magic-Boost%3A%20Boost%203D%20Generation%20with%20Mutli-View%20Conditioned%20Diffusion&body=Title%3A%20Magic-Boost%3A%20Boost%203D%20Generation%20with%20Mutli-View%20Conditioned%20Diffusion%0AAuthor%3A%20Fan%20Yang%20and%20Jianfeng%20Zhang%20and%20Yichun%20Shi%20and%20Bowen%20Chen%20and%20Chenxu%20Zhang%20and%20Huichao%20Zhang%20and%20Xiaofeng%20Yang%20and%20Jiashi%20Feng%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20Benefiting%20from%20the%20rapid%20development%20of%202D%20diffusion%20models%2C%203D%20content%0Acreation%20has%20made%20significant%20progress%20recently.%20One%20promising%20solution%0Ainvolves%20the%20fine-tuning%20of%20pre-trained%202D%20diffusion%20models%20to%20harness%20their%0Acapacity%20for%20producing%20multi-view%20images%2C%20which%20are%20then%20lifted%20into%20accurate%0A3D%20models%20via%20methods%20like%20fast-NeRFs%20or%20large%20reconstruction%20models.%20However%2C%0Aas%20inconsistency%20still%20exists%20and%20limited%20generated%20resolution%2C%20the%20generation%0Aresults%20of%20such%20methods%20still%20lack%20intricate%20textures%20and%20complex%20geometries.%0ATo%20solve%20this%20problem%2C%20we%20propose%20Magic-Boost%2C%20a%20multi-view%20conditioned%0Adiffusion%20model%20that%20significantly%20refines%20coarse%20generative%20results%20through%20a%0Abrief%20period%20of%20SDS%20optimization%20%28%24%5Csim15%24min%29.%20Compared%20to%20the%20previous%20text%0Aor%20single%20image%20based%20diffusion%20models%2C%20Magic-Boost%20exhibits%20a%20robust%0Acapability%20to%20generate%20images%20with%20high%20consistency%20from%20pseudo%20synthesized%0Amulti-view%20images.%20It%20provides%20precise%20SDS%20guidance%20that%20well%20aligns%20with%20the%0Aidentity%20of%20the%20input%20images%2C%20enriching%20the%20local%20detail%20in%20both%20geometry%20and%0Atexture%20of%20the%20initial%20generative%20results.%20Extensive%20experiments%20show%0AMagic-Boost%20greatly%20enhances%20the%20coarse%20inputs%20and%20generates%20high-quality%203D%0Aassets%20with%20rich%20geometric%20and%20textural%20details.%20%28Project%20Page%3A%0Ahttps%3A//magic-research.github.io/magic-boost/%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06429v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Magic-Boost%3A%20Boost%203D%20Generation%20with%20Mutli-View%20Conditioned%20Diffusion&entry.906535625=Fan%20Yang%20and%20Jianfeng%20Zhang%20and%20Yichun%20Shi%20and%20Bowen%20Chen%20and%20Chenxu%20Zhang%20and%20Huichao%20Zhang%20and%20Xiaofeng%20Yang%20and%20Jiashi%20Feng%20and%20Guosheng%20Lin&entry.1292438233=%20%20Benefiting%20from%20the%20rapid%20development%20of%202D%20diffusion%20models%2C%203D%20content%0Acreation%20has%20made%20significant%20progress%20recently.%20One%20promising%20solution%0Ainvolves%20the%20fine-tuning%20of%20pre-trained%202D%20diffusion%20models%20to%20harness%20their%0Acapacity%20for%20producing%20multi-view%20images%2C%20which%20are%20then%20lifted%20into%20accurate%0A3D%20models%20via%20methods%20like%20fast-NeRFs%20or%20large%20reconstruction%20models.%20However%2C%0Aas%20inconsistency%20still%20exists%20and%20limited%20generated%20resolution%2C%20the%20generation%0Aresults%20of%20such%20methods%20still%20lack%20intricate%20textures%20and%20complex%20geometries.%0ATo%20solve%20this%20problem%2C%20we%20propose%20Magic-Boost%2C%20a%20multi-view%20conditioned%0Adiffusion%20model%20that%20significantly%20refines%20coarse%20generative%20results%20through%20a%0Abrief%20period%20of%20SDS%20optimization%20%28%24%5Csim15%24min%29.%20Compared%20to%20the%20previous%20text%0Aor%20single%20image%20based%20diffusion%20models%2C%20Magic-Boost%20exhibits%20a%20robust%0Acapability%20to%20generate%20images%20with%20high%20consistency%20from%20pseudo%20synthesized%0Amulti-view%20images.%20It%20provides%20precise%20SDS%20guidance%20that%20well%20aligns%20with%20the%0Aidentity%20of%20the%20input%20images%2C%20enriching%20the%20local%20detail%20in%20both%20geometry%20and%0Atexture%20of%20the%20initial%20generative%20results.%20Extensive%20experiments%20show%0AMagic-Boost%20greatly%20enhances%20the%20coarse%20inputs%20and%20generates%20high-quality%203D%0Aassets%20with%20rich%20geometric%20and%20textural%20details.%20%28Project%20Page%3A%0Ahttps%3A//magic-research.github.io/magic-boost/%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06429v1&entry.124074799=Read"},
{"title": "MSS-PAE: Saving Autoencoder-based Outlier Detection from Unexpected\n  Reconstruction", "author": "Xu Tan and Jiawei Yang and Junqi Chen and Sylwan Rahardja and Susanto Rahardja", "abstract": "  AutoEncoders (AEs) are commonly used for machine learning tasks due to their\nintrinsic learning ability. This unique characteristic can be capitalized for\nOutlier Detection (OD). However conventional AE-based methods face the issue of\noverconfident decisions and unexpected reconstruction results of outliers,\nlimiting their performance in OD. To mitigate these issues, the Mean Squared\nError (MSE) and Negative Logarithmic Likelihood (NLL) were first analyzed, and\nthe importance of incorporating aleatoric uncertainty to AE-based OD was\nelucidated. Then the Weighted Negative Logarithmic Likelihood (WNLL) was\nproposed to adjust for the effect of uncertainty for different OD scenarios.\nMoreover, the Mean-Shift Scoring (MSS) method was proposed to utilize the local\nrelationship of data to reduce the issue of false inliers caused by AE.\nExperiments on 32 real-world OD datasets proved the effectiveness of the\nproposed methods. The combination of WNLL and MSS achieved 41% relative\nperformance improvement compared to the best baseline. In addition, MSS\nimproved the performance of multiple AE-based outlier detectors by an average\nof 20%. The proposed methods have the potential to advance AE's development in\nOD. The code is available on www.OutlierNet.com for reproducibility.\n", "link": "http://arxiv.org/abs/2304.00709v2", "date": "2024-04-09", "relevancy": 1.9973, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5524}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4893}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MSS-PAE%3A%20Saving%20Autoencoder-based%20Outlier%20Detection%20from%20Unexpected%0A%20%20Reconstruction&body=Title%3A%20MSS-PAE%3A%20Saving%20Autoencoder-based%20Outlier%20Detection%20from%20Unexpected%0A%20%20Reconstruction%0AAuthor%3A%20Xu%20Tan%20and%20Jiawei%20Yang%20and%20Junqi%20Chen%20and%20Sylwan%20Rahardja%20and%20Susanto%20Rahardja%0AAbstract%3A%20%20%20AutoEncoders%20%28AEs%29%20are%20commonly%20used%20for%20machine%20learning%20tasks%20due%20to%20their%0Aintrinsic%20learning%20ability.%20This%20unique%20characteristic%20can%20be%20capitalized%20for%0AOutlier%20Detection%20%28OD%29.%20However%20conventional%20AE-based%20methods%20face%20the%20issue%20of%0Aoverconfident%20decisions%20and%20unexpected%20reconstruction%20results%20of%20outliers%2C%0Alimiting%20their%20performance%20in%20OD.%20To%20mitigate%20these%20issues%2C%20the%20Mean%20Squared%0AError%20%28MSE%29%20and%20Negative%20Logarithmic%20Likelihood%20%28NLL%29%20were%20first%20analyzed%2C%20and%0Athe%20importance%20of%20incorporating%20aleatoric%20uncertainty%20to%20AE-based%20OD%20was%0Aelucidated.%20Then%20the%20Weighted%20Negative%20Logarithmic%20Likelihood%20%28WNLL%29%20was%0Aproposed%20to%20adjust%20for%20the%20effect%20of%20uncertainty%20for%20different%20OD%20scenarios.%0AMoreover%2C%20the%20Mean-Shift%20Scoring%20%28MSS%29%20method%20was%20proposed%20to%20utilize%20the%20local%0Arelationship%20of%20data%20to%20reduce%20the%20issue%20of%20false%20inliers%20caused%20by%20AE.%0AExperiments%20on%2032%20real-world%20OD%20datasets%20proved%20the%20effectiveness%20of%20the%0Aproposed%20methods.%20The%20combination%20of%20WNLL%20and%20MSS%20achieved%2041%25%20relative%0Aperformance%20improvement%20compared%20to%20the%20best%20baseline.%20In%20addition%2C%20MSS%0Aimproved%20the%20performance%20of%20multiple%20AE-based%20outlier%20detectors%20by%20an%20average%0Aof%2020%25.%20The%20proposed%20methods%20have%20the%20potential%20to%20advance%20AE%27s%20development%20in%0AOD.%20The%20code%20is%20available%20on%20www.OutlierNet.com%20for%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.00709v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSS-PAE%3A%20Saving%20Autoencoder-based%20Outlier%20Detection%20from%20Unexpected%0A%20%20Reconstruction&entry.906535625=Xu%20Tan%20and%20Jiawei%20Yang%20and%20Junqi%20Chen%20and%20Sylwan%20Rahardja%20and%20Susanto%20Rahardja&entry.1292438233=%20%20AutoEncoders%20%28AEs%29%20are%20commonly%20used%20for%20machine%20learning%20tasks%20due%20to%20their%0Aintrinsic%20learning%20ability.%20This%20unique%20characteristic%20can%20be%20capitalized%20for%0AOutlier%20Detection%20%28OD%29.%20However%20conventional%20AE-based%20methods%20face%20the%20issue%20of%0Aoverconfident%20decisions%20and%20unexpected%20reconstruction%20results%20of%20outliers%2C%0Alimiting%20their%20performance%20in%20OD.%20To%20mitigate%20these%20issues%2C%20the%20Mean%20Squared%0AError%20%28MSE%29%20and%20Negative%20Logarithmic%20Likelihood%20%28NLL%29%20were%20first%20analyzed%2C%20and%0Athe%20importance%20of%20incorporating%20aleatoric%20uncertainty%20to%20AE-based%20OD%20was%0Aelucidated.%20Then%20the%20Weighted%20Negative%20Logarithmic%20Likelihood%20%28WNLL%29%20was%0Aproposed%20to%20adjust%20for%20the%20effect%20of%20uncertainty%20for%20different%20OD%20scenarios.%0AMoreover%2C%20the%20Mean-Shift%20Scoring%20%28MSS%29%20method%20was%20proposed%20to%20utilize%20the%20local%0Arelationship%20of%20data%20to%20reduce%20the%20issue%20of%20false%20inliers%20caused%20by%20AE.%0AExperiments%20on%2032%20real-world%20OD%20datasets%20proved%20the%20effectiveness%20of%20the%0Aproposed%20methods.%20The%20combination%20of%20WNLL%20and%20MSS%20achieved%2041%25%20relative%0Aperformance%20improvement%20compared%20to%20the%20best%20baseline.%20In%20addition%2C%20MSS%0Aimproved%20the%20performance%20of%20multiple%20AE-based%20outlier%20detectors%20by%20an%20average%0Aof%2020%25.%20The%20proposed%20methods%20have%20the%20potential%20to%20advance%20AE%27s%20development%20in%0AOD.%20The%20code%20is%20available%20on%20www.OutlierNet.com%20for%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00709v2&entry.124074799=Read"},
{"title": "Deepfake Generation and Detection: A Benchmark and Survey", "author": "Gan Pei and Jiangning Zhang and Menghan Hu and Zhenyu Zhang and Chengjie Wang and Yunsheng Wu and Guangtao Zhai and Jian Yang and Chunhua Shen and Dacheng Tao", "abstract": "  In addition to the advancements in deepfake generation, corresponding\ndetection technologies need to continuously evolve to regulate the potential\nmisuse of deepfakes, such as for privacy invasion and phishing attacks. This\nsurvey comprehensively reviews the latest developments in deepfake generation\nand detection, summarizing and analyzing the current state of the art in this\nrapidly evolving field. We first unify task definitions, comprehensively\nintroduce datasets and metrics, and discuss the development of generation and\ndetection technology frameworks. Then, we discuss the development of several\nrelated sub-fields and focus on researching four mainstream deepfake fields:\npopular face swap, face reenactment, talking face generation, and facial\nattribute editing, as well as foreign detection. Subsequently, we\ncomprehensively benchmark representative methods on popular datasets for each\nfield, fully evaluating the latest and influential works published in top\nconferences/journals. Finally, we analyze the challenges and future research\ndirections of the discussed fields. We closely follow the latest developments\nin https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.\n", "link": "http://arxiv.org/abs/2403.17881v2", "date": "2024-04-09", "relevancy": 1.9903, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5051}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4988}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4933}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey&body=Title%3A%20Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey%0AAuthor%3A%20Gan%20Pei%20and%20Jiangning%20Zhang%20and%20Menghan%20Hu%20and%20Zhenyu%20Zhang%20and%20Chengjie%20Wang%20and%20Yunsheng%20Wu%20and%20Guangtao%20Zhai%20and%20Jian%20Yang%20and%20Chunhua%20Shen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20In%20addition%20to%20the%20advancements%20in%20deepfake%20generation%2C%20corresponding%0Adetection%20technologies%20need%20to%20continuously%20evolve%20to%20regulate%20the%20potential%0Amisuse%20of%20deepfakes%2C%20such%20as%20for%20privacy%20invasion%20and%20phishing%20attacks.%20This%0Asurvey%20comprehensively%20reviews%20the%20latest%20developments%20in%20deepfake%20generation%0Aand%20detection%2C%20summarizing%20and%20analyzing%20the%20current%20state%20of%20the%20art%20in%20this%0Arapidly%20evolving%20field.%20We%20first%20unify%20task%20definitions%2C%20comprehensively%0Aintroduce%20datasets%20and%20metrics%2C%20and%20discuss%20the%20development%20of%20generation%20and%0Adetection%20technology%20frameworks.%20Then%2C%20we%20discuss%20the%20development%20of%20several%0Arelated%20sub-fields%20and%20focus%20on%20researching%20four%20mainstream%20deepfake%20fields%3A%0Apopular%20face%20swap%2C%20face%20reenactment%2C%20talking%20face%20generation%2C%20and%20facial%0Aattribute%20editing%2C%20as%20well%20as%20foreign%20detection.%20Subsequently%2C%20we%0Acomprehensively%20benchmark%20representative%20methods%20on%20popular%20datasets%20for%20each%0Afield%2C%20fully%20evaluating%20the%20latest%20and%20influential%20works%20published%20in%20top%0Aconferences/journals.%20Finally%2C%20we%20analyze%20the%20challenges%20and%20future%20research%0Adirections%20of%20the%20discussed%20fields.%20We%20closely%20follow%20the%20latest%20developments%0Ain%20https%3A//github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17881v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey&entry.906535625=Gan%20Pei%20and%20Jiangning%20Zhang%20and%20Menghan%20Hu%20and%20Zhenyu%20Zhang%20and%20Chengjie%20Wang%20and%20Yunsheng%20Wu%20and%20Guangtao%20Zhai%20and%20Jian%20Yang%20and%20Chunhua%20Shen%20and%20Dacheng%20Tao&entry.1292438233=%20%20In%20addition%20to%20the%20advancements%20in%20deepfake%20generation%2C%20corresponding%0Adetection%20technologies%20need%20to%20continuously%20evolve%20to%20regulate%20the%20potential%0Amisuse%20of%20deepfakes%2C%20such%20as%20for%20privacy%20invasion%20and%20phishing%20attacks.%20This%0Asurvey%20comprehensively%20reviews%20the%20latest%20developments%20in%20deepfake%20generation%0Aand%20detection%2C%20summarizing%20and%20analyzing%20the%20current%20state%20of%20the%20art%20in%20this%0Arapidly%20evolving%20field.%20We%20first%20unify%20task%20definitions%2C%20comprehensively%0Aintroduce%20datasets%20and%20metrics%2C%20and%20discuss%20the%20development%20of%20generation%20and%0Adetection%20technology%20frameworks.%20Then%2C%20we%20discuss%20the%20development%20of%20several%0Arelated%20sub-fields%20and%20focus%20on%20researching%20four%20mainstream%20deepfake%20fields%3A%0Apopular%20face%20swap%2C%20face%20reenactment%2C%20talking%20face%20generation%2C%20and%20facial%0Aattribute%20editing%2C%20as%20well%20as%20foreign%20detection.%20Subsequently%2C%20we%0Acomprehensively%20benchmark%20representative%20methods%20on%20popular%20datasets%20for%20each%0Afield%2C%20fully%20evaluating%20the%20latest%20and%20influential%20works%20published%20in%20top%0Aconferences/journals.%20Finally%2C%20we%20analyze%20the%20challenges%20and%20future%20research%0Adirections%20of%20the%20discussed%20fields.%20We%20closely%20follow%20the%20latest%20developments%0Ain%20https%3A//github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17881v2&entry.124074799=Read"},
{"title": "Learning fast changing slow in spiking neural networks", "author": "Cristiano Capone and Paolo Muratore", "abstract": "  Reinforcement learning (RL) faces substantial challenges when applied to\nreal-life problems, primarily stemming from the scarcity of available data due\nto limited interactions with the environment. This limitation is exacerbated by\nthe fact that RL often demands a considerable volume of data for effective\nlearning. The complexity escalates further when implementing RL in recurrent\nspiking networks, where inherent noise introduced by spikes adds a layer of\ndifficulty. Life-long learning machines must inherently resolve the\nplasticity-stability paradox. Striking a balance between acquiring new\nknowledge and maintaining stability is crucial for artificial agents. To\naddress this challenge, we draw inspiration from machine learning technology\nand introduce a biologically plausible implementation of proximal policy\noptimization, referred to as lf-cs (learning fast changing slow). Our approach\nresults in two notable advancements: firstly, the capacity to assimilate new\ninformation into a new policy without requiring alterations to the current\npolicy; and secondly, the capability to replay experiences without experiencing\npolicy divergence. Furthermore, when contrasted with other experience replay\n(ER) techniques, our method demonstrates the added advantage of being\ncomputationally efficient in an online setting. We demonstrate that the\nproposed methodology enhances the efficiency of learning, showcasing its\npotential impact on neuromorphic and real-world applications.\n", "link": "http://arxiv.org/abs/2402.10069v2", "date": "2024-04-09", "relevancy": 1.9883, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5051}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4962}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4894}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20fast%20changing%20slow%20in%20spiking%20neural%20networks&body=Title%3A%20Learning%20fast%20changing%20slow%20in%20spiking%20neural%20networks%0AAuthor%3A%20Cristiano%20Capone%20and%20Paolo%20Muratore%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20faces%20substantial%20challenges%20when%20applied%20to%0Areal-life%20problems%2C%20primarily%20stemming%20from%20the%20scarcity%20of%20available%20data%20due%0Ato%20limited%20interactions%20with%20the%20environment.%20This%20limitation%20is%20exacerbated%20by%0Athe%20fact%20that%20RL%20often%20demands%20a%20considerable%20volume%20of%20data%20for%20effective%0Alearning.%20The%20complexity%20escalates%20further%20when%20implementing%20RL%20in%20recurrent%0Aspiking%20networks%2C%20where%20inherent%20noise%20introduced%20by%20spikes%20adds%20a%20layer%20of%0Adifficulty.%20Life-long%20learning%20machines%20must%20inherently%20resolve%20the%0Aplasticity-stability%20paradox.%20Striking%20a%20balance%20between%20acquiring%20new%0Aknowledge%20and%20maintaining%20stability%20is%20crucial%20for%20artificial%20agents.%20To%0Aaddress%20this%20challenge%2C%20we%20draw%20inspiration%20from%20machine%20learning%20technology%0Aand%20introduce%20a%20biologically%20plausible%20implementation%20of%20proximal%20policy%0Aoptimization%2C%20referred%20to%20as%20lf-cs%20%28learning%20fast%20changing%20slow%29.%20Our%20approach%0Aresults%20in%20two%20notable%20advancements%3A%20firstly%2C%20the%20capacity%20to%20assimilate%20new%0Ainformation%20into%20a%20new%20policy%20without%20requiring%20alterations%20to%20the%20current%0Apolicy%3B%20and%20secondly%2C%20the%20capability%20to%20replay%20experiences%20without%20experiencing%0Apolicy%20divergence.%20Furthermore%2C%20when%20contrasted%20with%20other%20experience%20replay%0A%28ER%29%20techniques%2C%20our%20method%20demonstrates%20the%20added%20advantage%20of%20being%0Acomputationally%20efficient%20in%20an%20online%20setting.%20We%20demonstrate%20that%20the%0Aproposed%20methodology%20enhances%20the%20efficiency%20of%20learning%2C%20showcasing%20its%0Apotential%20impact%20on%20neuromorphic%20and%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10069v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20fast%20changing%20slow%20in%20spiking%20neural%20networks&entry.906535625=Cristiano%20Capone%20and%20Paolo%20Muratore&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20faces%20substantial%20challenges%20when%20applied%20to%0Areal-life%20problems%2C%20primarily%20stemming%20from%20the%20scarcity%20of%20available%20data%20due%0Ato%20limited%20interactions%20with%20the%20environment.%20This%20limitation%20is%20exacerbated%20by%0Athe%20fact%20that%20RL%20often%20demands%20a%20considerable%20volume%20of%20data%20for%20effective%0Alearning.%20The%20complexity%20escalates%20further%20when%20implementing%20RL%20in%20recurrent%0Aspiking%20networks%2C%20where%20inherent%20noise%20introduced%20by%20spikes%20adds%20a%20layer%20of%0Adifficulty.%20Life-long%20learning%20machines%20must%20inherently%20resolve%20the%0Aplasticity-stability%20paradox.%20Striking%20a%20balance%20between%20acquiring%20new%0Aknowledge%20and%20maintaining%20stability%20is%20crucial%20for%20artificial%20agents.%20To%0Aaddress%20this%20challenge%2C%20we%20draw%20inspiration%20from%20machine%20learning%20technology%0Aand%20introduce%20a%20biologically%20plausible%20implementation%20of%20proximal%20policy%0Aoptimization%2C%20referred%20to%20as%20lf-cs%20%28learning%20fast%20changing%20slow%29.%20Our%20approach%0Aresults%20in%20two%20notable%20advancements%3A%20firstly%2C%20the%20capacity%20to%20assimilate%20new%0Ainformation%20into%20a%20new%20policy%20without%20requiring%20alterations%20to%20the%20current%0Apolicy%3B%20and%20secondly%2C%20the%20capability%20to%20replay%20experiences%20without%20experiencing%0Apolicy%20divergence.%20Furthermore%2C%20when%20contrasted%20with%20other%20experience%20replay%0A%28ER%29%20techniques%2C%20our%20method%20demonstrates%20the%20added%20advantage%20of%20being%0Acomputationally%20efficient%20in%20an%20online%20setting.%20We%20demonstrate%20that%20the%0Aproposed%20methodology%20enhances%20the%20efficiency%20of%20learning%2C%20showcasing%20its%0Apotential%20impact%20on%20neuromorphic%20and%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10069v2&entry.124074799=Read"},
{"title": "Automatically Learning HTN Methods from Landmarks", "author": "Ruoxi Li and Dana Nau and Mark Roberts and Morgan Fine-Morris", "abstract": "  Hierarchical Task Network (HTN) planning usually requires a domain engineer\nto provide manual input about how to decompose a planning problem. Even\nHTN-MAKER, a well-known method-learning algorithm, requires a domain engineer\nto annotate the tasks with information about what to learn. We introduce\nCURRICULAMA, an HTN method learning algorithm that completely automates the\nlearning process. It uses landmark analysis to compose annotated tasks and\nleverages curriculum learning to order the learning of methods from simpler to\nmore complex. This eliminates the need for manual input, resolving a core issue\nwith HTN-MAKER. We prove CURRICULAMA's soundness, and show experimentally that\nit has a substantially similar convergence rate in learning a complete set of\nmethods to HTN-MAKER.\n", "link": "http://arxiv.org/abs/2404.06325v1", "date": "2024-04-09", "relevancy": 1.9756, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5009}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4938}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4766}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatically%20Learning%20HTN%20Methods%20from%20Landmarks&body=Title%3A%20Automatically%20Learning%20HTN%20Methods%20from%20Landmarks%0AAuthor%3A%20Ruoxi%20Li%20and%20Dana%20Nau%20and%20Mark%20Roberts%20and%20Morgan%20Fine-Morris%0AAbstract%3A%20%20%20Hierarchical%20Task%20Network%20%28HTN%29%20planning%20usually%20requires%20a%20domain%20engineer%0Ato%20provide%20manual%20input%20about%20how%20to%20decompose%20a%20planning%20problem.%20Even%0AHTN-MAKER%2C%20a%20well-known%20method-learning%20algorithm%2C%20requires%20a%20domain%20engineer%0Ato%20annotate%20the%20tasks%20with%20information%20about%20what%20to%20learn.%20We%20introduce%0ACURRICULAMA%2C%20an%20HTN%20method%20learning%20algorithm%20that%20completely%20automates%20the%0Alearning%20process.%20It%20uses%20landmark%20analysis%20to%20compose%20annotated%20tasks%20and%0Aleverages%20curriculum%20learning%20to%20order%20the%20learning%20of%20methods%20from%20simpler%20to%0Amore%20complex.%20This%20eliminates%20the%20need%20for%20manual%20input%2C%20resolving%20a%20core%20issue%0Awith%20HTN-MAKER.%20We%20prove%20CURRICULAMA%27s%20soundness%2C%20and%20show%20experimentally%20that%0Ait%20has%20a%20substantially%20similar%20convergence%20rate%20in%20learning%20a%20complete%20set%20of%0Amethods%20to%20HTN-MAKER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06325v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatically%20Learning%20HTN%20Methods%20from%20Landmarks&entry.906535625=Ruoxi%20Li%20and%20Dana%20Nau%20and%20Mark%20Roberts%20and%20Morgan%20Fine-Morris&entry.1292438233=%20%20Hierarchical%20Task%20Network%20%28HTN%29%20planning%20usually%20requires%20a%20domain%20engineer%0Ato%20provide%20manual%20input%20about%20how%20to%20decompose%20a%20planning%20problem.%20Even%0AHTN-MAKER%2C%20a%20well-known%20method-learning%20algorithm%2C%20requires%20a%20domain%20engineer%0Ato%20annotate%20the%20tasks%20with%20information%20about%20what%20to%20learn.%20We%20introduce%0ACURRICULAMA%2C%20an%20HTN%20method%20learning%20algorithm%20that%20completely%20automates%20the%0Alearning%20process.%20It%20uses%20landmark%20analysis%20to%20compose%20annotated%20tasks%20and%0Aleverages%20curriculum%20learning%20to%20order%20the%20learning%20of%20methods%20from%20simpler%20to%0Amore%20complex.%20This%20eliminates%20the%20need%20for%20manual%20input%2C%20resolving%20a%20core%20issue%0Awith%20HTN-MAKER.%20We%20prove%20CURRICULAMA%27s%20soundness%2C%20and%20show%20experimentally%20that%0Ait%20has%20a%20substantially%20similar%20convergence%20rate%20in%20learning%20a%20complete%20set%20of%0Amethods%20to%20HTN-MAKER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06325v1&entry.124074799=Read"},
{"title": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models", "author": "Sebastian Bordt and Harsha Nori and Vanessa Rodrigues and Besmira Nushi and Rich Caruana", "abstract": "  While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. Without fine-tuning, we find them to be limited.\nThis suggests that much of the few-shot performance on novel datasets is due to\nthe LLM's world knowledge. Overall, our results highlight the importance of\ntesting whether an LLM has seen an evaluation dataset during pre-training. We\nmake the exposure tests we developed available as the tabmemcheck Python\npackage at https://github.com/interpretml/LLM-Tabular-Memorization-Checker\n", "link": "http://arxiv.org/abs/2404.06209v1", "date": "2024-04-09", "relevancy": 1.9721, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4821}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4732}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Sebastian%20Bordt%20and%20Harsha%20Nori%20and%20Vanessa%20Rodrigues%20and%20Besmira%20Nushi%20and%20Rich%20Caruana%0AAbstract%3A%20%20%20While%20many%20have%20shown%20how%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20applied%20to%20a%0Adiverse%20set%20of%20tasks%2C%20the%20critical%20issues%20of%20data%20contamination%20and%0Amemorization%20are%20often%20glossed%20over.%20In%20this%20work%2C%20we%20address%20this%20concern%20for%0Atabular%20data.%20Specifically%2C%20we%20introduce%20a%20variety%20of%20different%20techniques%20to%0Aassess%20whether%20a%20language%20model%20has%20seen%20a%20tabular%20dataset%20during%20training.%0AThis%20investigation%20reveals%20that%20LLMs%20have%20memorized%20many%20popular%20tabular%0Adatasets%20verbatim.%20We%20then%20compare%20the%20few-shot%20learning%20performance%20of%20LLMs%20on%0Adatasets%20that%20were%20seen%20during%20training%20to%20the%20performance%20on%20datasets%20released%0Aafter%20training.%20We%20find%20that%20LLMs%20perform%20better%20on%20datasets%20seen%20during%0Atraining%2C%20indicating%20that%20memorization%20leads%20to%20overfitting.%20At%20the%20same%20time%2C%0ALLMs%20show%20non-trivial%20performance%20on%20novel%20datasets%20and%20are%20surprisingly%20robust%0Ato%20data%20transformations.%20We%20then%20investigate%20the%20in-context%20statistical%0Alearning%20abilities%20of%20LLMs.%20Without%20fine-tuning%2C%20we%20find%20them%20to%20be%20limited.%0AThis%20suggests%20that%20much%20of%20the%20few-shot%20performance%20on%20novel%20datasets%20is%20due%20to%0Athe%20LLM%27s%20world%20knowledge.%20Overall%2C%20our%20results%20highlight%20the%20importance%20of%0Atesting%20whether%20an%20LLM%20has%20seen%20an%20evaluation%20dataset%20during%20pre-training.%20We%0Amake%20the%20exposure%20tests%20we%20developed%20available%20as%20the%20tabmemcheck%20Python%0Apackage%20at%20https%3A//github.com/interpretml/LLM-Tabular-Memorization-Checker%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06209v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models&entry.906535625=Sebastian%20Bordt%20and%20Harsha%20Nori%20and%20Vanessa%20Rodrigues%20and%20Besmira%20Nushi%20and%20Rich%20Caruana&entry.1292438233=%20%20While%20many%20have%20shown%20how%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20applied%20to%20a%0Adiverse%20set%20of%20tasks%2C%20the%20critical%20issues%20of%20data%20contamination%20and%0Amemorization%20are%20often%20glossed%20over.%20In%20this%20work%2C%20we%20address%20this%20concern%20for%0Atabular%20data.%20Specifically%2C%20we%20introduce%20a%20variety%20of%20different%20techniques%20to%0Aassess%20whether%20a%20language%20model%20has%20seen%20a%20tabular%20dataset%20during%20training.%0AThis%20investigation%20reveals%20that%20LLMs%20have%20memorized%20many%20popular%20tabular%0Adatasets%20verbatim.%20We%20then%20compare%20the%20few-shot%20learning%20performance%20of%20LLMs%20on%0Adatasets%20that%20were%20seen%20during%20training%20to%20the%20performance%20on%20datasets%20released%0Aafter%20training.%20We%20find%20that%20LLMs%20perform%20better%20on%20datasets%20seen%20during%0Atraining%2C%20indicating%20that%20memorization%20leads%20to%20overfitting.%20At%20the%20same%20time%2C%0ALLMs%20show%20non-trivial%20performance%20on%20novel%20datasets%20and%20are%20surprisingly%20robust%0Ato%20data%20transformations.%20We%20then%20investigate%20the%20in-context%20statistical%0Alearning%20abilities%20of%20LLMs.%20Without%20fine-tuning%2C%20we%20find%20them%20to%20be%20limited.%0AThis%20suggests%20that%20much%20of%20the%20few-shot%20performance%20on%20novel%20datasets%20is%20due%20to%0Athe%20LLM%27s%20world%20knowledge.%20Overall%2C%20our%20results%20highlight%20the%20importance%20of%0Atesting%20whether%20an%20LLM%20has%20seen%20an%20evaluation%20dataset%20during%20pre-training.%20We%0Amake%20the%20exposure%20tests%20we%20developed%20available%20as%20the%20tabmemcheck%20Python%0Apackage%20at%20https%3A//github.com/interpretml/LLM-Tabular-Memorization-Checker%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06209v1&entry.124074799=Read"},
{"title": "Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?", "author": "Yuan-Hong Liao and Rafid Mahmood and Sanja Fidler and David Acuna", "abstract": "  Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often\ninvolves collecting domain-specific training data, refining the network\narchitectures, or modifying the training recipes. In this work, we venture into\nan orthogonal direction and explore whether VLMs can improve their semantic\ngrounding by \"receiving\" feedback, without requiring in-domain data,\nfine-tuning, or modifications to the network architectures. We systematically\nanalyze this hypothesis using a feedback mechanism composed of a binary signal.\nWe find that if prompted appropriately, VLMs can utilize feedback both in a\nsingle step and iteratively, showcasing the potential of feedback as an\nalternative technique to improve grounding in internet-scale VLMs. Furthermore,\nVLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, we\nfind that this issue can be mitigated via a binary verification mechanism.\nFinally, we explore the potential and limitations of amalgamating these\nfindings and applying them iteratively to automatically enhance VLMs' grounding\nperformance, showing grounding accuracy consistently improves using automated\nfeedback across all models in all settings investigated. Overall, our iterative\nframework improves semantic grounding in VLMs by more than 15 accuracy points\nunder noise-free feedback and up to 5 accuracy points under a simple automated\nbinary verification mechanism. The project website is hosted at\nhttps://andrewliao11.github.io/vlms_feedback\n", "link": "http://arxiv.org/abs/2404.06510v1", "date": "2024-04-09", "relevancy": 1.9691, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5222}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4865}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Can%20Feedback%20Enhance%20Semantic%20Grounding%20in%20Large%20Vision-Language%20Models%3F&body=Title%3A%20Can%20Feedback%20Enhance%20Semantic%20Grounding%20in%20Large%20Vision-Language%20Models%3F%0AAuthor%3A%20Yuan-Hong%20Liao%20and%20Rafid%20Mahmood%20and%20Sanja%20Fidler%20and%20David%20Acuna%0AAbstract%3A%20%20%20Enhancing%20semantic%20grounding%20abilities%20in%20Vision-Language%20Models%20%28VLMs%29%20often%0Ainvolves%20collecting%20domain-specific%20training%20data%2C%20refining%20the%20network%0Aarchitectures%2C%20or%20modifying%20the%20training%20recipes.%20In%20this%20work%2C%20we%20venture%20into%0Aan%20orthogonal%20direction%20and%20explore%20whether%20VLMs%20can%20improve%20their%20semantic%0Agrounding%20by%20%22receiving%22%20feedback%2C%20without%20requiring%20in-domain%20data%2C%0Afine-tuning%2C%20or%20modifications%20to%20the%20network%20architectures.%20We%20systematically%0Aanalyze%20this%20hypothesis%20using%20a%20feedback%20mechanism%20composed%20of%20a%20binary%20signal.%0AWe%20find%20that%20if%20prompted%20appropriately%2C%20VLMs%20can%20utilize%20feedback%20both%20in%20a%0Asingle%20step%20and%20iteratively%2C%20showcasing%20the%20potential%20of%20feedback%20as%20an%0Aalternative%20technique%20to%20improve%20grounding%20in%20internet-scale%20VLMs.%20Furthermore%2C%0AVLMs%2C%20like%20LLMs%2C%20struggle%20to%20self-correct%20errors%20out-of-the-box.%20However%2C%20we%0Afind%20that%20this%20issue%20can%20be%20mitigated%20via%20a%20binary%20verification%20mechanism.%0AFinally%2C%20we%20explore%20the%20potential%20and%20limitations%20of%20amalgamating%20these%0Afindings%20and%20applying%20them%20iteratively%20to%20automatically%20enhance%20VLMs%27%20grounding%0Aperformance%2C%20showing%20grounding%20accuracy%20consistently%20improves%20using%20automated%0Afeedback%20across%20all%20models%20in%20all%20settings%20investigated.%20Overall%2C%20our%20iterative%0Aframework%20improves%20semantic%20grounding%20in%20VLMs%20by%20more%20than%2015%20accuracy%20points%0Aunder%20noise-free%20feedback%20and%20up%20to%205%20accuracy%20points%20under%20a%20simple%20automated%0Abinary%20verification%20mechanism.%20The%20project%20website%20is%20hosted%20at%0Ahttps%3A//andrewliao11.github.io/vlms_feedback%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06510v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Feedback%20Enhance%20Semantic%20Grounding%20in%20Large%20Vision-Language%20Models%3F&entry.906535625=Yuan-Hong%20Liao%20and%20Rafid%20Mahmood%20and%20Sanja%20Fidler%20and%20David%20Acuna&entry.1292438233=%20%20Enhancing%20semantic%20grounding%20abilities%20in%20Vision-Language%20Models%20%28VLMs%29%20often%0Ainvolves%20collecting%20domain-specific%20training%20data%2C%20refining%20the%20network%0Aarchitectures%2C%20or%20modifying%20the%20training%20recipes.%20In%20this%20work%2C%20we%20venture%20into%0Aan%20orthogonal%20direction%20and%20explore%20whether%20VLMs%20can%20improve%20their%20semantic%0Agrounding%20by%20%22receiving%22%20feedback%2C%20without%20requiring%20in-domain%20data%2C%0Afine-tuning%2C%20or%20modifications%20to%20the%20network%20architectures.%20We%20systematically%0Aanalyze%20this%20hypothesis%20using%20a%20feedback%20mechanism%20composed%20of%20a%20binary%20signal.%0AWe%20find%20that%20if%20prompted%20appropriately%2C%20VLMs%20can%20utilize%20feedback%20both%20in%20a%0Asingle%20step%20and%20iteratively%2C%20showcasing%20the%20potential%20of%20feedback%20as%20an%0Aalternative%20technique%20to%20improve%20grounding%20in%20internet-scale%20VLMs.%20Furthermore%2C%0AVLMs%2C%20like%20LLMs%2C%20struggle%20to%20self-correct%20errors%20out-of-the-box.%20However%2C%20we%0Afind%20that%20this%20issue%20can%20be%20mitigated%20via%20a%20binary%20verification%20mechanism.%0AFinally%2C%20we%20explore%20the%20potential%20and%20limitations%20of%20amalgamating%20these%0Afindings%20and%20applying%20them%20iteratively%20to%20automatically%20enhance%20VLMs%27%20grounding%0Aperformance%2C%20showing%20grounding%20accuracy%20consistently%20improves%20using%20automated%0Afeedback%20across%20all%20models%20in%20all%20settings%20investigated.%20Overall%2C%20our%20iterative%0Aframework%20improves%20semantic%20grounding%20in%20VLMs%20by%20more%20than%2015%20accuracy%20points%0Aunder%20noise-free%20feedback%20and%20up%20to%205%20accuracy%20points%20under%20a%20simple%20automated%0Abinary%20verification%20mechanism.%20The%20project%20website%20is%20hosted%20at%0Ahttps%3A//andrewliao11.github.io/vlms_feedback%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06510v1&entry.124074799=Read"},
{"title": "Anchor-based Multi-view Subspace Clustering with Hierarchical Feature\n  Descent", "author": "Qiyuan Ou and Siwei Wang and Pei Zhang and Sihang Zhou and En Zhu", "abstract": "  Multi-view clustering has attracted growing attention owing to its\ncapabilities of aggregating information from various sources and its promising\nhorizons in public affairs. Up till now, many advanced approaches have been\nproposed in recent literature. However, there are several ongoing difficulties\nto be tackled. One common dilemma occurs while attempting to align the features\nof different views. {Moreover, due to the fact that many existing multi-view\nclustering algorithms stem from spectral clustering, this results to cubic time\ncomplexity w.r.t. the number of dataset. However, we propose Anchor-based\nMulti-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to\ntackle the discrepancy among views through hierarchical feature descent and\nproject to a common subspace( STAGE 1), which reveals dependency of different\nviews. We further reduce the computational complexity to linear time cost\nthrough a unified sampling strategy in the common subspace( STAGE 2), followed\nby anchor-based subspace clustering to learn the bipartite graph collectively(\nSTAGE 3). }Extensive experimental results on public benchmark datasets\ndemonstrate that our proposed model consistently outperforms the\nstate-of-the-art techniques.\n", "link": "http://arxiv.org/abs/2310.07166v2", "date": "2024-04-09", "relevancy": 1.9634, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4696}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4671}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anchor-based%20Multi-view%20Subspace%20Clustering%20with%20Hierarchical%20Feature%0A%20%20Descent&body=Title%3A%20Anchor-based%20Multi-view%20Subspace%20Clustering%20with%20Hierarchical%20Feature%0A%20%20Descent%0AAuthor%3A%20Qiyuan%20Ou%20and%20Siwei%20Wang%20and%20Pei%20Zhang%20and%20Sihang%20Zhou%20and%20En%20Zhu%0AAbstract%3A%20%20%20Multi-view%20clustering%20has%20attracted%20growing%20attention%20owing%20to%20its%0Acapabilities%20of%20aggregating%20information%20from%20various%20sources%20and%20its%20promising%0Ahorizons%20in%20public%20affairs.%20Up%20till%20now%2C%20many%20advanced%20approaches%20have%20been%0Aproposed%20in%20recent%20literature.%20However%2C%20there%20are%20several%20ongoing%20difficulties%0Ato%20be%20tackled.%20One%20common%20dilemma%20occurs%20while%20attempting%20to%20align%20the%20features%0Aof%20different%20views.%20%7BMoreover%2C%20due%20to%20the%20fact%20that%20many%20existing%20multi-view%0Aclustering%20algorithms%20stem%20from%20spectral%20clustering%2C%20this%20results%20to%20cubic%20time%0Acomplexity%20w.r.t.%20the%20number%20of%20dataset.%20However%2C%20we%20propose%20Anchor-based%0AMulti-view%20Subspace%20Clustering%20with%20Hierarchical%20Feature%20Descent%28MVSC-HFD%29%20to%0Atackle%20the%20discrepancy%20among%20views%20through%20hierarchical%20feature%20descent%20and%0Aproject%20to%20a%20common%20subspace%28%20STAGE%201%29%2C%20which%20reveals%20dependency%20of%20different%0Aviews.%20We%20further%20reduce%20the%20computational%20complexity%20to%20linear%20time%20cost%0Athrough%20a%20unified%20sampling%20strategy%20in%20the%20common%20subspace%28%20STAGE%202%29%2C%20followed%0Aby%20anchor-based%20subspace%20clustering%20to%20learn%20the%20bipartite%20graph%20collectively%28%0ASTAGE%203%29.%20%7DExtensive%20experimental%20results%20on%20public%20benchmark%20datasets%0Ademonstrate%20that%20our%20proposed%20model%20consistently%20outperforms%20the%0Astate-of-the-art%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07166v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anchor-based%20Multi-view%20Subspace%20Clustering%20with%20Hierarchical%20Feature%0A%20%20Descent&entry.906535625=Qiyuan%20Ou%20and%20Siwei%20Wang%20and%20Pei%20Zhang%20and%20Sihang%20Zhou%20and%20En%20Zhu&entry.1292438233=%20%20Multi-view%20clustering%20has%20attracted%20growing%20attention%20owing%20to%20its%0Acapabilities%20of%20aggregating%20information%20from%20various%20sources%20and%20its%20promising%0Ahorizons%20in%20public%20affairs.%20Up%20till%20now%2C%20many%20advanced%20approaches%20have%20been%0Aproposed%20in%20recent%20literature.%20However%2C%20there%20are%20several%20ongoing%20difficulties%0Ato%20be%20tackled.%20One%20common%20dilemma%20occurs%20while%20attempting%20to%20align%20the%20features%0Aof%20different%20views.%20%7BMoreover%2C%20due%20to%20the%20fact%20that%20many%20existing%20multi-view%0Aclustering%20algorithms%20stem%20from%20spectral%20clustering%2C%20this%20results%20to%20cubic%20time%0Acomplexity%20w.r.t.%20the%20number%20of%20dataset.%20However%2C%20we%20propose%20Anchor-based%0AMulti-view%20Subspace%20Clustering%20with%20Hierarchical%20Feature%20Descent%28MVSC-HFD%29%20to%0Atackle%20the%20discrepancy%20among%20views%20through%20hierarchical%20feature%20descent%20and%0Aproject%20to%20a%20common%20subspace%28%20STAGE%201%29%2C%20which%20reveals%20dependency%20of%20different%0Aviews.%20We%20further%20reduce%20the%20computational%20complexity%20to%20linear%20time%20cost%0Athrough%20a%20unified%20sampling%20strategy%20in%20the%20common%20subspace%28%20STAGE%202%29%2C%20followed%0Aby%20anchor-based%20subspace%20clustering%20to%20learn%20the%20bipartite%20graph%20collectively%28%0ASTAGE%203%29.%20%7DExtensive%20experimental%20results%20on%20public%20benchmark%20datasets%0Ademonstrate%20that%20our%20proposed%20model%20consistently%20outperforms%20the%0Astate-of-the-art%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07166v2&entry.124074799=Read"},
{"title": "NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for\n  Large Language Models", "author": "Amit Dhurandhar and Tejaswini Pedapati and Ronny Luss and Soham Dan and Aurelie Lozano and Payel Das and Georgios Kollias", "abstract": "  Transformer-based Language Models have become ubiquitous in Natural Language\nProcessing (NLP) due to their impressive performance on various tasks. However,\nexpensive training as well as inference remains a significant impediment to\ntheir widespread applicability. While enforcing sparsity at various levels of\nthe model architecture has found promise in addressing scaling and efficiency\nissues, there remains a disconnect between how sparsity affects network\ntopology. Inspired by brain neuronal networks, we explore sparsity approaches\nthrough the lens of network topology. Specifically, we exploit mechanisms seen\nin biological networks, such as preferential attachment and redundant synapse\npruning, and show that principled, model-agnostic sparsity approaches are\nperformant and efficient across diverse NLP tasks, spanning both classification\n(such as natural language inference) and generation (summarization, machine\ntranslation), despite our sole objective not being optimizing performance.\nNeuroPrune is competitive with (or sometimes superior to) baselines on\nperformance and can be up to $10$x faster in terms of training time for a given\nlevel of sparsity, simultaneously exhibiting measurable improvements in\ninference time in many cases.\n", "link": "http://arxiv.org/abs/2404.01306v2", "date": "2024-04-09", "relevancy": 1.9619, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5071}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4885}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4858}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NeuroPrune%3A%20A%20Neuro-inspired%20Topological%20Sparse%20Training%20Algorithm%20for%0A%20%20Large%20Language%20Models&body=Title%3A%20NeuroPrune%3A%20A%20Neuro-inspired%20Topological%20Sparse%20Training%20Algorithm%20for%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Amit%20Dhurandhar%20and%20Tejaswini%20Pedapati%20and%20Ronny%20Luss%20and%20Soham%20Dan%20and%20Aurelie%20Lozano%20and%20Payel%20Das%20and%20Georgios%20Kollias%0AAbstract%3A%20%20%20Transformer-based%20Language%20Models%20have%20become%20ubiquitous%20in%20Natural%20Language%0AProcessing%20%28NLP%29%20due%20to%20their%20impressive%20performance%20on%20various%20tasks.%20However%2C%0Aexpensive%20training%20as%20well%20as%20inference%20remains%20a%20significant%20impediment%20to%0Atheir%20widespread%20applicability.%20While%20enforcing%20sparsity%20at%20various%20levels%20of%0Athe%20model%20architecture%20has%20found%20promise%20in%20addressing%20scaling%20and%20efficiency%0Aissues%2C%20there%20remains%20a%20disconnect%20between%20how%20sparsity%20affects%20network%0Atopology.%20Inspired%20by%20brain%20neuronal%20networks%2C%20we%20explore%20sparsity%20approaches%0Athrough%20the%20lens%20of%20network%20topology.%20Specifically%2C%20we%20exploit%20mechanisms%20seen%0Ain%20biological%20networks%2C%20such%20as%20preferential%20attachment%20and%20redundant%20synapse%0Apruning%2C%20and%20show%20that%20principled%2C%20model-agnostic%20sparsity%20approaches%20are%0Aperformant%20and%20efficient%20across%20diverse%20NLP%20tasks%2C%20spanning%20both%20classification%0A%28such%20as%20natural%20language%20inference%29%20and%20generation%20%28summarization%2C%20machine%0Atranslation%29%2C%20despite%20our%20sole%20objective%20not%20being%20optimizing%20performance.%0ANeuroPrune%20is%20competitive%20with%20%28or%20sometimes%20superior%20to%29%20baselines%20on%0Aperformance%20and%20can%20be%20up%20to%20%2410%24x%20faster%20in%20terms%20of%20training%20time%20for%20a%20given%0Alevel%20of%20sparsity%2C%20simultaneously%20exhibiting%20measurable%20improvements%20in%0Ainference%20time%20in%20many%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01306v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroPrune%3A%20A%20Neuro-inspired%20Topological%20Sparse%20Training%20Algorithm%20for%0A%20%20Large%20Language%20Models&entry.906535625=Amit%20Dhurandhar%20and%20Tejaswini%20Pedapati%20and%20Ronny%20Luss%20and%20Soham%20Dan%20and%20Aurelie%20Lozano%20and%20Payel%20Das%20and%20Georgios%20Kollias&entry.1292438233=%20%20Transformer-based%20Language%20Models%20have%20become%20ubiquitous%20in%20Natural%20Language%0AProcessing%20%28NLP%29%20due%20to%20their%20impressive%20performance%20on%20various%20tasks.%20However%2C%0Aexpensive%20training%20as%20well%20as%20inference%20remains%20a%20significant%20impediment%20to%0Atheir%20widespread%20applicability.%20While%20enforcing%20sparsity%20at%20various%20levels%20of%0Athe%20model%20architecture%20has%20found%20promise%20in%20addressing%20scaling%20and%20efficiency%0Aissues%2C%20there%20remains%20a%20disconnect%20between%20how%20sparsity%20affects%20network%0Atopology.%20Inspired%20by%20brain%20neuronal%20networks%2C%20we%20explore%20sparsity%20approaches%0Athrough%20the%20lens%20of%20network%20topology.%20Specifically%2C%20we%20exploit%20mechanisms%20seen%0Ain%20biological%20networks%2C%20such%20as%20preferential%20attachment%20and%20redundant%20synapse%0Apruning%2C%20and%20show%20that%20principled%2C%20model-agnostic%20sparsity%20approaches%20are%0Aperformant%20and%20efficient%20across%20diverse%20NLP%20tasks%2C%20spanning%20both%20classification%0A%28such%20as%20natural%20language%20inference%29%20and%20generation%20%28summarization%2C%20machine%0Atranslation%29%2C%20despite%20our%20sole%20objective%20not%20being%20optimizing%20performance.%0ANeuroPrune%20is%20competitive%20with%20%28or%20sometimes%20superior%20to%29%20baselines%20on%0Aperformance%20and%20can%20be%20up%20to%20%2410%24x%20faster%20in%20terms%20of%20training%20time%20for%20a%20given%0Alevel%20of%20sparsity%2C%20simultaneously%20exhibiting%20measurable%20improvements%20in%0Ainference%20time%20in%20many%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01306v2&entry.124074799=Read"},
{"title": "RhythmMamba: Fast Remote Physiological Measurement with Arbitrary Length\n  Videos", "author": "Bochao Zou and Zizheng Guo and Xiaocheng Hu and Huimin Ma", "abstract": "  Remote photoplethysmography (rPPG) is a non-contact method for detecting\nphysiological signals from facial videos, holding great potential in various\napplications such as healthcare, affective computing, and anti-spoofing.\nExisting deep learning methods struggle to address two core issues of rPPG\nsimultaneously: extracting weak rPPG signals from video segments with large\nspatiotemporal redundancy and understanding the periodic patterns of rPPG among\nlong contexts. This represents a trade-off between computational complexity and\nthe ability to capture long-range dependencies, posing a challenge for rPPG\nthat is suitable for deployment on mobile devices. Based on the in-depth\nexploration of Mamba's comprehension of spatial and temporal information, this\npaper introduces RhythmMamba, an end-to-end Mamba-based method that employs\nmulti-temporal Mamba to constrain both periodic patterns and short-term trends,\ncoupled with frequency domain feed-forward to enable Mamba to robustly\nunderstand the quasi-periodic patterns of rPPG. Extensive experiments show that\nRhythmMamba achieves state-of-the-art performance with reduced parameters and\nlower computational complexity. The proposed RhythmMamba can be applied to\nvideo segments of any length without performance degradation. The codes are\navailable at https://github.com/zizheng-guo/RhythmMamba.\n", "link": "http://arxiv.org/abs/2404.06483v1", "date": "2024-04-09", "relevancy": 1.9615, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5123}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4868}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4852}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RhythmMamba%3A%20Fast%20Remote%20Physiological%20Measurement%20with%20Arbitrary%20Length%0A%20%20Videos&body=Title%3A%20RhythmMamba%3A%20Fast%20Remote%20Physiological%20Measurement%20with%20Arbitrary%20Length%0A%20%20Videos%0AAuthor%3A%20Bochao%20Zou%20and%20Zizheng%20Guo%20and%20Xiaocheng%20Hu%20and%20Huimin%20Ma%0AAbstract%3A%20%20%20Remote%20photoplethysmography%20%28rPPG%29%20is%20a%20non-contact%20method%20for%20detecting%0Aphysiological%20signals%20from%20facial%20videos%2C%20holding%20great%20potential%20in%20various%0Aapplications%20such%20as%20healthcare%2C%20affective%20computing%2C%20and%20anti-spoofing.%0AExisting%20deep%20learning%20methods%20struggle%20to%20address%20two%20core%20issues%20of%20rPPG%0Asimultaneously%3A%20extracting%20weak%20rPPG%20signals%20from%20video%20segments%20with%20large%0Aspatiotemporal%20redundancy%20and%20understanding%20the%20periodic%20patterns%20of%20rPPG%20among%0Along%20contexts.%20This%20represents%20a%20trade-off%20between%20computational%20complexity%20and%0Athe%20ability%20to%20capture%20long-range%20dependencies%2C%20posing%20a%20challenge%20for%20rPPG%0Athat%20is%20suitable%20for%20deployment%20on%20mobile%20devices.%20Based%20on%20the%20in-depth%0Aexploration%20of%20Mamba%27s%20comprehension%20of%20spatial%20and%20temporal%20information%2C%20this%0Apaper%20introduces%20RhythmMamba%2C%20an%20end-to-end%20Mamba-based%20method%20that%20employs%0Amulti-temporal%20Mamba%20to%20constrain%20both%20periodic%20patterns%20and%20short-term%20trends%2C%0Acoupled%20with%20frequency%20domain%20feed-forward%20to%20enable%20Mamba%20to%20robustly%0Aunderstand%20the%20quasi-periodic%20patterns%20of%20rPPG.%20Extensive%20experiments%20show%20that%0ARhythmMamba%20achieves%20state-of-the-art%20performance%20with%20reduced%20parameters%20and%0Alower%20computational%20complexity.%20The%20proposed%20RhythmMamba%20can%20be%20applied%20to%0Avideo%20segments%20of%20any%20length%20without%20performance%20degradation.%20The%20codes%20are%0Aavailable%20at%20https%3A//github.com/zizheng-guo/RhythmMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06483v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RhythmMamba%3A%20Fast%20Remote%20Physiological%20Measurement%20with%20Arbitrary%20Length%0A%20%20Videos&entry.906535625=Bochao%20Zou%20and%20Zizheng%20Guo%20and%20Xiaocheng%20Hu%20and%20Huimin%20Ma&entry.1292438233=%20%20Remote%20photoplethysmography%20%28rPPG%29%20is%20a%20non-contact%20method%20for%20detecting%0Aphysiological%20signals%20from%20facial%20videos%2C%20holding%20great%20potential%20in%20various%0Aapplications%20such%20as%20healthcare%2C%20affective%20computing%2C%20and%20anti-spoofing.%0AExisting%20deep%20learning%20methods%20struggle%20to%20address%20two%20core%20issues%20of%20rPPG%0Asimultaneously%3A%20extracting%20weak%20rPPG%20signals%20from%20video%20segments%20with%20large%0Aspatiotemporal%20redundancy%20and%20understanding%20the%20periodic%20patterns%20of%20rPPG%20among%0Along%20contexts.%20This%20represents%20a%20trade-off%20between%20computational%20complexity%20and%0Athe%20ability%20to%20capture%20long-range%20dependencies%2C%20posing%20a%20challenge%20for%20rPPG%0Athat%20is%20suitable%20for%20deployment%20on%20mobile%20devices.%20Based%20on%20the%20in-depth%0Aexploration%20of%20Mamba%27s%20comprehension%20of%20spatial%20and%20temporal%20information%2C%20this%0Apaper%20introduces%20RhythmMamba%2C%20an%20end-to-end%20Mamba-based%20method%20that%20employs%0Amulti-temporal%20Mamba%20to%20constrain%20both%20periodic%20patterns%20and%20short-term%20trends%2C%0Acoupled%20with%20frequency%20domain%20feed-forward%20to%20enable%20Mamba%20to%20robustly%0Aunderstand%20the%20quasi-periodic%20patterns%20of%20rPPG.%20Extensive%20experiments%20show%20that%0ARhythmMamba%20achieves%20state-of-the-art%20performance%20with%20reduced%20parameters%20and%0Alower%20computational%20complexity.%20The%20proposed%20RhythmMamba%20can%20be%20applied%20to%0Avideo%20segments%20of%20any%20length%20without%20performance%20degradation.%20The%20codes%20are%0Aavailable%20at%20https%3A//github.com/zizheng-guo/RhythmMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06483v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning-Based Approach for a Single Vehicle\n  Persistent Surveillance Problem with Fuel Constraints", "author": "Hritik Bana and Manav Mishra and Saswata Sarkar and Sujeevraja Sanjeevi and Sujit PB and Kaarthik Sundar", "abstract": "  This article presents a deep reinforcement learning-based approach to tackle\na persistent surveillance mission requiring a single unmanned aerial vehicle\ninitially stationed at a depot with fuel or time-of-flight constraints to\nrepeatedly visit a set of targets with equal priority. Owing to the vehicle's\nfuel or time-of-flight constraints, the vehicle must be regularly refueled, or\nits battery must be recharged at the depot. The objective of the problem is to\ndetermine an optimal sequence of visits to the targets that minimizes the\nmaximum time elapsed between successive visits to any target while ensuring\nthat the vehicle never runs out of fuel or charge. We present a deep\nreinforcement learning algorithm to solve this problem and present the results\nof numerical experiments that corroborate the effectiveness of this approach in\ncomparison with common-sense greedy heuristics.\n", "link": "http://arxiv.org/abs/2404.06423v1", "date": "2024-04-09", "relevancy": 1.9601, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4966}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4872}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4805}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning-Based%20Approach%20for%20a%20Single%20Vehicle%0A%20%20Persistent%20Surveillance%20Problem%20with%20Fuel%20Constraints&body=Title%3A%20Deep%20Reinforcement%20Learning-Based%20Approach%20for%20a%20Single%20Vehicle%0A%20%20Persistent%20Surveillance%20Problem%20with%20Fuel%20Constraints%0AAuthor%3A%20Hritik%20Bana%20and%20Manav%20Mishra%20and%20Saswata%20Sarkar%20and%20Sujeevraja%20Sanjeevi%20and%20Sujit%20PB%20and%20Kaarthik%20Sundar%0AAbstract%3A%20%20%20This%20article%20presents%20a%20deep%20reinforcement%20learning-based%20approach%20to%20tackle%0Aa%20persistent%20surveillance%20mission%20requiring%20a%20single%20unmanned%20aerial%20vehicle%0Ainitially%20stationed%20at%20a%20depot%20with%20fuel%20or%20time-of-flight%20constraints%20to%0Arepeatedly%20visit%20a%20set%20of%20targets%20with%20equal%20priority.%20Owing%20to%20the%20vehicle%27s%0Afuel%20or%20time-of-flight%20constraints%2C%20the%20vehicle%20must%20be%20regularly%20refueled%2C%20or%0Aits%20battery%20must%20be%20recharged%20at%20the%20depot.%20The%20objective%20of%20the%20problem%20is%20to%0Adetermine%20an%20optimal%20sequence%20of%20visits%20to%20the%20targets%20that%20minimizes%20the%0Amaximum%20time%20elapsed%20between%20successive%20visits%20to%20any%20target%20while%20ensuring%0Athat%20the%20vehicle%20never%20runs%20out%20of%20fuel%20or%20charge.%20We%20present%20a%20deep%0Areinforcement%20learning%20algorithm%20to%20solve%20this%20problem%20and%20present%20the%20results%0Aof%20numerical%20experiments%20that%20corroborate%20the%20effectiveness%20of%20this%20approach%20in%0Acomparison%20with%20common-sense%20greedy%20heuristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06423v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning-Based%20Approach%20for%20a%20Single%20Vehicle%0A%20%20Persistent%20Surveillance%20Problem%20with%20Fuel%20Constraints&entry.906535625=Hritik%20Bana%20and%20Manav%20Mishra%20and%20Saswata%20Sarkar%20and%20Sujeevraja%20Sanjeevi%20and%20Sujit%20PB%20and%20Kaarthik%20Sundar&entry.1292438233=%20%20This%20article%20presents%20a%20deep%20reinforcement%20learning-based%20approach%20to%20tackle%0Aa%20persistent%20surveillance%20mission%20requiring%20a%20single%20unmanned%20aerial%20vehicle%0Ainitially%20stationed%20at%20a%20depot%20with%20fuel%20or%20time-of-flight%20constraints%20to%0Arepeatedly%20visit%20a%20set%20of%20targets%20with%20equal%20priority.%20Owing%20to%20the%20vehicle%27s%0Afuel%20or%20time-of-flight%20constraints%2C%20the%20vehicle%20must%20be%20regularly%20refueled%2C%20or%0Aits%20battery%20must%20be%20recharged%20at%20the%20depot.%20The%20objective%20of%20the%20problem%20is%20to%0Adetermine%20an%20optimal%20sequence%20of%20visits%20to%20the%20targets%20that%20minimizes%20the%0Amaximum%20time%20elapsed%20between%20successive%20visits%20to%20any%20target%20while%20ensuring%0Athat%20the%20vehicle%20never%20runs%20out%20of%20fuel%20or%20charge.%20We%20present%20a%20deep%0Areinforcement%20learning%20algorithm%20to%20solve%20this%20problem%20and%20present%20the%20results%0Aof%20numerical%20experiments%20that%20corroborate%20the%20effectiveness%20of%20this%20approach%20in%0Acomparison%20with%20common-sense%20greedy%20heuristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06423v1&entry.124074799=Read"},
{"title": "On adversarial training and the 1 Nearest Neighbor classifier", "author": "Amir Hagai and Yair Weiss", "abstract": "  The ability to fool deep learning classifiers with tiny perturbations of the\ninput has lead to the development of adversarial training in which the loss\nwith respect to adversarial examples is minimized in addition to the training\nexamples. While adversarial training improves the robustness of the learned\nclassifiers, the procedure is computationally expensive, sensitive to\nhyperparameters and may still leave the classifier vulnerable to other types of\nsmall perturbations. In this paper we analyze the adversarial robustness of the\n1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial\ntraining. We prove that under reasonable assumptions, the 1 NN classifier will\nbe robust to {\\em any} small image perturbation of the training images and will\ngive high adversarial accuracy on test images as the number of training\nexamples goes to infinity. In experiments with 45 different binary image\nclassification problems taken from CIFAR10, we find that 1NN outperform TRADES\n(a powerful adversarial training algorithm) in terms of average adversarial\naccuracy. In additional experiments with 69 pretrained robust models for\nCIFAR10, we find that 1NN outperforms almost all of them in terms of robustness\nto perturbations that are only slightly different from those seen during\ntraining. Taken together, our results suggest that modern adversarial training\nmethods still fall short of the robustness of the simple 1NN classifier. our\ncode can be found at\nhttps://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier\n", "link": "http://arxiv.org/abs/2404.06313v1", "date": "2024-04-09", "relevancy": 1.9593, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5051}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.484}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4663}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20adversarial%20training%20and%20the%201%20Nearest%20Neighbor%20classifier&body=Title%3A%20On%20adversarial%20training%20and%20the%201%20Nearest%20Neighbor%20classifier%0AAuthor%3A%20Amir%20Hagai%20and%20Yair%20Weiss%0AAbstract%3A%20%20%20The%20ability%20to%20fool%20deep%20learning%20classifiers%20with%20tiny%20perturbations%20of%20the%0Ainput%20has%20lead%20to%20the%20development%20of%20adversarial%20training%20in%20which%20the%20loss%0Awith%20respect%20to%20adversarial%20examples%20is%20minimized%20in%20addition%20to%20the%20training%0Aexamples.%20While%20adversarial%20training%20improves%20the%20robustness%20of%20the%20learned%0Aclassifiers%2C%20the%20procedure%20is%20computationally%20expensive%2C%20sensitive%20to%0Ahyperparameters%20and%20may%20still%20leave%20the%20classifier%20vulnerable%20to%20other%20types%20of%0Asmall%20perturbations.%20In%20this%20paper%20we%20analyze%20the%20adversarial%20robustness%20of%20the%0A1%20Nearest%20Neighbor%20%281NN%29%20classifier%20and%20compare%20its%20performance%20to%20adversarial%0Atraining.%20We%20prove%20that%20under%20reasonable%20assumptions%2C%20the%201%20NN%20classifier%20will%0Abe%20robust%20to%20%7B%5Cem%20any%7D%20small%20image%20perturbation%20of%20the%20training%20images%20and%20will%0Agive%20high%20adversarial%20accuracy%20on%20test%20images%20as%20the%20number%20of%20training%0Aexamples%20goes%20to%20infinity.%20In%20experiments%20with%2045%20different%20binary%20image%0Aclassification%20problems%20taken%20from%20CIFAR10%2C%20we%20find%20that%201NN%20outperform%20TRADES%0A%28a%20powerful%20adversarial%20training%20algorithm%29%20in%20terms%20of%20average%20adversarial%0Aaccuracy.%20In%20additional%20experiments%20with%2069%20pretrained%20robust%20models%20for%0ACIFAR10%2C%20we%20find%20that%201NN%20outperforms%20almost%20all%20of%20them%20in%20terms%20of%20robustness%0Ato%20perturbations%20that%20are%20only%20slightly%20different%20from%20those%20seen%20during%0Atraining.%20Taken%20together%2C%20our%20results%20suggest%20that%20modern%20adversarial%20training%0Amethods%20still%20fall%20short%20of%20the%20robustness%20of%20the%20simple%201NN%20classifier.%20our%0Acode%20can%20be%20found%20at%0Ahttps%3A//github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06313v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20adversarial%20training%20and%20the%201%20Nearest%20Neighbor%20classifier&entry.906535625=Amir%20Hagai%20and%20Yair%20Weiss&entry.1292438233=%20%20The%20ability%20to%20fool%20deep%20learning%20classifiers%20with%20tiny%20perturbations%20of%20the%0Ainput%20has%20lead%20to%20the%20development%20of%20adversarial%20training%20in%20which%20the%20loss%0Awith%20respect%20to%20adversarial%20examples%20is%20minimized%20in%20addition%20to%20the%20training%0Aexamples.%20While%20adversarial%20training%20improves%20the%20robustness%20of%20the%20learned%0Aclassifiers%2C%20the%20procedure%20is%20computationally%20expensive%2C%20sensitive%20to%0Ahyperparameters%20and%20may%20still%20leave%20the%20classifier%20vulnerable%20to%20other%20types%20of%0Asmall%20perturbations.%20In%20this%20paper%20we%20analyze%20the%20adversarial%20robustness%20of%20the%0A1%20Nearest%20Neighbor%20%281NN%29%20classifier%20and%20compare%20its%20performance%20to%20adversarial%0Atraining.%20We%20prove%20that%20under%20reasonable%20assumptions%2C%20the%201%20NN%20classifier%20will%0Abe%20robust%20to%20%7B%5Cem%20any%7D%20small%20image%20perturbation%20of%20the%20training%20images%20and%20will%0Agive%20high%20adversarial%20accuracy%20on%20test%20images%20as%20the%20number%20of%20training%0Aexamples%20goes%20to%20infinity.%20In%20experiments%20with%2045%20different%20binary%20image%0Aclassification%20problems%20taken%20from%20CIFAR10%2C%20we%20find%20that%201NN%20outperform%20TRADES%0A%28a%20powerful%20adversarial%20training%20algorithm%29%20in%20terms%20of%20average%20adversarial%0Aaccuracy.%20In%20additional%20experiments%20with%2069%20pretrained%20robust%20models%20for%0ACIFAR10%2C%20we%20find%20that%201NN%20outperforms%20almost%20all%20of%20them%20in%20terms%20of%20robustness%0Ato%20perturbations%20that%20are%20only%20slightly%20different%20from%20those%20seen%20during%0Atraining.%20Taken%20together%2C%20our%20results%20suggest%20that%20modern%20adversarial%20training%0Amethods%20still%20fall%20short%20of%20the%20robustness%20of%20the%20simple%201NN%20classifier.%20our%0Acode%20can%20be%20found%20at%0Ahttps%3A//github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06313v1&entry.124074799=Read"},
{"title": "Experimental System Design of an Active Fault-Tolerant Quadrotor", "author": "Jennifer Yeom and Roshan Balu T M B and Guanrui Li and Giuseppe Loianno", "abstract": "  Quadrotors have gained popularity over the last decade, aiding humans in\ncomplex tasks such as search and rescue, mapping and exploration. Despite their\nmechanical simplicity and versatility compared to other types of aerial\nvehicles, they remain vulnerable to rotor failures. In this paper, we propose\nan algorithmic and mechanical approach to addressing the quadrotor\nfault-tolerant problem in case of rotor failures. First, we present a\nfault-tolerant detection and control scheme that includes various attitude\nerror metrics. The scheme transitions to a fault-tolerant control mode by\nsurrendering the yaw control. Subsequently, to ensure compatibility with\nplatform sensing constraints, we investigate the relationship between\nvariations in robot rotational drag, achieved through a modular mechanical\ndesign appendage, resulting in yaw rates within sensor limits. This analysis\noffers a platform-agnostic framework for designing more reliable and robust\nquadrotors in the event of rotor failures. Extensive experimental results\nvalidate the proposed approach providing insights into successfully designing a\ncost-effective quadrotor capable of fault-tolerant control. The overall design\nenhances safety in scenarios of faulty rotors, without the need for additional\nsensors or computational resources.\n", "link": "http://arxiv.org/abs/2404.06340v1", "date": "2024-04-09", "relevancy": 1.9486, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5198}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4933}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4521}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Experimental%20System%20Design%20of%20an%20Active%20Fault-Tolerant%20Quadrotor&body=Title%3A%20Experimental%20System%20Design%20of%20an%20Active%20Fault-Tolerant%20Quadrotor%0AAuthor%3A%20Jennifer%20Yeom%20and%20Roshan%20Balu%20T%20M%20B%20and%20Guanrui%20Li%20and%20Giuseppe%20Loianno%0AAbstract%3A%20%20%20Quadrotors%20have%20gained%20popularity%20over%20the%20last%20decade%2C%20aiding%20humans%20in%0Acomplex%20tasks%20such%20as%20search%20and%20rescue%2C%20mapping%20and%20exploration.%20Despite%20their%0Amechanical%20simplicity%20and%20versatility%20compared%20to%20other%20types%20of%20aerial%0Avehicles%2C%20they%20remain%20vulnerable%20to%20rotor%20failures.%20In%20this%20paper%2C%20we%20propose%0Aan%20algorithmic%20and%20mechanical%20approach%20to%20addressing%20the%20quadrotor%0Afault-tolerant%20problem%20in%20case%20of%20rotor%20failures.%20First%2C%20we%20present%20a%0Afault-tolerant%20detection%20and%20control%20scheme%20that%20includes%20various%20attitude%0Aerror%20metrics.%20The%20scheme%20transitions%20to%20a%20fault-tolerant%20control%20mode%20by%0Asurrendering%20the%20yaw%20control.%20Subsequently%2C%20to%20ensure%20compatibility%20with%0Aplatform%20sensing%20constraints%2C%20we%20investigate%20the%20relationship%20between%0Avariations%20in%20robot%20rotational%20drag%2C%20achieved%20through%20a%20modular%20mechanical%0Adesign%20appendage%2C%20resulting%20in%20yaw%20rates%20within%20sensor%20limits.%20This%20analysis%0Aoffers%20a%20platform-agnostic%20framework%20for%20designing%20more%20reliable%20and%20robust%0Aquadrotors%20in%20the%20event%20of%20rotor%20failures.%20Extensive%20experimental%20results%0Avalidate%20the%20proposed%20approach%20providing%20insights%20into%20successfully%20designing%20a%0Acost-effective%20quadrotor%20capable%20of%20fault-tolerant%20control.%20The%20overall%20design%0Aenhances%20safety%20in%20scenarios%20of%20faulty%20rotors%2C%20without%20the%20need%20for%20additional%0Asensors%20or%20computational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06340v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Experimental%20System%20Design%20of%20an%20Active%20Fault-Tolerant%20Quadrotor&entry.906535625=Jennifer%20Yeom%20and%20Roshan%20Balu%20T%20M%20B%20and%20Guanrui%20Li%20and%20Giuseppe%20Loianno&entry.1292438233=%20%20Quadrotors%20have%20gained%20popularity%20over%20the%20last%20decade%2C%20aiding%20humans%20in%0Acomplex%20tasks%20such%20as%20search%20and%20rescue%2C%20mapping%20and%20exploration.%20Despite%20their%0Amechanical%20simplicity%20and%20versatility%20compared%20to%20other%20types%20of%20aerial%0Avehicles%2C%20they%20remain%20vulnerable%20to%20rotor%20failures.%20In%20this%20paper%2C%20we%20propose%0Aan%20algorithmic%20and%20mechanical%20approach%20to%20addressing%20the%20quadrotor%0Afault-tolerant%20problem%20in%20case%20of%20rotor%20failures.%20First%2C%20we%20present%20a%0Afault-tolerant%20detection%20and%20control%20scheme%20that%20includes%20various%20attitude%0Aerror%20metrics.%20The%20scheme%20transitions%20to%20a%20fault-tolerant%20control%20mode%20by%0Asurrendering%20the%20yaw%20control.%20Subsequently%2C%20to%20ensure%20compatibility%20with%0Aplatform%20sensing%20constraints%2C%20we%20investigate%20the%20relationship%20between%0Avariations%20in%20robot%20rotational%20drag%2C%20achieved%20through%20a%20modular%20mechanical%0Adesign%20appendage%2C%20resulting%20in%20yaw%20rates%20within%20sensor%20limits.%20This%20analysis%0Aoffers%20a%20platform-agnostic%20framework%20for%20designing%20more%20reliable%20and%20robust%0Aquadrotors%20in%20the%20event%20of%20rotor%20failures.%20Extensive%20experimental%20results%0Avalidate%20the%20proposed%20approach%20providing%20insights%20into%20successfully%20designing%20a%0Acost-effective%20quadrotor%20capable%20of%20fault-tolerant%20control.%20The%20overall%20design%0Aenhances%20safety%20in%20scenarios%20of%20faulty%20rotors%2C%20without%20the%20need%20for%20additional%0Asensors%20or%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06340v1&entry.124074799=Read"},
{"title": "What is the $\\textit{intrinsic}$ dimension of your binary data? -- and\n  how to compute it quickly", "author": "Tom Hanika and Tobias Hille", "abstract": "  Dimensionality is an important aspect for analyzing and understanding\n(high-dimensional) data. In their 2006 ICDM paper Tatti et al. answered the\nquestion for a (interpretable) dimension of binary data tables by introducing a\nnormalized correlation dimension. In the present work we revisit their results\nand contrast them with a concept based notion of intrinsic dimension (ID)\nrecently introduced for geometric data sets. To do this, we present a novel\napproximation for this ID that is based on computing concepts only up to a\ncertain support value. We demonstrate and evaluate our approximation using all\navailable datasets from Tatti et al., which have between 469 and 41271\nextrinsic dimensions.\n", "link": "http://arxiv.org/abs/2404.06326v1", "date": "2024-04-09", "relevancy": 1.9378, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3985}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3859}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3783}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20What%20is%20the%20%24%5Ctextit%7Bintrinsic%7D%24%20dimension%20of%20your%20binary%20data%3F%20--%20and%0A%20%20how%20to%20compute%20it%20quickly&body=Title%3A%20What%20is%20the%20%24%5Ctextit%7Bintrinsic%7D%24%20dimension%20of%20your%20binary%20data%3F%20--%20and%0A%20%20how%20to%20compute%20it%20quickly%0AAuthor%3A%20Tom%20Hanika%20and%20Tobias%20Hille%0AAbstract%3A%20%20%20Dimensionality%20is%20an%20important%20aspect%20for%20analyzing%20and%20understanding%0A%28high-dimensional%29%20data.%20In%20their%202006%20ICDM%20paper%20Tatti%20et%20al.%20answered%20the%0Aquestion%20for%20a%20%28interpretable%29%20dimension%20of%20binary%20data%20tables%20by%20introducing%20a%0Anormalized%20correlation%20dimension.%20In%20the%20present%20work%20we%20revisit%20their%20results%0Aand%20contrast%20them%20with%20a%20concept%20based%20notion%20of%20intrinsic%20dimension%20%28ID%29%0Arecently%20introduced%20for%20geometric%20data%20sets.%20To%20do%20this%2C%20we%20present%20a%20novel%0Aapproximation%20for%20this%20ID%20that%20is%20based%20on%20computing%20concepts%20only%20up%20to%20a%0Acertain%20support%20value.%20We%20demonstrate%20and%20evaluate%20our%20approximation%20using%20all%0Aavailable%20datasets%20from%20Tatti%20et%20al.%2C%20which%20have%20between%20469%20and%2041271%0Aextrinsic%20dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06326v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20is%20the%20%24%5Ctextit%7Bintrinsic%7D%24%20dimension%20of%20your%20binary%20data%3F%20--%20and%0A%20%20how%20to%20compute%20it%20quickly&entry.906535625=Tom%20Hanika%20and%20Tobias%20Hille&entry.1292438233=%20%20Dimensionality%20is%20an%20important%20aspect%20for%20analyzing%20and%20understanding%0A%28high-dimensional%29%20data.%20In%20their%202006%20ICDM%20paper%20Tatti%20et%20al.%20answered%20the%0Aquestion%20for%20a%20%28interpretable%29%20dimension%20of%20binary%20data%20tables%20by%20introducing%20a%0Anormalized%20correlation%20dimension.%20In%20the%20present%20work%20we%20revisit%20their%20results%0Aand%20contrast%20them%20with%20a%20concept%20based%20notion%20of%20intrinsic%20dimension%20%28ID%29%0Arecently%20introduced%20for%20geometric%20data%20sets.%20To%20do%20this%2C%20we%20present%20a%20novel%0Aapproximation%20for%20this%20ID%20that%20is%20based%20on%20computing%20concepts%20only%20up%20to%20a%0Acertain%20support%20value.%20We%20demonstrate%20and%20evaluate%20our%20approximation%20using%20all%0Aavailable%20datasets%20from%20Tatti%20et%20al.%2C%20which%20have%20between%20469%20and%2041271%0Aextrinsic%20dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06326v1&entry.124074799=Read"},
{"title": "Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks", "author": "Chonghua Wang and Haodong Duan and Songyang Zhang and Dahua Lin and Kai Chen", "abstract": "  Recently, the large language model (LLM) community has shown increasing\ninterest in enhancing LLMs' capability to handle extremely long documents. As\nvarious long-text techniques and model architectures emerge, the precise and\ndetailed evaluation of models' long-text capabilities has become increasingly\nimportant. Existing long-text evaluation benchmarks, such as L-Eval and\nLongBench, construct long-text test sets based on open-source datasets,\nfocusing mainly on QA and summarization tasks. These datasets include test\nsamples of varying lengths (from 2k to 32k+) entangled together, making it\nchallenging to assess model capabilities across different length ranges.\nMoreover, they do not cover the ultralong settings (100k+ tokens) that the\nlatest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a\nlength-adaptable benchmark for evaluating the long-context understanding of\nLLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which\nenable a more reliable evaluation of LLMs' long context capabilities. These\nbenchmarks support intricate manipulation of the length of test cases, and can\neasily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art\nclosed-source API models and 6 open-source models with Ada-LEval. The\nevaluation results demonstrate the limitations of current LLMs, especially in\nultra-long-context settings. Our code is available at\nhttps://github.com/open-compass/Ada-LEval.\n", "link": "http://arxiv.org/abs/2404.06480v1", "date": "2024-04-09", "relevancy": 1.9277, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5073}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4583}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ada-LEval%3A%20Evaluating%20long-context%20LLMs%20with%20length-adaptable%20benchmarks&body=Title%3A%20Ada-LEval%3A%20Evaluating%20long-context%20LLMs%20with%20length-adaptable%20benchmarks%0AAuthor%3A%20Chonghua%20Wang%20and%20Haodong%20Duan%20and%20Songyang%20Zhang%20and%20Dahua%20Lin%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Recently%2C%20the%20large%20language%20model%20%28LLM%29%20community%20has%20shown%20increasing%0Ainterest%20in%20enhancing%20LLMs%27%20capability%20to%20handle%20extremely%20long%20documents.%20As%0Avarious%20long-text%20techniques%20and%20model%20architectures%20emerge%2C%20the%20precise%20and%0Adetailed%20evaluation%20of%20models%27%20long-text%20capabilities%20has%20become%20increasingly%0Aimportant.%20Existing%20long-text%20evaluation%20benchmarks%2C%20such%20as%20L-Eval%20and%0ALongBench%2C%20construct%20long-text%20test%20sets%20based%20on%20open-source%20datasets%2C%0Afocusing%20mainly%20on%20QA%20and%20summarization%20tasks.%20These%20datasets%20include%20test%0Asamples%20of%20varying%20lengths%20%28from%202k%20to%2032k%2B%29%20entangled%20together%2C%20making%20it%0Achallenging%20to%20assess%20model%20capabilities%20across%20different%20length%20ranges.%0AMoreover%2C%20they%20do%20not%20cover%20the%20ultralong%20settings%20%28100k%2B%20tokens%29%20that%20the%0Alatest%20LLMs%20claim%20to%20achieve.%20In%20this%20paper%2C%20we%20introduce%20Ada-LEval%2C%20a%0Alength-adaptable%20benchmark%20for%20evaluating%20the%20long-context%20understanding%20of%0ALLMs.%20Ada-LEval%20includes%20two%20challenging%20subsets%2C%20TSort%20and%20BestAnswer%2C%20which%0Aenable%20a%20more%20reliable%20evaluation%20of%20LLMs%27%20long%20context%20capabilities.%20These%0Abenchmarks%20support%20intricate%20manipulation%20of%20the%20length%20of%20test%20cases%2C%20and%20can%0Aeasily%20produce%20text%20samples%20up%20to%20128k%20tokens.%20We%20evaluate%204%20state-of-the-art%0Aclosed-source%20API%20models%20and%206%20open-source%20models%20with%20Ada-LEval.%20The%0Aevaluation%20results%20demonstrate%20the%20limitations%20of%20current%20LLMs%2C%20especially%20in%0Aultra-long-context%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/open-compass/Ada-LEval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06480v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ada-LEval%3A%20Evaluating%20long-context%20LLMs%20with%20length-adaptable%20benchmarks&entry.906535625=Chonghua%20Wang%20and%20Haodong%20Duan%20and%20Songyang%20Zhang%20and%20Dahua%20Lin%20and%20Kai%20Chen&entry.1292438233=%20%20Recently%2C%20the%20large%20language%20model%20%28LLM%29%20community%20has%20shown%20increasing%0Ainterest%20in%20enhancing%20LLMs%27%20capability%20to%20handle%20extremely%20long%20documents.%20As%0Avarious%20long-text%20techniques%20and%20model%20architectures%20emerge%2C%20the%20precise%20and%0Adetailed%20evaluation%20of%20models%27%20long-text%20capabilities%20has%20become%20increasingly%0Aimportant.%20Existing%20long-text%20evaluation%20benchmarks%2C%20such%20as%20L-Eval%20and%0ALongBench%2C%20construct%20long-text%20test%20sets%20based%20on%20open-source%20datasets%2C%0Afocusing%20mainly%20on%20QA%20and%20summarization%20tasks.%20These%20datasets%20include%20test%0Asamples%20of%20varying%20lengths%20%28from%202k%20to%2032k%2B%29%20entangled%20together%2C%20making%20it%0Achallenging%20to%20assess%20model%20capabilities%20across%20different%20length%20ranges.%0AMoreover%2C%20they%20do%20not%20cover%20the%20ultralong%20settings%20%28100k%2B%20tokens%29%20that%20the%0Alatest%20LLMs%20claim%20to%20achieve.%20In%20this%20paper%2C%20we%20introduce%20Ada-LEval%2C%20a%0Alength-adaptable%20benchmark%20for%20evaluating%20the%20long-context%20understanding%20of%0ALLMs.%20Ada-LEval%20includes%20two%20challenging%20subsets%2C%20TSort%20and%20BestAnswer%2C%20which%0Aenable%20a%20more%20reliable%20evaluation%20of%20LLMs%27%20long%20context%20capabilities.%20These%0Abenchmarks%20support%20intricate%20manipulation%20of%20the%20length%20of%20test%20cases%2C%20and%20can%0Aeasily%20produce%20text%20samples%20up%20to%20128k%20tokens.%20We%20evaluate%204%20state-of-the-art%0Aclosed-source%20API%20models%20and%206%20open-source%20models%20with%20Ada-LEval.%20The%0Aevaluation%20results%20demonstrate%20the%20limitations%20of%20current%20LLMs%2C%20especially%20in%0Aultra-long-context%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/open-compass/Ada-LEval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06480v1&entry.124074799=Read"},
{"title": "Automatic Defect Detection in Sewer Network Using Deep Learning Based\n  Object Detector", "author": "Bach Ha and Birgit Schalter and Laura White and Joachim Koehler", "abstract": "  Maintaining sewer systems in large cities is important, but also time and\neffort consuming, because visual inspections are currently done manually. To\nreduce the amount of aforementioned manual work, defects within sewer pipes\nshould be located and classified automatically. In the past, multiple works\nhave attempted solving this problem using classical image processing, machine\nlearning, or a combination of those. However, each provided solution only focus\non detecting a limited set of defect/structure types, such as fissure, root,\nand/or connection. Furthermore, due to the use of hand-crafted features and\nsmall training datasets, generalization is also problematic. In order to\novercome these deficits, a sizable dataset with 14.7 km of various sewer pipes\nwere annotated by sewer maintenance experts in the scope of this work. On top\nof that, an object detector (EfficientDet-D0) was trained for automatic defect\ndetection. From the result of several expermients, peculiar natures of defects\nin the context of object detection, which greatly effect annotation and\ntraining process, are found and discussed. At the end, the final detector was\nable to detect 83% of defects in the test set; out of the missing 17%, only\n0.77% are very severe defects. This work provides an example of applying deep\nlearning-based object detection into an important but quiet engineering field.\nIt also gives some practical pointers on how to annotate peculiar \"object\",\nsuch as defects.\n", "link": "http://arxiv.org/abs/2404.06219v1", "date": "2024-04-09", "relevancy": 1.9211, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5258}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4762}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4662}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Defect%20Detection%20in%20Sewer%20Network%20Using%20Deep%20Learning%20Based%0A%20%20Object%20Detector&body=Title%3A%20Automatic%20Defect%20Detection%20in%20Sewer%20Network%20Using%20Deep%20Learning%20Based%0A%20%20Object%20Detector%0AAuthor%3A%20Bach%20Ha%20and%20Birgit%20Schalter%20and%20Laura%20White%20and%20Joachim%20Koehler%0AAbstract%3A%20%20%20Maintaining%20sewer%20systems%20in%20large%20cities%20is%20important%2C%20but%20also%20time%20and%0Aeffort%20consuming%2C%20because%20visual%20inspections%20are%20currently%20done%20manually.%20To%0Areduce%20the%20amount%20of%20aforementioned%20manual%20work%2C%20defects%20within%20sewer%20pipes%0Ashould%20be%20located%20and%20classified%20automatically.%20In%20the%20past%2C%20multiple%20works%0Ahave%20attempted%20solving%20this%20problem%20using%20classical%20image%20processing%2C%20machine%0Alearning%2C%20or%20a%20combination%20of%20those.%20However%2C%20each%20provided%20solution%20only%20focus%0Aon%20detecting%20a%20limited%20set%20of%20defect/structure%20types%2C%20such%20as%20fissure%2C%20root%2C%0Aand/or%20connection.%20Furthermore%2C%20due%20to%20the%20use%20of%20hand-crafted%20features%20and%0Asmall%20training%20datasets%2C%20generalization%20is%20also%20problematic.%20In%20order%20to%0Aovercome%20these%20deficits%2C%20a%20sizable%20dataset%20with%2014.7%20km%20of%20various%20sewer%20pipes%0Awere%20annotated%20by%20sewer%20maintenance%20experts%20in%20the%20scope%20of%20this%20work.%20On%20top%0Aof%20that%2C%20an%20object%20detector%20%28EfficientDet-D0%29%20was%20trained%20for%20automatic%20defect%0Adetection.%20From%20the%20result%20of%20several%20expermients%2C%20peculiar%20natures%20of%20defects%0Ain%20the%20context%20of%20object%20detection%2C%20which%20greatly%20effect%20annotation%20and%0Atraining%20process%2C%20are%20found%20and%20discussed.%20At%20the%20end%2C%20the%20final%20detector%20was%0Aable%20to%20detect%2083%25%20of%20defects%20in%20the%20test%20set%3B%20out%20of%20the%20missing%2017%25%2C%20only%0A0.77%25%20are%20very%20severe%20defects.%20This%20work%20provides%20an%20example%20of%20applying%20deep%0Alearning-based%20object%20detection%20into%20an%20important%20but%20quiet%20engineering%20field.%0AIt%20also%20gives%20some%20practical%20pointers%20on%20how%20to%20annotate%20peculiar%20%22object%22%2C%0Asuch%20as%20defects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06219v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Defect%20Detection%20in%20Sewer%20Network%20Using%20Deep%20Learning%20Based%0A%20%20Object%20Detector&entry.906535625=Bach%20Ha%20and%20Birgit%20Schalter%20and%20Laura%20White%20and%20Joachim%20Koehler&entry.1292438233=%20%20Maintaining%20sewer%20systems%20in%20large%20cities%20is%20important%2C%20but%20also%20time%20and%0Aeffort%20consuming%2C%20because%20visual%20inspections%20are%20currently%20done%20manually.%20To%0Areduce%20the%20amount%20of%20aforementioned%20manual%20work%2C%20defects%20within%20sewer%20pipes%0Ashould%20be%20located%20and%20classified%20automatically.%20In%20the%20past%2C%20multiple%20works%0Ahave%20attempted%20solving%20this%20problem%20using%20classical%20image%20processing%2C%20machine%0Alearning%2C%20or%20a%20combination%20of%20those.%20However%2C%20each%20provided%20solution%20only%20focus%0Aon%20detecting%20a%20limited%20set%20of%20defect/structure%20types%2C%20such%20as%20fissure%2C%20root%2C%0Aand/or%20connection.%20Furthermore%2C%20due%20to%20the%20use%20of%20hand-crafted%20features%20and%0Asmall%20training%20datasets%2C%20generalization%20is%20also%20problematic.%20In%20order%20to%0Aovercome%20these%20deficits%2C%20a%20sizable%20dataset%20with%2014.7%20km%20of%20various%20sewer%20pipes%0Awere%20annotated%20by%20sewer%20maintenance%20experts%20in%20the%20scope%20of%20this%20work.%20On%20top%0Aof%20that%2C%20an%20object%20detector%20%28EfficientDet-D0%29%20was%20trained%20for%20automatic%20defect%0Adetection.%20From%20the%20result%20of%20several%20expermients%2C%20peculiar%20natures%20of%20defects%0Ain%20the%20context%20of%20object%20detection%2C%20which%20greatly%20effect%20annotation%20and%0Atraining%20process%2C%20are%20found%20and%20discussed.%20At%20the%20end%2C%20the%20final%20detector%20was%0Aable%20to%20detect%2083%25%20of%20defects%20in%20the%20test%20set%3B%20out%20of%20the%20missing%2017%25%2C%20only%0A0.77%25%20are%20very%20severe%20defects.%20This%20work%20provides%20an%20example%20of%20applying%20deep%0Alearning-based%20object%20detection%20into%20an%20important%20but%20quiet%20engineering%20field.%0AIt%20also%20gives%20some%20practical%20pointers%20on%20how%20to%20annotate%20peculiar%20%22object%22%2C%0Asuch%20as%20defects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06219v1&entry.124074799=Read"},
{"title": "Improving Entropy-Based Test-Time Adaptation from a Clustering View", "author": "Guoliang Lin and Hanjiang Lai and Yan Pan and Jian Yin", "abstract": "  Domain shift is a common problem in the realistic world, where training data\nand test data follow different data distributions. To deal with this problem,\nfully test-time adaptation (TTA) leverages the unlabeled data encountered\nduring test time to adapt the model. In particular, entropy-based TTA (EBTTA)\nmethods, which minimize the prediction's entropy on test samples, have shown\ngreat success. In this paper, we introduce a new perspective on the EBTTA,\nwhich interprets these methods from a view of clustering. It is an iterative\nalgorithm: 1) in the assignment step, the forward process of the EBTTA models\nis the assignment of labels for these test samples, and 2) in the updating\nstep, the backward process is the update of the model via the assigned samples.\nBased on the interpretation, we can gain a deeper understanding of EBTTA.\nAccordingly, we offer an alternative explanation for why existing EBTTA methods\nare sensitive to initial assignments, nearest neighbor information, outliers,\nand batch size. This observation can guide us to put forward the improvement of\nEBTTA. We propose to use robust label assignment, locality-preserving\nconstraint, sample selection, and gradient accumulation to alleviate the above\nproblems. Experimental results demonstrate that our method can achieve\nconsistent improvements on various datasets. Code is provided in the\nsupplementary material.\n", "link": "http://arxiv.org/abs/2310.20327v5", "date": "2024-04-09", "relevancy": 1.9191, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4944}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4704}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4666}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Entropy-Based%20Test-Time%20Adaptation%20from%20a%20Clustering%20View&body=Title%3A%20Improving%20Entropy-Based%20Test-Time%20Adaptation%20from%20a%20Clustering%20View%0AAuthor%3A%20Guoliang%20Lin%20and%20Hanjiang%20Lai%20and%20Yan%20Pan%20and%20Jian%20Yin%0AAbstract%3A%20%20%20Domain%20shift%20is%20a%20common%20problem%20in%20the%20realistic%20world%2C%20where%20training%20data%0Aand%20test%20data%20follow%20different%20data%20distributions.%20To%20deal%20with%20this%20problem%2C%0Afully%20test-time%20adaptation%20%28TTA%29%20leverages%20the%20unlabeled%20data%20encountered%0Aduring%20test%20time%20to%20adapt%20the%20model.%20In%20particular%2C%20entropy-based%20TTA%20%28EBTTA%29%0Amethods%2C%20which%20minimize%20the%20prediction%27s%20entropy%20on%20test%20samples%2C%20have%20shown%0Agreat%20success.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20perspective%20on%20the%20EBTTA%2C%0Awhich%20interprets%20these%20methods%20from%20a%20view%20of%20clustering.%20It%20is%20an%20iterative%0Aalgorithm%3A%201%29%20in%20the%20assignment%20step%2C%20the%20forward%20process%20of%20the%20EBTTA%20models%0Ais%20the%20assignment%20of%20labels%20for%20these%20test%20samples%2C%20and%202%29%20in%20the%20updating%0Astep%2C%20the%20backward%20process%20is%20the%20update%20of%20the%20model%20via%20the%20assigned%20samples.%0ABased%20on%20the%20interpretation%2C%20we%20can%20gain%20a%20deeper%20understanding%20of%20EBTTA.%0AAccordingly%2C%20we%20offer%20an%20alternative%20explanation%20for%20why%20existing%20EBTTA%20methods%0Aare%20sensitive%20to%20initial%20assignments%2C%20nearest%20neighbor%20information%2C%20outliers%2C%0Aand%20batch%20size.%20This%20observation%20can%20guide%20us%20to%20put%20forward%20the%20improvement%20of%0AEBTTA.%20We%20propose%20to%20use%20robust%20label%20assignment%2C%20locality-preserving%0Aconstraint%2C%20sample%20selection%2C%20and%20gradient%20accumulation%20to%20alleviate%20the%20above%0Aproblems.%20Experimental%20results%20demonstrate%20that%20our%20method%20can%20achieve%0Aconsistent%20improvements%20on%20various%20datasets.%20Code%20is%20provided%20in%20the%0Asupplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20327v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Entropy-Based%20Test-Time%20Adaptation%20from%20a%20Clustering%20View&entry.906535625=Guoliang%20Lin%20and%20Hanjiang%20Lai%20and%20Yan%20Pan%20and%20Jian%20Yin&entry.1292438233=%20%20Domain%20shift%20is%20a%20common%20problem%20in%20the%20realistic%20world%2C%20where%20training%20data%0Aand%20test%20data%20follow%20different%20data%20distributions.%20To%20deal%20with%20this%20problem%2C%0Afully%20test-time%20adaptation%20%28TTA%29%20leverages%20the%20unlabeled%20data%20encountered%0Aduring%20test%20time%20to%20adapt%20the%20model.%20In%20particular%2C%20entropy-based%20TTA%20%28EBTTA%29%0Amethods%2C%20which%20minimize%20the%20prediction%27s%20entropy%20on%20test%20samples%2C%20have%20shown%0Agreat%20success.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20perspective%20on%20the%20EBTTA%2C%0Awhich%20interprets%20these%20methods%20from%20a%20view%20of%20clustering.%20It%20is%20an%20iterative%0Aalgorithm%3A%201%29%20in%20the%20assignment%20step%2C%20the%20forward%20process%20of%20the%20EBTTA%20models%0Ais%20the%20assignment%20of%20labels%20for%20these%20test%20samples%2C%20and%202%29%20in%20the%20updating%0Astep%2C%20the%20backward%20process%20is%20the%20update%20of%20the%20model%20via%20the%20assigned%20samples.%0ABased%20on%20the%20interpretation%2C%20we%20can%20gain%20a%20deeper%20understanding%20of%20EBTTA.%0AAccordingly%2C%20we%20offer%20an%20alternative%20explanation%20for%20why%20existing%20EBTTA%20methods%0Aare%20sensitive%20to%20initial%20assignments%2C%20nearest%20neighbor%20information%2C%20outliers%2C%0Aand%20batch%20size.%20This%20observation%20can%20guide%20us%20to%20put%20forward%20the%20improvement%20of%0AEBTTA.%20We%20propose%20to%20use%20robust%20label%20assignment%2C%20locality-preserving%0Aconstraint%2C%20sample%20selection%2C%20and%20gradient%20accumulation%20to%20alleviate%20the%20above%0Aproblems.%20Experimental%20results%20demonstrate%20that%20our%20method%20can%20achieve%0Aconsistent%20improvements%20on%20various%20datasets.%20Code%20is%20provided%20in%20the%0Asupplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20327v5&entry.124074799=Read"},
{"title": "ExIFFI and EIF+: Interpretability and Enhanced Generalizability to\n  Extend the Extended Isolation Forest", "author": "Alessio Arcudi and Davide Frizzo and Chiara Masiero and Gian Antonio Susto", "abstract": "  Anomaly Detection involves identifying unusual behaviors within complex\ndatasets and systems. While Machine Learning algorithms and Decision Support\nSystems (DSSs) offer effective solutions for this task, simply pinpointing\nanomalies may prove insufficient in real-world applications. Users require\ninsights into the rationale behind these predictions to facilitate root cause\nanalysis and foster trust in the model. However, the unsupervised nature of AD\npresents a challenge in developing interpretable tools. This paper addresses\nthis challenge by introducing ExIFFI, a novel interpretability approach\nspecifically designed to explain the predictions made by Extended Isolation\nForest. ExIFFI leverages feature importance to provide explanations at both\nglobal and local levels. This work also introduces EIF+, an enhanced variant of\nExtended Isolation Forest, conceived to improve its generalization capabilities\nthrough a different splitting hyperplanes design strategy. A comprehensive\ncomparative analysis is conducted, employing both synthetic and real-world\ndatasets to evaluate various unsupervised AD approaches. The analysis\ndemonstrates the effectiveness of ExIFFI in providing explanations for AD\npredictions. Furthermore, the paper explores the utility of ExIFFI as a feature\nselection technique in unsupervised settings. Finally, this work contributes to\nthe research community by providing open-source code, facilitating further\ninvestigation and reproducibility.\n", "link": "http://arxiv.org/abs/2310.05468v2", "date": "2024-04-09", "relevancy": 1.9018, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4875}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.484}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4599}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ExIFFI%20and%20EIF%2B%3A%20Interpretability%20and%20Enhanced%20Generalizability%20to%0A%20%20Extend%20the%20Extended%20Isolation%20Forest&body=Title%3A%20ExIFFI%20and%20EIF%2B%3A%20Interpretability%20and%20Enhanced%20Generalizability%20to%0A%20%20Extend%20the%20Extended%20Isolation%20Forest%0AAuthor%3A%20Alessio%20Arcudi%20and%20Davide%20Frizzo%20and%20Chiara%20Masiero%20and%20Gian%20Antonio%20Susto%0AAbstract%3A%20%20%20Anomaly%20Detection%20involves%20identifying%20unusual%20behaviors%20within%20complex%0Adatasets%20and%20systems.%20While%20Machine%20Learning%20algorithms%20and%20Decision%20Support%0ASystems%20%28DSSs%29%20offer%20effective%20solutions%20for%20this%20task%2C%20simply%20pinpointing%0Aanomalies%20may%20prove%20insufficient%20in%20real-world%20applications.%20Users%20require%0Ainsights%20into%20the%20rationale%20behind%20these%20predictions%20to%20facilitate%20root%20cause%0Aanalysis%20and%20foster%20trust%20in%20the%20model.%20However%2C%20the%20unsupervised%20nature%20of%20AD%0Apresents%20a%20challenge%20in%20developing%20interpretable%20tools.%20This%20paper%20addresses%0Athis%20challenge%20by%20introducing%20ExIFFI%2C%20a%20novel%20interpretability%20approach%0Aspecifically%20designed%20to%20explain%20the%20predictions%20made%20by%20Extended%20Isolation%0AForest.%20ExIFFI%20leverages%20feature%20importance%20to%20provide%20explanations%20at%20both%0Aglobal%20and%20local%20levels.%20This%20work%20also%20introduces%20EIF%2B%2C%20an%20enhanced%20variant%20of%0AExtended%20Isolation%20Forest%2C%20conceived%20to%20improve%20its%20generalization%20capabilities%0Athrough%20a%20different%20splitting%20hyperplanes%20design%20strategy.%20A%20comprehensive%0Acomparative%20analysis%20is%20conducted%2C%20employing%20both%20synthetic%20and%20real-world%0Adatasets%20to%20evaluate%20various%20unsupervised%20AD%20approaches.%20The%20analysis%0Ademonstrates%20the%20effectiveness%20of%20ExIFFI%20in%20providing%20explanations%20for%20AD%0Apredictions.%20Furthermore%2C%20the%20paper%20explores%20the%20utility%20of%20ExIFFI%20as%20a%20feature%0Aselection%20technique%20in%20unsupervised%20settings.%20Finally%2C%20this%20work%20contributes%20to%0Athe%20research%20community%20by%20providing%20open-source%20code%2C%20facilitating%20further%0Ainvestigation%20and%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05468v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExIFFI%20and%20EIF%2B%3A%20Interpretability%20and%20Enhanced%20Generalizability%20to%0A%20%20Extend%20the%20Extended%20Isolation%20Forest&entry.906535625=Alessio%20Arcudi%20and%20Davide%20Frizzo%20and%20Chiara%20Masiero%20and%20Gian%20Antonio%20Susto&entry.1292438233=%20%20Anomaly%20Detection%20involves%20identifying%20unusual%20behaviors%20within%20complex%0Adatasets%20and%20systems.%20While%20Machine%20Learning%20algorithms%20and%20Decision%20Support%0ASystems%20%28DSSs%29%20offer%20effective%20solutions%20for%20this%20task%2C%20simply%20pinpointing%0Aanomalies%20may%20prove%20insufficient%20in%20real-world%20applications.%20Users%20require%0Ainsights%20into%20the%20rationale%20behind%20these%20predictions%20to%20facilitate%20root%20cause%0Aanalysis%20and%20foster%20trust%20in%20the%20model.%20However%2C%20the%20unsupervised%20nature%20of%20AD%0Apresents%20a%20challenge%20in%20developing%20interpretable%20tools.%20This%20paper%20addresses%0Athis%20challenge%20by%20introducing%20ExIFFI%2C%20a%20novel%20interpretability%20approach%0Aspecifically%20designed%20to%20explain%20the%20predictions%20made%20by%20Extended%20Isolation%0AForest.%20ExIFFI%20leverages%20feature%20importance%20to%20provide%20explanations%20at%20both%0Aglobal%20and%20local%20levels.%20This%20work%20also%20introduces%20EIF%2B%2C%20an%20enhanced%20variant%20of%0AExtended%20Isolation%20Forest%2C%20conceived%20to%20improve%20its%20generalization%20capabilities%0Athrough%20a%20different%20splitting%20hyperplanes%20design%20strategy.%20A%20comprehensive%0Acomparative%20analysis%20is%20conducted%2C%20employing%20both%20synthetic%20and%20real-world%0Adatasets%20to%20evaluate%20various%20unsupervised%20AD%20approaches.%20The%20analysis%0Ademonstrates%20the%20effectiveness%20of%20ExIFFI%20in%20providing%20explanations%20for%20AD%0Apredictions.%20Furthermore%2C%20the%20paper%20explores%20the%20utility%20of%20ExIFFI%20as%20a%20feature%0Aselection%20technique%20in%20unsupervised%20settings.%20Finally%2C%20this%20work%20contributes%20to%0Athe%20research%20community%20by%20providing%20open-source%20code%2C%20facilitating%20further%0Ainvestigation%20and%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05468v2&entry.124074799=Read"},
{"title": "Robust agents learn causal world models", "author": "Jonathan Richens and Tom Everitt", "abstract": "  It has long been hypothesised that causal reasoning plays a fundamental role\nin robust and general intelligence. However, it is not known if agents must\nlearn causal models in order to generalise to new domains, or if other\ninductive biases are sufficient. We answer this question, showing that any\nagent capable of satisfying a regret bound under a large set of distributional\nshifts must have learned an approximate causal model of the data generating\nprocess, which converges to the true causal model for optimal agents. We\ndiscuss the implications of this result for several research areas including\ntransfer learning and causal inference.\n", "link": "http://arxiv.org/abs/2402.10877v5", "date": "2024-04-09", "relevancy": 1.9017, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4991}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4872}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4541}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20agents%20learn%20causal%20world%20models&body=Title%3A%20Robust%20agents%20learn%20causal%20world%20models%0AAuthor%3A%20Jonathan%20Richens%20and%20Tom%20Everitt%0AAbstract%3A%20%20%20It%20has%20long%20been%20hypothesised%20that%20causal%20reasoning%20plays%20a%20fundamental%20role%0Ain%20robust%20and%20general%20intelligence.%20However%2C%20it%20is%20not%20known%20if%20agents%20must%0Alearn%20causal%20models%20in%20order%20to%20generalise%20to%20new%20domains%2C%20or%20if%20other%0Ainductive%20biases%20are%20sufficient.%20We%20answer%20this%20question%2C%20showing%20that%20any%0Aagent%20capable%20of%20satisfying%20a%20regret%20bound%20under%20a%20large%20set%20of%20distributional%0Ashifts%20must%20have%20learned%20an%20approximate%20causal%20model%20of%20the%20data%20generating%0Aprocess%2C%20which%20converges%20to%20the%20true%20causal%20model%20for%20optimal%20agents.%20We%0Adiscuss%20the%20implications%20of%20this%20result%20for%20several%20research%20areas%20including%0Atransfer%20learning%20and%20causal%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10877v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20agents%20learn%20causal%20world%20models&entry.906535625=Jonathan%20Richens%20and%20Tom%20Everitt&entry.1292438233=%20%20It%20has%20long%20been%20hypothesised%20that%20causal%20reasoning%20plays%20a%20fundamental%20role%0Ain%20robust%20and%20general%20intelligence.%20However%2C%20it%20is%20not%20known%20if%20agents%20must%0Alearn%20causal%20models%20in%20order%20to%20generalise%20to%20new%20domains%2C%20or%20if%20other%0Ainductive%20biases%20are%20sufficient.%20We%20answer%20this%20question%2C%20showing%20that%20any%0Aagent%20capable%20of%20satisfying%20a%20regret%20bound%20under%20a%20large%20set%20of%20distributional%0Ashifts%20must%20have%20learned%20an%20approximate%20causal%20model%20of%20the%20data%20generating%0Aprocess%2C%20which%20converges%20to%20the%20true%20causal%20model%20for%20optimal%20agents.%20We%0Adiscuss%20the%20implications%20of%20this%20result%20for%20several%20research%20areas%20including%0Atransfer%20learning%20and%20causal%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10877v5&entry.124074799=Read"},
{"title": "Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks", "author": "Dimitrios Michail and Lefki-Ioanna Panagiotou and Charalampos Davalas and Ioannis Prapas and Spyros Kondylatos and Nikolaos Ioannis Bountos and Ioannis Papoutsis", "abstract": "  With climate change expected to exacerbate fire weather conditions, the\naccurate anticipation of wildfires on a global scale becomes increasingly\ncrucial for disaster mitigation. In this study, we utilize SeasFire, a\ncomprehensive global wildfire dataset with climate, vegetation, oceanic\nindices, and human-related variables, to enable seasonal wildfire forecasting\nwith machine learning. For the predictive analysis, we train deep learning\nmodels with different architectures that capture the spatio-temporal context\nleading to wildfires. Our investigation focuses on assessing the effectiveness\nof these models in predicting the presence of burned areas at varying\nforecasting time horizons globally, extending up to six months into the future,\nand on how different spatial or/and temporal context affects the performance of\nthe models. Our findings demonstrate the great potential of deep learning\nmodels in seasonal fire forecasting; longer input time-series leads to more\nrobust predictions across varying forecasting horizons, while integrating\nspatial information to capture wildfire spatio-temporal dynamics boosts\nperformance. Finally, our results hint that in order to enhance performance at\nlonger forecasting horizons, a larger receptive field spatially needs to be\nconsidered.\n", "link": "http://arxiv.org/abs/2404.06437v1", "date": "2024-04-09", "relevancy": 1.8975, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4789}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4757}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4712}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Seasonal%20Fire%20Prediction%20using%20Spatio-Temporal%20Deep%20Neural%20Networks&body=Title%3A%20Seasonal%20Fire%20Prediction%20using%20Spatio-Temporal%20Deep%20Neural%20Networks%0AAuthor%3A%20Dimitrios%20Michail%20and%20Lefki-Ioanna%20Panagiotou%20and%20Charalampos%20Davalas%20and%20Ioannis%20Prapas%20and%20Spyros%20Kondylatos%20and%20Nikolaos%20Ioannis%20Bountos%20and%20Ioannis%20Papoutsis%0AAbstract%3A%20%20%20With%20climate%20change%20expected%20to%20exacerbate%20fire%20weather%20conditions%2C%20the%0Aaccurate%20anticipation%20of%20wildfires%20on%20a%20global%20scale%20becomes%20increasingly%0Acrucial%20for%20disaster%20mitigation.%20In%20this%20study%2C%20we%20utilize%20SeasFire%2C%20a%0Acomprehensive%20global%20wildfire%20dataset%20with%20climate%2C%20vegetation%2C%20oceanic%0Aindices%2C%20and%20human-related%20variables%2C%20to%20enable%20seasonal%20wildfire%20forecasting%0Awith%20machine%20learning.%20For%20the%20predictive%20analysis%2C%20we%20train%20deep%20learning%0Amodels%20with%20different%20architectures%20that%20capture%20the%20spatio-temporal%20context%0Aleading%20to%20wildfires.%20Our%20investigation%20focuses%20on%20assessing%20the%20effectiveness%0Aof%20these%20models%20in%20predicting%20the%20presence%20of%20burned%20areas%20at%20varying%0Aforecasting%20time%20horizons%20globally%2C%20extending%20up%20to%20six%20months%20into%20the%20future%2C%0Aand%20on%20how%20different%20spatial%20or/and%20temporal%20context%20affects%20the%20performance%20of%0Athe%20models.%20Our%20findings%20demonstrate%20the%20great%20potential%20of%20deep%20learning%0Amodels%20in%20seasonal%20fire%20forecasting%3B%20longer%20input%20time-series%20leads%20to%20more%0Arobust%20predictions%20across%20varying%20forecasting%20horizons%2C%20while%20integrating%0Aspatial%20information%20to%20capture%20wildfire%20spatio-temporal%20dynamics%20boosts%0Aperformance.%20Finally%2C%20our%20results%20hint%20that%20in%20order%20to%20enhance%20performance%20at%0Alonger%20forecasting%20horizons%2C%20a%20larger%20receptive%20field%20spatially%20needs%20to%20be%0Aconsidered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06437v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seasonal%20Fire%20Prediction%20using%20Spatio-Temporal%20Deep%20Neural%20Networks&entry.906535625=Dimitrios%20Michail%20and%20Lefki-Ioanna%20Panagiotou%20and%20Charalampos%20Davalas%20and%20Ioannis%20Prapas%20and%20Spyros%20Kondylatos%20and%20Nikolaos%20Ioannis%20Bountos%20and%20Ioannis%20Papoutsis&entry.1292438233=%20%20With%20climate%20change%20expected%20to%20exacerbate%20fire%20weather%20conditions%2C%20the%0Aaccurate%20anticipation%20of%20wildfires%20on%20a%20global%20scale%20becomes%20increasingly%0Acrucial%20for%20disaster%20mitigation.%20In%20this%20study%2C%20we%20utilize%20SeasFire%2C%20a%0Acomprehensive%20global%20wildfire%20dataset%20with%20climate%2C%20vegetation%2C%20oceanic%0Aindices%2C%20and%20human-related%20variables%2C%20to%20enable%20seasonal%20wildfire%20forecasting%0Awith%20machine%20learning.%20For%20the%20predictive%20analysis%2C%20we%20train%20deep%20learning%0Amodels%20with%20different%20architectures%20that%20capture%20the%20spatio-temporal%20context%0Aleading%20to%20wildfires.%20Our%20investigation%20focuses%20on%20assessing%20the%20effectiveness%0Aof%20these%20models%20in%20predicting%20the%20presence%20of%20burned%20areas%20at%20varying%0Aforecasting%20time%20horizons%20globally%2C%20extending%20up%20to%20six%20months%20into%20the%20future%2C%0Aand%20on%20how%20different%20spatial%20or/and%20temporal%20context%20affects%20the%20performance%20of%0Athe%20models.%20Our%20findings%20demonstrate%20the%20great%20potential%20of%20deep%20learning%0Amodels%20in%20seasonal%20fire%20forecasting%3B%20longer%20input%20time-series%20leads%20to%20more%0Arobust%20predictions%20across%20varying%20forecasting%20horizons%2C%20while%20integrating%0Aspatial%20information%20to%20capture%20wildfire%20spatio-temporal%20dynamics%20boosts%0Aperformance.%20Finally%2C%20our%20results%20hint%20that%20in%20order%20to%20enhance%20performance%20at%0Alonger%20forecasting%20horizons%2C%20a%20larger%20receptive%20field%20spatially%20needs%20to%20be%0Aconsidered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06437v1&entry.124074799=Read"},
{"title": "DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion\n  Models", "author": "Zhenting Wang and Chen Chen and Lingjuan Lyu and Dimitris N. Metaxas and Shiqing Ma", "abstract": "  Recent text-to-image diffusion models have shown surprising performance in\ngenerating high-quality images. However, concerns have arisen regarding the\nunauthorized data usage during the training or fine-tuning process. One example\nis when a model trainer collects a set of images created by a particular artist\nand attempts to train a model capable of generating similar images without\nobtaining permission and giving credit to the artist. To address this issue, we\npropose a method for detecting such unauthorized data usage by planting the\ninjected memorization into the text-to-image diffusion models trained on the\nprotected dataset. Specifically, we modify the protected images by adding\nunique contents on these images using stealthy image warping functions that are\nnearly imperceptible to humans but can be captured and memorized by diffusion\nmodels. By analyzing whether the model has memorized the injected content\n(i.e., whether the generated images are processed by the injected\npost-processing function), we can detect models that had illegally utilized the\nunauthorized data. Experiments on Stable Diffusion and VQ Diffusion with\ndifferent model training or fine-tuning methods (i.e, LoRA, DreamBooth, and\nstandard training) demonstrate the effectiveness of our proposed method in\ndetecting unauthorized data usages. Code:\nhttps://github.com/ZhentingWang/DIAGNOSIS.\n", "link": "http://arxiv.org/abs/2307.03108v3", "date": "2024-04-09", "relevancy": 1.8958, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6461}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6361}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6246}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DIAGNOSIS%3A%20Detecting%20Unauthorized%20Data%20Usages%20in%20Text-to-image%20Diffusion%0A%20%20Models&body=Title%3A%20DIAGNOSIS%3A%20Detecting%20Unauthorized%20Data%20Usages%20in%20Text-to-image%20Diffusion%0A%20%20Models%0AAuthor%3A%20Zhenting%20Wang%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu%20and%20Dimitris%20N.%20Metaxas%20and%20Shiqing%20Ma%0AAbstract%3A%20%20%20Recent%20text-to-image%20diffusion%20models%20have%20shown%20surprising%20performance%20in%0Agenerating%20high-quality%20images.%20However%2C%20concerns%20have%20arisen%20regarding%20the%0Aunauthorized%20data%20usage%20during%20the%20training%20or%20fine-tuning%20process.%20One%20example%0Ais%20when%20a%20model%20trainer%20collects%20a%20set%20of%20images%20created%20by%20a%20particular%20artist%0Aand%20attempts%20to%20train%20a%20model%20capable%20of%20generating%20similar%20images%20without%0Aobtaining%20permission%20and%20giving%20credit%20to%20the%20artist.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20method%20for%20detecting%20such%20unauthorized%20data%20usage%20by%20planting%20the%0Ainjected%20memorization%20into%20the%20text-to-image%20diffusion%20models%20trained%20on%20the%0Aprotected%20dataset.%20Specifically%2C%20we%20modify%20the%20protected%20images%20by%20adding%0Aunique%20contents%20on%20these%20images%20using%20stealthy%20image%20warping%20functions%20that%20are%0Anearly%20imperceptible%20to%20humans%20but%20can%20be%20captured%20and%20memorized%20by%20diffusion%0Amodels.%20By%20analyzing%20whether%20the%20model%20has%20memorized%20the%20injected%20content%0A%28i.e.%2C%20whether%20the%20generated%20images%20are%20processed%20by%20the%20injected%0Apost-processing%20function%29%2C%20we%20can%20detect%20models%20that%20had%20illegally%20utilized%20the%0Aunauthorized%20data.%20Experiments%20on%20Stable%20Diffusion%20and%20VQ%20Diffusion%20with%0Adifferent%20model%20training%20or%20fine-tuning%20methods%20%28i.e%2C%20LoRA%2C%20DreamBooth%2C%20and%0Astandard%20training%29%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20in%0Adetecting%20unauthorized%20data%20usages.%20Code%3A%0Ahttps%3A//github.com/ZhentingWang/DIAGNOSIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.03108v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIAGNOSIS%3A%20Detecting%20Unauthorized%20Data%20Usages%20in%20Text-to-image%20Diffusion%0A%20%20Models&entry.906535625=Zhenting%20Wang%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu%20and%20Dimitris%20N.%20Metaxas%20and%20Shiqing%20Ma&entry.1292438233=%20%20Recent%20text-to-image%20diffusion%20models%20have%20shown%20surprising%20performance%20in%0Agenerating%20high-quality%20images.%20However%2C%20concerns%20have%20arisen%20regarding%20the%0Aunauthorized%20data%20usage%20during%20the%20training%20or%20fine-tuning%20process.%20One%20example%0Ais%20when%20a%20model%20trainer%20collects%20a%20set%20of%20images%20created%20by%20a%20particular%20artist%0Aand%20attempts%20to%20train%20a%20model%20capable%20of%20generating%20similar%20images%20without%0Aobtaining%20permission%20and%20giving%20credit%20to%20the%20artist.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20method%20for%20detecting%20such%20unauthorized%20data%20usage%20by%20planting%20the%0Ainjected%20memorization%20into%20the%20text-to-image%20diffusion%20models%20trained%20on%20the%0Aprotected%20dataset.%20Specifically%2C%20we%20modify%20the%20protected%20images%20by%20adding%0Aunique%20contents%20on%20these%20images%20using%20stealthy%20image%20warping%20functions%20that%20are%0Anearly%20imperceptible%20to%20humans%20but%20can%20be%20captured%20and%20memorized%20by%20diffusion%0Amodels.%20By%20analyzing%20whether%20the%20model%20has%20memorized%20the%20injected%20content%0A%28i.e.%2C%20whether%20the%20generated%20images%20are%20processed%20by%20the%20injected%0Apost-processing%20function%29%2C%20we%20can%20detect%20models%20that%20had%20illegally%20utilized%20the%0Aunauthorized%20data.%20Experiments%20on%20Stable%20Diffusion%20and%20VQ%20Diffusion%20with%0Adifferent%20model%20training%20or%20fine-tuning%20methods%20%28i.e%2C%20LoRA%2C%20DreamBooth%2C%20and%0Astandard%20training%29%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20in%0Adetecting%20unauthorized%20data%20usages.%20Code%3A%0Ahttps%3A//github.com/ZhentingWang/DIAGNOSIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.03108v3&entry.124074799=Read"},
{"title": "Large Language Models on Fine-grained Emotion Detection Dataset with\n  Data Augmentation and Transfer Learning", "author": "Kaipeng Wang and Zhi Jing and Yongye Su and Yikun Han", "abstract": "  This paper delves into enhancing the classification performance on the\nGoEmotions dataset, a large, manually annotated dataset for emotion detection\nin text. The primary goal of this paper is to address the challenges of\ndetecting subtle emotions in text, a complex issue in Natural Language\nProcessing (NLP) with significant practical applications. The findings offer\nvaluable insights into addressing the challenges of emotion detection in text\nand suggest directions for future research, including the potential for a\nsurvey paper that synthesizes methods and performances across various datasets\nin this domain.\n", "link": "http://arxiv.org/abs/2403.06108v2", "date": "2024-04-09", "relevancy": 1.8922, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4833}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4745}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4622}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20on%20Fine-grained%20Emotion%20Detection%20Dataset%20with%0A%20%20Data%20Augmentation%20and%20Transfer%20Learning&body=Title%3A%20Large%20Language%20Models%20on%20Fine-grained%20Emotion%20Detection%20Dataset%20with%0A%20%20Data%20Augmentation%20and%20Transfer%20Learning%0AAuthor%3A%20Kaipeng%20Wang%20and%20Zhi%20Jing%20and%20Yongye%20Su%20and%20Yikun%20Han%0AAbstract%3A%20%20%20This%20paper%20delves%20into%20enhancing%20the%20classification%20performance%20on%20the%0AGoEmotions%20dataset%2C%20a%20large%2C%20manually%20annotated%20dataset%20for%20emotion%20detection%0Ain%20text.%20The%20primary%20goal%20of%20this%20paper%20is%20to%20address%20the%20challenges%20of%0Adetecting%20subtle%20emotions%20in%20text%2C%20a%20complex%20issue%20in%20Natural%20Language%0AProcessing%20%28NLP%29%20with%20significant%20practical%20applications.%20The%20findings%20offer%0Avaluable%20insights%20into%20addressing%20the%20challenges%20of%20emotion%20detection%20in%20text%0Aand%20suggest%20directions%20for%20future%20research%2C%20including%20the%20potential%20for%20a%0Asurvey%20paper%20that%20synthesizes%20methods%20and%20performances%20across%20various%20datasets%0Ain%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06108v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20on%20Fine-grained%20Emotion%20Detection%20Dataset%20with%0A%20%20Data%20Augmentation%20and%20Transfer%20Learning&entry.906535625=Kaipeng%20Wang%20and%20Zhi%20Jing%20and%20Yongye%20Su%20and%20Yikun%20Han&entry.1292438233=%20%20This%20paper%20delves%20into%20enhancing%20the%20classification%20performance%20on%20the%0AGoEmotions%20dataset%2C%20a%20large%2C%20manually%20annotated%20dataset%20for%20emotion%20detection%0Ain%20text.%20The%20primary%20goal%20of%20this%20paper%20is%20to%20address%20the%20challenges%20of%0Adetecting%20subtle%20emotions%20in%20text%2C%20a%20complex%20issue%20in%20Natural%20Language%0AProcessing%20%28NLP%29%20with%20significant%20practical%20applications.%20The%20findings%20offer%0Avaluable%20insights%20into%20addressing%20the%20challenges%20of%20emotion%20detection%20in%20text%0Aand%20suggest%20directions%20for%20future%20research%2C%20including%20the%20potential%20for%20a%0Asurvey%20paper%20that%20synthesizes%20methods%20and%20performances%20across%20various%20datasets%0Ain%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06108v2&entry.124074799=Read"},
{"title": "GeoDirDock: Guiding Docking Along Geodesic Paths", "author": "Ra\u00fal Mi\u00f1\u00e1n and Javier Gallardo and \u00c1lvaro Ciudad and Alexis Molina", "abstract": "  This work introduces GeoDirDock (GDD), a novel approach to molecular docking\nthat enhances the accuracy and physical plausibility of ligand docking\npredictions. GDD guides the denoising process of a diffusion model along\ngeodesic paths within multiple spaces representing translational, rotational,\nand torsional degrees of freedom. Our method leverages expert knowledge to\ndirect the generative modeling process, specifically targeting desired\nprotein-ligand interaction regions. We demonstrate that GDD significantly\noutperforms existing blind docking methods in terms of RMSD accuracy and\nphysicochemical pose realism. Our results indicate that incorporating domain\nexpertise into the diffusion process leads to more biologically relevant\ndocking predictions. Additionally, we explore the potential of GDD for lead\noptimization in drug discovery through angle transfer in maximal common\nsubstructure (MCS) docking, showcasing its capability to predict ligand\norientations for chemically similar compounds accurately.\n", "link": "http://arxiv.org/abs/2404.06481v1", "date": "2024-04-09", "relevancy": 1.8737, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4816}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4699}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4318}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GeoDirDock%3A%20Guiding%20Docking%20Along%20Geodesic%20Paths&body=Title%3A%20GeoDirDock%3A%20Guiding%20Docking%20Along%20Geodesic%20Paths%0AAuthor%3A%20Ra%C3%BAl%20Mi%C3%B1%C3%A1n%20and%20Javier%20Gallardo%20and%20%C3%81lvaro%20Ciudad%20and%20Alexis%20Molina%0AAbstract%3A%20%20%20This%20work%20introduces%20GeoDirDock%20%28GDD%29%2C%20a%20novel%20approach%20to%20molecular%20docking%0Athat%20enhances%20the%20accuracy%20and%20physical%20plausibility%20of%20ligand%20docking%0Apredictions.%20GDD%20guides%20the%20denoising%20process%20of%20a%20diffusion%20model%20along%0Ageodesic%20paths%20within%20multiple%20spaces%20representing%20translational%2C%20rotational%2C%0Aand%20torsional%20degrees%20of%20freedom.%20Our%20method%20leverages%20expert%20knowledge%20to%0Adirect%20the%20generative%20modeling%20process%2C%20specifically%20targeting%20desired%0Aprotein-ligand%20interaction%20regions.%20We%20demonstrate%20that%20GDD%20significantly%0Aoutperforms%20existing%20blind%20docking%20methods%20in%20terms%20of%20RMSD%20accuracy%20and%0Aphysicochemical%20pose%20realism.%20Our%20results%20indicate%20that%20incorporating%20domain%0Aexpertise%20into%20the%20diffusion%20process%20leads%20to%20more%20biologically%20relevant%0Adocking%20predictions.%20Additionally%2C%20we%20explore%20the%20potential%20of%20GDD%20for%20lead%0Aoptimization%20in%20drug%20discovery%20through%20angle%20transfer%20in%20maximal%20common%0Asubstructure%20%28MCS%29%20docking%2C%20showcasing%20its%20capability%20to%20predict%20ligand%0Aorientations%20for%20chemically%20similar%20compounds%20accurately.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06481v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoDirDock%3A%20Guiding%20Docking%20Along%20Geodesic%20Paths&entry.906535625=Ra%C3%BAl%20Mi%C3%B1%C3%A1n%20and%20Javier%20Gallardo%20and%20%C3%81lvaro%20Ciudad%20and%20Alexis%20Molina&entry.1292438233=%20%20This%20work%20introduces%20GeoDirDock%20%28GDD%29%2C%20a%20novel%20approach%20to%20molecular%20docking%0Athat%20enhances%20the%20accuracy%20and%20physical%20plausibility%20of%20ligand%20docking%0Apredictions.%20GDD%20guides%20the%20denoising%20process%20of%20a%20diffusion%20model%20along%0Ageodesic%20paths%20within%20multiple%20spaces%20representing%20translational%2C%20rotational%2C%0Aand%20torsional%20degrees%20of%20freedom.%20Our%20method%20leverages%20expert%20knowledge%20to%0Adirect%20the%20generative%20modeling%20process%2C%20specifically%20targeting%20desired%0Aprotein-ligand%20interaction%20regions.%20We%20demonstrate%20that%20GDD%20significantly%0Aoutperforms%20existing%20blind%20docking%20methods%20in%20terms%20of%20RMSD%20accuracy%20and%0Aphysicochemical%20pose%20realism.%20Our%20results%20indicate%20that%20incorporating%20domain%0Aexpertise%20into%20the%20diffusion%20process%20leads%20to%20more%20biologically%20relevant%0Adocking%20predictions.%20Additionally%2C%20we%20explore%20the%20potential%20of%20GDD%20for%20lead%0Aoptimization%20in%20drug%20discovery%20through%20angle%20transfer%20in%20maximal%20common%0Asubstructure%20%28MCS%29%20docking%2C%20showcasing%20its%20capability%20to%20predict%20ligand%0Aorientations%20for%20chemically%20similar%20compounds%20accurately.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06481v1&entry.124074799=Read"},
{"title": "Unified Physical-Digital Attack Detection Challenge", "author": "Haocheng Yuan and Ajian Liu and Junze Zheng and Jun Wan and Jiankang Deng and Sergio Escalera and Hugo Jair Escalante and Isabelle Guyon and Zhen Lei", "abstract": "  Face Anti-Spoofing (FAS) is crucial to safeguard Face Recognition (FR)\nSystems. In real-world scenarios, FRs are confronted with both physical and\ndigital attacks. However, existing algorithms often address only one type of\nattack at a time, which poses significant limitations in real-world scenarios\nwhere FR systems face hybrid physical-digital threats. To facilitate the\nresearch of Unified Attack Detection (UAD) algorithms, a large-scale\nUniAttackData dataset has been collected. UniAttackData is the largest public\ndataset for Unified Attack Detection, with a total of 28,706 videos, where each\nunique identity encompasses all advanced attack types. Based on this dataset,\nwe organized a Unified Physical-Digital Face Attack Detection Challenge to\nboost the research in Unified Attack Detections. It attracted 136 teams for the\ndevelopment phase, with 13 qualifying for the final round. The results\nre-verified by the organizing team were used for the final ranking. This paper\ncomprehensively reviews the challenge, detailing the dataset introduction,\nprotocol definition, evaluation criteria, and a summary of published results.\nFinally, we focus on the detailed analysis of the highest-performing algorithms\nand offer potential directions for unified physical-digital attack detection\ninspired by this competition. Challenge Website:\nhttps://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024.\n", "link": "http://arxiv.org/abs/2404.06211v1", "date": "2024-04-09", "relevancy": 1.8687, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4796}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.473}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4563}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unified%20Physical-Digital%20Attack%20Detection%20Challenge&body=Title%3A%20Unified%20Physical-Digital%20Attack%20Detection%20Challenge%0AAuthor%3A%20Haocheng%20Yuan%20and%20Ajian%20Liu%20and%20Junze%20Zheng%20and%20Jun%20Wan%20and%20Jiankang%20Deng%20and%20Sergio%20Escalera%20and%20Hugo%20Jair%20Escalante%20and%20Isabelle%20Guyon%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20Face%20Anti-Spoofing%20%28FAS%29%20is%20crucial%20to%20safeguard%20Face%20Recognition%20%28FR%29%0ASystems.%20In%20real-world%20scenarios%2C%20FRs%20are%20confronted%20with%20both%20physical%20and%0Adigital%20attacks.%20However%2C%20existing%20algorithms%20often%20address%20only%20one%20type%20of%0Aattack%20at%20a%20time%2C%20which%20poses%20significant%20limitations%20in%20real-world%20scenarios%0Awhere%20FR%20systems%20face%20hybrid%20physical-digital%20threats.%20To%20facilitate%20the%0Aresearch%20of%20Unified%20Attack%20Detection%20%28UAD%29%20algorithms%2C%20a%20large-scale%0AUniAttackData%20dataset%20has%20been%20collected.%20UniAttackData%20is%20the%20largest%20public%0Adataset%20for%20Unified%20Attack%20Detection%2C%20with%20a%20total%20of%2028%2C706%20videos%2C%20where%20each%0Aunique%20identity%20encompasses%20all%20advanced%20attack%20types.%20Based%20on%20this%20dataset%2C%0Awe%20organized%20a%20Unified%20Physical-Digital%20Face%20Attack%20Detection%20Challenge%20to%0Aboost%20the%20research%20in%20Unified%20Attack%20Detections.%20It%20attracted%20136%20teams%20for%20the%0Adevelopment%20phase%2C%20with%2013%20qualifying%20for%20the%20final%20round.%20The%20results%0Are-verified%20by%20the%20organizing%20team%20were%20used%20for%20the%20final%20ranking.%20This%20paper%0Acomprehensively%20reviews%20the%20challenge%2C%20detailing%20the%20dataset%20introduction%2C%0Aprotocol%20definition%2C%20evaluation%20criteria%2C%20and%20a%20summary%20of%20published%20results.%0AFinally%2C%20we%20focus%20on%20the%20detailed%20analysis%20of%20the%20highest-performing%20algorithms%0Aand%20offer%20potential%20directions%20for%20unified%20physical-digital%20attack%20detection%0Ainspired%20by%20this%20competition.%20Challenge%20Website%3A%0Ahttps%3A//sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06211v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Physical-Digital%20Attack%20Detection%20Challenge&entry.906535625=Haocheng%20Yuan%20and%20Ajian%20Liu%20and%20Junze%20Zheng%20and%20Jun%20Wan%20and%20Jiankang%20Deng%20and%20Sergio%20Escalera%20and%20Hugo%20Jair%20Escalante%20and%20Isabelle%20Guyon%20and%20Zhen%20Lei&entry.1292438233=%20%20Face%20Anti-Spoofing%20%28FAS%29%20is%20crucial%20to%20safeguard%20Face%20Recognition%20%28FR%29%0ASystems.%20In%20real-world%20scenarios%2C%20FRs%20are%20confronted%20with%20both%20physical%20and%0Adigital%20attacks.%20However%2C%20existing%20algorithms%20often%20address%20only%20one%20type%20of%0Aattack%20at%20a%20time%2C%20which%20poses%20significant%20limitations%20in%20real-world%20scenarios%0Awhere%20FR%20systems%20face%20hybrid%20physical-digital%20threats.%20To%20facilitate%20the%0Aresearch%20of%20Unified%20Attack%20Detection%20%28UAD%29%20algorithms%2C%20a%20large-scale%0AUniAttackData%20dataset%20has%20been%20collected.%20UniAttackData%20is%20the%20largest%20public%0Adataset%20for%20Unified%20Attack%20Detection%2C%20with%20a%20total%20of%2028%2C706%20videos%2C%20where%20each%0Aunique%20identity%20encompasses%20all%20advanced%20attack%20types.%20Based%20on%20this%20dataset%2C%0Awe%20organized%20a%20Unified%20Physical-Digital%20Face%20Attack%20Detection%20Challenge%20to%0Aboost%20the%20research%20in%20Unified%20Attack%20Detections.%20It%20attracted%20136%20teams%20for%20the%0Adevelopment%20phase%2C%20with%2013%20qualifying%20for%20the%20final%20round.%20The%20results%0Are-verified%20by%20the%20organizing%20team%20were%20used%20for%20the%20final%20ranking.%20This%20paper%0Acomprehensively%20reviews%20the%20challenge%2C%20detailing%20the%20dataset%20introduction%2C%0Aprotocol%20definition%2C%20evaluation%20criteria%2C%20and%20a%20summary%20of%20published%20results.%0AFinally%2C%20we%20focus%20on%20the%20detailed%20analysis%20of%20the%20highest-performing%20algorithms%0Aand%20offer%20potential%20directions%20for%20unified%20physical-digital%20attack%20detection%0Ainspired%20by%20this%20competition.%20Challenge%20Website%3A%0Ahttps%3A//sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06211v1&entry.124074799=Read"},
{"title": "Dimensionality Reduction in Sentence Transformer Vector Databases with\n  Fast Fourier Transform", "author": "Vitaly Bulgakov and Alec Segal", "abstract": "  Dimensionality reduction in vector databases is pivotal for streamlining AI\ndata management, enabling efficient storage, faster computation, and improved\nmodel performance. This paper explores the benefits of reducing vector database\ndimensions, with a focus on computational efficiency and overcoming the curse\nof dimensionality. We introduce a novel application of Fast Fourier Transform\n(FFT) to dimensionality reduction, a method previously underexploited in this\ncontext. By demonstrating its utility across various AI domains, including\nRetrieval-Augmented Generation (RAG) models and image processing, this\nFFT-based approach promises to improve data retrieval processes and enhance the\nefficiency and scalability of AI solutions. The incorporation of FFT may not\nonly optimize operations in real-time processing and recommendation systems but\nalso extend to advanced image processing techniques, where dimensionality\nreduction can significantly improve performance and analysis efficiency. This\npaper advocates for the broader adoption of FFT in vector database management,\nmarking a significant stride towards addressing the challenges of data volume\nand complexity in AI research and applications. Unlike many existing\napproaches, we directly handle the embedding vectors produced by the model\nafter processing a test input.\n", "link": "http://arxiv.org/abs/2404.06278v1", "date": "2024-04-09", "relevancy": 1.8657, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5017}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4817}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dimensionality%20Reduction%20in%20Sentence%20Transformer%20Vector%20Databases%20with%0A%20%20Fast%20Fourier%20Transform&body=Title%3A%20Dimensionality%20Reduction%20in%20Sentence%20Transformer%20Vector%20Databases%20with%0A%20%20Fast%20Fourier%20Transform%0AAuthor%3A%20Vitaly%20Bulgakov%20and%20Alec%20Segal%0AAbstract%3A%20%20%20Dimensionality%20reduction%20in%20vector%20databases%20is%20pivotal%20for%20streamlining%20AI%0Adata%20management%2C%20enabling%20efficient%20storage%2C%20faster%20computation%2C%20and%20improved%0Amodel%20performance.%20This%20paper%20explores%20the%20benefits%20of%20reducing%20vector%20database%0Adimensions%2C%20with%20a%20focus%20on%20computational%20efficiency%20and%20overcoming%20the%20curse%0Aof%20dimensionality.%20We%20introduce%20a%20novel%20application%20of%20Fast%20Fourier%20Transform%0A%28FFT%29%20to%20dimensionality%20reduction%2C%20a%20method%20previously%20underexploited%20in%20this%0Acontext.%20By%20demonstrating%20its%20utility%20across%20various%20AI%20domains%2C%20including%0ARetrieval-Augmented%20Generation%20%28RAG%29%20models%20and%20image%20processing%2C%20this%0AFFT-based%20approach%20promises%20to%20improve%20data%20retrieval%20processes%20and%20enhance%20the%0Aefficiency%20and%20scalability%20of%20AI%20solutions.%20The%20incorporation%20of%20FFT%20may%20not%0Aonly%20optimize%20operations%20in%20real-time%20processing%20and%20recommendation%20systems%20but%0Aalso%20extend%20to%20advanced%20image%20processing%20techniques%2C%20where%20dimensionality%0Areduction%20can%20significantly%20improve%20performance%20and%20analysis%20efficiency.%20This%0Apaper%20advocates%20for%20the%20broader%20adoption%20of%20FFT%20in%20vector%20database%20management%2C%0Amarking%20a%20significant%20stride%20towards%20addressing%20the%20challenges%20of%20data%20volume%0Aand%20complexity%20in%20AI%20research%20and%20applications.%20Unlike%20many%20existing%0Aapproaches%2C%20we%20directly%20handle%20the%20embedding%20vectors%20produced%20by%20the%20model%0Aafter%20processing%20a%20test%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06278v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimensionality%20Reduction%20in%20Sentence%20Transformer%20Vector%20Databases%20with%0A%20%20Fast%20Fourier%20Transform&entry.906535625=Vitaly%20Bulgakov%20and%20Alec%20Segal&entry.1292438233=%20%20Dimensionality%20reduction%20in%20vector%20databases%20is%20pivotal%20for%20streamlining%20AI%0Adata%20management%2C%20enabling%20efficient%20storage%2C%20faster%20computation%2C%20and%20improved%0Amodel%20performance.%20This%20paper%20explores%20the%20benefits%20of%20reducing%20vector%20database%0Adimensions%2C%20with%20a%20focus%20on%20computational%20efficiency%20and%20overcoming%20the%20curse%0Aof%20dimensionality.%20We%20introduce%20a%20novel%20application%20of%20Fast%20Fourier%20Transform%0A%28FFT%29%20to%20dimensionality%20reduction%2C%20a%20method%20previously%20underexploited%20in%20this%0Acontext.%20By%20demonstrating%20its%20utility%20across%20various%20AI%20domains%2C%20including%0ARetrieval-Augmented%20Generation%20%28RAG%29%20models%20and%20image%20processing%2C%20this%0AFFT-based%20approach%20promises%20to%20improve%20data%20retrieval%20processes%20and%20enhance%20the%0Aefficiency%20and%20scalability%20of%20AI%20solutions.%20The%20incorporation%20of%20FFT%20may%20not%0Aonly%20optimize%20operations%20in%20real-time%20processing%20and%20recommendation%20systems%20but%0Aalso%20extend%20to%20advanced%20image%20processing%20techniques%2C%20where%20dimensionality%0Areduction%20can%20significantly%20improve%20performance%20and%20analysis%20efficiency.%20This%0Apaper%20advocates%20for%20the%20broader%20adoption%20of%20FFT%20in%20vector%20database%20management%2C%0Amarking%20a%20significant%20stride%20towards%20addressing%20the%20challenges%20of%20data%20volume%0Aand%20complexity%20in%20AI%20research%20and%20applications.%20Unlike%20many%20existing%0Aapproaches%2C%20we%20directly%20handle%20the%20embedding%20vectors%20produced%20by%20the%20model%0Aafter%20processing%20a%20test%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06278v1&entry.124074799=Read"},
{"title": "The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows", "author": "Humberto Reyes-Gonzalez and Riccardo Torre", "abstract": "  We propose the NFLikelihood, an unsupervised version, based on Normalizing\nFlows, of the DNNLikelihood proposed in Ref.[1]. We show, through realistic\nexamples, how Autoregressive Flows, based on affine and rational quadratic\nspline bijectors, are able to learn complicated high-dimensional Likelihoods\narising in High Energy Physics (HEP) analyses. We focus on a toy LHC analysis\nexample already considered in the literature and on two Effective Field Theory\nfits of flavor and electroweak observables, whose samples have been obtained\nthrought the HEPFit code. We discuss advantages and disadvantages of the\nunsupervised approach with respect to the supervised one and discuss possible\ninterplays of the two.\n", "link": "http://arxiv.org/abs/2309.09743v2", "date": "2024-04-09", "relevancy": 1.8601, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4751}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.463}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.463}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20NFLikelihood%3A%20an%20unsupervised%20DNNLikelihood%20from%20Normalizing%20Flows&body=Title%3A%20The%20NFLikelihood%3A%20an%20unsupervised%20DNNLikelihood%20from%20Normalizing%20Flows%0AAuthor%3A%20Humberto%20Reyes-Gonzalez%20and%20Riccardo%20Torre%0AAbstract%3A%20%20%20We%20propose%20the%20NFLikelihood%2C%20an%20unsupervised%20version%2C%20based%20on%20Normalizing%0AFlows%2C%20of%20the%20DNNLikelihood%20proposed%20in%20Ref.%5B1%5D.%20We%20show%2C%20through%20realistic%0Aexamples%2C%20how%20Autoregressive%20Flows%2C%20based%20on%20affine%20and%20rational%20quadratic%0Aspline%20bijectors%2C%20are%20able%20to%20learn%20complicated%20high-dimensional%20Likelihoods%0Aarising%20in%20High%20Energy%20Physics%20%28HEP%29%20analyses.%20We%20focus%20on%20a%20toy%20LHC%20analysis%0Aexample%20already%20considered%20in%20the%20literature%20and%20on%20two%20Effective%20Field%20Theory%0Afits%20of%20flavor%20and%20electroweak%20observables%2C%20whose%20samples%20have%20been%20obtained%0Athrought%20the%20HEPFit%20code.%20We%20discuss%20advantages%20and%20disadvantages%20of%20the%0Aunsupervised%20approach%20with%20respect%20to%20the%20supervised%20one%20and%20discuss%20possible%0Ainterplays%20of%20the%20two.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09743v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20NFLikelihood%3A%20an%20unsupervised%20DNNLikelihood%20from%20Normalizing%20Flows&entry.906535625=Humberto%20Reyes-Gonzalez%20and%20Riccardo%20Torre&entry.1292438233=%20%20We%20propose%20the%20NFLikelihood%2C%20an%20unsupervised%20version%2C%20based%20on%20Normalizing%0AFlows%2C%20of%20the%20DNNLikelihood%20proposed%20in%20Ref.%5B1%5D.%20We%20show%2C%20through%20realistic%0Aexamples%2C%20how%20Autoregressive%20Flows%2C%20based%20on%20affine%20and%20rational%20quadratic%0Aspline%20bijectors%2C%20are%20able%20to%20learn%20complicated%20high-dimensional%20Likelihoods%0Aarising%20in%20High%20Energy%20Physics%20%28HEP%29%20analyses.%20We%20focus%20on%20a%20toy%20LHC%20analysis%0Aexample%20already%20considered%20in%20the%20literature%20and%20on%20two%20Effective%20Field%20Theory%0Afits%20of%20flavor%20and%20electroweak%20observables%2C%20whose%20samples%20have%20been%20obtained%0Athrought%20the%20HEPFit%20code.%20We%20discuss%20advantages%20and%20disadvantages%20of%20the%0Aunsupervised%20approach%20with%20respect%20to%20the%20supervised%20one%20and%20discuss%20possible%0Ainterplays%20of%20the%20two.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09743v2&entry.124074799=Read"},
{"title": "On Early-stage Debunking Rumors on Twitter: Leveraging the Wisdom of\n  Weak Learners", "author": "Tu Nguyen and Cheng Li and Claudia Nieder\u00e9e", "abstract": "  Recently a lot of progress has been made in rumor modeling and rumor\ndetection for micro-blogging streams. However, existing automated methods do\nnot perform very well for early rumor detection, which is crucial in many\nsettings, e.g., in crisis situations. One reason for this is that aggregated\nrumor features such as propagation features, which work well on the long run,\nare - due to their accumulating characteristic - not very helpful in the early\nphase of a rumor. In this work, we present an approach for early rumor\ndetection, which leverages Convolutional Neural Networks for learning the\nhidden representations of individual rumor-related tweets to gain insights on\nthe credibility of each tweets. We then aggregate the predictions from the very\nbeginning of a rumor to obtain the overall event credits (so-called wisdom),\nand finally combine it with a time series based rumor classification model. Our\nextensive experiments show a clearly improved classification performance within\nthe critical very first hours of a rumor. For a better understanding, we also\nconduct an extensive feature evaluation that emphasized on the early stage and\nshows that the low-level credibility has best predictability at all phases of\nthe rumor lifetime.\n", "link": "http://arxiv.org/abs/1709.04402v2", "date": "2024-04-09", "relevancy": 1.8449, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.474}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4523}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Early-stage%20Debunking%20Rumors%20on%20Twitter%3A%20Leveraging%20the%20Wisdom%20of%0A%20%20Weak%20Learners&body=Title%3A%20On%20Early-stage%20Debunking%20Rumors%20on%20Twitter%3A%20Leveraging%20the%20Wisdom%20of%0A%20%20Weak%20Learners%0AAuthor%3A%20Tu%20Nguyen%20and%20Cheng%20Li%20and%20Claudia%20Nieder%C3%A9e%0AAbstract%3A%20%20%20Recently%20a%20lot%20of%20progress%20has%20been%20made%20in%20rumor%20modeling%20and%20rumor%0Adetection%20for%20micro-blogging%20streams.%20However%2C%20existing%20automated%20methods%20do%0Anot%20perform%20very%20well%20for%20early%20rumor%20detection%2C%20which%20is%20crucial%20in%20many%0Asettings%2C%20e.g.%2C%20in%20crisis%20situations.%20One%20reason%20for%20this%20is%20that%20aggregated%0Arumor%20features%20such%20as%20propagation%20features%2C%20which%20work%20well%20on%20the%20long%20run%2C%0Aare%20-%20due%20to%20their%20accumulating%20characteristic%20-%20not%20very%20helpful%20in%20the%20early%0Aphase%20of%20a%20rumor.%20In%20this%20work%2C%20we%20present%20an%20approach%20for%20early%20rumor%0Adetection%2C%20which%20leverages%20Convolutional%20Neural%20Networks%20for%20learning%20the%0Ahidden%20representations%20of%20individual%20rumor-related%20tweets%20to%20gain%20insights%20on%0Athe%20credibility%20of%20each%20tweets.%20We%20then%20aggregate%20the%20predictions%20from%20the%20very%0Abeginning%20of%20a%20rumor%20to%20obtain%20the%20overall%20event%20credits%20%28so-called%20wisdom%29%2C%0Aand%20finally%20combine%20it%20with%20a%20time%20series%20based%20rumor%20classification%20model.%20Our%0Aextensive%20experiments%20show%20a%20clearly%20improved%20classification%20performance%20within%0Athe%20critical%20very%20first%20hours%20of%20a%20rumor.%20For%20a%20better%20understanding%2C%20we%20also%0Aconduct%20an%20extensive%20feature%20evaluation%20that%20emphasized%20on%20the%20early%20stage%20and%0Ashows%20that%20the%20low-level%20credibility%20has%20best%20predictability%20at%20all%20phases%20of%0Athe%20rumor%20lifetime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1709.04402v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Early-stage%20Debunking%20Rumors%20on%20Twitter%3A%20Leveraging%20the%20Wisdom%20of%0A%20%20Weak%20Learners&entry.906535625=Tu%20Nguyen%20and%20Cheng%20Li%20and%20Claudia%20Nieder%C3%A9e&entry.1292438233=%20%20Recently%20a%20lot%20of%20progress%20has%20been%20made%20in%20rumor%20modeling%20and%20rumor%0Adetection%20for%20micro-blogging%20streams.%20However%2C%20existing%20automated%20methods%20do%0Anot%20perform%20very%20well%20for%20early%20rumor%20detection%2C%20which%20is%20crucial%20in%20many%0Asettings%2C%20e.g.%2C%20in%20crisis%20situations.%20One%20reason%20for%20this%20is%20that%20aggregated%0Arumor%20features%20such%20as%20propagation%20features%2C%20which%20work%20well%20on%20the%20long%20run%2C%0Aare%20-%20due%20to%20their%20accumulating%20characteristic%20-%20not%20very%20helpful%20in%20the%20early%0Aphase%20of%20a%20rumor.%20In%20this%20work%2C%20we%20present%20an%20approach%20for%20early%20rumor%0Adetection%2C%20which%20leverages%20Convolutional%20Neural%20Networks%20for%20learning%20the%0Ahidden%20representations%20of%20individual%20rumor-related%20tweets%20to%20gain%20insights%20on%0Athe%20credibility%20of%20each%20tweets.%20We%20then%20aggregate%20the%20predictions%20from%20the%20very%0Abeginning%20of%20a%20rumor%20to%20obtain%20the%20overall%20event%20credits%20%28so-called%20wisdom%29%2C%0Aand%20finally%20combine%20it%20with%20a%20time%20series%20based%20rumor%20classification%20model.%20Our%0Aextensive%20experiments%20show%20a%20clearly%20improved%20classification%20performance%20within%0Athe%20critical%20very%20first%20hours%20of%20a%20rumor.%20For%20a%20better%20understanding%2C%20we%20also%0Aconduct%20an%20extensive%20feature%20evaluation%20that%20emphasized%20on%20the%20early%20stage%20and%0Ashows%20that%20the%20low-level%20credibility%20has%20best%20predictability%20at%20all%20phases%20of%0Athe%20rumor%20lifetime.%0A&entry.1838667208=http%3A//arxiv.org/abs/1709.04402v2&entry.124074799=Read"},
{"title": "Faithful and Robust Local Interpretability for Textual Predictions", "author": "Gianluigi Lopardo and Frederic Precioso and Damien Garreau", "abstract": "  Interpretability is essential for machine learning models to be trusted and\ndeployed in critical domains. However, existing methods for interpreting text\nmodels are often complex, lack mathematical foundations, and their performance\nis not guaranteed. In this paper, we propose FRED (Faithful and Robust\nExplainer for textual Documents), a novel method for interpreting predictions\nover text. FRED offers three key insights to explain a model prediction: (1) it\nidentifies the minimal set of words in a document whose removal has the\nstrongest influence on the prediction, (2) it assigns an importance score to\neach token, reflecting its influence on the model's output, and (3) it provides\ncounterfactual explanations by generating examples similar to the original\ndocument, but leading to a different prediction. We establish the reliability\nof FRED through formal definitions and theoretical analyses on interpretable\nclassifiers. Additionally, our empirical evaluation against state-of-the-art\nmethods demonstrates the effectiveness of FRED in providing insights into text\nmodels.\n", "link": "http://arxiv.org/abs/2311.01605v3", "date": "2024-04-09", "relevancy": 1.5448, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.527}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5154}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5099}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Faithful%20and%20Robust%20Local%20Interpretability%20for%20Textual%20Predictions&body=Title%3A%20Faithful%20and%20Robust%20Local%20Interpretability%20for%20Textual%20Predictions%0AAuthor%3A%20Gianluigi%20Lopardo%20and%20Frederic%20Precioso%20and%20Damien%20Garreau%0AAbstract%3A%20%20%20Interpretability%20is%20essential%20for%20machine%20learning%20models%20to%20be%20trusted%20and%0Adeployed%20in%20critical%20domains.%20However%2C%20existing%20methods%20for%20interpreting%20text%0Amodels%20are%20often%20complex%2C%20lack%20mathematical%20foundations%2C%20and%20their%20performance%0Ais%20not%20guaranteed.%20In%20this%20paper%2C%20we%20propose%20FRED%20%28Faithful%20and%20Robust%0AExplainer%20for%20textual%20Documents%29%2C%20a%20novel%20method%20for%20interpreting%20predictions%0Aover%20text.%20FRED%20offers%20three%20key%20insights%20to%20explain%20a%20model%20prediction%3A%20%281%29%20it%0Aidentifies%20the%20minimal%20set%20of%20words%20in%20a%20document%20whose%20removal%20has%20the%0Astrongest%20influence%20on%20the%20prediction%2C%20%282%29%20it%20assigns%20an%20importance%20score%20to%0Aeach%20token%2C%20reflecting%20its%20influence%20on%20the%20model%27s%20output%2C%20and%20%283%29%20it%20provides%0Acounterfactual%20explanations%20by%20generating%20examples%20similar%20to%20the%20original%0Adocument%2C%20but%20leading%20to%20a%20different%20prediction.%20We%20establish%20the%20reliability%0Aof%20FRED%20through%20formal%20definitions%20and%20theoretical%20analyses%20on%20interpretable%0Aclassifiers.%20Additionally%2C%20our%20empirical%20evaluation%20against%20state-of-the-art%0Amethods%20demonstrates%20the%20effectiveness%20of%20FRED%20in%20providing%20insights%20into%20text%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01605v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faithful%20and%20Robust%20Local%20Interpretability%20for%20Textual%20Predictions&entry.906535625=Gianluigi%20Lopardo%20and%20Frederic%20Precioso%20and%20Damien%20Garreau&entry.1292438233=%20%20Interpretability%20is%20essential%20for%20machine%20learning%20models%20to%20be%20trusted%20and%0Adeployed%20in%20critical%20domains.%20However%2C%20existing%20methods%20for%20interpreting%20text%0Amodels%20are%20often%20complex%2C%20lack%20mathematical%20foundations%2C%20and%20their%20performance%0Ais%20not%20guaranteed.%20In%20this%20paper%2C%20we%20propose%20FRED%20%28Faithful%20and%20Robust%0AExplainer%20for%20textual%20Documents%29%2C%20a%20novel%20method%20for%20interpreting%20predictions%0Aover%20text.%20FRED%20offers%20three%20key%20insights%20to%20explain%20a%20model%20prediction%3A%20%281%29%20it%0Aidentifies%20the%20minimal%20set%20of%20words%20in%20a%20document%20whose%20removal%20has%20the%0Astrongest%20influence%20on%20the%20prediction%2C%20%282%29%20it%20assigns%20an%20importance%20score%20to%0Aeach%20token%2C%20reflecting%20its%20influence%20on%20the%20model%27s%20output%2C%20and%20%283%29%20it%20provides%0Acounterfactual%20explanations%20by%20generating%20examples%20similar%20to%20the%20original%0Adocument%2C%20but%20leading%20to%20a%20different%20prediction.%20We%20establish%20the%20reliability%0Aof%20FRED%20through%20formal%20definitions%20and%20theoretical%20analyses%20on%20interpretable%0Aclassifiers.%20Additionally%2C%20our%20empirical%20evaluation%20against%20state-of-the-art%0Amethods%20demonstrates%20the%20effectiveness%20of%20FRED%20in%20providing%20insights%20into%20text%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01605v3&entry.124074799=Read"},
{"title": "Industrial Application of 6D Pose Estimation for Robotic Manipulation in\n  Automotive Internal Logistics", "author": "Philipp Quentin and Dino Knoll and Daniel Goehring", "abstract": "  Despite the advances in robotics a large proportion of the of parts handling\ntasks in the automotive industry's internal logistics are not automated but\nstill performed by humans. A key component to competitively automate these\nprocesses is a 6D pose estimation that can handle a large number of different\nparts, is adaptable to new parts with little manual effort, and is sufficiently\naccurate and robust with respect to industry requirements. In this context, the\nquestion arises as to the current status quo with respect to these measures. To\naddress this we built a representative 6D pose estimation pipeline with\nstate-of-the-art components from economically scalable real to synthetic data\ngeneration to pose estimators and evaluated it on automotive parts with regards\nto a realistic sequencing process. We found that using the data generation\napproaches, the performance of the trained 6D pose estimators are promising,\nbut do not meet industry requirements. We reveal that the reason for this is\nthe inability of the estimators to provide reliable uncertainties for their\nposes, rather than the ability of to provide sufficiently accurate poses. In\nthis context we further analyzed how RGB- and RGB-D-based approaches compare\nagainst this background and show that they are differently vulnerable to the\ndomain gap induced by synthetic data.\n", "link": "http://arxiv.org/abs/2309.14265v2", "date": "2024-04-09", "relevancy": 1.6459, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5803}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5577}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5323}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Industrial%20Application%20of%206D%20Pose%20Estimation%20for%20Robotic%20Manipulation%20in%0A%20%20Automotive%20Internal%20Logistics&body=Title%3A%20Industrial%20Application%20of%206D%20Pose%20Estimation%20for%20Robotic%20Manipulation%20in%0A%20%20Automotive%20Internal%20Logistics%0AAuthor%3A%20Philipp%20Quentin%20and%20Dino%20Knoll%20and%20Daniel%20Goehring%0AAbstract%3A%20%20%20Despite%20the%20advances%20in%20robotics%20a%20large%20proportion%20of%20the%20of%20parts%20handling%0Atasks%20in%20the%20automotive%20industry%27s%20internal%20logistics%20are%20not%20automated%20but%0Astill%20performed%20by%20humans.%20A%20key%20component%20to%20competitively%20automate%20these%0Aprocesses%20is%20a%206D%20pose%20estimation%20that%20can%20handle%20a%20large%20number%20of%20different%0Aparts%2C%20is%20adaptable%20to%20new%20parts%20with%20little%20manual%20effort%2C%20and%20is%20sufficiently%0Aaccurate%20and%20robust%20with%20respect%20to%20industry%20requirements.%20In%20this%20context%2C%20the%0Aquestion%20arises%20as%20to%20the%20current%20status%20quo%20with%20respect%20to%20these%20measures.%20To%0Aaddress%20this%20we%20built%20a%20representative%206D%20pose%20estimation%20pipeline%20with%0Astate-of-the-art%20components%20from%20economically%20scalable%20real%20to%20synthetic%20data%0Ageneration%20to%20pose%20estimators%20and%20evaluated%20it%20on%20automotive%20parts%20with%20regards%0Ato%20a%20realistic%20sequencing%20process.%20We%20found%20that%20using%20the%20data%20generation%0Aapproaches%2C%20the%20performance%20of%20the%20trained%206D%20pose%20estimators%20are%20promising%2C%0Abut%20do%20not%20meet%20industry%20requirements.%20We%20reveal%20that%20the%20reason%20for%20this%20is%0Athe%20inability%20of%20the%20estimators%20to%20provide%20reliable%20uncertainties%20for%20their%0Aposes%2C%20rather%20than%20the%20ability%20of%20to%20provide%20sufficiently%20accurate%20poses.%20In%0Athis%20context%20we%20further%20analyzed%20how%20RGB-%20and%20RGB-D-based%20approaches%20compare%0Aagainst%20this%20background%20and%20show%20that%20they%20are%20differently%20vulnerable%20to%20the%0Adomain%20gap%20induced%20by%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14265v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Industrial%20Application%20of%206D%20Pose%20Estimation%20for%20Robotic%20Manipulation%20in%0A%20%20Automotive%20Internal%20Logistics&entry.906535625=Philipp%20Quentin%20and%20Dino%20Knoll%20and%20Daniel%20Goehring&entry.1292438233=%20%20Despite%20the%20advances%20in%20robotics%20a%20large%20proportion%20of%20the%20of%20parts%20handling%0Atasks%20in%20the%20automotive%20industry%27s%20internal%20logistics%20are%20not%20automated%20but%0Astill%20performed%20by%20humans.%20A%20key%20component%20to%20competitively%20automate%20these%0Aprocesses%20is%20a%206D%20pose%20estimation%20that%20can%20handle%20a%20large%20number%20of%20different%0Aparts%2C%20is%20adaptable%20to%20new%20parts%20with%20little%20manual%20effort%2C%20and%20is%20sufficiently%0Aaccurate%20and%20robust%20with%20respect%20to%20industry%20requirements.%20In%20this%20context%2C%20the%0Aquestion%20arises%20as%20to%20the%20current%20status%20quo%20with%20respect%20to%20these%20measures.%20To%0Aaddress%20this%20we%20built%20a%20representative%206D%20pose%20estimation%20pipeline%20with%0Astate-of-the-art%20components%20from%20economically%20scalable%20real%20to%20synthetic%20data%0Ageneration%20to%20pose%20estimators%20and%20evaluated%20it%20on%20automotive%20parts%20with%20regards%0Ato%20a%20realistic%20sequencing%20process.%20We%20found%20that%20using%20the%20data%20generation%0Aapproaches%2C%20the%20performance%20of%20the%20trained%206D%20pose%20estimators%20are%20promising%2C%0Abut%20do%20not%20meet%20industry%20requirements.%20We%20reveal%20that%20the%20reason%20for%20this%20is%0Athe%20inability%20of%20the%20estimators%20to%20provide%20reliable%20uncertainties%20for%20their%0Aposes%2C%20rather%20than%20the%20ability%20of%20to%20provide%20sufficiently%20accurate%20poses.%20In%0Athis%20context%20we%20further%20analyzed%20how%20RGB-%20and%20RGB-D-based%20approaches%20compare%0Aagainst%20this%20background%20and%20show%20that%20they%20are%20differently%20vulnerable%20to%20the%0Adomain%20gap%20induced%20by%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14265v2&entry.124074799=Read"},
{"title": "Wu's Method can Boost Symbolic AI to Rival Silver Medalists and\n  AlphaGeometry to Outperform Gold Medalists at IMO Geometry", "author": "Shiven Sinha and Ameya Prabhu and Ponnurangam Kumaraguru and Siddharth Bhat and Matthias Bethge", "abstract": "  Proving geometric theorems constitutes a hallmark of visual reasoning\ncombining both intuitive and logical skills. Therefore, automated theorem\nproving of Olympiad-level geometry problems is considered a notable milestone\nin human-level automated reasoning. The introduction of AlphaGeometry, a\nneuro-symbolic model trained with 100 million synthetic samples, marked a major\nbreakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO)\nproblems whereas the reported baseline based on Wu's method solved only ten. In\nthis note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry,\nand find that Wu's method is surprisingly strong. Wu's method alone can solve\n15 problems, and some of them are not solved by any of the other methods. This\nleads to two key findings: (i) Combining Wu's method with the classic synthetic\nmethods of deductive databases and angle, ratio, and distance chasing solves 21\nout of 30 methods by just using a CPU-only laptop with a time limit of 5\nminutes per problem. Essentially, this classic method solves just 4 problems\nless than AlphaGeometry and establishes the first fully symbolic baseline\nstrong enough to rival the performance of an IMO silver medalist. (ii) Wu's\nmethod even solves 2 of the 5 problems that AlphaGeometry failed to solve.\nThus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art\nfor automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the\nfirst AI method which outperforms an IMO gold medalist.\n", "link": "http://arxiv.org/abs/2404.06405v1", "date": "2024-04-09", "relevancy": 1.2741, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4394}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4234}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4132}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Wu%27s%20Method%20can%20Boost%20Symbolic%20AI%20to%20Rival%20Silver%20Medalists%20and%0A%20%20AlphaGeometry%20to%20Outperform%20Gold%20Medalists%20at%20IMO%20Geometry&body=Title%3A%20Wu%27s%20Method%20can%20Boost%20Symbolic%20AI%20to%20Rival%20Silver%20Medalists%20and%0A%20%20AlphaGeometry%20to%20Outperform%20Gold%20Medalists%20at%20IMO%20Geometry%0AAuthor%3A%20Shiven%20Sinha%20and%20Ameya%20Prabhu%20and%20Ponnurangam%20Kumaraguru%20and%20Siddharth%20Bhat%20and%20Matthias%20Bethge%0AAbstract%3A%20%20%20Proving%20geometric%20theorems%20constitutes%20a%20hallmark%20of%20visual%20reasoning%0Acombining%20both%20intuitive%20and%20logical%20skills.%20Therefore%2C%20automated%20theorem%0Aproving%20of%20Olympiad-level%20geometry%20problems%20is%20considered%20a%20notable%20milestone%0Ain%20human-level%20automated%20reasoning.%20The%20introduction%20of%20AlphaGeometry%2C%20a%0Aneuro-symbolic%20model%20trained%20with%20100%20million%20synthetic%20samples%2C%20marked%20a%20major%0Abreakthrough.%20It%20solved%2025%20of%2030%20International%20Mathematical%20Olympiad%20%28IMO%29%0Aproblems%20whereas%20the%20reported%20baseline%20based%20on%20Wu%27s%20method%20solved%20only%20ten.%20In%0Athis%20note%2C%20we%20revisit%20the%20IMO-AG-30%20Challenge%20introduced%20with%20AlphaGeometry%2C%0Aand%20find%20that%20Wu%27s%20method%20is%20surprisingly%20strong.%20Wu%27s%20method%20alone%20can%20solve%0A15%20problems%2C%20and%20some%20of%20them%20are%20not%20solved%20by%20any%20of%20the%20other%20methods.%20This%0Aleads%20to%20two%20key%20findings%3A%20%28i%29%20Combining%20Wu%27s%20method%20with%20the%20classic%20synthetic%0Amethods%20of%20deductive%20databases%20and%20angle%2C%20ratio%2C%20and%20distance%20chasing%20solves%2021%0Aout%20of%2030%20methods%20by%20just%20using%20a%20CPU-only%20laptop%20with%20a%20time%20limit%20of%205%0Aminutes%20per%20problem.%20Essentially%2C%20this%20classic%20method%20solves%20just%204%20problems%0Aless%20than%20AlphaGeometry%20and%20establishes%20the%20first%20fully%20symbolic%20baseline%0Astrong%20enough%20to%20rival%20the%20performance%20of%20an%20IMO%20silver%20medalist.%20%28ii%29%20Wu%27s%0Amethod%20even%20solves%202%20of%20the%205%20problems%20that%20AlphaGeometry%20failed%20to%20solve.%0AThus%2C%20by%20combining%20AlphaGeometry%20with%20Wu%27s%20method%20we%20set%20a%20new%20state-of-the-art%0Afor%20automated%20theorem%20proving%20on%20IMO-AG-30%2C%20solving%2027%20out%20of%2030%20problems%2C%20the%0Afirst%20AI%20method%20which%20outperforms%20an%20IMO%20gold%20medalist.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06405v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wu%27s%20Method%20can%20Boost%20Symbolic%20AI%20to%20Rival%20Silver%20Medalists%20and%0A%20%20AlphaGeometry%20to%20Outperform%20Gold%20Medalists%20at%20IMO%20Geometry&entry.906535625=Shiven%20Sinha%20and%20Ameya%20Prabhu%20and%20Ponnurangam%20Kumaraguru%20and%20Siddharth%20Bhat%20and%20Matthias%20Bethge&entry.1292438233=%20%20Proving%20geometric%20theorems%20constitutes%20a%20hallmark%20of%20visual%20reasoning%0Acombining%20both%20intuitive%20and%20logical%20skills.%20Therefore%2C%20automated%20theorem%0Aproving%20of%20Olympiad-level%20geometry%20problems%20is%20considered%20a%20notable%20milestone%0Ain%20human-level%20automated%20reasoning.%20The%20introduction%20of%20AlphaGeometry%2C%20a%0Aneuro-symbolic%20model%20trained%20with%20100%20million%20synthetic%20samples%2C%20marked%20a%20major%0Abreakthrough.%20It%20solved%2025%20of%2030%20International%20Mathematical%20Olympiad%20%28IMO%29%0Aproblems%20whereas%20the%20reported%20baseline%20based%20on%20Wu%27s%20method%20solved%20only%20ten.%20In%0Athis%20note%2C%20we%20revisit%20the%20IMO-AG-30%20Challenge%20introduced%20with%20AlphaGeometry%2C%0Aand%20find%20that%20Wu%27s%20method%20is%20surprisingly%20strong.%20Wu%27s%20method%20alone%20can%20solve%0A15%20problems%2C%20and%20some%20of%20them%20are%20not%20solved%20by%20any%20of%20the%20other%20methods.%20This%0Aleads%20to%20two%20key%20findings%3A%20%28i%29%20Combining%20Wu%27s%20method%20with%20the%20classic%20synthetic%0Amethods%20of%20deductive%20databases%20and%20angle%2C%20ratio%2C%20and%20distance%20chasing%20solves%2021%0Aout%20of%2030%20methods%20by%20just%20using%20a%20CPU-only%20laptop%20with%20a%20time%20limit%20of%205%0Aminutes%20per%20problem.%20Essentially%2C%20this%20classic%20method%20solves%20just%204%20problems%0Aless%20than%20AlphaGeometry%20and%20establishes%20the%20first%20fully%20symbolic%20baseline%0Astrong%20enough%20to%20rival%20the%20performance%20of%20an%20IMO%20silver%20medalist.%20%28ii%29%20Wu%27s%0Amethod%20even%20solves%202%20of%20the%205%20problems%20that%20AlphaGeometry%20failed%20to%20solve.%0AThus%2C%20by%20combining%20AlphaGeometry%20with%20Wu%27s%20method%20we%20set%20a%20new%20state-of-the-art%0Afor%20automated%20theorem%20proving%20on%20IMO-AG-30%2C%20solving%2027%20out%20of%2030%20problems%2C%20the%0Afirst%20AI%20method%20which%20outperforms%20an%20IMO%20gold%20medalist.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06405v1&entry.124074799=Read"},
{"title": "Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of\n  Figure Skating", "author": "Sheng-Lan Liu and Yu-Ning Ding and Gang Yan and Si-Fan Zhang and Jin-Rong Zhang and Wen-Yue Chen and Xue-Hai Xu", "abstract": "  The fine-grained action analysis of the existing action datasets is\nchallenged by insufficient action categories, low fine granularities, limited\nmodalities, and tasks. In this paper, we propose a Multi-modality and\nMulti-task dataset of Figure Skating (MMFS) which was collected from the World\nFigure Skating Championships. MMFS, which possesses action recognition and\naction quality assessment, captures RGB, skeleton, and is collected the score\nof actions from 11671 clips with 256 categories including spatial and temporal\nlabels. The key contributions of our dataset fall into three aspects as\nfollows. (1) Independently spatial and temporal categories are first proposed\nto further explore fine-grained action recognition and quality assessment. (2)\nMMFS first introduces the skeleton modality for complex fine-grained action\nquality assessment. (3) Our multi-modality and multi-task dataset encourage\nmore action analysis models. To benchmark our dataset, we adopt RGB-based and\nskeleton-based baseline methods for action recognition and action quality\nassessment.\n", "link": "http://arxiv.org/abs/2307.02730v3", "date": "2024-04-09", "relevancy": 1.5964, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5351}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5341}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5242}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fine-grained%20Action%20Analysis%3A%20A%20Multi-modality%20and%20Multi-task%20Dataset%20of%0A%20%20Figure%20Skating&body=Title%3A%20Fine-grained%20Action%20Analysis%3A%20A%20Multi-modality%20and%20Multi-task%20Dataset%20of%0A%20%20Figure%20Skating%0AAuthor%3A%20Sheng-Lan%20Liu%20and%20Yu-Ning%20Ding%20and%20Gang%20Yan%20and%20Si-Fan%20Zhang%20and%20Jin-Rong%20Zhang%20and%20Wen-Yue%20Chen%20and%20Xue-Hai%20Xu%0AAbstract%3A%20%20%20The%20fine-grained%20action%20analysis%20of%20the%20existing%20action%20datasets%20is%0Achallenged%20by%20insufficient%20action%20categories%2C%20low%20fine%20granularities%2C%20limited%0Amodalities%2C%20and%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20Multi-modality%20and%0AMulti-task%20dataset%20of%20Figure%20Skating%20%28MMFS%29%20which%20was%20collected%20from%20the%20World%0AFigure%20Skating%20Championships.%20MMFS%2C%20which%20possesses%20action%20recognition%20and%0Aaction%20quality%20assessment%2C%20captures%20RGB%2C%20skeleton%2C%20and%20is%20collected%20the%20score%0Aof%20actions%20from%2011671%20clips%20with%20256%20categories%20including%20spatial%20and%20temporal%0Alabels.%20The%20key%20contributions%20of%20our%20dataset%20fall%20into%20three%20aspects%20as%0Afollows.%20%281%29%20Independently%20spatial%20and%20temporal%20categories%20are%20first%20proposed%0Ato%20further%20explore%20fine-grained%20action%20recognition%20and%20quality%20assessment.%20%282%29%0AMMFS%20first%20introduces%20the%20skeleton%20modality%20for%20complex%20fine-grained%20action%0Aquality%20assessment.%20%283%29%20Our%20multi-modality%20and%20multi-task%20dataset%20encourage%0Amore%20action%20analysis%20models.%20To%20benchmark%20our%20dataset%2C%20we%20adopt%20RGB-based%20and%0Askeleton-based%20baseline%20methods%20for%20action%20recognition%20and%20action%20quality%0Aassessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.02730v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-grained%20Action%20Analysis%3A%20A%20Multi-modality%20and%20Multi-task%20Dataset%20of%0A%20%20Figure%20Skating&entry.906535625=Sheng-Lan%20Liu%20and%20Yu-Ning%20Ding%20and%20Gang%20Yan%20and%20Si-Fan%20Zhang%20and%20Jin-Rong%20Zhang%20and%20Wen-Yue%20Chen%20and%20Xue-Hai%20Xu&entry.1292438233=%20%20The%20fine-grained%20action%20analysis%20of%20the%20existing%20action%20datasets%20is%0Achallenged%20by%20insufficient%20action%20categories%2C%20low%20fine%20granularities%2C%20limited%0Amodalities%2C%20and%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20Multi-modality%20and%0AMulti-task%20dataset%20of%20Figure%20Skating%20%28MMFS%29%20which%20was%20collected%20from%20the%20World%0AFigure%20Skating%20Championships.%20MMFS%2C%20which%20possesses%20action%20recognition%20and%0Aaction%20quality%20assessment%2C%20captures%20RGB%2C%20skeleton%2C%20and%20is%20collected%20the%20score%0Aof%20actions%20from%2011671%20clips%20with%20256%20categories%20including%20spatial%20and%20temporal%0Alabels.%20The%20key%20contributions%20of%20our%20dataset%20fall%20into%20three%20aspects%20as%0Afollows.%20%281%29%20Independently%20spatial%20and%20temporal%20categories%20are%20first%20proposed%0Ato%20further%20explore%20fine-grained%20action%20recognition%20and%20quality%20assessment.%20%282%29%0AMMFS%20first%20introduces%20the%20skeleton%20modality%20for%20complex%20fine-grained%20action%0Aquality%20assessment.%20%283%29%20Our%20multi-modality%20and%20multi-task%20dataset%20encourage%0Amore%20action%20analysis%20models.%20To%20benchmark%20our%20dataset%2C%20we%20adopt%20RGB-based%20and%0Askeleton-based%20baseline%20methods%20for%20action%20recognition%20and%20action%20quality%0Aassessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.02730v3&entry.124074799=Read"},
{"title": "Apprentices to Research Assistants: Advancing Research with Large\n  Language Models", "author": "M. Namvarpour and A. Razi", "abstract": "  Large Language Models (LLMs) have emerged as powerful tools in various\nresearch domains. This article examines their potential through a literature\nreview and firsthand experimentation. While LLMs offer benefits like\ncost-effectiveness and efficiency, challenges such as prompt tuning, biases,\nand subjectivity must be addressed. The study presents insights from\nexperiments utilizing LLMs for qualitative analysis, highlighting successes and\nlimitations. Additionally, it discusses strategies for mitigating challenges,\nsuch as prompt optimization techniques and leveraging human expertise. This\nstudy aligns with the 'LLMs as Research Tools' workshop's focus on integrating\nLLMs into HCI data work critically and ethically. By addressing both\nopportunities and challenges, our work contributes to the ongoing dialogue on\ntheir responsible application in research.\n", "link": "http://arxiv.org/abs/2404.06404v1", "date": "2024-04-09", "relevancy": 1.4255, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4854}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4728}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4519}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Apprentices%20to%20Research%20Assistants%3A%20Advancing%20Research%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Apprentices%20to%20Research%20Assistants%3A%20Advancing%20Research%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20M.%20Namvarpour%20and%20A.%20Razi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20in%20various%0Aresearch%20domains.%20This%20article%20examines%20their%20potential%20through%20a%20literature%0Areview%20and%20firsthand%20experimentation.%20While%20LLMs%20offer%20benefits%20like%0Acost-effectiveness%20and%20efficiency%2C%20challenges%20such%20as%20prompt%20tuning%2C%20biases%2C%0Aand%20subjectivity%20must%20be%20addressed.%20The%20study%20presents%20insights%20from%0Aexperiments%20utilizing%20LLMs%20for%20qualitative%20analysis%2C%20highlighting%20successes%20and%0Alimitations.%20Additionally%2C%20it%20discusses%20strategies%20for%20mitigating%20challenges%2C%0Asuch%20as%20prompt%20optimization%20techniques%20and%20leveraging%20human%20expertise.%20This%0Astudy%20aligns%20with%20the%20%27LLMs%20as%20Research%20Tools%27%20workshop%27s%20focus%20on%20integrating%0ALLMs%20into%20HCI%20data%20work%20critically%20and%20ethically.%20By%20addressing%20both%0Aopportunities%20and%20challenges%2C%20our%20work%20contributes%20to%20the%20ongoing%20dialogue%20on%0Atheir%20responsible%20application%20in%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06404v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Apprentices%20to%20Research%20Assistants%3A%20Advancing%20Research%20with%20Large%0A%20%20Language%20Models&entry.906535625=M.%20Namvarpour%20and%20A.%20Razi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20in%20various%0Aresearch%20domains.%20This%20article%20examines%20their%20potential%20through%20a%20literature%0Areview%20and%20firsthand%20experimentation.%20While%20LLMs%20offer%20benefits%20like%0Acost-effectiveness%20and%20efficiency%2C%20challenges%20such%20as%20prompt%20tuning%2C%20biases%2C%0Aand%20subjectivity%20must%20be%20addressed.%20The%20study%20presents%20insights%20from%0Aexperiments%20utilizing%20LLMs%20for%20qualitative%20analysis%2C%20highlighting%20successes%20and%0Alimitations.%20Additionally%2C%20it%20discusses%20strategies%20for%20mitigating%20challenges%2C%0Asuch%20as%20prompt%20optimization%20techniques%20and%20leveraging%20human%20expertise.%20This%0Astudy%20aligns%20with%20the%20%27LLMs%20as%20Research%20Tools%27%20workshop%27s%20focus%20on%20integrating%0ALLMs%20into%20HCI%20data%20work%20critically%20and%20ethically.%20By%20addressing%20both%0Aopportunities%20and%20challenges%2C%20our%20work%20contributes%20to%20the%20ongoing%20dialogue%20on%0Atheir%20responsible%20application%20in%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06404v1&entry.124074799=Read"},
{"title": "Fast and Adaptive Multi-agent Planning under Collaborative Temporal\n  Logic Tasks via Poset Products", "author": "Zesen Liu and Meng Guo and Weimin Bao and Zhongkui Li", "abstract": "  Efficient coordination and planning is essential for large-scale multi-agent\nsystems that collaborate in a shared dynamic environment. Heuristic search\nmethods or learning-based approaches often lack the guarantee on correctness\nand performance. Moreover, when the collaborative tasks contain both spatial\nand temporal requirements, e.g., as Linear Temporal Logic (LTL) formulas,\nformal methods provide a verifiable framework for task planning. However, since\nthe planning complexity grows exponentially with the number of agents and the\nlength of the task formula, existing studies are mostly limited to small\nartificial cases. To address this issue, a new planning paradigm is proposed in\nthis work for system-wide temporal task formulas that are released online and\ncontinually. It avoids two common bottlenecks in the traditional methods, i.e.,\n(i) the direct translation of the complete task formula to the associated\nB\\\"uchi automaton; and (ii) the synchronized product between the B\\\"uchi\nautomaton and the transition models of all agents. Instead, an adaptive\nplanning algorithm is proposed that computes the product of relaxed\npartially-ordered sets (R-posets) on-the-fly, and assigns these subtasks to the\nagents subject to the ordering constraints. It is shown that the first valid\nplan can be derived with a polynomial time and memory complexity w.r.t. the\nsystem size and the formula length. Our method can take into account task\nformulas with a length of more than 400 and a fleet with more than $400$\nagents, while most existing methods fail at the formula length of 25 within a\nreasonable duration. The proposed method is validated on large fleets of\nservice robots in both simulation and hardware experiments.\n", "link": "http://arxiv.org/abs/2308.11373v2", "date": "2024-04-09", "relevancy": 1.4871, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5246}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5189}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4749}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Adaptive%20Multi-agent%20Planning%20under%20Collaborative%20Temporal%0A%20%20Logic%20Tasks%20via%20Poset%20Products&body=Title%3A%20Fast%20and%20Adaptive%20Multi-agent%20Planning%20under%20Collaborative%20Temporal%0A%20%20Logic%20Tasks%20via%20Poset%20Products%0AAuthor%3A%20Zesen%20Liu%20and%20Meng%20Guo%20and%20Weimin%20Bao%20and%20Zhongkui%20Li%0AAbstract%3A%20%20%20Efficient%20coordination%20and%20planning%20is%20essential%20for%20large-scale%20multi-agent%0Asystems%20that%20collaborate%20in%20a%20shared%20dynamic%20environment.%20Heuristic%20search%0Amethods%20or%20learning-based%20approaches%20often%20lack%20the%20guarantee%20on%20correctness%0Aand%20performance.%20Moreover%2C%20when%20the%20collaborative%20tasks%20contain%20both%20spatial%0Aand%20temporal%20requirements%2C%20e.g.%2C%20as%20Linear%20Temporal%20Logic%20%28LTL%29%20formulas%2C%0Aformal%20methods%20provide%20a%20verifiable%20framework%20for%20task%20planning.%20However%2C%20since%0Athe%20planning%20complexity%20grows%20exponentially%20with%20the%20number%20of%20agents%20and%20the%0Alength%20of%20the%20task%20formula%2C%20existing%20studies%20are%20mostly%20limited%20to%20small%0Aartificial%20cases.%20To%20address%20this%20issue%2C%20a%20new%20planning%20paradigm%20is%20proposed%20in%0Athis%20work%20for%20system-wide%20temporal%20task%20formulas%20that%20are%20released%20online%20and%0Acontinually.%20It%20avoids%20two%20common%20bottlenecks%20in%20the%20traditional%20methods%2C%20i.e.%2C%0A%28i%29%20the%20direct%20translation%20of%20the%20complete%20task%20formula%20to%20the%20associated%0AB%5C%22uchi%20automaton%3B%20and%20%28ii%29%20the%20synchronized%20product%20between%20the%20B%5C%22uchi%0Aautomaton%20and%20the%20transition%20models%20of%20all%20agents.%20Instead%2C%20an%20adaptive%0Aplanning%20algorithm%20is%20proposed%20that%20computes%20the%20product%20of%20relaxed%0Apartially-ordered%20sets%20%28R-posets%29%20on-the-fly%2C%20and%20assigns%20these%20subtasks%20to%20the%0Aagents%20subject%20to%20the%20ordering%20constraints.%20It%20is%20shown%20that%20the%20first%20valid%0Aplan%20can%20be%20derived%20with%20a%20polynomial%20time%20and%20memory%20complexity%20w.r.t.%20the%0Asystem%20size%20and%20the%20formula%20length.%20Our%20method%20can%20take%20into%20account%20task%0Aformulas%20with%20a%20length%20of%20more%20than%20400%20and%20a%20fleet%20with%20more%20than%20%24400%24%0Aagents%2C%20while%20most%20existing%20methods%20fail%20at%20the%20formula%20length%20of%2025%20within%20a%0Areasonable%20duration.%20The%20proposed%20method%20is%20validated%20on%20large%20fleets%20of%0Aservice%20robots%20in%20both%20simulation%20and%20hardware%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.11373v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Adaptive%20Multi-agent%20Planning%20under%20Collaborative%20Temporal%0A%20%20Logic%20Tasks%20via%20Poset%20Products&entry.906535625=Zesen%20Liu%20and%20Meng%20Guo%20and%20Weimin%20Bao%20and%20Zhongkui%20Li&entry.1292438233=%20%20Efficient%20coordination%20and%20planning%20is%20essential%20for%20large-scale%20multi-agent%0Asystems%20that%20collaborate%20in%20a%20shared%20dynamic%20environment.%20Heuristic%20search%0Amethods%20or%20learning-based%20approaches%20often%20lack%20the%20guarantee%20on%20correctness%0Aand%20performance.%20Moreover%2C%20when%20the%20collaborative%20tasks%20contain%20both%20spatial%0Aand%20temporal%20requirements%2C%20e.g.%2C%20as%20Linear%20Temporal%20Logic%20%28LTL%29%20formulas%2C%0Aformal%20methods%20provide%20a%20verifiable%20framework%20for%20task%20planning.%20However%2C%20since%0Athe%20planning%20complexity%20grows%20exponentially%20with%20the%20number%20of%20agents%20and%20the%0Alength%20of%20the%20task%20formula%2C%20existing%20studies%20are%20mostly%20limited%20to%20small%0Aartificial%20cases.%20To%20address%20this%20issue%2C%20a%20new%20planning%20paradigm%20is%20proposed%20in%0Athis%20work%20for%20system-wide%20temporal%20task%20formulas%20that%20are%20released%20online%20and%0Acontinually.%20It%20avoids%20two%20common%20bottlenecks%20in%20the%20traditional%20methods%2C%20i.e.%2C%0A%28i%29%20the%20direct%20translation%20of%20the%20complete%20task%20formula%20to%20the%20associated%0AB%5C%22uchi%20automaton%3B%20and%20%28ii%29%20the%20synchronized%20product%20between%20the%20B%5C%22uchi%0Aautomaton%20and%20the%20transition%20models%20of%20all%20agents.%20Instead%2C%20an%20adaptive%0Aplanning%20algorithm%20is%20proposed%20that%20computes%20the%20product%20of%20relaxed%0Apartially-ordered%20sets%20%28R-posets%29%20on-the-fly%2C%20and%20assigns%20these%20subtasks%20to%20the%0Aagents%20subject%20to%20the%20ordering%20constraints.%20It%20is%20shown%20that%20the%20first%20valid%0Aplan%20can%20be%20derived%20with%20a%20polynomial%20time%20and%20memory%20complexity%20w.r.t.%20the%0Asystem%20size%20and%20the%20formula%20length.%20Our%20method%20can%20take%20into%20account%20task%0Aformulas%20with%20a%20length%20of%20more%20than%20400%20and%20a%20fleet%20with%20more%20than%20%24400%24%0Aagents%2C%20while%20most%20existing%20methods%20fail%20at%20the%20formula%20length%20of%2025%20within%20a%0Areasonable%20duration.%20The%20proposed%20method%20is%20validated%20on%20large%20fleets%20of%0Aservice%20robots%20in%20both%20simulation%20and%20hardware%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.11373v2&entry.124074799=Read"},
{"title": "A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement", "author": "Ruirui Lin and Nantheera Anantrasirichai and Alexandra Malyugina and David Bull", "abstract": "  Distortions caused by low-light conditions are not only visually unpleasant\nbut also degrade the performance of computer vision tasks. The restoration and\nenhancement have proven to be highly beneficial. However, there are only a\nlimited number of enhancement methods explicitly designed for videos acquired\nin low-light conditions. We propose a Spatio-Temporal Aligned SUNet (STA-SUNet)\nmodel using a Swin Transformer as a backbone to capture low light video\nfeatures and exploit their spatio-temporal correlations. The STA-SUNet model is\ntrained on a novel, fully registered dataset (BVI), which comprises dynamic\nscenes captured under varying light conditions. It is further analysed\ncomparatively against various other models over three test datasets. The model\ndemonstrates superior adaptivity across all datasets, obtaining the highest\nPSNR and SSIM values. It is particularly effective in extreme low-light\nconditions, yielding fairly good visualisation results.\n", "link": "http://arxiv.org/abs/2403.02408v2", "date": "2024-04-09", "relevancy": 1.7485, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5901}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5888}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5606}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Spatio-temporal%20Aligned%20SUNet%20Model%20for%20Low-light%20Video%20Enhancement&body=Title%3A%20A%20Spatio-temporal%20Aligned%20SUNet%20Model%20for%20Low-light%20Video%20Enhancement%0AAuthor%3A%20Ruirui%20Lin%20and%20Nantheera%20Anantrasirichai%20and%20Alexandra%20Malyugina%20and%20David%20Bull%0AAbstract%3A%20%20%20Distortions%20caused%20by%20low-light%20conditions%20are%20not%20only%20visually%20unpleasant%0Abut%20also%20degrade%20the%20performance%20of%20computer%20vision%20tasks.%20The%20restoration%20and%0Aenhancement%20have%20proven%20to%20be%20highly%20beneficial.%20However%2C%20there%20are%20only%20a%0Alimited%20number%20of%20enhancement%20methods%20explicitly%20designed%20for%20videos%20acquired%0Ain%20low-light%20conditions.%20We%20propose%20a%20Spatio-Temporal%20Aligned%20SUNet%20%28STA-SUNet%29%0Amodel%20using%20a%20Swin%20Transformer%20as%20a%20backbone%20to%20capture%20low%20light%20video%0Afeatures%20and%20exploit%20their%20spatio-temporal%20correlations.%20The%20STA-SUNet%20model%20is%0Atrained%20on%20a%20novel%2C%20fully%20registered%20dataset%20%28BVI%29%2C%20which%20comprises%20dynamic%0Ascenes%20captured%20under%20varying%20light%20conditions.%20It%20is%20further%20analysed%0Acomparatively%20against%20various%20other%20models%20over%20three%20test%20datasets.%20The%20model%0Ademonstrates%20superior%20adaptivity%20across%20all%20datasets%2C%20obtaining%20the%20highest%0APSNR%20and%20SSIM%20values.%20It%20is%20particularly%20effective%20in%20extreme%20low-light%0Aconditions%2C%20yielding%20fairly%20good%20visualisation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02408v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Spatio-temporal%20Aligned%20SUNet%20Model%20for%20Low-light%20Video%20Enhancement&entry.906535625=Ruirui%20Lin%20and%20Nantheera%20Anantrasirichai%20and%20Alexandra%20Malyugina%20and%20David%20Bull&entry.1292438233=%20%20Distortions%20caused%20by%20low-light%20conditions%20are%20not%20only%20visually%20unpleasant%0Abut%20also%20degrade%20the%20performance%20of%20computer%20vision%20tasks.%20The%20restoration%20and%0Aenhancement%20have%20proven%20to%20be%20highly%20beneficial.%20However%2C%20there%20are%20only%20a%0Alimited%20number%20of%20enhancement%20methods%20explicitly%20designed%20for%20videos%20acquired%0Ain%20low-light%20conditions.%20We%20propose%20a%20Spatio-Temporal%20Aligned%20SUNet%20%28STA-SUNet%29%0Amodel%20using%20a%20Swin%20Transformer%20as%20a%20backbone%20to%20capture%20low%20light%20video%0Afeatures%20and%20exploit%20their%20spatio-temporal%20correlations.%20The%20STA-SUNet%20model%20is%0Atrained%20on%20a%20novel%2C%20fully%20registered%20dataset%20%28BVI%29%2C%20which%20comprises%20dynamic%0Ascenes%20captured%20under%20varying%20light%20conditions.%20It%20is%20further%20analysed%0Acomparatively%20against%20various%20other%20models%20over%20three%20test%20datasets.%20The%20model%0Ademonstrates%20superior%20adaptivity%20across%20all%20datasets%2C%20obtaining%20the%20highest%0APSNR%20and%20SSIM%20values.%20It%20is%20particularly%20effective%20in%20extreme%20low-light%0Aconditions%2C%20yielding%20fairly%20good%20visualisation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02408v2&entry.124074799=Read"},
{"title": "MetaMix: Meta-state Precision Searcher for Mixed-precision Activation\n  Quantization", "author": "Han-Byul Kim and Joo Hyung Lee and Sungjoo Yoo and Hong-Seok Kim", "abstract": "  Mixed-precision quantization of efficient networks often suffer from\nactivation instability encountered in the exploration of bit selections. To\naddress this problem, we propose a novel method called MetaMix which consists\nof bit selection and weight training phases. The bit selection phase iterates\ntwo steps, (1) the mixed-precision-aware weight update, and (2) the bit-search\ntraining with the fixed mixed-precision-aware weights, both of which combined\nreduce activation instability in mixed-precision quantization and contribute to\nfast and high-quality bit selection. The weight training phase exploits the\nweights and step sizes trained in the bit selection phase and fine-tunes them\nthereby offering fast training. Our experiments with efficient and\nhard-to-quantize networks, i.e., MobileNet v2 and v3, and ResNet-18 on ImageNet\nshow that our proposed method pushes the boundary of mixed-precision\nquantization, in terms of accuracy vs. operations, by outperforming both mixed-\nand single-precision SOTA methods.\n", "link": "http://arxiv.org/abs/2311.06798v2", "date": "2024-04-09", "relevancy": 1.4199, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4647}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4542}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MetaMix%3A%20Meta-state%20Precision%20Searcher%20for%20Mixed-precision%20Activation%0A%20%20Quantization&body=Title%3A%20MetaMix%3A%20Meta-state%20Precision%20Searcher%20for%20Mixed-precision%20Activation%0A%20%20Quantization%0AAuthor%3A%20Han-Byul%20Kim%20and%20Joo%20Hyung%20Lee%20and%20Sungjoo%20Yoo%20and%20Hong-Seok%20Kim%0AAbstract%3A%20%20%20Mixed-precision%20quantization%20of%20efficient%20networks%20often%20suffer%20from%0Aactivation%20instability%20encountered%20in%20the%20exploration%20of%20bit%20selections.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20a%20novel%20method%20called%20MetaMix%20which%20consists%0Aof%20bit%20selection%20and%20weight%20training%20phases.%20The%20bit%20selection%20phase%20iterates%0Atwo%20steps%2C%20%281%29%20the%20mixed-precision-aware%20weight%20update%2C%20and%20%282%29%20the%20bit-search%0Atraining%20with%20the%20fixed%20mixed-precision-aware%20weights%2C%20both%20of%20which%20combined%0Areduce%20activation%20instability%20in%20mixed-precision%20quantization%20and%20contribute%20to%0Afast%20and%20high-quality%20bit%20selection.%20The%20weight%20training%20phase%20exploits%20the%0Aweights%20and%20step%20sizes%20trained%20in%20the%20bit%20selection%20phase%20and%20fine-tunes%20them%0Athereby%20offering%20fast%20training.%20Our%20experiments%20with%20efficient%20and%0Ahard-to-quantize%20networks%2C%20i.e.%2C%20MobileNet%20v2%20and%20v3%2C%20and%20ResNet-18%20on%20ImageNet%0Ashow%20that%20our%20proposed%20method%20pushes%20the%20boundary%20of%20mixed-precision%0Aquantization%2C%20in%20terms%20of%20accuracy%20vs.%20operations%2C%20by%20outperforming%20both%20mixed-%0Aand%20single-precision%20SOTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06798v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaMix%3A%20Meta-state%20Precision%20Searcher%20for%20Mixed-precision%20Activation%0A%20%20Quantization&entry.906535625=Han-Byul%20Kim%20and%20Joo%20Hyung%20Lee%20and%20Sungjoo%20Yoo%20and%20Hong-Seok%20Kim&entry.1292438233=%20%20Mixed-precision%20quantization%20of%20efficient%20networks%20often%20suffer%20from%0Aactivation%20instability%20encountered%20in%20the%20exploration%20of%20bit%20selections.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20a%20novel%20method%20called%20MetaMix%20which%20consists%0Aof%20bit%20selection%20and%20weight%20training%20phases.%20The%20bit%20selection%20phase%20iterates%0Atwo%20steps%2C%20%281%29%20the%20mixed-precision-aware%20weight%20update%2C%20and%20%282%29%20the%20bit-search%0Atraining%20with%20the%20fixed%20mixed-precision-aware%20weights%2C%20both%20of%20which%20combined%0Areduce%20activation%20instability%20in%20mixed-precision%20quantization%20and%20contribute%20to%0Afast%20and%20high-quality%20bit%20selection.%20The%20weight%20training%20phase%20exploits%20the%0Aweights%20and%20step%20sizes%20trained%20in%20the%20bit%20selection%20phase%20and%20fine-tunes%20them%0Athereby%20offering%20fast%20training.%20Our%20experiments%20with%20efficient%20and%0Ahard-to-quantize%20networks%2C%20i.e.%2C%20MobileNet%20v2%20and%20v3%2C%20and%20ResNet-18%20on%20ImageNet%0Ashow%20that%20our%20proposed%20method%20pushes%20the%20boundary%20of%20mixed-precision%0Aquantization%2C%20in%20terms%20of%20accuracy%20vs.%20operations%2C%20by%20outperforming%20both%20mixed-%0Aand%20single-precision%20SOTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06798v2&entry.124074799=Read"},
{"title": "Quantum Circuit $C^*$-algebra Net", "author": "Yuka Hashimoto and Ryuichiro Hataya", "abstract": "  This paper introduces quantum circuit $C^*$-algebra net, which provides a\nconnection between $C^*$-algebra nets proposed in classical machine learning\nand quantum circuits. Using $C^*$-algebra, a generalization of the space of\ncomplex numbers, we can represent quantum gates as weight parameters of a\nneural network. By introducing additional parameters, we can induce interaction\namong multiple circuits constructed by quantum gates. This interaction enables\nthe circuits to share information among them, which contributes to improved\ngeneralization performance in machine learning tasks. As an application, we\npropose to use the quantum circuit $C^*$-algebra net to encode classical data\ninto quantum states, which enables us to integrate classical data into quantum\nalgorithms. Numerical results demonstrate that the interaction among circuits\nimproves performance significantly in image classification, and encoded data by\nthe quantum circuit $C^*$-algebra net are useful for downstream quantum machine\nlearning tasks.\n", "link": "http://arxiv.org/abs/2404.06218v1", "date": "2024-04-09", "relevancy": 1.4835, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3929}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3771}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3559}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Quantum%20Circuit%20%24C%5E%2A%24-algebra%20Net&body=Title%3A%20Quantum%20Circuit%20%24C%5E%2A%24-algebra%20Net%0AAuthor%3A%20Yuka%20Hashimoto%20and%20Ryuichiro%20Hataya%0AAbstract%3A%20%20%20This%20paper%20introduces%20quantum%20circuit%20%24C%5E%2A%24-algebra%20net%2C%20which%20provides%20a%0Aconnection%20between%20%24C%5E%2A%24-algebra%20nets%20proposed%20in%20classical%20machine%20learning%0Aand%20quantum%20circuits.%20Using%20%24C%5E%2A%24-algebra%2C%20a%20generalization%20of%20the%20space%20of%0Acomplex%20numbers%2C%20we%20can%20represent%20quantum%20gates%20as%20weight%20parameters%20of%20a%0Aneural%20network.%20By%20introducing%20additional%20parameters%2C%20we%20can%20induce%20interaction%0Aamong%20multiple%20circuits%20constructed%20by%20quantum%20gates.%20This%20interaction%20enables%0Athe%20circuits%20to%20share%20information%20among%20them%2C%20which%20contributes%20to%20improved%0Ageneralization%20performance%20in%20machine%20learning%20tasks.%20As%20an%20application%2C%20we%0Apropose%20to%20use%20the%20quantum%20circuit%20%24C%5E%2A%24-algebra%20net%20to%20encode%20classical%20data%0Ainto%20quantum%20states%2C%20which%20enables%20us%20to%20integrate%20classical%20data%20into%20quantum%0Aalgorithms.%20Numerical%20results%20demonstrate%20that%20the%20interaction%20among%20circuits%0Aimproves%20performance%20significantly%20in%20image%20classification%2C%20and%20encoded%20data%20by%0Athe%20quantum%20circuit%20%24C%5E%2A%24-algebra%20net%20are%20useful%20for%20downstream%20quantum%20machine%0Alearning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06218v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Circuit%20%24C%5E%2A%24-algebra%20Net&entry.906535625=Yuka%20Hashimoto%20and%20Ryuichiro%20Hataya&entry.1292438233=%20%20This%20paper%20introduces%20quantum%20circuit%20%24C%5E%2A%24-algebra%20net%2C%20which%20provides%20a%0Aconnection%20between%20%24C%5E%2A%24-algebra%20nets%20proposed%20in%20classical%20machine%20learning%0Aand%20quantum%20circuits.%20Using%20%24C%5E%2A%24-algebra%2C%20a%20generalization%20of%20the%20space%20of%0Acomplex%20numbers%2C%20we%20can%20represent%20quantum%20gates%20as%20weight%20parameters%20of%20a%0Aneural%20network.%20By%20introducing%20additional%20parameters%2C%20we%20can%20induce%20interaction%0Aamong%20multiple%20circuits%20constructed%20by%20quantum%20gates.%20This%20interaction%20enables%0Athe%20circuits%20to%20share%20information%20among%20them%2C%20which%20contributes%20to%20improved%0Ageneralization%20performance%20in%20machine%20learning%20tasks.%20As%20an%20application%2C%20we%0Apropose%20to%20use%20the%20quantum%20circuit%20%24C%5E%2A%24-algebra%20net%20to%20encode%20classical%20data%0Ainto%20quantum%20states%2C%20which%20enables%20us%20to%20integrate%20classical%20data%20into%20quantum%0Aalgorithms.%20Numerical%20results%20demonstrate%20that%20the%20interaction%20among%20circuits%0Aimproves%20performance%20significantly%20in%20image%20classification%2C%20and%20encoded%20data%20by%0Athe%20quantum%20circuit%20%24C%5E%2A%24-algebra%20net%20are%20useful%20for%20downstream%20quantum%20machine%0Alearning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06218v1&entry.124074799=Read"},
{"title": "Exploring the Potential of Large Foundation Models for Open-Vocabulary\n  HOI Detection", "author": "Ting Lei and Shaofeng Yin and Yang Liu", "abstract": "  Open-vocabulary human-object interaction (HOI) detection, which is concerned\nwith the problem of detecting novel HOIs guided by natural language, is crucial\nfor understanding human-centric scenes. However, prior zero-shot HOI detectors\noften employ the same levels of feature maps to model HOIs with varying\ndistances, leading to suboptimal performance in scenes containing human-object\npairs with a wide range of distances. In addition, these detectors primarily\nrely on category names and overlook the rich contextual information that\nlanguage can provide, which is essential for capturing open vocabulary concepts\nthat are typically rare and not well-represented by category names alone. In\nthis paper, we introduce a novel end-to-end open vocabulary HOI detection\nframework with conditional multi-level decoding and fine-grained semantic\nenhancement (CMD-SE), harnessing the potential of Visual-Language Models\n(VLMs). Specifically, we propose to model human-object pairs with different\ndistances with different levels of feature maps by incorporating a soft\nconstraint during the bipartite matching process. Furthermore, by leveraging\nlarge language models (LLMs) such as GPT models, we exploit their extensive\nworld knowledge to generate descriptions of human body part states for various\ninteractions. Then we integrate the generalizable and fine-grained semantics of\nhuman body parts to improve interaction recognition. Experimental results on\ntwo datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method\nachieves state-of-the-art results in open vocabulary HOI detection. The code\nand models are available at https://github.com/ltttpku/CMD-SE-release.\n", "link": "http://arxiv.org/abs/2404.06194v1", "date": "2024-04-09", "relevancy": 1.6398, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5409}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Potential%20of%20Large%20Foundation%20Models%20for%20Open-Vocabulary%0A%20%20HOI%20Detection&body=Title%3A%20Exploring%20the%20Potential%20of%20Large%20Foundation%20Models%20for%20Open-Vocabulary%0A%20%20HOI%20Detection%0AAuthor%3A%20Ting%20Lei%20and%20Shaofeng%20Yin%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Open-vocabulary%20human-object%20interaction%20%28HOI%29%20detection%2C%20which%20is%20concerned%0Awith%20the%20problem%20of%20detecting%20novel%20HOIs%20guided%20by%20natural%20language%2C%20is%20crucial%0Afor%20understanding%20human-centric%20scenes.%20However%2C%20prior%20zero-shot%20HOI%20detectors%0Aoften%20employ%20the%20same%20levels%20of%20feature%20maps%20to%20model%20HOIs%20with%20varying%0Adistances%2C%20leading%20to%20suboptimal%20performance%20in%20scenes%20containing%20human-object%0Apairs%20with%20a%20wide%20range%20of%20distances.%20In%20addition%2C%20these%20detectors%20primarily%0Arely%20on%20category%20names%20and%20overlook%20the%20rich%20contextual%20information%20that%0Alanguage%20can%20provide%2C%20which%20is%20essential%20for%20capturing%20open%20vocabulary%20concepts%0Athat%20are%20typically%20rare%20and%20not%20well-represented%20by%20category%20names%20alone.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20end-to-end%20open%20vocabulary%20HOI%20detection%0Aframework%20with%20conditional%20multi-level%20decoding%20and%20fine-grained%20semantic%0Aenhancement%20%28CMD-SE%29%2C%20harnessing%20the%20potential%20of%20Visual-Language%20Models%0A%28VLMs%29.%20Specifically%2C%20we%20propose%20to%20model%20human-object%20pairs%20with%20different%0Adistances%20with%20different%20levels%20of%20feature%20maps%20by%20incorporating%20a%20soft%0Aconstraint%20during%20the%20bipartite%20matching%20process.%20Furthermore%2C%20by%20leveraging%0Alarge%20language%20models%20%28LLMs%29%20such%20as%20GPT%20models%2C%20we%20exploit%20their%20extensive%0Aworld%20knowledge%20to%20generate%20descriptions%20of%20human%20body%20part%20states%20for%20various%0Ainteractions.%20Then%20we%20integrate%20the%20generalizable%20and%20fine-grained%20semantics%20of%0Ahuman%20body%20parts%20to%20improve%20interaction%20recognition.%20Experimental%20results%20on%0Atwo%20datasets%2C%20SWIG-HOI%20and%20HICO-DET%2C%20demonstrate%20that%20our%20proposed%20method%0Aachieves%20state-of-the-art%20results%20in%20open%20vocabulary%20HOI%20detection.%20The%20code%0Aand%20models%20are%20available%20at%20https%3A//github.com/ltttpku/CMD-SE-release.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06194v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Potential%20of%20Large%20Foundation%20Models%20for%20Open-Vocabulary%0A%20%20HOI%20Detection&entry.906535625=Ting%20Lei%20and%20Shaofeng%20Yin%20and%20Yang%20Liu&entry.1292438233=%20%20Open-vocabulary%20human-object%20interaction%20%28HOI%29%20detection%2C%20which%20is%20concerned%0Awith%20the%20problem%20of%20detecting%20novel%20HOIs%20guided%20by%20natural%20language%2C%20is%20crucial%0Afor%20understanding%20human-centric%20scenes.%20However%2C%20prior%20zero-shot%20HOI%20detectors%0Aoften%20employ%20the%20same%20levels%20of%20feature%20maps%20to%20model%20HOIs%20with%20varying%0Adistances%2C%20leading%20to%20suboptimal%20performance%20in%20scenes%20containing%20human-object%0Apairs%20with%20a%20wide%20range%20of%20distances.%20In%20addition%2C%20these%20detectors%20primarily%0Arely%20on%20category%20names%20and%20overlook%20the%20rich%20contextual%20information%20that%0Alanguage%20can%20provide%2C%20which%20is%20essential%20for%20capturing%20open%20vocabulary%20concepts%0Athat%20are%20typically%20rare%20and%20not%20well-represented%20by%20category%20names%20alone.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20end-to-end%20open%20vocabulary%20HOI%20detection%0Aframework%20with%20conditional%20multi-level%20decoding%20and%20fine-grained%20semantic%0Aenhancement%20%28CMD-SE%29%2C%20harnessing%20the%20potential%20of%20Visual-Language%20Models%0A%28VLMs%29.%20Specifically%2C%20we%20propose%20to%20model%20human-object%20pairs%20with%20different%0Adistances%20with%20different%20levels%20of%20feature%20maps%20by%20incorporating%20a%20soft%0Aconstraint%20during%20the%20bipartite%20matching%20process.%20Furthermore%2C%20by%20leveraging%0Alarge%20language%20models%20%28LLMs%29%20such%20as%20GPT%20models%2C%20we%20exploit%20their%20extensive%0Aworld%20knowledge%20to%20generate%20descriptions%20of%20human%20body%20part%20states%20for%20various%0Ainteractions.%20Then%20we%20integrate%20the%20generalizable%20and%20fine-grained%20semantics%20of%0Ahuman%20body%20parts%20to%20improve%20interaction%20recognition.%20Experimental%20results%20on%0Atwo%20datasets%2C%20SWIG-HOI%20and%20HICO-DET%2C%20demonstrate%20that%20our%20proposed%20method%0Aachieves%20state-of-the-art%20results%20in%20open%20vocabulary%20HOI%20detection.%20The%20code%0Aand%20models%20are%20available%20at%20https%3A//github.com/ltttpku/CMD-SE-release.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06194v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


